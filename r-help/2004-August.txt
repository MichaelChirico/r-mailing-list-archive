From MSchwartz at MedAnalytics.com  Sun Aug  1 01:42:24 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Sat, 31 Jul 2004 18:42:24 -0500
Subject: [R] pairwise difference operator
In-Reply-To: <1091237331.3940.71.camel@localhost.localdomain>
References: <1091230252.3702.39.camel@localhost.localdomain>
	<1091237331.3940.71.camel@localhost.localdomain>
Message-ID: <1091317344.3940.103.camel@localhost.localdomain>

On Fri, 2004-07-30 at 20:28, Marc Schwartz wrote:
> On Fri, 2004-07-30 at 18:30, Adaikalavan Ramasamy wrote:
> > There was a BioConductor thread today where the poster wanted to find
> > pairwise difference between columns of a matrix. I suggested the slow
> > solution below, hoping that someone might suggest a faster and/or more
> > elegant solution, but no other response.
> > 
> > I tried unsuccessfully with the apply() family. Searching the mailing
> > list was not very fruitful either. The closest I got to was a cryptic
> > chunk of code in pairwise.table().
> > 
> > Since I do use something similar myself occasionally, I am hoping
> > someone from the R-help list can suggest alternatives or past threads.
> > Thank you.

<snip>

In follow up to the posts on this last night, I created an updated
version of my function (though I will point out that Gabor's is faster,
as I will show below).

I realized that using the combinations() function had a potential
limitation, which is the limits of R's recursion depth, as Greg mentions
in the help for the function. It will require an adjustment when the
number of columns is about 45.

Thus, I modified the creation of the column combinations as noted below.
I also added some code to verify the input data type and to ensure that
the resultant structures remain matrices in the case of an input matrix
with ncol = 2, in which case, this function is of course, overkill.

Thus:

 pairwise.diffs <- function(x)
{
  stopifnot(is.matrix(x))

  # create column combination pairs
  prs <- cbind(rep(1:ncol(x), each = ncol(x)), 1:ncol(x))
  col.diffs <- prs[prs[, 1] < prs[, 2], , drop = FALSE]

  # do pairwise differences 
  result <- x[, col.diffs[, 1]] - x[, col.diffs[, 2], drop = FALSE]

  # set colnames
  if(is.null(colnames(x)))
    colnames(x) <- 1:ncol(x)

  colnames(result) <- paste(colnames(x)[col.diffs[, 1]], ".vs.", 
                            colnames(x)[col.diffs[, 2]], sep = "")
  result
}


Now to performance. I created a large 1,000 column matrix:

mat <- matrix(sample(100, 10000, replace = TRUE), ncol = 1000)
colnames(mat) <- 1:1000

> str(mat)
 int [1:10, 1:1000] 48 23 26 22 69 64 2 13 13 69 ...
 - attr(*, "dimnames")=List of 2
  ..$ : NULL
  ..$ : chr [1:1000] "1" "2" "3" "4" ...


Timing:

> gc();system.time(m <- pairwise.diffs(mat))
          used (Mb) gc trigger  (Mb)
Ncells 1541241 41.2    3094291  82.7
Vcells 7139074 54.5   17257300 131.7
[1] 1.14 0.19 1.39 0.00 0.00


> gc();system.time(g <- do.call("cbind", sapply(2:ncol(mat), 
                                    f, mat)))
          used (Mb) gc trigger  (Mb)
Ncells 1541241 41.2    3094291  82.7
Vcells 7139074 54.5   17257300 131.7
[1] 0.81 0.02 0.92 0.00 0.00


Comparisons:

> str(m)
 int [1:10, 1:499500] -47 -43 -35 -29 15 33 -53 -36 -17 57 ...
 - attr(*, "dimnames")=List of 2
  ..$ : NULL
  ..$ : chr [1:499500] "1.vs.2" "1.vs.3" "1.vs.4" "1.vs.5" ...


> str(g)
 int [1:10, 1:499500] -47 -43 -35 -29 15 33 -53 -36 -17 57 ...
 - attr(*, "dimnames")=List of 2
  ..$ : NULL
  ..$ : chr [1:499500] "1-2" "1-3" "1-4" "1-5" ...


> table(m == g)
 
   TRUE
4995000


HTH,

Marc Schwartz



From kjetil at acelerate.com  Sun Aug  1 02:50:34 2004
From: kjetil at acelerate.com (Kjetil Halvorsen)
Date: Sat, 31 Jul 2004 20:50:34 -0400
Subject: [R] smooth.spline
References: <410C1119.9060201@cs.ucr.edu>
Message-ID: <410C3E5A.6070807@acelerate.com>

look at packages
splines
mgcv

and a lot of others. To see how it is implemented, look a the references in
the help pages and the source code.

Kjetil Halvorsen


chuanjun zhang wrote:
> Dear Friends,
> Is there anybody know where I can get the code which implement the 
> smooth.spline function in R?
> I have to know how the smooth.spline is implemented.
> Thanks a lot.
> Best.
> Chuanjun
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From yanyu at CS.UCLA.EDU  Sun Aug  1 08:05:11 2004
From: yanyu at CS.UCLA.EDU (Yan Yu)
Date: Sat, 31 Jul 2004 23:05:11 -0700 (PDT)
Subject: [R] locfit
Message-ID: <Pine.GSO.4.58.0407312254310.24396@panther.cs.ucla.edu>

Hi, I have a Q on locfit,
  E.g., I have an input data set of the form {(x, y, v)} , (x, y) is the
location in a 2D space, v is the value at that location.
I am wondering for the output of locfit(), if a given point (x, y) is in
the  input data set, is the value at  (x, y) going to be the exactly same
as the input? or it depends on the local fitting function?
does my Q make sense?

Thanks a lot in advance,
yan



From ripley at stats.ox.ac.uk  Sun Aug  1 09:26:28 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 1 Aug 2004 08:26:28 +0100 (BST)
Subject: [R] smooth.spline
In-Reply-To: <410C3E5A.6070807@acelerate.com>
Message-ID: <Pine.LNX.4.44.0408010823480.16672-100000@gannet.stats>

On Sat, 31 Jul 2004, Kjetil Halvorsen wrote:

> look at packages
> splines
> mgcv

However, smooth.spline is in package stats.

> and a lot of others. To see how it is implemented, look a the references in
> the help pages and the source code.
> 
> Kjetil Halvorsen
> 
> 
> chuanjun zhang wrote:
> > Dear Friends,
> > Is there anybody know where I can get the code which implement the 
> > smooth.spline function in R?
> > I have to know how the smooth.spline is implemented.

Look in the source code, in files

src/library/stats/R/smspline.R
src/library/stats/src/qsbart.f
src/library/stats/src/sbart.c

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sun Aug  1 09:28:45 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 1 Aug 2004 08:28:45 +0100 (BST)
Subject: [R] locfit
In-Reply-To: <Pine.GSO.4.58.0407312254310.24396@panther.cs.ucla.edu>
Message-ID: <Pine.LNX.4.44.0408010826560.16672-100000@gannet.stats>

The short answer is `no', assuming you are talking about the function in 
the CRAN package called `locfit' (you did not say).

locfit has many options, but all do local smoothing, not interpolation.

On Sat, 31 Jul 2004, Yan Yu wrote:

> Hi, I have a Q on locfit,
>   E.g., I have an input data set of the form {(x, y, v)} , (x, y) is the
> location in a 2D space, v is the value at that location.
> I am wondering for the output of locfit(), if a given point (x, y) is in
> the  input data set, is the value at  (x, y) going to be the exactly same
> as the input? or it depends on the local fitting function?
> does my Q make sense?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From aleid2001 at yahoo.com  Sun Aug  1 17:09:49 2004
From: aleid2001 at yahoo.com (=?iso-8859-1?q?=20?=)
Date: Sun, 1 Aug 2004 16:09:49 +0100 (BST)
Subject: [R] PQL
Message-ID: <20040801150949.96683.qmail@web52807.mail.yahoo.com>

Dear Friends,

I need help in R cods for fitting the salamander
mating data using PQL, where this data include tow
random crossed random effects.

    BEST REGRADE

E-mail:aleid2001 at yahoo.com



From rab at nauticom.net  Sun Aug  1 18:14:39 2004
From: rab at nauticom.net (rab)
Date: Sun, 01 Aug 2004 12:14:39 -0400
Subject: [R] Strange Problem with "proj" and "aov" for split-plot analysis
	output
Message-ID: <410D16EF.2020005@nauticom.net>

I'm using R 1.8.1 on Red Hat Linux 9. I've worked out the linear model 
decomposition by hand. Using "aov" with "Error" to fit a split-plot 
model, I get very different results depending on whether I use the 
"data" argument for "aov" or not. When I use the "data" argument, the 
ANOVA table from "summary" is correct but the results "proj" are 
comletely wrong (in particular, the residuals for both strata). The same 
with the results from "model.tables" for effects and even means. 
However, if I attach the data frame and I don't use the "data" argument 
with "aov", the results for everything appear to be correct and agrees 
completely with the manual decomposition. (The sums of squares based on 
the components in the decomposition table agree completely with the 
"summary" of the "aov" object.) Is this a known problem? I also tried 
this on the Montana Rweb site and obtained the same incorrect results 
when usign the "data" argument. I've checked to make sure that the 
vector object names are not used outside of the data frame for the 
experiment data on my computer. There is no way this could happen when 
using Rweb.

Rick B.

Here are the details:

# here is the experiment data
 > choco.split.04
     subject brand   ctime type
1  christian     h 104.000   mc
3  christina     h  33.500   mc
5       ERIN     h  46.000   mc
7     gautam     h  25.295   mc
11       joe     h  75.000   mc
15       Lei     h  44.500   mc
9      helen     n  42.500   mc
13  jonathon     n  55.500   mc
17     pablo     n  47.000   mc
19     purin     n  97.000   mc
21     scott     n  85.000   mc
23     vince     n  65.850   mc
2  christian     h 168.000   wc
4  christina     h  37.500   wc
6       ERIN     h  45.500   wc
8     gautam     h  28.860   wc
12       joe     h  70.000   wc
16       Lei     h  52.500   wc
10     helen     n  57.500   wc
14  jonathon     n  46.000   wc
18     pablo     n  58.000   wc
20     purin     n 113.000   wc
22     scott     n  97.500   wc
24     vince     n  89.650   wc
# data as an R object
choco.split.04 <-
structure(list(subject = structure(c(1, 2, 3, 4, 6, 8, 5, 7,
9, 10, 11, 12, 1, 2, 3, 4, 6, 8, 5, 7, 9, 10, 11, 12), class = "factor", 
.Label = c("christian",
"christina", "ERIN", "gautam", "helen", "joe", "jonathon", "Lei",
"pablo", "purin", "scott", "vince")), brand = structure(c(1,
1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2,
2, 2), class = "factor", .Label = c("h", "n")), ctime = c(104,
33.5, 46, 25.295, 75, 44.5, 42.5, 55.5, 47, 97, 85, 65.85, 168,
37.5, 45.5, 28.86, 70, 52.5, 57.5, 46, 58, 113, 97.5, 89.65),
    type = structure(c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,
    2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2), class = "factor", .Label = c("mc",
    "wc"))), .Names = c("subject", "brand", "ctime", "type"), row.names 
= c("1",
"3", "5", "7", "11", "15", "9", "13", "17", "19", "21", "23",
"2", "4", "6", "8", "12", "16", "10", "14", "18", "20", "22",
"24"), class = "data.frame")
# split-plot analysis - using "aov" with argument "data"
 > summary(aov.split.04 <- 
aov(ctime~brand*type+Error(subject),data=choco.split.04))
 
Error: subject
          Df  Sum Sq Mean Sq F value Pr(>F)
brand      1   639.1   639.1  0.2972 0.5976
Residuals 10 21503.3  2150.3
 
Error: Within
           Df  Sum Sq Mean Sq F value  Pr(>F)
type        1  850.43  850.43  4.3326 0.06404 .
brand:type  1    1.16    1.16  0.0059 0.94037
Residuals  10 1962.86  196.29
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1

# here is the "proj" output - and it's all completely wrong

 > proj(aov.split.04)
(Intercept) :
   (Intercept)
1     66.04813
3     66.04813
5     66.04813
7     66.04813
11    66.04813
15    66.04813
9     66.04813
13    66.04813
17    66.04813
19    66.04813
21    66.04813
23    66.04813
2     66.04813
4     66.04813
6     66.04813
8     66.04813
12    66.04813
16    66.04813
10    66.04813
14    66.04813
18    66.04813
20    66.04813
22    66.04813
24    66.04813
attr(,"df")
attr(,"df")$df
(Intercept)
          1
 
attr(,"onedf")
attr(,"onedf")$onedf
[1] FALSE
 
attr(,"factors")
attr(,"factors")$"(Intercept)"
[1] "(Intercept)"
 
 
subject :
         brand  Residuals
1  -1.15279697 -50.326456
3  -1.49365647 -53.795729
5   7.94608934  -2.679088
7  -9.57184562   9.110149
11  8.31691964  31.336764
15  0.08387137  -8.782543
9  11.06119883  45.752063
13 -7.01978726  25.900087
17 -0.10587652   3.956638
19 -0.09918658   3.706633
21 -0.09353078   3.495274
23 -0.08867137   3.313676
2  -2.17537547  88.387613
4  -1.83451597  -0.519157
6  11.33663826 -52.337849
8  -4.60767598   7.731579
12 -6.06482008 -35.264686
16 -0.08387137   8.782543
10  1.75243781 -10.962735
14 -6.49281005  -2.332554
18  0.10587652  -3.956638
20  0.09918658  -3.706633
22  0.09353078  -3.495274
24  0.08867137  -3.313676
attr(,"df")
attr(,"df")$df
    brand Residuals
        1        10
 
attr(,"onedf")
attr(,"onedf")$onedf
[1] FALSE
 
attr(,"factors")
attr(,"factors")$brand
[1] "brand"
 
attr(,"factors")$Residuals
[1] "subject"
 
 
Within :
            type  brand:type  Residuals
1    1.396984343  0.04496907   3.011473
3    2.938588912  0.05656711   2.953101
5    3.145124762  0.05812095   2.945280
7    3.247625778  0.05889210   2.941399
11  -3.328231709 -0.22912276   5.730411
15  -2.538082170 -0.31105255 -25.391330
9    4.266448221  0.08884555   3.630619
13  -3.992792267 -0.27283438   3.666296
17 -10.936223358 -0.46063495   6.604985
19  -9.099857196  0.13554870  10.373726
21 -12.073986774  0.35505329  -3.922691
23 -12.879482604  0.38872464  -8.547527
2    6.021798052  0.07976318   2.836355
4    4.480193482  0.06816515   2.894728
6    4.273657632  0.06661130   2.902548
8    4.171156617  0.06584015   2.906430
12  -1.324148016 -0.18585237   6.575126
16   5.418913496  0.02243897 -28.523763
10   3.152334173  0.03588670   2.217210
14   9.399714308  0.30761541   3.257817
18   5.342284161 -0.47550361  -1.871322
20  -1.793920282  0.15803004  -5.774710
22   0.713084037 -0.03900621   2.788084
24  -0.001183597 -0.01706548   5.795756
attr(,"df")
attr(,"df")$df
      type brand:type  Residuals
         1          1         10
 
attr(,"onedf")
attr(,"onedf")$onedf
[1] FALSE
 
attr(,"factors")
attr(,"factors")$type
[1] "type"
 
attr(,"factors")$"brand:type"
[1] "brand" "type"
 
attr(,"factors")$Residuals
[1] "subject" "Within"

# here is the "model.tables" output - and it's all completely wrong

 > model.tables(aov.split.04)
Tables of effects
 
 brand
           h        n
     0.05825 -0.05825
rep 12.00000 12.00000
 
 type
        mc     wc
    -3.321  3.321
rep 12.000 12.000
 
 brand:type
     type
brand mc     wc
  h   -0.054  0.019
  rep  6.000  6.000
  n    0.039 -0.005
  rep  6.000  6.000

# here is "aov" without "data" argument - looks the same as before
 > attach(choco.split.04)
 > summary(aov.split.04 <- aov(ctime~brand*type+Error(subject)))
 
Error: subject
          Df  Sum Sq Mean Sq F value Pr(>F)
brand      1   639.1   639.1  0.2972 0.5976
Residuals 10 21503.3  2150.3
 
Error: Within
           Df  Sum Sq Mean Sq F value  Pr(>F)
type        1  850.43  850.43  4.3326 0.06404 .
brand:type  1    1.16    1.16  0.0059 0.94037
Residuals  10 1962.86  196.29
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1


# here is the "proj" output and it's correct
# as shown by manual decomposition
 > proj(aov.split.04)
(Intercept) :
   (Intercept)
1     66.04813
2     66.04813
3     66.04813
4     66.04813
5     66.04813
6     66.04813
7     66.04813
8     66.04813
9     66.04813
10    66.04813
11    66.04813
12    66.04813
13    66.04813
14    66.04813
15    66.04813
16    66.04813
17    66.04813
18    66.04813
19    66.04813
20    66.04813
21    66.04813
22    66.04813
23    66.04813
24    66.04813
attr(,"df")
attr(,"df")$df
(Intercept)
          1
 
attr(,"onedf")
attr(,"onedf")$onedf
[1] FALSE
 
attr(,"factors")
attr(,"factors")$"(Intercept)"
[1] "(Intercept)"
 
 
subject :
       brand  Residuals
1  -5.160208  75.112083
2  -5.160208 -25.387917
3  -5.160208 -15.137917
4  -5.160208 -33.810417
5  -5.160208  11.612083
6  -5.160208 -12.387917
7   5.160208 -21.208333
8   5.160208 -20.458333
9   5.160208 -18.708333
10  5.160208  33.791667
11  5.160208  20.041667
12  5.160208   6.541667
13 -5.160208  75.112083
14 -5.160208 -25.387917
15 -5.160208 -15.137917
16 -5.160208 -33.810417
17 -5.160208  11.612083
18 -5.160208 -12.387917
19  5.160208 -21.208333
20  5.160208 -20.458333
21  5.160208 -18.708333
22  5.160208  33.791667
23  5.160208  20.041667
24  5.160208   6.541667
attr(,"df")
attr(,"df")$df
    brand Residuals
        1        10
 
attr(,"onedf")
attr(,"onedf")$onedf
[1] FALSE
 
attr(,"factors")
attr(,"factors")$brand
[1] "brand"
 
attr(,"factors")$Residuals
[1] "subject"
 
 
Within :
        type brand:type   Residuals
1  -5.952708  -0.219375 -25.8279167
2  -5.952708  -0.219375   4.1720833
3  -5.952708  -0.219375   6.4220833
4  -5.952708  -0.219375   4.3895833
5  -5.952708  -0.219375   8.6720833
6  -5.952708  -0.219375   2.1720833
7  -5.952708   0.219375  -1.7666667
8  -5.952708   0.219375  10.4833333
9  -5.952708   0.219375   0.2333333
10 -5.952708   0.219375  -2.2666667
11 -5.952708   0.219375  -0.5166667
12 -5.952708   0.219375  -6.1666667
13  5.952708   0.219375  25.8279167
14  5.952708   0.219375  -4.1720833
15  5.952708   0.219375  -6.4220833
16  5.952708   0.219375  -4.3895833
17  5.952708   0.219375  -8.6720833
18  5.952708   0.219375  -2.1720833
19  5.952708  -0.219375   1.7666667
20  5.952708  -0.219375 -10.4833333
21  5.952708  -0.219375  -0.2333333
22  5.952708  -0.219375   2.2666667
23  5.952708  -0.219375   0.5166667
24  5.952708  -0.219375   6.1666667
attr(,"df")
attr(,"df")$df
      type brand:type  Residuals
         1          1         10
 
attr(,"onedf")
attr(,"onedf")$onedf
[1] FALSE
 
attr(,"factors")
attr(,"factors")$type
[1] "type"
 
attr(,"factors")$"brand:type"
[1] "brand" "type"
 
attr(,"factors")$Residuals
[1] "subject" "Within"

# here is "model.tables" output and it's correct
 > model.tables(aov.split.04)
Tables of effects
 
 brand
        h     n
    -5.16  5.16
rep 12.00 12.00
 
 type
        mc     wc
    -5.953  5.953
rep 12.000 12.000
 
 brand:type
     type
brand mc     wc
  h   -0.219  0.219
  rep  6.000  6.000
  n    0.219 -0.219
  rep  6.000  6.000

# version information
 > R.Version()
$platform
[1] "i686-pc-linux-gnu"
 
$arch
[1] "i686"
 
$os
[1] "linux-gnu"
 
$system
[1] "i686, linux-gnu"
 
$status
[1] ""
 
$major
[1] "1"
 
$minor
[1] "8.1"
 
$year
[1] "2003"
 
$month
[1] "11"
 
$day
[1] "21"
 
$language
[1] "R"



From jfbrennan at rogers.com  Sun Aug  1 19:48:00 2004
From: jfbrennan at rogers.com (Jim Brennan)
Date: Sun, 1 Aug 2004 13:48:00 -0400
Subject: [R] Strange Problem with "proj" and "aov" for split-plot
	analysisoutput
References: <410D16EF.2020005@nauticom.net>
Message-ID: <000801c477ef$b0f5bbc0$3b8ac445@slnt.phub.net.cable.rogers.com>

For what it is worth I have repeated this in R version 1.9 with the same
results.

Looks like the only the calls are different.

Call:
aov(formula = ctime ~ brand * type + Error(subject), data = choco.split.04)

and with data fram attached
Call:
aov(formula = ctime ~ brand * type + Error(subject))

and for some reason giving different results when put into proj

Jim
----- Original Message -----
From: "rab" <rab at nauticom.net>
To: <r-help at stat.math.ethz.ch>
Sent: Sunday, August 01, 2004 12:14 PM
Subject: [R] Strange Problem with "proj" and "aov" for split-plot
analysisoutput


> I'm using R 1.8.1 on Red Hat Linux 9. I've worked out the linear model
> decomposition by hand. Using "aov" with "Error" to fit a split-plot
> model, I get very different results depending on whether I use the
> "data" argument for "aov" or not. When I use the "data" argument, the
> ANOVA table from "summary" is correct but the results "proj" are
> comletely wrong (in particular, the residuals for both strata). The same
> with the results from "model.tables" for effects and even means.
> However, if I attach the data frame and I don't use the "data" argument
> with "aov", the results for everything appear to be correct and agrees
> completely with the manual decomposition. (The sums of squares based on
> the components in the decomposition table agree completely with the
> "summary" of the "aov" object.) Is this a known problem? I also tried
> this on the Montana Rweb site and obtained the same incorrect results
> when usign the "data" argument. I've checked to make sure that the
> vector object names are not used outside of the data frame for the
> experiment data on my computer. There is no way this could happen when
> using Rweb.
>
> Rick B.
>
> Here are the details:
>
> # here is the experiment data
>  > choco.split.04
>      subject brand   ctime type
> 1  christian     h 104.000   mc
> 3  christina     h  33.500   mc
> 5       ERIN     h  46.000   mc
> 7     gautam     h  25.295   mc
> 11       joe     h  75.000   mc
> 15       Lei     h  44.500   mc
> 9      helen     n  42.500   mc
> 13  jonathon     n  55.500   mc
> 17     pablo     n  47.000   mc
> 19     purin     n  97.000   mc
> 21     scott     n  85.000   mc
> 23     vince     n  65.850   mc
> 2  christian     h 168.000   wc
> 4  christina     h  37.500   wc
> 6       ERIN     h  45.500   wc
> 8     gautam     h  28.860   wc
> 12       joe     h  70.000   wc
> 16       Lei     h  52.500   wc
> 10     helen     n  57.500   wc
> 14  jonathon     n  46.000   wc
> 18     pablo     n  58.000   wc
> 20     purin     n 113.000   wc
> 22     scott     n  97.500   wc
> 24     vince     n  89.650   wc
> # data as an R object
> choco.split.04 <-
> structure(list(subject = structure(c(1, 2, 3, 4, 6, 8, 5, 7,
> 9, 10, 11, 12, 1, 2, 3, 4, 6, 8, 5, 7, 9, 10, 11, 12), class = "factor",
> .Label = c("christian",
> "christina", "ERIN", "gautam", "helen", "joe", "jonathon", "Lei",
> "pablo", "purin", "scott", "vince")), brand = structure(c(1,
> 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2,
> 2, 2), class = "factor", .Label = c("h", "n")), ctime = c(104,
> 33.5, 46, 25.295, 75, 44.5, 42.5, 55.5, 47, 97, 85, 65.85, 168,
> 37.5, 45.5, 28.86, 70, 52.5, 57.5, 46, 58, 113, 97.5, 89.65),
>     type = structure(c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,
>     2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2), class = "factor", .Label = c("mc",
>     "wc"))), .Names = c("subject", "brand", "ctime", "type"), row.names
> = c("1",
> "3", "5", "7", "11", "15", "9", "13", "17", "19", "21", "23",
> "2", "4", "6", "8", "12", "16", "10", "14", "18", "20", "22",
> "24"), class = "data.frame")
> # split-plot analysis - using "aov" with argument "data"
>  > summary(aov.split.04 <-
> aov(ctime~brand*type+Error(subject),data=choco.split.04))
>
> Error: subject
>           Df  Sum Sq Mean Sq F value Pr(>F)
> brand      1   639.1   639.1  0.2972 0.5976
> Residuals 10 21503.3  2150.3
>
> Error: Within
>            Df  Sum Sq Mean Sq F value  Pr(>F)
> type        1  850.43  850.43  4.3326 0.06404 .
> brand:type  1    1.16    1.16  0.0059 0.94037
> Residuals  10 1962.86  196.29
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
>
> # here is the "proj" output - and it's all completely wrong
>
>  > proj(aov.split.04)
> (Intercept) :
>    (Intercept)
> 1     66.04813
> 3     66.04813
> 5     66.04813
> 7     66.04813
> 11    66.04813
> 15    66.04813
> 9     66.04813
> 13    66.04813
> 17    66.04813
> 19    66.04813
> 21    66.04813
> 23    66.04813
> 2     66.04813
> 4     66.04813
> 6     66.04813
> 8     66.04813
> 12    66.04813
> 16    66.04813
> 10    66.04813
> 14    66.04813
> 18    66.04813
> 20    66.04813
> 22    66.04813
> 24    66.04813
> attr(,"df")
> attr(,"df")$df
> (Intercept)
>           1
>
> attr(,"onedf")
> attr(,"onedf")$onedf
> [1] FALSE
>
> attr(,"factors")
> attr(,"factors")$"(Intercept)"
> [1] "(Intercept)"
>
>
> subject :
>          brand  Residuals
> 1  -1.15279697 -50.326456
> 3  -1.49365647 -53.795729
> 5   7.94608934  -2.679088
> 7  -9.57184562   9.110149
> 11  8.31691964  31.336764
> 15  0.08387137  -8.782543
> 9  11.06119883  45.752063
> 13 -7.01978726  25.900087
> 17 -0.10587652   3.956638
> 19 -0.09918658   3.706633
> 21 -0.09353078   3.495274
> 23 -0.08867137   3.313676
> 2  -2.17537547  88.387613
> 4  -1.83451597  -0.519157
> 6  11.33663826 -52.337849
> 8  -4.60767598   7.731579
> 12 -6.06482008 -35.264686
> 16 -0.08387137   8.782543
> 10  1.75243781 -10.962735
> 14 -6.49281005  -2.332554
> 18  0.10587652  -3.956638
> 20  0.09918658  -3.706633
> 22  0.09353078  -3.495274
> 24  0.08867137  -3.313676
> attr(,"df")
> attr(,"df")$df
>     brand Residuals
>         1        10
>
> attr(,"onedf")
> attr(,"onedf")$onedf
> [1] FALSE
>
> attr(,"factors")
> attr(,"factors")$brand
> [1] "brand"
>
> attr(,"factors")$Residuals
> [1] "subject"
>
>
> Within :
>             type  brand:type  Residuals
> 1    1.396984343  0.04496907   3.011473
> 3    2.938588912  0.05656711   2.953101
> 5    3.145124762  0.05812095   2.945280
> 7    3.247625778  0.05889210   2.941399
> 11  -3.328231709 -0.22912276   5.730411
> 15  -2.538082170 -0.31105255 -25.391330
> 9    4.266448221  0.08884555   3.630619
> 13  -3.992792267 -0.27283438   3.666296
> 17 -10.936223358 -0.46063495   6.604985
> 19  -9.099857196  0.13554870  10.373726
> 21 -12.073986774  0.35505329  -3.922691
> 23 -12.879482604  0.38872464  -8.547527
> 2    6.021798052  0.07976318   2.836355
> 4    4.480193482  0.06816515   2.894728
> 6    4.273657632  0.06661130   2.902548
> 8    4.171156617  0.06584015   2.906430
> 12  -1.324148016 -0.18585237   6.575126
> 16   5.418913496  0.02243897 -28.523763
> 10   3.152334173  0.03588670   2.217210
> 14   9.399714308  0.30761541   3.257817
> 18   5.342284161 -0.47550361  -1.871322
> 20  -1.793920282  0.15803004  -5.774710
> 22   0.713084037 -0.03900621   2.788084
> 24  -0.001183597 -0.01706548   5.795756
> attr(,"df")
> attr(,"df")$df
>       type brand:type  Residuals
>          1          1         10
>
> attr(,"onedf")
> attr(,"onedf")$onedf
> [1] FALSE
>
> attr(,"factors")
> attr(,"factors")$type
> [1] "type"
>
> attr(,"factors")$"brand:type"
> [1] "brand" "type"
>
> attr(,"factors")$Residuals
> [1] "subject" "Within"
>
> # here is the "model.tables" output - and it's all completely wrong
>
>  > model.tables(aov.split.04)
> Tables of effects
>
>  brand
>            h        n
>      0.05825 -0.05825
> rep 12.00000 12.00000
>
>  type
>         mc     wc
>     -3.321  3.321
> rep 12.000 12.000
>
>  brand:type
>      type
> brand mc     wc
>   h   -0.054  0.019
>   rep  6.000  6.000
>   n    0.039 -0.005
>   rep  6.000  6.000
>
> # here is "aov" without "data" argument - looks the same as before
>  > attach(choco.split.04)
>  > summary(aov.split.04 <- aov(ctime~brand*type+Error(subject)))
>
> Error: subject
>           Df  Sum Sq Mean Sq F value Pr(>F)
> brand      1   639.1   639.1  0.2972 0.5976
> Residuals 10 21503.3  2150.3
>
> Error: Within
>            Df  Sum Sq Mean Sq F value  Pr(>F)
> type        1  850.43  850.43  4.3326 0.06404 .
> brand:type  1    1.16    1.16  0.0059 0.94037
> Residuals  10 1962.86  196.29
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
>
>
> # here is the "proj" output and it's correct
> # as shown by manual decomposition
>  > proj(aov.split.04)
> (Intercept) :
>    (Intercept)
> 1     66.04813
> 2     66.04813
> 3     66.04813
> 4     66.04813
> 5     66.04813
> 6     66.04813
> 7     66.04813
> 8     66.04813
> 9     66.04813
> 10    66.04813
> 11    66.04813
> 12    66.04813
> 13    66.04813
> 14    66.04813
> 15    66.04813
> 16    66.04813
> 17    66.04813
> 18    66.04813
> 19    66.04813
> 20    66.04813
> 21    66.04813
> 22    66.04813
> 23    66.04813
> 24    66.04813
> attr(,"df")
> attr(,"df")$df
> (Intercept)
>           1
>
> attr(,"onedf")
> attr(,"onedf")$onedf
> [1] FALSE
>
> attr(,"factors")
> attr(,"factors")$"(Intercept)"
> [1] "(Intercept)"
>
>
> subject :
>        brand  Residuals
> 1  -5.160208  75.112083
> 2  -5.160208 -25.387917
> 3  -5.160208 -15.137917
> 4  -5.160208 -33.810417
> 5  -5.160208  11.612083
> 6  -5.160208 -12.387917
> 7   5.160208 -21.208333
> 8   5.160208 -20.458333
> 9   5.160208 -18.708333
> 10  5.160208  33.791667
> 11  5.160208  20.041667
> 12  5.160208   6.541667
> 13 -5.160208  75.112083
> 14 -5.160208 -25.387917
> 15 -5.160208 -15.137917
> 16 -5.160208 -33.810417
> 17 -5.160208  11.612083
> 18 -5.160208 -12.387917
> 19  5.160208 -21.208333
> 20  5.160208 -20.458333
> 21  5.160208 -18.708333
> 22  5.160208  33.791667
> 23  5.160208  20.041667
> 24  5.160208   6.541667
> attr(,"df")
> attr(,"df")$df
>     brand Residuals
>         1        10
>
> attr(,"onedf")
> attr(,"onedf")$onedf
> [1] FALSE
>
> attr(,"factors")
> attr(,"factors")$brand
> [1] "brand"
>
> attr(,"factors")$Residuals
> [1] "subject"
>
>
> Within :
>         type brand:type   Residuals
> 1  -5.952708  -0.219375 -25.8279167
> 2  -5.952708  -0.219375   4.1720833
> 3  -5.952708  -0.219375   6.4220833
> 4  -5.952708  -0.219375   4.3895833
> 5  -5.952708  -0.219375   8.6720833
> 6  -5.952708  -0.219375   2.1720833
> 7  -5.952708   0.219375  -1.7666667
> 8  -5.952708   0.219375  10.4833333
> 9  -5.952708   0.219375   0.2333333
> 10 -5.952708   0.219375  -2.2666667
> 11 -5.952708   0.219375  -0.5166667
> 12 -5.952708   0.219375  -6.1666667
> 13  5.952708   0.219375  25.8279167
> 14  5.952708   0.219375  -4.1720833
> 15  5.952708   0.219375  -6.4220833
> 16  5.952708   0.219375  -4.3895833
> 17  5.952708   0.219375  -8.6720833
> 18  5.952708   0.219375  -2.1720833
> 19  5.952708  -0.219375   1.7666667
> 20  5.952708  -0.219375 -10.4833333
> 21  5.952708  -0.219375  -0.2333333
> 22  5.952708  -0.219375   2.2666667
> 23  5.952708  -0.219375   0.5166667
> 24  5.952708  -0.219375   6.1666667
> attr(,"df")
> attr(,"df")$df
>       type brand:type  Residuals
>          1          1         10
>
> attr(,"onedf")
> attr(,"onedf")$onedf
> [1] FALSE
>
> attr(,"factors")
> attr(,"factors")$type
> [1] "type"
>
> attr(,"factors")$"brand:type"
> [1] "brand" "type"
>
> attr(,"factors")$Residuals
> [1] "subject" "Within"
>
> # here is "model.tables" output and it's correct
>  > model.tables(aov.split.04)
> Tables of effects
>
>  brand
>         h     n
>     -5.16  5.16
> rep 12.00 12.00
>
>  type
>         mc     wc
>     -5.953  5.953
> rep 12.000 12.000
>
>  brand:type
>      type
> brand mc     wc
>   h   -0.219  0.219
>   rep  6.000  6.000
>   n    0.219 -0.219
>   rep  6.000  6.000
>
> # version information
>  > R.Version()
> $platform
> [1] "i686-pc-linux-gnu"
>
> $arch
> [1] "i686"
>
> $os
> [1] "linux-gnu"
>
> $system
> [1] "i686, linux-gnu"
>
> $status
> [1] ""
>
> $major
> [1] "1"
>
> $minor
> [1] "8.1"
>
> $year
> [1] "2003"
>
> $month
> [1] "11"
>
> $day
> [1] "21"
>
> $language
> [1] "R"
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From uleopold at science.uva.nl  Sun Aug  1 20:11:13 2004
From: uleopold at science.uva.nl (Ulrich Leopold)
Date: 01 Aug 2004 20:11:13 +0200
Subject: [R] Strange Problem with "proj" and "aov" for split-plot
	analysis output
In-Reply-To: <410D16EF.2020005@nauticom.net>
References: <410D16EF.2020005@nauticom.net>
Message-ID: <1091383873.12331.16.camel@snowdon.science.uva.nl>

> # split-plot analysis - using "aov" with argument "data"
>  > summary(aov.split.04 <- 
> aov(ctime~brand*type+Error(subject),data=choco.split.04))

As far as I can see, maybe you should not use the redirect command in
this case and skip the 'data' argument then it should work:

summary(aov(ctime~brand*type+Error(subject)))

else the results from 'aov(..)' are just redirected to aov.split.04 but
not passed to standard output for 'summary(...)'. And it probably does
not have a data argument?

Maybe someone else could explain why?

Ulrich



From canty at math.mcmaster.ca  Sun Aug  1 21:02:47 2004
From: canty at math.mcmaster.ca (Angelo Canty)
Date: Sun, 1 Aug 2004 15:02:47 -0400 (EDT)
Subject: [R] Problem with RGui and MASS
Message-ID: <Pine.LNX.4.44.0408011444430.29048-100000@mathserv>

Hi,

Using R1.9.1 for Windows with Windows 2000 (also XP), I have come
across the following problem that I have not seen before and am at
a loss to explain.  If I create an object using rlm from library(MASS)
and then save the workspace, I cannot reopen it using RGUI by double 
clicking on the .RData file as I normally do.  If I double click on
the icon, R starts and then gives the following message in a pop-up 
dialog.

Fatal error: Unable to restore saved data in .RData

On the R console there is the further message

Error: object 'family' not found whilst loading namespace 'MASS'

This does not occur when I use RTerm to start R 1.9.1 nor does it
happen with RGui 1.8.1.  Even more puzzling (at least to me) is that
if I open RGui 1.9.1 and use load to load the workspace then it
works fine.  This is a bit of a pain however as I have a lot of
.RData files in different directories and double clicking on them
opens RGui with the appropriate working directory.

Has anyone observed this before or know what the problem might be?
Is it something that I did wrong (on two computers)? 

Here is a very simple example of the code I have used.

rm(list=ls(all=T)) # start with a blank slate
x <- 1:10
y <- rnorm(10, 1+2*x)
model.rlm1 <- rlm(y~x)
q(save=T)

Thanks for your help,
Angelo

-- 
------------------------------------------------------------------
|   Angelo J. Canty                Email: cantya at mcmaster.ca     |
|   Mathematics and Statistics     Phone: (905) 525-9140 x 27079 |
|   McMaster University            Fax  : (905) 522-0935         |
|   1280 Main St. W.                                             |
|   Hamilton ON L8S 4K1                                          |



From ripley at stats.ox.ac.uk  Sun Aug  1 21:11:02 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 1 Aug 2004 20:11:02 +0100 (BST)
Subject: [R] Problem with RGui and MASS
In-Reply-To: <Pine.LNX.4.44.0408011444430.29048-100000@mathserv>
Message-ID: <Pine.LNX.4.44.0408012008110.23891-100000@gannet.stats>

Please make sure you have the current MASS -- this was worked-around a 
while back (and has been mentioned here several times), and 
update.packages() will almost certainly get you an update (post 1.9.1).

The cause is that namespace stats is not loaded when .RData is accessed,
but it is once R is running.  It should happen with Rterm started in the
same directory as Rgui.

On Sun, 1 Aug 2004, Angelo Canty wrote:

> Hi,
> 
> Using R1.9.1 for Windows with Windows 2000 (also XP), I have come
> across the following problem that I have not seen before and am at
> a loss to explain.  If I create an object using rlm from library(MASS)
> and then save the workspace, I cannot reopen it using RGUI by double 
> clicking on the .RData file as I normally do.  If I double click on
> the icon, R starts and then gives the following message in a pop-up 
> dialog.
> 
> Fatal error: Unable to restore saved data in .RData
> 
> On the R console there is the further message
> 
> Error: object 'family' not found whilst loading namespace 'MASS'
> 
> This does not occur when I use RTerm to start R 1.9.1 nor does it
> happen with RGui 1.8.1.  Even more puzzling (at least to me) is that
> if I open RGui 1.9.1 and use load to load the workspace then it
> works fine.  This is a bit of a pain however as I have a lot of
> .RData files in different directories and double clicking on them
> opens RGui with the appropriate working directory.
> 
> Has anyone observed this before or know what the problem might be?
> Is it something that I did wrong (on two computers)? 
> 
> Here is a very simple example of the code I have used.
> 
> rm(list=ls(all=T)) # start with a blank slate
> x <- 1:10
> y <- rnorm(10, 1+2*x)
> model.rlm1 <- rlm(y~x)
> q(save=T)
> 
> Thanks for your help,
> Angelo
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From yanyu at CS.UCLA.EDU  Sun Aug  1 21:12:20 2004
From: yanyu at CS.UCLA.EDU (Yan Yu)
Date: Sun, 1 Aug 2004 12:12:20 -0700 (PDT)
Subject: [R] locfit
In-Reply-To: <Pine.LNX.4.44.0408010826560.16672-100000@gannet.stats>
References: <Pine.LNX.4.44.0408010826560.16672-100000@gannet.stats>
Message-ID: <Pine.GSO.4.58.0408011210260.26773@panther.cs.ucla.edu>

Got it, Thanks a LOT for the info!
yan

On Sun, 1 Aug 2004, Prof Brian Ripley wrote:

> The short answer is `no', assuming you are talking about the function in
> the CRAN package called `locfit' (you did not say).
yes, that is "locfit" package i am taking about, sorry that i forgot to
mention that..

> locfit has many options, but all do local smoothing, not interpolation.
>
> On Sat, 31 Jul 2004, Yan Yu wrote:
>
> > Hi, I have a Q on locfit,
> >   E.g., I have an input data set of the form {(x, y, v)} , (x, y) is the
> > location in a 2D space, v is the value at that location.
> > I am wondering for the output of locfit(), if a given point (x, y) is in
> > the  input data set, is the value at  (x, y) going to be the exactly same
> > as the input? or it depends on the local fitting function?
> > does my Q make sense?
>
>



From HDoran at air.org  Sun Aug  1 21:24:08 2004
From: HDoran at air.org (Doran, Harold)
Date: Sun, 1 Aug 2004 15:24:08 -0400
Subject: [R] (no subject)
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7404044CC9@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040801/1f9cda8f/attachment.pl

From HDoran at air.org  Sun Aug  1 21:25:43 2004
From: HDoran at air.org (Doran, Harold)
Date: Sun, 1 Aug 2004 15:25:43 -0400
Subject: [R] Creating dummy codes
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7404044CCA@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040801/4f0745b8/attachment.pl

From chzhang at cs.ucr.edu  Sun Aug  1 21:29:20 2004
From: chzhang at cs.ucr.edu (chuanjun zhang)
Date: Sun, 01 Aug 2004 12:29:20 -0700
Subject: [R] smooth.spline codes
In-Reply-To: <20040801150949.96683.qmail@web52807.mail.yahoo.com>
References: <20040801150949.96683.qmail@web52807.mail.yahoo.com>
Message-ID: <410D4490.3020305@cs.ucr.edu>

Dear Friends,
The calculation of  penalty criterion in smooth.spline is  " pen.crit <- 
sum(wbar * (ybar - fit$ty) * ybar) " , but I think it should be   "   
pen.crit <- sum(wbar * (ybar - fit$ty) * (ybar-fit$ty))   " right ? 
Why the integral of the fit$ty is not included ?
Thanks a lot .
Chuanjun



From rolf at math.unb.ca  Sun Aug  1 22:04:32 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Sun, 1 Aug 2004 17:04:32 -0300 (ADT)
Subject: [R] Creating dummy codes
Message-ID: <200408012004.i71K4WCA005308@erdos.math.unb.ca>


(1) Why ***ON EARTH*** do you want to do that?  Just make ``grade'' into
a factor and use that factor directly.

(2) If you insist on doing it your way, class.ind() from package nnet
will get you most of the way there.  You'll need to strip off the
first (grade 2) column, give the matrix the column names you want,
and then convert the matrix into a data frame.

Or you could just roll your own code --- it's a triviality to write.
(The class.ind() function is 7 simple lines of raw R.)

				cheers,

					Rolf Turner
					rolf at math.unb.ca

===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===
Original message:

> Is there an efficient way to create a series of dummy codes from a
> single variable? For example, I have a variable, ``grade'' = {2, ..., 12}.
> I want to create  k-1 dummy codes for grade such that grade 2 is the
> base (i.e, grade 2 = 0).
> 
> I am hoping that the new variables can be labeled as grade.3, grade.4
> etc. I'll then use
> 
> 	grade <- paste("grade.", 3:12, sep="")
> 
> in as.formula to build the model.



From ripley at stats.ox.ac.uk  Sun Aug  1 22:09:45 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 1 Aug 2004 21:09:45 +0100 (BST)
Subject: [R] Creating dummy codes
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F7404044CCA@dc1ex2.air.org>
Message-ID: <Pine.LNX.4.44.0408012104100.23984-100000@gannet.stats>

On Sun, 1 Aug 2004, Doran, Harold wrote:

> Is there an efficient way to create a series of dummy codes from a
> single variable? For example, I have a variable, `grade' = {2, ????,
> 12}. I want to create k-1 dummy codes for grade such that grade 2 is the
> base (i.e, grade 2 =0).

Yes, and that's what R does by default.

> I am hoping that the new variables can be labeled as grade.3, grade.4
> etc. I'll then use
> 
> grade <- paste("grade.", 3:12, sep="") in as.formula to build the model.

Why?  Just set the levels of your factor to 2:12 and R's default treatment 
contrasts do this for you.  To see this, just call model.matrix, as in

grade. <- as.factor(sample(2:12, 100, replace=T))
model.matrix(~grade.)[, -1]

but the R model fitting functions will do that for you.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Sun Aug  1 22:13:20 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 01 Aug 2004 22:13:20 +0200
Subject: [R] Creating dummy codes
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F7404044CCA@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F7404044CCA@dc1ex2.air.org>
Message-ID: <x2k6wiaa73.fsf@biostat.ku.dk>

"Doran, Harold" <HDoran at air.org> writes:

> Is there an efficient way to create a series of dummy codes from a single variable? For example, I have a variable, `grade' = {2, ????, 12}. I want to create  k-1 dummy codes for grade such that grade 2 is the base (i.e, grade 2 =0).
> 
> I am hoping that the new variables can be labeled as grade.3, grade.4 etc. I'll then  use 
> 
> grade <- paste("grade.", 3:12, sep="") in as.formula to build the model.
> 
> Thanks,
> 
> Harold
> 
> 
> 	[[alternative HTML version deleted]]

[unfortunately, at least to some of us, the above is only slightly
less unreadable...]

No, you don't want to do that. Instead, do

fgrade <- factor(grade)
lm(y ~ fgrade) # or glm or coxph or whatever

If you really want the dummy variables, look at

model.matrix(~fgrade)

(minus the first (Intercept) column)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From f.calboli at ucl.ac.uk  Sun Aug  1 22:30:13 2004
From: f.calboli at ucl.ac.uk (Federico Calboli)
Date: Sun, 01 Aug 2004 22:30:13 +0200
Subject: [R] phylogenetic trees calculation
Message-ID: <1091392213.23593.14.camel@badger.home>

Dear all,

I would like to ask you the following:

I have data about different manuscripts, together with data about the
presence/absence of copying errors, in the days when manuscript were
really manuscripts. I would ideally like to use the data to draw a
phylogenetic tree, so that I can infer which ms was copied from which.
The error presence/absence is coded in binary format. The plan is to use
a maximun parsimony tree approach. The data looks like this toy example:

        "ms1"   "ms2"   "ms3"   "ms4"
"err1"  0       1       0       0
"err2"  0       0       1       1
"err3"  1       1       0       0
"err4"  0       0       0       1
"err5"  1       1       1       0
"err6"  1       1       0       1
"err7"  0       1       0       0
"err8"  0       1       0       1

Additionally I have a vector of weights for the errors (and more info to
be possibly used). 

Does R have a set of fuctions of a library that will calculate the a
phylogenetic tree from such data? I installed "ape", but, despite
reading the docs, I cannot find a function that would calculate a tree
from data like mine (my sight may be getting worse though). Any
suggestion is welcome.

Regards,

Federico Calboli


-- 
Federico C. F. Calboli

Dipartimento di Biologia Evoluzionistica Sperimentale
Universit?? di Bologna
Via Selmi, 3
40126 Bologna - ITALY

Tel - +39 051 2094187
Fax - +39 051 2094286
f.calboli at ucl.ac.uk
fcalboli at alma.unibo.it



From oliver at imcs.marine.rutgers.edu  Mon Aug  2 00:35:26 2004
From: oliver at imcs.marine.rutgers.edu (Matthew Oliver)
Date: Sun, 1 Aug 2004 18:35:26 -0400 (EDT)
Subject: [R] Neural Net Validation Sub Set
Message-ID: <200408012235.i71MZQ4R024461@imcs.marine.rutgers.edu>

Dear R users,
I have been playing with the nnet and predict.nnet functions and have two questions.

1) Is it possible to specify a validation set as well as a training set in the nnet 
function before using predict.nnet to test the nnet object against new data?

2) Is it possible to specify more than one layer of neurons?

Thanks in advance

Matt Oliver



From maj at stats.waikato.ac.nz  Sun Aug  1 01:40:07 2004
From: maj at stats.waikato.ac.nz (maj@stats.waikato.ac.nz)
Date: Sun, 1 Aug 2004 11:40:07 +1200 (NZST)
Subject: [R] Listing subsets
Message-ID: <4311.218.101.46.10.1091317207.squirrel@218.101.46.10>

How best may I form a list containing all choose(n,k) k-element subsets of
a vector x of length n? (and for subsequences?)

Murray Jorgensen



From astephen at efs.mq.edu.au  Mon Aug  2 05:12:23 2004
From: astephen at efs.mq.edu.au (Alec Stephenson)
Date: Mon, 02 Aug 2004 13:12:23 +1000
Subject: [R] Listing subsets
Message-ID: <s10e3dc1.090@efs04.efs.mq.edu.au>

See Programmer's Niche in Volume 1/1 of R News


Alec Stephenson                                               
Department of Statistics
Macquarie University
NSW 2109, Australia 

>>> <maj at stats.waikato.ac.nz> 08/01/04 09:40am >>>
How best may I form a list containing all choose(n,k) k-element subsets
of
a vector x of length n? (and for subsequences?)

Murray Jorgensen

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Mon Aug  2 08:37:47 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 2 Aug 2004 07:37:47 +0100 (BST)
Subject: [R] Neural Net Validation Sub Set
In-Reply-To: <200408012235.i71MZQ4R024461@imcs.marine.rutgers.edu>
Message-ID: <Pine.LNX.4.44.0408020735531.30735-100000@gannet.stats>

On Sun, 1 Aug 2004, Matthew Oliver wrote:

> Dear R users,
> I have been playing with the nnet and predict.nnet functions and have two questions.
> 
> 1) Is it possible to specify a validation set as well as a training set
> in the nnet function before using predict.nnet to test the nnet object
> against new data?

No, as nnet just trains.  What would the validation set be for?  
(Validation is testing a fitted model against new data.)

> 2) Is it possible to specify more than one layer of neurons?

Yes.  nnet is a wrapper for other functions, and see how it constructs a 
net -- the construction is general.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From pgraz at polytechnic.edu.na  Mon Aug  2 08:44:45 2004
From: pgraz at polytechnic.edu.na (F. Patrick Graz)
Date: Mon, 02 Aug 2004 07:44:45 +0100
Subject: [R] (no subject)
Message-ID: <410DE2DD.5060709@polytechnic.edu.na>



From chzhang at cs.ucr.edu  Mon Aug  2 08:46:51 2004
From: chzhang at cs.ucr.edu (chuanjun zhang)
Date: Sun, 01 Aug 2004 23:46:51 -0700
Subject: [R] smooth spline: alpha, spar, lamda
In-Reply-To: <s10e3dc1.090@efs04.efs.mq.edu.au>
References: <s10e3dc1.090@efs04.efs.mq.edu.au>
Message-ID: <410DE35B.1070508@cs.ucr.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040801/d59d71bb/attachment.pl

From maechler at stat.math.ethz.ch  Mon Aug  2 10:04:39 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 2 Aug 2004 10:04:39 +0200
Subject: [R] Not attaching zip files to postings.. {was "dudi.pca
	behaviour..."}
In-Reply-To: <a06110400bd31b1716691@[192.168.2.3]>
References: <a06110400bd31b1716691@[192.168.2.3]>
Message-ID: <16653.62871.769324.616870@gargle.gargle.HOWL>

>>>>> "Sebastien" == Sebastien Durand <sebastien.durand at umontreal.ca>
>>>>>     on Sat, 31 Jul 2004 16:57:07 -0400 writes:

    Sebastien> Hello,
    Sebastien> I not have attached in this e-mail the zipped 
    Sebastien> list of matrices I am using because it has 1 meg 
    Sebastien> once zipped and anyway we cannot send attached 
    Sebastien> files on r-help mailling list.

yes, some (few) you can, but definitely not zip files -- AFAIK
nowadays most viruses come as *.zip ...

The posting guide (below) tells you what to do instead of
attaching them.

Regards,
Martin



From maechler at stat.math.ethz.ch  Mon Aug  2 11:01:11 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 2 Aug 2004 11:01:11 +0200
Subject: [R] smooth.spline codes
In-Reply-To: <410D4490.3020305@cs.ucr.edu>
References: <410C1119.9060201@cs.ucr.edu>
	<410D4490.3020305@cs.ucr.edu>
Message-ID: <16654.727.212848.94466@gargle.gargle.HOWL>

>>>>> "chuanjun" == chuanjun zhang <chzhang at cs.ucr.edu>
>>>>>     on Sun, 01 Aug 2004 12:29:20 -0700 writes:

    chuanjun> The calculation of penalty criterion in smooth.spline 
    chuanjun> is " pen.crit <- sum(wbar * (ybar - fit$ty) *ybar) ", 
    chuanjun> but I think it should be " 
    chuanjun> pen.crit <- sum(wbar * (ybar - fit$ty) * (ybar-fit$ty)) "
    chuanjun> right ?  Why the integral of the
    chuanjun> fit$ty is not included ?  

First: This is only the pen.crit component of
smooth.spline()'s return value, and not the internally used
penalty criterion.

But then you are right.

    chuanjun> Why the integral of the fit$ty is not included ?

Probably the idea was to use the well known trick that

\sum_i{ (y_i - \bar{y})(y_i - \bar{y}) } =
\sum_i{ (y_i - \bar{y}) * y_i }
  since 
\sum_i{ (y_i - \bar{y}) \bar{y} } =
\bar{y} * \sum_i{ (y_i - \bar{y}) } = 0  (because the \sum  is 0).

but then whoever wrote that  pen.crit <- .......... line
confused the two, 'fit$ty' and 'ybar'.

I'm fixing this for R-devel.  
Note once more that this won't change anything in
smooth.spline() computation or it's more important return
value components.

I assume nobody has really used  smooth.spline()'s $pen.crit
component really in a quantitative way (or they should have
found the problem as well). 

Martin Maechler



From hb at maths.lth.se  Mon Aug  2 11:27:56 2004
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Mon, 2 Aug 2004 11:27:56 +0200
Subject: [R] Not attaching zip files to postings.. {was
	"dudi.pcabehaviour..."}
In-Reply-To: <16653.62871.769324.616870@gargle.gargle.HOWL>
Message-ID: <000601c47873$007d2470$e502eb82@hblaptop>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Martin Maechler
> Sent: Monday, August 02, 2004 10:05 AM
> To: Sebastien Durand
> Cc: r-help at stat.math.ethz.ch
> Subject: [R] Not attaching zip files to postings.. {was 
> "dudi.pcabehaviour..."}
> 
> 
> >>>>> "Sebastien" == Sebastien Durand <sebastien.durand at umontreal.ca>
> >>>>>     on Sat, 31 Jul 2004 16:57:07 -0400 writes:
> 
>     Sebastien> Hello,
>     Sebastien> I not have attached in this e-mail the zipped 
>     Sebastien> list of matrices I am using because it has 1 meg 
>     Sebastien> once zipped and anyway we cannot send attached 
>     Sebastien> files on r-help mailling list.
> 
> yes, some (few) you can, but definitely not zip files -- 
> AFAIK nowadays most viruses come as *.zip ...
> 
> The posting guide (below) tells you what to do instead of 
> attaching them.

It's not fun to receieve 1 MB files via a 56k modem (as I am sometimes
forced to use). Although not in the posting guide, I would appreciate if
that big attachements could be posted on a webpage with a URL to in the
message, if possible. Is this a good suggestion? Of course, such files will
be lost in the mail archives as time goes by.
 
> Regards,
> Martin

Best wishes

Henrik Bengtsson



From maechler at stat.math.ethz.ch  Mon Aug  2 11:43:38 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 2 Aug 2004 11:43:38 +0200
Subject: [R] Not attaching zip files to postings.. {was
	"dudi.pcabehaviour..."}
In-Reply-To: <000601c47873$007d2470$e502eb82@hblaptop>
References: <16653.62871.769324.616870@gargle.gargle.HOWL>
	<000601c47873$007d2470$e502eb82@hblaptop>
Message-ID: <16654.3274.576697.451340@gargle.gargle.HOWL>

>>>>> "HenrikB" == Henrik Bengtsson <hb at maths.lth.se>
>>>>>     on Mon, 2 Aug 2004 11:27:56 +0200 writes:

    >> -----Original Message-----
    >> From: r-help-bounces at stat.math.ethz.ch 
    >> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Martin Maechler
    >> Sent: Monday, August 02, 2004 10:05 AM
    >> To: Sebastien Durand
    >> Cc: r-help at stat.math.ethz.ch
    >> Subject: [R] Not attaching zip files to postings.. {was 
    >> "dudi.pcabehaviour..."}
    >> 
    >> 
    >> >>>>> "Sebastien" == Sebastien Durand <sebastien.durand at umontreal.ca>
    >> >>>>>     on Sat, 31 Jul 2004 16:57:07 -0400 writes:
    >> 
    Sebastien> Hello,
    Sebastien> I not have attached in this e-mail the zipped 
    Sebastien> list of matrices I am using because it has 1 meg 
    Sebastien> once zipped and anyway we cannot send attached 
    Sebastien> files on r-help mailling list.
    >> 
    >> yes, some (few) you can, but definitely not zip files -- 
    >> AFAIK nowadays most viruses come as *.zip ...
    >> 
    >> The posting guide (below) tells you what to do instead of 
    >> attaching them.

    HenrikB> It's not fun to receieve 1 MB files via a 56k modem
    HenrikB> (as I am sometimes forced to use). Although not in
    HenrikB> the posting guide, I would appreciate if that big
    HenrikB> attachements could be posted on a webpage with a
    HenrikB> URL to in the message, if possible. Is this a good
    HenrikB> suggestion? 

Definitely.  
And it has been made on the R lists more than once.
Note that the current limit for a single posting to R-help 
is 100K anyway.  I'd have to manually approve larger ones
(and I most often don't approve).

The only somewhat embarassing thing: When I wrote the above, I
thought this *was* in the posting guide but it isn't..

Henrik, would you be willing to send me a small "diff" against the
current posting-guide.html ?

    HenrikB> Of course, such files will be lost in
    HenrikB> the mail archives as time goes by.

yes, but their relevance will be degrading as well.

Martin



From michna at giub.unibe.ch  Mon Aug  2 11:57:49 2004
From: michna at giub.unibe.ch (Pavel Michna)
Date: Mon,  2 Aug 2004 11:57:49 +0200
Subject: [R] [R-pkgs] New package: RNetCDF
Message-ID: <1091440669.410e101d6cd9a@www.cx.unibe.ch>

Dear all,

I would like to announce the availability of a new package on CRAN:

RNetCDF: R Interface to NetCDF Datasets

This package provides an interface to Unidata's NetCDF library functions 
(version 3) and furthermore access to Unidata's udunits calendar
conversions. The routines and the documentation follow the NetCDF and
udunits C interface, so the corresponding manuals can be consulted for
more detailed information.

The main aims of this package are:
- to provide full read/write access to all NetCDF datasets (supporting
  all external NetCDF data types)
- to provide a low level interface which allows the user to write his
  own (customized) NetCDF read/write functions in a simple way

This package requires Unidata's NetCDF and the undunits library already 
installed on the system, which are both available from Unidata's website
(http://www.unidata.ucar.edu/packages/).


Feedback is greatly appreciated.

Pavel


I would like to thank Juerg Schmidli for his excellent comments and
suggestions during the development process of this package and This
Rutishauser for testing the package from the user's perspective.


------------------------------------------------------------------------

Pavel Michna (PhD Student)                  Tel:     +41 (0)31 631 85 42
Climatology and Meteorology (KLIMET)        Fax:     +41 (0)31 631 85 11
Institute of Geography                      E-Mail: michna at giub.unibe.ch
University of Bern
Hallerstrasse 12
CH-3012 Bern/Switzerland

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://www.stat.math.ethz.ch/mailman/listinfo/r-packages



From aleid2001 at yahoo.com  Mon Aug  2 16:16:42 2004
From: aleid2001 at yahoo.com (=?iso-8859-1?q?=20?=)
Date: Mon, 2 Aug 2004 15:16:42 +0100 (BST)
Subject: [R] GlmmPQL
Message-ID: <20040802141642.8868.qmail@web52809.mail.yahoo.com>

Dear friends,

I am apologise to ask again about PQL, because I did
not receive any answer from the first question.

Is it possible to fit a crossed random effects model
 using glmmPQL and, because I tyred to use this codes 


random=list(group=pdBlocked(list(pdIdent(~randf-1),pdIdent(~randm-1)))),
family=binomial, data=sala.data)>

 for the random effects part , but dos not give me the
same result as in the literatures, in particular, for
the random effects, and when I change the number of
groups it gives me different  result. So, If any one
has an idea, pleas help me

Note that the data what I am trying to fit are the
salamander mating data

E-mail:aleid2001 at yahoo.com



From georgi.boshnakov at umist.ac.uk  Mon Aug  2 16:46:37 2004
From: georgi.boshnakov at umist.ac.uk (Georgi Boshnakov)
Date: Mon, 2 Aug 2004 15:46:37 +0100
Subject: [R] Is  k  equivalent to  k:k ?
Message-ID: <006301c4789f$838b8ab0$ad7b5882@umapc173>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040802/e4100f83/attachment.pl

From hkawakat at qub.ac.uk  Mon Aug  2 17:04:30 2004
From: hkawakat at qub.ac.uk (Hiroyuki Kawakatsu)
Date: Mon, 2 Aug 2004 16:04:30 +0100
Subject: [R] help(arima) return value typo?
Message-ID: <Pine.CYG.4.58.0408021557570.4016@erdos>


in ?arima (R-1.9.1), the return value component 'convergence' should be
'code'?

(it's a pity there is no reliable way to check return value documentation
consistency with the code, or is there?)

h.
----------------------------------
Hiroyuki Kawakatsu
School of Management and Economics
25 University Square
Queen's University, Belfast
Belfast BT7 1NN
Northern Ireland
United Kingdom
Tel +44 (0)28 9097 3290
Fax +44 (0)28 9033 5156



From andy_liaw at merck.com  Mon Aug  2 17:08:30 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 2 Aug 2004 11:08:30 -0400
Subject: [R] Is  k  equivalent to  k:k ?
Message-ID: <3A822319EB35174CA3714066D590DCD504AF8155@usrymx25.merck.com>

See ?":", especially the `Value' section.  Numeric literals in R is double
by default.

Andy

> From: Georgi Boshnakov
> 
> Hi,
> 
> I wonder if the following (apparent) inconsistency is a bug 
> or feature. 
> Since scalars are simply vectors of length one I would think that 
>       a      and 
>       a:a 
> produce the same result. For example,
> 
> > identical(4.01,4.01:4.01)
> [1] TRUE
> 
> However,
> 
> identical(4,4:4)
> [1] FALSE
> 
> and
> 
> > identical(4.0,4.0:4.0)
> [1] FALSE
> 
> A closer look reveals that the colon operator produces 
> objects of different class, e.g.
> 
> > class(4)
> [1] "numeric"
> > class(4.0)
> [1] "numeric"
> 
> but
> 
> > class(4:4)
> [1] "integer"
> > class(4.0:4.0)
> [1] "integer"
> 
> 
> Georgi Boshnakov
> 
> --------------------------------------------------------------
> ----------------
> Dr Georgi Boshnakov                                tel.: +44  
> (0)161 200 3684
> Mathematics Department                           email: 
> georgi.boshnakov at umist.ac.uk
> UMIST
> P O Box 88    
> Manchester M60 1QD
> UK
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From MSchwartz at MedAnalytics.com  Mon Aug  2 17:09:52 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon, 02 Aug 2004 10:09:52 -0500
Subject: [R] Is  k  equivalent to  k:k ?
In-Reply-To: <006301c4789f$838b8ab0$ad7b5882@umapc173>
References: <006301c4789f$838b8ab0$ad7b5882@umapc173>
Message-ID: <1091459392.3007.19.camel@localhost.localdomain>

On Mon, 2004-08-02 at 09:46, Georgi Boshnakov wrote:
> Hi,
> 
> I wonder if the following (apparent) inconsistency is a bug or
> feature. 
> Since scalars are simply vectors of length one I would think that 
>       a      and 
>       a:a 
> produce the same result. For example,
> 
> > identical(4.01,4.01:4.01)
> [1] TRUE
> 
> However,
> 
> identical(4,4:4)
> [1] FALSE
> 
> and
> 
> > identical(4.0,4.0:4.0)
> [1] FALSE
> 
> A closer look reveals that the colon operator produces objects of
> different class, e.g.
> 
> > class(4)
> [1] "numeric"
> > class(4.0)
> [1] "numeric"
> 
> but
> 
> > class(4:4)
> [1] "integer"
> > class(4.0:4.0)
> [1] "integer"
> 
> 
> Georgi Boshnakov

The ":" operator is the functional equivalent of "seq(from=a, to=b)".

Note that the help for seq() indicates the following for the return
value:

"The result is of mode "integer" if from is (numerically equal to an)
integer and by is not specified."

Thus, when using the ":" operator, you get integers as the returned
value(s), which is what is happening in your final pair of examples.


If you look at the final example under ?identical, you will see:

identical(1, as.integer(1)) ## FALSE, stored as different types

This is because the first 1 is a double by default.


Thus, in the case of:

identical(4, 4:4)

the first 4 is of type double, while the 4:4 is of type single. Thus the
result is FALSE.

Now, on the other hand, try:

> typeof(seq(4, 4, by = 1))
[1] "double"

You see that the result of the sequence is of type double. Hence:

> identical(4, seq(4, 4, by = 1))
[1] TRUE


So to the question in your subject, no "k" (a double by default) is not
the same as "k:k" (a integer by default).

HTH,

Marc Schwartz



From MSchwartz at MedAnalytics.com  Mon Aug  2 17:14:05 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon, 02 Aug 2004 10:14:05 -0500
Subject: [R] Is  k  equivalent to  k:k ?
In-Reply-To: <1091459392.3007.19.camel@localhost.localdomain>
References: <006301c4789f$838b8ab0$ad7b5882@umapc173>
	<1091459392.3007.19.camel@localhost.localdomain>
Message-ID: <1091459645.3007.23.camel@localhost.localdomain>

On Mon, 2004-08-02 at 10:09, Marc Schwartz wrote:

<snip>

> Thus, in the case of:
> 
> identical(4, 4:4)
> 
> the first 4 is of type double, while the 4:4 is of type single. Thus the
> result is FALSE.

<snip>

Correction. The above sentence should read:

the first 4 is of type double, while the 4:4 is of type **INTEGER**.
Thus the result is FALSE.

Sorry about that.  Need more coffee....

Marc



From maechler at stat.math.ethz.ch  Mon Aug  2 17:15:45 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 2 Aug 2004 17:15:45 +0200
Subject: [R] Is  k  equivalent to  k:k ?
In-Reply-To: <006301c4789f$838b8ab0$ad7b5882@umapc173>
References: <006301c4789f$838b8ab0$ad7b5882@umapc173>
Message-ID: <16654.23201.495689.325958@gargle.gargle.HOWL>

>>>>> "Georgi" == Georgi Boshnakov <georgi.boshnakov at umist.ac.uk>
>>>>>     on Mon, 2 Aug 2004 15:46:37 +0100 writes:

    Georgi> Hi, I wonder if the following (apparent)
    Georgi> inconsistency is a bug or feature.  

a feature "of course".

    Georgi> Since scalars are simply vectors of length one I
    Georgi> would think that a and a:a produce the same result. 

Why would you?  
Have you read the documentation for ":" carefully?  

{Part this topic, namely that a numeric constant such as "4"
 is a "double" in R (but integer in S-plus since S+5.0)
 should probably finally end up in the R FAQ ...
}

Regards, Martin Maechler

    Georgi> For example,

    >> identical(4.01,4.01:4.01)
    Georgi> [1] TRUE

    Georgi> However,

    Georgi> identical(4,4:4)
    Georgi> [1] FALSE

    Georgi> and

    >> identical(4.0,4.0:4.0)
    Georgi> [1] FALSE

    Georgi> A closer look reveals that the colon operator produces objects of different class, e.g.

a closer look wouldn't have been necessary had you read and
understood the documentation.


    >> class(4)
    Georgi> [1] "numeric"
    >> class(4.0)
    Georgi> [1] "numeric"

    Georgi> but

    >> class(4:4)
    Georgi> [1] "integer"
    >> class(4.0:4.0)
    Georgi> [1] "integer"


    Georgi> Georgi Boshnakov
    Georgi> ------------------------------------------------------------------------------
    Georgi> Dr Georgi Boshnakov                                tel.: +44  (0)161 200 3684
    Georgi> Mathematics Department                           email: georgi.boshnakov at umist.ac.uk
    Georgi> UMIST
    Georgi> P O Box 88    
    Georgi> Manchester M60 1QD
    Georgi> UK


    Georgi> [[alternative HTML version deleted]]

	    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
	    do read the posting guide why you should reconfigure
	    your e-mail software



From kcummins at ucsd.edu  Mon Aug  2 17:37:21 2004
From: kcummins at ucsd.edu (Kevin Cummins)
Date: Mon, 02 Aug 2004 08:37:21 -0700
Subject: [R] logistic regression
Message-ID: <5.2.0.9.0.20040802083152.00b18ff0@popmail.ucsd.edu>


I have a system with a binary response variable that was hypothesized to 
follow a simple logistic function. The relationship between the continuous 
independent variable and the logit is clearly not monotonic. I have two 
questions. 1) Can anyone recommend a reference that describes my modeling 
options in this case, and 2) what facilities does R have to deal with this 
situation?

Thanks,
Kevin Cummins
School of Medicine
University of California San Diego



From ramasamy at cancer.org.uk  Mon Aug  2 17:42:23 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 02 Aug 2004 16:42:23 +0100
Subject: [R] pairwise difference operator
In-Reply-To: <1091317344.3940.103.camel@localhost.localdomain>
References: <1091230252.3702.39.camel@localhost.localdomain>
	<1091237331.3940.71.camel@localhost.localdomain>
	<1091317344.3940.103.camel@localhost.localdomain>
Message-ID: <1091461343.3257.47.camel@vpn202001.lif.icnet.uk>

Thank you to Marc Schwartz and Gabor Grothendieck for their responses.
Both solutions are useful.

It would be nice to generalise this problem to situations where other
operations besides difference. Maybe a new member of apply family -
pwapply for pairwise apply ? 

Of course the output would be different if the results of an operation
on two columns produce a vector (like pairwise difference here) or a
single value (like in correlation or pairwise t-test) and one need to
somehow account for this.

Thank you again.

Regards, Adai


On Sun, 2004-08-01 at 00:42, Marc Schwartz wrote:
> On Fri, 2004-07-30 at 20:28, Marc Schwartz wrote:
> > On Fri, 2004-07-30 at 18:30, Adaikalavan Ramasamy wrote:
> > > There was a BioConductor thread today where the poster wanted to find
> > > pairwise difference between columns of a matrix. I suggested the slow
> > > solution below, hoping that someone might suggest a faster and/or more
> > > elegant solution, but no other response.
> > > 
> > > I tried unsuccessfully with the apply() family. Searching the mailing
> > > list was not very fruitful either. The closest I got to was a cryptic
> > > chunk of code in pairwise.table().
> > > 
> > > Since I do use something similar myself occasionally, I am hoping
> > > someone from the R-help list can suggest alternatives or past threads.
> > > Thank you.
> 
> <snip>
> 
> In follow up to the posts on this last night, I created an updated
> version of my function (though I will point out that Gabor's is faster,
> as I will show below).
> 
> I realized that using the combinations() function had a potential
> limitation, which is the limits of R's recursion depth, as Greg mentions
> in the help for the function. It will require an adjustment when the
> number of columns is about 45.
> 
> Thus, I modified the creation of the column combinations as noted below.
> I also added some code to verify the input data type and to ensure that
> the resultant structures remain matrices in the case of an input matrix
> with ncol = 2, in which case, this function is of course, overkill.
> 
> Thus:
> 
>  pairwise.diffs <- function(x)
> {
>   stopifnot(is.matrix(x))
> 
>   # create column combination pairs
>   prs <- cbind(rep(1:ncol(x), each = ncol(x)), 1:ncol(x))
>   col.diffs <- prs[prs[, 1] < prs[, 2], , drop = FALSE]
> 
>   # do pairwise differences 
>   result <- x[, col.diffs[, 1]] - x[, col.diffs[, 2], drop = FALSE]
> 
>   # set colnames
>   if(is.null(colnames(x)))
>     colnames(x) <- 1:ncol(x)
> 
>   colnames(result) <- paste(colnames(x)[col.diffs[, 1]], ".vs.", 
>                             colnames(x)[col.diffs[, 2]], sep = "")
>   result
> }
> 
> 
> Now to performance. I created a large 1,000 column matrix:
> 
> mat <- matrix(sample(100, 10000, replace = TRUE), ncol = 1000)
> colnames(mat) <- 1:1000
> 
> > str(mat)
>  int [1:10, 1:1000] 48 23 26 22 69 64 2 13 13 69 ...
>  - attr(*, "dimnames")=List of 2
>   ..$ : NULL
>   ..$ : chr [1:1000] "1" "2" "3" "4" ...
> 
> 
> Timing:
> 
> > gc();system.time(m <- pairwise.diffs(mat))
>           used (Mb) gc trigger  (Mb)
> Ncells 1541241 41.2    3094291  82.7
> Vcells 7139074 54.5   17257300 131.7
> [1] 1.14 0.19 1.39 0.00 0.00
> 
> 
> > gc();system.time(g <- do.call("cbind", sapply(2:ncol(mat), 
>                                     f, mat)))
>           used (Mb) gc trigger  (Mb)
> Ncells 1541241 41.2    3094291  82.7
> Vcells 7139074 54.5   17257300 131.7
> [1] 0.81 0.02 0.92 0.00 0.00
> 
> 
> Comparisons:
> 
> > str(m)
>  int [1:10, 1:499500] -47 -43 -35 -29 15 33 -53 -36 -17 57 ...
>  - attr(*, "dimnames")=List of 2
>   ..$ : NULL
>   ..$ : chr [1:499500] "1.vs.2" "1.vs.3" "1.vs.4" "1.vs.5" ...
> 
> 
> > str(g)
>  int [1:10, 1:499500] -47 -43 -35 -29 15 33 -53 -36 -17 57 ...
>  - attr(*, "dimnames")=List of 2
>   ..$ : NULL
>   ..$ : chr [1:499500] "1-2" "1-3" "1-4" "1-5" ...
> 
> 
> > table(m == g)
>  
>    TRUE
> 4995000
> 
> 
> HTH,
> 
> Marc Schwartz
> 
> 
>



From Achim.Zeileis at wu-wien.ac.at  Mon Aug  2 17:46:38 2004
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Mon, 2 Aug 2004 17:46:38 +0200 (CEST)
Subject: [R] logistic regression
In-Reply-To: <5.2.0.9.0.20040802083152.00b18ff0@popmail.ucsd.edu>
References: <5.2.0.9.0.20040802083152.00b18ff0@popmail.ucsd.edu>
Message-ID: <Pine.LNX.4.58.0408021744190.16192@thorin.ci.tuwien.ac.at>

On Mon, 2 Aug 2004, Kevin Cummins wrote:

> I have a system with a binary response variable that was hypothesized to
> follow a simple logistic function. The relationship between the continuous
> independent variable and the logit is clearly not monotonic. I have two

The logit is a monotonic function...or do you mean something different
here?

> questions. 1) Can anyone recommend a reference that describes my modeling
> options in this case, and 2) what facilities does R have to deal with this
> situation?

Look at glm(), its man page and the references therein.

hth,
Z

> Thanks,
> Kevin Cummins
> School of Medicine
> University of California San Diego
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Mon Aug  2 17:50:53 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 2 Aug 2004 11:50:53 -0400
Subject: [R] logistic regression
Message-ID: <3A822319EB35174CA3714066D590DCD504AF815A@usrymx25.merck.com>

My suggestion would be a generalized additive model, which you can fit with
gam() in the `mgcv' package.  If you are adventureous you can also try it
`by hand' using glm() with spline terms.  You might also try lrm() with
splines (in the `Design' package) as described in Prof. Harrell's book
`Regression Modelling Strategies'.

HTH,
Andy

> From: Kevin Cummins
> 
> I have a system with a binary response variable that was 
> hypothesized to 
> follow a simple logistic function. The relationship between 
> the continuous 
> independent variable and the logit is clearly not monotonic. 
> I have two 
> questions. 1) Can anyone recommend a reference that describes 
> my modeling 
> options in this case, and 2) what facilities does R have to 
> deal with this 
> situation?
> 
> Thanks,
> Kevin Cummins
> School of Medicine
> University of California San Diego
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From spencer.graves at pdf.com  Mon Aug  2 18:00:14 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 02 Aug 2004 09:00:14 -0700
Subject: [R] logistic regression
In-Reply-To: <5.2.0.9.0.20040802083152.00b18ff0@popmail.ucsd.edu>
References: <5.2.0.9.0.20040802083152.00b18ff0@popmail.ucsd.edu>
Message-ID: <410E650E.8040303@pdf.com>

      Have you considered function "glm" with some polynomial in the 
independent variable?  Regarding a more general reference, have you seen 
Venables and Ripley (2002) Modern Applied Statistics with S, 4th ed. 
(Springer)?  If the answer to either of these questions is "no", you 
might get many answers to these and related questions from these 
references, including working the examples provided with the "glm" 
documentation -- and in reading the posting guide! 
"http://www.R-project.org/posting-guide.html". 

      hope this helps.  spencer graves

Kevin Cummins wrote:

>
> I have a system with a binary response variable that was hypothesized 
> to follow a simple logistic function. The relationship between the 
> continuous independent variable and the logit is clearly not 
> monotonic. I have two questions. 1) Can anyone recommend a reference 
> that describes my modeling options in this case, and 2) what 
> facilities does R have to deal with this situation?
>
> Thanks,
> Kevin Cummins
> School of Medicine
> University of California San Diego
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From t.dewez at brgm.fr  Mon Aug  2 18:28:28 2004
From: t.dewez at brgm.fr (Dewez Thomas)
Date: Mon, 2 Aug 2004 18:28:28 +0200 
Subject: [R] averaging 3D datasets
Message-ID: <D965434E9D6BD511AE3500306E01C8BE05DD68B8@SRV0015>

Dear R-users,

I've spent most of the day reading R documentation at length but couldn't
find something perhaps obvious.

I have a dataset made of 3 morphometric variables for a series of watershed
[log(slope); log(drainage_area); distance_to_outlet]

My aim is to predict the value of log(slope) for pairs of [drainage_area;
distance_to_outlet] (sounds like a plain linear model fitting, right,
nothing too fancy there). In the literature, the standard procedure is to
reduce the number of observations by computing the mean value of slope
insides bins of log(drainage_area) and distance_to_outlet respectively.
Usually, we start with dispersed point clouds and try desperately to reduce
the scatter, hence this binning and averaging procedure. 
How would you go about this?

That is:
1. how does one create bins in two (or n?) dimensions?
2. how does one how does one compute the mean(or median) value of log(slope)
inside each bin?

Any clue is welcome

Thomas
***
Le contenu de cet e-mail et de ses pi??ces jointes est destin?? ?? l'usage exclusif du 
(des) destinataire(s) express??ment d??sign??(s) comme tel(s). En cas de r??ception de cet 
 e-mail par erreur, le signaler ?? son exp??diteur et ne pas en divulguer le contenu. 
L'absence de virus a ??t?? v??rifi?? ??  l'??mission du message. Il convient n??anmoins de 
v??rifier l'absence de corruption ?? sa r??ception.

The contents of this email and any attachments are confidential. They are intended for 
the named recipient(s) only. If you have received this email in error please notify the 
system manager or  the sender immediately and do not disclose the contents to 
anyone or make copies. eSafe scanned this email for viruses, vandals and malicious 
content.
***



From ggrothendieck at myway.com  Mon Aug  2 19:10:54 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 2 Aug 2004 17:10:54 +0000 (UTC)
Subject: [R] pairwise difference operator
References: <1091230252.3702.39.camel@localhost.localdomain>
	<1091237331.3940.71.camel@localhost.localdomain>
	<1091317344.3940.103.camel@localhost.localdomain>
	<1091461343.3257.47.camel@vpn202001.lif.icnet.uk>
Message-ID: <loom.20040802T190335-780@post.gmane.org>

Adaikalavan Ramasamy <ramasamy <at> cancer.org.uk> writes:

: 
: Thank you to Marc Schwartz and Gabor Grothendieck for their responses.
: Both solutions are useful.
: 
: It would be nice to generalise this problem to situations where other
: operations besides difference. Maybe a new member of apply family -
: pwapply for pairwise apply ? 
: 
: Of course the output would be different if the results of an operation
: on two columns produce a vector (like pairwise difference here) or a
: single value (like in correlation or pairwise t-test) and one need to
: somehow account for this.

Since the general case would involve columns or array slices between
two not necessarily identical arrays I think the general case is really
just a sort of generalized inner product.

In the case that the generalized difference is a scalar its usually called 
a product and the Euclidean inner product is the most common and takes 
the form of matrix multiplication which can be done in the one of 
the following ways:

	res1 <- t(mat) %*% mat
	res2 <- crossprod(mat, mat)
	res3 <- crossprod(mat)

A generalized inner product, f, replacing the Euclidean one can be
obtained using a double apply like this:

	res4 <- apply(mat, 2, function(a) apply(mat, 2, function(b) sum(a*b)))

where sum(a*b) can be replaced by f(a,b) for a general inner product
function.  This actually works even if f returns a vector or other
array; however, you may need to reshape the result in that case.

In  the above cases the two matrices were the same but, as mentioned,
they need not be and you can't count on symmetry between f(a,b) and
f(b,a).   If your two matrices are the saame and  you can count on
symmetry then you may want only the lower triangular part and in
that case you can use lower.tri like this:

	res4[lower.tri(res4)]



From f.duan at yale.edu  Mon Aug  2 19:48:54 2004
From: f.duan at yale.edu (F Duan)
Date: Mon,  2 Aug 2004 13:48:54 -0400
Subject: [R] How to add a common title (or xlab,
	ylab) for multi-plots in the same window?
Message-ID: <1091468934.410e7e8667fa5@webmail.med.yale.edu>

Dear R people,

I am using par(mfrow=c()) to plot multi-figures in the same window. And I like 
to put a common title (and xlab, ylab) for all of plots. I have already left 
some margin by resetting omi values in par() and hided all (xlab, ylab) for 
each sub-plot. Could anyone tell me how to do that?

Thanks a lot,

Frank



From ramasamy at cancer.org.uk  Mon Aug  2 19:48:22 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 02 Aug 2004 18:48:22 +0100
Subject: [R] pairwise difference operator
In-Reply-To: <loom.20040802T190335-780@post.gmane.org>
References: <1091230252.3702.39.camel@localhost.localdomain>
	<1091237331.3940.71.camel@localhost.localdomain>
	<1091317344.3940.103.camel@localhost.localdomain>
	<1091461343.3257.47.camel@vpn202001.lif.icnet.uk>
	<loom.20040802T190335-780@post.gmane.org>
Message-ID: <1091468902.3257.91.camel@vpn202001.lif.icnet.uk>

Gabor, thank you. This is very helpful.

On Mon, 2004-08-02 at 18:10, Gabor Grothendieck wrote:
> Adaikalavan Ramasamy <ramasamy <at> cancer.org.uk> writes:
> 
> : 
> : Thank you to Marc Schwartz and Gabor Grothendieck for their responses.
> : Both solutions are useful.
> : 
> : It would be nice to generalise this problem to situations where other
> : operations besides difference. Maybe a new member of apply family -
> : pwapply for pairwise apply ? 
> : 
> : Of course the output would be different if the results of an operation
> : on two columns produce a vector (like pairwise difference here) or a
> : single value (like in correlation or pairwise t-test) and one need to
> : somehow account for this.
> 
> Since the general case would involve columns or array slices between
> two not necessarily identical arrays I think the general case is really
> just a sort of generalized inner product.
> 
> In the case that the generalized difference is a scalar its usually called 
> a product and the Euclidean inner product is the most common and takes 
> the form of matrix multiplication which can be done in the one of 
> the following ways:
> 
> 	res1 <- t(mat) %*% mat
> 	res2 <- crossprod(mat, mat)
> 	res3 <- crossprod(mat)
> 
> A generalized inner product, f, replacing the Euclidean one can be
> obtained using a double apply like this:
> 
> 	res4 <- apply(mat, 2, function(a) apply(mat, 2, function(b) sum(a*b)))
> 
> where sum(a*b) can be replaced by f(a,b) for a general inner product
> function.  This actually works even if f returns a vector or other
> array; however, you may need to reshape the result in that case.
> 
> In  the above cases the two matrices were the same but, as mentioned,
> they need not be and you can't count on symmetry between f(a,b) and
> f(b,a).   If your two matrices are the saame and  you can count on
> symmetry then you may want only the lower triangular part and in
> that case you can use lower.tri like this:
> 
> 	res4[lower.tri(res4)]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From rolf at math.unb.ca  Mon Aug  2 19:51:21 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Mon, 2 Aug 2004 14:51:21 -0300 (ADT)
Subject: [R] help(arima) return value typo?
Message-ID: <200408021751.i72HpLbk003640@erdos.math.unb.ca>

Hiroyuki Kawakatsu wrote:

> in ?arima (R-1.9.1), the return value component 'convergence' should
> be 'code'?

	By my reading of the code of arima() --- yes, you're right.

> (it's a pity there is no reliable way to check return value documentation
> consistency with the code, or is there?)

	Well, looking at the code is ***pretty*** reliable!

BTW:  R core might like to note that while I was looking at the code
I noticed the following lines:

        .
        .
        .
	trarma <- .Call("ARIMA_transPars", coef, arma, FALSE,
            PACKAGE = "stats")
        mod <- makeARIMA(trarma[[1]], trarma[[2]], Delta, kappa)
        if (ncxreg > 0)
            x <- x - xreg %*% coef[narma + (1:ncxreg)]
------> arimaSS(x, mod)
        val <- .Call("ARIMA_CSS", x, arma, trarma[[1]], trarma[[2]],
            as.integer(ncond), TRUE, PACKAGE = "stats")
        sigma2 <- val[[1]]
        .
        .
        .

The line indicated above seems to be hanging there, doing nothing.
Should ``arimaSS(x, mod)'' get assigned to some variable here, or
should that line simply be removed?  (If the former, it's a bug;
if the latter, a typo.)

					cheers,

						Rolf Turner
						rolf at math.unb.ca



From sundar.dorai-raj at PDF.COM  Mon Aug  2 19:53:43 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Mon, 02 Aug 2004 12:53:43 -0500
Subject: [R] How to add a common title (or xlab,	ylab) for multi-plots
	in the same window?
In-Reply-To: <1091468934.410e7e8667fa5@webmail.med.yale.edu>
References: <1091468934.410e7e8667fa5@webmail.med.yale.edu>
Message-ID: <410E7FA7.4010306@pdf.com>



F Duan wrote:

> Dear R people,
> 
> I am using par(mfrow=c()) to plot multi-figures in the same window. And I like 
> to put a common title (and xlab, ylab) for all of plots. I have already left 
> some margin by resetting omi values in par() and hided all (xlab, ylab) for 
> each sub-plot. Could anyone tell me how to do that?
> 
> Thanks a lot,
> 
> Frank
> 

?mtext allows this if outer = TRUE.

--sundar



From ligges at statistik.uni-dortmund.de  Mon Aug  2 19:55:58 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 02 Aug 2004 19:55:58 +0200
Subject: [R] How to add a common title (or xlab,	ylab) for multi-plots
	in the same window?
In-Reply-To: <1091468934.410e7e8667fa5@webmail.med.yale.edu>
References: <1091468934.410e7e8667fa5@webmail.med.yale.edu>
Message-ID: <410E802E.7060705@statistik.uni-dortmund.de>

F Duan wrote:
> Dear R people,
> 
> I am using par(mfrow=c()) to plot multi-figures in the same window. And I like 
> to put a common title (and xlab, ylab) for all of plots. I have already left 
> some margin by resetting omi values in par() and hided all (xlab, ylab) for 
> each sub-plot. Could anyone tell me how to do that?

Use title() or mtext() with argument outer=TRUE.

Uwe Ligges


> Thanks a lot,
> 
> Frank
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From gavin.simpson at ucl.ac.uk  Mon Aug  2 19:53:32 2004
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Mon, 02 Aug 2004 18:53:32 +0100
Subject: [R] How to add a common title (or xlab, ylab) for multi-plots
	in the same window?
In-Reply-To: <1091468934.410e7e8667fa5@webmail.med.yale.edu>
References: <1091468934.410e7e8667fa5@webmail.med.yale.edu>
Message-ID: <410E7F9C.1060702@ucl.ac.uk>

F Duan wrote:
> Dear R people,
> 
> I am using par(mfrow=c()) to plot multi-figures in the same window. And I like 
> to put a common title (and xlab, ylab) for all of plots. I have already left 
> some margin by resetting omi values in par() and hided all (xlab, ylab) for 
> each sub-plot. Could anyone tell me how to do that?
> 
> Thanks a lot,
> 
> Frank

see ?title

argument outer is used to place titles in the outer margin.

HTH

Gavin

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpson at ucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From partha_bagchi at hgsi.com  Mon Aug  2 20:07:20 2004
From: partha_bagchi at hgsi.com (partha_bagchi@hgsi.com)
Date: Mon, 2 Aug 2004 14:07:20 -0400
Subject: [R] How to add a common title (or xlab,
	ylab) for multi-plots in the same window?
Message-ID: <OF0E723961.D2297DC8-ON85256EE4.00637F21-85256EE4.00638CA0@hgsi.com>

?mtext





F Duan <f.duan at yale.edu>
Sent by: r-help-bounces at stat.math.ethz.ch
08/02/2004 01:48 PM

 
        To:     r-help at stat.math.ethz.ch
        cc: 
        Subject:        [R] How to add a common title (or xlab, ylab) for multi-plots in the same 
window?


Dear R people,

I am using par(mfrow=c()) to plot multi-figures in the same window. And I 
like
to put a common title (and xlab, ylab) for all of plots. I have already 
left
some margin by resetting omi values in par() and hided all (xlab, ylab) 
for
each sub-plot. Could anyone tell me how to do that?

Thanks a lot,

Frank

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Mon Aug  2 20:34:24 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 2 Aug 2004 18:34:24 +0000 (UTC)
Subject: [R] pairwise difference operator
References: <1091230252.3702.39.camel@localhost.localdomain>
	<1091237331.3940.71.camel@localhost.localdomain>
	<1091317344.3940.103.camel@localhost.localdomain>
	<1091461343.3257.47.camel@vpn202001.lif.icnet.uk>
	<loom.20040802T190335-780@post.gmane.org>
Message-ID: <loom.20040802T202916-43@post.gmane.org>

Gabor Grothendieck <ggrothendieck <at> myway.com> writes:

: 
: Adaikalavan Ramasamy <ramasamy <at> cancer.org.uk> writes:
: 
: : 
: : Thank you to Marc Schwartz and Gabor Grothendieck for their responses.
: : Both solutions are useful.
: : 
: : It would be nice to generalise this problem to situations where other
: : operations besides difference. Maybe a new member of apply family -
: : pwapply for pairwise apply ? 
: : 
: : Of course the output would be different if the results of an operation
: : on two columns produce a vector (like pairwise difference here) or a
: : single value (like in correlation or pairwise t-test) and one need to
: : somehow account for this.
: 
: Since the general case would involve columns or array slices between
: two not necessarily identical arrays I think the general case is really
: just a sort of generalized inner product.
: 
: In the case that the generalized difference is a scalar its usually called 
: a product and the Euclidean inner product is the most common and takes 
: the form of matrix multiplication which can be done in the one of 
: the following ways:
: 
: 	res1 <- t(mat) %*% mat
: 	res2 <- crossprod(mat, mat)
: 	res3 <- crossprod(mat)
: 
: A generalized inner product, f, replacing the Euclidean one can be
: obtained using a double apply like this:
: 
: 	res4 <- apply(mat, 2, function(a) apply(mat, 2, function(b) sum(a*b)))
: 
: where sum(a*b) can be replaced by f(a,b) for a general inner product
: function.  This actually works even if f returns a vector or other
: array; however, you may need to reshape the result in that case.
: 
: In  the above cases the two matrices were the same but, as mentioned,
: they need not be and you can't count on symmetry between f(a,b) and
: f(b,a).   If your two matrices are the saame and  you can count on
: symmetry then you may want only the lower triangular part and in
: that case you can use lower.tri like this:
: 
: 	res4[lower.tri(res4)]

In rereading what I had written it occurred to me that you
also may be interested in ?dist and the references therein
for a number of special cases and for converting matrices to 
dist objects.  Also see ?var



From ripley at stats.ox.ac.uk  Mon Aug  2 20:35:40 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 2 Aug 2004 19:35:40 +0100 (BST)
Subject: [R] help(arima) return value typo?
In-Reply-To: <200408021751.i72HpLbk003640@erdos.math.unb.ca>
Message-ID: <Pine.LNX.4.44.0408021931430.32638-100000@gannet.stats>

On Mon, 2 Aug 2004, Rolf Turner wrote:

> Hiroyuki Kawakatsu wrote:
> 
> > in ?arima (R-1.9.1), the return value component 'convergence' should
> > be 'code'?
> 
> 	By my reading of the code of arima() --- yes, you're right.
> 
> > (it's a pity there is no reliable way to check return value documentation
> > consistency with the code, or is there?)
> 
> 	Well, looking at the code is ***pretty*** reliable!

Depends if the reader understands the code, of course. See below.

> BTW:  R core might like to note that while I was looking at the code
> I noticed the following lines:
> 
>         .
>         .
>         .
> 	trarma <- .Call("ARIMA_transPars", coef, arma, FALSE,
>             PACKAGE = "stats")
>         mod <- makeARIMA(trarma[[1]], trarma[[2]], Delta, kappa)
>         if (ncxreg > 0)
>             x <- x - xreg %*% coef[narma + (1:ncxreg)]
> ------> arimaSS(x, mod)
>         val <- .Call("ARIMA_CSS", x, arma, trarma[[1]], trarma[[2]],
>             as.integer(ncond), TRUE, PACKAGE = "stats")
>         sigma2 <- val[[1]]
>         .
>         .
>         .
> 
> The line indicated above seems to be hanging there, doing nothing.
> Should ``arimaSS(x, mod)'' get assigned to some variable here, or
> should that line simply be removed?  (If the former, it's a bug;
> if the latter, a typo.)

There are other possibilities, and it is neither.

Hint: you don't think all unassigned calls to options() or 
library.dynam() are bugs or typos, do you?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rolf at math.unb.ca  Mon Aug  2 20:49:04 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Mon, 2 Aug 2004 15:49:04 -0300 (ADT)
Subject: [R] help(arima) return value typo?
Message-ID: <200408021849.i72In48Z014720@erdos.math.unb.ca>

Brian Ripley wrote:

> Hint: you don't think all unassigned calls to options() or 
> library.dynam() are bugs or typos, do you?

No I don't actually.  But I see them as operating in a context
in which ``side effects'' are desirable.  I of course considered
the possibility that the call ``arimaSS(x,mod)'' was there for
an induced side effect, but since that sort of thing is generally
considered very bad R programming I dismissed the notion out
of hand.

					cheers,

						Rolf Turner



From wviechtb at cyrus.psych.uiuc.edu  Mon Aug  2 22:00:07 2004
From: wviechtb at cyrus.psych.uiuc.edu (Wolfgang Viechtbauer)
Date: Mon, 2 Aug 2004 15:00:07 -0500 (CDT)
Subject: [R] Title with substitute and paste
Message-ID: <Pine.SOL.4.58.0408021333570.7251@stat.psych.uiuc.edu>

Hello All,

I am generating some plots where the title is generated with substitute
and paste. An example:

nval <- 20
plot(0,0)
title(substitute(paste("n = ", n), list(n = nval)))

But when compared to:

plot(0,0)
title("n = 20")

the title in the first plot looks slightly different (it is not in
bold). How can I get the two titles to look exactly the same?

Also, how can I generate the following plot:

plot(0,0)
title("Test\n n = 20")

with substitute and paste? I tried:

nval <- 20
plot(0,0)
title(substitute(paste("Test\n", "n = ", n), list(n = nval)))

but that doesn't produce the same result.

Thanks in advance for any suggestions!

-- 
Wolfgang Viechtbauer
Department of Psychology
University of Illinois, Urbana-Champaign



From rpeng at jhsph.edu  Mon Aug  2 22:13:39 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Mon, 02 Aug 2004 16:13:39 -0400
Subject: [R] Title with substitute and paste
In-Reply-To: <Pine.SOL.4.58.0408021333570.7251@stat.psych.uiuc.edu>
References: <Pine.SOL.4.58.0408021333570.7251@stat.psych.uiuc.edu>
Message-ID: <410EA073.20009@jhsph.edu>

Try, wrapping your substitutes()'s with an eval(), as in

title(eval(substitute(paste("n = ", n), list(n = nval))))

-roger

Wolfgang Viechtbauer wrote:
> Hello All,
> 
> I am generating some plots where the title is generated with substitute
> and paste. An example:
> 
> nval <- 20
> plot(0,0)
> title(substitute(paste("n = ", n), list(n = nval)))
> 
> But when compared to:
> 
> plot(0,0)
> title("n = 20")
> 
> the title in the first plot looks slightly different (it is not in
> bold). How can I get the two titles to look exactly the same?
> 
> Also, how can I generate the following plot:
> 
> plot(0,0)
> title("Test\n n = 20")
> 
> with substitute and paste? I tried:
> 
> nval <- 20
> plot(0,0)
> title(substitute(paste("Test\n", "n = ", n), list(n = nval)))
> 
> but that doesn't produce the same result.
> 
> Thanks in advance for any suggestions!
>



From ripley at stats.ox.ac.uk  Mon Aug  2 22:19:42 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 2 Aug 2004 21:19:42 +0100 (BST)
Subject: [R] Title with substitute and paste
In-Reply-To: <Pine.SOL.4.58.0408021333570.7251@stat.psych.uiuc.edu>
Message-ID: <Pine.LNX.4.44.0408022117040.20207-100000@gannet.stats>

On Mon, 2 Aug 2004, Wolfgang Viechtbauer wrote:

> Hello All,
> 
> I am generating some plots where the title is generated with substitute
> and paste. An example:
> 
> nval <- 20
> plot(0,0)
> title(substitute(paste("n = ", n), list(n = nval)))
> 
> But when compared to:
> 
> plot(0,0)
> title("n = 20")
> 
> the title in the first plot looks slightly different (it is not in
> bold). How can I get the two titles to look exactly the same?

The first is an expression, the second a character string so they are 
intentionally different.  Use the same type of object to get the same 
result.

> Also, how can I generate the following plot:
> 
> plot(0,0)
> title("Test\n n = 20")
> 
> with substitute and paste? I tried:
> 
> nval <- 20
> plot(0,0)
> title(substitute(paste("Test\n", "n = ", n), list(n = nval)))
> 
> but that doesn't produce the same result.

You want to be using something like

title(paste("Test\n n =", nval))

that is using a character string and not an expression.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From gunter.berton at gene.com  Mon Aug  2 22:24:24 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 02 Aug 2004 13:24:24 -0700
Subject: [R] Title with substitute and paste
References: <Pine.SOL.4.58.0408021333570.7251@stat.psych.uiuc.edu>
Message-ID: <410EA2F8.CAB22E10@gene.com>

Interesting ...

I believe that the problem is when you use the substitute construction, you
produce an expression object that is interpreted and rendered using the fonts via
the plotmath facility (?plotmath).

When you just use paste, the title is character and gets rendered via the
defaults of title.

You can make them the same by making them both character strings, e.g., vis

> nval <- 20
> plot(0,0)
> title(paste("n = ", nval))

for example.

Or you could wrap the text version in expression() to use plotmath fonts.

Cheers,
Bert

Bert Gunter

Non-Clinical Biostatistics
Genentech
MS: 240B
Phone: 650-467-7374


"The business of the statistician is to catalyze the scientific learning
process."

 -- George E.P. Box



From rkoenker at uiuc.edu  Mon Aug  2 22:59:02 2004
From: rkoenker at uiuc.edu (roger koenker)
Date: Mon, 2 Aug 2004 15:59:02 -0500
Subject: [R] readline, 64bit solaris build
Message-ID: <C85C4850-E4C6-11D8-A06B-000A95A7E3AA@uiuc.edu>

I've built a 64bit version of R 1.9.1 on a solaris 2.9 sparc system.  
Everything
seems quite functional, except that readline produces the following:

> configure:21277: checking for rl_callback_read_char in -lreadline
> configure:21307: cc -xarch=v9 -o conftest -xO5 -xlibmil -dalign
> -I/usr/local/include -L/us
> r/local/lib conftest.c -lreadline  -ldl -ltermcap -lm  >&5
> ld: warning: file /usr/local/lib/libreadline.a(callback.o): wrong ELF
> class: ELFCLASS32
> Undefined                       first referenced
>  symbol                             in file
> rl_callback_read_char               conftest.o
> ld: fatal: Symbol referencing errors. No output written to conftest
> configure:21313: $? = 1
> configure: failed program was:
> | /* confdefs.h.  */

in config.log...  do I interpret this correctly  as a gentle suggestion 
that
readline should be remade as a 64bit library?  Can this be done?  My
(superficial)  look at the readline docs didn't offer any advice in this
direction, nor did my googling produce any.

I realize that the real hidden message here is probably to accept
that it is time to make the transition to ESS, but I'm still resisting 
this,
and hope that someone could suggest a way to get readline going.



url:	www.econ.uiuc.edu/~roger        	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820



From ihok at hotmail.com  Mon Aug  2 23:03:51 2004
From: ihok at hotmail.com (Jack Tanner)
Date: Mon, 02 Aug 2004 17:03:51 -0400
Subject: [R] lapply drops colnames
In-Reply-To: <410974F1.3090306@pdf.com>
References: <41097228.6020906@hotmail.com> <410974F1.3090306@pdf.com>
Message-ID: <410EAC37.4060303@hotmail.com>

I want to iterate over a data frame by columns, and as I'm processing 
each column I want to know its column name.

 > a <- as.data.frame(list(1,2,3))
 > colnames(a) <- c("a", "b", "c")
 > colnames(a)
[1] "X1" "X2" "X3"
 > lapply(a, function(x) {print(colnames(x))})
NULL
NULL
NULL
$a
NULL

$b
NULL

$c
NULL

What is lapply doing? Why does it drop the column name of every column 
it's iterating over? How can I get the column name as a string?



From andy_liaw at merck.com  Mon Aug  2 23:19:20 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 2 Aug 2004 17:19:20 -0400
Subject: [R] lapply drops colnames
Message-ID: <3A822319EB35174CA3714066D590DCD504AF8166@usrymx25.merck.com>

Seems like you got your input and output mixed up a bit...

> From: Jack Tanner
> 
> I want to iterate over a data frame by columns, and as I'm processing 
> each column I want to know its column name.
> 
>  > a <- as.data.frame(list(1,2,3))
>  > colnames(a) <- c("a", "b", "c")
>  > colnames(a)

You must have the two lines above reversed, for the output you get below
suggests so.

> [1] "X1" "X2" "X3"


>  > lapply(a, function(x) {print(colnames(x))})
> NULL
> NULL
> NULL
> $a
> NULL
> 
> $b
> NULL
> 
> $c
> NULL

What happened is, essentially:

for (i in seq(along=a)) {
    print(colnames(a[[i]]))
}

so now you see what's wrong: colnames() of any columns in `a' is NULL,
because a single column of `a' is no longer a data frame, and thus has no
colnames.  So R prints `NULL' three times.  The final output is from
lapply(), which is a list with the same names as the input list, but the
components having values of the function being lapply()'ed: NULL.

Andy
 
> What is lapply doing? Why does it drop the column name of 
> every column 
> it's iterating over? How can I get the column name as a string?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From wolski at molgen.mpg.de  Mon Aug  2 23:27:21 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Mon, 02 Aug 2004 23:27:21 +0200
Subject: [R] lapply drops colnames
In-Reply-To: <410EAC37.4060303@hotmail.com>
References: <41097228.6020906@hotmail.com> <410974F1.3090306@pdf.com>
	<410EAC37.4060303@hotmail.com>
Message-ID: <200408022327210004.00A74326@mail.math.fu-berlin.de>

Hi!

The (column) - names are a property of the data.frame (list - from which data.frame inherits (at least theoretically (green book) how it is implemented in R I do not know.) The columns of the data.frame are lists again. The data.frama is the box and a column is a list in the data.frame list. lapply is working on list elements. The column list does not know anything about the list (data.frame) in which it is stored. Hence, it does not know also the name (index) at which it is stored. lapply is iterating through this container
 not randomly.
What you can do is to extend the column (list) by an addtional attribute  attr(mydataframe[i],"info")<-names(mydataframe)[i] and store theyr names in it.
After you have done it you can

lapply(a, function(x) {print(attr(x,"info"))})

Hope it helps.

Eryk


*********** REPLY SEPARATOR  ***********

On 8/2/2004 at 5:03 PM Jack Tanner wrote:

>>>I want to iterate over a data frame by columns, and as I'm processing 
>>>each column I want to know its column name.
>>>
>>> > a <- as.data.frame(list(1,2,3))
>>> > colnames(a) <- c("a", "b", "c")
>>> > colnames(a)
>>>[1] "X1" "X2" "X3"
>>> > lapply(a, function(x) {print(colnames(x))})
>>>NULL
>>>NULL
>>>NULL
>>>$a
>>>NULL
>>>
>>>$b
>>>NULL
>>>
>>>$c
>>>NULL
>>>
>>>What is lapply doing? Why does it drop the column name of every column 
>>>it's iterating over? How can I get the column name as a string?
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



Dipl. bio-chem. Eryk Witold Wolski    @    MPI-Moleculare Genetic   
Ihnestrasse 63-73 14195 Berlin       'v'    
tel: 0049-30-83875219               /   \    
mail: wolski at molgen.mpg.de        ---W-W----    http://www.molgen.mpg.de/~wolski



From ripley at stats.ox.ac.uk  Mon Aug  2 23:27:59 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 2 Aug 2004 22:27:59 +0100 (BST)
Subject: [R] readline, 64bit solaris build
In-Reply-To: <C85C4850-E4C6-11D8-A06B-000A95A7E3AA@uiuc.edu>
Message-ID: <Pine.LNX.4.44.0408022222420.23350-100000@gannet.stats>

On Mon, 2 Aug 2004, roger koenker wrote:

> I've built a 64bit version of R 1.9.1 on a solaris 2.9 sparc system.  
> Everything
> seems quite functional, except that readline produces the following:
> 
> > configure:21277: checking for rl_callback_read_char in -lreadline
> > configure:21307: cc -xarch=v9 -o conftest -xO5 -xlibmil -dalign
> > -I/usr/local/include -L/us
> > r/local/lib conftest.c -lreadline  -ldl -ltermcap -lm  >&5
> > ld: warning: file /usr/local/lib/libreadline.a(callback.o): wrong ELF
> > class: ELFCLASS32
> > Undefined                       first referenced
> >  symbol                             in file
> > rl_callback_read_char               conftest.o
> > ld: fatal: Symbol referencing errors. No output written to conftest
> > configure:21313: $? = 1
> > configure: failed program was:
> > | /* confdefs.h.  */
> 
> in config.log...  do I interpret this correctly as a gentle suggestion
> that readline should be remade as a 64bit library?  Can this be done?

Yes, yes.  You need to build it with cc -xarch=v9 or gcc -m64: looks like 
I used the latter and probably there was a good reason.

> My
> (superficial)  look at the readline docs didn't offer any advice in this
> direction, nor did my googling produce any.
> 
> I realize that the real hidden message here is probably to accept
> that it is time to make the transition to ESS, but I'm still resisting 
> this,

Um, you need a 64-bit emacs with readline for a 64-bit ESS ....

> and hope that someone could suggest a way to get readline going.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rkoenker at uiuc.edu  Mon Aug  2 23:55:56 2004
From: rkoenker at uiuc.edu (roger koenker)
Date: Mon, 2 Aug 2004 16:55:56 -0500
Subject: [R] readline, 64bit solaris build
In-Reply-To: <Pine.LNX.4.44.0408022222420.23350-100000@gannet.stats>
References: <Pine.LNX.4.44.0408022222420.23350-100000@gannet.stats>
Message-ID: <BB75C952-E4CE-11D8-A06B-000A95A7E3AA@uiuc.edu>

Many, many thanks, ./configure is now happy, and make is chugging 
along....


url:	www.econ.uiuc.edu/~roger        	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820

On Aug 2, 2004, at 4:27 PM, Prof Brian Ripley wrote:

> On Mon, 2 Aug 2004, roger koenker wrote:
>
>> I've built a 64bit version of R 1.9.1 on a solaris 2.9 sparc system.
>> Everything
>> seems quite functional, except that readline produces the following:
>>
>>> configure:21277: checking for rl_callback_read_char in -lreadline
>>> configure:21307: cc -xarch=v9 -o conftest -xO5 -xlibmil -dalign
>>> -I/usr/local/include -L/us
>>> r/local/lib conftest.c -lreadline  -ldl -ltermcap -lm  >&5
>>> ld: warning: file /usr/local/lib/libreadline.a(callback.o): wrong ELF
>>> class: ELFCLASS32
>>> Undefined                       first referenced
>>>  symbol                             in file
>>> rl_callback_read_char               conftest.o
>>> ld: fatal: Symbol referencing errors. No output written to conftest
>>> configure:21313: $? = 1
>>> configure: failed program was:
>>> | /* confdefs.h.  */
>>
>> in config.log...  do I interpret this correctly as a gentle suggestion
>> that readline should be remade as a 64bit library?  Can this be done?
>
> Yes, yes.  You need to build it with cc -xarch=v9 or gcc -m64: looks 
> like
> I used the latter and probably there was a good reason.
>
>> My
>> (superficial)  look at the readline docs didn't offer any advice in 
>> this
>> direction, nor did my googling produce any.
>>
>> I realize that the real hidden message here is probably to accept
>> that it is time to make the transition to ESS, but I'm still resisting
>> this,
>
> Um, you need a 64-bit emacs with readline for a 64-bit ESS ....
>
>> and hope that someone could suggest a way to get readline going.
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From kkelley at nd.edu  Tue Aug  3 00:00:27 2004
From: kkelley at nd.edu (Ken Kelley)
Date: Mon, 2 Aug 2004 17:00:27 -0500
Subject: [R] Returning singular nlme objects.
Message-ID: <000701c478dc$1ebf5390$45b74a81@ND.EDU>

Hi everyone.

I'm working with nlme and I have a question regarding nlme fits that fail
because of singularity issues. Specifically, there a way to return an nlme
object when the estimation process runs into a singular matrix? For example,
can the results up to the point of an error such as "Error in
solve.default(pdMatrix(a, fact = TRUE)) : system is computationally
singular" or "Error in MEestimate(nlmeSt, grpShrunk) : Singularity in
backsolve at level 0, block 1\n" be returned rather than only an error
message being returned?

Setting the "returnObject" nlme control option to TRUE seems to return an
nlme object only when the maximum number of iterations is reached without
meeting the convergence criterion. However, when in the estimation stage a
matrix becomes singular, the returnObject option does not return the nlme
object up to that point (since the maximum number of iterations wasn't
reached). If one reduces the number of iterations to a value less than the
iteration at which the singularity occurs, the results are output (when
returnObject=TRUE). What I want is for nlme to simply output the current
estimates both when the maximum number of iterations is reached and when a
singularity issue arises. 

I know it isn't generally good to make use of the results of nonconverging
results, but I'm interested in the estimates when the singularity is
realized. Specifically I'll use the information in a simulation study and
compare the results from the converging sets with the nonconverging sets,
etc.

Also, suppose one has a set of data where the values are generally small. If
one multiplies the data by some relatively large positive value, thus
rescaling the data, will issues of singular matrices in the estimation stage
be less problematic with the rescaled data than with the original data? 

Thanks very much for any help and/or thoughts,
Ken



From tsvetanski_tsvetelin at yahoo.com  Tue Aug  3 00:28:37 2004
From: tsvetanski_tsvetelin at yahoo.com (Tsvetelin Tsvetanski)
Date: Mon, 2 Aug 2004 15:28:37 -0700 (PDT)
Subject: [R] Estimating EGARCH processes with R
Message-ID: <20040802222837.68720.qmail@web61310.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040802/78d69e65/attachment.pl

From ihok at hotmail.com  Tue Aug  3 00:35:36 2004
From: ihok at hotmail.com (Jack Tanner)
Date: Mon, 02 Aug 2004 18:35:36 -0400
Subject: [R] lapply drops colnames
In-Reply-To: <200408022327210004.00A74326@mail.math.fu-berlin.de>
References: <41097228.6020906@hotmail.com> <410974F1.3090306@pdf.com>
	<410EAC37.4060303@hotmail.com>
	<200408022327210004.00A74326@mail.math.fu-berlin.de>
Message-ID: <410EC1B8.6070300@hotmail.com>

Wolski wrote:
> What you can do is to extend the column (list) by an addtional attribute  attr(mydataframe[i],"info")<-names(mydataframe)[i] and store theyr names in it.

OK, that's brilliant. Any ideas on how to do this automatically for 
every column in my dataframe? lapply(dataframe... fails for the obvious 
reason. Should I do something like this, or is for() to be avoided even 
in this case?

 > for(i in 1:length(a)) {print(names(a)[i])}



From DAVID.BICKEL at PIONEER.COM  Tue Aug  3 00:50:44 2004
From: DAVID.BICKEL at PIONEER.COM (Bickel, David)
Date: Mon, 2 Aug 2004 17:50:44 -0500
Subject: [R] random seed puzzle
Message-ID: <5F883C17941B9F4E80E5FA8C9F1C5E0E010FBA44@jhms08.phibred.com>

After reading the help page on set.seed, I am unsure about how to solve the following problem. I need to call function f a thousand times. The list of values returned by f, should be as random as possible. f calls g twice:
	f <- function(){g1 <- g(1); g2 <- g(2); c(g1; g2)}
The function g in turn calls sample and returns a number, but also depends on its argument, so, starting from the same seed, g(1) returns a different number than g(2). I need each call to g to start with the same random number seed, so that the values returned by f would not be affected by redefining it this way:
	f <- function(){g2 <- g(2); g1 <- g(1); c(g1; g2)}

To make the randomness requirement more precise:
	f.list <- lapply(1:1000, f)
	vec1 <- sapply(f.list, function(x){x[1]})
	vec2 <- sapply(f.list, function(x){x[2]})
The values of vec1 should be as iid as possible and the values of vec2 should be as iid as possible.

Any help with what I need to add to f would be greatly appreciated.

David
_____________________________
David Bickel  http://davidbickel.com
Research Scientist
Pioneer Hi-Bred International
Bioinformatics & Exploratory Research
7250 NW 62nd Ave., PO Box 552
Johnston, Iowa 50131-0552
515-334-4739 Tel
515-334-6634 Fax
david.bickel at pioneer.com, bickel at prueba.info



This communication is for use by the intended recipient and ...{{dropped}}



From tplate at blackmesacapital.com  Tue Aug  3 00:54:09 2004
From: tplate at blackmesacapital.com (Tony Plate)
Date: Mon, 02 Aug 2004 16:54:09 -0600
Subject: [R] lapply drops colnames
In-Reply-To: <410EC1B8.6070300@hotmail.com>
References: <41097228.6020906@hotmail.com> <410974F1.3090306@pdf.com>
	<410EAC37.4060303@hotmail.com>
	<200408022327210004.00A74326@mail.math.fu-berlin.de>
	<410EC1B8.6070300@hotmail.com>
Message-ID: <6.1.0.6.2.20040802165144.0cd0bd78@mailhost.blackmesacapital.com>

If you were preferring to use lapply() rather than for() for reasons of 
efficiency,you might want to test whether there actually is any 
difference.  In a little test case, involving a data frame with 10,000 
columns, I see no big difference.  The advantage of a for loop in your 
situation is that it makes it easy to get at the column names.

 > x <- data.frame(sapply(1:10000, FUN=rnorm, n=100))
 > system.time(x1 <- unlist(lapply(x, sum)))
[1] 0.31 0.01 0.33   NA   NA
 > system.time({x2 <- numeric(ncol(x)); for (i in seq(len=ncol(x))) x2[i] 
<- sum(x[[i]])})
[1] 0.27 0.00 0.27   NA   NA
 > all.equal(x1, x2)
[1] TRUE
 >

hope this helps,

Tony Plate

At Monday 04:35 PM 8/2/2004, Jack Tanner wrote:
>Wolski wrote:
>>What you can do is to extend the column (list) by an addtional 
>>attribute  attr(mydataframe[i],"info")<-names(mydataframe)[i] and store 
>>theyr names in it.
>
>OK, that's brilliant. Any ideas on how to do this automatically for every 
>column in my dataframe? lapply(dataframe... fails for the obvious reason. 
>Should I do something like this, or is for() to be avoided even in this case?
>
> > for(i in 1:length(a)) {print(names(a)[i])}
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Ashton.DeSilva at Buseco.monash.edu.au  Tue Aug  3 00:59:47 2004
From: Ashton.DeSilva at Buseco.monash.edu.au (Ashton DeSilva)
Date: Tue, 03 Aug 2004 08:59:47 +1000
Subject: [R] R.dll
Message-ID: <410EC763.DF1CC7BE@Buseco.monash.edu.au>

Hello

After conducting a simulation R closes down reporting the problem I have
listed below.  These are extracted from the event viewer option under
the control panel option in windows.   This problem occurs quite often
but is as far as I can tell fairly sparadic. I can run identicle
programs which run successfully the first time but on the second run it
does not (and vise-versa also).   Furthermore if I was to run this on
Windows 2000 then the problem doesn't seem to occur at all. 

Each fault happened at separate times:
Faulting application rgui.exe, version 1.91.30621.0, faulting module
r.dll, version 1.91.30621.0, fault address 0x00132900.

Faulting application rgui.exe, version 1.90.30412.0, faulting module
r.dll, version 1.90.30412.0, fault address 0x0002a40f.

Faulting application rgui.exe, version 1.90.30412.0, faulting module
r.dll, version 1.90.30412.0, fault address 0x000b9864.

Hanging application Rgui.exe, version 1.90.30412.0, hang module
hungapp, version 0.0.0.0, hang address 0x00000000.

2/8

Faulting application rgui.exe, version 1.91.30621.0, faulting module
r.dll, version 1.91.30621.0, fault address 0x0002a33c.


Regards

Ashton



From PAlspach at hortresearch.co.nz  Tue Aug  3 01:51:14 2004
From: PAlspach at hortresearch.co.nz (Peter Alspach)
Date: Tue, 03 Aug 2004 11:51:14 +1200
Subject: [R] Standard errors from glm
Message-ID: <s10f7c41.067@hrp3.palm.cri.nz>


Kia ora list members:

I'm having a little difficulty getting the correct standard errors from a glm.object (R 1.9.0 under Windows XP 5.1).  predict() will gives standard errors of the predicted values, but I am wanting the standard errors of the mean.

To clarify:

Assume I have a 4x3x2 factorial with 2 complete replications (i.e. 48 observations, I've appended a dummy set of data at the end of this message).  Call the treatments trt1 (4 levels), trt2 (3 levels) and trt3 (2 levels) and the replications rep - all are factors.  The observed data is S.  Then:

temp.aov <- aov(S~rep+trt1*trt2*trt3, data=dummy.data)
model.tables(temp.aov, type='mean', se=T)

Returns the means, but states "Design is unbalanced - use se.contrasts for se's" which is a little surprising since the design is balanced.  Nevertheless, se.contrast gives what I'd expect:

se.contrast(temp.aov, list(trt1==0, trt1==1), data=dummy.data)
[1] 5.960012

i.e. standard error of mean is 5.960012/sqrt(2) = 4.214, which is the sqrt(anova(temp.aov)[9,3]/12) as expected.  Similarly for interactions, e.g.:

se.contrast(temp.aov, list(trt1==0 & trt2==0, trt1==1 & trt2==1), data=dummy.data)/sqrt(2)
[1]  7.299494

How do I get the equivalent of these standard errors if I have used lm(), and by extension glm()?  I think I should be able to get these using predict(..., type='terms', se=T) or coef(summary()) but can't quite see how.

predict(lm(S~rep+trt1*trt2*trt3, data=dummy.data), type='terms', se=T)
or
predict(glm(cbind(S, 100-S)~rep+trt1*trt2*trt3, data=dummy.data, family='binomial'), type='terms', se=T)
or, as in my case,
predict(glm(cbind(S, 100-S)~rep+trt1*trt2*trt3, data=dummy.data, family='quasibinomial'), type='terms', se=T)


Thanks ........

Peter Alspach
HortResearch

dummy.data
trt1	trt2	trt3	rep	S
0	0	0	1	33
0	0	0	2	55
0	0	1	1	18
0	0	1	2	12
0	1	0	1	47
0	1	0	2	16
0	1	1	1	22
0	1	1	2	33
0	2	0	1	22
0	2	0	2	18
0	2	1	1	60
0	2	1	2	40
1	0	0	1	38
1	0	0	2	24
1	0	1	1	 8
1	0	1	2	14
1	1	0	1	69
1	1	0	2	42
1	1	1	1	42
1	1	1	2	44
1	2	0	1	48
1	2	0	2	26
1	2	1	1	46
1	2	1	2	33
2	0	0	1	48
2	0	0	2	46
2	0	1	1	24
2	0	1	2	 8
2	1	0	1	69
2	1	0	2	33
2	1	1	1	22
2	1	1	2	33
2	2	0	1	33
2	2	0	2	18
2	2	1	1	26
2	2	1	2	42
3	0	0	1	12
3	0	0	2	42
3	0	1	1	16
3	0	1	2	22
3	1	0	1	14
3	1	0	2	60
3	1	1	1	40
3	1	1	2	55
3	2	0	1	47
3	2	0	2	38
3	2	1	1	18
3	2	1	2	44


______________________________________________________

The contents of this e-mail are privileged and/or confidenti...{{dropped}}



From rpeng at jhsph.edu  Tue Aug  3 02:22:44 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Mon, 02 Aug 2004 20:22:44 -0400
Subject: [R] Standard errors from glm
In-Reply-To: <s10f7c41.067@hrp3.palm.cri.nz>
References: <s10f7c41.067@hrp3.palm.cri.nz>
Message-ID: <410EDAD4.7020204@jhsph.edu>

Try summary(glm.object)$coefficients.

-roger

Peter Alspach wrote:
> Kia ora list members:
> 
> I'm having a little difficulty getting the correct standard
> errors from a glm.object (R 1.9.0 under Windows XP 5.1).
> predict() will gives standard errors of the predicted values,
> but I am wanting the standard errors of the mean.
> 
> To clarify:
> 
> Assume I have a 4x3x2 factorial with 2 complete replications
> (i.e. 48 observations, I've appended a dummy set of data at
> the end of this message).  Call the treatments trt1 (4
> levels), trt2 (3 levels) and trt3 (2 levels) and the
> replications rep - all are factors.  The observed data is S.
> Then:
> 
> temp.aov <- aov(S~rep+trt1*trt2*trt3, data=dummy.data) 
> model.tables(temp.aov, type='mean', se=T)
> 
> Returns the means, but states "Design is unbalanced - use
> se.contrasts for se's" which is a little surprising since the
> design is balanced.  Nevertheless, se.contrast gives what I'd
> expect:
> 
> se.contrast(temp.aov, list(trt1==0, trt1==1), data=dummy.data)
>  [1] 5.960012
> 
> i.e. standard error of mean is 5.960012/sqrt(2) = 4.214, which
> is the sqrt(anova(temp.aov)[9,3]/12) as expected.  Similarly
> for interactions, e.g.:
> 
> se.contrast(temp.aov, list(trt1==0 & trt2==0, trt1==1 &
> trt2==1), data=dummy.data)/sqrt(2) [1]  7.299494
> 
> How do I get the equivalent of these standard errors if I have
> used lm(), and by extension glm()?  I think I should be able
> to get these using predict(..., type='terms', se=T) or
> coef(summary()) but can't quite see how.
> 
> predict(lm(S~rep+trt1*trt2*trt3, data=dummy.data),
> type='terms', se=T) or predict(glm(cbind(S,
> 100-S)~rep+trt1*trt2*trt3, data=dummy.data,
> family='binomial'), type='terms', se=T) or, as in my case, 
> predict(glm(cbind(S, 100-S)~rep+trt1*trt2*trt3,
> data=dummy.data, family='quasibinomial'), type='terms', se=T)
> 
> 
> Thanks ........
> 
> Peter Alspach HortResearch
> 
> dummy.data trt1	trt2	trt3	rep	S 0	0	0	1	33 0	0	0	2	55 0	0	1	1
> 18 0	0	1	2	12 0	1	0	1	47 0	1	0	2	16 0	1	1	1	22 0	1	1	2	33 0	2
> 0	1	22 0	2	0	2	18 0	2	1	1	60 0	2	1	2	40 1	0	0	1	38 1	0	0	2	24 
> 1	0	1	1	 8 1	0	1	2	14 1	1	0	1	69 1	1	0	2	42 1	1	1	1	42 1	1	1	2
> 44 1	2	0	1	48 1	2	0	2	26 1	2	1	1	46 1	2	1	2	33 2	0	0	1	48 2	0
> 0	2	46 2	0	1	1	24 2	0	1	2	 8 2	1	0	1	69 2	1	0	2	33 2	1	1	1	22 
> 2	1	1	2	33 2	2	0	1	33 2	2	0	2	18 2	2	1	1	26 2	2	1	2	42 3	0	0	1
> 12 3	0	0	2	42 3	0	1	1	16 3	0	1	2	22 3	1	0	1	14 3	1	0	2	60 3	1
> 1	1	40 3	1	1	2	55 3	2	0	1	47 3	2	0	2	38 3	2	1	1	18 3	2	1	2	44
> 
> 
> ______________________________________________________
> 
> The contents of this e-mail are privileged and/or
> confidenti...{{dropped}}
> 
> ______________________________________________ 
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help PLEASE
> do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From brandt at unt.edu  Tue Aug  3 02:38:20 2004
From: brandt at unt.edu (Patrick Brandt)
Date: Mon, 02 Aug 2004 19:38:20 -0500
Subject: [R] Estimating EGARCH processes with R
In-Reply-To: <20040802222837.68720.qmail@web61310.mail.yahoo.com>
References: <20040802222837.68720.qmail@web61310.mail.yahoo.com>
Message-ID: <410EDE7C.8080304@unt.edu>

There are a number of GARCH models available in the fSeries package -- 
including models with t an skew-t distributions.

--Patrick

-- 
Patrick T. Brandt
Assistant Professor
Department of Political Science
University of North Texas
brandt at unt.edu
http://www.psci.unt.edu/~brandt



Tsvetelin Tsvetanski wrote:

>Hallo,
> 
>I am a student specializing statistics and econometrics in germany. I know there is a way to program EGARCH-processes (time series analyses) in R. 
>If you are ackquainted with statistics already you know that there is nothing but a theorethical use of GARCH-Package in R. Not only because the distribution is gaussian, but also because the skewdness and leptokurthosis are not quite good estimated.
>Matlab Toolbox for estimating EGARCH-processes "GARCH 2.0.2" is therefore better, but not free....
>Well, the most things in life are not free, but knowledge must solve the problem.
> 
>Can you help me construct own EGARCH function wth t-distribution in R?
> 
>best regards
> 
>Tsvetelin Tsvetanski
>
>
>		
>---------------------------------
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From ggrothendieck at myway.com  Tue Aug  3 02:48:02 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 3 Aug 2004 00:48:02 +0000 (UTC)
Subject: [R] lapply drops colnames
References: <41097228.6020906@hotmail.com> <410974F1.3090306@pdf.com>
	<410EAC37.4060303@hotmail.com>
	<200408022327210004.00A74326@mail.math.fu-berlin.de>
	<410EC1B8.6070300@hotmail.com>
Message-ID: <loom.20040803T023017-540@post.gmane.org>

Jack Tanner <ihok <at> hotmail.com> writes:

> 
> Wolski wrote:
> > What you can do is to extend the column (list) by an addtional attribute 
> attr(mydataframe[i],"info")<-names(mydataframe)[i] and store theyr names in 
it.
> 
> OK, that's brilliant. Any ideas on how to do this automatically for 
> every column in my dataframe? lapply(dataframe... fails for the obvious 
> reason. Should I do something like this, or is for() to be avoided even 
> in this case?
> 
>  > for(i in 1:length(a)) {print(names(a)[i])}


You can also lapply or sapply over indices or even the column names
themselves like this:

data(iris)

# sapplying over indices

res <- sapply(1:ncol(iris), function(i) {
       cat("1st element of", colnames(iris)[i], "is", iris[1,i], "\n")
       iris[1,i]
} )
res

# sapplying over column names

res <- sapply(colnames(iris), function(s) {
       cat("1st element of", s, "is", iris[1,s], "\n")
       iris[1,s]
} )
res



From PAlspach at hortresearch.co.nz  Tue Aug  3 03:10:02 2004
From: PAlspach at hortresearch.co.nz (Peter Alspach)
Date: Tue, 03 Aug 2004 13:10:02 +1200
Subject: [R] Standard errors from glm
Message-ID: <s10f8ebb.028@hrp3.palm.cri.nz>


Roger

Thanks - I have tried that but it doesn't give the standard errors I required.

Using my example:

coef(summary(temp.lm)) gives:

                  Estimate Std. Error     t value     Pr(>|t|)
(Intercept)           44.5  10.535912  4.22364968 0.0003224616
rep2                  -1.0   4.214365 -0.23728369 0.8145375583
trt11                -13.0  14.598987 -0.89047272 0.3824325293
trt12                  3.0  14.598987  0.20549370 0.8389943428
trt13                -17.0  14.598987 -1.16446432 0.2561726432
.. etc ...

that is, standard error for the (intercept) is 10.54, rep 4.21, main effects 14.60, 2-way interactions 20.65 and 3-way interaction 29.20.  These can also be obtained via
sqrt(diag(vcov(temp.lm))).

It is not clear to me how to go from these estimates to those from the aov() call.  I have tried pre- and post- multiplying vcov() by the design matrix but this gives the same standard errors as predict(temp.lm, se=T); i.e. those of the predicted values.

Regards ........

Peter Alspach


>>> "Roger D. Peng" <rpeng at jhsph.edu> 03/08/04 12:22:44 >>>
Try summary(glm.object)$coefficients.

-roger

Peter Alspach wrote:
> Kia ora list members:
> 
> I'm having a little difficulty getting the correct standard
> errors from a glm.object (R 1.9.0 under Windows XP 5.1).
> predict() will gives standard errors of the predicted values,
> but I am wanting the standard errors of the mean.
> 
> To clarify:
> 
> Assume I have a 4x3x2 factorial with 2 complete replications
> (i.e. 48 observations, I've appended a dummy set of data at
> the end of this message).  Call the treatments trt1 (4
> levels), trt2 (3 levels) and trt3 (2 levels) and the
> replications rep - all are factors.  The observed data is S.
> Then:
> 
> temp.aov <- aov(S~rep+trt1*trt2*trt3, data=dummy.data) 
> model.tables(temp.aov, type='mean', se=T)
> 
> Returns the means, but states "Design is unbalanced - use
> se.contrasts for se's" which is a little surprising since the
> design is balanced.  Nevertheless, se.contrast gives what I'd
> expect:
> 
> se.contrast(temp.aov, list(trt1==0, trt1==1), data=dummy.data)
>  [1] 5.960012
> 
> i.e. standard error of mean is 5.960012/sqrt(2) = 4.214, which
> is the sqrt(anova(temp.aov)[9,3]/12) as expected.  Similarly
> for interactions, e.g.:
> 
> se.contrast(temp.aov, list(trt1==0 & trt2==0, trt1==1 &
> trt2==1), data=dummy.data)/sqrt(2) [1]  7.299494
> 
> How do I get the equivalent of these standard errors if I have
> used lm(), and by extension glm()?  I think I should be able
> to get these using predict(..., type='terms', se=T) or
> coef(summary()) but can't quite see how.
> 
> predict(lm(S~rep+trt1*trt2*trt3, data=dummy.data),
> type='terms', se=T) or predict(glm(cbind(S,
> 100-S)~rep+trt1*trt2*trt3, data=dummy.data,
> family='binomial'), type='terms', se=T) or, as in my case, 
> predict(glm(cbind(S, 100-S)~rep+trt1*trt2*trt3,
> data=dummy.data, family='quasibinomial'), type='terms', se=T)
> 
> 
> Thanks ........
> 
> Peter Alspach HortResearch
> 
> dummy.data trt1	trt2	trt3	rep	S 0	0	0	1	33 0	0	0	2	55 0	0	1	1
> 18 0	0	1	2	12 0	1	0	1	47 0	1	0	2	16 0	1	1	1	22 0	1	1	2	33 0	2
> 0	1	22 0	2	0	2	18 0	2	1	1	60 0	2	1	2	40 1	0	0	1	38 1	0	0	2	24 
> 1	0	1	1	 8 1	0	1	2	14 1	1	0	1	69 1	1	0	2	42 1	1	1	1	42 1	1	1	2
> 44 1	2	0	1	48 1	2	0	2	26 1	2	1	1	46 1	2	1	2	33 2	0	0	1	48 2	0
> 0	2	46 2	0	1	1	24 2	0	1	2	 8 2	1	0	1	69 2	1	0	2	33 2	1	1	1	22 
> 2	1	1	2	33 2	2	0	1	33 2	2	0	2	18 2	2	1	1	26 2	2	1	2	42 3	0	0	1
> 12 3	0	0	2	42 3	0	1	1	16 3	0	1	2	22 3	1	0	1	14 3	1	0	2	60 3	1
> 1	1	40 3	1	1	2	55 3	2	0	1	47 3	2	0	2	38 3	2	1	1	18 3	2	1	2	44
> 
> 
> ______________________________________________________
> 
> The contents of this e-mail are privileged and/or
> confidenti...{{dropped}}
> 
> ______________________________________________ 
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help PLEASE
> do read the posting guide!
> http://www.R-project.org/posting-guide.html 
> 

______________________________________________________

The contents of this e-mail are privileged and/or confidenti...{{dropped}}



From murdoch at stats.uwo.ca  Tue Aug  3 05:44:54 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 02 Aug 2004 23:44:54 -0400
Subject: [R] R.dll
In-Reply-To: <410EC763.DF1CC7BE@Buseco.monash.edu.au>
References: <410EC763.DF1CC7BE@Buseco.monash.edu.au>
Message-ID: <6u1ug0til7pgvkh2ltqqagc0qvei0jcncq@4ax.com>

On Tue, 03 Aug 2004 08:59:47 +1000, Ashton DeSilva
<Ashton.DeSilva at Buseco.monash.edu.au> wrote:

>Hello
>
>After conducting a simulation R closes down reporting the problem I have
>listed below.  These are extracted from the event viewer option under
>the control panel option in windows.   This problem occurs quite often
>but is as far as I can tell fairly sparadic. I can run identicle
>programs which run successfully the first time but on the second run it
>does not (and vise-versa also).   Furthermore if I was to run this on
>Windows 2000 then the problem doesn't seem to occur at all. 
>
>Each fault happened at separate times:
>Faulting application rgui.exe, version 1.91.30621.0, faulting module
>r.dll, version 1.91.30621.0, fault address 0x00132900.

You need to come up with code that reproduces the error before anyone
else is going to be able to help you with it.  Since you say this
happens during a simulation, the differences between runs may be due
to different random number seeds.  You could try things like

set.seed(1)
# Run simulation
set.seed(2)
# Run simulation

...

until you find a value that triggers the crash, then see if it happens
reproducibly.  If so, try to narrow things down:  print diagnostics so
you know how much of the simulation succeeded before the crash
happened.  Save .Random.seed just before the crash, and see if running
just 

.Random.seed <- saved.value
# Run line that caused the crash

causes the crash reproducibly.  If so, you'll have a nice bug report
to submit (or may have found a bug in your own code, or some
contributed package).

Duncan Murdoch



From murdoch at stats.uwo.ca  Tue Aug  3 06:03:00 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 03 Aug 2004 00:03:00 -0400
Subject: [R] random seed puzzle
In-Reply-To: <5F883C17941B9F4E80E5FA8C9F1C5E0E010FBA44@jhms08.phibred.com>
References: <5F883C17941B9F4E80E5FA8C9F1C5E0E010FBA44@jhms08.phibred.com>
Message-ID: <d23ug0tgbnm0o8sk1687ntlidlsgcfkgnj@4ax.com>

On Mon, 2 Aug 2004 17:50:44 -0500, "Bickel, David"
<DAVID.BICKEL at PIONEER.COM> wrote:

>After reading the help page on set.seed, I am unsure about how to solve the following problem. I need to call function f a thousand times. The list of values returned by f, should be as random as possible. f calls g twice:
>	f <- function(){g1 <- g(1); g2 <- g(2); c(g1; g2)}
>The function g in turn calls sample and returns a number, but also depends on its argument, so, starting from the same seed, g(1) returns a different number than g(2). I need each call to g to start with the same random number seed, so that the values returned by f would not be affected by redefining it this way:
>	f <- function(){g2 <- g(2); g1 <- g(1); c(g1; g2)}

This is a problem that's like what people doing simulations on
parallel computers worry about.  You might want to look at the sprng
package.  (I haven't used it, so I'm not sure it does what you need). 

Duncan Murdoch



From vietnguyen at fastmail.fm  Tue Aug  3 06:37:56 2004
From: vietnguyen at fastmail.fm (Viet Nguyen)
Date: Tue, 03 Aug 2004 14:37:56 +1000
Subject: [R] dots expansion
Message-ID: <410F16A4.6060809@fastmail.fm>

Hi list,

I'm trying to write a function similar to rbind, except that needs to 
add a factor to each component array before rbinding them together so 
that the rows from different arrays are distinguishable.

The problem that arose is how to loop through arguments in the dots 
"..." list. I need to get a hand on each of them but don't know how many 
of them there are and what their names are.

It'd be useful if I could look at how rbind(...) or c(...) do this but 
they are both Internal functions.

Thanks in anticipation of help!

Regards,
viet



From murdoch at stats.uwo.ca  Tue Aug  3 07:41:04 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 03 Aug 2004 01:41:04 -0400
Subject: [R] dots expansion
In-Reply-To: <410F16A4.6060809@fastmail.fm>
References: <410F16A4.6060809@fastmail.fm>
Message-ID: <v99ug0tt8j9pimgd0p3jukv2a8p5q9c394@4ax.com>

On Tue, 03 Aug 2004 14:37:56 +1000, Viet Nguyen
<vietnguyen at fastmail.fm> wrote:

>Hi list,
>
>I'm trying to write a function similar to rbind, except that needs to 
>add a factor to each component array before rbinding them together so 
>that the rows from different arrays are distinguishable.
>
>The problem that arose is how to loop through arguments in the dots 
>"..." list. I need to get a hand on each of them but don't know how many 
>of them there are and what their names are.
>
>It'd be useful if I could look at how rbind(...) or c(...) do this but 
>they are both Internal functions.

list(...) will create a list, one item per argument.  If the args are
named then the corresponding list item will be named.

Duncan Murdoch



From Meredith.Briggs at team.telstra.com  Tue Aug  3 07:51:43 2004
From: Meredith.Briggs at team.telstra.com (Briggs, Meredith M)
Date: Tue, 3 Aug 2004 15:51:43 +1000
Subject: [R] How does R vary from SAS?
Message-ID: <3B5823541A25D311B3B90008C7F9056410E356D4@ntmsg0092.corpmail.telstra.com.au>


Hello

I have to show that R has more functionality than SAS for MonteCarlo type exercises involving matrix algebra. As I has little experience with SAS is anyone able to help. I have a model written in R but the IT world in this company is not familiar with R and wants to use something like SAS.

thanks



From bido at mac.com  Tue Aug  3 08:01:11 2004
From: bido at mac.com (Francisco J. Bido)
Date: Tue, 3 Aug 2004 01:01:11 -0500
Subject: [R] Installing tseries package on MacOSX
Message-ID: <85747DDB-E512-11D8-ABCC-000393B90A0A@mac.com>

I have R 1.9.1 on MacOSX.  The automated package download appears 
broken in this version.   Is there any documentation on how to compile 
and install this the tseries package on MacOSX?  I've looked around and 
tried a few things but with luck.

Thanks!
-Francisco



From ripley at stats.ox.ac.uk  Tue Aug  3 08:10:56 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 3 Aug 2004 07:10:56 +0100 (BST)
Subject: [R] Standard errors from glm
In-Reply-To: <s10f7c41.067@hrp3.palm.cri.nz>
Message-ID: <Pine.LNX.4.44.0408030707370.24028-100000@gannet.stats>

On Tue, 3 Aug 2004, Peter Alspach wrote:

[Lines wrapped for legibility.]

> I'm having a little difficulty getting the correct standard errors from
> a glm.object (R 1.9.0 under Windows XP 5.1).  predict() will gives
> standard errors of the predicted values, but I am wanting the standard
> errors of the mean.
> 
> To clarify:
> 
> Assume I have a 4x3x2 factorial with 2 complete replications (i.e. 48
> observations, I've appended a dummy set of data at the end of this
> message).  Call the treatments trt1 (4 levels), trt2 (3 levels) and trt3
> (2 levels) and the replications rep - all are factors.  The observed
> data is S.  Then:
> 
> temp.aov <- aov(S~rep+trt1*trt2*trt3, data=dummy.data)
> model.tables(temp.aov, type='mean', se=T)
> 
> Returns the means, but states "Design is unbalanced - use se.contrasts
> for se's" which is a little surprising since the design is balanced.  

If you used the default treatment contrasts, it is not.  Try Helmert 
contrasts with aov().

> Nevertheless, se.contrast gives what I'd expect:
> 
> se.contrast(temp.aov, list(trt1==0, trt1==1), data=dummy.data)
> [1] 5.960012
> 
> i.e. standard error of mean is 5.960012/sqrt(2) = 4.214, which is the
> sqrt(anova(temp.aov)[9,3]/12) as expected.  Similarly for interactions,
> e.g.:
> 
> se.contrast(temp.aov, list(trt1==0 & trt2==0, trt1==1 & trt2==1), data=dummy.data)/sqrt(2)
> [1]  7.299494
> 
> How do I get the equivalent of these standard errors if I have used
> lm(), and by extension glm()?  I think I should be able to get these
> using predict(..., type='terms', se=T) or coef(summary()) but can't
> quite see how.

In either case you can predict something you want to estimate and use
predict(, se=TRUE).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Aug  3 08:21:56 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 3 Aug 2004 07:21:56 +0100 (BST)
Subject: [R] Installing tseries package on MacOSX
In-Reply-To: <85747DDB-E512-11D8-ABCC-000393B90A0A@mac.com>
Message-ID: <Pine.LNX.4.44.0408030719000.24028-100000@gannet.stats>

On Tue, 3 Aug 2004, Francisco J. Bido wrote:

> I have R 1.9.1 on MacOSX.  The automated package download appears 
> broken in this version.   

Your version?  It is not broken in the version R-core distributes, but you
may be using it incorrectly.

> Is there any documentation on how to compile 
> and install this the tseries package on MacOSX?  I've looked around and 
> tried a few things but with luck.

Get the tarball from CRAN, called tseries_0.9-22.tar.gz.
In a shell, run  R CMD INSTALL tseries_0.9-22.tar.gz.
That's it.

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

PLEASE do so, and give some useful information about what you are doing so 
we can help you correct it.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Wanzare at HCJP.com  Tue Aug  3 08:51:16 2004
From: Wanzare at HCJP.com (Manoj - Hachibushu Capital)
Date: Tue, 3 Aug 2004 15:51:16 +0900
Subject: [R] Data manipulation query
Message-ID: <1CBA12F2D414914989C723D196B287DC05566F@jp-svr-ex1.hcjp.com>

Hi,
	Not sure if I am making a simple problem complex but still here
we go:

	I have a data frame with four columns say, X1 X2 X3 and X4. I
want to break X4 into deciles and for each deciles obtained, I want to
see corresponding elements of X1. 

	Ideally, the output should be in a tabular fashion as shown
below:

	Deciles 1 Deciles 2	....	Deciles 10

	X1-1	   X1-2		 X1-99
	X1-5	   X1-3		
	X1-10

	Where X1-1...X1-100 are elements of column X1 that categorized
as per deciles

	Any pointers to help get the right structure would be greatly
appreciated!!

TIA.

Manoj



From vito_ricci at yahoo.com  Tue Aug  3 10:14:03 2004
From: vito_ricci at yahoo.com (=?iso-8859-1?q?Vito=20Ricci?=)
Date: Tue, 3 Aug 2004 10:14:03 +0200 (CEST)
Subject: [R] Data manipulation query 
Message-ID: <20040803081403.37115.qmail@web41204.mail.yahoo.com>

Hi,

see ? quantile to obtain deciles of variable X1

see ? cut to divide the range of 'x' into intervals
and codes the values in 'x' according to which
interval they fall.

se ? table to use the cross-classifying factors to
build a contingency table of the counts at each
combination of factor levels.

Best
Vito


Hi,
	Not sure if I am making a simple problem complex but
still here
we go:

	I have a data frame with four columns say, X1 X2 X3
and X4. I
want to break X4 into deciles and for each deciles
obtained, I want to
see corresponding elements of X1. 

	Ideally, the output should be in a tabular fashion as
shown
below:

	Deciles 1 Deciles 2	....	Deciles 10

	X1-1	   X1-2		 X1-99
	X1-5	   X1-3		
	X1-10

	Where X1-1...X1-100 are elements of column X1 that
categorized
as per deciles

	Any pointers to help get the right structure would be
greatly
appreciated!!

TIA.

Manoj

=====
Diventare costruttori di soluzioni

Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese http://www.modugno.it/archivio/cat_palese.shtml



From jinsong_zh at yahoo.com  Tue Aug  3 10:42:41 2004
From: jinsong_zh at yahoo.com (Jinsong Zhao)
Date: Tue, 3 Aug 2004 01:42:41 -0700 (PDT)
Subject: [R] How to select a whole column? Thanks! 
Message-ID: <20040803084241.49210.qmail@web20828.mail.yahoo.com>

Dear all,

I hope to remove a whole column from a data frame or matrix (> 2000
columns). All value in the column are same. The first thing is to
select those columns. 

For instance, I hope to remove the V3~6 column, for all the value in
those colume is zero.

  V3 V4 V5 V6     V7     V8     V9    V10
1  0  0  0  0  0.000  0.000  0.000  0.000
2  0  0  0  0  0.000  0.000  0.000  0.000
3  0  0  0  0  0.000  0.000  0.000  0.000
4  0  0  0  0  0.000  0.000  0.000  0.000
5  0  0  0  0  0.000  0.000  0.000  0.000
6  0  0  0  0 -0.001 -0.001 -0.001 -0.001
7  0  0  0  0  0.000  0.000  0.000 -0.001
8  0  0  0  0  0.000  0.000  0.000 -0.001
9  0  0  0  0 -0.009 -0.012 -0.015 -0.018

I mean how to select the first four columns.

Thank you very much for your consideration on this matter.

Best wishes,

Jinsong

=====
(Mr.) Jinsong Zhao
Ph.D. Candidate
School of the Environment
Nanjing University
22 Hankou Road, Nanjing 210093
P.R. China
E-mail: jinsong_zh at yahoo.com



From JonesW at kssg.com  Tue Aug  3 10:50:05 2004
From: JonesW at kssg.com (Wayne Jones)
Date: Tue, 3 Aug 2004 09:50:05 +0100 
Subject: [R] How to select a whole column? Thanks! 
Message-ID: <6B5A9304046AD411BD0200508BDFB6CB02BD1439@gimli.middleearth.kssg.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040803/dff0c2b8/attachment.pl

From ripley at stats.ox.ac.uk  Tue Aug  3 10:55:01 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 3 Aug 2004 09:55:01 +0100 (BST)
Subject: [R] How to select a whole column? Thanks! 
In-Reply-To: <20040803084241.49210.qmail@web20828.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0408030951480.29980-100000@gannet.stats>

On Tue, 3 Aug 2004, Jinsong Zhao wrote:

> I hope to remove a whole column from a data frame or matrix (> 2000
> columns). All value in the column are same. The first thing is to
> select those columns. 
> 
> For instance, I hope to remove the V3~6 column, for all the value in
> those colume is zero.
> 
>   V3 V4 V5 V6     V7     V8     V9    V10
> 1  0  0  0  0  0.000  0.000  0.000  0.000
> 2  0  0  0  0  0.000  0.000  0.000  0.000
> 3  0  0  0  0  0.000  0.000  0.000  0.000
> 4  0  0  0  0  0.000  0.000  0.000  0.000
> 5  0  0  0  0  0.000  0.000  0.000  0.000
> 6  0  0  0  0 -0.001 -0.001 -0.001 -0.001
> 7  0  0  0  0  0.000  0.000  0.000 -0.001
> 8  0  0  0  0  0.000  0.000  0.000 -0.001
> 9  0  0  0  0 -0.009 -0.012 -0.015 -0.018
> 
> I mean how to select the first four columns.

mydf2 <- mydf[-(1:4)]

If you wanted to remove all columns which were entirely zero, you could
use

cols <- sapply(mydf, function(x) all(x == 0))
mydf2 <- mydf[!cols]

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

which suggests you read `An Introduction to R', and that covers this and 
more.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From P.Lemmens at nici.kun.nl  Tue Aug  3 10:58:49 2004
From: P.Lemmens at nici.kun.nl (Paul Lemmens)
Date: Tue, 03 Aug 2004 10:58:49 +0200
Subject: [R] How to select a whole column? Thanks! 
In-Reply-To: <20040803084241.49210.qmail@web20828.mail.yahoo.com>
References: <20040803084241.49210.qmail@web20828.mail.yahoo.com>
Message-ID: <4A0376813D66EDD843B62E50@lemmens.socsci.kun.nl>

Hoi Jinsong,

--On dinsdag 3 augustus 2004 1:42 -0700 Jinsong Zhao <jinsong_zh at yahoo.com> 
wrote:
> For instance, I hope to remove the V3~6 column, for all the value in
> those colume is zero.
>
>   V3 V4 V5 V6     V7     V8     V9    V10
> 1  0  0  0  0  0.000  0.000  0.000  0.000
> 2  0  0  0  0  0.000  0.000  0.000  0.000
> 3  0  0  0  0  0.000  0.000  0.000  0.000
> 4  0  0  0  0  0.000  0.000  0.000  0.000
> 5  0  0  0  0  0.000  0.000  0.000  0.000
> 6  0  0  0  0 -0.001 -0.001 -0.001 -0.001
> 7  0  0  0  0  0.000  0.000  0.000 -0.001
> 8  0  0  0  0  0.000  0.000  0.000 -0.001
> 9  0  0  0  0 -0.009 -0.012 -0.015 -0.018
>
> I mean how to select the first four columns.
>
subset(df, select=-c(V3,V4,V5,V6))



-- 
Paul Lemmens
NICI, University of Nijmegen              ASCII Ribbon Campaign /"\
Montessorilaan 3 (B.01.05)                    Against HTML Mail \ /
NL-6525 HR Nijmegen                                              X
The Netherlands                                                 / \
Phonenumber    +31-24-3612648
Fax            +31-24-3616066



From maechler at stat.math.ethz.ch  Tue Aug  3 11:31:42 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 3 Aug 2004 11:31:42 +0200
Subject: [R] How to add a common title (or xlab, ylab) for multi-plots
	in the same window?
In-Reply-To: <410E7FA7.4010306@pdf.com>
References: <1091468934.410e7e8667fa5@webmail.med.yale.edu>
	<410E7FA7.4010306@pdf.com>
Message-ID: <16655.23422.97405.123006@gargle.gargle.HOWL>

>>>>> "Sundar" == Sundar Dorai-Raj <sundar.dorai-raj at pdf.com>
>>>>>     on Mon, 02 Aug 2004 12:53:43 -0500 writes:

    Sundar> F Duan wrote:

    >> I am using par(mfrow=c()) to plot multi-figures in the
    >> same window. And I like to put a common title (and xlab,
    >> ylab) for all of plots. I have already left some margin
    >> by resetting omi values in par() and hided all (xlab,
    >> ylab) for each sub-plot. Could anyone tell me how to do that?

    >> Thanks a lot,
    >> 
    >> Frank

   Sundar> ?mtext allows this if outer = TRUE.

and others have mentioned the same.
I'm not answering the "(or xlab, ylab)" part in the original
question, something which *is* answered by ``use mtext(..)''.

Because it such a common task to want an overall title for
"multi-plots", and furthermore, because in many typical
situations, the default "spacing" {par("mar") etc} around the
individual plots is too large (i.e. the plots get smaller than
necessary with their white space margins using too much of your
device's real estate), I had written a small utility function,
mult.fig(), many years ago {in 1990! for S-plus}, and made it
available in the 'sfsmisc' package for a while now.

The start of its help page is :

>> mult.fig               package:sfsmisc               R Documentation
>> 
>> Plot Setup for MULTiple FIGures, incl. Main Title
>> 
>> Description:
>> 
>>      Easy Setup for plotting multiple figures (in a rectangular layout)
>>      on one page.  It allows to specify a main title and uses _smart_
>>      defaults for several 'par' calls.

a typical use after
     install.packages("sfsmisc")
     library(sfsmisc)

is simply

     mult.fig(<#{plots}>, main = <overall title>)

such as the one in example(mult.fig)  :

     mult.fig(5, main= expression("Sine Functions " * sin(n * pi * x)))
     x <- seq(0, 1, len = 201)
     for (n in 1:5)
       plot(x, sin(n * pi * x), ylab ="", main = paste("n = ",n))
     par(old.par)

[BTW: The function has one really ugly feature (that I've unfortunately
      not eliminated long time ago and now it would break too
      much code): It defines the *GLOBAL* variable  'old.par'
      as seen in the above example.
 Good programming practice would use the return value of
 mult.fig()  (which *does* contain old.par!)
 Yuck! - what a bad programming style I was using 14 years ago....
]
      
Martin Maechler



From murdoch at stats.uwo.ca  Tue Aug  3 12:23:00 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 03 Aug 2004 06:23:00 -0400
Subject: [R] How does R vary from SAS?
In-Reply-To: <3B5823541A25D311B3B90008C7F9056410E356D4@ntmsg0092.corpmail.telstra.com.au>
References: <3B5823541A25D311B3B90008C7F9056410E356D4@ntmsg0092.corpmail.telstra.com.au>
Message-ID: <kmpug01087e9nvttju05588ggp70hhmder@4ax.com>

On Tue, 3 Aug 2004 15:51:43 +1000, "Briggs, Meredith M"
<Meredith.Briggs at team.telstra.com> wrote:

>
>Hello
>
>I have to show that R has more functionality than SAS for MonteCarlo type exercises involving matrix algebra. As I has little experience with SAS is anyone able to help. I have a model written in R but the IT world in this company is not familiar with R and wants to use something like SAS.

I'd ask for help to rewrite your simulation in SAS.  If it's easy,
then you don't have much of an argument; if it's hard, you do.

If you can't get anyone to help you write your simulation in SAS, then
that in itself should be an argument for R:  you've got it now, and
won't have it if SAS is a requirement.

Duncan Murdoch



From Arne.Muller at aventis.com  Tue Aug  3 13:24:15 2004
From: Arne.Muller at aventis.com (Arne.Muller@aventis.com)
Date: Tue, 3 Aug 2004 13:24:15 +0200
Subject: [R] strange tickmarks placing in image
Message-ID: <C80ECAFA2ACC1B45BE45D133ED660ADE0184621A@crbsmxsusr04.pharma.aventis.com>

Hello,

I've a problem aligning tickmarks to an image. I've created a correlation matrix for 84 datasets. I'm visualizing the matrix as an image with colour coding according to the correlation coefficient.

The 84 datasets are distributed over three factors, but the desgin is unbalanced, so that the tickmarks and the lables for the axis must not evenly distributed. A regular grid via the 'grid' function aligns perfectly with the image cells, but the tickmarks via "axis" are slightely shifted, and not aligned perfectly with the image cells. The offset is even stonger for the y-axis. The thing is that I don't want 84 lables at the axis, it's enough to have one lable for all the different factor level combinations, which results in 28 labels.

Maybe you have an idea how to setup the command to align the tick marks properly.
	
	thanks for your help, kind regards,

	Arne


Here are my commands:

> library(marrayPlots) # for the colors
> col <- maPalette(low='white', high='darkred', k=50)
> par(ps=8, cex=1, mar=c(1,5,5,1)) # space needed for lables @ axis 1 and 3

# x and y range from 1 to 84, x is the correlation matrix (dim = 84x84)
> image(1:84, 1:84, x, col=col, xaxt='n', yaxt='n', xlab='', ylab='')

# set up the axis, 28 lables, distributed un-evenly over the image axis
> axis(3, i, labels=names(l), las=2, tick=T)
> axis(2, i, labels=names(l), las=2, tick=T)
> grid(84, col='black', lty='solid') # grids each of the 84 cells

# this is where the lables come form, the number indicate the replicates
# per factor-level combinations
> l
    NEW:4:0   NEW:4:100   NEW:4:250   NEW:4:500  NEW:4:1000    NEW:24:0 
          3           3           3           3           3           3 
 NEW:24:100  NEW:24:250  NEW:24:500 NEW:24:1000     OLD:4:0   OLD:4:100 
          3           3           3           3           4           3 
  OLD:4:250   OLD:4:500  OLD:4:1000    OLD:24:0  OLD:24:100  OLD:24:250 
          2           3           3           4           3           2 
 OLD:24:500 OLD:24:1000     PRG:4:0   PRG:4:100   PRG:4:250  PRG:4:1000 
          3           3           3           3           3           3 
   PRG:24:0  PRG:24:100  PRG:24:250 PRG:24:1000 
          3           3           3           3 

# these are the positions along the axis for the tick marks,
# replicates from 1 to 3 (replicates of one factor-evel combination), 4 to 6  
# ...
> i
 [1]  3  6  9 12 15 18 21 24 27 30 34 37 39 42 45 49 52 54 57 60 63 66 69 72 75
[26] 78 81 84



From ottorino-luca.pantani at unifi.it  Tue Aug  3 13:39:56 2004
From: ottorino-luca.pantani at unifi.it (8rino-Luca Pantani)
Date: Tue, 03 Aug 2004 13:39:56 +0200
Subject: [R] (Lattice) How to improve the readability of a bwplot,
 i.e. separating groups somehow
Message-ID: <DAEBLEBOOMBHMLACIMKCEEDBCGAA.OLPantani@unifi.it>

Hi all,
first of all thanks for the answer to my previous question on lattice some
time ago.
In particular to Patrick Connolly for advices on netiquette (I hope this
time I'm doing right....)
and to Deepayan Sarkar fro the help on lattice.

Now, my nowaday problem.

Please consider the following

mydf<-cbind.data.frame(
RESPONSE = c(rnorm(9,rep(2:4,each=3),10),
             rnorm(9,rep(7:9,each=3),10),
             rnorm(9,rep(5:7,each=3),10),
             rnorm(9,rep(9:11,each=3),10)
             ),
STD =   c(rep("Std.Ext",18), rep("Std.Int",18)),
METHOD =  rep(c("A","B"),2, each=9),
VIAL =  rep(paste("Vial",rep(1:3, each=3)),4))
library(lattice)
my.theme<-list(background=list(col=0),
               strip.background=list(col="transparent"),
               box.dot=list(cex=0.1, col=1),
               box.umbrella=list(col=1,lty=1),
               box.rectangle=list(col= 1))
lset(my.theme)
bwplot(paste(METHOD,VIAL)~RESPONSE|STD, data=mydf)

as a (fictitious) experiment on a determination of a substance in 3 vials,
which was quantified
with an external (or internal) standard,
with two different methods (A or B)
with 3 injections per vial (replicates)

I would like to stress the difference between A and B (Method) in the
bwplot,
so I imagine I could distantiate the boxplots, or colour them according to
the "Method".

How can I add a blank line between "A Vial 3" and "B Vial 1"?
Or how can I change the colour of the boxplots? (say the three lower ones in
black, and the rest red)

Thanks

Ottorino-Luca Pantani, Universit?? di Firenze
Dip. Scienza del Suolo e Nutrizione della Pianta
P.zle Cascine 28 50144 Firenze Italia
Tel 39 055 3288 384 Fax 39 055 333 273 OLPantani at unifi.it
http://www.unifi.it/dssnp/



From emathieu at iurc.montp.inserm.fr  Tue Aug  3 13:42:13 2004
From: emathieu at iurc.montp.inserm.fr (Eve Mathieu)
Date: Tue, 03 Aug 2004 13:42:13 +0200
Subject: [R] nlminb vs optim
Message-ID: <5.0.2.1.2.20040803130302.00ab30e8@193.52.202.5>

Dear R-help group,

I have to maximize a likelihood with 40 parameters and I want to compare 
the MLE given by "nlminb" (Splus2000, on Windows) with those given by 
"optim" (R, on Unix).

1) On Splus,
The algorithm "nlminb" seems to converge (the parameters stabilize) , it 
stops after several iterations ( around 400) with the message :"FUNCTION 
EVALUATION LIMIT REACHED" . It gives a set of estimators, namely P, with 
-log(likelihood)=6104.455
	
What does it mean? Is the convergence reached?

2) On R,
Therefore, I initialize the function "optim" with the previous set of 
estimators P, and the algorithm continues the minimisation. It stops and 
gives a
(-log(likelihood))=6104.45, with the messages:

"there are 50 or more warnings"
( warnings() = "multi-arguments returns are deprecated" in a function used 
by the program)

$convergence=0

$message= CONVERGENCE:REL_REDUCTION_OF_F <= FACTR*EPSMCH

What does it mean? Is the convergence reached?

What can it be concluded from these two steps?

Thank you very much for your advices and help.



From andy_liaw at merck.com  Tue Aug  3 14:10:02 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 3 Aug 2004 08:10:02 -0400
Subject: [R] random seed puzzle
Message-ID: <3A822319EB35174CA3714066D590DCD504AF816B@usrymx25.merck.com>

My guess is that
http://cran.r-project.org/src/contrib/Descriptions/rlecuyer.html might be
relevant, too.

Cheers,
Andy

> From: Duncan Murdoch
> 
> On Mon, 2 Aug 2004 17:50:44 -0500, "Bickel, David"
> <DAVID.BICKEL at PIONEER.COM> wrote:
> 
> >After reading the help page on set.seed, I am unsure about 
> how to solve the following problem. I need to call function f 
> a thousand times. The list of values returned by f, should be 
> as random as possible. f calls g twice:
> >	f <- function(){g1 <- g(1); g2 <- g(2); c(g1; g2)}
> >The function g in turn calls sample and returns a number, 
> but also depends on its argument, so, starting from the same 
> seed, g(1) returns a different number than g(2). I need each 
> call to g to start with the same random number seed, so that 
> the values returned by f would not be affected by redefining 
> it this way:
> >	f <- function(){g2 <- g(2); g1 <- g(1); c(g1; g2)}
> 
> This is a problem that's like what people doing simulations on
> parallel computers worry about.  You might want to look at the sprng
> package.  (I haven't used it, so I'm not sure it does what you need). 
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From andy_liaw at merck.com  Tue Aug  3 13:58:24 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 3 Aug 2004 07:58:24 -0400
Subject: [R] dots expansion
Message-ID: <3A822319EB35174CA3714066D590DCD504AF816A@usrymx25.merck.com>

Duncan already gave the answer.  Here's a bit more detail:

rbindGroup <- function(...) {
    theList <- list(...)
    nGroup <- length(theList)
    size <- sapply(theList, nrow)
    result <- rbind(...)
    result <- cbind(result, rep(1:nGroup, each=size)
    result
}

You probably want to add more error checks (like making sure all arrays have
the same number of columns).

HTH,
Andy

> From: Viet Nguyen
> 
> Hi list,
> 
> I'm trying to write a function similar to rbind, except that needs to 
> add a factor to each component array before rbinding them together so 
> that the rows from different arrays are distinguishable.
> 
> The problem that arose is how to loop through arguments in the dots 
> "..." list. I need to get a hand on each of them but don't 
> know how many 
> of them there are and what their names are.
> 
> It'd be useful if I could look at how rbind(...) or c(...) do 
> this but 
> they are both Internal functions.
> 
> Thanks in anticipation of help!
> 
> Regards,
> viet
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Matt.Nunes at bristol.ac.uk  Tue Aug  3 15:30:55 2004
From: Matt.Nunes at bristol.ac.uk (Matt Nunes)
Date: Tue, 03 Aug 2004 14:30:55 +0100
Subject: [R] (no subject)
Message-ID: <15899421.1091543455@pc277.maths.bris.ac.uk>

hello.

I have a query about the Matrix package for R.  I wrote some code a while 
ago using Matrix version 1.6.2 with an early version of R, to do some least 
squares for regression:

xn
     [,1]      [,2]      [,3]      [,4]
[1,]    1 0.7429352 0.5519528 0.4100652
[2,]    1 0.7443713 0.5540886 0.4124477
[3,]    1 0.7447385 0.5546355 0.4130584
[4,]    1 0.7459597 0.5564558 0.4150936
>
> temp<-crossprod(xn)
> temp
         [,1]     [,2]      [,3]      [,4]
[1,] 4.000000 2.978005 2.2171327 1.6506648
[2,] 2.978005 2.217133 1.6506648 1.2289297
[3,] 2.217133 1.650665 1.2289297 0.9149473
[4,] 1.650665 1.228930 0.9149473 0.6811865
>
> solve.Matrix(temp,t(xn))
           [,1]      [,2]      [,3]       [,4]
[1,]   33397.34  122081.7 -241005.4   85527.48
[2,]   21063.72 -664920.2  812316.0 -168459.99
[3,] -236935.74 1125548.2 -877776.2  -10835.64
[4,]  199314.69 -608045.8  297509.1  111221.72

(Note:  here I used solve.Matrix since the generic solve said the matrix is 
singular).

I recently updated my versions of R to 1.9.1 and also Matrix package, but I 
can't seem to get any similar equivalent to work (I get error messages like 
".Call function name not in DLL for package Matrix" or "Lapack routine 
dpotrf returned error code 4").  Can anyone help me out?

many thanks

Matt Nunes



From deepayan at stat.wisc.edu  Tue Aug  3 15:50:05 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Tue, 3 Aug 2004 08:50:05 -0500
Subject: [R] (Lattice) How to improve the readability of a bwplot,
	i.e. separating groups somehow
In-Reply-To: <DAEBLEBOOMBHMLACIMKCEEDBCGAA.OLPantani@unifi.it>
References: <DAEBLEBOOMBHMLACIMKCEEDBCGAA.OLPantani@unifi.it>
Message-ID: <200408030850.05390.deepayan@stat.wisc.edu>

On Tuesday 03 August 2004 06:39, 8rino-Luca Pantani wrote:
> Hi all,
> first of all thanks for the answer to my previous question on lattice
> some time ago.
> In particular to Patrick Connolly for advices on netiquette (I hope
> this time I'm doing right....)
> and to Deepayan Sarkar fro the help on lattice.
>
> Now, my nowaday problem.
>
> Please consider the following
>
> mydf<-cbind.data.frame(
> RESPONSE = c(rnorm(9,rep(2:4,each=3),10),
>              rnorm(9,rep(7:9,each=3),10),
>              rnorm(9,rep(5:7,each=3),10),
>              rnorm(9,rep(9:11,each=3),10)
>              ),
> STD =   c(rep("Std.Ext",18), rep("Std.Int",18)),
> METHOD =  rep(c("A","B"),2, each=9),
> VIAL =  rep(paste("Vial",rep(1:3, each=3)),4))
> library(lattice)
> my.theme<-list(background=list(col=0),
>                strip.background=list(col="transparent"),
>                box.dot=list(cex=0.1, col=1),
>                box.umbrella=list(col=1,lty=1),
>                box.rectangle=list(col= 1))
> lset(my.theme)
> bwplot(paste(METHOD,VIAL)~RESPONSE|STD, data=mydf)
>
> as a (fictitious) experiment on a determination of a substance in 3
> vials, which was quantified
> with an external (or internal) standard,
> with two different methods (A or B)
> with 3 injections per vial (replicates)
>
> I would like to stress the difference between A and B (Method) in the
> bwplot,
> so I imagine I could distantiate the boxplots, or colour them
> according to the "Method".

Why not

bwplot(VIAL ~ RESPONSE | STD + METHOD, data=mydf)

?

> How can I add a blank line between "A Vial 3" and "B Vial 1"?

You could create a factor with a fake blank level. e.g.,


mydf$METHOD.VIAL <-
    factor(with(mydf, paste(METHOD, VIAL)), 
           levels = c("A Vial 1", "A Vial 2", "A Vial 3", 
                      " ", "B Vial 1", "B Vial 2", 
                      "B Vial 3"))

bwplot(METHOD.VIAL ~ RESPONSE | STD, data=mydf, drop = FALSE)


> Or how can I change the colour of the boxplots? (say the three lower
> ones in black, and the rest red)

That's going to be non-trivial. If you are really desparate, look at 

http://tolstoy.newcastle.edu.au/R/help/04/02/0848.html

Hope that helps,

Deepayan



From HDoran at air.org  Tue Aug  3 17:05:18 2004
From: HDoran at air.org (Doran, Harold)
Date: Tue, 3 Aug 2004 11:05:18 -0400
Subject: [R] attach data from tapply to dataframe
Message-ID: <88EAF3512A55DF46B06B1954AEF73F74047BF8DE@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040803/b9ec662a/attachment.pl

From tlumley at u.washington.edu  Tue Aug  3 17:33:29 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 3 Aug 2004 08:33:29 -0700 (PDT)
Subject: [R] nlminb vs optim
In-Reply-To: <5.0.2.1.2.20040803130302.00ab30e8@193.52.202.5>
References: <5.0.2.1.2.20040803130302.00ab30e8@193.52.202.5>
Message-ID: <Pine.A41.4.58.0408030831510.24644@homer07.u.washington.edu>

On Tue, 3 Aug 2004, Eve Mathieu wrote:

> Dear R-help group,
>
> I have to maximize a likelihood with 40 parameters and I want to compare
> the MLE given by "nlminb" (Splus2000, on Windows) with those given by
> "optim" (R, on Unix).
>
> 1) On Splus,
> The algorithm "nlminb" seems to converge (the parameters stabilize) , it
> stops after several iterations ( around 400) with the message :"FUNCTION
> EVALUATION LIMIT REACHED" . It gives a set of estimators, namely P, with
> -log(likelihood)=6104.455
>
> What does it mean? Is the convergence reached?

This is not a good list for asking S-PLUS questions, but it doesn't look
as if it converged.


> 2) On R,
> Therefore, I initialize the function "optim" with the previous set of
> estimators P, and the algorithm continues the minimisation. It stops and
> gives a
> (-log(likelihood))=6104.45, with the messages:
>
> "there are 50 or more warnings"
> ( warnings() = "multi-arguments returns are deprecated" in a function used
> by the program)
>
> $convergence=0
>
> $message= CONVERGENCE:REL_REDUCTION_OF_F <= FACTR*EPSMCH
>
> What does it mean? Is the convergence reached?
>

The help page for optim() says
       convergence: An integer code. '0' indicates successful convergence.

so, 0 indicates successful convergence (in optim()s opinion)

	-thomas



From spencer.graves at pdf.com  Tue Aug  3 18:34:44 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 03 Aug 2004 09:34:44 -0700
Subject: [R] attach data from tapply to dataframe
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F74047BF8DE@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F74047BF8DE@dc1ex2.air.org>
Message-ID: <410FBEA4.3020003@pdf.com>

Have you considered "cbind" and "rbind"?  If your data.frame has 
factors, they could present problems with "rbind".  Try 
'sapply(tenn.dat, class)'.  If you have only class character or numeric, 
use "cbind" to match the columns of tenn.dat both in name and class, 
then use "rbind" to combine it with the original. 

hope this helps.  spencer

Doran, Harold wrote:

>I am working with a longitudinal data set in the long format. This data
>set has three observations per grade level per year. Here are the first
>10 rows of the data frame:
>
> 
>
>  
>
>>tenn.dat[1:10,]
>>    
>>
>
> 
>
>year  schid type grade gain  se new cohort
>
>6  2001 100005    5     4 33.1 3.5   4      3
>
>7  2002 100005    5     4 33.9 3.9   4      2
>
>8  2003 100005    5     4 32.3 4.2   4      1
>
>10 2001 100005    5     5 22.9 4.0   5      4
>
>11 2002 100005    5     5 25.0 3.4   5      3
>
>12 2003 100005    5     5  7.8 3.8   5      2
>
>18 2001 100010    1     4 34.4 5.9   4      3
>
>19 2002 100010    1     4 27.8 5.6   4      2
>
>20 2003 100010    1     4 34.6 6.8   4      1
>
>22 2001 100010    1     5 21.1 4.8   5      4
>
> 
>
>I need to create a new column in this data frame with the mean gain for
>each grade by year and the sd for each grade by year.
>
> 
>
>So, I used tapply as follows:
>
> 
>
>tapply(tenn.dat[,5],tenn.dat[,c(1,4)],mean) and
>tapply(tenn.dat[,5],tenn.dat[,c(1,4)],sd)  which produces exactly the
>data I would like to attach in column 1 and 2 respectively. 
>
> 
>
>I am having a problem connecting this back with the corresponding rows
>in the data frame.
>
> 
>
>If I used only one factor instead of two, I was successful connecting
>this with the data frame using:
>
> 
>
>m.gain<-tapply(tenn.dat[,5],tenn.dat[,4],mean)
>
> 
>
>tenn.dat$m.gain<-m.gain[as.character(tenn.dat$grade)]
>
> 
>
>can anyone offer suggestions on a next step?
>
> 
>
>Thanks,
>
> 
>
>Harold
>
> 
>
> 
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From ggrothendieck at myway.com  Tue Aug  3 18:58:04 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 3 Aug 2004 16:58:04 +0000 (UTC)
Subject: [R] attach data from tapply to dataframe
References: <88EAF3512A55DF46B06B1954AEF73F74047BF8DE@dc1ex2.air.org>
Message-ID: <loom.20040803T185550-150@post.gmane.org>

Doran, Harold <HDoran <at> air.org> writes:

: 
: I am working with a longitudinal data set in the long format. This data
: set has three observations per grade level per year. Here are the first
: 10 rows of the data frame:
: 
: >tenn.dat[1:10,]
: 
: year  schid type grade gain  se new cohort
: 
: 6  2001 100005    5     4 33.1 3.5   4      3
: 
: 7  2002 100005    5     4 33.9 3.9   4      2
: 
: 8  2003 100005    5     4 32.3 4.2   4      1
: 
: 10 2001 100005    5     5 22.9 4.0   5      4
: 
: 11 2002 100005    5     5 25.0 3.4   5      3
: 
: 12 2003 100005    5     5  7.8 3.8   5      2
: 
: 18 2001 100010    1     4 34.4 5.9   4      3
: 
: 19 2002 100010    1     4 27.8 5.6   4      2
: 
: 20 2003 100010    1     4 34.6 6.8   4      1
: 
: 22 2001 100010    1     5 21.1 4.8   5      4
: 
: 
: I need to create a new column in this data frame with the mean gain for
: each grade by year and the sd for each grade by year.
: 
: So, I used tapply as follows:
: 
: tapply(tenn.dat[,5],tenn.dat[,c(1,4)],mean) and
: tapply(tenn.dat[,5],tenn.dat[,c(1,4)],sd)  which produces exactly the
: data I would like to attach in column 1 and 2 respectively. 
: 
: I am having a problem connecting this back with the corresponding rows
: in the data frame.
: 
: If I used only one factor instead of two, I was successful connecting
: this with the data frame using:
: 
: m.gain<-tapply(tenn.dat[,5],tenn.dat[,4],mean)
: 
: tenn.dat$m.gain<-m.gain[as.character(tenn.dat$grade)]
: 
: can anyone offer suggestions on a next step?


Suggest you use by, instead of  tapply, like this:

  f <- function(x) { x$mean.gain <- mean(x$gain); x$sd.gain <- sd(x$gain); x }
  res <- by(tenn, list(tenn$year, tenn$grade), f)
  do.call("rbind", res)



From schween at snafu.de  Tue Aug  3 19:46:30 2004
From: schween at snafu.de (Sven C. Koehler)
Date: Tue, 3 Aug 2004 19:46:30 +0200
Subject: [R] Using MASSv3's example from 8.7 in R?
Message-ID: <20040803174630.GA15548@coredump>

Dear list!

I am interested in learning about MLE and I wonder whether it is possible
to use the examples for maximum likelihood estimation given in 8.7 in
MASSv3 with R?  AFAIU R does not have a direct replacement for S-PLUS's
ms() which the examples use for the fitting, but optim() may be of help
for me.  However, I am not sure how I can convert the use of ms() using
optim()--is there an explanation how to do this somewhere?  Thanks.

Best wishes,

Sven C. Koehler

=========================================================================
Example from MASSv3 8.7
=========================================================================
library(MASS)
data(geyser)

attach(geyser)
truehist(waiting, xlim=c(35,110), ymax=0.04, h=5)
width.SJ(waiting)
wait.dns <- density(waiting, 200, width=10.24)
lines(wait.dns, lty=2)

lmix2 <- deriv3(
    ~ -log(p*dnorm((x-u1)/s1)/s1 + (1-p)*dnorm((x-u2)/s2)/s2),
    c("p", "u1", "s1", "u2", "s2"),
    function(x, p, u1, s1, u2, s2) NULL)

p0 <- c(p=mean(waiting < 70), u1=50, s1=5, u2=80, s2=5)

tr.ms <- function(info, theta, grad, scale, flags, fit.pars) {
    cat(round(info[3], 3), ":", siginf(theta), "\n")
    invisible()
}

wait.mix2 <- ms(~ lmix2(waiting, p, u1, s1, u2, s2),
        start=p0, data=geyser, trace=tr.ms)



From tiago17 at socrates.Berkeley.EDU  Tue Aug  3 19:50:45 2004
From: tiago17 at socrates.Berkeley.EDU (Tiago R Magalhaes)
Date: Tue, 3 Aug 2004 18:50:45 +0100
Subject: [R] basic questions: any place for them
Message-ID: <p06100500bd357f6495e6@[83.132.28.117]>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040803/9f2542cb/attachment.pl

From andy_liaw at merck.com  Tue Aug  3 20:05:24 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 3 Aug 2004 14:05:24 -0400
Subject: [R] basic questions: any place for them
Message-ID: <3A822319EB35174CA3714066D590DCD504AF8181@usrymx25.merck.com>

> From: Tiago R Magalhaes
> 
> Hi
> I have two basic questions, and here they go, but I was wondering as 
> well where can I ask these basic questions without bothering  you 
> people
> 
> 
> I've used Splus and now I'm using R and there's some functions that I 
> can't simply find
> 
> one:
> sort.col
> that allows data.frames to be sort by a given col
> (I saw the funtion sort but that's for vectors. and I can't believe I 
> can't find the function for data.frames)

This is sort of an FAQ.  The short answer is to use order().  I've posted a
suggestion for a sort() method for data.frame the last time this was asked.
You can find it in the list archive.
 
> the other function I wanted to know:
> convert.col.type
> that converts a given column of a data frame from let's say integer 
> to characters

This can usually be done by something like:

dat$thiscol <- as.blah(dat$thiscol)

[replace `as.blah' appropriately; e.g., as.character(), etc.]  Just be
careful with factors.

Andy

 
> Once again I'm sorry for these basic questions and since predictably 
> I'll have more of those if there's a basic-questions-list I would 
> love to know more about it
> 
> thanks
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From peter at ccc.mcmaster.ca  Tue Aug  3 20:07:51 2004
From: peter at ccc.mcmaster.ca (Peter Tait)
Date: Tue, 03 Aug 2004 14:07:51 -0400
Subject: [R] Adjusting two continuous variables by one continuous variable
Message-ID: <1c935d1c9301.1c93011c935d@cardio.on.ca>

Hi,

I want to look at the relationship between 2 continuous/quantitative variables while adjusting for a third continuous variable. For example: relationship between height and weight adjusted for age.
As I understand it using a glm would work if I had a categorical variable (I could make age into a categories) but I am interested to know if anyone knows how I can do this analysis with out making age into categories.

Any help would be appreciated.
Cheers
Peter



From gunter.berton at gene.com  Tue Aug  3 20:10:15 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 03 Aug 2004 11:10:15 -0700
Subject: [R] basic questions: any place for them
References: <p06100500bd357f6495e6@[83.132.28.117]>
Message-ID: <410FD507.208AEF5B@gene.com>

As you know, R is not S-Plus, though they for the most part share a common syntax
(S) and a lot of (nearly) identical functions with (nearly) identical behavior.
Nevertheless, there's lots of stuff in S-Plus that's not in R and vice-versa. So
you shouldn't expect to find all S-Plus functions in R. Almost certainly, the
functionality will be in R to write them yourself (and vice-versa).Less
certainly, there may be a function by a different name that does the same thing.
For this you should search Help files (?help.search) and the online search
facilities at CRAN first before posting to this list.

Cheers,
Bert

--

Bert Gunter

Non-Clinical Biostatistics
Genentech
MS: 240B
Phone: 650-467-7374


"The business of the statistician is to catalyze the scientific learning
process."

 -- George E.P. Box



From andy_liaw at merck.com  Tue Aug  3 20:18:36 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 3 Aug 2004 14:18:36 -0400
Subject: [R] Adjusting two continuous variables by one continuous vari able
Message-ID: <3A822319EB35174CA3714066D590DCD504AF8183@usrymx25.merck.com>

Graphically, you can use coplot() or lattice to see how the relationship
between height and weight changes with age.

With a linear model, you can do something like:

lm(height ~ weight * age, ...)

and work out the interpretation of the coefficients.

HTH,
Andy

> From: Peter Tait
> 
> Hi,
> 
> I want to look at the relationship between 2 
> continuous/quantitative variables while adjusting for a third 
> continuous variable. For example: relationship between height 
> and weight adjusted for age.
> As I understand it using a glm would work if I had a 
> categorical variable (I could make age into a categories) but 
> I am interested to know if anyone knows how I can do this 
> analysis with out making age into categories.
> 
> Any help would be appreciated.
> Cheers
> Peter



From ripley at stats.ox.ac.uk  Tue Aug  3 20:42:12 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 3 Aug 2004 19:42:12 +0100 (BST)
Subject: [R] Using MASSv3's example from 8.7 in R?
In-Reply-To: <20040803174630.GA15548@coredump>
Message-ID: <Pine.LNX.4.44.0408031938110.7226-100000@gannet.stats>

It is in MASS4, and even in the R scripts for MASS3 in MASS/scripts3 in 
your R installation.

And that is explained in the R complements on the book's web site.

On Tue, 3 Aug 2004, Sven C. Koehler wrote:

> Dear list!
> 
> I am interested in learning about MLE and I wonder whether it is possible
> to use the examples for maximum likelihood estimation given in 8.7 in
> MASSv3 with R?  AFAIU R does not have a direct replacement for S-PLUS's
> ms() which the examples use for the fitting, but optim() may be of help
> for me.  However, I am not sure how I can convert the use of ms() using
> optim()--is there an explanation how to do this somewhere?  Thanks.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rolf at math.unb.ca  Tue Aug  3 23:00:09 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Tue, 3 Aug 2004 18:00:09 -0300 (ADT)
Subject: [R] keep.source.pkgs()
Message-ID: <200408032100.i73L09jk000058@erdos.math.unb.ca>


Recently Brian Ripley had occasion to mock my inability to see a
comment in the code for arima(), in the stats package.  After
considerable dredging around in the r-news archives I found reference
to keep.source() and keep.source.pkgs(), which I conjectured just
***might*** possibly be the ``obvious'' resource to which Prof.
Ripley thought I ought to have resorted.

However, after substantial experimentation, I am still at a loss to
get at the original source code for arima() (replete with comments)
save by accessing the original, ``unmade'' source for R.  Not too
hard, but not exactly ``immediate''.

In the course of experimenting with keep.source.pkgs() I encountered
behaviour which I found mysterious and which I'd like to understand
better.  Also if there is an easier way to get at comments in code of
``system'' functions I'd like to know about it.

In experimenting with keep.source.pkgs() I did the following

	o executed options("keep.source.pkgs"), and got ``FALSE''
	o executed library(spatstat)
	o typed the name of a function in spatstat whose code has
	  comments (rmh.ppm) --- no comments appeared.
	o executed options(keep.source.pkgs=TRUE)
	o typed ``rmh.ppm'' again --- still no comments, as is to
	  be expected.
	o detached spatstat
	o reloaded it
	o type ``rmh.ppm'' again --- comments were there in all
	  their glory.  OMMMMMMMMMMMMMMMMM!

Then I went through the same procedure with the package util and the
function vignette therefrom.  (This function has comments in its
code.)  Except that I didn't do an initial library(util), since util
gets loaded automatically on startup.

After setting keep.source.pkgs to TRUE via options(), detaching util,
and reloading it --- still no comments, and attr(vignette,"source")
was NULL.

The help for options() says that keep.source.pkgs defaults to TRUE
if the environment variable R_KEEP_PKG_SOURCE is set to `yes'.
So my next experiment was to stop R, set this variable, restart
R, and then type ``vignette''.  Bingo.  Comments appeared, and
attr(vignette,"source") was there bigtime.

However attr(arima,"source") was still NULL (and of course comments
were non-existant).  Apparently the functions in stats get
***loaded*** from a binary file ``all.rda'' (in the
.../library/stats/R directory) rather than getting read in from ascii
files.  (One hesitates to ask ***why***, but .... why?)  Anyhow, I
guess that the binary versions in all.rda got built effectively with
keep.source.pkgs=FALSE, and so starting R with keep.source.pkgs=TRUE
doesn't help in this case.

So, some questions:

	1) Why do ``system'' packages, e.g. util, behave differently
	from ``optional'' packages?  I.e. why does the sequence

		. detach package
		. set keep.source.pkgs=TRUE
		. re-load package

	``work'', i.e. make the source available for functions in
	optional packages but not for functions in system packages?

	2) Is there a better way of getting to see the
	code-with-comments of system functions, rather than dicking
	around with the R_KEEP_PKG_SOURCE environment variable?

	3) Is there any way, other than retrieving the unmade source
	of R, to see the commented source code of functions in the
	stat package (and presumably methods, stats4, lattice, and
	nlme which also boast all.rda files)?

I wonder if someone with better people skills and more patience
than Prof. Ripley would be so kind as to answer!

				cheers,

					Rolf Turner
					rolf at math.unb.ca



From ihok at hotmail.com  Tue Aug  3 23:22:04 2004
From: ihok at hotmail.com (Jack Tanner)
Date: Tue, 03 Aug 2004 17:22:04 -0400
Subject: [R] write.table(NULL)
Message-ID: <411001FC.6060702@hotmail.com>

 > write.table(NULL)
Error in which(unlist(lapply(x, function(x) is.character(x) || 
is.factor(x)))) : argument to "which" is not logical

Is this correct behavior? It seems harsh to abort an entire run just 
because one of the tables you generated happened to be NULL.

-JT



From tlumley at u.washington.edu  Tue Aug  3 23:32:29 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 3 Aug 2004 14:32:29 -0700 (PDT)
Subject: [R] keep.source.pkgs()
In-Reply-To: <200408032100.i73L09jk000058@erdos.math.unb.ca>
References: <200408032100.i73L09jk000058@erdos.math.unb.ca>
Message-ID: <Pine.A41.4.58.0408031420490.209208@homer07.u.washington.edu>

On Tue, 3 Aug 2004, Rolf Turner wrote:
>
> However, after substantial experimentation, I am still at a loss to
> get at the original source code for arima() (replete with comments)
> save by accessing the original, ``unmade'' source for R.  Not too
> hard, but not exactly ``immediate''.

Personally, I find this the easiest way to look at the source. You can
browse the source on https://svn.r-project.org/R/ if you are using a
binary distribution, rather than downloading the whole thing.


> In the course of experimenting with keep.source.pkgs() I encountered
> behaviour which I found mysterious and which I'd like to understand
> better.  Also if there is an easier way to get at comments in code of
> ``system'' functions I'd like to know about it.
>
> In experimenting with keep.source.pkgs() I did the following

> 	o executed options("keep.source.pkgs"), and got ``FALSE''
> 	o executed library(spatstat)
> 	o typed the name of a function in spatstat whose code has
> 	  comments (rmh.ppm) --- no comments appeared.
> 	o executed options(keep.source.pkgs=TRUE)
> 	o typed ``rmh.ppm'' again --- still no comments, as is to
> 	  be expected.
> 	o detached spatstat
> 	o reloaded it
> 	o type ``rmh.ppm'' again --- comments were there in all
> 	  their glory.  OMMMMMMMMMMMMMMMMM!

Yes.

> Then I went through the same procedure with the package util and the
> function vignette therefrom.  (This function has comments in its
> code.)  Except that I didn't do an initial library(util), since util
> gets loaded automatically on startup.
>
> After setting keep.source.pkgs to TRUE via options(), detaching util,
> and reloading it --- still no comments, and attr(vignette,"source")
> was NULL.
>
> The help for options() says that keep.source.pkgs defaults to TRUE
> if the environment variable R_KEEP_PKG_SOURCE is set to `yes'.
> So my next experiment was to stop R, set this variable, restart
> R, and then type ``vignette''.  Bingo.  Comments appeared, and
> attr(vignette,"source") was there bigtime.

Isn't that nice.

> However attr(arima,"source") was still NULL (and of course comments
> were non-existant).  Apparently the functions in stats get
> ***loaded*** from a binary file ``all.rda'' (in the
> .../library/stats/R directory) rather than getting read in from ascii
> files.  (One hesitates to ask ***why***, but .... why?)

Because R starts much faster that way.

>							  Anyhow, I
> guess that the binary versions in all.rda got built effectively with
> keep.source.pkgs=FALSE, and so starting R with keep.source.pkgs=TRUE
> doesn't help in this case.
>
> So, some questions:
>
> 	1) Why do ``system'' packages, e.g. util, behave differently
> 	from ``optional'' packages?  I.e. why does the sequence
>
> 		. detach package
> 		. set keep.source.pkgs=TRUE
> 		. re-load package
>
> 	``work'', i.e. make the source available for functions in
> 	optional packages but not for functions in system packages?

Because, as you noted, the base packages are stored in binary form.  This
already speeds things up, and will have even more impact in 2.0.0 with
"lazy loading" of functions.


> 	2) Is there a better way of getting to see the
> 	code-with-comments of system functions, rather than dicking
> 	around with the R_KEEP_PKG_SOURCE environment variable?

Downloading the source, or looking at svn.r-project.org

> 	3) Is there any way, other than retrieving the unmade source
> 	of R, to see the commented source code of functions in the
> 	stat package (and presumably methods, stats4, lattice, and
> 	nlme which also boast all.rda files)?

It doesn't look like it.

> I wonder if someone with better people skills and more patience
> than Prof. Ripley would be so kind as to answer!


  When a Certain Guru rips strips off people (God knows he's done it to me
  often enough) on this list, there's a damned good reason for it.
   -- Rolf Turner (in a discussion about whether a friendly mailing list with
      more `customer service' attitude than R-help was needed)
      R-help (December 2003)


	-thomas



From tlumley at u.washington.edu  Tue Aug  3 23:42:40 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 3 Aug 2004 14:42:40 -0700 (PDT)
Subject: [R] write.table(NULL)
In-Reply-To: <411001FC.6060702@hotmail.com>
References: <411001FC.6060702@hotmail.com>
Message-ID: <Pine.A41.4.58.0408031436230.209208@homer07.u.washington.edu>

On Tue, 3 Aug 2004, Jack Tanner wrote:

>  > write.table(NULL)
> Error in which(unlist(lapply(x, function(x) is.character(x) ||
> is.factor(x)))) : argument to "which" is not logical
>
> Is this correct behavior? It seems harsh to abort an entire run just
> because one of the tables you generated happened to be NULL.
>

Well, yes, in a perfect world write.table(NULL) would just write no
output.  It's arguably even a bug, or at least the fact that the same
thing happens with a zero-length data frame is arguably a bug. I'll fix
it.

There are quite a lot of functions that treat you harshly when you supply
zero-length input.  This is usually for a similar reason to the one here:
when lapply returns no output the information as to whether it is no
logical values or no vectors or no whatever else gets lost.  It's a good
idea to be cautious about zero-length variables as a programmer.


	-thomas



From drf5n at maplepark.com  Tue Aug  3 23:54:04 2004
From: drf5n at maplepark.com (drf5n@maplepark.com)
Date: Tue, 3 Aug 2004 16:54:04 -0500 (CDT)
Subject: [R] random seed puzzle
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF816B@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF816B@usrymx25.merck.com>
Message-ID: <Pine.LNX.4.58.0408031519300.16797@maplepark.com>

> > On Mon, 2 Aug 2004 17:50:44 -0500, "Bickel, David"
> > <DAVID.BICKEL at PIONEER.COM> wrote:
> >
> > >After reading the help page on set.seed, I am unsure about
> > how to solve the following problem. I need to call function f
> > a thousand times. The list of values returned by f, should be
> > as random as possible. f calls g twice:
> > >	f <- function(){g1 <- g(1); g2 <- g(2); c(g1; g2)}
> > >The function g in turn calls sample and returns a number,
> > but also depends on its argument, so, starting from the same
> > seed, g(1) returns a different number than g(2). I need each
> > call to g to start with the same random number seed, so that
> > the values returned by f would not be affected by redefining
> > it this way:
> > >	f <- function(){g2 <- g(2); g1 <- g(1); c(g1; g2)}


You may need 2 or three independent streams of random numbers, and may
have to globally maintain their state variables.

rng.streams<-list()
set.seed(1); rng.streams[[1]]<-.Random.seed
set.seed(1964); rng.streams[[2]]<-.Random.seed
set.seed(1965); rng.streams[[3]]<-.Random.seed

g<-function(){print(sample(1:sample(1:10,1)))}

f<-function(){
  x<-rnorm(1)  # use a system rn
  rng.saved<-.Random.seed   # save system rng state
  .Random.seed<<-rng.streams[[1]] #  F's stream
  x[2]<-rnorm(1)
  rng.streams[[1]]<<-.Random.seed
  .Random.seed<<-rng.streams[[2]]  # 1st G stream
  g()
  rng.streams[[2]]<<-.Random.seed
  .Random.seed<<-rng.streams[[3]]  # get 2nd g stream
  g()
  rng.streams[[3]]<<-.Random.seed  # save 2nd G stream

  .Random.seed<<-rng.saved # restore system rng state
 x
}

rng.streams<-list()
set.seed(1); rng.streams[[1]]<-.Random.seed
set.seed(1964); rng.streams[[2]]<-.Random.seed
set.seed(1965); rng.streams[[3]]<-.Random.seed
f()
f()
f()
set.seed(1964); rng.streams[[2]]<-.Random.seed
f()  # note that the 1st g() sample is reset

You could also save the rng state in f() before the first call to g(),
and restore it before the second call to g(), so the two calls use
effectively one g() worth of variates from a single system rng stream, but
depending on the g() internals, there could be unintended correlation
between the g() results.

good luck,
Dave
-- 
 Dave Forrest
 drf at vims.edu                                    (804)684-7900w
 drf5n at maplepark.com                             (804)642-0662h
                                   http://maplepark.com/~drf5n/



From ripley at stats.ox.ac.uk  Tue Aug  3 23:55:37 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 3 Aug 2004 22:55:37 +0100 (BST)
Subject: [R] write.table(NULL)
In-Reply-To: <Pine.A41.4.58.0408031436230.209208@homer07.u.washington.edu>
Message-ID: <Pine.LNX.4.44.0408032247260.10686-100000@gannet.stats>

On Tue, 3 Aug 2004, Thomas Lumley wrote:

> On Tue, 3 Aug 2004, Jack Tanner wrote:
> 
> >  > write.table(NULL)
> > Error in which(unlist(lapply(x, function(x) is.character(x) ||
> > is.factor(x)))) : argument to "which" is not logical
> >
> > Is this correct behavior? It seems harsh to abort an entire run just
> > because one of the tables you generated happened to be NULL.
> >
> 
> Well, yes, in a perfect world write.table(NULL) would just write no
> output.  It's arguably even a bug, or at least the fact that the same
> thing happens with a zero-length data frame is arguably a bug. I'll fix
> it.

It's zero-column cases it gets in trouble with: zero-row cases are handled 
correctly AFAICS.  Depends what `zero-length' means.

We should change it, although what is the right output is less clear to
me. You can have zero-columns and non-zero rows and with row names.
Compare

> hills[FALSE]
NULL data frame with 35 rows

with
> as.matrix(hills[FALSE])

(a column of rownames).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rossini at blindglobe.net  Wed Aug  4 00:00:02 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Tue, 03 Aug 2004 15:00:02 -0700
Subject: [R] keep.source.pkgs()
In-Reply-To: <Pine.A41.4.58.0408031420490.209208@homer07.u.washington.edu>
	(Thomas
	Lumley's message of "Tue, 3 Aug 2004 14:32:29 -0700 (PDT)")
References: <200408032100.i73L09jk000058@erdos.math.unb.ca>
	<Pine.A41.4.58.0408031420490.209208@homer07.u.washington.edu>
Message-ID: <85acxbkhlp.fsf@servant.blindglobe.net>

Thomas Lumley <tlumley at u.washington.edu> writes:

> Personally, I find this the easiest way to look at the source. You can
> browse the source on https://svn.r-project.org/R/ if you are using a
> binary distribution, rather than downloading the whole thing.

Except that you probably don't want to, i.e. grab a copy via a
subversion client rather than a WWW-server.


-- 
Anthony Rossini			    Research Associate Professor
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From rolf at math.unb.ca  Wed Aug  4 00:34:32 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Tue, 3 Aug 2004 19:34:32 -0300 (ADT)
Subject: [R] keep.source.pkgs()
Message-ID: <200408032234.i73MYWXl002135@erdos.math.unb.ca>


Thomas Lumley wrote:

> > However attr(arima,"source") was still NULL (and of course comments
> > were non-existant).  Apparently the functions in stats get
> > ***loaded*** from a binary file ``all.rda'' (in the
> > .../library/stats/R directory) rather than getting read in from ascii
> > files.  (One hesitates to ask ***why***, but .... why?)
> 
> Because R starts much faster that way.

	Ah-ha!!!  That indeed makes perfect sense.  Thank you.

> > So, some questions:
> >
> > 	1) Why do ``system'' packages, e.g. utils, behave differently
> > 	from ``optional'' packages?  I.e. why does the sequence
> >
> > 		. detach package
> > 		. set keep.source.pkgs=TRUE
> > 		. re-load package
> >
> > 	``work'', i.e. make the source available for functions in
> > 	optional packages but not for functions in system packages?
> 
> Because, as you noted, the base packages are stored in binary form.  This
> already speeds things up, and will have even more impact in 2.0.0 with
> "lazy loading" of functions.

	This CANNOT be the (complete) explanation, because the utils
	package behaves differently from the stats package.

	What I mean is, your explanation explains why you never
	get the source attributes ***at all*** with the stats
	package.

	It doesn't seem to me to explain why setting
	keep.source.pkgs=TRUE, detaching and reloading doesn't work
	for utils, whereas effectively setting keep.source.pkgs=TRUE
	``a priori'' and then loading utils ***does*** work.

	And ***nothing*** works with the stats package. Moreover in
	.../library/utils/R you find the ascii source file ``utils''
	and ***NOT*** a binary ``all.rda'' file.

> > I wonder if someone with better people skills and more patience
> > than Prof. Ripley would be so kind as to answer!
> 
>   When a Certain Guru rips strips off people (God knows he's done it to me
>   often enough) on this list, there's a damned good reason for it.
>    -- Rolf Turner (in a discussion about whether a friendly mailing list with
>       more `customer service' attitude than R-help was needed)
>       R-help (December 2003)

	Oh, far be it from me to disagree.  Especially with myself!
	I do not dispute the right to rip strips.  I just wanted a
	comprehensible answer to my question, that's all.

				cheers,

					Rolf Turner
					rolf at math.unb.ca



From tlumley at u.washington.edu  Wed Aug  4 00:59:04 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 3 Aug 2004 15:59:04 -0700 (PDT)
Subject: [R] keep.source.pkgs()
In-Reply-To: <200408032234.i73MYWXl002135@erdos.math.unb.ca>
References: <200408032234.i73MYWXl002135@erdos.math.unb.ca>
Message-ID: <Pine.A41.4.58.0408031549180.209208@homer07.u.washington.edu>

On Tue, 3 Aug 2004, Rolf Turner wrote:

>
> Thomas Lumley wrote:
>
> > Because, as you noted, the base packages are stored in binary form.  This
> > already speeds things up, and will have even more impact in 2.0.0 with
> > "lazy loading" of functions.
>
> 	This CANNOT be the (complete) explanation, because the utils
> 	package behaves differently from the stats package.
>
> 	What I mean is, your explanation explains why you never
> 	get the source attributes ***at all*** with the stats
> 	package.
>
> 	It doesn't seem to me to explain why setting
> 	keep.source.pkgs=TRUE, detaching and reloading doesn't work
> 	for utils, whereas effectively setting keep.source.pkgs=TRUE
> 	``a priori'' and then loading utils ***does*** work.
>
> 	And ***nothing*** works with the stats package. Moreover in
> 	.../library/utils/R you find the ascii source file ``utils''
> 	and ***NOT*** a binary ``all.rda'' file.

That is why something works for utils and nothing works for stats.

 I can't remember the historical reason why utils isn't loaded in binary
form in 1.9.1.  In r-devel they are all loaded from binary databases when
used.

The reason why detach and reload doesn't work but restarting does work for
utils is that detach() does not unload the package, it only detaches it.
This distinction was introduced with namespaces.  A package can be loaded
but not in the search path.

For example

    > detach(utils)
    > help(detach)
    Error: couldn't find function "help"

but

    > utils::help(detach)

still works, showing that the package is loaded but not attached. If you
try to unload it

    > unloadNamespace("utils")
    Error in unloadNamespace("utils") : name space still used by: stats,
    methods

you can't.


	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From jawegelin at ucdavis.edu  Wed Aug  4 01:15:29 2004
From: jawegelin at ucdavis.edu (Jacob Wegelin)
Date: Tue, 3 Aug 2004 16:15:29 -0700 (PDT)
Subject: [R] lme fitted correlation of random effects: where is it? 
Message-ID: <Pine.OSX.4.53.0408031533110.721@biostat5.ucdavis.edu>


The print method for lme *prints out* the fitted correlation matrix for
the random effects. Is there any way to get these values as an object in
R?  I have examined the components of the lme object (called "junk" in the
example below) and the components of summary(junk) without finding these
numbers.

(How I did this: I dumped the entire lme object to a text file and then
used egrep to search the text file for one of the long strings of digits
that are in the printout. The string was not in the file.  It appears that
the print method computes the fitted correlation matrix on the fly??
Or where does it get it?)

Below is the printout of the correlation matrix. One could print it to a
file and then use perl to get the numbers back, but that seems a bit
circuitous. What am I overlooking?

Of course, this correlation matrix is not the sample correlation matrix of
the random effects, as is confirmed below.

Thanks for any ideas.

Jake Wegelin

> junk
Linear mixed-effects model fit by REML
  Data: blockmat
  Log-restricted-likelihood: -426.3231
  Fixed: RESP ~ -1 + intS + tS + intC + tC + intA + tA
        intS           tS         intC           tC         intA           tA
-0.049391372  0.034116310 -0.003453588  0.200868693  0.019390920 -0.233687228

Random effects:
 Formula: ~(-1 + intS + tS) + (-1 + intC + tC) + (-1 + intA + tA) | ID
 Structure: General positive-definite, Log-Cholesky parametrization
         StdDev    Corr
intS     0.2891005 intS   tS     intC   tC     intA
tS       0.5652176  0.026
intC     0.3550297  0.661  0.144
tC       0.5335840 -0.078  0.563 -0.008
intA     0.4261338 -0.560  0.109  0.034  0.189
tA       0.6932241 -0.092  0.423  0.058  0.096 -0.141
Residual 0.5847087

Number of Observations: 360
Number of Groups: 40

> cor(junk$coefficients$random$ID)
            intS         tS         intC           tC        intA          tA
intS  1.00000000 0.00969535  0.726342257 -0.113025813 -0.63004526 -0.08474931
tS    0.00969535 1.00000000  0.189830146  0.697937620  0.13437668  0.51908803
intC  0.72634226 0.18983015  1.000000000  0.005010784 -0.01763164  0.07153269
tC   -0.11302581 0.69793762  0.005010784  1.000000000  0.25276614  0.14386815
intA -0.63004526 0.13437668 -0.017631636  0.252766142  1.00000000 -0.16658238
tA   -0.08474931 0.51908803  0.071532693  0.143868146 -0.16658238  1.00000000

> version
         _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    1
minor    9.1
year     2004
month    06
day      21
language R



From bido at mac.com  Wed Aug  4 01:50:09 2004
From: bido at mac.com (Francisco J. Bido)
Date: Tue, 3 Aug 2004 18:50:09 -0500
Subject: [R] Installing tseries package on MacOSX
In-Reply-To: <Pine.LNX.4.44.0408030719000.24028-100000@gannet.stats>
References: <Pine.LNX.4.44.0408030719000.24028-100000@gannet.stats>
Message-ID: <DA954513-E5A7-11D8-ABCC-000393B90A0A@mac.com>

Thanks for the tip.

-F.

On Aug 3, 2004, at 1:21 AM, Prof Brian Ripley wrote:

> On Tue, 3 Aug 2004, Francisco J. Bido wrote:
>
>> I have R 1.9.1 on MacOSX.  The automated package download appears
>> broken in this version.
>
> Your version?  It is not broken in the version R-core distributes, but 
> you
> may be using it incorrectly.
>
>> Is there any documentation on how to compile
>> and install this the tseries package on MacOSX?  I've looked around 
>> and
>> tried a few things but with luck.
>
> Get the tarball from CRAN, called tseries_0.9-22.tar.gz.
> In a shell, run  R CMD INSTALL tseries_0.9-22.tar.gz.
> That's it.
>
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
> PLEASE do so, and give some useful information about what you are 
> doing so
> we can help you correct it.
>
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From PAlspach at hortresearch.co.nz  Wed Aug  4 02:23:15 2004
From: PAlspach at hortresearch.co.nz (Peter Alspach)
Date: Wed, 04 Aug 2004 12:23:15 +1200
Subject: [R] Standard errors from glm
Message-ID: <s110d549.052@hrp3.palm.cri.nz>


Dear Prof Ripley

Thanks for your reply and clarification.  However:

1.  Regarding model.tables() returning "Design is unbalanced".  Setting contrasts to Helmert does indeed make the design balanced, but model.tables() still returns "Design is unbalanced":

> options()$contrasts
        unordered           ordered 
"contr.treatment"      "contr.poly" 
> aov(S~rep+trt1*trt2*trt3, data=dummy.data)
Call:
...
Residual standard error: 14.59899 
Estimated effects may be unbalanced
> options(contrasts=c("contr.helmert", "contr.treatment"))
> aov(S~rep+trt1*trt2*trt3, data=dummy.data)
Call:
...
Residual standard error: 14.59899 
Estimated effects are balanced
> model.tables(aov(S~rep+trt1*trt2*trt3, data=dummy.data), se=T)
Design is unbalanced - use se.contrasts for se's
Tables of effects
...

However, this is a relatively minor issue, and covered in ?model.tables which clearly states that "The implementation is incomplete, and only the simpler cases have been tested thoroughly."

2.  You point out that "In either case you can predict something you want to estimate and use
predict(, se=TRUE)."  Doesn't this give the standard error of the predicted value, rather than the mean for, say, trt1 level 0?  For example:
> predict(temp.lm, newdata=data.frame(rep='1', trt1='0', trt2='1', trt3='0'), se=T)
$fit
[1] 32

$se.fit
[1] 10.53591

$df
[1] 23

$residual.scale
[1] 14.59899

Whereas from the analysis of variance table we can get the standard error of the mean for trt1 as being sqrt(anova(temp.lm)[9,3]/12) = 4.214365.  It is the equivalent of this latter value that I'm after in the glm() case.


>>> Prof Brian Ripley <ripley at stats.ox.ac.uk> 03/08/04 18:10:56 >>>
On Tue, 3 Aug 2004, Peter Alspach wrote:

[Lines wrapped for legibility.]

> I'm having a little difficulty getting the correct standard errors from
> a glm.object (R 1.9.0 under Windows XP 5.1).  predict() will gives
> standard errors of the predicted values, but I am wanting the standard
> errors of the mean.
> 
> To clarify:
> 
> Assume I have a 4x3x2 factorial with 2 complete replications (i.e. 48
> observations, I've appended a dummy set of data at the end of this
> message).  Call the treatments trt1 (4 levels), trt2 (3 levels) and trt3
> (2 levels) and the replications rep - all are factors.  The observed
> data is S.  Then:
> 
> temp.aov <- aov(S~rep+trt1*trt2*trt3, data=dummy.data)
> model.tables(temp.aov, type='mean', se=T)
> 
> Returns the means, but states "Design is unbalanced - use se.contrasts
> for se's" which is a little surprising since the design is balanced.  

If you used the default treatment contrasts, it is not.  Try Helmert 
contrasts with aov().

> Nevertheless, se.contrast gives what I'd expect:
> 
> se.contrast(temp.aov, list(trt1==0, trt1==1), data=dummy.data)
> [1] 5.960012
> 
> i.e. standard error of mean is 5.960012/sqrt(2) = 4.214, which is the
> sqrt(anova(temp.aov)[9,3]/12) as expected.  Similarly for interactions,
> e.g.:
> 
> se.contrast(temp.aov, list(trt1==0 & trt2==0, trt1==1 & trt2==1), data=dummy.data)/sqrt(2)
> [1]  7.299494
> 
> How do I get the equivalent of these standard errors if I have used
> lm(), and by extension glm()?  I think I should be able to get these
> using predict(..., type='terms', se=T) or coef(summary()) but can't
> quite see how.

In either case you can predict something you want to estimate and use
predict(, se=TRUE).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk 
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/ 
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595


______________________________________________________

The contents of this e-mail are privileged and/or confidenti...{{dropped}}



From tlumley at u.washington.edu  Wed Aug  4 02:36:40 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 3 Aug 2004 17:36:40 -0700 (PDT)
Subject: [R] lme fitted correlation of random effects: where is it? 
In-Reply-To: <Pine.OSX.4.53.0408031533110.721@biostat5.ucdavis.edu>
References: <Pine.OSX.4.53.0408031533110.721@biostat5.ucdavis.edu>
Message-ID: <Pine.A41.4.58.0408031729090.209208@homer07.u.washington.edu>

On Tue, 3 Aug 2004, Jacob Wegelin wrote:

>
> The print method for lme *prints out* the fitted correlation matrix for
> the random effects. Is there any way to get these values as an object in
> R?  I have examined the components of the lme object (called "junk" in the
> example below) and the components of summary(junk) without finding these
> numbers.
>
> (How I did this: I dumped the entire lme object to a text file and then
> used egrep to search the text file for one of the long strings of digits
> that are in the printout. The string was not in the file.  It appears that
> the print method computes the fitted correlation matrix on the fly??
> Or where does it get it?)
>

It's remarkably complicated.  If you look at
  getS3method("print","lme")
you see that it comes from
  print(summary(x$modelStruct), sigma=x$sigma)
You then look at what the class of x$modelstruct is
  > class(fm1$modelStruct)
  [1] "lmeStructInt" "lmeStruct"    "modelStruct"
and look at getS3method("print","lmeStructInt"), which doesn't exist. You
then try lmeStruct, with no luck, and then
  getS3method("print","modelStruct")

After a lot more chasing through methods you find that the correlation
matrix is computed by the generic function pdMatrix.  You probably want to
look at
   getS3method("pdMatrix","pdSymm")
and
   getS3method("pdMatrix","pdMat")

And all this may well change when Doug finishes lme4.

	-thomas



From HDoran at air.org  Wed Aug  4 02:55:41 2004
From: HDoran at air.org (Doran, Harold)
Date: Tue, 3 Aug 2004 20:55:41 -0400
Subject: [R] lme fitted correlation of random effects: where is it?
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7404044CD2@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040803/8bbf71f2/attachment.pl

From Duncan.Mackay at flinders.edu.au  Wed Aug  4 03:11:47 2004
From: Duncan.Mackay at flinders.edu.au (Duncan Mackay)
Date: Wed, 4 Aug 2004 10:41:47 +0930
Subject: [R] RGUI Console settings
Message-ID: <000001c479c0$03c52530$bee66081@duncanlt>

Hi all,
Would someone like to comment on the relation between the "buffer bytes"
and "lines" settings in the console settings? Are they interdependent?
If so, how? And why allow both to be tweaked? If not, what is the effect
of changing "lines", but leaving "buffer bytes" the same? 

I wanted to be able to see more of some large outputs in my console, as
I was losing the beginning of the outputs under the default console
settings. I changed the "lines" setting from 8000 to 16000, but that
didn't seem to change the no. of lines that would display (checked with
"for(i in 1:9000) print(i)"). So then I changed the "buffer bytes"
setting to 130000 and now the console will display what I want. But
clearly I don't really understand what is going on here ........
Thanks, Duncan


*****************************************
Dr. Duncan Mackay
School of Biological Sciences
Flinders University
GPO Box 2100
Adelaide
S.A.    5001
AUSTRALIA

Ph (08) 8201 2627    FAX (08) 8201 3015

http://www.scieng.flinders.edu.au/biology/people/mackay_d/index.html



From spencer.graves at pdf.com  Wed Aug  4 03:30:35 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 03 Aug 2004 18:30:35 -0700
Subject: [R] lme fitted correlation of random effects: where is it?
In-Reply-To: <Pine.A41.4.58.0408031729090.209208@homer07.u.washington.edu>
References: <Pine.OSX.4.53.0408031533110.721@biostat5.ucdavis.edu>
	<Pine.A41.4.58.0408031729090.209208@homer07.u.washington.edu>
Message-ID: <41103C3B.3030609@pdf.com>

      You may also wish to consider "VarCorr".  This function exists in 
both nlme and lme4.  Someone else mentioned "getVarCov".  However, that 
is not in the current lme4. 

      hope this helps.  spencer graves    

Thomas Lumley wrote:

>On Tue, 3 Aug 2004, Jacob Wegelin wrote:
>
>  
>
>>The print method for lme *prints out* the fitted correlation matrix for
>>the random effects. Is there any way to get these values as an object in
>>R?  I have examined the components of the lme object (called "junk" in the
>>example below) and the components of summary(junk) without finding these
>>numbers.
>>
>>(How I did this: I dumped the entire lme object to a text file and then
>>used egrep to search the text file for one of the long strings of digits
>>that are in the printout. The string was not in the file.  It appears that
>>the print method computes the fitted correlation matrix on the fly??
>>Or where does it get it?)
>>
>>    
>>
>
>It's remarkably complicated.  If you look at
>  getS3method("print","lme")
>you see that it comes from
>  print(summary(x$modelStruct), sigma=x$sigma)
>You then look at what the class of x$modelstruct is
>  > class(fm1$modelStruct)
>  [1] "lmeStructInt" "lmeStruct"    "modelStruct"
>and look at getS3method("print","lmeStructInt"), which doesn't exist. You
>then try lmeStruct, with no luck, and then
>  getS3method("print","modelStruct")
>
>After a lot more chasing through methods you find that the correlation
>matrix is computed by the generic function pdMatrix.  You probably want to
>look at
>   getS3method("pdMatrix","pdSymm")
>and
>   getS3method("pdMatrix","pdMat")
>
>And all this may well change when Doug finishes lme4.
>
>	-thomas
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From jfbrennan at rogers.com  Wed Aug  4 03:49:11 2004
From: jfbrennan at rogers.com (Jim Brennan)
Date: Tue, 3 Aug 2004 21:49:11 -0400
Subject: [R] Order of messgaes/ missing messages
Message-ID: <008901c479c5$3e08d160$3b8ac445@slnt.phub.net.cable.rogers.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040803/c060440e/attachment.pl

From vietnguyen at fastmail.fm  Wed Aug  4 05:08:41 2004
From: vietnguyen at fastmail.fm (Viet Nguyen)
Date: Wed, 04 Aug 2004 13:08:41 +1000
Subject: [R] dots expansion
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF816A@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF816A@usrymx25.merck.com>
Message-ID: <41105339.1070003@fastmail.fm>

Thanks to all who helped.

I used your ideas and code samples to write the following (for the 
benefit of people who will search this list later):

rbind.case <- function(..., name="case", values) {
    dots <- list(...);
    if (missing(values)) values <- 1:length(dots);
    if (length(values)!=length(dots))
      stop("length(values)!=length(list(...))");

    eval(parse(text=
               paste("cbind(rbind(...), ",name,
                     "=rep(values, sapply(dots, nrow)))",sep="")));
}


The function is to be used with data frames. It's not as good as it can 
be but it works for my purpose.

Cheers
viet



From P.Lemmens at nici.kun.nl  Wed Aug  4 08:10:41 2004
From: P.Lemmens at nici.kun.nl (Paul Lemmens)
Date: Wed, 04 Aug 2004 08:10:41 +0200
Subject: [R] basic questions: any place for them
In-Reply-To: <p06100500bd357f6495e6@[83.132.28.117]>
References: <p06100500bd357f6495e6@[83.132.28.117]>
Message-ID: <7E6E46C9BB3E2400DEED6CF4@lemmens.socsci.kun.nl>

Hoi Tiago,

--On dinsdag 3 augustus 2004 18:50 +0100 Tiago R Magalhaes 
<tiago17 at socrates.Berkeley.EDU> wrote:

> Once again I'm sorry for these basic questions and since predictably
> I'll have more of those if there's a basic-questions-list I would
> love to know more about it
>
Recently we discussed, on this list, that several online communities have 
dedicated discussion groups for R. One of them is Orkut.com. The name of 
the other one slips my mind, but if you search the archives for my name and 
orkut, then you'll probably find those emails quickly.

regards,
Paul



-- 
Paul Lemmens
NICI, University of Nijmegen              ASCII Ribbon Campaign /"\
Montessorilaan 3 (B.01.05)                    Against HTML Mail \ /
NL-6525 HR Nijmegen                                              X
The Netherlands                                                 / \
Phonenumber    +31-24-3612648
Fax            +31-24-3616066



From ripley at stats.ox.ac.uk  Wed Aug  4 08:13:42 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 4 Aug 2004 07:13:42 +0100 (BST)
Subject: [R] keep.source.pkgs()
In-Reply-To: <Pine.A41.4.58.0408031549180.209208@homer07.u.washington.edu>
Message-ID: <Pine.LNX.4.44.0408040708390.11434-100000@gannet.stats>

On Tue, 3 Aug 2004, Thomas Lumley wrote:

>  I can't remember the historical reason why utils isn't loaded in binary
> form in 1.9.1.  In r-devel they are all loaded from binary databases when
> used.

It's size.  The R code for utils is relatively small (relative to stats
and graphics, or lattice and nlme) and so using a saved image was only a
little faster. For r-devel the tradeoff is more biased in favour of binary
loading, but for example package splines is still stored as text.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Aug  4 08:25:44 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 4 Aug 2004 07:25:44 +0100 (BST)
Subject: [R] RGUI Console settings
In-Reply-To: <000001c479c0$03c52530$bee66081@duncanlt>
Message-ID: <Pine.LNX.4.44.0408040715150.11434-100000@gannet.stats>

On Wed, 4 Aug 2004, Duncan Mackay wrote:

> Would someone like to comment on the relation between the "buffer bytes"
> and "lines" settings in the console settings? Are they interdependent?

Both are limits, and you need enough of each.

> If so, how? And why allow both to be tweaked? If not, what is the effect
> of changing "lines", but leaving "buffer bytes" the same? 

Changing lines alone allows more (very) short lines.  Changing `buffer
bytes' alone allows longer lines.

> I wanted to be able to see more of some large outputs in my console, as
> I was losing the beginning of the outputs under the default console
> settings. I changed the "lines" setting from 8000 to 16000, but that
> didn't seem to change the no. of lines that would display (checked with
> "for(i in 1:9000) print(i)"). 

What it seems you did not notice is that you got less than 8000 lines, so
that was not the limit you were reaching.

> So then I changed the "buffer bytes"
> setting to 130000 and now the console will display what I want. But
> clearly I don't really understand what is going on here ........

Simple.  To run a large R job you need both enough time and enough memory.
Similarly there are two resources you need to store and display a large
piece of output in Rgui, lines and storage space.  The console needs both
to store the text and pointers to the beginnings of the lines.  Allowing 
both limits to be altered allows you to allocate your resources optimally.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From temiz at deprem.gov.tr  Wed Aug  4 09:38:49 2004
From: temiz at deprem.gov.tr (temiz)
Date: Wed, 04 Aug 2004 10:38:49 +0300
Subject: [R] which package for spatial autocorr.
Message-ID: <41109289.9080501@deprem.gov.tr>

hello

which package do you recommend for spatial autocorrelation ?

regards

Ahmet Temiz


______________________________________
Inflex - installed on mailserver for domain @deprem.gov.tr
Queries to: postmaster at deprem.gov.tr

______________________________________
The views and opinions expressed in this e-mail message are ...{{dropped}}



From ripley at stats.ox.ac.uk  Wed Aug  4 10:04:17 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 4 Aug 2004 09:04:17 +0100 (BST)
Subject: [R] which package for spatial autocorr.
In-Reply-To: <41109289.9080501@deprem.gov.tr>
Message-ID: <Pine.LNX.4.44.0408040900120.14677-100000@gannet.stats>

On Wed, 4 Aug 2004, temiz wrote:

> hello
> 
> which package do you recommend for spatial autocorrelation ?
> 
> regards
> 
> Ahmet Temiz

Spatial autocorrelation of what?

  sampled continuous surfaces
  lattice data
  aggregate data on irregularly-shaped units such as counties
  point patterns

and what do you want to do with it?

  test for it
  model its correlation structure
  account for it in a regression

Perhaps 'spdep' is the most useful for the most probable interpretation of 
your question.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From asemeria at cramont.it  Wed Aug  4 10:20:25 2004
From: asemeria at cramont.it (asemeria@cramont.it)
Date: Wed, 4 Aug 2004 10:20:25 +0200
Subject: [R] which package for spatial autocorr.
Message-ID: <OF48FB67F5.37D034E8-ONC1256EE6.002DD04C@tomware.it>

Have a look at fields, grasper, spatstat packages.
Bye
A.S.

----------------------------

Alessandro Semeria
Models and Simulations Laboratory
Montecatini Environmental Research Center (Edison Group),
Via Ciro Menotti 48,
48023 Marina di Ravenna (RA), Italy
Tel. +39 544 536811
Fax. +39 544 538663
E-mail: alessandro.semeria at cramont.it



From temiz at deprem.gov.tr  Wed Aug  4 10:27:15 2004
From: temiz at deprem.gov.tr (temiz)
Date: Wed, 04 Aug 2004 11:27:15 +0300
Subject: [R] which package for spatial autocorr.
In-Reply-To: <Pine.LNX.4.44.0408040900120.14677-100000@gannet.stats>
References: <Pine.LNX.4.44.0408040900120.14677-100000@gannet.stats>
Message-ID: <41109DE3.7070609@deprem.gov.tr>

Prof Brian Ripley wrote:

>On Wed, 4 Aug 2004, temiz wrote:
>
>  
>
>>hello
>>
>>which package do you recommend for spatial autocorrelation ?
>>
>>regards
>>
>>Ahmet Temiz
>>    
>>
>
>Spatial autocorrelation of what?
>
>  sampled continuous surfaces
>  lattice data
>  aggregate data on irregularly-shaped units such as counties
>  point patterns
>
>and what do you want to do with it?
>
>  test for it
>  model its correlation structure
>  account for it in a regression
>
>Perhaps 'spdep' is the most useful for the most probable interpretation of 
>your question.
>
>  
>
thank you for your interest

. what I want to do is that spatial autocorrelation of slope angle map.
. I think it belongs to interpolated continuous surface
. I want to know  whether there is a spatial  pattern  on the basis  of 
slope angle ?
  ( I am not sure if it makes sense )

regards


______________________________________
Inflex - installed on mailserver for domain @deprem.gov.tr
Queries to: postmaster at deprem.gov.tr

______________________________________
The views and opinions expressed in this e-mail message are ...{{dropped}}



From daniel.hoppe at univie.ac.at  Wed Aug  4 10:37:17 2004
From: daniel.hoppe at univie.ac.at (Daniel Hoppe)
Date: Wed, 4 Aug 2004 10:37:17 +0200
Subject: [R] profile mle in stats4 with ndeps option for optim
Message-ID: <000001c479fe$4b4a2160$4a8b8283@DH>

Hi!

When I use 

	fit <- mle(pnbd.ll, start=list(r=1,a=1,s=1,b=1), method="BFGS",
control=list(trace=traceLevel, REPORT=1, ndeps=c(1e-3,1e-3,5e-4,1e-3)))

and afterward profile(fit), the fit contains only NAs. This is due to
the fact, that optim still gets ndeps of original length while one
parameter has been excluded, so optim directly returns with an
(unreported) error message in line 87 of mle.r. For me, the hack 

diff mle.R mleorig2.r
87,91d86
<         if (!is.null(call$control$ndeps))
<         {
<               exclude <- which(names(call$start)==pi)
<               call$control$ndeps <- call$control$ndeps[-(exclude+1)]
<         }

works. I guess that the similar problem will occur for parscale. Beside
this I wonder if in this code snippet  (lines 87ff in mle.r)

        pfit <- try(eval(call), silent=TRUE)
        if(inherits(pfit, "try-error")) return(NA)
        else {

silent=TRUE has been set for any specific reason. In my experience with
programming in Java, it has nearly always turned out to be a bad choice,
not to report exceptions as debugging effort increases. Maybe there is a
global option I do not know of which can override silent=TRUE?

Best Regards,

Daniel



From ripley at stats.ox.ac.uk  Wed Aug  4 10:41:13 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 4 Aug 2004 09:41:13 +0100 (BST)
Subject: [R] which package for spatial autocorr.
In-Reply-To: <41109DE3.7070609@deprem.gov.tr>
Message-ID: <Pine.LNX.4.44.0408040937260.17923-100000@gannet.stats>

I think you are looking for the spatial correlogram or variogram.  Try
help.search("variogram").  There is a long list, and of those

fields, geoR, gstat, sgeostat, spatial

look the most relevant. As spatial ships with R and is simple I would 
start with that.

On Wed, 4 Aug 2004, temiz wrote:

> Prof Brian Ripley wrote:
> 
> >On Wed, 4 Aug 2004, temiz wrote:
> >
> >  
> >
> >>hello
> >>
> >>which package do you recommend for spatial autocorrelation ?
> >>
> >>regards
> >>
> >>Ahmet Temiz
> >>    
> >>
> >
> >Spatial autocorrelation of what?
> >
> >  sampled continuous surfaces
> >  lattice data
> >  aggregate data on irregularly-shaped units such as counties
> >  point patterns
> >
> >and what do you want to do with it?
> >
> >  test for it
> >  model its correlation structure
> >  account for it in a regression
> >
> >Perhaps 'spdep' is the most useful for the most probable interpretation of 
> >your question.
> >
> thank you for your interest
> 
> . what I want to do is that spatial autocorrelation of slope angle map.
> . I think it belongs to interpolated continuous surface
> . I want to know  whether there is a spatial  pattern  on the basis  of 
> slope angle ?
>   ( I am not sure if it makes sense )

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From hkawakat at qub.ac.uk  Wed Aug  4 10:23:25 2004
From: hkawakat at qub.ac.uk (Hiroyuki Kawakatsu)
Date: Wed, 4 Aug 2004 09:23:25 +0100
Subject: [R] undesirable arima() return code "feature"
Message-ID: <Pine.CYG.4.58.0408040858280.1672@erdos>

hi,

the return value 'code' from optim in arima() does not match the warning
message code if
(method != "CSS") && (tranform.pars==TRUE) && (final iteration MA estimate
is not invertible)

(res in the source code is overwritten by a re-fitting with invertible MA
estimates.) is this a desirable feature?

#---sample code (r-1.9.1)---

set.seed(27);
y <- rnorm(10);
out <- arima(y, c(0,0,1), method="ML", optim.control=list(maxit=3) );
print( paste("out$code =", out$code) );

#---end

h.
----------------------------------
Hiroyuki Kawakatsu
School of Management and Economics
25 University Square
Queen's University, Belfast
Belfast BT7 1NN
Northern Ireland
United Kingdom
Tel +44 (0)28 9097 3290
Fax +44 (0)28 9033 5156



From cantini at ifc.cnr.it  Wed Aug  4 11:45:06 2004
From: cantini at ifc.cnr.it (Federico Cantini)
Date: Wed, 04 Aug 2004 11:45:06 +0200
Subject: [R] load shared object
Message-ID: <1091612706.4047.8.camel@federico>

Hi all,
i have some problem in using shared objects.
I tried the example found in "Writing R Extension", pg. 33

I used the .c file prova.c :

-------------------------------------------------------------------
#include <R.h>
#include <Rinternals.h>

void convolve(double *a, int *na, double *b, int *nb, double *ab)
{
	int i, j , nab = *na + *nb - 1;
	
	for (i = 0; i < nab; i++)
		ab[i] = 0.0;
	for (i = 0; i < *na; i++)
		for (j = 0; i < *nb; i++)
			ab[i + j] += a[i] * b [j];
}
-------------------------------------------------------------------

I generated shared object with 
R CMD SHLIB prova.c

i run R and so

> dyn.load("prova.so", local = TRUE, now = TRUE)

>convolve1<-function(a,b).C("convolve",as.double(a),as.integer(length(a)),as.double(b),as.integer(length(b)),ab = double(length(a) + length(b) - 1)$ab)

> a<-1:10
> b<-a

typing convolve1(a,b) i get a Segmentation fault.

Can anyone tell me what's the problem

Regards

-- 
Federico Cantini <cantini at ifc.cnr.it>



From ripley at stats.ox.ac.uk  Wed Aug  4 11:57:43 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 4 Aug 2004 10:57:43 +0100 (BST)
Subject: [R] load shared object
In-Reply-To: <1091612706.4047.8.camel@federico>
Message-ID: <Pine.LNX.4.44.0408041052410.16733-100000@gannet.stats>

Which version of R is this?  (See the posting guide, please.)

Some versions of R (e.g. 1.8.0) already had "convolve" loaded and you need
to use another name or use the PACKAGE argument.  It works in the current
1.9.1.

On Wed, 4 Aug 2004, Federico Cantini wrote:

> Hi all,
> i have some problem in using shared objects.
> I tried the example found in "Writing R Extension", pg. 33
> 
> I used the .c file prova.c :
> 
> -------------------------------------------------------------------
> #include <R.h>
> #include <Rinternals.h>
> 
> void convolve(double *a, int *na, double *b, int *nb, double *ab)
> {
> 	int i, j , nab = *na + *nb - 1;
> 	
> 	for (i = 0; i < nab; i++)
> 		ab[i] = 0.0;
> 	for (i = 0; i < *na; i++)
> 		for (j = 0; i < *nb; i++)
> 			ab[i + j] += a[i] * b [j];
> }
> -------------------------------------------------------------------
> 
> I generated shared object with 
> R CMD SHLIB prova.c
> 
> i run R and so
> 
> > dyn.load("prova.so", local = TRUE, now = TRUE)
> 
> >convolve1<-function(a,b).C("convolve",as.double(a),as.integer(length(a)),as.double(b),as.integer(length(b)),ab = double(length(a) + length(b) - 1)$ab)
> 
> > a<-1:10
> > b<-a
> 
> typing convolve1(a,b) i get a Segmentation fault.
> 
> Can anyone tell me what's the problem

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From cantini at ifc.cnr.it  Wed Aug  4 12:02:43 2004
From: cantini at ifc.cnr.it (Federico Cantini)
Date: Wed, 04 Aug 2004 12:02:43 +0200
Subject: [R] load shared object
In-Reply-To: <Pine.LNX.4.44.0408041052410.16733-100000@gannet.stats>
References: <Pine.LNX.4.44.0408041052410.16733-100000@gannet.stats>
Message-ID: <1091613762.4047.13.camel@federico>

I run R version 1.9.1 and i tried using other name... 


Il mer, 2004-08-04 alle 11:57, Prof Brian Ripley ha scritto:
> Which version of R is this?  (See the posting guide, please.)
> 
> Some versions of R (e.g. 1.8.0) already had "convolve" loaded and you need
> to use another name or use the PACKAGE argument.  It works in the current
> 1.9.1.
> 
> On Wed, 4 Aug 2004, Federico Cantini wrote:
> 
> > Hi all,
> > i have some problem in using shared objects.
> > I tried the example found in "Writing R Extension", pg. 33
> > 
> > I used the .c file prova.c :
> > 
> > -------------------------------------------------------------------
> > #include <R.h>
> > #include <Rinternals.h>
> > 
> > void convolve(double *a, int *na, double *b, int *nb, double *ab)
> > {
> > 	int i, j , nab = *na + *nb - 1;
> > 	
> > 	for (i = 0; i < nab; i++)
> > 		ab[i] = 0.0;
> > 	for (i = 0; i < *na; i++)
> > 		for (j = 0; i < *nb; i++)
> > 			ab[i + j] += a[i] * b [j];
> > }
> > -------------------------------------------------------------------
> > 
> > I generated shared object with 
> > R CMD SHLIB prova.c
> > 
> > i run R and so
> > 
> > > dyn.load("prova.so", local = TRUE, now = TRUE)
> > 
> > >convolve1<-function(a,b).C("convolve",as.double(a),as.integer(length(a)),as.double(b),as.integer(length(b)),ab = double(length(a) + length(b) - 1)$ab)
> > 
> > > a<-1:10
> > > b<-a
> > 
> > typing convolve1(a,b) i get a Segmentation fault.
> > 
> > Can anyone tell me what's the problem
-- 
Federico Cantini <cantini at ifc.cnr.it>



From Matt.Nunes at bristol.ac.uk  Wed Aug  4 12:31:31 2004
From: Matt.Nunes at bristol.ac.uk (Matt Nunes)
Date: Wed, 04 Aug 2004 11:31:31 +0100
Subject: [R] R Matrix package solve
Message-ID: <6298203.1091619091@pc277.maths.bris.ac.uk>

hello.

I have a query about the Matrix package for R.  I wrote some code a while
ago using the Matrix package version 1.6.2 with an early version of R, to 
do some linear least squares for regression:

xn
     [,1]      [,2]      [,3]      [,4]
[1,]    1 0.7429352 0.5519528 0.4100652
[2,]    1 0.7443713 0.5540886 0.4124477
[3,]    1 0.7447385 0.5546355 0.4130584
[4,]    1 0.7459597 0.5564558 0.4150936
>
> temp<-crossprod(xn)
> temp
         [,1]     [,2]      [,3]      [,4]
[1,] 4.000000 2.978005 2.2171327 1.6506648
[2,] 2.978005 2.217133 1.6506648 1.2289297
[3,] 2.217133 1.650665 1.2289297 0.9149473
[4,] 1.650665 1.228930 0.9149473 0.6811865
>
> solve.Matrix(temp,t(xn))
           [,1]      [,2]      [,3]       [,4]
[1,]   33397.34  122081.7 -241005.4   85527.48
[2,]   21063.72 -664920.2  812316.0 -168459.99
[3,] -236935.74 1125548.2 -877776.2  -10835.64
[4,]  199314.69 -608045.8  297509.1  111221.72

(Note:  here I used solve.Matrix since the generic solve said the matrix is
singular).

I recently updated my versions of R to 1.9.1 and also the Matrix package, 
but I can't seem to get any similar equivalent matrix calculations to work 
(I get error messages like ".Call function name not in DLL for package 
Matrix" or "Lapack routine dpotrf returned error code 4").  I have also 
tried using symmetric matrix commands, but to no avail. Can anyone help me 
out?

many thanks

Matt Nunes


------------------------
Matt Nunes
School of Mathematics, University of Bristol,
University Walk, Bristol. BS8 1TW. UK.

office tel. 0117 3311662 (room 1.1 St. Michael's House)
Matt.Nunes at bristol.ac.uk
http://www.maths.bris.ac.uk/~maman/



From wolfgang-abele at web.de  Wed Aug  4 13:16:01 2004
From: wolfgang-abele at web.de (Wolfgang Abele)
Date: Wed, 04 Aug 2004 13:16:01 +0200
Subject: [R] Constructing a VAR model using dse
Message-ID: <627885425@web.de>

Content-Type: text/plain; charset="iso-8859-1"
Content-Transfer-Encoding: 7bit
Received-SPF: none (hypatia: domain of wolfgang-abele at web.de does not designate permitted sender hosts)
X-Virus-Scanned: by amavisd-new at stat.math.ethz.ch
X-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on hypatia.math.ethz.ch
X-Spam-Level: 
X-Spam-Status: No, hits=0.0 required=5.0 tests=BAYES_50 autolearn=no version=2.63

Hi everybody,

I'm trying to construct a VAR model where the output variables can influence each other in the same time period, for example:

x1_t = ax1_t-1 + bx2_t-1 + e1
x2_t = cx1_t + dx2_t-1 + e2

So x2_t is influenced by x1_t.

Does anybody know how to construct such a model using the dse package?

If I write AX = ... I know I could get rid of the A matrix by multiplying both sides with the inverse matrix A^(-1). Does this method always work or is it restricted to certain cases of the covariance matrix E?

Thanks a lot for your help!

Wolfgang
_______________________________________________________
WEB.DE Video-Mail - Sagen Sie mehr mit bewegten Bildern



From matthias.burger at epigenomics.com  Wed Aug  4 13:17:27 2004
From: matthias.burger at epigenomics.com (Matthias Burger)
Date: Wed, 04 Aug 2004 13:17:27 +0200
Subject: [R] installing package with version number using namespaces &
 dynamic library
Message-ID: <4110C5C7.9030207@epigenomics.com>



Hi,

I wonder whether a package with namespace & dynamic library can be installed 
with the version number attached, ie. with the argument --with-package-versions. 
Is this currently possible?


Using R 1.9.1 on Debian 3.0 I encounter a problem when trying to load a package 
installed with

R91 CMD INSTALL --with-package-versions -l /mnt/local/R/R-1.9.x-libs-EpiR RIO

in R started with
R91 --vanilla

 > .libPaths()
[1] "/mnt/local/R/R-1.9.x-libs"          "/mnt/local/R/R-1.9.x-libs-BioC"
[3] "/mnt/local/R/R-1.9.x-libs-EpiR"     "/mnt/local/R/R-1.9.1/lib/R/library"

either
 > library("RIO")
Error in library.dynam(pkg, pkg, lib) : shared library 'RIO_1.4.0' not found
Error: .onLoad failed in loadNamespace
Error in library("RIO") : package/namespace load failed
or
 > library("RIO_1.4.0")
Error in library.dynam(pkg, pkg, lib) : shared library 'RIO_1.4.0' not found
Error: .onLoad failed in loadNamespace
Error in library("RIO_1.4.0") : package/namespace load failed

fail.

Without the '--with-package-versions' option the installed package loads without 
problem with the call
library("RIO")


file Namesapce defines (amoung other directives):

useDynLib(RIO)

I also tryed
useDynLib("RIO_1.4.0")
to no avail. But RIO/libs contains the file RIO.so anyway.

I checked in 'Writing R Extensions" section 'Package name spaces' but did not 
find an answer.


Any hints?

Regards,

    Matthias


-- 
Matthias Burger                     Project Manager/ Biostatistician
Epigenomics AG    Kleine Praesidentenstr. 1    10178 Berlin, Germany
phone:+49-30-24345-371                          fax:+49-30-24345-555
http://www.epigenomics.com           matthias.burger at epigenomics.com



From rolf at math.unb.ca  Wed Aug  4 13:39:13 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Wed, 4 Aug 2004 08:39:13 -0300 (ADT)
Subject: [R] keep.source.pkgs()
Message-ID: <200408041139.i74BdDms017845@erdos.math.unb.ca>

Thomas Lumley wrote:

> The reason why detach and reload doesn't work but restarting does work for
> utils is that detach() does not unload the package, it only detaches it.
> This distinction was introduced with namespaces.  A package can be loaded
> but not in the search path.


Ah-ha.  The distinction between unloading and detaching --- which I
hadn't grokked --- would seem to be the key issue.  Now I get it.
Dankaschoen beaucoup.

				cheers,

					Rolf Turner



From jinsong_zh at yahoo.com  Wed Aug  4 13:45:26 2004
From: jinsong_zh at yahoo.com (Jinsong Zhao)
Date: Wed, 4 Aug 2004 04:45:26 -0700 (PDT)
Subject: [R] How to select a whole column? Thanks! 
In-Reply-To: <20040803084241.49210.qmail@web20828.mail.yahoo.com>
Message-ID: <20040804114526.6895.qmail@web20823.mail.yahoo.com>

Thank all of you for your kindly help.


--- Jinsong Zhao <jinsong_zh at yahoo.com> wrote:

> Dear all,
> 
> I hope to remove a whole column from a data frame or matrix (> 2000
> columns). All value in the column are same. The first thing is to
> select those columns. 
> 
> For instance, I hope to remove the V3~6 column, for all the value in
> those colume is zero.
> 
>   V3 V4 V5 V6     V7     V8     V9    V10
> 1  0  0  0  0  0.000  0.000  0.000  0.000
> 2  0  0  0  0  0.000  0.000  0.000  0.000
> 3  0  0  0  0  0.000  0.000  0.000  0.000
> 4  0  0  0  0  0.000  0.000  0.000  0.000
> 5  0  0  0  0  0.000  0.000  0.000  0.000
> 6  0  0  0  0 -0.001 -0.001 -0.001 -0.001
> 7  0  0  0  0  0.000  0.000  0.000 -0.001
> 8  0  0  0  0  0.000  0.000  0.000 -0.001
> 9  0  0  0  0 -0.009 -0.012 -0.015 -0.018
> 
> I mean how to select the first four columns.
> 
> Thank you very much for your consideration on this matter.
> 
> Best wishes,
> 
> Jinsong
> 


=====
(Mr.) Jinsong Zhao
Ph.D. Candidate
School of the Environment
Nanjing University
22 Hankou Road, Nanjing 210093
P.R. China
E-mail: jinsong_zh at yahoo.com



From ripley at stats.ox.ac.uk  Wed Aug  4 14:40:20 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 4 Aug 2004 13:40:20 +0100 (BST)
Subject: [R] installing package with version number using namespaces &
	dynamic library
In-Reply-To: <4110C5C7.9030207@epigenomics.com>
Message-ID: <Pine.LNX.4.44.0408041316230.32516-100000@gannet.stats>

On Wed, 4 Aug 2004, Matthias Burger wrote:

> I wonder whether a package with namespace & dynamic library can be installed 
> with the version number attached, ie. with the argument --with-package-versions. 
> Is this currently possible?

I've just successfully done this with package tree, so yes.

gannet% R CMD INSTALL --with-package-versions tree_1.0-16.tar.gz
gannet% R
...
> library(tree)
> library(tree, version="1.0-16")

both work.

The crucial piece of code in namespace loading is

        # load any dynamic libraries
        for (lib in nsInfo$dynlibs)
            library.dynam(lib, package, package.lib)

which is *not* the call to library.dynam you quote.  I think you have a
call to library.dynam in your .onLoad, rather than/as well as using the
useDynLib directive.  .onLoad *is* run with the versioned package name as 
its `pkgname' argument, and we should document that.


> Using R 1.9.1 on Debian 3.0 I encounter a problem when trying to load a package 
> installed with
> 
> R91 CMD INSTALL --with-package-versions -l /mnt/local/R/R-1.9.x-libs-EpiR RIO
> 
> in R started with
> R91 --vanilla
> 
>  > .libPaths()
> [1] "/mnt/local/R/R-1.9.x-libs"          "/mnt/local/R/R-1.9.x-libs-BioC"
> [3] "/mnt/local/R/R-1.9.x-libs-EpiR"     "/mnt/local/R/R-1.9.1/lib/R/library"
> 
> either
>  > library("RIO")
> Error in library.dynam(pkg, pkg, lib) : shared library 'RIO_1.4.0' not found
> Error: .onLoad failed in loadNamespace
> Error in library("RIO") : package/namespace load failed
> or
>  > library("RIO_1.4.0")
> Error in library.dynam(pkg, pkg, lib) : shared library 'RIO_1.4.0' not found
> Error: .onLoad failed in loadNamespace
> Error in library("RIO_1.4.0") : package/namespace load failed
> 
> fail.
> 
> Without the '--with-package-versions' option the installed package loads without 
> problem with the call
> library("RIO")
> 
> 
> file Namesapce defines (amoung other directives):
> 
> useDynLib(RIO)
> 
> I also tryed
> useDynLib("RIO_1.4.0")
> to no avail. But RIO/libs contains the file RIO.so anyway.

[As it should.]

> I checked in 'Writing R Extensions" section 'Package name spaces' but did not 
> find an answer.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jinsong_zh at yahoo.com  Wed Aug  4 14:55:05 2004
From: jinsong_zh at yahoo.com (Jinsong Zhao)
Date: Wed, 4 Aug 2004 05:55:05 -0700 (PDT)
Subject: [R] What's ``impres''?
Message-ID: <20040804125505.43808.qmail@web20821.mail.yahoo.com>

Dear all,

In one S+ program, I found the following code. 

if (impres) cat("blah blah blah \n")

Would someone here like to tell me what's the counterpart in R to
impres in S+?

Thanks in advance!

Best wishes,

Jinsong

=====
(Mr.) Jinsong Zhao
Ph.D. Candidate
School of the Environment
Nanjing University
22 Hankou Road, Nanjing 210093
P.R. China
E-mail: jinsong_zh at yahoo.com



From matthias.burger at epigenomics.com  Wed Aug  4 14:58:32 2004
From: matthias.burger at epigenomics.com (Matthias Burger)
Date: Wed, 04 Aug 2004 14:58:32 +0200
Subject: [R] installing package with version number using namespaces &
	dynamic library
In-Reply-To: <Pine.LNX.4.44.0408041316230.32516-100000@gannet.stats>
References: <Pine.LNX.4.44.0408041316230.32516-100000@gannet.stats>
Message-ID: <4110DD78.4010103@epigenomics.com>



Prof Brian Ripley wrote:
> On Wed, 4 Aug 2004, Matthias Burger wrote:
> 
> 
>>I wonder whether a package with namespace & dynamic library can be installed 
>>with the version number attached, ie. with the argument --with-package-versions. 
>>Is this currently possible?
> 
> 
> I've just successfully done this with package tree, so yes.
> 
> gannet% R CMD INSTALL --with-package-versions tree_1.0-16.tar.gz
> gannet% R
> ...
> 
>>library(tree)
>>library(tree, version="1.0-16")
> 
> 
> both work.
> 
> The crucial piece of code in namespace loading is
> 
>         # load any dynamic libraries
>         for (lib in nsInfo$dynlibs)
>             library.dynam(lib, package, package.lib)
> 
> which is *not* the call to library.dynam you quote.  I think you have a
> call to library.dynam in your .onLoad, rather than/as well as using the
> useDynLib directive.  .onLoad *is* run with the versioned package name as 
> its `pkgname' argument, and we should document that.
> 
Spot on! This indeed was my oversight, removing
library.dynam(pkg, pkg, lib) from the .onLoad function resolved the problem.

Thanks for the prompt help!

Best,

   Matthias


-- 
Matthias Burger                     Project Manager/ Biostatistician
Epigenomics AG    Kleine Praesidentenstr. 1    10178 Berlin, Germany
phone:+49-30-24345-371                          fax:+49-30-24345-555
http://www.epigenomics.com           matthias.burger at epigenomics.com



From andy_liaw at merck.com  Wed Aug  4 15:11:22 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 4 Aug 2004 09:11:22 -0400
Subject: [R] What's ``impres''?
Message-ID: <3A822319EB35174CA3714066D590DCD504AF8194@usrymx25.merck.com>

Seems like you are using code that someone other than yourself had written.
The best person to ask would be the original author of that code.  The way
you quote the code, there's no context for anyone to guess what `impres' is,
other than that the author probably expected it to be either a logical, or a
scalar numeric.  Similar to C, integer value can be used as logical
expression: 0 = FALSE, anything else is TRUE.

Andy

> From: Jinsong Zhao
> 
> Dear all,
> 
> In one S+ program, I found the following code. 
> 
> if (impres) cat("blah blah blah \n")
> 
> Would someone here like to tell me what's the counterpart in R to
> impres in S+?
> 
> Thanks in advance!
> 
> Best wishes,
> 
> Jinsong
> 
> =====
> (Mr.) Jinsong Zhao
> Ph.D. Candidate
> School of the Environment
> Nanjing University
> 22 Hankou Road, Nanjing 210093
> P.R. China
> E-mail: jinsong_zh at yahoo.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From pgilbert at bank-banque-canada.ca  Wed Aug  4 15:23:50 2004
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Wed, 04 Aug 2004 09:23:50 -0400
Subject: [R] Constructing a VAR model using dse
In-Reply-To: <627885425@web.de>
References: <627885425@web.de>
Message-ID: <4110E366.1040503@bank-banque-canada.ca>

Wolfgang Abele wrote:

> Hi everybody,
> 
> I'm trying to construct a VAR model where the output variables can influence each other in the same time period, for example:
> 
> x1_t = ax1_t-1 + bx2_t-1 + e1
> x2_t = cx1_t + dx2_t-1 + e2
> 
> So x2_t is influenced by x1_t.
> 
> Does anybody know how to construct such a model using the dse package?
> 
> If I write AX = ... I know I could get rid of the A matrix by multiplying both sides with the inverse matrix A^(-1). Does this method always work or is it restricted to certain cases of the covariance matrix E?

It almost always works. (There are lots of difficulties in multivariate 
time series, but not because of this.) If A is singular then there is a 
problem, but there is also a problem with your model in that case. 
Almost all estimation procedures impose the restriction that the model 
has been made identifiable by multiplying by A^(-1).  (Your A is often 
called A(0), the zero lag coefficient of the AR polynomial matrix.) If 
this restriction is not made, then some other identifying restriction 
has to be imposed.

If you know A because of some physical understanding of the system (i.e. 
the coefficient c in your equations above) then you can estimate in the 
usual form and recover the form you would like by multiplying through by 
A afterward.

Paul Gilbert
> 
> Thanks a lot for your help!
> 
> Wolfgang
> _______________________________________________________
> WEB.DE Video-Mail - Sagen Sie mehr mit bewegten Bildern
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Torsten.Hothorn at rzmail.uni-erlangen.de  Wed Aug  4 15:26:15 2004
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Wed, 4 Aug 2004 15:26:15 +0200 (CEST)
Subject: [R] R Matrix package solve
In-Reply-To: <6298203.1091619091@pc277.maths.bris.ac.uk>
References: <6298203.1091619091@pc277.maths.bris.ac.uk>
Message-ID: <Pine.LNX.4.51.0408041525320.819@artemis.imbe.med.uni-erlangen.de>


On Wed, 4 Aug 2004, Matt Nunes wrote:

> hello.
>
> I have a query about the Matrix package for R.  I wrote some code a while
> ago using the Matrix package version 1.6.2 with an early version of R, to
> do some linear least squares for regression:
>
> xn
>      [,1]      [,2]      [,3]      [,4]
> [1,]    1 0.7429352 0.5519528 0.4100652
> [2,]    1 0.7443713 0.5540886 0.4124477
> [3,]    1 0.7447385 0.5546355 0.4130584
> [4,]    1 0.7459597 0.5564558 0.4150936
> >
> > temp<-crossprod(xn)
> > temp
>          [,1]     [,2]      [,3]      [,4]
> [1,] 4.000000 2.978005 2.2171327 1.6506648
> [2,] 2.978005 2.217133 1.6506648 1.2289297
> [3,] 2.217133 1.650665 1.2289297 0.9149473
> [4,] 1.650665 1.228930 0.9149473 0.6811865
> >
> > solve.Matrix(temp,t(xn))
>            [,1]      [,2]      [,3]       [,4]
> [1,]   33397.34  122081.7 -241005.4   85527.48
> [2,]   21063.72 -664920.2  812316.0 -168459.99
> [3,] -236935.74 1125548.2 -877776.2  -10835.64
> [4,]  199314.69 -608045.8  297509.1  111221.72
>
> (Note:  here I used solve.Matrix since the generic solve said the matrix is
> singular).
>
> I recently updated my versions of R to 1.9.1 and also the Matrix package,
> but I can't seem to get any similar equivalent matrix calculations to work
> (I get error messages like ".Call function name not in DLL for package
> Matrix"

this very much looks like a corrupted package installation. Try
to reinstall  `Matrix'

Torsten

> or "Lapack routine dpotrf returned error code 4").  I have also
> tried using symmetric matrix commands, but to no avail. Can anyone help me
> out?
>
> many thanks
>
> Matt Nunes
>
>
> ------------------------
> Matt Nunes
> School of Mathematics, University of Bristol,
> University Walk, Bristol. BS8 1TW. UK.
>
> office tel. 0117 3311662 (room 1.1 St. Michael's House)
> Matt.Nunes at bristol.ac.uk
> http://www.maths.bris.ac.uk/~maman/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From Luisr at frs.fo  Wed Aug  4 16:05:53 2004
From: Luisr at frs.fo (Luis Rideau Cruz)
Date: Wed, 04 Aug 2004 15:05:53 +0100
Subject: [R] RODBC with MS Microsoft Access
Message-ID: <s110fb5a.069@ffdata.setur.fo>

Hi all,

I tried to retrieve a table from a desktop database and got an error.
The thing is that when I changed the table name it worked out.

The former names was "length-spring"
The new was test

Is "-" character not valid for tables?

Thank you

Luis Ridao Cruz
Fiskiranns??knarstovan
N??at??n 1
P.O. Box 3051
FR-110 T??rshavn
Faroe Islands
Phone:             +298 353900
Phone(direct): +298 353912
Mobile:             +298 580800
Fax:                 +298 353901
E-mail:              luisr at frs.fo
Web:                www.frs.fo



From ggrothendieck at myway.com  Wed Aug  4 16:14:41 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 4 Aug 2004 14:14:41 +0000 (UTC)
Subject: [R] dots expansion
References: <3A822319EB35174CA3714066D590DCD504AF816A@usrymx25.merck.com>
	<41105339.1070003@fastmail.fm>
Message-ID: <loom.20040804T160838-483@post.gmane.org>

Viet Nguyen <vietnguyen <at> fastmail.fm> writes:

> 
> Thanks to all who helped.
> 
> I used your ideas and code samples to write the following (for the 
> benefit of people who will search this list later):
> 
> rbind.case <- function(..., name="case", values) {
>     dots <- list(...);
>     if (missing(values)) values <- 1:length(dots);
>     if (length(values)!=length(dots))
>       stop("length(values)!=length(list(...))");
> 
>     eval(parse(text=
>                paste("cbind(rbind(...), ",name,
>                      "=rep(values, sapply(dots, nrow)))",sep="")));
> }
> 
> The function is to be used with data frames. It's not as good as it can 
> be but it works for my purpose.

Regarding improvements, eliminate the semicolons at the end of statements,
place the default value for values= in the arg list to make it more readable,
use stopifnot to check args (also for readability), add a check for 
data frames (which is mentioned after the code but not checked for, 
and eliminate the eval and rep calculations by simply lapplying over an 
index and appending the name column to each data frame in turn:


rbind.case <- function(..., name="case", values = seq(along = list(...))) 
# rbind the ... data frames together adding a column named name whose 
# value for rows from ith argument is values[i]
{
   dots <- list(...)
   stopifnot(length(dots) == length(values),
	   all(sapply(dots, inherits, "data.frame")))

   f <- function(i) { x <- dots[[i]]; x[,name] <- values[i]; x }
   do.call("rbind", lapply(seq(along = dots), f))
}



From andy_liaw at merck.com  Wed Aug  4 16:24:55 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 4 Aug 2004 10:24:55 -0400
Subject: [R] dots expansion
Message-ID: <3A822319EB35174CA3714066D590DCD504AF8198@usrymx25.merck.com>

In addition to Gabor's comments:

There's a reason why I didn't coerce the grouping variable to a factor.
rbind()ing data frames is much more expensive than rbind()ing
arrays/matrices.  Unless your data really have different data types in
different columns, it would mostly likely be better to work with the matrix
version of them.  If you really want a data frame with the grouping variable
as a factor, you can do the coercion afterward.

Andy

> From: Gabor Grothendieck
> 
> Viet Nguyen <vietnguyen <at> fastmail.fm> writes:
> 
> > 
> > Thanks to all who helped.
> > 
> > I used your ideas and code samples to write the following (for the 
> > benefit of people who will search this list later):
> > 
> > rbind.case <- function(..., name="case", values) {
> >     dots <- list(...);
> >     if (missing(values)) values <- 1:length(dots);
> >     if (length(values)!=length(dots))
> >       stop("length(values)!=length(list(...))");
> > 
> >     eval(parse(text=
> >                paste("cbind(rbind(...), ",name,
> >                      "=rep(values, sapply(dots, nrow)))",sep="")));
> > }
> > 
> > The function is to be used with data frames. It's not as 
> good as it can 
> > be but it works for my purpose.
> 
> Regarding improvements, eliminate the semicolons at the end 
> of statements,
> place the default value for values= in the arg list to make 
> it more readable,
> use stopifnot to check args (also for readability), add a check for 
> data frames (which is mentioned after the code but not checked for, 
> and eliminate the eval and rep calculations by simply 
> lapplying over an 
> index and appending the name column to each data frame in turn:
> 
> 
> rbind.case <- function(..., name="case", values = seq(along = 
> list(...))) 
> # rbind the ... data frames together adding a column named name whose 
> # value for rows from ith argument is values[i]
> {
>    dots <- list(...)
>    stopifnot(length(dots) == length(values),
> 	   all(sapply(dots, inherits, "data.frame")))
> 
>    f <- function(i) { x <- dots[[i]]; x[,name] <- values[i]; x }
>    do.call("rbind", lapply(seq(along = dots), f))
> }
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From liao1k at cmich.edu  Wed Aug  4 16:33:27 2004
From: liao1k at cmich.edu (Liao, Kexiao )
Date: Wed, 4 Aug 2004 10:33:27 -0400
Subject: [R] RE: Does tcltk matter for the plot
Message-ID: <291B348BC59B47468C7824603C3260829461B9@cmail3.central.cmich.local>

Hi Paul,
   I use executable codes of tex and latex from
http://aixpdslib.seas.ucla.edu/packages/tetex.html) and compile the
original R-1.9.1 source codes using C for AIX compiler V6.0 and Fortran
for AIX compiler 7.1.1 and generate 64 bit R environments. It works
well.

Kexiao


-----Original Message-----
From: Paul Murrell [mailto:p.murrell at auckland.ac.nz] 
Sent: Monday, August 02, 2004 6:10 PM
To: Liao, Kexiao 
Subject: Re: Does tcltk matter for the plot

Hi


Liao, Kexiao wrote:
> Hi Paul,
>   I still change the statement "return FALSE;" to "return NULL" in
> function newX11Desc * Rf_allocNewX11DeviceDesc(double ps) so that the
> codes can be compiled. Most of the R codes can be compiled
successfully.


Right, sorry, that change also needed to be made.


> However there is something incompatible with tcltk package on the AIX.
I
> wonder does that cause the problem of core dump when I try to run the
> plot() function from R? Thanks.


This just looks like pdflatex is not installed properly.  What happens 
if (completely separate from R) you try to pdflatex a latex file?

Paul


>>>>Building/Updating help pages for package 'tcltk'
>>>
>      Formats: text html latex example
> building/updating vignettes for package 'grid' ...
> exec(): 0509-036 Cannot load program pdflatex because of the following
> errors:
>         0509-150   Dependent module libgcc_s.a(shr.o) could not be
> loaded.
>         0509-022 Cannot load module libgcc_s.a(shr.o).
>         0509-026 System error: A file or directory in the path name
does
> not exist.
> /home/liao1k/r-1.9.1/R-1.9.1/bin/texi2dvi: pdflatex exited with bad
> status, quit
> ting.
> /home/liao1k/r-1.9.1/R-1.9.1/bin/texi2dvi: see displaylist.log for
> errors.
> Error in texi2dvi(texfile, pdf = TRUE, quiet = TRUE) :
>         running texi2dvi on displaylist.tex failed
> Execution halted
> make: 1254-004 The error code from the last command is 1.
> 
> 
> Stop.
> make: 1254-004 The error code from the last command is 2.
> 
> -----Original Message-----
> From: Paul Murrell [mailto:p.murrell at auckland.ac.nz] 
> Sent: Sunday, August 01, 2004 6:35 PM
> To: Liao, Kexiao 
> Cc: r-devel at stat.math.ethz.ch
> Subject: Re: [Rd] plot(x,y) core dump
> 
> Hi
> 
> Rf_allocNewX11DeviceDesc is called by newX11DeviceDriver and the
latter 
> needs to check the value returned.  I think Brian Ripley has made a
fix 
> for this problem in the development version of R.  The first few lines

> of newX11DeviceDriver now look like ...
> 
>      newX11Desc *xd;
>      char *fn;
> 
>      xd = Rf_allocNewX11DeviceDesc(pointsize);
>      if(!xd) return FALSE;
> 
> Unfortunately, your next problem may be that you can't start an X11 
> device (it looks like you are failing to allocate memory for the
> device).
> 
> Paul
> 
> 
> Liao, Kexiao wrote:
> 
>>Dear R Development Team,
>>
>>   I compile R-1.9.1 on AIX 5.2 under 2.9-aix51-020209, and xlf 7.1.
> 
> In
> 
>>order to let R compile successfully under gcc 2.9, I have to change
> 
> one
> 
>>C statement of file(RHOME//src/modules/X11/devX11.c) line 1768 from
>>"retrun FALSE" to "retrun NULL", following is C code snapshot:
>>
>> 
>>
>>newX11Desc * Rf_allocNewX11DeviceDesc(double ps)
>>
>>{
>>
>>    newX11Desc *xd;
>>
>>    /* allocate new device description */
>>
>>    if (!(xd = (newX11Desc*)calloc(1, sizeof(newX11Desc))))
>>
>>        return NULL;
>>
>> 
>>
>>    /* From here on, if we need to bail out with "error", */
>>
>>    /* then we must also free(xd). */
>>
>> 
>>
>>    /*  Font will load at first use.  */
>>
>> 
>>
>>    if (ps < 6 || ps > 24) ps = 12;
>>
>>    xd->fontface = -1;
>>
>>    xd->fontsize = -1;
>>
>>    xd->basefontface = 1;
>>
>>    xd->basefontsize = ps;
>>
>>    xd->handleOwnEvents = FALSE;
>>
>>    xd->window = (Window) NULL;
>>
>> 
>>
>>    return(xd);
>>
>>}
>>
>> 
>>
>>However I got core dump while I try to use plot() from R:
>>
>> 
>>
>> 
>>
>>R : Copyright 2004, The R Foundation for Statistical Computing
>>
>>Version 1.9.1  (2004-06-21), ISBN 3-900051-00-3
>>
>> 
>>
>>R is free software and comes with ABSOLUTELY NO WARRANTY.
>>
>>You are welcome to redistribute it under certain conditions.
>>
>>Type 'license()' or 'licence()' for distribution details.
>>
>> 
>>
>>R is a collaborative project with many contributors.
>>
>>Type 'contributors()' for more information and
>>
>>'citation()' on how to cite R in publications.
>>
>> 
>>
>>Type 'demo()' for some demos, 'help()' for on-line help, or
>>
>>'help.start()' for a HTML browser interface to help.
>>
>>Type 'q()' to quit R.
>>
>> 
>>
>>
>>
>>>help.start()
>>
>>
>>Making links in per-session dir ...
>>
>>If /usr/bin/netscape is already running, it is *not* restarted, and
>>
>>    you must switch to its window.
>>
>>Otherwise, be patient ...
>>
>>
>>
>>>x <- rnorm(50)
>>
>>
>>>y <- rnorm(x)
>>
>>
>>>plot(x,y)
>>
>>
>>Illegal instruction (core dumped)
>>
>> 
>>
>> 
>>
>>Can anyone give me some suggestion? Thanks!
>>
>> 
>>
>> 
>>
>>Kexiao
>>
>> 
>>
>>
>>	[[alternative HTML version deleted]]
>>
>>______________________________________________
>>R-devel at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-devel
> 
> 
> 


-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From CTMAN at txccc.org  Wed Aug  4 17:44:12 2004
From: CTMAN at txccc.org (Man, Chris T.)
Date: Wed, 4 Aug 2004 10:44:12 -0500
Subject: [R] How to extract z value from coxph
Message-ID: <064925768627DD4DAC63C36FD0A088063439E1@TCCEXV3A.ad.TexasChildrensHospital.org>

Hi all,

I have tried to use coxph to determine which variables are significant in survival analysis. I also would like to use their z scores as weights to build a linear classification model. Although the function outputs the z value, but I can't access to it, so that I can write code to extract them automatically and pass them to the next function. Can someone help?

Thanks,

Chris



From spencer.graves at pdf.com  Wed Aug  4 18:13:18 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 04 Aug 2004 09:13:18 -0700
Subject: [R] Order of messgaes/ missing messages
In-Reply-To: <008901c479c5$3e08d160$3b8ac445@slnt.phub.net.cable.rogers.com>
References: <008901c479c5$3e08d160$3b8ac445@slnt.phub.net.cable.rogers.com>
Message-ID: <41110B1D.80003@pdf.com>

      Might this depend on whether someone responds to all or only to 
the list?  If one responds to all, then the originator should receive 2 
copies, If s/he subscribes to the list.  The copy via the list is often 
delayed hours by the listserve.  Might this explain the issue?  spencer 
graves

Jim Brennan wrote:

>I am curious as to why I quite often receive responses to questions on the help before receiving the actual question. 
> For example Mr Graves has responded to Mr Lumley's response to Mr Wegelin yet I  ave not received Mr. Lumley's post while I have received the other two. 
>
>Cheers,
>
>Jim
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From RBaskin at ahrq.gov  Wed Aug  4 18:38:42 2004
From: RBaskin at ahrq.gov (Baskin, Robert)
Date: Wed, 4 Aug 2004 12:38:42 -0400 
Subject: [R] Order of messgaes/ missing messages
Message-ID: <6BCD3F430455B1418750004BCD279259029262DF@exchange2.ahrq.gov>

It has to do with the topology of the web - if you are web-wise closer to
the responder than the original sender you can receive the answer before the
question - even if both answer and question go only to the list.

I learned this suscribed to SAS-L when I was at UF - I was downstream from
Bob Hamer (then at VCU) who, at the time, seemed to answer many of the
questions on SAS-L.  He would get the questions quickly because at VCU he
was (web-wise) close to a source of sending - I would always get his
responses before the original question because he was my nearest neighbor on
the web - after he moved to Rutgers the whole phenomenon stopped  (and if I
recall John Williams became primary answerer).

I wondered how long it would take before someone on this list asked about
it.  I see the answer before question phenomenon on this list quite strongly
and for me it is often Spencer Graves' answer I see first - like this one :)
Bob




-----Original Message-----
From: Spencer Graves [mailto:spencer.graves at pdf.com] 
Sent: Wednesday, August 04, 2004 12:13 PM
To: Jim Brennan
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Order of messgaes/ missing messages


      Might this depend on whether someone responds to all or only to 
the list?  If one responds to all, then the originator should receive 2 
copies, If s/he subscribes to the list.  The copy via the list is often 
delayed hours by the listserve.  Might this explain the issue?  spencer 
graves

Jim Brennan wrote:

>I am curious as to why I quite often receive responses to questions on 
>the help before receiving the actual question.
> For example Mr Graves has responded to Mr Lumley's response to Mr Wegelin
yet I  ave not received Mr. Lumley's post while I have received the other
two. 
>
>Cheers,
>
>Jim
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list 
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html
>  
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From fdusonchet at eim.ch  Wed Aug  4 18:55:18 2004
From: fdusonchet at eim.ch (Fabrice Dusonchet)
Date: Wed, 4 Aug 2004 18:55:18 +0200
Subject: [R] fitting distributions
Message-ID: <0A2559C22AB50C4C8ED88EC58D60876A02431A1C@saint_ex.eim.ch>

Hello,

I also try to fit a skewed distribution (like skewed student t) to data
points. Do you have an idee howto do this???

thank you

fabrice dusonchet


***********************************************************************************

This email and any files transmitted with it are confidentia...{{dropped}}



From tlumley at u.washington.edu  Wed Aug  4 19:06:49 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 4 Aug 2004 10:06:49 -0700 (PDT)
Subject: [R] How to extract z value from coxph
In-Reply-To: <064925768627DD4DAC63C36FD0A088063439E1@TCCEXV3A.ad.TexasChildrensHospital.org>
References: <064925768627DD4DAC63C36FD0A088063439E1@TCCEXV3A.ad.TexasChildrensHospital.org>
Message-ID: <Pine.A41.4.58.0408041006001.93898@homer10.u.washington.edu>

On Wed, 4 Aug 2004, Man, Chris T. wrote:

> Hi all,
>
> I have tried to use coxph to determine which variables are significant
> in survival analysis. I also would like to use their z scores as weights
> to build a linear classification model. Although the function outputs
> the z value, but I can't access to it, so that I can write code to
> extract them automatically and pass them to the next function. Can
> someone help?
>

Use the accessor functions:

z <- coef(model)/sqrt(diag(vcov(model)))

as for most regression models.

	-thomas



From xiafeng at uiuc.edu  Wed Aug  4 19:23:39 2004
From: xiafeng at uiuc.edu (xiafeng@uiuc.edu)
Date: Wed, 4 Aug 2004 12:23:39 -0500
Subject: [R] spatial econometric model for a simultaneous
 system
Message-ID: <4867bae6.6aaeb46c.820fa00@expms3.cites.uiuc.edu>

Hi,

Is there any package or functions in R that can perform 
tests and estimation for a spatial econometric model in a 
simultaneous system?

Thank you!!!

Xia Feng



From xiafeng at uiuc.edu  Wed Aug  4 19:31:59 2004
From: xiafeng at uiuc.edu (xiafeng@uiuc.edu)
Date: Wed, 4 Aug 2004 12:31:59 -0500
Subject: [R] about the systemfit package
Message-ID: <66305265.6aaf779c.8269c00@expms3.cites.uiuc.edu>

Hi,

I had a problem to get some of the values when I 
used 'systemfit()' function. I specified arguments of 'method
(2SLS), eqns, eqnlables, inst, data' in 'systemfit()' 
function. And it failed to return the values such 
as 'residuals', 'x'(matrix of all regressors) etc as 
displayed on its reference manual page 23, 24.

Is this because I didn't specify other arguments 
in 'systemfit()' function or other problems?

Is there any other way to extract residuals, coefficients 
etc out of this simultaneous system after doing 'systemfit
()'? 

Thank you very much!!!

Xia Feng



From p.dalgaard at biostat.ku.dk  Wed Aug  4 19:55:03 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 04 Aug 2004 19:55:03 +0200
Subject: [R] How to extract z value from coxph
In-Reply-To: <Pine.A41.4.58.0408041006001.93898@homer10.u.washington.edu>
References: <064925768627DD4DAC63C36FD0A088063439E1@TCCEXV3A.ad.TexasChildrensHospital.org>
	<Pine.A41.4.58.0408041006001.93898@homer10.u.washington.edu>
Message-ID: <x2vffy23go.fsf@biostat.ku.dk>

Thomas Lumley <tlumley at u.washington.edu> writes:

> > to build a linear classification model. Although the function outputs
> > the z value, but I can't access to it, so that I can write code to
> > extract them automatically and pass them to the next function. Can
> > someone help?
> >
> 
> Use the accessor functions:
> 
> z <- coef(model)/sqrt(diag(vcov(model)))
> 
> as for most regression models.

However (and sorry if I sound like a broken record) it would be nice
if summary.coxph, instead of just using cat(), returned an object ...
as most other regression models do.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From aleid2001 at yahoo.com  Wed Aug  4 20:13:15 2004
From: aleid2001 at yahoo.com (=?iso-8859-1?q?=20?=)
Date: Wed, 4 Aug 2004 19:13:15 +0100 (BST)
Subject: [R] cross random effects
Message-ID: <20040804181315.11418.qmail@web52808.mail.yahoo.com>

Dear friends,

I have asked last few days about cross-random effects
using PQL, but I have not receive any answer because
might my question was not clear.

My question was about analysing the salamander mating
data using PQL. This data contain cross-random effects
for (male) and for (female). By opining MASS and lme
library. I wrote this code

sala.glmm <- glmmPQL(fixed=y~WSf*WSM,
random=list(experiment=pdBlocked(list(pdIdent(~randf-1),pdIdent(~randm-1)))),
family=binomial, data=sala.data).

Where
data neame=sala.glmm which contain
 y is response
 wsf is fixed effect
 wsm is fixed effects
 randf  is random effect
 random is random effect

The data contain three experiments at the same time.
The previous cod is work but it does not give me
accurate result especially for the random effects.

For experiment I wrote this code 

experiment <-
factor(c(rep(1,120),rep(2,120),rep(3,120)))
 because I have three experiments at the same time,
but if I change the experiment to e.g

experiment <- factor(c(rep(1,360)))

is still give answer but is not the right answer. So,
I am accusing my specification of the experiment
(group). If you have any suggestion pleas let me know.

   E-mail:aleid2001 at yahoo.com



From olliegator at hortanet.com  Wed Aug  4 20:34:22 2004
From: olliegator at hortanet.com (Miguel Figueiredo)
Date: Wed, 4 Aug 2004 18:34:22 +0000
Subject: [R] Asymptotic Regression Model
Message-ID: <200408041834.22249.olliegator@hortanet.com>

Hi listers,

I have some data (see attachment), and I fitted it to the Asymptotic 
Regression Model with NLSstAsymptotic(xy). But I want to know the 
significance of these fits. How can I accomplish this using R? 

Can anyone suggest some theoretical reading on this subject?

Thanks,

Miguel

-- 
Miguel Figueiredo
IT student / Marine Biologist
"Tem calma irm?o que a morte n?o precisa do teu sim, ? coisa certa, mais vale 
fazer da vida um festim." ALC
-------------- next part --------------
A non-text attachment was scrubbed...
Name: graph.png
Type: image/png
Size: 4005 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20040804/08dd5a86/graph.png

From wang at galton.uchicago.edu  Wed Aug  4 20:30:15 2004
From: wang at galton.uchicago.edu (Yong Wang)
Date: Wed, 4 Aug 2004 13:30:15 -0500 (CDT)
Subject: [R] Re: R-help Digest, Vol 18, Issue 3
In-Reply-To: <200408031004.i73A4Fas007341@hypatia.math.ethz.ch>
References: <200408031004.i73A4Fas007341@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.58.0408041330100.20651@aitken.uchicago.edu>



From ggrothendieck at myway.com  Wed Aug  4 21:04:57 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 4 Aug 2004 19:04:57 +0000 (UTC)
Subject: [R] dots expansion
References: <3A822319EB35174CA3714066D590DCD504AF8198@usrymx25.merck.com>
Message-ID: <loom.20040804T205650-950@post.gmane.org>


That's a good point.  Your original solution is simpler than the poster's
and likely faster than mine.  

At any rate, continuing the with simplicity, rather than performance, vein if
one does want to allow matrices or data frames then the solution I posted won't
work, even if you take away the check for data frames, since I was using the
following trick to append a column without using cbind:

	x[,"newcol"] <- whatever

That appends whatever as a column named "newcol" if x is a data frame but
gives an error if x is a matrix.  

In that case, we should use cbind, and I think the problem the poster was
having is that cbind(x, name = values) regards name as literally that and
not a variable.  To get around this we can temporarily define our own 
cbind which allows the column names to be specified.  The new cbind 
definition is elementary and neither the last line, which does the key 
processing, nor the rest of it involves any rep arithmetic, indices or evals:

rbind.case <- function(..., name = "case", values = seq(along = list(...))) 
# rbind the ... data frames or matrices together adding a column named name 
# whose value for rows from ith argument is values[i]
{

  dots <- list(...)
  stopifnot(length(dots) == length(values))

  # cbind x and y using indicated column names
  cbind <- function(x, y, names.x = colnames(x), names.y = colnames(y)) { 
    z <- base::cbind(x, y)
    colnames(z) <- c(names.x, names.y)
    z
  }

  do.call("rbind", mapply(cbind, dots, values, names.y = name, SIMPLIFY = F))

}




Liaw, Andy <andy_liaw <at> merck.com> writes:

: 
: In addition to Gabor's comments:
: 
: There's a reason why I didn't coerce the grouping variable to a factor.
: rbind()ing data frames is much more expensive than rbind()ing
: arrays/matrices.  Unless your data really have different data types in
: different columns, it would mostly likely be better to work with the matrix
: version of them.  If you really want a data frame with the grouping variable
: as a factor, you can do the coercion afterward.
: 
: Andy
: 
: > From: Gabor Grothendieck
: > 
: > Viet Nguyen <vietnguyen <at> fastmail.fm> writes:
: > 
: > > 
: > > Thanks to all who helped.
: > > 
: > > I used your ideas and code samples to write the following (for the 
: > > benefit of people who will search this list later):
: > > 
: > > rbind.case <- function(..., name="case", values) {
: > >     dots <- list(...);
: > >     if (missing(values)) values <- 1:length(dots);
: > >     if (length(values)!=length(dots))
: > >       stop("length(values)!=length(list(...))");
: > > 
: > >     eval(parse(text=
: > >                paste("cbind(rbind(...), ",name,
: > >                      "=rep(values, sapply(dots, nrow)))",sep="")));
: > > }
: > > 
: > > The function is to be used with data frames. It's not as 
: > good as it can 
: > > be but it works for my purpose.
: > 
: > Regarding improvements, eliminate the semicolons at the end 
: > of statements,
: > place the default value for values= in the arg list to make 
: > it more readable,
: > use stopifnot to check args (also for readability), add a check for 
: > data frames (which is mentioned after the code but not checked for, 
: > and eliminate the eval and rep calculations by simply 
: > lapplying over an 
: > index and appending the name column to each data frame in turn:
: > 
: > 
: > rbind.case <- function(..., name="case", values = seq(along = 
: > list(...))) 
: > # rbind the ... data frames together adding a column named name whose 
: > # value for rows from ith argument is values[i]
: > {
: >    dots <- list(...)
: >    stopifnot(length(dots) == length(values),
: > 	   all(sapply(dots, inherits, "data.frame")))
: > 
: >    f <- function(i) { x <- dots[[i]]; x[,name] <- values[i]; x }
: >    do.call("rbind", lapply(seq(along = dots), f))
: > }
: > 
: > ______________________________________________
: > R-help <at> stat.math.ethz.ch mailing list
: > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
: > PLEASE do read the posting guide! 
: > http://www.R-project.org/posting-guide.html
: > 
: >
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://www.stat.math.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
: 
:



From korponai at georgikon.hu  Wed Aug  4 21:35:32 2004
From: korponai at georgikon.hu (Korponai =?iso-8859-2?q?J=E1nos?=)
Date: Wed, 4 Aug 2004 21:35:32 +0200
Subject: [R] ERROR: compilation failed for package 'rgl'
Message-ID: <200408042135.32996.korponai@georgikon.hu>

X-BeenThere: r-help at stat.math.ethz.ch
X-Mailman-Version: 2.1.5
Precedence: list
Reply-To: korponai.janos at nyuduvizig.hu
List-Id: "Main R Mailing List: Primary help" <r-help.stat.math.ethz.ch>
List-Unsubscribe: <https://www.stat.math.ethz.ch/mailman/listinfo/r-help>,
	<mailto:r-help-request at stat.math.ethz.ch?subject=unsubscribe>
List-Archive: <https://www.stat.math.ethz.ch/pipermail/r-help>
List-Post: <mailto:r-help at stat.math.ethz.ch>
List-Help: <mailto:r-help-request at stat.math.ethz.ch?subject=help>
List-Subscribe: <https://www.stat.math.ethz.ch/mailman/listinfo/r-help>,
	<mailto:r-help-request at stat.math.ethz.ch?subject=subscribe>
X-List-Received-Date: Wed, 04 Aug 2004 19:33:56 -0000

Dear All,

I could not install rgl package. What is missing?

daphnia:~# R CMDR INSTALL rgl_0.64-13.tar.gz
* Installing *source* package 'rgl' ...
checking build system type... i686-pc-linux-gnu
checking host system type... i686-pc-linux-gnu
checking for gcc... gcc
checking for C compiler default output file name... a.out
checking whether the C compiler works... yes
checking whether we are cross compiling... no
checking for suffix of executables...
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ANSI C... none needed
checking how to run the C preprocessor... gcc -E
checking for X... libraries /usr/X11R6/lib, 
headers /usr/X11R6/include

checking for libpng-config... yes
configure: creating ./config.status
config.status: creating src/Makevars
** libs
g++ -I/usr/lib/R/include -I/usr/X11R6/include -DHAVE_PNG_H   
-mieee-fp -Wall -pedantic -fno-exceptions -fno-rtti -fPIC  -g 
-O2 -c x11lib.cpp -o x11lib.o
In file included from glgui.h:9,
                 from gui.h:10,
                 from x11gui.h:10,
                 from x11lib.cpp:13:
opengl.h:19:19: GL/gl.h: No such file or directory
opengl.h:20:20: GL/glu.h: No such file or directory
In file included from gui.h:10,
                 from x11gui.h:10,
                 from x11lib.cpp:13:
glgui.h:27: error: 'GLuint' is used as a type, but is not defined 
as a type.
glgui.h:28: error: 'GLuint' is used as a type, but is not defined 
as a type.
glgui.h:29: error: 'GLuint' is used as a type, but is not defined 
as a type.
glgui.h: In member function `void GLBitmapFont::enable()':
glgui.h:22: error: `listBase' undeclared (first use this 
function)
glgui.h:22: error: (Each undeclared identifier is reported only 
once for each
   function it appears in.)
glgui.h:22: error: `glListBase' undeclared (first use this 
function)
In file included from x11lib.cpp:13:
x11gui.h:13:20: GL/glx.h: No such file or directory
In file included from x11lib.cpp:13:
x11gui.h: At global scope:
x11gui.h:45: error: syntax error before `*' token
x11gui.h:52: error: 'GLXContext' is used as a type, but is not 
defined as a
   type.
make: *** [x11lib.o] Error 1
ERROR: compilation failed for package 'rgl'
** Removing '/usr/local/lib/R/site-library/rgl'
daphnia:~# version
         _
platform i386-pc-linux-gnu
arch     i386
os       linux-gnu
system   i386, linux-gnu
status
major    1
minor    9.0
year     2004
month    04
day      12
language R

-- 
Dr. Janos Korponai
West-Transdanubian District Environmental and Water Authority, 
Dept. Kis-Balaton
H-8360 Keszthely, Csik F. str. 1, Hungary
e-mail: korponai.janos at nyuduvizig.hu



From edd at debian.org  Wed Aug  4 21:43:55 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 4 Aug 2004 14:43:55 -0500
Subject: [R] ERROR: compilation failed for package 'rgl'
In-Reply-To: <200408042135.32996.korponai@georgikon.hu>
References: <200408042135.32996.korponai@georgikon.hu>
Message-ID: <20040804194355.GA21187@sonny.eddelbuettel.com>

On Wed, Aug 04, 2004 at 09:35:32PM +0200, Korponai J?nos wrote:
> I could not install rgl package. What is missing?
[...] 
> config.status: creating src/Makevars
> ** libs
> g++ -I/usr/lib/R/include -I/usr/X11R6/include -DHAVE_PNG_H   
> -mieee-fp -Wall -pedantic -fno-exceptions -fno-rtti -fPIC  -g 
> -O2 -c x11lib.cpp -o x11lib.o
> In file included from glgui.h:9,
>                  from gui.h:10,
>                  from x11gui.h:10,
>                  from x11lib.cpp:13:
> opengl.h:19:19: GL/gl.h: No such file or directory
> opengl.h:20:20: GL/glu.h: No such file or directory
          
These files are mssing. 

You need the headers for GL, probably from a -dev package for your Linux
distribution. I use xlibmesa-gl-dev and xlibmesa-glu-dev under Debian; other
distros probably use similar but different names. The important part is the
-dev, otherwise you have libraries (to run binaries) but no headers (to
compile against).

Debian users can get a binary RGl package via 
	$ apt-get install r-cran-rgl
	
Hope this helps, Dirk 

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx



From rossini at blindglobe.net  Wed Aug  4 21:40:29 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Wed, 04 Aug 2004 12:40:29 -0700
Subject: [R] ERROR: compilation failed for package 'rgl'
In-Reply-To: =?iso-8859-1?q?<200408042135.32996.korponai@georgikon.hu>_?=
	=?iso-8859-1?q?(Korponai_J=E1nos's_message_of_"Wed,
	_4_Aug_2004_21:35:32?= =?iso-8859-1?q?_+0200")?=
References: <200408042135.32996.korponai@georgikon.hu>
Message-ID: <857jsepu8i.fsf@servant.blindglobe.net>


OpenGL or MESA librares.

Korponai J??nos <korponai at georgikon.hu> writes:

> X-BeenThere: r-help at stat.math.ethz.ch
> X-Mailman-Version: 2.1.5
> Precedence: list
> Reply-To: korponai.janos at nyuduvizig.hu
> List-Id: "Main R Mailing List: Primary help" <r-help.stat.math.ethz.ch>
> List-Unsubscribe: <https://www.stat.math.ethz.ch/mailman/listinfo/r-help>,
> 	<mailto:r-help-request at stat.math.ethz.ch?subject=unsubscribe>
> List-Archive: <https://www.stat.math.ethz.ch/pipermail/r-help>
> List-Post: <mailto:r-help at stat.math.ethz.ch>
> List-Help: <mailto:r-help-request at stat.math.ethz.ch?subject=help>
> List-Subscribe: <https://www.stat.math.ethz.ch/mailman/listinfo/r-help>,
> 	<mailto:r-help-request at stat.math.ethz.ch?subject=subscribe>
> Sender: r-help-bounces at stat.math.ethz.ch
> Errors-To: r-help-bounces at stat.math.ethz.ch
>
> Dear All,
>
> I could not install rgl package. What is missing?
>
> daphnia:~# R CMDR INSTALL rgl_0.64-13.tar.gz
> * Installing *source* package 'rgl' ...
> checking build system type... i686-pc-linux-gnu
> checking host system type... i686-pc-linux-gnu
> checking for gcc... gcc
> checking for C compiler default output file name... a.out
> checking whether the C compiler works... yes
> checking whether we are cross compiling... no
> checking for suffix of executables...
> checking for suffix of object files... o
> checking whether we are using the GNU C compiler... yes
> checking whether gcc accepts -g... yes
> checking for gcc option to accept ANSI C... none needed
> checking how to run the C preprocessor... gcc -E
> checking for X... libraries /usr/X11R6/lib, 
> headers /usr/X11R6/include
>
> checking for libpng-config... yes
> configure: creating ./config.status
> config.status: creating src/Makevars
> ** libs
> g++ -I/usr/lib/R/include -I/usr/X11R6/include -DHAVE_PNG_H   
> -mieee-fp -Wall -pedantic -fno-exceptions -fno-rtti -fPIC  -g 
> -O2 -c x11lib.cpp -o x11lib.o
> In file included from glgui.h:9,
>                  from gui.h:10,
>                  from x11gui.h:10,
>                  from x11lib.cpp:13:
> opengl.h:19:19: GL/gl.h: No such file or directory
> opengl.h:20:20: GL/glu.h: No such file or directory
> In file included from gui.h:10,
>                  from x11gui.h:10,
>                  from x11lib.cpp:13:
> glgui.h:27: error: 'GLuint' is used as a type, but is not defined 
> as a type.
> glgui.h:28: error: 'GLuint' is used as a type, but is not defined 
> as a type.
> glgui.h:29: error: 'GLuint' is used as a type, but is not defined 
> as a type.
> glgui.h: In member function `void GLBitmapFont::enable()':
> glgui.h:22: error: `listBase' undeclared (first use this 
> function)
> glgui.h:22: error: (Each undeclared identifier is reported only 
> once for each
>    function it appears in.)
> glgui.h:22: error: `glListBase' undeclared (first use this 
> function)
> In file included from x11lib.cpp:13:
> x11gui.h:13:20: GL/glx.h: No such file or directory
> In file included from x11lib.cpp:13:
> x11gui.h: At global scope:
> x11gui.h:45: error: syntax error before `*' token
> x11gui.h:52: error: 'GLXContext' is used as a type, but is not 
> defined as a
>    type.
> make: *** [x11lib.o] Error 1
> ERROR: compilation failed for package 'rgl'
> ** Removing '/usr/local/lib/R/site-library/rgl'
> daphnia:~# version
>          _
> platform i386-pc-linux-gnu
> arch     i386
> os       linux-gnu
> system   i386, linux-gnu
> status
> major    1
> minor    9.0
> year     2004
> month    04
> day      12
> language R
>
> -- 
> Dr. Janos Korponai
> West-Transdanubian District Environmental and Water Authority, 
> Dept. Kis-Balaton
> H-8360 Keszthely, Csik F. str. 1, Hungary
> e-mail: korponai.janos at nyuduvizig.hu
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Anthony Rossini			    Research Associate Professor
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From ripley at stats.ox.ac.uk  Wed Aug  4 21:51:39 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 4 Aug 2004 20:51:39 +0100 (BST)
Subject: [R] ERROR: compilation failed for package 'rgl'
In-Reply-To: <200408042135.32996.korponai@georgikon.hu>
Message-ID: <Pine.LNX.4.44.0408042041250.4406-100000@gannet.stats>

On Wed, 4 Aug 2004, Korponai [iso-8859-2] J??nos wrote:

> I could not install rgl package. What is missing?

OpenGL, or at least its header files.  For Fedora Core 2, the GL headers
seems to be in xorg-x11-devel-6.7.0-5, for RH8.0 XFree86-devel-4.2.1-21.
In both cases, the same place as the X11 headers, which may be why the 
configure script does not check for them (but it should).

> daphnia:~# R CMDR INSTALL rgl_0.64-13.tar.gz
> * Installing *source* package 'rgl' ...
> checking build system type... i686-pc-linux-gnu
> checking host system type... i686-pc-linux-gnu
> checking for gcc... gcc
> checking for C compiler default output file name... a.out
> checking whether the C compiler works... yes
> checking whether we are cross compiling... no
> checking for suffix of executables...
> checking for suffix of object files... o
> checking whether we are using the GNU C compiler... yes
> checking whether gcc accepts -g... yes
> checking for gcc option to accept ANSI C... none needed
> checking how to run the C preprocessor... gcc -E
> checking for X... libraries /usr/X11R6/lib, 
> headers /usr/X11R6/include
> 
> checking for libpng-config... yes
> configure: creating ./config.status
> config.status: creating src/Makevars
> ** libs
> g++ -I/usr/lib/R/include -I/usr/X11R6/include -DHAVE_PNG_H   
> -mieee-fp -Wall -pedantic -fno-exceptions -fno-rtti -fPIC  -g 
> -O2 -c x11lib.cpp -o x11lib.o
> In file included from glgui.h:9,
>                  from gui.h:10,
>                  from x11gui.h:10,
>                  from x11lib.cpp:13:
> opengl.h:19:19: GL/gl.h: No such file or directory
> opengl.h:20:20: GL/glu.h: No such file or directory

...

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From HDoran at air.org  Wed Aug  4 22:17:01 2004
From: HDoran at air.org (Doran, Harold)
Date: Wed, 4 Aug 2004 16:17:01 -0400
Subject: [R] Concatenating variables
Message-ID: <88EAF3512A55DF46B06B1954AEF73F74047BFD47@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040804/6551dd6a/attachment.pl

From deepayan at stat.wisc.edu  Wed Aug  4 22:25:32 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Wed, 4 Aug 2004 15:25:32 -0500
Subject: [R] cross random effects
In-Reply-To: <20040804181315.11418.qmail@web52808.mail.yahoo.com>
References: <20040804181315.11418.qmail@web52808.mail.yahoo.com>
Message-ID: <200408041525.32888.deepayan@stat.wisc.edu>

On Wednesday 04 August 2004 01:13 pm,   wrote:
> Dear friends,
>
> I have asked last few days about cross-random effects
> using PQL, but I have not receive any answer because
> might my question was not clear.
>
> My question was about analysing the salamander mating
> data using PQL. This data contain cross-random effects
> for (male) and for (female). By opining MASS and lme
> library. I wrote this code

Firstly, these are packages, not libraries. Secondly, there's no package 
called lme (but I'm guessing you are talking about nlme).

> sala.glmm <- glmmPQL(fixed=y~WSf*WSM,
> random=list(experiment=pdBlocked(list(pdIdent(~randf-1),pdIdent(~randm-1)))
>), family=binomial, data=sala.data).
>
> Where
> data neame=sala.glmm which contain
>  y is response
>  wsf is fixed effect
>  wsm is fixed effects
>  randf  is random effect
>  random is random effect
>
> The data contain three experiments at the same time.
> The previous cod is work but it does not give me
> accurate result especially for the random effects.

This is a bit vague. How are you judging accuracy?

> For experiment I wrote this code
>
> experiment <-
> factor(c(rep(1,120),rep(2,120),rep(3,120)))
>  because I have three experiments at the same time,
> but if I change the experiment to e.g
>
> experiment <- factor(c(rep(1,360)))
>
> is still give answer but is not the right answer. So,
> I am accusing my specification of the experiment
> (group). If you have any suggestion pleas let me know.

It's difficult to suggest anything without knowing more about your data. What 
exactly do the variables randf and randm represent? If you really need 
crossed random effects (and it's not clear from what you have told us that 
you do), you might try the GLMM() function in the lme4 package. 

Deepayan



From andy_liaw at merck.com  Wed Aug  4 22:25:09 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 4 Aug 2004 16:25:09 -0400
Subject: [R] Concatenating variables
Message-ID: <3A822319EB35174CA3714066D590DCD504AF81A3@usrymx25.merck.com>

Try something like:

paste(tenn$up, tenn$down, tenn$stable, collapse="")

HTH,
Andy

> From: Doran, Harold
> 
> Hi all:
> 
>  
> 
> I'm having difficulty with something I believe is very simple, but I'm
> stuck. I have a large data frame that took days to clean and prepare.
> All I now need to do is concatenate three variables into a single
> column. For example, I have tenn$up, tenn$down, and tenn$stable which
> all have values of 1 or 0. I simply want to put all three columns
> together to create a pattern (e.g., 111, 101, 001).
> 
>  
> 
> I tried c(tenn$up,tenn$down,tenn$stable)
> 
>  
> 
> But this isn't working. Thanks for any help offered.
> 
>  
> 
> Harold
> 
>  
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From sundar.dorai-raj at PDF.COM  Wed Aug  4 22:32:31 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Wed, 04 Aug 2004 15:32:31 -0500
Subject: [R] Concatenating variables
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F74047BFD47@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F74047BFD47@dc1ex2.air.org>
Message-ID: <411147DF.8070605@pdf.com>



Doran, Harold wrote:

> Hi all:
> 
>  
> 
> I'm having difficulty with something I believe is very simple, but I'm
> stuck. I have a large data frame that took days to clean and prepare.
> All I now need to do is concatenate three variables into a single
> column. For example, I have tenn$up, tenn$down, and tenn$stable which
> all have values of 1 or 0. I simply want to put all three columns
> together to create a pattern (e.g., 111, 101, 001).
> 
>  
> 
> I tried c(tenn$up,tenn$down,tenn$stable)
> 
>  
> 
> But this isn't working. Thanks for any help offered.
> 
>  

Try ?paste unstead of `c':

paste(tenn$up, tenn$down, tenn$stable, sep = "")

--sundar



From ggrothendieck at myway.com  Wed Aug  4 22:50:14 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 4 Aug 2004 20:50:14 +0000 (UTC)
Subject: [R] Concatenating variables
References: <88EAF3512A55DF46B06B1954AEF73F74047BFD47@dc1ex2.air.org>
Message-ID: <loom.20040804T224450-915@post.gmane.org>

Doran, Harold <HDoran <at> air.org> writes:

: I'm having difficulty with something I believe is very simple, but I'm
: stuck. I have a large data frame that took days to clean and prepare.
: All I now need to do is concatenate three variables into a single
: column. For example, I have tenn$up, tenn$down, and tenn$stable which
: all have values of 1 or 0. I simply want to put all three columns
: together to create a pattern (e.g., 111, 101, 001).
: 
: I tried c(tenn$up,tenn$down,tenn$stable)
: 
: But this isn't working. Thanks for any help offered.
: 

Several people have already mentioned paste, which returns a character
result, and two other solutions, depending on what you are looking for, 
are:

   with(tenn, 100 * up + 10 * down + stable)  # numeric result

   with(tenn, interaction(up, down, stable, sep =""))  # factor result



From p.dalgaard at biostat.ku.dk  Wed Aug  4 22:50:28 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 04 Aug 2004 22:50:28 +0200
Subject: [R] ERROR: compilation failed for package 'rgl'
In-Reply-To: <200408042135.32996.korponai@georgikon.hu>
References: <200408042135.32996.korponai@georgikon.hu>
Message-ID: <x2r7qm1vcb.fsf@biostat.ku.dk>

Korponai J??nos <korponai at georgikon.hu> writes:

> X-BeenThere: r-help at stat.math.ethz.ch
> X-Mailman-Version: 2.1.5
...
> Sender: r-help-bounces at stat.math.ethz.ch
> Errors-To: r-help-bounces at stat.math.ethz.ch

[What happened with the mail headers? Confused my mail filtering.]

> I could not install rgl package. What is missing?
> 
> daphnia:~# R CMDR INSTALL rgl_0.64-13.tar.gz
                  ^

Here's an R too many, but the things that you're missing are include
files for GL code:

> opengl.h:19:19: GL/gl.h: No such file or directory
> opengl.h:20:20: GL/glu.h: No such file or directory
> x11gui.h:13:20: GL/glx.h: No such file or directory

You don't actually say what your platform is and the version info is
insufficient to distinguish between SuSE and RedHat for instance. What
you need to do exactly depends on distribution. In RedHat you seem to
need to install XFree86-devel whereas SuSE wants XFree86-Mesa-devel.
You probably also need the actual libraries: XFree86-Mesa-libGL and
XFree86-Mesa-libGLU (RH), respectively XFree86-Mesa (SuSE).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jfox at al.noaa.gov  Wed Aug  4 23:01:23 2004
From: jfox at al.noaa.gov (Jenny Fox)
Date: Wed, 4 Aug 2004 15:01:23 -0600
Subject: [R] Horizontal color bar in filled.contour()
Message-ID: <719B7E50-E659-11D8-BC0A-0050E485F84D@al.noaa.gov>

Hello.

I was wondering if it is possible to put the color bar (key) on the 
bottom of the graph, like the "horizontal" toggle in image.plot, rather 
than on the right side.  Does anyone know if this is possible?

--jenny



From p.dalgaard at biostat.ku.dk  Wed Aug  4 23:08:46 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 04 Aug 2004 23:08:46 +0200
Subject: [R] Concatenating variables
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F74047BFD47@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F74047BFD47@dc1ex2.air.org>
Message-ID: <x2n01a1uht.fsf@biostat.ku.dk>

"Doran, Harold" <HDoran at air.org> writes:

> Hi all:
> 
>  
> 
> I'm having difficulty with something I believe is very simple, but I'm
> stuck. I have a large data frame that took days to clean and prepare.
> All I now need to do is concatenate three variables into a single
> column. For example, I have tenn$up, tenn$down, and tenn$stable which
> all have values of 1 or 0. I simply want to put all three columns
> together to create a pattern (e.g., 111, 101, 001).

That's not contatenation!

> I tried c(tenn$up,tenn$down,tenn$stable)

but this is....

> But this isn't working. Thanks for any help offered.

It's working alright, just not doing what you want. How about

paste(tenn$up,tenn$down,tenn$stable,sep="")

?

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From sundar.dorai-raj at PDF.COM  Wed Aug  4 23:31:52 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Wed, 04 Aug 2004 16:31:52 -0500
Subject: [R] Horizontal color bar in filled.contour()
In-Reply-To: <719B7E50-E659-11D8-BC0A-0050E485F84D@al.noaa.gov>
References: <719B7E50-E659-11D8-BC0A-0050E485F84D@al.noaa.gov>
Message-ID: <411155C8.4090607@pdf.com>



Jenny Fox wrote:

> Hello.
> 
> I was wondering if it is possible to put the color bar (key) on the 
> bottom of the graph, like the "horizontal" toggle in image.plot, rather 
> than on the right side.  Does anyone know if this is possible?
> 
> --jenny
> 

It does not appear so by taking a rough looking at code for 
filled.contour. You should consider using levelplot in package:lattice.

library(lattice)
x <- seq(pi/4, 5 * pi, length = 100)
y <- seq(pi/4, 5 * pi, length = 100)
r <- as.vector(sqrt(outer(x^2, y^2, "+")))
grid <- expand.grid(x=x, y=y)
grid$z <- cos(r^2) * exp(-r/(pi^3))
lset(theme = col.whitebg())
levelplot(z~x*y, grid, cuts = 50, scales=list(log="e"), xlab="",
           ylab="", main="Weird Function", sub="with log scales",
            colorkey = list(space = "bottom"), region = TRUE)

--sundar



From deepayan at stat.wisc.edu  Thu Aug  5 00:32:02 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Wed, 4 Aug 2004 17:32:02 -0500
Subject: new sep argument for interaction (was: Re: [R] Concatenating
	variables)
In-Reply-To: <loom.20040804T224450-915@post.gmane.org>
References: <88EAF3512A55DF46B06B1954AEF73F74047BFD47@dc1ex2.air.org>
	<loom.20040804T224450-915@post.gmane.org>
Message-ID: <200408041732.02461.deepayan@stat.wisc.edu>

On Wednesday 04 August 2004 15:50, Gabor Grothendieck wrote:

> Several people have already mentioned paste, which returns a
> character result, and two other solutions, depending on what you are
> looking for, are:
>
>    with(tenn, 100 * up + 10 * down + stable)  # numeric result
>
>    with(tenn, interaction(up, down, stable, sep =""))  # factor
> result

Except that interaction doesn't have a sep= argument. 

I think it would be useful to have one, and this was also suggested by 
Ted Harding back in June:

http://tolstoy.newcastle.edu.au/R/help/04/06/0984.html

Any reason against implementing this?

Deepayan



From elvis at xlsolutions-corp.com  Thu Aug  5 00:34:33 2004
From: elvis at xlsolutions-corp.com (elvis@xlsolutions-corp.com)
Date: Wed,  4 Aug 2004 15:34:33 -0700
Subject: [R] Course***Advanced R/Splus Programming in San Francisco and
	Washington DC by XLsolutions Corp.
Message-ID: <20040804223433.1260.qmail@webmail04.mesa1.secureserver.net>

XSolutions Corp (www.xlsolutions-corp.com) is proud to announce
a 2-day "Advanced R/Splus programming" taught by R Development
Core Team Guru!


*********San Francisco ----------->  August 30-31, 2004
*********Washington DC ----------> TBD (Please email us)


         
           Reserve your seat Now @ Early Bird  (payment due AFTER the
class)

Registration:
Email Sue Turner: sue at xlsolutions-corp.com
Phone: 206-686-1578

www.xlsolutions-corp.com/training.htm

With the following outline:

- Overview of R/S fundamentals: Syntax and Semantics
- Class and Inheritance in R/S-Plus
- Concepts, Construction and good use of language objects
- Coercion and efficiency
- Object-oriented programming in R and S-Plus
- Advanced manipulation tools: Parse, Deparse, Substitute, etc.
- How to fully take advantage of Vectorization
- Generic and Method Functions; S4 (S-Plus 6)
- Search path, databases and frames Visibility
- Working with large objects
- Handling Properly Recursion and iterative calculations
- Managing loops; For (S-Plus) and for() loops
- Consequences of Lazy Evaluation
- Efficient Code practices for large computations
- Memory management and Resource monitoring
- Writing R/S-Plus functions to call compiled code
- Writing and debugging compiled code for R/S-Plus system
- Connecting R/S-Plus to External Data Sources
- Understanding the structure of model fitting functions in R/S-Plus
- Designing and Packaging efficiently a new model function


Please email us for the full description of the course with fees and
information on trainers. For example, Early-bird group research fee is
$995!
It'll also deal with lots of S-Plus efficiency issues and any special
topics
from participants is welcome.

Please let us know if you and your colleagues are interested in this
class
to take advantage of group discount. 
Register now to secure your seat in this course!

Cheers,

Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com



From jfbrennan at rogers.com  Thu Aug  5 02:06:14 2004
From: jfbrennan at rogers.com (Jim Brennan)
Date: Wed, 4 Aug 2004 20:06:14 -0400
Subject: [R] Order of messgaes/ missing messages
References: <6BCD3F430455B1418750004BCD279259029262DF@exchange2.ahrq.gov>
Message-ID: <005001c47a80$06c087a0$3b8ac445@slnt.phub.net.cable.rogers.com>

Thanks for the answers. It is interesting. I had thought everything would go
to a central server and then be sent out in order from there, but I guess I
just don't get the topology of the web. :-)
 I think there may be other issues though, since sometimes there are very
long, what seem like random, delays.
Jim
----- Original Message -----
From: "Baskin, Robert" <RBaskin at ahrq.gov>
To: "'Spencer Graves'" <spencer.graves at pdf.com>; "Jim Brennan"
<jfbrennan at rogers.com>
Cc: <r-help at stat.math.ethz.ch>
Sent: Wednesday, August 04, 2004 12:38 PM
Subject: RE: [R] Order of messgaes/ missing messages


> It has to do with the topology of the web - if you are web-wise closer to
> the responder than the original sender you can receive the answer before
the
> question - even if both answer and question go only to the list.
>
> I learned this suscribed to SAS-L when I was at UF - I was downstream from
> Bob Hamer (then at VCU) who, at the time, seemed to answer many of the
> questions on SAS-L.  He would get the questions quickly because at VCU he
> was (web-wise) close to a source of sending - I would always get his
> responses before the original question because he was my nearest neighbor
on
> the web - after he moved to Rutgers the whole phenomenon stopped  (and if
I
> recall John Williams became primary answerer).
>
> I wondered how long it would take before someone on this list asked about
> it.  I see the answer before question phenomenon on this list quite
strongly
> and for me it is often Spencer Graves' answer I see first - like this one
:)
> Bob
>
>
>
>
> -----Original Message-----
> From: Spencer Graves [mailto:spencer.graves at pdf.com]
> Sent: Wednesday, August 04, 2004 12:13 PM
> To: Jim Brennan
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Order of messgaes/ missing messages
>
>
>       Might this depend on whether someone responds to all or only to
> the list?  If one responds to all, then the originator should receive 2
> copies, If s/he subscribes to the list.  The copy via the list is often
> delayed hours by the listserve.  Might this explain the issue?  spencer
> graves
>
> Jim Brennan wrote:
>
> >I am curious as to why I quite often receive responses to questions on
> >the help before receiving the actual question.
> > For example Mr Graves has responded to Mr Lumley's response to Mr
Wegelin
> yet I  ave not received Mr. Lumley's post while I have received the other
> two.
> >
> >Cheers,
> >
> >Jim
> > [[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide!
> >http://www.R-project.org/posting-guide.html
> >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Thu Aug  5 02:15:25 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 5 Aug 2004 00:15:25 +0000 (UTC)
Subject: new sep argument for interaction (was: Re: [R]
	=?utf-8?b?Q29uY2F0ZW5hdGluZwl2YXJpYWJsZXMp?=
References: <88EAF3512A55DF46B06B1954AEF73F74047BFD47@dc1ex2.air.org>
	<loom.20040804T224450-915@post.gmane.org>
	<200408041732.02461.deepayan@stat.wisc.edu>
Message-ID: <loom.20040805T021221-289@post.gmane.org>

Deepayan Sarkar <deepayan <at> stat.wisc.edu> writes:

: 
: On Wednesday 04 August 2004 15:50, Gabor Grothendieck wrote:
: 
: > Several people have already mentioned paste, which returns a
: > character result, and two other solutions, depending on what you are
: > looking for, are:
: >
: >    with(tenn, 100 * up + 10 * down + stable)  # numeric result
: >
: >    with(tenn, interaction(up, down, stable, sep =""))  # factor
: > result
: 
: Except that interaction doesn't have a sep= argument. 

The interaction in Hmisc has a sep= argument.  (I did ?interaction before
posting but forgot that I had Hmisc loaded and did not notice that I was
looking at the help for Hmisc interaction rather than the base interaction.)

Thus, just to be definite, the above should have been:

  require(Hmisc)
  with(tenn, interaction(up, down, stable, sep = ""))



From ggrothendieck at myway.com  Thu Aug  5 02:20:32 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 5 Aug 2004 00:20:32 +0000 (UTC)
Subject: [R] Order of messgaes/ missing messages
References: <6BCD3F430455B1418750004BCD279259029262DF@exchange2.ahrq.gov>
	<005001c47a80$06c087a0$3b8ac445@slnt.phub.net.cable.rogers.com>
Message-ID: <loom.20040805T021649-752@post.gmane.org>


Note that if this bothers you then you could try a web-based
email systems (e.g. yahoo, hotmail, etc.) to receive your r-help 
messages since the path to any of them would be independent of your 
particular location on the net.

Jim Brennan <jfbrennan <at> rogers.com> writes:

: Thanks for the answers. It is interesting. I had thought everything would go
: to a central server and then be sent out in order from there, but I guess I
: just don't get the topology of the web. 
:  I think there may be other issues though, since sometimes there are very
: long, what seem like random, delays.
: Jim
: ----- Original Message -----
: From: "Baskin, Robert" <RBaskin <at> ahrq.gov>
: To: "'Spencer Graves'" <spencer.graves <at> pdf.com>; "Jim Brennan"
: <jfbrennan <at> rogers.com>
: Cc: <r-help <at> stat.math.ethz.ch>
: Sent: Wednesday, August 04, 2004 12:38 PM
: Subject: RE: [R] Order of messgaes/ missing messages
: 
: 
: > It has to do with the topology of the web - if you are web-wise closer to
: > the responder than the original sender you can receive the answer before
: the
: > question - even if both answer and question go only to the list.
: >
: > I learned this suscribed to SAS-L when I was at UF - I was downstream from
: > Bob Hamer (then at VCU) who, at the time, seemed to answer many of the
: > questions on SAS-L.  He would get the questions quickly because at VCU he
: > was (web-wise) close to a source of sending - I would always get his
: > responses before the original question because he was my nearest neighbor
: on
: > the web - after he moved to Rutgers the whole phenomenon stopped  (and if
: I
: > recall John Williams became primary answerer).
: >
: > I wondered how long it would take before someone on this list asked about
: > it.  I see the answer before question phenomenon on this list quite
: strongly
: > and for me it is often Spencer Graves' answer I see first - like this one
: :)
: > Bob
: >
: >
: >
: >
: > -----Original Message-----
: > From: Spencer Graves [mailto:spencer.graves <at> pdf.com]
: > Sent: Wednesday, August 04, 2004 12:13 PM
: > To: Jim Brennan
: > Cc: r-help <at> stat.math.ethz.ch
: > Subject: Re: [R] Order of messgaes/ missing messages
: >
: >
: >       Might this depend on whether someone responds to all or only to
: > the list?  If one responds to all, then the originator should receive 2
: > copies, If s/he subscribes to the list.  The copy via the list is often
: > delayed hours by the listserve.  Might this explain the issue?  spencer
: > graves
: >
: > Jim Brennan wrote:
: >
: > >I am curious as to why I quite often receive responses to questions on
: > >the help before receiving the actual question.
: > > For example Mr Graves has responded to Mr Lumley's response to Mr
: Wegelin
: > yet I  ave not received Mr. Lumley's post while I have received the other
: > two.
: > >
: > >Cheers,
: > >
: > >Jim
: > > [[alternative HTML version deleted]]



From huz88 at hotmail.com  Thu Aug  5 02:15:03 2004
From: huz88 at hotmail.com (Zach Hu)
Date: Wed, 4 Aug 2004 20:15:03 -0400
Subject: [R] Drawing multiple plots on one page for publication.
Message-ID: <BAY12-DAV5RFWMozgYL00002bd5@hotmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040804/8c48dddc/attachment.pl

From alexpegucci at yahoo.com  Thu Aug  5 03:52:12 2004
From: alexpegucci at yahoo.com (alex pegucci)
Date: Wed, 4 Aug 2004 18:52:12 -0700 (PDT)
Subject: [R] dispersion for the inverse.gaussian family
Message-ID: <20040805015212.36966.qmail@web14427.mail.yahoo.com>

Dear all,

I am trying to get a more accurate estimate of the dispersion parameter
of the inverse.gaussian family in a glm model. The one provided by the
summary.glm looks like only a rough estimate, when you calculate the
individual likelihoods and sum them using the dispersion reported by
summary.glm, it can get quite different from the reported
log-likelihood value. As discussed in a previous thread, for the Gamma
family, there is gamma.shape function that does this more accurately.
Is there a counterpart function that calculates ML estimator of the
dispersion for the inverse.gaussian family? 

Thanks,

Alex Pegucci



From andrewr at uidaho.edu  Thu Aug  5 04:03:40 2004
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Thu, 05 Aug 2004 12:03:40 +1000
Subject: [R] Drawing multiple plots on one page for publication.
Message-ID: <15518315142f.15142f155183@uidaho.edu>

Zach,

try:


pdf("Name.pdf")
par(mfrow=c(2,2))
par(mai=c(0.5,0.3, 0.6, 0.4))

plot(x, y, ..........)
plot(x, z ...........)
plot(z, y,...........)
plot(z+x, x-y, .....)
dev.off()


Andrew



From h.wickham at gmail.com  Thu Aug  5 04:06:48 2004
From: h.wickham at gmail.com (hadley wickham)
Date: Thu, 5 Aug 2004 14:06:48 +1200
Subject: [R] Order of messgaes/ missing messages
In-Reply-To: <loom.20040805T021649-752@post.gmane.org>
References: <6BCD3F430455B1418750004BCD279259029262DF@exchange2.ahrq.gov>
	<005001c47a80$06c087a0$3b8ac445@slnt.phub.net.cable.rogers.com>
	<loom.20040805T021649-752@post.gmane.org>
Message-ID: <f8e6ff0504080419061b14822a@mail.gmail.com>

> Note that if this bothers you then you could try a web-based
> email systems (e.g. yahoo, hotmail, etc.) to receive your r-help
> messages since the path to any of them would be independent of your
> particular location on the net.

I use gmail and still often recieve replies before the original
message.  Why should using webmail make a difference?  Surely it's the
r mailing list server that does all the sending (apart from specific
individual replies)?  And the listserve must recieve the reply after
the original, so it seems to me that it must be the time it takes to
get between the server and you that is varying.  Or is the listserve
sitting on some emails before sending them?  One test would be to see
if we recieve emails in the same order.

Hadley



From maj at stats.waikato.ac.nz  Thu Aug  5 04:25:35 2004
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Thu, 05 Aug 2004 14:25:35 +1200
Subject: [R] Order of messgaes/ missing messages
In-Reply-To: <f8e6ff0504080419061b14822a@mail.gmail.com>
References: <6BCD3F430455B1418750004BCD279259029262DF@exchange2.ahrq.gov>	<005001c47a80$06c087a0$3b8ac445@slnt.phub.net.cable.rogers.com>	<loom.20040805T021649-752@post.gmane.org>
	<f8e6ff0504080419061b14822a@mail.gmail.com>
Message-ID: <41119A9F.6060606@stats.waikato.ac.nz>

The TCP protocol sends a message as a stream of packets. When packets 
are lost over a particular link in the path from the sending machine at 
ETHZ to a recipient parts of the message will be re-transmitted. A 
message sent after another message can arrive before it if it 
experiences less packet loss on its path through the network. It's 
statistical, you see!

Murray Jorgensen

hadley wickham wrote:

>>Note that if this bothers you then you could try a web-based
>>email systems (e.g. yahoo, hotmail, etc.) to receive your r-help
>>messages since the path to any of them would be independent of your
>>particular location on the net.
> 
> 
> I use gmail and still often recieve replies before the original
> message.  Why should using webmail make a difference?  Surely it's the
> r mailing list server that does all the sending (apart from specific
> individual replies)?  And the listserve must recieve the reply after
> the original, so it seems to me that it must be the time it takes to
> get between the server and you that is varying.  Or is the listserve
> sitting on some emails before sending them?  One test would be to see
> if we recieve emails in the same order.
> 
> Hadley
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862



From ggrothendieck at myway.com  Thu Aug  5 04:59:23 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 5 Aug 2004 02:59:23 +0000 (UTC)
Subject: [R] Order of messgaes/ missing messages
References: <6BCD3F430455B1418750004BCD279259029262DF@exchange2.ahrq.gov>
	<005001c47a80$06c087a0$3b8ac445@slnt.phub.net.cable.rogers.com>
	<loom.20040805T021649-752@post.gmane.org>
	<f8e6ff0504080419061b14822a@mail.gmail.com>
Message-ID: <loom.20040805T045011-376@post.gmane.org>

hadley wickham <h.wickham <at> gmail.com> writes:

> 
> > Note that if this bothers you then you could try a web-based
> > email systems (e.g. yahoo, hotmail, etc.) to receive your r-help
> > messages since the path to any of them would be independent of your
> > particular location on the net.
> 
> I use gmail and still often recieve replies before the original
> message.  Why should using webmail make a difference?  

Because it depends on where the receiver of the message is on the net
and your receiver with ordinary mail is your ISP but the receiver with
webmail providers is the webmail provider's ISP and they are normally 
different.   (Note that I am not referring to sites like mail2web.com
which just interface to your ISP but to ones that actually have
their own distinct pop server through which you get your web based
mail.)

If one web mail doesn't work well try another.   Another thing you 
can do is to check gmane at
   http://news.gmane.org/gmane.comp.lang.r.general
or one of the r-help archive sites if you get a reply before the
message its replying to.  The original (and others) may be there
even if you have not received them.



From jfbrennan at rogers.com  Thu Aug  5 05:09:48 2004
From: jfbrennan at rogers.com (Jim Brennan)
Date: Wed, 4 Aug 2004 23:09:48 -0400
Subject: [R] Order of messgaes/ missing messages
References: <6BCD3F430455B1418750004BCD279259029262DF@exchange2.ahrq.gov>	<005001c47a80$06c087a0$3b8ac445@slnt.phub.net.cable.rogers.com>	<loom.20040805T021649-752@post.gmane.org><f8e6ff0504080419061b14822a@mail.gmail.com>
	<41119A9F.6060606@stats.waikato.ac.nz>
Message-ID: <00c401c47a99$ab4dbcc0$3b8ac445@slnt.phub.net.cable.rogers.com>

Ah that is the answer I was looking for! Thanks.
I can see that a webmail site would/might also experience the same
phenomenon. I was just curious-- not that it bothered me.

Cheers

Jim

----- Original Message -----
From: "Murray Jorgensen" <maj at stats.waikato.ac.nz>
To: "hadley wickham" <h.wickham at gmail.com>
Cc: <r-help at stat.math.ethz.ch>
Sent: Wednesday, August 04, 2004 10:25 PM
Subject: Re: [R] Order of messgaes/ missing messages


> The TCP protocol sends a message as a stream of packets. When packets
> are lost over a particular link in the path from the sending machine at
> ETHZ to a recipient parts of the message will be re-transmitted. A
> message sent after another message can arrive before it if it
> experiences less packet loss on its path through the network. It's
> statistical, you see!
>
> Murray Jorgensen
>
> hadley wickham wrote:
>
> >>Note that if this bothers you then you could try a web-based
> >>email systems (e.g. yahoo, hotmail, etc.) to receive your r-help
> >>messages since the path to any of them would be independent of your
> >>particular location on the net.
> >
> >
> > I use gmail and still often recieve replies before the original
> > message.  Why should using webmail make a difference?  Surely it's the
> > r mailing list server that does all the sending (apart from specific
> > individual replies)?  And the listserve must recieve the reply after
> > the original, so it seems to me that it must be the time it takes to
> > get between the server and you that is varying.  Or is the listserve
> > sitting on some emails before sending them?  One test would be to see
> > if we recieve emails in the same order.
> >
> > Hadley
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
> >
> >
>
> --
> Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
> Department of Statistics, University of Waikato, Hamilton, New Zealand
> Email: maj at waikato.ac.nz                                Fax 7 838 4155
> Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Thu Aug  5 05:33:50 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 5 Aug 2004 03:33:50 +0000 (UTC)
Subject: [R] Order of messgaes/ missing messages
References: <6BCD3F430455B1418750004BCD279259029262DF@exchange2.ahrq.gov>	<005001c47a80$06c087a0$3b8ac445@slnt.phub.net.cable.rogers.com>	<loom.20040805T021649-752@post.gmane.org><f8e6ff0504080419061b14822a@mail.gmail.com>
	<41119A9F.6060606@stats.waikato.ac.nz>
	<00c401c47a99$ab4dbcc0$3b8ac445@slnt.phub.net.cable.rogers.com>
Message-ID: <loom.20040805T052850-425@post.gmane.org>

Jim Brennan <jfbrennan <at> rogers.com> writes:

> Ah that is the answer I was looking for! Thanks.
> I can see that a webmail site would/might also experience the same
> phenomenon. I was just curious-- not that it bothered me.

It depends on what you mean by "same".  Any site can experience the
phenomenon but sites at different parts of the web will experience it
differently which is why receiving your email through a different 
email server (which is what using a web email provider would do) can,
in practice, largely solve your problem if you find one located 
on a good path from the sender.



From rab at nauticom.net  Thu Aug  5 06:46:02 2004
From: rab at nauticom.net (rab)
Date: Thu, 05 Aug 2004 00:46:02 -0400
Subject: [R] Strange Problem with "proj" and "aov" for split-plot	analysis
	output
In-Reply-To: <1091383873.12331.16.camel@snowdon.science.uva.nl>
References: <410D16EF.2020005@nauticom.net>
	<1091383873.12331.16.camel@snowdon.science.uva.nl>
Message-ID: <4111BB8A.5030300@nauticom.net>

Ulrich Leopold wrote:

>># split-plot analysis - using "aov" with argument "data"
>> > summary(aov.split.04 <- 
>>aov(ctime~brand*type+Error(subject),data=choco.split.04))
>>    
>>
>
>As far as I can see, maybe you should not use the redirect command in
>this case and skip the 'data' argument then it should work:
>
>summary(aov(ctime~brand*type+Error(subject)))
>
>else the results from 'aov(..)' are just redirected to aov.split.04 but
>not passed to standard output for 'summary(...)'. And it probably does
>not have a data argument?
>
>Maybe someone else could explain why?
>
>Ulrich
>
>
>  
>
I don't believe the redirect has anything to do with it. I get the same 
result even without the redirect.

Rick B.



From ripley at stats.ox.ac.uk  Thu Aug  5 07:28:08 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 5 Aug 2004 06:28:08 +0100 (BST)
Subject: [R] Drawing multiple plots on one page for publication.
In-Reply-To: <BAY12-DAV5RFWMozgYL00002bd5@hotmail.com>
Message-ID: <Pine.LNX.4.44.0408050625010.15275-100000@gannet.stats>

On Wed, 4 Aug 2004, Zach Hu wrote:

> I have been using R for a while but only recently tried to draw quality
> plots for publications.  I spent a few days tried to figure out how to
> draw multiple plots on a single page to a postscript or pdf file but not
> successful.  The drawing is perfect OK if it is displayed on the screen
> using x11() device.  E.g.
> 
> x11()
> par(mfrow=c(2,2))
> par(mai=c(0.5,0.3, 0.6, 0.4))
> 
> plot(x, y, ..........)
> plot(x, z ...........)
> plot(z, y,...........)
> plot(z+x, x-y, .....)
> 
> Four plots displayed on the screen when X-Win32 was set up.  Please
> advice how can I save this to a postscript or PDF file or any kind of
> file for publication and powerpoint slide?

Just replace `x11()' by `postscript()' or `pdf()': you may want to set the 
width, height and pointsize suitably.

[You can use dev.copy2eps to copy the screen, but for publication-quality 
plots should should replot on the desired end device to use the correct 
font metrics.]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Thu Aug  5 09:15:58 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 5 Aug 2004 09:15:58 +0200
Subject: [R] fitting distributions
In-Reply-To: <0A2559C22AB50C4C8ED88EC58D60876A02431A1C@saint_ex.eim.ch>
References: <0A2559C22AB50C4C8ED88EC58D60876A02431A1C@saint_ex.eim.ch>
Message-ID: <16657.57006.211201.106217@gargle.gargle.HOWL>

>>>>> "Fabrice" == Fabrice Dusonchet <fdusonchet at eim.ch>
>>>>>     on Wed, 4 Aug 2004 18:55:18 +0200 writes:

    Fabrice> I also try to fit a skewed distribution (like
    Fabrice> skewed student t) to data points. Do you have an
    Fabrice> idee howto do this???

One recommended approach would be to use the
mle() functionality in the standard package 'stats4' (was
package 'mle' in 1.8.x). 

It uses optim() but provides several extra niceties.

Bonnes salutations de Zurich!
Martin Maechler, 
Seminar f??r Statistik, ETH Zurich



From vito_ricci at yahoo.com  Thu Aug  5 09:19:58 2004
From: vito_ricci at yahoo.com (=?iso-8859-1?q?Vito=20Ricci?=)
Date: Thu, 5 Aug 2004 09:19:58 +0200 (CEST)
Subject: [R] spatial econometric model for a simultaneous system 
Message-ID: <20040805071958.47658.qmail@web41205.mail.yahoo.com>

Hi,

there are some R packages for spatial econometric
model.
Look in Packages:
(http://microarrays.unife.it/CRAN/src/contrib/PACKAGES.html)

for:

geoR - functions for geostatistical data analysis
geoRglm - a package for generalised linear spatial
models

grasper- Generalized Regression Analysis and Spatial
Predictions for R

GRASS Interface between GRASS 5.0 geographical
information system and R

sgeostat-An Object-oriented Framework for
Geostatistical Modeling in S+

spatialCovariance Computation of Spatial Covariance
Matrices for Data on Rectangles Using One Dimensional
Numerical Integration and Analytic Results

spatstat Spatial Point Pattern analysis, model-fitting
and simulation

spdep	Spatial dependence: weighting schemes,
statistics and models

splancs	Spatial and Space-Time Point Pattern Analysis

See also:

http://www.mayin.org/ajayshah/KB/R/R_for_economists.html

I hope be helpful.
Best
Vito


Hi,

Is there any package or functions in R that can
perform 
tests and estimation for a spatial econometric model
in a 
simultaneous system?

Thank you!!!

Xia Feng

=====
Diventare costruttori di soluzioni

Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese http://www.modugno.it/archivio/cat_palese.shtml



From jacques.veslot at cirad.fr  Thu Aug  5 09:30:42 2004
From: jacques.veslot at cirad.fr (Jacques VESLOT)
Date: Thu, 5 Aug 2004 11:30:42 +0400
Subject: [R] Cross-variograms
In-Reply-To: <Pine.LNX.4.44.0408050625010.15275-100000@gannet.stats>
Message-ID: <HHEDKBCGCMDOHEDELFBCAEKFCBAA.jacques.veslot@cirad.fr>

Dear All,

In order to appreciate the spatial link between to variables: X (numeric)
and Y (factor), measured on a grid at the same locations (i.e. a
geodata-like object with a two-column dataframe as $data), I want to use a
so-called cross-variogram based on such a formula:

h(r)= E[(X(0)-X(h))*(Y(0)-Y(h))] = D(0) - D(h) - D(-h)  where D(h) is the
covariance between X(0) and Y(h),

and then draw an 95-envelop estimated by sampling one dataset (Y) by
translation.

GeoR and geoRglm doesn't include cross-variograms, thus I tried with
variogram() in gstat. But, despite help pages and gstat website, I
encountered difficulties in understanding how to deal with this function,
and notably with the formula argument.

I tried to create the gstat object I need and get the related
cross-variogram (cv), just as follows:

g <- gstat(id="X", formula=X~1, locations=~x+y, data = mat)		## mat is a
4-column df with variables X and Y
g <- gstat(g, id="Y", formula=Y~1, locations=~x+y, data = mat)	## and
(x,y)-coords
v <- variogram(g, cutoff=15)
cv <- v$gamma[1:14]

But I am definitely not sure of the way to deal with formula argument, since
I didn't understand its role here, and how to create a gstat object with two
variables.

Could you please let me know if this is the right way to operate (and what
formula argument means here) ?


Best regards,

Jacques VESLOT
CIRAD



From maechler at stat.math.ethz.ch  Thu Aug  5 10:09:52 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 5 Aug 2004 10:09:52 +0200
Subject: [R] Re: new sep argument for interaction
In-Reply-To: <loom.20040805T021221-289@post.gmane.org>
References: <88EAF3512A55DF46B06B1954AEF73F74047BFD47@dc1ex2.air.org>
	<loom.20040804T224450-915@post.gmane.org>
	<200408041732.02461.deepayan@stat.wisc.edu>
	<loom.20040805T021221-289@post.gmane.org>
Message-ID: <16657.60240.293596.263809@gargle.gargle.HOWL>

>>>>> "Gabor" == Gabor Grothendieck <ggrothendieck at myway.com>
>>>>>     on Thu, 5 Aug 2004 00:15:25 +0000 (UTC) writes:

    Gabor> Deepayan Sarkar <deepayan <at> stat.wisc.edu> writes:
    Gabor> : 
    Gabor> : On Wednesday 04 August 2004 15:50, Gabor Grothendieck wrote:
    Gabor> : 
    Gabor> : > Several people have already mentioned paste, which returns a
    Gabor> : > character result, and two other solutions, depending on what you are
    Gabor> : > looking for, are:
    Gabor> : >
    Gabor> : >    with(tenn, 100 * up + 10 * down + stable)  # numeric result
    Gabor> : >
    Gabor> : >    with(tenn, interaction(up, down, stable, sep =""))  # factor
    Gabor> : > result
    Gabor> : 
    Gabor> : Except that interaction doesn't have a sep= argument. 

    Gabor> The interaction in Hmisc has a sep= argument.  (I did
    Gabor> ?interaction before posting but forgot that I had
    Gabor> Hmisc loaded and did not notice that I was looking at
    Gabor> the help for Hmisc interaction rather than the base
    Gabor> interaction.)

    Gabor> Thus, just to be definite, the above should have been:

    Gabor> require(Hmisc)
    Gabor> with(tenn, interaction(up, down, stable, sep = ""))

I apologize to Frank, but the fact that 'Hmisc' redefines
standard R functions {the "[.factor" redefinition being much
more problematic, and yes, "library(Hmisc)" now warns about it}
is one reason I'm sometimes recommending not to use it at all
even though I know about its many very nice pearls inside!
If there only was an 'Hmisc' version without any of these
redefinitions -- it would be one of my most recommended packages
(apart from the officially "Recommended" ones) !

Yes, I agree with Deepayan's suggestion that R's interaction()
function should grow a "sep" argument --- and I am adding it
*now*.

Martin Maechler



From jpgranadeiro at fc.ul.pt  Thu Aug  5 10:49:52 2004
From: jpgranadeiro at fc.ul.pt (J. Pedro Granadeiro)
Date: Thu, 5 Aug 2004 09:49:52 +0100
Subject: [R] Legend with different sized symbols
Message-ID: <200408050949.52852.jpgranadeiro@fc.ul.pt>

Dear list,

I wonder if it is possible to produce a legend with symbols of different sizes 
using a single legend command. I managed to do so more or less like in this 
crude example, but there is probably a smarter and more practical way:

set.seed(0)
plot(rnorm(100), rnorm(100), cex=rep(1:5,each=20))
x<-legend(-2,2.8, legend=1:5, pch="  ",y.intersp=2, bty="n")
points(x$text$x-.2,x$text$y, cex=1:5)

I tried with playing with cex inside the legend command, but this gave me 
funny results, since it manipulates the overall size of symbols plus text. I 
was looking for something like "pt.cex", but this probably it does not 
exist...

Thanks

Jose Pedro Granadeiro



From hagelstedt at iogt.se  Thu Aug  5 10:57:48 2004
From: hagelstedt at iogt.se (=?iso-8859-1?Q?=C5sa_Hagelstedt?=)
Date: Thu, 5 Aug 2004 10:57:48 +0200
Subject: [R] =?iso-8859-1?q?Fr=E5nvaro=2C_autosvar=3A_Message_could_not_b?=
	=?iso-8859-1?q?e_delivered?=
Message-ID: <F6B5B3E521D4DE429798AF066B102EAF013364A9@sesob03.sobernet.net>

Hej!

Jag har semester och ??r tillbaks p?? kontoret 9 augusti. L??ser mejlen sporadiskt p?? regniga dagar. Beh??ver du f?? tag p?? mig kan du ringa p?? mobilen, 0733-72 62 22. 

Hi!

I'm on hollidays until the 9th of August and will not read my e-mail every day. Call my mobile if it's urgent, +46 733 72 62 22

/??sa



From Luisr at frs.fo  Thu Aug  5 11:14:12 2004
From: Luisr at frs.fo (Luis Rideau Cruz)
Date: Thu, 05 Aug 2004 10:14:12 +0100
Subject: [R] RODBC with MS Microsoft Access
Message-ID: <s112087e.031@ffdata.setur.fo>



Luis Ridao Cruz
Fiskiranns??knarstovan
N??at??n 1
P.O. Box 3051
FR-110 T??rshavn
Faroe Islands
Phone:             +298 353900
Phone(direct): +298 353912
Mobile:             +298 580800
Fax:                 +298 353901
E-mail:              luisr at frs.fo
Web:                www.frs.fo

>>> "Luis Rideau Cruz" <Luisr at frs.fo> 04/08/2004 15:05:53 >>>
Hi all,

I tried to retrieve a table from a desktop database and got an error.
The thing is that when I changed the table name it worked out.

The former names was "length-spring"
The new was test

Is "-" character not valid for tables?

Thank you

Luis Ridao Cruz
Fiskiranns??knarstovan
N??at??n 1
P.O. Box 3051
FR-110 T??rshavn
Faroe Islands
Phone:             +298 353900
Phone(direct): +298 353912
Mobile:             +298 580800
Fax:                 +298 353901
E-mail:              luisr at frs.fo 
Web:                www.frs.fo 

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From MatUnt at gmx.de  Thu Aug  5 11:33:08 2004
From: MatUnt at gmx.de (Matthias Unterhuber)
Date: Thu, 5 Aug 2004 11:33:08 +0200 (MEST)
Subject: [R] How to enter partial hierarchical model in lme()
Message-ID: <28348.1091698388@www28.gmx.net>

I would need some help regarding a partial hierarchical model for anova.
Can anyone help me check whether the model is correct and what I should
enter in lme()?

My research design is (demonstrated for 4 subjects/Blocks):

      |     d1    |     d2    |
      |  a1 | a2  |  a1 | a2  |
      |b1|b2|b1|b2|b1|b2|b1|b2|   
Block1| x| x|  |  |  |  |  |  |
Block2|  |  | x| x|  |  |  |  |
Block3|  |  |  |  | x| x|  |  |
Block4|  |  |  |  |  |  | x| x|

Altogether there 92 subjects (about equal number for each condition)

A: random effect
B: random effect
Block: random effect
D: fixed effect


My analyis:

A and D are nested in Block, but A is not nested in D, and D is not nested
in A.

The model I propose is:


X~mu+A%in%Block+B+Block+D%in%Block+(A%in%Block):B+(A%in%Block):(D%in%Block)+B:Block
+(D%in%Block):B+(A%in%Block):(D%in%Block):B

My questions are: 

1. Is the model correct?
2. How do I type in the model information for lme?

Thanks in advance,

Matthias






-- 
NEU: WLAN-Router fr 0,- EUR* - auch fr DSL-Wechsler!
GMX DSL = supergnstig & kabellos http://www.gmx.net/de/go/dsl



From ggrothendieck at myway.com  Thu Aug  5 13:40:53 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 5 Aug 2004 11:40:53 +0000 (UTC)
Subject: [R] redefining core functionality (was: new sep argument for
	interaction)
References: <88EAF3512A55DF46B06B1954AEF73F74047BFD47@dc1ex2.air.org>
	<loom.20040804T224450-915@post.gmane.org>
	<200408041732.02461.deepayan@stat.wisc.edu>
	<loom.20040805T021221-289@post.gmane.org>
	<16657.60240.293596.263809@gargle.gargle.HOWL>
Message-ID: <loom.20040805T130011-799@post.gmane.org>

Martin Maechler <maechler <at> stat.math.ethz.ch> writes:

> I apologize to Frank, but the fact that 'Hmisc' redefines
> standard R functions {the "[.factor" redefinition being much

I was thinking about this too, although not initially in relation to 
Hmisc, as I recently posted a function called rbind.case, see

https://www.stat.math.ethz.ch/pipermail/r-help/2004-August/053993.html

in which I redefined cbind.   In that case the redefinition was
within another function so its scope was limited to the internals
of that function.  Also the redefinition is used so its use was
close to its definition, in fact in the very next line, so it seemed 
harmless and it does convey the idea of what the function is supposed 
to do. In that case, I guess I could have just as easily called it cbind2, 
which would have conveyed the idea that it's a cbind-like function 
yet eliminate any confusion at all.

In the case of a function exported from a package, the situation is
potentially less benign since it couples all code that uses that 
function to the package even if the invoking code has nothing to do 
with that package.

I see several problems and benefits with redefinitions.  First
the problems:

- If the redefinition _is_ backwardly compatible (just adds a parameter 
like interaction does) then its normally not a problem; however, if 
there were a bug in the redefinition then that bug potentially propogates 
to all functions in your workspace that use the function being redefined, 
even if they have nothing to do with the package in question.

- If the redefinition is _not_ backwardly compatible it may break code
not related to the package just by loading the package, even if such
code is otherwise independent of the package.

On the other head, I can see why one would want to do it:

- we often see exhortations on the list to users to develop their own code
rather than rely on the core group and if you want R to work in the way 
you like what better way then to redefine it in a package so that it 
does.  If, in particular, if you are hoping that such functionality 
ultimately gets included in the core then it seems like a natural 
progression to use the same name so that if/when it does happen you can 
continue to use the exact same code. (Returning to Hmisc, for the
moment, that is precisely the case with interaction where Frank
put in sep=, as well as left= by the way, and now at least sep=
is migrating the core.  Perhaps left= should be added too so that
interaction can be removed from Hmisc.   Also perhaps other
functionality in Hmisc that do redefinitions could be identified
and considered for possible movement to the core and, of course,
this potentially applies to other packages, too.)

- Also, from a practical viewpoint, using R's S3 class structure one 
cannot define factor2, say, to be a subclass of factor inheriting all its 
methods except for those overridden.  Thus if you want to change the
behavior of factors or other S3 classes you either have to provide 
interfaces to every factor method for your new factor2 class or you 
do the more expedient thing of just redefining the one method in factor 
that you want redefined.



From wolfram at fischer-zim.ch  Thu Aug  5 11:03:40 2004
From: wolfram at fischer-zim.ch (Wolfram Fischer)
Date: Thu, 5 Aug 2004 11:03:40 +0200
Subject: [R] import a bitmap image and add it to graphics display
In-Reply-To: <20021014221015.A1244@s1x.zimnet.ch>
References: <20021014221015.A1244@s1x.zimnet.ch>
Message-ID: <20040805090340.GA3526@s1x.local>

> Hello
> 
> Is there a possibility to import and add a bitmap image (png or 
> similar) to a R graphics display. It would be helpful e.g. to
> locate positions of points of a scanned map or to add a background
> to a R graphic.
> 
> Wolfram

I found the package pixmap with the functions
	x <- read.pnm( file )
and
	plot(x)
This plots pnm pictures as R graphics.
(Other picture formats can be converted to pnm pictures
by several pnm commands on Linux or by using gimp.)

Overlaying such a picture with a graph could be done as follows:
    library(pixmap)
    example(pixmap)
    plot(x)
    m <- 20 ; points( x=m*c(0,1,2), y=m*c(0,2,1), col='red', type='b', lwd=5 )

There rests a problem with the scale:
Question: How can I scale a pixmap object?

Wolfram



From kahra at mpsgr.it  Thu Aug  5 15:20:49 2004
From: kahra at mpsgr.it (Kahra Hannu)
Date: Thu, 5 Aug 2004 15:20:49 +0200
Subject: [R] fitting distributions
Message-ID: <C9FC71F7E9356F40AFE2ACC2099DE1471370E9@MAILSERVER-B.mpsgr.it>


>I also try to fit a skewed distribution (like skewed student t) to data
>points. Do you have an idee howto do this???

library(skewt)

y <- rskt(500,2,2) # simulate 500 observations from the skew t distribution with df=2 and gamma=2

skewtmle <- function(df,gamma){
return(-sum(log(dskt(y, df, gamma))))
}

fit <- mle(skewtmle,start=list(df=1,gamma=3),method="L-BFGS-B",lower=c(1e-8,-Inf))
summary(fit)
logLik(fit)
vcov(fit)
plot(profile(fit), absVal=F)
confint(fit)

Hannu Kahra 
Progetti Speciali 
Monte Paschi Asset Management SGR S.p.A. 
Via San Vittore, 37 - 20123 Milano, Italia



From Luisr at frs.fo  Thu Aug  5 15:24:57 2004
From: Luisr at frs.fo (Luis Rideau Cruz)
Date: Thu, 05 Aug 2004 14:24:57 +0100
Subject: [R] function output
Message-ID: <s1124341.067@ffdata.setur.fo>

I have the next function :

"extracting.lgddata"<-function(channel)
{
library(RODBC)
channel<-odbcDriverConnect("")
.......
.......
}

...... is just a sequence of SQL selects.
But when I try to call the objects created from the select they do not seem to have been created.
I checked with ls() and they actually do not exist.
What is the problem?

Thank you


Luis Ridao Cruz
Fiskiranns??knarstovan
N??at??n 1
P.O. Box 3051
FR-110 T??rshavn
Faroe Islands
Phone:             +298 353900
Phone(direct): +298 353912
Mobile:             +298 580800
Fax:                 +298 353901
E-mail:              luisr at frs.fo
Web:                www.frs.fo



From bwittner at jimmy.harvard.edu  Thu Aug  5 18:38:01 2004
From: bwittner at jimmy.harvard.edu (Ben Wittner)
Date: Thu, 5 Aug 2004 12:38:01 -0400 (EDT)
Subject: [R] or of a logical vector
Message-ID: <Pine.SOL.4.20.0408051228300.15412-100000@noah.dfci.harvard.edu>

Is there some fast (built-in?) way to get the OR of all the elements in a
logical vector?

In other words, is there some fast (built-in) version of the function vor
below?

Thanks.

-Ben

vor <- function(v) {
  ans <- v[1]
  if (length(v) > 1)
    for (i in 2:length(v))
      ans <- ans | v[i]
  ans
}



From murdoch at stats.uwo.ca  Thu Aug  5 17:41:17 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 05 Aug 2004 11:41:17 -0400
Subject: [R] tcltk:  repeat event while button is down?
Message-ID: <36l4h05guji9v9e551ul4bakng8o537ncg@4ax.com>

Is there a way in TCL/TK to trigger an event multiple times while a
button is held down?  I'd like to have an rgl scene continuously
rotate until the button is released.

Duncan Murdoch



From RAJDL at fnplzen.cz  Thu Aug  5 17:29:09 2004
From: RAJDL at fnplzen.cz (Rajdl Daniel)
Date: Thu, 5 Aug 2004 17:29:09 +0200
Subject: [R] ANOVA with repeated measurements (Dan Rajdl)
Message-ID: <DB879673B6E2E84C8517AB9A25C3706F394DCE@erika.fnplzen.cz>

Dear R-users,
I have some human serum lipid concentrations (cholesterol, apoB ...),
each lipid was measured (in the same person) for 6 times in different
time points (start, 3 months, 6 months, 12 months, 18 months, 24
months). There were 2 groups of participants: one with a nutritional
intervention and the other without it. I would like to know, whether
lipid concentrations differ among different time points and between the
2 groups.
Could anyone advise me, please, what method and how should I usein R? Or
redirect me, please, to a source, where I can find the answer.
Thank you,
Dan Rajdl
Dpt.Clin.Chem.Biochem.Hematol, Univ.Hosp.in Pilsen, Czech Republic



From gpagnon at emory.edu  Thu Aug  5 16:51:22 2004
From: gpagnon at emory.edu (Giuseppe Pagnoni)
Date: Thu, 05 Aug 2004 10:51:22 -0400
Subject: [R] Post-hoc t-tests in 2-way repeated measure ANOVA
Message-ID: <4112496A.4070606@emory.edu>

Hi all

I am running a 2-way repeated measure anova with 1 between-subjects 
factor (Group=treatment, control), and 1 within-subject factor (Time of 
measurement: time1, time2).  I extract the results of the anova with:

summary(aov(effect ~ Group*Time + Error=Subj/Time, data=mydata))

Now, this must be clearly a dumb question, but how can I quickly extract 
in R all the post-hoc t-tests for the simple main effects?

Also, while I am at it, how do I enter in the model a counfounding 
covariate (e.g., Age)?

And on a different matter, is there a way to receive interactive user 
input in an R script? Something like "Enter the name of  the factor:  ", 
or even more simply "Press <Enter> to see the result of the next 
analysis"....

thanks in advance for any suggestions!

    giuseppe

-- 
------------------------
Giuseppe Pagnoni, Ph.D.
Dept. of Psychiatry and Behavioral Sciences
1639 Pierce Drive, Suite 4000
WMB Bldg., Atlanta, GA 30322, U.S.
phone: 404-712-8431
fax: 404-727-3233
e-mail: gpagnon at emory.edu



From rolf at math.unb.ca  Thu Aug  5 16:13:49 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Thu, 5 Aug 2004 11:13:49 -0300 (ADT)
Subject: [R] Local library on Windoze.
Message-ID: <200408051413.i75EDncG005990@erdos.math.unb.ca>

I'm mystified by a Windoze ``phenomenon'' which has just bitten me.

I have a laptop which boots either Windoze or Linux; I (sad to say)
need the Windoze OS for teaching purposes.  I have R 1.9.1 installed
on the laptop in the Windoze OS.

Just now I decided to install,  under the Windoze version of R,
a library (``Misc'') of some local functions that I have collected
over the years.  I just zipped up an installed Unix version of Misc
(there is no compiled C or Fortran to make life complicated), ftp-ed
the zip file to the Windoze-running laptop, started R, installed the
package ....  everything went like a dream.

***THEN*** I noticed a typo in one of the help files from Misc.
So I decided to fix it.

First try:  Went to C:\Program Files\R\rw1091\library\Misc\man
and edited the file Misc.Rd, eliminating the typo.   Restarted
R, loaded Misc, looked at the help --- typo is still there.

Second try:  Fixed the typo in the Unix version, zipped it up again,
ftp-ed it to the Windoze-running laptop, re-installed it.  Restarted
R, loaded Misc, looked at the help --- typo is still there.

Third try:  Deleted the Misc directory from

	C:\Program Files\R\rw1091\library

started R, tried to load Misc, got told no such package --- fine,
it's gone.  Stopped R, restarted, re-installed Misc.  Checked on the
file

	C:\Program Files\R\rw1091\library\Misc\man\Misc.Rd

That file is fine; no typo in it.

Re-started R again, loaded Misc, looked at the help --- the *#%#$#
type is STILL there!!!

Why is there a ghost Misc library hanging around?  Where is it?
How do I get rid of it?

			cheers,

				Rolf Turner
				rolf at math.unb.ca



From ligges at statistik.uni-dortmund.de  Thu Aug  5 16:10:31 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 05 Aug 2004 16:10:31 +0200
Subject: [R] Legend with different sized symbols
In-Reply-To: <200408050949.52852.jpgranadeiro@fc.ul.pt>
References: <200408050949.52852.jpgranadeiro@fc.ul.pt>
Message-ID: <41123FD7.2090606@statistik.uni-dortmund.de>

J. Pedro Granadeiro wrote:

> Dear list,
> 
> I wonder if it is possible to produce a legend with symbols of different sizes 
> using a single legend command. I managed to do so more or less like in this 
> crude example, but there is probably a smarter and more practical way:
> 
> set.seed(0)
> plot(rnorm(100), rnorm(100), cex=rep(1:5,each=20))
> x<-legend(-2,2.8, legend=1:5, pch="  ",y.intersp=2, bty="n")
> points(x$text$x-.2,x$text$y, cex=1:5)
> 
> I tried with playing with cex inside the legend command, but this gave me 
> funny results, since it manipulates the overall size of symbols plus text. I 
> was looking for something like "pt.cex"

... and exactly that one exists in recent versions of R (e.g. R-1.9.1)!

Uwe Ligges


> , but this probably it does not exist...
> 
> Thanks
> 
> Jose Pedro Granadeiro
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From aleid2001 at yahoo.com  Thu Aug  5 15:56:40 2004
From: aleid2001 at yahoo.com (=?iso-8859-1?q?=20?=)
Date: Thu, 5 Aug 2004 14:56:40 +0100 (BST)
Subject: [R] cross random effects (more information abuot the data)
Message-ID: <20040805135640.32751.qmail@web52807.mail.yahoo.com>

Dear friends,

I have asked last few days about cross-random effects
using PQL, but I have not receive any answer because
might my question was not clear.

My question was about analysing the salamander mating
data using PQL. This data contain cross-random effects
for (male) and for (female). By opining MASS and lme
library. I wrote this code

sala.glmm <- glmmPQL(fixed=y~WSf*WSM,
random=list(experiment=pdBlocked(list(pdIdent(~randf-1),pdIdent(~randm-1)))),
family=binomial, data=sala.data).

Where
data neame=sala.glmm which contain
 y is response
 wsf is fixed effect
 wsm is fixed effects
 randf  is random effect
 random is random effect

The data contain three experiments at the same time.
The previous cod is work but it does not give me
accurate result especially for the random effects.

For experiment I wrote this code 

experiment <-
factor(c(rep(1,120),rep(2,120),rep(3,120)))
 because I have three experiments at the same time,
but if I change the experiment to e.g

experiment <- factor(c(rep(1,360)))

is still give answer but is not the right answer. So,
I am accusing my specification of the experiment
(group). If you have any suggestion pleas let me know.

   E-mail:aleid2001 at yahoo.com

Here I am going to gve mre details about the data.

the
> details about the data is:
>
>  
 The data are:
>  McCullagh and Nelder (1989,sec.14.5)polished an
> interesting set of data on the success of matting
> between male and female salamanders drawn from two
> populations, the rough butts (RB) and the white
sides
> (WS), that had been geographically isolated from
each
> other. In the first of three experiments, conducted
> during the summer of 1986, 10 RB females and 10 WS
> females were mated with three RB males and three WS
> males, for a total of six mating each over 24 days.
> Each of 10 RB males and 10 WS males likewise served
as
> mates for three females of each type. These same 40
> salamanders were used in a repeat experiment
conducted
> in the fall that involved no repetitions of the
> earlier mal-female pairs. A third experiment, also
> conducted in the fall, used a new set of 40 animals.
> Each experiment involved 30 mating of each of the
four
> gender-population combinations. Simple inspection of
> the data revealed that three of the crosses had
> success rates of about 70%, whereas the mating of WS
> females with RB MALES WAS SUCCESSFUL ONLY 25% OF the
> time. Evaluating the statistical significance of
these
> differences was complicated by the fact that the 360
> binary responses were not independent.
>
>  The model is used here is the mating probabilities
> are assumed to be the same for each of the three
> experiments. The random effects are assumed to be
> independent in each experiment. The male and
> Female effects are assumed to have different
variances
> but the variances are assumed
> To be the same across the three experiments.
>
>       Best Regared



From lecoutre at stat.ucl.ac.be  Thu Aug  5 20:19:05 2004
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Thu, 05 Aug 2004 20:19:05 +0200
Subject: [R] or of a logical vector
In-Reply-To: <Pine.SOL.4.20.0408051228300.15412-100000@noah.dfci.harvard .edu>
References: <Pine.SOL.4.20.0408051228300.15412-100000@noah.dfci.harvard.edu>
Message-ID: <6.0.1.1.2.20040805201739.01e980e0@stat4ux.stat.ucl.ac.be>


Hi Ben,

Always do consider that boolean vectors TRUE/FALSE are equivalent to 
integers 1/0.
What you want is to know wether one element of a vector is TRUE, which is:

 > sum(vec)>0

HTH,

Eric



At 18:38 5/08/2004, Ben Wittner wrote:
>Is there some fast (built-in?) way to get the OR of all the elements in a
>logical vector?
>
>In other words, is there some fast (built-in) version of the function vor
>below?
>
>Thanks.
>
>-Ben
>
>vor <- function(v) {
>   ans <- v[1]
>   if (length(v) > 1)
>     for (i in 2:length(v))
>       ans <- ans | v[i]
>   ans
>}
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Eric Lecoutre
UCL /  Institut de Statistique
Voie du Roman Pays, 20
1348 Louvain-la-Neuve
Belgium

tel: (+32)(0)10473050
lecoutre at stat.ucl.ac.be
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre

If the statistics are boring, then you've got the wrong numbers. -Edward 
Tufte



From spencer.graves at pdf.com  Thu Aug  5 20:19:31 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 05 Aug 2004 11:19:31 -0700
Subject: [R] or of a logical vector
In-Reply-To: <Pine.SOL.4.20.0408051228300.15412-100000@noah.dfci.harvard.edu>
References: <Pine.SOL.4.20.0408051228300.15412-100000@noah.dfci.harvard.edu>
Message-ID: <41127A33.2060108@pdf.com>

?any

hope this helps.  spencer graves

Ben Wittner wrote:

>Is there some fast (built-in?) way to get the OR of all the elements in a
>logical vector?
>
>In other words, is there some fast (built-in) version of the function vor
>below?
>
>Thanks.
>
>-Ben
>
>vor <- function(v) {
>  ans <- v[1]
>  if (length(v) > 1)
>    for (i in 2:length(v))
>      ans <- ans | v[i]
>  ans
>}
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From ggrothendieck at myway.com  Thu Aug  5 20:19:51 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 5 Aug 2004 18:19:51 +0000 (UTC)
Subject: [R] or of a logical vector
References: <Pine.SOL.4.20.0408051228300.15412-100000@noah.dfci.harvard.edu>
Message-ID: <loom.20040805T201652-546@post.gmane.org>

Ben Wittner <bwittner <at> jimmy.harvard.edu> writes:

: Is there some fast (built-in?) way to get the OR of all the elements in a
: logical vector?

Here are two possibilities:

max(x) == 1
sum(x) > 0

These use the fact that logicals used in arithmetic operations
are converted such that TRUE becomes 1 and FALSE becomes 0.



From andy_liaw at merck.com  Thu Aug  5 20:41:54 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 5 Aug 2004 14:41:54 -0400
Subject: [R] or of a logical vector
Message-ID: <3A822319EB35174CA3714066D590DCD504AF81A7@usrymx25.merck.com>

Is there anything wrong with sum(v) > 0?

Andy

> From: Ben Wittner
> 
> Is there some fast (built-in?) way to get the OR of all the 
> elements in a
> logical vector?
> 
> In other words, is there some fast (built-in) version of the 
> function vor
> below?
> 
> Thanks.
> 
> -Ben
> 
> vor <- function(v) {
>   ans <- v[1]
>   if (length(v) > 1)
>     for (i in 2:length(v))
>       ans <- ans | v[i]
>   ans
> }
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From jgentry at jimmy.harvard.edu  Thu Aug  5 20:48:09 2004
From: jgentry at jimmy.harvard.edu (Jeff Gentry)
Date: Thu, 5 Aug 2004 14:48:09 -0400 (EDT)
Subject: [R] Problem in method's Makefile?
Message-ID: <Pine.SOL.4.20.0408051425230.4703-100000@jaws.dfci.harvard.edu>

Hi there ...

Not too long after the switch to using Subversion I tried to checkout &
build R but encountered an error - and being short on time at that point
put it off to look at later.  So today I sat down again and was
encountering this error every time I would attempt to build:

 dumping R code in package 'methods'
 Saving namespace image ...
 initializing class and method definitions now ...done
 <environment: namespace:methods>
 usage: touch [-acfm] [-r file] [-t [[CC]YY]MMDDhhmm[.SS]] file ...
 *** Error code 1

I traced this down to the Makefile in methods:
$(top_builddir)/library/$(pkg)/R/$(pkg).rdb: $(top_builddir)/library/$(pkg)/R/all.rda
        @echo "tools:::makeLazyLoading(\"$(pkg)\")" | \
          R_DEFAULT_PACKAGES=NULL LC_COLLATE=C $(R_EXE) --slave >
/dev/null
        @cat /dev/null > $(top_builddir)/library/$(pkg)/R/all.rda
        @touch $^


I'm not sure what it is about that touch call, but commenting it out
causes things to build normally.  When I try to echo '$^', it appears to
be an empty string.

This is all on a FreeBSD machine - I'm assuming it is somehow OS specific
as this would have come up a bunch of times if it wasn't.  

-J



From hec.villafuerte at telgua.com.gt  Thu Aug  5 22:58:19 2004
From: hec.villafuerte at telgua.com.gt (Hector Villafuerte)
Date: Thu, 05 Aug 2004 12:58:19 -0800
Subject: [R] R interface to Python (in Windows)
In-Reply-To: <410EA2F8.CAB22E10@gene.com>
References: <Pine.SOL.4.58.0408021333570.7251@stat.psych.uiuc.edu>
	<410EA2F8.CAB22E10@gene.com>
Message-ID: <41129F6B.4090607@telgua.com.gt>

Hi,
I'm trying to install RPY (to interface Python and R).
This are the programs installed in my PC:
R - 1.9.0
Python - 2.3.4
pywin32 - 201
rpy - 0.3.5 (I'm using the executable version)

Everything installs nice, but when I try to use RPY a message box 
appears with this cryptic note:
"The procedure entry point malloc could not be located in the dynamic 
link library R.dll"

And then this shows in the interactive session:
 >>> import rpy
Traceback (most recent call last):
File "<input>", line 1, in ?
File "C:\Python23\Lib\site-packages\rpy.py", line 55, in ?
import _rpy
ImportError: DLL load failed: The specified procedure could not be found.
 >>>

Any ideas? Thanks in advance!
Hector



From gustavo at estatcamp.com.br  Thu Aug  5 21:02:50 2004
From: gustavo at estatcamp.com.br (Gustavo Pinheiro)
Date: Thu, 05 Aug 2004 16:02:50 -0300
Subject: [R] Windows build
Message-ID: <4112845A.8080905@estatcamp.com.br>

I've tried building R in Windows and had trouble compiling Bitmap.dll 
(make bitmapdll) and the docs (make docs). See make output below:

E:\R\R-1.9.1\src\gnuwin32>make bitmapdll
make -C bitmap
make[1]: Entering directory `/cygdrive/e/R/R-1.9.1/src/gnuwin32/bitmap'
make[3]: `libz.a' is up to date.
make[3]: scripts/makefile.std: No such file or directory
make[3]: *** No rule to make target `scripts/makefile.std'.  Stop.
make[2]: *** [buildpng] Error 2
make[1]: *** [all] Error 2
make[1]: Leaving directory `/cygdrive/e/R/R-1.9.1/src/gnuwin32/bitmap'
make: *** [bitmapdll] Error 2

I couldn't find 'scripts/makefile.std' anywhere. These are the files in 
src/gnuwin32/bitmap:

jpeg-6b (folder)
libpng  (folder)
INSTALL
jconfig.h
Makefile
Makefile.jpeg
rbitmap.c


For the 'make docs' part, what I got was lots of:

Underfull \hbox (badness 3407) in paragraph at lines 21211--21216
/aett10/a/aer10/. If /aett10/LINPACK /aer10/is /aett10/TRUE /aer10/the 
de-fault
 is /aett10/1e-7/aer10/, oth-er-wise it is
[351] [352] [353] [354] [355] [356] [357] [358]
Underfull \hbox (badness 1173) in paragraph at lines 21715--21715
[]/aer10/Double pre-ci-sion value, in dec-i-mal no-ta-tion of the form 
/aett10/
[-]m.ddde[+-]xx /aer10/or
[359] [360] [361]
! Interrmake[1]: *** Deleting file `refman.pdf'
uption.
<to be read again>
                   \@tempskipa
l.21899 \end{Value}

?
! Emergency stop.
<to be read again>
                   \@tempskipa

docs/README says that it is necessary to set a certain pool_size var to 
500000, but I was not able to find any conf file
with that var. By the way, I'm using fptex (default install).


Any help would be great.



From ripley at stats.ox.ac.uk  Thu Aug  5 21:03:26 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 5 Aug 2004 20:03:26 +0100 (BST)
Subject: [R] or of a logical vector
In-Reply-To: <6.0.1.1.2.20040805201739.01e980e0@stat4ux.stat.ucl.ac.be>
Message-ID: <Pine.LNX.4.44.0408052002210.4254-100000@gannet.stats>

See ?any, which does this directly.

On Thu, 5 Aug 2004, Eric Lecoutre wrote:

> 
> Hi Ben,
> 
> Always do consider that boolean vectors TRUE/FALSE are equivalent to 
> integers 1/0.

Not really.  They are in almost all arithmetic expressions.

> What you want is to know wether one element of a vector is TRUE, which is:
> 
>  > sum(vec)>0
> 
> HTH,
> 
> Eric
> 
> 
> 
> At 18:38 5/08/2004, Ben Wittner wrote:
> >Is there some fast (built-in?) way to get the OR of all the elements in a
> >logical vector?
> >
> >In other words, is there some fast (built-in) version of the function vor
> >below?
> >
> >Thanks.
> >
> >-Ben
> >
> >vor <- function(v) {
> >   ans <- v[1]
> >   if (length(v) > 1)
> >     for (i in 2:length(v))
> >       ans <- ans | v[i]
> >   ans
> >}
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> Eric Lecoutre
> UCL /  Institut de Statistique
> Voie du Roman Pays, 20
> 1348 Louvain-la-Neuve
> Belgium
> 
> tel: (+32)(0)10473050
> lecoutre at stat.ucl.ac.be
> http://www.stat.ucl.ac.be/ISpersonnel/lecoutre
> 
> If the statistics are boring, then you've got the wrong numbers. -Edward 
> Tufte
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andy_liaw at merck.com  Thu Aug  5 21:03:44 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 5 Aug 2004 15:03:44 -0400
Subject: [R] Local library on Windoze.
Message-ID: <3A822319EB35174CA3714066D590DCD504AF81A8@usrymx25.merck.com>

You need to _intsall_ or _build_ the package on Unix.  Those will generate
help files in various formats from the .Rd files.  

I.e., on the Unix side, do:

R CMD build --binary misc

then ftp the output over to Windoze and install as usual.

HTH,
Andy

> From: Rolf Turner
> 
> I'm mystified by a Windoze ``phenomenon'' which has just bitten me.
> 
> I have a laptop which boots either Windoze or Linux; I (sad to say)
> need the Windoze OS for teaching purposes.  I have R 1.9.1 installed
> on the laptop in the Windoze OS.
> 
> Just now I decided to install,  under the Windoze version of R,
> a library (``Misc'') of some local functions that I have collected
> over the years.  I just zipped up an installed Unix version of Misc
> (there is no compiled C or Fortran to make life complicated), ftp-ed
> the zip file to the Windoze-running laptop, started R, installed the
> package ....  everything went like a dream.
> 
> ***THEN*** I noticed a typo in one of the help files from Misc.
> So I decided to fix it.
> 
> First try:  Went to C:\Program Files\R\rw1091\library\Misc\man
> and edited the file Misc.Rd, eliminating the typo.   Restarted
> R, loaded Misc, looked at the help --- typo is still there.
> 
> Second try:  Fixed the typo in the Unix version, zipped it up again,
> ftp-ed it to the Windoze-running laptop, re-installed it.  Restarted
> R, loaded Misc, looked at the help --- typo is still there.
> 
> Third try:  Deleted the Misc directory from
> 
> 	C:\Program Files\R\rw1091\library
> 
> started R, tried to load Misc, got told no such package --- fine,
> it's gone.  Stopped R, restarted, re-installed Misc.  Checked on the
> file
> 
> 	C:\Program Files\R\rw1091\library\Misc\man\Misc.Rd
> 
> That file is fine; no typo in it.
> 
> Re-started R again, loaded Misc, looked at the help --- the *#%#$#
> type is STILL there!!!
> 
> Why is there a ghost Misc library hanging around?  Where is it?
> How do I get rid of it?
> 
> 			cheers,
> 
> 				Rolf Turner
> 				rolf at math.unb.ca
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From jgentry at jimmy.harvard.edu  Thu Aug  5 21:05:41 2004
From: jgentry at jimmy.harvard.edu (Jeff Gentry)
Date: Thu, 5 Aug 2004 15:05:41 -0400 (EDT)
Subject: [R] Re: or of a logical vector
Message-ID: <Pine.SOL.4.20.0408051502500.4703-100000@jaws.dfci.harvard.edu>

> Is there some fast (built-in?) way to get the OR of all the elements in
> a logical vector?

any() and all() should give you OR and AND, respectively.  Perhaps this
should be in as a 'see also' for '|' and '&'.

-J



From RBaskin at ahrq.gov  Thu Aug  5 21:15:22 2004
From: RBaskin at ahrq.gov (Baskin, Robert)
Date: Thu, 5 Aug 2004 15:15:22 -0400 
Subject: [R] or of a logical vector
Message-ID: <6BCD3F430455B1418750004BCD279259029262E3@exchange2.ahrq.gov>

I don't know how careful about coercing type you need to be.

Something like TRUE %in% outer(v,v,"|") may work for you but simpler
functions that do arithmetic and coerce the answer back to logical [e.g.,
as.logical(max(v))] might suffice.

> xt <- c(T,T,T)
> xf <- c(T,F,F)
> outer(xt, xf, "|")
     [,1] [,2] [,3]
[1,] TRUE TRUE TRUE
[2,] TRUE TRUE TRUE
[3,] TRUE TRUE TRUE
> outer(xf, xf, "|")
     [,1]  [,2]  [,3]
[1,] TRUE  TRUE  TRUE
[2,] TRUE FALSE FALSE
[3,] TRUE FALSE FALSE
> TRUE %in% outer(xf, xf, "|")
[1] TRUE
> TRUE %in% outer(!xt, !xt, "|")
[1] FALSE
> #if you don't mind sloppiness
> as.logical(max(xf))
[1] TRUE
> as.logical(max(!xt))
[1] FALSE
> as.logical(sum(xf^2))
[1] TRUE
> as.logical(sum((!xt)^2))
[1] FALSE
> #be careful
> as.logical(max(c(-1,0,0))
+ )
[1] FALSE
> 

bob




-----Original Message-----
From: Ben Wittner [mailto:bwittner at jimmy.harvard.edu] 
Sent: Thursday, August 05, 2004 12:38 PM
To: r-help at stat.math.ethz.ch
Subject: [R] or of a logical vector


Is there some fast (built-in?) way to get the OR of all the elements in a
logical vector?

In other words, is there some fast (built-in) version of the function vor
below?

Thanks.

-Ben

vor <- function(v) {
  ans <- v[1]
  if (length(v) > 1)
    for (i in 2:length(v))
      ans <- ans | v[i]
  ans
}

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Ted.Harding at nessie.mcc.ac.uk  Thu Aug  5 21:22:12 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 05 Aug 2004 20:22:12 +0100 (BST)
Subject: [R] or of a logical vector
In-Reply-To: <Pine.SOL.4.20.0408051228300.15412-100000@noah.dfci.harvard.edu>
Message-ID: <XFMail.040805202212.Ted.Harding@nessie.mcc.ac.uk>

On 05-Aug-04 Ben Wittner wrote:
> Is there some fast (built-in?) way to get the OR of all the
> elements in a logical vector?
> 
> In other words, is there some fast (built-in) version of the function
> vor below?
> 
> Thanks.
> 
> -Ben
> 
> vor <- function(v) {
>   ans <- v[1]
>   if (length(v) > 1)
>     for (i in 2:length(v))
>       ans <- ans | v[i]
>   ans
> }

It's a sort of cheating ("type-punning"), but so long as it's just
the "or" you're after then

  sum(v)>0

will implement your 'vor', i.e. give you FALSE if all v[i]==FALSE,
and TRUE if any v[i]==TRUE. And it's certainly fast.

Similarly,

  prod(v)>0

would implement a 'vand'.

But you can also do these with 'any' and 'all', e.g.

  any(v==TRUE)

and

  all(v==TRUE)

I'm not sure which of these two approaches would be faster, but
I doubt there's much in it.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 05-Aug-04                                       Time: 20:22:12
------------------------------ XFMail ------------------------------



From spencer.graves at pdf.com  Thu Aug  5 21:53:17 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 05 Aug 2004 12:53:17 -0700
Subject: [R] cross random effects (more information abuot the data)
In-Reply-To: <20040805135640.32751.qmail@web52807.mail.yahoo.com>
References: <20040805135640.32751.qmail@web52807.mail.yahoo.com>
Message-ID: <4112902D.7040003@pdf.com>

      Have you read the posting guide 
(http://www.R-project.org/posting-guide.html)?  Also, can you produce a 
simpler example with a few lines of R code that someone could copy from 
your email and paste into R to illustrate your problem.  I don't have 
time to read your email, but if you could reduce it by a factor of about 
20, with a toy example that someone else could easily understand, you 
might more likely get the answer you are seeking.  (You might also find 
the answer to your question by following carefully the steps outlined in 
the posting guide and trying to produce a toy example as just suggested.) 

      I'm sorry I can't be of more help.  I know how frustrating it can 
be to struggle for days with a problem like this. 
      spencer graves

aleid2001 at yahoo.com wrote:

>Dear friends,
>
>I have asked last few days about cross-random effects
>using PQL, but I have not receive any answer because
>might my question was not clear.
>
>My question was about analysing the salamander mating
>data using PQL. This data contain cross-random effects
>for (male) and for (female). By opining MASS and lme
>library. I wrote this code
>
>sala.glmm <- glmmPQL(fixed=y~WSf*WSM,
>random=list(experiment=pdBlocked(list(pdIdent(~randf-1),pdIdent(~randm-1)))),
>family=binomial, data=sala.data).
>
>Where
>data neame=sala.glmm which contain
> y is response
> wsf is fixed effect
> wsm is fixed effects
> randf  is random effect
> random is random effect
>
>The data contain three experiments at the same time.
>The previous cod is work but it does not give me
>accurate result especially for the random effects.
>
>For experiment I wrote this code 
>
>experiment <-
>factor(c(rep(1,120),rep(2,120),rep(3,120)))
> because I have three experiments at the same time,
>but if I change the experiment to e.g
>
>experiment <- factor(c(rep(1,360)))
>
>is still give answer but is not the right answer. So,
>I am accusing my specification of the experiment
>(group). If you have any suggestion pleas let me know.
>
>   E-mail:aleid2001 at yahoo.com
>
>Here I am going to gve mre details about the data.
>
>the
>  
>
>>details about the data is:
>>
>> 
>>    
>>
> The data are:
>  
>
>> McCullagh and Nelder (1989,sec.14.5)polished an
>>interesting set of data on the success of matting
>>between male and female salamanders drawn from two
>>populations, the rough butts (RB) and the white
>>    
>>
>sides
>  
>
>>(WS), that had been geographically isolated from
>>    
>>
>each
>  
>
>>other. In the first of three experiments, conducted
>>during the summer of 1986, 10 RB females and 10 WS
>>females were mated with three RB males and three WS
>>males, for a total of six mating each over 24 days.
>>Each of 10 RB males and 10 WS males likewise served
>>    
>>
>as
>  
>
>>mates for three females of each type. These same 40
>>salamanders were used in a repeat experiment
>>    
>>
>conducted
>  
>
>>in the fall that involved no repetitions of the
>>earlier mal-female pairs. A third experiment, also
>>conducted in the fall, used a new set of 40 animals.
>>Each experiment involved 30 mating of each of the
>>    
>>
>four
>  
>
>>gender-population combinations. Simple inspection of
>>the data revealed that three of the crosses had
>>success rates of about 70%, whereas the mating of WS
>>females with RB MALES WAS SUCCESSFUL ONLY 25% OF the
>>time. Evaluating the statistical significance of
>>    
>>
>these
>  
>
>>differences was complicated by the fact that the 360
>>binary responses were not independent.
>>
>> The model is used here is the mating probabilities
>>are assumed to be the same for each of the three
>>experiments. The random effects are assumed to be
>>independent in each experiment. The male and
>>Female effects are assumed to have different
>>    
>>
>variances
>  
>
>>but the variances are assumed
>>To be the same across the three experiments.
>>
>>      Best Regared
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From ripley at stats.ox.ac.uk  Thu Aug  5 21:56:17 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 5 Aug 2004 20:56:17 +0100 (BST)
Subject: [R] Windows build
In-Reply-To: <4112845A.8080905@estatcamp.com.br>
Message-ID: <Pine.LNX.4.44.0408052050520.6984-100000@gannet.stats>

On Thu, 5 Aug 2004, Gustavo Pinheiro wrote:

> I've tried building R in Windows and had trouble compiling Bitmap.dll 
> (make bitmapdll) and the docs (make docs). See make output below:
> 
> E:\R\R-1.9.1\src\gnuwin32>make bitmapdll
> make -C bitmap
> make[1]: Entering directory `/cygdrive/e/R/R-1.9.1/src/gnuwin32/bitmap'
> make[3]: `libz.a' is up to date.
> make[3]: scripts/makefile.std: No such file or directory
> make[3]: *** No rule to make target `scripts/makefile.std'.  Stop.
> make[2]: *** [buildpng] Error 2
> make[1]: *** [all] Error 2
> make[1]: Leaving directory `/cygdrive/e/R/R-1.9.1/src/gnuwin32/bitmap'
> make: *** [bitmapdll] Error 2
> 
> I couldn't find 'scripts/makefile.std' anywhere. These are the files in 
> src/gnuwin32/bitmap:
> 
> jpeg-6b (folder)
> libpng  (folder)
> INSTALL
> jconfig.h
> Makefile
> Makefile.jpeg
> rbitmap.c

It should be in libpng.  It certainly is in the current tarball, 
libpng-1.2.5.tar.gz, so please check how you have lost it.

> For the 'make docs' part, what I got was lots of:
> 
> Underfull \hbox (badness 3407) in paragraph at lines 21211--21216
> /aett10/a/aer10/. If /aett10/LINPACK /aer10/is /aett10/TRUE /aer10/the 
> de-fault
>  is /aett10/1e-7/aer10/, oth-er-wise it is
> [351] [352] [353] [354] [355] [356] [357] [358]
> Underfull \hbox (badness 1173) in paragraph at lines 21715--21715
> []/aer10/Double pre-ci-sion value, in dec-i-mal no-ta-tion of the form 
> /aett10/
> [-]m.ddde[+-]xx /aer10/or
> [359] [360] [361]
> ! Interrmake[1]: *** Deleting file `refman.pdf'
> uption.
> <to be read again>
>                    \@tempskipa
> l.21899 \end{Value}
> 
> ?
> ! Emergency stop.
> <to be read again>
>                    \@tempskipa
> 
> docs/README says that it is necessary to set a certain pool_size var to 
> 500000, but I was not able to find any conf file
> with that var. By the way, I'm using fptex (default install).

texmf.cnf does (or something very similar).  However, that looks like a
flakiness causing an interrupt.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From pwilkinson at videotron.ca  Thu Aug  5 21:59:39 2004
From: pwilkinson at videotron.ca (Peter Wilkinson)
Date: Thu, 05 Aug 2004 15:59:39 -0400
Subject: [R] R interface to Python (in Windows)
In-Reply-To: <41129F6B.4090607@telgua.com.gt>
References: <Pine.SOL.4.58.0408021333570.7251@stat.psych.uiuc.edu>
	<410EA2F8.CAB22E10@gene.com> <41129F6B.4090607@telgua.com.gt>
Message-ID: <6.0.3.0.0.20040805155654.01b7d030@pop.videotron.ca>

Hi there,

This is because rpy that is currently available was compiled for R1.8.1, 
and they have not released a compile for R1.9.1

There is an rpy mailing list where you can post your concern. you can get 
to it fromt he rpy pages as sourceforge.

You will need to find a copy of R 1.8.1 to use with that version. I will 
let you know if I find a copy as I have been looking for one myself.

Peter


At 04:58 PM 8/5/2004, Hector Villafuerte wrote:
>Hi,
>I'm trying to install RPY (to interface Python and R).
>This are the programs installed in my PC:
>R - 1.9.0
>Python - 2.3.4
>pywin32 - 201
>rpy - 0.3.5 (I'm using the executable version)
>
>Everything installs nice, but when I try to use RPY a message box appears 
>with this cryptic note:
>"The procedure entry point malloc could not be located in the dynamic link 
>library R.dll"
>
>And then this shows in the interactive session:
> >>> import rpy
>Traceback (most recent call last):
>File "<input>", line 1, in ?
>File "C:\Python23\Lib\site-packages\rpy.py", line 55, in ?
>import _rpy
>ImportError: DLL load failed: The specified procedure could not be found.
> >>>
>
>Any ideas? Thanks in advance!
>Hector
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Thu Aug  5 22:01:14 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 5 Aug 2004 21:01:14 +0100 (BST)
Subject: [R] R interface to Python (in Windows)
In-Reply-To: <41129F6B.4090607@telgua.com.gt>
Message-ID: <Pine.LNX.4.44.0408052057080.6984-100000@gannet.stats>

On Thu, 5 Aug 2004, Hector Villafuerte wrote:

> Hi,
> I'm trying to install RPY (to interface Python and R).

Since that is not R, you might do better to talk first to the maintainer.

> This are the programs installed in my PC:
> R - 1.9.0
> Python - 2.3.4
> pywin32 - 201
> rpy - 0.3.5 (I'm using the executable version)
> 
> Everything installs nice, but when I try to use RPY a message box 
> appears with this cryptic note:
> "The procedure entry point malloc could not be located in the dynamic 
> link library R.dll"

Well, that is quite correct, and means your RPY has been misbuild, at a
guess against R 1.8.x.  I guess it needs to be compiled against the
version of R you actually used. It's an RPY/rpy and not an R issue so this 
is not the right place.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Thu Aug  5 22:15:08 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 05 Aug 2004 22:15:08 +0200
Subject: [R] or of a logical vector
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF81A7@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF81A7@usrymx25.merck.com>
Message-ID: <x2hdrhgx4j.fsf@biostat.ku.dk>

"Liaw, Andy" <andy_liaw at merck.com> writes:

> Is there anything wrong with sum(v) > 0?

Yes, there is an any()-thing.... ;-)

(And NA handling differs too.)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From andy_liaw at merck.com  Thu Aug  5 22:23:17 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 5 Aug 2004 16:23:17 -0400
Subject: [R] or of a logical vector
Message-ID: <3A822319EB35174CA3714066D590DCD504AF81AD@usrymx25.merck.com>

Yes, of course:

> x <- c(TRUE, NA)
> any(x)
[1] TRUE
> sum(x) > 0
[1] NA

Andy

> From: Peter Dalgaard  
> 
> "Liaw, Andy" <andy_liaw at merck.com> writes:
> 
> > Is there anything wrong with sum(v) > 0?
> 
> Yes, there is an any()-thing.... ;-)
> 
> (And NA handling differs too.)
> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: 
> (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: 
> (+45) 35327907
> 
>



From Benjamin.STABLER at odot.state.or.us  Thu Aug  5 22:40:04 2004
From: Benjamin.STABLER at odot.state.or.us (Benjamin.STABLER@odot.state.or.us)
Date: Thu, 5 Aug 2004 13:40:04 -0700 
Subject: [R] R interface to Python (in Windows)
Message-ID: <76A000A82289D411952F001083F9DD06047FE81F@exsalem4-bu.odot.state.or.us>

I put a copy of R 1.8.1 for Windows on our FTP site at:
ftp://ftp.odot.state.or.us/outgoing/Test/.  It should be there for a few
days before it gets deleted.  


Benjamin Stabler
Transportation Planning Analysis Unit
Oregon Department of Transportation
555 13th Street NE, Suite 2
Salem, OR 97301  Ph: 503-986-4104

-----Original Message-----
From: Peter Wilkinson [mailto:pwilkinson at videotron.ca] 
Sent: Thursday, August 05, 2004 1:00 PM
To: Hector Villafuerte; tutor at python.org; r-help at stat.math.ethz.ch
Subject: Re: [R] R interface to Python (in Windows)


Hi there,

This is because rpy that is currently available was compiled for R1.8.1, 
and they have not released a compile for R1.9.1

There is an rpy mailing list where you can post your concern. you can get 
to it fromt he rpy pages as sourceforge.

You will need to find a copy of R 1.8.1 to use with that version. I will 
let you know if I find a copy as I have been looking for one myself.

Peter


At 04:58 PM 8/5/2004, Hector Villafuerte wrote:
>Hi,
>I'm trying to install RPY (to interface Python and R).
>This are the programs installed in my PC:
>R - 1.9.0
>Python - 2.3.4
>pywin32 - 201
>rpy - 0.3.5 (I'm using the executable version)
>
>Everything installs nice, but when I try to use RPY a message box appears 
>with this cryptic note:
>"The procedure entry point malloc could not be located in the dynamic link 
>library R.dll"
>
>And then this shows in the interactive session:
> >>> import rpy
>Traceback (most recent call last):
>File "<input>", line 1, in ?
>File "C:\Python23\Lib\site-packages\rpy.py", line 55, in ?
>import _rpy
>ImportError: DLL load failed: The specified procedure could not be found.
> >>>
>
>Any ideas? Thanks in advance!
>Hector
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Thu Aug  5 22:55:10 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 05 Aug 2004 22:55:10 +0200
Subject: [R] tcltk:  repeat event while button is down?
In-Reply-To: <36l4h05guji9v9e551ul4bakng8o537ncg@4ax.com>
References: <36l4h05guji9v9e551ul4bakng8o537ncg@4ax.com>
Message-ID: <x2d625gv9t.fsf@biostat.ku.dk>

Duncan Murdoch <murdoch at stats.uwo.ca> writes:

> Is there a way in TCL/TK to trigger an event multiple times while a
> button is held down?  I'd like to have an rgl scene continuously
> rotate until the button is released.

Something involving "after", I'd say. Set up the rotate action to
reschedule itself after N milliseconds, and the button release event
to "after cancel". 


Or, perhaps safer, set a flag and condition the "after" command on it.

I.e. something like

doit <- function() if(flag){rotate(); tkcmd("after",5,doit)}

on button press, 

flag <- TRUE ; doit()

and on button release

flag <- FALSE
       
That method has the drawback that one rotate() will occur after the
button release. 

The first suggestion would amount to

doit <- function() {rotate(); ID <<- tkcmd("after",5,doit)}

and then on button press (watch the scoping issues)

ID <- ""
doit()

and on release

tkcmd("after","cancel",ID)

which I think does work in pure Tcl, but I suspect that it in the
Windows implementation of R-Tcl can cause race conditions so that the
ID variable is outdated when the cancel takes place (Tcl guarantees
that scripts execute atomically; as it turns out, for good
reasons...). 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From murdoch at stats.uwo.ca  Thu Aug  5 23:07:15 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 05 Aug 2004 17:07:15 -0400
Subject: [R] function output
In-Reply-To: <s1124341.067@ffdata.setur.fo>
References: <s1124341.067@ffdata.setur.fo>
Message-ID: <r585h098kk8vgq1rbb3nabqdsbb2oc6dtm@4ax.com>

On Thu, 05 Aug 2004 14:24:57 +0100, "Luis Rideau Cruz" <Luisr at frs.fo>
wrote :

>I have the next function :
>
>"extracting.lgddata"<-function(channel)
>{
>library(RODBC)
>channel<-odbcDriverConnect("")
>.......
>.......
>}
>
>...... is just a sequence of SQL selects.
>But when I try to call the objects created from the select they do not seem to have been created.
>I checked with ls() and they actually do not exist.
>What is the problem?

R passes arguments by value, so the channel that got changed by this
call is a local copy, the change doesn't affect whatever you passed in
to the function.

It's best (from a long-term maintenance point of view) to try to write
your functions without side effects; they should return values, rather
than changing things outside of themselves.  Then when you use them
you make use of what comes out.

If you really want side effects, then investigate the <<- version of
assignment, but study carefully exactly what it does, and be prepared
for it to do something else entirely when you port your code to
S-PLUS.

Duncan Murdoch



From tlumley at u.washington.edu  Thu Aug  5 23:08:20 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 5 Aug 2004 14:08:20 -0700 (PDT)
Subject: [R] or of a logical vector
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF81A7@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF81A7@usrymx25.merck.com>
Message-ID: <Pine.A41.4.58.0408051407090.67154@homer07.u.washington.edu>

On Thu, 5 Aug 2004, Liaw, Andy wrote:

> Is there anything wrong with sum(v) > 0?
>

any(v) has the advantage of having the same NA handling as Ben's function,
so that any(v) is TRUE if there is at least one TRUE and any number of NAs
and is NA if there are NAs and no TRUEs.

	-thomas



From tlumley at u.washington.edu  Thu Aug  5 23:13:51 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 5 Aug 2004 14:13:51 -0700 (PDT)
Subject: [R] or of a logical vector
In-Reply-To: <XFMail.040805202212.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.040805202212.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.A41.4.58.0408051410420.67154@homer07.u.washington.edu>

On Thu, 5 Aug 2004 Ted.Harding at nessie.mcc.ac.uk wrote:
> But you can also do these with 'any' and 'all', e.g.
>
>   any(v==TRUE)
>

or any( (v==TRUE)==TRUE), or any( ((v==TRUE)==TRUE)==TRUE)...

Or, perhaps, any(v).

Lewis Carroll wrote a nice piece on this theme.


	-thomas



From murdoch at stats.uwo.ca  Thu Aug  5 23:13:56 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 05 Aug 2004 17:13:56 -0400
Subject: [R] tcltk:  repeat event while button is down?
In-Reply-To: <x2d625gv9t.fsf@biostat.ku.dk>
References: <36l4h05guji9v9e551ul4bakng8o537ncg@4ax.com>
	<x2d625gv9t.fsf@biostat.ku.dk>
Message-ID: <fl85h0ln0mc7vccu53djbu4rnbt5vjgmtj@4ax.com>

On 05 Aug 2004 22:55:10 +0200, Peter Dalgaard
<p.dalgaard at biostat.ku.dk> wrote :

>Duncan Murdoch <murdoch at stats.uwo.ca> writes:
>
>> Is there a way in TCL/TK to trigger an event multiple times while a
>> button is held down?  I'd like to have an rgl scene continuously
>> rotate until the button is released.
>
>Something involving "after", I'd say. Set up the rotate action to
>reschedule itself after N milliseconds, and the button release event
>to "after cancel". 

Thanks.  Just one more detail:  the docs I have must be old, they
don't talk about separate button press and button release events.  How
do I attach separate event handlers to those?

Duncan



From Soichi.Hayashi at acxiom.com  Fri Aug  6 00:19:37 2004
From: Soichi.Hayashi at acxiom.com (Hayashi Soichi - shayas)
Date: Thu, 5 Aug 2004 17:19:37 -0500
Subject: [R] Using pipe for input data
Message-ID: <EA80FFF5E80CD5118A81009027DE9DFC0F9C0D81@conmsx05.corp.acxiom.net>

Hi. 

I have asked this question before and Aaron J. Mackey and Tony Plate gave me
some great insight but I still can't figure out how to do what I am trying
to accomplish. So let me ask again...

What I am trying to do is to make R read data from pipe (stdin).

Say I have following files on my directory

my.dat
       apple 1
       orange 2
       grape 3

my.R
       d <- read.table( stdin(), header=F, dec='.',
col.names=c("name","type"), na.strings=c("xxxx"))
       summary(d)

and When I run this command
       
       cat my.dat | R CMD BATCH --vanilla --slave my.R

I am expecting to see the summery report for the datasource my.dat

But here is what I actually see in my.Rout
	> d <- read.table( stdin(), header=F, dec='.',
col.names=c("name","type"), na.strings=c("xxxx"))
	0: summary(d)
	1: proc.time()
	2: Error in scan(file = file, what = what, sep = sep, quote = quote,
dec = dec,  :
	line 1 did not have 2 elements 
	Execution halted

If I execute the content of the my.R on regular R command line, I can
actually "type in" all datasource and creates the correct summery report. So
I don't know why I can make R to read the input from the piped datasource...

Has anybody done something like this? 

Thanks,
Soichi Hayashi




**********************************************************************
The information contained in this communication is\ confiden...{{dropped}}



From gerifalte28 at hotmail.com  Fri Aug  6 00:59:33 2004
From: gerifalte28 at hotmail.com (F Z)
Date: Thu, 05 Aug 2004 22:59:33 +0000
Subject: [R] ANOVA with repeated measurements (Dan Rajdl)
Message-ID: <BAY2-F24kpB9pbKWZfE00001530@hotmail.com>

Dear Rajdl

I google I typed "[R] ANOVA repeated measures" and I found several good 
references.  Please try to do some research before posting a question.

Anyhow, you might want to try ?lme or ?aov and look for the "error" 
parameter" (if you want to use an "old schoool" method)


Cheers

Francisco

>From: "Rajdl Daniel" <RAJDL at fnplzen.cz>
>To: <r-help at stat.math.ethz.ch>
>Subject: [R] ANOVA with repeated measurements (Dan Rajdl)
>Date: Thu, 5 Aug 2004 17:29:09 +0200
>
>Dear R-users,
>I have some human serum lipid concentrations (cholesterol, apoB ...),
>each lipid was measured (in the same person) for 6 times in different
>time points (start, 3 months, 6 months, 12 months, 18 months, 24
>months). There were 2 groups of participants: one with a nutritional
>intervention and the other without it. I would like to know, whether
>lipid concentrations differ among different time points and between the
>2 groups.
>Could anyone advise me, please, what method and how should I usein R? Or
>redirect me, please, to a source, where I can find the answer.
>Thank you,
>Dan Rajdl
>Dpt.Clin.Chem.Biochem.Hematol, Univ.Hosp.in Pilsen, Czech Republic
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Fri Aug  6 01:20:38 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 06 Aug 2004 01:20:38 +0200
Subject: [R] tcltk:  repeat event while button is down?
In-Reply-To: <fl85h0ln0mc7vccu53djbu4rnbt5vjgmtj@4ax.com>
References: <36l4h05guji9v9e551ul4bakng8o537ncg@4ax.com>
	<x2d625gv9t.fsf@biostat.ku.dk>
	<fl85h0ln0mc7vccu53djbu4rnbt5vjgmtj@4ax.com>
Message-ID: <x24qnhgojd.fsf@biostat.ku.dk>

Duncan Murdoch <murdoch at stats.uwo.ca> writes:

> On 05 Aug 2004 22:55:10 +0200, Peter Dalgaard
> <p.dalgaard at biostat.ku.dk> wrote :
> 
> >Duncan Murdoch <murdoch at stats.uwo.ca> writes:
> >
> >> Is there a way in TCL/TK to trigger an event multiple times while a
> >> button is held down?  I'd like to have an rgl scene continuously
> >> rotate until the button is released.
> >
> >Something involving "after", I'd say. Set up the rotate action to
> >reschedule itself after N milliseconds, and the button release event
> >to "after cancel". 
> 
> Thanks.  Just one more detail:  the docs I have must be old, they
> don't talk about separate button press and button release events.  How
> do I attach separate event handlers to those?

<ButtonPress-1> (or just <1>), <ButtonRelease-1>, I believe. It's used
in the tkcanvas demo.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From xt_wang at cs.concordia.ca  Fri Aug  6 03:34:29 2004
From: xt_wang at cs.concordia.ca (xt_wang@cs.concordia.ca)
Date: Thu,  5 Aug 2004 21:34:29 -0400
Subject: [R] question about random variable generating function
Message-ID: <1091756069.4112e025ee3a0@mailhost.cs.concordia.ca>

Hello, everybody,

I have questions about functions in chapter 5: The R API:entry points for c 
code in "writing R Extensions".
They are :

1. double rnorm(double mu, double sigma)
2. double norm_rand()

question is: which of the above two functions can be used to generate a random 
variable of normal distribution?

If norm_rand() is the function, how can I generate random variables whose 
mean=0 and standard deviation=5? As I know, it is used to generate a random 
variable which mean=0,std=1.

I will appreciate very much if anybody can help me.

Thanks in advance!

Maggie



From astephen at efs.mq.edu.au  Fri Aug  6 03:50:55 2004
From: astephen at efs.mq.edu.au (Alec Stephenson)
Date: Fri, 06 Aug 2004 11:50:55 +1000
Subject: [R] question about random variable generating function
Message-ID: <s11370c4.007@efs04.efs.mq.edu.au>

Both: norm_rand generates a standard normal, rnorm allows mean and
standard deviation arguments, so using the latter is easiest.



Alec Stephenson                                               
Department of Statistics
Macquarie University
NSW 2109, Australia 

>>> <xt_wang at cs.concordia.ca> 08/06/04 11:34am >>>
Hello, everybody,

I have questions about functions in chapter 5: The R API:entry points
for c 
code in "writing R Extensions".
They are :

1. double rnorm(double mu, double sigma)
2. double norm_rand()

question is: which of the above two functions can be used to generate a
random 
variable of normal distribution?

If norm_rand() is the function, how can I generate random variables
whose 
mean=0 and standard deviation=5? As I know, it is used to generate a
random 
variable which mean=0,std=1.

I will appreciate very much if anybody can help me.

Thanks in advance!

Maggie

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From edd at debian.org  Fri Aug  6 06:25:12 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 5 Aug 2004 23:25:12 -0500
Subject: [R] Quantian 0.5.9.3 with over 400 CRAN and BioConductor packages
Message-ID: <20040806042511.GA14706@sonny.eddelbuettel.com>


[ Apologies for the cross-post, and I hope this is seen as sufficiently
  on-topic.  Anybody who feels otherwise, please drop me a line off-list. ]

Earlier this week I sent out the announcement below for the newest version
of Quantian. Quantian is a bootable dvd (based off Knoppix & clusterKnoppix)
with well over 1gb of scientific, numerical or quantitative software -- and
now contains almost all CRAN and BioConductor packages [1]. I would greatly
appreciate feedback, test reports, or suggestions. General questions are
more than welcome on the quantian-general at lists.alioth.debian.org list.

With best regards,  Dirk

[1] Generally speaking, packages like ROracle that require special header
files are excluded, as are a few Windows-specific packages. Package with
'free only for academic research' licenses are also excluded as Quantian is
being redistributed by a few dvd / cdrom re-sellers.




Executive Summary for a quick overview: 
  Quantian 0.5.9.3 adds over 360 GNU R packages from the CRAN and
  BioConductor archives, the GNU geda electronics design software, an almost
  complete debian-med suite, the complete scalapack development suite,
  atlas3 and lam development packages, fancy GL screensavers, lots of
  recommended documentation packages as well as a general update of over 300
  Debian packages bringing the size of the iso to over 1.5gb corresponding
  to 4.4gb uncompressed.



Announcing Quantian release 0.5.9.3
===================================


I   What is it?

    Quantian is a remastering of Knoppix, the self-configuring and directly
    bootable cdrom/dvd that turns any pc or laptop into a full-featured Linux
    workstation, and clusterKnoppix, which adds support for openMosix. 
    However, Quantian differs from (cluster)Knoppix by adding a large set 
    of programs of interest to applied or theoretical workers in 
    quantitative or data-driven fields. 

    See http://dirk.eddelbuettel.com/quantian.html for more details.

    
II  What is new?

    o Third release based on Knoppix 3.4 with many changes from both new and
      updated packages, still based on Knoppix 3.4 and the clusterKnoppix
      release from May 10 with kernel 2.4.26 with the 'testing status'
      openMosix patch as well as a non-openMosix kernel 2.6.6;

    o New Quantian software includes
      - way more GNU R packages generated with a modified version of a script
        by Albrecht Gebhardt -- about 360 packaged are added from CRAN
	and BioConductor provided unparalled depth for statistical computing
      - an almost complete debian-med of everything but the cms package
        (would Zope make sense for Quantian -- comments welcome!)
      - the GNU geda program suite for electronics design 
      - the scalapack suite for parallel linear algebra (pvm, lam and mpich)
      - lots of recommended documentation packages have been added 
      - header packages for atlas3 and lam are now included
      - plus lots of fancy GL screensavers;

    o Upgraded throughout against Debian testing with over 300 fresh packages;

    o Total size is now over 1.5gb for the compressed iso image,
      corresponding to more than 4.4gb of uncompressed software;

    o A new cdrom size version of 0.5.9.2, and a small 7gb bootable cdrom
      iso provided by Marco Caliari are available too;

    o New download site http://quantian.alioth.debian.org is up and running;
      note that the main site at U of Washington will be replaced in the
      near future -- stay tuned!
    
    o Mailing lists for Quantian up and running

      Following the 0.4.9.3 release, a Quantian project was opened on
      alioth.debian.org.  So far the only use of the alioth infrastructure
      has been the creation of two mailing lists

        quantian-announce	  for announcements, intended to be low volume
        quantian-general	  for general discussions about Quantian

      Please go to     

        http://alioth.debian.org/mail/?group_id=1425

      for subscription info etc., and start using the quantian-general lists
      for general questions, comments, suggestions or discussions about 
      Quantian.

      Quantian-general is subscribed to quantian-announce, so you only need
      to subscribe to one (but can of course subscribe to both).

      I also set the Reply-To: for this message to
      quantian-general at lists.alioth.debian.org   so that discussions can be
      continued on the list.

    o See http://dirk.eddelbuettel.com/quantian/changelog.html for details.


III Where do I get it?

    Downloads are available from the two main hosts both of which also
    provide rsync:

    o U of Washington:  
      - http://www.analytics.washington.edu/downloads/quantian
      - rsync://www.analytics.washington.edu::quantian

    o European mirrors, bittorrent site and cdrom vendors will hopefully
      catch up over the next few days. See 

	    http://dirk.eddelbuettel.com/quantian.html
 
      for download info.


IV  Known Bugs

    o None right now -- so please test away! 


V   Other items

    o Mailing lists have been created, see above.

    o Feedback / poll on package additions or removal

      As always, I welcome comments and suggestions about programs to be
      added or removed. Existing Debian packages get pushed to the front of
      the line.

      Please send feedback, questions, comments, ... to the 
      
	quantian-general at lists.alioth.debian.org

      list to maximise the number of eyes glancing at any one question.




-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx



From ripley at stats.ox.ac.uk  Fri Aug  6 08:22:57 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 6 Aug 2004 07:22:57 +0100 (BST)
Subject: [R] Using pipe for input data
In-Reply-To: <EA80FFF5E80CD5118A81009027DE9DFC0F9C0D81@conmsx05.corp.acxiom.net>
Message-ID: <Pine.LNX.4.44.0408060713500.22854-100000@gannet.stats>

The C stdin is used *always* to read commands from on Unix R, and even on 
console versions stdin() is where the commands are read from.  

R CMD BATCH is approximately giving you

R --vanilla --slave < my.R

and piping to such a command is going to do nothing for you.
Your command read.table(stdin() ... is going to read from the script my.R.

On Thu, 5 Aug 2004, Hayashi Soichi - shayas wrote:

> I have asked this question before and Aaron J. Mackey and Tony Plate gave me
> some great insight but I still can't figure out how to do what I am trying
> to accomplish. So let me ask again...
> 
> What I am trying to do is to make R read data from pipe (stdin).
> 
> Say I have following files on my directory
> 
> my.dat
>        apple 1
>        orange 2
>        grape 3
> 
> my.R
>        d <- read.table( stdin(), header=F, dec='.',
> col.names=c("name","type"), na.strings=c("xxxx"))
>        summary(d)
> 
> and When I run this command
>        
>        cat my.dat | R CMD BATCH --vanilla --slave my.R
> 
> I am expecting to see the summery report for the datasource my.dat
> 
> But here is what I actually see in my.Rout
> 	> d <- read.table( stdin(), header=F, dec='.',
> col.names=c("name","type"), na.strings=c("xxxx"))
> 	0: summary(d)
> 	1: proc.time()
> 	2: Error in scan(file = file, what = what, sep = sep, quote = quote,
> dec = dec,  :
> 	line 1 did not have 2 elements 
> 	Execution halted
> 
> If I execute the content of the my.R on regular R command line, I can
> actually "type in" all datasource and creates the correct summery report. So
> I don't know why I can make R to read the input from the piped datasource...

See the above analysis.  What I don't know is why you expected this to
work: did you look at the sources, e.g. the file BATCH?  If not - `great
insight' - the sources are the definitive documentation.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Freja.Vamborg at astrazeneca.com  Fri Aug  6 09:38:04 2004
From: Freja.Vamborg at astrazeneca.com (Freja.Vamborg@astrazeneca.com)
Date: Fri, 6 Aug 2004 09:38:04 +0200 
Subject: [R] speeding up functions for large datasets
Message-ID: <950E0EFCFDB083468E2A8BBC812E5E98954FD1@seludsmsx02.selu.astrazeneca.net>

Dear R-helpers, 
I'm dealing with large datasets, say tables of 60 000 times 12 or so, and
some of the functions are (too ) slow and I'm therefore trying to find ways
to speed them up.
I've found that for instance for-loops are slow in R (both by testing and by
searching through mail archives etc )
Are there any more well known arguments that are slow in R, ,maybe at data
representation level, code-writing, reading in the data.
I've also tried incorporating C-code, which works well, but I'd also like to
find other, maybe more "shortcut" ways.

Thanks in advance, 
Freja!

From ripley at stats.ox.ac.uk  Fri Aug  6 10:07:32 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 6 Aug 2004 09:07:32 +0100 (BST)
Subject: [R] speeding up functions for large datasets
In-Reply-To: <950E0EFCFDB083468E2A8BBC812E5E98954FD1@seludsmsx02.selu.astrazeneca.net>
Message-ID: <Pine.LNX.4.44.0408060901430.26412-100000@gannet.stats>

On Fri, 6 Aug 2004 Freja.Vamborg at astrazeneca.com wrote:

> Dear R-helpers, 
> I'm dealing with large datasets, say tables of 60 000 times 12 or so, and
> some of the functions are (too ) slow and I'm therefore trying to find ways
> to speed them up.
> I've found that for instance for-loops are slow in R (both by testing and by
> searching through mail archives etc )

I don't think that is really true, but it is the case that using
row-by-row operations in your situation would be slow *if they are
unnecessary*. It is a question of choosing the right algorithmic approach,
not whether it is implemented by for-loops or lapply or ....

> Are there any more well known arguments that are slow in R, ,maybe at data
> representation level, code-writing, reading in the data.
> I've also tried incorporating C-code, which works well, but I'd also like to
> find other, maybe more "shortcut" ways.

`S Programming' (see the R FAQ) has a whole chapter on this sort of thing, 
with examples.  More generally you want to take a `whole object' view and 
use indexing and other vectorized operations.

Note also that what is slow does change with the version of R and 
especially how much memory you have installed.  The first step is to get 
enough RAM.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From armougom at igs.cnrs-mrs.fr  Fri Aug  6 10:11:17 2004
From: armougom at igs.cnrs-mrs.fr (Fabrice Armougom)
Date: Fri, 06 Aug 2004 10:11:17 +0200
Subject: [R] imput data in cclust
Message-ID: <41133D25.9B28AE06@igs.cnrs-mrs.fr>



I would like to see an example of a data matrix for cclust and how to
import it to cclust.
In fact, i don't know how to give my imput for cclust program!

i test this file 

1 0.23 1.52
2 0.52 1.25
3 0.13 1.89
4 0.78 1.11

i do  
>library(cclust)
>x<-scan("test.matrice.phyl")
>cclust(x,2,method="kmeans")

i have this error message:

Error in sample(length(x), size, replace, prob) :
        invalid first argument


Now if i do :
>library(cclust)
>x<-read.table("test.matrice.phyl")
>cclust(x,2,method="kmeans")

i've this error message :
Error in as.double.default(x) : (list) object cannot be coerced to
double

Is someone can help me??

-- 
ARMOUGOM Fabrice
CNRS UPR 2589 -Laboratoire Information genonique et structurale-
31 chemin Joseph Aiguier
13402 Marseille cedex 20, FRANCE
tel. +33 (0)4 91 16 44 51



From armougom at igs.cnrs-mrs.fr  Fri Aug  6 10:14:23 2004
From: armougom at igs.cnrs-mrs.fr (Fabrice Armougom)
Date: Fri, 06 Aug 2004 10:14:23 +0200
Subject: [R] transformation
Message-ID: <41133DDF.2B2AEA69@igs.cnrs-mrs.fr>


Is there any fonction in R package  which can transform a multiple 
protein alignement 
in a matrix for cclust?





-- 
ARMOUGOM Fabrice
CNRS UPR 2589 -Laboratoire Information genonique et structurale-
31 chemin Joseph Aiguier
13402 Marseille cedex 20, FRANCE
tel. +33 (0)4 91 16 44 51



From ripley at stats.ox.ac.uk  Fri Aug  6 10:20:22 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 6 Aug 2004 09:20:22 +0100 (BST)
Subject: [R] imput data in cclust
In-Reply-To: <41133D25.9B28AE06@igs.cnrs-mrs.fr>
Message-ID: <Pine.LNX.4.44.0408060917560.26485-100000@gannet.stats>

Please read the help on the function (in package cclust, not mentioned by 
you).  It contains two examples, and

       x: Data matrix where columns correspond to variables and rows to
          observations

Your first attempt is a vector and your second is a data frame.  cclust 
requires a *matrix*, so please give it one.

On Fri, 6 Aug 2004, Fabrice Armougom wrote:

> 
> 
> I would like to see an example of a data matrix for cclust and how to
> import it to cclust.
> In fact, i don't know how to give my imput for cclust program!
> 
> i test this file 
> 
> 1 0.23 1.52
> 2 0.52 1.25
> 3 0.13 1.89
> 4 0.78 1.11
> 
> i do  
> >library(cclust)
> >x<-scan("test.matrice.phyl")
> >cclust(x,2,method="kmeans")
> 
> i have this error message:
> 
> Error in sample(length(x), size, replace, prob) :
>         invalid first argument
> 
> 
> Now if i do :
> >library(cclust)
> >x<-read.table("test.matrice.phyl")
> >cclust(x,2,method="kmeans")
> 
> i've this error message :
> Error in as.double.default(x) : (list) object cannot be coerced to
> double

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Joris.DeWolf at cropdesign.com  Fri Aug  6 11:16:15 2004
From: Joris.DeWolf at cropdesign.com (Joris DeWolf)
Date: Fri, 06 Aug 2004 11:16:15 +0200
Subject: [R] Lattice: how to index in a custom panel function?
Message-ID: <41134C5F.6010405@cropdesign.com>

Hi,

I have a lattice xyplot that contains panels according to FactorA, and 
curves for the 2 levels of Factor B within a panel.
I try to add text in the panels of a lattice graph. I suppose I have to 
write a custom function (panel.txt).
What I really would like is to adapt the text in the panel according to 
the levels of FactorA.
In the manuals, I find examples for the strips using which.given and 
which.panel. But this does not work for the panels...

Can Anybody give a hint? Thanks
Joris

I work with R 1.9.1 under Linux


panel.txt = function(sometxt,x,y,...){
    grid.text(sometxt,x,y)
}

xyplot(data = Pdata, P ~ DAS | FactorA,
   groups = FactorB,
   type ="s",
   col = c("red","blue"),
  panel = function(x,y,...){
        panel.abline(h = 100, lty = 5, lwd =0.5, col = "darkgrey")
        panel.txt(mytext, 0.2, 0.8)
}





-- 

====================================================================== 
Joris De Wolf
CropDesign N.V. 
Plant Evaluation Group
Technologiepark 3 
B-9052 Zwijnaarde 
Belgium 
Tel. : +32 9 242 91 55
Fax  : +32 9 241 91 73
====================================================================== 



confidentiality notice:
The information contained in this e-mail is confidential and...{{dropped}}



From jeaneid at chass.utoronto.ca  Fri Aug  6 12:47:04 2004
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Fri, 6 Aug 2004 06:47:04 -0400
Subject: [R] speeding up functions for large datasets
In-Reply-To: <950E0EFCFDB083468E2A8BBC812E5E98954FD1@seludsmsx02.selu.astrazeneca.net>
Message-ID: <Pine.SGI.4.40.0408060645440.31071480-100000@origin.chass.utoronto.ca>

you might want to turn your data into a matrix. You get much much faster
for  loops doing that.

Jean,

On Fri, 6 Aug 2004 Freja.Vamborg at astrazeneca.com wrote:

> Dear R-helpers,
> I'm dealing with large datasets, say tables of 60 000 times 12 or so, and
> some of the functions are (too ) slow and I'm therefore trying to find ways
> to speed them up.
> I've found that for instance for-loops are slow in R (both by testing and by
> searching through mail archives etc )
> Are there any more well known arguments that are slow in R, ,maybe at data
> representation level, code-writing, reading in the data.
> I've also tried incorporating C-code, which works well, but I'd also like to
> find other, maybe more "shortcut" ways.
>
> Thanks in advance,
> Freja!
>



From ajayshah at mayin.org  Fri Aug  6 12:47:40 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Fri, 6 Aug 2004 16:17:40 +0530
Subject: [R] Elementary questions about data structures
Message-ID: <20040806104740.GM31360@igidr.ac.in>

Folks,

S_t = (x_t, y_t) is the state of a system at time t. There is an
iterative function which converts S_t into S_{t+1}. I can easily write
this in R like this:

       iterate <- function(S) {
         list(S$x+1, S$y+1)
       }

So this function eats S_t and makes S_{t+1} and I can say
S2 <- iterate(S1)

My question: suppose I want to iterate from 1..10, what is the data
structure that is appropriate to store all these lists?

How, in R, does one make "vectors of lists"? I want to think of the
state vector at time t as a "record" and then I want to have a
vectorfull of them. When I'm done, I want to be able to make pictures
of the time-series of S$x and the time-series of S$y. How is this
done?

I tried some things:

> l1=list(x=2,y=3)
> l2=list(y=7,x=8)              # <--  note the order
> S = data.frame(x=1,y=1)       # odd way of initialising initial state
> S[2,] = l1
> S[3,] = l2
> S
  x y
1 1 1
2 2 3
3 7 8                           # <-- note it came out wrong.

I was very puzzled that he was wrong in how he handled the l2
assignment. How is it that the order matters? I thought lists were
handled in a more abstract way, where R knows that l2 is a collection
of l2$x and l2$y, without concern about the order in which they came
in. I had hoped that he would see that in S, there is an x and a y,
and that he would marry things up correctly. He doesn't.

What I would like to do is something like this:

S[1,] = list(x=1,y=2)                     # line 1
for (i in 2:10) {
  S[i,] = iterate(S[(i-1),])
}

I get errors at line 1 saying Error: Object "S" not found

If this could work, I know I'd be able to happily deal with the
time-series vector S$x and the time-series vector S$y.

Thanks a lot,

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From pes603 at bangor.ac.uk  Fri Aug  6 12:48:00 2004
From: pes603 at bangor.ac.uk (N.Callow)
Date: Fri, 06 Aug 2004 11:48:00 +0100
Subject: [R] squared semi-partial correlation
Message-ID: <411361E0.BA237FCF@bangor.ac.uk>

Hi, can you help? I am really struggling to find the answer to the
following question.

I have conducted a standard multiple regression using SPSS. The dependent
variable is CE, and the independent variables are Ibelieve and Tbelieve.
Because the independent variables are highly correlated I have examined the
squared semi-partial correlations. I
understand that the Ibelieve squared semi partial correlation accounts for
a significant proportion of the variance in CE t= 2.689, whereas the
tbelieve does not t = .556. However, I want to see if the squared
semi-partial correlations are significantly different TO EACH OTHER
(Ibelieve .168, Tbelieve .035 N= 220). How do I do this?  I assume that I
will need to do this "by hand" with an equation? Also, do I have to take
into account that the squared semi-partial correlations are correlated in
terms of the dependent variable? 
Thanks very much.
Nichola


-- 
http://www.shes.bangor.ac.uk/home.html
------------ 
Dr Nichola Callow (C.Psychol.)
School of Sport, Health, and Exercise Science
University of Wales, Bangor
George Building
Holyhead Road
Bangor
Gwynedd LL57 2PX
Tel: 44 (0) 1248 388243
Fax: 44 (0) 1248 371053
E-Mail: n.callow at bangor.ac.uk



From HDoran at air.org  Fri Aug  6 13:16:23 2004
From: HDoran at air.org (Doran, Harold)
Date: Fri, 6 Aug 2004 07:16:23 -0400
Subject: [R] Comparing rows in a dataframe
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7404044CD8@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040806/e1413a9e/attachment.pl

From joseclaudio.faria at terra.com.br  Fri Aug  6 13:16:10 2004
From: joseclaudio.faria at terra.com.br (=?Windows-1252?Q?Jos=E9_Cl=E1udio_Faria?=)
Date: Fri, 6 Aug 2004 08:16:10 -0300
Subject: [R] Script editors: Tinn-R as a new option under windows
Message-ID: <004301c47ba6$c9c3daa0$03fea8c0@sapetinga>

Tinn-R

The Tinn is an editor in development with the Delphi 5 of the Borland,
addressed to programmers, of free code under GPL (GNU Public License),
developed under the operational system Windows.

The Tinn-R version contains enhancements to allow syntax highlighting
of S language (in *.R, *.r, *.Q or *.q files), that is the language
used by the free statistical environment R (http://www.r-project.org).

It also pops up an additional menu and toolbar when it detects either
Rgui, or SciViews-R (see http://www.sciviews.org/SciViews-R)
running on the same computer (see Tinn-R.pdf atacched).

These add-ons interact with the R console and allow it to submit a code
in part or in whole and to control R directly.
So far the adaptation was tested on Windows XP (used in development)
Windows 95/98 and 2000.

Tinn-R is small, free, simple and efficient. It replaces
the basic code editor which is provided by Rgui or SciViews-R.

The distributed zip file (Tinn-R version 0.0.8.8 r1.04.zip)
must be unzipped first. It contains a 'ReadMe.pdf' file that
explains how to use Tinn-R in details, as well as a couple of
example scripts in addition to 'Tinn-R.exe'.

There is file to install for Tinn-R. The whole program is contained
in a small 'Tinn-R.exe' file (1.5 Mb). Just place this file where you want
to on your computer disk and run it from there... that's all!

Source code of the program is available upon request.

A page for Tinn-R to download is on the SciViews Web Site at
http://www.sciviews.org/Tinn-R and an entry in
http://www.r-project.org/GUI in the section
'IDE/script Editors'.

Best regards,


Jos Cludio Faria
UESC/DCET
Brasil
73-634.2779
joseclaudio.faria at terra.com.br
jc_faria at uol.com.br



From andy_liaw at merck.com  Fri Aug  6 13:53:18 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 6 Aug 2004 07:53:18 -0400
Subject: [R] Comparing rows in a dataframe
Message-ID: <3A822319EB35174CA3714066D590DCD504AF81B1@usrymx25.merck.com>

I would sort the data frame by the grouping variables and order of the
scores, then use tapply and diff to compute the successive differences by
groups.  You may want to pad the output by a zero to make it the same length
as the original scores.

Andy

> From: Doran, Harold
> 
> Hello
>  
> I have a longitudinal dataframe organized in the long format 
> and would like to make comparison between successive rows if 
> certain conditions apply. Specifically, I have four variables 
> of interest: grade, score, year, and schid, associated with 
> each school with 3 measurements per school per grade, 
> therefore the rows are temporally ordered and each school 
> occupies multiple rows. For example, a school may have grades 
> 4,5,6 and three observations per grade at years 2001-2003 and 
> therefore occupies 9 rows.
>  
> So, my goal is to make a comparison between score 1 and 2 
> (same column, next row) if the score is from the same school, 
> same grade, but one year later and create a new binary 
> variable if score 1 < score 2.
>  
> Something like, ifelse(data[x,4] with data[x,4], if 
> schid=schid and grade = grade, and year = year + 1, 1, 0)
>  
> So, I will end up with a new variable in the data frame where 
> 1 indicates that subsequent score is larger than the score 
> for the same grade in the same school in year t-1.
>  
> I'm a little confused how conditional selection would work in 
> such a scenario. 
>  
> Many thanks,
>  
> Harold
>  
>  
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From maechler at stat.math.ethz.ch  Fri Aug  6 14:11:12 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 6 Aug 2004 14:11:12 +0200
Subject: [R] squared semi-partial correlation
In-Reply-To: <411361E0.BA237FCF@bangor.ac.uk>
References: <411361E0.BA237FCF@bangor.ac.uk>
Message-ID: <16659.30048.542903.564790@gargle.gargle.HOWL>

>>>>> "N" == N Callow <pes603 at bangor.ac.uk>
>>>>>     on Fri, 06 Aug 2004 11:48:00 +0100 writes:

    N> Hi, can you help? I am really struggling to find the
    N> answer to the following question.

    N> I have conducted a standard multiple regression using SPSS.

so why do you ask here?

R-help is for questions/problems/remarks about (or at least
strongly related to) R.

    N>  <..............>

    .....................
    N> PLEASE do read the posting guide!
    N>	      http://www.R-project.org/posting-guide.html

Indeed, please do!



From andy_liaw at merck.com  Fri Aug  6 14:11:25 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 6 Aug 2004 08:11:25 -0400
Subject: [R] squared semi-partial correlation
Message-ID: <3A822319EB35174CA3714066D590DCD504AF81B3@usrymx25.merck.com>

Please, this is a mailing list about R, not SPSS.  And what exact is
`semi-partial correlation'?

Andy

> From: N.Callow
> 
> Hi, can you help? I am really struggling to find the answer to the
> following question.
> 
> I have conducted a standard multiple regression using SPSS. 
> The dependent
> variable is CE, and the independent variables are Ibelieve 
> and Tbelieve.
> Because the independent variables are highly correlated I 
> have examined the
> squared semi-partial correlations. I
> understand that the Ibelieve squared semi partial correlation 
> accounts for
> a significant proportion of the variance in CE t= 2.689, whereas the
> tbelieve does not t = .556. However, I want to see if the squared
> semi-partial correlations are significantly different TO EACH OTHER
> (Ibelieve .168, Tbelieve .035 N= 220). How do I do this?  I 
> assume that I
> will need to do this "by hand" with an equation? Also, do I 
> have to take
> into account that the squared semi-partial correlations are 
> correlated in
> terms of the dependent variable? 
> Thanks very much.
> Nichola
> 
> 
> -- 
> http://www.shes.bangor.ac.uk/home.html
> ------------ 
> Dr Nichola Callow (C.Psychol.)
> School of Sport, Health, and Exercise Science
> University of Wales, Bangor
> George Building
> Holyhead Road
> Bangor
> Gwynedd LL57 2PX
> Tel: 44 (0) 1248 388243
> Fax: 44 (0) 1248 371053
> E-Mail: n.callow at bangor.ac.uk
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From dieter.menne at menne-biomed.de  Fri Aug  6 14:42:11 2004
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 6 Aug 2004 14:42:11 +0200
Subject: [R] Simple Lookup... why so slow
Message-ID: <INEGIMHGODBGKFPOJBBMGEAECCAA.dieter.menne@menne-biomed.de>

Dear List,

At 32 degrees Celsius in the office, I was too lazy to figure out
the correct xapplytion for a simple lookup problem
and regressed to well-known c-style. Only to see my
computer hang forever doing 10000 indexed offset calculation.
Boiled down, the problem is shown below; needs a few milliseconds
in c. Looking at the timing results of n=2000 and n=4000,
this is not linear in time, so something I don't understand
must go on.

And, just as an aside: why is $-indexing so much faster (!)
than numeric indexing?

Dieter

(all on Windows, latest R-Version)
----

# Generate Data set
StartDay = matrix(as.integer(runif(80)*20),nrow=4)
n=4000
PatDay = data.frame(Day = as.integer(runif(n)*20)+50,
                       Pat= as.integer(runif(n)*20)+1,
                       Treat = as.integer(runif(n)*4)+1,
                       DayOff=NA) # reserve output space
# Correct for days offset
ti= system.time(
  for (i in 1:n)
    PatDay$DayOff[i] = PatDay$Day[i]-StartDay[PatDay$Treat[i],PatDay$Pat[i]]
  )
cat("$Style index",n,ti[3],"\n");
# n= 2000 3 seconds
# n= 4000 15 seconds

# I first believed using numeric indexes could be faster...
ti= system.time(
  for (i in 1:n)
    PatDay[i,4] = PatDay[i,1]-StartDay[PatDay[i,3],PatDay[i,2]]
  )
cat("Numeric index", n,ti[3],"\n");
# n=2000 12 seconds
# n=4000 53 seconds



From dtrenkler at nts6.oec.uni-osnabrueck.de  Fri Aug  6 15:13:18 2004
From: dtrenkler at nts6.oec.uni-osnabrueck.de (Trenkler, Dietrich)
Date: Fri, 6 Aug 2004 15:13:18 +0200 
Subject: [R] Bug in qnorm or pnorm?
Message-ID: <FB75CFC167F3D311B11D00A0CC20FB0E88554F@nts7.oec.Uni-Osnabrueck.DE>

I found the following strange behavior  using qnorm() and pnorm():

> x<-8.21;x-qnorm(pnorm(x))
[1] 0.0004638484
> x<-8.22;x-qnorm(pnorm(x))
[1] 0.01046385
> x<-8.23;x-qnorm(pnorm(x))
[1] 0.02046385
> x<-8.24;x-qnorm(pnorm(x))
[1] 0.03046385
> x<-8.25;x-qnorm(pnorm(x))
[1] 0.04046385
> x<-8.26;x-qnorm(pnorm(x))
[1] 0.05046385
> x<-8.27;x-qnorm(pnorm(x))
[1] 0.06046385
> x<-8.28;x-qnorm(pnorm(x))
[1] 0.07046385
> x<-8.29;x-qnorm(pnorm(x))
[1] 0.08046385
> x<-8.30;x-qnorm(pnorm(x))
[1] -Inf


Given that pnorm(8.30) delivers 1 shouldn't we get Inf 
for  x<-8.30;x-qnorm(pnorm(x)) ?

Thanks in advance.

Dietrich Trenkler

--
Dietrich Trenkler   Universit??t Osnabr??ck                                  
FB Wirtschaftswissenschaften           
Rolandstr.8              D-49069 Osnabr??ck

dtrenkler at nts6.oec.uni-osnabrueck.de



From deepayan at stat.wisc.edu  Fri Aug  6 15:22:47 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Fri, 6 Aug 2004 08:22:47 -0500
Subject: [R] Lattice: how to index in a custom panel function?
In-Reply-To: <41134C5F.6010405@cropdesign.com>
References: <41134C5F.6010405@cropdesign.com>
Message-ID: <200408060822.47805.deepayan@stat.wisc.edu>

On Friday 06 August 2004 04:16, Joris DeWolf wrote:
> Hi,
>
> I have a lattice xyplot that contains panels according to FactorA,
> and curves for the 2 levels of Factor B within a panel.
> I try to add text in the panels of a lattice graph. I suppose I have
> to write a custom function (panel.txt).
> What I really would like is to adapt the text in the panel according
> to the levels of FactorA.
> In the manuals, I find examples for the strips using which.given and
> which.panel. But this does not work for the panels...
>
> Can Anybody give a hint? Thanks
> Joris
>
> I work with R 1.9.1 under Linux
>
>
> panel.txt = function(sometxt,x,y,...){
>     grid.text(sometxt,x,y)
> }
>
> xyplot(data = Pdata, P ~ DAS | FactorA,
>    groups = FactorB,
>    type ="s",
>    col = c("red","blue"),
>   panel = function(x,y,...){
>         panel.abline(h = 100, lty = 5, lwd =0.5, col = "darkgrey")
>         panel.txt(mytext, 0.2, 0.8)
> }

As described in ?xyplot under the entry for 'panel', your panel function 
can have an argument called panel.number which would index the levels 
of the conditioning variable (or combinations thereof if there are more 
than one). e.g., 

  panel = function(x,y, panel.number, ...){
      cat(paste("In panel", panel.number, "\n"))
      panel.abline(h = 100, lty = 5, lwd =0.5, col = "darkgrey")
      panel.txt(mytext, 0.2, 0.8)
  }

Deepayan



From dtrenkler at nts6.oec.uni-osnabrueck.de  Fri Aug  6 15:30:34 2004
From: dtrenkler at nts6.oec.uni-osnabrueck.de (Trenkler, Dietrich)
Date: Fri, 6 Aug 2004 15:30:34 +0200 
Subject: [R] Bug in qnorm or pnorm?
Message-ID: <FB75CFC167F3D311B11D00A0CC20FB0E885550@nts7.oec.Uni-Osnabrueck.DE>



> -----Original Message-----
> From:	Trenkler, Dietrich 
> Sent:	Friday, August 06, 2004 3:13 PM
> To:	'r-help'
> Subject:	[R] Bug in qnorm or pnorm?
> 
> I found the following strange behavior  using qnorm() and pnorm():
> 
> > x<-8.21;x-qnorm(pnorm(x))
> [1] 0.0004638484
> > x<-8.22;x-qnorm(pnorm(x))
> [1] 0.01046385
> > x<-8.23;x-qnorm(pnorm(x))
> [1] 0.02046385
> > x<-8.24;x-qnorm(pnorm(x))
> [1] 0.03046385
> > x<-8.25;x-qnorm(pnorm(x))
> [1] 0.04046385
> > x<-8.26;x-qnorm(pnorm(x))
> [1] 0.05046385
> > x<-8.27;x-qnorm(pnorm(x))
> [1] 0.06046385
> > x<-8.28;x-qnorm(pnorm(x))
> [1] 0.07046385
> > x<-8.29;x-qnorm(pnorm(x))
> [1] 0.08046385
> > x<-8.30;x-qnorm(pnorm(x))
> [1] -Inf
> 
> 
> Given that pnorm(8.30) delivers 1 shouldn't we get Inf 
> for  x<-8.30;x-qnorm(pnorm(x)) ?
> 
> Thanks in advance.
> 
	[Dietrich Trenkler]  Oops, forgot to mention:

	> unlist(R.Version())
	         platform              arch                os
system 
	"i386-pc-mingw32"            "i386"         "mingw32"   "i386,
mingw32" 
	           status             major             minor
year 
	               ""               "1"             "9.1"
"2004" 
	            month               day          language 
	             "06"              "21"               "R" 

	D. Trenkler



From deepayan at stat.wisc.edu  Fri Aug  6 15:31:00 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Fri, 6 Aug 2004 08:31:00 -0500
Subject: [R] Bug in qnorm or pnorm?
In-Reply-To: <FB75CFC167F3D311B11D00A0CC20FB0E88554F@nts7.oec.Uni-Osnabrueck.DE>
References: <FB75CFC167F3D311B11D00A0CC20FB0E88554F@nts7.oec.Uni-Osnabrueck.DE>
Message-ID: <200408060831.00186.deepayan@stat.wisc.edu>

On Friday 06 August 2004 08:13, Trenkler, Dietrich wrote:

> Given that pnorm(8.30) delivers 1 shouldn't we get Inf
> for  x<-8.30;x-qnorm(pnorm(x)) ?

Why? 

> pnorm(8.30)
[1] 1
> qnorm(pnorm(8.30)) ## same as qnorm(1)
[1] Inf
> 8.30 - qnorm(pnorm(8.30)) ## same as 8.30 - Inf
[1] -Inf

This seems perfectly acceptable to me for all reasonable definitions of 
Inf.

Deepayan



From ramasamy at cancer.org.uk  Fri Aug  6 15:32:10 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Fri, 06 Aug 2004 14:32:10 +0100
Subject: [R] Elementary questions about data structures
In-Reply-To: <20040806104740.GM31360@igidr.ac.in>
References: <20040806104740.GM31360@igidr.ac.in>
Message-ID: <1091799130.3500.42.camel@localhost.localdomain>

You will have to sort out the list before assigning. I am not sure if
there is an auto-sort ability in any of the function. Try this :

list1 <- list(x=2, y=3)
list2 <- list(y=7, x=8)

out <- data.frame( matrix(NA, nc=2, nr=5) )
colnames(out) <- c("x", "y")

out[1, ] <- unlist( list1[ colnames(out) ] )
out[2, ] <- unlist( list2[ colnames(out) ] )
out
   x  y
1  2  3
2  8  7
3 NA NA
4 NA NA
5 NA NA

If you are expecting one single named variable in each case, then
unlist(list1)[ colnames(out) ] will also give you the same answer.

Please do use "<-" as the assignment operator instead of "=" which is
used in named arguments in function. Also note that it is difficult to
distinguish "S = l1" and "S = 11"

If you know the total number of rows, it is better to pre-allocate a
matrix/dataframe than to grow it on the fly. If all your data is
numeric, use a matrix instead.


On Fri, 2004-08-06 at 11:47, Ajay Shah wrote:
> Folks,
> 
> S_t = (x_t, y_t) is the state of a system at time t. There is an
> iterative function which converts S_t into S_{t+1}. I can easily write
> this in R like this:
> 
>        iterate <- function(S) {
>          list(S$x+1, S$y+1)
>        }
> 
> So this function eats S_t and makes S_{t+1} and I can say
> S2 <- iterate(S1)
> 
> My question: suppose I want to iterate from 1..10, what is the data
> structure that is appropriate to store all these lists?
> 
> How, in R, does one make "vectors of lists"? I want to think of the
> state vector at time t as a "record" and then I want to have a
> vectorfull of them. When I'm done, I want to be able to make pictures
> of the time-series of S$x and the time-series of S$y. How is this
> done?
> 
> I tried some things:
> 
> > l1=list(x=2,y=3)
> > l2=list(y=7,x=8)              # <--  note the order
> > S = data.frame(x=1,y=1)       # odd way of initialising initial state
> > S[2,] = l1
> > S[3,] = l2
> > S
>   x y
> 1 1 1
> 2 2 3
> 3 7 8                           # <-- note it came out wrong.
> 
> I was very puzzled that he was wrong in how he handled the l2
> assignment. How is it that the order matters? I thought lists were
> handled in a more abstract way, where R knows that l2 is a collection
> of l2$x and l2$y, without concern about the order in which they came
> in. I had hoped that he would see that in S, there is an x and a y,
> and that he would marry things up correctly. He doesn't.
> 
> What I would like to do is something like this:
> 
> S[1,] = list(x=1,y=2)                     # line 1
> for (i in 2:10) {
>   S[i,] = iterate(S[(i-1),])
> }
> 
> I get errors at line 1 saying Error: Object "S" not found
> 
> If this could work, I know I'd be able to happily deal with the
> time-series vector S$x and the time-series vector S$y.
> 
> Thanks a lot,



From Luisr at frs.fo  Fri Aug  6 15:43:42 2004
From: Luisr at frs.fo (Luis Rideau Cruz)
Date: Fri, 06 Aug 2004 14:43:42 +0100
Subject: [R] list of frames without first element
Message-ID: <s1139924.023@ffdata.setur.fo>

R-help,

I have a list of several data frames.

I want to compute the "rowSums" of the columns of these data frames but  first one.

Something like this 

                    lapply(my.list,rowSums)

Thank you


Luis Ridao Cruz
Fiskiranns??knarstovan
N??at??n 1
P.O. Box 3051
FR-110 T??rshavn
Faroe Islands
Phone:             +298 353900
Phone(direct): +298 353912
Mobile:             +298 580800
Fax:                 +298 353901
E-mail:              luisr at frs.fo
Web:                www.frs.fo



From dtrenkler at nts6.oec.uni-osnabrueck.de  Fri Aug  6 15:51:29 2004
From: dtrenkler at nts6.oec.uni-osnabrueck.de (Trenkler, Dietrich)
Date: Fri, 6 Aug 2004 15:51:29 +0200 
Subject: [R] Bug in qnorm or pnorm?
Message-ID: <FB75CFC167F3D311B11D00A0CC20FB0E885551@nts7.oec.Uni-Osnabrueck.DE>



> -----Original Message-----
> From:	Deepayan Sarkar 
> Sent:	Friday, August 06, 2004 3:31 PM
> To:	r-help at stat.math.ethz.ch
> Subject:	Re: [R] Bug in qnorm or pnorm?
> 
> On Friday 06 August 2004 08:13, Trenkler, Dietrich wrote:
> 
> > Given that pnorm(8.30) delivers 1 shouldn't we get Inf
> > for  x<-8.30;x-qnorm(pnorm(x)) ?
> 
> Why? 
> 
> > pnorm(8.30)
> [1] 1
> > qnorm(pnorm(8.30)) ## same as qnorm(1)
> [1] Inf
> > 8.30 - qnorm(pnorm(8.30)) ## same as 8.30 - Inf
> [1] -Inf
> 
> This seems perfectly acceptable to me for all reasonable definitions of 
> Inf.
> 
> Deepayan
> 
> 
	[Dietrich Trenkler]  Yes of course, you're right. I meant 
	qnorm(pnorm(8.3))  should deliver Inf -- as it does.  
	Thank you.

	Dietrich.



From andy_liaw at merck.com  Fri Aug  6 16:09:17 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 6 Aug 2004 10:09:17 -0400
Subject: [R] list of frames without first element
Message-ID: <3A822319EB35174CA3714066D590DCD504AF81B7@usrymx25.merck.com>

> From: Luis Rideau Cruz
> 
> R-help,
> 
> I have a list of several data frames.
> 
> I want to compute the "rowSums" of the columns of these data 
> frames but  first one.
> 
> Something like this 
> 
>                     lapply(my.list,rowSums)

You're almost there:

 lapply(my.list, function(x) rowSums(x[-1]))

Andy

 
> Thank you
> 
> 
> Luis Ridao Cruz
> Fiskiranns??knarstovan
> N??at??n 1
> P.O. Box 3051
> FR-110 T??rshavn
> Faroe Islands
> Phone:             +298 353900
> Phone(direct): +298 353912
> Mobile:             +298 580800
> Fax:                 +298 353901
> E-mail:              luisr at frs.fo
> Web:                www.frs.fo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ligges at statistik.uni-dortmund.de  Fri Aug  6 16:16:37 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 06 Aug 2004 16:16:37 +0200
Subject: [R] list of frames without first element
In-Reply-To: <s1139924.023@ffdata.setur.fo>
References: <s1139924.023@ffdata.setur.fo>
Message-ID: <411392C5.5080308@statistik.uni-dortmund.de>

Luis Rideau Cruz wrote:

> R-help,
> 
> I have a list of several data frames.
> 
> I want to compute the "rowSums" of the columns of these data frames but  first one.

... "but first data.frame" or "but first column"?

"but first data.frame":
  lapply(my.list[-1], rowSums)

"but first column":
  lapply(my.list, function(x) rowSums(x[,-1]))

Uwe Ligges


> 
> Something like this 
> 
>                     lapply(my.list,rowSums)
> 
> Thank you
> 
> 
> Luis Ridao Cruz
> Fiskiranns??knarstovan
> N??at??n 1
> P.O. Box 3051
> FR-110 T??rshavn
> Faroe Islands
> Phone:             +298 353900
> Phone(direct): +298 353912
> Mobile:             +298 580800
> Fax:                 +298 353901
> E-mail:              luisr at frs.fo
> Web:                www.frs.fo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From informatics at myhelios.net  Fri Aug  6 16:25:15 2004
From: informatics at myhelios.net (David L. Van Brunt, Ph.D.)
Date: Fri, 6 Aug 2004 09:25:15 -0500
Subject: [R] Using pipe for input data
In-Reply-To: <Pine.LNX.4.44.0408060713500.22854-100000@gannet.stats>
References: <Pine.LNX.4.44.0408060713500.22854-100000@gannet.stats>
Message-ID: <6F149C95-E7B4-11D8-810D-000393B2A94A@myhelios.net>

I've been watching with interest on this one. I've had problems with 
looping a randomForest procedure within R, and despite heroic efforts 
by the R developers this problem hasn't been solved.

I was thinking that if I could loop through the data extraction (from a 
MySQL) in the system, then call R inside the loop with the new data, 
and append a file with the results.

Does this seem feasible?

On Aug 6, 2004, at 1:22 AM, Prof Brian Ripley wrote:

> The C stdin is used *always* to read commands from on Unix R, and even 
> on
> console versions stdin() is where the commands are read from.
>
> R CMD BATCH is approximately giving you
>
> R --vanilla --slave < my.R
>
> and piping to such a command is going to do nothing for you.
> Your command read.table(stdin() ... is going to read from the script 
> my.R.
>
> On Thu, 5 Aug 2004, Hayashi Soichi - shayas wrote:
>
>> I have asked this question before and Aaron J. Mackey and Tony Plate 
>> gave me
>> some great insight but I still can't figure out how to do what I am 
>> trying
>> to accomplish. So let me ask again...
>>
>> What I am trying to do is to make R read data from pipe (stdin).
>>
>> Say I have following files on my directory
>>
>> my.dat
>>        apple 1
>>        orange 2
>>        grape 3
>>
>> my.R
>>        d <- read.table( stdin(), header=F, dec='.',
>> col.names=c("name","type"), na.strings=c("xxxx"))
>>        summary(d)
>>
>> and When I run this command
>>
>>        cat my.dat | R CMD BATCH --vanilla --slave my.R
>>
>> I am expecting to see the summery report for the datasource my.dat
>>
>> But here is what I actually see in my.Rout
>> 	> d <- read.table( stdin(), header=F, dec='.',
>> col.names=c("name","type"), na.strings=c("xxxx"))
>> 	0: summary(d)
>> 	1: proc.time()
>> 	2: Error in scan(file = file, what = what, sep = sep, quote = quote,
>> dec = dec,  :
>> 	line 1 did not have 2 elements
>> 	Execution halted
>>
>> If I execute the content of the my.R on regular R command line, I can
>> actually "type in" all datasource and creates the correct summery 
>> report. So
>> I don't know why I can make R to read the input from the piped 
>> datasource...
>
> See the above analysis.  What I don't know is why you expected this to
> work: did you look at the sources, e.g. the file BATCH?  If not - 
> `great
> insight' - the sources are the definitive documentation.
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From dieter.menne at menne-biomed.de  Fri Aug  6 16:06:23 2004
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 6 Aug 2004 16:06:23 +0200
Subject: [R] Re: Simple Lookup... why so slow
Message-ID: <INEGIMHGODBGKFPOJBBMGEAFCCAA.dieter.menne@menne-biomed.de>


Ok, found it out. Things are really speedy when you first store result in a
vector, and cbind the vector to the data frame later.

Assuming that copying is involved, this would explain to me that my first
approach was so much slower, but I don't understand why time goes up more
than linearily with n.

Dieter

--

# Generate Data set
StartDay = matrix(as.integer(runif(80)*20),nrow=4)
n=8000
PatDay = data.frame(Day = as.integer(runif(n)*20)+50,
                       Pat= as.integer(runif(n)*20)+1,
                       Treat = as.integer(runif(n)*4)+1
                       )
DayOff = rep(NA,n)
# Correct for days offset
ti= system.time(
  for (i in 1:n)
# bad
#    PatDay$DayOff[i] =
PatDay$Day[i]-StartDay[PatDay$Treat[i],PatDay$Pat[i]]
# good
    DayOff[i] = PatDay$Day[i]-StartDay[PatDay$Treat[i],PatDay$Pat[i]]
  )
PatDay$DayOff = DayOff
cat("Separate Vector first",n,ti[3],"\n");
# n= 4000 0.43 seconds



From tlumley at u.washington.edu  Fri Aug  6 16:46:45 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 6 Aug 2004 07:46:45 -0700 (PDT)
Subject: [R] Re: Simple Lookup... why so slow
In-Reply-To: <INEGIMHGODBGKFPOJBBMGEAFCCAA.dieter.menne@menne-biomed.de>
References: <INEGIMHGODBGKFPOJBBMGEAFCCAA.dieter.menne@menne-biomed.de>
Message-ID: <Pine.A41.4.58.0408060744170.157758@homer03.u.washington.edu>

On Fri, 6 Aug 2004, Dieter Menne wrote:

>
> Ok, found it out. Things are really speedy when you first store result in a
> vector, and cbind the vector to the data frame later.
>
> Assuming that copying is involved, this would explain to me that my first
> approach was so much slower, but I don't understand why time goes up more
> than linearily with n.
>

The garbage collection overhead and then the overhead of paging to disk
will often make the time nonlinear for problems that use a lot of memory.

	-thomas



From l.houdusse at cerep.fr  Fri Aug  6 16:51:19 2004
From: l.houdusse at cerep.fr (Laurent Houdusse)
Date: Fri, 6 Aug 2004 16:51:19 +0200 
Subject: [R] hmtest object
Message-ID: <BA420EFAAC96D311A7A0006097D37BDB04515CC9@EOLE>

Hi!

Could you tell me what is p.value.raw in a hmtest object?
And what are the others properties of hmtest object

Thanks



From ramasamy at cancer.org.uk  Fri Aug  6 16:45:03 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Fri, 06 Aug 2004 15:45:03 +0100
Subject: [R] Simple Lookup... why so slow
In-Reply-To: <INEGIMHGODBGKFPOJBBMGEAECCAA.dieter.menne@menne-biomed.de>
References: <INEGIMHGODBGKFPOJBBMGEAECCAA.dieter.menne@menne-biomed.de>
Message-ID: <1091803503.3500.126.camel@localhost.localdomain>

The first 2 solutions are vastly slower than the last 3 simply because
they use the for() loop. The vectorised versions are definitely faster.

# Solution 1 : list extraction operator
aa <- rep(NA, n); bb <- rep(NA, n)

system.time( for (i in 1:n) {
  aa[i] <- PatDay$Day[i] - StartDay[PatDay$Treat[i], PatDay$Pat[i]] } )
[1] 0.33 0.00 0.33 0.00 0.00

# Solution 2 : numeric index with for loop
system.time( for (i in 1:n){ 
   bb[i] <-  PatDay[i,1]-StartDay[PatDay[i,3],PatDay[i,2]] } )
[1] 15.43  0.12 17.76  0.00  0.00


# Solution 3 : Vectorised operation with numeric index
system.time( cc <- PatDay[ , 1] - StartDay[ as.matrix(PatDay[, 3:2]) ] )
[1] 0.01 0.00 0.01 0.00 0.00

# Solution 4 : Vectorised operation with named index
> system.time( dd <- PatDay[ , "Day"] - StartDay[ as.matrix(PatDay[,
c("Treat", "Pat")]) ] )
[1] 0.01 0.00 0.01 0.00 0.00

# Solution 5 : Vectorised operation with list extractor
system.time( ee <- PatDay$Day - StartDay[ cbind(PatDay$Treat,PatDay$Pat)
] )
[1] 0 0 0 0 0


There is insufficient precision to say which of the parameterised
operation is faster. So I tried the same thing with n=400,000 and the
last 3 gave the following timing

Solution 3 : [1] 1.67 0.21 1.89 0.00 0.00
Solution 4 : [1] 2.55 0.21 2.77 0.00 0.00
Solution 5 : [1] 0.25 0.03 0.28 0.00 0.00

However, when I redefined PatDay as matrix, for n=400,000

Solution 3 : [1] 0.48 0.04 0.51 0.00 0.00
Solution 4 : [1] 0.26 0.04 0.31 0.00 0.00


Just to make sure all the answer are the same, try this

cor( cbind(aa, bb, cc, dd) )
   aa bb cc dd
aa  1  1  1  1
bb  1  1  1  1
cc  1  1  1  1
dd  1  1  1  1

or the slow way : all.equal(aa, bb); all.equal(aa, cc); ...

Regards, Adai


On Fri, 2004-08-06 at 13:42, Dieter Menne wrote:
> Dear List,
> 
> At 32 degrees Celsius in the office, I was too lazy to figure out
> the correct xapplytion for a simple lookup problem
> and regressed to well-known c-style. Only to see my
> computer hang forever doing 10000 indexed offset calculation.
> Boiled down, the problem is shown below; needs a few milliseconds
> in c. Looking at the timing results of n=2000 and n=4000,
> this is not linear in time, so something I don't understand
> must go on.
> 
> And, just as an aside: why is $-indexing so much faster (!)
> than numeric indexing?
> 
> Dieter
> 
> (all on Windows, latest R-Version)
> ----
> 
> # Generate Data set
> StartDay = matrix(as.integer(runif(80)*20),nrow=4)
> n=4000
> PatDay = data.frame(Day = as.integer(runif(n)*20)+50,
>                        Pat= as.integer(runif(n)*20)+1,
>                        Treat = as.integer(runif(n)*4)+1,
>                        DayOff=NA) # reserve output space
> # Correct for days offset
> ti= system.time(
>   for (i in 1:n)
>     PatDay$DayOff[i] = PatDay$Day[i]-StartDay[PatDay$Treat[i],PatDay$Pat[i]]
>   )
> cat("$Style index",n,ti[3],"\n");
> # n= 2000 3 seconds
> # n= 4000 15 seconds
> 
> # I first believed using numeric indexes could be faster...
> ti= system.time(
>   for (i in 1:n)
>     PatDay[i,4] = PatDay[i,1]-StartDay[PatDay[i,3],PatDay[i,2]]
>   )
> cat("Numeric index", n,ti[3],"\n");
> # n=2000 12 seconds
> # n=4000 53 seconds
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From dieter.menne at menne-biomed.de  Fri Aug  6 17:10:31 2004
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 6 Aug 2004 17:10:31 +0200
Subject: [R] Simple Lookup... why so slow
In-Reply-To: <1091803503.3500.126.camel@localhost.localdomain>
Message-ID: <INEGIMHGODBGKFPOJBBMAEAGCCAA.dieter.menne@menne-biomed.de>

Adaikalavan,

thanks for your phantastic summary. Solution 5 is the version I was looking
for, but I left out the cbind in good old c-fashion.

> # Solution 5 : Vectorised operation with list extractor
> system.time( ee <- PatDay$Day - StartDay[ cbind(PatDay$Treat,PatDay$Pat)


Dieter



From johannesson1 at llnl.gov  Fri Aug  6 17:17:34 2004
From: johannesson1 at llnl.gov (Gardar Johannesson)
Date: Fri, 06 Aug 2004 08:17:34 -0700
Subject: [R] Re: Simple Lookup... why so slow
In-Reply-To: <INEGIMHGODBGKFPOJBBMGEAFCCAA.dieter.menne@menne-biomed.de>
Message-ID: <5.2.0.9.2.20040806081343.02783ae8@poptop.llnl.gov>

On other notes, try to leverage R's vector capabilities when 
possible.  Your DayOff variable can also be computed via

DayOff = PatDay$Day - StartDay[cbind(PatDay$Treat,PatDay$Pat)]


Gardar

At 04:06 PM 8/6/2004 +0200, Dieter Menne wrote:

>Ok, found it out. Things are really speedy when you first store result in a
>vector, and cbind the vector to the data frame later.
>
>Assuming that copying is involved, this would explain to me that my first
>approach was so much slower, but I don't understand why time goes up more
>than linearily with n.
>
>Dieter
>
>--
>
># Generate Data set
>StartDay = matrix(as.integer(runif(80)*20),nrow=4)
>n=8000
>PatDay = data.frame(Day = as.integer(runif(n)*20)+50,
>                        Pat= as.integer(runif(n)*20)+1,
>                        Treat = as.integer(runif(n)*4)+1
>                        )
>DayOff = rep(NA,n)
># Correct for days offset
>ti= system.time(
>   for (i in 1:n)
># bad
>#    PatDay$DayOff[i] =
>PatDay$Day[i]-StartDay[PatDay$Treat[i],PatDay$Pat[i]]
># good
>     DayOff[i] = PatDay$Day[i]-StartDay[PatDay$Treat[i],PatDay$Pat[i]]
>   )
>PatDay$DayOff = DayOff
>cat("Separate Vector first",n,ti[3],"\n");
># n= 4000 0.43 seconds
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From HDoran at air.org  Fri Aug  6 17:18:44 2004
From: HDoran at air.org (Doran, Harold)
Date: Fri, 6 Aug 2004 11:18:44 -0400
Subject: [R] reshape (was: Comparing rows in a dataframe)
Message-ID: <88EAF3512A55DF46B06B1954AEF73F74047C00FF@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040806/967c2197/attachment.pl

From hec.villafuerte at telgua.com.gt  Fri Aug  6 19:38:42 2004
From: hec.villafuerte at telgua.com.gt (Hector Villafuerte)
Date: Fri, 06 Aug 2004 09:38:42 -0800
Subject: [R] R interface to Python (in Windows) [SOLVED]
In-Reply-To: <76A000A82289D411952F001083F9DD06047FE81F@exsalem4-bu.odot.state.or.us>
References: <76A000A82289D411952F001083F9DD06047FE81F@exsalem4-bu.odot.state.or.us>
Message-ID: <4113C222.5090200@telgua.com.gt>

Thanks Ben and to you all!
I've installed R1.8.1 and RPY works just fine now.
I'm planning on migrating to Linux in a couple of months (python 
scripts, mysql dbs, among other things...)
I'm eager to add R features to my analysis.
Thanks again!
Hector


Benjamin.STABLER at odot.state.or.us wrote:

>I put a copy of R 1.8.1 for Windows on our FTP site at:
>ftp://ftp.odot.state.or.us/outgoing/Test/.  It should be there for a few
>days before it gets deleted.  
>
>
>Benjamin Stabler
>Transportation Planning Analysis Unit
>Oregon Department of Transportation
>555 13th Street NE, Suite 2
>Salem, OR 97301  Ph: 503-986-4104
>
>-----Original Message-----
>From: Peter Wilkinson [mailto:pwilkinson at videotron.ca] 
>Sent: Thursday, August 05, 2004 1:00 PM
>To: Hector Villafuerte; tutor at python.org; r-help at stat.math.ethz.ch
>Subject: Re: [R] R interface to Python (in Windows)
>
>
>Hi there,
>
>This is because rpy that is currently available was compiled for R1.8.1, 
>and they have not released a compile for R1.9.1
>
>There is an rpy mailing list where you can post your concern. you can get 
>to it fromt he rpy pages as sourceforge.
>
>You will need to find a copy of R 1.8.1 to use with that version. I will 
>let you know if I find a copy as I have been looking for one myself.
>
>Peter
>
>
>At 04:58 PM 8/5/2004, Hector Villafuerte wrote:
>  
>
>>Hi,
>>I'm trying to install RPY (to interface Python and R).
>>This are the programs installed in my PC:
>>R - 1.9.0
>>Python - 2.3.4
>>pywin32 - 201
>>rpy - 0.3.5 (I'm using the executable version)
>>
>>Everything installs nice, but when I try to use RPY a message box appears 
>>with this cryptic note:
>>"The procedure entry point malloc could not be located in the dynamic link 
>>library R.dll"
>>
>>And then this shows in the interactive session:
>>    
>>
>>>>>import rpy
>>>>>          
>>>>>
>>Traceback (most recent call last):
>>File "<input>", line 1, in ?
>>File "C:\Python23\Lib\site-packages\rpy.py", line 55, in ?
>>import _rpy
>>ImportError: DLL load failed: The specified procedure could not be found.
>>    
>>
>>Any ideas? Thanks in advance!
>>Hector
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>>    
>>
>http://www.R-project.org/posting-guide.html
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
>http://www.R-project.org/posting-guide.html
>
>
>  
>



From peter at fe.up.pt  Fri Aug  6 17:47:50 2004
From: peter at fe.up.pt (Peter Ho)
Date: Fri, 06 Aug 2004 16:47:50 +0100
Subject: [R]: (Lattice): Overlaying more than one trend surface using
	contourplot() and wireframe()
Message-ID: <4113A826.2030503@fe.up.pt>

Hi,

Is there a way to plot more than one trend surface using the functions 
contourplot() and wireframe(). I have found an add=T in contour(), but 
no equivalent argument in contourplot() and wireframe()?

I have taken the example 11-2 (pages 441-451) from Design and analysis 
of experiments (Montgomery  2001, 5th edition) to see if this could be 
done in R. I have managed to plot individual  contours for each response 
after using the surf.ls() and trmat() functions from the Spatial 
package, but have been unable to overlay the contours.   Ideally, it 
would be nice to also choose a different set of colours for shading the 
surface, so that when combing surfaces, we can immediately identify the 
optimum conditions. The final overlaid plot is shown on page 451 (Figure 
11-16) of the book.


Are there alternatve ways to also do this using other pakages ?

The R scipts for this examples can be found at the end of this email.

Thanks

Peter
..........................
################################################################################
g<- c(80,80,90,90,85,85,85,85,85,92.07,77.93,85,85)
h<- c(170,180,170,180,175,175,175,175,175,175,175,182.07,167.93)
Yield <- c(76.5,77.0,78.0,79.5,79.9,80.3,80.0,79.7,79.8,78.4,75.6,78.5,77.0)
viscosity <- c(62,60,66,59,72,69,68,70,71,68,71,58,57)
molwt <- 
c(2940,3470,3680,3890,3480,3200,3410,3290,3500,3360,3020,3630,3150)
###################################################################################
## Fit 2-order Polynomial trend for Yield
Yield.ls <- surf.ls(2, g, h, Yield)
trsurfY<- trmat(Yield.ls,min(g), max(g),min(h), max(h),100)
trsurfY[c("x","y")]<- expand.grid(x=trsurfY$x,y=trsurfY$y)
## Fit 2-order Polynomial trend for viscosity
viscosity.ls <- surf.ls(2, g, h, viscosity)
trsurfV<- trmat(viscosity.ls,min(g), max(g),min(h), max(h),100)
trsurfV[c("x","y")]<- expand.grid(x=trsurfV$x,y=trsurfV$y)
## Fit 1-order Polynomial trend for molecular weight
molwt.ls <- surf.ls(1, g, h, molwt)
trsurfMW<- trmat(molwt.ls,min(g), max(g),min(h), max(h),100)
trsurfMW[c("x","y")]<- expand.grid(x=trsurfMW$x,y=trsurfMW$y)
####################################################################################
### Contourplot for yield
contourplot(z ~ x*y, data = trsurfY,
          cuts = 10, region = TRUE,
          xlab = "Time",
          ylab = "Temperature",
          main = "yield",
          col.regions = trellis.par.get("regions")$col)
### Contourplot for viscosity
contourplot(z ~ x*y, data = trsurfV,
          cuts = 10, region = F,
          xlab = "Time",
          ylab = "Temperature",
          main = "yield",
          col.regions = trellis.par.get("regions")$col)
### Contourplot for molecular weight
contourplot(z ~ x*y, data = trsurfMW,
          cuts = 10, region = TRUE,
          xlab = "Time",
          ylab = "Temperature",
          main = "yield",
          col.regions = trellis.par.get("regions")$col)
##################################################################################



From Roger.Bivand at nhh.no  Fri Aug  6 17:54:53 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 6 Aug 2004 17:54:53 +0200 (CEST)
Subject: [R] import a bitmap image and add it to graphics display
In-Reply-To: <20040805090340.GA3526@s1x.local>
Message-ID: <Pine.LNX.4.44.0408061750050.2522-100000@reclus.nhh.no>

On Thu, 5 Aug 2004, Wolfram Fischer wrote:

> > Hello
> > 
> > Is there a possibility to import and add a bitmap image (png or 
> > similar) to a R graphics display. It would be helpful e.g. to
> > locate positions of points of a scanned map or to add a background
> > to a R graphic.
> > 
> > Wolfram
> 
> I found the package pixmap with the functions
> 	x <- read.pnm( file )
> and
> 	plot(x)
> This plots pnm pictures as R graphics.
> (Other picture formats can be converted to pnm pictures
> by several pnm commands on Linux or by using gimp.)
> 
> Overlaying such a picture with a graph could be done as follows:
>     library(pixmap)
>     example(pixmap)
>     plot(x)
>     m <- 20 ; points( x=m*c(0,1,2), y=m*c(0,2,1), col='red', type='b', lwd=5 )
> 
> There rests a problem with the scale:
> Question: How can I scale a pixmap object?
> 

This is covered in the code for the addlogo function in the pixmap 
package:

> help("addlogo-methods")
> getMethod("addlogo", "pixmap")

which uses the pixmap bounding box and user-supplied coordinates and an 
aspect argument to scale a pixmap object. If the function itself doesn't 
do what you need, it does show the slots you could use directly.

Roger


> Wolfram
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From ce227 at cam.ac.uk  Fri Aug  6 18:25:44 2004
From: ce227 at cam.ac.uk (Cristian Echeverria)
Date: Fri, 6 Aug 2004 12:25:44 -0400
Subject: [R] Generalised least square with binomial errors ?
Message-ID: <001901c47bd2$05c37260$f6256f83@plantsci.cam.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040806/2c7b39ab/attachment.pl

From tlumley at u.washington.edu  Fri Aug  6 18:40:18 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 6 Aug 2004 09:40:18 -0700 (PDT)
Subject: [R] reshape (was: Comparing rows in a dataframe)
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F74047C00FF@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F74047C00FF@dc1ex2.air.org>
Message-ID: <Pine.A41.4.58.0408060937410.204840@homer06.u.washington.edu>

On Fri, 6 Aug 2004, Doran, Harold wrote:

> Hi all:
>
>
>
> I solved the previous stated problem in something of a brute force way
> (but it works). I seem to now be running into one little hiccup using
> reshape. Here is a quick snip of the data in long format:
>
<snip>
>
> Now, I want to reshape this into the wide format. However, when I do I
> get NAs in the stability columns. Here is what I have done and the
> result.
>
> wide<- reshape(tennshort, idvar = c("schid","grade"), timevar = "year",
> v.names="stability", direction="wide")
>

reshape() doesn't support multiple idvars (it isn't documented to).  You
have to do something like

tennshort$ID<-with(tennshort, interaction(schid,grade))
wide<- reshape(tennshort, idvar = "ID", timevar = "year",
 v.names="stability", direction="wide")


This might be a useful extra feature in reshape, as might the ability to
specify multiple time variables (eg year, season)

	-thomas



From darkjacknife at hotmail.com  Fri Aug  6 18:40:43 2004
From: darkjacknife at hotmail.com (angel hellraiser)
Date: Fri, 06 Aug 2004 16:40:43 +0000
Subject: [R] about lme
Message-ID: <BAY22-F21yfiTDJ818S00006c92@hotmail.com>

Hi R-users:

I've got a problem with lme.

In Rail data,I try to model the next one

lme(travel ~ Rail, data = Rail, random = ~ Rail | Rail)

I want travel = Rail(i) + Rail(j) + epsilon(i,j)

say, an effect fixed for every Rail and other for Rail(j) random, R says
fewer observations or false convergence.

Why can't I model an effect fixed and other random?

Thanks in advance.

My e-mail is darkjacknife at hotmail.com


Desc??rgalo y pru??balo 2 meses gratis.



From deepayan at stat.wisc.edu  Fri Aug  6 19:06:38 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Fri, 6 Aug 2004 12:06:38 -0500
Subject: [R]: (Lattice): Overlaying more than one trend surface using
	contourplot() and wireframe()
In-Reply-To: <4113A826.2030503@fe.up.pt>
References: <4113A826.2030503@fe.up.pt>
Message-ID: <200408061206.38110.deepayan@stat.wisc.edu>

On Friday 06 August 2004 10:47, Peter Ho wrote:
> Hi,
>
> Is there a way to plot more than one trend surface using the
> functions contourplot() and wireframe(). I have found an add=T in
> contour(), but no equivalent argument in contourplot() and
> wireframe()?
>
> I have taken the example 11-2 (pages 441-451) from Design and
> analysis of experiments (Montgomery  2001, 5th edition) to see if
> this could be done in R. I have managed to plot individual  contours
> for each response after using the surf.ls() and trmat() functions
> from the Spatial package, but have been unable to overlay the
> contours.   Ideally, it would be nice to also choose a different set
> of colours for shading the surface, so that when combing surfaces, we
> can immediately identify the optimum conditions. The final overlaid
> plot is shown on page 451 (Figure 11-16) of the book.

I don't have access to that book, so a scan or a rough sketch of what 
that looks like would be helpful.

Ideally, this sort of thing should be done by using a groups= argument. 
This needs your data to be restructured into a single data frame, e.g.:

trsurfY$z <- as.vector(trsurfY$z)
trsurfV$z <- as.vector(trsurfV$z)
trsurfMW$z <- as.vector(trsurfMW$z)

trsurf.comb <- rbind(as.data.frame(trsurfY),
                     as.data.frame(trsurfV),
                     as.data.frame(trsurfMW))
trsurf.comb$g <- 
    factor(rep(c("Y", "V", "MW"), 
           each = length(trsurfY$x)))


The subsequent call should be like 

contourplot(z ~ x * y, trsurf.comb, groups = g)

Unfortunately, contourplot has no idea how to deal with groups, so you 
will have to tell it by writing your own panel function:


contourplot(z ~ x * y, trsurf.comb, 
            subset = g %in% c("V", "Y"), 
            groups = g, cuts = 30,
            panel = function(..., groups, subscripts) {
                gvals <- levels(groups)
                for (i in gvals)
                {
         panel.levelplot(...,
             subscripts = subscripts[groups[subscripts] == i])
         ## panel.contourplot should also work here,
         ## but doesn't because I forgot to export it
                }
            })

Note that the range of the z-values in your 3rd group (MV) is very 
different from the other 2, so trying to include all 3 won't work 
unless you specify an appropriate 'at' vector that covers the ranges of 
all 3 surfaces (by default it will try to put contours at evenly spaced 
locations over the whole range, almost all of which will miss the 
data).

I have no idea what you were hoping to do with levelplots (where the 
inter-contour regions are colored).

You can do something similar with wireframe to get the 3 surfaces 
together. It does know how to handle groups, so the call is simpler. 
(Note that this doesn't really work if your surfaces intersect, but you 
can usually get by with making the grid fine enough.):


wireframe(z ~ x * y, trsurf.comb, groups = g,
          subset = g %in% c("V", "Y"), shade = TRUE)


Hope that helps,

Deepayan



From hastie at stanford.edu  Fri Aug  6 19:35:36 2004
From: hastie at stanford.edu (Trevor Hastie)
Date: Fri, 6 Aug 2004 10:35:36 -0700
Subject: [R] [R-pkgs] gam --- a new contributed package
Message-ID: <030f01c47bdb$c8d6a160$ec6640ab@stuk>

I have contributed a "gam" library to CRAN,
which implements "Generalized Additive Models".

This implementation follows closely the description in 
the GAM chapter 7 of the "white" book "Statistical Models in S"
(Chambers & Hastie (eds), 1992, Wadsworth), as well as the philosophy
in "Generalized Additive Models" (Hastie & Tibshirani 1990, Chapman and
Hall). Hence it behaves pretty much like the Splus version of GAM.

Note: this gam library and functions therein are different from the
gam function in package mgcv, and both libraries should not be used
simultaneously.

The gam library allows both local regression (loess) and smoothing
spline smoothers, and uses backfitting and local scoring to fit gams.
It also allows users to supply their own smoothing methods which can
then be included in gam fits.

The gam function in mgcv uses only smoothing spline smoothers, with a
focus on automatic parameter selection via gcv. 

Some of the features of the gam library:

* full compatibility with the R functions glm and lm - a fitted gam
  inherits from class "glm" and "lm"

* print, summary, anova, predict and plot methods are provided, as
  well as the usual extractor methods like coefficients, residuals etc

* the method step.gam provides a flexible and customizable approach to
  model selection. 

Some differences with the Splus version of gam:

* predictions with new data are improved, without need for the
  "safe.predict.gam" function. This was partly facilitated by
  the improved prediction strategy used in R for GLMs and LMs

* Currently the only backfitting algorithm is all.wam. In the earlier
  versions of gam, dedicated fortran routines fit models that had only
  smoothing spline terms (s.wam) or all local regression terms
  (lo.wam), which in fact made calls back to Splus to update the
  working response and weights. These were designed for efficiency. It
  seems now with much faster computers this efficiency is no longer
  needed, and all.wam is modular and "visible"

 
This package is numbered 0.9 in anticipation of a few bug fixes and
glitches. I have tested many aspects of the functions, but there are
always a few that slip by. I will be happy to hear of any problems,
bugs and suggestions.

Plans for future versions:

* exact standard error calculations. gam employs approximations as
  described in the white book. With a bit more computing (now
  possible), we will have a function that computes exact standard
  errors along the lines described in the GAM book page 127. 

Trevor Hastie

--------------------------------------------------------------------
  Trevor Hastie                                  hastie at stanford.edu  
  Professor, Department of Statistics, Stanford University
  Phone: (650) 725-2231 (Statistics)          Fax: (650) 725-8977  
  (650) 498-5233 (Biostatistics)   Fax: (650) 725-6951
  URL: http://www-stat.stanford.edu/~hastie  
  address: room 104, Department of Statistics, Sequoia Hall
           390 Serra Mall, Stanford University, CA 94305-4065

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://www.stat.math.ethz.ch/mailman/listinfo/r-packages



From cere at u.washington.edu  Fri Aug  6 09:23:32 2004
From: cere at u.washington.edu (Cere Davis)
Date: Fri, 06 Aug 2004 00:23:32 -0700
Subject: [R] iteration introspection?
Message-ID: <cevbll$1l8$1@sea.gmane.org>


Hi everyone,

I want to perform a regex substitution on line #1 in a file based on the 
contents of line #2.  same is true for line 11 and line 12 etc...

With the look at each line of a file rolling forward method it seems to 
me that I will not be able to use iterators like 'each' for this 
operation unless I am able to manipulate or even know of the position of 
the file pointer from within the iterator block but I don't know of a 
way to do this.

Does anyone know how I can learn what the value of my iterator is within 
  a loop?

Thanks,
Cere



From ripley at stats.ox.ac.uk  Fri Aug  6 20:11:58 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 6 Aug 2004 19:11:58 +0100 (BST)
Subject: [R] iteration introspection?
In-Reply-To: <cevbll$1l8$1@sea.gmane.org>
Message-ID: <Pine.LNX.4.44.0408061908440.18843-100000@gannet.stats>

On Fri, 6 Aug 2004, Cere Davis wrote:

> I want to perform a regex substitution on line #1 in a file based on the 
> contents of line #2.  same is true for line 11 and line 12 etc...
> 
> With the look at each line of a file rolling forward method it seems to 
> me that I will not be able to use iterators like 'each' for this 
> operation unless I am able to manipulate or even know of the position of 
> the file pointer from within the iterator block but I don't know of a 
> way to do this.

In R, we use connections to access files and the function seek() will tell
you where you are on the file and move you elsewhere, separately for read 
and for write.

> Does anyone know how I can learn what the value of my iterator is within 
>   a loop?

The iterator is `i' in for(i in 1:n), so you can access it directly.  I 
suspect that was not the Q you wanted to ask.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From eugenedalt at yahoo.com  Fri Aug  6 21:27:04 2004
From: eugenedalt at yahoo.com (eugene dalt)
Date: Fri, 6 Aug 2004 12:27:04 -0700 (PDT)
Subject: [R] Looking for R  course in Atlanta
In-Reply-To: <Pine.LNX.4.44.0408061908440.18843-100000@gannet.stats>
Message-ID: <20040806192704.85824.qmail@web10906.mail.yahoo.com>

I am looking to attend an R course this summer
in Atlanta area. Any help?

Thanks alot
Eugene



From m_nica at hotmail.com  Fri Aug  6 21:35:19 2004
From: m_nica at hotmail.com (Mihai Nica)
Date: Fri, 6 Aug 2004 14:35:19 -0500
Subject: [R] Looking for R  course in Atlanta
References: <20040806192704.85824.qmail@web10906.mail.yahoo.com>
Message-ID: <BAY18-DAV5pw4SvcDfS00004c04@hotmail.com>

see attached link

http://tolstoy.newcastle.edu.au/R/help/04/07/1491.html

Mihai Nica
Jackson State University
155 B Parkhurst Dr.
Jackson, MS 39202
601 969 5423
----- Original Message ----- 
From: "eugene dalt" <eugenedalt at yahoo.com>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, August 06, 2004 2:27 PM
Subject: [R] Looking for R course in Atlanta


> I am looking to attend an R course this summer
> in Atlanta area. Any help?
>
> Thanks alot
> Eugene
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>



From gerifalte28 at hotmail.com  Fri Aug  6 21:44:15 2004
From: gerifalte28 at hotmail.com (F Z)
Date: Fri, 06 Aug 2004 19:44:15 +0000
Subject: [R] about lme
Message-ID: <BAY2-F17EYvZmDvHbYx0000312e@hotmail.com>

Hi

I suspect that what you want to do is to have a randon intercept for Rails. 
i.e.:

m<-lme(travel ~ Rail, data = Rail, random = ~ 1 | Rail)


You might want to try the documentation under C:\Program 
Files\R\rw1091\library\lme4\doc and also the book "Mixed-effect models in S 
and S-Plus" by Pinheiro and Bates.


Francisco

>From: "angel hellraiser" <darkjacknife at hotmail.com>
>To: r-help at stat.math.ethz.ch
>Subject: [R] about lme
>Date: Fri, 06 Aug 2004 16:40:43 +0000
>
>Hi R-users:
>
>I've got a problem with lme.
>
>In Rail data,I try to model the next one
>
>lme(travel ~ Rail, data = Rail, random = ~ Rail | Rail)
>
>I want travel = Rail(i) + Rail(j) + epsilon(i,j)
>
>say, an effect fixed for every Rail and other for Rail(j) random, R says
>fewer observations or false convergence.
>
>Why can't I model an effect fixed and other random?
>
>Thanks in advance.
>
>My e-mail is darkjacknife at hotmail.com
>
>
>Descrgalo y prubalo 2 meses gratis.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Fri Aug  6 21:58:04 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 06 Aug 2004 12:58:04 -0700
Subject: [R] about lme
In-Reply-To: <BAY2-F17EYvZmDvHbYx0000312e@hotmail.com>
References: <BAY2-F17EYvZmDvHbYx0000312e@hotmail.com>
Message-ID: <4113E2CC.9010907@pdf.com>

      In case the suggested model is NOT what you want, the book by 
Pinheiro and Bates describes other models that may be closer to what you 
want.  Also, you might wish to consider lme in both packages nlme and 
lme4.  They are different, and overall lme4 is better, though it may not 
be in any particular case.  Bates is the overall architect for both, and 
the list of people who know more about this subject than he does is 
exceedingly small. 

      hope this helps.  spencer graves

F Z wrote:

> Hi
>
> I suspect that what you want to do is to have a randon intercept for 
> Rails. i.e.:
>
> m<-lme(travel ~ Rail, data = Rail, random = ~ 1 | Rail)
>
>
> You might want to try the documentation under C:\Program 
> Files\R\rw1091\library\lme4\doc and also the book "Mixed-effect models 
> in S and S-Plus" by Pinheiro and Bates.
>
>
> Francisco
>
>> From: "angel hellraiser" <darkjacknife at hotmail.com>
>> To: r-help at stat.math.ethz.ch
>> Subject: [R] about lme
>> Date: Fri, 06 Aug 2004 16:40:43 +0000
>>
>> Hi R-users:
>>
>> I've got a problem with lme.
>>
>> In Rail data,I try to model the next one
>>
>> lme(travel ~ Rail, data = Rail, random = ~ Rail | Rail)
>>
>> I want travel = Rail(i) + Rail(j) + epsilon(i,j)
>>
>> say, an effect fixed for every Rail and other for Rail(j) random, R says
>> fewer observations or false convergence.
>>
>> Why can't I model an effect fixed and other random?
>>
>> Thanks in advance.
>>
>> My e-mail is darkjacknife at hotmail.com
>>
>>
>> Desc??rgalo y pru??balo 2 meses gratis.
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From liai at mail.nih.gov  Fri Aug  6 22:03:48 2004
From: liai at mail.nih.gov (Li, Aiguo (NIH/NCI))
Date: Fri, 6 Aug 2004 16:03:48 -0400 
Subject: [R] questions related to ploting in R
Message-ID: <16A0583FB1644E4DB8C0A0265028B6FD28B928@nihexchange13.nih.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040806/40bfd7d5/attachment.pl

From RichardsTJ2 at UPMC.EDU  Fri Aug  6 22:37:13 2004
From: RichardsTJ2 at UPMC.EDU (Richards, Thomas)
Date: Fri, 6 Aug 2004 16:37:13 -0400
Subject: [R] permax for windows?
Message-ID: <2554B4CA518D504A81E6908E39478A6A04965043@1upmc-msx10.isdip.upmc.edu>

Hello:

I am using R 1.9.1 under windows.
I am interested in the permax package for analyzing DNA microarray data.
I see, from install.packages(), that the windows binary version is 1.2.1, but that source code exists for version 2.2.

Version 2.2 has much more functionality, especially for clustered data.  So, I wonder whether anyone has made a binary windows release for version 2.2 of this package.  Thanks in advance.

Tom Richards



From p.dalgaard at biostat.ku.dk  Fri Aug  6 22:55:14 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 06 Aug 2004 22:55:14 +0200
Subject: [R] permax for windows?
In-Reply-To: <2554B4CA518D504A81E6908E39478A6A04965043@1upmc-msx10.isdip.upmc.edu>
References: <2554B4CA518D504A81E6908E39478A6A04965043@1upmc-msx10.isdip.upmc.edu>
Message-ID: <x2llgsc7gt.fsf@biostat.ku.dk>

"Richards, Thomas" <RichardsTJ2 at UPMC.EDU> writes:

> Hello:
> 
> I am using R 1.9.1 under windows. I am interested in the permax
> package for analyzing DNA microarray data. I see, from
> install.packages(), that the windows binary version is 1.2.1, but
> that source code exists for version 2.2.

Er, did install.packages() tell about 2.2??! CRAN sources are 1.2.1
too. Wouldn't know why the newer version hasn't reached CRAN (or
BioConductor).
 
> Version 2.2 has much more functionality, especially for clustered
> data. So, I wonder whether anyone has made a binary windows release
> for version 2.2 of this package. Thanks in advance.

Looks like the author did:

http://biowww.dfci.harvard.edu/~gray/permax/permax_2.2.zip

Download and use "install from local zip file", I assume.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Ted.Harding at nessie.mcc.ac.uk  Sat Aug  7 11:53:41 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 07 Aug 2004 10:53:41 +0100 (BST)
Subject: [R] iteration introspection?
In-Reply-To: <cevbll$1l8$1@sea.gmane.org>
Message-ID: <XFMail.040807105341.Ted.Harding@nessie.mcc.ac.uk>

On 06-Aug-04 Cere Davis wrote:
> 
> Hi everyone,
> 
> I want to perform a regex substitution on line #1 in a file
> based on the contents of line #2.  same is true for line 11
> and line 12 etc...
> 
> With the look at each line of a file rolling forward method
> it seems to me that I will not be able to use iterators like
> 'each' for this operation unless I am able to manipulate or
> even know of the position of the file pointer from within the
> iterator block but I don't know of a way to do this.
> 
> Does anyone know how I can learn what the value of my iterator
> is within  a loop?

It's not clear from your description why you are thinking of
using R for this.

Normally, in the situation described in your first paragraph,
I would run the file (in plain text format of course) through
'awk', with the 'awk' program written so as to delay the output
of line n (suitably modified) until line (n+1) had been read,
enabling the computation of the modification to line n. You can
certainly keep track of the value of n, so that (e.g. as you
describe) you only do it for n = 1, 11, 21, ...

One argument for using 'awk', where possible, is that it has
very comprehensive resources for handling regular expressions,
which your description implies a need for.

However, doing it this way would need the substitution to be
sufficiently straightforward to compute that you could do it
using the resources of 'awk'. If the computation needed the
resources of some sophisticated function in R (even if only
something like perturbing the value of a variable in line n
by adding a random deviate from a non-central t with non
centrality parameter determined from line (n+1)), then I guess
you would need to do it inside R.

But in that sort of case, what's wrong with reading in the
original file to become a dataframe in R, and explicitly looping
through the lines of the dataframe using a variable say "i",
whose value would always be accessible within the loop?

For instance, if "DF" is the name of the dataframe, then
DF[i,] is row i of DF, and DF has nrow(DF) rows of data.
So your loop could be like

  for(i in (1:nrow(DF)-1)){
    if(i%%10 == 1){
      look at DF[i+1,] and then do whatever you need
      to do with DF[i,]
    }
  }

Then you can write out the modified DF to a file using, say,
'write.table'.

Hoping this helps,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 07-Aug-04                                       Time: 10:53:41
------------------------------ XFMail ------------------------------



From jjoshua at kdupg.edu.my  Sat Aug  7 13:57:25 2004
From: jjoshua at kdupg.edu.my (J. Joshua Thomas)
Date: Sat, 7 Aug 2004 19:57:25 +0800
Subject: [R] Textfile into R'
Message-ID: <E393CC5D5FAFB04EAA8E2DBD965BFCA0263B16@staffm.domain.kdupg.edu.my>

Hi,

I have the chromosome results in a text file. 
My problem is how to call the textfile values inside R' so that i can plot.

Anyone can help me!!


J.Joshua Thomas
Lecturer 
Computing & Information systems
KDU College Penang Campus
Malaysia



From ripley at stats.ox.ac.uk  Sat Aug  7 14:17:03 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 7 Aug 2004 13:17:03 +0100 (BST)
Subject: [R] Textfile into R'
In-Reply-To: <E393CC5D5FAFB04EAA8E2DBD965BFCA0263B16@staffm.domain.kdupg.edu.my>
Message-ID: <Pine.LNX.4.44.0408071312040.20555-100000@gannet.stats>

There is an `R Data Import/Export Manual'.  Please consult it to find
several answers.

On Sat, 7 Aug 2004, J. Joshua Thomas wrote:

> I have the chromosome results in a text file. 
> My problem is how to call the textfile values inside R' so that i can plot.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at myway.com  Sat Aug  7 17:44:46 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 7 Aug 2004 15:44:46 +0000 (UTC)
Subject: [R] iteration introspection?
References: <cevbll$1l8$1@sea.gmane.org>
Message-ID: <loom.20040807T173553-592@post.gmane.org>

Cere Davis <cere <at> u.washington.edu> writes:

> 
> Hi everyone,
> 
> I want to perform a regex substitution on line #1 in a file based on the 
> contents of line #2.  same is true for line 11 and line 12 etc...
> 
> With the look at each line of a file rolling forward method it seems to 
> me that I will not be able to use iterators like 'each' for this 
> operation unless I am able to manipulate or even know of the position of 
> the file pointer from within the iterator block but I don't know of a 
> way to do this.
> 
> Does anyone know how I can learn what the value of my iterator is within 
>   a loop?


Assuming you want to process the lines in pairs read them all in
and then loop:

   lines <- readLines("my.txt")
   n <- seq(along = lines)
   for( i in seq(1,n,2)) {
      # process line[i] and line[i+1] ...
   }

Or if you want to read them in a pair at a time:

   con <- file("my.txt", "r")
   while(length(line <- readLines(con, 1))) {
      next.line <- readLines(con, 1)
      stopifnot(length(next.line))
      # process line and next.line
   }
   close(con)

If you want to read them in one by one rather than in pairs then
have a look at ?pushBack



From ggrothendieck at myway.com  Sat Aug  7 17:59:10 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 7 Aug 2004 15:59:10 +0000 (UTC)
Subject: [R] iteration introspection?
References: <cevbll$1l8$1@sea.gmane.org>
	<loom.20040807T173553-592@post.gmane.org>
Message-ID: <loom.20040807T175707-919@post.gmane.org>

Gabor Grothendieck <ggrothendieck <at> myway.com> writes:

: 
: Cere Davis <cere <at> u.washington.edu> writes:
: 
: > 
: > Hi everyone,
: > 
: > I want to perform a regex substitution on line #1 in a file based on the 
: > contents of line #2.  same is true for line 11 and line 12 etc...
: > 
: > With the look at each line of a file rolling forward method it seems to 
: > me that I will not be able to use iterators like 'each' for this 
: > operation unless I am able to manipulate or even know of the position of 
: > the file pointer from within the iterator block but I don't know of a 
: > way to do this.
: > 
: > Does anyone know how I can learn what the value of my iterator is within 
: >   a loop?
: 
: Assuming you want to process the lines in pairs read them all in
: and then loop:

Sorry. There was an error in this.  Here it is corrected:
 
    lines <- readLines("my.txt")
    n <- length(lines)
    for( i in seq(1,n,2)) {
       # process line[i] and line[i+1] ...
    }
 
: Or if you want to read them in a pair at a time:
: 
:    con <- file("my.txt", "r")
:    while(length(line <- readLines(con, 1))) {
:       next.line <- readLines(con, 1)
:       stopifnot(length(next.line))
:       # process line and next.line
:    }
:    close(con)
: 
: If you want to read them in one by one rather than in pairs then
: have a look at ?pushBack



From ggrothendieck at myway.com  Sat Aug  7 18:12:19 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 7 Aug 2004 16:12:19 +0000 (UTC)
Subject: [R] iteration introspection?
References: <cevbll$1l8$1@sea.gmane.org>
	<XFMail.040807105341.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <loom.20040807T175932-925@post.gmane.org>

 <Ted.Harding <at> nessie.mcc.ac.uk> writes:

: 
: On 06-Aug-04 Cere Davis wrote:
: > 
: > Hi everyone,
: > 
: > I want to perform a regex substitution on line #1 in a file
: > based on the contents of line #2.  same is true for line 11
: > and line 12 etc...
: > 
: > With the look at each line of a file rolling forward method
: > it seems to me that I will not be able to use iterators like
: > 'each' for this operation unless I am able to manipulate or
: > even know of the position of the file pointer from within the
: > iterator block but I don't know of a way to do this.
: > 
: > Does anyone know how I can learn what the value of my iterator
: > is within  a loop?
: 
: It's not clear from your description why you are thinking of
: using R for this.
: 
: Normally, in the situation described in your first paragraph,
: I would run the file (in plain text format of course) through
: 'awk', 

I think the main potential benefit of using awk or other filter
to preprocess input is that it might be faster.  

However, if that is not an issue the various string routines in R
such as substring, substr, paste, sub, gsub, regexp, nchar and
others are sufficiently powerful that these days I find that
I just do it all in R, not using awk or other external 
filter at all.  This has the advantage of keeping everything 
in one language and making the code independent of external
programs which might vary by installation or OS.



From peter at fe.up.pt  Sat Aug  7 18:29:31 2004
From: peter at fe.up.pt (Peter Ho)
Date: Sat, 07 Aug 2004 17:29:31 +0100
Subject: [R]: (Lattice): Overlaying more than one trend surface using
	contourplot() and wireframe()
In-Reply-To: <200408061206.38110.deepayan@stat.wisc.edu>
References: <4113A826.2030503@fe.up.pt>
	<200408061206.38110.deepayan@stat.wisc.edu>
Message-ID: <4115036B.6080901@fe.up.pt>

Deepayan,

Thanks alot for your help. I managed to rescale the 3rd response 
variable, which was what Montgomery did in  the example. Plotting the 
3rd surface was as easy as including a 3rd argument for "MW".

The only problem now is to work out how to shade a particlar area of 
interest, depending on selected constrainst for each response. I ??ll  
try working on that next.

Many thanks again


Peter


Deepayan Sarkar wrote:

>On Friday 06 August 2004 10:47, Peter Ho wrote:
>  
>
>>Hi,
>>
>>Is there a way to plot more than one trend surface using the
>>functions contourplot() and wireframe(). I have found an add=T in
>>contour(), but no equivalent argument in contourplot() and
>>wireframe()?
>>
>>I have taken the example 11-2 (pages 441-451) from Design and
>>analysis of experiments (Montgomery  2001, 5th edition) to see if
>>this could be done in R. I have managed to plot individual  contours
>>for each response after using the surf.ls() and trmat() functions
>>from the Spatial package, but have been unable to overlay the
>>contours.   Ideally, it would be nice to also choose a different set
>>of colours for shading the surface, so that when combing surfaces, we
>>can immediately identify the optimum conditions. The final overlaid
>>plot is shown on page 451 (Figure 11-16) of the book.
>>    
>>
>
>I don't have access to that book, so a scan or a rough sketch of what 
>that looks like would be helpful.
>
>Ideally, this sort of thing should be done by using a groups= argument. 
>This needs your data to be restructured into a single data frame, e.g.:
>
>trsurfY$z <- as.vector(trsurfY$z)
>trsurfV$z <- as.vector(trsurfV$z)
>trsurfMW$z <- as.vector(trsurfMW$z)
>
>trsurf.comb <- rbind(as.data.frame(trsurfY),
>                     as.data.frame(trsurfV),
>                     as.data.frame(trsurfMW))
>trsurf.comb$g <- 
>    factor(rep(c("Y", "V", "MW"), 
>           each = length(trsurfY$x)))
>
>
>The subsequent call should be like 
>
>contourplot(z ~ x * y, trsurf.comb, groups = g)
>
>Unfortunately, contourplot has no idea how to deal with groups, so you 
>will have to tell it by writing your own panel function:
>
>
>contourplot(z ~ x * y, trsurf.comb, 
>            subset = g %in% c("V", "Y"), 
>            groups = g, cuts = 30,
>            panel = function(..., groups, subscripts) {
>                gvals <- levels(groups)
>                for (i in gvals)
>                {
>         panel.levelplot(...,
>             subscripts = subscripts[groups[subscripts] == i])
>         ## panel.contourplot should also work here,
>         ## but doesn't because I forgot to export it
>                }
>            })
>
>Note that the range of the z-values in your 3rd group (MV) is very 
>different from the other 2, so trying to include all 3 won't work 
>unless you specify an appropriate 'at' vector that covers the ranges of 
>all 3 surfaces (by default it will try to put contours at evenly spaced 
>locations over the whole range, almost all of which will miss the 
>data).
>
>I have no idea what you were hoping to do with levelplots (where the 
>inter-contour regions are colored).
>
>You can do something similar with wireframe to get the 3 surfaces 
>together. It does know how to handle groups, so the call is simpler. 
>(Note that this doesn't really work if your surfaces intersect, but you 
>can usually get by with making the grid fine enough.):
>
>
>wireframe(z ~ x * y, trsurf.comb, groups = g,
>          subset = g %in% c("V", "Y"), shade = TRUE)
>
>
>Hope that helps,
>
>Deepayan
>
>
>
>  
>



From deepayan at stat.wisc.edu  Sat Aug  7 19:37:47 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Sat, 7 Aug 2004 12:37:47 -0500
Subject: [R]: (Lattice): Overlaying more than one trend surface using
	contourplot() and wireframe()
In-Reply-To: <4115036B.6080901@fe.up.pt>
References: <4113A826.2030503@fe.up.pt>
	<200408061206.38110.deepayan@stat.wisc.edu>
	<4115036B.6080901@fe.up.pt>
Message-ID: <200408071237.47633.deepayan@stat.wisc.edu>

On Saturday 07 August 2004 11:29, Peter Ho wrote:
> Deepayan,
>
> Thanks alot for your help. I managed to rescale the 3rd response
> variable, which was what Montgomery did in  the example. Plotting the
> 3rd surface was as easy as including a 3rd argument for "MW".
>
> The only problem now is to work out how to shade a particlar area of
> interest, depending on selected constrainst for each response. I ??ll
> try working on that next.

If I understand you correctly, this may not be trivial. The only way I 
can think of is to duplicate the surface (which is sort of cheating), 
and set complementary parts to NA. For example, consider

bar = expand.grid(x = seq(-1, 1, length = 50), 
                  y = seq(-1, 1, length = 50))
bar$z = with(bar, exp(-x^2) * exp(-abs(y)))
wireframe(z ~ x * y, bar)


Then, to highlight the region where z > 0.5, you could do 


foo = rbind(bar, bar)
foo$g = rep(1:2, each = nrow(bar))
foo$z[foo$g == 1 & foo$z < 0.4] <- NA
foo$z[foo$g == 2 & foo$z > 0.5] <- NA
wireframe(z ~ x * y, foo, groups = g)

In your case, the groups argument to wireframe will have to be the 
interaction of the earlier grouping variable and the new one.

Deepayan



From judith.baltsar at gmx.de  Sat Aug  7 21:39:42 2004
From: judith.baltsar at gmx.de (judith.baltsar@gmx.de)
Date: Sat, 07 Aug 2004 21:39:42 +0200
Subject: [R] Question about NPMC
Message-ID: <41154C1E.24825.1A234A0@localhost>

Hello all,
I plan using the NPMC procedure and now I am wondering if this is equivalent
to the "Steel-Dwass test" found in the KyPlot program. The author of KyPlot cites 

Steel, R.G.D: A rank sum test for comparing all pairs of treatments.
Technometrics, 2: 
197-207, 1960.

and

Dwass, M.: Some k-sample rank-order tests. In Contributions to Probability
and Statistics (Eds I. Olkin et al.), Stanford University Press, pp.198-202,
1960.

for this method. Is it the same test or are there any differences?

Thanks a lot
Judith Baltsar



From jamesrgraham at mac.com  Sun Aug  8 03:38:32 2004
From: jamesrgraham at mac.com (James R. Graham)
Date: Sat, 7 Aug 2004 21:38:32 -0400
Subject: [R] Question on Differentiating Two Populations in R
Message-ID: <A852C622-E8DB-11D8-A8D7-000A956A62E4@mac.com>

Hello All,

Forgive me if this a blatantly newbie question or not germane to the 
list, but i was wondering if my current approach to my problem is the 
best way in R.

I have two experimental datasets (positive and negative) of differing 
lengths and a large number of ways of numerically expressing the data 
by using various scales to represent each data point.

I am looking for a scale that will allow me to differentiate between 
the positive and negative populations.

Each dataset is simply a list of numbers: 43 numbers in the positive 
case and 9 in the negative (small sets, i know, but it's all the data i 
currently have) and I have hundreds of scales.

I assign each dataset to a variable using scan() (each are in separate 
files).

My initial comparison of the two datasets is simply a boxplot with the 
hope that the two do not overlap too much...

Is this the way you would approach this problem? Is there an easier way 
of doing this in R?

Any and all help is greatly appreciated!

james



From rossini at blindglobe.net  Sun Aug  8 05:18:53 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Sat, 07 Aug 2004 20:18:53 -0700
Subject: [R] Question on Differentiating Two Populations in R
In-Reply-To: <A852C622-E8DB-11D8-A8D7-000A956A62E4@mac.com> (James R.
	Graham's message of "Sat, 7 Aug 2004 21:38:32 -0400")
References: <A852C622-E8DB-11D8-A8D7-000A956A62E4@mac.com>
Message-ID: <85u0veiag2.fsf@servant.blindglobe.net>

Sounds like you need a course in statistics.  This is a 2-sample
comparison problem.  It might be worth finding a local stat
consultant, it shouldn't take much time.


"James R. Graham" <jamesrgraham at mac.com> writes:

> Hello All,
>
> Forgive me if this a blatantly newbie question or not germane to the
> list, but i was wondering if my current approach to my problem is the
> best way in R.
>
> I have two experimental datasets (positive and negative) of differing
> lengths and a large number of ways of numerically expressing the data
> by using various scales to represent each data point.
>
> I am looking for a scale that will allow me to differentiate between
> the positive and negative populations.
>
> Each dataset is simply a list of numbers: 43 numbers in the positive
> case and 9 in the negative (small sets, i know, but it's all the data
> i currently have) and I have hundreds of scales.
>
> I assign each dataset to a variable using scan() (each are in separate
> files).
>
> My initial comparison of the two datasets is simply a boxplot with the
> hope that the two do not overlap too much...
>
> Is this the way you would approach this problem? Is there an easier
> way of doing this in R?
>
> Any and all help is greatly appreciated!
>
> james
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Anthony Rossini			    Research Associate Professor
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From jfbrennan at rogers.com  Sun Aug  8 16:09:17 2004
From: jfbrennan at rogers.com (Jim Brennan)
Date: Sun, 8 Aug 2004 10:09:17 -0400
Subject: [R] Question on Differentiating Two Populations in R
References: <A852C622-E8DB-11D8-A8D7-000A956A62E4@mac.com>
Message-ID: <001601c47d51$4c8c42a0$3b8ac445@slnt.phub.net.cable.rogers.com>

If I am understanding correctly you can have a look at
 ? t.test
and ? wilcox.test
depending on assumptions.

Jim

----- Original Message -----
From: "James R. Graham" <jamesrgraham at mac.com>
To: <r-help at stat.math.ethz.ch>
Sent: Saturday, August 07, 2004 9:38 PM
Subject: [R] Question on Differentiating Two Populations in R


> Hello All,
>
> Forgive me if this a blatantly newbie question or not germane to the
> list, but i was wondering if my current approach to my problem is the
> best way in R.
>
> I have two experimental datasets (positive and negative) of differing
> lengths and a large number of ways of numerically expressing the data
> by using various scales to represent each data point.
>
> I am looking for a scale that will allow me to differentiate between
> the positive and negative populations.
>
> Each dataset is simply a list of numbers: 43 numbers in the positive
> case and 9 in the negative (small sets, i know, but it's all the data i
> currently have) and I have hundreds of scales.
>
> I assign each dataset to a variable using scan() (each are in separate
> files).
>
> My initial comparison of the two datasets is simply a boxplot with the
> hope that the two do not overlap too much...
>
> Is this the way you would approach this problem? Is there an easier way
> of doing this in R?
>
> Any and all help is greatly appreciated!
>
> james
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From HDoran at air.org  Sun Aug  8 18:24:02 2004
From: HDoran at air.org (Doran, Harold)
Date: Sun, 8 Aug 2004 12:24:02 -0400
Subject: [R] Forcing Sweave text to fit
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7404044CDF@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040808/3b4e3141/attachment.pl

From deepayan at stat.wisc.edu  Sun Aug  8 18:29:27 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Sun, 8 Aug 2004 11:29:27 -0500
Subject: [R] Forcing Sweave text to fit
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F7404044CDF@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F7404044CDF@dc1ex2.air.org>
Message-ID: <200408081129.28087.deepayan@stat.wisc.edu>

On Sunday 08 August 2004 11:24, Doran, Harold wrote:
> List
>
> I am preparing a document in Sweave where I would like the R output
> on the file. So I have used
>
>  <<echo=T>>=
> text
> @
>
> In some cases, I have text that is very long. Consequently, the tex
> file has my text going way out into the margins and the document
> looks a little sloppy. Is there a way to force the text to wrap
> around instead of going out into the margins?

AFAIR, you want to set 

options(width = 40) 

or something similar in a (preferably non-echoed) 'initialization' chunk 
at the top of your document. 

Deepayan



From michael.waters at dtn.ntl.com  Sun Aug  8 19:20:59 2004
From: michael.waters at dtn.ntl.com (Dr Mike Waters)
Date: Sun, 8 Aug 2004 18:20:59 +0100
Subject: [R] R packages install problems linux - X not found (WhiteBox EL 3)
Message-ID: <005601c47d6c$12f5b8d0$6600a8c0@mainman>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040808/c892e398/attachment.pl

From mi2kelgrum at yahoo.com  Sun Aug  8 19:26:04 2004
From: mi2kelgrum at yahoo.com (Mikkel Grum)
Date: Sun, 8 Aug 2004 10:26:04 -0700 (PDT)
Subject: [R] vectorized lines
Message-ID: <20040808172604.63809.qmail@web60201.mail.yahoo.com>

Hi,

I thought that the following code would give me a set
of parallel lines on a plot as in the second example.

q<-c(-2253,-2119,-1985,-1850)
p<-c(1,2,3,4)
a<-rep(min(p),4)
b<-rep(max(p),4)
plot(p,q)

# example 1
lines(c(a,b),c(q,q))

Now this gives me the lines I really want:

# example 2
lines(c(a[1],b[1]),c(q[1],q[1]))
lines(c(a[2],b[2]),c(q[2],q[2]))
lines(c(a[3],b[3]),c(q[3],q[3]))
lines(c(a[4],b[4]),c(q[4],q[4]))

I assumed that example 1 was a shorter (vectorized)
way of writing example 2?? What have I got wrong?

cheers,
Mikkel



From bates at wisc.edu  Sun Aug  8 19:32:07 2004
From: bates at wisc.edu (Douglas Bates)
Date: Sun, 08 Aug 2004 12:32:07 -0500
Subject: [R] R packages install problems linux - X not found (WhiteBox
	EL 3)
In-Reply-To: <005601c47d6c$12f5b8d0$6600a8c0@mainman>
References: <005601c47d6c$12f5b8d0$6600a8c0@mainman>
Message-ID: <41166397.9070300@wisc.edu>

Dr Mike Waters wrote:

> I am used to using R under Windows, but have done an install of 1.9.1 under
> WhiteBox linux 3 (based on RHEL 3). This all went without a hitch, along
> with most of the additional package installs. However, while trying to
> install car and rgl I hit a problem regarding the X environment not being
> found. As I was doing the install from a console *within* the X environment,
> this is obviously down to a missing environment variable or link. The X11
> directories all seem to be in the usual places. I've checked as much as I
> can through the archives and googled around, but to no avail. Any help
> appreciated.

Or a missing development package.  In many Linux distributions the 
include files for X11 are in a separate package from the run-time 
libraries.  I have never used WhiteBox Linux but I imagine that will be 
the case for that distribution too.  Check to see if there is a package 
with a name like xlibs-dev or x-dev.



From ligges at statistik.uni-dortmund.de  Sun Aug  8 19:51:35 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 08 Aug 2004 19:51:35 +0200
Subject: [R] vectorized lines
In-Reply-To: <20040808172604.63809.qmail@web60201.mail.yahoo.com>
References: <20040808172604.63809.qmail@web60201.mail.yahoo.com>
Message-ID: <41166827.3060200@statistik.uni-dortmund.de>

Mikkel Grum wrote:

> Hi,
> 
> I thought that the following code would give me a set
> of parallel lines on a plot as in the second example.
> 
> q<-c(-2253,-2119,-1985,-1850)
> p<-c(1,2,3,4)
> a<-rep(min(p),4)
> b<-rep(max(p),4)
> plot(p,q)
> 
> # example 1
> lines(c(a,b),c(q,q))
> 
> Now this gives me the lines I really want:
> 
> # example 2
> lines(c(a[1],b[1]),c(q[1],q[1]))
> lines(c(a[2],b[2]),c(q[2],q[2]))
> lines(c(a[3],b[3]),c(q[3],q[3]))
> lines(c(a[4],b[4]),c(q[4],q[4]))
> 
> I assumed that example 1 was a shorter (vectorized)
> way of writing example 2?? What have I got wrong?

See ?segments

Uwe Ligges


> cheers,
> Mikkel
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From MSchwartz at MedAnalytics.com  Sun Aug  8 19:53:32 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Sun, 08 Aug 2004 12:53:32 -0500
Subject: [R] R packages install problems linux - X not found (WhiteBox
	EL 3)
In-Reply-To: <41166397.9070300@wisc.edu>
References: <005601c47d6c$12f5b8d0$6600a8c0@mainman>
	<41166397.9070300@wisc.edu>
Message-ID: <1091987611.25174.25.camel@localhost.localdomain>

On Sun, 2004-08-08 at 12:32, Douglas Bates wrote:
> Dr Mike Waters wrote:
> 
> > I am used to using R under Windows, but have done an install of 1.9.1 under
> > WhiteBox linux 3 (based on RHEL 3). This all went without a hitch, along
> > with most of the additional package installs. However, while trying to
> > install car and rgl I hit a problem regarding the X environment not being
> > found. As I was doing the install from a console *within* the X environment,
> > this is obviously down to a missing environment variable or link. The X11
> > directories all seem to be in the usual places. I've checked as much as I
> > can through the archives and googled around, but to no avail. Any help
> > appreciated.
> 
> Or a missing development package.  In many Linux distributions the 
> include files for X11 are in a separate package from the run-time 
> libraries.  I have never used WhiteBox Linux but I imagine that will be 
> the case for that distribution too.  Check to see if there is a package 
> with a name like xlibs-dev or x-dev.


Just to amplify on Doug's comments, the RPM in question should be
something like:

XFree86-devel-...

where the "..." is replaced the by version numbering schema.

I am presuming that WhiteBox has not yet changed over to the use of
X.org in place of XFree86 at this point. If it has, then the RPM would
be something like:

xorg-x11-devel-...

An easy way to check for this would be to open a console window and use:

rpm -q XFree86-devel

in the first case or:

rpm -q xorg-x11-devel

in the second case.

If nothing is returned by the command, then it would confirm that you
are missing the requisite RPM.

In the case of the RGL package, you might want to review this recent
thread:

https://www.stat.math.ethz.ch/pipermail/r-help/2004-August/thread.html

which indicates some issues related to the same devel libraries,
including the XFree86-Mesa-libGL (or xorg-x11-Mesa-libGL) and
XFree86-Mesa-libGLU (or xorg-x11-Mesa-libGLU) RPMS.

HTH,

Marc Schwartz



From MSchwartz at MedAnalytics.com  Sun Aug  8 20:08:46 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Sun, 08 Aug 2004 13:08:46 -0500
Subject: [R] R packages install problems linux - X not found (WhiteBox
	EL 3)
In-Reply-To: <1091987611.25174.25.camel@localhost.localdomain>
References: <005601c47d6c$12f5b8d0$6600a8c0@mainman>
	<41166397.9070300@wisc.edu>
	<1091987611.25174.25.camel@localhost.localdomain>
Message-ID: <1091988526.25174.28.camel@localhost.localdomain>

On Sun, 2004-08-08 at 12:53, Marc Schwartz wrote:
> In the case of the RGL package, you might want to review this recent
> thread:
> 
> https://www.stat.math.ethz.ch/pipermail/r-help/2004-August/thread.html


Correction on the above URL. I pasted the wrong one here. It should be:

https://www.stat.math.ethz.ch/pipermail/r-help/2004-August/053994.html

Marc



From jfbrennan at rogers.com  Sun Aug  8 20:30:31 2004
From: jfbrennan at rogers.com (Jim Brennan)
Date: Sun, 8 Aug 2004 14:30:31 -0400
Subject: [R] vectorized lines
References: <20040808172604.63809.qmail@web60201.mail.yahoo.com>
Message-ID: <003901c47d75$ca253c20$3b8ac445@slnt.phub.net.cable.rogers.com>

abline(h=q) might be useful

Jim
----- Original Message -----
From: "Mikkel Grum" <mi2kelgrum at yahoo.com>
To: "RHelp" <r-help at stat.math.ethz.ch>
Sent: Sunday, August 08, 2004 1:26 PM
Subject: [R] vectorized lines


> Hi,
>
> I thought that the following code would give me a set
> of parallel lines on a plot as in the second example.
>
> q<-c(-2253,-2119,-1985,-1850)
> p<-c(1,2,3,4)
> a<-rep(min(p),4)
> b<-rep(max(p),4)
> plot(p,q)
>
> # example 1
> lines(c(a,b),c(q,q))
>
> Now this gives me the lines I really want:
>
> # example 2
> lines(c(a[1],b[1]),c(q[1],q[1]))
> lines(c(a[2],b[2]),c(q[2],q[2]))
> lines(c(a[3],b[3]),c(q[3],q[3]))
> lines(c(a[4],b[4]),c(q[4],q[4]))
>
> I assumed that example 1 was a shorter (vectorized)
> way of writing example 2?? What have I got wrong?
>
> cheers,
> Mikkel
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From S.Nyangoma at cs.rug.nl  Sun Aug  8 20:58:04 2004
From: S.Nyangoma at cs.rug.nl (Stephen Nyangoma)
Date: 08 Aug 2004 20:58:04 +0200
Subject: [R] manipulating strings
In-Reply-To: <200408081012.i78A51NL020257@hypatia.math.ethz.ch>
References: <200408081012.i78A51NL020257@hypatia.math.ethz.ch>
Message-ID: <1091991484.14712.11.camel@iwi142>

Hi
I have a called fil consisting of the following strings.


> fil
  [1] " 102.2 639"   " 104.2 224"   " 105.1 1159"  " 107.1 1148"  
      " 108.1 1376"
  [6] " 109.2 1092"  " 111.2 1238"  " 112.2 349"   " 113.1 1204"  
      " 114.1 537"
 [11] " 115.0 303"   " 116.1 490"   " 117.2 202"   " 118.1 1864"  
      " 119.0 357"


I want to get a data frame like

Time    Obs
102.2   639
104.2   224
105.1  1159
107.1  1148
108.1  1376
109.2  1092
111.2  1238
112.2   349
113.1  1204  
114.1   537
etc

Can anyone see an efficient way of doing this?

Thanks. Stephen



From michael.waters at dtn.ntl.com  Sun Aug  8 21:11:42 2004
From: michael.waters at dtn.ntl.com (Dr Mike Waters)
Date: Sun, 8 Aug 2004 20:11:42 +0100
Subject: FW: [R] R packages install problems linux - X not found (WhiteBoxEL 3)
Message-ID: <006701c47d7b$8a578f20$6600a8c0@mainman>



-----Original Message-----
From: Dr Mike Waters [mailto:michael.waters at dtn.ntl.com] 
Sent: 08 August 2004 20:11
To: 'MSchwartz at MedAnalytics.com'
Subject: RE: [R] R packages install problems linux - X not found (WhiteBoxEL
3)

-----Original Message-----
> Correction on the above URL. I pasted the wrong one here. It should be:

> https://www.stat.math.ethz.ch/pipermail/r-help/2004-August/053994.html
>
> Marc
>


Thanks for the responses guys.

I used to have RH9 installed on this machine and I found out about the
separate developer packages then. I thought that I had got the relevant
XFree devel package installed, but although it showed up in the rpm database
as being present, the required files were not present. I did a forced rpm
upgrade from the WhiteBox updates directory and that problem is now fixed,
at least for car. Marc, thanks for the pointer on the rgl problem. However,
I have a slightly different problem with the install of this package. It
gets through to the point where it tries to make the rgl.so from the various
.o files and fails then, as follows:

--------------------------------------------------------------------

g++ -I/usr/lib/R/include -I/usr/X11R6/include -DHAVE_PNG_H
-I/usr/include -I/usr/local/include  -Wall -pedantic -fno-exceptions
-fno-rtti -fPIC  -O2 -g -march=i386 -mcpu=i686 -c glgui.cpp -o glgui.o

g++  -L/usr/local/lib -o rgl.so x11lib.o x11gui.o types.o math.o fps.o
pixmap.o gui.o api.o device.o devicemanager.o rglview.o scene.o glgui.o
-L/usr/X11R6/lib -L/usr/lib -lstdc++ -lX11 -lXext -lGL -lGLU -lpng
/usr/lib/gcc-lib/i386-redhat-linux/3.2.3/../../../crt1.o(.text+0x18): In
function `_start':
: undefined reference to `main'
x11lib.o(.text+0x84): In function `set_R_handler':
/tmp/R.INSTALL.13414/rgl/src/x11gui.h:33: undefined reference to
`R_InputHandlers'
x11lib.o(.text+0x92):/tmp/R.INSTALL.13414/rgl/src/x11gui.h:33: undefined
reference to `addInputHandler'
x11lib.o(.text+0xfb): In function `unset_R_handler':
/tmp/R.INSTALL.13414/rgl/src/x11lib.cpp:52: undefined reference to
`R_InputHandlers'
x11lib.o(.text+0x103):/tmp/R.INSTALL.13414/rgl/src/x11lib.cpp:52:
undefined reference to `removeInputHandler'
collect2: ld returned 1 exit status
make: *** [rgl.so] Error 1
ERROR: compilation failed for package 'rgl'
** Removing '/usr/lib/R/library/rgl'

---------------------------------------------------------------------

No doubt another failed dependency........... DOH!

Regards

M



From MSchwartz at MedAnalytics.com  Sun Aug  8 21:19:40 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Sun, 08 Aug 2004 14:19:40 -0500
Subject: [R] manipulating strings
In-Reply-To: <1091991484.14712.11.camel@iwi142>
References: <200408081012.i78A51NL020257@hypatia.math.ethz.ch>
	<1091991484.14712.11.camel@iwi142>
Message-ID: <1091992780.25174.38.camel@localhost.localdomain>

On Sun, 2004-08-08 at 13:58, Stephen Nyangoma wrote:
> Hi
> I have a called fil consisting of the following strings.
> 
> 
> > fil
>   [1] " 102.2 639"   " 104.2 224"   " 105.1 1159"  " 107.1 1148"  
>       " 108.1 1376"
>   [6] " 109.2 1092"  " 111.2 1238"  " 112.2 349"   " 113.1 1204"  
>       " 114.1 537"
>  [11] " 115.0 303"   " 116.1 490"   " 117.2 202"   " 118.1 1864"  
>       " 119.0 357"
> 
> 
> I want to get a data frame like
> 
> Time    Obs
> 102.2   639
> 104.2   224
> 105.1  1159
> 107.1  1148
> 108.1  1376
> 109.2  1092
> 111.2  1238
> 112.2   349
> 113.1  1204  
> 114.1   537
> etc
> 
> Can anyone see an efficient way of doing this?
> 
> Thanks. Stephen

Try this:

# Create strings
MyStrings <- c(" 102.2 639",  " 104.2 224", " 105.1 1159",
               " 107.1 1148", " 108.1 1376", " 109.2 1092",
               " 111.2 1238", " 112.2 349",  " 113.1 1204",
               " 114.1 537",  " 115.0 303",  " 116.1 490",
               " 117.2 202",  " 118.1 1864", " 119.0 357")

> MyStrings
 [1] " 102.2 639"  " 104.2 224"  " 105.1 1159" " 107.1 1148"
 [5] " 108.1 1376" " 109.2 1092" " 111.2 1238" " 112.2 349" 
 [9] " 113.1 1204" " 114.1 537"  " 115.0 303"  " 116.1 490" 
[13] " 117.2 202"  " 118.1 1864" " 119.0 357" 


# Now convert to a data frame, by first using strsplit(), to break up
# each of the vector elements into three components, using " " as a
# split character. This returns a list, which we then convert to vector,
# using unlist(). Then use matrix() to convert the vector into a two
# dimensional object with 3 cols. Use 'byrow = TRUE' so that we fill
# the matrix row by row. Then take only the second and third columns 
# from the matrix and convert them into a data frame.
df <- as.data.frame(matrix(unlist(strsplit(MyStrings, split = " ")),
                    ncol = 3, byrow = TRUE)[, 2:3])

# Finally, set the colnames
colnames(df) <- c("Time", "Obs")

> df
    Time  Obs
1  102.2  639
2  104.2  224
3  105.1 1159
4  107.1 1148
5  108.1 1376
6  109.2 1092
7  111.2 1238
8  112.2  349
9  113.1 1204
10 114.1  537
11 115.0  303
12 116.1  490
13 117.2  202
14 118.1 1864
15 119.0  357


Note that the above presumes that your strings (character vectors) have
a leading " " in them and the Time and Obs elements are also separated
by a " " in each.

See ?strsplit for more information.

HTH,

Marc Schwartz



From MSchwartz at MedAnalytics.com  Sun Aug  8 21:46:46 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Sun, 08 Aug 2004 14:46:46 -0500
Subject: [R] R packages install problems linux - X not found (WhiteBoxEL 3)
In-Reply-To: <006601c47d7b$628a59a0$6600a8c0@mainman>
References: <006601c47d7b$628a59a0$6600a8c0@mainman>
Message-ID: <1091994406.25174.58.camel@localhost.localdomain>

On Sun, 2004-08-08 at 14:10, Dr Mike Waters wrote:

snip

> Thanks for the responses guys.
> 
> I used to have RH9 installed on this machine and I found out about the
> separate developer packages then. I thought that I had got the relevant
> XFree devel package installed, but although it showed up in the rpm database
> as being present, the required files were not present. I did a forced rpm
> upgrade from the WhiteBox updates directory and that problem is now fixed,
> at least for car. Marc, thanks for the pointer on the rgl problem. However,
> I have a slightly different problem with the install of this package. It
> gets through to the point where it tries to make the rgl.so from the various
> .o files and fails then, as follows:
> 
> --------------------------------------------------------------------
> 
> g++ -I/usr/lib/R/include -I/usr/X11R6/include -DHAVE_PNG_H
> -I/usr/include -I/usr/local/include  -Wall -pedantic -fno-exceptions
> -fno-rtti -fPIC  -O2 -g -march=i386 -mcpu=i686 -c glgui.cpp -o glgui.o
> 
> g++  -L/usr/local/lib -o rgl.so x11lib.o x11gui.o types.o math.o fps.o
> pixmap.o gui.o api.o device.o devicemanager.o rglview.o scene.o glgui.o
> -L/usr/X11R6/lib -L/usr/lib -lstdc++ -lX11 -lXext -lGL -lGLU -lpng
> /usr/lib/gcc-lib/i386-redhat-linux/3.2.3/../../../crt1.o(.text+0x18): In
> function `_start':
> : undefined reference to `main'
> x11lib.o(.text+0x84): In function `set_R_handler':
> /tmp/R.INSTALL.13414/rgl/src/x11gui.h:33: undefined reference to
> `R_InputHandlers'
> x11lib.o(.text+0x92):/tmp/R.INSTALL.13414/rgl/src/x11gui.h:33: undefined
> reference to `addInputHandler'
> x11lib.o(.text+0xfb): In function `unset_R_handler':
> /tmp/R.INSTALL.13414/rgl/src/x11lib.cpp:52: undefined reference to
> `R_InputHandlers'
> x11lib.o(.text+0x103):/tmp/R.INSTALL.13414/rgl/src/x11lib.cpp:52:
> undefined reference to `removeInputHandler'
> collect2: ld returned 1 exit status
> make: *** [rgl.so] Error 1
> ERROR: compilation failed for package 'rgl'
> ** Removing '/usr/lib/R/library/rgl'
> 
> ---------------------------------------------------------------------
> 
> No doubt another failed dependency........... DOH!
> 
> Regards


I am concerned by your indications of previously having had RH9 on the
same box and that you had to force an update of the XFree Devel RPM.
Forcing the installation of an RPM is almost always a bad thing.

When you installed WB on the system, did you do a "clean" installation
or some type of "upgrade"?

If the latter, it is reasonable to consider that there may be some level
of mixing and matching of RPMS from the two distributions going on. This
could result in a level of marginally or wholly incompatible versions of
RPMS being installed.

Could you clarify that point?

Also, be sure that you have the same versions of the XFree series RPMS
installed.

Use:

rpm -qa | grep XFree

in a console and be sure that the RPMS return the same version schema.
If not, it is possible that one of your problems is the mixing of
versions.

Take note of the output of the above and be sure that the
XFree86-Mesa-libGL and XFree86-Mesa-libGLU RPMS are installed as well.

Some of the messages above would also suggest a problem finding R
related headers. How did you install R? This may be a red herring of
sorts, given the other problems, but may be helpful.

Marc



From ggrothendieck at myway.com  Sun Aug  8 22:03:41 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 8 Aug 2004 20:03:41 +0000 (UTC)
Subject: [R] manipulating strings
References: <200408081012.i78A51NL020257@hypatia.math.ethz.ch>
	<1091991484.14712.11.camel@iwi142>
Message-ID: <loom.20040808T220106-233@post.gmane.org>

Stephen Nyangoma <S.Nyangoma <at> cs.rug.nl> writes:

: 
: Hi
: I have a called fil consisting of the following strings.
: 
: > fil
:   [1] " 102.2 639"   " 104.2 224"   " 105.1 1159"  " 107.1 1148"  
:       " 108.1 1376"
:   [6] " 109.2 1092"  " 111.2 1238"  " 112.2 349"   " 113.1 1204"  
:       " 114.1 537"
:  [11] " 115.0 303"   " 116.1 490"   " 117.2 202"   " 118.1 1864"  
:       " 119.0 357"
: 
: I want to get a data frame like
: 
: Time    Obs
: 102.2   639
: 104.2   224
: 105.1  1159
: 107.1  1148
: 108.1  1376
: 109.2  1092
: 111.2  1238
: 112.2   349
: 113.1  1204  
: 114.1   537
: etc
: 
: Can anyone see an efficient way of doing this?

textConnection can be used to allow read.table to read them
as if it were reading from a file:

   read.table(textConnection(fil), col.names = c("Time", "Obs"))



From ripley at stats.ox.ac.uk  Sun Aug  8 22:22:08 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 8 Aug 2004 21:22:08 +0100 (BST)
Subject: [R] R packages install problems linux - X not found (WhiteBoxEL 3)
In-Reply-To: <1091994406.25174.58.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.44.0408082110130.9160-100000@gannet.stats>

On Sun, 8 Aug 2004, Marc Schwartz wrote:

> On Sun, 2004-08-08 at 14:10, Dr Mike Waters wrote:
> 
> snip
> 
> > Thanks for the responses guys.
> > 
> > I used to have RH9 installed on this machine and I found out about the
> > separate developer packages then. I thought that I had got the relevant
> > XFree devel package installed, but although it showed up in the rpm database
> > as being present, the required files were not present. I did a forced rpm
> > upgrade from the WhiteBox updates directory and that problem is now fixed,
> > at least for car. Marc, thanks for the pointer on the rgl problem. However,
> > I have a slightly different problem with the install of this package. It
> > gets through to the point where it tries to make the rgl.so from the various
> > .o files and fails then, as follows:
> > 
> > --------------------------------------------------------------------
> > 
> > g++ -I/usr/lib/R/include -I/usr/X11R6/include -DHAVE_PNG_H
> > -I/usr/include -I/usr/local/include  -Wall -pedantic -fno-exceptions
> > -fno-rtti -fPIC  -O2 -g -march=i386 -mcpu=i686 -c glgui.cpp -o glgui.o
> > 
> > g++  -L/usr/local/lib -o rgl.so x11lib.o x11gui.o types.o math.o fps.o
> > pixmap.o gui.o api.o device.o devicemanager.o rglview.o scene.o glgui.o
> > -L/usr/X11R6/lib -L/usr/lib -lstdc++ -lX11 -lXext -lGL -lGLU -lpng
> > /usr/lib/gcc-lib/i386-redhat-linux/3.2.3/../../../crt1.o(.text+0x18): In
> > function `_start':
> > : undefined reference to `main'
> > x11lib.o(.text+0x84): In function `set_R_handler':
> > /tmp/R.INSTALL.13414/rgl/src/x11gui.h:33: undefined reference to
> > `R_InputHandlers'
> > x11lib.o(.text+0x92):/tmp/R.INSTALL.13414/rgl/src/x11gui.h:33: undefined
> > reference to `addInputHandler'
> > x11lib.o(.text+0xfb): In function `unset_R_handler':
> > /tmp/R.INSTALL.13414/rgl/src/x11lib.cpp:52: undefined reference to
> > `R_InputHandlers'
> > x11lib.o(.text+0x103):/tmp/R.INSTALL.13414/rgl/src/x11lib.cpp:52:
> > undefined reference to `removeInputHandler'
> > collect2: ld returned 1 exit status
> > make: *** [rgl.so] Error 1
> > ERROR: compilation failed for package 'rgl'
> > ** Removing '/usr/lib/R/library/rgl'
> > 
> > ---------------------------------------------------------------------
> > 
> > No doubt another failed dependency........... DOH!
> > 
> > Regards
> 
> 
> I am concerned by your indications of previously having had RH9 on the
> same box and that you had to force an update of the XFree Devel RPM.
> Forcing the installation of an RPM is almost always a bad thing.
> 
> When you installed WB on the system, did you do a "clean" installation
> or some type of "upgrade"?
> 
> If the latter, it is reasonable to consider that there may be some level
> of mixing and matching of RPMS from the two distributions going on. This
> could result in a level of marginally or wholly incompatible versions of
> RPMS being installed.
> 
> Could you clarify that point?
> 
> Also, be sure that you have the same versions of the XFree series RPMS
> installed.
> 
> Use:
> 
> rpm -qa | grep XFree
> 
> in a console and be sure that the RPMS return the same version schema.
> If not, it is possible that one of your problems is the mixing of
> versions.
> 
> Take note of the output of the above and be sure that the
> XFree86-Mesa-libGL and XFree86-Mesa-libGLU RPMS are installed as well.
> 
> Some of the messages above would also suggest a problem finding R
> related headers. How did you install R? This may be a red herring of
> sorts, given the other problems, but may be helpful.

I think it is the exact point.  Those entry points are in R.bin, 
so should be missing.  The g++ line is missing `-shared', which is picked 
up from R_HOME/etc/Makeconf, specifically from

SHLIB_CXXLDFLAGS = -shared

So the R installation doesn't have the right flags.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From gpagnon at emory.edu  Sun Aug  8 23:00:41 2004
From: gpagnon at emory.edu (Giuseppe Pagnoni)
Date: Sun, 8 Aug 2004 17:00:41 -0400
Subject: [R] (REPOST) Simple main effects in 2-way repeated measure ANOVA
Message-ID: <01D7ACA6-E97E-11D8-81F8-000D932922EA@emory.edu>

Hi all

I am running a 2-way repeated measure anova with 1 between-subjects 
factor (Group=treatment, control), and 1 within-subject factor (Time of 
measurement: time1, time2).  I extract the results of the anova with:

summary(aov(effect ~ Group*Time + Error=Subj/Time, data=mydata))

Now, this must be clearly a dumb question, but how can I quickly 
extract in R all the post-hoc t-tests for the simple main effects?

Also, while I am at it, how do I enter in the model a counfounding 
covariate (e.g., Age)?

And on a different matter, is there a way to receive interactive user 
input in an R script? Something like "Enter the name of  the factor:  
", or even more simply "Press <Enter> to see the result of the next 
analysis"....

thanks in advance for any suggestions!

    giuseppe




Giuseppe Pagnoni
Dept. Psychiatry and Behavioral Sciences
Emory University
1639 Pierce Drive, Suite 4000
Atlanta, GA, 30322
tel: 404.712.8431
fax: 404.727.3233



From ripley at stats.ox.ac.uk  Sun Aug  8 23:20:37 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 8 Aug 2004 22:20:37 +0100 (BST)
Subject: [R] (REPOST) Simple main effects in 2-way repeated measure ANOVA
In-Reply-To: <01D7ACA6-E97E-11D8-81F8-000D932922EA@emory.edu>
Message-ID: <Pine.LNX.4.44.0408082212530.11410-100000@gannet.stats>

Why has this been REPOSTed?  It was delivered last Thursday.

On Sun, 8 Aug 2004, Giuseppe Pagnoni wrote:

> I am running a 2-way repeated measure anova with 1 between-subjects 
> factor (Group=treatment, control), and 1 within-subject factor (Time of 
> measurement: time1, time2).  I extract the results of the anova with:
> 
> summary(aov(effect ~ Group*Time + Error=Subj/Time, data=mydata))

That's not valid syntax for an R formula. Did you mean Error(Subj/Time)?

> Now, this must be clearly a dumb question, but how can I quickly 
> extract in R all the post-hoc t-tests for the simple main effects?

I very much hope you cannot, as you have specified an interaction, and
you should not want t-tests for main effects in the presence of an 
interaction, and certainly not with the default R coding.

Did you mean Group + Time?

> Also, while I am at it, how do I enter in the model a counfounding 
> covariate (e.g., Age)?
> 
> And on a different matter, is there a way to receive interactive user 
> input in an R script? Something like "Enter the name of  the factor:  
> ", or even more simply "Press <Enter> to see the result of the next 
> analysis"....

?readline, or cat + scan or ... using connections.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From d.scott at auckland.ac.nz  Mon Aug  9 03:59:28 2004
From: d.scott at auckland.ac.nz (David Scott)
Date: Mon, 9 Aug 2004 13:59:28 +1200 (NZST)
Subject: [R] Time zones
Message-ID: <Pine.LNX.4.60.0408091314520.1368@stat71.stat.auckland.ac.nz>


I am analysing some data collected over a number of months from Allentown, 
PA, which is just north of Philadelphia. I am using as.POSIXct for dates 
and times, and I need to get the timezone specification correct.

Going on the documentation for DateTimeClasses, I believe one way to 
specify the correct time zone is

tz="EST5EDT"

I would be grateful for any advice on this. I ask because I haven't been 
able to find a list of all the possible time zones or formats that can be 
used. I have done a bit of googling without to much success.

I have read Gabor's article in the recent R Newsletter too.

David Scott
_________________________________________________________________
David Scott	Department of Statistics, Tamaki Campus
 		The University of Auckland, PB 92019
 		Auckland	NEW ZEALAND
Phone: +64 9 373 7599 ext 86830		Fax: +64 9 373 7000
Email:	d.scott at auckland.ac.nz


Graduate Officer, Department of Statistics



From mcclatchie.sam at saugov.sa.gov.au  Mon Aug  9 04:55:09 2004
From: mcclatchie.sam at saugov.sa.gov.au (McClatchie, Sam (PIRSA-SARDI))
Date: Mon, 9 Aug 2004 12:25:09 +0930 
Subject: [R] returns the value of a polynomial of degree n evaluated at x. 
Message-ID: <032A8573186A2B4EBBAEFA5784D0523502BCB031@sagemsg0007.sagemsmrd01.sa.gov.au>

> Background:
> OS: Linux Mandrake 9.1
> release: R 1.9.0 
> editor: Xemacs 21.4
> frontend: ESS 5.1.23
> ---------------------------------
> 
> Colleagues
> 
Is there a function in R that is similar to polyval in matlab? (y =
polyval(p,x) returns the value of a polynomial of degree n evaluated at x.
The input argument p is a vector of length n+1 whose elements are the
coefficients in descending powers of the polynomial to be evaluated).
<http://www.mathworks.com/access/helpdesk/help/techdoc/ref/polyval.html> 
 
I have possibly missed it in the R help files, so please excuse me if I
have. "Polynomial evaluation" yielded no result in a search of help, and the
functions poly() and locpoly() don't seem to be what I am after.

> Sam
> ----
> Sam McClatchie,
> Sub-program leader, Pelagic Fisheries
> South Australian Aquatic Sciences Centre
> PO Box 120, Henley Beach 5022
> Adelaide, South Australia
> email <mcclatchie.sam at saugov.sa.gov.au>
> Telephone: (61-8) 8200 2448
> FAX: (61-8) 8200 2481
> Research home page <http://www.smcc.150m.com/>
>   
>                    /\
>       ...>><xX(??> 
>                 //// \\\\
>                    <??)Xx><<
>               /////  \\\\\\
>                         ><(((??> 
>   >><(((??>   ...>><xX(??>O<??)Xx><<
>



From andy_liaw at merck.com  Mon Aug  9 05:27:38 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sun, 8 Aug 2004 23:27:38 -0400
Subject: [R] returns the value of a polynomial of degree n evaluated a t x.
Message-ID: <3A822319EB35174CA3714066D590DCD504AF81D2@usrymx25.merck.com>

Try something like:

install.packages("polynom")
library(polynom)
predict(polynomial(rev(p)), x)

HTH,
Andy

> From: McClatchie, Sam (PIRSA-SARDI)
> 
> > Background:
> > OS: Linux Mandrake 9.1
> > release: R 1.9.0 
> > editor: Xemacs 21.4
> > frontend: ESS 5.1.23
> > ---------------------------------
> > 
> > Colleagues
> > 
> Is there a function in R that is similar to polyval in matlab? (y =
> polyval(p,x) returns the value of a polynomial of degree n 
> evaluated at x.
> The input argument p is a vector of length n+1 whose elements are the
> coefficients in descending powers of the polynomial to be evaluated).
> <http://www.mathworks.com/access/helpdesk/help/techdoc/ref/pol
yval.html> 
 
I have possibly missed it in the R help files, so please excuse me if I
have. "Polynomial evaluation" yielded no result in a search of help, and the
functions poly() and locpoly() don't seem to be what I am after.

> Sam
> ----
> Sam McClatchie,
> Sub-program leader, Pelagic Fisheries
> South Australian Aquatic Sciences Centre
> PO Box 120, Henley Beach 5022
> Adelaide, South Australia
> email <mcclatchie.sam at saugov.sa.gov.au>
> Telephone: (61-8) 8200 2448
> FAX: (61-8) 8200 2481
> Research home page <http://www.smcc.150m.com/>
>   
>                    /\
>       ...>><xX(??> 
>                 //// \\\\
>                    <??)Xx><<
>               /////  \\\\\\
>                         ><(((??> 
>   >><(((??>   ...>><xX(??>O<??)Xx><<
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From samaali at illite.u-strasbg.fr  Mon Aug  9 09:16:27 2004
From: samaali at illite.u-strasbg.fr (Mehrez Samaali)
Date: Mon, 09 Aug 2004 09:16:27 +0200
Subject: [R] import data from splus
Message-ID: <6.1.1.1.0.20040807162833.01ba5d78@mailserver.u-strasbg.fr>

Dear Sir,
I am using R for the first time and I am looking for a function or a 
command that allows to import Splus data into R. I have already heard about 
functions called "readSdata" and readSfiles" in the "Rstreams" package. I 
did not find the last new version of this package on the CRAN and its old 
one did not work on R 1.8.0 (the vesrion that I am using).
Would you please mention me the updated package that I should install and 
possibly the suitable function to apply.

Thanks in adavance.
M.S.



From ripley at stats.ox.ac.uk  Mon Aug  9 09:29:24 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 9 Aug 2004 08:29:24 +0100 (BST)
Subject: [R] Time zones
In-Reply-To: <Pine.LNX.4.60.0408091314520.1368@stat71.stat.auckland.ac.nz>
Message-ID: <Pine.LNX.4.44.0408090803290.12126-100000@gannet.stats>

It depends on your OS, which you haven't told us.  And OSes tend to hide 
the information quite effectively.

On a Unix/Linux system, try 'man tzset'.  That reads named timeszones from
a directory, nowadays /usr/share/zoneinfo or /usr/share/lib/zoneinfo or
something like that.

On Windows, the C runtime has rules like (from the _tzset entry in MSDN)

  TZ=tzn[+|-]hh[:mm[:ss] ][dzn]

  tzn
  Three-letter time-zone name, such as PST. You must specify the correct 
  offset from local time to UTC.

  hh
  Difference in hours between UTC and local time. Optionally signed.

  mm
  Minutes. Separated from hh by a colon (:).

  ss
  Seconds. Separated from mm by a colon (:).

  dzn
  Three-letter daylight-saving-time zone such as PDT. If daylight saving 
  time is never in effect in the locality, set TZ without a value for dzn. 
  The C run-time library assumes the United States's rules for 
  implementing the calculation of Daylight Saving Time (DST). 

so it seems EST5EDT would be correct (but I didn't find a list of 
names).

Did you try EST5EDT to see if it seemed to work?


On Mon, 9 Aug 2004, David Scott wrote:

> I am analysing some data collected over a number of months from Allentown, 
> PA, which is just north of Philadelphia. I am using as.POSIXct for dates 
> and times, and I need to get the timezone specification correct.
> 
> Going on the documentation for DateTimeClasses, I believe one way to 
> specify the correct time zone is
> 
> tz="EST5EDT"
> 
> I would be grateful for any advice on this. I ask because I haven't been 
> able to find a list of all the possible time zones or formats that can be 
> used. I have done a bit of googling without to much success.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Aug  9 09:31:11 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 9 Aug 2004 08:31:11 +0100 (BST)
Subject: [R] returns the value of a polynomial of degree n evaluated at x. 
In-Reply-To: <032A8573186A2B4EBBAEFA5784D0523502BCB031@sagemsg0007.sagemsmrd01.sa.gov.au>
Message-ID: <Pine.LNX.4.44.0408090830210.12126-100000@gannet.stats>

See package polynom on CRAN.

On Mon, 9 Aug 2004, McClatchie, Sam (PIRSA-SARDI) wrote:

> Is there a function in R that is similar to polyval in matlab? (y =
> polyval(p,x) returns the value of a polynomial of degree n evaluated at x.
> The input argument p is a vector of length n+1 whose elements are the
> coefficients in descending powers of the polynomial to be evaluated).
> <http://www.mathworks.com/access/helpdesk/help/techdoc/ref/polyval.html> 
>  
> I have possibly missed it in the R help files, so please excuse me if I
> have. "Polynomial evaluation" yielded no result in a search of help, and the
> functions poly() and locpoly() don't seem to be what I am after.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Aug  9 09:39:23 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 9 Aug 2004 08:39:23 +0100 (BST)
Subject: [R] import data from splus
In-Reply-To: <6.1.1.1.0.20040807162833.01ba5d78@mailserver.u-strasbg.fr>
Message-ID: <Pine.LNX.4.44.0408090832320.12126-100000@gannet.stats>

Please read the `R Data Import/Export' Manual that ships with R.  You are
looking for package foreign.

On Mon, 9 Aug 2004, Mehrez Samaali wrote:

> I am using R for the first time and I am looking for a function or a 
> command that allows to import Splus data into R. I have already heard about 
> functions called "readSdata" and readSfiles" in the "Rstreams" package. 

Those functions were made obselete by package foreign in April 2001.  
However did a first-time user hear about them?

> I did not find the last new version of this package on the CRAN

It's in the Archive.

> and its old one did not work on R 1.8.0 (the vesrion that I am using).

That's already rather old, but does have package foreign.

> Would you please mention me the updated package that I should install and 
> possibly the suitable function to apply.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jinss at hkusua.hku.hk  Mon Aug  9 09:41:52 2004
From: jinss at hkusua.hku.hk (Jin Shusong)
Date: Mon, 9 Aug 2004 15:41:52 +0800
Subject: [R] ess settings
Message-ID: <20040809074152.GA6568@S77>

Dear R users,

  I downloaded an ess-5.2.2 package and hope to use emacs as
front to work with R.  I modified ~/.emacs and start R by
M-x R.   But I found when I quited  the R session, it still asked me to
save the work space or not.  I added inferior-R-args with
"--save" args.  But it seemed that it had no effect.   Can
you tell me how to pass the arguments other than
--no-readline to R.  Thanks a lot.
Below is the part for ess settings.

    (setq load-path (cons "~/.emacs.d/lisp/ess-5.2.2/lisp" load-path))
    (require 'ess-site)
    (setq ess-default-style  "GNU")
    (setq inferior-ess-program "R")
    (setq inferior-R-program-name "~/.local/bin/R")	  ; unix systems
    (setq inferior-R-args "--save --no-readline")
    (setq ess-pre-run-hook
      '((lambda ()  (setq ess-directory "~/study/R/")
                    (setq ess-ask-for-ess-directory nil)
                    )))
-- 


                       Yours  Sincerely

                               Jin



From t.dewez at brgm.fr  Mon Aug  9 11:15:58 2004
From: t.dewez at brgm.fr (Dewez Thomas)
Date: Mon, 9 Aug 2004 11:15:58 +0200 
Subject: [R] displaying computation outputs inside "for" loops
Message-ID: <D965434E9D6BD511AE3500306E01C8BE05DD68E7@SRV0015>

Dear R-users,

I am puzzled by for loops and am kind of ashamed to ask because it is so
simple. There must be something I am missing in the way they are executed.

Basically, I would like to iterate a given number of time and generate a
bunch of stats (that's what loops are designed for, right?). Before doing
this I simply want to test simple procedure and see if they work (ie got the
syntax right - my main problem as I am new to R - and produce expected
results).

Even for something as basic as
for (i in 1:3) {i}

I get no screen output. Shouldn't R systematically display i for every loop
just like I am requesting with invoking "i"? When checking at the end of the
looping, i is indeed assigned to 5 but I cannot get intermediate values.

Further testing shows that i takes all the values in turn.
> for (i in 1:3) {str(i)}
 int 1
 int 2
 int 3

but summary(i) doesn't display anything. Isn't there something weird with
this? Am I expecting something wrong and for loops just don't work that way,
unless using str() command? I tried print() and other descriptive commands
but to no avail.

A quick explanation would be grately appreciated

Thomas
***
Le contenu de cet e-mail et de ses pi??ces jointes est destin...{{dropped}}



From dimitris.rizopoulos at med.kuleuven.ac.be  Mon Aug  9 11:26:08 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Mon, 9 Aug 2004 11:26:08 +0200
Subject: [R] displaying computation outputs inside "for" loops
References: <D965434E9D6BD511AE3500306E01C8BE05DD68E7@SRV0015>
Message-ID: <009101c47df2$e7e13430$ad133a86@www.domain>

Hi Thomas,

is this what you would like to get,

for(i in 1:3){
  x <- rnorm(5)
  cat("the values of `x' are:", format(x), "\n")
  cat("computation", i, "finished\n\n")
}

for(i in 1:3) print(i)


I hope this helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Doctoral Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Dewez Thomas" <t.dewez at brgm.fr>
To: "'R mailing list'" <r-help at stat.math.ethz.ch>
Sent: Monday, August 09, 2004 11:15 AM
Subject: [R] displaying computation outputs inside "for" loops


> Dear R-users,
>
> I am puzzled by for loops and am kind of ashamed to ask because it
is so
> simple. There must be something I am missing in the way they are
executed.
>
> Basically, I would like to iterate a given number of time and
generate a
> bunch of stats (that's what loops are designed for, right?). Before
doing
> this I simply want to test simple procedure and see if they work (ie
got the
> syntax right - my main problem as I am new to R - and produce
expected
> results).
>
> Even for something as basic as
> for (i in 1:3) {i}
>
> I get no screen output. Shouldn't R systematically display i for
every loop
> just like I am requesting with invoking "i"? When checking at the
end of the
> looping, i is indeed assigned to 5 but I cannot get intermediate
values.
>
> Further testing shows that i takes all the values in turn.
> > for (i in 1:3) {str(i)}
>  int 1
>  int 2
>  int 3
>
> but summary(i) doesn't display anything. Isn't there something weird
with
> this? Am I expecting something wrong and for loops just don't work
that way,
> unless using str() command? I tried print() and other descriptive
commands
> but to no avail.
>
> A quick explanation would be grately appreciated
>
> Thomas
> ***
> Le contenu de cet e-mail et de ses pi??ces jointes est
destin...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Robert.Wania at mailbox.tu-dresden.de  Mon Aug  9 11:34:05 2004
From: Robert.Wania at mailbox.tu-dresden.de (Robert Wania)
Date: Mon, 9 Aug 2004 11:34:05 +0200
Subject: [R] Installing RMySQL on Windows XP for R 1.9.1
Message-ID: <LNEIJNDFENKGFIGPJAANMEGICCAA.Robert.Wania@mailbox.tu-dresden.de>

Hi,

I'm thankful for any help on installing RMySQL on Windows XP for R 1.9.1

The thread
http://tolstoy.newcastle.edu.au/R/help/04/01/1021.html
already tried to discuss the matter. It concludes that Windows Sources for Packages are alike the Linux ones.

So I extracted the files from the RMySQL_*.tar.gz from
http://stat.bell-labs.com/RS-DBI/download/index.html
and stuffed them into a zip-file which was willingly taken by the install routine of my RGui. It said the well known "updating HTML
package descriptions" but when using library(RMySQL) the error "Error in testRversion(descfields) : This package has not been
installed properly. See the Note in ?library". Obviously the note wasn't very helpful and so I ask you for any help.

Regards
Robert



From donald.lehmann at pharmacology.oxford.ac.uk  Mon Aug  9 11:25:43 2004
From: donald.lehmann at pharmacology.oxford.ac.uk (Donald Lehmann)
Date: Mon, 09 Aug 2004 10:25:43 +0100
Subject: [R] linear regression
Message-ID: <6.0.1.1.0.20040809100343.01e9a190@phar0083.herald.ox.ac.uk>

Dear Consultant

I've done linear regression successfully on R a few times before.  But this 
time it keeps telling me:-

"Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :
         0 (non-NA) cases"

The model is:-

fm1 <- lm(TS.CM ~ AGE + SEX + HFE.Y.01 + TFC2B.01 + HFE.Y.01*TFC2B.01, data 
= IRONresults, subset = DIAG2.1D == 0)
summary (fm1)

TS.CM is a continuous variable (%s), sex is coded 0 = women, 1 = men, 
DIAG2.1D is coded 0 = non-demented, 1 = ALzheimer's disease and the genes, 
HFE.Y.01 & TFC2B.01, are coded 0 = non-carrier and 1 = carrier

I've tried recoding the data to use 1 & 2, instead of 0 & 1, and I've 
removed the rows with missing data.  I've also tried putting "...lm(formula 
= TS.CM ~ ..."  But I always get the same error message

What am I doing wrong?

A related question: what's the minimum no of data points for regression 
analysis to work?  We have only 23 cases carrying both genes out of 447 and 
only 8 out of 264 in the above subset (ie non-demented).  I seem to 
remember hearing somewhere that you needed a minimum of ~30 (?), so 
probably this wouldn't work anyway.  Still, I'd like to know what I was 
doing wrong!

Many thanks

Donald (Lehmann)



From hb at maths.lth.se  Mon Aug  9 11:59:23 2004
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Mon, 9 Aug 2004 11:59:23 +0200
Subject: [R] Installing RMySQL on Windows XP for R 1.9.1
In-Reply-To: <LNEIJNDFENKGFIGPJAANMEGICCAA.Robert.Wania@mailbox.tu-dresden.de>
Message-ID: <000801c47df7$8c99cba0$e502eb82@hblaptop>

Hi, I cannot help you install RMySQL on Windows, but most likely RMySQL
contains non-R code too, such as C code, that requires compilation and then
your gunzip-untar-rezip approach will not work. 

For your information, you can use the RODBC package instead. I recently
downloaded and installed a MySQL server on my Windows XP *Pro* machine. Then
I use the RODBC package (on CRAN and installs directly on Windows) to
connect to it. In order for this to work you have to have an ODBC service
running on your machine. You will find one for most Windows versions called
MyODBC on the mysql.com download pages (links at the right margin). When it
works you will be connected to you db from R by 

library(RODBC)
channel <- odbcConnect("MyDatabase", uid="root")   # or some other username
df <- sqlQuery("select * from myTable")

See also ?RODBC. I just did the above two days ago from scratch and it was
pretty easy. I won't time to help you more now since I'm writing up my
thesis...

Cheers

Henrik Bengtsson


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Robert Wania
> Sent: Monday, August 09, 2004 11:34 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Installing RMySQL on Windows XP for R 1.9.1
> 
> 
> Hi,
> 
> I'm thankful for any help on installing RMySQL on Windows XP 
> for R 1.9.1
> 
> The thread http://tolstoy.newcastle.edu.au/R/help/04/01/1021.html
> already tried to discuss the matter. It concludes that 
> Windows Sources for Packages are alike the Linux ones.
> 
> So I extracted the files from the RMySQL_*.tar.gz from 
> http://stat.bell-labs.com/RS-DBI/download/index.html
> and stuffed them into a zip-file which was willingly taken by 
> the install routine of my RGui. It said the well known 
> "updating HTML package descriptions" but when using 
> library(RMySQL) the error "Error in testRversion(descfields) 
> : This package has not been installed properly. See the Note 
> in ?library". Obviously the note wasn't very helpful and so I 
> ask you for any help.
> 
> Regards
> Robert
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From hb at maths.lth.se  Mon Aug  9 12:00:55 2004
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Mon, 9 Aug 2004 12:00:55 +0200
Subject: [R] Installing RMySQL on Windows XP for R 1.9.1
Message-ID: <000901c47df7$c2f47010$e502eb82@hblaptop>

The code snippet was incorrect; here the correct one (I hope)

library(RODBC)
channel <- odbcConnect("MyDatabase", uid="root")   # or some other username
df <- sqlQuery(channel, "select * from myTable")

/Henrik

> -----Original Message-----
> From: Henrik Bengtsson [mailto:hb at maths.lth.se] 
> Sent: Monday, August 09, 2004 11:59 AM
> To: 'Robert Wania'; 'r-help at stat.math.ethz.ch'
> Subject: RE: [R] Installing RMySQL on Windows XP for R 1.9.1
> 
> 
> Hi, I cannot help you install RMySQL on Windows, but most 
> likely RMySQL contains non-R code too, such as C code, that 
> requires compilation and then your gunzip-untar-rezip 
> approach will not work. 
> 
> For your information, you can use the RODBC package instead. 
> I recently downloaded and installed a MySQL server on my 
> Windows XP *Pro* machine. Then I use the RODBC package (on 
> CRAN and installs directly on Windows) to connect to it. In 
> order for this to work you have to have an ODBC service 
> running on your machine. You will find one for most Windows 
> versions called MyODBC on the mysql.com download pages (links 
> at the right margin). When it works you will be connected to 
> you db from R by 
> 
> library(RODBC)
> channel <- odbcConnect("MyDatabase", uid="root")   # or some 
> other username
> df <- sqlQuery("select * from myTable")
> 
> See also ?RODBC. I just did the above two days ago from 
> scratch and it was pretty easy. I won't time to help you more 
> now since I'm writing up my thesis...
> 
> Cheers
> 
> Henrik Bengtsson
> 
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Robert Wania
> > Sent: Monday, August 09, 2004 11:34 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] Installing RMySQL on Windows XP for R 1.9.1
> > 
> > 
> > Hi,
> > 
> > I'm thankful for any help on installing RMySQL on Windows XP
> > for R 1.9.1
> > 
> > The thread http://tolstoy.newcastle.edu.au/R/help/04/01/1021.html
> > already tried to discuss the matter. It concludes that
> > Windows Sources for Packages are alike the Linux ones.
> > 
> > So I extracted the files from the RMySQL_*.tar.gz from
> > http://stat.bell-labs.com/RS-DBI/download/index.html
> > and stuffed them into a zip-file which was willingly taken by 
> > the install routine of my RGui. It said the well known 
> > "updating HTML package descriptions" but when using 
> > library(RMySQL) the error "Error in testRversion(descfields) 
> > : This package has not been installed properly. See the Note 
> > in ?library". Obviously the note wasn't very helpful and so I 
> > ask you for any help.
> > 
> > Regards
> > Robert
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> > 
>



From mkondrin at hppi.troitsk.ru  Tue Aug 10 15:25:07 2004
From: mkondrin at hppi.troitsk.ru (M.Kondrin)
Date: Tue, 10 Aug 2004 06:25:07 -0700
Subject: [R] /minus or /hyphen in PS output
Message-ID: <4118CCB3.7070203@hppi.troitsk.ru>

Dear R devolepers!
Wouldn't you change src/main/devPS.c file (patch will follow). Problem 
is  with  minus sign in ps output. The original code forces the name 
/minus to the character number 45 (-). But /minus symbol is not defined 
in standard Adobe encoding and (even worse) is not defined in most type1 
fonts (where the name /hyphen is used instead), in this case you will 
have an empty space instead of -. Redefining encoding for this fonts 
does not help because encoding vector in ps output got "corrected" and 
/hyphen (or any other name I have selected for 45'th character) is 
replaced with /minus. I have tried this patch and it works for me.

------------------------------------------------

449,450c449
<       //      if(c == 45) strcpy(dest, "/minus"); else
<           strcpy(dest, state->p0);
---
 >       if(c == 45) strcpy(dest, "/minus"); else strcpy(dest, state->p0);



From ripley at stats.ox.ac.uk  Mon Aug  9 12:22:26 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 9 Aug 2004 11:22:26 +0100 (BST)
Subject: [R] Installing RMySQL on Windows XP for R 1.9.1
In-Reply-To: <000801c47df7$8c99cba0$e502eb82@hblaptop>
Message-ID: <Pine.LNX.4.44.0408091114450.2556-100000@gannet.stats>

You do need a binary version of  RMySQL.

See http://cran.r-project.org/bin/windows/contrib/1.9/@ReadMe
which tells you to get it from

http://stat.bell-labs.com/RS-DBI/download

the very page you got the sources from.  I do think you should read all of
the pages you look at, as in both cases you have ignored crucial 
information.

On Mon, 9 Aug 2004, Henrik Bengtsson wrote:

> Hi, I cannot help you install RMySQL on Windows, but most likely RMySQL
> contains non-R code too, such as C code, that requires compilation and then
> your gunzip-untar-rezip approach will not work. 

It never works, and no one said it did.  The rw-FAQ answers this question,
and the message in the archives said

	Please consult the rw-FAQ for how to install packages. 

> For your information, you can use the RODBC package instead. I recently
> downloaded and installed a MySQL server on my Windows XP *Pro* machine. Then
> I use the RODBC package (on CRAN and installs directly on Windows) to
> connect to it. In order for this to work you have to have an ODBC service
> running on your machine. You will find one for most Windows versions called
> MyODBC on the mysql.com download pages (links at the right margin). When it
> works you will be connected to you db from R by 
> 
> library(RODBC)
> channel <- odbcConnect("MyDatabase", uid="root")   # or some other username
> df <- sqlQuery("select * from myTable")
> 
> See also ?RODBC. I just did the above two days ago from scratch and it was
> pretty easy. I won't time to help you more now since I'm writing up my
> thesis...
> 
> Cheers
> 
> Henrik Bengtsson
> 
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Robert Wania
> > Sent: Monday, August 09, 2004 11:34 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] Installing RMySQL on Windows XP for R 1.9.1
> > 
> > 
> > Hi,
> > 
> > I'm thankful for any help on installing RMySQL on Windows XP 
> > for R 1.9.1
> > 
> > The thread http://tolstoy.newcastle.edu.au/R/help/04/01/1021.html
> > already tried to discuss the matter. It concludes that 
> > Windows Sources for Packages are alike the Linux ones.
> > 
> > So I extracted the files from the RMySQL_*.tar.gz from 
> > http://stat.bell-labs.com/RS-DBI/download/index.html
> > and stuffed them into a zip-file which was willingly taken by 
> > the install routine of my RGui. It said the well known 
> > "updating HTML package descriptions" but when using 
> > library(RMySQL) the error "Error in testRversion(descfields) 
> > : This package has not been installed properly. See the Note 
> > in ?library". Obviously the note wasn't very helpful and so I 
> > ask you for any help.

It was *very* helpful.  Your failure to read the rw-FAQ was unhelpful.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Mon Aug  9 12:21:15 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 09 Aug 2004 12:21:15 +0200
Subject: [R] linear regression
In-Reply-To: <6.0.1.1.0.20040809100343.01e9a190@phar0083.herald.ox.ac.uk>
References: <6.0.1.1.0.20040809100343.01e9a190@phar0083.herald.ox.ac.uk>
Message-ID: <x2vffsfw84.fsf@biostat.ku.dk>

Donald Lehmann <donald.lehmann at pharmacology.oxford.ac.uk> writes:

> Dear Consultant
> 
> I've done linear regression successfully on R a few times before.  But
> this time it keeps telling me:-
> 
> "Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) :
>          0 (non-NA) cases"
> 
> The model is:-
> 
> fm1 <- lm(TS.CM ~ AGE + SEX + HFE.Y.01 + TFC2B.01 + HFE.Y.01*TFC2B.01,
> data = IRONresults, subset = DIAG2.1D == 0)
> summary (fm1)
> 
> TS.CM is a continuous variable (%s), sex is coded 0 = women, 1 = men,
> DIAG2.1D is coded 0 = non-demented, 1 = ALzheimer's disease and the
> genes, HFE.Y.01 & TFC2B.01, are coded 0 = non-carrier and 1 = carrier
> 
> I've tried recoding the data to use 1 & 2, instead of 0 & 1, and I've
> removed the rows with missing data.  I've also tried putting
> "...lm(formula = TS.CM ~ ..."  But I always get the same error message
> 
> What am I doing wrong?


You don't need to give the main effects when there's a "*" term
(that's a SASism, the R equivalent is ":" and a*b == a+b+a:b by
definition), but that is hardly the main problem.

Could you have a look at this? :

with(IRONresults, complete.cases(TS.CM, AGE, SEX, HFE.Y.01, TFC2B.01))

If you get all FALSE, you'll know what hit you...
 
> A related question: what's the minimum no of data points for
> regression analysis to work?  We have only 23 cases carrying both
> genes out of 447 and only 8 out of 264 in the above subset (ie
> non-demented).  I seem to remember hearing somewhere that you needed a
> minimum of ~30 (?), so probably this wouldn't work anyway.  Still, I'd
> like to know what I was doing wrong!

Technically, you just need linearly independent predictors and more
observations than parameters (incl. the intercept). Other bounds get
bandied about on what should be required for a *meaningful* analysis
(like "10 observations per parameter"), but these are quite heuristic
and empirical in nature.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From stephen at inf.ed.ac.uk  Mon Aug  9 12:27:56 2004
From: stephen at inf.ed.ac.uk (Stephen Eglen)
Date: Mon, 9 Aug 2004 11:27:56 +0100
Subject: [R] ess settings
In-Reply-To: <20040809074152.GA6568@S77>
References: <20040809074152.GA6568@S77>
Message-ID: <16663.20908.725076.701031@bushmills.inf.ed.ac.uk>


Dear Jin,

First, we have a mailing list for ESS questions; this is probably more
relevant than r-help for these questions.

 https://stat.ethz.ch/mailman/listinfo/ess-help

Just last week this came up, and although the ESS documentation
suggests setting inferior-R-args, it doesn't seem to be used.  I will
look into solutions for this.  IN the meantime, there is a temporary
workaround mentioned in the mailing lists; see thread starting

 https://stat.ethz.ch/pipermail/ess-help/2004-August/002031.html

Best wishes,
Stephen

Jin Shusong writes:
 > Dear R users,
 > 
 >   I downloaded an ess-5.2.2 package and hope to use emacs as
 > front to work with R.  I modified ~/.emacs and start R by
 > M-x R.   But I found when I quited  the R session, it still asked me to
 > save the work space or not.  I added inferior-R-args with
 > "--save" args.  But it seemed that it had no effect.   Can
 > you tell me how to pass the arguments other than
 > --no-readline to R.  Thanks a lot.
 > Below is the part for ess settings.
 > 
 >     (setq load-path (cons "~/.emacs.d/lisp/ess-5.2.2/lisp" load-path))
 >     (require 'ess-site)
 >     (setq ess-default-style  "GNU")
 >     (setq inferior-ess-program "R")
 >     (setq inferior-R-program-name "~/.local/bin/R")	  ; unix systems
 >     (setq inferior-R-args "--save --no-readline")
 >     (setq ess-pre-run-hook
 >       '((lambda ()  (setq ess-directory "~/study/R/")
 >                     (setq ess-ask-for-ess-directory nil)
 >                     )))
 > -- 
 > 
 > 
 >                        Yours  Sincerely
 > 
 >                                Jin
 > 
 >



From monica.palaseanu-lovejoy at stud.man.ac.uk  Mon Aug  9 12:31:23 2004
From: monica.palaseanu-lovejoy at stud.man.ac.uk (Monica Palaseanu-Lovejoy)
Date: Mon, 9 Aug 2004 11:31:23 +0100
Subject: [R] Memory failure!!!!
Message-ID: <E1Bu7R8-000NV2-1x@probity.mcc.ac.uk>

Hi,

I am trying to increase the memory R can use. I am running R 
under Windows on a machine with 2 GB of physical RAM and 4GB 
of paged memory.

I wrote in the R property windows --sdi --max-mem-size=4094M, 
but the R itself when it is doing a bayesian modelling (geoR) it 
stops at 1,096K and i get memory errors because it cannot 
allocate a new segment of about 500K of memory.

I don't have Visual Basic so i cannot use the other commands 
suggested in Help.

Also, if i am using memory.size(max=TRUE) i get a value 
corresponding to about 1024K, and if i am using 
memory.limit(size=NA) i get a value of about 4000K.

How can i force R to use more memory?

thank you for any suggestion,

Monica
Monica Palaseanu-Lovejoy
University of Manchester
School of Geography
Mansfield Cooper Bld. 3.21
Oxford Road
Manchester M13 9PL
England, UK
Tel: +44 (0) 275 8689
Email: monica.palaseanu-lovejoy at stud.man.ac.uk



From ce227 at cam.ac.uk  Mon Aug  9 12:39:22 2004
From: ce227 at cam.ac.uk (Cristian Echeverria)
Date: Mon, 9 Aug 2004 06:39:22 -0400
Subject: [R] GLS with binomial errors ?
Message-ID: <008a01c47dfd$21e7e520$f6256f83@plantsci.cam.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040809/261ea76f/attachment.pl

From paolo at directwave.com.br  Mon Aug  9 13:01:55 2004
From: paolo at directwave.com.br (Paolo Tommasini)
Date: Mon, 09 Aug 2004 08:01:55 -0300
Subject: [R] (no subject)
Message-ID: <411759A3.9030700@directwave.com.br>

Hi ! mu name is Paolo and  I teach statistic in a small college in 
Brazil and we use Linux. We started to use R as our statitical software. 
I,ve been trying to use some commands I see in the documentation online 
but i doesn not work. Commands like

simple.hist.and.boxplot  and  others related with graphing. Do I need a special ppackage ? how do I load it ?


Thanks



From ligges at statistik.uni-dortmund.de  Mon Aug  9 13:13:35 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 09 Aug 2004 13:13:35 +0200
Subject: [R] Memory failure!!!!
In-Reply-To: <E1Bu7R8-000NV2-1x@probity.mcc.ac.uk>
References: <E1Bu7R8-000NV2-1x@probity.mcc.ac.uk>
Message-ID: <41175C5F.2080409@statistik.uni-dortmund.de>

Monica Palaseanu-Lovejoy wrote:

> Hi,
> 
> I am trying to increase the memory R can use. I am running R 
> under Windows on a machine with 2 GB of physical RAM and 4GB 
> of paged memory.
> 
> I wrote in the R property windows --sdi --max-mem-size=4094M, 
> but the R itself when it is doing a bayesian modelling (geoR) it 
> stops at 1,096K and i get memory errors because it cannot 
> allocate a new segment of about 500K of memory.

In that case either
a) your memory is too fragmented and you should start a new R session or
b) or you need much more than 500K, and you are just getting the error 
from one allocation (out of many) that fails.

The next question is which version of Windows this is. Some versions do 
not support that much memory, others have a hole of 512M (???, PCI 
address space is within the 4GB address space, AFAIR)...


> I don't have Visual Basic so i cannot use the other commands 
> suggested in Help.
> 
> Also, if i am using memory.size(max=TRUE) i get a value 
> corresponding to about 1024K, and if i am using 
> memory.limit(size=NA) i get a value of about 4000K.

I hope you mean about 4000*M*...
Or more precisely, what you get is:
 > 4094*1024*1024
[1] 4292870144

Uwe Ligges


> How can i force R to use more memory?
> 
> thank you for any suggestion,
> 
> Monica
> Monica Palaseanu-Lovejoy
> University of Manchester
> School of Geography
> Mansfield Cooper Bld. 3.21
> Oxford Road
> Manchester M13 9PL
> England, UK
> Tel: +44 (0) 275 8689
> Email: monica.palaseanu-lovejoy at stud.man.ac.uk
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Robert.Wania at mailbox.tu-dresden.de  Mon Aug  9 13:17:53 2004
From: Robert.Wania at mailbox.tu-dresden.de (Robert Wania)
Date: Mon, 9 Aug 2004 13:17:53 +0200
Subject: [R] Installing RMySQL on Windows XP for R 1.9.1
In-Reply-To: <Pine.LNX.4.44.0408091114450.2556-100000@gannet.stats>
Message-ID: <LNEIJNDFENKGFIGPJAANIEGLCCAA.Robert.Wania@mailbox.tu-dresden.de>

I apologize for mixing the source and binary packages. (Although R didn't complain? Nevertheless this is no excuse)

After installing the binary package the only problem was the libmysql.dll, which was not found by R (but it was within the package).
I put it in the windows/system32 directory and everything works find.

Thanks for the instant help
Robert

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Prof Brian Ripley
Sent: Monday, August 09, 2004 12:22 PM
To: Henrik Bengtsson
Cc: r-help at stat.math.ethz.ch; 'Robert Wania'
Subject: RE: [R] Installing RMySQL on Windows XP for R 1.9.1


You do need a binary version of  RMySQL.

See http://cran.r-project.org/bin/windows/contrib/1.9/@ReadMe
which tells you to get it from

http://stat.bell-labs.com/RS-DBI/download

the very page you got the sources from.  I do think you should read all of
the pages you look at, as in both cases you have ignored crucial
information.

On Mon, 9 Aug 2004, Henrik Bengtsson wrote:

> Hi, I cannot help you install RMySQL on Windows, but most likely RMySQL
> contains non-R code too, such as C code, that requires compilation and then
> your gunzip-untar-rezip approach will not work.

It never works, and no one said it did.  The rw-FAQ answers this question,
and the message in the archives said

	Please consult the rw-FAQ for how to install packages.

> For your information, you can use the RODBC package instead. I recently
> downloaded and installed a MySQL server on my Windows XP *Pro* machine. Then
> I use the RODBC package (on CRAN and installs directly on Windows) to
> connect to it. In order for this to work you have to have an ODBC service
> running on your machine. You will find one for most Windows versions called
> MyODBC on the mysql.com download pages (links at the right margin). When it
> works you will be connected to you db from R by
>
> library(RODBC)
> channel <- odbcConnect("MyDatabase", uid="root")   # or some other username
> df <- sqlQuery("select * from myTable")
>
> See also ?RODBC. I just did the above two days ago from scratch and it was
> pretty easy. I won't time to help you more now since I'm writing up my
> thesis...
>
> Cheers
>
> Henrik Bengtsson
>
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Robert Wania
> > Sent: Monday, August 09, 2004 11:34 AM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] Installing RMySQL on Windows XP for R 1.9.1
> >
> >
> > Hi,
> >
> > I'm thankful for any help on installing RMySQL on Windows XP
> > for R 1.9.1
> >
> > The thread http://tolstoy.newcastle.edu.au/R/help/04/01/1021.html
> > already tried to discuss the matter. It concludes that
> > Windows Sources for Packages are alike the Linux ones.
> >
> > So I extracted the files from the RMySQL_*.tar.gz from
> > http://stat.bell-labs.com/RS-DBI/download/index.html
> > and stuffed them into a zip-file which was willingly taken by
> > the install routine of my RGui. It said the well known
> > "updating HTML package descriptions" but when using
> > library(RMySQL) the error "Error in testRversion(descfields)
> > : This package has not been installed properly. See the Note
> > in ?library". Obviously the note wasn't very helpful and so I
> > ask you for any help.

It was *very* helpful.  Your failure to read the rw-FAQ was unhelpful.

--
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Mon Aug  9 13:19:31 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 9 Aug 2004 12:19:31 +0100 (BST)
Subject: [R] Memory failure!!!!
In-Reply-To: <E1Bu7R8-000NV2-1x@probity.mcc.ac.uk>
Message-ID: <Pine.LNX.4.44.0408091214100.2716-100000@gannet.stats>

On Mon, 9 Aug 2004, Monica Palaseanu-Lovejoy wrote:

> I am trying to increase the memory R can use. I am running R 
> under Windows on a machine with 2 GB of physical RAM and 4GB 
> of paged memory.
> 
> I wrote in the R property windows --sdi --max-mem-size=4094M, 
> but the R itself when it is doing a bayesian modelling (geoR) it 
> stops at 1,096K and i get memory errors because it cannot 
> allocate a new segment of about 500K of memory.

Please read the rw-FAQ.  Windows cannot allocate 4Gb to a user process, 
and unless you have followed the instructions there you are limited to 
2Gb (and probably a bit less).

Also please read the posting guide and report the exact message you got,
as it requests. Unless it says something about memory limits, it is
because the memory could not be obtained from the OS.

> I don't have Visual Basic so i cannot use the other commands 
> suggested in Help.

What has Visual Basic to do with this?

> Also, if i am using memory.size(max=TRUE) i get a value 
> corresponding to about 1024K, and if i am using 
> memory.limit(size=NA) i get a value of about 4000K.
> 
> How can i force R to use more memory?

By making it available to R.  Try rebooting your machine before running R, 
to reduce memory fragmentation.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tobias_verbeke at skynet.be  Mon Aug  9 13:49:26 2004
From: tobias_verbeke at skynet.be (Tobias Verbeke)
Date: Mon, 9 Aug 2004 11:49:26 +0000
Subject: [R] (no subject)
In-Reply-To: <411759A3.9030700@directwave.com.br>
References: <411759A3.9030700@directwave.com.br>
Message-ID: <20040809114926.393c71f2.tobias_verbeke@skynet.be>

On Mon, 09 Aug 2004 08:01:55 -0300
Paolo Tommasini <paolo at directwave.com.br> wrote:

> Hi ! mu name is Paolo and  I teach statistic in a small college in 
> Brazil and we use Linux. We started to use R as our statitical software. 
> I,ve been trying to use some commands I see in the documentation online 
> but i doesn not work. Commands like
> 
> simple.hist.and.boxplot  and  others related with graphing. Do I need a special ppackage ? how do I load it ?
> 

This function is probably a function of the package "Simple"
that accompanies the user-contributed introduction to R "simpleR"

http://www.math.csi.cuny.edu/Statistics/R/simpleR/index.html

Download the tarball on this webpage 

http://www.math.csi.cuny.edu/Statistics/R/simpleR/Simple_0.4.tar.gz

and install it (as root) as follows:

# R CMD INSTALL Simple_0.4.tar.gz

Then (as a normal user) you can load the package (from within
R using the library() command)

> library(Simple)

to see an example of this simple.hist.and.boxplot() function
do

> example(simple.hist.and.boxplot)

to see the help for it, use

>?simple.hist.and.boxplot

For graphics commands you do not need to use contributed
packages. On a standard R installation, a lot is already
available.

See ?hist, ?boxplot, ?plot etc. or the Graphics section
of the R manual "An Introduction to R".

HTH,
Tobias



From ligges at statistik.uni-dortmund.de  Mon Aug  9 14:01:21 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 09 Aug 2004 14:01:21 +0200
Subject: [R] looking for graphic functions; was: (no subject)
In-Reply-To: <411759A3.9030700@directwave.com.br>
References: <411759A3.9030700@directwave.com.br>
Message-ID: <41176791.40805@statistik.uni-dortmund.de>

Paolo Tommasini wrote:

> Hi ! mu name is Paolo and  I teach statistic in a small college in 
> Brazil and we use Linux. We started to use R as our statitical software. 
> I,ve been trying to use some commands I see in the documentation online 
> but i doesn not work. Commands like
> 
> simple.hist.and.boxplot  and  others related with graphing. Do I need a 
> special ppackage ? how do I load it ?
> 

Please use a sensible subject line (and see the posting guide mentioned 
below).
Where did you find anything about a function "simple.hist.and.boxplot"?
If you are talking about the online document "simpleR" by John Verzani, 
I'd suggest to look at the corresponding R package mentioned in that 
document. If it is not in there, either hack it yourself or ask the author.

Have you already looked at ?hist and ?boxplot?

Uwe Ligges


> Thanks
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From i.visser at uva.nl  Mon Aug  9 13:57:23 2004
From: i.visser at uva.nl (Ingmar Visser)
Date: Mon, 09 Aug 2004 13:57:23 +0200
Subject: [R] linear constraint optim with bounds/reparametrization
Message-ID: <BD3D3343.5C42%i.visser@uva.nl>

Hello All,

I would like to optimize a (log-)likelihood function subject to a number of
linear constraints between parameters. These constraints are equality
constraints of the form A%*%theta=c, ie (1,1) %*% 0.8,0.2)^t = 1 meaning
that these parameters should sum to one. Moreover, there are bounds on the
individual parameters, in most cases that I am considering parameters are
bound between zero and one because they are probabilities.
My problems/questions are the following:

1) constrOptim does not work in this case because it only fits inequality
constraints, ie A%*%theta > =  c
                          ---
As a result when providing starting values constrOptim exits with an error
saying that the initial point is not feasible, which it isn't because it is
not in the interior of the constraint space.

Is there an alternative to constrOptim that can handle such strict
(equality) linear constraints?

2) Another option of course would be to reparametrize the problem as
follows. I will illustrate with an example:

I have parameters:
> p
      [,1]
 [1,]  0.8
 [2,]  0.2
 [3,]  0.2
 [4,]  0.8
 [5,]  0.6
 [6,]  0.1
 [7,]  0.3
 [8,]  0.1
 [9,]  0.3
[10,]  0.6
[11,]  0.5
[12,]  0.5

and the following constraints (all these constraint amount to certain
probabilities summing to one, ie the first two parameters sum to one, the
second pair of parameters  sum to one etc):

> A
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]
[1,]    1    1    0    0    0    0    0    0    0     0     0     0
[2,]    0    0    1    1    0    0    0    0    0     0     0     0
[3,]    0    0    0    0    1    1    1    0    0     0     0     0
[4,]    0    0    0    0    0    0    0    1    1     1     0     0
[5,]    0    0    0    0    0    0    0    0    0     0     1     1

and the bounds on  the constraints are:

> ci
      [,1]
 [1,]    1
 [2,]    1
 [3,]    1
 [4,]    1
 [5,]    1

The bounds on the parameters are all zero and one. In order to reparametrize
I could use z=ginv(b)%*%(p-ginv(A)%*%cc) with b=Null(t(A)) and optimize z
instead of p using p=ginv(a)%*%ci + b%*%z

My question is however what the bounds on z are?

In the above example z is:
> z
            [,1]
[1,] -0.23333333
[2,] -0.03333333
[3,] -0.57974349
[4,] -0.37974349
[5,] -0.07974349
[6,] -0.18856181
[7,] -0.18856181

which conforms to the constraints, so these values can be used as an intial
estimate. If I knew the bounds on z I could use optim with method="L-BFGS".

I hope this problem is sufficiently clear to elicit response, if not please
let me know.

ingmar

ps: to reproduce above example use the following:

p=matrix(c(0.8, 0.2, 0.20, 0.8, 0.6, 0.1, 0.3, 0.1, 0.3, 0.6, 0.5,
0.5),nc=1)

A = matrix(c(1, 1, 0, 0, 0, 0, 0, 0, 0,  0,  0,  0
, 0, 0, 1, 1, 0, 0, 0, 0, 0,  0,  0,  0
, 0, 0, 0, 0, 1, 1, 1, 0, 0,  0,  0,  0
, 0, 0, 0, 0, 0, 0, 0, 1, 1,  1,  0,  0
, 0, 0, 0, 0, 0, 0, 0, 0, 0,  0,  1,  1),nc=12,byrow=T)

ci=matrix(rep(1,5),nc=1)

b=Null(t(A))

z=ginv(b)%*%(p-ginv(A)%*%cc)

pp=ginv(a)%*%ci + b%*%z



From maechler at stat.math.ethz.ch  Mon Aug  9 10:45:46 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 9 Aug 2004 10:45:46 +0200
Subject: [R] URL change for Statistik ETH and web interface to mailing lists.
Message-ID: <16663.14778.68030.32860@gargle.gargle.HOWL>

       [exceptional cross-posting on purpose]

Dear users/readers of our mailing lists,

The following ONLY affects the web interface to our mailing lists. 
This is both for subscription changes and the mailing list archives.

For a long time, our "Statistics ETHZ" home page has been
available both as
    http://www.stat.math.ethz.ch/	[old default]
and http://stat.ethz.ch/		[new default]

For several reasons however, the longer URL above has been the
``default'', or the one to which stat.ethz.ch was automagically converted.
This now has finally been switched two hours ago, such that
http://stat.ethz.ch/  is now the official URL in all respects and
http://www.stat.math.ethz.ch/ is just an alias to the new URL
(and it seems to behave strangely just now for me, but that
 should be very temporary).

For existing mailing lists and their web interface, the change
change has to happen ``inside mailman'' (by calling the correct
python script and changing the lists URL explictly) 
once for each list.

I wanted to first announce this widely and do the change in
about 24 hours or so.
You may have to accept the SSL certificate again {and it's not
from a so called "trusted agency" since that would cost us lots
of money we rather spend differently}.

Regards,

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH Zurich, Switzerland



From michael.waters at dtn.ntl.com  Mon Aug  9 15:13:29 2004
From: michael.waters at dtn.ntl.com (Dr Mike Waters)
Date: Mon, 9 Aug 2004 14:13:29 +0100
Subject: [R] R packages install problems linux - X not found (WhiteBoxEL 3)
In-Reply-To: <1091994406.25174.58.camel@localhost.localdomain>
Message-ID: <00ca01c47e12$a9ded280$6600a8c0@mainman>

> I am concerned by your indications of previously having had RH9 on the
> same box and that you had to force an update of the XFree Devel RPM.
> Forcing the installation of an RPM is almost always a bad thing.
>
> When you installed WB on the system, did you do a "clean" installation
> or some type of "upgrade"?
>
> If the latter, it is reasonable to consider that there may be some level
> of mixing and matching of RPMS from the two distributions going on. This
> could result in a level of marginally or wholly incompatible versions of
> RPMS being installed.
>
> Could you clarify that point?
>
> Also, be sure that you have the same versions of the XFree series RPMS
> installed.
>
> Use:
>
> rpm -qa | grep XFree
>
> in a console and be sure that the RPMS return the same version schema.
> If not, it is possible that one of your problems is the mixing of
> versions.
>
> Take note of the output of the above and be sure that the
> XFree86-Mesa-libGL and XFree86-Mesa-libGLU RPMS are installed as well.
>
> Some of the messages above would also suggest a problem finding R
> related headers. How did you install R? This may be a red herring of
> sorts, given the other problems, but may be helpful.
>
> Marc
>
> ______________________________________________


Marc,

Sorry for the confusion yesterday - in my defence, it was very hot and humid
here in Hampshire (31 Celsius at 15:00hrs and still 25 at 20:00hrs). 

What had happened was that I had done a clean install of WB Linux, including
the XFree86 and other developer packages. However, the on-line updating
system updated the XFree86 packages to a newer sub version. It seems that it
didn't do this correctly for the XFree86 developer package, which was
missing vital files. However it showed up in the rpm database as being
installed (i.e. rpm -qa | grep XFree showed it thus). I downloaded another
rpm for this manually and I only forced the upgrade because it was the same
version as already 'installed' (as far as the rpm database was concerned). I
assumed that all dependencies were sorted out through the install in the
first place.

I only mentioned RH9 to show that I had some familiarity with the RedHat
policy of separating out the 'includes' etc into a separate developer
package.

Once all this had been sorted out, I was then left with a compilation error
which pointed to a missing dependency or similar, which was not due to
missing developer packages, but, as you and Prof Ripley correctly point out,
from the R installation itself. Having grown fat and lazy on using R under
the MS Windows environment, I was struggling to identify the precise nature
of this remaining problem.

As regards the R installation, I did this from the RH9 binary for version
1.9.1, as I did not think that the Fedora Core 2 binary would be appropriate
here. Perhaps I should now compile from the source instead?

Regards

M



From Matt.Nunes at bristol.ac.uk  Mon Aug  9 15:33:52 2004
From: Matt.Nunes at bristol.ac.uk (Matt Nunes)
Date: Mon, 09 Aug 2004 14:33:52 +0100
Subject: [R] R Matrix package solve
In-Reply-To: <Pine.LNX.4.51.0408041525320.819@artemis.imbe.med.uni-erlangen.de>
References: <Pine.LNX.4.51.0408041525320.819@artemis.imbe.med.uni-erlangen.d e>
Message-ID: <18600000.1092062032@pc277.maths.bris.ac.uk>

torsten,

thanks for the mail.  I have reinstalled the Matrix package, but still get 
the same problems.  Any ideas?

matt nunes.

--On 04 August 2004 3:26pm +0200 Torsten Hothorn 
<Torsten.Hothorn at rzmail.uni-erlangen.de> wrote:

>
> On Wed, 4 Aug 2004, Matt Nunes wrote:
>
>> hello.
>>
>> I have a query about the Matrix package for R.  I wrote some code a while
>> ago using the Matrix package version 1.6.2 with an early version of R, to
>> do some linear least squares for regression:
>>
>> xn
>>      [,1]      [,2]      [,3]      [,4]
>> [1,]    1 0.7429352 0.5519528 0.4100652
>> [2,]    1 0.7443713 0.5540886 0.4124477
>> [3,]    1 0.7447385 0.5546355 0.4130584
>> [4,]    1 0.7459597 0.5564558 0.4150936
>> >
>> > temp<-crossprod(xn)
>> > temp
>>          [,1]     [,2]      [,3]      [,4]
>> [1,] 4.000000 2.978005 2.2171327 1.6506648
>> [2,] 2.978005 2.217133 1.6506648 1.2289297
>> [3,] 2.217133 1.650665 1.2289297 0.9149473
>> [4,] 1.650665 1.228930 0.9149473 0.6811865
>> >
>> > solve.Matrix(temp,t(xn))
>>            [,1]      [,2]      [,3]       [,4]
>> [1,]   33397.34  122081.7 -241005.4   85527.48
>> [2,]   21063.72 -664920.2  812316.0 -168459.99
>> [3,] -236935.74 1125548.2 -877776.2  -10835.64
>> [4,]  199314.69 -608045.8  297509.1  111221.72
>>
>> (Note:  here I used solve.Matrix since the generic solve said the matrix
>> is singular).
>>
>> I recently updated my versions of R to 1.9.1 and also the Matrix package,
>> but I can't seem to get any similar equivalent matrix calculations to
>> work (I get error messages like ".Call function name not in DLL for
>> package Matrix"
>
> this very much looks like a corrupted package installation. Try
> to reinstall  `Matrix'
>
> Torsten
>
>> or "Lapack routine dpotrf returned error code 4").  I have also
>> tried using symmetric matrix commands, but to no avail. Can anyone help
>> me out?
>>
>> many thanks
>>
>> Matt Nunes
>>
>>



From kahra at mpsgr.it  Mon Aug  9 15:58:26 2004
From: kahra at mpsgr.it (Kahra Hannu)
Date: Mon, 9 Aug 2004 15:58:26 +0200
Subject: [R] linear constraint optim with bounds/reparametrization
Message-ID: <C9FC71F7E9356F40AFE2ACC2099DE147149633@MAILSERVER-B.mpsgr.it>

> from Ingmar Visser:

>I would like to optimize a (log-)likelihood function subject to a number of
>linear constraints between parameters. These constraints are equality
>constraints of the form A%*%theta=c, ie (1,1) %*% 0.8,0.2)^t = 1 meaning
>that these parameters should sum to one. Moreover, there are bounds on the
>individual parameters, in most cases that I am considering parameters are
>bound between zero and one because they are probabilities.
>My problems/questions are the following:

>1) constrOptim does not work in this case because it only fits inequality
>constraints, ie A%*%theta > =  c
                          ---
I was struggling with the same problem a few weeks ago in the portfolio optimization context. You can impose equality constraints by using inequality constraints >= and <= simultaneously. See the example bellow. 

>As a result when providing starting values constrOptim exits with an error
>saying that the initial point is not feasible, which it isn't because it is
>not in the interior of the constraint space.

In constrOptim the feasible region is defined by ui%*%theta-ci >=0. My first attempt to obtain feasible starting values was based on solving for theta from ui%*%theta = ci. Some of the items in the right hand side of the feasible region are, however, very often very small negative numbers, and hence, theta is not feasible. Next, I started to increase ci by epsilon ("a slack variable") and checked if the result was feasible. The code is in the example bellow. If ui is not a square matrix, theta is obtained by simulation. It is helpfull to know the upper and lower limits of the theta vector. In my case they are often [0,1]. Usually only 2-3 simulations are required.

In the example, the weights (parameters) have limits [0,1] such that their sum is limited to unity, as in your case. See, how Amat and b0 are defined.

V <- matrix(c(0.12,0,0,0,0.21,0,0,0,0.28),3,3)              # variances
Cor <- matrix(c(1,0.25,0.25,0.25,1,0.45,0.05,0.45,1),3,3)   # correlations
sigma <- t(V)%*%Cor%*%V                                     # covariance matrix
ER <- c(0.07,0.12,0.18)                                     # expected returns
Dmat <- sigma                                               # adopted from solve.QP
dvec <- rep(0,3)                                            #    "      "     "
k <- 3                                                      # number of assets
reslow <- rep(0,k)                                          # lower limits for portfolio weights
reshigh <- rep(1,k)                                         # upper limits for portfolio weights
pm <- 0.10                                                  # target return                      
rfree <- 0.05                                               # risk-free return
risk.aversion <- 2.5                                        # risk aversion parameter
####### RISKLESS = FALSE; SHORTS = TRUE ; CONSTRAINTS = TRUE ########################################
a1 <- rep(1, k)                
a2 <- as.vector(ER)+rfree  
a3 <- matrix(0,k,k)
diag(a3) <- 1    
Amat <- t(rbind(a1, a2, a3, -a3))
b0 <- c(1,pm,reslow, -reshigh)

objectf.mean <- function(x)                                                 # object function
{
return(sum(t(x)%*%ER-1/2*risk.aversion*t(x)%*%sigma%*%x)-(t(x)%*%ER-pm)) 
}

# Getting starting values for constrOptim

        ui <- t(Amat)                         
        ci <- b0
        if (dim(ui)[1] == dim(ui)[2])     
        {
        test <- F
        cieps <- ci
            while(test==F) 
            {
            theta <- qr.solve(ui,cieps)
            foo <- (ui%*%theta-ci)              # check if the initial values are in the feasible area
            test <- all(foo > 0)
            if(test==T) initial <- theta        # initial values for portfolio weights
            cieps <- cieps+0.1
            }
         } 
         if (dim(ui)[1] != dim(ui)[2])          # if Amat is not square, simulate the starting values        
         {
        test <- F
        i <- 1
        while(test==F)
            {
            theta <- runif(k, min = 0, max = 1)
            foo <- (ui%*%theta-ci)
            test <- all(foo > 0)                # and check that theta is feasible
            if (test == T) initial <- theta
            i <- i+1
            }    
        }

initial

res <- constrOptim(initial, objectf.mean, NULL, ui=t(Amat), ci=b0, control = list(fnscale=-1))
res$par                                     # portfolio weights (parameters)                    
y <- t(res$par)%*%ER                        # return on the portfolio
y



I hope this helps.

Hannu Kahra 
Progetti Speciali 
Monte Paschi Asset Management SGR S.p.A. 
Via San Vittore, 37 - 20123 Milano, Italia 
Tel.: +39 02 43828 754 
Mobile: +39 333 876 1558 
Fax: +39 02 43828 247 
E-mail: kahra at mpsgr.it 
Web: www.mpsam.it 

"Le informazioni trasmesse sono da intendersi per esclusivo uso del destinatario e possono contenere informazioni e materiale confidenziale e privilegiato. Qualsiasi correzione, inoltro e divulgazione in qualsiasi forma e modo anche a tenore generale ?? del tutto proibita. Se avete ricevuto per errore il presente messaggio, cortesemente contattate subito il mittente e cancellate da qualsiasi supporto il messaggio e gli allegati a voi giunti. Tutti gli usi illegali saranno perseguiti penalmente e civilmente da Monte Paschi Asset Management SGR S.p.A."

"The information transmitted are intended only for the person or entity to wich it is addressed and may contain confidential and/or privileged material. Any review, retransmission, dissemination, taking of any action in reliance upon, or other general use are strictly prohibited. If you received this in error, please contact immediately the sender and delete the material from any computer. All the illegal uses will be persecuted by Monte Paschi Asset Management SGR S.p.A."



From MSchwartz at MedAnalytics.com  Mon Aug  9 16:12:40 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon, 09 Aug 2004 09:12:40 -0500
Subject: [R] R packages install problems linux - X not found (WhiteBoxEL 3)
In-Reply-To: <00ca01c47e12$a9ded280$6600a8c0@mainman>
References: <00ca01c47e12$a9ded280$6600a8c0@mainman>
Message-ID: <1092060760.25174.134.camel@localhost.localdomain>

On Mon, 2004-08-09 at 08:13, Dr Mike Waters wrote:

<snip>

> Marc,
> 
> Sorry for the confusion yesterday - in my defence, it was very hot and humid
> here in Hampshire (31 Celsius at 15:00hrs and still 25 at 20:00hrs). 
> 
> What had happened was that I had done a clean install of WB Linux, including
> the XFree86 and other developer packages. However, the on-line updating
> system updated the XFree86 packages to a newer sub version. It seems that it
> didn't do this correctly for the XFree86 developer package, which was
> missing vital files. However it showed up in the rpm database as being
> installed (i.e. rpm -qa | grep XFree showed it thus). I downloaded another
> rpm for this manually and I only forced the upgrade because it was the same
> version as already 'installed' (as far as the rpm database was concerned). I
> assumed that all dependencies were sorted out through the install in the
> first place.

OK, that helps. I still have a lingering concern that, given the facts
above, there may be other integrity issues in the RPM database, if not
elsewhere.

>From reading the WB web site FAQ's
(http://www.whiteboxlinux.org/faq.html) , it appears that they are using
up2date/yum for system updates. Depending upon the version in use, there
have been issues especially with up2date (hangs, incomplete updates,
etc.) which could result in other problems. I use yum via the console
here (under FC2), though I note that a GUI version of yum has been
created, including replacing the RHN/up2date system tray alert icon.

A thought relative to this specifically:

If there is or may be an integrity problem related to the rpm database,
you should review the information here:

http://www.rpm.org/hintskinks/repairdb/

which provides instructions on repairing the database. Note the
important caveats regarding backups, etc.

The two key steps there are to remove any residual lock files using (as
root):

rm -f /var/lib/rpm/__*

and then rebuilding the rpm database using (also as root):

rpm -vv --rebuilddb

I think that there needs to be some level of comfort that this basic
foundation for the system is intact and correct.

> I only mentioned RH9 to show that I had some familiarity with the RedHat
> policy of separating out the 'includes' etc into a separate developer
> package.
> 
> Once all this had been sorted out, I was then left with a compilation error
> which pointed to a missing dependency or similar, which was not due to
> missing developer packages, but, as you and Prof Ripley correctly point out,
> from the R installation itself. Having grown fat and lazy on using R under
> the MS Windows environment, I was struggling to identify the precise nature
> of this remaining problem.
> 
> As regards the R installation, I did this from the RH9 binary for version
> 1.9.1, as I did not think that the Fedora Core 2 binary would be appropriate
> here. Perhaps I should now compile from the source instead?

I would not use the FC2 RPM, since FC2 has many underlying changes not
the least of which includes the use of the 2.6 kernel series and the
change from XFree86 to x.org. Both changes resulted in significant havoc
during the FC2 testing phases and there was at least one issue here with
R due to the change in X.

According to the WB FAQs:

"If you cannot find a package built specifically for RHEL3 or WBEL3 you
can try a package for RH9 since many of the packages in RHEL3 are the
exact same packages as appeared in RH9."

Thus, it would seem reasonable to use the RH9 RPM that Martyn has
created. An alternative would certainly be to compile R from the source
tarball.

In either case, I would remove the current installation of R and after
achieving a level of comfort that your RPM database is OK, reinstall R
using one of the above methods. Pay close attention to any output during
the installation process, noting any error or warning messages that may
occur.

If you go the RPM route, be sure that the MD5SUM of the RPM file matches
the value that Martyn has listed on CRAN to ensure that the file has
been downloaded in an intact fashion.

These are my thoughts at this point. You need to get to a point where
the underlying system is stable and intact, then get R to the same state
before attempting to install new packages.

HTH,

Marc



From tlumley at u.washington.edu  Mon Aug  9 16:28:30 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 9 Aug 2004 07:28:30 -0700 (PDT)
Subject: [R] displaying computation outputs inside "for" loops
In-Reply-To: <D965434E9D6BD511AE3500306E01C8BE05DD68E7@SRV0015>
References: <D965434E9D6BD511AE3500306E01C8BE05DD68E7@SRV0015>
Message-ID: <Pine.A41.4.58.0408090726240.263586@homer10.u.washington.edu>

On Mon, 9 Aug 2004, Dewez Thomas wrote:

> Dear R-users,
>
> I am puzzled by for loops and am kind of ashamed to ask because it is so
> simple. There must be something I am missing in the way they are executed.
>

If you want an explanation, rather than just a way to get printing, look
at FAQ 7.18.

	-thomas



From medvedm at UCMAIL.UC.EDU  Mon Aug  9 16:47:16 2004
From: medvedm at UCMAIL.UC.EDU (Medvedovic, Mario (medvedm))
Date: Mon, 9 Aug 2004 10:47:16 -0400 
Subject: [R] hclust-segmentation fault
Message-ID: <886EF25AF8BEF64EB89A820EF84064FF052E1485@UCMAIL4>

I am getting the "Segmentation fault" when using hclust in R-1.9.1 running
under SuSe 9.0 64-bit kernel on a dual opteron system with 8G of RAM. 
I was wandering if anybody could offer any insight?
Thanks,
mario.



From V.Khamenia at biovision-discovery.de  Mon Aug  9 16:52:21 2004
From: V.Khamenia at biovision-discovery.de (Khamenia, Valery)
Date: Mon, 9 Aug 2004 16:52:21 +0200 
Subject: [R] built-in Sweave-like documentation in R-2.x
Message-ID: <D15343265276D31197BC00A024A6C110C79336@EXS_BDC>

Hi devels,

  i did not find at this page:

    http://developer.r-project.org/ideas.txt

  any ideas concerning incorporating documentation 
  possibilities (say, Sweave-based) into R-scripts.

  Was it discussed already? 
  (If discussed, then what is the decision/conclusion then?)

thanks,
Valery



From tlumley at u.washington.edu  Mon Aug  9 16:52:16 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 9 Aug 2004 07:52:16 -0700 (PDT)
Subject: [R] linear constraint optim with bounds/reparametrization
In-Reply-To: <C9FC71F7E9356F40AFE2ACC2099DE147149633@MAILSERVER-B.mpsgr.it>
References: <C9FC71F7E9356F40AFE2ACC2099DE147149633@MAILSERVER-B.mpsgr.it>
Message-ID: <Pine.A41.4.58.0408090732530.263586@homer10.u.washington.edu>

On Mon, 9 Aug 2004, Kahra Hannu wrote:

> >1) constrOptim does not work in this case because it only fits inequality
> >constraints, ie A%*%theta > =  c
>                           --- I was struggling with the same problem a
> few weeks ago in the portfolio optimization context. You can impose
> equality constraints by using inequality constraints >= and <=
> simultaneously. See the example bellow.
>

Ick. You do not want to use constrOptim for equality constraints.
constrOptim is a log-barrier interior-point method, meaning that it adds
a multiple of log(A%*%theta-c) to the objective function. This is a really
bad idea as a way of faking equality constraints.

Use Lagrange multipliers and optim.

	-thomas



From Christoph.Scherber at uni-jena.de  Mon Aug  9 16:55:55 2004
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Mon, 09 Aug 2004 16:55:55 +0200
Subject: [R] log-linear contrasts
Message-ID: <4117907B.7070806@uni-jena.de>

Dear list members,

suppose I have a one-way ANOVA where the explanatory variable is of 
class ordered(). How can I define, instead of the default linear, 
quadratic, cubic contrasts, a set of log-linear contrasts corresponding 
to logb(c(1,2,4,8,16,60)+1,2) ?

Regards,
Christoph



From deepayan at stat.wisc.edu  Mon Aug  9 16:57:40 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Mon, 9 Aug 2004 09:57:40 -0500
Subject: [R] built-in Sweave-like documentation in R-2.x
In-Reply-To: <D15343265276D31197BC00A024A6C110C79336@EXS_BDC>
References: <D15343265276D31197BC00A024A6C110C79336@EXS_BDC>
Message-ID: <200408090957.40057.deepayan@stat.wisc.edu>

On Monday 09 August 2004 09:52, Khamenia, Valery wrote:
> Hi devels,
>
>   i did not find at this page:
>
>     http://developer.r-project.org/ideas.txt
>
>   any ideas concerning incorporating documentation
>   possibilities (say, Sweave-based) into R-scripts.
>
>   Was it discussed already?
>   (If discussed, then what is the decision/conclusion then?)

See the 'Writing R Extensions' manual, specifically 

Creating R Packages -> Writing package vignettes

HTH,

Deepayan



From plummer at iarc.fr  Mon Aug  9 16:59:14 2004
From: plummer at iarc.fr (Martyn Plummer)
Date: Mon, 09 Aug 2004 16:59:14 +0200
Subject: [R] R packages install problems linux - X not found (WhiteBoxEL 3)
In-Reply-To: <Pine.LNX.4.44.0408082110130.9160-100000@gannet.stats>
References: <Pine.LNX.4.44.0408082110130.9160-100000@gannet.stats>
Message-ID: <1092063554.5303.203.camel@nemo>

On Sun, 2004-08-08 at 22:22, Prof Brian Ripley wrote:
> On Sun, 8 Aug 2004, Marc Schwartz wrote:
> 
> > On Sun, 2004-08-08 at 14:10, Dr Mike Waters wrote:
> > 
> > snip
> > 
> > > Thanks for the responses guys.
> > > 
> > > I used to have RH9 installed on this machine and I found out about the
> > > separate developer packages then. I thought that I had got the relevant
> > > XFree devel package installed, but although it showed up in the rpm database
> > > as being present, the required files were not present. I did a forced rpm
> > > upgrade from the WhiteBox updates directory and that problem is now fixed,
> > > at least for car. Marc, thanks for the pointer on the rgl problem. However,
> > > I have a slightly different problem with the install of this package. It
> > > gets through to the point where it tries to make the rgl.so from the various
> > > .o files and fails then, as follows:
> > > 
> > > --------------------------------------------------------------------
> > > 
> > > g++ -I/usr/lib/R/include -I/usr/X11R6/include -DHAVE_PNG_H
> > > -I/usr/include -I/usr/local/include  -Wall -pedantic -fno-exceptions
> > > -fno-rtti -fPIC  -O2 -g -march=i386 -mcpu=i686 -c glgui.cpp -o glgui.o
> > > 
> > > g++  -L/usr/local/lib -o rgl.so x11lib.o x11gui.o types.o math.o fps.o
> > > pixmap.o gui.o api.o device.o devicemanager.o rglview.o scene.o glgui.o
> > > -L/usr/X11R6/lib -L/usr/lib -lstdc++ -lX11 -lXext -lGL -lGLU -lpng
> > > /usr/lib/gcc-lib/i386-redhat-linux/3.2.3/../../../crt1.o(.text+0x18): In
> > > function `_start':
> > > : undefined reference to `main'
> > > x11lib.o(.text+0x84): In function `set_R_handler':
> > > /tmp/R.INSTALL.13414/rgl/src/x11gui.h:33: undefined reference to
> > > `R_InputHandlers'
> > > x11lib.o(.text+0x92):/tmp/R.INSTALL.13414/rgl/src/x11gui.h:33: undefined
> > > reference to `addInputHandler'
> > > x11lib.o(.text+0xfb): In function `unset_R_handler':
> > > /tmp/R.INSTALL.13414/rgl/src/x11lib.cpp:52: undefined reference to
> > > `R_InputHandlers'
> > > x11lib.o(.text+0x103):/tmp/R.INSTALL.13414/rgl/src/x11lib.cpp:52:
> > > undefined reference to `removeInputHandler'
> > > collect2: ld returned 1 exit status
> > > make: *** [rgl.so] Error 1
> > > ERROR: compilation failed for package 'rgl'
> > > ** Removing '/usr/lib/R/library/rgl'
> > > 
> > > ---------------------------------------------------------------------
> > > 
> > > No doubt another failed dependency........... DOH!
> > > 
> > > Regards
> > 
> > 
> > I am concerned by your indications of previously having had RH9 on the
> > same box and that you had to force an update of the XFree Devel RPM.
> > Forcing the installation of an RPM is almost always a bad thing.
> > 
> > When you installed WB on the system, did you do a "clean" installation
> > or some type of "upgrade"?
> > 
> > If the latter, it is reasonable to consider that there may be some level
> > of mixing and matching of RPMS from the two distributions going on. This
> > could result in a level of marginally or wholly incompatible versions of
> > RPMS being installed.
> > 
> > Could you clarify that point?
> > 
> > Also, be sure that you have the same versions of the XFree series RPMS
> > installed.
> > 
> > Use:
> > 
> > rpm -qa | grep XFree
> > 
> > in a console and be sure that the RPMS return the same version schema.
> > If not, it is possible that one of your problems is the mixing of
> > versions.
> > 
> > Take note of the output of the above and be sure that the
> > XFree86-Mesa-libGL and XFree86-Mesa-libGLU RPMS are installed as well.
> > 
> > Some of the messages above would also suggest a problem finding R
> > related headers. How did you install R? This may be a red herring of
> > sorts, given the other problems, but may be helpful.
> 
> I think it is the exact point.  Those entry points are in R.bin, 
> so should be missing.  The g++ line is missing `-shared', which is picked 
> up from R_HOME/etc/Makeconf, specifically from
> 
> SHLIB_CXXLDFLAGS = -shared
> 
> So the R installation doesn't have the right flags.

This could be my fault. It transpires that this is not set correctly in
the RPMs for RH9 and 8.x.  I will look into it (this is most probably
due to a missing BuildRequires).  This isn't the first time someone has
had this problem. I apologize for not fixing it, but at the time I was
convinced the RPMS were OK.

Martyn



From maechler at stat.math.ethz.ch  Mon Aug  9 17:02:29 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 9 Aug 2004 17:02:29 +0200
Subject: [R] hclust-segmentation fault
In-Reply-To: <886EF25AF8BEF64EB89A820EF84064FF052E1485@UCMAIL4>
References: <886EF25AF8BEF64EB89A820EF84064FF052E1485@UCMAIL4>
Message-ID: <16663.37381.691439.505461@gargle.gargle.HOWL>

>>>>> "MarioM" == Medvedovic, Mario (medvedm) <medvedm at UCMAIL.UC.EDU>
>>>>>     on Mon, 9 Aug 2004 10:47:16 -0400  writes:

    MarioM> I am getting the "Segmentation fault" when using
    MarioM> hclust in R-1.9.1 running under SuSe 9.0 64-bit
    MarioM> kernel on a dual opteron system with 8G of RAM.  I
    MarioM> was wandering if anybody could offer any insight?

{ where will you get when "wandering" around?  :-) }

I have seen other problematic behavior on a 64-bit kernel Linux
on the Opteron though not with SuSE and only with R-devel (R-2.0.0-unstable)

Can you give a reproducible example please
(as the posting guide asks you to)?

Martin Maechler



From ripley at stats.ox.ac.uk  Mon Aug  9 17:13:34 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 9 Aug 2004 16:13:34 +0100 (BST)
Subject: [R] hclust-segmentation fault
In-Reply-To: <886EF25AF8BEF64EB89A820EF84064FF052E1485@UCMAIL4>
Message-ID: <Pine.LNX.4.44.0408091612001.1168-100000@gannet.stats>

On Mon, 9 Aug 2004, Medvedovic, Mario (medvedm) wrote:

> I am getting the "Segmentation fault" when using hclust in R-1.9.1 running
> under SuSe 9.0 64-bit kernel on a dual opteron system with 8G of RAM. 
> I was wandering if anybody could offer any insight?

Please try to use the debugger to supply more information, or give us some 
code we can reproduce on a similar system to see if we can reproduce the 
segfault.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From umalvarez at fata.unam.mx  Mon Aug  9 17:16:00 2004
From: umalvarez at fata.unam.mx (Ulises Mora Alvarez)
Date: Mon, 9 Aug 2004 10:16:00 -0500 (CDT)
Subject: [R] built-in Sweave-like documentation in R-2.x
In-Reply-To: <D15343265276D31197BC00A024A6C110C79336@EXS_BDC>
Message-ID: <Pine.LNX.4.44.0408091012220.28389-100000@athena.fata.unam.mx>

Hi!

I'm not a developer, but you can take a look at the article 

Sweave, Part II: Package Vignettes 

in R-news Vol. 3/2, October 2003.

Regards.

On Mon, 9 Aug 2004, Khamenia, Valery wrote:

> Hi devels,
> 
>   i did not find at this page:
> 
>     http://developer.r-project.org/ideas.txt
> 
>   any ideas concerning incorporating documentation 
>   possibilities (say, Sweave-based) into R-scripts.
> 
>   Was it discussed already? 
>   (If discussed, then what is the decision/conclusion then?)
> 
> thanks,
> Valery
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Ulises M. Alvarez
LAB. DE ONDAS DE CHOQUE
FISICA APLICADA Y TECNOLOGIA AVANZADA
UNAM
-u-m-a-l-v-a-r-e-z- at -f-a-t-a- dot -u-n-a-m- dot -m-x-



From gpagnon at emory.edu  Mon Aug  9 17:19:26 2004
From: gpagnon at emory.edu (Giuseppe Pagnoni)
Date: Mon, 09 Aug 2004 11:19:26 -0400
Subject: [R] (REPOST) Simple main effects in 2-way repeated measure ANOVA
In-Reply-To: <Pine.LNX.4.44.0408082212530.11410-100000@gannet.stats>
References: <Pine.LNX.4.44.0408082212530.11410-100000@gannet.stats>
Message-ID: <411795FE.5030704@emory.edu>

Dear Prof. Ripley

I am sorry that reposting the mail was perceived as not complying to the 
correct list etiquette: I do not usually frequent the list and I just 
thought that the message went unnoticed.  I apologize for the insistence.

On the subject matter of the e-mail, yes, there was a typo, the error 
term was indeed Error(Subj/Time).  But I am still interested in 
examining the simple main effects, when the interaction of Time and 
Group is significant:

Time:
------
ControlGroup: before vs after
TreatmentGroup: before vs after

Group:
------
Before:  ControlGroup vs TreatmentGroup
After: ControlGroup vs TreatmentGroup

Is this incorrect statistical praxis?  Other software packages display 
the results for the above tests after the ANOVA.  However, I couldn't 
find a simple way to extract this information in R.

Also, I am not sure how to include a confounding variable, like Age, in 
the formulation of the model.


Thanks for your help

                giuseppe





Prof Brian Ripley wrote:

>Why has this been REPOSTed?  It was delivered last Thursday.
>
>On Sun, 8 Aug 2004, Giuseppe Pagnoni wrote:
>
>  
>
>>I am running a 2-way repeated measure anova with 1 between-subjects 
>>factor (Group=treatment, control), and 1 within-subject factor (Time of 
>>measurement: time1, time2).  I extract the results of the anova with:
>>
>>summary(aov(effect ~ Group*Time + Error=Subj/Time, data=mydata))
>>    
>>
>
>That's not valid syntax for an R formula. Did you mean Error(Subj/Time)?
>
>  
>
>>Now, this must be clearly a dumb question, but how can I quickly 
>>extract in R all the post-hoc t-tests for the simple main effects?
>>    
>>
>
>I very much hope you cannot, as you have specified an interaction, and
>you should not want t-tests for main effects in the presence of an 
>interaction, and certainly not with the default R coding.
>
>Did you mean Group + Time?
>
>  
>
>>Also, while I am at it, how do I enter in the model a counfounding 
>>covariate (e.g., Age)?
>>
>>And on a different matter, is there a way to receive interactive user 
>>input in an R script? Something like "Enter the name of  the factor:  
>>", or even more simply "Press <Enter> to see the result of the next 
>>analysis"....
>>    
>>
>
>?readline, or cat + scan or ... using connections.
>
>  
>


-- 
------------------------
Giuseppe Pagnoni, Ph.D.
Dept. of Psychiatry and Behavioral Sciences
1639 Pierce Drive, Suite 4000
WMB Bldg., Atlanta, GA 30322, U.S.
phone: 404-712-8431
fax: 404-727-3233
e-mail: gpagnon at emory.edu



From V.Khamenia at biovision-discovery.de  Mon Aug  9 17:23:21 2004
From: V.Khamenia at biovision-discovery.de (Khamenia, Valery)
Date: Mon, 9 Aug 2004 17:23:21 +0200 
Subject: AW: [R] built-in Sweave-like documentation in R-2.x
Message-ID: <D15343265276D31197BC00A024A6C110C79337@EXS_BDC>

> See the 'Writing R Extensions' manual, specifically 
> Creating R Packages -> Writing package vignettes

thank you, i saw this entry. However, this entry is rather 
about how to include documents (in particular Sweave-based) 
into a package. But I have meant smth else.

Let me explain in example. Today I use emacs as 
environment for my R-sessions. I am quite happy to 
use Sweave, but not happy, that there is no good 
possibility to run some selected *part* of the R-code.
Indeed, the R-chunks are alternated with 
Latex-chunks in Sweave. So, if one would like to 
execute, say, three consequent R-chunks (C-c C-r in ESS-mode), 
then the corresponding Latex-chunks between the 
R-chunks will cause "side-effects". Actually, it is
because neither ESS nor R could accept Latex-chunks
and the markup symbols of Sweave. This makes interactive 
work in .Rnw files not fully convenient.

Making the above short: would it be a bad idea to
allow Latex-friendly documentation chunks in 
R-language?

Thank you.

P.S. I am sorry if i missed something well-known 
concerning the subj.
--
Valery.



From medvedm at UCMAIL.UC.EDU  Mon Aug  9 17:31:45 2004
From: medvedm at UCMAIL.UC.EDU (Medvedovic, Mario (medvedm))
Date: Mon, 9 Aug 2004 11:31:45 -0400 
Subject: [R] hclust-segmentation fault
Message-ID: <886EF25AF8BEF64EB89A820EF84064FF052E1487@UCMAIL4>

Well, the use of debugger will take some time, but here is a simple code
that invariably causes the fault. 
Mario.

indata<-matrix(rnorm(1000,0,1),ncol=10)
ed<-dist(indata)
hc.e<-hclust(ed,"average")


>-----Original Message-----
>From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
>Sent: Monday, August 09, 2004 11:14 AM
>To: Medvedovic, Mario (medvedm)
>Cc: 'r-help at stat.math.ethz.ch'
>Subject: Re: [R] hclust-segmentation fault
>
>
>On Mon, 9 Aug 2004, Medvedovic, Mario (medvedm) wrote:
>
>> I am getting the "Segmentation fault" when using hclust in 
>R-1.9.1 running
>> under SuSe 9.0 64-bit kernel on a dual opteron system with 
>8G of RAM. 
>> I was wandering if anybody could offer any insight?
>
>Please try to use the debugger to supply more information, or 
>give us some 
>code we can reproduce on a similar system to see if we can 
>reproduce the 
>segfault.
>
>-- 
>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>University of Oxford,             Tel:  +44 1865 272861 (self)
>1 South Parks Road,                     +44 1865 272866 (PA)
>Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From rossini at blindglobe.net  Mon Aug  9 17:29:55 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Mon, 09 Aug 2004 08:29:55 -0700
Subject: [R] built-in Sweave-like documentation in R-2.x
In-Reply-To: <D15343265276D31197BC00A024A6C110C79336@EXS_BDC> (Valery
	Khamenia's message of "Mon, 9 Aug 2004 16:52:21 +0200")
References: <D15343265276D31197BC00A024A6C110C79336@EXS_BDC>
Message-ID: <857js8qqh8.fsf@servant.blindglobe.net>


What exactly do you mean by this?

1. generation of Sweave-style docs from R programs or interaction?  A
related approach is the reviveable statistical analysis (the
authors/researchers will have to forgive me for not recalling the WWW
site or their Uni!)

2. tools for doing docs and analysis at the same time?  Emacs Speaks
Statistics has supported this with R since last century (1997 or so).

3. the vignettes of Bioconductor?

4. a text book in line with the above?

best,
-tony


"Khamenia, Valery" <V.Khamenia at biovision-discovery.de> writes:

> Hi devels,
>
>   i did not find at this page:
>
>     http://developer.r-project.org/ideas.txt
>
>   any ideas concerning incorporating documentation 
>   possibilities (say, Sweave-based) into R-scripts.
>
>   Was it discussed already? 
>   (If discussed, then what is the decision/conclusion then?)
>
> thanks,
> Valery
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Anthony Rossini			    Research Associate Professor
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From deepayan at stat.wisc.edu  Mon Aug  9 17:43:34 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Mon, 9 Aug 2004 10:43:34 -0500
Subject: AW: [R] built-in Sweave-like documentation in R-2.x
In-Reply-To: <D15343265276D31197BC00A024A6C110C79337@EXS_BDC>
References: <D15343265276D31197BC00A024A6C110C79337@EXS_BDC>
Message-ID: <200408091043.34409.deepayan@stat.wisc.edu>

On Monday 09 August 2004 10:23, Khamenia, Valery wrote:
> > See the 'Writing R Extensions' manual, specifically
> > Creating R Packages -> Writing package vignettes
>
> thank you, i saw this entry. However, this entry is rather
> about how to include documents (in particular Sweave-based)
> into a package. But I have meant smth else.
>
> Let me explain in example. Today I use emacs as
> environment for my R-sessions. I am quite happy to
> use Sweave, but not happy, that there is no good
> possibility to run some selected *part* of the R-code.
> Indeed, the R-chunks are alternated with
> Latex-chunks in Sweave. So, if one would like to
> execute, say, three consequent R-chunks (C-c C-r in ESS-mode),
> then the corresponding Latex-chunks between the
> R-chunks will cause "side-effects". Actually, it is
> because neither ESS nor R could accept Latex-chunks
> and the markup symbols of Sweave. This makes interactive
> work in .Rnw files not fully convenient.

Is selecting and 'C-c C-r'-ing the 3 chunks separately that bad?

The pieces for what you want are there. You can use Stangle() on a 
Sweave format file to extract the R code chunks. In your usage, a crude 
implementation would involve writing your selection to a file, running 
Stangle on it, and source()-ing the resulting .R file. 

Others may have better suggestions.

Deepayan



From ripley at stats.ox.ac.uk  Mon Aug  9 17:51:05 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 9 Aug 2004 16:51:05 +0100 (BST)
Subject: [R] hclust-segmentation fault
In-Reply-To: <886EF25AF8BEF64EB89A820EF84064FF052E1487@UCMAIL4>
Message-ID: <Pine.LNX.4.44.0408091647090.2863-100000@gannet.stats>

On Mon, 9 Aug 2004, Medvedovic, Mario (medvedm) wrote:

> Well, the use of debugger will take some time, but here is a simple code
> that invariably causes the fault. 
> Mario.
> 
> indata<-matrix(rnorm(1000,0,1),ncol=10)
> ed<-dist(indata)
> hc.e<-hclust(ed,"average")

Works fine on R 1.9.1 on our dual Opteron 248 under FC2.

We know of some pertinent compiler bugs on x86_64, so is this gcc 3.3.3 
or later?

> >-----Original Message-----
> >From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> >Sent: Monday, August 09, 2004 11:14 AM
> >To: Medvedovic, Mario (medvedm)
> >Cc: 'r-help at stat.math.ethz.ch'
> >Subject: Re: [R] hclust-segmentation fault
> >
> >
> >On Mon, 9 Aug 2004, Medvedovic, Mario (medvedm) wrote:
> >
> >> I am getting the "Segmentation fault" when using hclust in 
> >R-1.9.1 running
> >> under SuSe 9.0 64-bit kernel on a dual opteron system with 
> >8G of RAM. 
> >> I was wandering if anybody could offer any insight?
> >
> >Please try to use the debugger to supply more information, or 
> >give us some 
> >code we can reproduce on a similar system to see if we can 
> >reproduce the 
> >segfault.
> >
> >-- 
> >Brian D. Ripley,                  ripley at stats.ox.ac.uk
> >Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> >University of Oxford,             Tel:  +44 1865 272861 (self)
> >1 South Parks Road,                     +44 1865 272866 (PA)
> >Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From subianto at cs.uu.nl  Mon Aug  9 17:46:38 2004
From: subianto at cs.uu.nl (Muhammad Subianto)
Date: Mon, 09 Aug 2004 17:46:38 +0200
Subject: [R] R packages install problems linux - X not found (WhiteBoxEL 3)
In-Reply-To: <1092060760.25174.134.camel@localhost.localdomain>
References: <00ca01c47e12$a9ded280$6600a8c0@mainman>
	<1092060760.25174.134.camel@localhost.localdomain>
Message-ID: <cf868u$1eg$1@sea.gmane.org>


Maybe you can try with the other RHEL clone like CentOS-3:
http://install.linux.duke.edu/pub/linux/add-on/distrib/centos-3.1/i386/rpms/R-1.9.0-0.duke.1.el3.i386.rpm
http://install.linux.duke.edu/pub/linux/add-on/distrib/centos-3.1/i386/srpms/R-1.9.0-0.duke.1.el3.src.rpm

Best wishes,
Muhammad Subianto

On this day 09/08/2004 04:12 PM, Marc Schwartz wrote:
> On Mon, 2004-08-09 at 08:13, Dr Mike Waters wrote:
> 
> <snip>
> 
>>Marc,
>>
>>Sorry for the confusion yesterday - in my defence, it was very hot and humid
>>here in Hampshire (31 Celsius at 15:00hrs and still 25 at 20:00hrs). 
>>
>>What had happened was that I had done a clean install of WB Linux, including
>>the XFree86 and other developer packages. However, the on-line updating
>>system updated the XFree86 packages to a newer sub version. It seems that it
>>didn't do this correctly for the XFree86 developer package, which was
>>missing vital files. However it showed up in the rpm database as being
>>installed (i.e. rpm -qa | grep XFree showed it thus). I downloaded another
>>rpm for this manually and I only forced the upgrade because it was the same
>>version as already 'installed' (as far as the rpm database was concerned). I
>>assumed that all dependencies were sorted out through the install in the
>>first place.
> 
> 
> OK, that helps. I still have a lingering concern that, given the facts
> above, there may be other integrity issues in the RPM database, if not
> elsewhere.
> 
>>From reading the WB web site FAQ's
> (http://www.whiteboxlinux.org/faq.html) , it appears that they are using
> up2date/yum for system updates. Depending upon the version in use, there
> have been issues especially with up2date (hangs, incomplete updates,
> etc.) which could result in other problems. I use yum via the console
> here (under FC2), though I note that a GUI version of yum has been
> created, including replacing the RHN/up2date system tray alert icon.
> 
> A thought relative to this specifically:
> 
> If there is or may be an integrity problem related to the rpm database,
> you should review the information here:
> 
> http://www.rpm.org/hintskinks/repairdb/
> 
> which provides instructions on repairing the database. Note the
> important caveats regarding backups, etc.
> 
> The two key steps there are to remove any residual lock files using (as
> root):
> 
> rm -f /var/lib/rpm/__*
> 
> and then rebuilding the rpm database using (also as root):
> 
> rpm -vv --rebuilddb
> 
> I think that there needs to be some level of comfort that this basic
> foundation for the system is intact and correct.
> 
> 
>>I only mentioned RH9 to show that I had some familiarity with the RedHat
>>policy of separating out the 'includes' etc into a separate developer
>>package.
>>
>>Once all this had been sorted out, I was then left with a compilation error
>>which pointed to a missing dependency or similar, which was not due to
>>missing developer packages, but, as you and Prof Ripley correctly point out,
>>from the R installation itself. Having grown fat and lazy on using R under
>>the MS Windows environment, I was struggling to identify the precise nature
>>of this remaining problem.
>>
>>As regards the R installation, I did this from the RH9 binary for version
>>1.9.1, as I did not think that the Fedora Core 2 binary would be appropriate
>>here. Perhaps I should now compile from the source instead?
> 
> 
> I would not use the FC2 RPM, since FC2 has many underlying changes not
> the least of which includes the use of the 2.6 kernel series and the
> change from XFree86 to x.org. Both changes resulted in significant havoc
> during the FC2 testing phases and there was at least one issue here with
> R due to the change in X.
> 
> According to the WB FAQs:
> 
> "If you cannot find a package built specifically for RHEL3 or WBEL3 you
> can try a package for RH9 since many of the packages in RHEL3 are the
> exact same packages as appeared in RH9."
> 
> Thus, it would seem reasonable to use the RH9 RPM that Martyn has
> created. An alternative would certainly be to compile R from the source
> tarball.
> 
> In either case, I would remove the current installation of R and after
> achieving a level of comfort that your RPM database is OK, reinstall R
> using one of the above methods. Pay close attention to any output during
> the installation process, noting any error or warning messages that may
> occur.
> 
> If you go the RPM route, be sure that the MD5SUM of the RPM file matches
> the value that Martyn has listed on CRAN to ensure that the file has
> been downloaded in an intact fashion.
> 
> These are my thoughts at this point. You need to get to a point where
> the underlying system is stable and intact, then get R to the same state
> before attempting to install new packages.
> 
> HTH,
> 
> Marc
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From rossini at blindglobe.net  Mon Aug  9 17:54:55 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Mon, 09 Aug 2004 08:54:55 -0700
Subject: AW: [R] built-in Sweave-like documentation in R-2.x
In-Reply-To: <D15343265276D31197BC00A024A6C110C79337@EXS_BDC> (Valery
	Khamenia's message of "Mon, 9 Aug 2004 17:23:21 +0200")
References: <D15343265276D31197BC00A024A6C110C79337@EXS_BDC>
Message-ID: <85smaws3w0.fsf@servant.blindglobe.net>

If you name chunks that should be run together the same thing
(i.e. similar to how Noweb suggests that you should create documents),
then ess-eval-thread will evaluate  all chunks in the thread
(i.e. that noweb would have sent to the same file upon tangling).

This is similar to the ess-eval-chunk  ESS function.


"Khamenia, Valery" <V.Khamenia at biovision-discovery.de> writes:

>> See the 'Writing R Extensions' manual, specifically 
>> Creating R Packages -> Writing package vignettes
>
> thank you, i saw this entry. However, this entry is rather 
> about how to include documents (in particular Sweave-based) 
> into a package. But I have meant smth else.
>
> Let me explain in example. Today I use emacs as 
> environment for my R-sessions. I am quite happy to 
> use Sweave, but not happy, that there is no good 
> possibility to run some selected *part* of the R-code.
> Indeed, the R-chunks are alternated with 
> Latex-chunks in Sweave. So, if one would like to 
> execute, say, three consequent R-chunks (C-c C-r in ESS-mode), 
> then the corresponding Latex-chunks between the 
> R-chunks will cause "side-effects". Actually, it is
> because neither ESS nor R could accept Latex-chunks
> and the markup symbols of Sweave. This makes interactive 
> work in .Rnw files not fully convenient.
>
> Making the above short: would it be a bad idea to
> allow Latex-friendly documentation chunks in 
> R-language?
>
> Thank you.
>
> P.S. I am sorry if i missed something well-known 
> concerning the subj.
> --
> Valery.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Anthony Rossini			    Research Associate Professor
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From rossini at blindglobe.net  Mon Aug  9 17:57:45 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Mon, 09 Aug 2004 08:57:45 -0700
Subject: AW: [R] built-in Sweave-like documentation in R-2.x
In-Reply-To: <200408091043.34409.deepayan@stat.wisc.edu> (Deepayan Sarkar's
	message of "Mon, 9 Aug 2004 10:43:34 -0500")
References: <D15343265276D31197BC00A024A6C110C79337@EXS_BDC>
	<200408091043.34409.deepayan@stat.wisc.edu>
Message-ID: <85oelks3ra.fsf@servant.blindglobe.net>

Deepayan Sarkar <deepayan at stat.wisc.edu> writes:

>
> Is selecting and 'C-c C-r'-ing the 3 chunks separately that bad?

Yes.  The UI should take care of it for him.

> Others may have better suggestions.

A bit more work on the chunk evaluation approach within Emacs is one;
it almost does what is needed, but not quite.  

best,
-tony

-- 
Anthony Rossini			    Research Associate Professor
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From datkins at u.washington.edu  Mon Aug  9 18:03:00 2004
From: datkins at u.washington.edu (Dave Atkins)
Date: Mon, 09 Aug 2004 09:03:00 -0700
Subject: Follow-up Q Re:  [R] displaying computation outputs inside "for" loops
In-Reply-To: <200408091010.i79A917H027004@hypatia.math.ethz.ch>
References: <200408091010.i79A917H027004@hypatia.math.ethz.ch>
Message-ID: <4117A034.5040505@u.washington.edu>


I have a somewhat related question.  A while back I was doing some simulations 
using for() loops, and I wanted to keep track of the iterations using a line of 
code quite similar to what Dimitris presented below.  Instead of printing the 
iteration message at the end of each iteration (actually, at the end of every 
100th), nothing was printed until the for() loop was complete, and *then* all 
the iteration messages were reported.  Extending Dimitris' example:

for(i in 1:10000){
   x <- rnorm(5)
   cat("the values of `x' are:", format(x), "\n")
   cat("computation", i, "finished\n\n")
}

demonstrates what I'm referring to (using R v1.9.1 with Windows XP): Iterations 
are not printed as they occur, only at the end of the for() loop computation.

An interesting wrinkle is that I sometimes use Emacs (and ESS).  Running the 
code from within Emacs prints the iterations as they occur; also, if I switch to 
other programs and then back to R, iterations have been printed.

Can anybody help me out with what's going on (and how to print iterations from 
within a for() loop as they occur)?

Thanks, Dave

Dave Atkins, PhD
University of Washington

Message: 28
Date: Mon, 9 Aug 2004 11:26:08 +0200
From: "Dimitris Rizopoulos" <dimitris.rizopoulos at med.kuleuven.ac.be>
Subject: Re: [R] displaying computation outputs inside "for" loops
To: "Dewez Thomas" <t.dewez at brgm.fr>
Cc: r-help at stat.math.ethz.ch
Message-ID: <009101c47df2$e7e13430$ad133a86 at www.domain>
Content-Type: text/plain;	charset="iso-8859-1"

Hi Thomas,

is this what you would like to get,

for(i in 1:3){
   x <- rnorm(5)
   cat("the values of `x' are:", format(x), "\n")
   cat("computation", i, "finished\n\n")
}

for(i in 1:3) print(i)


I hope this helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Doctoral Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
      http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message -----
From: "Dewez Thomas" <t.dewez at brgm.fr>
To: "'R mailing list'" <r-help at stat.math.ethz.ch>
Sent: Monday, August 09, 2004 11:15 AM
Subject: [R] displaying computation outputs inside "for" loops



 >> Dear R-users,
 >>
 >> I am puzzled by for loops and am kind of ashamed to ask because it

is so

 >> simple. There must be something I am missing in the way they are

executed.

 >>
 >> Basically, I would like to iterate a given number of time and

generate a

 >> bunch of stats (that's what loops are designed for, right?). Before

doing

 >> this I simply want to test simple procedure and see if they work (ie

got the

 >> syntax right - my main problem as I am new to R - and produce

expected

 >> results).
 >>
 >> Even for something as basic as
 >> for (i in 1:3) {i}
 >>
 >> I get no screen output. Shouldn't R systematically display i for

every loop

 >> just like I am requesting with invoking "i"? When checking at the

end of the

 >> looping, i is indeed assigned to 5 but I cannot get intermediate

values.

 >>
 >> Further testing shows that i takes all the values in turn.
 >
 >>> > for (i in 1:3) {str(i)}
 >
 >>  int 1
 >>  int 2
 >>  int 3
 >>
 >> but summary(i) doesn't display anything. Isn't there something weird

with

 >> this? Am I expecting something wrong and for loops just don't work

that way,

 >> unless using str() command? I tried print() and other descriptive

commands

 >> but to no avail.
 >>
 >> A quick explanation would be grately appreciated
 >>
 >> Thomas
 >> ***
 >> Le contenu de cet e-mail et de ses pi??ces jointes est

destin...{{dropped}}

 >>
 >> ______________________________________________
 >> R-help at stat.math.ethz.ch mailing list
 >> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
 >> PLEASE do read the posting guide!

http://www.R-project.org/posting-guide.html



From sfalcon at fhcrc.org  Mon Aug  9 18:07:15 2004
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Mon, 9 Aug 2004 09:07:15 -0700
Subject: [R] Approaches to using RUnit
Message-ID: <20040809160713.GA16017@queenbee.fhcrc.org>

Having used JUnit and PyUnit, I was pleased to see the release of the
RUnit package on CRAN.

I'm wondering if there are any RUnit users out there that would be
willing to share some tips on how they organize their code to work with
RUnit.

Specifically, I'm wondering about the best way to load/import/source the
functions to be tested.  I would like to end up with a script, testall
or some such, that allows me to run all the unit tests in a given
directory.


One way I've thought about looks like:

myfunc.R
--------------
# some functions here
--------------

runit_myfunc.R
--------------
source("myfunc.R")

# test function here
--------------


+ seth



From maechler at stat.math.ethz.ch  Mon Aug  9 18:15:51 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 9 Aug 2004 18:15:51 +0200
Subject: [R] hclust-segmentation fault
In-Reply-To: <886EF25AF8BEF64EB89A820EF84064FF052E1487@UCMAIL4>
References: <886EF25AF8BEF64EB89A820EF84064FF052E1487@UCMAIL4>
Message-ID: <16663.41783.177012.894076@gargle.gargle.HOWL>

>>>>> "MarioM" == Medvedovic, Mario (medvedm) <medvedm at UCMAIL.UC.EDU>
>>>>>     on Mon, 9 Aug 2004 11:31:45 -0400  writes:

    MarioM> Well, the use of debugger will take some time
well
did you try

    R -d gdb

    <... GDB welcome messages....>
    <... GDB welcome messages....>

    run

and then the R commands and after the segmentation fault, simply
    bt
        { = 'backtrace' =^=  traceback() in R }
?

    MarioM> but here is a simple code that invariably causes the
    MarioM> fault.  Mario.

     indata <- matrix(rnorm(1000,0,1),ncol=10)
     ed <- dist(indata)
     hc.e <- hclust(ed,"average")

to make this really reproducible you'd have to give a 
    set.seed(<n>)
at the beginning. But since you say it's "invariably", that's ok.
As for Prof Ripley's, these R calls also work here on our Opteron
machine w/o a problem.

Martin Maechler


    >> -----Original Message----- From: Prof Brian Ripley
    >> [mailto:ripley at stats.ox.ac.uk] Sent: Monday, August 09,
    >> 2004 11:14 AM To: Medvedovic, Mario (medvedm) Cc:
    >> 'r-help at stat.math.ethz.ch' Subject: Re: [R]
    >> hclust-segmentation fault
    >> 
    >> 
    >> On Mon, 9 Aug 2004, Medvedovic, Mario (medvedm) wrote:
    >> 
    >>> I am getting the "Segmentation fault" when using hclust
    >>> in
    >> R-1.9.1 running
    >>> under SuSe 9.0 64-bit kernel on a dual opteron system
    >>> with
    >> 8G of RAM.
    >>> I was wandering if anybody could offer any insight?
    >>  Please try to use the debugger to supply more
    >> information, or give us some code we can reproduce on a
    >> similar system to see if we can reproduce the segfault.
    >> 
    >> -- 
    >> Brian D. Ripley, ripley at stats.ox.ac.uk Professor of
    >> Applied Statistics, http://www.stats.ox.ac.uk/~ripley/
    >> University of Oxford, Tel: +44 1865 272861 (self) 1 South
    >> Parks Road, +44 1865 272866 (PA) Oxford OX1 3TG, UK Fax:
    >> +44 1865 272595
    >> 

    MarioM> ______________________________________________
    MarioM> R-help at stat.math.ethz.ch mailing list
    MarioM> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
    MarioM> PLEASE do read the posting guide!
    MarioM> http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Mon Aug  9 18:34:23 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 9 Aug 2004 17:34:23 +0100 (BST)
Subject: Follow-up Q Re: [R] displaying computation outputs inside "for"
	loops
In-Reply-To: <4117A034.5040505@u.washington.edu>
Message-ID: <Pine.LNX.4.44.0408091733310.3273-100000@gannet.stats>

On Mon, 9 Aug 2004, Dave Atkins wrote:

> 
> I have a somewhat related question.  A while back I was doing some simulations 
> using for() loops, and I wanted to keep track of the iterations using a line of 
> code quite similar to what Dimitris presented below.  Instead of printing the 
> iteration message at the end of each iteration (actually, at the end of every 
> 100th), nothing was printed until the for() loop was complete, and *then* all 
> the iteration messages were reported.  Extending Dimitris' example:
> 
> for(i in 1:10000){
>    x <- rnorm(5)
>    cat("the values of `x' are:", format(x), "\n")
>    cat("computation", i, "finished\n\n")
> }
> 
> demonstrates what I'm referring to (using R v1.9.1 with Windows XP): Iterations 
> are not printed as they occur, only at the end of the for() loop computation.

This *is* in the rw-FAQ.  Please see the posting guide.

> An interesting wrinkle is that I sometimes use Emacs (and ESS).  Running the 
> code from within Emacs prints the iterations as they occur; also, if I switch to 
> other programs and then back to R, iterations have been printed.
-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rossini at blindglobe.net  Mon Aug  9 18:37:42 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Mon, 09 Aug 2004 09:37:42 -0700
Subject: [R] Approaches to using RUnit
In-Reply-To: <20040809160713.GA16017@queenbee.fhcrc.org> (Seth Falcon's
	message of "Mon, 9 Aug 2004 09:07:15 -0700")
References: <20040809160713.GA16017@queenbee.fhcrc.org>
Message-ID: <85hdrci7xl.fsf@servant.blindglobe.net>

Seth Falcon <sfalcon at fhcrc.org> writes:

> Having used JUnit and PyUnit, I was pleased to see the release of the
> RUnit package on CRAN.
>
> I'm wondering if there are any RUnit users out there that would be
> willing to share some tips on how they organize their code to work with
> RUnit.
>
> Specifically, I'm wondering about the best way to load/import/source the
> functions to be tested.  I would like to end up with a script, testall
> or some such, that allows me to run all the unit tests in a given
> directory.

Hi Seth -

The version that I was working on (before the other folks released a
less eclectic and more functional/general package) had a

     testrunner(test=c("list","of","functions"),testall=FALSE)

API, along with a preliminary tcltk 

     GUIrunner() 

with a similar intended API.  I've been trying to find the initiative
to integrate (overlay) those tools with the current RUnit as an
add-on, but the difference is that I attached tests as S4 object
instantiations.

Stop by if you want to chat a bit about it.

best,
-tony

-- 
Anthony Rossini			    Research Associate Professor
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From spencer.graves at pdf.com  Mon Aug  9 18:45:47 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 09 Aug 2004 09:45:47 -0700
Subject: [R] linear constraint optim with bounds/reparametrization
In-Reply-To: <Pine.A41.4.58.0408090732530.263586@homer10.u.washington.edu>
References: <C9FC71F7E9356F40AFE2ACC2099DE147149633@MAILSERVER-B.mpsgr.it>
	<Pine.A41.4.58.0408090732530.263586@homer10.u.washington.edu>
Message-ID: <4117AA3B.4020909@pdf.com>

Hi, Tom: 

      Why is adding "a multiple of log(A*theta-c) to the objective 
function ... a really bad idea as a way of faking equality constraints"? 

      I've used Lagrange multipliers on other occasions, but if computer 
time is cheaper than the time to work out the Lagrange multiplier 
approach, why is it a bad idea to add violation of constraints to the 
objective function?  I've done it myself in the past and have gotten 
what looked like sensible results. 

      Best Wishes,
      spencer graves

Thomas Lumley wrote:

>On Mon, 9 Aug 2004, Kahra Hannu wrote:
>
>  
>
>>>1) constrOptim does not work in this case because it only fits inequality
>>>constraints, ie A%*%theta > =  c
>>>      
>>>
>>                          --- I was struggling with the same problem a
>>few weeks ago in the portfolio optimization context. You can impose
>>equality constraints by using inequality constraints >= and <=
>>simultaneously. See the example bellow.
>>
>>    
>>
>
>Ick. You do not want to use constrOptim for equality constraints.
>constrOptim is a log-barrier interior-point method, meaning that it adds
>a multiple of log(A%*%theta-c) to the objective function. This is a really
>bad idea as a way of faking equality constraints.
>
>Use Lagrange multipliers and optim.
>
>	-thomas
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From plummer at iarc.fr  Mon Aug  9 18:51:38 2004
From: plummer at iarc.fr (Martyn Plummer)
Date: Mon, 09 Aug 2004 18:51:38 +0200
Subject: [R] R packages install problems linux - X not found (WhiteBoxEL 3)
In-Reply-To: <00ca01c47e12$a9ded280$6600a8c0@mainman>
References: <00ca01c47e12$a9ded280$6600a8c0@mainman>
Message-ID: <1092070298.5303.331.camel@nemo>

On Mon, 2004-08-09 at 15:13, Dr Mike Waters wrote:

  [SNIP]

> Once all this had been sorted out, I was then left with a compilation error
> which pointed to a missing dependency or similar, which was not due to
> missing developer packages, but, as you and Prof Ripley correctly point out,
> from the R installation itself. Having grown fat and lazy on using R under
> the MS Windows environment, I was struggling to identify the precise nature
> of this remaining problem.
> 
> As regards the R installation, I did this from the RH9 binary for version
> 1.9.1, as I did not think that the Fedora Core 2 binary would be appropriate
> here. Perhaps I should now compile from the source instead?

The Red Hat 9 RPM is appropriate for you, but it is currently buggy, so
you are best compiling from source.  I need to add gcc-g++ to the list
of "BuildRequires" (packages that must be installed at build time). 
This isn't strictly necessary to build R, but if R is configured without
a C++ compiler, of course it does not know how to build packages with
C++ code in them.

Martyn



From datkins at u.washington.edu  Mon Aug  9 18:51:27 2004
From: datkins at u.washington.edu (Dave Atkins)
Date: Mon, 09 Aug 2004 09:51:27 -0700
Subject: Follow-up Q Re: [R] displaying computation outputs inside "for"
	loops
In-Reply-To: <Pine.LNX.4.44.0408091733310.3273-100000@gannet.stats>
References: <Pine.LNX.4.44.0408091733310.3273-100000@gannet.stats>
Message-ID: <4117AB8F.2010604@u.washington.edu>


I have now been enlightened via Windows FAQ 6.2 on "When using Rgui the output 
to the console seems to be delayed."  Ctrl-W toggles whether console output is 
buffered.

[As a side note: Damn!  I was hoping to avoid that response (i.e., having my 
ignorance pointed out), but all my searches of R-help and FAQs had focused on 
"for()" and "iteration," which didn't find the right answer.  Knowing the 
"right" question to ask goes a long way...]

Thanks, Dave

Prof Brian Ripley wrote:

> On Mon, 9 Aug 2004, Dave Atkins wrote:
> 
> 
>>I have a somewhat related question.  A while back I was doing some simulations 
>>using for() loops, and I wanted to keep track of the iterations using a line of 
>>code quite similar to what Dimitris presented below.  Instead of printing the 
>>iteration message at the end of each iteration (actually, at the end of every 
>>100th), nothing was printed until the for() loop was complete, and *then* all 
>>the iteration messages were reported.  Extending Dimitris' example:
>>
>>for(i in 1:10000){
>>   x <- rnorm(5)
>>   cat("the values of `x' are:", format(x), "\n")
>>   cat("computation", i, "finished\n\n")
>>}
>>
>>demonstrates what I'm referring to (using R v1.9.1 with Windows XP): Iterations 
>>are not printed as they occur, only at the end of the for() loop computation.
> 
> 
> This *is* in the rw-FAQ.  Please see the posting guide.
> 
> 
>>An interesting wrinkle is that I sometimes use Emacs (and ESS).  Running the 
>>code from within Emacs prints the iterations as they occur; also, if I switch to 
>>other programs and then back to R, iterations have been printed.



From tlumley at u.washington.edu  Mon Aug  9 18:56:05 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 9 Aug 2004 09:56:05 -0700 (PDT)
Subject: [R] linear constraint optim with bounds/reparametrization
In-Reply-To: <4117AA3B.4020909@pdf.com>
References: <C9FC71F7E9356F40AFE2ACC2099DE147149633@MAILSERVER-B.mpsgr.it>
	<Pine.A41.4.58.0408090732530.263586@homer10.u.washington.edu>
	<4117AA3B.4020909@pdf.com>
Message-ID: <Pine.A41.4.58.0408090954080.17042@homer11.u.washington.edu>

On Mon, 9 Aug 2004, Spencer Graves wrote:

> Hi, Tom:
>
>       Why is adding "a multiple of log(A*theta-c) to the objective
> function ... a really bad idea as a way of faking equality constraints"?

Because it is infinite everywhere on the feasible set: log(0)?

It's fine to add constraints to the problem, but these aren't sensible
constraints for that purpose.

	-thomas



From tlumley at u.washington.edu  Mon Aug  9 19:13:06 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 9 Aug 2004 10:13:06 -0700 (PDT)
Subject: [R] FAQs (was: displaying computation outputs inside "for" loops)
In-Reply-To: <4117AB8F.2010604@u.washington.edu>
References: <Pine.LNX.4.44.0408091733310.3273-100000@gannet.stats>
	<4117AB8F.2010604@u.washington.edu>
Message-ID: <Pine.A41.4.58.0408091000300.17042@homer11.u.washington.edu>

On Mon, 9 Aug 2004, Dave Atkins wrote:
> [As a side note: Damn!  I was hoping to avoid that response (i.e., having my
> ignorance pointed out), but all my searches of R-help and FAQs had focused on
> "for()" and "iteration," which didn't find the right answer.  Knowing the
> "right" question to ask goes a long way...]

This illustrates why reading the FAQs is a useful thing to do. You can
find these things out before they cause problems for you (that is, you
all, not specifically Dave).

With help pages and the email archives it can be difficult to find the
right index terms. The FAQs are short enough that you can just read the
whole thing, and the answer is likely to be clearer and more detailed than
you will get from asking the question again. (If it isn't, then asking
"can someone clear up this question about FAQ 7.13?" is likely to get
useful help).


	-thomas



From Brian.Beckage at uvm.edu  Mon Aug  9 19:27:57 2004
From: Brian.Beckage at uvm.edu (Brian Beckage)
Date: Mon, 9 Aug 2004 13:27:57 -0400
Subject: [R] Simultaneous subscripts and superscripts
Message-ID: <p06110403bd3d5c93b4e5@[10.0.1.2]>

Dear List,

I'd like to add text to a plot where a text symbol has both a 
superscript and subscript.  For example, the variable S with a 
superscript 2 and a subscript t.  I have been able to accomplish this 
using either

expression( paste(S,atop(scriptstyle(2),scriptstyle(t))) )

or

expression( {S[t]}^2 )

but the spacing isn't quite right (or rather what I'd like) using 
either of these.  By 'right' I mean the placement and spacing don't 
quite coincide with what I'd expect if I superimposed S^2 with S[t]. 
Is there a way to place BOTH a subscript and superscript so that they 
are placed identically as if I had overlayed S^2 and S[t]?

I'm using R 1.9.0 on Mac OSX.

Thanks,
Brian


-- 
*********************************************************************
Brian Beckage
Department of Botany
University of Vermont
Marsh Life Science Building
Burlington, VT 05405



From andy_liaw at merck.com  Mon Aug  9 19:44:04 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 9 Aug 2004 13:44:04 -0400
Subject: [R] Simultaneous subscripts and superscripts
Message-ID: <3A822319EB35174CA3714066D590DCD504AF81E5@usrymx25.merck.com>

expression(S[t]^2) looks pretty good for me (on windows()).

Andy

> From: Brian Beckage
> 
> Dear List,
> 
> I'd like to add text to a plot where a text symbol has both a 
> superscript and subscript.  For example, the variable S with a 
> superscript 2 and a subscript t.  I have been able to accomplish this 
> using either
> 
> expression( paste(S,atop(scriptstyle(2),scriptstyle(t))) )
> 
> or
> 
> expression( {S[t]}^2 )
> 
> but the spacing isn't quite right (or rather what I'd like) using 
> either of these.  By 'right' I mean the placement and spacing don't 
> quite coincide with what I'd expect if I superimposed S^2 with S[t]. 
> Is there a way to place BOTH a subscript and superscript so that they 
> are placed identically as if I had overlayed S^2 and S[t]?
> 
> I'm using R 1.9.0 on Mac OSX.
> 
> Thanks,
> Brian
> 
> 
> -- 
> *********************************************************************
> Brian Beckage
> Department of Botany
> University of Vermont
> Marsh Life Science Building
> Burlington, VT 05405
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ligges at statistik.uni-dortmund.de  Mon Aug  9 19:54:14 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 09 Aug 2004 19:54:14 +0200
Subject: [R] Simultaneous subscripts and superscripts
In-Reply-To: <p06110403bd3d5c93b4e5@[10.0.1.2]>
References: <p06110403bd3d5c93b4e5@[10.0.1.2]>
Message-ID: <4117BA46.3020408@statistik.uni-dortmund.de>

Brian Beckage wrote:

> Dear List,
> 
> I'd like to add text to a plot where a text symbol has both a 
> superscript and subscript.  For example, the variable S with a 
> superscript 2 and a subscript t.  I have been able to accomplish this 
> using either
> 
> expression( paste(S,atop(scriptstyle(2),scriptstyle(t))) )
> 
> or
> 
> expression( {S[t]}^2 )
> 
> but the spacing isn't quite right (or rather what I'd like) using either 
> of these.  By 'right' I mean the placement and spacing don't quite 
> coincide with what I'd expect if I superimposed S^2 with S[t]. Is there 
> a way to place BOTH a subscript and superscript so that they are placed 
> identically as if I had overlayed S^2 and S[t]?
> 
> I'm using R 1.9.0 on Mac OSX.
> 
> Thanks,
> Brian
> 

I think that depends on the font in use. If letters don't have the same 
width, it is difficult to calculate pretty bounding boxes ... and the 
expected behaviour is to left-align. Hence it's as expected.

Are you volunteering to add some fuzz to plotmath.c? I guess you will 
have to add quite a lot of code for this tiny step... (and R is not TeX, 
BTW).

Uwe Ligges



From Brian.Beckage at uvm.edu  Mon Aug  9 19:55:17 2004
From: Brian.Beckage at uvm.edu (Brian Beckage)
Date: Mon, 9 Aug 2004 13:55:17 -0400
Subject: [R] Simultaneous subscripts and superscripts
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF81E5@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF81E5@usrymx25.merck.com>
Message-ID: <p06110405bd3d6997c1fd@[10.0.1.2]>

Yep, it looks good on my system as well.  I had tried 
expression(S^2[t]) but not expression(S[t]^2) and the order matters.

Thanks for the help.

Brian





Brian

At 1:44 PM -0400 8/9/04, Liaw, Andy wrote:
>expression(S[t]^2) looks pretty good for me (on windows()).
>
>Andy
>
>>  From: Brian Beckage
>>
>>  Dear List,
>>
>>  I'd like to add text to a plot where a text symbol has both a
>>  superscript and subscript.  For example, the variable S with a
>>  superscript 2 and a subscript t.  I have been able to accomplish this
>>  using either
>>
>>  expression( paste(S,atop(scriptstyle(2),scriptstyle(t))) )
>>
>>  or
>>
>>  expression( {S[t]}^2 )
>>
>>  but the spacing isn't quite right (or rather what I'd like) using
>>  either of these.  By 'right' I mean the placement and spacing don't
>>  quite coincide with what I'd expect if I superimposed S^2 with S[t].
>>  Is there a way to place BOTH a subscript and superscript so that they
>>  are placed identically as if I had overlayed S^2 and S[t]?
>>
>>  I'm using R 1.9.0 on Mac OSX.
>>
>>  Thanks,
>>  Brian
>>
>>
>>  --
>>  *********************************************************************
>>  Brian Beckage
>>  Department of Botany
>>  University of Vermont
>>  Marsh Life Science Building
>>  Burlington, VT 05405
>>
>>  ______________________________________________
>>  R-help at stat.math.ethz.ch mailing list
>>  https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>  PLEASE do read the posting guide!
>>  http://www.R-project.org/posting-guide.html
>>
>>
>
>
>------------------------------------------------------------------------------
>Notice:  This e-mail message, together with any attachments, 
>contains information of Merck & Co., Inc. (One Merck Drive, 
>Whitehouse Station, New Jersey, USA 08889), and/or its affiliates 
>(which may be known outside the United States as Merck Frosst, Merck 
>Sharp & Dohme or MSD and in Japan, as Banyu) that may be 
>confidential, proprietary copyrighted and/or legally privileged. It 
>is intended solely for the use of the individual or entity named on 
>this message.  If you are not the intended recipient, and have 
>received this message in error, please notify us immediately by 
>reply e-mail and then delete it from your system.
>------------------------------------------------------------------------------


-- 
*********************************************************************
Brian Beckage
Department of Botany
University of Vermont
Marsh Life Science Building
Burlington, VT 05405

Phone:  802 656-0197
Fax  :  802 656-0440
email:  Brian.Beckage at uvm.edu
web  :  www.uvm.edu/~bbeckage



From rolf at math.unb.ca  Mon Aug  9 20:03:31 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Mon, 9 Aug 2004 15:03:31 -0300 (ADT)
Subject: [R] Simultaneous subscripts and superscripts
Message-ID: <200408091803.i79I3Vcr007671@erdos.math.unb.ca>

Andy Liaw wrote

> expression(S[t]^2) looks pretty good to me (on windows()).

And to me too (sparc-sun-solaris2.9; R version 1.9.1).

			cheers,

				Rolf Turner



From andy_liaw at merck.com  Mon Aug  9 20:10:36 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 9 Aug 2004 14:10:36 -0400
Subject: [R] Simultaneous subscripts and superscripts
Message-ID: <3A822319EB35174CA3714066D590DCD504AF81EA@usrymx25.merck.com>

I guess it's good to type what makes sense:  I can understand squaring S[t],
but it's hard for me to fathom subscripting S^2...

> -----Original Message-----
> From: Brian Beckage [mailto:Brian.Beckage at uvm.edu] 
> Sent: Monday, August 09, 2004 1:55 PM
> To: Liaw, Andy; r-help at stat.math.ethz.ch
> Subject: RE: [R] Simultaneous subscripts and superscripts
> 
> 
> Yep, it looks good on my system as well.  I had tried 
> expression(S^2[t]) but not expression(S[t]^2) and the order matters.
> 
> Thanks for the help.
> 
> Brian
> 
> 
> 
> 
> 
> Brian
> 
> At 1:44 PM -0400 8/9/04, Liaw, Andy wrote:
> >expression(S[t]^2) looks pretty good for me (on windows()).
> >
> >Andy
> >
> >>  From: Brian Beckage
> >>
> >>  Dear List,
> >>
> >>  I'd like to add text to a plot where a text symbol has both a
> >>  superscript and subscript.  For example, the variable S with a
> >>  superscript 2 and a subscript t.  I have been able to 
> accomplish this
> >>  using either
> >>
> >>  expression( paste(S,atop(scriptstyle(2),scriptstyle(t))) )
> >>
> >>  or
> >>
> >>  expression( {S[t]}^2 )
> >>
> >>  but the spacing isn't quite right (or rather what I'd like) using
> >>  either of these.  By 'right' I mean the placement and 
> spacing don't
> >>  quite coincide with what I'd expect if I superimposed S^2 
> with S[t].
> >>  Is there a way to place BOTH a subscript and superscript 
> so that they
> >>  are placed identically as if I had overlayed S^2 and S[t]?
> >>
> >>  I'm using R 1.9.0 on Mac OSX.
> >>
> >>  Thanks,
> >>  Brian
> >>
> >>
> >>  --
> >>  
> *********************************************************************
> >>  Brian Beckage
> >>  Department of Botany
> >>  University of Vermont
> >>  Marsh Life Science Building
> >>  Burlington, VT 05405
> >>
> >>  ______________________________________________
> >>  R-help at stat.math.ethz.ch mailing list
> >>  https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >>  PLEASE do read the posting guide!
> >>  http://www.R-project.org/posting-guide.html
> >>
> >>
> >
> >
> >-------------------------------------------------------------
> -----------------
> >Notice:  This e-mail message, together with any attachments, 
> >contains information of Merck & Co., Inc. (One Merck Drive, 
> >Whitehouse Station, New Jersey, USA 08889), and/or its affiliates 
> >(which may be known outside the United States as Merck Frosst, Merck 
> >Sharp & Dohme or MSD and in Japan, as Banyu) that may be 
> >confidential, proprietary copyrighted and/or legally privileged. It 
> >is intended solely for the use of the individual or entity named on 
> >this message.  If you are not the intended recipient, and have 
> >received this message in error, please notify us immediately by 
> >reply e-mail and then delete it from your system.
> >-------------------------------------------------------------
> -----------------
> 
> 
> -- 
> *********************************************************************
> Brian Beckage
> Department of Botany
> University of Vermont
> Marsh Life Science Building
> Burlington, VT 05405
> 
> Phone:  802 656-0197
> Fax  :  802 656-0440
> email:  Brian.Beckage at uvm.edu
> web  :  www.uvm.edu/~bbeckage
> *********************************************************************
> 
>



From Brian.Beckage at uvm.edu  Mon Aug  9 20:15:44 2004
From: Brian.Beckage at uvm.edu (Brian Beckage)
Date: Mon, 9 Aug 2004 14:15:44 -0400
Subject: [R] Simultaneous subscripts and superscripts
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF81EA@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF81EA@usrymx25.merck.com>
Message-ID: <p06110407bd3d6f90285f@[10.0.1.2]>

Yes, I agree.  I thought I had tried it both way, but I guess I was 
too quick to look for more complicated solutions.

Brian


At 2:10 PM -0400 8/9/04, Liaw, Andy wrote:
>I guess it's good to type what makes sense:  I can understand squaring S[t],
>but it's hard for me to fathom subscripting S^2...
>
>>  -----Original Message-----
>>  From: Brian Beckage [mailto:Brian.Beckage at uvm.edu]
>>  Sent: Monday, August 09, 2004 1:55 PM
>>  To: Liaw, Andy; r-help at stat.math.ethz.ch
>>  Subject: RE: [R] Simultaneous subscripts and superscripts
>>
>>
>>  Yep, it looks good on my system as well.  I had tried
>>  expression(S^2[t]) but not expression(S[t]^2) and the order matters.
>>
>>  Thanks for the help.
>>
>>  Brian
>>
>>
>>
>>
>>
>>  Brian
>>
>>  At 1:44 PM -0400 8/9/04, Liaw, Andy wrote:
>>  >expression(S[t]^2) looks pretty good for me (on windows()).
>>  >
>>  >Andy
>>  >
>>  >>  From: Brian Beckage
>>  >>
>>  >>  Dear List,
>>  >>
>>  >>  I'd like to add text to a plot where a text symbol has both a
>>  >>  superscript and subscript.  For example, the variable S with a
>>  >>  superscript 2 and a subscript t.  I have been able to
>>  accomplish this
>>  >>  using either
>>  >>
>>  >>  expression( paste(S,atop(scriptstyle(2),scriptstyle(t))) )
>>  >>
>>  >>  or
>>  >>
>>  >>  expression( {S[t]}^2 )
>>  >>
>>  >>  but the spacing isn't quite right (or rather what I'd like) using
>>  >>  either of these.  By 'right' I mean the placement and
>>  spacing don't
>>  >>  quite coincide with what I'd expect if I superimposed S^2
>>  with S[t].
>>  >>  Is there a way to place BOTH a subscript and superscript
>>  so that they
>>  >>  are placed identically as if I had overlayed S^2 and S[t]?
>>  >>
>>  >>  I'm using R 1.9.0 on Mac OSX.
>>  >>
>>  >>  Thanks,
>>  >>  Brian
>>  >>
>>  >>
>>  >>  --
>>  >> 
>>  *********************************************************************
>>  >>  Brian Beckage
>>  >>  Department of Botany
>>  >>  University of Vermont
>>  >>  Marsh Life Science Building
>>  >>  Burlington, VT 05405
>>  >>
>>  >>  ______________________________________________
>>  >>  R-help at stat.math.ethz.ch mailing list
>>  >>  https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>  >>  PLEASE do read the posting guide!
>>  >>  http://www.R-project.org/posting-guide.html
>>  >>
>>  >>
>>  >
>>  >
>>  >-------------------------------------------------------------
>>  -----------------
>>  >Notice:  This e-mail message, together with any attachments,
>>  >contains information of Merck & Co., Inc. (One Merck Drive,
>>  >Whitehouse Station, New Jersey, USA 08889), and/or its affiliates
>>  >(which may be known outside the United States as Merck Frosst, Merck
>>  >Sharp & Dohme or MSD and in Japan, as Banyu) that may be
>>  >confidential, proprietary copyrighted and/or legally privileged. It
>>  >is intended solely for the use of the individual or entity named on
>>  >this message.  If you are not the intended recipient, and have
>>  >received this message in error, please notify us immediately by
>>  >reply e-mail and then delete it from your system.
>>  >-------------------------------------------------------------
>>  -----------------
>>
>>
>>  --
>>  *********************************************************************
>>  Brian Beckage
>>  Department of Botany
>>  University of Vermont
>>  Marsh Life Science Building
>>  Burlington, VT 05405
>>
>>  Phone:  802 656-0197
>>  Fax  :  802 656-0440
>>  email:  Brian.Beckage at uvm.edu
>>  web  :  www.uvm.edu/~bbeckage
>>  *********************************************************************
>>
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
*********************************************************************
Brian Beckage
Department of Botany
University of Vermont
Marsh Life Science Building
Burlington, VT 05405

Phone:  802 656-0197
Fax  :  802 656-0440
email:  Brian.Beckage at uvm.edu
web  :  www.uvm.edu/~bbeckage



From lw7s at cms.mail.virginia.edu  Mon Aug  9 20:27:48 2004
From: lw7s at cms.mail.virginia.edu (Lijuan  Wang)
Date: Mon, 09 Aug 2004 14:27:48 -0400
Subject: [R] Need help on this problem!
Message-ID: <web-81863984@cgatepro-4.mail.virginia.edu>

Hi everyone,
I have posted a similar question to this list, but I don't 
get a reply. I really want to solve this problem, so I 
post it again... 
I am trying to use R to fit some mixed-effects models for 
a nested data. The data is a simulated data with 111 
subjects. Each subject has 6 waves' data.  Below are the 
first two subjects' data : 
> simu1[1:12,]
Grouped Data: gf ~ age | id
    id occasion       age       gf  tau         b0 
      b1        b2 
  1  1        1  4.056714 35.08943 14.8  14.502078 
5.209333 -5.199452
  2  1        2 14.056714 87.18276 14.8  14.502078 
5.209333 -5.199452
  3  1        3 29.056714 91.19566 14.8  14.502078 
5.209333 -5.199452
  4  1        4 42.056714 91.32411 14.8  14.502078 
5.209333 -5.199452
  5  1        5 57.056714 91.47233 14.8  14.502078 
5.209333 -5.199452
  6  1        6 65.056714 91.55138 14.8  14.502078 
5.209333 -5.199452
  7  2        1  3.353627 16.94166 14.8  -0.331536 
5.293478 -5.392813
  8  2        2 13.353627 69.87644 14.8  -0.331536 
5.293478 -5.392813
  9  2        3 28.353627 76.18644 14.8  -0.331536 
5.293478 -5.392813
10  2        4 41.353627 74.89508 14.8  -0.331536 5.293478 
-5.392813
11  2        5 56.353627 73.40505 14.8  -0.331536 5.293478 
-5.392813
12  2        6 64.353627 72.61037 14.8  -0.331536 5.293478 
-5.392813
Firstly, I used lme to fit a mixed effects model with 
quadratic curve.
> simu1$age2<-simu1$age^2
> mm.lme.2<-lme(gf~age+age2,random=~age+age2|id,data=simu1)
It works well:
> summary(mm.lme.2)
Linear mixed-effects model fit by REML
  Data: simu1 
        AIC      BIC    logLik
   5222.225 5267.193 -2601.113

Random effects:
  Formula: ~age + age2 | id
  Structure: General positive-definite, Log-Cholesky 
parametrization
             StdDev       Corr        
(Intercept) 2.729384e+00 (Intr) age  
age         1.068434e-01 0.648       
age2        8.285027e-34 0.000  0.000
Residual    1.082344e+01             

Fixed effects: gf ~ age + age2 
                 Value Std.Error  DF   t-value p-value
(Intercept) 23.902926 1.1606163 553  20.59503  <.0001
age          3.046429 0.0815675 553  37.34854  <.0001
age2        -0.034015 0.0011395 553 -29.85176  <.0001
  Correlation: 
      (Intr) age   
age  -0.790       
age2  0.697 -0.964

Standardized Within-Group Residuals:
        Min         Q1        Med         Q3        Max 
-2.5233682 -0.6801665 -0.1735055  0.5243569  2.7772921 

Number of Observations: 666
Number of Groups: 111 

However, when I tried to fit the following model and the 
starting value is very close to the true value:
> simu.nlme<-nlme(gf~b00 + b10 * age + b20 * 
max(0,(age-tau0)),data=simu1,fixed=b00+b10+b20+tau0~1,random=b00+b10+b20+tau0~1,group=~id, 
start=c(b00=4.08,b10=5.32,b20=-5.29,tau0=14.8),method="REML")
It shows following error:
**************************************
Error in MEEM(object, conLin, control$niterEM) : 
         Singularity in backsolve at level 0, block 1
**************************************
I also fitted the same model by using SAS and winBUGS. 
They all worked well... 

Do you have any suggestions on this problem?
Any reply will be appreciated!
Lijuan



From vograno at evafunds.com  Mon Aug  9 20:27:58 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Mon, 9 Aug 2004 11:27:58 -0700
Subject: [R] error when calling debugger()
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A563D084@phost015.EVAFUNDS.intermedia.net>

Hi,

I am getting an error message when I am trying to run the debugger() on
the last.dump. The debugger() stops after I make a selection. Could
someone please suggest what it might mean? The R log is included below.
This is R-1.8.1 on RH 7.3.

Thanks, Vadim

> load("last.dump.rda")
> debugger(last.dump)
Message:  Error in split(x, f) : Group length is 0 but data length > 0
Available environments had calls:
1: try({
2: local(for (ticker in univ$ticker) {
3: eval.parent(substitute(eval(quote(expr), envir)))
4: eval(expr, p)
5: eval(expr, envir, enclos)
6: eval(quote(for (ticker in univ$ticker) {
7: eval(expr, envir, enclos)
8: local(for (i in l) {
9: eval.parent(substitute(eval(quote(expr), envir)))
10: eval(expr, p)
11: eval(expr, envir, enclos)
12: eval(quote(for (i in l) {
13: eval(expr, envir, enclos)
14: accumulate(accu, lapply(split(seq(length(key)), key), function(i) {
15: lapply(split(seq(length(key)), key), function(i) {
16: split(seq(length(key)), key)
17: split.default(seq(length(key)), key)

Enter an environment number, or 0 to exit  Selection: 1
Error in get(.obj, envir = dump[[.selection]]) : 
	recursive default argument reference



From Jason.L.Higbee at stls.frb.org  Mon Aug  9 20:55:53 2004
From: Jason.L.Higbee at stls.frb.org (Jason.L.Higbee@stls.frb.org)
Date: Mon, 9 Aug 2004 13:55:53 -0500
Subject: [R] Easy acf and pacf for irregular time series in R
Message-ID: <20040809185555.6DF001D636@p3fed1.frb.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040809/2361e319/attachment.pl

From pburns at pburns.seanet.com  Mon Aug  9 22:12:04 2004
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Mon, 09 Aug 2004 21:12:04 +0100
Subject: [R] error when calling debugger()
In-Reply-To: <C698D707214E6F4AB39AB7096C3DE5A563D084@phost015.EVAFUNDS.intermedia.net>
References: <C698D707214E6F4AB39AB7096C3DE5A563D084@phost015.EVAFUNDS.intermedia.net>
Message-ID: <4117DA94.2070503@pburns.seanet.com>

I ran across this behavior in debugger a while ago -- it is R Problem 2656.
As far as I know, no one has made any sense of what is happening in
debugger, but the bug that was the cause of debugger being called was
trying to subscript columns of a matrix that didn't exist.  (You can see the
example in the bug report.)  That doesn't seem like much of a hint in this
case, however.


Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Vadim Ogranovich wrote:

>Hi,
>
>I am getting an error message when I am trying to run the debugger() on
>the last.dump. The debugger() stops after I make a selection. Could
>someone please suggest what it might mean? The R log is included below.
>This is R-1.8.1 on RH 7.3.
>
>Thanks, Vadim
>
>  
>
>>load("last.dump.rda")
>>debugger(last.dump)
>>    
>>
>Message:  Error in split(x, f) : Group length is 0 but data length > 0
>Available environments had calls:
>1: try({
>2: local(for (ticker in univ$ticker) {
>3: eval.parent(substitute(eval(quote(expr), envir)))
>4: eval(expr, p)
>5: eval(expr, envir, enclos)
>6: eval(quote(for (ticker in univ$ticker) {
>7: eval(expr, envir, enclos)
>8: local(for (i in l) {
>9: eval.parent(substitute(eval(quote(expr), envir)))
>10: eval(expr, p)
>11: eval(expr, envir, enclos)
>12: eval(quote(for (i in l) {
>13: eval(expr, envir, enclos)
>14: accumulate(accu, lapply(split(seq(length(key)), key), function(i) {
>15: lapply(split(seq(length(key)), key), function(i) {
>16: split(seq(length(key)), key)
>17: split.default(seq(length(key)), key)
>
>Enter an environment number, or 0 to exit  Selection: 1
>Error in get(.obj, envir = dump[[.selection]]) : 
>	recursive default argument reference
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>



From treebc at telus.net  Mon Aug  9 22:14:59 2004
From: treebc at telus.net (bcatton)
Date: Mon, 9 Aug 2004 13:14:59 -0700
Subject: [R] The sm Package - axis limit values and axis removal
Message-ID: <000001c47e4d$8c548f30$8bc534d1@forestry.ubc.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040809/c9cd3f94/attachment.pl

From f.duan at yale.edu  Mon Aug  9 22:32:00 2004
From: f.duan at yale.edu (F Duan)
Date: Mon,  9 Aug 2004 16:32:00 -0400
Subject: [R] How to import specific column(s) using "read.table"? 
Message-ID: <1092083520.4117df4084926@webmail.med.yale.edu>

Dear R people,

I have a very big tab-delim txt file with header and I only want to import 
several columns into R. I checked the options for "read.table" and only 
found "nrows" which lets you specify the maximum number of rows to read in. 
Although I can use some text editors (e.g., wordpad) to edit the txt file first 
before running R, I feel it?s not very convenient. The reason for me to do this 
is that if I import the whole file into R, it will eat up too much of my 
system?s memory. Even after I remove it later, I still can?t release the memory.

Anyone has any suggestions?

Thank you very much,

Frank



From tlumley at u.washington.edu  Mon Aug  9 22:52:01 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 9 Aug 2004 13:52:01 -0700 (PDT)
Subject: [R] How to import specific column(s) using "read.table"? 
In-Reply-To: <1092083520.4117df4084926@webmail.med.yale.edu>
References: <1092083520.4117df4084926@webmail.med.yale.edu>
Message-ID: <Pine.A41.4.58.0408091336370.17042@homer11.u.washington.edu>

On Mon, 9 Aug 2004, F Duan wrote:

> Dear R people,
>
> I have a very big tab-delim txt file with header and I only want to import
> several columns into R. I checked the options for "read.table" and only
> found "nrows" which lets you specify the maximum number of rows to read in.
> Although I can use some text editors (e.g., wordpad) to edit the txt file first
> before running R, I feel it?s not very convenient. The reason for me to do this
> is that if I import the whole file into R, it will eat up too much of my
> system?s memory. Even after I remove it later, I still can?t release the memory.
>

You can't avoid reading the whole file, but you can avoid having it in
memory.

I'll assume you know how many lines are in the file, call it N. (this
isn't necessary  but it is tidier) and that you are interested in columns
10 and 110, both numeric

If you do something like

inputfile<-file("inputfile.txt",open="r")
result<-data.frame(col10=numeric(N), col110=numeric(N))
chunksize<-1000
nchunks<- ceiling(N/1000)

for(i in 1:nchunks){
	chunk<-read.table(inputfile,nrows=chunksize)
	result[ (i-1)*chunksize+ (1:chunksize),]<-chunk[,c(10,110)]
}

close(inputfile)

you can choose the chunk size so that the memory use is not too bad.

There are also more efficient ways that make you do more of the work (eg
read in lines of text with readLines and use regular expressions to
extract the columns you need)

	-thomas



From rossini at blindglobe.net  Mon Aug  9 22:52:31 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Mon, 09 Aug 2004 13:52:31 -0700
Subject: [R] How to import specific column(s) using "read.table"?
In-Reply-To: <Pine.A41.4.58.0408091336370.17042@homer11.u.washington.edu>
	(Thomas
	Lumley's message of "Mon, 9 Aug 2004 13:52:01 -0700 (PDT)")
References: <1092083520.4117df4084926@webmail.med.yale.edu>
	<Pine.A41.4.58.0408091336370.17042@homer11.u.washington.edu>
Message-ID: <857js8ghkg.fsf@servant.blindglobe.net>


If you've got access to unix tools (i.e. linux or cygwin), consider
the "cut" command.  Great for column selection.


Thomas Lumley <tlumley at u.washington.edu> writes:

> On Mon, 9 Aug 2004, F Duan wrote:
>
>> Dear R people,
>>
>> I have a very big tab-delim txt file with header and I only want to import
>> several columns into R. I checked the options for "read.table" and only
>> found "nrows" which lets you specify the maximum number of rows to read in.
>> Although I can use some text editors (e.g., wordpad) to edit the txt file first
>> before running R, I feel it?s not very convenient. The reason for me to do this
>> is that if I import the whole file into R, it will eat up too much of my
>> system?s memory. Even after I remove it later, I still can?t release the memory.
>>
>
> You can't avoid reading the whole file, but you can avoid having it in
> memory.
>
> I'll assume you know how many lines are in the file, call it N. (this
> isn't necessary  but it is tidier) and that you are interested in columns
> 10 and 110, both numeric
>
> If you do something like
>
> inputfile<-file("inputfile.txt",open="r")
> result<-data.frame(col10=numeric(N), col110=numeric(N))
> chunksize<-1000
> nchunks<- ceiling(N/1000)
>
> for(i in 1:nchunks){
> 	chunk<-read.table(inputfile,nrows=chunksize)
> 	result[ (i-1)*chunksize+ (1:chunksize),]<-chunk[,c(10,110)]
> }
>
> close(inputfile)
>
> you can choose the chunk size so that the memory use is not too bad.
>
> There are also more efficient ways that make you do more of the work (eg
> read in lines of text with readLines and use regular expressions to
> extract the columns you need)
>
> 	-thomas
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Anthony Rossini			    Research Associate Professor
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From ripley at stats.ox.ac.uk  Mon Aug  9 22:56:42 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 9 Aug 2004 21:56:42 +0100 (BST)
Subject: [R] How to import specific column(s) using "read.table"? 
In-Reply-To: <1092083520.4117df4084926@webmail.med.yale.edu>
Message-ID: <Pine.LNX.4.44.0408092152270.12240-100000@gannet.stats>

There is no way for read.table to skip columns.  It is however very easy 
to do this with a preprocessing of the table: cut, awk and perl all come 
to mind, and you could do it in R too, reading a block of rows at a time 
and writing them back out.

scan() can skip columns, but I would still use preprocessing with scan.

On Mon, 9 Aug 2004, F Duan wrote:

> I have a very big tab-delim txt file with header and I only want to import 
> several columns into R. I checked the options for "read.table" and only 
> found "nrows" which lets you specify the maximum number of rows to read in. 
> Although I can use some text editors (e.g., wordpad) to edit the txt file first 
> before running R, I feel it?s not very convenient. The reason for me to do this 
> is that if I import the whole file into R, it will eat up too much of my 
> system?s memory. Even after I remove it later, I still can?t release the memory.

The peculiar quotes suggest this is Windows -- the Rtools we use to build 
R there contain a cut.exe.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sfan_2004 at hotmail.com  Mon Aug  9 23:11:50 2004
From: sfan_2004 at hotmail.com (Jessie F)
Date: Mon, 09 Aug 2004 14:11:50 -0700
Subject: [R] Using R "boxplot" function in Excel
Message-ID: <BAY22-F32DuTrNksglY0001a491@hotmail.com>

Hi, I have downloaded the "R-Com and I was able to run "Interactive Graphics 
Demo 2" in excel. However, I couldn't create my own boxplot. Whenever I 
tried to run any code, it always say" Error in loading DLL", even 
"=rput(A1,A2:A20)". Any idea about what's going wrong? A detailed 
explaination about how to use R-Excel tool would be greatly appreciated.
Thanks a lot in advance!

PS: I would like you to use the following data as an example.
A	 B	 C
12.5186182	7.394714354	6.58360308
11.37597453	16.66820087	3.900166247
7.059103407	9.696804606	3.738396698
13.80587153	21.95622475	5.365668029
7.933769009	9.572635842	4.195704277
14.80409653	12.39208079	6.883236109
8.974253685	12.02387754	5.842696863
7.6083609	18.08369863	4.75223318
10.01654143	10.61151753	4.940416728
10.22753966	7.59634933	5.150066626
9.638591817	17.68393592	5.427933173
12.9405328	17.35731932	5.079704705
7.758718564	14.28801913	5.319497531
9.873025445	16.89445473	5.044402668
8.023517946	16.28102329	5.637006679
7.214663381	24.19544618	5.083052782
11.82039457	5.482319845	5.26250973
8.432808752	14.50188112	7.040906111
10.41255589	8.92899781	3.335806595
14.0030136	18.31841647	3.26446583
9.75501396	18.97398026	6.075650289
11.25837687	16.9443803	5.077193363
13.51650669	22.33716661	2.850945874





From vograno at evafunds.com  Tue Aug 10 02:07:40 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Mon, 9 Aug 2004 17:07:40 -0700
Subject: [R] on.exit() inside local()
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A563D0AD@phost015.EVAFUNDS.intermedia.net>

Hi,

Since I routinely open files in a loop I've developed a habit of using
on.exit() to close them. Since on.exit() needs to be called within a
function I use eval() as a surrogate. For example:

for (fileName in c("a", "b")) eval({
	con <- file(fileName);
	on.exit(close(con))
	}) 

and con will be closed no matter what.


However it stopped working once I wrapped the loop in local():
> local(
+       for (foo in seq(2)) eval({
+         on.exit(cat(foo, "\n"))
+       })
+ )
Error in cat(foo, "\n") : Object "foo" not found


W/o local()it works just fine
>       for (foo in seq(2)) eval({
+         on.exit(cat(foo, "\n"))
+       })
1 
2 

The reason I wanted the local() is to keep 'foo' from interfering with
the existing environments, but somehow this breaks the thing.
At this point I am stuck. Could someone please tell what's going on?

Thanks,
Vadim



From gplomp at brain.riken.jp  Tue Aug 10 05:40:54 2004
From: gplomp at brain.riken.jp (Gijs Plomp)
Date: Tue, 10 Aug 2004 12:40:54 +0900
Subject: [R] Enduring LME
 =?windows-1252?q?confusion=85_or_Psychologists_and_M?=
 =?windows-1252?q?ixed-Effects?=
Message-ID: <411843C6.8030309@brain.riken.jp>

Dear ExpeRts,

Suppose I have a typical psychological experiment that is a 
within-subjects design with multiple crossed variables and a continuous 
response variable. Subjects are considered a random effect. So I could model
 > aov1 <- aov(resp~fact1*fact2+Error(subj/(fact1*fact2))

However, this only holds for orthogonal designs with equal numbers of 
observation and no missing values. These assumptions are easily violated 
so I seek refuge in fitting a mixed-effects model with the nlme library.
 > lme1 <- lme(resp~fact1*fact2, random=~1|subj)

When testing the 'significance? of the effects of my factors, with 
anova(lme1), the degrees of freedom that lme uses in the denominator 
spans all observations and is identical for all factors and their 
interaction. I read in a previous post on the list ("[R] Help with lme 
basics") that this is inherent to lme. I studied the instructive book of 
Pinheiro & Bates and I understand why the degrees of freedom are 
assigned as they are, but think it may not be appropriate in this case. 
Used in this way it seems that lme is more prone to type 1 errors than aov.

To get more conservative degrees of freedom one could model
 > lme2 <- lme(resp~fact1*fact2, random=~1|subj/fact1/fact2)

But this is not a correct model because it assumes the factors to be 
hierarchically ordered, which they are not.

Another alternative is to model the random effect using a matrix, as 
seen in "[R] lme and mixed effects" on this list.
 > lme3 <- (resp~fact1*fact2, random=list(subj=pdIdent(form=~fact1-1),  
subj=~1,  fact2=~1)

This provides 'correct? degrees of freedom for fact1, but not for the 
other effects and I must confess that I don't understand this use of 
matrices, I?m not a statistician.

My questions thus come down to this:

1. When aov?s assumptions are violated, can lme provide the right model 
for within-subjects designs where multiple fixed effects are NOT 
hierarchically ordered?

2. Are the degrees of freedom in anova(lme1) the right ones to report? 
If so, how do I convince a reviewer that, despite the large number of 
degrees of freedom, lme does provide a conservative evaluation of the 
effects? If not, how does one get the right denDf in a way that can be 
easily understood?

I hope that my confusion is all due to an ignorance of statistics and 
that someone on this list will kindly point that out to me. I do realize 
that this type of question has been asked before, but think that an 
illuminating answer can help R spread into the psychological community.



From ggrothendieck at myway.com  Tue Aug 10 07:09:06 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 10 Aug 2004 05:09:06 +0000 (UTC)
Subject: [R] How to import specific column(s) using "read.table"?
References: <1092083520.4117df4084926@webmail.med.yale.edu>
Message-ID: <loom.20040810T070123-553@post.gmane.org>

F Duan <f.duan <at> yale.edu> writes:

> I have a very big tab-delim txt file with header and I only want to import 
> several columns into R. I checked the options for "read.table" and only 

Try using scan with the what=list(...) and flush=TRUE arguments.  
For example, if your data looks like this:

1 2 3 4 
5 6 7 8 
9 10 11 12
13 14 15 16

then you could read columns 2 and 4 into a list with:

   scan(myfile, what = list(0, NULL, 0), flush = TRUE)

or read in and convert to a data frame via:

   do.call("cbind", scan(myfile, what = list(0, NULL, 0), flush = TRUE))



From ggrothendieck at myway.com  Tue Aug 10 08:05:40 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 10 Aug 2004 06:05:40 +0000 (UTC)
Subject: [R] on.exit() inside local()
References: <C698D707214E6F4AB39AB7096C3DE5A563D0AD@phost015.EVAFUNDS.intermedia.net>
Message-ID: <loom.20040810T072222-735@post.gmane.org>

Vadim Ogranovich <vograno <at> evafunds.com> writes:

: 
: Hi,
: 
: Since I routinely open files in a loop I've developed a habit of using
: on.exit() to close them. Since on.exit() needs to be called within a
: function I use eval() as a surrogate. For example:
: 
: for (fileName in c("a", "b")) eval({
: 	con <- file(fileName);
: 	on.exit(close(con))
: 	}) 
: 
: and con will be closed no matter what.
: 
: However it stopped working once I wrapped the loop in local():
: > local(
: +       for (foo in seq(2)) eval({
: +         on.exit(cat(foo, "\n"))
: +       })
: + )
: Error in cat(foo, "\n") : Object "foo" not found
: 
: 
: W/o local()it works just fine
: >       for (foo in seq(2)) eval({
: +         on.exit(cat(foo, "\n"))
: +       })
: 1 
: 2 
: 
: The reason I wanted the local() is to keep 'foo' from interfering with
: the existing environments, but somehow this breaks the thing.
: At this point I am stuck. Could someone please tell what's going on?

The on.exit code is executing in an environment whose parent is
namespace:base and so cannot access the environment created by
local.  Use evalq, instead of eval, which has the effect of
running the on.exit code in the environment created by
local:

local({
   for (i in c("a", "b")) evalq(
         on.exit(cat(i, "\n"))
      )
})

or use an inner local, which has the effect of creating
a new environment for each iteration of the loop in which
the on.exit code runs:

local({
   for (i in c("a", "b")) local(
      on.exit(cat(i, "\n"))
   )
})



From ggrothendieck at myway.com  Tue Aug 10 08:09:11 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 10 Aug 2004 06:09:11 +0000 (UTC)
Subject: [R] How to import specific column(s) using "read.table"?
References: <1092083520.4117df4084926@webmail.med.yale.edu>
	<loom.20040810T070123-553@post.gmane.org>
Message-ID: <loom.20040810T080806-990@post.gmane.org>

Gabor Grothendieck <ggrothendieck <at> myway.com> writes:

: 
: F Duan <f.duan <at> yale.edu> writes:
: 
: > I have a very big tab-delim txt file with header and I only want to import 
: > several columns into R. I checked the options for "read.table" and only 
: 
: Try using scan with the what=list(...) and flush=TRUE arguments.  
: For example, if your data looks like this:
: 
: 1 2 3 4 
: 5 6 7 8 
: 9 10 11 12
: 13 14 15 16
: 
: then you could read columns 2 and 4 into a list with:
: 

oops. That should be 1 and 3.

:    scan(myfile, what = list(0, NULL, 0), flush = TRUE)
: 
: or read in and convert to a data frame via:
: 
:    do.call("cbind", scan(myfile, what = list(0, NULL, 0), flush = TRUE))



From Toby.Patterson at csiro.au  Tue Aug 10 08:34:42 2004
From: Toby.Patterson at csiro.au (Toby.Patterson@csiro.au)
Date: Tue, 10 Aug 2004 16:34:42 +1000
Subject: [R] date axes and formats  in levelplot
Message-ID: <C4178DC99E08604EA5E2BDB989F09380025D0C78@extas2-hba.tas.csiro.au>

Hi all (and particularly Deepayan), 

A while back Deepayan helped me with the query in the text below (thanks
again). Specifically it was about changing the way that dates plotted on
the axes of lattice plots. 

While this works using xyplot, everything comes apart when I use
levelplot. The axis labels on the date axis are shown as the integer
representation of the date (number of seconds since the origin I
assume). I guess that the POSIX dates are getting coerced into numeric
objects somewhere along the way and that there is no easy fix for this. 

I would be really grateful if there is a work around that would allow me
to plot recognizable dates. Any suggestions?

Cheers 
Toby  

-----Original Message-----
From: Deepayan Sarkar [mailto:deepayan at stat.wisc.edu] 
Sent: Monday, July 05, 2004 12:16 PM
To: r-help at stat.math.ethz.ch
Cc: Patterson, Toby (Marine, Hobart)
Subject: Re: [R] date Axes and formats in lattice plots

On Sunday 04 July 2004 21:02, Toby.Patterson at csiro.au wrote:
> All,
>
> I have some data of animal movements that I'm plotting using xyplot()
> from lattice. I want to have the date (class POSIXct object) on the
> Y-axis and the animals longitude on X-axis. Eg.
>
> xyplot(date ~ longitude, groups = animal, data = my.data)
>
> with data like:
>
>      animal   ptt year month day    lon                date
> 125 03P0014 13273 2003     7  10 150.38 2003-07-10 14:03:48
> 126 03P0192 20890 2003     7  10 151.13 2003-07-10 14:00:47
> 127 03P0197 30466 2003     7  10 150.74 2003-07-10 14:02:21
> ...etc
>
> It all works fine except for the format of the dates that gets
> displayed.
>
> I am not sure what I need to change within the lattice frame work to
> get a specific date format (eg. "%Y-%-m-%d"). Does anyone have any
> tips or, even better, some example code that they could pass on?

For R 1.9.0 and above, you should be able to do this with 

xyplot(date ~ longitude, groups = animal, data = my.data,
       scales = list(y = list(format = "%Y-%-m-%d")))

Deepayan



From V.Khamenia at biovision-discovery.de  Tue Aug 10 09:29:19 2004
From: V.Khamenia at biovision-discovery.de (Khamenia, Valery)
Date: Tue, 10 Aug 2004 09:29:19 +0200
Subject: AW: AW: [R] built-in Sweave-like documentation in R-2.x
Message-ID: <D15343265276D31197BC00A024A6C110C79339@EXS_BDC>

> > Is selecting and 'C-c C-r'-ing the 3 chunks separately that bad?
> 
> Yes.  The UI should take care of it for him.

right.
 
> > Others may have better suggestions.
> 
> A bit more work on the chunk evaluation approach within Emacs is one;
> it almost does what is needed, but not quite.  

why almost, but not quite?

...without these "almost, but not quite" I would rather
confirm your statemnt :)

--
Valery



From vito_ricci at yahoo.com  Tue Aug 10 09:34:22 2004
From: vito_ricci at yahoo.com (=?iso-8859-1?q?Vito=20Ricci?=)
Date: Tue, 10 Aug 2004 09:34:22 +0200 (CEST)
Subject: [R] Using R "boxplot" function in Excel
Message-ID: <20040810073422.16584.qmail@web41209.mail.yahoo.com>

Hi,

I tried to create boxplot in Excel using Rcom (ver.
1.0) and it works correctly. Explain better A,B,C: are
three samples? groups? do you want a boxplot using
A,B,C for grouping?
Maybe there are corrupted files in Rcom installation,
re-install Rcom.

Best
Vito




Hi, I have downloaded the "R-Com and I was able to run
"Interactive Graphics 
Demo 2" in excel. However, I couldn't create my own
boxplot. Whenever I 
tried to run any code, it always say" Error in loading
DLL", even 
"=rput(A1,A2:A20)". Any idea about what's going wrong?
A detailed 
explaination about how to use R-Excel tool would be
greatly appreciated.
Thanks a lot in advance!

PS: I would like you to use the following data as an
example.
A	 B	 C
12.5186182	7.394714354	6.58360308
11.37597453	16.66820087	3.900166247
7.059103407	9.696804606	3.738396698
13.80587153	21.95622475	5.365668029
7.933769009	9.572635842	4.195704277
14.80409653	12.39208079	6.883236109
8.974253685	12.02387754	5.842696863
7.6083609	18.08369863	4.75223318
10.01654143	10.61151753	4.940416728
10.22753966	7.59634933	5.150066626
9.638591817	17.68393592	5.427933173
12.9405328	17.35731932	5.079704705
7.758718564	14.28801913	5.319497531
9.873025445	16.89445473	5.044402668
8.023517946	16.28102329	5.637006679
7.214663381	24.19544618	5.083052782
11.82039457	5.482319845	5.26250973
8.432808752	14.50188112	7.040906111
10.41255589	8.92899781	3.335806595
14.0030136	18.31841647	3.26446583
9.75501396	18.97398026	6.075650289
11.25837687	16.9443803	5.077193363
13.51650669	22.33716661	2.850945874

=====
Diventare costruttori di soluzioni

Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese http://www.modugno.it/archivio/cat_palese.shtml



From vito_ricci at yahoo.com  Tue Aug 10 09:45:00 2004
From: vito_ricci at yahoo.com (=?iso-8859-1?q?Vito=20Ricci?=)
Date: Tue, 10 Aug 2004 09:45:00 +0200 (CEST)
Subject: [R] Using R "boxplot" function in Excel
Message-ID: <20040810074500.78766.qmail@web41204.mail.yahoo.com>

Hi,

there is a mailing list about R Com, if you still have
problems write to that list.

See:

http://mailman.csd.univie.ac.at/mailman/listinfo/rcom-l

Best
Vito




Hi, I have downloaded the "R-Com and I was able to run
"Interactive Graphics 
Demo 2" in excel. However, I couldn't create my own
boxplot. Whenever I 
tried to run any code, it always say" Error in loading
DLL", even 
"=rput(A1,A2:A20)". Any idea about what's going wrong?
A detailed 
explaination about how to use R-Excel tool would be
greatly appreciated.
Thanks a lot in advance!

PS: I would like you to use the following data as an
example.
A	 B	 C
12.5186182	7.394714354	6.58360308
11.37597453	16.66820087	3.900166247
7.059103407	9.696804606	3.738396698
13.80587153	21.95622475	5.365668029
7.933769009	9.572635842	4.195704277
14.80409653	12.39208079	6.883236109
8.974253685	12.02387754	5.842696863
7.6083609	18.08369863	4.75223318
10.01654143	10.61151753	4.940416728
10.22753966	7.59634933	5.150066626
9.638591817	17.68393592	5.427933173
12.9405328	17.35731932	5.079704705
7.758718564	14.28801913	5.319497531
9.873025445	16.89445473	5.044402668
8.023517946	16.28102329	5.637006679
7.214663381	24.19544618	5.083052782
11.82039457	5.482319845	5.26250973
8.432808752	14.50188112	7.040906111
10.41255589	8.92899781	3.335806595
14.0030136	18.31841647	3.26446583
9.75501396	18.97398026	6.075650289
11.25837687	16.9443803	5.077193363
13.51650669	22.33716661	2.850945874

=====
Diventare costruttori di soluzioni

Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese http://www.modugno.it/archivio/cat_palese.shtml



From V.Khamenia at biovision-discovery.de  Tue Aug 10 09:53:53 2004
From: V.Khamenia at biovision-discovery.de (Khamenia, Valery)
Date: Tue, 10 Aug 2004 09:53:53 +0200
Subject: AW: [R] built-in Sweave-like documentation in R-2.x
Message-ID: <D15343265276D31197BC00A024A6C110C7933A@EXS_BDC>

hi tony,

> What exactly do you mean by this?
> 1. generation of Sweave-style docs from R programs or interaction?  

neither (if i correctly interpret your question).

> 2. tools for doing docs and analysis at the same time?  Emacs Speaks
> Statistics has supported this with R since last century (1997 or so).

as you have seen, i use emacs and even since last century :)

> 3. the vignettes of Bioconductor?

not sure.

> 4. a text book in line with the above?

nope.

I think just smarter C-c C-r would be kind of trade-off here.

hm, maybe there are some other voices here similar to mine?
It would be easier to discuss the subj. 

--
Valery.



From Wanzare at HCJP.com  Tue Aug 10 10:18:24 2004
From: Wanzare at HCJP.com (Manoj - Hachibushu Capital)
Date: Tue, 10 Aug 2004 17:18:24 +0900
Subject: [R] RSPerl on Redhat 9 (i386 box) and R-1.9.1
Message-ID: <1CBA12F2D414914989C723D196B287DC055675@jp-svr-ex1.hcjp.com>

Hi,
	Sorry for the open-ended nature of the question but was *anyone*
able to successfully install RSPerl(version 0.5-7) on Redhat 9, i386 box
for the latest R version (1.9.1)?
	
	I tried far too many things to make it work but am unable to get
it working for any case (R within Perl or Perl within R... my main
interest is R within perl).

	If anyone is interested, I can document all my (unsuccessful)
efforts that I tried to make it work. 
	
Cheers

Manoj



From i.visser at uva.nl  Tue Aug 10 10:42:50 2004
From: i.visser at uva.nl (Ingmar Visser)
Date: Tue, 10 Aug 2004 10:42:50 +0200
Subject: [R] linear constraint optim with bounds/reparametrization
In-Reply-To: <Pine.A41.4.58.0408090732530.263586@homer10.u.washington.edu>
Message-ID: <BD3E572A.5E5E%i.visser@uva.nl>

On 8/9/04 4:52 PM, "Thomas Lumley" <tlumley at u.washington.edu> wrote:

> On Mon, 9 Aug 2004, Kahra Hannu wrote:
> 
>>> 1) constrOptim does not work in this case because it only fits inequality
>>> constraints, ie A%*%theta > =  c
>>                           --- I was struggling with the same problem a
>> few weeks ago in the portfolio optimization context. You can impose
>> equality constraints by using inequality constraints >= and <=
>> simultaneously. See the example bellow.
>> 
> 
> Ick. You do not want to use constrOptim for equality constraints.
> constrOptim is a log-barrier interior-point method, meaning that it adds
> a multiple of log(A%*%theta-c) to the objective function. This is a really
> bad idea as a way of faking equality constraints.
> 
> Use Lagrange multipliers and optim.

Is there a package that does all that for me? Or is there example code that
does something similar?

ingmar



From Luisr at frs.fo  Tue Aug 10 11:28:04 2004
From: Luisr at frs.fo (Luis Rideau Cruz)
Date: Tue, 10 Aug 2004 10:28:04 +0100
Subject: [R] two-way ANOVA
Message-ID: <s118a33a.077@ffdata.setur.fo>

R-help

This is more a statistic thing than an R question.

I have length measurements of organisms which I want to use for an two-way ANOVA( fixed factors)
The problem is that I have different number of replicates for each combination of factors.

What are the strengths and weakness of these approach when I applly aov function in R.
The help file states  : 
     'aov' is designed for balanced designs, and the results can be
     hard to interpret without balance.....

Thank you 


Luis Ridao Cruz
Fiskiranns??knarstovan
N??at??n 1
P.O. Box 3051
FR-110 T??rshavn
Faroe Islands
Phone:             +298 353900
Phone(direct): +298 353912
Mobile:             +298 580800
Fax:                 +298 353901
E-mail:              luisr at frs.fo
Web:                www.frs.fo



From spencer.graves at pdf.com  Tue Aug 10 12:25:22 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 10 Aug 2004 06:25:22 -0400
Subject: [R] linear constraint optim with bounds/reparametrization
In-Reply-To: <BD3E572A.5E5E%i.visser@uva.nl>
References: <BD3E572A.5E5E%i.visser@uva.nl>
Message-ID: <4118A292.8010206@pdf.com>

	  If A%*%theta>c, then log(c-A%*%theta) returns NA.  if A%*%theta<c, log(A%*%theta-c) returns NA.  Only when A%*%theta==c do you get a "number" from log(A%*%theta-c), and that's (-Inf).  

	  However, for an equality constraint, I've had good luck by with an objective function that adds something like the following to my objective function:  

	  constraintViolationPenalty*(A%*%theta-c)^2,  

where "constraintViolationPenalty" is passed via "..." in a call to optim.  If I want only (A%*%theta>=c), then I might write this as follows:  

	  constraintViolationPenalty*(A%*%theta<c)*(A%*%theta-c)^2  

           This term is everywhere differentiable and is 0 when the 
constraint is satisfied. 

            I may first run optim with a modest value for 
constraintViolationPenalty then restart it with the output of the 
initial run as starting values and with a larger value for 
constraintViolationPenalty. 

      hope this helps.  spencer graves

Ingmar Visser wrote:

>On 8/9/04 4:52 PM, "Thomas Lumley" <tlumley at u.washington.edu> wrote:
>
>  
>
>>On Mon, 9 Aug 2004, Kahra Hannu wrote:
>>
>>    
>>
>>>>1) constrOptim does not work in this case because it only fits inequality
>>>>constraints, ie A%*%theta > =  c
>>>>        
>>>>
>>>                          --- I was struggling with the same problem a
>>>few weeks ago in the portfolio optimization context. You can impose
>>>equality constraints by using inequality constraints >= and <=
>>>simultaneously. See the example bellow.
>>>
>>>      
>>>
>>Ick. You do not want to use constrOptim for equality constraints.
>>constrOptim is a log-barrier interior-point method, meaning that it adds
>>a multiple of log(A%*%theta-c) to the objective function. This is a really
>>bad idea as a way of faking equality constraints.
>>
>>Use Lagrange multipliers and optim.
>>    
>>
>
>Is there a package that does all that for me? Or is there example code that
>does something similar?
>
>ingmar
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From spencer.graves at pdf.com  Tue Aug 10 12:44:20 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 10 Aug 2004 06:44:20 -0400
Subject: [R] Enduring LME =?windows-1252?Q?confusion=85_or_Psycho?=
	=?windows-1252?Q?logists_and_Mixed-Effects?=
In-Reply-To: <411843C6.8030309@brain.riken.jp>
References: <411843C6.8030309@brain.riken.jp>
Message-ID: <4118A704.1030303@pdf.com>

      Have you considered trying a Monte Carlo?  The significance 
probabilities for unbalanced anovas use approximations.  Package nlme 
provides "simulate.lme" to facilitate this.  I believe this function is 
also mentioned in Pinheiro and Bates (2000). 

      hope this helps.  spencer graves
p.s.  You could try the same thing in both library(nlme) and 
library(lme4).  Package lme4 is newer and, at least for most cases, 
better. 

Gijs Plomp wrote:

> Dear ExpeRts,
>
> Suppose I have a typical psychological experiment that is a 
> within-subjects design with multiple crossed variables and a 
> continuous response variable. Subjects are considered a random effect. 
> So I could model
> > aov1 <- aov(resp~fact1*fact2+Error(subj/(fact1*fact2))
>
> However, this only holds for orthogonal designs with equal numbers of 
> observation and no missing values. These assumptions are easily 
> violated so I seek refuge in fitting a mixed-effects model with the 
> nlme library.
> > lme1 <- lme(resp~fact1*fact2, random=~1|subj)
>
> When testing the 'significance? of the effects of my factors, with 
> anova(lme1), the degrees of freedom that lme uses in the denominator 
> spans all observations and is identical for all factors and their 
> interaction. I read in a previous post on the list ("[R] Help with lme 
> basics") that this is inherent to lme. I studied the instructive book 
> of Pinheiro & Bates and I understand why the degrees of freedom are 
> assigned as they are, but think it may not be appropriate in this 
> case. Used in this way it seems that lme is more prone to type 1 
> errors than aov.
>
> To get more conservative degrees of freedom one could model
> > lme2 <- lme(resp~fact1*fact2, random=~1|subj/fact1/fact2)
>
> But this is not a correct model because it assumes the factors to be 
> hierarchically ordered, which they are not.
>
> Another alternative is to model the random effect using a matrix, as 
> seen in "[R] lme and mixed effects" on this list.
> > lme3 <- (resp~fact1*fact2, random=list(subj=pdIdent(form=~fact1-1),  
> subj=~1,  fact2=~1)
>
> This provides 'correct? degrees of freedom for fact1, but not for the 
> other effects and I must confess that I don't understand this use of 
> matrices, I?m not a statistician.
>
> My questions thus come down to this:
>
> 1. When aov?s assumptions are violated, can lme provide the right 
> model for within-subjects designs where multiple fixed effects are NOT 
> hierarchically ordered?
>
> 2. Are the degrees of freedom in anova(lme1) the right ones to report? 
> If so, how do I convince a reviewer that, despite the large number of 
> degrees of freedom, lme does provide a conservative evaluation of the 
> effects? If not, how does one get the right denDf in a way that can be 
> easily understood?
>
> I hope that my confusion is all due to an ignorance of statistics and 
> that someone on this list will kindly point that out to me. I do 
> realize that this type of question has been asked before, but think 
> that an illuminating answer can help R spread into the psychological 
> community.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From david.crabb at ntu.ac.uk  Tue Aug 10 13:52:11 2004
From: david.crabb at ntu.ac.uk (Crabb, David)
Date: Tue, 10 Aug 2004 12:52:11 +0100
Subject: [R] Help with Normal Range Estimation for repated measures 
Message-ID: <E2AEBF332DE0BE43966A19AB2622749D62F282@poplar.ads.ntu.ac.uk>

I would be grateful if members of the list could point me in the
direction of any code (preferably in R) that will allow me to estimate
95th percentiles from a set of repeated measurements. For example, we
are interested in a clinical measurement where we have 3 measures for 14
subjects and 2 measurements on 24 subjects and single measurement on 36
subjects. We want to combine these to form a Normal range by using
something that takes account that some of the measures are repeats.
Something non-parametric would be ideal like a weighted empirical
distribution function. In other words we don't simply want to use 84
single values from the 84 subjects but use all the data (but we are
aware this needs to be corrected for).

Any help, however small, with this problem will be gratefully received.


-----------------------------------------------
Dr. David Crabb
School of Science,
The Nottingham Trent University,
Clifton Campus, Nottingham. NG11 8NS
Tel: 0115 848 3275   Fax: 0115 848 6690



From vito_ricci at yahoo.com  Tue Aug 10 14:23:59 2004
From: vito_ricci at yahoo.com (=?iso-8859-1?q?Vito=20Ricci?=)
Date: Tue, 10 Aug 2004 14:23:59 +0200 (CEST)
Subject: [R] Help with Normal Range Estimation for repated measures
Message-ID: <20040810122359.54783.qmail@web41201.mail.yahoo.com>

Hi David,

I can only suggest you to see these links concerning
quantile estimation:

http://www.stanford.edu/class/msande223/handouts/lecturenotes09.pdf

http://www.math.ntnu.no/mmr2002/papers/invited/Bordes.pdf

maybe you find something helpful for your work.

See also ? quantile() in R.

I hope I give you a small help.

Best

Vito


I would be grateful if members of the list could point
me in the
direction of any code (preferably in R) that will
allow me to estimate
95th percentiles from a set of repeated measurements.
For example, we
are interested in a clinical measurement where we have
3 measures for 14
subjects and 2 measurements on 24 subjects and single
measurement on 36
subjects. We want to combine these to form a Normal
range by using
something that takes account that some of the measures
are repeats.
Something non-parametric would be ideal like a
weighted empirical
distribution function. In other words we don't simply
want to use 84
single values from the 84 subjects but use all the
data (but we are
aware this needs to be corrected for).

Any help, however small, with this problem will be
gratefully received.


-----------------------------------------------
Dr. David Crabb
School of Science,
The Nottingham Trent University,
Clifton Campus, Nottingham. NG11 8NS
Tel: 0115 848 3275   Fax: 0115 848 6690

=====
Diventare costruttori di soluzioni

Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese http://www.modugno.it/archivio/cat_palese.shtml



From a.trapletti at bluewin.ch  Tue Aug 10 14:54:22 2004
From: a.trapletti at bluewin.ch (Adrian Trapletti)
Date: Tue, 10 Aug 2004 14:54:22 +0200
Subject: [R] Easy acf and pacf for irregular time series in R
Message-ID: <4118C57E.9080400@bluewin.ch>

>
>
>R:
>
>Is there an easy way to get the acf and pacf for an irregular times 
>series?  That is, the acf and pacf with lag lengths that are in units of 
>time, not observation number.
>  
>
There are several solutions available depending on the particular 
problem, some of them statistically "cleaner" than others:
For example eliminate non-business days (NA's) from the series and 
compute the acf and pacf (e.g. with na.remove from tseries).
For example interpolate to get a regular series and compute acf and pacf 
(e.g. with approx.irts from tseries).
For example use a methodology which can treat NA's (e.g. use Kalman 
filtering from ts (R-1.8.1) now ??) and compute the acf and pacf from 
the estimated model...

best
Adrian

>
>Thanks,
>
>Jason Higbee
>Research Associate
>Federal Reserve Bank of St. Louis
>The views expressed in this email are the author's and not necessarily 
>those of the Federal Reserve Bank of St. Louis or the Federal Reserve 
>System
>	[[alternative HTML version deleted]]
>



From michael.waters at dtn.ntl.com  Tue Aug 10 15:15:39 2004
From: michael.waters at dtn.ntl.com (Dr Mike Waters)
Date: Tue, 10 Aug 2004 14:15:39 +0100
Subject: [R] R packages install problems linux - X not found (WhiteBoxEL 3)
In-Reply-To: <1092060760.25174.134.camel@localhost.localdomain>
Message-ID: <014701c47edc$26274830$6600a8c0@mainman>


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Marc Schwartz
> Sent: 09 August 2004 15:13
> To: Dr Mike Waters
> Cc: R-Help
> Subject: RE: [R] R packages install problems linux - X not 
> found (WhiteBoxEL 3)
> 
> 
> On Mon, 2004-08-09 at 08:13, Dr Mike Waters wrote:
> 
> <snip>
> 
> > Marc,
> > 
> > Sorry for the confusion yesterday - in my defence, it was 
> very hot and 
> > humid here in Hampshire (31 Celsius at 15:00hrs and still 25 at 
> > 20:00hrs).
> > 
> > What had happened was that I had done a clean install of WB Linux, 
> > including the XFree86 and other developer packages. However, the 
> > on-line updating system updated the XFree86 packages to a newer sub 
> > version. It seems that it didn't do this correctly for the XFree86 
> > developer package, which was missing vital files. However 
> it showed up 
> > in the rpm database as being installed (i.e. rpm -qa | grep XFree 
> > showed it thus). I downloaded another rpm for this manually 
> and I only 
> > forced the upgrade because it was the same version as already 
> > 'installed' (as far as the rpm database was concerned). I 
> assumed that 
> > all dependencies were sorted out through the install in the first 
> > place.
> 
> OK, that helps. I still have a lingering concern that, given 
> the facts above, there may be other integrity issues in the 
> RPM database, if not elsewhere.
> 
> >From reading the WB web site FAQ's
> (http://www.whiteboxlinux.org/faq.html) , it appears that 
> they are using up2date/yum for system updates. Depending upon 
> the version in use, there have been issues especially with 
> up2date (hangs, incomplete updates,
> etc.) which could result in other problems. I use yum via the 
> console here (under FC2), though I note that a GUI version of 
> yum has been created, including replacing the RHN/up2date 
> system tray alert icon.
> 
> A thought relative to this specifically:
> 
> If there is or may be an integrity problem related to the rpm 
> database, you should review the information here:
> 
> http://www.rpm.org/hintskinks/repairdb/
> 
> which provides instructions on repairing the database. Note 
> the important caveats regarding backups, etc.
> 
> The two key steps there are to remove any residual lock files 
> using (as
> root):
> 
> rm -f /var/lib/rpm/__*
> 
> and then rebuilding the rpm database using (also as root):
> 
> rpm -vv --rebuilddb
> 
> I think that there needs to be some level of comfort that 
> this basic foundation for the system is intact and correct.
> 
> > I only mentioned RH9 to show that I had some familiarity with the 
> > RedHat policy of separating out the 'includes' etc into a separate 
> > developer package.
> > 
> > Once all this had been sorted out, I was then left with a 
> compilation 
> > error which pointed to a missing dependency or similar, 
> which was not 
> > due to missing developer packages, but, as you and Prof Ripley 
> > correctly point out, from the R installation itself. Having 
> grown fat 
> > and lazy on using R under the MS Windows environment, I was 
> struggling 
> > to identify the precise nature of this remaining problem.
> > 
> > As regards the R installation, I did this from the RH9 binary for 
> > version 1.9.1, as I did not think that the Fedora Core 2 
> binary would 
> > be appropriate here. Perhaps I should now compile from the source 
> > instead?
> 
> I would not use the FC2 RPM, since FC2 has many underlying 
> changes not the least of which includes the use of the 2.6 
> kernel series and the change from XFree86 to x.org. Both 
> changes resulted in significant havoc during the FC2 testing 
> phases and there was at least one issue here with R due to 
> the change in X.
> 
> According to the WB FAQs:
> 
> "If you cannot find a package built specifically for RHEL3 or 
> WBEL3 you can try a package for RH9 since many of the 
> packages in RHEL3 are the exact same packages as appeared in RH9."
> 
> Thus, it would seem reasonable to use the RH9 RPM that Martyn 
> has created. An alternative would certainly be to compile R 
> from the source tarball.
> 
> In either case, I would remove the current installation of R 
> and after achieving a level of comfort that your RPM database 
> is OK, reinstall R using one of the above methods. Pay close 
> attention to any output during the installation process, 
> noting any error or warning messages that may occur.
> 
> If you go the RPM route, be sure that the MD5SUM of the RPM 
> file matches the value that Martyn has listed on CRAN to 
> ensure that the file has been downloaded in an intact fashion.
> 
> These are my thoughts at this point. You need to get to a 
> point where the underlying system is stable and intact, then 
> get R to the same state before attempting to install new packages.
> 
> HTH,
> 
> Marc
> 
>From unpacking the tarball and running ./configure in the R source
directory, I obtain the fact that crti.o is needed by ld.so and was not
found. This file is not present on the system. This file, along with crtn.o
is usually installed by the gnu libc packages, I believe. However, I know
that not all *nix distributions include these files among their packages.
>From a web search, I have not been able to ascertain whether this lack of a
crti.o is due to there not being one in the distribution, or to another
incomplete package install.

So, I did a completely fresh installation of WhiteBox, followed by R built
from source, checked that it ran and then installed the R packages. Only
then did I run up2date. At least crti.o and crtn.o are still there this
time, along with the XFree86 includes.....

A bit of a cautionary tale, all in all. 

Thanks for all the help and support.

Regards

M



From rossini at blindglobe.net  Tue Aug 10 16:05:48 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Tue, 10 Aug 2004 07:05:48 -0700
Subject: AW: [R] built-in Sweave-like documentation in R-2.x
In-Reply-To: <D15343265276D31197BC00A024A6C110C7933A@EXS_BDC> (Valery
	Khamenia's message of "Tue, 10 Aug 2004 09:53:53 +0200")
References: <D15343265276D31197BC00A024A6C110C7933A@EXS_BDC>
Message-ID: <85hdrbks03.fsf@servant.blindglobe.net>

"Khamenia, Valery" <V.Khamenia at biovision-discovery.de> writes:

>
> I think just smarter C-c C-r would be kind of trade-off here.
>
> hm, maybe there are some other voices here similar to mine?
> It would be easier to discuss the subj. 

Within ESS, you've got the ess-thread-eval (similar to
ess-chunk-eval), so the guts for cross-chunk evals are there, so the
next part would be as you say, making C-c C-r SWeave-aware.

An simpler alternative to code would be to allow one to
eval-chunk-and-step, stepping through chunks, similar to C-c C-n for
stepping through lines.  Would this solve the basic problem?  3 x (2
or 3 keystrokes) for 3 chunks.

I can't imagine an evaluation which would cross chunks but use only
part of chunks (this suggests bad programming design to me), but
perhaps you (or others) have an example of when this functionality
would be useful?  (i.e. actual regions to eval which cross code-chunk
boundaries but contain 1 or 2 incomplete code-chunks?).

best,
-tony





-- 
Anthony Rossini			    Research Associate Professor
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From deepayan at stat.wisc.edu  Tue Aug 10 16:19:07 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Tue, 10 Aug 2004 09:19:07 -0500
Subject: [R] Re: date axes and formats  in levelplot
In-Reply-To: <C4178DC99E08604EA5E2BDB989F09380025D0C78@extas2-hba.tas.csiro.au>
References: <C4178DC99E08604EA5E2BDB989F09380025D0C78@extas2-hba.tas.csiro.au>
Message-ID: <200408100919.07704.deepayan@stat.wisc.edu>

On Tuesday 10 August 2004 01:34, Toby.Patterson at csiro.au wrote:
> Hi all (and particularly Deepayan),
>
> A while back Deepayan helped me with the query in the text below
> (thanks again). Specifically it was about changing the way that dates
> plotted on the axes of lattice plots.
>
> While this works using xyplot, everything comes apart when I use
> levelplot. The axis labels on the date axis are shown as the integer
> representation of the date (number of seconds since the origin I
> assume). I guess that the POSIX dates are getting coerced into
> numeric objects somewhere along the way and that there is no easy fix
> for this.

You are right. At first glance, it appears that I have been negligent in 
properly updating the default prepanel function for levelplot to handle 
DateTime objects. For now, you could use xyplot's default instead: 

levelplot(z ~ x * y, 
          prepanel = lattice:::prepanel.default.xyplot)

Add a 'scales = list(axs = "i")' to get a better looking result (with 
the bordering rectangles partially clipped).

Deepayan



From MSchwartz at MedAnalytics.com  Tue Aug 10 16:30:21 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Tue, 10 Aug 2004 09:30:21 -0500
Subject: [R] R packages install problems linux - X not found (WhiteBoxEL 3)
In-Reply-To: <014701c47edc$26274830$6600a8c0@mainman>
References: <014701c47edc$26274830$6600a8c0@mainman>
Message-ID: <1092148220.7824.83.camel@localhost.localdomain>

On Tue, 2004-08-10 at 08:15, Dr Mike Waters wrote:

<snip> 

> >From unpacking the tarball and running ./configure in the R source
> directory, I obtain the fact that crti.o is needed by ld.so and was not
> found. This file is not present on the system. This file, along with crtn.o
> is usually installed by the gnu libc packages, I believe. However, I know
> that not all *nix distributions include these files among their packages.
> >From a web search, I have not been able to ascertain whether this lack of a
> crti.o is due to there not being one in the distribution, or to another
> incomplete package install.
> 
> So, I did a completely fresh installation of WhiteBox, followed by R built
> from source, checked that it ran and then installed the R packages. Only
> then did I run up2date. At least crti.o and crtn.o are still there this
> time, along with the XFree86 includes.....
> 
> A bit of a cautionary tale, all in all. 
> 
> Thanks for all the help and support.
> 
> Regards
> 
> M


Mike,

>From my FC2 system:

$ rpm -qf /usr/lib/crti.o
glibc-devel-2.3.3-27

$ rpm -qf /usr/lib/crtn.o
glibc-devel-2.3.3-27

So, you are correct relative to the source of these two files. A follow
up question might be, did you include the "devel" packages during your
initial install? If not, that would explain the lack of these files. if
you did, then it would add another data point to support the notion that
your system was, to some level, compromised and a clean install was
probably needed, rather than just trying to re-create the RPM database.

Glad that you are up and running at this point. Given Martyn's follow up
messages, it looks like there may be an issue with the RH9 RPM, so for
the time being using the source tarball would be appropriate.

Best regards,

Marc



From klaus.juenemann at epigenomics.com  Tue Aug 10 16:53:49 2004
From: klaus.juenemann at epigenomics.com (Klaus Juenemann)
Date: Tue, 10 Aug 2004 16:53:49 +0200
Subject: [R] Approaches to using RUnit
Message-ID: <4118E17D.4030204@epigenomics.com>

Hi Seth,

first of all note that it was a deliberate decision to leave it up to 
the RUnit user to load all the functions and packages to be tested 
because loading and sourcing is always very site-specific. RUnit just 
assumes that all functionality to be tested is already present in the R 
session.

If you don't organize your code into packages but source individual R 
files your approach to source the code at the beginning of a test file 
looks the right thing to do.

We mainly use packages and the code we use to test packages A and B, 
say, looks like this:

library("A")
library("B")
testsuite.A <- defineTestSuite("A", "location_of_package_A/tests")
testsuite.B <- defineTestSuite("B", "location_of_package_B/tests")
testresult <- runTestSuite(list(testsuite.A, testsuite.B))
printHTMLProtocol(testresult, "location_of_testProtocol")

We use the tests subdirectory of a package to store our RUnit tests even 
though this is not really according to R conventions.

The nice thing is that this code can be executed in batch mode from a 
shell script. This script is executed nightly (and before starting R 
checks out and installs the packages from CVS). In this way, we know the 
test status of our code every morning.


Hope this helps,
Klaus










-- 
Klaus Juenemann                   Software Engineer/ Biostatistician
Epigenomics AG    Kleine Praesidentenstr. 1    10178 Berlin, Germany
phone:+49-30-24345-393                          fax:+49-30-24345-555
http://www.epigenomics.com           klaus.juenemann at epigenomics.com



From pallier at lscp.ehess.fr  Tue Aug 10 17:55:20 2004
From: pallier at lscp.ehess.fr (Christophe Pallier)
Date: Tue, 10 Aug 2004 17:55:20 +0200
Subject: [R] Re: Enduring LME
 =?windows-1252?q?confusion=85_or_Psychologists_a?=
 =?windows-1252?q?nd_Mixed-Effects?=
In-Reply-To: <200408101005.i7AA4seH031463@hypatia.math.ethz.ch>
References: <200408101005.i7AA4seH031463@hypatia.math.ethz.ch>
Message-ID: <4118EFE8.2010908@lscp.ehess.fr>

Hello,

>>
>> Suppose I have a typical psychological experiment that is a 
>> within-subjects design with multiple crossed variables and a 
>> continuous response variable. Subjects are considered a random 
>> effect. So I could model
>> > aov1 <- aov(resp~fact1*fact2+Error(subj/(fact1*fact2))
>>
>> However, this only holds for orthogonal designs with equal numbers of 
>> observation and no missing values. These assumptions are easily 
>> violated so I seek refuge in fitting a mixed-effects model with the 
>> nlme library.
>

I suppose that you have, for each subject, enough observations to 
compute his/her average response for each combination of factor1 and 
factor2, no?
If this is the case, you can perform the analysis with the above formula 
on the data obtained by 'aggregate(resp,list(subj,fact1,fact2),mean)'.

This is an analysis with only *within-subject* factors and there 
*cannot* be a problem of unequal number of observation when you have 
only within-subject factors (supposing you have at least one 
observations for each subject in each condition).

I believe the problem with unequal number of observations only  occurs 
when you have at least two crossed *between-subject* (group) variables.

Let's imagine you have two binary group factors (A and B) yielding four 
subgroups of subjects, and for some reason, you do have the same number 
of observations in each subgroup,
Then there are several ways of defining the main effects of A and B.

In many cases, the most reasonable definition of the main effect of A is 
to take the average of A in B1 and in B2 (thus ignoring the number of 
observations, or weithting equally the four subgroups).
To test the null hypothesis of no difference in A when all groups are 
equally weighted, one common approach in psychology is to pretend that 
the number of observation is each group is equal to the harmonic mean of 
the number of observations in each subgroups. The sums of square thud 
obtained can be compared with the error sum of square in the standard 
anova to form an F-test.
This is called the "unweighted" approach.

This can easily be done 'by hand' in R, but there is another approach:

You get equivalent statistics as in the unweighted anova when you use so 
called 'type III' sums of square (I read this in Howell, 1987 
'Statistical methods in psychology',
and in John Fox book 'An R and S-plus companion to appied regression, p. 
140).

It is possible to get type III sums of square using John Fox 'car' library.

library(car)
contrasts(A)=contr.sum
contrasts(B)=contr.sum
Anova(aov(resp~A*B),type='III')



You can compute the equally weighted cell means defining the effect of A 
with, say:

with(aggregate(resp,list(a=a,b=b),mean),tapply(x,a,mean))


I have seen some people advise against using 'type III' sums of square 
but I do not know their rationale. The important thing, it seems to me, 
is to know
which null hypothesis is  tested in a given test. If indeed the  type 
III sums of square test the effect on equally weighted means, they seem 
okay to me
(when this is indeed the hypothesis I want to test). 

Sorry for not answering any of your questions about the use of 'lme' (I 
hope others will do), but I feel that 'lme' is not needed in the context 
of unequal cell frequencies.
(I am happy to be corrected if I am wrong). It seems to me that 'lme' is 
useful when some assumptions of standard anova are violate (e.g. with 
repeated measurements when the assumption of sphericity is false), or 
when you have several random factors.

Christophe Pallier
http://www.pallier.org



From drf5n at maplepark.com  Tue Aug 10 17:28:16 2004
From: drf5n at maplepark.com (David Forrest)
Date: Tue, 10 Aug 2004 10:28:16 -0500 (CDT)
Subject: AW: [R] built-in Sweave-like documentation in R-2.x
In-Reply-To: <85hdrbks03.fsf@servant.blindglobe.net>
References: <D15343265276D31197BC00A024A6C110C7933A@EXS_BDC>
	<85hdrbks03.fsf@servant.blindglobe.net>
Message-ID: <Pine.LNX.4.58.0408100932420.23924@maplepark.com>

On Tue, 10 Aug 2004, A.J. Rossini wrote:

> "Khamenia, Valery" <V.Khamenia at biovision-discovery.de> writes:
>
> >
> > I think just smarter C-c C-r would be kind of trade-off here.
> >
> > hm, maybe there are some other voices here similar to mine?
> > It would be easier to discuss the subj.
>
> Within ESS, you've got the ess-thread-eval (similar to
> ess-chunk-eval), so the guts for cross-chunk evals are there, so the
> next part would be as you say, making C-c C-r SWeave-aware.
>
> An simpler alternative to code would be to allow one to
> eval-chunk-and-step, stepping through chunks, similar to C-c C-n for
> stepping through lines.  Would this solve the basic problem?  3 x (2
> or 3 keystrokes) for 3 chunks.

I'd really like something like that -- I'll often be using repeated C-c
C-n and accidently hit one on the end of the chunk and end up in
TeX-normal mode due to AUCTeX, requiring a save and revert.
Functionality like (M-n c M-x ess-eval-chunk) bound to a key would be
great.  But with my elisp skills, I can't quite make this work:

(defun ess-eval-chunk-and-step (vis)
  "Tangle the current chunk, send to the ESS process, and move on
to the next chunk.

Arg has same meaning as for `ess-eval-region'."
  (interactive "P")
  (ess-eval-chunk vis)
  (noweb-next-code-chunk)
)


Dave
-- 
 Dave Forrest
 drf at vims.edu                                    (804)684-7900w
 drf5n at maplepark.com                             (804)642-0662h
                                   http://maplepark.com/~drf5n/



From Camarda at demogr.mpg.de  Tue Aug 10 19:02:40 2004
From: Camarda at demogr.mpg.de (Camarda, Carlo Giovanni)
Date: Tue, 10 Aug 2004 19:02:40 +0200
Subject: [R] persp, array and colors
Message-ID: <3699CDBC4ED5D511BE6400306E1C0D8106B4101A@hermes.demogr.mpg.de>

Dear R-users,
I'd like to plot a three-dimensional surface and at the meantime I'm using
an array. I would like to have the values of my first matrix in the heights
of the plot and the colors of the single facet taking into account the
second matrix.
I hope that the next code will help all of you to understand better my
issue, 
Thanks in advance, Giancarlo
############################
## creating my array
m1 <- runif(30,min=1, max=10)
m2 <- c(rep(0,12), rep(1,5), rep(0,3), rep(1,30-12-5-3))
mm <- array(c(m1,m2), dim=c(6,5,2))

## colors
colo <- c("red", "blue")

## axis
x  <- 1:6
y  <- 1:5
z  <- mm[,,1]
z1 <- mm[,,2]

## surface with heights and colors 
## related to the first matrix (no good)
persp(x, y, z, theta = 30, phi = 30, expand = 0.5, col = colo,
        ltheta = 120, ticktype = "detailed",
        xlab = "X", ylab = "Y", zlab = "values" )

## surface with heights and colors 
## related to the second matrix (no good as well)
persp(x, y, z1, theta = 30, phi = 30, expand = 0.5, col = colo,
        ltheta = 120, ticktype = "detailed",
        xlab = "X", ylab = "Y", zlab = "values" )
############################



+++++
This mail has been sent through the MPI for Demographic Rese...{{dropped}}



From msvika at mscc.huji.ac.il  Tue Aug 10 20:49:40 2004
From: msvika at mscc.huji.ac.il (Victoria Landsman)
Date: Tue, 10 Aug 2004 20:49:40 +0200
Subject: [R] Question about mle function 
Message-ID: <007701c47f0a$cb4c6b00$8600a8c0@home2>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040810/76c1fc1d/attachment.pl

From Edward.Dick at noaa.gov  Tue Aug 10 20:32:52 2004
From: Edward.Dick at noaa.gov (Edward Dick)
Date: Tue, 10 Aug 2004 11:32:52 -0700
Subject: [R] summary output for inverse Gaussian GLM
Message-ID: <411914D4.7090407@noaa.gov>

I'm getting inconsistent output about the link function from the
summary() command when fitting an inverse Gaussian GLM:


 > summary(glm(ig.formula, family=inverse.gaussian(link = "log"),
+ data=mydata, start=start.vals))$call
glm(formula = ig.formula, family = inverse.gaussian(link = "log"),
     data = mydata, start = start.vals)
 > summary(glm(ig.formula, family=inverse.gaussian(link = "log"),
+ data=mydata, start=start.vals))$family

Family: inverse.gaussian
Link function: 1/mu^2


Has anyone else run into this problem?
I'm running v1.9.1 in Windows.

    E.J.



From tlumley at u.washington.edu  Tue Aug 10 21:29:15 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 10 Aug 2004 12:29:15 -0700 (PDT)
Subject: [R] summary output for inverse Gaussian GLM
In-Reply-To: <411914D4.7090407@noaa.gov>
References: <411914D4.7090407@noaa.gov>
Message-ID: <Pine.A41.4.58.0408101225490.60326@homer05.u.washington.edu>

On Tue, 10 Aug 2004, Edward Dick wrote:

> I'm getting inconsistent output about the link function from the
> summary() command when fitting an inverse Gaussian GLM:
>

Yes, it's a bug in the name, not in the result, fortunately.  The
inverse.gaussian family object always has the name of the link set to
1/m^2, regardless of the actual link function.

	-thomas



From f.duan at yale.edu  Tue Aug 10 21:55:45 2004
From: f.duan at yale.edu (F Duan)
Date: Tue, 10 Aug 2004 15:55:45 -0400
Subject: [R] How to import specific column(s) using "read.table"?
In-Reply-To: <loom.20040810T080806-990@post.gmane.org>
References: <1092083520.4117df4084926@webmail.med.yale.edu>
	<loom.20040810T070123-553@post.gmane.org>
	<loom.20040810T080806-990@post.gmane.org>
Message-ID: <1092167745.41192841a95b3@webmail.med.yale.edu>

Thanks a lot. 

Your way works perfect. And one more tiny question related to your codes:

My data file has many columns to be omitted (suppose the first 20 ones), but I 
found "scan(myfile, what=list(rep(NULL, 20), rep(0, 5))" doesn't work. I had to 
to type "NULL" 20 times and "0" five times in the "list(...)". 

But anyway, it works and saves a lot of memory for me. Thank you again.

Frank 


Quoting Gabor Grothendieck <ggrothendieck at myway.com>:

> Gabor Grothendieck <ggrothendieck <at> myway.com> writes:
> 
> : 
> : F Duan <f.duan <at> yale.edu> writes:
> : 
> : > I have a very big tab-delim txt file with header and I only want to
> import 
> : > several columns into R. I checked the options for ??"read.table??" and only
> 
> : 
> : Try using scan with the what=list(...) and flush=TRUE arguments.  
> : For example, if your data looks like this:
> : 
> : 1 2 3 4 
> : 5 6 7 8 
> : 9 10 11 12
> : 13 14 15 16
> : 
> : then you could read columns 2 and 4 into a list with:
> : 
> 
> oops. That should be 1 and 3.
> 
> :    scan(myfile, what = list(0, NULL, 0), flush = TRUE)
> : 
> : or read in and convert to a data frame via:
> : 
> :    do.call("cbind", scan(myfile, what = list(0, NULL, 0), flush = TRUE))
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
>



From andy_liaw at merck.com  Tue Aug 10 21:58:10 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 10 Aug 2004 15:58:10 -0400
Subject: [R] How to import specific column(s) using "read.table"?
Message-ID: <3A822319EB35174CA3714066D590DCD504AF81FC@usrymx25.merck.com>

Use as.list.

Andy

> From: F Duan
> 
> Thanks a lot. 
> 
> Your way works perfect. And one more tiny question related to 
> your codes:
> 
> My data file has many columns to be omitted (suppose the 
> first 20 ones), but I 
> found "scan(myfile, what=list(rep(NULL, 20), rep(0, 5))" 
> doesn't work. I had to 
> to type "NULL" 20 times and "0" five times in the "list(...)". 
> 
> But anyway, it works and saves a lot of memory for me. Thank 
> you again.
> 
> Frank 
> 
> 
> Quoting Gabor Grothendieck <ggrothendieck at myway.com>:
> 
> > Gabor Grothendieck <ggrothendieck <at> myway.com> writes:
> > 
> > : 
> > : F Duan <f.duan <at> yale.edu> writes:
> > : 
> > : > I have a very big tab-delim txt file with header and I 
> only want to
> > import 
> > : > several columns into R. I checked the options for 
> ??"read.table??" and only
> > 
> > : 
> > : Try using scan with the what=list(...) and flush=TRUE arguments.  
> > : For example, if your data looks like this:
> > : 
> > : 1 2 3 4 
> > : 5 6 7 8 
> > : 9 10 11 12
> > : 13 14 15 16
> > : 
> > : then you could read columns 2 and 4 into a list with:
> > : 
> > 
> > oops. That should be 1 and 3.
> > 
> > :    scan(myfile, what = list(0, NULL, 0), flush = TRUE)
> > : 
> > : or read in and convert to a data frame via:
> > : 
> > :    do.call("cbind", scan(myfile, what = list(0, NULL, 0), 
> flush = TRUE))
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From juderyan at tcindex.com  Tue Aug 10 21:56:38 2004
From: juderyan at tcindex.com (Jude Ryan)
Date: Tue, 10 Aug 2004 15:56:38 -0400
Subject: [R] Error message in function mars() in package mda
Message-ID: <41192876.2040503@tcindex.com>

Hi,

I am using function mars() in package mda to find knots in a whole bunch 
of predictor variables. I hope to be able to replicate all or some of 
the basis functions that the MARS software from Salford Systems creates. 
When I ran mars() on a small dataset, I was able to get the knots. 
However, when I tried running mars() on a larger dataset (145 predictor 
variables), for a different project, I get the following error message:

 > fit1 <- mars(disney2[,-146], disney2[,146])
Error in mars(disney2[, -146], disney2[, 146]) :
        NA/NaN/Inf in foreign function call (arg 5)
In addition: Warning messages:
1: NAs introduced by coercion
2: NAs introduced by coercion
 >

Does arg 5 refer to the 5th column in my dataset? This seems to be a 
data problem, is this correct?

Are there any other functions in R that will give me the knots for a set 
of predictor variables?

Any help is greatly appreciated.

Thanks,

Jude



From tplate at blackmesacapital.com  Tue Aug 10 22:05:06 2004
From: tplate at blackmesacapital.com (Tony Plate)
Date: Tue, 10 Aug 2004 14:05:06 -0600
Subject: [R] How to import specific column(s) using "read.table"?
In-Reply-To: <1092167745.41192841a95b3@webmail.med.yale.edu>
References: <1092083520.4117df4084926@webmail.med.yale.edu>
	<loom.20040810T070123-553@post.gmane.org>
	<loom.20040810T080806-990@post.gmane.org>
	<1092167745.41192841a95b3@webmail.med.yale.edu>
Message-ID: <6.1.0.6.2.20040810140318.0cf56750@mailhost.blackmesacapital.com>

At Tuesday 01:55 PM 8/10/2004, F Duan wrote:
>Thanks a lot.
>
>Your way works perfect. And one more tiny question related to your codes:
>
>My data file has many columns to be omitted (suppose the first 20 ones), 
>but I
>found "scan(myfile, what=list(rep(NULL, 20), rep(0, 5))" doesn't work. I 
>had to
>to type "NULL" 20 times and "0" five times in the "list(...)".

That's because rep(NULL, 20) returns a single NULL -- it's not obvious what 
else it could sensibly return.  What you need to do is replicate 20 times a 
list containing NULL (and a list containing NULL is quite a different 
object to NULL).  E.g.:

 > rep(NULL, 20)
NULL
 > c(rep(list(NULL), 3), rep(list(0), 2))
[[1]]:
NULL

[[2]]:
NULL

[[3]]:
NULL

[[4]]:
[1] 0

[[5]]:
[1] 0

 >

Tony Plate



>But anyway, it works and saves a lot of memory for me. Thank you again.
>
>Frank
>
>
>Quoting Gabor Grothendieck <ggrothendieck at myway.com>:
>
> > Gabor Grothendieck <ggrothendieck <at> myway.com> writes:
> >
> > :
> > : F Duan <f.duan <at> yale.edu> writes:
> > :
> > : > I have a very big tab-delim txt file with header and I only want to
> > import
> > : > several columns into R. I checked the options for ??"read.table??" 
> and only
> >
> > :
> > : Try using scan with the what=list(...) and flush=TRUE arguments.
> > : For example, if your data looks like this:
> > :
> > : 1 2 3 4
> > : 5 6 7 8
> > : 9 10 11 12
> > : 13 14 15 16
> > :
> > : then you could read columns 2 and 4 into a list with:
> > :
> >
> > oops. That should be 1 and 3.
> >
> > :    scan(myfile, what = list(0, NULL, 0), flush = TRUE)
> > :
> > : or read in and convert to a data frame via:
> > :
> > :    do.call("cbind", scan(myfile, what = list(0, NULL, 0), flush = TRUE))
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Tue Aug 10 22:58:04 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 10 Aug 2004 22:58:04 +0200
Subject: [R] Question about mle function
In-Reply-To: <007701c47f0a$cb4c6b00$8600a8c0@home2>
References: <007701c47f0a$cb4c6b00$8600a8c0@home2>
Message-ID: <x27js6sobn.fsf@biostat.ku.dk>

"Victoria Landsman" <msvika at mscc.huji.ac.il> writes:

> Dear all, 
> I'd like to find the mle esttimates using the mle function  
> mle(negloglik, start = list(), fixed=list(), method="..."). 
> I am using the L-BGFS-B method and I don't supply the gradient function. Is there a way to print the gradients found at the solution value? 

No. The "details" slot in an mle object is simply the return value
from optim(), and that doesn't provide the gradient. Might be an idea
to change that, though, since the gradient is obviously not zero where
the box constraints are active.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Tue Aug 10 23:16:50 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 10 Aug 2004 22:16:50 +0100 (BST)
Subject: [R] Question about mle function
In-Reply-To: <x27js6sobn.fsf@biostat.ku.dk>
Message-ID: <Pine.LNX.4.44.0408102208420.530-100000@gannet.stats>

On 10 Aug 2004, Peter Dalgaard wrote:

> "Victoria Landsman" <msvika at mscc.huji.ac.il> writes:
> 
> > Dear all, 
> > I'd like to find the mle esttimates using the mle function  
> > mle(negloglik, start = list(), fixed=list(), method="..."). 
> > I am using the L-BGFS-B method and I don't supply the gradient
> > function. Is there a way to print the gradients found at the solution
> > value?
> 
> No. The "details" slot in an mle object is simply the return value
> from optim(), and that doesn't provide the gradient. Might be an idea
> to change that, though, since the gradient is obviously not zero where
> the box constraints are active.

Not so easy, as optim does not return it and indeed the gradient is not 
known at R level (it is computed in the C code, for L-BFGS-B rather deep 
inside at that).  It may not even have been computed at the final 
solution.  We would need something similar to do_optimHess, or even to 
make use of that code as it does evaluate the gradient at the solution.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From lauraholt_983 at hotmail.com  Tue Aug 10 23:39:58 2004
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Tue, 10 Aug 2004 16:39:58 -0500
Subject: [R] hist and normal curve
Message-ID: <BAY12-F33v7GOGeJBjM0004dbc6@hotmail.com>

Hi R People:

I have a data set and I want to use the hist command and produce a 
histogram.
Then I want to superimpose a normal curve over the histogram.

Is there a simple way to do this, please?

R version 1.9.1 Windows.

thanks in advance,
Sincerely,
Laura Holt
mailto: lauraholt_983 at hotmail.com






From s.gatehouse at unsw.edu.au  Tue Aug 10 23:51:34 2004
From: s.gatehouse at unsw.edu.au (simon gatehouse)
Date: Wed, 11 Aug 2004 07:51:34 +1000
Subject: [R] Polar decomposition of a rectangular matrix
Message-ID: <000001c47f24$3456f200$85a9ab95@bees.unsw.edu.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040811/c31ba7cd/attachment.pl

From sundar.dorai-raj at PDF.COM  Tue Aug 10 23:51:37 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Tue, 10 Aug 2004 14:51:37 -0700
Subject: [R] hist and normal curve
In-Reply-To: <BAY12-F33v7GOGeJBjM0004dbc6@hotmail.com>
References: <BAY12-F33v7GOGeJBjM0004dbc6@hotmail.com>
Message-ID: <41194369.9000309@pdf.com>



Laura Holt wrote:

> Hi R People:
> 
> I have a data set and I want to use the hist command and produce a 
> histogram.
> Then I want to superimpose a normal curve over the histogram.
> 
> Is there a simple way to do this, please?
> 
> R version 1.9.1 Windows.
> 
> thanks in advance,
> Sincerely,
> Laura Holt
> mailto: lauraholt_983 at hotmail.com
> 
> 

This has been answered numerous times. See, for example,

http://finzi.psych.upenn.edu/R/Rhelp01/archive/5029.html

I found this using "superimpose histogram normal density" on the "R Site 
Search" page.

--sundar



From christian_mora at vtr.net  Wed Aug 11 00:44:56 2004
From: christian_mora at vtr.net (Christian Mora)
Date: Tue, 10 Aug 2004 18:44:56 -0400
Subject: [R] intersect two files
Message-ID: <000001c47f2b$a90a6ad0$2f5b68c8@CPQ26719243321>

Hi all;
Im working with two datasets in R, say data1 and data2. Both datasets
are composed of several rows and columns (dataframe) and some of the
rows are identical in both datasets. Im wondering if there is any way to
remove from one set, say data1, the rows that are identical in the other
set, say data2, using R?
Thanks for any hint in advance
Christian



From andy_liaw at merck.com  Wed Aug 11 01:01:26 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 10 Aug 2004 19:01:26 -0400
Subject: [R] intersect two files
Message-ID: <3A822319EB35174CA3714066D590DCD504AF8200@usrymx25.merck.com>

You have not given enough info.  Do the data sets have the same columns?  If
not, you need to tell us more about how you can tell whether one row of a
data frame is `identical' to some row of another.

Assuming the columns are the same between the two, the basic idea is to
combine all columns into a single vector for each, then check which elements
of one is in the other.  Something like (code untested!):

id1 <- do.call("paste", c(data1, sep=":")
id2 <- do.call("paste", c(data2, sep=":")
## Rows of data1 that are in data2:
r1 <- which(id1 %in% id2)

## Remove:
data1.reduced <- data1[-r1,]

Andy


> From: Christian Mora
> 
> Hi all;
> Im working with two datasets in R, say data1 and data2. Both datasets
> are composed of several rows and columns (dataframe) and some of the
> rows are identical in both datasets. Im wondering if there is 
> any way to
> remove from one set, say data1, the rows that are identical 
> in the other
> set, say data2, using R?
> Thanks for any hint in advance
> Christian
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ramasamy at cancer.org.uk  Wed Aug 11 01:16:26 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Wed, 11 Aug 2004 00:16:26 +0100
Subject: [R] intersect two files
In-Reply-To: <000001c47f2b$a90a6ad0$2f5b68c8@CPQ26719243321>
References: <000001c47f2b$a90a6ad0$2f5b68c8@CPQ26719243321>
Message-ID: <1092179739.3075.11.camel@localhost.localdomain>

In short, merge with all=FALSE followed by removal of redundant columns might do the trick. 
If rownames serve as common key, use the argument by=0.

See http://tolstoy.newcastle.edu.au/R/help/04/07/1250.html and many
other hits on http://maths.newcastle.edu.au/~rking/R/


On Tue, 2004-08-10 at 23:44, Christian Mora wrote:
> Hi all;
> Im working with two datasets in R, say data1 and data2. Both datasets
> are composed of several rows and columns (dataframe) and some of the
> rows are identical in both datasets. Im wondering if there is any way to
> remove from one set, say data1, the rows that are identical in the other
> set, say data2, using R?
> Thanks for any hint in advance
> Christian
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From lauraholt_983 at hotmail.com  Wed Aug 11 03:59:35 2004
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Tue, 10 Aug 2004 20:59:35 -0500
Subject: [R] str and Surv objects
Message-ID: <BAY12-F4I76rOJhKPc900050321@hotmail.com>

Dear R People:

I used the "Surv" function to produce the following object:
>a <- Surv(1:4,2:5,c(0,1,1,0)) a
[1] (1,2+] (2,3 ] (3,4 ] (4,5+]
>str(a)
Error in "[.Surv"(object, 1:ile) : subscript out of bounds
>

Why does str(a) give an error, please?  Or did I do something wrong?

Thanks in advance.
R Version 1.9.1 Windows
Sincerely,
Laura Holt
mailto: lauraholt_983 at hotmail.com





From andy_liaw at merck.com  Wed Aug 11 04:19:23 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 10 Aug 2004 22:19:23 -0400
Subject: [R] str and Surv objects
Message-ID: <3A822319EB35174CA3714066D590DCD504AF8202@usrymx25.merck.com>

Buried in str.default is a line:

    iv.len <- round(2.5 * v.len)

which seems to be the culprit.  v.len is 4, which seems correct for the data
given.  Why multiply by 2.5?

Andy

> From: Laura Holt
> 
> Dear R People:
> 
> I used the "Surv" function to produce the following object:
> >a <- Surv(1:4,2:5,c(0,1,1,0)) a
> [1] (1,2+] (2,3 ] (3,4 ] (4,5+]
> >str(a)
> Error in "[.Surv"(object, 1:ile) : subscript out of bounds
> >
> 
> Why does str(a) give an error, please?  Or did I do something wrong?
> 
> Thanks in advance.
> R Version 1.9.1 Windows
> Sincerely,
> Laura Holt
> mailto: lauraholt_983 at hotmail.com
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From gwang at nrel.colostate.edu  Wed Aug 11 04:22:48 2004
From: gwang at nrel.colostate.edu (Guiming Wang)
Date: Wed, 11 Aug 2004 02:22:48 -0000
Subject: [R] Observation error in arima
Message-ID: <005f01c46385$860dad60$6b685281@nrelwang>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040811/b3a70f13/attachment.pl

From michael.waters at dtn.ntl.com  Wed Aug 11 05:56:56 2004
From: michael.waters at dtn.ntl.com (Dr Mike Waters)
Date: Wed, 11 Aug 2004 04:56:56 +0100
Subject: [R] R packages install problems linux - X not found(WhiteBoxEL 3)
In-Reply-To: <1092148220.7824.83.camel@localhost.localdomain>
Message-ID: <018401c47f57$3f060d20$6600a8c0@mainman>

Marc,

Yes - the glibc-devel package was shown to be installed using rpm -qa. It is
also one of the packages upgraded by up2date from the original version
supplied with the WhiteBox distribution. I concluded that there were
probably more such improperly/incompletely upgraded packages and cut my
losses. Everything seems to be fine second time around. I must have been
unlucky.

Regards

Mike

> -----Original Message-----
> From: Marc Schwartz [mailto:MSchwartz at MedAnalytics.com] 
> Sent: 10 August 2004 15:30
> To: Dr Mike Waters
> Cc: R-Help
> Subject: RE: [R] R packages install problems linux - X not 
> found(WhiteBoxEL 3)
> 
> 
> On Tue, 2004-08-10 at 08:15, Dr Mike Waters wrote:
> 
> <snip> 
> 
> > >From unpacking the tarball and running ./configure in the R source
> > directory, I obtain the fact that crti.o is needed by ld.so and was 
> > not found. This file is not present on the system. This file, along 
> > with crtn.o is usually installed by the gnu libc packages, 
> I believe. 
> > However, I know that not all *nix distributions include these files 
> > among their packages.
> > >From a web search, I have not been able to ascertain whether this 
> > >lack of a
> > crti.o is due to there not being one in the distribution, or to 
> > another incomplete package install.
> > 
> > So, I did a completely fresh installation of WhiteBox, 
> followed by R 
> > built from source, checked that it ran and then installed the R 
> > packages. Only then did I run up2date. At least crti.o and 
> crtn.o are 
> > still there this time, along with the XFree86 includes.....
> > 
> > A bit of a cautionary tale, all in all.
> > 
> > Thanks for all the help and support.
> > 
> > Regards
> > 
> > M
> 
> 
> Mike,
> 
> >From my FC2 system:
> 
> $ rpm -qf /usr/lib/crti.o
> glibc-devel-2.3.3-27
> 
> $ rpm -qf /usr/lib/crtn.o
> glibc-devel-2.3.3-27
> 
> So, you are correct relative to the source of these two 
> files. A follow up question might be, did you include the 
> "devel" packages during your initial install? If not, that 
> would explain the lack of these files. if you did, then it 
> would add another data point to support the notion that your 
> system was, to some level, compromised and a clean install 
> was probably needed, rather than just trying to re-create the 
> RPM database.
> 
> Glad that you are up and running at this point. Given 
> Martyn's follow up messages, it looks like there may be an 
> issue with the RH9 RPM, so for the time being using the 
> source tarball would be appropriate.
> 
> Best regards,
> 
> Marc
> 
> 
>



From maechler at stat.math.ethz.ch  Wed Aug 11 08:33:11 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 11 Aug 2004 08:33:11 +0200
Subject: [R] str and Surv objects
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF8202@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF8202@usrymx25.merck.com>
Message-ID: <16665.48551.955761.207471@gargle.gargle.HOWL>

>>>>> "AndyL" == Liaw, Andy <andy_liaw at merck.com>
>>>>>     on Tue, 10 Aug 2004 22:19:23 -0400 writes:

    AndyL> Buried in str.default is a line:
    AndyL> iv.len <- round(2.5 * v.len)

    AndyL> which seems to be the culprit.  v.len is 4, which
    AndyL> seems correct for the data given.  Why multiply by
    AndyL> 2.5?

because 'v.len' is the length to use for 'double'
'iv.len' is the one to use for 'integer' (*iv* = integer vector).

2.5 is a fudge factor determined `by experience, examples, trial
and error'

I'll fix this bug differently (than not using '2.5*' ).
Martin Maechler

    AndyL> Andy

    >> From: Laura Holt
    >> 
    >> Dear R People:
    >> 
    >> I used the "Surv" function to produce the following object:
    >> >a <- Surv(1:4,2:5,c(0,1,1,0)) a
    >> [1] (1,2+] (2,3 ] (3,4 ] (4,5+]
    >> >str(a)
    >> Error in "[.Surv"(object, 1:ile) : subscript out of bounds
    >> >
    >> 
    >> Why does str(a) give an error, please?  Or did I do something wrong?
    >> 
    >> Thanks in advance.
    >> R Version 1.9.1 Windows
    >> Sincerely,
    >> Laura Holt
    >> mailto: lauraholt_983 at hotmail.com
    >> 
    >> 

    >> ______________________________________________
    >> R-help at stat.math.ethz.ch mailing list
    >> https://stat.ethz.ch/mailman/listinfo/r-help
    >> PLEASE do read the posting guide! 
    >> http://www.R-project.org/posting-guide.html
    >> 
    >> 

    AndyL> ______________________________________________
    AndyL> R-help at stat.math.ethz.ch mailing list
    AndyL> https://stat.ethz.ch/mailman/listinfo/r-help
    AndyL> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Wed Aug 11 08:42:50 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 11 Aug 2004 07:42:50 +0100 (BST)
Subject: [R] Observation error in arima
In-Reply-To: <005f01c46385$860dad60$6b685281@nrelwang>
Message-ID: <Pine.LNX.4.44.0408110740360.8393-100000@gannet.stats>

An ARIMA model, by definition, has no observation error.  And the 
definition used is on the help page.

On Tue, 6 Jul 2004, Guiming Wang wrote:

> Does anyone know how to include observation errors in the arima of R.  
> I read the manual and tried the example codes, but did not find the
> solution.  From the outputs of the components "model", it seems to me
> that the default setting of the arima does not include the observational
> error in the fitting. Am I right on this one?  Thanks in advance.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Wed Aug 11 09:06:59 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 11 Aug 2004 09:06:59 +0200
Subject: [R] persp, array and colors
In-Reply-To: <3699CDBC4ED5D511BE6400306E1C0D8106B4101A@hermes.demogr.mpg.de>
References: <3699CDBC4ED5D511BE6400306E1C0D8106B4101A@hermes.demogr.mpg.de>
Message-ID: <4119C593.20504@statistik.uni-dortmund.de>

Camarda, Carlo Giovanni wrote:
> Dear R-users,
> I'd like to plot a three-dimensional surface and at the meantime I'm using
> an array. I would like to have the values of my first matrix in the heights
> of the plot and the colors of the single facet taking into account the
> second matrix.
> I hope that the next code will help all of you to understand better my
> issue, 

Have you read ?persp carefully? Have you looked at the examples in 
demo(persp)? Where is the problem?



> Thanks in advance, Giancarlo
> ############################
> ## creating my array
> m1 <- runif(30,min=1, max=10)
> m2 <- c(rep(0,12), rep(1,5), rep(0,3), rep(1,30-12-5-3))
> mm <- array(c(m1,m2), dim=c(6,5,2))
> 
> ## colors
> colo <- c("red", "blue")
> 
> ## axis
> x  <- 1:6
> y  <- 1:5
> z  <- mm[,,1]
> z1 <- mm[,,2]
> 
> ## surface with heights and colors 
> ## related to the first matrix (no good)
> persp(x, y, z, theta = 30, phi = 30, expand = 0.5, col = colo,
>         ltheta = 120, ticktype = "detailed",
>         xlab = "X", ylab = "Y", zlab = "values" )

Use something like col = colo[mm[-1,-1,2]+1] instead of col=colo. Note 
that the color matrix must be (m-1)x(n-1) if z is   m x n !!!

Uwe Ligges


> ## surface with heights and colors 
> ## related to the second matrix (no good as well)
> persp(x, y, z1, theta = 30, phi = 30, expand = 0.5, col = colo,
>         ltheta = 120, ticktype = "detailed",
>         xlab = "X", ylab = "Y", zlab = "values" )
> ############################
> 
> 
> 
> +++++
> This mail has been sent through the MPI for Demographic Rese...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From tbl at codan.dk  Wed Aug 11 09:48:32 2004
From: tbl at codan.dk (Tine Buch-Larsen)
Date: Wed, 11 Aug 2004 09:48:32 +0200
Subject: [R] Function: integrate
Message-ID: <OFC41CA1C3.CB0BB5BD-ONC1256EED.00283555@codan.dk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040811/9ada86bd/attachment.pl

From ripley at stats.ox.ac.uk  Wed Aug 11 10:06:10 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 11 Aug 2004 09:06:10 +0100 (BST)
Subject: [R] Function: integrate
In-Reply-To: <OFC41CA1C3.CB0BB5BD-ONC1256EED.00283555@codan.dk>
Message-ID: <Pine.LNX.4.44.0408110858510.10210-100000@gannet.stats>

Your function is not even differentiable.  Simple integration rules will 
have error iff they span a point of non-differentiability so there is no 
way an automated routine will get a sensible error estimate for choosing 
adaptation.

You can integrate this function exactly, so why use integrate?  If it is 
a sample of a smooth function, interpolate by say cubic splines rather 
than piecewise linearly (?splinefun).

BTW, did you consult the reference on the help page?

On Wed, 11 Aug 2004, Tine Buch-Larsen wrote:

> I have a question regarding the function "integrate".
> Why does integrate sometimes returns "extremely bad integrand behaviour", 
> when I integrate a well-define function between 0 and ,1 and how can I 
> solve the problem.
> The following is an example of the problem:
> x = seq(0, 1, length=50) 
> y = c(0.6697534, 0.7868631, 0.8793606, 0.9404326, 0.9933826, 1.0441545, 
> 1.0861214, 1.1076160, 1.0943228, 1.0620462, 1.0334792, 1.0176313, 
> 1.0189657, 1.0286591, 1.0635902, 1.0815423, 1.0952207, 1.0985036, 
> 1.0782912, 1.0411431, 1.0056234, 0.9641239, 0.9360296, 0.9212780, 
> 0.9057849, 0.8850640, 0.8836523, 0.8780735, 0.8793521, 0.8651096, 
> 0.8526203, 0.8280131, 0.8029167, 0.7886007, 0.7844318, 0.7838392, 
> 0.7996804, 0.8162264, 0.8245920, 0.8315790, 0.8465705, 0.8623909, 
> 0.8948042, 0.9145519, 0.9141968, 0.9188292, 0.9278436, 0.9264173, 
> 0.9381125, 0.9527778)
> fct = approxfun(x, y, method="linear", rule=2)
> integrate(fct, 0, 1)$value
> 
> My R version is 1.7.1

Oh please, that's in need of an update.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From V.Khamenia at biovision-discovery.de  Wed Aug 11 10:07:04 2004
From: V.Khamenia at biovision-discovery.de (Khamenia, Valery)
Date: Wed, 11 Aug 2004 10:07:04 +0200
Subject: AW: AW: [R] built-in Sweave-like documentation in R-2.x
Message-ID: <D15343265276D31197BC00A024A6C110C7933F@EXS_BDC>

> An simpler alternative to code would be to allow one to
> eval-chunk-and-step, stepping through chunks, similar to C-c C-n for
> stepping through lines.  Would this solve the basic problem?  3 x (2
> or 3 keystrokes) for 3 chunks.

hm, not really ideal...


> I can't imagine an evaluation which would cross chunks but use only
> part of chunks (this suggests bad programming design to me), but
> perhaps you (or others) have an example of when this functionality
> would be useful?  (i.e. actual regions to eval which cross code-chunk
> boundaries but contain 1 or 2 incomplete code-chunks?).

hm, when?..
The very typical situation for me today is R-code 
with structure like this:

##############################
# Section 1. Loading data 
# sometime execution time is *very* long (up to 5 minuutes with read.table)
# This section have to be executed quite rarely
...

# Section 2. Some initial data preparations and preprocessing
# execution time might be long. This section have to be 
# executed a bit more often then Section 1.
...

# Section 3. Task-oriented part
# execution time might be very long,
# this section is executed very often.
...
##############################

actually the Section 3 motivated me to start this thread.
because it usually consists of many chunk alternations 
and should be reexecuted many times during development.

--
Valery



From maechler at stat.math.ethz.ch  Wed Aug 11 10:13:08 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 11 Aug 2004 10:13:08 +0200
Subject: [R] str and Surv objects
In-Reply-To: <BAY12-F4I76rOJhKPc900050321@hotmail.com>
References: <BAY12-F4I76rOJhKPc900050321@hotmail.com>
Message-ID: <16665.54548.752922.258181@gargle.gargle.HOWL>

>>>>> "Laura" == Laura Holt <lauraholt_983 at hotmail.com>
>>>>>     on Tue, 10 Aug 2004 20:59:35 -0500 writes:

    Laura> Dear R People:
    Laura> I used the "Surv" function to produce the following object:
    >> a <- Surv(1:4,2:5,c(0,1,1,0)) 
    >> a
    Laura> [1] (1,2+] (2,3 ] (3,4 ] (4,5+]
    >> str(a)
    Laura> Error in "[.Surv"(object, 1:ile) : subscript out of bounds


    Laura> Why does str(a) give an error, please?  

because it assumes something that is probably true for (almost?)
all but these somewhat `crazy' Surv objects
{and that's not the "fault" of Thomas Lumley, the maintainer of
 "survival" for R, but rather the original 'survival' authors for S}:

## On one hand,  a  fulfills  
 is.array(a) & is.atomic(a) ## TRUE

 length(a) ## gives 12
	   ## but
 a[1:12]   ## gives an error and all a[k]  for k > 4 give errors
 length(as.character(a)) ## gives 4


so, I'll have to work around this problem in str.default()
and think about writing a  str.Surv()  (S3) method which would
be cleaner [code proposals for the latter are welcome, on R-devel, or to me!]

    Laura> Or did I do something wrong?

no you didn't.  You've discovered a bug -- I wonder why that
hasn't been found before.
It must be almost as old as R, well at least as old as these
[class="Surv", type="counting"] objects exist in the survival
package.

Here is the patch to the source in "R-patched",
"R-devel" will get a slightly different patch, if not
and str.Surv() method.

--- src/library/utils/R/str.R	(revision 30602)
+++ src/library/utils/R/str.R	(working copy)
@@ -247,9 +247,13 @@
 		ob <- if(le > iv.len) object[seq(len=iv.len)] else object
 		ao <- abs(ob <- ob[!is.na(ob)])
 	    }
+            else if(iSurv)
+                le <- length(object <- as.character(object))
 	    if(int.surv || (all(ao > 1e-10 | ao==0) && all(ao < 1e10| ao==0) &&
 			    all(ob == signif(ob, digits.d)))) {
-		v.len <- iv.len
+		if(!(iSurv && a$type == "counting"))
+                    ## use integer-like length
+                    v.len <- iv.len
 		format.fun <- function(x)x
 	    } else {
 		v.len <- round(1.25 * v.len)



From maechler at stat.math.ethz.ch  Wed Aug 11 11:18:13 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 11 Aug 2004 11:18:13 +0200
Subject: [R] Error message in function mars() in package mda
In-Reply-To: <41192876.2040503@tcindex.com>
References: <41192876.2040503@tcindex.com>
Message-ID: <16665.58453.46020.671990@gargle.gargle.HOWL>

>>>>> "Jude" == Jude Ryan <juderyan at tcindex.com>
>>>>>     on Tue, 10 Aug 2004 15:56:38 -0400 writes:

    Jude> Hi, I am using function mars() in package mda to find
    Jude> knots in a whole bunch of predictor variables. I hope
    Jude> to be able to replicate all or some of the basis
    Jude> functions that the MARS software from Salford Systems
    Jude> creates.  When I ran mars() on a small dataset, I was
    Jude> able to get the knots.  However, when I tried running
    Jude> mars() on a larger dataset (145 predictor variables),
    Jude> for a different project, I get the following error
    Jude> message:

    >> fit1 <- mars(disney2[,-146], disney2[,146])
    Jude> Error in mars(disney2[, -146], disney2[, 146]) :
    Jude> NA/NaN/Inf in foreign function call (arg 5)
    Jude> In addition: Warning messages:
    Jude> 1: NAs introduced by coercion
    Jude> 2: NAs introduced by coercion
    >> 

    Jude> Does arg 5 refer to the 5th column in my dataset? 
no.

    Jude> This seems to be a data problem, is this correct?

we cannot know with the little information you give.
Please read (and follow!!) the posting guide!
-> reproducible example !
-> contact the maintainer of the package in question!

The following reproducible example doesn't show the problem you
mentioned:

> library(mda)
> set.seed(101)
> X <- matrix(rnorm(200 * 150), 200, 150)
> fX <- mars(X[,-146], X[,146])
> str(fX)
List of 15
 $ call          : language mars(x = X[, -146], y = X[, 146])
 $ all.terms     : int [1:93] 1 2 3 4 5 6 7 8 9 10 ...
 $ selected.terms: int [1:25] 1 2 3 4 5 10 12 13 14 16 ...
 $ penalty       : num 2
 $ degree        : num 1
 $ nk            : num 299
 $ thresh        : num 0.001
 $ gcv           : num 0.72
 $ factor        : num [1:99, 1:149] 0 0 0 0 0 0 0 0 0 0 ...
  ..- attr(*, "dimnames")=List of 2
  .. ..$ : NULL
  .. ..$ : NULL
 $ cuts          : num [1:99, 1:149] 0 0 0 0 0 0 0 0 0 0 ...
 $ residuals     : num [1:200, 1]  0.789  0.111 -0.212  1.017 -0.391 ...
 $ fitted.values : num [1:200, 1] -0.216 -0.249 -0.116 -0.274 -0.353 ...
 $ lenb          : int 99
 $ coefficients  : num [1:25, 1]  0.696  0.491  1.013 -1.043 -0.409 ...
 $ x             : num [1:200, 1:25] 1 1 1 1 1 1 1 1 1 1 ...
 - attr(*, "class")= chr "mars"
> 

BTW (as a note to the maintainer of "mda"):

  print.mars()  {and maybe summary.mars()} is severly lacking...

    Jude> Are there any other functions in R that will give me
    Jude> the knots for a set of predictor variables?

maybe, quite probably. This question is too vague ["the" knots?]


Regards,
Martin Maechler



From Luisr at frs.fo  Wed Aug 11 11:49:03 2004
From: Luisr at frs.fo (Luis Rideau Cruz)
Date: Wed, 11 Aug 2004 10:49:03 +0100
Subject: [R] type III sum of squares
Message-ID: <s119f9a2.075@ffdata.setur.fo>

R-help

What are the strengths and weakness of 'aov' in 'car' package?

My model looks something like this :

library(car)
aov(lm(fish.length~zone*area,data=my.data))

Thank you

Luis Ridao Cruz
Fiskiranns??knarstovan
N??at??n 1
P.O. Box 3051
FR-110 T??rshavn
Faroe Islands
Phone:             +298 353900
Phone(direct): +298 353912
Mobile:             +298 580800
Fax:                 +298 353901
E-mail:              luisr at frs.fo
Web:                www.frs.fo



From Luisr at frs.fo  Wed Aug 11 12:24:38 2004
From: Luisr at frs.fo (Luis Rideau Cruz)
Date: Wed, 11 Aug 2004 11:24:38 +0100
Subject: [R] titles to plots
Message-ID: <s11a01f8.081@ffdata.setur.fo>

R-help

par(mfrow=c(2,4))
lapply(lgd.aov.sum,function (x) {plot(x[,1],x[,3])
})

What I get is a 2x4 layout plot
How can I get the names argument of lgd.aov.sum and pass it as title names for every plot??

Thank you

Luis Ridao Cruz
Fiskiranns??knarstovan
N??at??n 1
P.O. Box 3051
FR-110 T??rshavn
Faroe Islands
Phone:             +298 353900
Phone(direct): +298 353912
Mobile:             +298 580800
Fax:                 +298 353901
E-mail:              luisr at frs.fo
Web:                www.frs.fo



From pallier at lscp.ehess.fr  Wed Aug 11 13:42:07 2004
From: pallier at lscp.ehess.fr (Christophe Pallier)
Date: Wed, 11 Aug 2004 13:42:07 +0200
Subject: [R] type III sum of squares 
Message-ID: <411A060F.9080502@lscp.ehess.fr>

>What are the strengths and weakness of 'aov' in 'car' package?
>My model looks something like this :
> library(car)
> aov(lm(fish.length~zone*area,data=my.data))


'aov' is in the package 'stats', not in 'car'. (see ?aov)

One of the interests of 'aov' (compared to 'lm') is that using the 'Error' term in the formula allows to analyse designs  where sampling occurs at several levels (as in some though this term is not really correct)..

You may be thinking of the *Anova* function in the car package (?)
'Anova(model)' allows to compute so-called 'type II' and 'type III' sums of squares 
sparing you the need to play with the order of terms if you used 'anova' (notice the lowercase).

Type III sums of square are useful in factorial designs with unequal number of observations. 
When the factors are coded with the contrasts contr.sum or contr.helmert, the test for the main effect of a factor weights equally all subgroups 
(see John Fox's book : "An R and S-plus Companion to Applied Regression", p.140). 

I discussed this topic in a recent thread on this list ([R] Re: Enduring LME confusion.... or Psychologists and Mixed-Effects)

The archive of R-help contains several post about this topic.
 
I hope this helps.

Christophe Pallier
http://www.pallier.org



From ligges at statistik.uni-dortmund.de  Wed Aug 11 12:56:19 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 11 Aug 2004 12:56:19 +0200
Subject: [R] type III sum of squares
In-Reply-To: <s119f9a2.075@ffdata.setur.fo>
References: <s119f9a2.075@ffdata.setur.fo>
Message-ID: <4119FB53.5020807@statistik.uni-dortmund.de>

Luis Rideau Cruz wrote:

> R-help
> 
> What are the strengths and weakness of 'aov' in 'car' package?

Have you read John Fox's book this packages correpsonds to? Do it, it 
is great.

Uwe Ligges


> My model looks something like this :
> 
> library(car)
> aov(lm(fish.length~zone*area,data=my.data))
> 
> Thank you
> 
> Luis Ridao Cruz
> Fiskiranns??knarstovan
> N??at??n 1
> P.O. Box 3051
> FR-110 T??rshavn
> Faroe Islands
> Phone:             +298 353900
> Phone(direct): +298 353912
> Mobile:             +298 580800
> Fax:                 +298 353901
> E-mail:              luisr at frs.fo
> Web:                www.frs.fo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Wed Aug 11 12:58:50 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 11 Aug 2004 12:58:50 +0200
Subject: [R] titles to plots
In-Reply-To: <s11a01f8.081@ffdata.setur.fo>
References: <s11a01f8.081@ffdata.setur.fo>
Message-ID: <4119FBEA.1080004@statistik.uni-dortmund.de>

Luis Rideau Cruz wrote:

> R-help
> 
> par(mfrow=c(2,4))
> lapply(lgd.aov.sum,function (x) {plot(x[,1],x[,3])
> })
> 
> What I get is a 2x4 layout plot
> How can I get the names argument of lgd.aov.sum and pass it as title names for every plot??

a) What is lgd.aov.sum?
b) What do you mean whith names argument? Do you mean any attribute?

Please read the posting guide (see below - cited at the end of this 
message).

Uwe Ligges



> Thank you
> 
> Luis Ridao Cruz
> Fiskiranns??knarstovan
> N??at??n 1
> P.O. Box 3051
> FR-110 T??rshavn
> Faroe Islands
> Phone:             +298 353900
> Phone(direct): +298 353912
> Mobile:             +298 580800
> Fax:                 +298 353901
> E-mail:              luisr at frs.fo
> Web:                www.frs.fo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ym at climpact.com  Wed Aug 11 13:00:34 2004
From: ym at climpact.com (Yves Magliulo)
Date: 11 Aug 2004 13:00:34 +0200
Subject: [R] titles to plots
In-Reply-To: <s11a01f8.081@ffdata.setur.fo>
References: <s11a01f8.081@ffdata.setur.fo>
Message-ID: <1092222033.18329.51.camel@new-york.climpact.net>

hi,

see option mfg in par
?par for more info.



Le mer 11/08/2004 ?? 12:24, Luis Rideau Cruz a ??crit :
> R-help
> 
> par(mfrow=c(2,4))
> lapply(lgd.aov.sum,function (x) {plot(x[,1],x[,3])
> })
> 
> What I get is a 2x4 layout plot
> How can I get the names argument of lgd.aov.sum and pass it as title names for every plot??
> 
> Thank you
> 
> Luis Ridao Cruz
> Fiskiranns??knarstovan
> N??at??n 1
> P.O. Box 3051
> FR-110 T??rshavn
> Faroe Islands
> Phone:             +298 353900
> Phone(direct): +298 353912
> Mobile:             +298 580800
> Fax:                 +298 353901
> E-mail:              luisr at frs.fo
> Web:                www.frs.fo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
-- 
------
Yves Magliulo <ym at climpact.com>
R&D Engineer, CLIMPACT

Tel.   : +33 (0) 1 55 07 85 77
Fax.   : +33 (0) 1 55 07 85 79
Universite Pierre et Marie Curie
Boite 101 - Tour 45 - 5eme etage - Couloir 45/46
4 place Jussieu, 75252 Paris CEDEX 05, France



From t.dewez at brgm.fr  Wed Aug 11 15:48:31 2004
From: t.dewez at brgm.fr (Dewez Thomas)
Date: Wed, 11 Aug 2004 15:48:31 +0200
Subject: [R] Advice on picking a regression method
Message-ID: <D965434E9D6BD511AE3500306E01C8BE05DD68FF@SRV0015>

Dear R-users,

There are tons of methods out there for fitting independant variables to a
dependent variable. All stats books tell you about the assumptions behind
OLS (ordinary least squares) and warn against abusive use of the method
(which many of us do disregard by lack of a better knowledge). Most
introductory text books stop there and don't tell you what the next best
option might be. I am aware that there might be many depending on the type
of study so here are the data to sort this question out.

In this instance, I am performing a regression on observations whose
residuals show heteroscedasticity (the variance of residuals is small for
small dependant variable values and increases for larger ones), which
violates one assumption of the OLS method. Which of the numerous options
should I choose? glm, robust lm, ...

The problem is kept simple for now. I only try to explain the log of local
topographic slope (dependent variable) with regard to the distance to the
outlet of a catchment (independent variable) for a fixed drained area. Both
variables are continuous.

I ordered Venables and Ripley 2002, which I suspect is a sound reading for
advanced stats with R, but it has not arrived yet and I need to move on
asap. Any advice or pointer to the appropriate literature is greatly
appreciated.

Thomas

Dr Thomas Dewez
ENTEC Post-Doctoral Fellow 
ARN - MAS
BRGM (French Geological Survey)
3 Av. C. Guillemin
45000 Orleans - France

Phone: +33 (0)2 38644606
Fax: +33 (0)2 38643361
***
Le contenu de cet e-mail et de ses pi??ces jointes est destin...{{dropped}}



From fw at acoustics.dk  Wed Aug 11 15:20:12 2004
From: fw at acoustics.dk (Florian Wickelmaier)
Date: Wed, 11 Aug 2004 15:20:12 +0200 (MEST)
Subject: [R] [R-pkgs] New package: eba
Message-ID: <Pine.GSO.4.33.0408111452280.15406-100000@zil.kom.auc.dk>

I have recently uploaded a new package to CRAN called eba,
which stands for elimination-by-aspects models.

It allows for fitting and testing probabilistic choice models,
especially the BTL model (Bradley & Terry, 1952; Luce, 1959),
elimination-by-aspects (EBA) models (Tversky, 1972), and
preference tree (Pretree) models (Tversky & Sattath, 1979).

I believe that the functions in this package can be useful
in many fields, like marketing, consumer research, psychological
scaling, and food testing, to name only a few.

Detailed information about probabilistic choice models and
how to fit them can be found in a recent publication:

Wickelmaier, F. & Schmid, C. (2004). A Matlab function to estimate
  choice model parameters from paired-comparison data. Behavior
  Research Methods, Instruments, and Computers, 36(1), 29-40.

I would be glad to receive some feedback or comments.

---------------------------------------------------------------
 Florian Wickelmaier
 Dipl.-Psych., Research Assistant       office: B5-209
 Department of Acoustics                phone: (+45) 9635 8720
 Fredrik Bajers Vej 7 B5                fax: (+45) 9815 2144
 DK-9220 Aalborg East                   e-mail: fw at acoustics.dk

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From vito_ricci at yahoo.com  Wed Aug 11 16:37:06 2004
From: vito_ricci at yahoo.com (=?iso-8859-1?q?Vito=20Ricci?=)
Date: Wed, 11 Aug 2004 16:37:06 +0200 (CEST)
Subject: [R] Advice on picking a regression method
Message-ID: <20040811143706.50219.qmail@web41202.mail.yahoo.com>

Dear Thomas,

I believe a GLS (Generalized Least Squares, known also
as Aitken estimator) estimate could be used in case of
heteroskedasticity of residuals. See:

? lm.gls  in MASS package or ? gls in nlme package

Another way is to study the relentionship between x
(indipendent variable) and the variance of y
(dependent variable) usig a graphic. I read, when I
was a student, there some transformation which can
reduce heteroskedasticity , but I don't remember
particulars.

For theory about GLS:

http://cran.r-project.org/doc/contrib/Fox-Companion/appendix-timeseries-regression.pdf
(here you can find some applications of GLS with R)

http://www.sinica.edu.tw/as/ssrc/ckuan/pdf/et01/ch4.pdf

http://jackman.stanford.edu/papers/gls.pdf


Best
Vito






Dear R-users,

There are tons of methods out there for fitting
independant variables to a
dependent variable. All stats books tell you about the
assumptions behind
OLS (ordinary least squares) and warn against abusive
use of the method
(which many of us do disregard by lack of a better
knowledge). Most
introductory text books stop there and don't tell you
what the next best
option might be. I am aware that there might be many
depending on the type
of study so here are the data to sort this question
out.

In this instance, I am performing a regression on
observations whose
residuals show heteroscedasticity (the variance of
residuals is small for
small dependant variable values and increases for
larger ones), which
violates one assumption of the OLS method. Which of
the numerous options
should I choose? glm, robust lm, ...

The problem is kept simple for now. I only try to
explain the log of local
topographic slope (dependent variable) with regard to
the distance to the
outlet of a catchment (independent variable) for a
fixed drained area. Both
variables are continuous.

I ordered Venables and Ripley 2002, which I suspect is
a sound reading for
advanced stats with R, but it has not arrived yet and
I need to move on
asap. Any advice or pointer to the appropriate
literature is greatly
appreciated.

Thomas

Dr Thomas Dewez
ENTEC Post-Doctoral Fellow 
ARN - MAS
BRGM (French Geological Survey)
3 Av. C. Guillemin
45000 Orleans - France

=====
Diventare costruttori di soluzioni

Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese http://www.modugno.it/archivio/cat_palese.shtml



From Yingfu.Xie at sekon.slu.se  Wed Aug 11 16:38:45 2004
From: Yingfu.Xie at sekon.slu.se (Yingfu Xie)
Date: Wed, 11 Aug 2004 16:38:45 +0200
Subject: [R] always NaN after some running in R, but all  fine in S-plus
Message-ID: <9BE977B02923D311AADA00105AF497830350530E@tilia.slu.se>

Hello, S-plus and R helpers,(sorry for cross-post)

I wrote some simple C code for one likelihood to be optimized (using
optim(MASS)). I use same function, same data, same starting points and same
DLL in R and S-plus for comparison. (I compiled it with 'Rcmd SHLIB
likelihood.c' and the header files of it include only R.h and math.h). While
it works quite fine in S-plus, it forever returns NaN for the value of
log-likelihood after several normal running in R. What is worse, it can
never return a normal value once NaN appears. I just cannot understand why.
Has anyone had similar experience? I searched R and S-plus archive for
nothing. I attach more details below and really appreciate any help from
you!

R:1.9.0  S-plus:6.2 OS: Windows 2000

Best Regards,
Yingfu

The output in R are:

> symbol.C("like")
[1] "like"
> likelihood(data=data03) # The likelihood function calling .C("like",...)
[1] 5850.12
> myoptim(data=data03) # Optimization using optim with fn=likelihood

Initial parameters are
1 10 0.2 0.2 0.2 0.1 0.1 0.1 0.1 0.2 0.3 0.5 
1 10 0.2 0.2 0.2[1] 5850.154     #I print first 5 parameters and the value 
1.001 10 0.2 0.2 0.2[1] 5850.308 #of the minus log-likelihood every run
0.999 10 0.2 0.2 0.2[1] 5849.997
...........                      #similar outputs are omitted
1 10 0.2 0.2 0.2[1] 5848.578
0 0 0 0 0[1] NaN                 #It seems that here it jumps to the edge of
0.001 0 0 0 0[1] NaN             #box constrain, all zeros in my case.
.............                    #And it returns NaN. From now on, it is  
0 0 0 0 0[1] NaN                 # forever NaN,
0 0 0 0 0[1] NaN                 # even if it jumps back near to the initial

0.995734 9.95734 0.1991468 0.1991468 0.1991468[1] NaN    #parameters.
0.996734 9.95734 0.1991468 0.1991468 0.1991468[1] NaN
.............
1 10 0.2 0.2 0.201[1] NaN
1 10 0.2 0.2 0.199[1] NaN
$par
 [1]  1.0 10.0  0.2  0.2  0.2  0.1  0.1  0.1  0.1  0.2  0.3  0.5

$value          # Since NaN is not allowed with method L-BFGS-B, I set it 
[1] 1e+05       # returns a bigger 10000 if the value is NaN or NA.

$counts
function gradient 
       7        7 

$convergence
[1] 0

What is more, it became worse now:
> likelihood(data=data03) #It returns a NaN also!!
[1] NaN
> myoptim(data=data03)
Initial parameters are
1 10 0.2 0.2 0.2 0.1 0.1 0.1 0.1 0.2 0.3 0.5 
1 10 0.2 0.2 0.2[1] NaN
1.001 10 0.2 0.2 0.2[1] NaN
................
1 10 0.2 0.2 0.2[1] NaN
$par
 [1]  1.0 10.0  0.2  0.2  0.2  0.1  0.1  0.1  0.1  0.2  0.3  0.5

$value
[1] 1e+05

$counts
function gradient 
       1        1 

$convergence
[1] 0

$message
[1] "CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL"

Now, the problem seems lie in my C code and algorithm. However, in S-plus,
it works fine:

> is.loaded("like")
[1] T
> myoptim(data=data03)
Initial parameters are
 1 10 0.2 0.2 0.2 0.2 0.1 0.1 0.1 0.1 0.3 0.5 
 [1]  1.0 10.0  0.2  0.2  0.2 [1] 5834.419
 [1]  1.001 10.000  0.200  0.200 [1] 5834.579
..............                                  #it never jumps to zeros
[1] 0.7701213 4.3442599 0.1424875 0.3072238 0.1274840 [1] 5106.236
$par:
 [1] 0.77012134 4.34425988 0.14248754 0.30722383 0.12748400
 [6] 0.48420116 0.00000000 0.02095689 0.00000000 0.61156935
[11] 0.77179635 0.00000000

$value:
[1] 5106.049

$counts:
 function gradient 
       21       21

$convergence:
[1] 0

$message:
[1] "CONVERGENCE: REL_REDUCTION_OF_F <= FACTR*EPSMCH"

I tried the 'bad' parameters in S-plus:
> likelihood(par=rep(0,12),data=data03) #It is NA.
[1] NA
> likelihood(data=data03)               #It is normal again.
[1] 5834.421

Thank you very much for your time!
###########################################

This message has been scanned by F-Secure\ Anti-Virus for Mi...{{dropped}}



From andy_liaw at merck.com  Wed Aug 11 16:49:43 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 11 Aug 2004 10:49:43 -0400
Subject: [R] Advice on picking a regression method
Message-ID: <3A822319EB35174CA3714066D590DCD504AF8206@usrymx25.merck.com>

Just my $0.02...

Depending on what you are going to do with the model, heteroscedasticity
might be low on the list of things you should worry about.  I'd say that the
assumption that the model is a straight line might be high, if not the
highest, on that list.  That might be a reasonable assumption in your case,
but you definitely should investigate.

If straight line is a reasonable model for the data, then OLS may not be
such a bad thing, if you don't have skewed data or outliers.  You should try
several methods and see which looks most reasonable.  (I don't think there's
anything wrong with trying different methods of fitting the same model, at
least it seems less dangerous than choosing among many models fitted with
the same method.)

Non-constant variance only affects efficiency of the estimator and the
inference (CI, hythothesis tests).  If you need to do inference, you need to
address that, and two most popular ways are weighted least squares and
transformation.

HTH,
Andy

> From: Dewez Thomas
> 
> Dear R-users,
> 
> There are tons of methods out there for fitting independant 
> variables to a
> dependent variable. All stats books tell you about the 
> assumptions behind
> OLS (ordinary least squares) and warn against abusive use of 
> the method
> (which many of us do disregard by lack of a better knowledge). Most
> introductory text books stop there and don't tell you what 
> the next best
> option might be. I am aware that there might be many 
> depending on the type
> of study so here are the data to sort this question out.
> 
> In this instance, I am performing a regression on observations whose
> residuals show heteroscedasticity (the variance of residuals 
> is small for
> small dependant variable values and increases for larger ones), which
> violates one assumption of the OLS method. Which of the 
> numerous options
> should I choose? glm, robust lm, ...
> 
> The problem is kept simple for now. I only try to explain the 
> log of local
> topographic slope (dependent variable) with regard to the 
> distance to the
> outlet of a catchment (independent variable) for a fixed 
> drained area. Both
> variables are continuous.
> 
> I ordered Venables and Ripley 2002, which I suspect is a 
> sound reading for
> advanced stats with R, but it has not arrived yet and I need 
> to move on
> asap. Any advice or pointer to the appropriate literature is greatly
> appreciated.
> 
> Thomas
> 
> Dr Thomas Dewez
> ENTEC Post-Doctoral Fellow 
> ARN - MAS
> BRGM (French Geological Survey)
> 3 Av. C. Guillemin
> 45000 Orleans - France
> 
> Phone: +33 (0)2 38644606
> Fax: +33 (0)2 38643361
> ***
> Le contenu de cet e-mail et de ses pi??ces jointes est 
> destin...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From shawp at mail.nih.gov  Wed Aug 11 17:15:51 2004
From: shawp at mail.nih.gov (Shaw, Philip (NIH/NIMH))
Date: Wed, 11 Aug 2004 11:15:51 -0400
Subject: [R] (no subject)
Message-ID: <5F9DE1C25708B04EAD634A1AE3D91130044D053C@nihexchange20.nih.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040811/b31fce3e/attachment.pl

From andy_liaw at merck.com  Wed Aug 11 17:22:05 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 11 Aug 2004 11:22:05 -0400
Subject: [R] (no subject)
Message-ID: <3A822319EB35174CA3714066D590DCD504AF8208@usrymx25.merck.com>

Something like:

newVar <- ifelse(group == 1 & diagnosis == 1, 'positive', 'negative')

[You might want to coerce that to a factor if you wish.]

Andy

> From: Shaw, Philip (NIH/NIMH)
> 
> Hi
>  
> A very simple question (from a R novice)
>  
> How do I create a new variable within R.  I have two existing 
> variables (eg
> group number 1,2,3 and diagnosis 1,2,3).  How would I create 
> a new variable
> in which those in group1 with diagnosis 1 are assigned 
> 'positive' and the
> remainder are 'negative'??
>  
> Many thanks- and sorry it's so simple!
>  
> Philip 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From edwardeb at unos.org  Wed Aug 11 18:30:35 2004
From: edwardeb at unos.org (Erick Edwards)
Date: Wed, 11 Aug 2004 12:30:35 -0400
Subject: [R] Stratified Survival Estimates
Message-ID: <127C4108DB99F24B869F8F75589E746244E08D@EXCH2003.unos.corp>

Using R version 1.8.1 for Windows, I obtain an error message using the following code.  The data frame was constructed in the counting process style, where V1 is the start time, V2 is the stop time, and V3 is the censoring indicator.  There are no zero-length time intervals.  Variable V4 is the stratification factor (gender: F,M).

S<-Surv(V1,V2,V3)
fit<-survfit(S ~ V4,data=test.dat)

summary(fit) produces the following error message:

Call: survfit(formula = S ~ V4)

                V4=F 
Error in as.matrix(x) : (subscript) logical subscript too long


Using no stratification factor in the survfit function, I get no error using summary.  

I need to be able to write the survival curve for each stratum to a an external file.

Any help appreciated.

Erick B. Edwards



From tlumley at u.washington.edu  Wed Aug 11 18:53:58 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 11 Aug 2004 09:53:58 -0700 (PDT)
Subject: [R] Stratified Survival Estimates
In-Reply-To: <127C4108DB99F24B869F8F75589E746244E08D@EXCH2003.unos.corp>
References: <127C4108DB99F24B869F8F75589E746244E08D@EXCH2003.unos.corp>
Message-ID: <Pine.A41.4.58.0408110951180.258664@homer10.u.washington.edu>

On Wed, 11 Aug 2004, Erick Edwards wrote:

> Using R version 1.8.1 for Windows, I obtain an error message using the

It would be worth upgrading, though it wouldn't fix this particular
problem.

> following code.  The data frame was constructed in the counting process
> style, where V1 is the start time, V2 is the stop time, and V3 is the
> censoring indicator.  There are no zero-length time intervals.
> Variable V4 is the stratification factor (gender: F,M).
>
> S<-Surv(V1,V2,V3)
> fit<-survfit(S ~ V4,data=test.dat)
>
> summary(fit) produces the following error message:
>
> Call: survfit(formula = S ~ V4)
>
>                 V4=F
> Error in as.matrix(x) : (subscript) logical subscript too long
>
>
> I need to be able to write the survival curve for each stratum to a an
> external file.
>

You should still be able to get the survival curves written out.

fit[1]$time and fit[1]$surv give time and survival for the first stratum,
fit[2]$time and fit[2]$surv for the second stratum.

Thanks for the bug report.

	-thomas



From ottorino-luca.pantani at unifi.it  Wed Aug 11 19:00:33 2004
From: ottorino-luca.pantani at unifi.it (8rino-Luca Pantani)
Date: Wed, 11 Aug 2004 19:00:33 +0200
Subject: [R] Xemacs do not want to execute help.start() with R1.9.1
Message-ID: <DNEELNJCLGBOLHCFLMHBOEALCFAA.OLPantani@unifi.it>

Hi all,
I 've just reinstalled from the scratch my Windows 2000 (Italian version)
on a formatted HD, because I like to clean
the environment from time to time (say once a year).

I'm currently using XEmacs-21.4.13 and R (rw1091)
and Xemacs is configured as described in
http://socserv.socsci.mcmaster.ca/jfox/Books/Companion/ESS/ess-xemacs.pdf

The problem is that the command
help.start()
added in the  c\Program Files\R\rw1091\etc\Rprofile
do not work and I receive from Xemacs at the startup the following two
lines:
????????????????????????????????????????????
Error: couldn't find function "help.start"
> options(STERM='iESS', editor='winclient.exe'> >
????????????????????????????????????????????
If I type
help.start()
in the upper as well as in the lower window,
the HTML help start in a new window.

This happened also before the reinstallation of the whole OS,
but it did not happened when I was running a previous R version (1.7.??)

Any suggestions?

Thanks for your time.

Ottorino
Univ Of Florence



From rolf at math.unb.ca  Wed Aug 11 19:19:04 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Wed, 11 Aug 2004 14:19:04 -0300 (ADT)
Subject: [R] Xemacs do not want to execute help.start() with R1.9.1
Message-ID: <200408111719.i7BHJ4A0006295@erdos.math.unb.ca>


This is really a question about .Rprofile and is becoming an FAQ.
The phenomenon you observe is due to the reorganization of the system
packages in R 1.9.x:  The base package had segments broken off into
new separate packages, e.g. utils, where help.start() can be found.

In the ``NEWS'' file can be found the following paragraph:

	Users may notice that code in .Rprofile is run with only the
	new base loaded and so functions may now not be found.  For
	example, ps.options(horizontal = TRUE) should be preceded by
	library(graphics) or called as graphics::ps.options or,
	better, set as a hook -- see ?setHook.

So you could get help.start() to work by invoking it as

	utils::help.start().


				cheers,

					Rolf Turner
					rolf at math.unb.ca



From maechler at stat.math.ethz.ch  Wed Aug 11 19:23:42 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 11 Aug 2004 19:23:42 +0200
Subject: [R] Xemacs do not want to execute help.start() with R1.9.1
In-Reply-To: <DNEELNJCLGBOLHCFLMHBOEALCFAA.OLPantani@unifi.it>
References: <DNEELNJCLGBOLHCFLMHBOEALCFAA.OLPantani@unifi.it>
Message-ID: <16666.22046.93284.583054@gargle.gargle.HOWL>

I've forwarded your question to the appropriate mailing list:
ESS-help, see http://stat.ethz.ch/mailman/listinfo/ess-help/

Martin Maechler



From jeroschh at ohsu.edu  Wed Aug 11 19:35:06 2004
From: jeroschh at ohsu.edu (Michael Jerosch-Herold)
Date: Wed, 11 Aug 2004 10:35:06 -0700
Subject: [R] circular structures in NLME
Message-ID: <s119f66d.019@ohsu.edu>


I have a problem where I am looking at a circular structure and I want
to examine the spatial correlation pattern in the random effects portion
of the model. I am looking at 8 sectors of a circular patch, and plan to
use the corExp class. The sector number is the position variable, but
for the calculation of the distance between two sectors I need to take
into account the fact that the sectors are arranged in a circular
pattern. I have read through some of the NLME documentation, it is not
clear to me how I can specify the "distance function" appropriately.

Any help/suggestions would be much appreciated.

Michael Jerosch-Herold



From atcdias at biologia.ufrj.br  Wed Aug 11 21:52:20 2004
From: atcdias at biologia.ufrj.br (=?ISO-8859-1?Q?Andr=E9_Tavares?==?ISO-8859-1?Q?Corr=EAa_Dias?=)
Date: Wed, 11 Aug 2004 16:52:20 -0300 (BRT)
Subject: [R] GLM with binomial distribution: a bug?
Message-ID: <1092253940.411a78f412452@webmail.biologia.ufrj.br>


I?m trying to run a factorial model with binomial error distribution on R
program but I had some problems.  I'm quite sure it is a bug and would like to 
know if it was alread corrected. The output don?t gives me the effects of 
factors, it mix the names of factors with the names of factor?s levels. For 
instances: specieserythroxylum ('species? is the name of the factor 
and 'erythroxylum? the name of one of the species used).
I ran these same data and structure with an ANOVA and it worked well. Could it
be some kind of bug or am I missing some step?


Thanks and all the best.
Andr??



-------------------------------
Andr?? Tavares Corr??a Dias
Laborat??rio de Ecologia Vegetal
Departamento de Ecologia - IB
Universidade Federal do Rio de Janeiro
Ilha do Fund??o, CCS
Rio de Janeiro, RJ. Brasil
CEP 21941-590    CP 68020
Tel. (21)2562-6377





-------------------------------
Andr?? Tavares Corr??a Dias
Laborat??rio de Ecologia Vegetal
Departamento de Ecologia - IB
Universidade Federal do Rio de Janeiro
Ilha do Fund??o, CCS
Rio de Janeiro, RJ. Brasil
CEP 21941-590    CP 68020
Tel. (21)2562-6377



From tlumley at u.washington.edu  Wed Aug 11 21:57:22 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 11 Aug 2004 12:57:22 -0700 (PDT)
Subject: [R] GLM with binomial distribution: a bug?
In-Reply-To: <1092253940.411a78f412452@webmail.biologia.ufrj.br>
References: <1092253940.411a78f412452@webmail.biologia.ufrj.br>
Message-ID: <Pine.A41.4.58.0408111254130.258664@homer10.u.washington.edu>


The output is giving you the coefficients, which is what is supposed to
happen.

If you want an anova (more precisely, analysis of deviance) table use the
anova() function on your fitted model.

	-thomas


On Wed, 11 Aug 2004, [ISO-8859-1] Andr?? Tavares[ISO-8859-1] Corr??a Dias wrote:

>
> I?m trying to run a factorial model with binomial error distribution on R
> program but I had some problems.  I'm quite sure it is a bug and would like to
> know if it was alread corrected. The output don?t gives me the effects of
> factors, it mix the names of factors with the names of factor?s levels. For
> instances: specieserythroxylum ('species? is the name of the factor
> and 'erythroxylum? the name of one of the species used).
> I ran these same data and structure with an ANOVA and it worked well. Could it
> be some kind of bug or am I missing some step?
>
>
> Thanks and all the best.
> Andr??
>
>
>
> -------------------------------
> Andr?? Tavares Corr??a Dias
> Laborat??rio de Ecologia Vegetal
> Departamento de Ecologia - IB
> Universidade Federal do Rio de Janeiro
> Ilha do Fund??o, CCS
> Rio de Janeiro, RJ. Brasil
> CEP 21941-590    CP 68020
> Tel. (21)2562-6377
>
>
>
>
>
> -------------------------------
> Andr?? Tavares Corr??a Dias
> Laborat??rio de Ecologia Vegetal
> Departamento de Ecologia - IB
> Universidade Federal do Rio de Janeiro
> Ilha do Fund??o, CCS
> Rio de Janeiro, RJ. Brasil
> CEP 21941-590    CP 68020
> Tel. (21)2562-6377
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From andy_liaw at merck.com  Wed Aug 11 22:03:53 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 11 Aug 2004 16:03:53 -0400
Subject: [R] GLM with binomial distribution: a bug?
Message-ID: <3A822319EB35174CA3714066D590DCD504AF820C@usrymx25.merck.com>

> From: Andr?? TavaresCorr??a Dias
> 
> I'm trying to run a factorial model with binomial error 
> distribution on R
> program but I had some problems.  I'm quite sure it is a bug 
> and would like to 
> know if it was alread corrected. The output don't gives me 
> the effects of 
> factors, it mix the names of factors with the names of 
> factor's levels. For 
> instances: specieserythroxylum ('species' is the name of the factor 
> and 'erythroxylum' the name of one of the species used).
> I ran these same data and structure with an ANOVA and it 
> worked well. Could it
> be some kind of bug or am I missing some step?

The latter, I believe.  glm() fits a regression model.  If you have factors,
they enter the model as contrasts.  What you are seeing are the coefficients
for the contrasts.  If you want to see factor effect, use anova() on the glm
object.  Prof. Fox's `car' package on CRAN also has the Anova() function,
which you might find useful.

Please don't confuse your mis-understanding with bugs!

Andy

 
> 
> Thanks and all the best.
> Andr??
> 
> 
> 
> -------------------------------
> Andr?? Tavares Corr??a Dias
> Laborat??rio de Ecologia Vegetal
> Departamento de Ecologia - IB
> Universidade Federal do Rio de Janeiro
> Ilha do Fund??o, CCS
> Rio de Janeiro, RJ. Brasil
> CEP 21941-590    CP 68020
> Tel. (21)2562-6377
> 
> 
> 
> 
> 
> -------------------------------
> Andr?? Tavares Corr??a Dias
> Laborat??rio de Ecologia Vegetal
> Departamento de Ecologia - IB
> Universidade Federal do Rio de Janeiro
> Ilha do Fund??o, CCS
> Rio de Janeiro, RJ. Brasil
> CEP 21941-590    CP 68020
> Tel. (21)2562-6377
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From john.maindonald at anu.edu.au  Thu Aug 12 00:28:07 2004
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Thu, 12 Aug 2004 08:28:07 +1000
Subject: =?WINDOWS-1252?Q?Fwd:_[R]_Enduring_LME_confusion=85_or_Psycholog?=
	=?WINDOWS-1252?Q?ists_and_Mixed-Effects?=
Message-ID: <B7F152B6-EBE5-11D8-888A-000A95CDA0F2@anu.edu.au>

In my undertstanding of the problem, the model
   lme1 <- lme(resp~fact1*fact2, random=~1|subj)
should be ok, providing that variances are homogenous both between & 
within subjects.  The function will sort out which factors & 
interactions are to be compared within subjects, & which between 
subjects.  The problem with df's arises (for lme() in nlme, but not in 
lme4), when random effects are crossed, I believe.

It is difficult to give a general rule on the effect of imbalance; it 
depends on the relative contributions of the two variances and the 
nature of the imbalance.  There should be a rule that people who ask 
these sorts of questions are required to make their data available 
either (if the data set is small) as part of their message or (if data 
are extensive) on a web site.  Once the results of the analysis are on 
display, it is often possible to make an informed guess on the likely 
impact.  Use of simulate.lme() seems like a good idea.
John Maindonald.

On 11 Aug 2004, at 8:05 PM, r-help-request at stat.math.ethz.ch wrote:

> From: Spencer Graves <spencer.graves at pdf.com>
> Date: 10 August 2004 8:44:20 PM
> To: Gijs Plomp <gplomp at brain.riken.jp>
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Enduring LME confusion.... or Psychologists and 
> Mixed-Effects
>
>
>      Have you considered trying a Monte Carlo?  The significance 
> probabilities for unbalanced anovas use approximations.  Package nlme 
> provides "simulate.lme" to facilitate this.  I believe this function 
> is also mentioned in Pinheiro and Bates (2000).
>      hope this helps.  spencer graves
> p.s.  You could try the same thing in both library(nlme) and 
> library(lme4).  Package lme4 is newer and, at least for most cases, 
> better.
> Gijs Plomp wrote:
>
>> Dear ExpeRts,
>>
>> Suppose I have a typical psychological experiment that is a 
>> within-subjects design with multiple crossed variables and a 
>> continuous response variable. Subjects are considered a random 
>> effect. So I could model
>> > aov1 <- aov(resp~fact1*fact2+Error(subj/(fact1*fact2))
>>
>> However, this only holds for orthogonal designs with equal numbers of 
>> observation and no missing values. These assumptions are easily 
>> violated so I seek refuge in fitting a mixed-effects model with the 
>> nlme library.
>> > lme1 <- lme(resp~fact1*fact2, random=~1|subj)
>>
>> When testing the 'significance? of the effects of my factors, with 
>> anova(lme1), the degrees of freedom that lme uses in the denominator 
>> spans all observations and is identical for all factors and their 
>> interaction. I read in a previous post on the list ("[R] Help with 
>> lme basics") that this is inherent to lme. I studied the instructive 
>> book of Pinheiro & Bates and I understand why the degrees of freedom 
>> are assigned as they are, but think it may not be appropriate in this 
>> case. Used in this way it seems that lme is more prone to type 1 
>> errors than aov.
>>
>> To get more conservative degrees of freedom one could model
>> > lme2 <- lme(resp~fact1*fact2, random=~1|subj/fact1/fact2)
>>
>> But this is not a correct model because it assumes the factors to be 
>> hierarchically ordered, which they are not.
>>
>> Another alternative is to model the random effect using a matrix, as 
>> seen in "[R] lme and mixed effects" on this list.
>> > lme3 <- (resp~fact1*fact2, random=list(subj=pdIdent(form=~fact1-1), 
>>  subj=~1,  fact2=~1)
>>
>> This provides 'correct? degrees of freedom for fact1, but not for the 
>> other effects and I must confess that I don't understand this use of 
>> matrices, I?m not a statistician.
>>
>> My questions thus come down to this:
>>
>> 1. When aov?s assumptions are violated, can lme provide the right 
>> model for within-subjects designs where multiple fixed effects are 
>> NOT hierarchically ordered?
>>
>> 2. Are the degrees of freedom in anova(lme1) the right ones to 
>> report? If so, how do I convince a reviewer that, despite the large 
>> number of degrees of freedom, lme does provide a conservative 
>> evaluation of the effects? If not, how does one get the right denDf 
>> in a way that can be easily understood?
>>
>> I hope that my confusion is all due to an ignorance of statistics and 
>> that someone on this list will kindly point that out to me. I do 
>> realize that this type of question has been asked before, but think 
>> that an illuminating answer can help R spread into the psychological 
>> community.
>>
John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Bioinformation Science, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.



From stephane.dray at umontreal.ca  Thu Aug 12 01:20:37 2004
From: stephane.dray at umontreal.ca (Stephane DRAY)
Date: Wed, 11 Aug 2004 19:20:37 -0400
Subject: [R] Calling Lapack From C code
Message-ID: <5.2.1.1.0.20040811191135.00b4ce78@magellan.umontreal.ca>

Hello List,
I have create a small package with C code. In the C code, I call Fortran 
routines of Lapack. Here is the call in the C function:

F77_CALL(dgesvd)(&jobu, &jobvt,&size, &size,A, &size, 
D,U,&size,V,&size,&work1, &lwork,&error);

In the src directory, I have a Makevars File with:
PKG_LIBS =  $(LAPACK_LIBS) $(BLAS_LIBS) $(FLIBS)

And in the beginning of C code,  I include the following lines
#include <stddef.h>
#include <math.h>
#include <time.h>
#include <stdio.h>
#include <string.h>
#include <stdlib.h>
#include <assert.h>
#include <Rmath.h>
#include <R.h>
#include <R_ext/Utils.h>
#include "adesub.h"

I have compiled the Package on my PC under windows XP (Rcmd install works 
fine).
I have try to install it on another computer with R-1.9.1 running under MAC 
OS X (Panther 10.3.4), but the installation failed with the following message:

FORWARD.C:328: error: `dgesvd_' undeclared (first use this function)
FORWARD.C:328: error: (Each undeclared identifier is reported only once for
    each function it appears in.)
make: *** [FORWARD.o] Error 1
ERROR: compilation failed for package 'Packfor2'
** Removing 
'/Library/Frameworks/R.framework/Versions/1.9.1/Resources/library/Packfor2'

I have no idea why this installation does not work.

Any advice would be very appreciate.

Thanks in advance,

Sincerely.


St??phane DRAY
-------------------------------------------------------------------------------------------------- 

D??partement des Sciences Biologiques
Universit?? de Montr??al, C.P. 6128, succursale centre-ville
Montr??al, Qu??bec H3C 3J7, Canada

Tel : (514) 343-6111 poste 1233         Fax : (514) 343-2293
E-mail : stephane.dray at umontreal.ca
-------------------------------------------------------------------------------------------------- 

Web                                          http://www.steph280.freesurf.fr/



From ggrothendieck at myway.com  Thu Aug 12 05:16:04 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 12 Aug 2004 03:16:04 +0000 (UTC)
Subject: [R] on.exit() inside local()
References: <C698D707214E6F4AB39AB7096C3DE5A563D0AD@phost015.EVAFUNDS.intermedia.net>
	<loom.20040810T072222-735@post.gmane.org>
Message-ID: <loom.20040812T044226-985@post.gmane.org>

Gabor Grothendieck <ggrothendieck <at> myway.com> writes:

: 
: Vadim Ogranovich <vograno <at> evafunds.com> writes:
: 
: : 
: : Hi,
: : 
: : Since I routinely open files in a loop I've developed a habit of using
: : on.exit() to close them. Since on.exit() needs to be called within a
: : function I use eval() as a surrogate. For example:
: : 
: : for (fileName in c("a", "b")) eval({
: : 	con <- file(fileName);
: : 	on.exit(close(con))
: : 	}) 
: : 
: : and con will be closed no matter what.
: : 
: : However it stopped working once I wrapped the loop in local():
: : > local(
: : +       for (foo in seq(2)) eval({
: : +         on.exit(cat(foo, "\n"))
: : +       })
: : + )
: : Error in cat(foo, "\n") : Object "foo" not found
: : 
: : 
: : W/o local()it works just fine
: : >       for (foo in seq(2)) eval({
: : +         on.exit(cat(foo, "\n"))
: : +       })
: : 1 
: : 2 
: : 
: : The reason I wanted the local() is to keep 'foo' from interfering with
: : the existing environments, but somehow this breaks the thing.
: : At this point I am stuck. Could someone please tell what's going on?
: 
: The on.exit code is executing in an environment whose parent is
: namespace:base and so cannot access the environment created by
: local.  Use evalq, instead of eval, which has the effect of
: running the on.exit code in the environment created by
: local:
: 
: local({
:    for (i in c("a", "b")) evalq(
:          on.exit(cat(i, "\n"))
:       )
: })
: 
: or use an inner local, which has the effect of creating
: a new environment for each iteration of the loop in which
: the on.exit code runs:
: 
: local({
:    for (i in c("a", "b")) local(
:       on.exit(cat(i, "\n"))
:    )
: })


Just one additional thought.  You could use sapply in which case the
i gets hidden in the function anyways:

   junk <- sapply( c("a", "b"), function(i) 
                        on.exit(cat(i, "\n")) 
           )

or maybe:

   invisible(
      sapply( c("a", "b"), function(i) 
                   on.exit(cat(i, "\n")) 
      )
   )



From gplomp at brain.riken.jp  Thu Aug 12 10:13:14 2004
From: gplomp at brain.riken.jp (Gijs Plomp)
Date: Thu, 12 Aug 2004 17:13:14 +0900
Subject: Fwd: [R] Enduring LME =?windows-1252?Q?confusion=85_or_Psych?=
	=?windows-1252?Q?ologists_and_Mixed-Effects_?=
Message-ID: <411B269A.3080404@brain.riken.jp>

I will follow the suggestion of John Maindonald and present the problem 
by example with some data.

I also follow the advice to use mean scores, somewhat reluctantly 
though. I know it is common practice in psychology, but wouldnt it be 
more elegant if one could use all the data points in an analysis?

The data for 5 subjects (myData) are provided at the bottom of this 
message. It is a crossed within-subject design with three factors and 
reaction time (RT) as the dependent variable.

An initial repeated-measures model would be:
aov1<-aov(RT~fact1*fact2*fact3+Error(sub/(fact1*fact2*fact3)),data=myData)

Aov complains that the effects involving fact3 are unbalanced:
 > aov1
....
Stratum 4: sub:fact3
Terms:
                      fact3   Residuals
Sum of Squares  4.81639e-07 5.08419e-08
Deg. of Freedom           2           8

Residual standard error: 7.971972e-05

6 out of 8 effects not estimable
Estimated effects may be unbalanced
....
 
Presumably this is because fact3 has three levels and the other ones 
only two, making this a 'non-orthogonal design.

I then fit an equivalent mixed-effects model:
lme1<-lme(RT~fact1*fact2*fact3,data=meanData2,random=~1|sub)

Subsequently I test if my factors had any effect on RT:
 > anova(lme1)
                  numDF denDF   F-value p-value

(Intercept)           1    44 105294024  <.0001
fact1                 1    44        22  <.0001
fact2                 1    44         7  0.0090
fact3                 2    44        19  <.0001
fact1:fact2           1    44         9  0.0047
fact1:fact3           2    44         1  0.4436
fact2:fact3           2    44         1  0.2458
fact1:fact2:fact3     2    44         0  0.6660

Firstly, why are the F-values in the output whole numbers?

The effects are estimated over the whole range of the dataset and this 
is so because all three factors are nested under subjects, on the same 
level. This, however, seems to inflate the F-values compared to the 
anova(aov1) output, e.g.
 >  anova(aov1)
....
Error: sub:fact2
          Df     Sum Sq    Mean Sq F value Pr(>F)
fact2      1 9.2289e-08 9.2289e-08  2.2524 0.2078
Residuals  4 1.6390e-07 4.0974e-08
....

I realize that the (probably faulty) aov model may not be directly 
compared to the lme model, but my concern is if the lme estimation of 
the effects is right, and if so, how can a nave skeptic be convinced of 
this?

The suggestion to use simulate.lme() to find this out seems good, but I 
cant seem to get it working (from "[R] lme: simulate.lme in R" it seems 
it may never work in R).

I have also followed the suggestion to fit the exact same model with 
lme4. However, format of the anova output does not give me the 
estimation in the way nlme does. More importantly, the degrees of 
freedom in the denominator dont change, theyre still large:
 > library(lme4)
 > lme4_1<-lme(RT~fact1*fact2*fact3,random=~1|sub,data=myData)
 > anova(lme4_1)
Analysis of Variance Table

                     Df    Sum Sq   Mean Sq Denom F value    Pr(>F)  
fact1I                1 2.709e-07 2.709e-07    48 21.9205 2.360e-05 ***
fact2I                1 9.229e-08 9.229e-08    48  7.4665  0.008772 **
fact3L                1 4.906e-08 4.906e-08    48  3.9691  0.052047 .
fact3M                1 4.326e-07 4.326e-07    48 34.9972 3.370e-07 ***
fact1I:fact2I         1 1.095e-07 1.095e-07    48  8.8619  0.004552 **
fact1I:fact3L         1 8.988e-10 8.988e-10    48  0.0727  0.788577  
fact1I:fact3M         1 1.957e-08 1.957e-08    48  1.5834  0.214351  
fact2I:fact3L         1 3.741e-09 3.741e-09    48  0.3027  0.584749  
fact2I:fact3M         1 3.207e-08 3.207e-08    48  2.5949  0.113767  
fact1I:fact2I:fact3L  1 2.785e-09 2.785e-09    48  0.2253  0.637162  
fact1I:fact2I:fact3M  1 7.357e-09 7.357e-09    48  0.5952  0.444206  
---
 
I hope that by providing a sample of the data someone can help me out on 
the questions I asked in my previous mail:

 >> 1. When aovs assumptions are violated, can lme provide the right
 >> model for within-subjects designs where multiple fixed effects are
 >> NOT hierarchically ordered?
 >>
 >> 2. Are the degrees of freedom in anova(lme1) the right ones to
 >> report? If so, how do I convince a reviewer that, despite the large
 >> number of degrees of freedom, lme does provide a conservative
 >> evaluation of the effects? If not, how does one get the right denDf
 >> in a way that can be easily understood? 

If anyone thinks he can help me better by looking at the entire data 
set, I very much welcome them to email me for further discussion.

In great appreciation of your help and work for the R-community,

Gijs Plomp
g.plomp at brain.riken.jp

 

 >myData
sub fact1 fact2 fact3        RT
1   s1     C     C     G 0.9972709
2   s2     C     C     G 0.9981664
3   s3     C     C     G 0.9976909
4   s4     C     C     G 0.9976047
5   s5     C     C     G 0.9974346
6   s1     I     C     G 0.9976206
7   s2     I     C     G 0.9981980
8   s3     I     C     G 0.9980503
9   s4     I     C     G 0.9980620
10  s5     I     C     G 0.9977682
11  s1     C     I     G 0.9976633
12  s2     C     I     G 0.9981558
13  s3     C     I     G 0.9979286
14  s4     C     I     G 0.9980474
15  s5     C     I     G 0.9976030
16  s1     I     I     G 0.9977088
17  s2     I     I     G 0.9981506
18  s3     I     I     G 0.9980494
19  s4     I     I     G 0.9981183
20  s5     I     I     G 0.9976804
21  s1     C     C     L 0.9975495
22  s2     C     C     L 0.9981248
23  s3     C     C     L 0.9979146
24  s4     C     C     L 0.9974583
25  s5     C     C     L 0.9976865
26  s1     I     C     L 0.9977107
27  s2     I     C     L 0.9982071
28  s3     I     C     L 0.9980966
29  s4     I     C     L 0.9980372
30  s5     I     C     L 0.9976303
31  s1     C     I     L 0.9976152
32  s2     C     I     L 0.9982363
33  s3     C     I     L 0.9978750
34  s4     C     I     L 0.9981402
35  s5     C     I     L 0.9977018
36  s1     I     I     L 0.9978076
37  s2     I     I     L 0.9981699
38  s3     I     I     L 0.9980628
39  s4     I     I     L 0.9981092
40  s5     I     I     L 0.9977054
41  s1     C     C     M 0.9978842
42  s2     C     C     M 0.9982752
43  s3     C     C     M 0.9980277
44  s4     C     C     M 0.9978250
45  s5     C     C     M 0.9978353
46  s1     I     C     M 0.9979674
47  s2     I     C     M 0.9983277
48  s3     I     C     M 0.9981954
49  s4     I     C     M 0.9981703
50  s5     I     C     M 0.9980047
51  s1     C     I     M 0.9976940
52  s2     C     I     M 0.9983019
53  s3     C     I     M 0.9982484
54  s4     C     I     M 0.9981784
55  s5     C     I     M 0.9978177
56  s1     I     I     M 0.9978636
57  s2     I     I     M 0.9982188
58  s3     I     I     M 0.9982024
59  s4     I     I     M 0.9982358
60  s5     I     I     M 0.9978581



From david.crabb at ntu.ac.uk  Thu Aug 12 10:44:29 2004
From: david.crabb at ntu.ac.uk (Crabb, David)
Date: Thu, 12 Aug 2004 09:44:29 +0100
Subject: [R] Help with generating data from a 'not quite' Normal distriburtion
Message-ID: <E2AEBF332DE0BE43966A19AB2622749D62F28B@poplar.ads.ntu.ac.uk>

I would be very grateful for any help from members of this list for what
might be a simple problem...

We are trying to simulate the behaviour of a clinical measurement in a
series of computer experiments. This is simple enough to do in R if we
assume the measurements to be Gaussian, but their empirical distribution
has a much higher peak at the mean and the distribution has much longer
tails. (The distribution is quite symmetrical) Can anyone suggest any
distributions I could fit to this data, and better still how I can then
generate random data from this 'distribution' using R?

-----------------------------------------------
Dr. David Crabb
School of Science,
The Nottingham Trent University,
Clifton Campus, Nottingham. NG11 8NS
Tel: 0115 848 3275   Fax: 0115 848 6690



From dimitris.rizopoulos at med.kuleuven.ac.be  Thu Aug 12 10:50:52 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Thu, 12 Aug 2004 10:50:52 +0200
Subject: [R] Help with generating data from a 'not quite' Normal
	distriburtion
References: <E2AEBF332DE0BE43966A19AB2622749D62F28B@poplar.ads.ntu.ac.uk>
Message-ID: <002501c48049$80568be0$ad133a86@www.domain>

Hi David,

you could try a Student's t distribution with appropriate degrees of
freedom and extra scale paremter, i.e.,

?rt
rgt <- function(n, mu=0, sigma=1, df=stop("no df arg")) mu+sigma*rt(n,
df=df)

I hope this helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Doctoral Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "Crabb, David" <david.crabb at ntu.ac.uk>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, August 12, 2004 10:44 AM
Subject: [R] Help with generating data from a 'not quite' Normal
distriburtion


> I would be very grateful for any help from members of this list for
what
> might be a simple problem...
>
> We are trying to simulate the behaviour of a clinical measurement in
a
> series of computer experiments. This is simple enough to do in R if
we
> assume the measurements to be Gaussian, but their empirical
distribution
> has a much higher peak at the mean and the distribution has much
longer
> tails. (The distribution is quite symmetrical) Can anyone suggest
any
> distributions I could fit to this data, and better still how I can
then
> generate random data from this 'distribution' using R?
>
> -----------------------------------------------
> Dr. David Crabb
> School of Science,
> The Nottingham Trent University,
> Clifton Campus, Nottingham. NG11 8NS
> Tel: 0115 848 3275   Fax: 0115 848 6690
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From vito_ricci at yahoo.com  Thu Aug 12 10:55:40 2004
From: vito_ricci at yahoo.com (=?iso-8859-1?q?Vito=20Ricci?=)
Date: Thu, 12 Aug 2004 10:55:40 +0200 (CEST)
Subject: [R] Help with generating data from a 'not quite' Normal
	distriburtion
Message-ID: <20040812085540.69556.qmail@web41209.mail.yahoo.com>

Hi,

the Student's t distribution   could be considered:
it's symmetrical, but with a low number of degree of
freedom is different from Normal distribution I think
in the way you said:"has a much higher peak at the
mean and the distribution has much longer
tails. " Try to use:

rt(n, df) where n=number of obs. df=degree of freedom.

for samples simulations.

Best
Vito


I would be very grateful for any help from members of
this list for what
might be a simple problem...

We are trying to simulate the behaviour of a clinical
measurement in a
series of computer experiments. This is simple enough
to do in R if we
assume the measurements to be Gaussian, but their
empirical distribution
has a much higher peak at the mean and the
distribution has much longer
tails. (The distribution is quite symmetrical) Can
anyone suggest any
distributions I could fit to this data, and better
still how I can then
generate random data from this 'distribution' using R?

-----------------------------------------------
Dr. David Crabb
School of Science,
The Nottingham Trent University,
Clifton Campus, Nottingham. NG11 8NS
Tel: 0115 848 3275   Fax: 0115 848 6690

=====
Diventare costruttori di soluzioni

Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese http://www.modugno.it/archivio/cat_palese.shtml



From vito_ricci at yahoo.com  Thu Aug 12 10:59:23 2004
From: vito_ricci at yahoo.com (=?iso-8859-1?q?Vito=20Ricci?=)
Date: Thu, 12 Aug 2004 10:59:23 +0200 (CEST)
Subject: [R] Help with generating data from a 'not quite' Normal
	distriburtion
Message-ID: <20040812085923.69901.qmail@web41209.mail.yahoo.com>

Hi,

Also the Cauchy's distribution could be good:

rcauchy(n, location = 0, scale = 1)


Best
Vito


I would be very grateful for any help from members of
this list for what
might be a simple problem...

We are trying to simulate the behaviour of a clinical
measurement in a
series of computer experiments. This is simple enough
to do in R if we
assume the measurements to be Gaussian, but their
empirical distribution
has a much higher peak at the mean and the
distribution has much longer
tails. (The distribution is quite symmetrical) Can
anyone suggest any
distributions I could fit to this data, and better
still how I can then
generate random data from this 'distribution' using R?

-----------------------------------------------
Dr. David Crabb
School of Science,
The Nottingham Trent University,
Clifton Campus, Nottingham. NG11 8NS
Tel: 0115 848 3275   Fax: 0115 848 6690

=====
Diventare costruttori di soluzioni

Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese http://www.modugno.it/archivio/cat_palese.shtml



From Rau at demogr.mpg.de  Thu Aug 12 11:36:40 2004
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Thu, 12 Aug 2004 11:36:40 +0200
Subject: [R] Giving a first good impression of R to Social Scientists
Message-ID: <3699CDBC4ED5D511BE6400306E1C0D81030A0BE3@hermes.demogr.mpg.de>

Dear all,

in the coming Winter Semester, I will be a teaching assistant for a course
in Survival Analysis. My job will be to do the lab sessions. The software
used for these lab sessions will be R. Most of the students have a
background in social sciences and the only stats package they used so far is
most likely SPSS.
So I assume they might be quite surprised the first time they see R ("where
is my rectangular data window?", "where do I have to click to make a new
variable?", ...).

That is why would like to ask the experts on this list if anyone of you has
encountered a similar experience and what you could advise to persuade
people quickly that it is worth learning a new software?
I imagined to give them a short presentation about the nice capabilities
what R can do which would be impossible or troublesome with conventional
software like SPSS.[1] The reason is that I want to create an atmosphere
where people have a positive attitude towards learning a new software right
from the beginning. This would make it easier for me and I guess also the
students learn more and faster if they have a positive first encounter with
R.

(Afterwards I plan to introduce them to the basics of R with the help of
Venables/Smith/R Core Team: "An Introduction to R", Dalgaard "Introductory
Statistics with R" and Krause/Olson "The Basics of S and S-Plus" before
doing any survival analysis relevant exercises.)

I would appreciate any suggestions!

Thanks for your help,
Roland


[1] I originally thought to show them how easy it is to estimate in R a
Kaplan-Meier-Survival curve in the presence of left truncation, whereas I
have seen no possibility so far to do that in SPSS (that could be also due
to my lack of knowledge using SPSS) but the KM-estimator is a topic during
the course, so I can not use this example.


+++++
This mail has been sent through the MPI for Demographic Rese...{{dropped}}



From vito_ricci at yahoo.com  Thu Aug 12 11:50:24 2004
From: vito_ricci at yahoo.com (=?iso-8859-1?q?Vito=20Ricci?=)
Date: Thu, 12 Aug 2004 11:50:24 +0200 (CEST)
Subject: [R] Giving a first good impression of R to Social Scientists
Message-ID: <20040812095024.69416.qmail@web41201.mail.yahoo.com>

Hi,

do you know there are several GUI for R? See:

http://www.sciviews.org/_rgui/

and in particular:

http://socserv.mcmaster.ca/jfox/Misc/Rcmdr/

R-Commander is quite like GUI of commercial softwares.
Give a look: they can help your pupils which are able
to use SPSS.
Although I prefer command line, for me is simplier.

Best

Vito
 

Dear all,

in the coming Winter Semester, I will be a teaching
assistant for a course
in Survival Analysis. My job will be to do the lab
sessions. The software
used for these lab sessions will be R. Most of the
students have a
background in social sciences and the only stats
package they used so far is
most likely SPSS.
So I assume they might be quite surprised the first
time they see R ("where
is my rectangular data window?", "where do I have to
click to make a new
variable?", ...).

That is why would like to ask the experts on this list
if anyone of you has
encountered a similar experience and what you could
advise to persuade
people quickly that it is worth learning a new
software?
I imagined to give them a short presentation about the
nice capabilities
what R can do which would be impossible or troublesome
with conventional
software like SPSS.[1] The reason is that I want to
create an atmosphere
where people have a positive attitude towards learning
a new software right
from the beginning. This would make it easier for me
and I guess also the
students learn more and faster if they have a positive
first encounter with
R.

(Afterwards I plan to introduce them to the basics of
R with the help of
Venables/Smith/R Core Team: "An Introduction to R",
Dalgaard "Introductory
Statistics with R" and Krause/Olson "The Basics of S
and S-Plus" before
doing any survival analysis relevant exercises.)

I would appreciate any suggestions!

Thanks for your help,
Roland


[1] I originally thought to show them how easy it is
to estimate in R a
Kaplan-Meier-Survival curve in the presence of left
truncation, whereas I
have seen no possibility so far to do that in SPSS
(that could be also due
to my lack of knowledge using SPSS) but the
KM-estimator is a topic during
the course, so I can not use this example.



http://socserv.mcmaster.ca/jfox/Misc/Rcmdr/

=====
Diventare costruttori di soluzioni

Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese http://www.modugno.it/archivio/cat_palese.shtml



From Rau at demogr.mpg.de  Thu Aug 12 12:19:30 2004
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Thu, 12 Aug 2004 12:19:30 +0200
Subject: [R] RE: Giving a first good impression of R to Social Scientists
Message-ID: <3699CDBC4ED5D511BE6400306E1C0D81030A0BE5@hermes.demogr.mpg.de>

Hi,

> -----Original Message-----
> From:	Vito Ricci [SMTP:vito_ricci at yahoo.com]
> do you know there are several GUI for R? See:
	[...]
> R-Commander is quite like GUI of commercial softwares.
> 
> 
	Yes, I do know the R-Commander. But I did not want to give them a
GUI but rather expose them to the command line after I demonstrated that the
steep learning curve in the beginning is worth the effort for the final
results.

	That is why I wanted to ask the list if anyone has faced the same
situation to persuade students to use R. Are social science students most
impressionable with some nice graphs (e.g. filled.contour) or will they get
a more positive attitude if I used the "R as an overgrown calculator" like
in Peter Dalgaards book? Or should I write an SPSS script to perform a
certain task and demonstrate how easy, compact, and elegant it is to fulfill
the same job in R? Just telling them "We will use R during our course"
without any explanation would be not a good choice in my opinion.

	As I have written before: I would like the students to trust me that
it is worth to invest some extra energy in the beginning. I do not expect to
receive any prepared demonstration from anyone of you. I am more curious
about your teaching experiences and how you got people enthusiastic to use
this software.

	Thanks,
	Roland



+++++
This mail has been sent through the MPI for Demographic Rese...{{dropped}}



From susana.barbosa at fc.up.pt  Thu Aug 12 12:29:28 2004
From: susana.barbosa at fc.up.pt (susana barbosa)
Date: Thu, 12 Aug 2004 11:29:28 +0100
Subject: [R] Giving a first good impression of R to Social Scientists
Message-ID: <200408121129.28664.susana.barbosa@fc.up.pt>


Hi,

I have encountered big difficulties trying to persuade my undergraduate
students, with very "slight" background either in statistics or computing to
use R instead of SPSS.

 I tried to start with a sort of very, very simple sample session, just for
showing that R is not as complicated as they think and it is worth trying....
The idea was to give them a kind of simple lab guide they could follow at
their own pace, and stimulate them to search in help files, etc... I think it
is important that they get used to try around the examples and documentation
to solve specific problems...


Best
Susana


--
Susana Barbosa
Departamento de Matematica Aplicada
Faculdade de Ci??ncias, Universidade Porto
Rua do Campo Alegre, 687, 4169-007, Porto
Tel: 220 100 840
Fax: 220 100 809

-------------------------------------------------------

-- 
Susana Barbosa
Departamento de Matematica Aplicada
Faculdade de Ci??ncias, Universidade Porto
Rua do Campo Alegre, 687, 4169-007, Porto
Tel: 220 100 840
Fax: 220 100 809



From rn001 at cebas.csic.es  Thu Aug 12 12:53:28 2004
From: rn001 at cebas.csic.es (javier garcia - CEBAS)
Date: Thu, 12 Aug 2004 12:53:28 +0200
Subject: [R] error using daisy() in library(cluster). Bug?
Message-ID: <20040812105252.61433A7AC4@cebas.csic.es>

Hi,
I'm using the cluster library to examine multivariate data.
The data come from a connection to a postgres database, and I did a short R 
script to do the analisys. With the cluster version included in R1.8.0, daisy 
worked well for my data, but now, when I call daisy, I obtain the following 
messages:
---------
Error in if (any(sx == 0)) { : missing value where TRUE/FALSE needed
In addition: Warning message:
binary variable(s) 116 treated as interval scaled in: 
daisy(concentracion.data.frame, stand = TRUE)
---------

Al the variables in my dataframe are numeric. Although I've got NA values, 
and I've seen that if a do the analisys for a subset of the dataframe, 
selecting just columns with no NA, the result is good.
Could this be a bug?

Thanks, and best regards

Javier



From HDoran at air.org  Thu Aug 12 13:22:39 2004
From: HDoran at air.org (Doran, Harold)
Date: Thu, 12 Aug 2004 07:22:39 -0400
Subject: =?utf-8?Q?RE=3A_=5BR=5D_Enduring_LME_confusion=E2=80=A6_?=
	=?utf-8?Q?or_Psychologists_and_Mixed-Effec?= =?utf-8?Q?ts?=
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7404044CEC@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040812/b9f5152c/attachment.pl

From maechler at stat.math.ethz.ch  Thu Aug 12 13:41:56 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 12 Aug 2004 13:41:56 +0200
Subject: [R] Help with generating data from a 'not quite' Normal
	distriburtion
In-Reply-To: <20040812085923.69901.qmail@web41209.mail.yahoo.com>
References: <20040812085923.69901.qmail@web41209.mail.yahoo.com>
Message-ID: <16667.22404.220467.285444@gargle.gargle.HOWL>

>>>>> "Vito" == Vito Ricci <vito_ricci at yahoo.com>
>>>>>     on Thu, 12 Aug 2004 10:59:23 +0200 (CEST) writes:

    Vito> Hi, Also the Cauchy's distribution could be good:

    Vito> rcauchy(n, location = 0, scale = 1)

"also" is an exaggeration, after you already told him to use the
t-distribution family:

Cauchy = t-Dist(*, df = 1) !


    DCrabb> I would be very grateful for any help from members of
    DCrabb> this list for what might be a simple problem...

    DCrabb> We are trying to simulate the behaviour of a clinical
    DCrabb> measurement in a series of computer experiments. This
    DCrabb> is simple enough to do in R if we assume the
    DCrabb> measurements to be Gaussian, but their empirical
    DCrabb> distribution has a much higher peak at the mean and
    DCrabb> the distribution has much longer tails. (The
    DCrabb> distribution is quite symmetrical) Can anyone suggest
    DCrabb> any distributions I could fit to this data, and better
    DCrabb> still how I can then generate random data from this
    DCrabb> 'distribution' using R?

I'd first try with the t distribution, using  fitdistr() from
package MASS, e.g.,

  > x <- rt(1000, df = 1.5)
  > library(MASS)
  > fx <- fitdistr(x, densfun = "t")
  > fx
	  m             s            df     
    -0.01396785    1.04338151    1.57749052 
   ( 0.04426267) ( 0.04766543) ( 0.10809543)
  > 

(so it *does* estimate location and scale in addition to the df).

If you read the help page
  > ?fitdistr

you'll see in the example that estimating 'df' is said to be
problematic.
AFAIK it can be better to reparametrize, possibly using 1/df or
log(df) as new parameter.
{but then you can't use fitdistr() but rather mle() and the
 log likelihood or optim() directly}.

Martin Maechler



From ripley at stats.ox.ac.uk  Thu Aug 12 14:04:38 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 12 Aug 2004 13:04:38 +0100 (BST)
Subject: =?utf-8?Q?RE=3A_=5BR=5D_Enduring_LME_confusion=E2=80=A6_?=
	=?utf-8?Q?or_Psychologists_and_Mixed-Effec?= =?utf-8?Q?ts?=
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F7404044CEC@dc1ex2.air.org>
Message-ID: <Pine.LNX.4.44.0408121302050.16094-100000@gannet.stats>

On Thu, 12 Aug 2004, Doran, Harold wrote:

>  lme fits models using restricted maximum likelihood by default. So, I
> believe this is why you have a different DF. If you include method="ML"
> in the modeling function the DF should be similar to aov.

It is REML and not ML that generalizes the classical multistratum AoV.
In any case, REML and ML use the same subspaces, just different methods 
for estimating variances within them.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Aug 12 14:08:03 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 12 Aug 2004 13:08:03 +0100 (BST)
Subject: [R] Help with generating data from a 'not quite' Normal
	distriburtion
In-Reply-To: <16667.22404.220467.285444@gargle.gargle.HOWL>
Message-ID: <Pine.LNX.4.44.0408121305010.16094-100000@gannet.stats>

On Thu, 12 Aug 2004, Martin Maechler wrote:

> >>>>> "Vito" == Vito Ricci <vito_ricci at yahoo.com>
> >>>>>     on Thu, 12 Aug 2004 10:59:23 +0200 (CEST) writes:
> 
>     Vito> Hi, Also the Cauchy's distribution could be good:
> 
>     Vito> rcauchy(n, location = 0, scale = 1)
> 
> "also" is an exaggeration, after you already told him to use the
> t-distribution family:
> 
> Cauchy = t-Dist(*, df = 1) !
> 
> 
>     DCrabb> I would be very grateful for any help from members of
>     DCrabb> this list for what might be a simple problem...
> 
>     DCrabb> We are trying to simulate the behaviour of a clinical
>     DCrabb> measurement in a series of computer experiments. This
>     DCrabb> is simple enough to do in R if we assume the
>     DCrabb> measurements to be Gaussian, but their empirical
>     DCrabb> distribution has a much higher peak at the mean and
>     DCrabb> the distribution has much longer tails. (The
>     DCrabb> distribution is quite symmetrical) Can anyone suggest
>     DCrabb> any distributions I could fit to this data, and better
>     DCrabb> still how I can then generate random data from this
>     DCrabb> 'distribution' using R?
> 
> I'd first try with the t distribution, using  fitdistr() from
> package MASS, e.g.,
> 
>   > x <- rt(1000, df = 1.5)
>   > library(MASS)
>   > fx <- fitdistr(x, densfun = "t")
>   > fx
> 	  m             s            df     
>     -0.01396785    1.04338151    1.57749052 
>    ( 0.04426267) ( 0.04766543) ( 0.10809543)
>   > 
> 
> (so it *does* estimate location and scale in addition to the df).
> 
> If you read the help page
>   > ?fitdistr
> 
> you'll see in the example that estimating 'df' is said to be
> problematic.
> AFAIK it can be better to reparametrize, possibly using 1/df or
> log(df) as new parameter.
> {but then you can't use fitdistr() but rather mle() and the
>  log likelihood or optim() directly}.

It is the use of ML for the df that is *in theory* problematic, not the
optimization per se.  See the reference, p.110, for some of the 
literature.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Thu Aug 12 14:24:09 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 12 Aug 2004 14:24:09 +0200
Subject: [R] error using daisy() in library(cluster). Bug?
In-Reply-To: <20040812105252.61433A7AC4@cebas.csic.es>
References: <20040812105252.61433A7AC4@cebas.csic.es>
Message-ID: <16667.24937.604461.865258@gargle.gargle.HOWL>

??Hola Javier!

since I am the maintainer of the cluster  
*package* (not "library"), I'm interested to find out more about
this problem.  I assume, you now use R 1.9.1.

Can you give us an example we can reproduce?
Give the exact R commands you use and 
maybe attach the save()d data file (*.rda) in a private e-mail?

Or do this on R-help and give an URL where one can download the
data (you can't attach such binary files for R-help).

Thank you,
Martin Maechler

>>>>> "javier" == javier garcia <- CEBAS <rn001 at cebas.csic.es>>
>>>>>     on Thu, 12 Aug 2004 12:53:28 +0200 writes:

    javier> Hi, I'm using the cluster library to examine
    javier> multivariate data.  The data come from a connection
    javier> to a postgres database, and I did a short R script
    javier> to do the analisys. With the cluster version
    javier> included in R1.8.0, daisy worked well for my data,
    javier> but now, when I call daisy, I obtain the following
    javier> messages: --------- Error in if (any(sx == 0)) { :
    javier> missing value where TRUE/FALSE needed In addition:
    javier> Warning message: binary variable(s) 116 treated as
    javier> interval scaled in: daisy(concentracion.data.frame,
    javier> stand = TRUE) ---------

    javier> Al the variables in my dataframe are
    javier> numeric. Although I've got NA values, and I've seen
    javier> that if a do the analisys for a subset of the
    javier> dataframe, selecting just columns with no NA, the
    javier> result is good.  Could this be a bug?

    javier> Thanks, and best regards

    javier> Javier



From john.maindonald at anu.edu.au  Thu Aug 12 14:49:31 2004
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Thu, 12 Aug 2004 22:49:31 +1000
Subject: [R] Re: R-help Digest, Vol 18, Issue 12
In-Reply-To: <200408121003.i7CA3ThZ012507@hypatia.math.ethz.ch>
References: <200408121003.i7CA3ThZ012507@hypatia.math.ethz.ch>
Message-ID: <0DEFB40B-EC5E-11D8-B4C5-000A95CDA0F2@anu.edu.au>

The message for aov1 was "Estimated effects <may> be unbalanced".  The  
effects are not unbalanced.  The design is 'orthogonal'.

The problem is that there are not enough degrees of freedom to estimate  
all those error terms.  If you change the model to:
   aov1 <-  
aov(RT~fact1*fact2*fact3+Error(sub/(fact1+fact2+fact3)),data=myData)

or to

   aov2 <-  
aov(RT~fact1*fact2*fact3+Error(sub/ 
((fact1+fact2+fact3)^2)),data=myData)

all is well.  This last model (aov2) seems to me to have an excessive  
number of error terms.

The lme model lme(RT~fact1*fact2*fact3, random=~1|sub, data=myData)
is equivalent to aov0 <- aov(RT~fact1*fact2*fact3+Error(sub),  
data=myData)
It can be verified that the estimated variance components can be used  
to reproduce the mean squares in the anova table.

A conservative approach is to estimate e.g. contrasts involving fact1  
for each subject separately, then comparing these against SE estimates  
that have 4df (5 subjects -1).  This is the ultimate check -- are  
claimed effects consistent against the 5 subjects?  The standard errors  
should though, probably be based on some variety of averaged variance.

I do not know how to set up the equivalents of aov1 and aov2 (where the  
errors are indeed crossed) in lme4.

John Maindonald.

On 12 Aug 2004, at 8:03 PM, r-help-request at stat.math.ethz.ch wrote:

> I will follow the suggestion of John Maindonald and present the  
> problem by example with some data.
>
> I also follow the advice to use mean scores, somewhat reluctantly  
> though. I know it is common practice in psychology, but wouldnt it be  
> more elegant if one could use all the data points in an analysis?
>
> The data for 5 subjects (myData) are provided at the bottom of this  
> message. It is a crossed within-subject design with three factors and  
> reaction time (RT) as the dependent variable.
>
> An initial repeated-measures model would be:
> aov1<-aov(RT~fact1*fact2*fact3+Error(sub/ 
> (fact1*fact2*fact3)),data=myData)
>
> Aov complains that the effects involving fact3 are unbalanced:
> > aov1
> ....
> Stratum 4: sub:fact3
> Terms:
>                      fact3   Residuals
> Sum of Squares  4.81639e-07 5.08419e-08
> Deg. of Freedom           2           8
>
> Residual standard error: 7.971972e-05
>
> 6 out of 8 effects not estimable
> Estimated effects may be unbalanced
> ....
> Presumably this is because fact3 has three levels and the other ones  
> only two, making this a 'non-orthogonal design.
>
> I then fit an equivalent mixed-effects model:
> lme1<-lme(RT~fact1*fact2*fact3,data=meanData2,random=~1|sub)
>
> Subsequently I test if my factors had any effect on RT:
> > anova(lme1)
>                  numDF denDF   F-value p-value
>
> (Intercept)           1    44 105294024  <.0001
> fact1                 1    44        22  <.0001
> fact2                 1    44         7  0.0090
> fact3                 2    44        19  <.0001
> fact1:fact2           1    44         9  0.0047
> fact1:fact3           2    44         1  0.4436
> fact2:fact3           2    44         1  0.2458
> fact1:fact2:fact3     2    44         0  0.6660
>
> Firstly, why are the F-values in the output whole numbers?
>
> The effects are estimated over the whole range of the dataset and this  
> is so because all three factors are nested under subjects, on the same  
> level. This, however, seems to inflate the F-values compared to the  
> anova(aov1) output, e.g.
> >  anova(aov1)
> ....
> Error: sub:fact2
>          Df     Sum Sq    Mean Sq F value Pr(>F)
> fact2      1 9.2289e-08 9.2289e-08  2.2524 0.2078
> Residuals  4 1.6390e-07 4.0974e-08
> ....
>
> I realize that the (probably faulty) aov model may not be directly  
> compared to the lme model, but my concern is if the lme estimation of  
> the effects is right, and if so, how can a nave skeptic be convinced  
> of this?
>
> The suggestion to use simulate.lme() to find this out seems good, but  
> I cant seem to get it working (from "[R] lme: simulate.lme in R" it  
> seems it may never work in R).
>
> I have also followed the suggestion to fit the exact same model with  
> lme4. However, format of the anova output does not give me the  
> estimation in the way nlme does. More importantly, the degrees of  
> freedom in the denominator dont change, theyre still large:
> > library(lme4)
> > lme4_1<-lme(RT~fact1*fact2*fact3,random=~1|sub,data=myData)
> > anova(lme4_1)
> Analysis of Variance Table
>
>                     Df    Sum Sq   Mean Sq Denom F value    Pr(>F)   
> fact1I                1 2.709e-07 2.709e-07    48 21.9205 2.360e-05  
> ***
> fact2I                1 9.229e-08 9.229e-08    48  7.4665  0.008772 **
> fact3L                1 4.906e-08 4.906e-08    48  3.9691  0.052047 .
> fact3M                1 4.326e-07 4.326e-07    48 34.9972 3.370e-07 ***
> fact1I:fact2I         1 1.095e-07 1.095e-07    48  8.8619  0.004552 **
> fact1I:fact3L         1 8.988e-10 8.988e-10    48  0.0727  0.788577   
> fact1I:fact3M         1 1.957e-08 1.957e-08    48  1.5834  0.214351   
> fact2I:fact3L         1 3.741e-09 3.741e-09    48  0.3027  0.584749   
> fact2I:fact3M         1 3.207e-08 3.207e-08    48  2.5949  0.113767   
> fact1I:fact2I:fact3L  1 2.785e-09 2.785e-09    48  0.2253  0.637162   
> fact1I:fact2I:fact3M  1 7.357e-09 7.357e-09    48  0.5952  0.444206   
> ---
> I hope that by providing a sample of the data someone can help me out  
> on the questions I asked in my previous mail:
>
John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Bioinformation Science, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.



From kahra at mpsgr.it  Thu Aug 12 15:06:15 2004
From: kahra at mpsgr.it (Kahra Hannu)
Date: Thu, 12 Aug 2004 15:06:15 +0200
Subject: [R] Help with generating data from a 'not quite' Normal
	distriburtion
Message-ID: <C9FC71F7E9356F40AFE2ACC2099DE14714963C@MAILSERVER-B.mpsgr.it>

Consider using the HyperbolicDist package. With the package you can both fit the hyperbolic distribution to your data and generate random numbers from the distribution. Hyperbolic distribution/s provide/s good fit to financial returns that commonly exhibit high peaks and heavy tails.

Hannu Kahra  

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Crabb, David
Sent: Thursday, August 12, 2004 10:44 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Help with generating data from a 'not quite' Normal
distriburtion


I would be very grateful for any help from members of this list for what
might be a simple problem...

We are trying to simulate the behaviour of a clinical measurement in a
series of computer experiments. This is simple enough to do in R if we
assume the measurements to be Gaussian, but their empirical distribution
has a much higher peak at the mean and the distribution has much longer
tails. (The distribution is quite symmetrical) Can anyone suggest any
distributions I could fit to this data, and better still how I can then
generate random data from this 'distribution' using R?

-----------------------------------------------
Dr. David Crabb
School of Science,
The Nottingham Trent University,
Clifton Campus, Nottingham. NG11 8NS
Tel: 0115 848 3275   Fax: 0115 848 6690

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From tlumley at u.washington.edu  Thu Aug 12 17:02:14 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 12 Aug 2004 08:02:14 -0700 (PDT)
Subject: [R] Giving a first good impression of R to Social Scientists
In-Reply-To: <3699CDBC4ED5D511BE6400306E1C0D81030A0BE3@hermes.demogr.mpg.de>
References: <3699CDBC4ED5D511BE6400306E1C0D81030A0BE3@hermes.demogr.mpg.de>
Message-ID: <Pine.A41.4.58.0408120734260.73838@homer09.u.washington.edu>

On Thu, 12 Aug 2004, Rau, Roland wrote:
>
> That is why would like to ask the experts on this list if anyone of you has
> encountered a similar experience and what you could advise to persuade
> people quickly that it is worth learning a new software?

One problem is that it may not be true.  Unless these people are going to
be doing their own statistics in the future (which is probably true only
for a minority) they might actually be better off with a point and click
interface.  I'm (obviously) not arguing that SPSS is a better statistical
environment than R, but it is easier to learn, and in 10 or 15 weeks they
may not get to see the benefits of R.


	-thomas



From B.Rowlingson at lancaster.ac.uk  Thu Aug 12 17:24:28 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Thu, 12 Aug 2004 16:24:28 +0100
Subject: [R] Giving a first good impression of R to Social Scientists
In-Reply-To: <Pine.A41.4.58.0408120734260.73838@homer09.u.washington.edu>
References: <3699CDBC4ED5D511BE6400306E1C0D81030A0BE3@hermes.demogr.mpg.de>
	<Pine.A41.4.58.0408120734260.73838@homer09.u.washington.edu>
Message-ID: <411B8BAC.2050906@lancaster.ac.uk>

Thomas Lumley wrote:
> On Thu, 12 Aug 2004, Rau, Roland wrote:
> 
>>That is why would like to ask the experts on this list if anyone of you has
>>encountered a similar experience and what you could advise to persuade
>>people quickly that it is worth learning a new software?
> 

  The usual way of teaching R seems to be bottom-up. Here's the command 
prompt, type some arithmetic, make some assignments, learn about 
function calls and arguments, write your own functions, write your own 
packages.

  Perhaps a top-down approach might help certain cases. People using 
point-n-click packages tend to use a limited range of analyses. Write 
some functions that do these analyses, or give them wrappers so that 
they get something like:

  > myData = readDataFile("foo.dat")
    Read 4 variables: Z, Age, Sex, Disease

  > analyseThis(myData, response="Z", covariate="Age")

   Z = 0.36 * Age, Significance level = 0.932

  or whatever. Really spoon feed the things they need to do. Make it 
really easy, foolproof.

  Then show them what's behind the analyseThis() function. How its not 
even part of the R distribution. How easy you made it for a beginner to 
do a complex and novel analysis. Then maybe it'll "click" for them, and 
they'll see how having a programming language behind their statistics 
functions lets them explore in ways not thought possible with the 
point-n-click paradigm. Perhaps they'll start editing analyseThis() and 
write analyseThat(), start thinking for themselves.

  Or maybe they'll just stare at you blankly...

Baz



From jg_liao at yahoo.com  Thu Aug 12 17:28:18 2004
From: jg_liao at yahoo.com (Jason Liao)
Date: Thu, 12 Aug 2004 08:28:18 -0700 (PDT)
Subject: [R] truly object oriented programming in R
Message-ID: <20040812152818.69617.qmail@web53706.mail.yahoo.com>

Good morning! I recently implemented a KD tree in JAVA for faster
kernel density estimation (part of the code follows). It went well. To
hook it with R, however, has proved more difficult. My question is: is
it possible to implement the algorithm in R? My impression seems to
indicate no as the code requires a complete class-object framework that
R does not support. But is there an R package or something that may
make it possible? Thanks in advance for your help.

Java implementation of KD tree:

public class Kdnode {
        	
        private double[] center; //center of the bounding box
        private double diameter; //maximum distance from center to
anywhere within the bounding box
        private int numOfPoints; //number of source data points in the
bounding box 
        
        private Kdnode left, right;

	
	public Kdnode(double[][] points, int split_dim, int [][]
sortedIndices, double[][] bBox) {
           //bBox: the bounding box, 1st row the lower bound, 2nd row
the upper bound 
                numOfPoints = points.length;
		int d = points[0].length;				
                
                center = new double[d];
                for(int j=0; j<d; j++) center[j] =
(bBox[0][j]+bBox[1][j])/2.;
                diameter = get_diameter(bBox);
                
		if(numOfPoints==1) {
                  diameter = 0.;
                  for(int j=0; j<d; j++) center[j] = points[0][j];
		  left = null;
		  right = null;	                  
		}
		else { 			  
                  int middlePoint =
sortedIndices[split_dim][numOfPoints/2];                      
		  double splitValue = points[middlePoint][split_dim];
                
                  middlePoint =
sortedIndices[split_dim][numOfPoints/2-1]; 
                  double splitValue_small =
points[middlePoint][split_dim]; 

		  int left_size = numOfPoints/2;
                  int right_size = numOfPoints - left_size;			

		  double[][] leftPoints = new double[left_size][d];
                  double[][] rightPoints = new double[right_size][d];		


		  int[][] leftSortedIndices = new int[d][left_size];				
		  int[][] rightSortedIndices = new int[d][right_size];
				
		  int left_counter = 0, right_counter = 0;				
		  int[] splitInfo = new int [numOfPoints];
				
		  for(int i = 0; i < numOfPoints; i++) {
		    if(points[i][split_dim] < splitValue) {
			for(int j=0; j<d; j++) leftPoints[left_counter][j] = points[i][j];
		       	splitInfo[i] = right_counter;
                        left_counter++;
                    }
		
		    else {
			for(int j=0; j<d; j++) rightPoints[right_counter][j] = points[i][j];
			splitInfo[i] = left_counter; 
                        right_counter++;
                    }
                  }
			// modify appropriately the indices to correspond to the new lists
			for(int i = 0; i < d; i++) {
				int left_index = 0, right_index = 0;
				for(int j = 0; j < numOfPoints; j++) {
					if(points[sortedIndices[i][j]][split_dim] < splitValue) 
						leftSortedIndices[i][left_index++] = sortedIndices[i][j] -
splitInfo[sortedIndices[i][j]];					
					else    rightSortedIndices[i][right_index++] = sortedIndices[i][j]
- splitInfo[sortedIndices[i][j]];
                                }				
			}
			
			// Recursively compute the kdnodes for the points in the two
splitted spaces			                        
			double[][] leftBBox = new double[2][];
			double[][] rightBBox = new double[2][];      
                        
                        for(int i=0; i<2; i++) {
                                leftBBox[i] =
(double[])bBox[i].clone();
                                rightBBox[i] =
(double[])bBox[i].clone();                              
                            }
                        
                        leftBBox[1][split_dim] = splitValue_small;
                        rightBBox[0][split_dim] = splitValue; 
			
                        int next_dim = (split_dim + 1) % (d);
			left = new Kdnode(leftPoints, next_dim, leftSortedIndices,
leftBBox);			
			right = new Kdnode(rightPoints, next_dim, rightSortedIndices,
rightBBox);
		}
	}
	
      
        public double evaluate(double[] target, double delta, double
bandwidth) throws Exception
        {         
            
             double dis_2_center = Common.distance(target,
center)/bandwidth;
             double dm = diameter/bandwidth;                       
             
             if(dis_2_center >= 1+dm) return 0.; 
             if(numOfPoints==1) return Common.K(dis_2_center);
             
             /*if(dis_2_center<1) 
             {
                 double temp2 = dm*Common.KDeriv(dis_2_center);  
                 if(temp2<delta) return
Common.K(dis_2_center)*numOfPoints;
             } */
                          
             return left.evaluate(target,delta, bandwidth) +
right.evaluate(target,delta, bandwidth); 
        }    
        
                 
         public double get_diameter(double[][] bBox)
        {
            double value = 0., diff;
            for (int i=0; i<bBox[0].length;i++)
            {
                diff = (bBox[1][i] - bBox[0][i])/2.;
                value += diff*diff;
            }
            return Math.sqrt(value);
        }       
}

=====
Jason Liao, http://www.geocities.com/jg_liao
Dept. of Biostatistics, http://www2.umdnj.edu/bmtrxweb
University of Medicine and Dentistry of New Jersey
phone 732-235-5429, School of Public Health office
phone 732-235-8611, Cancer Institute of New Jersey office
moble phone 908-720-4205



From ggrothendieck at myway.com  Thu Aug 12 17:40:52 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 12 Aug 2004 15:40:52 +0000 (UTC)
Subject: [R] truly object oriented programming in R
References: <20040812152818.69617.qmail@web53706.mail.yahoo.com>
Message-ID: <loom.20040812T173739-400@post.gmane.org>

Jason Liao <jg_liao <at> yahoo.com> writes:

: 
: Good morning! I recently implemented a KD tree in JAVA for faster
: kernel density estimation (part of the code follows). It went well. To
: hook it with R, however, has proved more difficult. My question is: is
: it possible to implement the algorithm in R? My impression seems to
: indicate no as the code requires a complete class-object framework that
: R does not support. But is there an R package or something that may
: make it possible? Thanks in advance for your help.

R comes with the S3 and S4 object systems out-of-the-box and there is an
addon package oo.R available at:

   http://www.maths.lth.se/help/R/R.classes/ 

that provides a more conventional OO system.   Its likely that one or more
of these would satisfy your requirements.



From kahra at mpsgr.it  Thu Aug 12 17:56:05 2004
From: kahra at mpsgr.it (Kahra Hannu)
Date: Thu, 12 Aug 2004 17:56:05 +0200
Subject: [R] linear constraint optim with bounds/reparametrization
Message-ID: <C9FC71F7E9356F40AFE2ACC2099DE14714963D@MAILSERVER-B.mpsgr.it>

>From Spencer Graves:

>However, for an equality constraint, I've had good luck by with an objective function that adds something like the
>following to my objective function: constraintViolationPenalty*(A%*%theta-c)^2, where "constraintViolationPenalty" is
>passed via "..." in a call to optim.

I applied Spencer's suggestion to a set of eight different constrained portfolio optimization problems. It seems to give a usable practice to solve the portfolio problem, when the QP optimizer is not applicable. After all, practical portfolio management is more an art than a science.   

>I may first run optim with a modest value for constraintViolationPenalty then restart it with the output of the 
>initial run as starting values and with a larger value for constraintViolationPenalty. 

I wrote a loop that starts with a small value for the penalty and stops when the change of the function value, when increasing the penalty, is less than epsilon. I found that epsilon = 1e-06 provides a reasonable accuracy with respect to computational time.

Spencer, many thanks for your suggestion.

Hannu Kahra



From maechler at stat.math.ethz.ch  Thu Aug 12 17:59:21 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 12 Aug 2004 17:59:21 +0200
Subject: [R] error using daisy() in library(cluster). Bug?
In-Reply-To: <16667.35009.755244.52927@gargle.gargle.HOWL>
References: <20040812105252.61433A7AC4@cebas.csic.es>
	<20040812124455.59D0CA7AC3@cebas.csic.es>
	<16667.27758.210695.428385@gargle.gargle.HOWL>
	<20040812142750.481B1A7AC4@cebas.csic.es>
	<16667.35009.755244.52927@gargle.gargle.HOWL>
Message-ID: <16667.37849.634789.455341@gargle.gargle.HOWL>

[Reverted back to R-help, after private exchange]

>>>>> "MM" == Martin Maechler <maechler at stat.math.ethz.ch>
>>>>>     on Thu, 12 Aug 2004 17:12:01 +0200 writes:

>>>>> "javier" == javier garcia <- CEBAS <rn001 at cebas.csic.es>>
>>>>>     on Thu, 12 Aug 2004 16:28:27 +0200 writes:

    javier> Martin; Yes I know that there are variables with all
    javier> five values 'NA'. I've left them as they are just
    javier> because of saving a couple of lines in the script,
    javier> and because I like to see that they are there,
    javier> although all values are 'NA'.  I don't expect they
    javier> are used in the analysis, but are they the source of
    javier> the problem?

    MM> yes, but only because of "stand = TRUE".

    MM> Yes, one could imagine that it might be good when
    MM> standardizing these "all NA variables" would work

    MM> I'll think a bit more about it.  Thank you for the
    MM> example.

Ok. I've thought (and looked at the R code) a bit longer.
Also considered the fact (you mentioned) that this worked in R 1.8.0.
Hence, I'm considering the current behavior a bug.

Here is the patch (apply to cluster/R/daisy.q in the *source*
 or at the appriopriate place in <cluster_installed>/R/cluster ) :

--- daisy.q	2004/06/25 16:17:47	1.17
+++ daisy.q	2004/08/12 15:23:26
@@ -78,8 +78,8 @@
     if(all(type2 == "I")) {
 	if(stand) {
             x <- scale(x, center = TRUE, scale = FALSE) #-> 0-means
-            sx <- colMeans(abs(x))
-            if(any(sx == 0)) {
+	    sx <- colMeans(abs(x), na.rm = TRUE)# can still have NA's
+	    if(0 %in% sx) {
                 warning(sQuote("x"), " has constant columns ",
                         pColl(which(sx == 0)), "; these are standardized to 0")
                 sx[sx == 0] <- 1


Thank you for helping to find and fix this bug.
Martin Maechler, ETH Zurich, Switzerland

    javier> El Jue 12 Ago 2004 15:11, MM escribi??:

    >>> Javier, I could well read your .RData and try your
    >>> script to produce the same error from daisy().
    >>> 
    >>> Your dataframe is of dimension 5 x 180 and has many
    >>> variables that have all five values 'NA' (see below).
    >>> 
    >>> You can't expect to use these, do you?  Martin



From ggrothendieck at myway.com  Thu Aug 12 18:14:07 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 12 Aug 2004 16:14:07 +0000 (UTC)
Subject: [R] RE: Giving a first good impression of R to Social Scientists
References: <3699CDBC4ED5D511BE6400306E1C0D81030A0BE5@hermes.demogr.mpg.de>
Message-ID: <loom.20040812T175128-1@post.gmane.org>

Rau, Roland <Rau <at> demogr.mpg.de> writes:

> 	Yes, I do know the R-Commander. But I did not want to give them a
> GUI but rather expose them to the command line after I demonstrated that the
> steep learning curve in the beginning is worth the effort for the final
> results.

Note that Rcmdr displays all the underlying generated R code that does
the analysis as it runs so you are exposed to the command line.  This
might pique the interest of students wishing to learn more while giving
an easy-to-use and immediately useful environment for those who just want
to get results in the shortest most direction fashion.



From sfalcon at fhcrc.org  Thu Aug 12 18:25:07 2004
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Thu, 12 Aug 2004 09:25:07 -0700
Subject: [R] Approaches to using RUnit
In-Reply-To: <4118E17D.4030204@epigenomics.com>
References: <4118E17D.4030204@epigenomics.com>
Message-ID: <20040812162505.GA23691@queenbee.fhcrc.org>

On Tue, Aug 10, 2004 at 04:53:49PM +0200, Klaus Juenemann wrote:
> If you don't organize your code into packages but source individual R
> files your approach to source the code at the beginning of a test file
> looks the right thing to do.

Appears to be working pretty well for me too ;-)

> We mainly use packages and the code we use to test packages A and B, 
> say, looks like 

SNIP

> We use the tests subdirectory of a package to store our RUnit tests
> even though this is not really according to R conventions.

In an off list exchange with A.J. Rossini, we discussed an alternative
for using RUnit in a package.  The idea was to put the runit_*.R files
(containing test code) into somePackage/inst/runit/ and then put a
script, say dorunit.R inside somePackage/test/ that would create the
test suite's similar to the code you included in your mail.  The
advantage of this would be that the unit tests would run using R CMD
check.

In the next week or so I hope to package-ify some code and try this out.  


+ seth



From andy_liaw at merck.com  Thu Aug 12 18:25:03 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 12 Aug 2004 12:25:03 -0400
Subject: [R] Giving a first good impression of R to Social
 Scientists
Message-ID: <3A822319EB35174CA3714066D590DCD504AF8214@usrymx25.merck.com>

> From: Barry Rowlingson
> 
> Thomas Lumley wrote:
> > On Thu, 12 Aug 2004, Rau, Roland wrote:
> > 
> >>That is why would like to ask the experts on this list if 
> anyone of you has
> >>encountered a similar experience and what you could advise 
> to persuade
> >>people quickly that it is worth learning a new software?
> > 
> 
>   The usual way of teaching R seems to be bottom-up. Here's 
> the command 
> prompt, type some arithmetic, make some assignments, learn about 
> function calls and arguments, write your own functions, write 
> your own 
> packages.
> 
>   Perhaps a top-down approach might help certain cases. People using 
> point-n-click packages tend to use a limited range of analyses. Write 
> some functions that do these analyses, or give them wrappers so that 
> they get something like:
> 
>   > myData = readDataFile("foo.dat")
>     Read 4 variables: Z, Age, Sex, Disease
> 
>   > analyseThis(myData, response="Z", covariate="Age")
> 
>    Z = 0.36 * Age, Significance level = 0.932
> 
>   or whatever. Really spoon feed the things they need to do. Make it 
> really easy, foolproof.

The problem is that the only `fool' that had been `proof' against is the one
that the developer(s) had imagined.  One cannot under-estimate users'
ability to out-fool the developers' imagination...

Cheers,
Andy

 
>   Then show them what's behind the analyseThis() function. 
> How its not 
> even part of the R distribution. How easy you made it for a 
> beginner to 
> do a complex and novel analysis. Then maybe it'll "click" for 
> them, and 
> they'll see how having a programming language behind their statistics 
> functions lets them explore in ways not thought possible with the 
> point-n-click paradigm. Perhaps they'll start editing 
> analyseThis() and 
> write analyseThat(), start thinking for themselves.
> 
>   Or maybe they'll just stare at you blankly...
> 
> Baz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From sfalcon at fhcrc.org  Thu Aug 12 18:28:06 2004
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Thu, 12 Aug 2004 09:28:06 -0700
Subject: [R] truly object oriented programming in R
In-Reply-To: <20040812152818.69617.qmail@web53706.mail.yahoo.com>
References: <20040812152818.69617.qmail@web53706.mail.yahoo.com>
Message-ID: <20040812162804.GB23691@queenbee.fhcrc.org>

For an overview of the OOP R package, see
http://cran.r-project.org/doc/Rnews/Rnews_2001-3.pdf

+ seth



From tlumley at u.washington.edu  Thu Aug 12 18:46:07 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 12 Aug 2004 09:46:07 -0700 (PDT)
Subject: [R] truly object oriented programming in R
In-Reply-To: <20040812152818.69617.qmail@web53706.mail.yahoo.com>
References: <20040812152818.69617.qmail@web53706.mail.yahoo.com>
Message-ID: <Pine.A41.4.58.0408120926170.284152@homer03.u.washington.edu>

On Thu, 12 Aug 2004, Jason Liao wrote:

> Good morning! I recently implemented a KD tree in JAVA for faster
> kernel density estimation (part of the code follows). It went well. To
> hook it with R, however, has proved more difficult. My question is: is
> it possible to implement the algorithm in R? My impression seems to
> indicate no as the code requires a complete class-object framework that
> R does not support. But is there an R package or something that may
> make it possible? Thanks in advance for your help.


This code doesn't seem to have any  requirement for a class-object
framework. The methods can all be written as functions, there isn't any
use of inheritance or polymorphism. The data structure can then be a list.

Now, you might want to make this list an object, to improve printing or to
make it easier to check that the functions don't get called with arguments
that aren't really k-d trees.  This is well within the facilities of even
the S3 method system.

AFAICS the only class/object facility that Java provides and the
"methods" package doesn't is enforcement of "private" methods and data,
which has absolutely no impact on the complexity of programs (it can
affect how easy code *maintenance* is, because it forces you to decide
what is and isn't in your API, but that's a separate issue).

The old S3 class system is weaker, since it doesn't support function
polymorphism based on more than one of the arguments and doesn't have
reliable reflectance facilities.


	-thomas


>
> Java implementation of KD tree:
>
> public class Kdnode {
>
>         private double[] center; //center of the bounding box
>         private double diameter; //maximum distance from center to
> anywhere within the bounding box
>         private int numOfPoints; //number of source data points in the
> bounding box
>
>         private Kdnode left, right;
>
>
> 	public Kdnode(double[][] points, int split_dim, int [][]
> sortedIndices, double[][] bBox) {
>            //bBox: the bounding box, 1st row the lower bound, 2nd row
> the upper bound
>                 numOfPoints = points.length;
> 		int d = points[0].length;
>
>                 center = new double[d];
>                 for(int j=0; j<d; j++) center[j] =
> (bBox[0][j]+bBox[1][j])/2.;
>                 diameter = get_diameter(bBox);
>
> 		if(numOfPoints==1) {
>                   diameter = 0.;
>                   for(int j=0; j<d; j++) center[j] = points[0][j];
> 		  left = null;
> 		  right = null;
> 		}
> 		else {
>                   int middlePoint =
> sortedIndices[split_dim][numOfPoints/2];
> 		  double splitValue = points[middlePoint][split_dim];
>
>                   middlePoint =
> sortedIndices[split_dim][numOfPoints/2-1];
>                   double splitValue_small =
> points[middlePoint][split_dim];
>
> 		  int left_size = numOfPoints/2;
>                   int right_size = numOfPoints - left_size;
>
> 		  double[][] leftPoints = new double[left_size][d];
>                   double[][] rightPoints = new double[right_size][d];
>
>
> 		  int[][] leftSortedIndices = new int[d][left_size];
> 		  int[][] rightSortedIndices = new int[d][right_size];
>
> 		  int left_counter = 0, right_counter = 0;
> 		  int[] splitInfo = new int [numOfPoints];
>
> 		  for(int i = 0; i < numOfPoints; i++) {
> 		    if(points[i][split_dim] < splitValue) {
> 			for(int j=0; j<d; j++) leftPoints[left_counter][j] = points[i][j];
> 		       	splitInfo[i] = right_counter;
>                         left_counter++;
>                     }
>
> 		    else {
> 			for(int j=0; j<d; j++) rightPoints[right_counter][j] = points[i][j];
> 			splitInfo[i] = left_counter;
>                         right_counter++;
>                     }
>                   }
> 			// modify appropriately the indices to correspond to the new lists
> 			for(int i = 0; i < d; i++) {
> 				int left_index = 0, right_index = 0;
> 				for(int j = 0; j < numOfPoints; j++) {
> 					if(points[sortedIndices[i][j]][split_dim] < splitValue)
> 						leftSortedIndices[i][left_index++] = sortedIndices[i][j] -
> splitInfo[sortedIndices[i][j]];
> 					else    rightSortedIndices[i][right_index++] = sortedIndices[i][j]
> - splitInfo[sortedIndices[i][j]];
>                                 }
> 			}
>
> 			// Recursively compute the kdnodes for the points in the two
> splitted spaces
> 			double[][] leftBBox = new double[2][];
> 			double[][] rightBBox = new double[2][];
>
>                         for(int i=0; i<2; i++) {
>                                 leftBBox[i] =
> (double[])bBox[i].clone();
>                                 rightBBox[i] =
> (double[])bBox[i].clone();
>                             }
>
>                         leftBBox[1][split_dim] = splitValue_small;
>                         rightBBox[0][split_dim] = splitValue;
>
>                         int next_dim = (split_dim + 1) % (d);
> 			left = new Kdnode(leftPoints, next_dim, leftSortedIndices,
> leftBBox);
> 			right = new Kdnode(rightPoints, next_dim, rightSortedIndices,
> rightBBox);
> 		}
> 	}
>
>
>         public double evaluate(double[] target, double delta, double
> bandwidth) throws Exception
>         {
>
>              double dis_2_center = Common.distance(target,
> center)/bandwidth;
>              double dm = diameter/bandwidth;
>
>              if(dis_2_center >= 1+dm) return 0.;
>              if(numOfPoints==1) return Common.K(dis_2_center);
>
>              /*if(dis_2_center<1)
>              {
>                  double temp2 = dm*Common.KDeriv(dis_2_center);
>                  if(temp2<delta) return
> Common.K(dis_2_center)*numOfPoints;
>              } */
>
>              return left.evaluate(target,delta, bandwidth) +
> right.evaluate(target,delta, bandwidth);
>         }
>
>
>          public double get_diameter(double[][] bBox)
>         {
>             double value = 0., diff;
>             for (int i=0; i<bBox[0].length;i++)
>             {
>                 diff = (bBox[1][i] - bBox[0][i])/2.;
>                 value += diff*diff;
>             }
>             return Math.sqrt(value);
>         }
> }
>
> =====
> Jason Liao, http://www.geocities.com/jg_liao
> Dept. of Biostatistics, http://www2.umdnj.edu/bmtrxweb
> University of Medicine and Dentistry of New Jersey
> phone 732-235-5429, School of Public Health office
> phone 732-235-8611, Cancer Institute of New Jersey office
> moble phone 908-720-4205
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From jg_liao at yahoo.com  Thu Aug 12 19:20:14 2004
From: jg_liao at yahoo.com (Jason Liao)
Date: Thu, 12 Aug 2004 10:20:14 -0700 (PDT)
Subject: [R] truly object oriented programming in R
In-Reply-To: <Pine.A41.4.58.0408120926170.284152@homer03.u.washington.edu>
Message-ID: <20040812172014.5683.qmail@web53709.mail.yahoo.com>

Dear Thomas,
Thank you very much again for taking time to answer my questions. I am
sorry that my knoweldge of R is limited as I have only learned what is
necessary to do my work. In the KD tree, we have this recursive data
structure in that each knod has two children knods and this process
continues until the data points are divided. Does R's list support this
recursive data structure? If yes, can you give a sample program? 
Regards,
Jason

--- Thomas Lumley <tlumley at u.washington.edu> wrote:

> On Thu, 12 Aug 2004, Jason Liao wrote:
> 
> > Good morning! I recently implemented a KD tree in JAVA for faster
> > kernel density estimation (part of the code follows). It went well.
> To
> > hook it with R, however, has proved more difficult. My question is:
> is
> > it possible to implement the algorithm in R? My impression seems to
> > indicate no as the code requires a complete class-object framework
> that
> > R does not support. But is there an R package or something that may
> > make it possible? Thanks in advance for your help.
> 
> 
> This code doesn't seem to have any  requirement for a class-object
> framework. The methods can all be written as functions, there isn't
> any
> use of inheritance or polymorphism. The data structure can then be a
> list.
> 
> Now, you might want to make this list an object, to improve printing
> or to
> make it easier to check that the functions don't get called with
> arguments
> that aren't really k-d trees.  This is well within the facilities of
> even
> the S3 method system.
> 
> AFAICS the only class/object facility that Java provides and the
> "methods" package doesn't is enforcement of "private" methods and
> data,
> which has absolutely no impact on the complexity of programs (it can
> affect how easy code *maintenance* is, because it forces you to
> decide
> what is and isn't in your API, but that's a separate issue).
> 
> The old S3 class system is weaker, since it doesn't support function
> polymorphism based on more than one of the arguments and doesn't have
> reliable reflectance facilities.
> 
> 
> 	-thomas
> 
> 
> >
> > Java implementation of KD tree:
> >
> > public class Kdnode {
> >
> >         private double[] center; //center of the bounding box
> >         private double diameter; //maximum distance from center to
> > anywhere within the bounding box
> >         private int numOfPoints; //number of source data points in
> the
> > bounding box
> >
> >         private Kdnode left, right;
> >
> >
> > 	public Kdnode(double[][] points, int split_dim, int [][]
> > sortedIndices, double[][] bBox) {
> >            //bBox: the bounding box, 1st row the lower bound, 2nd
> row
> > the upper bound
> >                 numOfPoints = points.length;
> > 		int d = points[0].length;
> >
> >                 center = new double[d];
> >                 for(int j=0; j<d; j++) center[j] =
> > (bBox[0][j]+bBox[1][j])/2.;
> >                 diameter = get_diameter(bBox);
> >
> > 		if(numOfPoints==1) {
> >                   diameter = 0.;
> >                   for(int j=0; j<d; j++) center[j] = points[0][j];
> > 		  left = null;
> > 		  right = null;
> > 		}
> > 		else {
> >                   int middlePoint =
> > sortedIndices[split_dim][numOfPoints/2];
> > 		  double splitValue = points[middlePoint][split_dim];
> >
> >                   middlePoint =
> > sortedIndices[split_dim][numOfPoints/2-1];
> >                   double splitValue_small =
> > points[middlePoint][split_dim];
> >
> > 		  int left_size = numOfPoints/2;
> >                   int right_size = numOfPoints - left_size;
> >
> > 		  double[][] leftPoints = new double[left_size][d];
> >                   double[][] rightPoints = new
> double[right_size][d];
> >
> >
> > 		  int[][] leftSortedIndices = new int[d][left_size];
> > 		  int[][] rightSortedIndices = new int[d][right_size];
> >
> > 		  int left_counter = 0, right_counter = 0;
> > 		  int[] splitInfo = new int [numOfPoints];
> >
> > 		  for(int i = 0; i < numOfPoints; i++) {
> > 		    if(points[i][split_dim] < splitValue) {
> > 			for(int j=0; j<d; j++) leftPoints[left_counter][j] =
> points[i][j];
> > 		       	splitInfo[i] = right_counter;
> >                         left_counter++;
> >                     }
> >
> > 		    else {
> > 			for(int j=0; j<d; j++) rightPoints[right_counter][j] =
> points[i][j];
> > 			splitInfo[i] = left_counter;
> >                         right_counter++;
> >                     }
> >                   }
> > 			// modify appropriately the indices to correspond to the new
> lists
> > 			for(int i = 0; i < d; i++) {
> > 				int left_index = 0, right_index = 0;
> > 				for(int j = 0; j < numOfPoints; j++) {
> > 					if(points[sortedIndices[i][j]][split_dim] < splitValue)
> > 						leftSortedIndices[i][left_index++] = sortedIndices[i][j] -
> > splitInfo[sortedIndices[i][j]];
> > 					else    rightSortedIndices[i][right_index++] =
> sortedIndices[i][j]
> > - splitInfo[sortedIndices[i][j]];
> >                                 }
> > 			}
> >
> > 			// Recursively compute the kdnodes for the points in the two
> > splitted spaces
> > 			double[][] leftBBox = new double[2][];
> > 			double[][] rightBBox = new double[2][];
> >
> >                         for(int i=0; i<2; i++) {
> >                                 leftBBox[i] =
> > (double[])bBox[i].clone();
> >                                 rightBBox[i] =
> > (double[])bBox[i].clone();
> >                             }
> >
> >                         leftBBox[1][split_dim] = splitValue_small;
> >                         rightBBox[0][split_dim] = splitValue;
> >
> >                         int next_dim = (split_dim + 1) % (d);
> > 			left = new Kdnode(leftPoints, next_dim, leftSortedIndices,
> > leftBBox);
> > 			right = new Kdnode(rightPoints, next_dim, rightSortedIndices,
> > rightBBox);
> > 		}
> > 	}
> >
> >
> >         public double evaluate(double[] target, double delta,
> double
> > bandwidth) throws Exception
> >         {
> >
> >              double dis_2_center = Common.distance(target,
> > center)/bandwidth;
> >              double dm = diameter/bandwidth;
> >
> >              if(dis_2_center >= 1+dm) return 0.;
> >              if(numOfPoints==1) return Common.K(dis_2_center);
> >
> >              /*if(dis_2_center<1)
> >              {
> >                  double temp2 = dm*Common.KDeriv(dis_2_center);
> >                  if(temp2<delta) return
> > Common.K(dis_2_center)*numOfPoints;
> >              } */
> >
> >              return left.evaluate(target,delta, bandwidth) +
> > right.evaluate(target,delta, bandwidth);
> >         }
> >
> >
> >          public double get_diameter(double[][] bBox)
> >         {
> >             double value = 0., diff;
> >             for (int i=0; i<bBox[0].length;i++)
> >             {
> >                 diff = (bBox[1][i] - bBox[0][i])/2.;
> >                 value += diff*diff;
> >             }
> >             return Math.sqrt(value);
> >         }
> > }
> >
> > =====
> > Jason Liao, http://www.geocities.com/jg_liao
> > Dept. of Biostatistics, http://www2.umdnj.edu/bmtrxweb
> > University of Medicine and Dentistry of New Jersey
> > phone 732-235-5429, School of Public Health office
> > phone 732-235-8611, Cancer Institute of New Jersey office
> > moble phone 908-720-4205
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> 
=== message truncated ===


=====
Jason Liao, http://www.geocities.com/jg_liao
Dept. of Biostatistics, http://www2.umdnj.edu/bmtrxweb
University of Medicine and Dentistry of New Jersey
phone 732-235-5429, School of Public Health office
phone 732-235-8611, Cancer Institute of New Jersey office
moble phone 908-720-4205



From joern at kamradt.net  Thu Aug 12 19:50:10 2004
From: joern at kamradt.net (Joern Kamradt)
Date: Thu, 12 Aug 2004 13:50:10 -0400
Subject: [R] rgl snapshot command
Message-ID: <0E3F6658-EC88-11D8-9325-000A95BA95C8@kamradt.net>

Hi,
I am using the rgl package for 3D display. Unfortunately, I am not able 
to get the snapshot command running.
I tried the following:

 > example(rgl.surface)

rgl.sr> data(volcano)
rgl.sr> y <- 2 * volcano
rgl.sr> x <- 10 * (1:nrow(y))
rgl.sr> z <- 10 * (1:ncol(y))
rgl.sr> ylim <- range(y)
rgl.sr> ylen <- ylim[2] - ylim[1] + 1
rgl.sr> colorlut <- terrain.colors(ylen)
rgl.sr> col <- colorlut[y - ylim[1] + 1]
rgl.sr> rgl.clear()
rgl.sr> rgl.surface(x, z, y, color = col)
 > rgl.snapshot(filename="./volcano.png",fmt="png")
[1] "failed"

Any help is highly appreciated

Joern



__________________________

Joern Kamradt, MD
Cancer Genetic Branch
National Human Genome Research Institute
National Institutes of Health
50 South Drive, building 50, room 5147
Bethesda, MD 20892-8000, USA
Phone#: +1 (301) 496 5382
FAX#:     +1 (301) 402 3241
Email: jkamradt at nhgri.nih.gov
Website: www.genome.gov



From sfalcon at fhcrc.org  Thu Aug 12 19:53:37 2004
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Thu, 12 Aug 2004 10:53:37 -0700
Subject: [R] truly object oriented programming in R
In-Reply-To: <20040812172014.5683.qmail@web53709.mail.yahoo.com>
References: <Pine.A41.4.58.0408120926170.284152@homer03.u.washington.edu>
	<20040812172014.5683.qmail@web53709.mail.yahoo.com>
Message-ID: <20040812175337.GE23691@queenbee.fhcrc.org>

Hi Jason,

On Thu, Aug 12, 2004 at 10:20:14AM -0700, Jason Liao wrote:
> Does R's list support this recursive data structure? If yes, can you
> give a sample program?

Not sure if this is what you are looking for, but here's a quick linked
list example using R's lists.

# ---------------------8<---------------------------
newNode <- function(value) {
  list(data=value, child=NULL)
}

insertNode <- function(headNode, node) {
  node$child <- headNode
  node
}

printNodes <- function(headNode) {
  curNode <- headNode
  while (TRUE) {
    print(curNode$data)
    if (is.null(curNode$child ))
      break
    curNode <- curNode$child
  }
}

head <- newNode(1)
head <- insertNode(head, newNode(2))
head <- insertNode(head, newNode(3))
head <- insertNode(head, newNode(4))

printNodes(head)
# ---------------------8<---------------------------

The thing that's very different from, say, Java is that everything is an
object in R --- there isn't a notion of a *reference* to an object,
which is why in the above I had to say "head <- insertNode(...)" where
as in Java you could pass in a reference to head and have the method
modify what it points to.

I think there are some ways around this, at least syntactically, using
various tricks with environment(), but I don't yet understand them well
enough to comment further.

hope that's helpful,

+ seth



From ggrothendieck at myway.com  Thu Aug 12 19:54:47 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 12 Aug 2004 17:54:47 +0000 (UTC)
Subject: [R] truly object oriented programming in R
References: <Pine.A41.4.58.0408120926170.284152@homer03.u.washington.edu>
	<20040812172014.5683.qmail@web53709.mail.yahoo.com>
Message-ID: <loom.20040812T194757-185@post.gmane.org>

Jason Liao <jg_liao <at> yahoo.com> writes:

> In the KD tree, we have this 
>[...]
> recursive data structure? If yes, can you give a sample program? 
>

For an example, see the R loess.smooth function which uses k-d trees.  
Try ?loess.smooth for info and just type

   loess.smooth

like that exactly to see source.  It calls a C function that calls a FORTRAN
function to do the real work, presumably for efficiency reasons.  You can
view those by looking at:

   loess-README
   loessc.c
   loessf.f

at this link:

   https://svn.r-project.org/R/branches/R-1-9-patches/src/library/stats/src/

Unfortunately the above is in FORTRAN whereas you seem to be looking for a
pure R solution but hopefully that will get you started.



From Achim.Zeileis at wu-wien.ac.at  Thu Aug 12 20:01:47 2004
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 12 Aug 2004 20:01:47 +0200
Subject: [R] [R-pkgs] "new" package fortunes 1.0-3
Message-ID: <20040812200147.3e098447.Achim.Zeileis@wu-wien.ac.at>

Dear useRs,

I used the summer months to work on all of my packages,
and so this is the first of a sequence of announcements
of "new" or updated packages. The "new" packages are new 
in the sense that previous versions had been on CRAN for
some months but hadn't been announced to the R community
via this list until now.

All packages are available from the CRAN master site
in source form - binary versions should be available
from the mirrors in the next days.


So the first announcement is for fortunes 1.0-3:

The fortunes package provides simple infrastructure for
reading fortunes from a .csv file and displaying them.
Furthermore, it contains a growing list of fortunes
related to R, mainly collected from the mailing lists
but also from quotes at conferences. The author list
contains me (as I've written the R code) as well as
the people who contributed quotes by sending me a mail.
The original "authors" of each quote are always given
in the respective fortune.

For those of you who want to see a quote each time they 
start up R: you can add to your .Rprofile something like

if(interactive()) { library(fortunes); fortune() }

If you want to create your own list of fortunes you can
simply add another fortune collection in .csv format.
Of course, it would also be great if you could contribute
some quotes to the package...simply send me an e-mail.

Enjoy!
Z


-----------------
Package: fortunes
Version: 1.0-3
Date: 2004-08-10
Title: R Fortunes
Author: Achim Zeileis, fortune contributions from Torsten Hothorn, Peter
        Dalgaard, Uwe Ligges, Kevin Wright, Martin Maechler
Maintainer: Achim Zeileis <Achim.Zeileis at wu-wien.ac.at>
Description: R Fortunes
Depends: R (>= 1.4.0)
License: GPL

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From Achim.Zeileis at wu-wien.ac.at  Thu Aug 12 20:03:28 2004
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 12 Aug 2004 20:03:28 +0200
Subject: [R] [R-pkgs] "new" package sandwich 0.1-3
Message-ID: <20040812200328.0c22c1d6.Achim.Zeileis@wu-wien.ac.at>

Dear useRs,

here is the announcement for the next "new" package:
sandwich 0.1-3.

sandwich provides heteroskedasticity (and autocorrelation)
consistent covariance matrix estimators (also called HC
and HAC estimators).

The former are implemented in the function vcovHC() (which
was available in strucchange  before - and independently
in hccm() in John Fox's car package).

And the latter are implemented in the function vcovHAC().
This implements sandwich-type estimators in a rather 
flexible way, allowing for user-defined weights or 
weight functions. It builds on some of the functionality
which was before available in Thomas Lumley's weave package
(not on CRAN). In particular it makes available the
class of WEAVE estimators introduced by Lumley & Heagerty (1999)
in the function weave() which is a convenience interface to
vcovHAC(). Furthermore, it implements the class of kernel
HAC estimators with automatic bandwidth-selection of
Andrews (1991) in the function kernHAC(), which is again a
convenience interface to vcovHAC().

Best wishes,
Z

-----------------
Package: sandwich
Version: 0.1-3
Date: 2004-07-19
Title: Robust Covariance Matrix Estimators
Author: Thomas Lumley, Achim Zeileis
Maintainer: Achim Zeileis <Achim.Zeileis at wu-wien.ac.at>
Description: Model-robust standard error estimators for time series
             and longitudinal data.
Depends: zoo, R (>= 1.5.0)
License: GPL version 2

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From Achim.Zeileis at wu-wien.ac.at  Thu Aug 12 20:04:26 2004
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 12 Aug 2004 20:04:26 +0200
Subject: [R] [R-pkgs] "new" package zoo 0.2-0
Message-ID: <20040812200426.20295e3d.Achim.Zeileis@wu-wien.ac.at>

Dear useRs,

yet another "new" package: zoo 0.2-0.

zoo provides a simple S3 class and methods for totally
ordered indexed observations such as irregular time
series. Although there are other packages for irregular
time series available on CRAN (Giles Heywood's its 
package and irts() in Adrian Trapletti's tseries package)
I wrote this package because I needed something which
provides simple infrastructure for observations with
(almost) arbitrary indexes (and not only "POSIXct" time
stampes as in its() and irts()). And it was at least also
useful for Gabor Grothendieck who provided most of the
updates to this version.

Best wishes,
Z

------------

Package: zoo
Version: 0.2-0
Date: 2004-08-12
Title: Z's ordered observations
Author: Achim Zeileis, Gabor Grothendieck
Maintainer: Achim Zeileis <Achim.Zeileis at wu-wien.ac.at>
Description: A class with methods for totally ordered indexed
observations             such as irregular time series.
Depends: R (>= 1.7.0)
License: GPL

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From Achim.Zeileis at wu-wien.ac.at  Thu Aug 12 20:05:06 2004
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 12 Aug 2004 20:05:06 +0200
Subject: [R] [R-pkgs] updated package strucchange 1.2-4
Message-ID: <20040812200506.78e22588.Achim.Zeileis@wu-wien.ac.at>

Dear useRs,

the strucchange package for testing for structural change
has been updated: the current version is 1.2-4.
The most significant additions were two functions gefp()
and efpFunctional().

gefp() implements a class of generalized M-fluctuation
tests for testing for parameter instability or structural
change in general parametric models including generalized
linear models (GLMs). 

efpFunctional() provides infrastructure for inference based
on functionals applied to empirical fluctuation processes
such as automatic tabulation of critical values and a choice
of a suitable visualization method.

The theory behind both functions is described in Zeileis
& Hornik (2003), the implementation ideas are explained in
Zeileis (2004). Links to both papers are available from
my web page: http://www.ci.tuwien.ac.at/~zeileis/

Best wishes,
Z

--------------------

Package: strucchange
Version: 1.2-4
Date: 2004-08-10
Title: Testing for Structural Change
Author: Achim Zeileis, Friedrich Leisch, Bruce Hansen,
        Kurt Hornik, Christian Kleiber, Andrea Peters
Maintainer: Achim Zeileis <Achim.Zeileis at wu-wien.ac.at>
Description: Testing, dating and monitoring of structural change in
             linear regression relationships.
             strucchange features tests/methods from the generalized
             fluctuation test framework as well as from the F test (Chow
             test) framework. This includes methods to fit, plot and
             test fluctuation processes (e.g., CUSUM, MOSUM,
             recursive/moving estimates) and F statistics, respectively.
             It is possible to monitor incoming data online using
             fluctuation processes.
             Finally, the breakpoints in regression models with
             structural changes can be estimated together with
             confidence intervals. Emphasis is always given to methods
             for visualizing the data.
Depends: sandwich, zoo, R (>= 1.5.0)
License: GPL

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From Achim.Zeileis at wu-wien.ac.at  Thu Aug 12 20:05:09 2004
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 12 Aug 2004 20:05:09 +0200
Subject: [R] [R-pkgs] updated package ineq 0.2-4
Message-ID: <20040812200509.17710696.Achim.Zeileis@wu-wien.ac.at>

Dear useRs,

my last announcement is an update of the ineq package
for measuring inequality, concentration and poverty.
The current version is now 0.2-4.

Thanks to suggestions from Rein Halbersma the Pen()
function for plotting Pen's parade was improved and now
allows for much more flexibility. See the help page
for examples.

Best wishes,
Z

-------------

Package: ineq
Version: 0.2-4
Date: 2004-08-10
Title: Measuring inequality, concentration and poverty
Author: Achim Zeileis
Maintainer: Achim Zeileis <Achim.Zeileis at wu-wien.ac.at>
Description: Inequality, concentration and poverty measures
             Lorenz curves (empirical and theoretical)
License: GPL

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From tlumley at u.washington.edu  Thu Aug 12 20:47:23 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 12 Aug 2004 11:47:23 -0700 (PDT)
Subject: [R] truly object oriented programming in R
In-Reply-To: <20040812172014.5683.qmail@web53709.mail.yahoo.com>
References: <20040812172014.5683.qmail@web53709.mail.yahoo.com>
Message-ID: <Pine.A41.4.58.0408121103300.284152@homer03.u.washington.edu>

On Thu, 12 Aug 2004, Jason Liao wrote:

> Dear Thomas,
> Thank you very much again for taking time to answer my questions. I am
> sorry that my knoweldge of R is limited as I have only learned what is
> necessary to do my work. In the KD tree, we have this recursive data
> structure in that each knod has two children knods and this process
> continues until the data points are divided. Does R's list support this
> recursive data structure? If yes, can you give a sample program?

Yes, the elements of a list can be lists. For example, a simple binary
tree could have lists with elements left, right, key, and data

## create a new single node
newtree<-function(key,data){ list(left=NULL,right=NULL, key=key,
data=data)}

## add a node to a sorted tree
addnode<-function(tree, key, data){

	if (key<=tree$key){
	   if (is.null(tree$left))
		tree$left<-newtree(data=data,key=key)
           else
                tree$left<-addnode(tree$left,key,data)
	} else {
	   if (is.null(tree$right))
                tree$right<-newtree(data=data,key=key)
           else
                tree$right<-addnode(tree$left,key,data)

        }
	return(tree)
}


## inorder traversal.  action() is any function that takes key and data
## arguments
applyinorder<-function(tree, action){

	c(if (!is.null(tree$left))
	      applyinorder(tree$left,action),
	action(tree$key,tree$data),
	if (!is.null(tree$right))
	      applyinorder(tree$right, action))

}


## an example
> a<-newtree("R","two optional method systems and first-class functions")
> a<-addnode(a,"Java","compulsory object system")
> a<-addnode(a,"C","No built-in support but that needn't stop you")
> a<-addnode(a,"C++","If C++ is your hammer, everything looks like a
thumb")
> applyinorder(a,function(key,data) paste(key,data,sep=": "))
[1] "C: No built-in support but that needn't stop you"
[2] "C++: If C++ is your hammer, everything looks like a thumb"
[3] "Java: compulsory object system"
[4] "R: two optional method systems and first-class functions"



From tlumley at u.washington.edu  Thu Aug 12 20:49:09 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 12 Aug 2004 11:49:09 -0700 (PDT)
Subject: [R] truly object oriented programming in R
In-Reply-To: <20040812175337.GE23691@queenbee.fhcrc.org>
References: <Pine.A41.4.58.0408120926170.284152@homer03.u.washington.edu>
	<20040812172014.5683.qmail@web53709.mail.yahoo.com>
	<20040812175337.GE23691@queenbee.fhcrc.org>
Message-ID: <Pine.A41.4.58.0408121148160.284152@homer03.u.washington.edu>

On Thu, 12 Aug 2004, Seth Falcon wrote:

> The thing that's very different from, say, Java is that everything is an
> object in R --- there isn't a notion of a *reference* to an object,
> which is why in the above I had to say "head <- insertNode(...)" where
> as in Java you could pass in a reference to head and have the method
> modify what it points to.
>
> I think there are some ways around this, at least syntactically, using
> various tricks with environment(), but I don't yet understand them well
> enough to comment further.
>

Yes, and there is support in packages for other object systems in addition
to the two built-in ones.

Some of us feel that if you want Java you know where to find it...


	-thomas



From richard.urbano at vanderbilt.edu  Thu Aug 12 20:51:38 2004
From: richard.urbano at vanderbilt.edu (Richard Urbano)
Date: Thu, 12 Aug 2004 13:51:38 -0500
Subject: [R] .Random.seed error 
Message-ID: <200408121851.i7CIpLxb001240@smtp03.smtp.vanderbilt.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040812/c769ddca/attachment.pl

From ripley at stats.ox.ac.uk  Thu Aug 12 21:08:16 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 12 Aug 2004 20:08:16 +0100 (BST)
Subject: [R] .Random.seed error 
In-Reply-To: <200408121851.i7CIpLxb001240@smtp03.smtp.vanderbilt.edu>
Message-ID: <Pine.LNX.4.44.0408122003400.15320-100000@gannet.stats>

On Thu, 12 Aug 2004, Richard Urbano wrote:

> I have this snippet of code from an example in Dr. Harrel's book "Regression
> Modeling Strategies" p 501

That's in a section called `S-PLUS Examples'.
                            ^^^^^^
> n<-2000
> .Random.seed <-c(49,39,17,36,23,0,43,51,6,54,50,1)
> age <-50 + 12 * rnorm(n)
> age
> 
> I get the error message:  Error in rnorm(n) : .Random.seed[1] is NOT a valid
> RNG kind (code)
> 
> I have tried this on Windows and Linux  R versions 1.8.1, 1.9.0, and 1.9.1

But you did not try the S-PLUS examples in S-PLUS ....

> If I comment out the .Random.seed line and call set.seed(49),  set.seed(39)
> etc before each call to a random generator function, everyone is HAPPY.

> Does anyone have a suggestion?

Don't confuse S-PLUS and R.

Read the R documentation before posting, as the posting guide asks -- see 
?.Random.seed for the explanation of the format of .Random.seed.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From nicolai at catpipe.net  Thu Aug 12 21:52:17 2004
From: nicolai at catpipe.net (Nicolai Petri)
Date: Thu, 12 Aug 2004 21:52:17 +0200
Subject: [R] Fwd: Timebased predictions in postgresql.
Message-ID: <200408122152.17728.nicolai@catpipe.net>

Hi r-people :)

I'm sorry to disturb but I must admit that I know amazingly little about R and 
similar statistics-packages/languages and I'm kind of lost on where to start.
I'm currently working on a datacollection framework for postgresql (The db 
doesn't really matter except that I hope to use PL/R) and I would like to be 
able to predict future values preferable 1 day or more ahead. The highest 
resolution on the historic data is 4 minutes but I'm already resampling that 
to whatever I need, so if it would be better to use 30min or other 
reasolution (because of performance) it would be perfectly ok.

The types of statistics in the database is typically network io/ cpu usage, 
temperatur, etc. and I will rarely have holes in the history. 

Can anyone tell me how (or give me a hint) I can predict traffic or similar 
maybe one or more days ahead when I have the data xxdays back ? (And many how 
many days / which interval would be optimal for best performance/precision).

You can also tell me it's impossible, but I think it could be really cool to 
present graphs of expected cpu-load or network IO to our users.

Can it be done in R (and PL/R) ?

Best regards,
Nicolai Petri
catpipe Systems Aps



From medvedm at UCMAIL.UC.EDU  Thu Aug 12 22:00:54 2004
From: medvedm at UCMAIL.UC.EDU (Medvedovic, Mario (medvedm))
Date: Thu, 12 Aug 2004 16:00:54 -0400
Subject: [R] hclust-segmentation fault
Message-ID: <886EF25AF8BEF64EB89A820EF84064FF052E149B@UCMAIL4>

Thanks a lot for the assistance. It was an installation mistake - apparently
we did not have BLAS libraries installed. The compiler did not seem to have
been a problem in this case, althought it is an older version (gcc 3.3.1).
Mario.


>-----Original Message-----
>From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
>Sent: Monday, August 09, 2004 11:51 AM
>To: Medvedovic, Mario (medvedm)
>Cc: 'r-help at stat.math.ethz.ch'
>Subject: RE: [R] hclust-segmentation fault
>
>
>On Mon, 9 Aug 2004, Medvedovic, Mario (medvedm) wrote:
>
>> Well, the use of debugger will take some time, but here is a 
>simple code
>> that invariably causes the fault. 
>> Mario.
>> 
>> indata<-matrix(rnorm(1000,0,1),ncol=10)
>> ed<-dist(indata)
>> hc.e<-hclust(ed,"average")
>
>Works fine on R 1.9.1 on our dual Opteron 248 under FC2.
>
>We know of some pertinent compiler bugs on x86_64, so is this 
>gcc 3.3.3 
>or later?
>
>> >-----Original Message-----
>> >From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
>> >Sent: Monday, August 09, 2004 11:14 AM
>> >To: Medvedovic, Mario (medvedm)
>> >Cc: 'r-help at stat.math.ethz.ch'
>> >Subject: Re: [R] hclust-segmentation fault
>> >
>> >
>> >On Mon, 9 Aug 2004, Medvedovic, Mario (medvedm) wrote:
>> >
>> >> I am getting the "Segmentation fault" when using hclust in 
>> >R-1.9.1 running
>> >> under SuSe 9.0 64-bit kernel on a dual opteron system with 
>> >8G of RAM. 
>> >> I was wandering if anybody could offer any insight?
>> >
>> >Please try to use the debugger to supply more information, or 
>> >give us some 
>> >code we can reproduce on a similar system to see if we can 
>> >reproduce the 
>> >segfault.
>> >
>> >-- 
>> >Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> >Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> >University of Oxford,             Tel:  +44 1865 272861 (self)
>> >1 South Parks Road,                     +44 1865 272866 (PA)
>> >Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>> >
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html
>> 
>> 
>
>-- 
>Brian 
>D. Ripley,                  ripley at stats.ox.ac.uk
>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>University of Oxford,             Tel:  +44 1865 272861 (self)
>1 South Parks Road,                     +44 1865 272866 (PA)
>Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From larsenmtl at comcast.net  Thu Aug 12 22:20:10 2004
From: larsenmtl at comcast.net (larsenmtl@comcast.net)
Date: Thu, 12 Aug 2004 20:20:10 +0000
Subject: [R] Error Using pm.getabst()
Message-ID: <081220042020.9248.411BD0FA0007E858000024202200763704049B03020A9C9D0E04@comcast.net>

R Users:

After installing Bioconductor, RSXML and all the relevant Win32 DLLs (libxml2, zlib, iconv), I receive the following error message when using pm.getabst()

Error in xmlRoot(absts) : no applicable method for "xmlRoot"

I receive this when using the example from help(pm.getabst).  

Downloading the target XML file, parsing it with xmlTreeParse and applying xmlRoot returns no error.

Your thoughts/suggestions are appreciated.

Mark Larsen



From jeroschh at ohsu.edu  Thu Aug 12 23:10:31 2004
From: jeroschh at ohsu.edu (Michael Jerosch-Herold)
Date: Thu, 12 Aug 2004 14:10:31 -0700
Subject: [R] correlation structures in NLME
Message-ID: <s11b7a5e.005@ohsu.edu>

I am using the latest version of R on a Windows machine and get the
following error when I try to initialize a correlation structure with
the function corAR1 in NLME. This example is taken from the book of
Pinheiro and Bates, so it should work. What is going wrong?

> library(nlme)
> data(Orthodont)
> cs1AR1 <- corAR1(0.8, form= ~1 | Subject)
> cs1AR1 <- initialize(cs1AR1, data = Orthodont)
Error in methodsPackageMetaName("C", name) : 
        The name of the object (e.g,. a class or generic function) to
find in the meta-data must be a single string (got a character vector of
length 2)
In addition: Warning message: 
the condition has length > 1 and only the first element will be used
in: if (!is.na(match(Class, .BasicClasses))) return(newBasic(Class,  


Thank you!

Michael Jerosch-Herold



From ripley at stats.ox.ac.uk  Thu Aug 12 23:24:14 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 12 Aug 2004 22:24:14 +0100 (BST)
Subject: [R] correlation structures in NLME
In-Reply-To: <s11b7a5e.005@ohsu.edu>
Message-ID: <Pine.LNX.4.44.0408122220580.17817-100000@gannet.stats>

On Thu, 12 Aug 2004, Michael Jerosch-Herold wrote:

> I am using the latest version of R on a Windows machine and get the
> following error when I try to initialize a correlation structure with
> the function corAR1 in NLME. This example is taken from the book of
> Pinheiro and Bates, so it should work. What is going wrong?

That book is about S(-PLUS), so there is no reason why it `should work' in
R.

> > library(nlme)
> > data(Orthodont)
> > cs1AR1 <- corAR1(0.8, form= ~1 | Subject)
> > cs1AR1 <- initialize(cs1AR1, data = Orthodont)
> Error in methodsPackageMetaName("C", name) : 
>         The name of the object (e.g,. a class or generic function) to
> find in the meta-data must be a single string (got a character vector of
> length 2)
> In addition: Warning message: 
> the condition has length > 1 and only the first element will be used
> in: if (!is.na(match(Class, .BasicClasses))) return(newBasic(Class,  

Try

> find("initialize")
[1] "package:methods"

so that's not right.

Have you looked in the R scripts in package nlme?  I think it is
Initialize() in R.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rgentlem at jimmy.harvard.edu  Thu Aug 12 23:28:24 2004
From: rgentlem at jimmy.harvard.edu (Robert Gentleman)
Date: Thu, 12 Aug 2004 17:28:24 -0400
Subject: [R] Error Using pm.getabst()
In-Reply-To: <081220042020.9248.411BD0FA0007E858000024202200763704049B03020A9C9D0E04@comcast.net>;
	from larsenmtl@comcast.net on Thu, Aug 12, 2004 at 08:20:10PM +0000
References: <081220042020.9248.411BD0FA0007E858000024202200763704049B03020A9C9D0E04@comcast.net>
Message-ID: <20040812172824.E22016@jimmy.harvard.edu>

You will almost surely do better to ask about Bioconductor packages on
the Bioconductor mailing list. Next, it is helpful to know what
versions of things you are using.

As for your problem, did you look to see what kind of object absts is?
There seems to be no default method for xmlRoot, and it is likely that
the call to create the absts object failed (prior to this). You might
want to try stepping through the commands, one at a time and checking
each step. Often, the problem arises because you have not properly set
up your connection to the internet and so none of the querying
software will work.

  Robert

On Thu, Aug 12, 2004 at 08:20:10PM +0000, larsenmtl at comcast.net wrote:
> R Users:
> 
> After installing Bioconductor, RSXML and all the relevant Win32 DLLs (libxml2, zlib, iconv), I receive the following error message when using pm.getabst()
> 
> Error in xmlRoot(absts) : no applicable method for "xmlRoot"
> 
> I receive this when using the example from help(pm.getabst).  
> 
> Downloading the target XML file, parsing it with xmlTreeParse and applying xmlRoot returns no error.
> 
> Your thoughts/suggestions are appreciated.
> 
> Mark Larsen
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
+---------------------------------------------------------------------------+
| Robert Gentleman                 phone : (617) 632-5250                   |
| Associate Professor              fax:   (617)  632-2444                   |
| Department of Biostatistics      office: M1B20                            |
| Harvard School of Public Health  email: rgentlem at jimmy.harvard.edu        |
+---------------------------------------------------------------------------+



From CTMAN at txccc.org  Thu Aug 12 23:41:26 2004
From: CTMAN at txccc.org (Man, Chris T.)
Date: Thu, 12 Aug 2004 16:41:26 -0500
Subject: [R] coxph question
Message-ID: <064925768627DD4DAC63C36FD0A0880601ECF4A7@TCCEXV3A.ad.TexasChildrensHospital.org>

Hi all,

I have many variables to test using cox model (coxph), and I am only interested in those variables with p value less than 0.01. Is there a quick way to do this automatically instead of looking at the output of each variable?
Plus, is there a method in R that I could adjust the multiple testing in coxph?

Thanks,

Chris



From mayeul.kauffmann at tiscali.fr  Fri Aug 13 03:03:05 2004
From: mayeul.kauffmann at tiscali.fr (Mayeul KAUFFMANN)
Date: Fri, 13 Aug 2004 03:03:05 +0200
Subject: [R] How to use the whole dataset (including between events) in Cox
	model (time-varying covariates) ?
Message-ID: <000501c480d1$4eb6cea0$3ccb9853@amd>

Hello,

coxph does not use any information that are in the dataset between event
times (or "death times") , since computation only occurs at event  times.

For instance, removing observations when there is no event at that time in
the whole dataset does not change the results:
> set.seed(1)
> data <-
as.data.frame(cbind(start=c(1:5,1:5,1:4),stop=c(2:6,2:6,2:5),status=c(rep(
0,7),1,rep(0,5),1),id=c(rep(1,5),rep(2,5),rep(3,4)),x1=rnorm(14)))
> data
start stop status id x1
1 1 2 0 1 -0.6264538
2 2 3 0 1 0.1836433
3 3 4 0 1 -0.8356286
4 4 5 0 1 1.5952808
5 5 6 0 1 0.3295078
6 1 2 0 2 -0.8204684
7 2 3 0 2 0.4874291
8 3 4 1 2 0.7383247
9 4 5 0 2 0.5757814
10 5 6 0 2 -0.3053884
11 1 2 0 3 1.5117812
12 2 3 0 3 0.3898432
13 3 4 0 3 -0.6212406
14 4 5 1 3 -2.2146999
coxph(Surv(start, stop,status)~ cluster(id)+x1,data=data ,robust=T)
coxph(Surv(start, stop,status)~ cluster(id)+x1,data=subset(data,stop %in%
4:5) ,robust=T) # the same !!! (except n)

First, some data is lost.
Second, this loss could be an important problem when  there is a
time-varying covariate that changes quicker than the frequency  of events.
Specifically, I have a covariate which has low values most of the time. It
sometimes jumps to high values and that is hypothesized as greatly
increasing the risk of an event.
With rare events, the effect of this covariate will only be measured at
event times. Chances are that the only time such a covariate is recorded
at high level, the individual for which it is measured as being high is
having an event.
This may bias the estimated coefficient.

Here is my question:
How to fully use the dataset?

(that is: how to have really _time-varying_ covariates (even if they
change step by step, not continuously), not covariates whose changes are
measured only at event time )

Ideally, the full dataset would be use to estimate the parameters, or at
least to estimate the standard error of the estimated parameters.
Any ideas ???
.
.
.

A second best (which might require less work) would be to use all the
dataset to assess the predictive power of the model.

Maybe by using the expected number of events for an individual over the
time interval that they were observed to be at risk
> predict(coxfit,type="expected")
and compare it with observed number of events
(does it use all data and takes into account all the baseline hazard, even
between events?)


Or, if not,  following Brian D. Ripley suggestion about the baseline
hazard: "As an approximation you can smooth the fitted
baseline cumulative hazard (e.g. by package pspline) and ask for its
derivative" (https://stat.ethz.ch/pipermail/r-help/2004-July/052376.html)

the following code could be use to estimate (and plot) a smooth baseline
hazard:
> t<-seq(min(data$start),max(data$stop),length=100)
> lines(t,
predict(sm.spline(x=basehaz(coxfit)[,2],y=basehaz(coxfit)[,1],norder=2),
t,1))
#there is a problem with this code. One should add the contraint that the
baseline hazard cannot be negative.

The following computes the parametric part of the cox model.
> risk <- predict(coxfit, type='risk') # gives exp(X'b)

something like
> baseline.hazard*risk
would give the true risk at any time (but it would be probably much harder
to compute)

which could help assess the predictive power of the model.
(still a lot of work)

Thanks in advance for any help or comment.

Mayeul KAUFFMANN
Univ. Pierre Mendes France
Grenoble - France



From mayeul.kauffmann at tiscali.fr  Fri Aug 13 03:16:48 2004
From: mayeul.kauffmann at tiscali.fr (Mayeul KAUFFMANN)
Date: Fri, 13 Aug 2004 03:16:48 +0200
Subject: [R] coxph question
Message-ID: <000301c480d3$35d91350$3ccb9853@amd>

> I have many variables to test using cox model (coxph), and I am only
interested in those variables with p value less than 0.01. Is there a
quick way to do this automatically instead of looking at the output of
each variable?
> Chris

I guess you need covariate selection.
for a lengthy discussion of another method (AIC/BIC), look at last month
archive:
https://www.stat.math.ethz.ch/pipermail/r-help/2004-July/subject.html#53519

and try using
> library(MASS)
> stepAIC (...)

Most of the time, it starts removing the covariate with the lower p-value.


Mayeul KAUFFMANN
Univ. Pierre Mendes France
Grenoble - France



From davidD at qimr.edu.au  Fri Aug 13 03:34:42 2004
From: davidD at qimr.edu.au (David Duffy)
Date: Fri, 13 Aug 2004 11:34:42 +1000 (EST)
Subject: [R] pnorm, qnorm
In-Reply-To: <200408071020.i77AFU3P023524@hypatia.math.ethz.ch>
References: <200408071020.i77AFU3P023524@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.58.0408131132120.12086@orpheus.qimr.edu.au>

Trenkler, Dietrich said:
>
> I found the following strange behavior  using qnorm() and pnorm():
>
> > x<-8.21;x-qnorm(pnorm(x))
> [1] 0.0004638484
> > x<-8.28;x-qnorm(pnorm(x))
> [1] 0.07046385
> > x<-8.29;x-qnorm(pnorm(x))
> [1] 0.08046385
> > x<-8.30;x-qnorm(pnorm(x))
> [1] -Inf
>
 qnorm(1-.Machine$double.eps)
 [1] 8.12589



From larsenmtl at comcast.net  Fri Aug 13 04:20:47 2004
From: larsenmtl at comcast.net (lars)
Date: Thu, 12 Aug 2004 22:20:47 -0400
Subject: [R] Error Using pm.getabst()
In-Reply-To: <20040812172824.E22016@jimmy.harvard.edu>
References: <081220042020.9248.411BD0FA0007E858000024202200763704049B03020A9C9D0E04@comcast.net>
	<20040812172824.E22016@jimmy.harvard.edu>
Message-ID: <1092363647.21064.14.camel@LarsLinuxBox>

Robert,

Thank you for your reply.  I thought RSXML was an R (CRAN) package? I
realize your package is part of bioconductor so I'll try the
bioconductor mailing list as well.  

Also and more importantly I took your suggestion and stepped through the
calls.   It seems the failure in pm.getabst() occurs when creating absts
with the pubmed function.  When I'm back at work, I'll have to research
why it fails further.  Right now, while I'm at home, it works
flawlessly.

Any reason this would fail behind a corporate firewall as opposed to my
home network?

Oh and my apologies for posting so hastily without the full
information.  R - 1.9.1, annotate - 1.4.0, RSXML - 0.97, all on a
Windows2000 OS.

Thanks,

Mark Larsen


On Thu, 2004-08-12 at 17:28, Robert Gentleman wrote: 
> You will almost surely do better to ask about Bioconductor packages on
> the Bioconductor mailing list. Next, it is helpful to know what
> versions of things you are using.
> 
> As for your problem, did you look to see what kind of object absts is?
> There seems to be no default method for xmlRoot, and it is likely that
> the call to create the absts object failed (prior to this). You might
> want to try stepping through the commands, one at a time and checking
> each step. Often, the problem arises because you have not properly set
> up your connection to the internet and so none of the querying
> software will work.
> 
>   Robert
> 
> On Thu, Aug 12, 2004 at 08:20:10PM +0000, larsenmtl at comcast.net wrote:
> > R Users:
> > 
> > After installing Bioconductor, RSXML and all the relevant Win32 DLLs (libxml2, zlib, iconv), I receive the following error message when using pm.getabst()
> > 
> > Error in xmlRoot(absts) : no applicable method for "xmlRoot"
> > 
> > I receive this when using the example from help(pm.getabst).  
> > 
> > Downloading the target XML file, parsing it with xmlTreeParse and applying xmlRoot returns no error.
> > 
> > Your thoughts/suggestions are appreciated.
> > 
> > Mark Larsen
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From m.g.walker at massey.ac.nz  Fri Aug 13 04:55:49 2004
From: m.g.walker at massey.ac.nz (Matthew Walker)
Date: Fri, 13 Aug 2004 14:55:49 +1200
Subject: [R] Object oriented programming resources
Message-ID: <411C2DB5.7030206@massey.ac.nz>

Hi,

I'm looking for resources to read about the object-oriented features of R.

I have looked through the "Manuals" page on r-project.org.  The most 
useful of the documents seemed to be the "draft of the R language 
definition".  However it had only about 6 pages on the topic. 

I have also used Google, but my problem here is that "R" appears in a 
*lot* of webpages!  I tried limiting the search by using 
"site:r-project.org", but didn't find anything very useful.

Specifically, I'm trying to find information on "member variables" (I 
think that's the correct term), as I'd like to copy this concept from C++:

class a {
  ...
private:
  int x;  // I think the term for this is a member variable
};

Thanks for your thoughts,

Matthew



From vograno at evafunds.com  Fri Aug 13 05:06:43 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Thu, 12 Aug 2004 20:06:43 -0700
Subject: [R] Object oriented programming resources
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A563D213@phost015.EVAFUNDS.intermedia.net>

The Bioconductor project posts a short tutorial "A guide to using S4
Objects" under "Developer Page" frame. I've found it useful. 

Note that R-s S4-classes approach to OOP is very different from the one
of C++ or Java. Yet you will find member vars, they are called slots.

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Matthew Walker
> Sent: Thursday, August 12, 2004 7:56 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Object oriented programming resources
> 
> Hi,
> 
> I'm looking for resources to read about the object-oriented 
> features of R.
> 
> I have looked through the "Manuals" page on r-project.org.  
> The most useful of the documents seemed to be the "draft of 
> the R language definition".  However it had only about 6 
> pages on the topic. 
> 
> I have also used Google, but my problem here is that "R" appears in a
> *lot* of webpages!  I tried limiting the search by using 
> "site:r-project.org", but didn't find anything very useful.
> 
> Specifically, I'm trying to find information on "member 
> variables" (I think that's the correct term), as I'd like to 
> copy this concept from C++:
> 
> class a {
>   ...
> private:
>   int x;  // I think the term for this is a member variable };
> 
> Thanks for your thoughts,
> 
> Matthew
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ozric at web.de  Fri Aug 13 08:25:51 2004
From: ozric at web.de (Christian Schulz)
Date: Fri, 13 Aug 2004 08:25:51 +0200
Subject: [R] Fwd: Timebased predictions in postgresql.
In-Reply-To: <200408122152.17728.nicolai@catpipe.net>
References: <200408122152.17728.nicolai@catpipe.net>
Message-ID: <200408130825.51302.ozric@web.de>

Hi,

hints for your  r-related  day ahead predictions you find in the second part 
of Louis Torgo's very usefuel: DataMining with R

http://www.liacc.up.pt/~ltorgo/ DataMiningWithR/PDF/DataMiningWithR.pdf 

And for your PL/SQL intention:
http://www.omegahat.org/RSPostgres/

christian


Am Donnerstag, 12. August 2004 21:52 schrieb Nicolai Petri:
> Hi r-people :)
>
> I'm sorry to disturb but I must admit that I know amazingly little about R
> and similar statistics-packages/languages and I'm kind of lost on where to
> start. I'm currently working on a datacollection framework for postgresql
> (The db doesn't really matter except that I hope to use PL/R) and I would
> like to be able to predict future values preferable 1 day or more ahead.
> The highest resolution on the historic data is 4 minutes but I'm already
> resampling that to whatever I need, so if it would be better to use 30min
> or other reasolution (because of performance) it would be perfectly ok.
>
> The types of statistics in the database is typically network io/ cpu usage,
> temperatur, etc. and I will rarely have holes in the history.
>
> Can anyone tell me how (or give me a hint) I can predict traffic or similar
> maybe one or more days ahead when I have the data xxdays back ? (And many
> how many days / which interval would be optimal for best
> performance/precision).
>
> You can also tell me it's impossible, but I think it could be really cool
> to present graphs of expected cpu-load or network IO to our users.
>
> Can it be done in R (and PL/R) ?
>
> Best regards,
> Nicolai Petri
> catpipe Systems Aps
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Fri Aug 13 08:40:17 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 13 Aug 2004 07:40:17 +0100 (BST)
Subject: [R] How to use the whole dataset (including between events) in
	Cox model (time-varying covariates) ?
In-Reply-To: <000501c480d1$4eb6cea0$3ccb9853@amd>
Message-ID: <Pine.LNX.4.44.0408130736250.18608-100000@gannet.stats>

This is the consequence of the use of partial likelihood in the Cox model.
You should read the literature on this point (for example, have you read
Cox, 1972 and all its discussion, or Anderson, Borgan, Gill & Keiding?).

It is not an R question.  You need to make more assumptions, such as a 
smooth baseline hazard, and you can always use parametric models and a 
full likelihood (but you may have to program them yourself).


On Fri, 13 Aug 2004, Mayeul KAUFFMANN wrote:

> Hello,
> 
> coxph does not use any information that are in the dataset between event
> times (or "death times") , since computation only occurs at event  times.
> 
> For instance, removing observations when there is no event at that time in
> the whole dataset does not change the results:
> > set.seed(1)
> > data <-
> as.data.frame(cbind(start=c(1:5,1:5,1:4),stop=c(2:6,2:6,2:5),status=c(rep(
> 0,7),1,rep(0,5),1),id=c(rep(1,5),rep(2,5),rep(3,4)),x1=rnorm(14)))
> > data
> start stop status id x1
> 1 1 2 0 1 -0.6264538
> 2 2 3 0 1 0.1836433
> 3 3 4 0 1 -0.8356286
> 4 4 5 0 1 1.5952808
> 5 5 6 0 1 0.3295078
> 6 1 2 0 2 -0.8204684
> 7 2 3 0 2 0.4874291
> 8 3 4 1 2 0.7383247
> 9 4 5 0 2 0.5757814
> 10 5 6 0 2 -0.3053884
> 11 1 2 0 3 1.5117812
> 12 2 3 0 3 0.3898432
> 13 3 4 0 3 -0.6212406
> 14 4 5 1 3 -2.2146999
> coxph(Surv(start, stop,status)~ cluster(id)+x1,data=data ,robust=T)
> coxph(Surv(start, stop,status)~ cluster(id)+x1,data=subset(data,stop %in%
> 4:5) ,robust=T) # the same !!! (except n)
> 
> First, some data is lost.
> Second, this loss could be an important problem when  there is a
> time-varying covariate that changes quicker than the frequency  of events.
> Specifically, I have a covariate which has low values most of the time. It
> sometimes jumps to high values and that is hypothesized as greatly
> increasing the risk of an event.
> With rare events, the effect of this covariate will only be measured at
> event times. Chances are that the only time such a covariate is recorded
> at high level, the individual for which it is measured as being high is
> having an event.
> This may bias the estimated coefficient.
> 
> Here is my question:
> How to fully use the dataset?
> 
> (that is: how to have really _time-varying_ covariates (even if they
> change step by step, not continuously), not covariates whose changes are
> measured only at event time )
> 
> Ideally, the full dataset would be use to estimate the parameters, or at
> least to estimate the standard error of the estimated parameters.
> Any ideas ???
> .
> .
> .
> 
> A second best (which might require less work) would be to use all the
> dataset to assess the predictive power of the model.
> 
> Maybe by using the expected number of events for an individual over the
> time interval that they were observed to be at risk
> > predict(coxfit,type="expected")
> and compare it with observed number of events
> (does it use all data and takes into account all the baseline hazard, even
> between events?)
> 
> 
> Or, if not,  following Brian D. Ripley suggestion about the baseline
> hazard: "As an approximation you can smooth the fitted
> baseline cumulative hazard (e.g. by package pspline) and ask for its
> derivative" (https://stat.ethz.ch/pipermail/r-help/2004-July/052376.html)
> 
> the following code could be use to estimate (and plot) a smooth baseline
> hazard:
> > t<-seq(min(data$start),max(data$stop),length=100)
> > lines(t,
> predict(sm.spline(x=basehaz(coxfit)[,2],y=basehaz(coxfit)[,1],norder=2),
> t,1))
> #there is a problem with this code. One should add the contraint that the
> baseline hazard cannot be negative.
> 
> The following computes the parametric part of the cox model.
> > risk <- predict(coxfit, type='risk') # gives exp(X'b)
> 
> something like
> > baseline.hazard*risk
> would give the true risk at any time (but it would be probably much harder
> to compute)
> 
> which could help assess the predictive power of the model.
> (still a lot of work)
> 
> Thanks in advance for any help or comment.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Carsten.Colombier at efv.admin.ch  Fri Aug 13 09:08:55 2004
From: Carsten.Colombier at efv.admin.ch (Carsten.Colombier@efv.admin.ch)
Date: Fri, 13 Aug 2004 09:08:55 +0200
Subject: [R] Robust R^2
Message-ID: <2CAE512CEB72EE448AADE3444E1FB7185B488B@ad04mexefd3.ad.admin.ch>

Dear R-users,

Do you know if there is any function in R available which calculates an R^2
for a robust MM-regression?


With best regards,
Carsten Colombier

Dr. Carsten Colombier
Economist
Group of Economic Advisers
Swiss Federal Finance Administration
Bundesgasse 3
CH-3003 Bern

phone +41 31 322 63 32
fax +41 31 323 08 33
email: carsten.colombier at efv.admin.ch
www.efv.admin.ch



From ligges at statistik.uni-dortmund.de  Fri Aug 13 09:22:49 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 13 Aug 2004 09:22:49 +0200
Subject: [R] rgl snapshot command
In-Reply-To: <0E3F6658-EC88-11D8-9325-000A95BA95C8@kamradt.net>
References: <0E3F6658-EC88-11D8-9325-000A95BA95C8@kamradt.net>
Message-ID: <411C6C49.5070202@statistik.uni-dortmund.de>

Joern Kamradt wrote:

> Hi,
> I am using the rgl package for 3D display. Unfortunately, I am not able 
> to get the snapshot command running.
> I tried the following:
> 
>  > example(rgl.surface)
> 
> rgl.sr> data(volcano)
> rgl.sr> y <- 2 * volcano
> rgl.sr> x <- 10 * (1:nrow(y))
> rgl.sr> z <- 10 * (1:ncol(y))
> rgl.sr> ylim <- range(y)
> rgl.sr> ylen <- ylim[2] - ylim[1] + 1
> rgl.sr> colorlut <- terrain.colors(ylen)
> rgl.sr> col <- colorlut[y - ylim[1] + 1]
> rgl.sr> rgl.clear()
> rgl.sr> rgl.surface(x, z, y, color = col)
>  > rgl.snapshot(filename="./volcano.png",fmt="png")
> [1] "failed"
> 
> Any help is highly appreciated

Works for me on WinNT4, R-1.9.1, rgl-0.64-13.

Please read the posting guide to learn that you missed to specify:
OS (and its version), R version, rgl version. Also, for questions like 
this, it is a good idea to ask the package maintainer.

Uwe Ligges


> Joern
> 
> 
> 
> __________________________
> 
> Joern Kamradt, MD
> Cancer Genetic Branch
> National Human Genome Research Institute
> National Institutes of Health
> 50 South Drive, building 50, room 5147
> Bethesda, MD 20892-8000, USA
> Phone#: +1 (301) 496 5382
> FAX#:     +1 (301) 402 3241
> Email: jkamradt at nhgri.nih.gov
> Website: www.genome.gov
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From joehl at gmx.de  Fri Aug 13 10:45:09 2004
From: joehl at gmx.de (=?ISO-8859-1?Q?=22Jens_Oehlschl=E4gel=22?=)
Date: Fri, 13 Aug 2004 10:45:09 +0200 (MEST)
Subject: [R] truly object oriented programming in R
Message-ID: <30361.1092386709@www6.gmx.net>

Dear Jason,

Of course you can do almost eveything in R, but I have rarely seen someone
prototyping something in Java to then implement it in R. KD-trees are for
performance, and interpreted R is not really fast. So interfacing some
foreign code might really be the better choice here.

Opportunity 1
=============
Since you have already Java code, the most straightforward choice might seem
interfacing the Java code via SJava or RJava. However, though there are
packages RJava and SJava, it looks like there is no recommended Java
interface which is maintained across all relevant platforms, and thus
nothing at CRAN. Given the richness of available Java software, this is a
pitty, but a reality. I am currently trying to remove remaining memory leaks
from SJava 0.66 for Windows, but this is a one time effort, not a
maintenance promise.

Opportunity 2
=============
Get some standard KD-tree C-code and modify it to your needs. Then interface
it using the C- or Call-interface, to write an access function for every
operation you want to perform on the KD-tree from R.

Opportunity 3
=============
Write your own C-code operating directly on R-data structures. In this case
you do not only interface a KD-tree, you really have it in R but you can
have the most important operations coded in C. But this is the technically
most challenging one. You need to to know C, read "Writing R Extensions"
very carefully, you need to understand the sets of macros defined in
Rdefines.h and Rinternals.h, especially you need to understand all the
PROTECT macros and how to avoid overloading the protect-stack, because you
will need to reprotect your KD-tree very often.

Hope that helps

Best


Jens Oehlschl??gel

-- 
NEU: WLAN-Router f??r 0,- EUR* - auch f??r DSL-Wechsler!
GMX DSL = superg??nstig & kabellos http://www.gmx.net/de/go/dsl



From luc at ifc.cnr.it  Fri Aug 13 10:48:59 2004
From: luc at ifc.cnr.it (luc)
Date: Fri, 13 Aug 2004 10:48:59 +0200
Subject: [R] how to plot an array with labels
Message-ID: <9E198BDA-ED05-11D8-9800-000D93356BF8@ifc.cnr.it>

How can i plot an array and instead of having on the x labels the 
indexes of the array I want to display an other String array of the 
same length



From david_foreman at doctors.org.uk  Fri Aug 13 10:08:38 2004
From: david_foreman at doctors.org.uk (david_foreman@doctors.org.uk)
Date: Fri, 13 Aug 2004 10:08:38 (GMT)
Subject: [R] Re: extracting datasets from aregImpute objects
Message-ID: <1092391719_117440@drn10msi01>

I've tried doing this by specifying x=TRUE, which provides me with a single imputation, that has been useful.  However, the help file possibly suggests that I should get a flat-file matrix of n.impute imputations, presumably with indexing.  I'm a bit stuck using alternatives to aregImpute, as neither MICE nor Amelia seem to like my dataset, and Frank Harrell no longer recommends Transcan for multiple imputations.


_______________________________________________________________________
Most doctors use http://www.Doctors.net.uk e-mail.
Move to a free professional address with spam and virus protection.



From ripley at stats.ox.ac.uk  Fri Aug 13 12:24:39 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 13 Aug 2004 11:24:39 +0100 (BST)
Subject: [R] Robust R^2
In-Reply-To: <2CAE512CEB72EE448AADE3444E1FB7185B488B@ad04mexefd3.ad.admin.ch>
Message-ID: <Pine.LNX.4.44.0408131121480.2156-100000@gannet.stats>

On Fri, 13 Aug 2004 Carsten.Colombier at efv.admin.ch wrote:

> Do you know if there is any function in R available which calculates an R^2
> for a robust MM-regression?

It's very easy to do, *but* why would you want a non-robust measure for a 
robust fit?

[Just 1 - sum(residuals(fit)^2)/sum((y-mean(y))^2), for suitable y which 
model.extract will give you.  Unless you have no intercept or weights, of 
course.]

Or were you looking for a robust alternative to R^2?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mayeul.kauffmann at tiscali.fr  Fri Aug 13 12:37:01 2004
From: mayeul.kauffmann at tiscali.fr (Mayeul KAUFFMANN)
Date: Fri, 13 Aug 2004 12:37:01 +0200
Subject: [R] How to use the whole dataset (including between events) in
	Cox model (time-varying covariates) ?
References: <Pine.LNX.4.44.0408130736250.18608-100000@gannet.stats>
Message-ID: <000e01c48121$7838aaa0$29d59a53@amd>


> you can always use parametric models and a
> full likelihood (but you may have to program them yourself).
> Prof Brian Ripley

I started trying this but I could not make the counting process notation
work on this.
(Andersen, P.K. and Gill, R.D. (1982). Cox's regression model for counting
processes: A large sample study. Ann. stat. 10 , 1100-1120).
I think it is only (currently) available for Cox model with R.

survreg(Surv(start, stop,status)~  x1,data=data )
Error in survreg(Surv(start, stop, status) ~ x1, data = data) :
 Invalid survival type



From mayeul.kauffmann at tiscali.fr  Fri Aug 13 13:00:56 2004
From: mayeul.kauffmann at tiscali.fr (Mayeul KAUFFMANN)
Date: Fri, 13 Aug 2004 13:00:56 +0200
Subject: [R] how to plot an array with labels
Message-ID: <001501c48124$cf575770$29d59a53@amd>

>How can i plot an array and instead of having on the x labels the 
>indexes of the array I want to display an other String array of the 
>same length

Do this:
> plot(myarray,xaxt="n",xlab="")
> axis(1,at=1:length(myarray),lab=my.vector.of.names)



From Luisr at frs.fo  Fri Aug 13 13:40:24 2004
From: Luisr at frs.fo (Luis Rideau Cruz)
Date: Fri, 13 Aug 2004 12:40:24 +0100
Subject: [R] lapply problem
Message-ID: <s11cb6ba.049@ffdata.setur.fo>

R-help,

I wish to replace NULL elements(or missing) in the following list :
> z2
$cod
    mean       sd 
62.56190 12.65452 

$haddock
    mean       sd 
36.61490 11.50365 

$ling
    mean       sd 
86.17949 20.43587 

$saithe
     mean        sd 
50.275847  5.453606 

$whiting
NULL

$"norway pout"
     mean        sd 
13.739623  1.393104 

$"great silver smelt"
     mean        sd 
25.600000  1.549193 

$"norway hadock"
     mean        sd 
24.685225  3.902989

If I use 
lapply(z2,function(x) ifelse(is.null(x),0,as.numeric(x))) 

I get 

> z2
$cod
[1] 62.5619

$haddock
[1] 36.6149

$ling
[1] 86.17949

$saithe
[1] 50.27585

$whiting
[1] 0

$"norway pout"
[1] 13.73962

$"great silver smelt"
[1] 25.6

$"norway hadock"
[1] 24.68522

i.e,only the first element in z2

And if I use

> z2<-lapply(msd.sumin.Z2S,function(x) ifelse(is.na(x),0,x))
Warning message: 
is.na() applied to non-(list or vector) in: is.na(x) 
> z2
$cod
    mean       sd 
62.56190 12.65452 

$haddock
    mean       sd 
36.61490 11.50365 

$ling
    mean       sd 
86.17949 20.43587 

$saithe
     mean        sd 
50.275847  5.453606 

$whiting
logical(0)

$"norway pout"
     mean        sd 
13.739623  1.393104 

$"great silver smelt"
     mean        sd 
25.600000  1.549193 

$"norway hadock"
     mean        sd 
24.685225  3.902989 



which cannot be coreced to data frame(which is my aim)

How can it be resolve??

Thank you


Luis Ridao Cruz
Fiskiranns??knarstovan
N??at??n 1
P.O. Box 3051
FR-110 T??rshavn
Faroe Islands
Phone:             +298 353900
Phone(direct): +298 353912
Mobile:             +298 580800
Fax:                 +298 353901
E-mail:              luisr at frs.fo
Web:                www.frs.fo



From dimitris.rizopoulos at med.kuleuven.ac.be  Fri Aug 13 13:47:55 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Fri, 13 Aug 2004 13:47:55 +0200
Subject: [R] lapply problem
References: <s11cb6ba.049@ffdata.setur.fo>
Message-ID: <002a01c4812b$62f29b60$ad133a86@www.domain>

Hi Luis,

try to use,


lapply(z2, function(x) if(is.null(x)) c(0,0) else x)


I hope this helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Doctoral Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "Luis Rideau Cruz" <Luisr at frs.fo>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, August 13, 2004 1:40 PM
Subject: [R] lapply problem


> R-help,
>
> I wish to replace NULL elements(or missing) in the following list :
> > z2
> $cod
>     mean       sd
> 62.56190 12.65452
>
> $haddock
>     mean       sd
> 36.61490 11.50365
>
> $ling
>     mean       sd
> 86.17949 20.43587
>
> $saithe
>      mean        sd
> 50.275847  5.453606
>
> $whiting
> NULL
>
> $"norway pout"
>      mean        sd
> 13.739623  1.393104
>
> $"great silver smelt"
>      mean        sd
> 25.600000  1.549193
>
> $"norway hadock"
>      mean        sd
> 24.685225  3.902989
>
> If I use
> lapply(z2,function(x) ifelse(is.null(x),0,as.numeric(x)))
>
> I get
>
> > z2
> $cod
> [1] 62.5619
>
> $haddock
> [1] 36.6149
>
> $ling
> [1] 86.17949
>
> $saithe
> [1] 50.27585
>
> $whiting
> [1] 0
>
> $"norway pout"
> [1] 13.73962
>
> $"great silver smelt"
> [1] 25.6
>
> $"norway hadock"
> [1] 24.68522
>
> i.e,only the first element in z2
>
> And if I use
>
> > z2<-lapply(msd.sumin.Z2S,function(x) ifelse(is.na(x),0,x))
> Warning message:
> is.na() applied to non-(list or vector) in: is.na(x)
> > z2
> $cod
>     mean       sd
> 62.56190 12.65452
>
> $haddock
>     mean       sd
> 36.61490 11.50365
>
> $ling
>     mean       sd
> 86.17949 20.43587
>
> $saithe
>      mean        sd
> 50.275847  5.453606
>
> $whiting
> logical(0)
>
> $"norway pout"
>      mean        sd
> 13.739623  1.393104
>
> $"great silver smelt"
>      mean        sd
> 25.600000  1.549193
>
> $"norway hadock"
>      mean        sd
> 24.685225  3.902989
>
>
>
> which cannot be coreced to data frame(which is my aim)
>
> How can it be resolve??
>
> Thank you
>
>
> Luis Ridao Cruz
> Fiskiranns??knarstovan
> N??at??n 1
> P.O. Box 3051
> FR-110 T??rshavn
> Faroe Islands
> Phone:             +298 353900
> Phone(direct): +298 353912
> Mobile:             +298 580800
> Fax:                 +298 353901
> E-mail:              luisr at frs.fo
> Web:                www.frs.fo
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Fri Aug 13 13:48:03 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 13 Aug 2004 13:48:03 +0200
Subject: [R] lapply problem
In-Reply-To: <s11cb6ba.049@ffdata.setur.fo>
References: <s11cb6ba.049@ffdata.setur.fo>
Message-ID: <x2smar2r9o.fsf@biostat.ku.dk>

"Luis Rideau Cruz" <Luisr at frs.fo> writes:

> R-help,
> 
> I wish to replace NULL elements(or missing) in the following list :
...
> If I use 
> lapply(z2,function(x) ifelse(is.null(x),0,as.numeric(x))) 
...
> i.e,only the first element in z2

ifelse(....) gives a result that is the same length as the condition,
and is.null(x) has length 1. You want

if (is.null(x)) 0 else as.numeric(x)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From l.houdusse at cerep.fr  Fri Aug 13 13:56:01 2004
From: l.houdusse at cerep.fr (Laurent Houdusse)
Date: Fri, 13 Aug 2004 13:56:01 +0200
Subject: [R] simtest for Dunnett's test
Message-ID: <BA420EFAAC96D311A7A0006097D37BDB04515CFA@EOLE>

Hi!

I use simtest fonction of multcomp package to compile a Dunnett's test.
I have 10 treatments and one control group, so i create a matrix with:

m<-matrix(0,10,11)
m[1,1]<--1
m[1,2]<-1
m[2,1]<--1
m[2,3]<-1
m[3,1]<--1
m[3,4]<-1
m[4,1]<--1
m[4,5]<-1
m[5,1]<--1
m[5,6]<-1
m[6,1]<--1
m[6,7]<-1
m[7,1]<--1
m[7,8]<-1
m[8,1]<--1
m[8,9]<-1
m[9,1]<--1
m[9,10]<-1
m[10,1]<--1
m[10,11]<-1
rownames(m)<-c("vehicle+vehicle vs. morphne
60mg/kg+vehicle","vehicle+vehicle vs. Naloxone
30mg/kg+vehicle","vehicle+vehicle vs. Naloxone 30mg/kg+Morphine
60mg/kg","vehicle+vehicle vs. RWJ-69528-000-B
0.1mg/kg+vehicle","vehicle+vehicle vs. RWJ-69528-000-B
1mg/kg+vehicle","vehicle+vehicle vs. RWJ-69528-000-B 0.1mg/kg+morphine
60mg/kg","vehicle+vehicle vs. RWJ-69528-000-B 1mg/kg+morphine
60mg/kg","vehicle+vehicle vs. vehicle+acetaminophen
600mg/kg","vehicle+vehicle vs. RWJ-69528-000-B 0.1mg/kg+acetaminophen
600mg/kg","vehicle+vehicle vs. RWJ-69528-000-B 1mg/kg+acetaminophen
600mg/kg")
y<-c(21.5,14.7,16.7,18.8,17.3,15.527,17.551,17.199,15.895,14.388,30,30,30,21
.2,14.6,30.83,30.622,30.465,24.563,31.416,30,30,30,30,19.6,17.61,30.346,20.8
,17.045,18.129,17.9,15.4,30,15.9,22.9,31.909,25.633,24.6,20.355,18.807,30,18
.9,16.4,18.2,22.3,21.3,28.571,17.357,16.983,30.40,26.3,24.4,25,24.1,14.4,10.
256,13.266,29.6,15.344,18.69,30,30,25.7,30,28.5,30,30.32,30,19.879,30.716,25
.2,30,30,30,30,30,14.063,28.811,30.396,30.609,30,14.1,30,30,30,26.322,30.386
,28.583,15.77,29.341,30,17.1,30,30,30,30,20.451,30,23.967,30.507,30,30,30,27
.7,30,28.5,22.568,32.379,30.374,28.263)
h<-factor(c(rep(1:11, each = 10)))
result<-summary(simtest(y ~ h,cmatrix=cbind(0,m)))
rownames(result$estimate)
result


I want to compare my results with the results obtained with SigmaStat
Software.
In my results, i retrieve a correct q' value with the simtest t value
How can i say if P<0.05 like the SigmStat results?
Can i use p raw value, p bonf value, p adj value or other to compare
directly? Or can i use the Dunnett's tables?

Thanks for your help!




Laurent Houdusse 
Analyste Programmeur



From andy_liaw at merck.com  Fri Aug 13 14:05:44 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 13 Aug 2004 08:05:44 -0400
Subject: [R] simtest for Dunnett's test
Message-ID: <3A822319EB35174CA3714066D590DCD504AF821D@usrymx25.merck.com>

Before you do any of that, you should realize the fact that simtest does
_not_ do Dunnett's test:  It can use `Dunnett' _contrasts_ to do comparisons
with a control, but the actual procedure is different.  You are unlikely to
get the same result as other packages that perform standard Dunnett's test
(or Tukey's, for that matter).  

Andy

> From: Laurent Houdusse
> 
> Hi!
> 
> I use simtest fonction of multcomp package to compile a 
> Dunnett's test.
> I have 10 treatments and one control group, so i create a matrix with:
> 
> m<-matrix(0,10,11)
> m[1,1]<--1
> m[1,2]<-1
> m[2,1]<--1
> m[2,3]<-1
> m[3,1]<--1
> m[3,4]<-1
> m[4,1]<--1
> m[4,5]<-1
> m[5,1]<--1
> m[5,6]<-1
> m[6,1]<--1
> m[6,7]<-1
> m[7,1]<--1
> m[7,8]<-1
> m[8,1]<--1
> m[8,9]<-1
> m[9,1]<--1
> m[9,10]<-1
> m[10,1]<--1
> m[10,11]<-1
> rownames(m)<-c("vehicle+vehicle vs. morphne
> 60mg/kg+vehicle","vehicle+vehicle vs. Naloxone
> 30mg/kg+vehicle","vehicle+vehicle vs. Naloxone 30mg/kg+Morphine
> 60mg/kg","vehicle+vehicle vs. RWJ-69528-000-B
> 0.1mg/kg+vehicle","vehicle+vehicle vs. RWJ-69528-000-B
> 1mg/kg+vehicle","vehicle+vehicle vs. RWJ-69528-000-B 0.1mg/kg+morphine
> 60mg/kg","vehicle+vehicle vs. RWJ-69528-000-B 1mg/kg+morphine
> 60mg/kg","vehicle+vehicle vs. vehicle+acetaminophen
> 600mg/kg","vehicle+vehicle vs. RWJ-69528-000-B 0.1mg/kg+acetaminophen
> 600mg/kg","vehicle+vehicle vs. RWJ-69528-000-B 1mg/kg+acetaminophen
> 600mg/kg")
> y<-c(21.5,14.7,16.7,18.8,17.3,15.527,17.551,17.199,15.895,14.3
> 88,30,30,30,21
> .2,14.6,30.83,30.622,30.465,24.563,31.416,30,30,30,30,19.6,17.
> 61,30.346,20.8
> ,17.045,18.129,17.9,15.4,30,15.9,22.9,31.909,25.633,24.6,20.35
> 5,18.807,30,18
> .9,16.4,18.2,22.3,21.3,28.571,17.357,16.983,30.40,26.3,24.4,25
> ,24.1,14.4,10.
> 256,13.266,29.6,15.344,18.69,30,30,25.7,30,28.5,30,30.32,30,19
> .879,30.716,25
> .2,30,30,30,30,30,14.063,28.811,30.396,30.609,30,14.1,30,30,30
> ,26.322,30.386
> ,28.583,15.77,29.341,30,17.1,30,30,30,30,20.451,30,23.967,30.5
> 07,30,30,30,27
> .7,30,28.5,22.568,32.379,30.374,28.263)
> h<-factor(c(rep(1:11, each = 10)))
> result<-summary(simtest(y ~ h,cmatrix=cbind(0,m)))
> rownames(result$estimate)
> result
> 
> 
> I want to compare my results with the results obtained with SigmaStat
> Software.
> In my results, i retrieve a correct q' value with the simtest t value
> How can i say if P<0.05 like the SigmStat results?
> Can i use p raw value, p bonf value, p adj value or other to compare
> directly? Or can i use the Dunnett's tables?
> 
> Thanks for your help!
> 
> 
> 
> 
> Laurent Houdusse 
> Analyste Programmeur
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From l.houdusse at cerep.fr  Fri Aug 13 14:16:59 2004
From: l.houdusse at cerep.fr (Laurent Houdusse)
Date: Fri, 13 Aug 2004 14:16:59 +0200
Subject: [R] simtest for Dunnett's test
Message-ID: <BA420EFAAC96D311A7A0006097D37BDB04515CFB@EOLE>

However, on several tests, my results are correct: 
The t-value seems always to correspond to the q'value calculated by
SigmaStat...  

So,What do I have to use to find the same results as SigmaStat (value and
comparaison)?


Laurent Houdusse
Analyste Programmeur



-----Message d'origine-----
De : Liaw, Andy [mailto:andy_liaw at merck.com] 
Envoy?? : vendredi 13 ao??t 2004 14:06
?? : Laurent Houdusse; 'r-help at stat.math.ethz.ch'
Objet : RE: [R] simtest for Dunnett's test


Before you do any of that, you should realize the fact that simtest does
_not_ do Dunnett's test:  It can use `Dunnett' _contrasts_ to do comparisons
with a control, but the actual procedure is different.  You are unlikely to
get the same result as other packages that perform standard Dunnett's test
(or Tukey's, for that matter).  

Andy

> From: Laurent Houdusse
> 
> Hi!
> 
> I use simtest fonction of multcomp package to compile a
> Dunnett's test.
> I have 10 treatments and one control group, so i create a matrix with:
> 
> m<-matrix(0,10,11)
> m[1,1]<--1
> m[1,2]<-1
> m[2,1]<--1
> m[2,3]<-1
> m[3,1]<--1
> m[3,4]<-1
> m[4,1]<--1
> m[4,5]<-1
> m[5,1]<--1
> m[5,6]<-1
> m[6,1]<--1
> m[6,7]<-1
> m[7,1]<--1
> m[7,8]<-1
> m[8,1]<--1
> m[8,9]<-1
> m[9,1]<--1
> m[9,10]<-1
> m[10,1]<--1
> m[10,11]<-1
> rownames(m)<-c("vehicle+vehicle vs. morphne 
> 60mg/kg+vehicle","vehicle+vehicle vs. Naloxone 
> 30mg/kg+vehicle","vehicle+vehicle vs. Naloxone 30mg/kg+Morphine 
> 60mg/kg","vehicle+vehicle vs. RWJ-69528-000-B 
> 0.1mg/kg+vehicle","vehicle+vehicle vs. RWJ-69528-000-B 
> 1mg/kg+vehicle","vehicle+vehicle vs. RWJ-69528-000-B 0.1mg/kg+morphine 
> 60mg/kg","vehicle+vehicle vs. RWJ-69528-000-B 1mg/kg+morphine 
> 60mg/kg","vehicle+vehicle vs. vehicle+acetaminophen 
> 600mg/kg","vehicle+vehicle vs. RWJ-69528-000-B 0.1mg/kg+acetaminophen 
> 600mg/kg","vehicle+vehicle vs. RWJ-69528-000-B 1mg/kg+acetaminophen
> 600mg/kg") 
> y<-c(21.5,14.7,16.7,18.8,17.3,15.527,17.551,17.199,15.895,14.3
> 88,30,30,30,21 
> .2,14.6,30.83,30.622,30.465,24.563,31.416,30,30,30,30,19.6,17.
> 61,30.346,20.8 
> ,17.045,18.129,17.9,15.4,30,15.9,22.9,31.909,25.633,24.6,20.35
> 5,18.807,30,18 
> .9,16.4,18.2,22.3,21.3,28.571,17.357,16.983,30.40,26.3,24.4,25
> ,24.1,14.4,10. 
> 256,13.266,29.6,15.344,18.69,30,30,25.7,30,28.5,30,30.32,30,19
> .879,30.716,25 
> .2,30,30,30,30,30,14.063,28.811,30.396,30.609,30,14.1,30,30,30
> ,26.322,30.386 
> ,28.583,15.77,29.341,30,17.1,30,30,30,30,20.451,30,23.967,30.5
> 07,30,30,30,27
> .7,30,28.5,22.568,32.379,30.374,28.263)
> h<-factor(c(rep(1:11, each = 10)))
> result<-summary(simtest(y ~ h,cmatrix=cbind(0,m)))
> rownames(result$estimate)
> result
> 
> 
> I want to compare my results with the results obtained with SigmaStat 
> Software. In my results, i retrieve a correct q' value with the 
> simtest t value How can i say if P<0.05 like the SigmStat results?
> Can i use p raw value, p bonf value, p adj value or other to compare
> directly? Or can i use the Dunnett's tables?
> 
> Thanks for your help!
> 
> 
> 
> 
> Laurent Houdusse
> Analyste Programmeur
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 


----------------------------------------------------------------------------
--
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From Torsten.Hothorn at rzmail.uni-erlangen.de  Fri Aug 13 14:18:42 2004
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Fri, 13 Aug 2004 14:18:42 +0200 (CEST)
Subject: [R] simtest for Dunnett's test
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF821D@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF821D@usrymx25.merck.com>
Message-ID: <Pine.LNX.4.51.0408131415030.25161@artemis.imbe.med.uni-erlangen.de>


On Fri, 13 Aug 2004, Liaw, Andy wrote:

> Before you do any of that, you should realize the fact that simtest does
> _not_ do Dunnett's test:  It can use `Dunnett' _contrasts_ to do comparisons
> with a control, but the actual procedure is different.  You are unlikely to
> get the same result as other packages that perform standard Dunnett's test
> (or Tukey's, for that matter).
>

exactly. btw, you don't need to specify the contrast matrix
yourself for this type of question:

R> summary(simtest(y ~ h))

         Simultaneous tests: Dunnett contrasts

Call:
simtest.formula(formula = y ~ h)

         Dunnett contrasts for factor h

<...>

Absolute Error Tolerance:  0.001

Coefficients:
       Estimate t value Std.Err. p raw p Bonf p adj
h11-h1   12.022  -5.272     2.28 0.000  0.000 0.000
h7-h1    11.556  -5.067     2.28 0.000  0.000 0.000
h8-h1    10.952  -4.803     2.28 0.000  0.000 0.000
h2-h1    10.414  -4.567     2.28 0.000  0.000 0.000
h10-h1   10.247  -4.493     2.28 0.000  0.000 0.000
h9-h1     9.494  -4.163     2.28 0.000  0.000 0.000
h3-h1     7.397  -3.244     2.28 0.002  0.006 0.006
h4-h1     5.384  -2.361     2.28 0.020  0.061 0.053
h5-h1     5.085  -2.230     2.28 0.028  0.061 0.053
h6-h1     3.180  -1.394     2.28 0.166  0.166 0.166

does exactly the same because the `type' argument defaults to `Dunnett'.

Best,

Torsten

> Andy
>
> > From: Laurent Houdusse
> >
> > Hi!
> >
> > I use simtest fonction of multcomp package to compile a
> > Dunnett's test.
> > I have 10 treatments and one control group, so i create a matrix with:
> >
> > m<-matrix(0,10,11)
> > m[1,1]<--1
> > m[1,2]<-1
> > m[2,1]<--1
> > m[2,3]<-1
> > m[3,1]<--1
> > m[3,4]<-1
> > m[4,1]<--1
> > m[4,5]<-1
> > m[5,1]<--1
> > m[5,6]<-1
> > m[6,1]<--1
> > m[6,7]<-1
> > m[7,1]<--1
> > m[7,8]<-1
> > m[8,1]<--1
> > m[8,9]<-1
> > m[9,1]<--1
> > m[9,10]<-1
> > m[10,1]<--1
> > m[10,11]<-1
> > rownames(m)<-c("vehicle+vehicle vs. morphne
> > 60mg/kg+vehicle","vehicle+vehicle vs. Naloxone
> > 30mg/kg+vehicle","vehicle+vehicle vs. Naloxone 30mg/kg+Morphine
> > 60mg/kg","vehicle+vehicle vs. RWJ-69528-000-B
> > 0.1mg/kg+vehicle","vehicle+vehicle vs. RWJ-69528-000-B
> > 1mg/kg+vehicle","vehicle+vehicle vs. RWJ-69528-000-B 0.1mg/kg+morphine
> > 60mg/kg","vehicle+vehicle vs. RWJ-69528-000-B 1mg/kg+morphine
> > 60mg/kg","vehicle+vehicle vs. vehicle+acetaminophen
> > 600mg/kg","vehicle+vehicle vs. RWJ-69528-000-B 0.1mg/kg+acetaminophen
> > 600mg/kg","vehicle+vehicle vs. RWJ-69528-000-B 1mg/kg+acetaminophen
> > 600mg/kg")
> > y<-c(21.5,14.7,16.7,18.8,17.3,15.527,17.551,17.199,15.895,14.3
> > 88,30,30,30,21
> > .2,14.6,30.83,30.622,30.465,24.563,31.416,30,30,30,30,19.6,17.
> > 61,30.346,20.8
> > ,17.045,18.129,17.9,15.4,30,15.9,22.9,31.909,25.633,24.6,20.35
> > 5,18.807,30,18
> > .9,16.4,18.2,22.3,21.3,28.571,17.357,16.983,30.40,26.3,24.4,25
> > ,24.1,14.4,10.
> > 256,13.266,29.6,15.344,18.69,30,30,25.7,30,28.5,30,30.32,30,19
> > .879,30.716,25
> > .2,30,30,30,30,30,14.063,28.811,30.396,30.609,30,14.1,30,30,30
> > ,26.322,30.386
> > ,28.583,15.77,29.341,30,17.1,30,30,30,30,20.451,30,23.967,30.5
> > 07,30,30,30,27
> > .7,30,28.5,22.568,32.379,30.374,28.263)
> > h<-factor(c(rep(1:11, each = 10)))
> > result<-summary(simtest(y ~ h,cmatrix=cbind(0,m)))
> > rownames(result$estimate)
> > result
> >
> >
> > I want to compare my results with the results obtained with SigmaStat
> > Software.
> > In my results, i retrieve a correct q' value with the simtest t value
> > How can i say if P<0.05 like the SigmStat results?
> > Can i use p raw value, p bonf value, p adj value or other to compare
> > directly? Or can i use the Dunnett's tables?
> >
> > Thanks for your help!
> >
> >
> >
> >
> > Laurent Houdusse
> > Analyste Programmeur
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From l.houdusse at cerep.fr  Fri Aug 13 14:22:15 2004
From: l.houdusse at cerep.fr (Laurent Houdusse)
Date: Fri, 13 Aug 2004 14:22:15 +0200
Subject: [R] simtest for Dunnett's test
Message-ID: <BA420EFAAC96D311A7A0006097D37BDB04515CFC@EOLE>

Yes, but my control group is not always in the first colonne.
So, I can have to a matrix like this:
1  0  0  -1  0
0  1  0  -1  0
0  0  1  -1  0
0  0  0  -1  0

Laurent Houdusse
Analyste Programmeur



-----Message d'origine-----
De : Torsten Hothorn [mailto:Torsten.Hothorn at rzmail.uni-erlangen.de] 
Envoy?? : vendredi 13 ao??t 2004 14:19
?? : Liaw, Andy; Laurent Houdusse
Cc : 'r-help at stat.math.ethz.ch'
Objet : RE: [R] simtest for Dunnett's test



On Fri, 13 Aug 2004, Liaw, Andy wrote:

> Before you do any of that, you should realize the fact that simtest 
> does _not_ do Dunnett's test:  It can use `Dunnett' _contrasts_ to do 
> comparisons with a control, but the actual procedure is different.  
> You are unlikely to get the same result as other packages that perform 
> standard Dunnett's test (or Tukey's, for that matter).
>

exactly. btw, you don't need to specify the contrast matrix yourself for
this type of question:

R> summary(simtest(y ~ h))

         Simultaneous tests: Dunnett contrasts

Call:
simtest.formula(formula = y ~ h)

         Dunnett contrasts for factor h

<...>

Absolute Error Tolerance:  0.001

Coefficients:
       Estimate t value Std.Err. p raw p Bonf p adj
h11-h1   12.022  -5.272     2.28 0.000  0.000 0.000
h7-h1    11.556  -5.067     2.28 0.000  0.000 0.000
h8-h1    10.952  -4.803     2.28 0.000  0.000 0.000
h2-h1    10.414  -4.567     2.28 0.000  0.000 0.000
h10-h1   10.247  -4.493     2.28 0.000  0.000 0.000
h9-h1     9.494  -4.163     2.28 0.000  0.000 0.000
h3-h1     7.397  -3.244     2.28 0.002  0.006 0.006
h4-h1     5.384  -2.361     2.28 0.020  0.061 0.053
h5-h1     5.085  -2.230     2.28 0.028  0.061 0.053
h6-h1     3.180  -1.394     2.28 0.166  0.166 0.166

does exactly the same because the `type' argument defaults to `Dunnett'.

Best,

Torsten

> Andy
>
> > From: Laurent Houdusse
> >
> > Hi!
> >
> > I use simtest fonction of multcomp package to compile a Dunnett's 
> > test. I have 10 treatments and one control group, so i create a 
> > matrix with:
> >
> > m<-matrix(0,10,11)
> > m[1,1]<--1
> > m[1,2]<-1
> > m[2,1]<--1
> > m[2,3]<-1
> > m[3,1]<--1
> > m[3,4]<-1
> > m[4,1]<--1
> > m[4,5]<-1
> > m[5,1]<--1
> > m[5,6]<-1
> > m[6,1]<--1
> > m[6,7]<-1
> > m[7,1]<--1
> > m[7,8]<-1
> > m[8,1]<--1
> > m[8,9]<-1
> > m[9,1]<--1
> > m[9,10]<-1
> > m[10,1]<--1
> > m[10,11]<-1
> > rownames(m)<-c("vehicle+vehicle vs. morphne 
> > 60mg/kg+vehicle","vehicle+vehicle vs. Naloxone 
> > 30mg/kg+vehicle","vehicle+vehicle vs. Naloxone 30mg/kg+Morphine 
> > 60mg/kg","vehicle+vehicle vs. RWJ-69528-000-B 
> > 0.1mg/kg+vehicle","vehicle+vehicle vs. RWJ-69528-000-B 
> > 1mg/kg+vehicle","vehicle+vehicle vs. RWJ-69528-000-B 
> > 0.1mg/kg+morphine 60mg/kg","vehicle+vehicle vs. RWJ-69528-000-B 
> > 1mg/kg+morphine 60mg/kg","vehicle+vehicle vs. vehicle+acetaminophen 
> > 600mg/kg","vehicle+vehicle vs. RWJ-69528-000-B 
> > 0.1mg/kg+acetaminophen 600mg/kg","vehicle+vehicle vs. 
> > RWJ-69528-000-B 1mg/kg+acetaminophen
> > 600mg/kg")
> > y<-c(21.5,14.7,16.7,18.8,17.3,15.527,17.551,17.199,15.895,14.3
> > 88,30,30,30,21
> > .2,14.6,30.83,30.622,30.465,24.563,31.416,30,30,30,30,19.6,17.
> > 61,30.346,20.8
> > ,17.045,18.129,17.9,15.4,30,15.9,22.9,31.909,25.633,24.6,20.35
> > 5,18.807,30,18
> > .9,16.4,18.2,22.3,21.3,28.571,17.357,16.983,30.40,26.3,24.4,25
> > ,24.1,14.4,10.
> > 256,13.266,29.6,15.344,18.69,30,30,25.7,30,28.5,30,30.32,30,19
> > .879,30.716,25
> > .2,30,30,30,30,30,14.063,28.811,30.396,30.609,30,14.1,30,30,30
> > ,26.322,30.386
> > ,28.583,15.77,29.341,30,17.1,30,30,30,30,20.451,30,23.967,30.5
> > 07,30,30,30,27
> > .7,30,28.5,22.568,32.379,30.374,28.263)
> > h<-factor(c(rep(1:11, each = 10)))
> > result<-summary(simtest(y ~ h,cmatrix=cbind(0,m)))
> > rownames(result$estimate)
> > result
> >
> >
> > I want to compare my results with the results obtained with 
> > SigmaStat Software. In my results, i retrieve a correct q' value 
> > with the simtest t value How can i say if P<0.05 like the SigmStat 
> > results? Can i use p raw value, p bonf value, p adj value or other 
> > to compare directly? Or can i use the Dunnett's tables?
> >
> > Thanks for your help!
> >
> >
> >
> >
> > Laurent Houdusse
> > Analyste Programmeur
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>



From andy_liaw at merck.com  Fri Aug 13 14:24:01 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 13 Aug 2004 08:24:01 -0400
Subject: [R] simtest for Dunnett's test
Message-ID: <3A822319EB35174CA3714066D590DCD504AF821E@usrymx25.merck.com>

> From: Laurent Houdusse 
> 
> However, on several tests, my results are correct: 
> The t-value seems always to correspond to the q'value calculated by
> SigmaStat...  
> 
> So,What do I have to use to find the same results as 
> SigmaStat (value and
> comparaison)?

Seems like you don't have much of a choice:

1. Accept the fact that they are different, and use simtest.  I believe the
authors have good reasons to believe that their procedures are better.

2. Roll your own.  You might be able to take shortcuts by looking for
existing C or Fortran code for Dunnett's procedure and try to link that to
R.

Andy
 
> 
> Laurent Houdusse
> Analyste Programmeur
> 
> 
> 
> -----Message d'origine-----
> De : Liaw, Andy [mailto:andy_liaw at merck.com] 
> Envoy?? : vendredi 13 ao??t 2004 14:06
> ?? : Laurent Houdusse; 'r-help at stat.math.ethz.ch'
> Objet : RE: [R] simtest for Dunnett's test
> 
> 
> Before you do any of that, you should realize the fact that 
> simtest does
> _not_ do Dunnett's test:  It can use `Dunnett' _contrasts_ to 
> do comparisons
> with a control, but the actual procedure is different.  You 
> are unlikely to
> get the same result as other packages that perform standard 
> Dunnett's test
> (or Tukey's, for that matter).  
> 
> Andy
> 
> > From: Laurent Houdusse
> > 
> > Hi!
> > 
> > I use simtest fonction of multcomp package to compile a
> > Dunnett's test.
> > I have 10 treatments and one control group, so i create a 
> matrix with:
> > 
> > m<-matrix(0,10,11)
> > m[1,1]<--1
> > m[1,2]<-1
> > m[2,1]<--1
> > m[2,3]<-1
> > m[3,1]<--1
> > m[3,4]<-1
> > m[4,1]<--1
> > m[4,5]<-1
> > m[5,1]<--1
> > m[5,6]<-1
> > m[6,1]<--1
> > m[6,7]<-1
> > m[7,1]<--1
> > m[7,8]<-1
> > m[8,1]<--1
> > m[8,9]<-1
> > m[9,1]<--1
> > m[9,10]<-1
> > m[10,1]<--1
> > m[10,11]<-1
> > rownames(m)<-c("vehicle+vehicle vs. morphne 
> > 60mg/kg+vehicle","vehicle+vehicle vs. Naloxone 
> > 30mg/kg+vehicle","vehicle+vehicle vs. Naloxone 30mg/kg+Morphine 
> > 60mg/kg","vehicle+vehicle vs. RWJ-69528-000-B 
> > 0.1mg/kg+vehicle","vehicle+vehicle vs. RWJ-69528-000-B 
> > 1mg/kg+vehicle","vehicle+vehicle vs. RWJ-69528-000-B 
> 0.1mg/kg+morphine 
> > 60mg/kg","vehicle+vehicle vs. RWJ-69528-000-B 1mg/kg+morphine 
> > 60mg/kg","vehicle+vehicle vs. vehicle+acetaminophen 
> > 600mg/kg","vehicle+vehicle vs. RWJ-69528-000-B 
> 0.1mg/kg+acetaminophen 
> > 600mg/kg","vehicle+vehicle vs. RWJ-69528-000-B 1mg/kg+acetaminophen
> > 600mg/kg") 
> > y<-c(21.5,14.7,16.7,18.8,17.3,15.527,17.551,17.199,15.895,14.3
> > 88,30,30,30,21 
> > .2,14.6,30.83,30.622,30.465,24.563,31.416,30,30,30,30,19.6,17.
> > 61,30.346,20.8 
> > ,17.045,18.129,17.9,15.4,30,15.9,22.9,31.909,25.633,24.6,20.35
> > 5,18.807,30,18 
> > .9,16.4,18.2,22.3,21.3,28.571,17.357,16.983,30.40,26.3,24.4,25
> > ,24.1,14.4,10. 
> > 256,13.266,29.6,15.344,18.69,30,30,25.7,30,28.5,30,30.32,30,19
> > .879,30.716,25 
> > .2,30,30,30,30,30,14.063,28.811,30.396,30.609,30,14.1,30,30,30
> > ,26.322,30.386 
> > ,28.583,15.77,29.341,30,17.1,30,30,30,30,20.451,30,23.967,30.5
> > 07,30,30,30,27
> > .7,30,28.5,22.568,32.379,30.374,28.263)
> > h<-factor(c(rep(1:11, each = 10)))
> > result<-summary(simtest(y ~ h,cmatrix=cbind(0,m)))
> > rownames(result$estimate)
> > result
> > 
> > 
> > I want to compare my results with the results obtained with 
> SigmaStat 
> > Software. In my results, i retrieve a correct q' value with the 
> > simtest t value How can i say if P<0.05 like the SigmStat results?
> > Can i use p raw value, p bonf value, p adj value or other to compare
> > directly? Or can i use the Dunnett's tables?
> > 
> > Thanks for your help!
> > 
> > 
> > 
> > 
> > Laurent Houdusse
> > Analyste Programmeur
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > 
> > 
> 
> 
> --------------------------------------------------------------
> --------------
> --
> Notice:  This e-mail message, together with any attachments, contains
> information of Merck & Co., Inc. (One Merck Drive, Whitehouse 
> Station, New
> Jersey, USA 08889), and/or its affiliates (which may be known 
> outside the
> United States as Merck Frosst, Merck Sharp & Dohme or MSD and 
> in Japan, as
> Banyu) that may be confidential, proprietary copyrighted 
> and/or legally
> privileged. It is intended solely for the use of the 
> individual or entity
> named on this message.  If you are not the intended 
> recipient, and have
> received this message in error, please notify us immediately 
> by reply e-mail
> and then delete it from your system.
> --------------------------------------------------------------
> --------------
> --
> 
>



From l.houdusse at cerep.fr  Fri Aug 13 14:33:50 2004
From: l.houdusse at cerep.fr (Laurent Houdusse)
Date: Fri, 13 Aug 2004 14:33:50 +0200
Subject: [R] simtest for Dunnett's test
Message-ID: <BA420EFAAC96D311A7A0006097D37BDB04515CFE@EOLE>

Ok,
I use simtest because i retrieve the same result
But i don't know how to do the comparaison of P


Laurent Houdusse
Analyste Programmeur



-----Message d'origine-----
De : Liaw, Andy [mailto:andy_liaw at merck.com] 
Envoy?? : vendredi 13 ao??t 2004 14:24
?? : Laurent Houdusse; 'r-help at stat.math.ethz.ch'
Objet : RE: RE : [R] simtest for Dunnett's test


> From: Laurent Houdusse
> 
> However, on several tests, my results are correct:
> The t-value seems always to correspond to the q'value calculated by
> SigmaStat...  
> 
> So,What do I have to use to find the same results as
> SigmaStat (value and
> comparaison)?

Seems like you don't have much of a choice:

1. Accept the fact that they are different, and use simtest.  I believe the
authors have good reasons to believe that their procedures are better.

2. Roll your own.  You might be able to take shortcuts by looking for
existing C or Fortran code for Dunnett's procedure and try to link that to
R.

Andy
 
> 
> Laurent Houdusse
> Analyste Programmeur
> 
> 
> 
> -----Message d'origine-----
> De : Liaw, Andy [mailto:andy_liaw at merck.com]
> Envoy?? : vendredi 13 ao??t 2004 14:06
> ?? : Laurent Houdusse; 'r-help at stat.math.ethz.ch'
> Objet : RE: [R] simtest for Dunnett's test
> 
> 
> Before you do any of that, you should realize the fact that
> simtest does
> _not_ do Dunnett's test:  It can use `Dunnett' _contrasts_ to 
> do comparisons
> with a control, but the actual procedure is different.  You 
> are unlikely to
> get the same result as other packages that perform standard 
> Dunnett's test
> (or Tukey's, for that matter).  
> 
> Andy
> 
> > From: Laurent Houdusse
> > 
> > Hi!
> > 
> > I use simtest fonction of multcomp package to compile a Dunnett's 
> > test. I have 10 treatments and one control group, so i create a
> matrix with:
> > 
> > m<-matrix(0,10,11)
> > m[1,1]<--1
> > m[1,2]<-1
> > m[2,1]<--1
> > m[2,3]<-1
> > m[3,1]<--1
> > m[3,4]<-1
> > m[4,1]<--1
> > m[4,5]<-1
> > m[5,1]<--1
> > m[5,6]<-1
> > m[6,1]<--1
> > m[6,7]<-1
> > m[7,1]<--1
> > m[7,8]<-1
> > m[8,1]<--1
> > m[8,9]<-1
> > m[9,1]<--1
> > m[9,10]<-1
> > m[10,1]<--1
> > m[10,11]<-1
> > rownames(m)<-c("vehicle+vehicle vs. morphne
> > 60mg/kg+vehicle","vehicle+vehicle vs. Naloxone 
> > 30mg/kg+vehicle","vehicle+vehicle vs. Naloxone 30mg/kg+Morphine 
> > 60mg/kg","vehicle+vehicle vs. RWJ-69528-000-B 
> > 0.1mg/kg+vehicle","vehicle+vehicle vs. RWJ-69528-000-B 
> > 1mg/kg+vehicle","vehicle+vehicle vs. RWJ-69528-000-B 
> 0.1mg/kg+morphine
> > 60mg/kg","vehicle+vehicle vs. RWJ-69528-000-B 1mg/kg+morphine
> > 60mg/kg","vehicle+vehicle vs. vehicle+acetaminophen 
> > 600mg/kg","vehicle+vehicle vs. RWJ-69528-000-B 
> 0.1mg/kg+acetaminophen
> > 600mg/kg","vehicle+vehicle vs. RWJ-69528-000-B 1mg/kg+acetaminophen
> > 600mg/kg")
> > y<-c(21.5,14.7,16.7,18.8,17.3,15.527,17.551,17.199,15.895,14.3
> > 88,30,30,30,21 
> > .2,14.6,30.83,30.622,30.465,24.563,31.416,30,30,30,30,19.6,17.
> > 61,30.346,20.8 
> > ,17.045,18.129,17.9,15.4,30,15.9,22.9,31.909,25.633,24.6,20.35
> > 5,18.807,30,18 
> > .9,16.4,18.2,22.3,21.3,28.571,17.357,16.983,30.40,26.3,24.4,25
> > ,24.1,14.4,10. 
> > 256,13.266,29.6,15.344,18.69,30,30,25.7,30,28.5,30,30.32,30,19
> > .879,30.716,25 
> > .2,30,30,30,30,30,14.063,28.811,30.396,30.609,30,14.1,30,30,30
> > ,26.322,30.386 
> > ,28.583,15.77,29.341,30,17.1,30,30,30,30,20.451,30,23.967,30.5
> > 07,30,30,30,27
> > .7,30,28.5,22.568,32.379,30.374,28.263)
> > h<-factor(c(rep(1:11, each = 10)))
> > result<-summary(simtest(y ~ h,cmatrix=cbind(0,m)))
> > rownames(result$estimate)
> > result
> > 
> > 
> > I want to compare my results with the results obtained with
> SigmaStat
> > Software. In my results, i retrieve a correct q' value with the
> > simtest t value How can i say if P<0.05 like the SigmStat results?
> > Can i use p raw value, p bonf value, p adj value or other to compare
> > directly? Or can i use the Dunnett's tables?
> > 
> > Thanks for your help!
> > 
> > 
> > 
> > 
> > Laurent Houdusse
> > Analyste Programmeur
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > 
> > 
> 
> 
> --------------------------------------------------------------
> --------------
> --
> Notice:  This e-mail message, together with any attachments, contains 
> information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, 
> New Jersey, USA 08889), and/or its affiliates (which may be known
> outside the
> United States as Merck Frosst, Merck Sharp & Dohme or MSD and 
> in Japan, as
> Banyu) that may be confidential, proprietary copyrighted 
> and/or legally
> privileged. It is intended solely for the use of the 
> individual or entity
> named on this message.  If you are not the intended 
> recipient, and have
> received this message in error, please notify us immediately 
> by reply e-mail
> and then delete it from your system.
> --------------------------------------------------------------
> --------------
> --
> 
> 


----------------------------------------------------------------------------
--
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From Torsten.Hothorn at rzmail.uni-erlangen.de  Fri Aug 13 14:37:52 2004
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Fri, 13 Aug 2004 14:37:52 +0200 (CEST)
Subject: [R] simtest for Dunnett's test
In-Reply-To: <BA420EFAAC96D311A7A0006097D37BDB04515CFC@EOLE>
References: <BA420EFAAC96D311A7A0006097D37BDB04515CFC@EOLE>
Message-ID: <Pine.LNX.4.51.0408131436440.25161@artemis.imbe.med.uni-erlangen.de>


> Yes, but my control group is not always in the first colonne.
> So, I can have to a matrix like this:
> 1  0  0  -1  0
> 0  1  0  -1  0
> 0  0  1  -1  0
> 0  0  0  -1  0
>

see the `base' argument to `contrMat'.

Torsten

> Laurent Houdusse
> Analyste Programmeur
>
>
>
> -----Message d'origine-----
> De : Torsten Hothorn [mailto:Torsten.Hothorn at rzmail.uni-erlangen.de]
> Envoy?? : vendredi 13 ao??t 2004 14:19
> ?? : Liaw, Andy; Laurent Houdusse
> Cc : 'r-help at stat.math.ethz.ch'
> Objet : RE: [R] simtest for Dunnett's test
>
>
>
> On Fri, 13 Aug 2004, Liaw, Andy wrote:
>
> > Before you do any of that, you should realize the fact that simtest
> > does _not_ do Dunnett's test:  It can use `Dunnett' _contrasts_ to do
> > comparisons with a control, but the actual procedure is different.
> > You are unlikely to get the same result as other packages that perform
> > standard Dunnett's test (or Tukey's, for that matter).
> >
>
> exactly. btw, you don't need to specify the contrast matrix yourself for
> this type of question:
>
> R> summary(simtest(y ~ h))
>
>          Simultaneous tests: Dunnett contrasts
>
> Call:
> simtest.formula(formula = y ~ h)
>
>          Dunnett contrasts for factor h
>
> <...>
>
> Absolute Error Tolerance:  0.001
>
> Coefficients:
>        Estimate t value Std.Err. p raw p Bonf p adj
> h11-h1   12.022  -5.272     2.28 0.000  0.000 0.000
> h7-h1    11.556  -5.067     2.28 0.000  0.000 0.000
> h8-h1    10.952  -4.803     2.28 0.000  0.000 0.000
> h2-h1    10.414  -4.567     2.28 0.000  0.000 0.000
> h10-h1   10.247  -4.493     2.28 0.000  0.000 0.000
> h9-h1     9.494  -4.163     2.28 0.000  0.000 0.000
> h3-h1     7.397  -3.244     2.28 0.002  0.006 0.006
> h4-h1     5.384  -2.361     2.28 0.020  0.061 0.053
> h5-h1     5.085  -2.230     2.28 0.028  0.061 0.053
> h6-h1     3.180  -1.394     2.28 0.166  0.166 0.166
>
> does exactly the same because the `type' argument defaults to `Dunnett'.
>
> Best,
>
> Torsten
>
> > Andy
> >
> > > From: Laurent Houdusse
> > >
> > > Hi!
> > >
> > > I use simtest fonction of multcomp package to compile a Dunnett's
> > > test. I have 10 treatments and one control group, so i create a
> > > matrix with:
> > >
> > > m<-matrix(0,10,11)
> > > m[1,1]<--1
> > > m[1,2]<-1
> > > m[2,1]<--1
> > > m[2,3]<-1
> > > m[3,1]<--1
> > > m[3,4]<-1
> > > m[4,1]<--1
> > > m[4,5]<-1
> > > m[5,1]<--1
> > > m[5,6]<-1
> > > m[6,1]<--1
> > > m[6,7]<-1
> > > m[7,1]<--1
> > > m[7,8]<-1
> > > m[8,1]<--1
> > > m[8,9]<-1
> > > m[9,1]<--1
> > > m[9,10]<-1
> > > m[10,1]<--1
> > > m[10,11]<-1
> > > rownames(m)<-c("vehicle+vehicle vs. morphne
> > > 60mg/kg+vehicle","vehicle+vehicle vs. Naloxone
> > > 30mg/kg+vehicle","vehicle+vehicle vs. Naloxone 30mg/kg+Morphine
> > > 60mg/kg","vehicle+vehicle vs. RWJ-69528-000-B
> > > 0.1mg/kg+vehicle","vehicle+vehicle vs. RWJ-69528-000-B
> > > 1mg/kg+vehicle","vehicle+vehicle vs. RWJ-69528-000-B
> > > 0.1mg/kg+morphine 60mg/kg","vehicle+vehicle vs. RWJ-69528-000-B
> > > 1mg/kg+morphine 60mg/kg","vehicle+vehicle vs. vehicle+acetaminophen
> > > 600mg/kg","vehicle+vehicle vs. RWJ-69528-000-B
> > > 0.1mg/kg+acetaminophen 600mg/kg","vehicle+vehicle vs.
> > > RWJ-69528-000-B 1mg/kg+acetaminophen
> > > 600mg/kg")
> > > y<-c(21.5,14.7,16.7,18.8,17.3,15.527,17.551,17.199,15.895,14.3
> > > 88,30,30,30,21
> > > .2,14.6,30.83,30.622,30.465,24.563,31.416,30,30,30,30,19.6,17.
> > > 61,30.346,20.8
> > > ,17.045,18.129,17.9,15.4,30,15.9,22.9,31.909,25.633,24.6,20.35
> > > 5,18.807,30,18
> > > .9,16.4,18.2,22.3,21.3,28.571,17.357,16.983,30.40,26.3,24.4,25
> > > ,24.1,14.4,10.
> > > 256,13.266,29.6,15.344,18.69,30,30,25.7,30,28.5,30,30.32,30,19
> > > .879,30.716,25
> > > .2,30,30,30,30,30,14.063,28.811,30.396,30.609,30,14.1,30,30,30
> > > ,26.322,30.386
> > > ,28.583,15.77,29.341,30,17.1,30,30,30,30,20.451,30,23.967,30.5
> > > 07,30,30,30,27
> > > .7,30,28.5,22.568,32.379,30.374,28.263)
> > > h<-factor(c(rep(1:11, each = 10)))
> > > result<-summary(simtest(y ~ h,cmatrix=cbind(0,m)))
> > > rownames(result$estimate)
> > > result
> > >
> > >
> > > I want to compare my results with the results obtained with
> > > SigmaStat Software. In my results, i retrieve a correct q' value
> > > with the simtest t value How can i say if P<0.05 like the SigmStat
> > > results? Can i use p raw value, p bonf value, p adj value or other
> > > to compare directly? Or can i use the Dunnett's tables?
> > >
> > > Thanks for your help!
> > >
> > >
> > >
> > >
> > > Laurent Houdusse
> > > Analyste Programmeur
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > >
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From clecio at ime.usp.br  Fri Aug 13 15:25:02 2004
From: clecio at ime.usp.br (=?iso-8859-1?b?Q2zpY2lv?= da Silva Ferreira)
Date: Fri, 13 Aug 2004 10:25:02 -0300
Subject: [R] Install R for Windows
Message-ID: <1092403502.411cc12ef33ed@webmail.ime.usp.br>

  Hi

I'm a Brazilian student and I'd like to work with R. 
But, I had some problems to install R for windows with shortcut of the
statisticals packages in tool bar.
I'll be glad if you could help me to install it without miss the additionals
packages I had install (more than 50!!)
 
 Thanks!
 Cl??cio



From mayeul.kauffmann at tiscali.fr  Fri Aug 13 16:00:22 2004
From: mayeul.kauffmann at tiscali.fr (Mayeul KAUFFMANN)
Date: Fri, 13 Aug 2004 16:00:22 +0200
Subject: [R] How to use the whole dataset (including between events) in
	Cox model (time-varying covariates) ?
References: <Pine.LNX.4.44.0408130736250.18608-100000@gannet.stats>
Message-ID: <002501c4813d$e0375f90$29d59a53@amd>

> > coxph does not use any information that are in the dataset between
event
> > times (or "death times") , since computation only occurs at event
times.
> This is the consequence of the use of partial likelihood in the Cox
model.
>[...]
>You need to make more assumptions, such as a
>smooth baseline hazard, and you can always use parametric models and a
>full likelihood (but you may have to program them yourself).
> Brian D. Ripley

If I'm not wrong, another alternative  might be to use explicitely a
Poisson model (following Dickman et al., who propose another method of
fitting a model close to Cox's model. Reference at the end.).

So my question is on R syntax (sorry if it is a naive question):
does the following do the job I think it does (full likelihood - using
full dataset - for a model close to Cox's) ?

summary(glm(formula = status ~ x1+x2+offset(log((stop-start)/365.25 )),
family = poisson(link = log), na.action =na.omit, control = list(epsilon =
0.001, maxit = 50, trace = F),data=Xstep2))

Dickman et al. recommand the offset ln(yj)  where yj is person-time at
risk for the observation. start and stop are days in my dataset. About two
thirds of my observations are one-year long. status= 1 for event, 0 for
censored observations.
The ten "coef" estimated with coxph are nearly the same as the ten
"estimate" with glm, the p-values are close. (The covariate for which it
is not the case is a covariate which changes very quickly, and thus may be
badly measured with partial likelihood, with computation only at death
time).

I think the baseline hazard is constant here. Dickman et al. use  link
ln(muj ?d*j ) where d*j is the known baseline hazard for observation/at
time j. They say:
 "d*j is the expected number of deaths (due to causes other than the
cancer of interest and
estimated from general population mortality rates") [...]  Fitting the
model requires software which supports the estimation of generalized
linear models with the so-called user-defined link functions. Most general
purpose statistical software packages support this feature, including SAS
(from version 6.10), Stata (from version 7), S-plus, R and GLIM."").

First, I do not know how to specify such a link function in R.
Second, if I can specify such alink, I could use (in place of d*j), the
smooth baseline estimated after doing a Cox regression. But I don't know
how to fit (for instance) a piecewise constant baseline hazard with a
Poison glm, except trying all possible models (within a given class) with
a for( ) loop and taking the highest loglikelihood.

Thank you a lot for any help.

Mayeul KAUFFMANN
Univ. Pierre Mendes France
Grenoble - France

Reference:
Dickman;Sloggett, Hills, Hakulinen, "Regression models for relative
survival",  Statist. Med. 2004; 23:51-64 (DOI: 10.1002/sim.1597)
available at
http://www.pauldickman.com/publications/regression_models_for_relative_survival.pdf

they say:
The underlying model is an additive hazards model where the total hazard
is written as the sum of the known baseline hazard and the excess hazard
associated with a diagnosis of cancer.
"We assume that the number of deaths, dj , for observation j can be
described by a Poisson
distribution, dj follows Poisson(muj) where muj =lambdaj.yj  and yj is
person-time at risk for the observation.
The observations can represent either [...] individual patients or subject
bands (as in Section 3)."
fiting the model requires software which supports


----- Original Message ----- 
From: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
To: "Mayeul KAUFFMANN" <mayeul.kauffmann at tiscali.fr>
Cc: <r-help at stat.math.ethz.ch>
Sent: Friday, August 13, 2004 8:40 AM
Subject: Re: [R] How to use the whole dataset (including between events)
in Cox model (time-varying covariates) ?



From ligges at statistik.uni-dortmund.de  Fri Aug 13 16:13:46 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 13 Aug 2004 16:13:46 +0200
Subject: [R] Install R for Windows
In-Reply-To: <1092403502.411cc12ef33ed@webmail.ime.usp.br>
References: <1092403502.411cc12ef33ed@webmail.ime.usp.br>
Message-ID: <411CCC9A.8000401@statistik.uni-dortmund.de>

Cl??cio da Silva Ferreira wrote:

>   Hi
> 
> I'm a Brazilian student and I'd like to work with R. 
> But, I had some problems to install R for windows with shortcut of the
> statisticals packages in tool bar.

What do you mean with "shortcut of the statisticals packages in tool 
bar." What are the "statistical packages" -- R packages?  What is the 
"tool bar"?


> I'll be glad if you could help me to install it without miss the additionals
> packages I had install (more than 50!!)

Do you have them already installed or are you going to install them?
In the latter case, use install.packages().

Sounds to me like the problems are Windows specific, not R specific. If 
I am right, please contact your local IT stuff (or Microsoft) for support.

Uwe Ligges


>  Thanks!
>  Cl??cio
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From tlumley at u.washington.edu  Fri Aug 13 16:21:00 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 13 Aug 2004 07:21:00 -0700 (PDT)
Subject: [R] How to use the whole dataset (including between events) in
	Cox model (time-varying covariates) ?
In-Reply-To: <000e01c48121$7838aaa0$29d59a53@amd>
References: <Pine.LNX.4.44.0408130736250.18608-100000@gannet.stats>
	<000e01c48121$7838aaa0$29d59a53@amd>
Message-ID: <Pine.A41.4.58.0408130713480.65336@homer07.u.washington.edu>

On Fri, 13 Aug 2004, Mayeul KAUFFMANN wrote:

>
> > you can always use parametric models and a
> > full likelihood (but you may have to program them yourself).
> > Prof Brian Ripley
>
> I started trying this but I could not make the counting process notation
> work on this.
> (Andersen, P.K. and Gill, R.D. (1982). Cox's regression model for counting
> processes: A large sample study. Ann. stat. 10 , 1100-1120).
> I think it is only (currently) available for Cox model with R.
>
> survreg(Surv(start, stop,status)~  x1,data=data )
> Error in survreg(Surv(start, stop, status) ~ x1, data = data) :
>  Invalid survival type
>

Yes. survreg() fits parametric accelerated failure models, not
proportional hazards models, and time-varying covariates present more
difficulties for accelerated failure models.

However, you were concerned about bias because the covariates at event
times are not representative.   If this is the case, the bias will still
be present in a parametric proportional hazards model, and you do not have
proportional hazards.  The Cox model gives consistent estimates whenever a
parametric proportional hazards model does, and the loss of efficiency is
typically very small.

To recover inter-event information, and especially if it is needed to
remove bias, you probably need a joint model for the event process and the
covariate process.


	-thomas



From Luisr at frs.fo  Fri Aug 13 16:22:55 2004
From: Luisr at frs.fo (Luis Rideau Cruz)
Date: Fri, 13 Aug 2004 15:22:55 +0100
Subject: [R] barplot and names.arg
Message-ID: <s11cdcd8.075@ffdata.setur.fo>

R-help

Is there any option to get closer the x-axis and names.arg from barplot?

Thank you

Luis Ridao Cruz
Fiskiranns??knarstovan
N??at??n 1
P.O. Box 3051
FR-110 T??rshavn
Faroe Islands
Phone:             +298 353900
Phone(direct): +298 353912
Mobile:             +298 580800
Fax:                 +298 353901
E-mail:              luisr at frs.fo
Web:                www.frs.fo



From jggardne at uiuc.edu  Fri Aug 13 16:25:52 2004
From: jggardne at uiuc.edu (Justin G. Gardner)
Date: Fri, 13 Aug 2004 09:25:52 -0500
Subject: [R] removing observations
Message-ID: <ce25f043.6f40ab5d.8235800@expms4.cites.uiuc.edu>

Hello R list.

I am sure this is an easy question, please forgive my 
ignorance of R.


I have a panel data set (25 years) and I would like to break 
it up by year, and run a few tests.  How is this done in R?  
In STATA it is quite simple (keep if year =1970).  If someone 
could give me an example of how to do it for a single year I 
am sure I could make a little loop to do it for all 25.

Thanks,
JGG.



From CTMAN at txccc.org  Fri Aug 13 16:35:33 2004
From: CTMAN at txccc.org (Man, Chris T.)
Date: Fri, 13 Aug 2004 09:35:33 -0500
Subject: [R] coxph question
Message-ID: <064925768627DD4DAC63C36FD0A0880601ECF4A9@TCCEXV3A.ad.TexasChildrensHospital.org>

Thanks Mayeul,

I actually would like to test each variable individually and use those have low p-value to build a classifier (not in cox model). Therefore, I need to write a function to subset those low p-value variables, instead of putting them as covariates. Any ideas?

Chris

-----Original Message-----
From: Mayeul KAUFFMANN [mailto:mayeul.kauffmann at tiscali.fr]
Sent: Thursday, August 12, 2004 8:17 PM
To: r-help at stat.math.ethz.ch
Subject: [R] coxph question


> I have many variables to test using cox model (coxph), and I am only
interested in those variables with p value less than 0.01. Is there a
quick way to do this automatically instead of looking at the output of
each variable?
> Chris

I guess you need covariate selection.
for a lengthy discussion of another method (AIC/BIC), look at last month
archive:
https://www.stat.math.ethz.ch/pipermail/r-help/2004-July/subject.html#53519

and try using
> library(MASS)
> stepAIC (...)

Most of the time, it starts removing the covariate with the lower p-value.


Mayeul KAUFFMANN
Univ. Pierre Mendes France
Grenoble - France

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Fri Aug 13 16:39:04 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 13 Aug 2004 16:39:04 +0200
Subject: [R] removing observations
In-Reply-To: <ce25f043.6f40ab5d.8235800@expms4.cites.uiuc.edu>
References: <ce25f043.6f40ab5d.8235800@expms4.cites.uiuc.edu>
Message-ID: <x2oelf2jcn.fsf@biostat.ku.dk>

"Justin G. Gardner" <jggardne at uiuc.edu> writes:

> Hello R list.
> 
> I am sure this is an easy question, please forgive my 
> ignorance of R.
> 
> 
> I have a panel data set (25 years) and I would like to break 
> it up by year, and run a few tests.  How is this done in R?  
> In STATA it is quite simple (keep if year =1970).  If someone 
> could give me an example of how to do it for a single year I 
> am sure I could make a little loop to do it for all 25.


There are multiple answers, mostly involving the subset(), tapply(),
or by() functions. So use help(subset), etc.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jg_liao at yahoo.com  Fri Aug 13 16:46:43 2004
From: jg_liao at yahoo.com (Jason Liao)
Date: Fri, 13 Aug 2004 07:46:43 -0700 (PDT)
Subject: [R] truly object oriented programming in R Thank you all
In-Reply-To: <Pine.A41.4.58.0408121103300.284152@homer03.u.washington.edu>
Message-ID: <20040813144643.64979.qmail@web53704.mail.yahoo.com>

My deep appreciation to everyone who responded to my question. I am
digesting your proposals (a little distracted by my governor's dramatic
resignation in New Jersey). It seems that I am able to implement the
algorithm in R (with the framework kindly provided by Thomas). I will
post the code when it is done. Currently, implemented in Java, the KD
tree kernel density algorithm can be an order faster than the double
loop brute force method. I will find out this will remain true in R.

Regards,
Jason
 
--- Thomas Lumley <tlumley at u.washington.edu> wrote:

> On Thu, 12 Aug 2004, Jason Liao wrote:
> 
> > Dear Thomas,
> > Thank you very much again for taking time to answer my questions. I
> am
> > sorry that my knoweldge of R is limited as I have only learned what
> is
> > necessary to do my work. In the KD tree, we have this recursive
> data
> > structure in that each knod has two children knods and this process
> > continues until the data points are divided. Does R's list support
> this
> > recursive data structure? If yes, can you give a sample program?
> 
> Yes, the elements of a list can be lists. For example, a simple
> binary
> tree could have lists with elements left, right, key, and data
> 
> ## create a new single node
> newtree<-function(key,data){ list(left=NULL,right=NULL, key=key,
> data=data)}
> 
> ## add a node to a sorted tree
> addnode<-function(tree, key, data){
> 
> 	if (key<=tree$key){
> 	   if (is.null(tree$left))
> 		tree$left<-newtree(data=data,key=key)
>            else
>                 tree$left<-addnode(tree$left,key,data)
> 	} else {
> 	   if (is.null(tree$right))
>                 tree$right<-newtree(data=data,key=key)
>            else
>                 tree$right<-addnode(tree$left,key,data)
> 
>         }
> 	return(tree)
> }
> 
> 
> ## inorder traversal.  action() is any function that takes key and
> data
> ## arguments
> applyinorder<-function(tree, action){
> 
> 	c(if (!is.null(tree$left))
> 	      applyinorder(tree$left,action),
> 	action(tree$key,tree$data),
> 	if (!is.null(tree$right))
> 	      applyinorder(tree$right, action))
> 
> }
> 
> 
> ## an example
> > a<-newtree("R","two optional method systems and first-class
> functions")
> > a<-addnode(a,"Java","compulsory object system")
> > a<-addnode(a,"C","No built-in support but that needn't stop you")
> > a<-addnode(a,"C++","If C++ is your hammer, everything looks like a
> thumb")
> > applyinorder(a,function(key,data) paste(key,data,sep=": "))
> [1] "C: No built-in support but that needn't stop you"
> [2] "C++: If C++ is your hammer, everything looks like a thumb"
> [3] "Java: compulsory object system"
> [4] "R: two optional method systems and first-class functions"
> 
> 
> 

=====
Jason Liao, http://www.geocities.com/jg_liao
Dept. of Biostatistics, http://www2.umdnj.edu/bmtrxweb
University of Medicine and Dentistry of New Jersey
phone 732-235-5429, School of Public Health office
phone 732-235-8611, Cancer Institute of New Jersey office
moble phone 908-720-4205



From MSchwartz at MedAnalytics.com  Fri Aug 13 16:46:57 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 13 Aug 2004 09:46:57 -0500
Subject: [R] barplot and names.arg
In-Reply-To: <s11cdcd8.075@ffdata.setur.fo>
References: <s11cdcd8.075@ffdata.setur.fo>
Message-ID: <1092408416.6357.15.camel@localhost.localdomain>

On Fri, 2004-08-13 at 09:22, Luis Rideau Cruz wrote:
> R-help
> 
> Is there any option to get closer the x-axis and names.arg from barplot?
> 
> Thank you

Using mtext() you can do something like the following:

data(VADeaths)

# Now place labels closer to the x axis
# set 'axisnames' to FALSE so the default
# labels are not drawn. Also note that barplot() 
# returns the bar midpoints, so set 'mp' to the return
# values
mp <- barplot(VADeaths, axisnames = FALSE)

# Now use mtext() for the axis labels
mtext(text = colnames(VADeaths), side = 1, at = mp, line = 0)

# clean up
rm(VADeaths)


You can adjust the 'line = 0' argument to move the labels closer to and
farther away from the axis.

HTH,

Marc Schwartz



From mayeul.kauffmann at tiscali.fr  Fri Aug 13 16:47:46 2004
From: mayeul.kauffmann at tiscali.fr (Mayeul KAUFFMANN)
Date: Fri, 13 Aug 2004 16:47:46 +0200
Subject: [R] How to use the whole dataset (including between events) in
	Cox model (time-varying covariates) ?
References: <Pine.LNX.4.44.0408130736250.18608-100000@gannet.stats>
Message-ID: <003701c48144$80fefbd0$29d59a53@amd>

>First, I do not know how to specify such a link function in R.
>Second, if I can specify such alink, I could use (in place of d*j), the
>smooth baseline estimated after doing a Cox regression. But I don't know
>how to fit (for instance) a piecewise constant baseline hazard with a
>Poison glm

I found a possible answer to one of my own question, sorry for the
nuisance.
Simply add
+factor(year)
to the poisson model will fit a piecewise constant baseline hazard (one
step every year).
or
+cut(year,br=seq(1961,1997,by=2))
to have one step every two years.
(Hope the rest is correct...)


Mayeul KAUFFMANN
Univ. Pierre Mendes France
Grenoble - France


----- Original Message ----- 
From: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
To: "Mayeul KAUFFMANN" <mayeul.kauffmann at tiscali.fr>
Cc: <r-help at stat.math.ethz.ch>
Sent: Friday, August 13, 2004 8:40 AM
Subject: Re: [R] How to use the whole dataset (including between events)
in Cox model (time-varying covariates) ?


> This is the consequence of the use of partial likelihood in the Cox
model.
> You should read the literature on this point (for example, have you read
> Cox, 1972 and all its discussion, or Anderson, Borgan, Gill & Keiding?).
>
> It is not an R question.  You need to make more assumptions, such as a
> smooth baseline hazard, and you can always use parametric models and a
> full likelihood (but you may have to program them yourself).
>
>
> On Fri, 13 Aug 2004, Mayeul KAUFFMANN wrote:
>
> > Hello,
> >
> > coxph does not use any information that are in the dataset between
event
> > times (or "death times") , since computation only occurs at event
times.
> >
> > For instance, removing observations when there is no event at that
time in
> > the whole dataset does not change the results:
> > > set.seed(1)
> > > data <-
> >
as.data.frame(cbind(start=c(1:5,1:5,1:4),stop=c(2:6,2:6,2:5),status=c(rep(
> > 0,7),1,rep(0,5),1),id=c(rep(1,5),rep(2,5),rep(3,4)),x1=rnorm(14)))
> > > data
> > start stop status id x1
> > 1 1 2 0 1 -0.6264538
> > 2 2 3 0 1 0.1836433
> > 3 3 4 0 1 -0.8356286
> > 4 4 5 0 1 1.5952808
> > 5 5 6 0 1 0.3295078
> > 6 1 2 0 2 -0.8204684
> > 7 2 3 0 2 0.4874291
> > 8 3 4 1 2 0.7383247
> > 9 4 5 0 2 0.5757814
> > 10 5 6 0 2 -0.3053884
> > 11 1 2 0 3 1.5117812
> > 12 2 3 0 3 0.3898432
> > 13 3 4 0 3 -0.6212406
> > 14 4 5 1 3 -2.2146999
> > coxph(Surv(start, stop,status)~ cluster(id)+x1,data=data ,robust=T)
> > coxph(Surv(start, stop,status)~ cluster(id)+x1,data=subset(data,stop
%in%
> > 4:5) ,robust=T) # the same !!! (except n)
> >
> > First, some data is lost.
> > Second, this loss could be an important problem when  there is a
> > time-varying covariate that changes quicker than the frequency  of
events.
> > Specifically, I have a covariate which has low values most of the
time. It
> > sometimes jumps to high values and that is hypothesized as greatly
> > increasing the risk of an event.
> > With rare events, the effect of this covariate will only be measured
at
> > event times. Chances are that the only time such a covariate is
recorded
> > at high level, the individual for which it is measured as being high
is
> > having an event.
> > This may bias the estimated coefficient.
> >
> > Here is my question:
> > How to fully use the dataset?
> >
> > (that is: how to have really _time-varying_ covariates (even if they
> > change step by step, not continuously), not covariates whose changes
are
> > measured only at event time )
> >
> > Ideally, the full dataset would be use to estimate the parameters, or
at
> > least to estimate the standard error of the estimated parameters.
> > Any ideas ???
> > .
> > .
> > .
> >
> > A second best (which might require less work) would be to use all the
> > dataset to assess the predictive power of the model.
> >
> > Maybe by using the expected number of events for an individual over
the
> > time interval that they were observed to be at risk
> > > predict(coxfit,type="expected")
> > and compare it with observed number of events
> > (does it use all data and takes into account all the baseline hazard,
even
> > between events?)
> >
> >
> > Or, if not,  following Brian D. Ripley suggestion about the baseline
> > hazard: "As an approximation you can smooth the fitted
> > baseline cumulative hazard (e.g. by package pspline) and ask for its
> > derivative"
(https://stat.ethz.ch/pipermail/r-help/2004-July/052376.html)
> >
> > the following code could be use to estimate (and plot) a smooth
baseline
> > hazard:
> > > t<-seq(min(data$start),max(data$stop),length=100)
> > > lines(t,
> >
predict(sm.spline(x=basehaz(coxfit)[,2],y=basehaz(coxfit)[,1],norder=2),
> > t,1))
> > #there is a problem with this code. One should add the contraint that
the
> > baseline hazard cannot be negative.
> >
> > The following computes the parametric part of the cox model.
> > > risk <- predict(coxfit, type='risk') # gives exp(X'b)
> >
> > something like
> > > baseline.hazard*risk
> > would give the true risk at any time (but it would be probably much
harder
> > to compute)
> >
> > which could help assess the predictive power of the model.
> > (still a lot of work)
> >
> > Thanks in advance for any help or comment.
>
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From ottorino-luca.pantani at unifi.it  Fri Aug 13 16:46:56 2004
From: ottorino-luca.pantani at unifi.it (8rino-Luca Pantani)
Date: Fri, 13 Aug 2004 16:46:56 +0200
Subject: R: [R] removing observations
In-Reply-To: <ce25f043.6f40ab5d.8235800@expms4.cites.uiuc.edu>
Message-ID: <DNEELNJCLGBOLHCFLMHBAEBMCFAA.OLPantani@unifi.it>

"by" is perhaps what you looking for
try to write "?by" for help

>I have a panel data set (25 years) and I would like to break 
>it up by year, and run a few tests.  How is this done in R?  
>In STATA it is quite simple (keep if year =1970).  If someone 
>could give me an example of how to do it for a single year I 
>am sure I could make a little loop to do it for all 25.

Otto



From rpeng at jhsph.edu  Fri Aug 13 17:06:28 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Fri, 13 Aug 2004 11:06:28 -0400
Subject: [R] removing observations
In-Reply-To: <ce25f043.6f40ab5d.8235800@expms4.cites.uiuc.edu>
References: <ce25f043.6f40ab5d.8235800@expms4.cites.uiuc.edu>
Message-ID: <411CD8F4.9050707@jhsph.edu>

You probably want something like

subset(dataset, year == 1970)

and loop through the years.

Or, perhaps even classier, would be

datalist <- split(dataset, as.factor(year))
results <- lapply(datalist, myAnalysisFunction)

-roger

Justin G. Gardner wrote:
> Hello R list.
> 
> I am sure this is an easy question, please forgive my 
> ignorance of R.
> 
> 
> I have a panel data set (25 years) and I would like to break 
> it up by year, and run a few tests.  How is this done in R?  
> In STATA it is quite simple (keep if year =1970).  If someone 
> could give me an example of how to do it for a single year I 
> am sure I could make a little loop to do it for all 25.
> 
> Thanks,
> JGG.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From mayeul.kauffmann at tiscali.fr  Fri Aug 13 17:06:24 2004
From: mayeul.kauffmann at tiscali.fr (Mayeul KAUFFMANN)
Date: Fri, 13 Aug 2004 17:06:24 +0200
Subject: [R] coxph question
References: <064925768627DD4DAC63C36FD0A0880601ECF4A9@TCCEXV3A.ad.TexasChildrensHospital.org>
Message-ID: <004101c48147$19da3b10$29d59a53@amd>

Chris,
I understood the following: you want to try every single covariate in a
cox model with only one covariate, then take the best ones according to
p-value.

Assume your columns  look like:
stop status event x1 x2 x3 etc
You want to add column 3 (x1), then 4, etc.

I suggest a for() loop:

z<-NULL;for(i in 3:ncol(data))
{coxtmp <- coxph(Surv(stop,status)~ data[,i]) #you can modify the formula
for
                                                       #adding covariates
in any case, for instance
beta<-coxtmp$coefficients[1]
se<-sqrt(diag(coxtmp$var))[1]
z<- rbind(z,c(i,beta,se,pvalue=signif(1 - pchisq((beta/ se)^2, 1), 4)))
print (i)}

#then select the covariates according to the p values:
z[z$pvalue<.01,"i"]

Hope it helps.

Mayeul KAUFFMANN
Univ. Pierre Mendes France
Grenoble - France


----- Original Message ----- 
Thanks Mayeul,

I actually would like to test each variable individually and use those
have low p-value to build a classifier (not in cox model). Therefore, I
need to write a function to subset those low p-value variables, instead of
putting them as covariates. Any ideas?

Chris

-----Original Message-----

> I have many variables to test using cox model (coxph), and I am only
interested in those variables with p value less than 0.01. Is there a
quick way to do this automatically instead of looking at the output of
each variable?
> Chris



From hjohnson at cbmi.pitt.edu  Fri Aug 13 18:28:17 2004
From: hjohnson at cbmi.pitt.edu (Johnson, Heather)
Date: Fri, 13 Aug 2004 12:28:17 -0400
Subject: [R] Question from Newbie on PostScript and Multiple Plots
Message-ID: <09D04633DAED5F489FFEB6A0EAF22C16ECAF97@pikachu.health.pitt.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040813/7920b7f0/attachment.pl

From MSchwartz at MedAnalytics.com  Fri Aug 13 18:48:15 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 13 Aug 2004 11:48:15 -0500
Subject: [R] Question from Newbie on PostScript and Multiple Plots
In-Reply-To: <09D04633DAED5F489FFEB6A0EAF22C16ECAF97@pikachu.health.pitt.edu>
References: <09D04633DAED5F489FFEB6A0EAF22C16ECAF97@pikachu.health.pitt.edu>
Message-ID: <1092415695.6357.33.camel@localhost.localdomain>

On Fri, 2004-08-13 at 11:28, Johnson, Heather wrote:
> Hi,
>  
> As I'm pretty new to R I hope this question isn't too basic.  
>  
> I am currently looping through my dataset and for each iteration am
> producing three separate plots.  When I output these plots to the screen
> they are nicely grouped as three plots per page, however, when I try to send
> it to a PostScript file I get one page for each plot.  I have adjusted my
> postscript options so that my plots are the size that I want and the paper
> is set to portrait, I just can't figure out how to get all three plots on
> one page in the postscript file.  I've been through the archives on the list
> (albeit not exhaustively) and the manuals available on the R site and cannot
> figure out how to solve my problem.
>  
> Thanks,
> -Heather


Either one of the following work for me:

# Do 3 plots in a 2 x 2 matrix
postscript(file = "ThreePlots.ps", horizontal = FALSE)
par(mfrow = c(2, 2))
plot(1:5)
barplot(1:5)
boxplot(rnorm(10))
dev.off()

# Do 3 x 1
postscript(file = "ThreePlots.ps", horizontal = FALSE)
par(mfrow = c(3, 1))
plot(1:5)
barplot(1:5)
boxplot(rnorm(10))
dev.off()

Can you provide an example of the code that you are using?

Marc Schwartz



From partha_bagchi at hgsi.com  Fri Aug 13 18:50:24 2004
From: partha_bagchi at hgsi.com (partha_bagchi@hgsi.com)
Date: Fri, 13 Aug 2004 12:50:24 -0400
Subject: [R] Question from Newbie on PostScript and Multiple Plots
Message-ID: <OFE6EEEBF0.07C7C9F2-ON85256EEF.005C7936-85256EEF.005C81C5@hgsi.com>

Have you looked at ?par("mfrow") ?






"Johnson, Heather" <hjohnson at cbmi.pitt.edu>
Sent by: r-help-bounces at stat.math.ethz.ch
08/13/2004 12:28 PM

 
        To:     "'r-help at lists.R-project.org'" <r-help at stat.math.ethz.ch>
        cc: 
        Subject:        [R] Question from Newbie on PostScript and Multiple Plots


Hi,

As I'm pretty new to R I hope this question isn't too basic.

I am currently looping through my dataset and for each iteration am
producing three separate plots.  When I output these plots to the screen
they are nicely grouped as three plots per page, however, when I try to 
send
it to a PostScript file I get one page for each plot.  I have adjusted my
postscript options so that my plots are the size that I want and the paper
is set to portrait, I just can't figure out how to get all three plots on
one page in the postscript file.  I've been through the archives on the 
list
(albeit not exhaustively) and the manuals available on the R site and 
cannot
figure out how to solve my problem.

Thanks,
-Heather

[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From hjohnson at cbmi.pitt.edu  Fri Aug 13 19:10:22 2004
From: hjohnson at cbmi.pitt.edu (Johnson, Heather)
Date: Fri, 13 Aug 2004 13:10:22 -0400
Subject: [R] Question from Newbie on PostScript and Multiple Plots
Message-ID: <09D04633DAED5F489FFEB6A0EAF22C16ECAF99@pikachu.health.pitt.edu>

Marc and Partha,

Thank you both for your suggestions of using par("mfrow"), that's exactly
what I needed to do.  In fact I already had it in the code (which I didn't
write originally) and simply had to put my postscript statement in front of
it. :)

It works great now!  Thanks for the quick responses.

Heather

-----Original Message-----
From: Marc Schwartz [mailto:MSchwartz at MedAnalytics.com] 
Sent: Friday, August 13, 2004 11:48 AM
To: Johnson, Heather
Cc: 'r-help at lists.R-project.org'
Subject: Re: [R] Question from Newbie on PostScript and Multiple Plots


On Fri, 2004-08-13 at 11:28, Johnson, Heather wrote:
> Hi,
>  
> As I'm pretty new to R I hope this question isn't too basic.
>  
> I am currently looping through my dataset and for each iteration am 
> producing three separate plots.  When I output these plots to the 
> screen they are nicely grouped as three plots per page, however, when 
> I try to send it to a PostScript file I get one page for each plot.  I 
> have adjusted my postscript options so that my plots are the size that 
> I want and the paper is set to portrait, I just can't figure out how 
> to get all three plots on one page in the postscript file.  I've been 
> through the archives on the list (albeit not exhaustively) and the 
> manuals available on the R site and cannot figure out how to solve my 
> problem.
>  
> Thanks,
> -Heather


Either one of the following work for me:

# Do 3 plots in a 2 x 2 matrix
postscript(file = "ThreePlots.ps", horizontal = FALSE) par(mfrow = c(2, 2))
plot(1:5)
barplot(1:5)
boxplot(rnorm(10))
dev.off()

# Do 3 x 1
postscript(file = "ThreePlots.ps", horizontal = FALSE) par(mfrow = c(3, 1))
plot(1:5)
barplot(1:5)
boxplot(rnorm(10))
dev.off()

Can you provide an example of the code that you are using?

Marc Schwartz



From bournephysio at shaw.ca  Fri Aug 13 20:10:01 2004
From: bournephysio at shaw.ca (Doug Bourne)
Date: Fri, 13 Aug 2004 12:10:01 -0600
Subject: [R] cell counting in R
Message-ID: <16bba7016b9968.16b996816bba70@shaw.ca>

I have a bunch of jpg files that I need to count cells on.  Some cells are green and some cells are red.  Is there a function like locator that I could use to manually count the cells?  I would like to be able to click on a cell, have it marked so I don't count it again and have a counter keep track.  Locator comes close but I would only be able to count one colour at a time.  Is there a way I can do this in R?

Thanks,
Doug



From rbaer at atsu.edu  Fri Aug 13 21:12:52 2004
From: rbaer at atsu.edu (Robert W. Baer, Ph.D.)
Date: Fri, 13 Aug 2004 14:12:52 -0500
Subject: [R] cell counting in R
References: <16bba7016b9968.16b996816bba70@shaw.ca>
Message-ID: <007501c48169$88379630$b407010a@BigBaer>

You might find it easier to do this type of thing with an open-source image
analysis program such as ImageJ: http://rsb.info.nih.gov/ij/

Rob


----- Original Message ----- 
From: "Doug Bourne" <bournephysio at shaw.ca>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, August 13, 2004 1:10 PM
Subject: [R] cell counting in R


> I have a bunch of jpg files that I need to count cells on.  Some cells are
green and some cells are red.  Is there a function like locator that I could
use to manually count the cells?  I would like to be able to click on a
cell, have it marked so I don't count it again and have a counter keep
track.  Locator comes close but I would only be able to count one colour at
a time.  Is there a way I can do this in R?
>
> Thanks,
> Doug
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>



From nair at sdsc.edu  Fri Aug 13 21:28:33 2004
From: nair at sdsc.edu (T. Murlidharan Nair)
Date: Fri, 13 Aug 2004 12:28:33 -0700
Subject: [R] Tutorials on R
Message-ID: <411D1661.2000901@sdsc.edu>

Hi !!
Is there a good tutorial for the R language ? I really find it hard to 
find methods in R.
Thanks and Cheers ../Murli



From rpeng at jhsph.edu  Fri Aug 13 21:34:27 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Fri, 13 Aug 2004 15:34:27 -0400
Subject: [R] Tutorials on R
In-Reply-To: <411D1661.2000901@sdsc.edu>
References: <411D1661.2000901@sdsc.edu>
Message-ID: <411D17C3.1020607@jhsph.edu>

An Introduction to R is pretty good 
(http://cran.r-project.org/doc/manuals/R-intro.pdf).  And there are 
many others available at

http://cran.r-project.org/other-docs.html

-roger

T. Murlidharan Nair wrote:
> Hi !!
> Is there a good tutorial for the R language ? I really find it hard to 
> find methods in R.
> Thanks and Cheers ../Murli
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Charles.Annis at StatisticalEngineering.com  Fri Aug 13 21:45:52 2004
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Fri, 13 Aug 2004 15:45:52 -0400
Subject: [R] Tutorials on R
In-Reply-To: <411D1661.2000901@sdsc.edu>
Message-ID: <200408131945.i7DJjiNd022462@hypatia.math.ethz.ch>

There are a number of resources but I believe the best for starters is
_Introductory Statistics with R_, by Peter Dalgaard.  It's inexpensive ($35)
and very well written, easy to read and understand even if you don't know a
lot of statistics.  In fact the book's intention is to teach you stats by
way of R, so you get a "Two-fer."  If you can only have one book, get this
one.

Now, if your level of statistical sophistication is a bit further along then
the book is Venables and Ripley, _Modern Applied Statistics with S_.  But if
you are just starting out, and you want to learn R the easy way (learning by
doing), then get Dalgaard.  It's a great book.


Charles Annis, P.E.
 
Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  503-217-5849
http://www.StatisticalEngineering.com

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of T. Murlidharan Nair
Sent: Friday, August 13, 2004 3:29 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Tutorials on R

Hi !!
Is there a good tutorial for the R language ? I really find it hard to 
find methods in R.
Thanks and Cheers ../Murli

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From gunter.berton at gene.com  Fri Aug 13 22:17:05 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 13 Aug 2004 13:17:05 -0700
Subject: [R] Tutorials on R
References: <200408131945.i7DJjiNd022462@hypatia.math.ethz.ch>
Message-ID: <411D21C1.EFA9A126@gene.com>

"An Introduction to R" by Venables, Smith, and the R Development Core Team is a
nicely written little paperback that is even easier on the pocketbook ($14 on
Amazon.com) than Dalgaard's . It's a nicely written basic introduction the the
language that makes no attempt to teach statistics.

Cheers,
Bert Gunter

Non-Clinical Biostatistics
Genentech
MS: 240B
Phone: 650-467-7374


"The business of the statistician is to catalyze the scientific learning
process."

 -- George E.P. Box



From pierre.bady at univ-lyon1.fr  Fri Aug 13 22:38:24 2004
From: pierre.bady at univ-lyon1.fr (Pierre BADY)
Date: Fri, 13 Aug 2004 22:38:24 +0200 (CEST)
Subject: [R] how to plot an array with labels
In-Reply-To: <9E198BDA-ED05-11D8-9800-000D93356BF8@ifc.cnr.it>
References: <9E198BDA-ED05-11D8-9800-000D93356BF8@ifc.cnr.it>
Message-ID: <1092429504.411d26c084ab0@webmail.univ-lyon1.fr>

Hello to all,

There are several functions to perform scatter diagrams with labels in the 
library 'ade4'.

#########################

library(ade4)

bob <- as.data.frame((array(1:5, c(4,2))))
row.names(bob)
row.names(bob) <- paste("row",1:4,sep=".")
row.names(bob)
par(mfrow=c(1,2))
s.label(bob, sub="row.names",grid=F)
newnames <- paste("newrow",1:4,sep=".")
newnames
s.label(bob,label=newnames,sub="new names",grid=F)

# There are more examples in the help.

? s.label 

hope this helps

P.BADY 



Quoting luc <luc at ifc.cnr.it>:
> How can i plot an array and instead of having on the x labels the 
> indexes of the array I want to display an other String array of the 
> same length
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 



---------------------------------------------------------
Pierre BADY 
Universit?? Claude Bernard Lyon 1
UMR CNRS 5023, LEHF
bat Alphonse Forel
43 boulevard du 11 novembre 1918 
F-69622 VILLEURBANNE CEDEX 
FRANCE
TEL : +33 (0)4 72 44 62 34 
FAX : +33 (0)4 72 43 28 92 
MEL : pierre.bady at univ-lyon1.fr
http://limnologie.univ-lyon1.fr



From jfbrennan at rogers.com  Fri Aug 13 22:40:55 2004
From: jfbrennan at rogers.com (Jim Brennan)
Date: Fri, 13 Aug 2004 16:40:55 -0400
Subject: [R] Tutorials on R
References: <411D1661.2000901@sdsc.edu>
Message-ID: <000e01c48175$d565ecc0$3b8ac445@slnt.phub.net.cable.rogers.com>

There are quite a few helpful free contributions--under contributed go
figure-- on the R site.
"Using R for Data Analysis and Graphics" by John Maindonald (PDF [702kB],
data sets and scripts are available at JM's homepage).
 I found this a big help( Thanks to John Maindonald) as well as trying to
solve some problems on the help list and seeing how the "pros" do it
properly.

----- Original Message -----
From: "T. Murlidharan Nair" <nair at sdsc.edu>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, August 13, 2004 3:28 PM
Subject: [R] Tutorials on R


> Hi !!
> Is there a good tutorial for the R language ? I really find it hard to
> find methods in R.
> Thanks and Cheers ../Murli
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From m.g.walker at massey.ac.nz  Sat Aug 14 02:19:08 2004
From: m.g.walker at massey.ac.nz (Matthew Walker)
Date: Sat, 14 Aug 2004 12:19:08 +1200
Subject: [R] Private methods
Message-ID: <411D5A7C.3050605@massey.ac.nz>

Hi,

I'm trying to understand R's object oriented abilities.

I have read (in a very recent email to r-help) that R's methods package 
does not support private methods.  However, I also looked at the source 
for the function "is":

 > is
function (object, class2)
{
    cl <- .class1(object)
    if (missing(class2))
        return(extends(cl))
    if (.identC(cl, class2) || .identC(class2, "ANY"))
        return(TRUE)
    ...

The first line of the function, the assignment to cl, seems to call a 
function called ".class1".  I was unable to see the source for ".class1":

 > .class1
Error: Object ".class1" not found

It seems to me that either (a) I don't understand what I'm doing, or (b) 
".class1" is a very private method.  The help system finds nothing about 
".class1". 

Could someone please help me to understand this.  Most helpful of all 
would be pointers on where I could find more information.  (So far I 
have found only two documents on R's object oriented abilities:  "A 
guide to using S4 Objects" and "A draft of the R language definition".)

Thanks,

Matthew



From dmb at mrc-dunn.cam.ac.uk  Sat Aug 14 02:58:45 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Sat, 14 Aug 2004 01:58:45 +0100 (BST)
Subject: [R] numerical accuracy, dumb question
Message-ID: <Pine.LNX.4.21.0408140155480.12570-100000@mail.mrc-dunn.cam.ac.uk>


I store an id as a big number, could this be a problem?

Should I convert to at string when I use read.table(...

example id's

1001001001001
1001001001002
...
1002001002005


Bigest is probably 

1011001001001

Ta, 
Dan.



From maustin at amgen.com  Sat Aug 14 03:34:22 2004
From: maustin at amgen.com (Austin, Matt)
Date: Fri, 13 Aug 2004 18:34:22 -0700
Subject: [R] Private methods
Message-ID: <E7D5AB4811D20B489622AABA9C53859101F1111C@teal-exch.amgen.com>

.class1 is an 'internal' method in the namespace of the package 'methods'.
It can be accessed with the ::: operator.  A good start for documentation
may be Vol 3/1 of R-News.


>methods:::.class1
function (x) 
class(x)[[1]]
<environment: namespace:methods>

--Matt


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Matthew Walker
Sent: Friday, August 13, 2004 17:19 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Private methods


Hi,

I'm trying to understand R's object oriented abilities.

I have read (in a very recent email to r-help) that R's methods package 
does not support private methods.  However, I also looked at the source 
for the function "is":

 > is
function (object, class2)
{
    cl <- .class1(object)
    if (missing(class2))
        return(extends(cl))
    if (.identC(cl, class2) || .identC(class2, "ANY"))
        return(TRUE)
    ...

The first line of the function, the assignment to cl, seems to call a 
function called ".class1".  I was unable to see the source for ".class1":

 > .class1
Error: Object ".class1" not found

It seems to me that either (a) I don't understand what I'm doing, or (b) 
".class1" is a very private method.  The help system finds nothing about 
".class1". 

Could someone please help me to understand this.  Most helpful of all 
would be pointers on where I could find more information.  (So far I 
have found only two documents on R's object oriented abilities:  "A 
guide to using S4 Objects" and "A draft of the R language definition".)

Thanks,

Matthew

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Sat Aug 14 04:02:57 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 13 Aug 2004 22:02:57 -0400
Subject: [R] numerical accuracy, dumb question
Message-ID: <3A822319EB35174CA3714066D590DCD504AF8226@usrymx25.merck.com>

If I'm not mistaken, numerics are read in as doubles, so that shouldn't be a
problem.  However, I'd try using factor or character.

Andy

> From: Dan Bolser
> 
> I store an id as a big number, could this be a problem?
> 
> Should I convert to at string when I use read.table(...
> 
> example id's
> 
> 1001001001001
> 1001001001002
> ...
> 1002001002005
> 
> 
> Bigest is probably 
> 
> 1011001001001
> 
> Ta, 
> Dan.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From tlumley at u.washington.edu  Sat Aug 14 04:23:48 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 13 Aug 2004 19:23:48 -0700 (PDT)
Subject: [R] Private methods
In-Reply-To: <411D5A7C.3050605@massey.ac.nz>
References: <411D5A7C.3050605@massey.ac.nz>
Message-ID: <Pine.A41.4.58.0408131907310.196544@homer07.u.washington.edu>

On Sat, 14 Aug 2004, Matthew Walker wrote:
> I have read (in a very recent email to r-help) that R's methods package
> does not support private methods.  However, I also looked at the source
> for the function "is":
>
>  > is
> function (object, class2)
> {
>     cl <- .class1(object)
>     if (missing(class2))
>         return(extends(cl))
>     if (.identC(cl, class2) || .identC(class2, "ANY"))
>         return(TRUE)
>     ...
>
> The first line of the function, the assignment to cl, seems to call a
> function called ".class1".  I was unable to see the source for ".class1":
>
>  > .class1
> Error: Object ".class1" not found
>
> It seems to me that either (a) I don't understand what I'm doing, or (b)
> ".class1" is a very private method.  The help system finds nothing about
> ".class1".
>

R does have a way to hide functions so that they won't be accidentally
accessed, but these functions are private to a package, not to a
class.

In a package that has a namespace, functions are only visible outside the
package if they are explicitly exported.  This can be overriden with the
::: operator
> methods:::.class1
function (x)
class(x)[[1]]
<environment: namespace:methods>
but should you should resist the temptation to do this.

You may think this sounds like a way to define private methods. The
problem is that they are too private.  Subclasses defined outside the
package don't inherit any access to these functions, so they can't really
be regarded as class methods.

	-thomas



From dataanalytics at rediffmail.com  Sat Aug 14 04:37:17 2004
From: dataanalytics at rediffmail.com (data Analytics)
Date: 14 Aug 2004 02:37:17 -0000
Subject: [R] SPSS, social science majors, and R
Message-ID: <20040814023717.4487.qmail@webmail27.rediffmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040814/77cdd36d/attachment.pl

From spencer.graves at pdf.com  Sat Aug 14 04:40:29 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 13 Aug 2004 22:40:29 -0400
Subject: [R] Giving a first good impression of R to Social Scientists
In-Reply-To: <Pine.A41.4.58.0408120734260.73838@homer09.u.washington.edu>
References: <3699CDBC4ED5D511BE6400306E1C0D81030A0BE3@hermes.demogr.mpg.de>
	<Pine.A41.4.58.0408120734260.73838@homer09.u.washington.edu>
Message-ID: <411D7B9D.9010103@pdf.com>

      I think I agree with Prof. Lumley:   For me, R would likely be the 
platform of choice for most new statistical algorithm development.  It 
has a steep learning curve, and I would therefore not recommend it for 
anyone who would not likely be involved in this kind of activity (unless 
my target audience could not afford simpler software that could handle 
the problems they will most likely encounter).  Moreover, the learning 
curve could be even more difficult for many social scientists than for 
statisticians, mathematicians, engineers, physical scientists, who might 
more likely have more background in vector spaces and programming in 
other language(s). 

      I'm concerned that the time they spend learning R syntax might 
reduce substantially the time they have for learning statistical 
concepts that might be more important for them in the long term.  I 
retract this reservations if, for example, the target audience already 
knows something about structural equation modeling and want to extend 
the R "sem" package for novel applications. 

      hope this helps.  spencer graves

Thomas Lumley wrote:

>On Thu, 12 Aug 2004, Rau, Roland wrote:
>  
>
>>That is why would like to ask the experts on this list if anyone of you has
>>encountered a similar experience and what you could advise to persuade
>>people quickly that it is worth learning a new software?
>>    
>>
>
>One problem is that it may not be true.  Unless these people are going to
>be doing their own statistics in the future (which is probably true only
>for a minority) they might actually be better off with a point and click
>interface.  I'm (obviously) not arguing that SPSS is a better statistical
>environment than R, but it is easier to learn, and in 10 or 15 weeks they
>may not get to see the benefits of R.
>
>
>	-thomas
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From MSchwartz at MedAnalytics.com  Sat Aug 14 04:41:53 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 13 Aug 2004 21:41:53 -0500
Subject: [R] numerical accuracy, dumb question
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF8226@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF8226@usrymx25.merck.com>
Message-ID: <1092451313.6357.95.camel@localhost.localdomain>

Part of that decision may depend upon how big the dataset is and what is
intended to be done with the ID's:

> object.size(1011001001001)
[1] 36

> object.size("1011001001001")
[1] 52

> object.size(factor("1011001001001"))
[1] 244


They will by default, as Andy indicates, be read and stored as doubles.
They are too large for integers, at least on my system:

> .Machine$integer.max
[1] 2147483647

Converting to a character might make sense, with only a minimal memory
penalty. However, using a factor results in a notable memory penalty, if
the attributes of a factor are not needed.

If any mathematical operations are to be performed with the ID's then
leaving them as doubles makes most sense.

Dan, more information on the numerical characteristics of your system
can be found by using:

.Machine

See ?.Machine and ?object.size for more information.

HTH,

Marc Schwartz


On Fri, 2004-08-13 at 21:02, Liaw, Andy wrote:
> If I'm not mistaken, numerics are read in as doubles, so that shouldn't be a
> problem.  However, I'd try using factor or character.
> 
> Andy
> 
> > From: Dan Bolser
> > 
> > I store an id as a big number, could this be a problem?
> > 
> > Should I convert to at string when I use read.table(...
> > 
> > example id's
> > 
> > 1001001001001
> > 1001001001002
> > ...
> > 1002001002005
> > 
> > 
> > Bigest is probably 
> > 
> > 1011001001001
> > 
> > Ta, 
> > Dan.
> >



From olau at fas.harvard.edu  Sat Aug 14 06:49:11 2004
From: olau at fas.harvard.edu (Olivia Lau)
Date: Sat, 14 Aug 2004 00:49:11 -0400
Subject: [R] RE: Giving a first good impression of R to Social Scientists
References: <200408131009.i7DA7EHZ024157@hypatia.math.ethz.ch>
Message-ID: <000b01c481ba$650a4ad0$0800a8c0@olau>

Dear Roland,

Have you looked at Zelig (http://gking.harvard.edu/zelig)?
Several professors in my department are going to use it to teach
R to political science undergraduates and graduate students this
fall.  We just presented it at the Political Methodology
meeting, and will present it again at the American Political
Science Association meeting, so we hope that other departments
will start to use Zelig as a teaching tool (for applied social
science in general, as an alternative to Stata, etc.).

Although a GUI would be good, students won't learn to use R that
way.  I think that the key to getting them to use the command
line interface is to draw an analogy between R and English (or
another language):  There are rules of syntax; here they are; if
you get a "syntax error", you should look for the following
common errors; here are some simple examples and demos that you
can/want to follow (because you're interested in the problem);
and here are the models in a logical format.

Social scientists aren't statisticians, but they're pretty
clever.  They probably had to learn at least one foreign
language in university, and they're probably pretty careful
writers in any language, so making R seem like just another
language will make R seem *easy* to use.

Yours,

Olivia Lau

> On Thu, 12 Aug 2004, Rau, Roland wrote:
> >
> > That is why would like to ask the experts on this list if
anyone of you has
> > encountered a similar experience and what you could advise
to persuade
> > people quickly that it is worth learning a new software?
>
> One problem is that it may not be true.  Unless these people
are going to
> be doing their own statistics in the future (which is probably
true only
> for a minority) they might actually be better off with a point
and click
> interface.  I'm (obviously) not arguing that SPSS is a better
statistical
> environment than R, but it is easier to learn, and in 10 or 15
weeks they
> may not get to see the benefits of R.
>
>
> -thomas
>
>
>
> ------------------------------
>
> Message: 12
> Date: Thu, 12 Aug 2004 16:24:28 +0100
> From: Barry Rowlingson <B.Rowlingson at lancaster.ac.uk>
> Subject: Re: [R] Giving a first good impression of R to Social
> Scientists
> To: "'r-help at stat.math.ethz.ch'" <r-help at stat.math.ethz.ch>
> Message-ID: <411B8BAC.2050906 at lancaster.ac.uk>
> Content-Type: text/plain; charset=us-ascii; format=flowed
>
> Thomas Lumley wrote:
> > On Thu, 12 Aug 2004, Rau, Roland wrote:
> >
> >>That is why would like to ask the experts on this list if
anyone of you has
> >>encountered a similar experience and what you could advise
to persuade
> >>people quickly that it is worth learning a new software?
> >
>
>   The usual way of teaching R seems to be bottom-up. Here's
the command
> prompt, type some arithmetic, make some assignments, learn
about
> function calls and arguments, write your own functions, write
your own
> packages.
>
>   Perhaps a top-down approach might help certain cases. People
using
> point-n-click packages tend to use a limited range of
analyses. Write
> some functions that do these analyses, or give them wrappers
so that
> they get something like:
>
>   > myData = readDataFile("foo.dat")
>     Read 4 variables: Z, Age, Sex, Disease
>
>   > analyseThis(myData, response="Z", covariate="Age")
>
>    Z = 0.36 * Age, Significance level = 0.932
>
>   or whatever. Really spoon feed the things they need to do.
Make it
> really easy, foolproof.
>
>   Then show them what's behind the analyseThis() function. How
its not
> even part of the R distribution. How easy you made it for a
beginner to
> do a complex and novel analysis. Then maybe it'll "click" for
them, and
> they'll see how having a programming language behind their
statistics
> functions lets them explore in ways not thought possible with
the
> point-n-click paradigm. Perhaps they'll start editing
analyseThis() and
> write analyseThat(), start thinking for themselves.
>
>   Or maybe they'll just stare at you blankly...
>
> Baz
>
>
>
> ------------------------------
>
> Message: 13
> Date: Thu, 12 Aug 2004 08:28:18 -0700 (PDT)
> From: Jason Liao <jg_liao at yahoo.com>
> Subject: [R] truly object oriented programming in R
> To: r-help at stat.math.ethz.ch
> Message-ID:
<20040812152818.69617.qmail at web53706.mail.yahoo.com>
> Content-Type: text/plain; charset=us-ascii
>
> Good morning! I recently implemented a KD tree in JAVA for
faster
> kernel density estimation (part of the code follows). It went
well. To
> hook it with R, however, has proved more difficult. My
question is: is
> it possible to implement the algorithm in R? My impression
seems to
> indicate no as the code requires a complete class-object
framework that
> R does not support. But is there an R package or something
that may
> make it possible? Thanks in advance for your help.
>
> Java implementation of KD tree:
>
> public class Kdnode {
>
>         private double[] center; //center of the bounding box
>         private double diameter; //maximum distance from
center to
> anywhere within the bounding box
>         private int numOfPoints; //number of source data
points in the
> bounding box
>
>         private Kdnode left, right;
>
>
> public Kdnode(double[][] points, int split_dim, int [][]
> sortedIndices, double[][] bBox) {
>            //bBox: the bounding box, 1st row the lower bound,
2nd row
> the upper bound
>                 numOfPoints = points.length;
> int d = points[0].length;
>
>                 center = new double[d];
>                 for(int j=0; j<d; j++) center[j] =
> (bBox[0][j]+bBox[1][j])/2.;
>                 diameter = get_diameter(bBox);
>
> if(numOfPoints==1) {
>                   diameter = 0.;
>                   for(int j=0; j<d; j++) center[j] =
points[0][j];
>   left = null;
>   right = null;
> }
> else {
>                   int middlePoint =
> sortedIndices[split_dim][numOfPoints/2];
>   double splitValue = points[middlePoint][split_dim];
>
>                   middlePoint =
> sortedIndices[split_dim][numOfPoints/2-1];
>                   double splitValue_small =
> points[middlePoint][split_dim];
>
>   int left_size = numOfPoints/2;
>                   int right_size = numOfPoints - left_size;
>
>   double[][] leftPoints = new double[left_size][d];
>                   double[][] rightPoints = new
double[right_size][d];
>
>
>   int[][] leftSortedIndices = new int[d][left_size];
>   int[][] rightSortedIndices = new int[d][right_size];
>
>   int left_counter = 0, right_counter = 0;
>   int[] splitInfo = new int [numOfPoints];
>
>   for(int i = 0; i < numOfPoints; i++) {
>     if(points[i][split_dim] < splitValue) {
> for(int j=0; j<d; j++) leftPoints[left_counter][j] =
points[i][j];
>        splitInfo[i] = right_counter;
>                         left_counter++;
>                     }
>
>     else {
> for(int j=0; j<d; j++) rightPoints[right_counter][j] =
points[i][j];
> splitInfo[i] = left_counter;
>                         right_counter++;
>                     }
>                   }
> // modify appropriately the indices to correspond to the new
lists
> for(int i = 0; i < d; i++) {
> int left_index = 0, right_index = 0;
> for(int j = 0; j < numOfPoints; j++) {
> if(points[sortedIndices[i][j]][split_dim] < splitValue)
> leftSortedIndices[i][left_index++] = sortedIndices[i][j] -
> splitInfo[sortedIndices[i][j]];
> else    rightSortedIndices[i][right_index++] =
sortedIndices[i][j]
> - splitInfo[sortedIndices[i][j]];
>                                 }
> }
>
> // Recursively compute the kdnodes for the points in the two
> splitted spaces
> double[][] leftBBox = new double[2][];
> double[][] rightBBox = new double[2][];
>
>                         for(int i=0; i<2; i++) {
>                                 leftBBox[i] =
> (double[])bBox[i].clone();
>                                 rightBBox[i] =
> (double[])bBox[i].clone();
>                             }
>
>                         leftBBox[1][split_dim] =
splitValue_small;
>                         rightBBox[0][split_dim] = splitValue;
>
>                         int next_dim = (split_dim + 1) % (d);
> left = new Kdnode(leftPoints, next_dim, leftSortedIndices,
> leftBBox);
> right = new Kdnode(rightPoints, next_dim, rightSortedIndices,
> rightBBox);
> }
> }
>
>
>         public double evaluate(double[] target, double delta,
double
> bandwidth) throws Exception
>         {
>
>              double dis_2_center = Common.distance(target,
> center)/bandwidth;
>              double dm = diameter/bandwidth;
>
>              if(dis_2_center >= 1+dm) return 0.;
>              if(numOfPoints==1) return Common.K(dis_2_center);
>
>              /*if(dis_2_center<1)
>              {
>                  double temp2 =
dm*Common.KDeriv(dis_2_center);
>                  if(temp2<delta) return
> Common.K(dis_2_center)*numOfPoints;
>              } */
>
>              return left.evaluate(target,delta, bandwidth) +
> right.evaluate(target,delta, bandwidth);
>         }
>
>
>          public double get_diameter(double[][] bBox)
>         {
>             double value = 0., diff;
>             for (int i=0; i<bBox[0].length;i++)
>             {
>                 diff = (bBox[1][i] - bBox[0][i])/2.;
>                 value += diff*diff;
>             }
>             return Math.sqrt(value);
>         }
> }
>
> =====
> Jason Liao, http://www.geocities.com/jg_liao
> Dept. of Biostatistics, http://www2.umdnj.edu/bmtrxweb
> University of Medicine and Dentistry of New Jersey
> phone 732-235-5429, School of Public Health office
> phone 732-235-8611, Cancer Institute of New Jersey office
> moble phone 908-720-4205
>
>
>
> ------------------------------
>
> Message: 14
> Date: Thu, 12 Aug 2004 15:40:52 +0000 (UTC)
> From: Gabor Grothendieck <ggrothendieck at myway.com>
> Subject: Re: [R] truly object oriented programming in R
> To: r-help at stat.math.ethz.ch
> Message-ID: <loom.20040812T173739-400 at post.gmane.org>
> Content-Type: text/plain; charset=us-ascii
>
> Jason Liao <jg_liao <at> yahoo.com> writes:
>
> :
> : Good morning! I recently implemented a KD tree in JAVA for
faster
> : kernel density estimation (part of the code follows). It
went well. To
> : hook it with R, however, has proved more difficult. My
question is: is
> : it possible to implement the algorithm in R? My impression
seems to
> : indicate no as the code requires a complete class-object
framework that
> : R does not support. But is there an R package or something
that may
> : make it possible? Thanks in advance for your help.
>
> R comes with the S3 and S4 object systems out-of-the-box and
there is an
> addon package oo.R available at:
>
>    http://www.maths.lth.se/help/R/R.classes/
>
> that provides a more conventional OO system.   Its likely that
one or more
> of these would satisfy your requirements.
>
>
>
> ------------------------------
>
> Message: 15
> Date: Thu, 12 Aug 2004 17:56:05 +0200
> From: "Kahra Hannu" <kahra at mpsgr.it>
> Subject: RE: [R] linear constraint optim with
bounds/reparametrization
> To: "Spencer Graves" <spencer.graves at pdf.com>, "Ingmar Visser"
> <i.visser at uva.nl>
> Cc: Thomas Lumley <tlumley at u.washington.edu>,
R-help at stat.math.ethz.ch
> Message-ID:
> <C9FC71F7E9356F40AFE2ACC2099DE14714963D at MAILSERVER-B.mpsgr.it>
> Content-Type: text/plain; charset="iso-8859-1"
>
> >From Spencer Graves:
>
> >However, for an equality constraint, I've had good luck by
with an objective function that adds something like the
> >following to my objective function:
constraintViolationPenalty*(A%*%theta-c)^2, where
"constraintViolationPenalty" is
> >passed via "..." in a call to optim.
>
> I applied Spencer's suggestion to a set of eight different
constrained portfolio optimization problems. It seems to give a
usable practice to solve the portfolio problem, when the QP
optimizer is not applicable. After all, practical portfolio
management is more an art than a science.
>
> >I may first run optim with a modest value for
constraintViolationPenalty then restart it with the output of
the
> >initial run as starting values and with a larger value for
constraintViolationPenalty.
>
> I wrote a loop that starts with a small value for the penalty
and stops when the change of the function value, when increasing
the penalty, is less than epsilon. I found that epsilon = 1e-06
provides a reasonable accuracy with respect to computational
time.
>
> Spencer, many thanks for your suggestion.
>
> Hannu Kahra
>
>
>
> ------------------------------
>
> Message: 16
> Date: Thu, 12 Aug 2004 17:59:21 +0200
> From: Martin Maechler <maechler at stat.math.ethz.ch>
> Subject: Re: [R] error using daisy() in library(cluster). Bug?
> To: Javier Garcia - CEBAS <rn001 at cebas.csic.es>
> Cc: R-help at stat.math.ethz.ch
> Message-ID: <16667.37849.634789.455341 at gargle.gargle.HOWL>
> Content-Type: text/plain; charset=iso-8859-1
>
> [Reverted back to R-help, after private exchange]
>
> >>>>> "MM" == Martin Maechler <maechler at stat.math.ethz.ch>
> >>>>>     on Thu, 12 Aug 2004 17:12:01 +0200 writes:
>
> >>>>> "javier" == javier garcia <- CEBAS
<rn001 at cebas.csic.es>>
> >>>>>     on Thu, 12 Aug 2004 16:28:27 +0200 writes:
>
>     javier> Martin; Yes I know that there are variables with
all
>     javier> five values 'NA'. I've left them as they are just
>     javier> because of saving a couple of lines in the script,
>     javier> and because I like to see that they are there,
>     javier> although all values are 'NA'.  I don't expect they
>     javier> are used in the analysis, but are they the source
of
>     javier> the problem?
>
>     MM> yes, but only because of "stand = TRUE".
>
>     MM> Yes, one could imagine that it might be good when
>     MM> standardizing these "all NA variables" would work
>
>     MM> I'll think a bit more about it.  Thank you for the
>     MM> example.
>
> Ok. I've thought (and looked at the R code) a bit longer.
> Also considered the fact (you mentioned) that this worked in R
1.8.0.
> Hence, I'm considering the current behavior a bug.
>
> Here is the patch (apply to cluster/R/daisy.q in the *source*
>  or at the appriopriate place in
<cluster_installed>/R/cluster ) :
>
> --- daisy.q 2004/06/25 16:17:47 1.17
> +++ daisy.q 2004/08/12 15:23:26
> @@ -78,8 +78,8 @@
>      if(all(type2 == "I")) {
>   if(stand) {
>              x <- scale(x, center = TRUE, scale = FALSE) #->
0-means
> -            sx <- colMeans(abs(x))
> -            if(any(sx == 0)) {
> +     sx <- colMeans(abs(x), na.rm = TRUE)# can still have
NA's
> +     if(0 %in% sx) {
>                  warning(sQuote("x"), " has constant columns
",
>                          pColl(which(sx == 0)), "; these are
standardized to 0")
>                  sx[sx == 0] <- 1
>
>
> Thank you for helping to find and fix this bug.
> Martin Maechler, ETH Zurich, Switzerland
>
>     javier> El Jue 12 Ago 2004 15:11, MM escribi??:
>
>     >>> Javier, I could well read your .RData and try your
>     >>> script to produce the same error from daisy().
>     >>>
>     >>> Your dataframe is of dimension 5 x 180 and has many
>     >>> variables that have all five values 'NA' (see below).
>     >>>
>     >>> You can't expect to use these, do you?  Martin
>
>
>
> ------------------------------
>
> Message: 17
> Date: Thu, 12 Aug 2004 16:14:07 +0000 (UTC)
> From: Gabor Grothendieck <ggrothendieck at myway.com>
> Subject: Re: [R] RE: Giving a first good impression of R to
Social
> Scientists
> To: r-help at stat.math.ethz.ch
> Message-ID: <loom.20040812T175128-1 at post.gmane.org>
> Content-Type: text/plain; charset=us-ascii
>
> Rau, Roland <Rau <at> demogr.mpg.de> writes:
>
> > Yes, I do know the R-Commander. But I did not want to give
them a
> > GUI but rather expose them to the command line after I
demonstrated that the
> > steep learning curve in the beginning is worth the effort
for the final
> > results.
>
> Note that Rcmdr displays all the underlying generated R code
that does
> the analysis as it runs so you are exposed to the command
line.  This
> might pique the interest of students wishing to learn more
while giving
> an easy-to-use and immediately useful environment for those
who just want
> to get results in the shortest most direction fashion.
>
>
>
> ------------------------------
>
> Message: 18
> Date: Thu, 12 Aug 2004 09:25:07 -0700
> From: Seth Falcon <sfalcon at fhcrc.org>
> Subject: Re: [R] Approaches to using RUnit
> To: r-help at stat.math.ethz.ch
> Message-ID: <20040812162505.GA23691 at queenbee.fhcrc.org>
> Content-Type: text/plain; charset=us-ascii
>
> On Tue, Aug 10, 2004 at 04:53:49PM +0200, Klaus Juenemann
wrote:
> > If you don't organize your code into packages but source
individual R
> > files your approach to source the code at the beginning of a
test file
> > looks the right thing to do.
>
> Appears to be working pretty well for me too ;-)
>
> > We mainly use packages and the code we use to test packages
A and B,
> > say, looks like
>
> SNIP
>
> > We use the tests subdirectory of a package to store our
RUnit tests
> > even though this is not really according to R conventions.
>
> In an off list exchange with A.J. Rossini, we discussed an
alternative
> for using RUnit in a package.  The idea was to put the
runit_*.R files
> (containing test code) into somePackage/inst/runit/ and then
put a
> script, say dorunit.R inside somePackage/test/ that would
create the
> test suite's similar to the code you included in your mail.
The
> advantage of this would be that the unit tests would run using
R CMD
> check.
>
> In the next week or so I hope to package-ify some code and try
this out.
>
>
> + seth
>
>
>
> ------------------------------
>
> Message: 19
> Date: Thu, 12 Aug 2004 12:25:03 -0400
> From: "Liaw, Andy" <andy_liaw at merck.com>
> Subject: RE: [R] Giving a first good impression of R to Social
> Scientists
> To: "'Barry Rowlingson'" <B.Rowlingson at lancaster.ac.uk>,
> "'r-help at stat.math.ethz.ch'" <r-help at stat.math.ethz.ch>
> Message-ID:
> <3A822319EB35174CA3714066D590DCD504AF8214 at usrymx25.merck.com>
> Content-Type: text/plain
>
> > From: Barry Rowlingson
> >
> > Thomas Lumley wrote:
> > > On Thu, 12 Aug 2004, Rau, Roland wrote:
> > >
> > >>That is why would like to ask the experts on this list if
> > anyone of you has
> > >>encountered a similar experience and what you could advise
> > to persuade
> > >>people quickly that it is worth learning a new software?
> > >
> >
> >   The usual way of teaching R seems to be bottom-up. Here's
> > the command
> > prompt, type some arithmetic, make some assignments, learn
about
> > function calls and arguments, write your own functions,
write
> > your own
> > packages.
> >
> >   Perhaps a top-down approach might help certain cases.
People using
> > point-n-click packages tend to use a limited range of
analyses. Write
> > some functions that do these analyses, or give them wrappers
so that
> > they get something like:
> >
> >   > myData = readDataFile("foo.dat")
> >     Read 4 variables: Z, Age, Sex, Disease
> >
> >   > analyseThis(myData, response="Z", covariate="Age")
> >
> >    Z = 0.36 * Age, Significance level = 0.932
> >
> >   or whatever. Really spoon feed the things they need to do.
Make it
> > really easy, foolproof.
>
> The problem is that the only `fool' that had been `proof'
against is the one
> that the developer(s) had imagined.  One cannot under-estimate
users'
> ability to out-fool the developers' imagination...
>
> Cheers,
> Andy
>
>
> >   Then show them what's behind the analyseThis() function.
> > How its not
> > even part of the R distribution. How easy you made it for a
> > beginner to
> > do a complex and novel analysis. Then maybe it'll "click"
for
> > them, and
> > they'll see how having a programming language behind their
statistics
> > functions lets them explore in ways not thought possible
with the
> > point-n-click paradigm. Perhaps they'll start editing
> > analyseThis() and
> > write analyseThat(), start thinking for themselves.
> >
> >   Or maybe they'll just stare at you blankly...
> >
> > Baz
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html



From bjg at network-theory.co.uk  Sat Aug 14 11:46:31 2004
From: bjg at network-theory.co.uk (Brian Gough)
Date: 14 Aug 2004 10:46:31 +0100
Subject: [R] numerical accuracy, dumb question
In-Reply-To: Dan Bolser's message of "Sat, 14 Aug 2004 01:58:45 +0100 (BST)"
References: <Pine.LNX.4.21.0408140155480.12570-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <87llgi6oi0.fsf@network-theory.co.uk>

Dan Bolser <dmb at mrc-dunn.cam.ac.uk> writes:

> I store an id as a big number, could this be a problem?

If there are ids with significant leading zeros, or too big to be
represented accurately (>2^53)--you won't get any warning about it,
just silent truncation.  So best practice would be to keep them as
character strings, using colClasses= in read.table().

-- 
Brian Gough

Network Theory Ltd,
Publishing the R Reference Manuals --- http://www.network-theory.co.uk/R/base/



From tplate at blackmesacapital.com  Sat Aug 14 15:42:31 2004
From: tplate at blackmesacapital.com (Tony Plate)
Date: Sat, 14 Aug 2004 07:42:31 -0600
Subject: [R] numerical accuracy, dumb question
In-Reply-To: <1092451313.6357.95.camel@localhost.localdomain>
References: <3A822319EB35174CA3714066D590DCD504AF8226@usrymx25.merck.com>
	<1092451313.6357.95.camel@localhost.localdomain>
Message-ID: <6.1.0.6.2.20040814073336.063d4778@mailhost.blackmesacapital.com>

At Friday 08:41 PM 8/13/2004, Marc Schwartz wrote:
>Part of that decision may depend upon how big the dataset is and what is
>intended to be done with the ID's:
>
> > object.size(1011001001001)
>[1] 36
>
> > object.size("1011001001001")
>[1] 52
>
> > object.size(factor("1011001001001"))
>[1] 244
>
>
>They will by default, as Andy indicates, be read and stored as doubles.
>They are too large for integers, at least on my system:
>
> > .Machine$integer.max
>[1] 2147483647
>
>Converting to a character might make sense, with only a minimal memory
>penalty. However, using a factor results in a notable memory penalty, if
>the attributes of a factor are not needed.

That depends on how long the vectors are.  The memory overhead for factors 
is per vector, with only 4 bytes used for each additional element (if the 
level already appears).  The memory overhead for character data is per 
element -- there is no amortization for repeated values.

 > object.size(factor("1011001001001"))
[1] 244
 > 
object.size(factor(rep(c("1011001001001","111001001001","001001001001","011001001001"),1)))
[1] 308
 > # bytes per element in factor, for length 4:
 > 
object.size(factor(rep(c("1011001001001","111001001001","001001001001","011001001001"),1)))/4
[1] 77
 > # bytes per element in factor, for length 1000:
 > 
object.size(factor(rep(c("1011001001001","111001001001","001001001001","011001001001"),250)))/1000
[1] 4.292
 > # bytes per element in character data, for length 1000:
 > 
object.size(as.character(factor(rep(c("1011001001001","111001001001","001001001001","011001001001"),250))))/1000
[1] 20.028
 >

So, for long vectors with relatively few different values, storage as 
factors is far more memory efficient (this is because the character data is 
stored only once per level, and each element is stored as a 4-byte 
integer).  (The above was done on Windows 2000).

-- Tony Plate

>If any mathematical operations are to be performed with the ID's then
>leaving them as doubles makes most sense.
>
>Dan, more information on the numerical characteristics of your system
>can be found by using:
>
>.Machine
>
>See ?.Machine and ?object.size for more information.
>
>HTH,
>
>Marc Schwartz
>
>
>On Fri, 2004-08-13 at 21:02, Liaw, Andy wrote:
> > If I'm not mistaken, numerics are read in as doubles, so that shouldn't 
> be a
> > problem.  However, I'd try using factor or character.
> >
> > Andy
> >
> > > From: Dan Bolser
> > >
> > > I store an id as a big number, could this be a problem?
> > >
> > > Should I convert to at string when I use read.table(...
> > >
> > > example id's
> > >
> > > 1001001001001
> > > 1001001001002
> > > ...
> > > 1002001002005
> > >
> > >
> > > Bigest is probably
> > >
> > > 1011001001001
> > >
> > > Ta,
> > > Dan.
> > >
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From dmb at mrc-dunn.cam.ac.uk  Sat Aug 14 16:04:44 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Sat, 14 Aug 2004 15:04:44 +0100 (BST)
Subject: [R] numerical accuracy, dumb question
In-Reply-To: <6.1.0.6.2.20040814073336.063d4778@mailhost.blackmesacapital.com>
Message-ID: <Pine.LNX.4.21.0408141503320.14992-100000@mail.mrc-dunn.cam.ac.uk>


Thanks all for the expert advice and guidance.



From f.harrell at vanderbilt.edu  Sat Aug 14 16:10:25 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sat, 14 Aug 2004 09:10:25 -0500
Subject: [R] Re: extracting datasets from aregImpute objects
Message-ID: <411E1D51.9090602@vanderbilt.edu>

From: <david_foreman at doctors.org.uk>
Subject: [R] Re: extracting datasets from aregImpute objects
To: <r-help at stat.math.ethz.ch>
Message-ID: <1092391719_117440 at drn10msi01>
Content-Type: text/plain; charset="us-ascii"

I've tried doing this by specifying x=TRUE, which provides me with a 
single imputation, that has been useful.  However, the help file 
possibly suggests that I should get a flat-file matrix of n.impute 
imputations, presumably with indexing.  I'm a bit stuck using 
alternatives to aregImpute, as neither MICE nor Amelia seem to like my 
dataset, and Frank Harrell no longer recommends Transcan for multiple 
imputations.

-----

David,

aregImpute produces a list containing the multiple imputations:

w <- aregImpute(. . .)
w$imputed$blood.pressure   # gets m by k matrix
  # m = number of subjects with blood pressure missing,
  # k = number of multiple imputations

To get a completed dataset (but for only one draw of the k multiple 
imputations) see how fit.mult.impute does it.  I have just added the 
following example to the help file for aregImpute.

set.seed(23)
x <- runif(200)
y <- x + runif(200, -.05, .05)
y[1:20] <- NA
d <- data.frame(x,y)
f <- aregImpute(~ x + y, n.impute=10, match='closest', data=d)
# Here is how to create a completed dataset for imputation
# number 3 as fit.mult.impute would do automatically.  In this
# degenerate case changing 3 to 1-2,4-10 will not alter the results.
completed <- d
imputed <- impute.transcan(f, imputation=3, data=d, list.out=TRUE,
                            pr=FALSE, check=FALSE)
completed[names(imputed)] <- imputed
completed  # 200 by 2 data frame

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From MSchwartz at MedAnalytics.com  Sat Aug 14 19:01:59 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Sat, 14 Aug 2004 12:01:59 -0500
Subject: [R] numerical accuracy, dumb question
In-Reply-To: <6.1.0.6.2.20040814073336.063d4778@mailhost.blackmesacapital.com>
References: <3A822319EB35174CA3714066D590DCD504AF8226@usrymx25.merck.com>
	<1092451313.6357.95.camel@localhost.localdomain>
	<6.1.0.6.2.20040814073336.063d4778@mailhost.blackmesacapital.com>
Message-ID: <1092502918.6357.277.camel@localhost.localdomain>

On Sat, 2004-08-14 at 08:42, Tony Plate wrote:
> At Friday 08:41 PM 8/13/2004, Marc Schwartz wrote:
> >Part of that decision may depend upon how big the dataset is and what is
> >intended to be done with the ID's:
> >
> > > object.size(1011001001001)
> >[1] 36
> >
> > > object.size("1011001001001")
> >[1] 52
> >
> > > object.size(factor("1011001001001"))
> >[1] 244
> >
> >
> >They will by default, as Andy indicates, be read and stored as doubles.
> >They are too large for integers, at least on my system:
> >
> > > .Machine$integer.max
> >[1] 2147483647
> >
> >Converting to a character might make sense, with only a minimal memory
> >penalty. However, using a factor results in a notable memory penalty, if
> >the attributes of a factor are not needed.
> 
> That depends on how long the vectors are.  The memory overhead for factors 
> is per vector, with only 4 bytes used for each additional element (if the 
> level already appears).  The memory overhead for character data is per 
> element -- there is no amortization for repeated values.
> 
>  > object.size(factor("1011001001001"))
> [1] 244
>  > 
> object.size(factor(rep(c("1011001001001","111001001001","001001001001","011001001001"),1)))
> [1] 308
>  > # bytes per element in factor, for length 4:
>  > 
> object.size(factor(rep(c("1011001001001","111001001001","001001001001","011001001001"),1)))/4
> [1] 77
>  > # bytes per element in factor, for length 1000:
>  > 
> object.size(factor(rep(c("1011001001001","111001001001","001001001001","011001001001"),250)))/1000
> [1] 4.292
>  > # bytes per element in character data, for length 1000:
>  > 
> object.size(as.character(factor(rep(c("1011001001001","111001001001","001001001001","011001001001"),250))))/1000
> [1] 20.028
>  >
> 
> So, for long vectors with relatively few different values, storage as 
> factors is far more memory efficient (this is because the character data is 
> stored only once per level, and each element is stored as a 4-byte 
> integer).  (The above was done on Windows 2000).
> 
> -- Tony Plate


Good point Tony. I was making the, perhaps incorrect assumption, that
the ID's were unique or relatively so. However, as it turns out, even
that assumption is relevant only to a certain extent with respect to how
much memory is required.

What is interesting (and presumably I need to do some more reading on
how R stores objects internally) is that the incremental amount of
memory is not consistent on a per element basis for a given object,
though there is a pattern. It is also dependent upon the size of the new
elements to be added, as I note at the bottom.

This all of course presumes that object.size() is giving a reasonable
approximation of the amount of memory actually allocated to an object,
for which the notes in ?object.size raise at least some doubt. This is a
critical assumption for the data below, which is on FC2 on a P4.

For example:

> object.size("a")
[1] 44

> object.size(letters)
[1] 340

In the second case, as Tony has noted, the size of letters (a character
vector) is not 26 * 44.

Now note:

> object.size(c("a", "b"))
[1] 52
> object.size(c("a", "b", "c"))
[1] 68
> object.size(c("a", "b", "c", "d"))
[1] 76
> object.size(c("a", "b", "c", "d", "e"))
[1] 92

The incremental sizes are a sequence of 8 and 16.

Now for a factor:

> object.size(factor("a"))
[1] 236
> object.size(factor(c("a", "b")))
[1] 244
> object.size(factor(c("a", "b", "c")))
[1] 268
> object.size(factor(c("a", "b", "c", "d")))
[1] 276
> object.size(factor(c("a", "b", "c", "d", "e")))
[1] 300

The incremental sizes are a sequence of 8 and 24.


Using elements along the lines of Dan's:

> object.size("1000000000000")
[1] 52
> object.size(c("1000000000000", "1000000000001"))
[1] 68
> object.size(c("1000000000000", "1000000000001", "1000000000002"))
[1] 92
> object.size(c("1000000000000", "1000000000001", "1000000000002",
                "1000000000003"))
[1] 108
> object.size(c("1000000000000", "1000000000001", "1000000000002",
                "1000000000003", "1000000000004"))
[1] 132

The sequence is 16 and 24.

For factors:

> object.size(factor("1000000000000")
[1] 244
> object.size(factor(c("1000000000000", "1000000000001")))
[1] 260
> object.size(factor(c("1000000000000", "1000000000001",
                       "1000000000002")))
[1] 292
> object.size(factor(c("1000000000000", "1000000000001",
                       "1000000000002", "1000000000003")))
[1] 308
> object.size(factor(c("1000000000000", "1000000000001",
                       "1000000000002", "1000000000003",
                       "1000000000004")))
[1] 340

The sequence is 24 and 32.


So, the incremental size seems to alternate as elements are added. 

The behavior above would perhaps suggest that memory is allocated to
objects to enable pairs of elements to be added. When the second element
of the pair is added, only a minimal incremental amount of additional
memory (and presumably time) is required.

However, when I add a "third" element, there is additional memory
required to store that new element because the object needs to be
adjusted in a more fundamental way to handle this new element.

There also appears to be some memory allocation "adjustment" at play
here. Note:

> object.size(factor("1000000000000"))
[1] 244

> object.size(factor("1000000000000", "a"))
[1] 236

In the second case, the amount of memory reported actually declines by 8
bytes. This suggests (to some extent consistent with my thoughts above)
that when the object is initially created, there is space for two new
elements and that space is allocated based upon the size of the first
element. When the second element is added, the space required is
adjusted based upon the actual size of the second element.

Again, all of the above presumes that object.size() is reporting correct
information.

Thanks,

Marc



From MSchwartz at MedAnalytics.com  Sat Aug 14 19:15:37 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Sat, 14 Aug 2004 12:15:37 -0500
Subject: [R] numerical accuracy, dumb question
In-Reply-To: <1092502918.6357.277.camel@localhost.localdomain>
References: <3A822319EB35174CA3714066D590DCD504AF8226@usrymx25.merck.com>
	<1092451313.6357.95.camel@localhost.localdomain>
	<6.1.0.6.2.20040814073336.063d4778@mailhost.blackmesacapital.com>
	<1092502918.6357.277.camel@localhost.localdomain>
Message-ID: <1092503737.6357.294.camel@localhost.localdomain>

On Sat, 2004-08-14 at 12:01, Marc Schwartz wrote:

> There also appears to be some memory allocation "adjustment" at play
> here. Note:
> 
> > object.size(factor("1000000000000"))
> [1] 244
> 
> > object.size(factor("1000000000000", "a"))
> [1] 236


Arggh.

Negate that last comment. I had a typo in the second example. It should
be:

> object.size(factor(c("1000000000000", "a")))
[1] 252

which of course results in an increase in memory.

Geez. Time for lunch.

Marc



From ripley at stats.ox.ac.uk  Sat Aug 14 20:19:23 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 14 Aug 2004 19:19:23 +0100 (BST)
Subject: [R] numerical accuracy, dumb question
In-Reply-To: <1092502918.6357.277.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.44.0408141841480.12580-100000@gannet.stats>

On Sat, 14 Aug 2004, Marc Schwartz wrote:

> > object.size("a")
> [1] 44
> 
> > object.size(letters)
> [1] 340
> 
> In the second case, as Tony has noted, the size of letters (a character
> vector) is not 26 * 44.

Of course not.  Both are character vectors, so have the overhead of any R
object plus an allocation for pointers to the elements plus an amount for
each element of the vector (see the end).

These calculations differ on 32-bit and 64-bit machines.  For a 32-bit
machine storage is in units of either 28 bytes (Ncells) or 8 bytes
(Vcells) so single-letter characters are wasteful, viz

> object.size("aaaaaaa")
[1] 44

That is 1 Ncell and 2 Vcells, 1 for the string (7 bytes plus terminator)
and 1 for the pointer.

Whereas

> object.size(letters)
[1] 340

has 1 Ncell and 39 Vcells, 26 for the strings and 13 for the pointers 
(which fit two to a Vcell).

Note that repeated character strings may share storage, so for example

> object.size(rep("a", 26))
[1] 340

is wrong (140, I think).  And that makes comparisons with factors depend
on exactly how they were created, for a character vector there probably is 
a lot of sharing.

I have a feeling that these calculations are off for character vectors, as 
each element is a CHARSXP and so may have an Ncell not accounted for by 
object.size.  (`May' because of potential sharing.)  Would anyone who is 
sure like to confirm or deny this?

It ought to be possible to improve the estimates for character vectors a 
bit as we can detect sharing amongst the elements.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From xyan0 at yahoo.com  Sat Aug 14 20:31:12 2004
From: xyan0 at yahoo.com (xianghe yan)
Date: Sat, 14 Aug 2004 11:31:12 -0700 (PDT)
Subject: [R] ROracle connection problem
Message-ID: <20040814183112.91970.qmail@web14203.mail.yahoo.com>

Hi,

Could somebody help me to solve this following
problem?  I just begin to learn how to connect my
Oracle database with R.  

> library(DBI)

> library(ROracle)
Warning message: 
DLL attempted to change FPU control word from 8001f to
9001f 

> ora=dbDriver("Oracle")
Error in initialize(value, ...) : Invalid names for
slots of class OraDriver: Id
> 


My system is:

Window XP,
Oracle 9.2 
R1.9.0

Thank you very much

Xianghe

Celera Genomics



From christoph.lehmann at gmx.ch  Sat Aug 14 20:34:21 2004
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Sat, 14 Aug 2004 20:34:21 +0200
Subject: [R] association rules in R
Message-ID: <411E5B2D.9090101@gmx.ch>

Hi

I am interested in data mining problems. Has anybody ever programmed and 
worked with association rules in R?

I am very grateful for any hint.

Best regards

Christoph



From ivo_welch-rstat8783 at mailblocks.com  Sat Aug 14 21:10:44 2004
From: ivo_welch-rstat8783 at mailblocks.com (ivo_welch-rstat8783@mailblocks.com)
Date: Sat, 14 Aug 2004 12:10:44 -0700
Subject: [R] R Cookbook
In-Reply-To: <200408141010.i7EA6iOw032247@hypatia.math.ethz.ch>
References: <200408141010.i7EA6iOw032247@hypatia.math.ethz.ch>
Message-ID: <200408141910.i7EJAk0S029859@hypatia.math.ethz.ch>


is anyone writing an R cookbook (ala the perl cookbook)?  this would be 
more for programming and graphics task than a statistics textbook.   
This seems more like a manufacturing defect than a random occurrance.

if not, if I can fit it into my schedule, I may start one slowly on my 
website based on snippets I needed and/or found---maybe even for 
eventual publication.   obviously, I am not a great choice for 
authoring such a book, because I am not a Rexpert.  I really would 
rather just buy one from someone else than write one.

please drop me a note, either if you
  [a] know of someone who is writing such a book, or
   [b] information that I would find useful, and for which I could 
obtain non-exclusive permission to include it in a published book (of 
course, with proper attribution to the real authors/inventors).

regards,

/iaw

---
ivo welch
professor of finance and economics
brown / nber / yale



From MSchwartz at MedAnalytics.com  Sat Aug 14 22:53:11 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Sat, 14 Aug 2004 15:53:11 -0500
Subject: [R] numerical accuracy, dumb question
In-Reply-To: <Pine.LNX.4.44.0408141841480.12580-100000@gannet.stats>
References: <Pine.LNX.4.44.0408141841480.12580-100000@gannet.stats>
Message-ID: <1092516791.11910.58.camel@localhost.localdomain>

On Sat, 2004-08-14 at 13:19, Prof Brian Ripley wrote:
> On Sat, 14 Aug 2004, Marc Schwartz wrote:
> 
> > > object.size("a")
> > [1] 44
> > 
> > > object.size(letters)
> > [1] 340
> > 
> > In the second case, as Tony has noted, the size of letters (a character
> > vector) is not 26 * 44.
> 
> Of course not.  Both are character vectors, so have the overhead of any R
> object plus an allocation for pointers to the elements plus an amount for
> each element of the vector (see the end).
> 
> These calculations differ on 32-bit and 64-bit machines.  For a 32-bit
> machine storage is in units of either 28 bytes (Ncells) or 8 bytes
> (Vcells) so single-letter characters are wasteful, viz
> 
> > object.size("aaaaaaa")
> [1] 44
> 
> That is 1 Ncell and 2 Vcells, 1 for the string (7 bytes plus terminator)
> and 1 for the pointer.
> 
> Whereas
> 
> > object.size(letters)
> [1] 340
> 
> has 1 Ncell and 39 Vcells, 26 for the strings and 13 for the pointers 
> (which fit two to a Vcell).
> 
> Note that repeated character strings may share storage, so for example
> 
> > object.size(rep("a", 26))
> [1] 340
> 
> is wrong (140, I think).  And that makes comparisons with factors depend
> on exactly how they were created, for a character vector there probably is 
> a lot of sharing.
> 
> I have a feeling that these calculations are off for character vectors, as 
> each element is a CHARSXP and so may have an Ncell not accounted for by 
> object.size.  (`May' because of potential sharing.)  Would anyone who is 
> sure like to confirm or deny this?
> 
> It ought to be possible to improve the estimates for character vectors a 
> bit as we can detect sharing amongst the elements.

Prof. Ripley,

Thanks for the clarifications. 

I'll need to spend some time reading through R-exts.pdf and
Rinternals.h.

Regards,

Marc



From y.benita at wanadoo.nl  Sun Aug 15 00:44:25 2004
From: y.benita at wanadoo.nl (Yair Benita)
Date: Sun, 15 Aug 2004 00:44:25 +0200
Subject: [R] How to display the equation of ECDF
Message-ID: <7E68530A-EE43-11D8-8015-003065C4E4B4@wanadoo.nl>

Hi,
Using the ecdf (Empirical Cumulative Distribution Function) one can 
compute a plot. I was wondering if there is a way to get the equation 
used to draw the plot.

thanks,
Yair



From spencer.graves at pdf.com  Sun Aug 15 01:03:29 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 14 Aug 2004 19:03:29 -0400
Subject: [R] association rules in R
In-Reply-To: <411E5B2D.9090101@gmx.ch>
References: <411E5B2D.9090101@gmx.ch>
Message-ID: <411E9A41.8040909@pdf.com>

      What kind of association rules?  Might %in% or is.element help?  
(PLEASE do read the posting guide! 
"http://www.R-project.org/posting-guide.html".  By following this guide, 
you may get the answer quicker than this list can respond, and if not, 
the exercise might help you formulate your question in a way that may 
more likely elicit useful replies.)

      hope this helps.  spencer graves

Christoph Lehmann wrote:

> Hi
>
> I am interested in data mining problems. Has anybody ever programmed 
> and worked with association rules in R?
>
> I am very grateful for any hint.
>
> Best regards
>
> Christoph
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From rolf at math.unb.ca  Sun Aug 15 01:08:50 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Sat, 14 Aug 2004 20:08:50 -0300 (ADT)
Subject: [R] How to display the equation of ECDF
Message-ID: <200408142308.i7EN8oEe006841@erdos.math.unb.ca>

Yair Benita wrote:

> Using the ecdf (Empirical Cumulative Distribution Function) one can
> compute a plot. I was wondering if there is a way to get the equation
> used to draw the plot.

?ecdf

i.e. RTFM!

				cheers,

					Rolf Turner
					rolf at math.unb.ca



From spencer.graves at pdf.com  Sun Aug 15 01:10:44 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 14 Aug 2004 19:10:44 -0400
Subject: [R] How to display the equation of ECDF
In-Reply-To: <7E68530A-EE43-11D8-8015-003065C4E4B4@wanadoo.nl>
References: <7E68530A-EE43-11D8-8015-003065C4E4B4@wanadoo.nl>
Message-ID: <411E9BF4.90203@pdf.com>

      Did you try typing "ecdf" (without the parentheses identifying it 
as a function) at a prompt?  When I did that just now, I found that 
"ecdf" calls "approxfun", and I could get a function definition by 
typing that. 

      hope this helps. 
      spencer graves
p.s.  PLEASE do read the posting guide! 
"http://www.R-project.org/posting-guide.html".  You might get the answer 
quicker from following this guide.  If not, the exercise might help you 
formulate your question in a way that might elicit more useful 
response(s). 

Yair Benita wrote:

> Hi,
> Using the ecdf (Empirical Cumulative Distribution Function) one can 
> compute a plot. I was wondering if there is a way to get the equation 
> used to draw the plot.
>
> thanks,
> Yair
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From chzhang at cs.ucr.edu  Sun Aug 15 01:14:04 2004
From: chzhang at cs.ucr.edu (Chuanjun Zhang)
Date: Sat, 14 Aug 2004 16:14:04 -0700
Subject: [R] how to draw two graphs in one graph window
In-Reply-To: <411E9A41.8040909@pdf.com>
References: <411E5B2D.9090101@gmx.ch> <411E9A41.8040909@pdf.com>
Message-ID: <411E9CBC.90207@cs.ucr.edu>



From pshannon at systemsbiology.org  Sun Aug 15 01:33:01 2004
From: pshannon at systemsbiology.org (Paul Shannon)
Date: 14 Aug 2004 16:33:01 -0700
Subject: [R] Rserve needs (but cannot find) libR.a (or maybe it's .so)
Message-ID: <EXCHANGEfvuaQ20qXW2000013d7@exchange.systemsbiology.net>

I have successfully installed Rserv (http://stats.math.uni-augsburg.de/Rserve)
on Mac OS, but I have trouble on two different linux platforms.

   R CMD INSTALL Rserve_0.3-10.tar.gz

fails with this message

  ** libs
  gcc -g -O2 -I/usr/local/include -L/usr/local/lib  Rserv.c -o Rserve  \
     -DDAEMON -O -I/usr/local/lib/R/include -Iinclude -I. -lR -L/usr/local/lib/R/bin -ldl -lcrypt 
  /usr/bin/ld: cannot find -lR
  collect2: ld returned 1 exit status
  make: *** [Rserve] Error 1
  ERROR: compilation failed for package 'Rserve'

Sure enough, when I look, I cannot find either libR.a or librR.so on either
linus system.

On the Mac, I -do- find libR.dylib.

Can anyone help with this?

Many thanks -

 - Paul Shannon
   Institute for Systems Biology
   Seattle



From rossini at blindglobe.net  Sun Aug 15 01:33:32 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Sat, 14 Aug 2004 16:33:32 -0700
Subject: [R] Rserve needs (but cannot find) libR.a (or maybe it's .so)
In-Reply-To: <EXCHANGEfvuaQ20qXW2000013d7@exchange.systemsbiology.net> (Paul
	Shannon's message of "14 Aug 2004 16:33:01 -0700")
References: <EXCHANGEfvuaQ20qXW2000013d7@exchange.systemsbiology.net>
Message-ID: <85zn4xs3ar.fsf@servant.blindglobe.net>


Need to install R with the shared libraries (it's a config option).


Paul Shannon <pshannon at systemsbiology.org> writes:

> I have successfully installed Rserv (http://stats.math.uni-augsburg.de/Rserve)
> on Mac OS, but I have trouble on two different linux platforms.
>
>    R CMD INSTALL Rserve_0.3-10.tar.gz
>
> fails with this message
>
>   ** libs
>   gcc -g -O2 -I/usr/local/include -L/usr/local/lib  Rserv.c -o Rserve  \
>      -DDAEMON -O -I/usr/local/lib/R/include -Iinclude -I. -lR -L/usr/local/lib/R/bin -ldl -lcrypt 
>   /usr/bin/ld: cannot find -lR
>   collect2: ld returned 1 exit status
>   make: *** [Rserve] Error 1
>   ERROR: compilation failed for package 'Rserve'
>
> Sure enough, when I look, I cannot find either libR.a or librR.so on either
> linus system.
>
> On the Mac, I -do- find libR.dylib.
>
> Can anyone help with this?
>
> Many thanks -
>
>  - Paul Shannon
>    Institute for Systems Biology
>    Seattle
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Anthony Rossini			    Research Associate Professor
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From levin001 at 123mail.cl  Sun Aug 15 02:14:32 2004
From: levin001 at 123mail.cl (Peyuco Porras Porras .)
Date: Sat, 14 Aug 2004 20:14:32 -0400
Subject: [R] calibration/validation sets
Message-ID: <14090201402b96.1402b961409020@123mail.cl>

Hi; 
Does anyone know how to create a calibration and validation set from a particular dataset? I have a dataframe with nearly 20,000 rows! and I would like to select (randomly) a subset from the original dataset (...I found how to do that) to use as calibration set. However, I don't know how to remove this "calibration" set from the original dataframe in order to get my "validation" set.....Any hint will be greatly appreciated. 
TT



From maustin at amgen.com  Sun Aug 15 02:41:01 2004
From: maustin at amgen.com (Austin, Matt)
Date: Sat, 14 Aug 2004 17:41:01 -0700
Subject: [R] calibration/validation sets
Message-ID: <E7D5AB4811D20B489622AABA9C53859101F1111D@teal-exch.amgen.com>

You could keep a row index vector like in the following example.

> data(iris)
> indx <- sample(nrow(iris), 20, replace=FALSE)
> train <- iris[indx,]
> test  <- iris[-indx,]

--Matt


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Peyuco Porras
Porras .
Sent: Saturday, August 14, 2004 17:15 PM
To: R-help at stat.math.ethz.ch
Subject: [R] calibration/validation sets
Importance: High


Hi; 
Does anyone know how to create a calibration and validation set from a
particular dataset? I have a dataframe with nearly 20,000 rows! and I would
like to select (randomly) a subset from the original dataset (...I found how
to do that) to use as calibration set. However, I don't know how to remove
this "calibration" set from the original dataframe in order to get my
"validation" set.....Any hint will be greatly appreciated. 
TT

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Kevin.Wang at maths.anu.edu.au  Sun Aug 15 02:48:54 2004
From: Kevin.Wang at maths.anu.edu.au (Kevin Wang)
Date: Sun, 15 Aug 2004 10:48:54 +1000 (EST)
Subject: [R] calibration/validation sets
In-Reply-To: <14090201402b96.1402b961409020@123mail.cl>
References: <14090201402b96.1402b961409020@123mail.cl>
Message-ID: <Pine.GSO.4.58.0408151045480.9499@yin>

Hi,

On Sat, 14 Aug 2004, Peyuco Porras Porras . wrote:

> Hi;
> Does anyone know how to create a calibration and validation set from a particular dataset? I have a dataframe with nearly 20,000 rows! and I would like to select (randomly) a subset from the original dataset (...I found how to do that) to use as calibration set. However, I don't know how to remove this "calibration" set from the original dataframe in order to get my "validation" set.....Any hint will be greatly appreciated.

A really quick way, suppose you want to have 30% of your dataset as the
validation set:
> iris.id = sample(nrow(iris), nrow(iris) * 0.3)
> iris.valid = iris[iris.id, ]
> iris.train = iris[-iris.id, ]
> nrow(iris.valid)
[1] 45
> nrow(iris.train)
[1] 105

The first line takes a sample of 30% of the number of rows in the Iris
data.  The second line does a subetting of those samples -- the validation
set.  The third takes what's left -- the training set.  This is perhaps
not efficient and the code can definitely be simplified...but it's Sunday
morning and I haven't had my morning coffee yet :D

Cheers,

Kevin


--------------------------------
Ko-Kang Kevin Wang
PhD Student
Centre for Mathematics and its Applications
Building 27, Room 1004
Mathematical Sciences Institute (MSI)
Australian National University
Canberra, ACT 0200
Australia
Homepage: http://wwwmaths.anu.edu.au/~wangk/
Ph (W): +61-2-6125-2431
Ph (H): +61-2-6125-7407
Ph (M): +61-40-451-8301



From andy_liaw at merck.com  Sun Aug 15 03:05:22 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 14 Aug 2004 21:05:22 -0400
Subject: [R] calibration/validation sets
Message-ID: <3A822319EB35174CA3714066D590DCD504AF822D@usrymx25.merck.com>

There are many ways to do this.  One example, supposing your data is in
`myData':

## randomly pick 1/3 for validation:
valid.idx <- sample(nrow(myData), round(nrow(myData)/3), replace=FALSE) 

## training set:
myData.tr <- myData[-valid.idx,]
## validation set:
myData.valid <- myData[valid.idx,]

HTH,
Andy

> From: Peyuco Porras Porras .
> 
> Hi; 
> Does anyone know how to create a calibration and validation 
> set from a particular dataset? I have a dataframe with nearly 
> 20,000 rows! and I would like to select (randomly) a subset 
> from the original dataset (...I found how to do that) to use 
> as calibration set. However, I don't know how to remove this 
> "calibration" set from the original dataframe in order to get 
> my "validation" set.....Any hint will be greatly appreciated. 
> TT
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From nusbj at hotmail.com  Sun Aug 15 03:38:44 2004
From: nusbj at hotmail.com (Z P)
Date: Sun, 15 Aug 2004 09:38:44 +0800
Subject: [R] Dirichlet-Multinomial
Message-ID: <BAY22-F14e0qsiUJ40H000462d4@hotmail.com>

Dear all,

Is there any package in R, which can do the Dirichlet-Multinomial model fit? 
It is a generalization of the beta-binomial model. I know Prof. Lindsay has 
a package, whcih can estimate the beta-binomial parameter well.  Is there 
any counter part for Dirichlet-Multinomial? Thanks.

Regards,

Zhen



From ramasamy at cancer.org.uk  Sun Aug 15 04:39:32 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Sun, 15 Aug 2004 03:39:32 +0100
Subject: [R] how to draw two graphs in one graph window
In-Reply-To: <411E9CBC.90207@cs.ucr.edu>
References: <411E5B2D.9090101@gmx.ch> <411E9A41.8040909@pdf.com>
	<411E9CBC.90207@cs.ucr.edu>
Message-ID: <1092537572.7527.43.camel@localhost.localdomain>

?par

On Sun, 2004-08-15 at 00:14, Chuanjun Zhang wrote:
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From rwatersg at yahoo.com  Sun Aug 15 05:32:14 2004
From: rwatersg at yahoo.com (Robert Waters)
Date: Sat, 14 Aug 2004 20:32:14 -0700 (PDT)
Subject: [R] index and by groups statement
Message-ID: <20040815033214.43527.qmail@web90108.mail.scd.yahoo.com>

Dear R-users

Im working with a dataset that contains information
for 8 groups of data and I need to select a sample of
certain size (100 cubic feet by group) from this
database for each of these 8 groups. To clarify, here
is the starting code Im working with:

k<-nrow(dataset)
ix<-sort(runif(k),index.return=TRUE)$ix
M<-max(which(cumsum(dataset$volume[ix])<100))+1 
test<-dataset[ix[1:M],]

However, I don't know how to specify in this code the
instruction: "by groups"

Does anyone have an idea how to do this?

Thanks in advance

RW



From ramasamy at cancer.org.uk  Sun Aug 15 06:28:54 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Sun, 15 Aug 2004 05:28:54 +0100
Subject: [R] index and by groups statement
In-Reply-To: <20040815033214.43527.qmail@web90108.mail.scd.yahoo.com>
References: <20040815033214.43527.qmail@web90108.mail.scd.yahoo.com>
Message-ID: <1092544134.7527.120.camel@localhost.localdomain>

If understand you correctly, you have a variable that groups each
observations into one of eight categories. And there several hundred
observations from each category. Now, you want to sample only 100
observations from each category. It this is right, then the following
might help :

   set.seed(123)
   num <- rnorm( length(g) )                    # response variable
   g <- sample( LETTERS[1:8], 1200, replace=T ) # grouping variable
   table(g)
      A   B   C   D   E   F   G   H 
    146 153 131 166 140 164 163 137 


You can either store an list of 100 representative indexes (indexList)
from each category or store the value instead (valueList)

   indexList <- tapply( 1:length(g), g, function(x) sample(x, 100) )  
   valueList <- tapply( num, g, function(x) sample(x, 100) )

The first is easier to double check with
   for(i in 1:8) print(mean(g[ unlist(indexList[[i]]) ] == LETTERS[i]))


If you only want the summary from these 100 sampled values, then you do
not need to store any index or value, but calculate the summary
directly. For example, lets say the median
 
   tapply( num, g, function(x) median( sample(x, 100) ) )


Hope this helps, Adai




On Sun, 2004-08-15 at 04:32, Robert Waters wrote:
> Dear R-users
> 
> Im working with a dataset that contains information
> for 8 groups of data and I need to select a sample of
> certain size (100 cubic feet by group) from this
> database for each of these 8 groups. To clarify, here
> is the starting code Im working with:
> 
> k<-nrow(dataset)
> ix<-sort(runif(k),index.return=TRUE)$ix
> M<-max(which(cumsum(dataset$volume[ix])<100))+1 
> test<-dataset[ix[1:M],]
> 
> However, I don't know how to specify in this code the
> instruction: "by groups"
> 
> Does anyone have an idea how to do this?
> 
> Thanks in advance
> 
> RW
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Rau at demogr.mpg.de  Sun Aug 15 13:00:17 2004
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Sun, 15 Aug 2004 13:00:17 +0200
Subject: [R] how to draw two graphs in one graph window
Message-ID: <3699CDBC4ED5D511BE6400306E1C0D81030A0C0D@hermes.demogr.mpg.de>

Hi,


	-----Original Message-----
	From:	r-help-bounces at stat.math.ethz.ch
[SMTP:r-help-bounces at stat.math.ethz.ch] On Behalf Of Chuanjun Zhang
	Sent:	Sunday, August 15, 2004 1:14 AM
	To:	r-help at stat.math.ethz.ch
	Subject:	[R] how to draw two graphs in one graph window

it is not really clear (at least not to me) what you mean.
a) Do you want to add two graphs into one figure?
b) Do you want to add two graphs on the same page?

The following two simple code examples might help to do what you want. 

### ad a)
x <- 1:100
plot(x=x, y=x, type="l", col="red")
lines(x=x, y=x/2, col="blue")
### ad b)
x <- 1:100
par(mfrow=c(1,2))
plot(x=x, y=x, type="l", col="red")
plot(x=x, y=x^2, type="l", col="blue")


Please try to be more specific in your next postings. And please read the
posting guide (see bottom of every message). It suggests to use
- help.search("keyword")
- read the online help
- check the relevant FAQs
- read at least the relevant section in "An Introduction to R" which is
shipped with every binary distribution of R.
Relevant for your question is section 12 "Graphical Procedures".
There you will find all the information you need to get started (especially
sections 12.1, 12.2, 12.4, 12.5).

Best,
Roland


+++++
This mail has been sent through the MPI for Demographic Rese...{{dropped}}



From tobias.sing at mpi-sb.mpg.de  Sun Aug 15 13:12:10 2004
From: tobias.sing at mpi-sb.mpg.de (Tobias Sing)
Date: Sun, 15 Aug 2004 13:12:10 +0200
Subject: [R] association rules in R
Message-ID: <53446338@web.de>

I think there is no R package for association rules yet. You might want to write a wrapper around one of the many freely available association rule miners instead, e.g. Christian Borgelt's nice implementation of the apriori algorithm:
http://fuzzy.cs.uni-magdeburg.de/~borgelt/apriori.html

HTH,
Tobias.

Christoph Lehmann wrote:

> Hi
>
> I am interested in data mining problems. Has anybody ever programmed 
> and worked with association rules in R?
>
> I am very grateful for any hint.
>
> Best regards
>
> Christoph

__________________________________________________________________________ 
Tobias Sing                         phone: +49 681 9325 315  
Max-Planck-Institut f??r Informatik  fax  : +49 681 9325 399  
Stuhlsatzenhausweg 85               email: tobias.sing at mpi-sb.mpg.de 
66123 Saarbr??cken, Germany          web  : http://www.mpi-sb.mpg.de/~tsing 



From spencer.graves at pdf.com  Sun Aug 15 13:33:02 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 15 Aug 2004 07:33:02 -0400
Subject: [R] How to display the equation of ECDF
In-Reply-To: <7E68530A-EE43-11D8-8015-003065C4E4B4@wanadoo.nl>
References: <7E68530A-EE43-11D8-8015-003065C4E4B4@wanadoo.nl>
Message-ID: <411F49EE.2000107@pdf.com>

	  Just type "ecdf" at a command prompt like you type "pi" and get back 
3.141593.  hope this helps.  spencer graves

p.s.  Have you looked at "An Introduction to R" [available in the upper 
left after help.start() or on www.r-project.org -> Documentation -> 
Manuals?

#################################
Yair Benita wrote:

 > I wanted to get the equation of the cumulative distribution. Looking 
through the manual I found the ecdf function used it as follows:
 >
 > x contains about 3000 real values. Here is my command line:
 >
 > plot.ecdf(ecdf(x))
 >
 > This gives me a plot with an exponential curve with a y axis of Fn(x) 
and I would like to see the equation of the function plotted.
 >
 > How can you type ecdf with no parantheses? it gives me an error. Can 
you give me an example?
 >
 > Thanks,
 > Yair
 >
 >
##########################
      Did you try typing "ecdf" (without the parentheses identifying it
as a function) at a prompt?  When I did that just now, I found that
"ecdf" calls "approxfun", and I could get a function definition by
typing that.

      hope this helps.
      spencer graves
p.s.  PLEASE do read the posting guide!
"http://www.R-project.org/posting-guide.html".  You might get the answer
quicker from following this guide.  If not, the exercise might help you
formulate your question in a way that might elicit more useful
response(s).

Yair Benita wrote:

> Hi,
> Using the ecdf (Empirical Cumulative Distribution Function) one can 
> compute a plot. I was wondering if there is a way to get the equation 
> used to draw the plot.
>
> thanks,
> Yair
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From Christoph.Scherber at uni-jena.de  Sun Aug 15 14:28:03 2004
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Sun, 15 Aug 2004 14:28:03 +0200
Subject: [R] analysis of life tables
Message-ID: <411F56D3.2070902@uni-jena.de>

Dear all,

How can I analyze a life table (e.g. for a cohort of insects) in R?

I have 20 insects in 200 cages with two different treatments, whose 
survival is followed over time, such that, e.g., in one treatment, the 
number of animals surviving is c(20,18,16,12,10,8,4,0), while in the 
other treatment the survival is c(20,20,18,18,16,15,15,14) at 8 
subsequent time intervals.

I would very much appreciate any suggestions on how to analyze such a 
dataset.

Best regards
Christoph.



From ozric at web.de  Sun Aug 15 18:57:49 2004
From: ozric at web.de (Christian Schulz)
Date: Sun, 15 Aug 2004 18:57:49 +0200
Subject: [R] Dirichlet-Multinomial
In-Reply-To: <BAY22-F14e0qsiUJ40H000462d4@hotmail.com>
References: <BAY22-F14e0qsiUJ40H000462d4@hotmail.com>
Message-ID: <200408151857.50163.ozric@web.de>

Perhaps  package  VGAM with drichlet  and Zelig  helps.

christian


Am Sonntag, 15. August 2004 03:38 schrieb Z P:
> Dear all,
>
> Is there any package in R, which can do the Dirichlet-Multinomial model
> fit? It is a generalization of the beta-binomial model. I know Prof.
> Lindsay has a package, whcih can estimate the beta-binomial parameter well.
>  Is there any counter part for Dirichlet-Multinomial? Thanks.
>
> Regards,
>
> Zhen
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From laura at env.leeds.ac.uk  Mon Aug 16 01:11:13 2004
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Mon, 16 Aug 2004 00:11:13 +0100 (BST)
Subject: [R] Stacking Vectors/Dataframes
Message-ID: <Pine.LNX.4.44.0408160001080.7794-100000@gw.env.leeds.ac.uk>

Hello,

Is there a simple way of stacking/merging two dataframes in R? I want to
stack them piece-wise, not simply add one whole dataframe to the bottom of
the other. I want to create as follows:

x.frame:
aX1  bX1  cX1  ... zX1
aX2  bX2  cX2  ... zX2
...  ...  ...  ... ...
aX99 bX99 cX99 ... zX99

y.frame:
aY1  bY1  cY1  ... zY1
aY2  bY2  cY2  ... zY2
...  ...  ...  ... ...
aY99 bY99 cY99 ... zY99

new.frame:
aX1  bX1  cX1  ... zX1
aY1  bY1  cY1  ... zY1
aX2  bX2  cX2  ... zX2
aY2  bY2  cY2  ... tY2
...  ...  ...  ... ...
aX99 bX99 cX99 ... tX99
aY99 bY99 cY99 ... tY99

I have tried to use a for loop (simply assigning and also with rbind) to
do this but am having difficulty correctly assigning the destination in the new dataframe. Can
anyone offer a quick and easy way of doing this (or even a long winded one
if it works!!)

Thank you in advance,

Laura Quinn
Institute of Atmospheric Science
School of the Environment
University of Leeds
Leeds
LS2 9JT

tel: +44 113 343 1596
fax: +44 113 343 6716
mail: laura at env.leeds.ac.uk



From andy_liaw at merck.com  Mon Aug 16 01:24:10 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sun, 15 Aug 2004 19:24:10 -0400
Subject: [R] Stacking Vectors/Dataframes
Message-ID: <3A822319EB35174CA3714066D590DCD504AF822F@usrymx25.merck.com>

I believe interleave() in the `gregmisc' package can do what you want.

Cheers,
Andy

> From: Laura Quinn
> 
> Hello,
> 
> Is there a simple way of stacking/merging two dataframes in 
> R? I want to
> stack them piece-wise, not simply add one whole dataframe to 
> the bottom of
> the other. I want to create as follows:
> 
> x.frame:
> aX1  bX1  cX1  ... zX1
> aX2  bX2  cX2  ... zX2
> ...  ...  ...  ... ...
> aX99 bX99 cX99 ... zX99
> 
> y.frame:
> aY1  bY1  cY1  ... zY1
> aY2  bY2  cY2  ... zY2
> ...  ...  ...  ... ...
> aY99 bY99 cY99 ... zY99
> 
> new.frame:
> aX1  bX1  cX1  ... zX1
> aY1  bY1  cY1  ... zY1
> aX2  bX2  cX2  ... zX2
> aY2  bY2  cY2  ... tY2
> ...  ...  ...  ... ...
> aX99 bX99 cX99 ... tX99
> aY99 bY99 cY99 ... tY99
> 
> I have tried to use a for loop (simply assigning and also 
> with rbind) to
> do this but am having difficulty correctly assigning the 
> destination in the new dataframe. Can
> anyone offer a quick and easy way of doing this (or even a 
> long winded one
> if it works!!)
> 
> Thank you in advance,
> 
> Laura Quinn
> Institute of Atmospheric Science
> School of the Environment
> University of Leeds
> Leeds
> LS2 9JT
> 
> tel: +44 113 343 1596
> fax: +44 113 343 6716
> mail: laura at env.leeds.ac.uk
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From spencer.graves at pdf.com  Mon Aug 16 04:19:27 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 15 Aug 2004 22:19:27 -0400
Subject: [R] analysis of life tables
In-Reply-To: <411F56D3.2070902@uni-jena.de>
References: <411F56D3.2070902@uni-jena.de>
Message-ID: <412019AF.4050701@pdf.com>

      Have you considered library(survival) including "survreg"? 

      Also, have you "read the posting guide! 
http://www.R-project.org/posting-guide.html"?  You might be able to get 
answers to many questions faster than you could from this list.  And 
even if you don't get the answer, the posting guide might help you 
formulate your question so the response(s) might be more helpful than 
otherwise. 

      I got 60 hits for "life table" in an "R site search" at 
"www.r-project.org" -> search;  that included other suggestions that 
might interest you. 

      hope this helps. 
      spencer graves

Christoph Scherber wrote:

> Dear all,
>
> How can I analyze a life table (e.g. for a cohort of insects) in R?
>
> I have 20 insects in 200 cages with two different treatments, whose 
> survival is followed over time, such that, e.g., in one treatment, the 
> number of animals surviving is c(20,18,16,12,10,8,4,0), while in the 
> other treatment the survival is c(20,20,18,18,16,15,15,14) at 8 
> subsequent time intervals.
>
> I would very much appreciate any suggestions on how to analyze such a 
> dataset.
>
> Best regards
> Christoph.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From Carsten.Colombier at efv.admin.ch  Mon Aug 16 09:32:08 2004
From: Carsten.Colombier at efv.admin.ch (Carsten.Colombier@efv.admin.ch)
Date: Mon, 16 Aug 2004 09:32:08 +0200
Subject: [R] mutlicollinearity and MM-regression
Message-ID: <2CAE512CEB72EE448AADE3444E1FB7185B4893@ad04mexefd3.ad.admin.ch>

Dear R users,

Usually the variance-inflation factor, which is based on R^2, is used as a
measure for multicollinearity. But, in contrast to OLS regression there is
no robust R^2 available for MM-regressions in R. Do you know if an
equivalent or an alternative nmeasure of multicollinearity is available for
MM-regression in R?


With best regards,
Carsten Colombier

Dr. Carsten Colombier
Economist
Group of Economic Advisers
Swiss Federal Finance Administration
Bundesgasse 3
CH-3003 Bern

phone +41 31 322 63 32
fax +41 31 323 08 33
email: carsten.colombier at efv.admin.ch
www.efv.admin.ch



From ripley at stats.ox.ac.uk  Mon Aug 16 09:44:38 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 16 Aug 2004 08:44:38 +0100 (BST)
Subject: [R] mutlicollinearity and MM-regression
In-Reply-To: <2CAE512CEB72EE448AADE3444E1FB7185B4893@ad04mexefd3.ad.admin.ch>
Message-ID: <Pine.LNX.4.44.0408160842560.27542-100000@gannet.stats>

On Mon, 16 Aug 2004 Carsten.Colombier at efv.admin.ch wrote:

> Dear R users,
> 
> Usually the variance-inflation factor, which is based on R^2, is used as a
> measure for multicollinearity. 

I disagree, strongly, that this is `usual' practice.

> But, in contrast to OLS regression there is
> no robust R^2 available for MM-regressions in R. Do you know if an
> equivalent or an alternative nmeasure of multicollinearity is available for
> MM-regression in R?

?kappa .

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Thomas.Bock at ptb.de  Mon Aug 16 10:09:25 2004
From: Thomas.Bock at ptb.de (Thomas.Bock@ptb.de)
Date: Mon, 16 Aug 2004 10:09:25 +0200
Subject: [R] place 4 ini files
Message-ID: <OF12C1EF2B.CB8DED24-ONC1256EF2.0028D654-C1256EF2.002CCF12@bs.ptb.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040816/b0d8515e/attachment.pl

From asemeria at cramont.it  Mon Aug 16 10:28:50 2004
From: asemeria at cramont.it (asemeria@cramont.it)
Date: Mon, 16 Aug 2004 10:28:50 +0200
Subject: [R] place 4 ini files
Message-ID: <OF707CF7FE.91F4A931-ONC1256EF2.002E9595@tomware.it>

I think was better for you to save data
with save("your.ini.RData") and use load("your.ini.RData")
to load it.
Best!
A.S.

----------------------------

Alessandro Semeria
Models and Simulations Laboratory
Montecatini Environmental Research Center (Edison Group),
Via Ciro Menotti 48,
48023 Marina di Ravenna (RA), Italy
Tel. +39 544 536811
Fax. +39 544 538663
E-mail: alessandro.semeria at cramont.it



From ripley at stats.ox.ac.uk  Mon Aug 16 10:47:39 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 16 Aug 2004 09:47:39 +0100 (BST)
Subject: [R] place 4 ini files
In-Reply-To: <OF12C1EF2B.CB8DED24-ONC1256EF2.0028D654-C1256EF2.002CCF12@bs.ptb.de>
Message-ID: <Pine.LNX.4.44.0408160942150.29136-100000@gannet.stats>

Please do read what "writing R extensions" says about the data directory.
This is not a correct use of that directory, and may well fail in the next 
release.

I suggest you install them in an ini directory, by putting them in 
inst/ini in the package sources.

On Mon, 16 Aug 2004 Thomas.Bock at ptb.de wrote:

> I collect some lab specific functions for a package.
> Some of this functions need initial data 
> (ini files written in R-code). My question is:
> Where is the place in the package structure for such files; 
> how to load the files?
> I think something like:
> 
> source(paste(R.home(),"/library/__pkg__/data/__.ini.R", sep=""))

That makes assumptions about where a package is installed.  Use 
system.file(), e.g.

source(system.file("ini", "foo_ini.R", package="__pkg__"))

> looks a bit clumsy.
> I haven't find something about this 
> in the "writing R extensions".

See the discussion of `inst'.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From meinhardploner at gmx.net  Mon Aug 16 11:40:29 2004
From: meinhardploner at gmx.net (Meinhard GMX)
Date: Mon, 16 Aug 2004 11:40:29 +0200
Subject: [R] plot.table on R 1.9.1
Message-ID: <009401c48375$118508c0$4c75fb0a@ibm4cbh62tbodc>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040816/67505b20/attachment.pl

From ligges at statistik.uni-dortmund.de  Mon Aug 16 11:53:34 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 16 Aug 2004 11:53:34 +0200
Subject: [R] plot.table on R 1.9.1
In-Reply-To: <009401c48375$118508c0$4c75fb0a@ibm4cbh62tbodc>
References: <009401c48375$118508c0$4c75fb0a@ibm4cbh62tbodc>
Message-ID: <4120841E.9040600@statistik.uni-dortmund.de>

Meinhard GMX wrote:

> Hello!
> What is wrong on my system?
> I have downloaded today R 1.9.1 on my PC with Windows.
> 
> plot.table isn't available in package <base> and neither in package <graphics> (as suggested after ?plot.table). 

No. plot.table() is in package "graphics":

graphics:::plot.table

but hidden in the Namespace. It is a method of the generic plot(). For 
regular use please call plot() instead of plot.table().

Uwe Ligges



> Kind regards
> Meinhard Ploner
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Luisr at frs.fo  Mon Aug 16 11:53:33 2004
From: Luisr at frs.fo (Luis Rideau Cruz)
Date: Mon, 16 Aug 2004 10:53:33 +0100
Subject: [R] adding sem to a plot
Message-ID: <s1209235.032@ffdata.setur.fo>

R-help,

I have a barplot and I wish to add standard erros (or deviations) bars to it.
How?

Thank you

Luis Ridao Cruz
Fiskiranns??knarstovan
N??at??n 1
P.O. Box 3051
FR-110 T??rshavn
Faroe Islands
Phone:             +298 353900
Phone(direct): +298 353912
Mobile:             +298 580800
Fax:                 +298 353901
E-mail:              luisr at frs.fo
Web:                www.frs.fo



From ripley at stats.ox.ac.uk  Mon Aug 16 11:56:12 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 16 Aug 2004 10:56:12 +0100 (BST)
Subject: [R] plot.table on R 1.9.1
In-Reply-To: <009401c48375$118508c0$4c75fb0a@ibm4cbh62tbodc>
Message-ID: <Pine.LNX.4.44.0408161051420.7129-100000@gannet.stats>

I don't see anything in ?plot.table about calling plot.table: the usage is 
stated as

     ## S3 method for class 'table':
     plot(x, type = "h", ylim = c(0, max(x)), lwd = 2,
           xlab = NULL, ylab = NULL, frame.plot = is.num, ...)

Perhaps you are trying to use it incorrectly?  plot.table is a method for 
the generic function plot(), and plot() can find it.  You are not 
supposed to be able to invoke it directly, but

	getS3method("plot", "table")

will show it to you.

On Mon, 16 Aug 2004, Meinhard GMX wrote:

> What is wrong on my system?

Nothing, probably.

> I have downloaded today R 1.9.1 on my PC with Windows.
> 
> plot.table isn't available in package <base> and neither in package
> <graphics> (as suggested after ?plot.table).

Where does it say it is available?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Kevin.Wang at maths.anu.edu.au  Mon Aug 16 11:56:49 2004
From: Kevin.Wang at maths.anu.edu.au (Kevin Wang)
Date: Mon, 16 Aug 2004 19:56:49 +1000 (EST)
Subject: [R] plot.table on R 1.9.1
In-Reply-To: <009401c48375$118508c0$4c75fb0a@ibm4cbh62tbodc>
References: <009401c48375$118508c0$4c75fb0a@ibm4cbh62tbodc>
Message-ID: <Pine.GSO.4.58.0408161956010.5326@yin>

Hi,

On Mon, 16 Aug 2004, Meinhard GMX wrote:

> Hello!
> What is wrong on my system?
> I have downloaded today R 1.9.1 on my PC with Windows.
>
> plot.table isn't available in package <base> and neither in package <graphics> (as suggested after ?plot.table).

You should not use plot.table.  Use plot(x) where x is of table object and
it should work.

Cheers,

Kevin

--------------------------------
Ko-Kang Kevin Wang
PhD Student
Centre for Mathematics and its Applications
Building 27, Room 1004
Mathematical Sciences Institute (MSI)
Australian National University
Canberra, ACT 0200
Australia

Homepage: http://wwwmaths.anu.edu.au/~wangk/
Ph (W): +61-2-6125-2431
Ph (H): +61-2-6125-7407
Ph (M): +61-40-451-8301



From p.dalgaard at biostat.ku.dk  Mon Aug 16 12:04:28 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 16 Aug 2004 12:04:28 +0200
Subject: [R] adding sem to a plot
In-Reply-To: <s1209235.032@ffdata.setur.fo>
References: <s1209235.032@ffdata.setur.fo>
Message-ID: <x24qn3h00j.fsf@biostat.ku.dk>

"Luis Rideau Cruz" <Luisr at frs.fo> writes:

> R-help,
> 
> I have a barplot and I wish to add standard erros (or deviations) bars to it.
> How?

par(ask=T);example(barplot)

should give you the general idea.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From dimitris.rizopoulos at med.kuleuven.ac.be  Mon Aug 16 12:11:01 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Mon, 16 Aug 2004 12:11:01 +0200
Subject: [R] adding sem to a plot
References: <s1209235.032@ffdata.setur.fo>
Message-ID: <002c01c48379$5a8c1d70$ad133a86@www.domain>

Hi Luis,

take a look at these two links:

http://tolstoy.newcastle.edu.au/R/help/04/06/1129.html

http://tolstoy.newcastle.edu.au/R/help/04/06/1134.html

I hope this helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Doctoral Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "Luis Rideau Cruz" <Luisr at frs.fo>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, August 16, 2004 11:53 AM
Subject: [R] adding sem to a plot


> R-help,
>
> I have a barplot and I wish to add standard erros (or deviations)
bars to it.
> How?
>
> Thank you
>
> Luis Ridao Cruz
> Fiskiranns??knarstovan
> N??at??n 1
> P.O. Box 3051
> FR-110 T??rshavn
> Faroe Islands
> Phone:             +298 353900
> Phone(direct): +298 353912
> Mobile:             +298 580800
> Fax:                 +298 353901
> E-mail:              luisr at frs.fo
> Web:                www.frs.fo
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Kevin.Wang at maths.anu.edu.au  Mon Aug 16 12:14:48 2004
From: Kevin.Wang at maths.anu.edu.au (Kevin Wang)
Date: Mon, 16 Aug 2004 20:14:48 +1000 (EST)
Subject: [R] adding sem to a plot
In-Reply-To: <s1209235.032@ffdata.setur.fo>
References: <s1209235.032@ffdata.setur.fo>
Message-ID: <Pine.GSO.4.58.0408162014001.8847@yin>

Hi,

On Mon, 16 Aug 2004, Luis Rideau Cruz wrote:

> R-help,
>
> I have a barplot and I wish to add standard erros (or deviations) bars to it.
> How?

Marc Schwartz has an article on this in R News 3/2 (October 2003).

HTH,

Kevin

--------------------------------
Ko-Kang Kevin Wang
PhD Student
Centre for Mathematics and its Applications
Building 27, Room 1004
Mathematical Sciences Institute (MSI)
Australian National University
Canberra, ACT 0200
Australia

Homepage: http://wwwmaths.anu.edu.au/~wangk/
Ph (W): +61-2-6125-2431
Ph (H): +61-2-6125-7407
Ph (M): +61-40-451-8301



From john_hendrickx at yahoo.com  Mon Aug 16 12:53:17 2004
From: john_hendrickx at yahoo.com (John Hendrickx)
Date: Mon, 16 Aug 2004 03:53:17 -0700 (PDT)
Subject: [R] mutlicollinearity and MM-regression
In-Reply-To: <2CAE512CEB72EE448AADE3444E1FB7185B4893@ad04mexefd3.ad.admin.ch>
Message-ID: <20040816105317.22134.qmail@web52705.mail.yahoo.com>

--- Carsten.Colombier at efv.admin.ch wrote:

> Dear R users,
> 
> Usually the variance-inflation factor, which is based on R^2, is
> used as a
> measure for multicollinearity. But, in contrast to OLS regression
> there is
> no robust R^2 available for MM-regressions in R. Do you know if an
> equivalent or an alternative nmeasure of multicollinearity is
> available for
> MM-regression in R?
> 
I'm not sure what MM-regression is. But I've just put a general
purpose tool for evaluating collinearity on my website. See
http://www.xs4all.nl/~jhckx/R/perturb/

The perturb programs works by adding small random changes
(perturbations) to selected variables. Categorical variables are
randomly misclassified. This process is repeated a specified number
of times, after which the impact of the perturbations on parameter
stability can be evaluated. It should work with any R-procedure that
has a formula.

The package also contains colldiag, for calculating condition indexes
and variance decomposition proportions. Since this only works on the
independent variables, it should work for your problem as well.

Feedback welcomed. I plan to submit the package to CRAN in a few
days, after I get the help files updated



From liai at mail.nih.gov  Mon Aug 16 14:39:25 2004
From: liai at mail.nih.gov (Li, Aiguo (NIH/NCI))
Date: Mon, 16 Aug 2004 08:39:25 -0400
Subject: [R] Does anybody runs R on the hp ML 370 or ML570 servers?
Message-ID: <16A0583FB1644E4DB8C0A0265028B6FD28B93F@nihexchange13.nih.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040816/72484678/attachment.pl

From ripley at stats.ox.ac.uk  Mon Aug 16 15:00:41 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 16 Aug 2004 14:00:41 +0100 (BST)
Subject: [R] Does anybody runs R on the hp ML 370 or ML570 servers?
In-Reply-To: <16A0583FB1644E4DB8C0A0265028B6FD28B93F@nihexchange13.nih.gov>
Message-ID: <Pine.LNX.4.44.0408161357130.24092-100000@gannet.stats>

On Mon, 16 Aug 2004, Li, Aiguo (NIH/NCI) wrote:

> I am trying to buy a hp server to run R and to complete some other tasks
> with limited bugets.  The r-project.org site recommended that R will run on
> hppa-hp-hpux.  

I don't think they are _recommended_ there.

> However this system is out of our buget and ML system from hp
> is much cheaper.  Is there anybody running R on ML370 or ML 570 systems?  If
> you can provide me with some other information related that will be
> appreciated.

Those are just Xeon processors in a standard Wintel box.  What OS?
(They run Linux, Solaris, SCO Unix, Windows ....)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From laura at env.leeds.ac.uk  Mon Aug 16 15:33:45 2004
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Mon, 16 Aug 2004 14:33:45 +0100 (BST)
Subject: [R] Stacking Vectors/Dataframes
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF822F@usrymx25.merck.com>
Message-ID: <Pine.LNX.4.44.0408161432010.7794-100000@gw.env.leeds.ac.uk>

As our IT man is currently on holiday I am not able to upgrade to version
1.9.0(or 1.9.1) at the moment, and I see that the gregmisc library will
not work on earlier versions (I am using 1.8.0). Does anyone have any
other suggestions how I might be able to acheive this?

Thank you

Laura Quinn
Institute of Atmospheric Science
School of the Environment
University of Leeds
Leeds
LS2 9JT

tel: +44 113 343 1596
fax: +44 113 343 6716
mail: laura at env.leeds.ac.uk

On Sun, 15 Aug 2004, Liaw, Andy wrote:

> I believe interleave() in the `gregmisc' package can do what you want.
>
> Cheers,
> Andy
>
> > From: Laura Quinn
> >
> > Hello,
> >
> > Is there a simple way of stacking/merging two dataframes in
> > R? I want to
> > stack them piece-wise, not simply add one whole dataframe to
> > the bottom of
> > the other. I want to create as follows:
> >
> > x.frame:
> > aX1  bX1  cX1  ... zX1
> > aX2  bX2  cX2  ... zX2
> > ...  ...  ...  ... ...
> > aX99 bX99 cX99 ... zX99
> >
> > y.frame:
> > aY1  bY1  cY1  ... zY1
> > aY2  bY2  cY2  ... zY2
> > ...  ...  ...  ... ...
> > aY99 bY99 cY99 ... zY99
> >
> > new.frame:
> > aX1  bX1  cX1  ... zX1
> > aY1  bY1  cY1  ... zY1
> > aX2  bX2  cX2  ... zX2
> > aY2  bY2  cY2  ... tY2
> > ...  ...  ...  ... ...
> > aX99 bX99 cX99 ... tX99
> > aY99 bY99 cY99 ... tY99
> >
> > I have tried to use a for loop (simply assigning and also
> > with rbind) to
> > do this but am having difficulty correctly assigning the
> > destination in the new dataframe. Can
> > anyone offer a quick and easy way of doing this (or even a
> > long winded one
> > if it works!!)
> >
> > Thank you in advance,
> >
> > Laura Quinn
> > Institute of Atmospheric Science
> > School of the Environment
> > University of Leeds
> > Leeds
> > LS2 9JT
> >
> > tel: +44 113 343 1596
> > fax: +44 113 343 6716
> > mail: laura at env.leeds.ac.uk
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
>
>
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachments, contains information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New Jersey, USA 08889), and/or its affiliates (which may be known outside the United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as Banyu) that may be confidential, proprietary copyrighted and/or legally privileged. It is intended solely for the use of the individual or entity named on this message.  If you are not the intended recipient, and have received this message in error, please notify us immediately by reply e-mail and then delete it from your system.
> ------------------------------------------------------------------------------
>



From h.wickham at gmail.com  Mon Aug 16 15:50:55 2004
From: h.wickham at gmail.com (hadley wickham)
Date: Mon, 16 Aug 2004 08:50:55 -0500
Subject: [R] Stacking Vectors/Dataframes
In-Reply-To: <Pine.LNX.4.44.0408161432010.7794-100000@gw.env.leeds.ac.uk>
References: <Pine.LNX.4.44.0408161432010.7794-100000@gw.env.leeds.ac.uk>
Message-ID: <f8e6ff050408160650278ee35f@mail.gmail.com>

Hi Laura,

Off the top of my head I'd suggest something like this (assuming your
data frames are named a and b)

a$ord <- 1:nrow(a)
b$ord <- (1:nrow(b))*2
c <- rbind(a, b)
c <- c[order(c$ord), ]

ie.  add a new column that has the desired ordering, then join the two
data frames together, then reorder.  (code not checked).

Hadley



From FWS4 at CDRH.FDA.GOV  Mon Aug 16 15:54:35 2004
From: FWS4 at CDRH.FDA.GOV (Samuelson, Frank*)
Date: Mon, 16 Aug 2004 09:54:35 -0400
Subject: [R] Stacking Vectors/Dataframes
Message-ID: <644D9337A02FC24689647BF9E48EC39E08ABB862@drm556>


or something like
new.frame<-rbind(x.frame,y.frame);  # A frame of the right size.
new.frame[seq(1,nrow(x.frame),by=2),] <- x.frame  # Assign every other row
new.frame[seq(2,nrow(x.frame),by=2),] <- y.frame  # Assign every other row.

-----Original Message-----
From: Laura Quinn [mailto:laura at env.leeds.ac.uk] 
Sent: Sunday, August 15, 2004 7:11 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Stacking Vectors/Dataframes


Hello,

Is there a simple way of stacking/merging two dataframes in R? I want to
stack them piece-wise, not simply add one whole dataframe to the bottom of
the other. I want to create as follows:

x.frame:
aX1  bX1  cX1  ... zX1
aX2  bX2  cX2  ... zX2
...  ...  ...  ... ...
aX99 bX99 cX99 ... zX99

y.frame:
aY1  bY1  cY1  ... zY1
aY2  bY2  cY2  ... zY2
...  ...  ...  ... ...
aY99 bY99 cY99 ... zY99

new.frame:
aX1  bX1  cX1  ... zX1
aY1  bY1  cY1  ... zY1
aX2  bX2  cX2  ... zX2
aY2  bY2  cY2  ... tY2
...  ...  ...  ... ...
aX99 bX99 cX99 ... tX99
aY99 bY99 cY99 ... tY99

I have tried to use a for loop (simply assigning and also with rbind) to
do this but am having difficulty correctly assigning the destination in the
new dataframe. Can
anyone offer a quick and easy way of doing this (or even a long winded one
if it works!!)

Thank you in advance,

Laura Quinn
Institute of Atmospheric Science
School of the Environment
University of Leeds
Leeds
LS2 9JT

tel: +44 113 343 1596
fax: +44 113 343 6716
mail: laura at env.leeds.ac.uk

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From FWS4 at CDRH.FDA.GOV  Mon Aug 16 15:57:04 2004
From: FWS4 at CDRH.FDA.GOV (Samuelson, Frank*)
Date: Mon, 16 Aug 2004 09:57:04 -0400
Subject: [R] Stacking Vectors/Dataframes
Message-ID: <644D9337A02FC24689647BF9E48EC39E08ABB863@drm556>

Oops there was a bug...

new.frame<-rbind(x.frame,y.frame);  # A frame of the right size.
new.frame[seq(1,nrow(new.frame),by=2),] <- x.frame  # Assign every other row
new.frame[seq(2,nrow(new.frame),by=2),] <- y.frame  # Assign every other
row.


-----Original Message-----
From: Samuelson, Frank* 
Sent: Monday, August 16, 2004 9:55 AM
To: 'r-help at stat.math.ethz.ch'
Subject: Re: [R] Stacking Vectors/Dataframes



or something like
new.frame<-rbind(x.frame,y.frame);  # A frame of the right size.
new.frame[seq(1,nrow(x.frame),by=2),] <- x.frame  # Assign every other row
new.frame[seq(2,nrow(x.frame),by=2),] <- y.frame  # Assign every other row.

-----Original Message-----
From: Laura Quinn [mailto:laura at env.leeds.ac.uk] 
Sent: Sunday, August 15, 2004 7:11 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Stacking Vectors/Dataframes


Hello,

Is there a simple way of stacking/merging two dataframes in R? I want to
stack them piece-wise, not simply add one whole dataframe to the bottom of
the other. I want to create as follows:

x.frame:
aX1  bX1  cX1  ... zX1
aX2  bX2  cX2  ... zX2
...  ...  ...  ... ...
aX99 bX99 cX99 ... zX99

y.frame:
aY1  bY1  cY1  ... zY1
aY2  bY2  cY2  ... zY2
...  ...  ...  ... ...
aY99 bY99 cY99 ... zY99

new.frame:
aX1  bX1  cX1  ... zX1
aY1  bY1  cY1  ... zY1
aX2  bX2  cX2  ... zX2
aY2  bY2  cY2  ... tY2
...  ...  ...  ... ...
aX99 bX99 cX99 ... tX99
aY99 bY99 cY99 ... tY99

I have tried to use a for loop (simply assigning and also with rbind) to
do this but am having difficulty correctly assigning the destination in the
new dataframe. Can
anyone offer a quick and easy way of doing this (or even a long winded one
if it works!!)

Thank you in advance,

Laura Quinn
Institute of Atmospheric Science
School of the Environment
University of Leeds
Leeds
LS2 9JT

tel: +44 113 343 1596
fax: +44 113 343 6716
mail: laura at env.leeds.ac.uk

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From MSchwartz at MedAnalytics.com  Mon Aug 16 16:01:49 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon, 16 Aug 2004 09:01:49 -0500
Subject: [R] Stacking Vectors/Dataframes
In-Reply-To: <Pine.LNX.4.44.0408161432010.7794-100000@gw.env.leeds.ac.uk>
References: <Pine.LNX.4.44.0408161432010.7794-100000@gw.env.leeds.ac.uk>
Message-ID: <1092664909.5439.11.camel@localhost.localdomain>

Archived versions of gregmisc (and other packages) are available from:

http://cran.r-project.org/src/contrib/Archive/

Download one of the older versions (ie. 0.8.5) and install it from a
console using R CMD INSTALL.

If you are restricted from installing packages to the main R tree (ie.
you do not have the requisite permissions), see R FAQ 5.2 regarding
installing packages to alternate locations.

HTH,

Marc Schwartz

On Mon, 2004-08-16 at 08:33, Laura Quinn wrote:
> As our IT man is currently on holiday I am not able to upgrade to version
> 1.9.0(or 1.9.1) at the moment, and I see that the gregmisc library will
> not work on earlier versions (I am using 1.8.0). Does anyone have any
> other suggestions how I might be able to acheive this?
> 
> Thank you
> 
> Laura Quinn
> Institute of Atmospheric Science
> School of the Environment
> University of Leeds
> Leeds
> LS2 9JT
> 
> tel: +44 113 343 1596
> fax: +44 113 343 6716
> mail: laura at env.leeds.ac.uk
> 
> On Sun, 15 Aug 2004, Liaw, Andy wrote:
> 
> > I believe interleave() in the `gregmisc' package can do what you want.
> >
> > Cheers,
> > Andy
> >
> > > From: Laura Quinn
> > >
> > > Hello,
> > >
> > > Is there a simple way of stacking/merging two dataframes in
> > > R? I want to
> > > stack them piece-wise, not simply add one whole dataframe to
> > > the bottom of
> > > the other. I want to create as follows:
> > >
> > > x.frame:
> > > aX1  bX1  cX1  ... zX1
> > > aX2  bX2  cX2  ... zX2
> > > ...  ...  ...  ... ...
> > > aX99 bX99 cX99 ... zX99
> > >
> > > y.frame:
> > > aY1  bY1  cY1  ... zY1
> > > aY2  bY2  cY2  ... zY2
> > > ...  ...  ...  ... ...
> > > aY99 bY99 cY99 ... zY99
> > >
> > > new.frame:
> > > aX1  bX1  cX1  ... zX1
> > > aY1  bY1  cY1  ... zY1
> > > aX2  bX2  cX2  ... zX2
> > > aY2  bY2  cY2  ... tY2
> > > ...  ...  ...  ... ...
> > > aX99 bX99 cX99 ... tX99
> > > aY99 bY99 cY99 ... tY99
> > >
> > > I have tried to use a for loop (simply assigning and also
> > > with rbind) to
> > > do this but am having difficulty correctly assigning the
> > > destination in the new dataframe. Can
> > > anyone offer a quick and easy way of doing this (or even a
> > > long winded one
> > > if it works!!)
> > >
> > > Thank you in advance,



From maechler at stat.math.ethz.ch  Mon Aug 16 16:11:27 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 16 Aug 2004 16:11:27 +0200
Subject: [R] Stacking Vectors/Dataframes
In-Reply-To: <Pine.LNX.4.44.0408161432010.7794-100000@gw.env.leeds.ac.uk>
References: <3A822319EB35174CA3714066D590DCD504AF822F@usrymx25.merck.com>
	<Pine.LNX.4.44.0408161432010.7794-100000@gw.env.leeds.ac.uk>
Message-ID: <16672.49295.387233.710175@gargle.gargle.HOWL>

>>>>> "Laura" == Laura Quinn <laura at env.leeds.ac.uk>
>>>>>     on Mon, 16 Aug 2004 14:33:45 +0100 (BST) writes:

    Laura> As our IT man is currently on holiday I am not able
    Laura> to upgrade to version 1.9.0(or 1.9.1) at the moment,
    Laura> and I see that the gregmisc library will not work on
    Laura> earlier versions (I am using 1.8.0). 

well, how on earth dares he go on holiday without having updated
R to something more current !!




:-) ;-)


Martin



From ramasamy at cancer.org.uk  Mon Aug 16 16:17:53 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 16 Aug 2004 15:17:53 +0100
Subject: [R] Stacking Vectors/Dataframes
In-Reply-To: <Pine.LNX.4.44.0408161432010.7794-100000@gw.env.leeds.ac.uk>
References: <Pine.LNX.4.44.0408161432010.7794-100000@gw.env.leeds.ac.uk>
Message-ID: <1092665873.32064.219.camel@localhost.localdomain>

Yes, make a local installation on your home directory or another machine
which you have write access to.

AFAIK, you cannot update R itself. You will need to install R-1.9.1 and
delete the old version as opposed to upgrading from it. You can try to
salvage the "lib" directory but it is much better to re-download the
latest versions of packages if internet access is not a problem.

R-1.8.0 is fairly outdated anyway.

Regards, Adai


On Mon, 2004-08-16 at 14:33, Laura Quinn wrote:
> As our IT man is currently on holiday I am not able to upgrade to version
> 1.9.0(or 1.9.1) at the moment, and I see that the gregmisc library will
> not work on earlier versions (I am using 1.8.0). Does anyone have any
> other suggestions how I might be able to acheive this?
> 
> Thank you
> 
> Laura Quinn
> Institute of Atmospheric Science
> School of the Environment
> University of Leeds
> Leeds
> LS2 9JT
> 
> tel: +44 113 343 1596
> fax: +44 113 343 6716
> mail: laura at env.leeds.ac.uk
> 
> On Sun, 15 Aug 2004, Liaw, Andy wrote:
> 
> > I believe interleave() in the `gregmisc' package can do what you want.
> >
> > Cheers,
> > Andy
> >
> > > From: Laura Quinn
> > >
> > > Hello,
> > >
> > > Is there a simple way of stacking/merging two dataframes in
> > > R? I want to
> > > stack them piece-wise, not simply add one whole dataframe to
> > > the bottom of
> > > the other. I want to create as follows:
> > >
> > > x.frame:
> > > aX1  bX1  cX1  ... zX1
> > > aX2  bX2  cX2  ... zX2
> > > ...  ...  ...  ... ...
> > > aX99 bX99 cX99 ... zX99
> > >
> > > y.frame:
> > > aY1  bY1  cY1  ... zY1
> > > aY2  bY2  cY2  ... zY2
> > > ...  ...  ...  ... ...
> > > aY99 bY99 cY99 ... zY99
> > >
> > > new.frame:
> > > aX1  bX1  cX1  ... zX1
> > > aY1  bY1  cY1  ... zY1
> > > aX2  bX2  cX2  ... zX2
> > > aY2  bY2  cY2  ... tY2
> > > ...  ...  ...  ... ...
> > > aX99 bX99 cX99 ... tX99
> > > aY99 bY99 cY99 ... tY99
> > >
> > > I have tried to use a for loop (simply assigning and also
> > > with rbind) to
> > > do this but am having difficulty correctly assigning the
> > > destination in the new dataframe. Can
> > > anyone offer a quick and easy way of doing this (or even a
> > > long winded one
> > > if it works!!)
> > >
> > > Thank you in advance,
> > >
> > > Laura Quinn
> > > Institute of Atmospheric Science
> > > School of the Environment
> > > University of Leeds
> > > Leeds
> > > LS2 9JT
> > >
> > > tel: +44 113 343 1596
> > > fax: +44 113 343 6716
> > > mail: laura at env.leeds.ac.uk
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > >
> > >
> >
> >
> > ------------------------------------------------------------------------------
> > Notice:  This e-mail message, together with any attachments, contains information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New Jersey, USA 08889), and/or its affiliates (which may be known outside the United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as Banyu) that may be confidential, proprietary copyrighted and/or legally privileged. It is intended solely for the use of the individual or entity named on this message.  If you are not the intended recipient, and have received this message in error, please notify us immediately by reply e-mail and then delete it from your system.
> > ------------------------------------------------------------------------------
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From liai at mail.nih.gov  Mon Aug 16 16:58:49 2004
From: liai at mail.nih.gov (Li, Aiguo (NIH/NCI))
Date: Mon, 16 Aug 2004 10:58:49 -0400
Subject: [R] Does anybody has a build on the following systems?
Message-ID: <16A0583FB1644E4DB8C0A0265028B6FD28B941@nihexchange13.nih.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040816/6c46a8b2/attachment.pl

From danbebber at forestecology.co.uk  Mon Aug 16 17:23:06 2004
From: danbebber at forestecology.co.uk (Dan Bebber)
Date: Mon, 16 Aug 2004 16:23:06 +0100
Subject: [R] Multiple logistic curves
Message-ID: <000001c483a4$ee18e2a0$442501a3@plants.ox.ac.uk>

Dear list,

Apologies, I have sent this message before but received no replies so I'm
trying again just in case...

Motivated by the discovery of 'loglet analysis'
(http://phe.rockefeller.edu/LogletLab/) that allows decomposition of growth
curves into a series of logistic equations, I attempted to do the same thing
in R.

#SIMULATED DATA
Time <- 1:200
pop.size <- SSlogis(Time,10,20,5) + SSlogis(Time,20,100,20) +
rnorm(length(Time))
ts.plot(pop.size)

#MY ANALYSIS
results <- nls(pop.size ~ SSlogis(Time, Asym1, xmid1, scal1) + SSlogis(Time,
Asym2, xmid2, scal2),
start = list(Asym1=5, xmid1=15, scal1=30, Asym2=25, xmid2=60, scal2=25))

THE RESULT
I get the error message:
Error in nls(size ~ SSlogis(Time, Asym1, xmid1, scal1) + SSlogis(Time,  :
        step factor 0.000488281 reduced below `minFactor' of 0.000976563

Any hints in making this analysis work would be greatly appreciated.

Dan Bebber
____________________________
Dr. Daniel P. Bebber
Department of Plant Sciences
University of Oxford
South Parks Road
Oxford
OX1 3RB
Tel. 01865 275060
Web. http://www.forestecology.co.uk/



From christoph.lehmann at gmx.ch  Mon Aug 16 17:43:36 2004
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Mon, 16 Aug 2004 17:43:36 +0200
Subject: [R] huge array with floats: allocation error
Message-ID: <4120D628.2070707@gmx.ch>

Hi

After searching through a couples of documents and the mailing list I 
dare to ask it here

I need to define an array with the size 64 x 64 x 16 x 1000 for 
single-precision floating-point numbers. With 1G RAM I get always the 
error:

"cannot allocate vector of size 458752 Kb"
"reached total allocation of 1022MB: see help(memory.size)"

I consulted memory.size() but it didn't help me.

so my question: I know that there is NO float type in R. Is there any 
way to solve my problem, without increasing the RAM?

many thanks

Cheers!
Christoph



From sraghavan at mmm.com  Mon Aug 16 18:25:57 2004
From: sraghavan at mmm.com (sraghavan@mmm.com)
Date: Mon, 16 Aug 2004 11:25:57 -0500
Subject: [R] using nls to fit a four parameter logistic model
Message-ID: <OF24E63BBE.69F12D62-ON86256EF2.005A3E49-86256EF2.005A448D@mmm.com>





Shalini Raghavan
3M Pharmaceuticals Research
Building 270-03-A-10, 3M Center
St. Paul, MN  55144
E-mail: sraghavan at mmm.com
Tel:  651-736-2575
Fax:  651-733-5096

----- Forwarded by Shalini Raghavan/US-Corporate/3M/US on 08/16/2004 11:25
AM -----
                                                                           
             Shalini                                                       
             Raghavan/US-Corpo                                             
             rate/3M/US                                                 To 
                                       r-help at stat.math.ethz.ch.           
             08/16/2004 08:57                                           cc 
             AM                                                            
                                                                   Subject 
                                       Fw: using nls to fit a four         
                                       parameter logistic model            
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           







I am working on what appears to be a fairly simple problem for the
following data

 test=data.frame(cbind(conc=c(25000, 12500, 6250, 3125, 1513, 781, 391,
195, 97.7, 48.4, 24, 12, 6, 3, 1.5, 0.001),
 il10=c(330269, 216875, 104613, 51372, 26842, 13256, 7255, 3049, 1849, 743,
480, 255, 241, 128, 103, 50)))
> test
        conc   il10
1  25000.000 330269
2  12500.000 216875
3   6250.000 104613
4   3125.000  51372
5   1513.000  26842
6    781.000  13256
7    391.000   7255
8    195.000   3049
9     97.700   1849
10    48.400    743
11    24.000    480
12    12.000    255
13     6.000    241
14     3.000    128
15     1.500    103
16     0.001     50

I am able to fit the above data to the equation

> nls(log(il10)~A+(B-A)/(1+exp((xmid-log(conc))/scal)),data=test,
+  start = list(A=log(0.001), B=log(100000),
+ xmid=log(6000),scal=0.8))
Nonlinear regression model
  model:  log(il10) ~ A + (B - A)/(1 + exp((xmid - log(conc))/scal))
   data:  test
        A         B      xmid      scal
 3.796457 14.705159  6.410144  2.507653
 residual sum-of-squares:  0.1667462


But in attempting to achieve a fit to what is commonly known as the hill
equation, which is a four parameter fit that is used widely in biological
data analysis

nls(log(il10)~A+(B-A)/(1+(log(conc)/xmid )^scal),data=test,
+ start = list(A=log(0.001), B=log(100000),  xmid=log(6000),scal=0.8))

Nonlinear regression model
  model:  log(il10) ~ A + (B - A)/(1 + (log(conc)/xmid )^scal)

Error in numericDeriv(form[[3]], names(ind), env) :
        Missing value or an Infinity produced when evaluating the model



Please would someone offer a suggestion

Shalini



From ripley at stats.ox.ac.uk  Mon Aug 16 18:31:56 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 16 Aug 2004 17:31:56 +0100 (BST)
Subject: [R] huge array with floats: allocation error
In-Reply-To: <4120D628.2070707@gmx.ch>
Message-ID: <Pine.LNX.4.44.0408161717270.7154-100000@gannet.stats>

On Mon, 16 Aug 2004, Christoph Lehmann wrote:

> After searching through a couples of documents and the mailing list I 
> dare to ask it here

I don't believe you read the rw-FAQ as the posting guide asks, though.
You seem to be working under Windows, without saying so (and the posting 
guide does ask you to).  So that's `a couples of documents' worth
`searching through'.

> I need to define an array with the size 64 x 64 x 16 x 1000 for 
> single-precision floating-point numbers. With 1G RAM I get always the 
> error:
> 
> "cannot allocate vector of size 458752 Kb"
> "reached total allocation of 1022MB: see help(memory.size)"
> 
> I consulted memory.size() but it didn't help me.

This *is* covered in the rw-FAQ, as well as on that help page, viz

     Command-line flag '--max-mem-size' sets the maximum value of
     obtainable memory (including a very small amount of housekeeping
     overhead).

> so my question: I know that there is NO float type in R. Is there any 
> way to solve my problem, without increasing the RAM?

Your array has 65 million numbers. That's well under the size of your RAM 
in R's doubles (500MB). And

> x <- rep(0, 64 * 64 * 16 * 1000)
> dim(x) <- c(64,64,16,1000)

does work on a 1Gb Windows machine, so your problem is in however (you 
didn't tell us) you were trying to do it.  Again, see the posting guide.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From paolo at directwave.com.br  Mon Aug 16 18:48:56 2004
From: paolo at directwave.com.br (Paolo Tommasini)
Date: Mon, 16 Aug 2004 13:48:56 -0300
Subject: [R] (no subject)
Message-ID: <4120E578.1040400@directwave.com.br>

Hi there I teach a statistics class with R ( the first time ever with R) 
and some of my students have windows at home and they want to be able to 
graphs tjhey make in our college ( that only has Linux) and oopen with 
windows, I think the best wya would be saving it as a JPEG file but I 
don't know how to do it ! I only know how to do it with EPS
dev.copy2eps(file="bla.eps")
how can I save it as JPEG  ? or BMP any other format that is OK for 
windows ?

thank you very much

Paolo



From christoph.lehmann at gmx.ch  Mon Aug 16 18:52:49 2004
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Mon, 16 Aug 2004 18:52:49 +0200
Subject: [R] huge array with floats: allocation error
In-Reply-To: <Pine.LNX.4.44.0408161717270.7154-100000@gannet.stats>
References: <Pine.LNX.4.44.0408161717270.7154-100000@gannet.stats>
Message-ID: <4120E661.1010307@gmx.ch>

Thank you, Prof. Ripley


> I don't believe you read the rw-FAQ as the posting guide asks, though.
> You seem to be working under Windows, without saying so (and the posting 
> guide does ask you to).  So that's `a couples of documents' worth
> `searching through'.
I apologize for not being more precise. Fact is:

 >>x <- rep(0, 64 * 64 * 16 * 1000)
 >>dim(x) <- c(64,64,16,1000)

indeed DOES work on (I tried it on several machines):
(i) Linux Suse9.1 box with 1G RAM
(ii) Windows WinXp with 1G RAM
(iii) Windows WinXp with 2G RAM

all with R Version 1.9.1

but:

tst.array <- array(0, c(64, 64, 16, 1000))
Error: cannot allocate vector of size 512000 Kb

does not work on none of these machines/RAM combinations

why is this?

many thanks for a further hint. I am sure I overlooked something very 
basic- forgive me.

Christoph



From Roger.Bivand at nhh.no  Mon Aug 16 19:03:50 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 16 Aug 2004 19:03:50 +0200 (CEST)
Subject: [R] portable graphics output; was: (no subject)
In-Reply-To: <4120E578.1040400@directwave.com.br>
Message-ID: <Pine.LNX.4.44.0408161901180.878-100000@reclus.nhh.no>

On Mon, 16 Aug 2004, Paolo Tommasini wrote:

> Hi there I teach a statistics class with R ( the first time ever with R) 
> and some of my students have windows at home and they want to be able to 
> graphs tjhey make in our college ( that only has Linux) and oopen with 
> windows, I think the best wya would be saving it as a JPEG file but I 
> don't know how to do it ! I only know how to do it with EPS
> dev.copy2eps(file="bla.eps")
> how can I save it as JPEG  ? or BMP any other format that is OK for 
> windows ?

I think the PDF device may be most help, giving very portable graphics 
that scale well. Otherwise, PNG graphics with larger width and height are 
also good: see

?pdf

and

?png

> 
> thank you very much
> 
> Paolo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From ahenningsen at email.uni-kiel.de  Mon Aug 16 19:13:52 2004
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Mon, 16 Aug 2004 19:13:52 +0200
Subject: [R] (no subject)
In-Reply-To: <4120E578.1040400@directwave.com.br>
References: <4120E578.1040400@directwave.com.br>
Message-ID: <200408161913.52792.ahenningsen@email.uni-kiel.de>

look at
> ?jpeg
PNG format might be better:
> ?png

Arne

On Monday 16 August 2004 18:48, Paolo Tommasini wrote:
> Hi there I teach a statistics class with R ( the first time ever with R)
> and some of my students have windows at home and they want to be able to
> graphs tjhey make in our college ( that only has Linux) and oopen with
> windows, I think the best wya would be saving it as a JPEG file but I
> don't know how to do it ! I only know how to do it with EPS
> dev.copy2eps(file="bla.eps")
> how can I save it as JPEG  ? or BMP any other format that is OK for
> windows ?
>
> thank you very much
>
> Paolo
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From mhassan at scitegic.com  Mon Aug 16 19:25:08 2004
From: mhassan at scitegic.com (Moises Hassan)
Date: Mon, 16 Aug 2004 10:25:08 -0700
Subject: [R] capture stderr in Windows
Message-ID: <830D8D4719112B418ABBC3A0EBA9581272EAD2@webmail.scitegic.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040816/4c51cb16/attachment.pl

From JEFHEN at SAFECO.com  Mon Aug 16 19:33:16 2004
From: JEFHEN at SAFECO.com (HENRIKSON, JEFFREY)
Date: Mon, 16 Aug 2004 10:33:16 -0700
Subject: [R] turning off automatic coersion from list to matrix
Message-ID: <9410EC84C0872141B27A2726613EF45D10976B6B@psmrdcex01.psm.pin.safeco.com>


Hello,

I am having trouble understanding how R is coercing between matrices and
lists in the following example.  I have an aggregate behavior I like:

aggregate(a[,"num"],by=list(product=a[,"product"],region=a[,"region"]),
sum)

Now in reality I have more columns than just product and region, and
need to pick different combinations.  So I want to abstract this into a
function.  Example use:

myagg(a,c("product","region"))
  
But I am having trouble because by= requires a list and apply and sapply
seem to cast things back to the "matrix" type automatically.  Can I turn
this off?  Eg:

data.class(sapply(c("product","region"),function(i) {a[,i]}))
[1] "matrix"

whereas this would be acceptable to by=

data.class(list(product=a[,"product"],region=a[,"region"]))
[1] "list"


Regards,


Jeff Henrikson



From murdoch at stats.uwo.ca  Mon Aug 16 19:43:52 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 16 Aug 2004 13:43:52 -0400
Subject: [R] capture stderr in Windows
In-Reply-To: <830D8D4719112B418ABBC3A0EBA9581272EAD2@webmail.scitegic.com>
References: <830D8D4719112B418ABBC3A0EBA9581272EAD2@webmail.scitegic.com>
Message-ID: <f0s1i0ltti3p837388jiq1eljngacdit20@4ax.com>

On Mon, 16 Aug 2004 10:25:08 -0700, "Moises Hassan"
<mhassan at scitegic.com> wrote :

>I'm using the following command to run R in Windows
>
> 
>
>"Rterm --no-save --no-restore < "Rscriptfile" > "Rstdoutfile"
>
> 
>
>How can I capture the text sent by R to stderr in a file?

That depends on your shell.  The standard Windows command shell
COMMAND.COM or CMD.EXE provides no easy way to do this.  There are
lots of replacement shells around that can do it; I use Cygwin's bash
shell most of the time.

There are also programs on the net that do nothing but redirect
standard handles, e.g. <http://www.commandline.co.uk/mtee/index.html>.
I've never tested these.

Duncan Murdoch



From ligges at statistik.uni-dortmund.de  Mon Aug 16 19:48:32 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 16 Aug 2004 19:48:32 +0200
Subject: [R] capture stderr in Windows
In-Reply-To: <830D8D4719112B418ABBC3A0EBA9581272EAD2@webmail.scitegic.com>
References: <830D8D4719112B418ABBC3A0EBA9581272EAD2@webmail.scitegic.com>
Message-ID: <4120F370.7020805@statistik.uni-dortmund.de>

Moises Hassan wrote:

> I'm using the following command to run R in Windows
> 
>  
> 
> "Rterm --no-save --no-restore < "Rscriptfile" > "Rstdoutfile"
> 
>  
> 
> How can I capture the text sent by R to stderr in a file?

"Rterm --no-save --no-restore < "Rscriptfile" 2>&1 "Rstdoutfile"

I'd rather use R CMD BATCH anyway.

Uwe Ligges

>  
> 
> Thanks, 
> 
>    - Moises
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From zelickr at pdx.edu  Mon Aug 16 19:48:36 2004
From: zelickr at pdx.edu (Randy Zelick)
Date: Mon, 16 Aug 2004 10:48:36 -0700 (PDT)
Subject: [R] extract a row
Message-ID: <Pine.GSO.4.44.0408161040130.6425-100000@gere.odin.pdx.edu>

Hello there,

Using 1.9.0 on WinXP...

I have a data frame, one column of which is named "rate". The column has
text entries like "fast", "medium", "slow", "very slow", and so forth. I
have not tried to make them factors, but maybe R did this automatically.

Anyway, I would like to display on the console rows that meet a rate
criterion.

So I want to type something like:

dataframe[rate=="slow"]

and get back this sort of output:

test  subject  trial  rate  score

 34    B27      3     slow   27
 55    B55      4     slow   34

where test, subject, trial, rate, and score are all the
dataframe's columns.

How do I do that??

Thanks,

=Randy=


R. Zelick				email: zelickr at pdx.edu
Department of Biology			voice: 503-725-3086
Portland State University		fax:   503-725-3888

mailing:
P.O. Box 751
Portland, OR 97207

shipping:
1719 SW 10th Ave, Room 246
Portland, OR 97201



From murdoch at stats.uwo.ca  Mon Aug 16 19:57:31 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 16 Aug 2004 13:57:31 -0400
Subject: [R] capture stderr in Windows
In-Reply-To: <830D8D4719112B418ABBC3A0EBA9581272EAD2@webmail.scitegic.com>
References: <830D8D4719112B418ABBC3A0EBA9581272EAD2@webmail.scitegic.com>
Message-ID: <53t1i09uokq7uiqfqg7jl2i7jshvvum1e0@4ax.com>

I wrote the message below, but it's just plain wrong.

The CMD.EXE shell in Win XP (and 2K?) allows redirection of stderr in
the usual Unix style:

  Rterm --no-save --no-restore < "Rscriptfile" > "Rstdoutfile"
2>"Rstderrfile"

You can also use "2>&1" to redirect stderr into the stdout stream, so
both go to Rstdoutfile.

Duncan Murdoch


>On Mon, 16 Aug 2004 10:25:08 -0700, "Moises Hassan" <mhassan at scitegic.com> wrote :
>
>>I'm using the following command to run R in Windows
>>
>> 
>>
>>"Rterm --no-save --no-restore < "Rscriptfile" > "Rstdoutfile"
>>
>> 
>>
>>How can I capture the text sent by R to stderr in a file?
>
>That depends on your shell.  The standard Windows command shell COMMAND.COM or CMD.EXE provides no easy way to do this.  There are lots of replacement shells around that can do it; I use Cygwin's bash shell most of the time.
>
>There are also programs on the net that do nothing but redirect standard handles, e.g. <http://www.commandline.co.uk/mtee/index.html>. I've never tested these.
>
>Duncan Murdoch



From ramasamy at cancer.org.uk  Mon Aug 16 20:15:53 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 16 Aug 2004 19:15:53 +0100
Subject: [R] extract a row
In-Reply-To: <Pine.GSO.4.44.0408161040130.6425-100000@gere.odin.pdx.edu>
References: <Pine.GSO.4.44.0408161040130.6425-100000@gere.odin.pdx.edu>
Message-ID: <1092680152.32064.252.camel@localhost.localdomain>

df[ df$rate==slow, ]


On Mon, 2004-08-16 at 18:48, Randy Zelick wrote:
> Hello there,
> 
> Using 1.9.0 on WinXP...
> 
> I have a data frame, one column of which is named "rate". The column has
> text entries like "fast", "medium", "slow", "very slow", and so forth. I
> have not tried to make them factors, but maybe R did this automatically.
> 
> Anyway, I would like to display on the console rows that meet a rate
> criterion.
> 
> So I want to type something like:
> 
> dataframe[rate=="slow"]
> 
> and get back this sort of output:
> 
> test  subject  trial  rate  score
> 
>  34    B27      3     slow   27
>  55    B55      4     slow   34
> 
> where test, subject, trial, rate, and score are all the
> dataframe's columns.
> 
> How do I do that??
> 
> Thanks,
> 
> =Randy=
> 
> 
> R. Zelick				email: zelickr at pdx.edu
> Department of Biology			voice: 503-725-3086
> Portland State University		fax:   503-725-3888
> 
> mailing:
> P.O. Box 751
> Portland, OR 97207
> 
> shipping:
> 1719 SW 10th Ave, Room 246
> Portland, OR 97201
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Mon Aug 16 20:45:27 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 16 Aug 2004 19:45:27 +0100 (BST)
Subject: [R] capture stderr in Windows
In-Reply-To: <53t1i09uokq7uiqfqg7jl2i7jshvvum1e0@4ax.com>
Message-ID: <Pine.LNX.4.44.0408161940030.7410-100000@gannet.stats>

On Mon, 16 Aug 2004, Duncan Murdoch wrote:

> The CMD.EXE shell in Win XP (and 2K?) allows redirection of stderr in
> the usual Unix style:

(Yes, all NT-based versions of Windows.)

>   Rterm --no-save --no-restore < "Rscriptfile" > "Rstdoutfile"
> 2>"Rstderrfile"
> 
> You can also use "2>&1" to redirect stderr into the stdout stream, so
> both go to Rstdoutfile.

[But the order matters, so first redirect stdout and then redirect stderr 
to stdout.]

And for completeness, 

1) Under non-NT Windows (95/98/ME) stderr is the same as stdout since its 
shells don't know about stderr.

2) This is in all the rw-FAQ, Q2.10

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rduarte at unicamp.br  Mon Aug 16 21:23:07 2004
From: rduarte at unicamp.br (Rodrigo Drummond)
Date: Mon, 16 Aug 2004 16:23:07 -0300 (BRT)
Subject: [R] Sum of squares simultaneous test procedure (SS-STP)
In-Reply-To: <200408121011.i7CA3u4e012983@hypatia.math.ethz.ch>
References: <200408121011.i7CA3u4e012983@hypatia.math.ethz.ch>
Message-ID: <2525.143.106.4.184.1092684187.squirrel@143.106.4.184>

Hi all,

I need to run a sum of squares simultaneous test procedure (SS-STP) with
R, although I didn't find a function to do this. The command 'se.contrast?
returns the standard errors for one or more contrasts in an analysis of
variance, although it is designed for planned comparisons, and I need a
test for unplanned ones.
Can someone help me?
Thanks a lot!
Rodrigo Drummond

____________________________________________
Rodrigo D. Drummond
Laboratorio Genoma Funcional
Centro de Biologia Molecular e Eng. Genetica
Universidade Estadual de Campinas
Caixa Postal 6010
13083-875 - Campinas - SP - Brasil
Tel: xx-19-3788-1119 Fax: xx-19-3788-1089



From OBENCHAIN_ROBERT_L at Lilly.com  Mon Aug 16 22:01:41 2004
From: OBENCHAIN_ROBERT_L at Lilly.com (Robert L Obenchain)
Date: Mon, 16 Aug 2004 15:01:41 -0500
Subject: [R] Interacting with Clusters...
Message-ID: <OF56C994D3.EE59E1A6-ON05256EF2.006AF921@EliLilly.lilly.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040816/432d3e02/attachment.pl

From gb at stat.umu.se  Tue Aug 17 05:09:53 2004
From: gb at stat.umu.se (Gran Brostrm)
Date: Mon, 16 Aug 2004 23:09:53 -0400
Subject: [R] analysis of life tables
In-Reply-To: <411F56D3.2070902@uni-jena.de>
References: <411F56D3.2070902@uni-jena.de>
Message-ID: <20040817030953.GA14879@stat.umu.se>

On Sun, Aug 15, 2004 at 02:28:03PM +0200, Christoph Scherber wrote:
> Dear all,
> 
> How can I analyze a life table (e.g. for a cohort of insects) in R?
> 
> I have 20 insects in 200 cages with two different treatments, whose 
> survival is followed over time, such that, e.g., in one treatment, the 
> number of animals surviving is c(20,18,16,12,10,8,4,0), while in the 
> other treatment the survival is c(20,20,18,18,16,15,15,14) at 8 
> subsequent time intervals.

One way of doing it would be to create an individual-based data frame
from your two life tables, which would give you 

> dat
   enter exit event treatment
1      0    1     1         1
2      0    1     1         1
3      0    2     1         1
4      0    2     1         1
5      0    3     1         1
6      0    3     1         1
7      0    3     1         1
8      0    3     1         1
9      0    4     1         1
10     0    4     1         1
11     0    5     1         1
12     0    5     1         1
13     0    6     1         1
14     0    6     1         1
15     0    6     1         1
16     0    6     1         1
17     0    7     1         1
18     0    7     1         1
19     0    7     1         1
20     0    7     1         1
21     0    2     1         2
22     0    2     1         2
23     0    4     1         2
24     0    4     1         2
25     0    5     1         2
26     0    7     1         2
27     0    7     0         2
28     0    7     0         2
29     0    7     0         2
30     0    7     0         2
31     0    7     0         2
32     0    7     0         2
33     0    7     0         2
34     0    7     0         2
35     0    7     0         2
36     0    7     0         2
37     0    7     0         2
38     0    7     0         2
39     0    7     0         2
40     0    7     0         2

where 'exit' represents the ordered time intervals. Then you can choose 
between a discrete-time (recommended) and a continuous-time Cox regression.
In package 'eha' that corresponds to the functions 'mlreg' and 'coxreg',
respectively. Output from mlreg:
----------------------------------------------------------------------
> mlreg(Surv(enter, exit, event) ~ treatment, data = dat)
Call:
mlreg(formula = Surv(enter, exit, event) ~ treatment, data = dat)

Covariate           Mean       Coef  Rel.Risk       Wald p
treatment
               1    0.419     0         1           (reference)
               2    0.581    -2.121     0.120        0.000

Events                    26
Total time at risk           210
Max. log. likelihood      -64.734
LR test statistic         21.9
Degrees of freedom        1
Overall p-value           2.82840e-06
-----------------------------------------------------------------------

This corresponds essentially to a binomial regression approach, where you 
regard the number of deaths in each time interval for each treatment as 
the outcome of a binomial experiment with n = riskset size at the beginning
of the interval.

An alternative would be Poisson regression with riskset size as an offset.
-- 
 G??ran Brostr??m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume?? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume??, Sweden             e-mail: gb at stat.umu.se



From richard_raubertas at merck.com  Tue Aug 17 00:06:18 2004
From: richard_raubertas at merck.com (Raubertas, Richard)
Date: Mon, 16 Aug 2004 18:06:18 -0400
Subject: [R] Dotplot with nested factors
Message-ID: <B88F4BCF37DD0847937C1C98255291FB0175C03D@uswsmx05.merck.com>

I am using the dotplot function from the lattice package to
display a quantitative variable versus two factors, say 'a' and
'b'.  The levels of 'a' are nested within levels of 'b'.  The 
issue is that dotplot includes all the levels of 'a' in each panel
(conditioning on 'b'), even though many are empty in any given 
panel.  A toy example is

dat <- data.frame(a=letters[1:5], b=c("A","A","A","B","B"), y=1:5)
dotplot(y ~ a | b, data=dat)

(In the real data, there are far more levels of 'a' and 'b', so each
panel ends up with all its data squashed into a small portion of 
the available space.)

I would like to show only the levels of 'a' actually present 
in a given panel.  I hoped that setting 'relation="free"' would 
cause each panel to adjust its axis to match the set of 'a' values 
observed in that panel, but unfortunately it seems 'relation' is 
ignored for factors.  Would it make sense to have 'relation' work
for factors?  Is there some other way to accomplish the same effect?

Rich Raubertas
Merck & Co.



From kbartz at loyaltymatrix.com  Tue Aug 17 00:26:56 2004
From: kbartz at loyaltymatrix.com (Kevin Bartz)
Date: Mon, 16 Aug 2004 15:26:56 -0700
Subject: [R] extract a row
In-Reply-To: <Pine.GSO.4.44.0408161040130.6425-100000@gere.odin.pdx.edu>
Message-ID: <20040816223301.3C9413FC42@omta16.mta.everyone.net>

The easiest way to do that is

subset(dataframe, rate == "slow").

Please let me know if you have any more questions.

Kevin

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Randy Zelick
Sent: Monday, August 16, 2004 10:49 AM
To: R list server posting
Subject: [R] extract a row

Hello there,

Using 1.9.0 on WinXP...

I have a data frame, one column of which is named "rate". The column has
text entries like "fast", "medium", "slow", "very slow", and so forth. I
have not tried to make them factors, but maybe R did this automatically.

Anyway, I would like to display on the console rows that meet a rate
criterion.

So I want to type something like:

dataframe[rate=="slow"]

and get back this sort of output:

test  subject  trial  rate  score

 34    B27      3     slow   27
 55    B55      4     slow   34

where test, subject, trial, rate, and score are all the
dataframe's columns.

How do I do that??

Thanks,

=Randy=


R. Zelick				email: zelickr at pdx.edu
Department of Biology			voice: 503-725-3086
Portland State University		fax:   503-725-3888

mailing:
P.O. Box 751
Portland, OR 97207

shipping:
1719 SW 10th Ave, Room 246
Portland, OR 97201

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From deepayan at stat.wisc.edu  Tue Aug 17 01:54:44 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Mon, 16 Aug 2004 18:54:44 -0500
Subject: [R] Re: Dotplot with nested factors
In-Reply-To: <B88F4BCF37DD0847937C1C98255291FB0175C03D@uswsmx05.merck.com>
References: <B88F4BCF37DD0847937C1C98255291FB0175C03D@uswsmx05.merck.com>
Message-ID: <200408161854.44151.deepayan@stat.wisc.edu>

On Monday 16 August 2004 17:06, Raubertas, Richard wrote:
> I am using the dotplot function from the lattice package to
> display a quantitative variable versus two factors, say 'a' and
> 'b'.  The levels of 'a' are nested within levels of 'b'.  The
> issue is that dotplot includes all the levels of 'a' in each panel
> (conditioning on 'b'), even though many are empty in any given
> panel.  A toy example is
>
> dat <- data.frame(a=letters[1:5], b=c("A","A","A","B","B"), y=1:5)
> dotplot(y ~ a | b, data=dat)
>
> (In the real data, there are far more levels of 'a' and 'b', so each
> panel ends up with all its data squashed into a small portion of
> the available space.)
>
> I would like to show only the levels of 'a' actually present
> in a given panel.  I hoped that setting 'relation="free"' would
> cause each panel to adjust its axis to match the set of 'a' values
> observed in that panel, but unfortunately it seems 'relation' is
> ignored for factors.  Would it make sense to have 'relation' work
> for factors?  Is there some other way to accomplish the same effect?

Yes and no, respectively.

This was recently brought up by John Maindonald off-list, and I had a 
tentative fix planned. Unfortunately, I just realized that it would 
only work when the 'present' levels within each panel are contiguous 
levels of the factor. I'll have to think a bit more about it. Whatever 
the fix, it would have to break the current API, so it's not going to 
be available before R 2.0.0. 

Here's a bad workaround (bad because it would give wrong answers if 
relation != "free"):

dotplot(y ~ a | b, data=dat,
        scales = list(x = list(relation = "free")),
        prepanel = function(x, y, ...) {
            ans <- list()
            if (is.factor(x)) {
                x <- x[drop = TRUE]
                ans$xlim <- levels(x)
            }
            if (is.factor(x)) {
                y <- y[drop = TRUE]
                ans$ylim <- levels(y)
            }
            ans
        },
        panel = function (x, y, ...) {
            x <- x[drop = TRUE]
            y <- y[drop = TRUE]
            panel.dotplot(x, y, ...)
        })


Deepayan



From lauraholt_983 at hotmail.com  Tue Aug 17 04:59:52 2004
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Mon, 16 Aug 2004 21:59:52 -0500
Subject: [R] ticks on the Time Axis
Message-ID: <BAY12-F31wxKoWM9XCW0009c4a0@hotmail.com>

Dear R People:

I have the following montly time series
>ya.ts
      Jan  Feb  Mar  Apr  May  Jun  Jul  Aug  Sep  Oct  Nov  Dec
2001  3.7 -0.8  0.3 -1.5 -0.2 -0.4  2.5 -1.0 -1.2 -1.2  0.4 -0.5
2002  0.5  0.0 -0.8 -1.0  0.6  0.8 -0.5 -2.4  1.3  1.4 -0.1  0.5
>plot(ya.ts)

When the plot is constructed, the ticks on the horizontal (time) axis are 
2001.0, 2001.5, and so on.

Is there a way to set up ticks such as J,F,M,A,M......by months, please?

Thank you!
Sincerely,
Laura Holt
mailto: lauraholt_983 at hotmail.com



From jfbrennan at rogers.com  Tue Aug 17 05:26:31 2004
From: jfbrennan at rogers.com (Jim Brennan)
Date: Mon, 16 Aug 2004 23:26:31 -0400
Subject: [R] ticks on the Time Axis
References: <BAY12-F31wxKoWM9XCW0009c4a0@hotmail.com>
Message-ID: <00a101c4840a$02e02580$3b8ac445@slnt.phub.net.cable.rogers.com>

? par
? axis
basically you turn off the default labels and give your vector of character
labels
----- Original Message -----
From: "Laura Holt" <lauraholt_983 at hotmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, August 16, 2004 10:59 PM
Subject: [R] ticks on the Time Axis


> Dear R People:
>
> I have the following montly time series
> >ya.ts
>       Jan  Feb  Mar  Apr  May  Jun  Jul  Aug  Sep  Oct  Nov  Dec
> 2001  3.7 -0.8  0.3 -1.5 -0.2 -0.4  2.5 -1.0 -1.2 -1.2  0.4 -0.5
> 2002  0.5  0.0 -0.8 -1.0  0.6  0.8 -0.5 -2.4  1.3  1.4 -0.1  0.5
> >plot(ya.ts)
>
> When the plot is constructed, the ticks on the horizontal (time) axis are
> 2001.0, 2001.5, and so on.
>
> Is there a way to set up ticks such as J,F,M,A,M......by months, please?
>
> Thank you!
> Sincerely,
> Laura Holt
> mailto: lauraholt_983 at hotmail.com
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From lauraholt_983 at hotmail.com  Tue Aug 17 06:47:46 2004
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Mon, 16 Aug 2004 23:47:46 -0500
Subject: [R] Months on the horizontal axis
Message-ID: <BAY12-F42lT3XIhFTN60009cdfc@hotmail.com>

Dear R People:

Just in case anyone is interested, here is a particular solution to the 
months on the horizontal axis question:

>mon1
[1] "J" "F" "M" "A" "M" "J" "J" "A" "S" "O" "N" "D"
>mon2 <- rep(mon1,2)
>mon2
[1] "J" "F" "M" "A" "M" "J" "J" "A" "S" "O" "N" "D" "J" "F" "M" "A" "M" "J" 
"J"
[20] "A" "S" "O" "N" "D"
>ya.ts
      Jan  Feb  Mar  Apr  May  Jun  Jul  Aug  Sep  Oct  Nov  Dec
2001  3.7 -0.8  0.3 -1.5 -0.2 -0.4  2.5 -1.0 -1.2 -1.2  0.4 -0.5
2002  0.5  0.0 -0.8 -1.0  0.6  0.8 -0.5 -2.4  1.3  1.4 -0.1  0.5
>mon1
[1] "J" "F" "M" "A" "M" "J" "J" "A" "S" "O" "N" "D"
>mon2 <- rep(mon1,2)
>mon2
[1] "J" "F" "M" "A" "M" "J" "J" "A" "S" "O" "N" "D" "J" "F" "M" "A" "M" "J" 
"J"
[20] "A" "S" "O" "N" "D"
>#I looked at the original plot
>plot(ya.ts)
>#Now I start cleaning things up
>plot(ya.ts,axes=F,ylab=" ",xlab="2001 - 2002",ylim=c(-3,4))
md1 <- ((0:23)/12)+2001
>md1
[1] 2001.000 2001.083 2001.167 2001.250 2001.333 2001.417 2001.500 2001.583
[9] 2001.667 2001.750 2001.833 2001.917 2002.000 2002.083 2002.167 2002.250
[17] 2002.333 2002.417 2002.500 2002.583 2002.667 2002.750 2002.833 2002.917
>axis(1,at=md1,labels=mon2,pos=-3)
>axis(2,pos=2001)
>title("Months on the Horizontal Axis")
>

Thanks for listening and thanks for the many kind responses.

Sincerely,
Laura
mailto: lauraholt_983 at hotmail.com



From ggrothendieck at myway.com  Tue Aug 17 06:53:54 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 17 Aug 2004 04:53:54 +0000 (UTC)
Subject: [R] ticks on the Time Axis
References: <BAY12-F31wxKoWM9XCW0009c4a0@hotmail.com>
Message-ID: <loom.20040817T062714-403@post.gmane.org>

Laura Holt <lauraholt_983 <at> hotmail.com> writes:

: 
: Dear R People:
: 
: I have the following montly time series
: >ya.ts
:       Jan  Feb  Mar  Apr  May  Jun  Jul  Aug  Sep  Oct  Nov  Dec
: 2001  3.7 -0.8  0.3 -1.5 -0.2 -0.4  2.5 -1.0 -1.2 -1.2  0.4 -0.5
: 2002  0.5  0.0 -0.8 -1.0  0.6  0.8 -0.5 -2.4  1.3  1.4 -0.1  0.5
: >plot(ya.ts)
: 
: When the plot is constructed, the ticks on the horizontal (time) axis are 
: 2001.0, 2001.5, and so on.
: 
: Is there a way to set up ticks such as J,F,M,A,M......by months, please?

This is not quite what you are asking for since the months are in 
numbers (Jan = 01) and it may print only every third month if its
too cramped but its easy (no messing with axes) and it may be good 
enough.  It uses the fact that the chron package will plot numeric 
months and years.

   require(chron)
   ya.start.date <- chron("1/1/1")
   ya.dates <- seq(ya.start.date, length = length(ya.ts), by = "month")
   plot(ya.dates, ya.ts)

If you want to automatically construct ya.start.date above from ya.ts,
use the fact that year-month-day character format is accepted by Date to 
get the start date as a Date, convert that to chron and then use that in 
place of the ya.start.date <- line above:

   ya.start.date <- chron(as.Date(paste(c(start(ya.ts),1), collapse="-")))



From ggrothendieck at myway.com  Tue Aug 17 07:16:57 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 17 Aug 2004 05:16:57 +0000 (UTC)
Subject: [R] turning off automatic coersion from list to matrix
References: <9410EC84C0872141B27A2726613EF45D10976B6B@psmrdcex01.psm.pin.safeco.com>
Message-ID: <loom.20040817T070216-532@post.gmane.org>

HENRIKSON, JEFFREY <JEFHEN <at> SAFECO.com> writes:

: 
: Hello,
: 
: I am having trouble understanding how R is coercing between matrices and
: lists in the following example.  I have an aggregate behavior I like:
: 
: aggregate(a[,"num"],by=list(product=a[,"product"],region=a[,"region"]),
: sum)
: 
: Now in reality I have more columns than just product and region, and
: need to pick different combinations.  So I want to abstract this into a
: function.  Example use:
: 
: myagg(a,c("product","region"))
: 
: But I am having trouble because by= requires a list and apply and sapply
: seem to cast things back to the "matrix" type automatically.  Can I turn
: this off?  Eg:
: 
: data.class(sapply(c("product","region"),function(i) {a[,i]}))
: [1] "matrix"
: 
: whereas this would be acceptable to by=
: 
: data.class(list(product=a[,"product"],region=a[,"region"]))
: [1] "list"


I think you are looking for lapply but, actually, its even easier.

Note that a data frame is *already* a list thus you can do things
like this:

data(warpbreaks)
aggregate(warpbreaks[,"breaks",drop=F], warpbreaks[,c("wool","tension")], mean)

or you can use numeric indices like this:

aggregate(warpbreaks[,1,drop=FALSE], warpbreaks[,2:3], mean)



From ajayshah at mayin.org  Tue Aug 17 09:41:52 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Tue, 17 Aug 2004 13:11:52 +0530
Subject: [R] An entire data frame which is a time-series?
Message-ID: <20040817074152.GA19099@igidr.ac.in>

I have :

raw <- read.table("monthly.text", skip=3, sep="|",
                col.names=c("junk", "junk2",
                  "wpi", "g.wpi", "wpi.primary", "g.wpi.primary",
                  "wpi.fuel", "g.wpi.fuel", "wpi.manuf", "g.wpi.manuf",
                  "cpi.iw", "g.cpi.iw", "cpi.unme", "g.cpi.unme",
                  "cpi.al", "g.cpi.al", "cpi.rl", "g.cpi.rl"))

Now I can do things like:

  g.wpi = ts(raw$g.wpi, frequency=12, start=c(1994,7))

and it works fine. One by one, I can make time-series objects.

Is there a way to tell R that the entire data frame is a set of
time-series, so that I don't have to go column by column and make a
new ts() out of each?

I tried:

  M = ts(raw, frequency=12, start=c(1994,7))
  ts.plot(M[,"wpi"], M[,"wpi.manuf"])

but this gives nonsense results. Also, syntax like M$wpi is a lot
nicer than M[,"wpi"]. Any ideas about what might work?


An unrelated suggestion: I found the documentation of ts() to be quite
daunting. I have been around time-series and computer programming for
decades. But it took me a while to handle the basics : to read in a
file, to make time-series vectors, to run ARMA models. This stuff
ought to be easier to learn. I tried to write an ARMA example, and put
it up on the web, which would've been a godsend to me if I had found
it earlier (http://www.mayin.org/~ajayshah/KB/R/tsa.html).

I believe that the R documentation framework could do well by always
having a 2000 word conceptual introduction + little tutorial on each
package, instead of straight jumping into man pages on each function
(which is the only documentation that we have presently).

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From e.pebesma at geog.uu.nl  Tue Aug 17 11:18:34 2004
From: e.pebesma at geog.uu.nl (Edzer J. Pebesma)
Date: Tue, 17 Aug 2004 11:18:34 +0200
Subject: [R] Cross-variograms
Message-ID: <4121CD6A.9040807@geog.uu.nl>

Jacques, provided that X and Y are colocated (i.e., have
exactly the same observation locations), you get the
cross variogram right; the definition of this cross
variogram is however:

gamma(h)= E[(X(s)-X(s+h))*(Y(s)-Y(s+h))]

also, where you select:

cv <- v$gamma[1:14]

you may be better off using the more general

v$gamma[v$id == "X.Y"]

Best regards,
--
Edzer



From rn001 at cebas.csic.es  Tue Aug 17 12:48:24 2004
From: rn001 at cebas.csic.es (javier garcia - CEBAS)
Date: Tue, 17 Aug 2004 12:48:24 +0200
Subject: [R] strptime() bug? And additional problem in package "tseries"
Message-ID: <20040817104651.07FF8A7AC4@cebas.csic.es>

Hi all, I've got some problems with irts objects, one of which could be a bug:

1) Read a table with several columns from Postgres and the first column is 
Timestamp with timezone (this is OK). An extract is:

raincida$ts:
 [2039] "25/03/2000 22:00:00 UTC" "25/03/2000 23:00:00 UTC"
 [2041] "26/03/2000 00:00:00 UTC" "26/03/2000 01:00:00 UTC"
 [2043] "26/03/2000 02:00:00 UTC" "26/03/2000 03:00:00 UTC"
 [2045] "26/03/2000 04:00:00 UTC" "26/03/2000 05:00:00 UTC"

2) Try to extract time from this column of the dataframe (bug?)

> lluvia.strptime <- strptime(raincida$ts, format="%d/%m/%Y %H:%M:%S")

# An extract is:

 [2038] "2000-03-25 21:00:00" "2000-03-25 22:00:00" "2000-03-25 23:00:00"
 [2041] "2000-03-26 00:00:00" "2000-03-26 01:00:00" "2000-03-26 03:00:00"
 [2044] "2000-03-26 03:00:00" "2000-03-26 04:00:00" "2000-03-26 05:00:00"

# note that element [2043] is wrong. This happens several times in 
# the dataset. This will produce an eventual error because of omitted
# and duplicated values 

3) The additional problem is related with function time() for irts objects.
I try to make an irts from several columns of the table read:

> rain.irts <- 
irts(as.POSIXct(lluvia.strptime,tz="GMT"),cbind(raincida[[8]],raincida[[9]],raincida[[10]],raincida[[11]],raincida[[12]],raincida[[13]],raincida[[14]])) 

# this step doesn't seem to have any further problem. An extract is:

2000-03-25 22:00:00 GMT 0.275 0 0.07875 0.2 0 0.025 23.65
2000-03-25 23:00:00 GMT 0.275 0 0.07875 0.2 0 0.025 23.65
2000-03-26 00:00:00 GMT 0 0 0.001667 0.008333 0 0 0.5322
2000-03-26 01:00:00 GMT 0 0 0.001667 0.008333 0 0 0.5322
2000-03-26 03:00:00 GMT 0 0 0.001667 0.008333 0 0 0.5322
2000-03-26 03:00:00 GMT 0 0 0.001667 0.008333 0 0 0.5322
2000-03-26 04:00:00 GMT 0 0 0.001667 0.008333 0 0 0.5322

# But I try to extract the time part:

>time(rain.irts, tz='GMT')

# An extract is:

 [2039] "2000-03-25 23:00:00 CET"  "2000-03-26 00:00:00 CET"
 [2041] "2000-03-26 01:00:00 CET"  "2000-03-26 03:00:00 CEST"
 [2043] "2000-03-26 05:00:00 CEST" "2000-03-26 05:00:00 CEST"

# There isn't a way for this time to be shown as 'GMT'? I guess sometimes it 
is shown as 'CET' and other times as 'CEST' depending of the lag between the 
locale and gmt (utc) times. But for me this is an additional problem as the 
output shows one or two hours more that UTC time.

Thanks all, and best regards,

Javier G.



From ripley at stats.ox.ac.uk  Tue Aug 17 13:04:36 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 17 Aug 2004 12:04:36 +0100 (BST)
Subject: NO bug in Re: [R] strptime() bug? And additional problem in package
	"tseries"
In-Reply-To: <20040817104651.07FF8A7AC4@cebas.csic.es>
Message-ID: <Pine.LNX.4.44.0408171153450.13374-100000@gannet.stats>

There is no bug in R here.  There was a change to DST in Spain at 2am on
2000-03-26, and they are *printed* as times in your locale, as documented.

Please read the posting guide and FAQ about what is a bug.
Also, please try not to confuse an object and its printed representation.

On Tue, 17 Aug 2004, javier garcia - CEBAS wrote:

> Hi all, I've got some problems with irts objects, one of which could be a bug:
> 
> 1) Read a table with several columns from Postgres and the first column is 
> Timestamp with timezone (this is OK). An extract is:
> 
> raincida$ts:
>  [2039] "25/03/2000 22:00:00 UTC" "25/03/2000 23:00:00 UTC"
>  [2041] "26/03/2000 00:00:00 UTC" "26/03/2000 01:00:00 UTC"
>  [2043] "26/03/2000 02:00:00 UTC" "26/03/2000 03:00:00 UTC"
>  [2045] "26/03/2000 04:00:00 UTC" "26/03/2000 05:00:00 UTC"
> 
> 2) Try to extract time from this column of the dataframe (bug?)
> 
> > lluvia.strptime <- strptime(raincida$ts, format="%d/%m/%Y %H:%M:%S")
> 
> # An extract is:

NO!  That is an extract of *printing* lluvia.strptime, which will give you
the times in your current time zone, as documented.

>  [2038] "2000-03-25 21:00:00" "2000-03-25 22:00:00" "2000-03-25 23:00:00"
>  [2041] "2000-03-26 00:00:00" "2000-03-26 01:00:00" "2000-03-26 03:00:00"
>  [2044] "2000-03-26 03:00:00" "2000-03-26 04:00:00" "2000-03-26 05:00:00"
> 
> # note that element [2043] is wrong. This happens several times in 
> # the dataset. This will produce an eventual error because of omitted
> # and duplicated values 

I think you want to use as.POSIXct(lluvia.strptime, tz="GMT") to get 
what you may have intended.

....

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From merser at tiscali.dk  Tue Aug 17 14:14:18 2004
From: merser at tiscali.dk (merser@tiscali.dk)
Date: Tue, 17 Aug 2004 14:14:18 +0200
Subject: [R] table and getting  rownames
Message-ID: <411A08E700000478@cpfe3.be.tisc.dk>

hi there
say that i have this table
>x<-table(adoc, oarb)
>x
               oarb
                      0   1
adoc
    ab                1   0
    am                5   1
    ba               14   1
    cc              271   3
    ch               87   2
    dz              362   6
    fl                7   0
    fs               84   2

is there an easy way to get the row names or row numbers of rows with
oarb==0
i.e. (ab, fl) or (1, 7)

regards soren



From ripley at stats.ox.ac.uk  Tue Aug 17 14:39:48 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 17 Aug 2004 13:39:48 +0100 (BST)
Subject: [R] table and getting  rownames
In-Reply-To: <411A08E700000478@cpfe3.be.tisc.dk>
Message-ID: <Pine.LNX.4.44.0408171337180.19009-100000@gannet.stats>

On Tue, 17 Aug 2004 merser at tiscali.dk wrote:

> say that i have this table
> >x<-table(adoc, oarb)
> >x
>                oarb
>                       0   1
> adoc
>     ab                1   0
>     am                5   1
>     ba               14   1
>     cc              271   3
>     ch               87   2
>     dz              362   6
>     fl                7   0
>     fs               84   2
> 
> is there an easy way to get the row names or row numbers of rows with
> oarb==0

That seems to be with *entry* zero, not oarb = 0?

> i.e. (ab, fl) or (1, 7)

rows(x)[x==0]
rownames(x)[rows(x)[x==0]]

will do what I think you meant to ask.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Tue Aug 17 14:35:51 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 17 Aug 2004 14:35:51 +0200
Subject: [R] table and getting  rownames
In-Reply-To: <411A08E700000478@cpfe3.be.tisc.dk>
References: <411A08E700000478@cpfe3.be.tisc.dk>
Message-ID: <x2oelat00o.fsf@biostat.ku.dk>

merser at tiscali.dk writes:

> hi there
> say that i have this table
> >x<-table(adoc, oarb)
> >x
>                oarb
>                       0   1
> adoc
>     ab                1   0
>     am                5   1
>     ba               14   1
>     cc              271   3
>     ch               87   2
>     dz              362   6
>     fl                7   0
>     fs               84   2
> 
> is there an easy way to get the row names or row numbers of rows with
> oarb==0
> i.e. (ab, fl) or (1, 7)

Something like

which(x[,"1"]==0)
rownames(x)[x[,"1"]==0]

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From david_foreman at doctors.org.uk  Tue Aug 17 13:08:07 2004
From: david_foreman at doctors.org.uk (david_foreman@doctors.org.uk)
Date: Tue, 17 Aug 2004 13:08:07 (GMT)
Subject: [R] Re: Thanks Frank, setting graph parameters,
	and why social scientists don't use R
Message-ID: <1092748088_19480@drn10msi01>

First, many thanks to Frank Harrell for once again helping me out.  This actually relates to the next point, which is my contribution to the 'why don't social scientists use R' discussion.  I am a hybrid social scientist(child psychiatrist) who trained on SPSS.  Many of my difficulties in coming to terms with R have been to do with trying to apply the logic underlying SPSS, with dire results.  You do not want to know how long I spent looking for a 'recode' command in R, to change factor names and classes.....

I think the solution is to combine a graphical interface that encourages command line use (such as Rcommander) with the analyse(this) paradigm suggested, but also explaining how one can a) display the code on a separate window ('page' is only an obvious command once you know it), and b) how one can then save one's modification, make it generally available, and not overwrite the unmodified version (again, thanks, Frank).  Finally, one would need to change the emphasis in basic statistical teaching from 'the right test' to 'the right model'.  That should get people used to R's logic.

If a rabbit starts to use R, s/he is likely to head for the help files associated with each function, which can assume that the reader can make sense of gnomic utterances like "Omit 'var' to impute all variables, creating new variables in 'search' position 'where'".  I still don't know what that one means (as I don't understand search positions, or why they're important).  This can be very offputting, and could lead the rabbit to return to familiar SPSS territory.

Finally, friendlier error messages would also help. It took me 3 days, and opening every function I could, to work out that '...cannot find function xxx.data.frame...' meant that MICE was unable to make a polychotomous logistic imputation model converge for the variable immediately preceding it.


I am now off to the help files and FAQs to find out how to change graph parameters, as the plot.mids function in MICE a) doesn't allow one to select a subset of variables, and b) tells me that the graph it wants to produce on the whole of my 26 variable dataset is too big to fit on the (windows) plotting device.  Unless anyone wants to tell me how/where? (which of course is why, in the end, R is EASIER to use than SPSS)


---------- Original Message ----------------------------------
From: r-help-request at stat.math.ethz.ch
Reply-To: r-help at stat.math.ethz.ch
Date:  Sun, 15 Aug 2004 12:10:22 +0200

>Send R-help mailing list submissions to
>	r-help at stat.math.ethz.ch
>
>To subscribe or unsubscribe via the World Wide Web, visit
>	https://stat.ethz.ch/mailman/listinfo/r-help
>or, via email, send a message with subject or body 'help' to
>	r-help-request at stat.math.ethz.ch
>
>You can reach the person managing the list at
>	r-help-owner at stat.math.ethz.ch
>
>When replying, please edit your Subject line so it is more specific
>than "Re: Contents of R-help digest..."
>
>
>Today's Topics:
>
>   1. Re: numerical accuracy, dumb question (Brian Gough)
>   2. RE: numerical accuracy, dumb question (Tony Plate)
>   3. RE: numerical accuracy, dumb question (Dan Bolser)
>   4. Re: extracting datasets from aregImpute objects
>      (Frank E Harrell Jr)
>   5. RE: numerical accuracy, dumb question (Marc Schwartz)
>   6. RE: numerical accuracy, dumb question (Marc Schwartz)
>   7. RE: numerical accuracy, dumb question (Prof Brian Ripley)
>   8. ROracle connection problem (xianghe yan)
>   9. association rules in R (Christoph Lehmann)
>  10. R Cookbook (ivo_welch-rstat8783 at mailblocks.com)
>  11. RE: numerical accuracy, dumb question (Marc Schwartz)
>  12. How to display the equation of ECDF (Yair Benita)
>  13. Re: association rules in R (Spencer Graves)
>  14. Re: How to display the equation of ECDF (Rolf Turner)
>  15. Re: How to display the equation of ECDF (Spencer Graves)
>  16. how to draw two graphs in one graph window (Chuanjun Zhang)
>  17. Rserve needs (but cannot find) libR.a (or maybe it's .so)
>      (Paul Shannon)
>  18. Re: Rserve needs (but cannot find) libR.a (or maybe it's .so)
>      (A.J. Rossini)
>  19. calibration/validation sets (Peyuco Porras Porras .)
>  20. RE: calibration/validation sets (Austin, Matt)
>  21. Re: calibration/validation sets (Kevin Wang)
>  22. RE: calibration/validation sets (Liaw, Andy)
>  23. Dirichlet-Multinomial (Z P)
>  24. Re: how to draw two graphs in one graph window
>      (Adaikalavan Ramasamy)
>  25. index and by groups statement (Robert Waters)
>  26. Re: index and by groups statement (Adaikalavan Ramasamy)
>
>
>----------------------------------------------------------------------
>
>Message: 1
>Date: 14 Aug 2004 10:46:31 +0100
>From: Brian Gough <bjg at network-theory.co.uk>
>Subject: Re: [R] numerical accuracy, dumb question
>To: Dan Bolser <dmb at mrc-dunn.cam.ac.uk>
>Cc: r-help at stat.math.ethz.ch
>Message-ID: <87llgi6oi0.fsf at network-theory.co.uk>
>
>Dan Bolser <dmb at mrc-dunn.cam.ac.uk> writes:
>
>> I store an id as a big number, could this be a problem?
>
>If there are ids with significant leading zeros, or too big to be
>represented accurately (>2^53)--you won't get any warning about it,
>just silent truncation.  So best practice would be to keep them as
>character strings, using colClasses= in read.table().
>
>--
>Brian Gough
>
>Network Theory Ltd,
>Publishing the R Reference Manuals --- http://www.network-theory.co.uk/R/base/
>
>
>
>------------------------------
>
>Message: 2
>Date: Sat, 14 Aug 2004 07:42:31 -0600
>From: Tony Plate <tplate at blackmesacapital.com>
>Subject: RE: [R] numerical accuracy, dumb question
>To: MSchwartz at MedAnalytics.com, Dan Bolser <dmb at mrc-dunn.cam.ac.uk>
>Cc: R-Help <r-help at stat.math.ethz.ch>
>Message-ID:
>	<6.1.0.6.2.20040814073336.063d4778 at mailhost.blackmesacapital.com>
>Content-Type: text/plain; charset="us-ascii"; format=flowed
>
>At Friday 08:41 PM 8/13/2004, Marc Schwartz wrote:
>>Part of that decision may depend upon how big the dataset is and what is
>>intended to be done with the ID's:
>>
>> > object.size(1011001001001)
>>[1] 36
>>
>> > object.size("1011001001001")
>>[1] 52
>>
>> > object.size(factor("1011001001001"))
>>[1] 244
>>
>>
>>They will by default, as Andy indicates, be read and stored as doubles.
>>They are too large for integers, at least on my system:
>>
>> > .Machine$integer.max
>>[1] 2147483647
>>
>>Converting to a character might make sense, with only a minimal memory
>>penalty. However, using a factor results in a notable memory penalty, if
>>the attributes of a factor are not needed.
>
>That depends on how long the vectors are.  The memory overhead for factors
>is per vector, with only 4 bytes used for each additional element (if the
>level already appears).  The memory overhead for character data is per
>element -- there is no amortization for repeated values.
>
> > object.size(factor("1011001001001"))
>[1] 244
> >
>object.size(factor(rep(c("1011001001001","111001001001","001001001001","011001001001"),1)))
>[1] 308
> > # bytes per element in factor, for length 4:
> >
>object.size(factor(rep(c("1011001001001","111001001001","001001001001","011001001001"),1)))/4
>[1] 77
> > # bytes per element in factor, for length 1000:
> >
>object.size(factor(rep(c("1011001001001","111001001001","001001001001","011001001001"),250)))/1000
>[1] 4.292
> > # bytes per element in character data, for length 1000:
> >
>object.size(as.character(factor(rep(c("1011001001001","111001001001","001001001001","011001001001"),250))))/1000
>[1] 20.028
> >
>
>So, for long vectors with relatively few different values, storage as
>factors is far more memory efficient (this is because the character data is
>stored only once per level, and each element is stored as a 4-byte
>integer).  (The above was done on Windows 2000).
>
>-- Tony Plate
>
>>If any mathematical operations are to be performed with the ID's then
>>leaving them as doubles makes most sense.
>>
>>Dan, more information on the numerical characteristics of your system
>>can be found by using:
>>
>>.Machine
>>
>>See ?.Machine and ?object.size for more information.
>>
>>HTH,
>>
>>Marc Schwartz
>>
>>
>>On Fri, 2004-08-13 at 21:02, Liaw, Andy wrote:
>> > If I'm not mistaken, numerics are read in as doubles, so that shouldn't
>> be a
>> > problem.  However, I'd try using factor or character.
>> >
>> > Andy
>> >
>> > > From: Dan Bolser
>> > >
>> > > I store an id as a big number, could this be a problem?
>> > >
>> > > Should I convert to at string when I use read.table(...
>> > >
>> > > example id's
>> > >
>> > > 1001001001001
>> > > 1001001001002
>> > > ...
>> > > 1002001002005
>> > >
>> > >
>> > > Bigest is probably
>> > >
>> > > 1011001001001
>> > >
>> > > Ta,
>> > > Dan.
>> > >
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>------------------------------
>
>Message: 3
>Date: Sat, 14 Aug 2004 15:04:44 +0100 (BST)
>From: Dan Bolser <dmb at mrc-dunn.cam.ac.uk>
>Subject: RE: [R] numerical accuracy, dumb question
>Cc: R-Help <r-help at stat.math.ethz.ch>
>Message-ID:
>	<Pine.LNX.4.21.0408141503320.14992-100000 at mail.mrc-dunn.cam.ac.uk>
>Content-Type: TEXT/PLAIN; charset=US-ASCII
>
>
>Thanks all for the expert advice and guidance.
>
>
>
>------------------------------
>
>Message: 4
>Date: Sat, 14 Aug 2004 09:10:25 -0500
>From: Frank E Harrell Jr <f.harrell at vanderbilt.edu>
>Subject: [R] Re: extracting datasets from aregImpute objects
>To: R-help at stat.math.ethz.ch
>Message-ID: <411E1D51.9090602 at vanderbilt.edu>
>Content-Type: text/plain; charset=us-ascii; format=flowed
>
>From: <david_foreman at doctors.org.uk>
>Subject: [R] Re: extracting datasets from aregImpute objects
>To: <r-help at stat.math.ethz.ch>
>Message-ID: <1092391719_117440 at drn10msi01>
>Content-Type: text/plain; charset="us-ascii"
>
>I've tried doing this by specifying x=TRUE, which provides me with a
>single imputation, that has been useful.  However, the help file
>possibly suggests that I should get a flat-file matrix of n.impute
>imputations, presumably with indexing.  I'm a bit stuck using
>alternatives to aregImpute, as neither MICE nor Amelia seem to like my
>dataset, and Frank Harrell no longer recommends Transcan for multiple
>imputations.
>
>-----
>
>David,
>
>aregImpute produces a list containing the multiple imputations:
>
>w <- aregImpute(. . .)
>w$imputed$blood.pressure   # gets m by k matrix
>  # m = number of subjects with blood pressure missing,
>  # k = number of multiple imputations
>
>To get a completed dataset (but for only one draw of the k multiple
>imputations) see how fit.mult.impute does it.  I have just added the
>following example to the help file for aregImpute.
>
>set.seed(23)
>x <- runif(200)
>y <- x + runif(200, -.05, .05)
>y[1:20] <- NA
>d <- data.frame(x,y)
>f <- aregImpute(~ x + y, n.impute=10, match='closest', data=d)
># Here is how to create a completed dataset for imputation
># number 3 as fit.mult.impute would do automatically.  In this
># degenerate case changing 3 to 1-2,4-10 will not alter the results.
>completed <- d
>imputed <- impute.transcan(f, imputation=3, data=d, list.out=TRUE,
>                            pr=FALSE, check=FALSE)
>completed[names(imputed)] <- imputed
>completed  # 200 by 2 data frame
>
>--
>Frank E Harrell Jr   Professor and Chair           School of Medicine
>                      Department of Biostatistics   Vanderbilt University
>
>
>
>------------------------------
>
>Message: 5
>Date: Sat, 14 Aug 2004 12:01:59 -0500
>From: Marc Schwartz <MSchwartz at MedAnalytics.com>
>Subject: RE: [R] numerical accuracy, dumb question
>To: Tony Plate <tplate at blackmesacapital.com>
>Cc: R-Help <r-help at stat.math.ethz.ch>
>Message-ID: <1092502918.6357.277.camel at localhost.localdomain>
>Content-Type: text/plain
>
>On Sat, 2004-08-14 at 08:42, Tony Plate wrote:
>> At Friday 08:41 PM 8/13/2004, Marc Schwartz wrote:
>> >Part of that decision may depend upon how big the dataset is and what is
>> >intended to be done with the ID's:
>> >
>> > > object.size(1011001001001)
>> >[1] 36
>> >
>> > > object.size("1011001001001")
>> >[1] 52
>> >
>> > > object.size(factor("1011001001001"))
>> >[1] 244
>> >
>> >
>> >They will by default, as Andy indicates, be read and stored as doubles.
>> >They are too large for integers, at least on my system:
>> >
>> > > .Machine$integer.max
>> >[1] 2147483647
>> >
>> >Converting to a character might make sense, with only a minimal memory
>> >penalty. However, using a factor results in a notable memory penalty, if
>> >the attributes of a factor are not needed.
>>
>> That depends on how long the vectors are.  The memory overhead for factors
>> is per vector, with only 4 bytes used for each additional element (if the
>> level already appears).  The memory overhead for character data is per
>> element -- there is no amortization for repeated values.
>>
>>  > object.size(factor("1011001001001"))
>> [1] 244
>>  >
>> object.size(factor(rep(c("1011001001001","111001001001","001001001001","011001001001"),1)))
>> [1] 308
>>  > # bytes per element in factor, for length 4:
>>  >
>> object.size(factor(rep(c("1011001001001","111001001001","001001001001","011001001001"),1)))/4
>> [1] 77
>>  > # bytes per element in factor, for length 1000:
>>  >
>> object.size(factor(rep(c("1011001001001","111001001001","001001001001","011001001001"),250)))/1000
>> [1] 4.292
>>  > # bytes per element in character data, for length 1000:
>>  >
>> object.size(as.character(factor(rep(c("1011001001001","111001001001","001001001001","011001001001"),250))))/1000
>> [1] 20.028
>>  >
>>
>> So, for long vectors with relatively few different values, storage as
>> factors is far more memory efficient (this is because the character data is
>> stored only once per level, and each element is stored as a 4-byte
>> integer).  (The above was done on Windows 2000).
>>
>> -- Tony Plate
>
>
>Good point Tony. I was making the, perhaps incorrect assumption, that
>the ID's were unique or relatively so. However, as it turns out, even
>that assumption is relevant only to a certain extent with respect to how
>much memory is required.
>
>What is interesting (and presumably I need to do some more reading on
>how R stores objects internally) is that the incremental amount of
>memory is not consistent on a per element basis for a given object,
>though there is a pattern. It is also dependent upon the size of the new
>elements to be added, as I note at the bottom.
>
>This all of course presumes that object.size() is giving a reasonable
>approximation of the amount of memory actually allocated to an object,
>for which the notes in ?object.size raise at least some doubt. This is a
>critical assumption for the data below, which is on FC2 on a P4.
>
>For example:
>
>> object.size("a")
>[1] 44
>
>> object.size(letters)
>[1] 340
>
>In the second case, as Tony has noted, the size of letters (a character
>vector) is not 26 * 44.
>
>Now note:
>
>> object.size(c("a", "b"))
>[1] 52
>> object.size(c("a", "b", "c"))
>[1] 68
>> object.size(c("a", "b", "c", "d"))
>[1] 76
>> object.size(c("a", "b", "c", "d", "e"))
>[1] 92
>
>The incremental sizes are a sequence of 8 and 16.
>
>Now for a factor:
>
>> object.size(factor("a"))
>[1] 236
>> object.size(factor(c("a", "b")))
>[1] 244
>> object.size(factor(c("a", "b", "c")))
>[1] 268
>> object.size(factor(c("a", "b", "c", "d")))
>[1] 276
>> object.size(factor(c("a", "b", "c", "d", "e")))
>[1] 300
>
>The incremental sizes are a sequence of 8 and 24.
>
>
>Using elements along the lines of Dan's:
>
>> object.size("1000000000000")
>[1] 52
>> object.size(c("1000000000000", "1000000000001"))
>[1] 68
>> object.size(c("1000000000000", "1000000000001", "1000000000002"))
>[1] 92
>> object.size(c("1000000000000", "1000000000001", "1000000000002",
>                "1000000000003"))
>[1] 108
>> object.size(c("1000000000000", "1000000000001", "1000000000002",
>                "1000000000003", "1000000000004"))
>[1] 132
>
>The sequence is 16 and 24.
>
>For factors:
>
>> object.size(factor("1000000000000")
>[1] 244
>> object.size(factor(c("1000000000000", "1000000000001")))
>[1] 260
>> object.size(factor(c("1000000000000", "1000000000001",
>                       "1000000000002")))
>[1] 292
>> object.size(factor(c("1000000000000", "1000000000001",
>                       "1000000000002", "1000000000003")))
>[1] 308
>> object.size(factor(c("1000000000000", "1000000000001",
>                       "1000000000002", "1000000000003",
>                       "1000000000004")))
>[1] 340
>
>The sequence is 24 and 32.
>
>
>So, the incremental size seems to alternate as elements are added.
>
>The behavior above would perhaps suggest that memory is allocated to
>objects to enable pairs of elements to be added. When the second element
>of the pair is added, only a minimal incremental amount of additional
>memory (and presumably time) is required.
>
>However, when I add a "third" element, there is additional memory
>required to store that new element because the object needs to be
>adjusted in a more fundamental way to handle this new element.
>
>There also appears to be some memory allocation "adjustment" at play
>here. Note:
>
>> object.size(factor("1000000000000"))
>[1] 244
>
>> object.size(factor("1000000000000", "a"))
>[1] 236
>
>In the second case, the amount of memory reported actually declines by 8
>bytes. This suggests (to some extent consistent with my thoughts above)
>that when the object is initially created, there is space for two new
>elements and that space is allocated based upon the size of the first
>element. When the second element is added, the space required is
>adjusted based upon the actual size of the second element.
>
>Again, all of the above presumes that object.size() is reporting correct
>information.
>
>Thanks,
>
>Marc
>
>
>
>------------------------------
>
>Message: 6
>Date: Sat, 14 Aug 2004 12:15:37 -0500
>From: Marc Schwartz <MSchwartz at MedAnalytics.com>
>Subject: RE: [R] numerical accuracy, dumb question
>To: Tony Plate <tplate at blackmesacapital.com>
>Cc: R-Help <r-help at stat.math.ethz.ch>
>Message-ID: <1092503737.6357.294.camel at localhost.localdomain>
>Content-Type: text/plain
>
>On Sat, 2004-08-14 at 12:01, Marc Schwartz wrote:
>
>> There also appears to be some memory allocation "adjustment" at play
>> here. Note:
>>
>> > object.size(factor("1000000000000"))
>> [1] 244
>>
>> > object.size(factor("1000000000000", "a"))
>> [1] 236
>
>
>Arggh.
>
>Negate that last comment. I had a typo in the second example. It should
>be:
>
>> object.size(factor(c("1000000000000", "a")))
>[1] 252
>
>which of course results in an increase in memory.
>
>Geez. Time for lunch.
>
>Marc
>
>
>
>------------------------------
>
>Message: 7
>Date: Sat, 14 Aug 2004 19:19:23 +0100 (BST)
>From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
>Subject: RE: [R] numerical accuracy, dumb question
>To: Marc Schwartz <MSchwartz at medanalytics.com>
>Cc: R-Help <r-help at stat.math.ethz.ch>, Tony Plate
>	<tplate at blackmesacapital.com>
>Message-ID: <Pine.LNX.4.44.0408141841480.12580-100000 at gannet.stats>
>Content-Type: TEXT/PLAIN; charset=US-ASCII
>
>On Sat, 14 Aug 2004, Marc Schwartz wrote:
>
>> > object.size("a")
>> [1] 44
>>
>> > object.size(letters)
>> [1] 340
>>
>> In the second case, as Tony has noted, the size of letters (a character
>> vector) is not 26 * 44.
>
>Of course not.  Both are character vectors, so have the overhead of any R
>object plus an allocation for pointers to the elements plus an amount for
>each element of the vector (see the end).
>
>These calculations differ on 32-bit and 64-bit machines.  For a 32-bit
>machine storage is in units of either 28 bytes (Ncells) or 8 bytes
>(Vcells) so single-letter characters are wasteful, viz
>
>> object.size("aaaaaaa")
>[1] 44
>
>That is 1 Ncell and 2 Vcells, 1 for the string (7 bytes plus terminator)
>and 1 for the pointer.
>
>Whereas
>
>> object.size(letters)
>[1] 340
>
>has 1 Ncell and 39 Vcells, 26 for the strings and 13 for the pointers
>(which fit two to a Vcell).
>
>Note that repeated character strings may share storage, so for example
>
>> object.size(rep("a", 26))
>[1] 340
>
>is wrong (140, I think).  And that makes comparisons with factors depend
>on exactly how they were created, for a character vector there probably is
>a lot of sharing.
>
>I have a feeling that these calculations are off for character vectors, as
>each element is a CHARSXP and so may have an Ncell not accounted for by
>object.size.  (`May' because of potential sharing.)  Would anyone who is
>sure like to confirm or deny this?
>
>It ought to be possible to improve the estimates for character vectors a
>bit as we can detect sharing amongst the elements.
>
>--
>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>University of Oxford,             Tel:  +44 1865 272861 (self)
>1 South Parks Road,                     +44 1865 272866 (PA)
>Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>
>
>------------------------------
>
>Message: 8
>Date: Sat, 14 Aug 2004 11:31:12 -0700 (PDT)
>From: xianghe yan <xyan0 at yahoo.com>
>Subject: [R] ROracle connection problem
>To: R-help at stat.math.ethz.ch
>Message-ID: <20040814183112.91970.qmail at web14203.mail.yahoo.com>
>Content-Type: text/plain; charset=us-ascii
>
>Hi,
>
>Could somebody help me to solve this following
>problem?  I just begin to learn how to connect my
>Oracle database with R.
>
>> library(DBI)
>
>> library(ROracle)
>Warning message:
>DLL attempted to change FPU control word from 8001f to
>9001f
>
>> ora=dbDriver("Oracle")
>Error in initialize(value, ...) : Invalid names for
>slots of class OraDriver: Id
>>
>
>
>My system is:
>
>Window XP,
>Oracle 9.2
>R1.9.0
>
>Thank you very much
>
>Xianghe
>
>Celera Genomics
>
>
>
>------------------------------
>
>Message: 9
>Date: Sat, 14 Aug 2004 20:34:21 +0200
>From: Christoph Lehmann <christoph.lehmann at gmx.ch>
>Subject: [R] association rules in R
>To: r-help at stat.math.ethz.ch
>Message-ID: <411E5B2D.9090101 at gmx.ch>
>Content-Type: text/plain; charset=us-ascii; format=flowed
>
>Hi
>
>I am interested in data mining problems. Has anybody ever programmed and
>worked with association rules in R?
>
>I am very grateful for any hint.
>
>Best regards
>
>Christoph
>
>
>
>------------------------------
>
>Message: 10
>Date: Sat, 14 Aug 2004 12:10:44 -0700
>From: <ivo_welch-rstat8783 at mailblocks.com>
>Subject: [R] R Cookbook
>To: r-help at stat.math.ethz.ch
>Message-ID: <200408141910.i7EJAk0S029859 at hypatia.math.ethz.ch>
>Content-Type: text/plain; charset="us-ascii"; format=flowed
>
>
>is anyone writing an R cookbook (ala the perl cookbook)?  this would be
>more for programming and graphics task than a statistics textbook.
>This seems more like a manufacturing defect than a random occurrance.
>
>if not, if I can fit it into my schedule, I may start one slowly on my
>website based on snippets I needed and/or found---maybe even for
>eventual publication.   obviously, I am not a great choice for
>authoring such a book, because I am not a Rexpert.  I really would
>rather just buy one from someone else than write one.
>
>please drop me a note, either if you
>  [a] know of someone who is writing such a book, or
>   [b] information that I would find useful, and for which I could
>obtain non-exclusive permission to include it in a published book (of
>course, with proper attribution to the real authors/inventors).
>
>regards,
>
>/iaw
>
>---
>ivo welch
>professor of finance and economics
>brown / nber / yale
>
>
>
>------------------------------
>
>Message: 11
>Date: Sat, 14 Aug 2004 15:53:11 -0500
>From: Marc Schwartz <MSchwartz at MedAnalytics.com>
>Subject: RE: [R] numerical accuracy, dumb question
>To: Prof Brian Ripley <ripley at stats.ox.ac.uk>
>Cc: R-Help <r-help at stat.math.ethz.ch>, Tony Plate
>	<tplate at blackmesacapital.com>
>Message-ID: <1092516791.11910.58.camel at localhost.localdomain>
>Content-Type: text/plain
>
>On Sat, 2004-08-14 at 13:19, Prof Brian Ripley wrote:
>> On Sat, 14 Aug 2004, Marc Schwartz wrote:
>>
>> > > object.size("a")
>> > [1] 44
>> >
>> > > object.size(letters)
>> > [1] 340
>> >
>> > In the second case, as Tony has noted, the size of letters (a character
>> > vector) is not 26 * 44.
>>
>> Of course not.  Both are character vectors, so have the overhead of any R
>> object plus an allocation for pointers to the elements plus an amount for
>> each element of the vector (see the end).
>>
>> These calculations differ on 32-bit and 64-bit machines.  For a 32-bit
>> machine storage is in units of either 28 bytes (Ncells) or 8 bytes
>> (Vcells) so single-letter characters are wasteful, viz
>>
>> > object.size("aaaaaaa")
>> [1] 44
>>
>> That is 1 Ncell and 2 Vcells, 1 for the string (7 bytes plus terminator)
>> and 1 for the pointer.
>>
>> Whereas
>>
>> > object.size(letters)
>> [1] 340
>>
>> has 1 Ncell and 39 Vcells, 26 for the strings and 13 for the pointers
>> (which fit two to a Vcell).
>>
>> Note that repeated character strings may share storage, so for example
>>
>> > object.size(rep("a", 26))
>> [1] 340
>>
>> is wrong (140, I think).  And that makes comparisons with factors depend
>> on exactly how they were created, for a character vector there probably is
>> a lot of sharing.
>>
>> I have a feeling that these calculations are off for character vectors, as
>> each element is a CHARSXP and so may have an Ncell not accounted for by
>> object.size.  (`May' because of potential sharing.)  Would anyone who is
>> sure like to confirm or deny this?
>>
>> It ought to be possible to improve the estimates for character vectors a
>> bit as we can detect sharing amongst the elements.
>
>Prof. Ripley,
>
>Thanks for the clarifications.
>
>I'll need to spend some time reading through R-exts.pdf and
>Rinternals.h.
>
>Regards,
>
>Marc
>
>
>
>------------------------------
>
>Message: 12
>Date: Sun, 15 Aug 2004 00:44:25 +0200
>From: Yair Benita <y.benita at wanadoo.nl>
>Subject: [R] How to display the equation of ECDF
>To: r-help at stat.math.ethz.ch
>Message-ID: <7E68530A-EE43-11D8-8015-003065C4E4B4 at wanadoo.nl>
>Content-Type: text/plain; charset=US-ASCII; format=flowed
>
>Hi,
>Using the ecdf (Empirical Cumulative Distribution Function) one can
>compute a plot. I was wondering if there is a way to get the equation
>used to draw the plot.
>
>thanks,
>Yair
>
>
>
>------------------------------
>
>Message: 13
>Date: Sat, 14 Aug 2004 19:03:29 -0400
>From: Spencer Graves <spencer.graves at pdf.com>
>Subject: Re: [R] association rules in R
>To: Christoph Lehmann <christoph.lehmann at gmx.ch>
>Cc: r-help at stat.math.ethz.ch
>Message-ID: <411E9A41.8040909 at pdf.com>
>Content-Type: text/plain; charset=ISO-8859-1; format=flowed
>
>      What kind of association rules?  Might %in% or is.element help?
>(PLEASE do read the posting guide!
>"http://www.R-project.org/posting-guide.html".  By following this guide,
>you may get the answer quicker than this list can respond, and if not,
>the exercise might help you formulate your question in a way that may
>more likely elicit useful replies.)
>
>      hope this helps.  spencer graves
>
>Christoph Lehmann wrote:
>
>> Hi
>>
>> I am interested in data mining problems. Has anybody ever programmed
>> and worked with association rules in R?
>>
>> I am very grateful for any hint.
>>
>> Best regards
>>
>> Christoph
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>
>
>
>------------------------------
>
>Message: 14
>Date: Sat, 14 Aug 2004 20:08:50 -0300 (ADT)
>From: Rolf Turner <rolf at math.unb.ca>
>Subject: Re: [R] How to display the equation of ECDF
>To: y.benita at wanadoo.nl
>Cc: r-help at stat.math.ethz.ch
>Message-ID: <200408142308.i7EN8oEe006841 at erdos.math.unb.ca>
>
>Yair Benita wrote:
>
>> Using the ecdf (Empirical Cumulative Distribution Function) one can
>> compute a plot. I was wondering if there is a way to get the equation
>> used to draw the plot.
>
>?ecdf
>
>i.e. RTFM!
>
>				cheers,
>
>					Rolf Turner
>					rolf at math.unb.ca
>
>
>
>------------------------------
>
>Message: 15
>Date: Sat, 14 Aug 2004 19:10:44 -0400
>From: Spencer Graves <spencer.graves at pdf.com>
>Subject: Re: [R] How to display the equation of ECDF
>To: Yair Benita <y.benita at wanadoo.nl>
>Cc: r-help at stat.math.ethz.ch
>Message-ID: <411E9BF4.90203 at pdf.com>
>Content-Type: text/plain; charset=ISO-8859-1; format=flowed
>
>      Did you try typing "ecdf" (without the parentheses identifying it
>as a function) at a prompt?  When I did that just now, I found that
>"ecdf" calls "approxfun", and I could get a function definition by
>typing that.
>
>      hope this helps.
>      spencer graves
>p.s.  PLEASE do read the posting guide!
>"http://www.R-project.org/posting-guide.html".  You might get the answer
>quicker from following this guide.  If not, the exercise might help you
>formulate your question in a way that might elicit more useful
>response(s).
>
>Yair Benita wrote:
>
>> Hi,
>> Using the ecdf (Empirical Cumulative Distribution Function) one can
>> compute a plot. I was wondering if there is a way to get the equation
>> used to draw the plot.
>>
>> thanks,
>> Yair
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>
>
>
>------------------------------
>
>Message: 16
>Date: Sat, 14 Aug 2004 16:14:04 -0700
>From: Chuanjun Zhang <chzhang at cs.ucr.edu>
>Subject: [R] how to draw two graphs in one graph window
>To: r-help at stat.math.ethz.ch
>Message-ID: <411E9CBC.90207 at cs.ucr.edu>
>Content-Type: text/plain; charset=us-ascii; format=flowed
>
>
>
>------------------------------
>
>Message: 17
>Date: 14 Aug 2004 16:33:01 -0700
>From: Paul Shannon <pshannon at systemsbiology.org>
>Subject: [R] Rserve needs (but cannot find) libR.a (or maybe it's .so)
>To: R-help at stat.math.ethz.ch
>Cc: simon.urbanek at math.uni-augsburg.de
>Message-ID: <EXCHANGEfvuaQ20qXW2000013d7 at exchange.systemsbiology.net>
>
>I have successfully installed Rserv (http://stats.math.uni-augsburg.de/Rserve)
>on Mac OS, but I have trouble on two different linux platforms.
>
>   R CMD INSTALL Rserve_0.3-10.tar.gz
>
>fails with this message
>
>  ** libs
>  gcc -g -O2 -I/usr/local/include -L/usr/local/lib  Rserv.c -o Rserve  \
>     -DDAEMON -O -I/usr/local/lib/R/include -Iinclude -I. -lR -L/usr/local/lib/R/bin -ldl -lcrypt
>  /usr/bin/ld: cannot find -lR
>  collect2: ld returned 1 exit status
>  make: *** [Rserve] Error 1
>  ERROR: compilation failed for package 'Rserve'
>
>Sure enough, when I look, I cannot find either libR.a or librR.so on either
>linus system.
>
>On the Mac, I -do- find libR.dylib.
>
>Can anyone help with this?
>
>Many thanks -
>
> - Paul Shannon
>   Institute for Systems Biology
>   Seattle
>
>
>
>------------------------------
>
>Message: 18
>Date: Sat, 14 Aug 2004 16:33:32 -0700
>From: rossini at blindglobe.net (A.J. Rossini)
>Subject: Re: [R] Rserve needs (but cannot find) libR.a (or maybe it's
>	.so)
>To: Paul Shannon <pshannon at systemsbiology.org>
>Cc: R-help at stat.math.ethz.ch, simon.urbanek at math.uni-augsburg.de
>Message-ID: <85zn4xs3ar.fsf at servant.blindglobe.net>
>Content-Type: text/plain; charset=us-ascii
>
>
>Need to install R with the shared libraries (it's a config option).
>
>
>Paul Shannon <pshannon at systemsbiology.org> writes:
>
>> I have successfully installed Rserv (http://stats.math.uni-augsburg.de/Rserve)
>> on Mac OS, but I have trouble on two different linux platforms.
>>
>>    R CMD INSTALL Rserve_0.3-10.tar.gz
>>
>> fails with this message
>>
>>   ** libs
>>   gcc -g -O2 -I/usr/local/include -L/usr/local/lib  Rserv.c -o Rserve  \
>>      -DDAEMON -O -I/usr/local/lib/R/include -Iinclude -I. -lR -L/usr/local/lib/R/bin -ldl -lcrypt
>>   /usr/bin/ld: cannot find -lR
>>   collect2: ld returned 1 exit status
>>   make: *** [Rserve] Error 1
>>   ERROR: compilation failed for package 'Rserve'
>>
>> Sure enough, when I look, I cannot find either libR.a or librR.so on either
>> linus system.
>>
>> On the Mac, I -do- find libR.dylib.
>>
>> Can anyone help with this?
>>
>> Many thanks -
>>
>>  - Paul Shannon
>>    Institute for Systems Biology
>>    Seattle
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>
>--
>Anthony Rossini			    Research Associate Professor
>rossini at u.washington.edu            http://www.analytics.washington.edu/
>Biomedical and Health Informatics   University of Washington
>Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
>UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
>FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email
>
>CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}
>
>
>
>------------------------------
>
>Message: 19
>Date: Sat, 14 Aug 2004 20:14:32 -0400
>From: "Peyuco Porras Porras ." <levin001 at 123mail.cl>
>Subject: [R] calibration/validation sets
>To: R-help at stat.math.ethz.ch
>Message-ID: <14090201402b96.1402b961409020 at 123mail.cl>
>Content-Type: text/plain; charset=us-ascii
>
>Hi;
>Does anyone know how to create a calibration and validation set from a particular dataset? I have a dataframe with nearly 20,000 rows! and I would like to select (randomly) a subset from the original dataset (...I found how to do that) to use as calibration set. However, I don't know how to remove this "calibration" set from the original dataframe in order to get my "validation" set.....Any hint will be greatly appreciated.
>TT
>
>
>
>------------------------------
>
>Message: 20
>Date: Sat, 14 Aug 2004 17:41:01 -0700
>From: "Austin, Matt" <maustin at amgen.com>
>Subject: RE: [R] calibration/validation sets
>To: "'Peyuco Porras Porras .'" <levin001 at 123mail.cl>,
>	R-help at stat.math.ethz.ch
>Message-ID:
>	<E7D5AB4811D20B489622AABA9C53859101F1111D at teal-exch.amgen.com>
>Content-Type: text/plain;	charset="iso-8859-1"
>
>You could keep a row index vector like in the following example.
>
>> data(iris)
>> indx <- sample(nrow(iris), 20, replace=FALSE)
>> train <- iris[indx,]
>> test  <- iris[-indx,]
>
>--Matt
>
>
>-----Original Message-----
>From: r-help-bounces at stat.math.ethz.ch
>[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Peyuco Porras
>Porras .
>Sent: Saturday, August 14, 2004 17:15 PM
>To: R-help at stat.math.ethz.ch
>Subject: [R] calibration/validation sets
>Importance: High
>
>
>Hi;
>Does anyone know how to create a calibration and validation set from a
>particular dataset? I have a dataframe with nearly 20,000 rows! and I would
>like to select (randomly) a subset from the original dataset (...I found how
>to do that) to use as calibration set. However, I don't know how to remove
>this "calibration" set from the original dataframe in order to get my
>"validation" set.....Any hint will be greatly appreciated.
>TT
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
>http://www.R-project.org/posting-guide.html
>
>
>
>------------------------------
>
>Message: 21
>Date: Sun, 15 Aug 2004 10:48:54 +1000 (EST)
>From: Kevin Wang <Kevin.Wang at maths.anu.edu.au>
>Subject: Re: [R] calibration/validation sets
>To: "Peyuco Porras Porras ." <levin001 at 123mail.cl>
>Cc: R-help at stat.math.ethz.ch
>Message-ID: <Pine.GSO.4.58.0408151045480.9499 at yin>
>Content-Type: TEXT/PLAIN; charset="US-ASCII"
>
>Hi,
>
>On Sat, 14 Aug 2004, Peyuco Porras Porras . wrote:
>
>> Hi;
>> Does anyone know how to create a calibration and validation set from a particular dataset? I have a dataframe with nearly 20,000 rows! and I would like to select (randomly) a subset from the original dataset (...I found how to do that) to use as calibration set. However, I don't know how to remove this "calibration" set from the original dataframe in order to get my "validation" set.....Any hint will be greatly appreciated.
>
>A really quick way, suppose you want to have 30% of your dataset as the
>validation set:
>> iris.id = sample(nrow(iris), nrow(iris) * 0.3)
>> iris.valid = iris[iris.id, ]
>> iris.train = iris[-iris.id, ]
>> nrow(iris.valid)
>[1] 45
>> nrow(iris.train)
>[1] 105
>
>The first line takes a sample of 30% of the number of rows in the Iris
>data.  The second line does a subetting of those samples -- the validation
>set.  The third takes what's left -- the training set.  This is perhaps
>not efficient and the code can definitely be simplified...but it's Sunday
>morning and I haven't had my morning coffee yet :D
>
>Cheers,
>
>Kevin
>
>
>--------------------------------
>Ko-Kang Kevin Wang
>PhD Student
>Centre for Mathematics and its Applications
>Building 27, Room 1004
>Mathematical Sciences Institute (MSI)
>Australian National University
>Canberra, ACT 0200
>Australia
>Homepage: http://wwwmaths.anu.edu.au/~wangk/
>Ph (W): +61-2-6125-2431
>Ph (H): +61-2-6125-7407
>Ph (M): +61-40-451-8301
>
>
>
>------------------------------
>
>Message: 22
>Date: Sat, 14 Aug 2004 21:05:22 -0400
>From: "Liaw, Andy" <andy_liaw at merck.com>
>Subject: RE: [R] calibration/validation sets
>To: "'Peyuco Porras Porras .'" <levin001 at 123mail.cl>,
>	R-help at stat.math.ethz.ch
>Message-ID:
>	<3A822319EB35174CA3714066D590DCD504AF822D at usrymx25.merck.com>
>Content-Type: text/plain
>
>There are many ways to do this.  One example, supposing your data is in
>`myData':
>
>## randomly pick 1/3 for validation:
>valid.idx <- sample(nrow(myData), round(nrow(myData)/3), replace=FALSE)
>
>## training set:
>myData.tr <- myData[-valid.idx,]
>## validation set:
>myData.valid <- myData[valid.idx,]
>
>HTH,
>Andy
>
>> From: Peyuco Porras Porras .
>>
>> Hi;
>> Does anyone know how to create a calibration and validation
>> set from a particular dataset? I have a dataframe with nearly
>> 20,000 rows! and I would like to select (randomly) a subset
>> from the original dataset (...I found how to do that) to use
>> as calibration set. However, I don't know how to remove this
>> "calibration" set from the original dataframe in order to get
>> my "validation" set.....Any hint will be greatly appreciated.
>> TT
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>>
>
>
>
>------------------------------
>
>Message: 23
>Date: Sun, 15 Aug 2004 09:38:44 +0800
>From: "Z P" <nusbj at hotmail.com>
>Subject: [R] Dirichlet-Multinomial
>To: r-help at stat.math.ethz.ch
>Message-ID: <BAY22-F14e0qsiUJ40H000462d4 at hotmail.com>
>Content-Type: text/plain; format=flowed
>
>Dear all,
>
>Is there any package in R, which can do the Dirichlet-Multinomial model fit?
>It is a generalization of the beta-binomial model. I know Prof. Lindsay has
>a package, whcih can estimate the beta-binomial parameter well.  Is there
>any counter part for Dirichlet-Multinomial? Thanks.
>
>Regards,
>
>Zhen
>
>
>
>------------------------------
>
>Message: 24
>Date: Sun, 15 Aug 2004 03:39:32 +0100
>From: Adaikalavan Ramasamy <ramasamy at cancer.org.uk>
>Subject: Re: [R] how to draw two graphs in one graph window
>To: Chuanjun Zhang <chzhang at cs.ucr.edu>
>Cc: R-help <r-help at stat.math.ethz.ch>
>Message-ID: <1092537572.7527.43.camel at localhost.localdomain>
>Content-Type: text/plain
>
>?par
>
>On Sun, 2004-08-15 at 00:14, Chuanjun Zhang wrote:
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>
>
>
>------------------------------
>
>Message: 25
>Date: Sat, 14 Aug 2004 20:32:14 -0700 (PDT)
>From: Robert Waters <rwatersg at yahoo.com>
>Subject: [R] index and by groups statement
>To: R-help at stat.math.ethz.ch
>Message-ID: <20040815033214.43527.qmail at web90108.mail.scd.yahoo.com>
>Content-Type: text/plain; charset=us-ascii
>
>Dear R-users
>
>Im working with a dataset that contains information
>for 8 groups of data and I need to select a sample of
>certain size (100 cubic feet by group) from this
>database for each of these 8 groups. To clarify, here
>is the starting code Im working with:
>
>k<-nrow(dataset)
>ix<-sort(runif(k),index.return=TRUE)$ix
>M<-max(which(cumsum(dataset$volume[ix])<100))+1
>test<-dataset[ix[1:M],]
>
>However, I don't know how to specify in this code the
>instruction: "by groups"
>
>Does anyone have an idea how to do this?
>
>Thanks in advance
>
>RW
>
>
>
>------------------------------
>
>Message: 26
>Date: Sun, 15 Aug 2004 05:28:54 +0100
>From: Adaikalavan Ramasamy <ramasamy at cancer.org.uk>
>Subject: Re: [R] index and by groups statement
>To: Robert Waters <rwatersg at yahoo.com>
>Cc: R-help <r-help at stat.math.ethz.ch>
>Message-ID: <1092544134.7527.120.camel at localhost.localdomain>
>Content-Type: text/plain
>
>If understand you correctly, you have a variable that groups each
>observations into one of eight categories. And there several hundred
>observations from each category. Now, you want to sample only 100
>observations from each category. It this is right, then the following
>might help :
>
>   set.seed(123)
>   num <- rnorm( length(g) )                    # response variable
>   g <- sample( LETTERS[1:8], 1200, replace=T ) # grouping variable
>   table(g)
>      A   B   C   D   E   F   G   H
>    146 153 131 166 140 164 163 137
>
>
>You can either store an list of 100 representative indexes (indexList)
>from each category or store the value instead (valueList)
>
>   indexList <- tapply( 1:length(g), g, function(x) sample(x, 100) )
>   valueList <- tapply( num, g, function(x) sample(x, 100) )
>
>The first is easier to double check with
>   for(i in 1:8) print(mean(g[ unlist(indexList[[i]]) ] == LETTERS[i]))
>
>
>If you only want the summary from these 100 sampled values, then you do
>not need to store any index or value, but calculate the summary
>directly. For example, lets say the median
>
>   tapply( num, g, function(x) median( sample(x, 100) ) )
>
>
>Hope this helps, Adai
>
>
>
>
>On Sun, 2004-08-15 at 04:32, Robert Waters wrote:
>> Dear R-users
>>
>> Im working with a dataset that contains information
>> for 8 groups of data and I need to select a sample of
>> certain size (100 cubic feet by group) from this
>> database for each of these 8 groups. To clarify, here
>> is the starting code Im working with:
>>
>> k<-nrow(dataset)
>> ix<-sort(runif(k),index.return=TRUE)$ix
>> M<-max(which(cumsum(dataset$volume[ix])<100))+1
>> test<-dataset[ix[1:M],]
>>
>> However, I don't know how to specify in this code the
>> instruction: "by groups"
>>
>> Does anyone have an idea how to do this?
>>
>> Thanks in advance
>>
>> RW
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>
>
>
>------------------------------
>
>_______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>End of R-help Digest, Vol 18, Issue 15
>**************************************
>
>________________________________________________________________________
>Doctors.net.uk e-mail protects you from viruses and unsolicited messages
>________________________________________________________________________
>


_______________________________________________________________________
Most doctors use http://www.Doctors.net.uk e-mail.
Move to a free professional address with spam and virus protection.



From rpeng at jhsph.edu  Tue Aug 17 15:14:00 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Tue, 17 Aug 2004 09:14:00 -0400
Subject: [R] Re: Thanks Frank, setting graph parameters,	and why social
	scientists don't use R
In-Reply-To: <1092748088_19480@drn10msi01>
References: <1092748088_19480@drn10msi01>
Message-ID: <41220498.4030301@jhsph.edu>

I'm just curious, but how do social scientists, or anyone else for 
that matter, learn SPSS, besides taking a class?

-roger

david_foreman at doctors.org.uk wrote:
> First, many thanks to Frank Harrell for once again helping me out.
> This actually relates to the next point, which is my contribution
> to the 'why don't social scientists use R' discussion.  I am a
> hybrid social scientist(child psychiatrist) who trained on SPSS.
> Many of my difficulties in coming to terms with R have been to do
> with trying to apply the logic underlying SPSS, with dire results.
> You do not want to know how long I spent looking for a 'recode'
> command in R, to change factor names and classes.....
> 
> I think the solution is to combine a graphical interface that
> encourages command line use (such as Rcommander) with the
> analyse(this) paradigm suggested, but also explaining how one can
> a) display the code on a separate window ('page' is only an obvious
> command once you know it), and b) how one can then save one's
> modification, make it generally available, and not overwrite the
> unmodified version (again, thanks, Frank).  Finally, one would need
> to change the emphasis in basic statistical teaching from 'the
> right test' to 'the right model'.  That should get people used to
> R's logic.
> 
> If a rabbit starts to use R, s/he is likely to head for the help
> files associated with each function, which can assume that the
> reader can make sense of gnomic utterances like "Omit 'var' to
> impute all variables, creating new variables in 'search' position
> 'where'".  I still don't know what that one means (as I don't
> understand search positions, or why they're important).  This can
> be very offputting, and could lead the rabbit to return to familiar
> SPSS territory.
> 
> Finally, friendlier error messages would also help. It took me 3
> days, and opening every function I could, to work out that
> '...cannot find function xxx.data.frame...' meant that MICE was
> unable to make a polychotomous logistic imputation model converge
> for the variable immediately preceding it.
> 
> 
> I am now off to the help files and FAQs to find out how to change
> graph parameters, as the plot.mids function in MICE a) doesn't
> allow one to select a subset of variables, and b) tells me that the
> graph it wants to produce on the whole of my 26 variable dataset is
> too big to fit on the (windows) plotting device.  Unless anyone
> wants to tell me how/where? (which of course is why, in the end, R
> is EASIER to use than SPSS)



From James_A_Rogers at groton.pfizer.com  Tue Aug 17 15:43:24 2004
From: James_A_Rogers at groton.pfizer.com (Rogers, James A [PGRD Groton])
Date: Tue, 17 Aug 2004 09:43:24 -0400
Subject: [R] using nls to fit a four parameter logistic model
Message-ID: <C735670CCC69D61193DA0002A58EE9900D7F537A@groexmb07.pfizer.com>

Shalini,

I think your "hill equation" is meant to just be an alternative
parameterization of the four parameter logistic (BTW, the "hill
*coefficient*" is a function of the slope parameter of the FPL, but I don't
believe "hill equation" is standard terminology). Note "conc" is the input
in this parameterization, not "log(conc)". 

> nls(log(il10)~A+(B-A)/(1+(conc/xmid )^scal),data=test,
+             start = list(A=3.5, B=15,
+               xmid=600,scal=1/2.5))
Nonlinear regression model
  model:  log(il10) ~ A + (B - A)/(1 + (conc/xmid)^scal) 
   data:  test 
          A           B        xmid        scal 
 14.7051665   3.7964534 607.9822962   0.3987786 
 residual sum-of-squares:  0.1667462 

To see the equivalence to the other parametrization that you used, note

> 1/2.507653
[1] 0.3987793
> log(607.9822962)
[1] 6.410146

--Jim

> Message: 17
> Date: Mon, 16 Aug 2004 11:25:57 -0500
> From: sraghavan at mmm.com
> Subject: [R] using nls to fit a four parameter logistic model
> To: r-help at stat.math.ethz.ch
> Message-ID:
> 	<OF24E63BBE.69F12D62-ON86256EF2.005A3E49-86256EF2.005A448D at mmm.com>
> Content-Type: text/plain; charset=US-ASCII
> 
> I am working on what appears to be a fairly simple problem for the
> following data
> 
>  test=data.frame(cbind(conc=c(25000, 12500, 6250, 3125, 1513, 781, 391,
> 195, 97.7, 48.4, 24, 12, 6, 3, 1.5, 0.001),
>  il10=c(330269, 216875, 104613, 51372, 26842, 13256, 7255, 3049, 1849,
743,
> 480, 255, 241, 128, 103, 50)))
> I am able to fit the above data to the equation
> 
> > nls(log(il10)~A+(B-A)/(1+exp((xmid-log(conc))/scal)),data=test,
> +  start = list(A=log(0.001), B=log(100000),
> + xmid=log(6000),scal=0.8))
> Nonlinear regression model
>   model:  log(il10) ~ A + (B - A)/(1 + exp((xmid - log(conc))/scal))
>    data:  test
>         A         B      xmid      scal
>  3.796457 14.705159  6.410144  2.507653
>  residual sum-of-squares:  0.1667462
> 
> 
> But in attempting to achieve a fit to what is commonly known as the hill
> equation, which is a four parameter fit that is used widely in biological
> data analysis
> 
> nls(log(il10)~A+(B-A)/(1+(log(conc)/xmid )^scal),data=test,
> + start = list(A=log(0.001), B=log(100000),  xmid=log(6000),scal=0.8))
> 
> Nonlinear regression model
>   model:  log(il10) ~ A + (B - A)/(1 + (log(conc)/xmid )^scal)
> 
> Error in numericDeriv(form[[3]], names(ind), env) :
>         Missing value or an Infinity produced when evaluating the model
> 
> 
> 
> Please would someone offer a suggestion
> 
> Shalini

James A. Rogers 
Manager, Nonclinical Statistics
PGR&D Groton Labs
Eastern Point Road (MS 260-1331)
Groton, CT 06340
office: (860) 686-0786
fax: (860) 715-5445
 


LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}



From ahenningsen at email.uni-kiel.de  Tue Aug 17 16:01:42 2004
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Tue, 17 Aug 2004 16:01:42 +0200
Subject: [R] Bug in colnames of data.frames?
Message-ID: <200408171601.42496.ahenningsen@email.uni-kiel.de>

Hi,

I am using R 1.9.1 on on i686 PC with SuSE Linux 9.0.

I have a data.frame, e.g.:

> myData <- data.frame( var1 = c( 1:4 ), var2 = c (5:8 ) )

If I add a new column by

> myData$var3 <- myData[ , "var1" ] + myData[ , "var2" ]

everything is fine, but if I omit the commas:

> myData$var4 <- myData[ "var1" ] + myData[ "var2" ]

the name shown above the 4th column is not "var4":

> myData
  var1 var2 var3 var1
1    1    5    6    6
2    2    6    8    8
3    3    7   10   10
4    4    8   12   12

but names() and colnames() return the expected name:

> names( myData )
[1] "var1" "var2" "var3" "var4"
> colnames( myData )
[1] "var1" "var2" "var3" "var4"

And it is even worse: I am not able to change the name shown above the 4th 
column:
> names( myData )[ 4 ] <- "var5"
> myData
  var1 var2 var3 var1
1    1    5    6    6
2    2    6    8    8
3    3    7   10   10
4    4    8   12   12

I guess that this is a bug, isn't it?

Arne



From ligges at statistik.uni-dortmund.de  Tue Aug 17 16:22:50 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 17 Aug 2004 16:22:50 +0200
Subject: [R] Bug in colnames of data.frames?
In-Reply-To: <200408171601.42496.ahenningsen@email.uni-kiel.de>
References: <200408171601.42496.ahenningsen@email.uni-kiel.de>
Message-ID: <412214BA.9040504@statistik.uni-dortmund.de>

Arne Henningsen wrote:

> Hi,
> 
> I am using R 1.9.1 on on i686 PC with SuSE Linux 9.0.
> 
> I have a data.frame, e.g.:
> 
> 
>>myData <- data.frame( var1 = c( 1:4 ), var2 = c (5:8 ) )
> 
> 
> If I add a new column by
> 
> 
>>myData$var3 <- myData[ , "var1" ] + myData[ , "var2" ]
> 
> 
> everything is fine, but if I omit the commas:
> 
> 
>>myData$var4 <- myData[ "var1" ] + myData[ "var2" ]

This bug is the user ... ;-)

Type:  str(myData)
`data.frame':   4 obs. of  3 variables:
  $ var1: int  1 2 3 4
  $ var2: int  5 6 7 8
  $ var4:`data.frame':   4 obs. of  1 variable:
   ..$ var1: int  6 8 10 12

Aha! You have created a data.frame consisting of one column! What you 
mean really mean is
  myData$var5 <- myData[[ "var1" ]] + myData[[ "var2" ]]

Uwe Ligges





> 
> the name shown above the 4th column is not "var4":
> 
> 
>>myData
> 
>   var1 var2 var3 var1
> 1    1    5    6    6
> 2    2    6    8    8
> 3    3    7   10   10
> 4    4    8   12   12
> 
> but names() and colnames() return the expected name:
> 
> 
>>names( myData )
> 
> [1] "var1" "var2" "var3" "var4"
> 
>>colnames( myData )
> 
> [1] "var1" "var2" "var3" "var4"
> 
> And it is even worse: I am not able to change the name shown above the 4th 
> column:
> 
>>names( myData )[ 4 ] <- "var5"
>>myData
> 
>   var1 var2 var3 var1
> 1    1    5    6    6
> 2    2    6    8    8
> 3    3    7   10   10
> 4    4    8   12   12
> 
> I guess that this is a bug, isn't it?
> 
> Arne
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From rn001 at cebas.csic.es  Tue Aug 17 16:25:12 2004
From: rn001 at cebas.csic.es (javier garcia - CEBAS)
Date: Tue, 17 Aug 2004 16:25:12 +0200
Subject: [R] Fwd: strptime() problem?
Message-ID: <20040817142338.DE037A7AC4@cebas.csic.es>

Hi all;
I've already send a similar e-mail to the list and Prof. Brian Ripley 
answered me but my doubts remain unresolved. Thanks for the clarification, 
but perhaps I wasn't clear enough in posting my questions.

I've got a postgres database which I read into R. The first column is
Timestamp with timezone, and my data are already in UTC format. An 'printed' 
extract of R character column, resulting from the timestamptz field is:

raincida$ts:

 [2039] "25/03/2000 22:00:00 UTC" "25/03/2000 23:00:00 UTC"
 [2041] "26/03/2000 00:00:00 UTC" "26/03/2000 01:00:00 UTC"
 [2043] "26/03/2000 02:00:00 UTC" "26/03/2000 03:00:00 UTC"
 [2045] "26/03/2000 04:00:00 UTC" "26/03/2000 05:00:00 UTC"

#And I need to convert this character column into POSIXct, for eventual work. 
#As I can see in the documentation, the process is to use strptime(), what 
#creates an object POSIXlt and doesn't allow to specify that the time zone of 
#the data is already UTC; followed by as.POSIXct()

> lluvia.strptime <- strptime(raincida$ts, format="%d/%m/%Y %H:%M:%S")
> lluvia.strptime.POSIXct <- as.POSIXct(lluvia.strptime,tz="GMT")

A "printed" extract is:

 [2039] "2000-03-25 22:00:00 GMT" "2000-03-25 23:00:00 GMT"
 [2041] "2000-03-26 00:00:00 GMT" "2000-03-26 01:00:00 GMT"
 [2043] "2000-03-26 03:00:00 GMT" "2000-03-26 03:00:00 GMT"
 [2045] "2000-03-26 04:00:00 GMT" "2000-03-26 05:00:00 GMT"

As we can see, elements [2043] differ. Shouldn't they be similar as the rest 
of the other shown elements? I thought this was a bug, but it seems that I've 
got and conceptual error.(?). This happens several times in my data, and 
produces eventual errors.

Please, how could I resolved this?

Thanks all, and best regards,

Javier G.



From p.dalgaard at biostat.ku.dk  Tue Aug 17 16:24:09 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 17 Aug 2004 16:24:09 +0200
Subject: [R] Bug in colnames of data.frames?
In-Reply-To: <200408171601.42496.ahenningsen@email.uni-kiel.de>
References: <200408171601.42496.ahenningsen@email.uni-kiel.de>
Message-ID: <x27jrxsv06.fsf@biostat.ku.dk>

Arne Henningsen <ahenningsen at email.uni-kiel.de> writes:

> Hi,
> 
> I am using R 1.9.1 on on i686 PC with SuSE Linux 9.0.
> 
> I have a data.frame, e.g.:
> 
> > myData <- data.frame( var1 = c( 1:4 ), var2 = c (5:8 ) )
> 
> If I add a new column by
> 
> > myData$var3 <- myData[ , "var1" ] + myData[ , "var2" ]
> 
> everything is fine, but if I omit the commas:
> 
> > myData$var4 <- myData[ "var1" ] + myData[ "var2" ]
> 
> the name shown above the 4th column is not "var4":
> 
> > myData
>   var1 var2 var3 var1
> 1    1    5    6    6
> 2    2    6    8    8
> 3    3    7   10   10
> 4    4    8   12   12
> 
> but names() and colnames() return the expected name:
> 
> > names( myData )
> [1] "var1" "var2" "var3" "var4"
> > colnames( myData )
> [1] "var1" "var2" "var3" "var4"
> 
> And it is even worse: I am not able to change the name shown above the 4th 
> column:
> > names( myData )[ 4 ] <- "var5"
> > myData
>   var1 var2 var3 var1
> 1    1    5    6    6
> 2    2    6    8    8
> 3    3    7   10   10
> 4    4    8   12   12
> 
> I guess that this is a bug, isn't it?

Nope:

> str(myData)
`data.frame':   4 obs. of  4 variables:
 $ var1: int  1 2 3 4
 $ var2: int  5 6 7 8
 $ var3: int  6 8 10 12
 $ var4:`data.frame':   4 obs. of  1 variable:
  ..$ var1: int  6 8 10 12

It's slightly peculiar, but if a column of a data frame is itself a
rectangular structure (data frame or matrix), then the "innermost" names
are used. Cf.

> myData[,"var4"] <- cbind(xyzzy=5:2)
> myData
  var1 var2 var3 xyzzy
1    1    5    6     5
2    2    6    8     4
3    3    7   10     3
4    4    8   12     2


Arguably, one might prefer

  var1 var2 var3  var4
                 xyzzy
1    1    5    6     5
2    2    6    8     4
3    3    7   10     3
4    4    8   12     2

or something like that, but it's hardly a bug.


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Tue Aug 17 16:29:14 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 17 Aug 2004 15:29:14 +0100 (BST)
Subject: [R] Bug in colnames of data.frames? -- NOT
In-Reply-To: <200408171601.42496.ahenningsen@email.uni-kiel.de>
Message-ID: <Pine.LNX.4.44.0408171521030.30095-100000@gannet.stats>

This is not a bug, and BTW data frames have names not colnames.
As I have said already today, don't confuse the printed repesentation of 
an object with the object itself.

On Tue, 17 Aug 2004, Arne Henningsen wrote:

> I am using R 1.9.1 on on i686 PC with SuSE Linux 9.0.
> 
> I have a data.frame, e.g.:
> 
> > myData <- data.frame( var1 = c( 1:4 ), var2 = c (5:8 ) )
> 
> If I add a new column by
> 
> > myData$var3 <- myData[ , "var1" ] + myData[ , "var2" ]
> 
> everything is fine, but if I omit the commas:
> 
> > myData$var4 <- myData[ "var1" ] + myData[ "var2" ]
> 
> the name shown above the 4th column is not "var4":
> 
> > myData
>   var1 var2 var3 var1
> 1    1    5    6    6
> 2    2    6    8    8
> 3    3    7   10   10
> 4    4    8   12   12
> 
> but names() and colnames() return the expected name:
> 
> > names( myData )
> [1] "var1" "var2" "var3" "var4"
> > colnames( myData )
> [1] "var1" "var2" "var3" "var4"
> 
> And it is even worse: I am not able to change the name shown above the 4th 
> column:
> > names( myData )[ 4 ] <- "var5"
> > myData
>   var1 var2 var3 var1
> 1    1    5    6    6
> 2    2    6    8    8
> 3    3    7   10   10
> 4    4    8   12   12
> 
> I guess that this is a bug, isn't it?

No.  Take a look at the fourth column more carefully.

> myData[4]
  var1
1    6
2    8
3   10
4   12

> class(myData[4])
[1] "data.frame"

You included a single-column data frame in your data frame.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Luisr at frs.fo  Tue Aug 17 16:30:12 2004
From: Luisr at frs.fo (Luis Rideau Cruz)
Date: Tue, 17 Aug 2004 15:30:12 +0100
Subject: [R] levels of  factor
Message-ID: <s122248a.021@ffdata.setur.fo>

R-help,

I have a data frame wich I subset like :

a <- subset(df,df$"column2" %in% c("factor1","factor2")  & df$"column2"==1)

But when I type levels(a$"column2") I still get the same levels as in df (my original data frame)

Why is that?
Is it right?

Luis

Luis Ridao Cruz
Fiskiranns??knarstovan
N??at??n 1
P.O. Box 3051
FR-110 T??rshavn
Faroe Islands
Phone:             +298 353900
Phone(direct): +298 353912
Mobile:             +298 580800
Fax:                 +298 353901
E-mail:              luisr at frs.fo
Web:                www.frs.fo



From spencer.graves at pdf.com  Tue Aug 17 16:29:35 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 17 Aug 2004 10:29:35 -0400
Subject: [R] using nls to fit a four parameter logistic model
In-Reply-To: <OF24E63BBE.69F12D62-ON86256EF2.005A3E49-86256EF2.005A448D@mmm.com>
References: <OF24E63BBE.69F12D62-ON86256EF2.005A3E49-86256EF2.005A448D@mmm.com>
Message-ID: <4122164F.9000502@pdf.com>

      In your second model, log(conc) is negative for conc = 0.001.  
This observation will generate NA for (log(conc)/xmid)^scal unless scal 
is an integer or xmid is also negative.  In the latter case, 
(log(conc)/xmid)^scal will be NA for all but that last value unless scal 
is an integer. 

      What do your biological references do with this model for 
concentrations less than 1? 

      If you delete that observation, the algorithm can still die 
testing a value for xmid <= 0.  To avoid these cases, I routine 
parameterize problems like this in terms of ln.xmid, something like the 
following: 
        
            log(il10)~A+(B-A)/(1+(log(conc)/exp(ln.xmid))^scal). 

      hope this helps.  spencer graves

sraghavan at mmm.com wrote:

>
>Shalini Raghavan
>3M Pharmaceuticals Research
>Building 270-03-A-10, 3M Center
>St. Paul, MN  55144
>E-mail: sraghavan at mmm.com
>Tel:  651-736-2575
>Fax:  651-733-5096
>
>----- Forwarded by Shalini Raghavan/US-Corporate/3M/US on 08/16/2004 11:25
>AM -----
>                                                                           
>             Shalini                                                       
>             Raghavan/US-Corpo                                             
>             rate/3M/US                                                 To 
>                                       r-help at stat.math.ethz.ch.           
>             08/16/2004 08:57                                           cc 
>             AM                                                            
>                                                                   Subject 
>                                       Fw: using nls to fit a four         
>                                       parameter logistic model            
>                                                                           
>                                                                           
>                                                                           
>                                                                           
>                                                                           
>                                                                           
>
>
>
>
>
>
>
>I am working on what appears to be a fairly simple problem for the
>following data
>
> test=data.frame(cbind(conc=c(25000, 12500, 6250, 3125, 1513, 781, 391,
>195, 97.7, 48.4, 24, 12, 6, 3, 1.5, 0.001),
> il10=c(330269, 216875, 104613, 51372, 26842, 13256, 7255, 3049, 1849, 743,
>480, 255, 241, 128, 103, 50)))
>  
>
>>test
>>    
>>
>        conc   il10
>1  25000.000 330269
>2  12500.000 216875
>3   6250.000 104613
>4   3125.000  51372
>5   1513.000  26842
>6    781.000  13256
>7    391.000   7255
>8    195.000   3049
>9     97.700   1849
>10    48.400    743
>11    24.000    480
>12    12.000    255
>13     6.000    241
>14     3.000    128
>15     1.500    103
>16     0.001     50
>
>I am able to fit the above data to the equation
>
>  
>
>>nls(log(il10)~A+(B-A)/(1+exp((xmid-log(conc))/scal)),data=test,
>>    
>>
>+  start = list(A=log(0.001), B=log(100000),
>+ xmid=log(6000),scal=0.8))
>Nonlinear regression model
>  model:  log(il10) ~ A + (B - A)/(1 + exp((xmid - log(conc))/scal))
>   data:  test
>        A         B      xmid      scal
> 3.796457 14.705159  6.410144  2.507653
> residual sum-of-squares:  0.1667462
>
>
>But in attempting to achieve a fit to what is commonly known as the hill
>equation, which is a four parameter fit that is used widely in biological
>data analysis
>
>nls(log(il10)~A+(B-A)/(1+(log(conc)/xmid )^scal),data=test,
>+ start = list(A=log(0.001), B=log(100000),  xmid=log(6000),scal=0.8))
>
>Nonlinear regression model
>  model:  log(il10) ~ A + (B - A)/(1 + (log(conc)/xmid )^scal)
>
>Error in numericDeriv(form[[3]], names(ind), env) :
>        Missing value or an Infinity produced when evaluating the model
>
>
>
>Please would someone offer a suggestion
>
>Shalini
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From MSchwartz at MedAnalytics.com  Tue Aug 17 16:34:08 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Tue, 17 Aug 2004 09:34:08 -0500
Subject: [R] Bug in colnames of data.frames?
In-Reply-To: <200408171601.42496.ahenningsen@email.uni-kiel.de>
References: <200408171601.42496.ahenningsen@email.uni-kiel.de>
Message-ID: <1092753248.20361.21.camel@localhost.localdomain>

On Tue, 2004-08-17 at 09:01, Arne Henningsen wrote:
> Hi,
> 
> I am using R 1.9.1 on on i686 PC with SuSE Linux 9.0.
> 
> I have a data.frame, e.g.:
> 
> > myData <- data.frame( var1 = c( 1:4 ), var2 = c (5:8 ) )
> 
> If I add a new column by
> 
> > myData$var3 <- myData[ , "var1" ] + myData[ , "var2" ]
> 
> everything is fine, but if I omit the commas:
> 
> > myData$var4 <- myData[ "var1" ] + myData[ "var2" ]
> 
> the name shown above the 4th column is not "var4":
> 
> > myData
>   var1 var2 var3 var1
> 1    1    5    6    6
> 2    2    6    8    8
> 3    3    7   10   10
> 4    4    8   12   12
> 
> but names() and colnames() return the expected name:
> 
> > names( myData )
> [1] "var1" "var2" "var3" "var4"
> > colnames( myData )
> [1] "var1" "var2" "var3" "var4"
> 
> And it is even worse: I am not able to change the name shown above the 4th 
> column:
> > names( myData )[ 4 ] <- "var5"
> > myData
>   var1 var2 var3 var1
> 1    1    5    6    6
> 2    2    6    8    8
> 3    3    7   10   10
> 4    4    8   12   12
> 
> I guess that this is a bug, isn't it?
> 
> Arne


Here is a hint:

# This returns an integer vector
> str(myData[ , "var1" ] + myData[ , "var2" ])
 int [1:4] 6 8 10 12


# This returns a data.frame
> str(myData[ "var1" ] + myData[ "var2" ])
`data.frame':	4 obs. of  1 variable:
 $ var1: int  6 8 10 12


> str(myData)
`data.frame':	4 obs. of  5 variables:
 $ var1: int  1 2 3 4
 $ var2: int  5 6 7 8
 $ var3: int  6 8 10 12
 $ var4:`data.frame':	4 obs. of  1 variable:
  ..$ var1: int  6 8 10 12


Take a look at the details, value and coercion sections of ?.data.frame

HTH,

Marc Schwartz



From MSchwartz at MedAnalytics.com  Tue Aug 17 16:52:56 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Tue, 17 Aug 2004 09:52:56 -0500
Subject: [R] Bug in colnames of data.frames?
In-Reply-To: <1092753248.20361.21.camel@localhost.localdomain>
References: <200408171601.42496.ahenningsen@email.uni-kiel.de>
	<1092753248.20361.21.camel@localhost.localdomain>
Message-ID: <1092754376.20361.27.camel@localhost.localdomain>

On Tue, 2004-08-17 at 09:34, Marc Schwartz wrote:

> Take a look at the details, value and coercion sections of
> ?.data.frame

This must be my week for typos. That should be:

?[.data.frame (in ESS)

or

?"[.data.frame" (otherwise)

Marc



From MSchwartz at MedAnalytics.com  Tue Aug 17 17:19:40 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Tue, 17 Aug 2004 10:19:40 -0500
Subject: [R] levels of  factor
In-Reply-To: <s122248a.021@ffdata.setur.fo>
References: <s122248a.021@ffdata.setur.fo>
Message-ID: <1092755979.20361.51.camel@localhost.localdomain>

On Tue, 2004-08-17 at 09:30, Luis Rideau Cruz wrote:
> R-help,
> 
> I have a data frame wich I subset like :
> 
> a <- subset(df,df$"column2" %in% c("factor1","factor2")  & df$"column2"==1)
> 
> But when I type levels(a$"column2") I still get the same levels as in df (my original data frame)
> 
> Why is that?

The default for [.factor is:

x[i, drop = FALSE]

Hence, unused factor levels are retained.

> Is it right?

Yes.

If you want to explicitly recode the factor based upon only those levels
that are actually in use, you can do something like the following:

a <- factor(a)


However, I am a bit unclear as to the logic of the subset statement that
you are using, perhaps b/c I don't know what your data is.

You seem to be subsetting 'column2' on both the factor levels and a
presumed numeric code. Is that really what you want to do?

You might want to review the "Warning" section in ?factor

BTW, when using subset(), the evaluation takes place within the data
frame, so you do not need to use df$"column2" in the function call. You
can just use column2, for example:

subset(df, column2 %in% c("factor1", "factor2"))

See ?factor and ?"[.factor" for more information.

HTH,

Marc Schwartz



From ggrothendieck at myway.com  Tue Aug 17 17:57:46 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 17 Aug 2004 11:57:46 -0400 (EDT)
Subject: [R] Fwd: strptime() problem?
Message-ID: <20040817155746.9C62E3994@mprdmxin.myway.com>



I am in a different time zone, EDT, on Windows XP and can't
replicate this but you might try reading the latest R News
article on dates and times for some ideas, viz. page 32 of:

   http://cran.r-project.org/doc/Rnews/Rnews_2004-1.pd

In particular, try converting them to chron and then doing 
your manipulations in chron or else convert them from chron to 
POSIXct:

   require(chron)
   r.asc <- raincida$ts
   r.chron <- chron(substring(r.asc, 1, 10), 
             substring(r.asc, 12, 19), format = c("d/m/y", "h:m:s"))

   r.ct <- as.POSIXct(r.chron)
   format(r.ct, tz="GMT")   # display POSIXct in GMT

Date:   	Tue, 17 Aug 2004 16:25:12 +0200
From:   	javier garcia - CEBAS <rn001 at cebas.csic.es>
To:   	Prof Brian Ripley <ripley at stats.ox.ac.uk>, <r-help at stat.math.ethz.ch>
Subject:   	[R] Fwd: strptime() problem?

Hi all;
I've already send a similar e-mail to the list and Prof. Brian Ripley
answered me but my doubts remain unresolved. Thanks for the clarification,
but perhaps I wasn't clear enough in posting my questions.

I've got a postgres database which I read into R. The first column is
Timestamp with timezone, and my data are already in UTC format. An 'printed'
extract of R character column, resulting from the timestamptz field is:

raincida$ts:

[2039] "25/03/2000 22:00:00 UTC" "25/03/2000 23:00:00 UTC"
[2041] "26/03/2000 00:00:00 UTC" "26/03/2000 01:00:00 UTC"
[2043] "26/03/2000 02:00:00 UTC" "26/03/2000 03:00:00 UTC"
[2045] "26/03/2000 04:00:00 UTC" "26/03/2000 05:00:00 UTC"

#And I need to convert this character column into POSIXct, for eventual work.
#As I can see in the documentation, the process is to use strptime(), what
#creates an object POSIXlt and doesn't allow to specify that the time zone of
#the data is already UTC; followed by as.POSIXct()

> lluvia.strptime <- strptime(raincida$ts, format="%d/%m/%Y %H:%M:%S")
> lluvia.strptime.POSIXct <- as.POSIXct(lluvia.strptime,tz="GMT")

A "printed" extract is:

[2039] "2000-03-25 22:00:00 GMT" "2000-03-25 23:00:00 GMT"
[2041] "2000-03-26 00:00:00 GMT" "2000-03-26 01:00:00 GMT"
[2043] "2000-03-26 03:00:00 GMT" "2000-03-26 03:00:00 GMT"
[2045] "2000-03-26 04:00:00 GMT" "2000-03-26 05:00:00 GMT"

As we can see, elements [2043] differ. Shouldn't they be similar as the rest
of the other shown elements? I thought this was a bug, but it seems that I've
got and conceptual error.(?). This happens several times in my data, and
produces eventual errors.

Please, how could I resolved this?

Thanks all, and best regards,

Javier G.



From ggrothendieck at myway.com  Tue Aug 17 17:59:48 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 17 Aug 2004 15:59:48 +0000 (UTC)
Subject: [R] Fwd: strptime() problem?
References: <20040817142338.DE037A7AC4@cebas.csic.es>
Message-ID: <loom.20040817T175236-878@post.gmane.org>

javier garcia - CEBAS <rn001 <at> cebas.csic.es> writes:

: 
: Hi all;
: I've already send a similar e-mail to the list and Prof. Brian Ripley 
: answered me but my doubts remain unresolved. Thanks for the clarification, 
: but perhaps I wasn't clear enough in posting my questions.
: 
: I've got a postgres database which I read into R. The first column is
: Timestamp with timezone, and my data are already in UTC format. An 'printed' 
: extract of R character column, resulting from the timestamptz field is:
: 
: raincida$ts:
: 
:  [2039] "25/03/2000 22:00:00 UTC" "25/03/2000 23:00:00 UTC"
:  [2041] "26/03/2000 00:00:00 UTC" "26/03/2000 01:00:00 UTC"
:  [2043] "26/03/2000 02:00:00 UTC" "26/03/2000 03:00:00 UTC"
:  [2045] "26/03/2000 04:00:00 UTC" "26/03/2000 05:00:00 UTC"
: 
: #And I need to convert this character column into POSIXct, for eventual 
work. 
: #As I can see in the documentation, the process is to use strptime(), what 
: #creates an object POSIXlt and doesn't allow to specify that the time zone 
of 
: #the data is already UTC; followed by as.POSIXct()
: 
: > lluvia.strptime <- strptime(raincida$ts, format="%d/%m/%Y %H:%M:%S")
: > lluvia.strptime.POSIXct <- as.POSIXct(lluvia.strptime,tz="GMT")
: 
: A "printed" extract is:
: 
:  [2039] "2000-03-25 22:00:00 GMT" "2000-03-25 23:00:00 GMT"
:  [2041] "2000-03-26 00:00:00 GMT" "2000-03-26 01:00:00 GMT"
:  [2043] "2000-03-26 03:00:00 GMT" "2000-03-26 03:00:00 GMT"
:  [2045] "2000-03-26 04:00:00 GMT" "2000-03-26 05:00:00 GMT"
: 
: As we can see, elements [2043] differ. Shouldn't they be similar as the rest 
: of the other shown elements? I thought this was a bug, but it seems that 
I've 
: got and conceptual error.(?). This happens several times in my data, and 
: produces eventual errors.
: 
: Please, how could I resolved this?

[Sorry if this gets posted twice.  I had a problem posting and
not sure if the first one ever got sent.]

I am in a different time zone, EDT, on Windows XP and can't
replicate this but you might try reading the latest R News
article on dates and times for some ideas, viz. page 32 of:

   http://cran.r-project.org/doc/Rnews/Rnews_2004-1.pd

In particular, try converting the datetimes to chron and then doing 
your manipulations in chron or else converting them from chron to 
POSIXct rather than going through POSIXlt:

   require(chron)
   r.asc <- raincida$ts
   r.chron <- chron(substring(r.asc, 1, 10), 
             substring(r.asc, 12, 19), format = c("d/m/y", "h:m:s"))

   r.ct <- as.POSIXct(r.chron)
   format(r.ct, tz="GMT") # display in GMT



From Whit.Armstrong at tudor.com  Tue Aug 17 18:14:37 2004
From: Whit.Armstrong at tudor.com (Whit Armstrong)
Date: Tue, 17 Aug 2004 12:14:37 -0400
Subject: [R] Fwd: strptime() problem?
Message-ID: <7669F018DC9DD711AEC500065B3D5ABF02CAD176@tudor.com>

Javier,

I recently had a problem with dates.  This example might shed some light on
your problem.

> x <- ISOdate(rep(2000,2),rep(3,2),rep(26,2),hour=0)
> x

[1] "2000-03-26 GMT" "2000-03-26 GMT"

> unclass(x)

[1] 954028800 954028800
attr(,"tzone")
[1] "GMT"

When one creates a date with ISOdate, the resulting object is of class
POSIXct and is given the attribute "tzone" which is set to "GMT."

When one prints an object of class POSIXct the function "print.POSIXct" is
called:
> print.POSIXct
function (x, ...) 
{
    print(format(x, usetz = TRUE, ...), ...)
    invisible(x)
}
<environment: namespace:base>
> 

So, that function is just calling "format" which gets dispatched to
"format.POSIXct":

> format.POSIXct
function (x, format = "", tz = "", usetz = FALSE, ...) 
{
    if (!inherits(x, "POSIXct")) 
        stop("wrong class")
    if (missing(tz) && !is.null(tzone <- attr(x, "tzone"))) 
        tz <- tzone
    structure(format.POSIXlt(as.POSIXlt(x, tz), format, usetz, 
        ...), names = names(x))
}
<environment: namespace:base>
> 

Now, if one looks carefully at this code, you will see that it tests for the
attribute "tzone" on the object that is passed in.  If it finds that
attribute, then it is passed on to "format.POSIXlt" (which is the function
that ultimately does the printing).  If there is no "tzone" attribute, then
"" is passed to "format.POSIXlt" as the tzone, which causes the object to be
printed in your locale specific format.

See:

> attr(x,"tzone") <- ""
> x
[1] "2000-03-25 19:00:00 Eastern Standard Time" "2000-03-25 19:00:00 Eastern
Standard Time"
> attr(x,"tzone") <- "GMT"
> x
[1] "2000-03-26 GMT" "2000-03-26 GMT"
> 

Now this is the part that really got me confused:

> x
[1] "2000-03-26 GMT" "2000-03-26 GMT"
> x[1]
[1] "2000-03-25 19:00:00 Eastern Standard Time"
> 

What happens in the above case is that the code for "[.POSIXct" looks like
this:

> get("[.POSIXct")
function (x, ..., drop = TRUE) 
{
    cl <- oldClass(x)
    class(x) <- NULL
    val <- NextMethod("[")
    class(val) <- cl
    val
}
<environment: namespace:base>
> 

The attribute "tzone" is not preserved!!  when "val" is created from the
call to NextMethod, its class is restored, but not its "tzone" attribute.
So any dates of class POSIXct that are printed after they have been
subscripted ("[") will have their "tzone" attribute stripped, and will print
in the local specific format.

For your specific case, I would convert all my dates to POSIXct, then set
the attribute "tzone" to "GMT."  After that, be very careful when
subscripting them, or you will find them printing in local specific formats
again.

for you:
> y <- strptime("4/3/2000",format="%m/%d/%Y")
> y
[1] "2000-04-03"
> y <- as.POSIXct(y,"GMT")
> y
[1] "2000-04-03 GMT"
> unclass(y)
[1] 954720000
attr(,"tzone")
[1] "GMT"
> 

I think that should straighten out your problem.

Hope that helps,
Whit



From gunter.berton at gene.com  Tue Aug 17 18:20:23 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 17 Aug 2004 09:20:23 -0700
Subject: [R] Re: Thanks Frank, setting graph parameters,and why social 
	scientists don't use R
References: <1092748088_19480@drn10msi01>
Message-ID: <41223047.9FC4CA06@gene.com>

A few comments:

First, your remarks are interesting and, I would say, mainly well founded. However, I think they are in many respects irrelevant, although they do point to the much bigger underlying issue, which Roger Peng also hinted at in his reply.

I think they are sensible because R IS difficult; the documentation is often challenging, which is not surprising given (a) the inherent complexity of R; (b) the difficulty in writing good documentation, especially when many of the functions being documented are inherently technical, so subject matter knowledge (CS, statistics, numerical analysis ,...) must be assumed; (c) the documentation has been written by a variety of mostly statistical types as a sidelight of their main professional activities -- none of these writers are ** professional documenters ** (whatever that may mean)
and some of them even speak ENglish as a second or third language. My own take is that the documentation for Core R and many of the packages is remarkably well done given these realities, and my hat is off to those who have produced it. Nevertheless, I agree, it is challenging -- it MUST be.

But they are irrelevant because the fundamental issue **is** that there is an inherent tension between ease of use and power/flexibility. Writing good GUI's for anything is hard, very hard. For a project such as R, it doesn't make sense, although it may to write GUI's for small subsets of R targeted at specific audiences (as in BioConductor, RCommander, etc.). But even this is hard to do well and takes a lot of time and effort. So, IMHO, there never will be nor ever should/could be an overall GUI for R: it is too complex and needs to be too extensible and flexible to constrain it in
that way.

However, I believe the larger question that both you and Roger Peng hint at is more important: not "How does a social scientist learn to use R," but how does any scientist/technologist for whom experimental design and data analysis forms a large component of their work gain the necessary technical background in statistics and related disciplines (linear algebra, numerical analysis, ...) to ** know how to use the statistical tools they need that R provides.**  Software like SPSS must assume a limited collection of methods to present to their customers in an effective GUI. Their strategy
**must** be (this is NOT a criticism) to "dumb it down" so that they can provide coherent albeit limited data analysis strategies. As you have explicitly stated, users who wish to venture outside those narrow paradigms are simply out of luck. R was designed from the outset not to be so constrained, but the cost is that you must know a good deal to use it effectively. It is obvious from the questions posted to this list that even something as "simple" as lm() often demands from users technical statistical understanding far beyond what they have. So we see fairly frequently indications
of misunderstanding and confusion in using R. But the problem isn't R -- it's that users don't know enough statistics.

I wish I could say I had an answer for this, but I don't have a clue. I do not thing it's fair to expect a mechnical engineer or psychologist or biologist to have the numerous math and statistical courses and experience in their training that would provide the base they need. For one thing, they don't have the time in their studies for this; for another, they may not have the background or interest -- they are, after all, mechanical engineers or biologists, not statisticians. Unfortunately, they could do their jobs as engineers and scientists a lot better if they did know more
statistics.  To me, it's a fundamental conundrum, and no one is to blame. It's just the reality, but it is the source for all kinds of frustrations on both sides of the statistical divide, which both you and Roger expressed in your own ways.

Obviously, all of this is just personal ranting, so I would love to hear alternative views. An thanks again for your clear and interesting comments.

Cheers,
Bert

david_foreman at doctors.org.uk wrote:

> First, many thanks to Frank Harrell for once again helping me out.  This actually relates to the next point, which is my contribution to the 'why don't social scientists use R' discussion.  I am a hybrid social scientist(child psychiatrist) who trained on SPSS.  Many of my difficulties in coming to terms with R have been to do with trying to apply the logic underlying SPSS, with dire results.  You do not want to know how long I spent looking for a 'recode' command in R, to change factor names and classes.....
>
> I think the solution is to combine a graphical interface that encourages command line use (such as Rcommander) with the analyse(this) paradigm suggested, but also explaining how one can a) display the code on a separate window ('page' is only an obvious command once you know it), and b) how one can then save one's modification, make it generally available, and not overwrite the unmodified version (again, thanks, Frank).  Finally, one would need to change the emphasis in basic statistical teaching from 'the right test' to 'the right model'.  That should get people used to R's logic.
>
> If a rabbit starts to use R, s/he is likely to head for the help files associated with each function, which can assume that the reader can make sense of gnomic utterances like "Omit 'var' to impute all variables, creating new variables in 'search' position 'where'".  I still don't know what that one means (as I don't understand search positions, or why they're important).  This can be very offputting, and could lead the rabbit to return to familiar SPSS territory.
>
> Finally, friendlier error messages would also help. It took me 3 days, and opening every function I could, to work out that '...cannot find function xxx.data.frame...' meant that MICE was unable to make a polychotomous logistic imputation model converge for the variable immediately preceding it.
>
> I am now off to the help files and FAQs to find out how to change graph parameters, as the plot.mids function in MICE a) doesn't allow one to select a subset of variables, and b) tells me that the graph it wants to produce on the whole of my 26 variable dataset is too big to fit on the (windows) plotting device.  Unless anyone wants to tell me how/where? (which of course is why, in the end, R is EASIER to use than SPSS)

--

Bert Gunter

Non-Clinical Biostatistics
Genentech
MS: 240B
Phone: 650-467-7374


"The business of the statistician is to catalyze the scientific learning process."

 -- George E.P. Box



From ggrothendieck at myway.com  Tue Aug 17 18:27:12 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 17 Aug 2004 16:27:12 +0000 (UTC)
Subject: [R] An entire data frame which is a time-series?
References: <20040817074152.GA19099@igidr.ac.in>
Message-ID: <loom.20040817T181440-277@post.gmane.org>

Ajay Shah <ajayshah <at> mayin.org> writes:

: 
: I have :
: 
: raw <- read.table("monthly.text", skip=3, sep="|",
:                 col.names=c("junk", "junk2",
:                   "wpi", "g.wpi", "wpi.primary", "g.wpi.primary",
:                   "wpi.fuel", "g.wpi.fuel", "wpi.manuf", "g.wpi.manuf",
:                   "cpi.iw", "g.cpi.iw", "cpi.unme", "g.cpi.unme",
:                   "cpi.al", "g.cpi.al", "cpi.rl", "g.cpi.rl"))
: 
: Now I can do things like:
: 
:   g.wpi = ts(raw$g.wpi, frequency=12, start=c(1994,7))
: 
: and it works fine. One by one, I can make time-series objects.
: 
: Is there a way to tell R that the entire data frame is a set of
: time-series, so that I don't have to go column by column and make a
: new ts() out of each?
: 
: I tried:
: 
:   M = ts(raw, frequency=12, start=c(1994,7))
:   ts.plot(M[,"wpi"], M[,"wpi.manuf"])
: 
: but this gives nonsense results. 

Converting a data frame to a ts object seems to work for me:

R> my.df <- data.frame(a = 1:4, b = 5:8)
R> my.ts <- ts(my.df, start=c(2000,4), freq=12)
R> my.ts.a <- my.ts[,"a"]
R> my.ts.a
     Apr May Jun Jul
2000   1   2   3   4

Suggest you provide a small reproduceable example that illustrates
the problem.


: Also, syntax like M$wpi is a lot
: nicer than M[,"wpi"]. Any ideas about what might work?

R> "$.ts" <- function(x, i) x[,i]
R> my.ts$a
     Apr May Jun Jul
2000   1   2   3   4



From kbartz at loyaltymatrix.com  Tue Aug 17 18:46:00 2004
From: kbartz at loyaltymatrix.com (Kevin Bartz)
Date: Tue, 17 Aug 2004 09:46:00 -0700
Subject: [R] levels of  factor
In-Reply-To: <s122248a.021@ffdata.setur.fo>
Message-ID: <20040817164600.C93CE400A0@omta14.mta.everyone.net>

Believe it or not, that's a feature, not a bug. The idea is that the factor
COULD take on those levels, even if it doesn't in your particular subset. To
drop them, you would have to re-initialize the factor as such:

a$column2 <- factor(a$column2)

Or, you could just download the Hmisc package, which redefines the subset
operator "[" to behave as you'd like. Personally, I think the default
behavior is clearer, however.

By the way, there are some problems with your code. First of all, you should
drop the quotes around column2--they're unnecessary. Secondly, your subset
is redundant: only one of your factor levels can be numbered 1, so only one
of the levels "factor1" and "factor2" is getting included in the result
(whichever is numbered 1 -- I'm guessing it's "factor1"). Was this your
intention?

Kevin

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Luis Rideau Cruz
Sent: Tuesday, August 17, 2004 7:30 AM
To: r-help at stat.math.ethz.ch
Subject: [R] levels of factor

R-help,

I have a data frame wich I subset like :

a <- subset(df,df$"column2" %in% c("factor1","factor2")  & df$"column2"==1)

But when I type levels(a$"column2") I still get the same levels as in df (my
original data frame)

Why is that?
Is it right?

Luis

Luis Ridao Cruz
Fiskiranns??knarstovan
N??at??n 1
P.O. Box 3051
FR-110 T??rshavn
Faroe Islands
Phone:             +298 353900
Phone(direct): +298 353912
Mobile:             +298 580800
Fax:                 +298 353901
E-mail:              luisr at frs.fo
Web:                www.frs.fo

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ahenningsen at email.uni-kiel.de  Tue Aug 17 18:57:19 2004
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Tue, 17 Aug 2004 18:57:19 +0200
Subject: [R] Bug in colnames of data.frames?
In-Reply-To: <x27jrxsv06.fsf@biostat.ku.dk>
References: <200408171601.42496.ahenningsen@email.uni-kiel.de>
	<x27jrxsv06.fsf@biostat.ku.dk>
Message-ID: <200408171857.19386.ahenningsen@email.uni-kiel.de>

Thank you for all your answers!

I agree with you that it is not a bug. My mistake was that I thought that a 
data frame is similar to a matrix, but as ?data.frame says they "... share 
many of the properties of matrices and of lists".

I never used the feature that a data.frame can contain objects with a 
rectangular structure so far. And I wonder if this feature is rather useful 
or confusing.

I think Peter's idea (see below) to show the structure of a data frame is very 
appealing, e.g.:

> myData[,"var4"] <- cbind(xyzzy=5:2)
> myData
  var1 var2 var3  var4
                 xyzzy
1    1    5    6     5
2    2    6    8     4
3    3    7   10     3
4    4    8   12     2

I think the current presentation
> myData
  var1 var2 var3 xyzzy
1    1    5    6     5
2    2    6    8     4
3    3    7   10     3
4    4    8   12     2

is confusing because it is not directly (without another command like str()) 
apparent why myData[[ "var1" ]] works fine while myData[[ "xyzzy" ]] does 
not. 

Best wishes,
Arne

On Tuesday 17 August 2004 16:24, Peter Dalgaard wrote:
> Arne Henningsen <ahenningsen at email.uni-kiel.de> writes:
> > Hi,
> >
> > I am using R 1.9.1 on on i686 PC with SuSE Linux 9.0.
> >
> > I have a data.frame, e.g.:
> > > myData <- data.frame( var1 = c( 1:4 ), var2 = c (5:8 ) )
> >
> > If I add a new column by
> >
> > > myData$var3 <- myData[ , "var1" ] + myData[ , "var2" ]
> >
> > everything is fine, but if I omit the commas:
> > > myData$var4 <- myData[ "var1" ] + myData[ "var2" ]
> >
> > the name shown above the 4th column is not "var4":
> > > myData
> >
> >   var1 var2 var3 var1
> > 1    1    5    6    6
> > 2    2    6    8    8
> > 3    3    7   10   10
> > 4    4    8   12   12
> >
> > but names() and colnames() return the expected name:
> > > names( myData )
> >
> > [1] "var1" "var2" "var3" "var4"
> >
> > > colnames( myData )
> >
> > [1] "var1" "var2" "var3" "var4"
> >
> > And it is even worse: I am not able to change the name shown above the
> > 4th
> >
> > column:
> > > names( myData )[ 4 ] <- "var5"
> > > myData
> >
> >   var1 var2 var3 var1
> > 1    1    5    6    6
> > 2    2    6    8    8
> > 3    3    7   10   10
> > 4    4    8   12   12
> >
> > I guess that this is a bug, isn't it?
>
> Nope:
> > str(myData)
>
> `data.frame':   4 obs. of  4 variables:
>  $ var1: int  1 2 3 4
>  $ var2: int  5 6 7 8
>  $ var3: int  6 8 10 12
>  $ var4:`data.frame':   4 obs. of  1 variable:
>   ..$ var1: int  6 8 10 12
>
> It's slightly peculiar, but if a column of a data frame is itself a
> rectangular structure (data frame or matrix), then the "innermost" names
> are used. Cf.
>
> > myData[,"var4"] <- cbind(xyzzy=5:2)
> > myData
>
>   var1 var2 var3 xyzzy
> 1    1    5    6     5
> 2    2    6    8     4
> 3    3    7   10     3
> 4    4    8   12     2
>
>
> Arguably, one might prefer
>
>   var1 var2 var3  var4
>                  xyzzy
> 1    1    5    6     5
> 2    2    6    8     4
> 3    3    7   10     3
> 4    4    8   12     2
>
> or something like that, but it's hardly a bug.



From krista at aha.demon.nl  Tue Aug 17 21:12:55 2004
From: krista at aha.demon.nl (Krista Haanstra)
Date: Tue, 17 Aug 2004 21:12:55 +0200
Subject: [R] survdiff
Message-ID: <AOEIIMAIPOIICDBMGIEOCEBMCAAA.krista@aha.demon.nl>

Hello,

As I am quitte an ignorant user of R, excuse me for any wrongfull usage of
all the terms.
My question relates to the statistics behind the survdiff function in the
package survival.
My textbook knowledge of the logrank test tells me that if I want to compare
two survival curves, I have to take the sum of the factors: (O-E)^2/E of
both groups, which will give me the Chisq.
If I calculate this by hand, I get a different value than the one R is
giving me.
Actually, the (O-E)^2/E that R gives me, those I agree with, but if I then
take the sum, this is not the chisq R gives.
Two questions:
- How is Chisq calculated in R?
- What does the column (O-E)^2/V mean? What is V, and how does this possibly
relate to the calculated Chisq?

The syntax would be something like this:

gr1<-c(1,2,3,4,5)
gr <-rep(1,length(gr1))
gr2<-c(6,7,8,9)
gr<-c(gr,rep(2,length(gr2)))
surv<-c(gr1,gr2)
event<-rep(1,7)
event<-c(event,0,0)
mydatafile<-cbind(surv,gr,event)
mydatafile <- data.frame(mydatafile)
mydatafile$gr<-factor(mydatafile$gr)

library(survival)

mydatafile.LR<-survdiff(Surv(surv,event)~gr,data=mydatafile)

print(mydatafile.LR)

And the response would be:

Call:
survdiff(formula = Surv(surv, event) ~ gr, data = mydatafile)

     N Observed Expected (O-E)^2/E (O-E)^2/V
gr=1 5        5     2.02      4.41      7.91
gr=2 4        2     4.98      1.79      7.91

 Chisq= 7.9  on 1 degrees of freedom, p= 0.00491

But, as said, I, with my textbook knowledge, I would calculate Chisq as:
4.41+1.19=6.20

Hopefully someone can clarify this for me.

Sincerely,

Krista Haanstra



From mhassan at scitegic.com  Tue Aug 17 23:08:46 2004
From: mhassan at scitegic.com (Moises Hassan)
Date: Tue, 17 Aug 2004 14:08:46 -0700
Subject: [R] aov summary to matrix
Message-ID: <830D8D4719112B418ABBC3A0EBA9581272EAD6@webmail.scitegic.com>

Is there an easy way of converting an aov.summary into a matrix in which
the rows are the factor names and the columns are Df, Sum Sq, Mean Sq, F
value and Pr. 

For example, convert 

            Df Sum Sq Mean Sq F value   Pr(>F)   
block        5 343.29   68.66  4.4467 0.015939 * 
N            1 189.28  189.28 12.2587 0.004372 **
P            1   8.40    8.40  0.5441 0.474904   
K            1  95.20   95.20  6.1657 0.028795 * 
N:P          1  21.28   21.28  1.3783 0.263165   
N:K          1  33.14   33.14  2.1460 0.168648   
P:K          1   0.48    0.48  0.0312 0.862752   
Residuals   12 185.29   15.44                    
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1


To

Factor      Df Sum Sq  Mean Sq F value   Pr
block        5 343.29   68.66  4.4467 0.015939
N            1 189.28  189.28 12.2587 0.004372
P            1   8.40    8.40  0.5441 0.474904   
K            1  95.20   95.20  6.1657 0.028795
N:P          1  21.28   21.28  1.3783 0.263165   
N:K          1  33.14   33.14  2.1460 0.168648   
P:K          1   0.48    0.48  0.0312 0.862752   
Residuals   12 185.29   15.44    NA      NA


Thanks,
   - Moises



From ripley at stats.ox.ac.uk  Tue Aug 17 23:32:54 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 17 Aug 2004 22:32:54 +0100 (BST)
Subject: [R] aov summary to matrix
In-Reply-To: <830D8D4719112B418ABBC3A0EBA9581272EAD6@webmail.scitegic.com>
Message-ID: <Pine.LNX.4.44.0408172226190.10846-100000@gannet.stats>

On Tue, 17 Aug 2004, Moises Hassan wrote:

> Is there an easy way of converting an aov.summary into a matrix in which
> the rows are the factor names and the columns are Df, Sum Sq, Mean Sq, F
> value and Pr. 

You are confusing the printed representation with the object (which seems 
today's favourite misconception).

as.matrix(summary(npk.aov)[[1]])

is a matrix (to full precision) as you seek, although I would prefer to 
work with the data frame which is returned.

(Note: your output is from MASS4 & example(aov), unattributed.)

> For example, convert 
> 
>             Df Sum Sq Mean Sq F value   Pr(>F)   
> block        5 343.29   68.66  4.4467 0.015939 * 
> N            1 189.28  189.28 12.2587 0.004372 **
> P            1   8.40    8.40  0.5441 0.474904   
> K            1  95.20   95.20  6.1657 0.028795 * 
> N:P          1  21.28   21.28  1.3783 0.263165   
> N:K          1  33.14   33.14  2.1460 0.168648   
> P:K          1   0.48    0.48  0.0312 0.862752   
> Residuals   12 185.29   15.44                    
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
> 
> 
> To
> 
> Factor      Df Sum Sq  Mean Sq F value   Pr
> block        5 343.29   68.66  4.4467 0.015939
> N            1 189.28  189.28 12.2587 0.004372
> P            1   8.40    8.40  0.5441 0.474904   
> K            1  95.20   95.20  6.1657 0.028795
> N:P          1  21.28   21.28  1.3783 0.263165   
> N:K          1  33.14   33.14  2.1460 0.168648   
> P:K          1   0.48    0.48  0.0312 0.862752   
> Residuals   12 185.29   15.44    NA      NA

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Tue Aug 17 23:37:06 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 17 Aug 2004 23:37:06 +0200
Subject: [R] survdiff
In-Reply-To: <AOEIIMAIPOIICDBMGIEOCEBMCAAA.krista@aha.demon.nl>
References: <AOEIIMAIPOIICDBMGIEOCEBMCAAA.krista@aha.demon.nl>
Message-ID: <x2acwtsayl.fsf@biostat.ku.dk>

"Krista Haanstra" <krista at aha.demon.nl> writes:

> As I am quitte an ignorant user of R, excuse me for any wrongfull usage of
> all the terms.
> My question relates to the statistics behind the survdiff function in the
> package survival.
> My textbook knowledge of the logrank test tells me that if I want to compare
> two survival curves, I have to take the sum of the factors: (O-E)^2/E of
> both groups, which will give me the Chisq.
> If I calculate this by hand, I get a different value than the one R is
> giving me.
> Actually, the (O-E)^2/E that R gives me, those I agree with, but if I then
> take the sum, this is not the chisq R gives.
> Two questions:
> - How is Chisq calculated in R?
> - What does the column (O-E)^2/V mean? What is V, and how does this possibly
> relate to the calculated Chisq?

You really need to read a theory book for this, but here's the basic idea:

V is the theoretical variance of O-E for the first group. If O-E is
approximately normally distributed, as it will be in large samples,
then (O-E)^2/V will be approximately chi-squared distributed on 1 DF.

In *other* models, notably those for contingency tables, the same idea
works out as the familiar sum((O-E)^2/E) formula. That formula has
historically been used for the logrank test too, and it still appears
in some textbooks, but as it turns out, it is not actually correct
(although often quite close).

[To fix ideas, consider testing for a given p in the binomial
distribution, you can either say O=x E=np V=npq and get

chisq = (x-np)^2/npq 

or have O = (x, n-x), E = (np, nq) and get

chisq =  (x-np)^2/np + ((n-x) - nq)^2/nq

and a little calculus show that the latter expression is

 = (x-np)^2*(1/np + 1/nq) = (x-np)^2 * (p+q)/npq

so the two formulas are one and the same. In this case!]
-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From sundar.dorai-raj at PDF.COM  Tue Aug 17 23:46:02 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Tue, 17 Aug 2004 16:46:02 -0500
Subject: [R] aov summary to matrix
In-Reply-To: <830D8D4719112B418ABBC3A0EBA9581272EAD6@webmail.scitegic.com>
References: <830D8D4719112B418ABBC3A0EBA9581272EAD6@webmail.scitegic.com>
Message-ID: <41227C9A.3080803@pdf.com>



Moises Hassan wrote:

> Is there an easy way of converting an aov.summary into a matrix in which
> the rows are the factor names and the columns are Df, Sum Sq, Mean Sq, F
> value and Pr. 
> 
> For example, convert 
> 
>             Df Sum Sq Mean Sq F value   Pr(>F)   
> block        5 343.29   68.66  4.4467 0.015939 * 
> N            1 189.28  189.28 12.2587 0.004372 **
> P            1   8.40    8.40  0.5441 0.474904   
> K            1  95.20   95.20  6.1657 0.028795 * 
> N:P          1  21.28   21.28  1.3783 0.263165   
> N:K          1  33.14   33.14  2.1460 0.168648   
> P:K          1   0.48    0.48  0.0312 0.862752   
> Residuals   12 185.29   15.44                    
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
> 
> 
> To
> 
> Factor      Df Sum Sq  Mean Sq F value   Pr
> block        5 343.29   68.66  4.4467 0.015939
> N            1 189.28  189.28 12.2587 0.004372
> P            1   8.40    8.40  0.5441 0.474904   
> K            1  95.20   95.20  6.1657 0.028795
> N:P          1  21.28   21.28  1.3783 0.263165   
> N:K          1  33.14   33.14  2.1460 0.168648   
> P:K          1   0.48    0.48  0.0312 0.862752   
> Residuals   12 185.29   15.44    NA      NA
> 
> 

Try this:

example(aov)
as.data.frame.summary.aovlist <- function(x) {
   if(length(x) == 1) {
     as.data.frame(x[[1]])
   } else {
     lapply(unlist(x, FALSE), as.data.frame)
   }
}
x1 <- summary(npk.aov)
x2 <- summary(npk.aovE)
as.data.frame(x1)
as.data.frame(x2)


--sundar



From drf5n at maplepark.com  Wed Aug 18 00:36:25 2004
From: drf5n at maplepark.com (David Forrest)
Date: Tue, 17 Aug 2004 17:36:25 -0500 (CDT)
Subject: [R] Bug in colnames of data.frames?
In-Reply-To: <200408171857.19386.ahenningsen@email.uni-kiel.de>
References: <200408171601.42496.ahenningsen@email.uni-kiel.de>
	<x27jrxsv06.fsf@biostat.ku.dk>
	<200408171857.19386.ahenningsen@email.uni-kiel.de>
Message-ID: <Pine.LNX.4.58.0408171710400.558@maplepark.com>

On Tue, 17 Aug 2004, Arne Henningsen wrote:

> Thank you for all your answers!
>
> I agree with you that it is not a bug. My mistake was that I thought that a
> data frame is similar to a matrix, but as ?data.frame says they "... share
> many of the properties of matrices and of lists".
...
>
> I think the current presentation
> > myData
>   var1 var2 var3 xyzzy
> 1    1    5    6     5
> 2    2    6    8     4
> 3    3    7   10     3
> 4    4    8   12     2
>
> is confusing because it is not directly (without another command like str())
> apparent why myData[[ "var1" ]] works fine while myData[[ "xyzzy" ]] does
> not.

In some ways it is a bug -- in the documentation, print.data.frame, or
format.data.frame

Consider assigning a wider dataframe to var4:

myData<-data.frame(matrix(1:12,4),var4=I(data.frame(xyzzy=5:2,plugh=1:4)))
myData  # error
class(myData[["var4"]])<-"data.frame"
myData  # gives indications of sub-variables by var.xyzzy, var.plugh
myData[["var4.plugh"]]  # NULL
myData[["var4"]][["plugh"]]

str(myData)

By the way, is there a way of making such an assignment in one step
without the I() class() hack?

dave
-- 
 Dave Forrest
 drf at vims.edu                                    (804)684-7900w
 drf5n at maplepark.com                             (804)642-0662h
                                   http://maplepark.com/~drf5n/



From jwdougherty at mcihispeed.net  Wed Aug 18 02:56:06 2004
From: jwdougherty at mcihispeed.net (John)
Date: Tue, 17 Aug 2004 17:56:06 -0700
Subject: [R] Re: Thanks Frank, setting graph parameters,
	=?iso-8859-1?q?=09and_why_social_scientists_don=27t_use?= R
In-Reply-To: <41220498.4030301@jhsph.edu>
References: <1092748088_19480@drn10msi01> <41220498.4030301@jhsph.edu>
Message-ID: <200408171756.06669.jwdougherty@mcihispeed.net>

On Tuesday 17 August 2004 06:14, Roger D. Peng wrote:
> I'm just curious, but how do social scientists, or anyone else for
> that matter, learn SPSS, besides taking a class?
>
They sit down with a book, a computer, and data they desperately need to 
analyze and start working.  SPSS documentation and some of the third party 
works are fairly thorough, and pretty gentle, and the writings fits the 
expectations of someone who has had only the initiatory stats courses.  Your 
teacher emphasizes checking the normality of the data, so you look for the 
means of measuring it and the tests that tell you whether it is significant 
or not, after very carefully considering the nature of your data in the light 
of the assumptions made in the SPSS tests make.  You are far less concerned 
with the real mathematical mechanics than you are about meeting the 
expectations of the professor.  SPSS, SYSTAT, NCSS and similar programs all 
support this kind work.  Many social science professors don't really know 
enough to judge your work beyond similar expectations THEY learned from their 
own professors.  It's sad, but the way it works in many schools.

J



From jwdougherty at mcihispeed.net  Wed Aug 18 03:32:21 2004
From: jwdougherty at mcihispeed.net (John)
Date: Tue, 17 Aug 2004 18:32:21 -0700
Subject: [R] Re: Thanks Frank, setting graph parameters,
	and why social  scientists don't use R
In-Reply-To: <41223047.9FC4CA06@gene.com>
References: <1092748088_19480@drn10msi01> <41223047.9FC4CA06@gene.com>
Message-ID: <200408171832.21493.jwdougherty@mcihispeed.net>

On Tuesday 17 August 2004 09:20, Berton Gunter wrote:
> A few comments:

It has been decades since I used SPSS.  At that time, to really work with it 
you edited a text file program that identified the data file and variable 
columns you wanted to work with.  You assembled the flow of work commands 
after carefully going through the SPSS documentation.  After you were ready, 
you ran the program and crossed your fingers.  R IS complex, enough so that 
the useability at a basic level is readily achievable.  What it lacks is 
simply the Stat 1 and Stat 101 packages that lead users from the very basics 
covered in introductory statistics texts into more profound analyses that 
some many R users are interested in.  There are some texts, such as Peter 
Daalgard's Introductory Statistics with R, which is a very useful book.  
However, from a student's view point Chapter 1 focuses on R, everything from 
the R Language to R programming.  The statistics chapters that follow almost 
seem to be used as an adjunct to teaching R rather than vice versa.  For some 
social science students, a package that leads more gradually into R would 
probably be a big help learning learning the language while getting their 
feet wet in statistics.

John



From syldan4 at hotmail.com  Wed Aug 18 03:40:41 2004
From: syldan4 at hotmail.com (syl dan)
Date: Wed, 18 Aug 2004 01:40:41 +0000
Subject: [R] logistic -normal model
Message-ID: <BAY12-F92r8gPAAIwpl000a908d@hotmail.com>

I am working with a logistic-normal model (i.e, GLMM with random intercept 
model) by Bayesian method. BUt I met some difficulities for programming by 
R. Is there anyone have experience of this  model or the R code I can refer 
as example?

Thanks for your help.

Syl



From konradb at few.vu.nl  Wed Aug 18 06:24:47 2004
From: konradb at few.vu.nl (Konrad Banachewicz)
Date: Wed, 18 Aug 2004 06:24:47 +0200
Subject: [R] creating a plot
Message-ID: <4122DA0F.8050707@few.vu.nl>

Hi,
I have a time series plot to produce, yet I want the x-axis to be 
labelled with dates
(stored on another array) and not with observation numbers. Can anyone 
suggest me how?
Thanks.

                                                                         
   Konrad



From aragon at berkeley.edu  Wed Aug 18 07:42:30 2004
From: aragon at berkeley.edu (Tomas Aragon)
Date: Tue, 17 Aug 2004 22:42:30 -0700 (PDT)
Subject: [R] creating a plot
In-Reply-To: <4122DA0F.8050707@few.vu.nl>
Message-ID: <20040818054230.44813.qmail@web80104.mail.yahoo.com>

--- Konrad Banachewicz <konradb at few.vu.nl> wrote:

> Hi,
> I have a time series plot to produce, yet I want the x-axis to be 
> labelled with dates
> (stored on another array) and not with observation numbers. Can
> anyone 
> suggest me how?
> Thanks.
> 
>                                                                      
>    
>    Konrad
> 
Try checking out http://www.medepi.net/data/wnv/index.html at bottom of
page. 
Tomas

=====
Tomas Aragon, MD, DrPH, Director
Center for Infectious Disease Preparedness
UC Berkeley School of Public Health
1918 University Ave., 4th Fl., MC-7350
Berkeley, CA 94720-7350
http://www.idready.org



From gb at stat.umu.se  Wed Aug 18 16:04:20 2004
From: gb at stat.umu.se (Gran Brostrm)
Date: Wed, 18 Aug 2004 10:04:20 -0400
Subject: [R] Revert a factor to its numeric values
Message-ID: <20040818140420.GA21931@stat.umu.se>

I'm trying a recommendation on the help page for 'factor':

> x <- c(1, 2, 1, 2)
> x <- factor(x, labels = c("one", "two"))
> x
[1] one two one two
Levels: one two
> as.numeric(levels(x))[x]
[1] NA NA NA NA
Warning message:
NAs introduced by coercion
 
Also,

> as.numeric(as.character(x))
[1] NA NA NA NA
Warning message:
NAs introduced by coercion

What am I doing wrong? This is R-1.9.1, Linux (debian installation)

Another question: I have a factor with four levels, which I want 
to collapse to two. How do I do it in the simplest possible way?

Thanks,

G??ran 
-- 
 G??ran Brostr??m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume?? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume??, Sweden             e-mail: gb at stat.umu.se



From wia at zhwin.ch  Wed Aug 18 10:10:30 2004
From: wia at zhwin.ch (Wildi Marc, wia)
Date: Wed, 18 Aug 2004 10:10:30 +0200
Subject: [R] (no subject)
Message-ID: <53A181E56FB0694ABFD212F8AEDA7F6F01063C98@langouste.zhwin.ch>

Hi
 
Does anybody know from an R-package devoted to sample selection problems (Heckman's lambda, Lewbel, ...)?
 
Thanks and best regards
 
Marc Wildi



From ripley at stats.ox.ac.uk  Wed Aug 18 10:30:50 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 18 Aug 2004 09:30:50 +0100 (BST)
Subject: [R] Revert a factor to its numeric values
In-Reply-To: <20040818140420.GA21931@stat.umu.se>
Message-ID: <Pine.LNX.4.44.0408180928320.28263-100000@gannet.stats>

On Wed, 18 Aug 2004, G??ran Brostr??m wrote:

> I'm trying a recommendation on the help page for 'factor':
> 
> > x <- c(1, 2, 1, 2)
> > x <- factor(x, labels = c("one", "two"))
> > x
> [1] one two one two
> Levels: one two
> > as.numeric(levels(x))[x]
> [1] NA NA NA NA
> Warning message:
> NAs introduced by coercion
>  
> Also,
> 
> > as.numeric(as.character(x))
> [1] NA NA NA NA
> Warning message:
> NAs introduced by coercion
> 
> What am I doing wrong? This is R-1.9.1, Linux (debian installation)

Your factor is made up of "one", "two", which are not numeric -- don't
expect R to speak English (or Swedish).  You could just as easily have
used labels = c("apples", "oranges").

> Another question: I have a factor with four levels, which I want 
> to collapse to two. How do I do it in the simplest possible way?

via levels<- : there is an example on the help page for levels.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From gb at stat.umu.se  Wed Aug 18 16:51:30 2004
From: gb at stat.umu.se (Gran Brostrm)
Date: Wed, 18 Aug 2004 10:51:30 -0400
Subject: [R] Revert a factor to its numeric values
In-Reply-To: <Pine.LNX.4.44.0408180928320.28263-100000@gannet.stats>
References: <20040818140420.GA21931@stat.umu.se>
	<Pine.LNX.4.44.0408180928320.28263-100000@gannet.stats>
Message-ID: <20040818145130.GA24417@stat.umu.se>

On Wed, Aug 18, 2004 at 09:30:50AM +0100, Prof Brian Ripley wrote:
> On Wed, 18 Aug 2004, G??ran Brostr??m wrote:
> 
> > I'm trying a recommendation on the help page for 'factor':
> > 
> > > x <- c(1, 2, 1, 2)
> > > x <- factor(x, labels = c("one", "two"))
> > > x
> > [1] one two one two
> > Levels: one two
> > > as.numeric(levels(x))[x]
> > [1] NA NA NA NA
> > Warning message:
> > NAs introduced by coercion
> >  
> > Also,
> > 
> > > as.numeric(as.character(x))
> > [1] NA NA NA NA
> > Warning message:
> > NAs introduced by coercion
> > 
> > What am I doing wrong? This is R-1.9.1, Linux (debian installation)
> 
> Your factor is made up of "one", "two", which are not numeric -- don't
> expect R to speak English (or Swedish).  You could just as easily have
> used labels = c("apples", "oranges").

Didn't work either. :-)

I really want the underlying numeric codes (convert the variable to numeric).
'as.integer(x)' seems to do the trick, is that correct? Also 'as.numeric(x)', although it is "meaningless" according to the help page.

> 
> > Another question: I have a factor with four levels, which I want 
> > to collapse to two. How do I do it in the simplest possible way?
> 
> via levels<- : there is an example on the help page for levels.

Thanks; exactly what I was looking for.

G??ran



From ripley at stats.ox.ac.uk  Wed Aug 18 11:00:56 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 18 Aug 2004 10:00:56 +0100 (BST)
Subject: [R] Revert a factor to its numeric values
In-Reply-To: <20040818145130.GA24417@stat.umu.se>
Message-ID: <Pine.LNX.4.44.0408180957350.4375-100000@gannet.stats>

On Wed, 18 Aug 2004, G??ran Brostr??m wrote:

> On Wed, Aug 18, 2004 at 09:30:50AM +0100, Prof Brian Ripley wrote:
> > On Wed, 18 Aug 2004, G??ran Brostr??m wrote:
> > 
> > > I'm trying a recommendation on the help page for 'factor':
> > > 
> > > > x <- c(1, 2, 1, 2)
> > > > x <- factor(x, labels = c("one", "two"))
> > > > x
> > > [1] one two one two
> > > Levels: one two
> > > > as.numeric(levels(x))[x]
> > > [1] NA NA NA NA
> > > Warning message:
> > > NAs introduced by coercion
> > >  
> > > Also,
> > > 
> > > > as.numeric(as.character(x))
> > > [1] NA NA NA NA
> > > Warning message:
> > > NAs introduced by coercion
> > > 
> > > What am I doing wrong? This is R-1.9.1, Linux (debian installation)
> > 
> > Your factor is made up of "one", "two", which are not numeric -- don't
> > expect R to speak English (or Swedish).  You could just as easily have
> > used labels = c("apples", "oranges").
> 
> Didn't work either. :-)
> 
> I really want the underlying numeric codes (convert the variable to numeric).
> 'as.integer(x)' seems to do the trick, is that correct? Also 'as.numeric(x)', although it is "meaningless" according to the help page.

as.integer is OK, but unclass() is the usual way (it doesn't lose the 
other attributes such as names).

x <- factor(1:10)
names(x) <- letters[1:10]
> as.integer(x)
 [1]  1  2  3  4  5  6  7  8  9 10
> unclass(x)
 a  b  c  d  e  f  g  h  i  j
 1  2  3  4  5  6  7  8  9 10
attr(,"levels")
 [1] "1"  "2"  "3"  "4"  "5"  "6"  "7"  "8"  "9"  "10"


> > > Another question: I have a factor with four levels, which I want 
> > > to collapse to two. How do I do it in the simplest possible way?
> > 
> > via levels<- : there is an example on the help page for levels.
> 
> Thanks; exactly what I was looking for.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Wed Aug 18 11:57:38 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Aug 2004 11:57:38 +0200
Subject: [R] Revert a factor to its numeric values
In-Reply-To: <20040818140420.GA21931@stat.umu.se>
References: <20040818140420.GA21931@stat.umu.se>
Message-ID: <x23c2k22gd.fsf@biostat.ku.dk>

gb at stat.umu.se (G??ran Brostr??m) writes:

> I'm trying a recommendation on the help page for 'factor':
> 
> > x <- c(1, 2, 1, 2)
> > x <- factor(x, labels = c("one", "two"))
> > x
> [1] one two one two
> Levels: one two
> > as.numeric(levels(x))[x]
> [1] NA NA NA NA
> Warning message:
> NAs introduced by coercion
>  
> Also,
> 
> > as.numeric(as.character(x))
> [1] NA NA NA NA
> Warning message:
> NAs introduced by coercion
> 
> What am I doing wrong? This is R-1.9.1, Linux (debian installation)

You appear to be assuming tha R knows how to make numbers from text
strings

as.numeric(c("jedan","dva","tri","ceteri"))

is not going to give you 1,2,3,4 either (not even in a Croatian
locale).

The suggestion on the help page works for numeric levels: 

>  x <- c(1, 2, 1, 2)
>  x <- factor(x, labels = c("5", "7"))
>  as.numeric(levels(x))[x]
[1] 5 7 5 7


> Another question: I have a factor with four levels, which I want 
> to collapse to two. How do I do it in the simplest possible way?

 
> x <- factor(c("jedan","dva","tri","ceteri"))
> x
[1] jedan  dva    tri    ceteri
Levels: ceteri dva jedan tri
> levels(x) <- c("even","even","odd","odd")
> x
[1] odd  even odd  even
Levels: even odd


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Luisr at frs.fo  Wed Aug 18 12:22:56 2004
From: Luisr at frs.fo (Luis Rideau Cruz)
Date: Wed, 18 Aug 2004 11:22:56 +0100
Subject: [R] function(...) {}
Message-ID: <s1233c18.001@ffdata.setur.fo>

R-help

Assignments within functions are local and temporary,right?(function 1)
If arguments to another function (function 2) are objects created in function 1 and need to call function 2,,,,,,,then it won't work,Am i right?
I have nested the functions but still do not work.

How can this (it may be simple but I don' know how to get around this) be done??

Thank you




Luis Ridao Cruz
Fiskiranns??knarstovan
N??at??n 1
P.O. Box 3051
FR-110 T??rshavn
Faroe Islands
Phone:             +298 353900
Phone(direct): +298 353912
Mobile:             +298 580800
Fax:                 +298 353901
E-mail:              luisr at frs.fo
Web:                www.frs.fo



From spencer.graves at pdf.com  Wed Aug 18 12:25:10 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 18 Aug 2004 06:25:10 -0400
Subject: [R] logistic -normal model
In-Reply-To: <BAY12-F92r8gPAAIwpl000a908d@hotmail.com>
References: <BAY12-F92r8gPAAIwpl000a908d@hotmail.com>
Message-ID: <41232E86.6070509@pdf.com>

      1.  Have you also tried "glmmPQL" in library(MASS)?  It is 
supposed to do the same thing.  I had better luck with GLMM, but you 
might try "glmmPQL" if you haven't already. 

      2.  PLEASE do read the posting guide! 
"http://www.R-project.org/posting-guide.html".  It may help you find 
answers to some questions yourself.  Failing that, it may help you 
express your question in a form that more likely elicit a more 
informative response. 

      hope this helps.  spencer graves

syl dan wrote:

> I am working with a logistic-normal model (i.e, GLMM with random 
> intercept model) by Bayesian method. BUt I met some difficulities for 
> programming by R. Is there anyone have experience of this  model or 
> the R code I can refer as example?
>
> Thanks for your help.
>
> Syl
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From john.maindonald at anu.edu.au  Wed Aug 18 12:57:22 2004
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Wed, 18 Aug 2004 20:57:22 +1000
Subject: [R] Re: Thanks Frank, setting graph parameters,
	and why social  scientists don't use R
In-Reply-To: <200408181003.i7IA3T7h024445@hypatia.math.ethz.ch>
References: <200408181003.i7IA3T7h024445@hypatia.math.ethz.ch>
Message-ID: <618EBFA8-F105-11D8-9E68-000A95CDA0F2@anu.edu.au>

There are answers that could and should be applied in specific 
situations.  At least in academia and in substantial research teams, 
statisticians ought to have a prominent part in many of the research 
teams.  Senior statisticians should have a prominent role in deciding 
the teams to which this applies.  why should it be ok to do combine 
high levels of chemical expertise with truly appalling statistical 
misunderstandings, to the extent that the suppose chemical insights are 
not what they appear to be?

There should be a major focus on training application area students on 
training them to understand important ideas, to recognize when they are 
out of their depth, and to work with statisticians.

There should be much more use of statisticians in the refereeing of 
published papers.  Editors need to seek advice from experienced 
statisticians (some do) on what sorts of papers are candidates for 
statistical refereeing.

Publication in an archive of the data that have been used for a paper 
could be a huge help, so that others can check whether the data really 
do support the conclusion.  Even better, as Robert Gentleman has 
argued, would/will be papers that can be processed through Sweave or 
its equivalent.

Really enlightened people (in the statistical sense) in the applied 
communities will latch onto R, as some are doing, because the 
limitations inherent in much other software so often lead to crippled 
and/or misleading analyses.  Increasingly, we can hope that it will 
become difficult for statistics to in various applied area communities 
to proceed on its merry way, ignorant of or ignoring most of what has 
happened in the mainstream statistical community in the past 20 years.

The statistical community needs to be a lot more aggressive in 
demanding adequate standards of data analysis in applied areas, at the 
same time suggesting ways in which it can work with application area 
people to improve standards.

It is also fair to comment that the situation is very uneven.  There 
are some areas where the standards are pretty reasonable, at least for 
the types of problems that typically come up in those areas.
John Maindonald.

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Bioinformation Science, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.


On 18 Aug 2004, Bert Gunter wrote:
So we see fairly frequently indications
of misunderstanding and confusion in using R. But the problem isn't R 
-- it's that users don't know enough statistics.

. . . .
I wish I could say I had an answer for this, but I don't have a clue. I 
do not thing it's fair to expect a mechnical engineer or psychologist 
or biologist to have the numerous math and statistical courses and 
experience in their training that would provide the base they need. For 
one thing, they don't have the time in their studies for this; for 
another, they may not have the background or interest -- they are, 
after all, mechanical engineers or biologists, not statisticians. 
Unfortunately, they could do their jobs as engineers and scientists a 
lot better if they did know more
statistics.  To me, it's a fundamental conundrum, and no one is to 
blame. It's just the reality, but it is the source for all kinds of 
frustrations on both sides of the statistical divide, which both you 
and Roger expressed in your own ways.
. . . .



From David_Foreman at doctors.net.uk  Wed Aug 18 13:16:00 2004
From: David_Foreman at doctors.net.uk (David Foreman)
Date: Wed, 18 Aug 2004 12:16:00 0100
Subject: [R] Re: Thanks Frank, setting graph parameters,
	and why socialscientists don't use R
Message-ID: <40ED169500740D57@eeyor.london.ongenie.net> (added by
	postmaster@mail.o2.co.uk)

Medicine in the UK uses (or used, it's changing) the 'apprenticeship model' which in my case meant my own datasets (providing the motivation) the programme and its supporting written documentation (SPSS manuals double as quite good statistical textbooks, so I could link the concepts to the software) and various supervisors and referees (providing ego-bruising feedback).  I found the courses I attended distinctly unmemorable, no matter how ego-syntonic they were, largely because I didn't care about the sample datasets supplied. Other students I have known and taught/supervised report similar experiences.



From ggrothendieck at myway.com  Wed Aug 18 14:01:17 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 18 Aug 2004 12:01:17 +0000 (UTC)
Subject: [R] function(...) =?utf-8?b?e30=?=
References: <s1233c18.001@ffdata.setur.fo>
Message-ID: <loom.20040818T135014-471@post.gmane.org>

Luis Rideau Cruz <Luisr <at> frs.fo> writes:

> 
> R-help
> 
> Assignments within functions are local and temporary,right?(function 1)
> If arguments to another function (function 2) are objects created in 
function 1 and need to call function
> 2,,,,,,,then it won't work,Am i right?
> I have nested the functions but still do not work.
> 
> How can this (it may be simple but I don' know how to get around this) be 
done??

Try this:

f1 <- function() {
   f2 <- function(x) assign(as.character(substitute(x)), x+1, parent.frame())
   z <- 1
   f2(z)
   z   # x has value 
}
f1() # 2

# Actually, you don't have to nest them since parent.frame refers to the
# environment of the caller:

f2 <- function(x) assign(as.character(substitute(x)), x+1, parent.frame())
f1 <- function() {
   z <- 1
   f2(z)
   z   # x has value 
}
f1() # 2



From rmh at temple.edu  Wed Aug 18 15:36:00 2004
From: rmh at temple.edu (Richard M. Heiberger)
Date: Wed, 18 Aug 2004 09:36:00 -0400
Subject: [R] Statistical Analysis and Data Display: An
 Intermediate Course with Examples in S-Plus, R, and SAS
Message-ID: <be722c79.71cfa09e.81a1200@po-d.temple.edu>

My new book 
 Statistical Analysis and Data Display: An Intermediate Course with 
 Examples in S-Plus, R, and SAS 
by 
 Richard M. Heiberger and Burt Holland 
has just been published by Springer.  You can get information at 

 http://www.springeronline.com/0-387-40270-5 


We will be giving a web seminar on the book on August 25, 2004 at 11 AM Philadelphia 
time (EDT=GMT-4).  For details see 
 http://insightful.com/news_events/webcasts/pharm04/heiberger.asp 

The seminar will be archived and available for download after the live presentation.



From spencer.graves at pdf.com  Wed Aug 18 16:27:49 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 18 Aug 2004 10:27:49 -0400
Subject: [R] all.equal and names? 
Message-ID: <41236765.9010504@pdf.com>

      How can I compare two objects for structure, names, values, etc.?  
With R 1.9.1 under Windows 2000, the obvious choice "all.equal" ignores 
names and compares only values: 

 > all.equal(1, c(a=1))
[1] TRUE

      Under S-Plus 6.2, I get the comparison I expected: 

 > all.equal(1, c(a = 1))
[1] "target, current classes differ: integer : 
named"                                                   
[2] "class of target is \"integer\", class of current is \"named\" 
(coercing current to class of target)"

      Thanks,
      Spencer Graves



From gabriel.serendip at hipernet.com.br  Wed Aug 18 16:36:17 2004
From: gabriel.serendip at hipernet.com.br (Gabriel Erbano)
Date: Wed, 18 Aug 2004 11:36:17 -0300
Subject: [R] Approximate Entropy
Message-ID: <BD48EFB1.69C3%gabriel.serendip@hipernet.com.br>

Greetings,

After looking at the site and googling about it, I still can't find if R can
calculate the approximate entropy (ApEn) statistic. Does anybody knows if
there's already such thing?

Thanks

Gabriel Erbano



From murdoch at stats.uwo.ca  Wed Aug 18 16:39:04 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 18 Aug 2004 10:39:04 -0400
Subject: [R] all.equal and names? 
In-Reply-To: <41236765.9010504@pdf.com>
References: <41236765.9010504@pdf.com>
Message-ID: <ncq6i0t1up1nh1ed203ea95mjq086knc7t@4ax.com>

On Wed, 18 Aug 2004 10:27:49 -0400, Spencer Graves
<spencer.graves at pdf.com> wrote :

>      How can I compare two objects for structure, names, values, etc.?  
>With R 1.9.1 under Windows 2000, the obvious choice "all.equal" ignores 
>names and compares only values: 
>
> > all.equal(1, c(a=1))
>[1] TRUE
>
>      Under S-Plus 6.2, I get the comparison I expected: 
>
> > all.equal(1, c(a = 1))
>[1] "target, current classes differ: integer : 
>named"                                                   
>[2] "class of target is \"integer\", class of current is \"named\" 
>(coercing current to class of target)"

If you want the explanation you're out of luck, but identical() does
the test:

> identical(1, c(a = 1))
[1] FALSE

Duncan Murdoch



From joanne.butler at acadiau.ca  Wed Aug 18 16:48:26 2004
From: joanne.butler at acadiau.ca (Joanne Butler)
Date: Wed, 18 Aug 2004 11:48:26 -0300
Subject: [R] downloading the R program
Message-ID: <E8016ACBF7C98F4690B42A7127A6A7F702DFB96E@exchange.ad.acadiau.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040818/b8c04aec/attachment.pl

From HankeA at mar.dfo-mpo.gc.ca  Wed Aug 18 16:51:03 2004
From: HankeA at mar.dfo-mpo.gc.ca (Hanke, Alex)
Date: Wed, 18 Aug 2004 11:51:03 -0300
Subject: [R] Xtable method for coxph, bug?
Message-ID: <E37EEC6DE3A0C5439B7E7B07406C24AE124A05@msgmarsta01.bio.dfo.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040818/54c1c551/attachment.pl

From ligges at statistik.uni-dortmund.de  Wed Aug 18 16:57:14 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 18 Aug 2004 16:57:14 +0200
Subject: [R] downloading the R program
In-Reply-To: <E8016ACBF7C98F4690B42A7127A6A7F702DFB96E@exchange.ad.acadiau.ca>
References: <E8016ACBF7C98F4690B42A7127A6A7F702DFB96E@exchange.ad.acadiau.ca>
Message-ID: <41236E4A.6080401@statistik.uni-dortmund.de>

Joanne Butler wrote:

> I am trying to download the R program, but am having trouble. I have
> read through the instructions, but do not seem to be able to do it
> properly. Can you tell me the step-by-step instructions?

Either look at http://cran.r-project.org/ or read the "R Installation 
and Administration" manual for more information, it is available at the 
page mentioned above.

Uwe Ligges


>  
> 
> Joanne L. Butler, Post-doctoral Fellow
> 
> Equity and Technology Research Project
> 
> c/o Department of Sociology
> 
> Acadia University
> 
> Wolfville, N.S.  B4P 2R6
> 
> phone (902) 585-1535
> 
> fax (902) 585-1769
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ramasamy at cancer.org.uk  Wed Aug 18 17:06:46 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Wed, 18 Aug 2004 16:06:46 +0100
Subject: [R] downloading the R program
In-Reply-To: <E8016ACBF7C98F4690B42A7127A6A7F702DFB96E@exchange.ad.acadiau.ca>
References: <E8016ACBF7C98F4690B42A7127A6A7F702DFB96E@exchange.ad.acadiau.ca>
Message-ID: <1092841606.4116.3.camel@vpn202001.lif.icnet.uk>

What difficulties, instruction and operating system are you talking
about ?

1. Go to www.r-project.org
2. Click on CRAN under downloads
3. Choose a mirror
4. Go to pre-compiled binaries and select your OS 
5. If windows, choose base and right click and save on the exe file



On Wed, 2004-08-18 at 15:48, Joanne Butler wrote:
> I am trying to download the R program, but am having trouble. I have
> read through the instructions, but do not seem to be able to do it
> properly. Can you tell me the step-by-step instructions?
> 
>  
> 
> Joanne L. Butler, Post-doctoral Fellow
> 
> Equity and Technology Research Project
> 
> c/o Department of Sociology
> 
> Acadia University
> 
> Wolfville, N.S.  B4P 2R6
> 
> phone (902) 585-1535
> 
> fax (902) 585-1769
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From partha_bagchi at hgsi.com  Wed Aug 18 17:06:33 2004
From: partha_bagchi at hgsi.com (partha_bagchi@hgsi.com)
Date: Wed, 18 Aug 2004 11:06:33 -0400
Subject: [R] downloading the R program
Message-ID: <OF6EE4D60D.C965D8D0-ON85256EF4.0052C203-85256EF4.0052FFBD@hgsi.com>

I am not sure which platform you are on. However, if you go to CRAN 
(cran.us.r-project.org/) you should be able to get step-by-step 
instruction. Please read the posting guide. It can answer a lot of 
questions for you.

HTH,
Partha





"Joanne Butler" <joanne.butler at acadiau.ca>
Sent by: r-help-bounces at stat.math.ethz.ch
08/18/2004 11:48 AM

 
        To:     <R-help at stat.math.ethz.ch>
        cc: 
        Subject:        [R] downloading the R program


I am trying to download the R program, but am having trouble. I have
read through the instructions, but do not seem to be able to do it
properly. Can you tell me the step-by-step instructions?



Joanne L. Butler, Post-doctoral Fellow

Equity and Technology Research Project

c/o Department of Sociology

Acadia University

Wolfville, N.S.  B4P 2R6

phone (902) 585-1535

fax (902) 585-1769




[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From merser at image.dk  Wed Aug 18 17:13:39 2004
From: merser at image.dk (=?iso-8859-1?Q?S=F8ren_Merser?=)
Date: Wed, 18 Aug 2004 17:13:39 +0200
Subject: [R] table and getting  rownames
References: <411A08E700000478@cpfe3.be.tisc.dk> <x2oelat00o.fsf@biostat.ku.dk>
Message-ID: <001b01c48538$9aa23140$8b00a8c0@IBM>

exactly what i needed
thanks a lot
soren

----- Original Message ----- 
From: "Peter Dalgaard" <p.dalgaard at biostat.ku.dk>
To: <merser at tiscali.dk>
Cc: "R-help" <r-help at stat.math.ethz.ch>
Sent: Tuesday, August 17, 2004 2:35 PM
Subject: Re: [R] table and getting rownames


> merser at tiscali.dk writes:
> 
> > hi there
> > say that i have this table
> > >x<-table(adoc, oarb)
> > >x
> >                oarb
> >                       0   1
> > adoc
> >     ab                1   0
> >     am                5   1
> >     ba               14   1
> >     cc              271   3
> >     ch               87   2
> >     dz              362   6
> >     fl                7   0
> >     fs               84   2
> > 
> > is there an easy way to get the row names or row numbers of rows with
> > oarb==0
> > i.e. (ab, fl) or (1, 7)
> 
> Something like
> 
> which(x[,"1"]==0)
> rownames(x)[x[,"1"]==0]
> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907
>



From tlumley at u.washington.edu  Wed Aug 18 17:47:00 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 18 Aug 2004 08:47:00 -0700 (PDT)
Subject: [R] survdiff
In-Reply-To: <x2acwtsayl.fsf@biostat.ku.dk>
References: <AOEIIMAIPOIICDBMGIEOCEBMCAAA.krista@aha.demon.nl>
	<x2acwtsayl.fsf@biostat.ku.dk>
Message-ID: <Pine.A41.4.58.0408180841550.199006@homer10.u.washington.edu>

On Tue, 17 Aug 2004, Peter Dalgaard wrote:

>
> You really need to read a theory book for this, but here's the basic idea:
>
> V is the theoretical variance of O-E for the first group. If O-E is
> approximately normally distributed, as it will be in large samples,
> then (O-E)^2/V will be approximately chi-squared distributed on 1 DF.
>
> In *other* models, notably those for contingency tables, the same idea
> works out as the familiar sum((O-E)^2/E) formula. That formula has
> historically been used for the logrank test too, and it still appears
> in some textbooks, but as it turns out, it is not actually correct
> (although often quite close).
>

You don't necessarily need a theory book --- sufficiently old biostat
textbooks may have this. For example, Fisher & van Belle (1993)
"Biostatistics: a methodology for the health sciences" gives both formulas
and explains that the simpler one is a useful approximation for hand
calculation, with a worked example.

Now that we have computers no-one needs to use the approximation, and most
of that information has been taken out of the second edition.

	-thomas



From tlumley at u.washington.edu  Wed Aug 18 17:51:11 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 18 Aug 2004 08:51:11 -0700 (PDT)
Subject: [R] Revert a factor to its numeric values
In-Reply-To: <20040818140420.GA21931@stat.umu.se>
References: <20040818140420.GA21931@stat.umu.se>
Message-ID: <Pine.A41.4.58.0408180849340.199006@homer10.u.washington.edu>

On Wed, 18 Aug 2004, G??ran Brostr??m wrote:

> I'm trying a recommendation on the help page for 'factor':
>
> > x <- c(1, 2, 1, 2)
> > x <- factor(x, labels = c("one", "two"))
> > x
> [1] one two one two
> Levels: one two
> > as.numeric(levels(x))[x]
> [1] NA NA NA NA
> Warning message:
> NAs introduced by coercion
>

usually when people want to "revert a factor to its numeric values" they
mean that the labels are numbers and they want those numbers.  In that
case as.numeric(x) or unclass(x) are wrong because they give you the
underlying codes.  You, somewhat unusually, actually want the underlying
codes, which you get with unclass(x).

	-thomas



From spencer.graves at pdf.com  Wed Aug 18 18:02:02 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 18 Aug 2004 12:02:02 -0400
Subject: [R] all.equal and names?
In-Reply-To: <ncq6i0t1up1nh1ed203ea95mjq086knc7t@4ax.com>
References: <41236765.9010504@pdf.com>
	<ncq6i0t1up1nh1ed203ea95mjq086knc7t@4ax.com>
Message-ID: <41237D7A.5070507@pdf.com>

Hi, Duncan: 

      Thanks much.  I think I remember reading about both "all.equal" 
and "identical" in Venables and Ripley (2002) MASS.  Unfortunately, I 
don't have MASS handy now, and I could not find it otherwise, so I asked. 

      What needs to happen to upgrade the "all.equal" documentation to 
add "identical" to the "see also"? 

      Best Wishes,
      Spencer

Duncan Murdoch wrote:

>On Wed, 18 Aug 2004 10:27:49 -0400, Spencer Graves
><spencer.graves at pdf.com> wrote :
>
>  
>
>>     How can I compare two objects for structure, names, values, etc.?  
>>With R 1.9.1 under Windows 2000, the obvious choice "all.equal" ignores 
>>names and compares only values: 
>>
>>    
>>
>>>all.equal(1, c(a=1))
>>>      
>>>
>>[1] TRUE
>>
>>     Under S-Plus 6.2, I get the comparison I expected: 
>>
>>    
>>
>>>all.equal(1, c(a = 1))
>>>      
>>>
>>[1] "target, current classes differ: integer : 
>>named"                                                   
>>[2] "class of target is \"integer\", class of current is \"named\" 
>>(coercing current to class of target)"
>>    
>>
>
>If you want the explanation you're out of luck, but identical() does
>the test:
>
>  
>
>>identical(1, c(a = 1))
>>    
>>
>[1] FALSE
>
>Duncan Murdoch
>  
>



From MSchwartz at MedAnalytics.com  Wed Aug 18 18:18:39 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Wed, 18 Aug 2004 11:18:39 -0500
Subject: [R] all.equal and names?
In-Reply-To: <41237D7A.5070507@pdf.com>
References: <41236765.9010504@pdf.com>
	<ncq6i0t1up1nh1ed203ea95mjq086knc7t@4ax.com> <41237D7A.5070507@pdf.com>
Message-ID: <1092845918.5904.14.camel@localhost.localdomain>

It is in the Description now (at least for 1.9.1 patched):

all.equal(x,y) is a utility to compare R objects x and y testing `near
equality'. If they are different, comparison is still made to some
extent, and a report of the differences is returned. Don't use all.equal
directly in if expressions"either use identical or combine the two, as
shown in the documentation for identical.

There is also a reference to:

attr.all.equal(target, current, ...)

on the same help page, which returns the following using the example:

> attr.all.equal(1, c(a=1))
[1] "names for current but not for target"

Not quite the same message as S-PLUS however.

HTH,

Marc


On Wed, 2004-08-18 at 11:02, Spencer Graves wrote:
> Hi, Duncan: 
> 
>       Thanks much.  I think I remember reading about both "all.equal" 
> and "identical" in Venables and Ripley (2002) MASS.  Unfortunately, I 
> don't have MASS handy now, and I could not find it otherwise, so I asked. 
> 
>       What needs to happen to upgrade the "all.equal" documentation to 
> add "identical" to the "see also"? 
> 
>       Best Wishes,
>       Spencer
> 
> Duncan Murdoch wrote:
> 
> >On Wed, 18 Aug 2004 10:27:49 -0400, Spencer Graves
> ><spencer.graves at pdf.com> wrote :
> >
> >  
> >
> >>     How can I compare two objects for structure, names, values, etc.?  
> >>With R 1.9.1 under Windows 2000, the obvious choice "all.equal" ignores 
> >>names and compares only values: 
> >>
> >>    
> >>
> >>>all.equal(1, c(a=1))
> >>>      
> >>>
> >>[1] TRUE
> >>
> >>     Under S-Plus 6.2, I get the comparison I expected: 
> >>
> >>    
> >>
> >>>all.equal(1, c(a = 1))
> >>>      
> >>>
> >>[1] "target, current classes differ: integer : 
> >>named"                                                   
> >>[2] "class of target is \"integer\", class of current is \"named\" 
> >>(coercing current to class of target)"
> >>    
> >>
> >
> >If you want the explanation you're out of luck, but identical() does
> >the test:
> >
> >  
> >
> >>identical(1, c(a = 1))
> >>    
> >>
> >[1] FALSE
> >
> >Duncan Murdoch
> >  
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From sgilpin at gmail.com  Wed Aug 18 19:59:40 2004
From: sgilpin at gmail.com (Scott Gilpin)
Date: Wed, 18 Aug 2004 11:59:40 -0600
Subject: [R] Memory Problems in R
Message-ID: <5739cc2f040818105940ba0a63@mail.gmail.com>

Hello everyone -

I have a couple of questions about memory management of large objects.
Thanks in advance for your response.

I'm running R version 1.9.1 on solaris 8, compiled as a 32 bit app. 
My system has 12.0 GB of memory, with usually ~ 11GB free.  I checked
system limits using ulimit, and there is nothing set that would limit
the maximum amount of memory for a process (with the exception of an
8MB stack size).  I've also checked the amount of memory available to
R using mem.limits(), and there is no limit set.

I'm running into two problems.  The first is the error  "cannot
allocate vector of size XXXXX" - I know this has been discussed
several times on this mailing list, but it usually seems the user does
not have enough memory on their system, or does not have the memory
limits set correctly.  I don't believe this is the case in this
situation.  I verified that I don't have any objects in memory when R
starts up, and that memory limits are set to NA.  Here is some output:

> ls()
character(0)
> mem.limits()
nsize vsize 
   NA    NA 
> gc()
         used (Mb) gc trigger (Mb)
Ncells 432197 11.6     531268 14.2
Vcells 116586  0.9     786432  6.0
> v<-rep(0,268435431)
Error: cannot allocate vector of size 2097151 Kb
> v<-rep(0,268435430)
> object.size(v)
[1] 2147483468
> gc()
            used   (Mb) gc trigger   (Mb)
Ncells    432214   11.6     741108   19.8
Vcells 268552029 2048.9  268939773 2051.9


Does R have a limit set on the size of an object that it will
allocate?  I know that the entire application will only be able to use
4GB of memory (because it's only 32bit), but I haven't found anything
in the R documentation or the help lists that indicates there is
maximum on the size of an object.  I understand there will be problems
if an object is greater than 2GB and needs to be copied - but will R
limit the creation of such an object?  It's also my understanding that
the garbage collector won't move objects and this may cause memory to
become fragmented - but I'm seeing these issues on startup when there
are no objects in memory.


My second problem is with matrices and the garbage collector, and the
limits it sets for gc trigger after a matrix is created.  When I
create a vector of approximately 500MB, R sets the gc trigger to be
slightly above this amount.  The gc trigger also seems to correspond
to the process size (as output by top).  When I create a matrix of
approximately 500MB, R sets the gc trigger to be roughly 3 times the
size of the matrix (and the process size is ~ 1.5GB).  Therefor, when
I try to create larger matrices, where 3x the size of the matrix is
greater than 4GB, R gives me an error.  Is there anything I can do to
create large matrices?  Or do I have to manipulate large objects as a
vector?

Output from the 3 different scenarios is below:

1) - can't create a matrix, but can create a vector

[Previously saved workspace restored]

> m<-matrix(rep(0,25000*10000),nrow=10000)
Error: cannot allocate vector of size 1953125 Kb
> v<-rep(0,25000*10000)
> object.size(v)/1024
[1] 1953125
> 

2) gc trigger is set slightly higher than the size of the vector

> ls()
character(0)
> mem.limits()
nsize vsize 
   NA    NA 
> gc()
         used (Mb) gc trigger (Mb)
Ncells 432197 11.6     531268 14.2
Vcells 116586  0.9     786432  6.0
> v<-rep(0,(2510)*(25000))
> object.size(v)
[1] 5.02e+08
> gc()
           used  (Mb) gc trigger  (Mb)
Ncells   432210  11.6     667722  17.9
Vcells 62866589 479.7   63247172 482.6
> 

3) gc trigger is set ~ 3x the size of the matrix

> ls()
character(0)
> mem.limits()
nsize vsize 
   NA    NA 
> gc()
         used (Mb) gc trigger (Mb)
Ncells 432197 11.6     531268 14.2
Vcells 116586  0.9     786432  6.0
> A<-matrix(rep(0,(2510)*(25000)),nrow=(2510),ncol=(25000))
> object.size(A)
[1] 502000120
> gc()
           used  (Mb) gc trigger   (Mb)
Ncells   432213  11.6     741108   19.8
Vcells 62866590 479.7  188640940 1439.3
>



From rpeng at jhsph.edu  Wed Aug 18 20:10:43 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Wed, 18 Aug 2004 14:10:43 -0400
Subject: [R] Memory Problems in R
In-Reply-To: <5739cc2f040818105940ba0a63@mail.gmail.com>
References: <5739cc2f040818105940ba0a63@mail.gmail.com>
Message-ID: <41239BA3.50502@jhsph.edu>

There is a limit on how long a single vector can be, and I think it's 
2GB (even on 64-bit platforms).  Not sure on how the gc trigger is set....

-roger

Scott Gilpin wrote:
> Hello everyone -
> 
> I have a couple of questions about memory management of large objects.
> Thanks in advance for your response.
> 
> I'm running R version 1.9.1 on solaris 8, compiled as a 32 bit app. 
> My system has 12.0 GB of memory, with usually ~ 11GB free.  I checked
> system limits using ulimit, and there is nothing set that would limit
> the maximum amount of memory for a process (with the exception of an
> 8MB stack size).  I've also checked the amount of memory available to
> R using mem.limits(), and there is no limit set.
> 
> I'm running into two problems.  The first is the error  "cannot
> allocate vector of size XXXXX" - I know this has been discussed
> several times on this mailing list, but it usually seems the user does
> not have enough memory on their system, or does not have the memory
> limits set correctly.  I don't believe this is the case in this
> situation.  I verified that I don't have any objects in memory when R
> starts up, and that memory limits are set to NA.  Here is some output:
> 
> 
>>ls()
> 
> character(0)
> 
>>mem.limits()
> 
> nsize vsize 
>    NA    NA 
> 
>>gc()
> 
>          used (Mb) gc trigger (Mb)
> Ncells 432197 11.6     531268 14.2
> Vcells 116586  0.9     786432  6.0
> 
>>v<-rep(0,268435431)
> 
> Error: cannot allocate vector of size 2097151 Kb
> 
>>v<-rep(0,268435430)
>>object.size(v)
> 
> [1] 2147483468
> 
>>gc()
> 
>             used   (Mb) gc trigger   (Mb)
> Ncells    432214   11.6     741108   19.8
> Vcells 268552029 2048.9  268939773 2051.9
> 
> 
> Does R have a limit set on the size of an object that it will
> allocate?  I know that the entire application will only be able to use
> 4GB of memory (because it's only 32bit), but I haven't found anything
> in the R documentation or the help lists that indicates there is
> maximum on the size of an object.  I understand there will be problems
> if an object is greater than 2GB and needs to be copied - but will R
> limit the creation of such an object?  It's also my understanding that
> the garbage collector won't move objects and this may cause memory to
> become fragmented - but I'm seeing these issues on startup when there
> are no objects in memory.
> 
> 
> My second problem is with matrices and the garbage collector, and the
> limits it sets for gc trigger after a matrix is created.  When I
> create a vector of approximately 500MB, R sets the gc trigger to be
> slightly above this amount.  The gc trigger also seems to correspond
> to the process size (as output by top).  When I create a matrix of
> approximately 500MB, R sets the gc trigger to be roughly 3 times the
> size of the matrix (and the process size is ~ 1.5GB).  Therefor, when
> I try to create larger matrices, where 3x the size of the matrix is
> greater than 4GB, R gives me an error.  Is there anything I can do to
> create large matrices?  Or do I have to manipulate large objects as a
> vector?
> 
> Output from the 3 different scenarios is below:
> 
> 1) - can't create a matrix, but can create a vector
> 
> [Previously saved workspace restored]
> 
> 
>>m<-matrix(rep(0,25000*10000),nrow=10000)
> 
> Error: cannot allocate vector of size 1953125 Kb
> 
>>v<-rep(0,25000*10000)
>>object.size(v)/1024
> 
> [1] 1953125
> 
> 
> 2) gc trigger is set slightly higher than the size of the vector
> 
> 
>>ls()
> 
> character(0)
> 
>>mem.limits()
> 
> nsize vsize 
>    NA    NA 
> 
>>gc()
> 
>          used (Mb) gc trigger (Mb)
> Ncells 432197 11.6     531268 14.2
> Vcells 116586  0.9     786432  6.0
> 
>>v<-rep(0,(2510)*(25000))
>>object.size(v)
> 
> [1] 5.02e+08
> 
>>gc()
> 
>            used  (Mb) gc trigger  (Mb)
> Ncells   432210  11.6     667722  17.9
> Vcells 62866589 479.7   63247172 482.6
> 
> 
> 3) gc trigger is set ~ 3x the size of the matrix
> 
> 
>>ls()
> 
> character(0)
> 
>>mem.limits()
> 
> nsize vsize 
>    NA    NA 
> 
>>gc()
> 
>          used (Mb) gc trigger (Mb)
> Ncells 432197 11.6     531268 14.2
> Vcells 116586  0.9     786432  6.0
> 
>>A<-matrix(rep(0,(2510)*(25000)),nrow=(2510),ncol=(25000))
>>object.size(A)
> 
> [1] 502000120
> 
>>gc()
> 
>            used  (Mb) gc trigger   (Mb)
> Ncells   432213  11.6     741108   19.8
> Vcells 62866590 479.7  188640940 1439.3
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Wed Aug 18 20:45:54 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 18 Aug 2004 19:45:54 +0100 (BST)
Subject: [R] Memory Problems in R
In-Reply-To: <41239BA3.50502@jhsph.edu>
Message-ID: <Pine.LNX.4.44.0408181917370.22124-100000@gannet.stats>

On Wed, 18 Aug 2004, Roger D. Peng wrote:

> There is a limit on how long a single vector can be, and I think it's 
> 2GB (even on 64-bit platforms).  Not sure on how the gc trigger is set....

There is a limit of R_SIZE_T_MAX bytes, but that is defined as ULONG_MAX
which should be 4GB-1 on a 32-bit platform, and much more on a 64-bit
platform.

The example works on a 64-bit platform, which demonstrates that there is
no 2GB limit there.

If you hit the length limit, the message is of the form

	cannot allocate vector of length ...

Looking at the code in memory.c it seems that

	    if (size >= (LONG_MAX / sizeof(VECREC)) - sizeof(SEXPREC_ALIGN) ||
		(s = malloc(sizeof(SEXPREC_ALIGN) + size * sizeof(VECREC)))
		== NULL) {
		/* reset the vector heap limit */
		R_VSize = old_R_VSize;
		errorcall(R_NilValue, "cannot allocate vector of size %lu Kb",
			  (size * sizeof(VECREC))/1024);
	    }

has a limit of LONG_MAX bytes for a vector.  I think that is 
unintentional, and you might like to try ULONG_MAX there and re-compile.
But it really doesn't make much difference as there is very little you can 
do with an object taking up more than half the maximum memory size
except access bits of it (and that is what DBMSes are for).


A few comments:

1) Of course R does have objects in memory, 12.5Mb of them according to 
gc.  You are not starting with a clean slate.  Hopefully malloc has 
allocated them in a compact group.

2) Solaris has been a 64-bit OS for at least 7 years and you really should
be using a 64-bit build of R if you plan on exceeding 1Gb.

3) To create a matrix efficiently, create a vector and assign a dim.  I
gave an example on R-help yesterday, so please check the archives.

matrix() makes a copy of the data and so needs double the space you are
thinking it does.  Take a look at the source code:

    PROTECT(snr = allocMatrix(TYPEOF(vals), nr, nc));
    if(lendat) {
	if (isVector(vals))
	    copyMatrix(snr, vals, byrow);
	else
	    copyListMatrix(snr, vals, byrow);

4) The source code is the documentation here.  I suspect no one person 
knows all the details.


> Scott Gilpin wrote:
> > Hello everyone -
> > 
> > I have a couple of questions about memory management of large objects.
> > Thanks in advance for your response.
> > 
> > I'm running R version 1.9.1 on solaris 8, compiled as a 32 bit app. 
> > My system has 12.0 GB of memory, with usually ~ 11GB free.  I checked
> > system limits using ulimit, and there is nothing set that would limit
> > the maximum amount of memory for a process (with the exception of an
> > 8MB stack size).  I've also checked the amount of memory available to
> > R using mem.limits(), and there is no limit set.
> > 
> > I'm running into two problems.  The first is the error  "cannot
> > allocate vector of size XXXXX" - I know this has been discussed
> > several times on this mailing list, but it usually seems the user does
> > not have enough memory on their system, or does not have the memory
> > limits set correctly.  I don't believe this is the case in this
> > situation.  I verified that I don't have any objects in memory when R
> > starts up, and that memory limits are set to NA.  Here is some output:
> > 
> > 
> >>ls()
> > 
> > character(0)
> > 
> >>mem.limits()
> > 
> > nsize vsize 
> >    NA    NA 
> > 
> >>gc()
> > 
> >          used (Mb) gc trigger (Mb)
> > Ncells 432197 11.6     531268 14.2
> > Vcells 116586  0.9     786432  6.0
> > 
> >>v<-rep(0,268435431)
> > 
> > Error: cannot allocate vector of size 2097151 Kb
> > 
> >>v<-rep(0,268435430)
> >>object.size(v)
> > 
> > [1] 2147483468
> > 
> >>gc()
> > 
> >             used   (Mb) gc trigger   (Mb)
> > Ncells    432214   11.6     741108   19.8
> > Vcells 268552029 2048.9  268939773 2051.9
> > 
> > 
> > Does R have a limit set on the size of an object that it will
> > allocate?  I know that the entire application will only be able to use
> > 4GB of memory (because it's only 32bit), but I haven't found anything
> > in the R documentation or the help lists that indicates there is
> > maximum on the size of an object.  I understand there will be problems
> > if an object is greater than 2GB and needs to be copied - but will R
> > limit the creation of such an object?  It's also my understanding that
> > the garbage collector won't move objects and this may cause memory to
> > become fragmented - but I'm seeing these issues on startup when there
> > are no objects in memory.
> > 
> > 
> > My second problem is with matrices and the garbage collector, and the
> > limits it sets for gc trigger after a matrix is created.  When I
> > create a vector of approximately 500MB, R sets the gc trigger to be
> > slightly above this amount.  The gc trigger also seems to correspond
> > to the process size (as output by top).  When I create a matrix of
> > approximately 500MB, R sets the gc trigger to be roughly 3 times the
> > size of the matrix (and the process size is ~ 1.5GB).  Therefor, when
> > I try to create larger matrices, where 3x the size of the matrix is
> > greater than 4GB, R gives me an error.  Is there anything I can do to
> > create large matrices?  Or do I have to manipulate large objects as a
> > vector?
> > 
> > Output from the 3 different scenarios is below:
> > 
> > 1) - can't create a matrix, but can create a vector
> > 
> > [Previously saved workspace restored]
> > 
> > 
> >>m<-matrix(rep(0,25000*10000),nrow=10000)
> > 
> > Error: cannot allocate vector of size 1953125 Kb
> > 
> >>v<-rep(0,25000*10000)
> >>object.size(v)/1024
> > 
> > [1] 1953125
> > 
> > 
> > 2) gc trigger is set slightly higher than the size of the vector
> > 
> > 
> >>ls()
> > 
> > character(0)
> > 
> >>mem.limits()
> > 
> > nsize vsize 
> >    NA    NA 
> > 
> >>gc()
> > 
> >          used (Mb) gc trigger (Mb)
> > Ncells 432197 11.6     531268 14.2
> > Vcells 116586  0.9     786432  6.0
> > 
> >>v<-rep(0,(2510)*(25000))
> >>object.size(v)
> > 
> > [1] 5.02e+08
> > 
> >>gc()
> > 
> >            used  (Mb) gc trigger  (Mb)
> > Ncells   432210  11.6     667722  17.9
> > Vcells 62866589 479.7   63247172 482.6
> > 
> > 
> > 3) gc trigger is set ~ 3x the size of the matrix
> > 
> > 
> >>ls()
> > 
> > character(0)
> > 
> >>mem.limits()
> > 
> > nsize vsize 
> >    NA    NA 
> > 
> >>gc()
> > 
> >          used (Mb) gc trigger (Mb)
> > Ncells 432197 11.6     531268 14.2
> > Vcells 116586  0.9     786432  6.0
> > 
> >>A<-matrix(rep(0,(2510)*(25000)),nrow=(2510),ncol=(25000))
> >>object.size(A)
> > 
> > [1] 502000120
> > 
> >>gc()
> > 
> >            used  (Mb) gc trigger   (Mb)
> > Ncells   432213  11.6     741108   19.8
> > Vcells 62866590 479.7  188640940 1439.3
> > 
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rog at stanford.edu  Wed Aug 18 20:51:44 2004
From: rog at stanford.edu (Roger Levy)
Date: 18 Aug 2004 11:51:44 -0700
Subject: [R] labeled break statements in R?
Message-ID: <258yccz3cv.fsf@stanford.edu>

Hi,

Are there labeled break statements in R?  i.e., something along the
lines of

TOPLOOP: for(i in 1:m) {
  for(j in 1:n) {
    ...
    if(condition) {
       break TOPLOOP
    }
  }
}

Thanks,

Roger



From andy_liaw at merck.com  Wed Aug 18 20:57:29 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 18 Aug 2004 14:57:29 -0400
Subject: [R] Does anybody runs R on the hp ML 370 or ML570 servers?
Message-ID: <3A822319EB35174CA3714066D590DCD504AF823C@usrymx25.merck.com>

> From: Prof Brian Ripley
> 
> On Mon, 16 Aug 2004, Li, Aiguo (NIH/NCI) wrote:
> 
> > I am trying to buy a hp server to run R and to complete 
> some other tasks
> > with limited bugets.  The r-project.org site recommended 
> that R will run on
> > hppa-hp-hpux.  
> 
> I don't think they are _recommended_ there.
> 
> > However this system is out of our buget and ML system from hp
> > is much cheaper.  Is there anybody running R on ML370 or ML 
> 570 systems?  If
> > you can provide me with some other information related that will be
> > appreciated.
> 
> Those are just Xeon processors in a standard Wintel box.  What OS?
> (They run Linux, Solaris, SCO Unix, Windows ....)

I believe one of ours is a ML530 (dual Xeon 2.4 GHz).  We've been putting
Mandrake on it (currently 9.0; have not had a chance to upgrade yet).  Had
no problem running/compiling R at all.

For a limited budget, IMHO these are hardly a good choice.  We got our first
dual Opterons with twice the RAM (but IDE instead of SCSI HDD) at around the
same price as the Compaq/HP, and that Opteron-based box will run rings
around the Compaq.

Andy

 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From glover at math.ubc.ca  Wed Aug 18 21:09:42 2004
From: glover at math.ubc.ca (Clive Glover)
Date: Wed, 18 Aug 2004 12:09:42 -0700 (PDT)
Subject: [R] calling R from Perl
Message-ID: <Pine.GSO.4.56.0408181206290.24705@pascal.math.ubc.ca>

Hello,

I am trying to call R from Perl running on Windows 2000.  I have looked
through the previous posts regarding running R from Perl and all have
referred to the RSPerl package at Omegahat.  Unfortunately the
documentation for this package specifically states that it only works in
Unix at the moment.  Does anyone else have any suggestions about the best
way to do this in the Windows environment?

Thanks

Clive



From ripley at stats.ox.ac.uk  Wed Aug 18 21:09:59 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 18 Aug 2004 20:09:59 +0100 (BST)
Subject: [R] labeled break statements in R?
In-Reply-To: <258yccz3cv.fsf@stanford.edu>
Message-ID: <Pine.LNX.4.44.0408182006090.25449-100000@gannet.stats>

On 18 Aug 2004, Roger Levy wrote:

> Are there labeled break statements in R?  i.e., something along the
> lines of
> 
> TOPLOOP: for(i in 1:m) {
>   for(j in 1:n) {
>     ...
>     if(condition) {
>        break TOPLOOP
>     }
>   }
> }

No, but if you find yourself using nested for loops it is very likely that 
you are not thinking in the right way for a vector language.

R does have a `R Language Definition' manual.  It's long been an
unfinished draft but it is not so incomplete as to omit reserved words.
It and the S reference books are the places to research questions like 
this.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From cliff at ms.washington.edu  Wed Aug 18 21:16:21 2004
From: cliff at ms.washington.edu (Cliff Lunneborg)
Date: Wed, 18 Aug 2004 12:16:21 -0700
Subject: [R]...Why social scientists don't use R 
Message-ID: <006b01c48557$d90970a0$6401a8c0@C56909A>

Berton Gunter has written in part:

> A few comments:

> First, your remarks are interesting and, I would say, mainly well
founded. However, I think they > are in many respects irrelevant,
although they do point to the much bigger underlying issue,
> which Roger Peng also hinted at in his reply.

> I think they are sensible because R IS difficult; the documentation is
often challenging, which is
> not surprising given (a) the inherent complexity of R; (b) the
difficulty in writing good
> documentation, especially when many of the functions being documented
are inherently
> technical, so subject matter knowledge (CS, statistics, numerical
analysis ,...) must be
> assumed;

My experience has been that the real challenge is not understanding the
documentation, but  finding it. Once I know the names of one or more
candidate functions I am happily on my way. One of the delights of
reading r-help is that one keeps discovering useful functions. In the
best of all possible worlds I could ask an intelligent agent to summon
up the k-nearest neighbor functions that would "do X." Not likely. Years
ago StatSci Europe published a handy little "Complete Listing of S-PLUS
Functions", categorized in some way. I found it useful. Something
similar for R would not go amiss. I know, it would want to be 420 pages
rather than 42.

**********************************************************
Cliff Lunneborg, Professor Emeritus, Statistics &
Psychology, University of Washington, Seattle
cliff at ms.washington.edu



From murdoch at stats.uwo.ca  Wed Aug 18 21:39:49 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 18 Aug 2004 15:39:49 -0400
Subject: [R]...Why social scientists don't use R 
In-Reply-To: <006b01c48557$d90970a0$6401a8c0@C56909A>
References: <006b01c48557$d90970a0$6401a8c0@C56909A>
Message-ID: <heb7i0tr27fijgog828t9e6ikv0vh1jov2@4ax.com>

On Wed, 18 Aug 2004 12:16:21 -0700, "Cliff Lunneborg"
<cliff at ms.washington.edu> wrote :

> Years
>ago StatSci Europe published a handy little "Complete Listing of S-PLUS
>Functions", categorized in some way. I found it useful. Something
>similar for R would not go amiss. I know, it would want to be 420 pages
>rather than 42.

The R Reference manual does this for the base packages.  The HTML help
pages come sort of close for other packages, though they're on a
separate page for each package.  If you really want it all in one
place, it would presumably be fairly easy to modify the code that
produces those and have everything appear on one big page, or write a
script that glued together all the <RHOME>/library/html/00Index.html
files.

Duncan Murdoch



From ripley at stats.ox.ac.uk  Wed Aug 18 21:40:12 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 18 Aug 2004 20:40:12 +0100 (BST)
Subject: [R]...Why social scientists don't use R 
In-Reply-To: <006b01c48557$d90970a0$6401a8c0@C56909A>
Message-ID: <Pine.LNX.4.44.0408182032450.27905-100000@gannet.stats>

On Wed, 18 Aug 2004, Cliff Lunneborg wrote:

> Berton Gunter has written in part:
> 
> > A few comments:
> 
> > First, your remarks are interesting and, I would say, mainly well
> founded. However, I think they > are in many respects irrelevant,
> although they do point to the much bigger underlying issue,
> > which Roger Peng also hinted at in his reply.
> 
> > I think they are sensible because R IS difficult; the documentation is
> often challenging, which is
> > not surprising given (a) the inherent complexity of R; (b) the
> difficulty in writing good
> > documentation, especially when many of the functions being documented
> are inherently
> > technical, so subject matter knowledge (CS, statistics, numerical
> analysis ,...) must be
> > assumed;
> 
> My experience has been that the real challenge is not understanding the
> documentation, but  finding it. Once I know the names of one or more
> candidate functions I am happily on my way. One of the delights of
> reading r-help is that one keeps discovering useful functions. In the
> best of all possible worlds I could ask an intelligent agent to summon
> up the k-nearest neighbor functions that would "do X." Not likely. 

help.search does a better job than it is given credit for.

> Years ago StatSci Europe published a handy little "Complete Listing of
> S-PLUS Functions", categorized in some way. I found it useful. Something
> similar for R would not go amiss. I know, it would want to be 420 pages
> rather than 42.

What is R in this context?  There are several hundred addons on CRAN, BioC
and elsewhere.  R's HTML search or help.search will give you a complete
listing over installed packages by `keyword', which is what the `Complete
Listing of S-PLUS Functions' I saw was about.

Windows users should try the full-text searches in CHM help, especially 
for package stats.

The problem is to know what to search for.  To pick a recent example,
to use `logistic-normal model' for a random-intercept GLMM is not going to 
work, but Googling will usually bring up synonyms.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sdavis2 at mail.nih.gov  Wed Aug 18 21:44:04 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Wed, 18 Aug 2004 15:44:04 -0400
Subject: [R] calling R from Perl
In-Reply-To: <Pine.GSO.4.56.0408181206290.24705@pascal.math.ubc.ca>
References: <Pine.GSO.4.56.0408181206290.24705@pascal.math.ubc.ca>
Message-ID: <F63886D6-F14E-11D8-B46E-000A95D7BA10@mail.nih.gov>

Clive,

Have a look at Statistics::R 
(http://search.cpan.org/~gmpassos/Statistics-R-0.02).  I'm not sure if 
it works well with Windows, but it is the only other option that I know 
of to work directly with the R-interpreter.  However, you can always 
create a batch file and write it to a file and then call R.

Sean

On Aug 18, 2004, at 3:09 PM, Clive Glover wrote:

> Hello,
>
> I am trying to call R from Perl running on Windows 2000.  I have looked
> through the previous posts regarding running R from Perl and all have
> referred to the RSPerl package at Omegahat.  Unfortunately the
> documentation for this package specifically states that it only works 
> in
> Unix at the moment.  Does anyone else have any suggestions about the 
> best
> way to do this in the Windows environment?
>
> Thanks
>
> Clive
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From baron at psych.upenn.edu  Wed Aug 18 21:50:30 2004
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Wed, 18 Aug 2004 15:50:30 -0400
Subject: [R]...Why social scientists don't use R
In-Reply-To: <006b01c48557$d90970a0$6401a8c0@C56909A>
References: <006b01c48557$d90970a0$6401a8c0@C56909A>
Message-ID: <20040818195029.GA22000@psych>

On 08/18/04 12:16, Cliff Lunneborg wrote:
>My experience has been that the real challenge is not understanding the
>documentation, but  finding it. Once I know the names of one or more
>candidate functions I am happily on my way. One of the delights of
>reading r-help is that one keeps discovering useful functions. In the
>best of all possible worlds I could ask an intelligent agent to summon
>up the k-nearest neighbor functions that would "do X."

I have found the HtDig search engine at my site (accessible
through "Search" on the left side of the main R page, or directly
as http://finzi.psych.upenn.edu) to be pretty useful in this
regard, although it is a long way from artificial intelligence,
which would recognize similar meanings.

It fails for me mostly when different disciplines have different
names for the same thing.  (Economists hate to admit that many of
the statistical ideas they use were invented/discovered by
psychologists.)

That said, I'm thinking of switching to the Xapian search engine
(http://www.redhat.com/archives/fedora-devel-list/2004-July/msg01576.html),
and I would welcome any opinions about it.  HtDig is a pain; only
one version of it (an old one) seems to work on Fedora Core 2,
and it now takes almost 10 hours to update each month on a very
fast computer (Pentium 4 2.80GHz with Serial ATA disk
controller).

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron



From biocperi at yahoo.com  Wed Aug 18 23:07:07 2004
From: biocperi at yahoo.com (S Peri)
Date: Wed, 18 Aug 2004 14:07:07 -0700 (PDT)
Subject: [R] Incremental operator
In-Reply-To: <Pine.GSO.4.56.0408181206290.24705@pascal.math.ubc.ca>
Message-ID: <20040818210707.51154.qmail@web50007.mail.yahoo.com>

Hi group, 
 I am trying to get the LocusID numbers from my affy
expression matrix.  

I instantiated rownames function to get an object with
all the probe IDs.

> where.affy.at <- rownames(gliexp)

Now I wanted to get another object with the LocusIDs
in it like the following.  However, I get the
following error. How come i = i +1 is not considered
as incrementation here. I know there is some trouble
in defining. I come from Python background so I am
stuck. Could any one help me please. 


>for (i in where.affy.at){
+   gene.locusid.affy <- get(i,env= hgu95av2LOCUSID)
+ i = i + 1
+ gene.locusid.affy
+ }
Error in i + 1 : non-numeric argument to binary
operator


Thank you. 
PS



From ihok at hotmail.com  Wed Aug 18 23:12:18 2004
From: ihok at hotmail.com (Jack Tanner)
Date: Wed, 18 Aug 2004 17:12:18 -0400
Subject: [R] paired t-test vs pairwise t-test
Message-ID: <BAY22-F222ltpNebT9s00066712@hotmail.com>

What's the difference between t.test(x, y) and pairwise.t.test()? Is it just 
that the former takes two vectors, whereas the latter takes a vector and a 
factor?



From ripley at stats.ox.ac.uk  Wed Aug 18 23:29:06 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 18 Aug 2004 22:29:06 +0100 (BST)
Subject: [R] Incremental operator
In-Reply-To: <20040818210707.51154.qmail@web50007.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0408182224270.13300-100000@gannet.stats>

What is gliexp?  str(gliexp) would have been useful information to give.

Assuming it is an R matrix, the rownames are _names_, not numbers.
Surely in all languages with for loops it is bad idea to manipulate
the loop index inside the loop, but you definitely cannot do
arithmetic on character strings.

R does have debugging facilities and it would be a good idea to get
used to them.

options(error=dump.frames)
... some work ....
debugger()
 choose an environment
 i -- will tell you what it is.

On Wed, 18 Aug 2004, S Peri wrote:

>  I am trying to get the LocusID numbers from my affy
> expression matrix.  
> 
> I instantiated rownames function to get an object with
> all the probe IDs.
> 
> > where.affy.at <- rownames(gliexp)
> 
> Now I wanted to get another object with the LocusIDs
> in it like the following.  However, I get the
> following error. How come i = i +1 is not considered
> as incrementation here. I know there is some trouble
> in defining. I come from Python background so I am
> stuck. Could any one help me please. 
> 
> 
> >for (i in where.affy.at){
> +   gene.locusid.affy <- get(i,env= hgu95av2LOCUSID)
> + i = i + 1
> + gene.locusid.affy
> + }
> Error in i + 1 : non-numeric argument to binary
> operator

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Wed Aug 18 23:35:30 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Aug 2004 23:35:30 +0200
Subject: [R] paired t-test vs pairwise t-test
In-Reply-To: <BAY22-F222ltpNebT9s00066712@hotmail.com>
References: <BAY22-F222ltpNebT9s00066712@hotmail.com>
Message-ID: <x2d61oyvrx.fsf@biostat.ku.dk>

"Jack Tanner" <ihok at hotmail.com> writes:

> What's the difference between t.test(x, y) and pairwise.t.test()? Is
> it just that the former takes two vectors, whereas the latter takes a
> vector and a factor?

No. 

You might try reading the help pages and run the examples there...

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From mhassan at scitegic.com  Wed Aug 18 23:40:25 2004
From: mhassan at scitegic.com (Moises Hassan)
Date: Wed, 18 Aug 2004 14:40:25 -0700
Subject: [R] distance to cluster center
Message-ID: <830D8D4719112B418ABBC3A0EBA9581272EAD9@webmail.scitegic.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040818/63d69e16/attachment.pl

From nov_tao at yahoo.com  Wed Aug 18 23:42:57 2004
From: nov_tao at yahoo.com (Y C Tao)
Date: Wed, 18 Aug 2004 14:42:57 -0700 (PDT)
Subject: [R] header line generated write.table
Message-ID: <20040818214257.1319.qmail@web53505.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040818/93c62a83/attachment.pl

From edd at debian.org  Wed Aug 18 23:47:41 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 18 Aug 2004 16:47:41 -0500
Subject: [R] calling R from Perl
In-Reply-To: <F63886D6-F14E-11D8-B46E-000A95D7BA10@mail.nih.gov>
References: <Pine.GSO.4.56.0408181206290.24705@pascal.math.ubc.ca>
	<F63886D6-F14E-11D8-B46E-000A95D7BA10@mail.nih.gov>
Message-ID: <20040818214741.GA15328@sonny.eddelbuettel.com>

On Wed, Aug 18, 2004 at 03:44:04PM -0400, Sean Davis wrote:
> Clive,
> 
> Have a look at Statistics::R 
> (http://search.cpan.org/~gmpassos/Statistics-R-0.02).  I'm not sure if 
> it works well with Windows, but it is the only other option that I know 
> of to work directly with the R-interpreter.  However, you can always 
> create a batch file and write it to a file and then call R.

Or you can pipe to R from Perl, and collect the results from the same
(two-way) pipe.  Tedious to write, but reliable once setup.  Should work on
Windoze too.

For a concrete example, here is a part of something I did last month:

my $pid = open2(*READ_R, *WRITE_R, 'R', '--slave', '--silent');
print WRITE_R "
stopifnot(require(reposTools, quiet=TRUE))
[... more R commands ...]
q('no')
";
while ($line = <READ_R>) {
  chomp $line;
  print "[$line]\n" if $debug;
  [... more Perl commands ...]  
}
close(WRITE_R);
close(READ_R);


You need to be careful about quoting: I used outer "" and inner '' for
strings which works. You also need to escape the $ used to access data.frame
elements, or Perl will try to expand it.

Data transfer to and fro is a bit an issue -- here I simply printed as
csv-style file to stdout, and read it in Perl.  Quick and dirty.

Statistics::R is much closer to this paradigm than to RSPerl.  RSPerl is
promising, but, alas, like so many things on omegahat not quite finished.

Dirk

> 
> Sean
> 
> On Aug 18, 2004, at 3:09 PM, Clive Glover wrote:
> 
> >Hello,
> >
> >I am trying to call R from Perl running on Windows 2000.  I have looked
> >through the previous posts regarding running R from Perl and all have
> >referred to the RSPerl package at Omegahat.  Unfortunately the
> >documentation for this package specifically states that it only works 
> >in
> >Unix at the moment.  Does anyone else have any suggestions about the 
> >best
> >way to do this in the Windows environment?
> >
> >Thanks
> >
> >Clive
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! 
> >http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx



From anafava at uiuc.edu  Wed Aug 18 23:54:27 2004
From: anafava at uiuc.edu (anafava@uiuc.edu)
Date: Wed, 18 Aug 2004 16:54:27 -0500
Subject: [R] Gee
Message-ID: <aa4e647b.71fcfcae.12746100@expms2.cites.uiuc.edu>

I am trying to learn the gee function in R. So I try to 
generate some data and use this function. I have the 
following lines:

######################################## Gee

# Generating lny=10+2*Si-Si^2+eta
# eta ~ N(0,1)
# Si ~ U(0,11)

eta <- vector(mode="numeric",100)
eta <- rnorm(100)

Si <- vector(mode="numeric",100)
Si <- runif(100, min=0, max=11)

lny <- vector(mode="numeric",100)
lny <- 10+2*Si-Si^2+eta

id <- vector(mode="numeric",100)
id <- (1:100)


cons <- vector(mode="numeric",100)
for(i in 1:100) {
cons[i] <- 1
}

Si2 <- vector(mode="numeric",100)
for(j in 1:100) {
Si2[j] <- Si[j]^2
}

geedat <- data.frame(Si=Si,Si2=Si2,lny=lny,id=id,cons=cons)

lnyhat <- gee(lny~cons+Si-Si2, id=id, data=geedat, na.action)

And I received the following error message:

[1] "Beginning Cgee S-function, @(#) geeformula.q 4.13 
98/01/27"
Error in "[.data.frame"(structure(list(lny = c
(9.92388214744737, 2.54332321404939,  : 
        invalid subscript type

I don't know what I am doing wrong. May someone help me?

Thanks
Ana



From MSchwartz at MedAnalytics.com  Thu Aug 19 00:09:14 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Wed, 18 Aug 2004 17:09:14 -0500
Subject: [R] header line generated write.table
In-Reply-To: <20040818214257.1319.qmail@web53505.mail.yahoo.com>
References: <20040818214257.1319.qmail@web53505.mail.yahoo.com>
Message-ID: <1092866954.5904.44.camel@localhost.localdomain>

On Wed, 2004-08-18 at 16:42, Y C Tao wrote:
> I want to write following data frame into a CSV file:
>  
>           Col1   Col2  Col3
> Row1   1       1     1
> Row2   2       2     2
>  
> where Row1, Row2 are the row names and Col1, Col2, Col3 are the column
> names.
>  
> The correct CSV file should be:
> ,"Col1","Col2","Col3"
> Row1,1,1,1
> Row2,2,2,2
>  
> However, the one generated by R using write.table(x, file="xyz.csv",
> sep=",") has a header line that reads:
> "Col1","Col2","Col3"
> without the comma at the very beginning.
>  
> As a result, if you open the file in Excel, the column names are not
> correct (shifted to the left by one column).
>  
> Is there a way to get around this?
>  
> Thanks!

The solution is on the help page for ?write.table:

Details

Normally there is no column name for a column of row names. If
col.names=NA a blank column name is added. This can be used to write CSV
files for input to spreadsheets.


Also, the first example on that page gives you:

## To write a CSV file for input to Excel one might use
write.table(x, file = "foo.csv", sep = ",", col.names = NA)


Thus:

> write.table(x, col.names = NA, sep = ",")
"","Col1","Col2","Col3"
"Row1",1,1,1
"Row2",2,2,2

HTH,

Marc Schwartz



From seavey at cs.wisc.edu  Thu Aug 19 00:43:22 2004
From: seavey at cs.wisc.edu (Beverly Seavey)
Date: Wed, 18 Aug 2004 17:43:22 -0500 (CDT)
Subject: [R] How do I add rows to a table?
Message-ID: <Pine.GSO.4.58.0408181741001.15081@nova5.cs.wisc.edu>



 if I read from 1 file:

    inp1 <- scan("data1",list(0,0))
    inp2 <- scan("data2",list(0,0))

    allInp <- c(inp1,inp2)

 I get a table with 4 columns.

 How can I get a table with 2 columns and more rows?



From Kevin.Wang at maths.anu.edu.au  Thu Aug 19 00:53:55 2004
From: Kevin.Wang at maths.anu.edu.au (Kevin Wang)
Date: Thu, 19 Aug 2004 08:53:55 +1000 (EST)
Subject: [R] How do I add rows to a table?
In-Reply-To: <Pine.GSO.4.58.0408181741001.15081@nova5.cs.wisc.edu>
References: <Pine.GSO.4.58.0408181741001.15081@nova5.cs.wisc.edu>
Message-ID: <Pine.GSO.4.58.0408190853220.14411@yin>

Hi,

On Wed, 18 Aug 2004, Beverly Seavey wrote:

>     inp1 <- scan("data1",list(0,0))
>     inp2 <- scan("data2",list(0,0))
>
>     allInp <- c(inp1,inp2)
>
>  I get a table with 4 columns.

If I understand you correctly...

Have you tried cbind()?

Cheers,

Kevin

--------------------------------
Ko-Kang Kevin Wang
PhD Student
Centre for Mathematics and its Applications
Building 27, Room 1004
Mathematical Sciences Institute (MSI)
Australian National University
Canberra, ACT 0200
Australia

Homepage: http://wwwmaths.anu.edu.au/~wangk/
Ph (W): +61-2-6125-2431
Ph (H): +61-2-6125-7407
Ph (M): +61-40-451-8301



From Meredith.Briggs at team.telstra.com  Thu Aug 19 01:47:33 2004
From: Meredith.Briggs at team.telstra.com (Briggs, Meredith M)
Date: Thu, 19 Aug 2004 09:47:33 +1000
Subject: [R] Do you know if you can map a large minmum spanning tree in R?
Message-ID: <3B5823541A25D311B3B90008C7F9056410E35778@ntmsg0092.corpmail.telstra.com.au>



	Do you know if you can map in R?
	I have my minimum spanning tree, but as there are 1371 nodes (all over Australia) I'd like to be able to "graph" them as they actually would be on the map.
Do you know if this is possible?



From jim.lemon at uts.edu.au  Thu Aug 19 02:01:13 2004
From: jim.lemon at uts.edu.au (Jim Lemon)
Date: Thu, 19 Aug 2004 10:01:13 +1000
Subject: [R] Getting data loaded
Message-ID: <0I2O00C2N2K2XF@mail.uts.edu.au>

Hi,

I have been informed of a bug in the concord package, in that the data files 
containing the tabulated critical values for Kendall's W are not loaded on 
the command library(concord).

I had assumed that the lines in install.R would correspond to the commands to 
load data in R, e.g.

data(Wcrit01)
data(Wcrit05)

While these work on the command line, I get the errors:

library(concord)
Warning messages:
1: Data set 'Wcrit01' not found in: data(Wcrit01)
2: Data set 'Wcrit05' not found in: data(Wcrit05)

The section on this in R-exts doesn't seem to have any information on how to 
write the lines in install.R, nor does Checking and Building Packages. I 
managed to locate a few install.R files in other packages, but they were all 
empty. Any hints?

Jim

Dr Jim Lemon
Research Psychologist
Health Psychology Unit
University of Technology, Sydney

Feel free to ignore any garbage beneath this line.

-- 

DISCLAIMER: This email message and any accompanying attachme...{{dropped}}



From mdsumner at utas.edu.au  Thu Aug 19 02:18:22 2004
From: mdsumner at utas.edu.au (Michael Sumner)
Date: Thu, 19 Aug 2004 10:18:22 +1000
Subject: [R] Do you know if you can map a large minmum spanning tree in R?
In-Reply-To: <3B5823541A25D311B3B90008C7F9056410E35778@ntmsg0092.corpmai
	l.telstra.com.au>
References: <3B5823541A25D311B3B90008C7F9056410E35778@ntmsg0092.corpmail.telstra.com.au>
Message-ID: <6.0.0.22.1.20040819101041.02296fc8@postoffice.utas.edu.au>

At 09:47 AM 8/19/2004, Briggs, Meredith M wrote:



>         Do you know if you can map in R?
>         I have my minimum spanning tree, but as there are 1371 nodes (all 
> over Australia) I'd like to be able to "graph" them as they actually 
> would be on the map.
>Do you know if this is possible?

You can certainly "map" in R.  Depending on the coordinate system of your 
data . . .
but, e.g. - if it's lat/lon - perhaps the easiest way is to install the 
"maps" package and you can add the continental outlines to an existing plot:

## display nodes code here . . .
library(maps)
map('world',add=T,xlim=c(109,157),ylim=c(-47,-7))

There are plenty of other options, if you have your own map data (or want 
to use another source).  Feel free to provide more detail about your 
current plotting methods and coordinate system.

Also, the package "mapdata" contains a high resolution continental dataset 
-"worldHires"

Hope that helps, Mike.





###############################################

Michael Sumner - PhD. candidate
Maths and Physics (ACE CRC & IASOS) and Zoology (AWRU)
University of Tasmania
Private Bag 77, Hobart, Tas 7001, Australia
Phone: 6226 1752



From murdoch at stats.uwo.ca  Thu Aug 19 02:35:21 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 18 Aug 2004 20:35:21 -0400
Subject: [R] all.equal and names?
In-Reply-To: <41237D7A.5070507@pdf.com>
References: <41236765.9010504@pdf.com>
	<ncq6i0t1up1nh1ed203ea95mjq086knc7t@4ax.com>
	<41237D7A.5070507@pdf.com>
Message-ID: <cvs7i0tjr7lsqcegosocu1ma505sjp33gv@4ax.com>

On Wed, 18 Aug 2004 12:02:02 -0400, Spencer Graves
<spencer.graves at pdf.com> wrote:

>Hi, Duncan: 
>
>      Thanks much.  I think I remember reading about both "all.equal" 
>and "identical" in Venables and Ripley (2002) MASS.  Unfortunately, I 
>don't have MASS handy now, and I could not find it otherwise, so I asked. 
>
>      What needs to happen to upgrade the "all.equal" documentation to 
>add "identical" to the "see also"? 

I just did it.  It was there in the text, but should also have been in
see-also.

In general to get something added to the docs, the best way is to
collect a few similar things, classify them as doc errors, suggested
improvements, etc, and post them to R-devel (if you're not sure
they'll be accepted) or to R-bugs (if they are sure things).  It's
definitely best to submit suggested replacement text.

Duncan Murdoch



From Fiona.M.Wade at team.telstra.com  Thu Aug 19 03:15:26 2004
From: Fiona.M.Wade at team.telstra.com (Wade, Fiona M)
Date: Thu, 19 Aug 2004 11:15:26 +1000
Subject: [R] Do you know if you can map a large minimum spanning tree in R?
Message-ID: <61411576E951D211AF330008C7245DD916900166@ntmsg0005.corpmail.telstra.com.au>


Thanks Mike.
My data has longitude and latitude coords and I used distAB {clim.pact}
then mst {ape} to calculate my minimum spanning tree.  The nodes are
telecoms sites from all over Australia.  My goal is to determine the
minimum cost of linking them via cabling, and I'm starting by
calculating the distance "as the crow flies", but will probably
eventually need to calculate the rectilinear distances also.
I am a very newbie user of R, but have had experience with other
stats/programming software such as SAS, however no longer have access to
SAS so I've turned to R.  I also have tried using MapInfo with the data
exported from R, but have found that not so intuitive to learn on the
fly.
Back to R - I'm using W2K, and have managed to graph the tree using
plot(mdist,graph="nsca") where mdist is the output matrix from my mst
command, however this is not terribly map-like, so I'm looking for a
better display that can be embedded in a document.
Any assistance gratefully received!
Fiona.

> Fiona Wade
> Project Manager
> MARA
> F&A
> Telstra Corporation Limited
> Tel: 03 9634 5674
> Fax: 03 9634 2874
> Email: fiona.m.wade at team.telstra.com
> 
> The information contained in this e-mail message may be confidential.
If you are not the intended recipient, any use of, interference with,
disclosure or copying of this material is unauthorised and prohibited.
If you have received this message in error, please notify me by reply
e-mail and then delete the message.
> 


-----Original Message-----
From: Michael Sumner [mailto:mdsumner at utas.edu.au]
Sent: Thursday, 19 August 2004 10:18 AM
To: Briggs, Meredith M; r-help at stat.math.ethz.ch
Cc: Wade, Fiona M
Subject: Re: [R] Do you know if you can map a large minmum spanning tree
in R?


At 09:47 AM 8/19/2004, Briggs, Meredith M wrote:



>         Do you know if you can map in R?
>         I have my minimum spanning tree, but as there are 1371 nodes
(all 
> over Australia) I'd like to be able to "graph" them as they actually 
> would be on the map.
>Do you know if this is possible?

You can certainly "map" in R.  Depending on the coordinate system of
your 
data . . .
but, e.g. - if it's lat/lon - perhaps the easiest way is to install the 
"maps" package and you can add the continental outlines to an existing
plot:

## display nodes code here . . .
library(maps)
map('world',add=T,xlim=c(109,157),ylim=c(-47,-7))

There are plenty of other options, if you have your own map data (or
want 
to use another source).  Feel free to provide more detail about your 
current plotting methods and coordinate system.

Also, the package "mapdata" contains a high resolution continental
dataset 
-"worldHires"

Hope that helps, Mike.





###############################################

Michael Sumner - PhD. candidate
Maths and Physics (ACE CRC & IASOS) and Zoology (AWRU)
University of Tasmania
Private Bag 77, Hobart, Tas 7001, Australia
Phone: 6226 1752



From maustin at amgen.com  Thu Aug 19 03:16:28 2004
From: maustin at amgen.com (Austin, Matt)
Date: Wed, 18 Aug 2004 18:16:28 -0700
Subject: [R] Gee
Message-ID: <E7D5AB4811D20B489622AABA9C53859101F11143@teal-exch.amgen.com>

The error you had in your program was because of the na.action in the
function call.  If you specify the parameter you need to give it a function
such as na.omit or na.fail

There would be a second error if you fixed this because you are including an
intercept and the gee function will automatically fit an intercept also,
this will lead to to columns being the exact same in the design matrix.

The following code should get you where you want to be.

eta <- rnorm(100)
Si <- runif(100, min=0, max=11)
lny <- 10+2*Si-Si^2+eta
id <- (1:100)
conc <- rep(1,100)
Si2 <- Si^2

geedat <- data.frame(Si=Si,Si2=Si2,lny=lny,id=id)

lnyhat <- gee(lny ~ Si - Si2, id=id, data=geedat)
lnyhat <- gee(lny ~ conc+Si - Si2 -1, id=id, data=geedat)

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of anafava at uiuc.edu
Sent: Wednesday, August 18, 2004 14:54 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Gee


I am trying to learn the gee function in R. So I try to 
generate some data and use this function. I have the 
following lines:

######################################## Gee

# Generating lny=10+2*Si-Si^2+eta
# eta ~ N(0,1)
# Si ~ U(0,11)

eta <- vector(mode="numeric",100)
eta <- rnorm(100)

Si <- vector(mode="numeric",100)
Si <- runif(100, min=0, max=11)

lny <- vector(mode="numeric",100)
lny <- 10+2*Si-Si^2+eta

id <- vector(mode="numeric",100)
id <- (1:100)


cons <- vector(mode="numeric",100)
for(i in 1:100) {
cons[i] <- 1
}

Si2 <- vector(mode="numeric",100)
for(j in 1:100) {
Si2[j] <- Si[j]^2
}

geedat <- data.frame(Si=Si,Si2=Si2,lny=lny,id=id,cons=cons)

lnyhat <- gee(lny~cons+Si-Si2, id=id, data=geedat, na.action)

And I received the following error message:

[1] "Beginning Cgee S-function, @(#) geeformula.q 4.13 
98/01/27"
Error in "[.data.frame"(structure(list(lny = c
(9.92388214744737, 2.54332321404939,  : 
        invalid subscript type

I don't know what I am doing wrong. May someone help me?

Thanks
Ana

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Simon.Blomberg at anu.edu.au  Thu Aug 19 04:05:32 2004
From: Simon.Blomberg at anu.edu.au (Simon)
Date: Thu, 19 Aug 2004 12:05:32 +1000
Subject: [R] glmmPQL in R and S-PLUS 6 - differing results
Message-ID: <a06110400bd49b2bcea3c@[150.203.51.113]>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040819/8393516a/attachment.pl

From rog at stanford.edu  Thu Aug 19 04:11:25 2004
From: rog at stanford.edu (Roger Levy)
Date: 18 Aug 2004 19:11:25 -0700
Subject: [R] labeled break statements in R?
In-Reply-To: <Pine.LNX.4.44.0408182006090.25449-100000@gannet.stats>
References: <Pine.LNX.4.44.0408182006090.25449-100000@gannet.stats>
Message-ID: <25fz6jvpv6.fsf@stanford.edu>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> On 18 Aug 2004, Roger Levy wrote:
> 
> > Are there labeled break statements in R?  i.e., something along the
> > lines of
> > 
> > TOPLOOP: for(i in 1:m) {
> >   for(j in 1:n) {
> >     ...
> >     if(condition) {
> >        break TOPLOOP
> >     }
> >   }
> > }
> 
> No, but if you find yourself using nested for loops it is very likely that 
> you are not thinking in the right way for a vector language.
> 
> R does have a `R Language Definition' manual.  It's long been an
> unfinished draft but it is not so incomplete as to omit reserved words.
> It and the S reference books are the places to research questions like 
> this.

Many thanks for the fast response -- I actually had looked in the R
language manual, and nothing was said about labeled breaks one way or
the other.  The section on looping was terse enough that I thought it
might have been omitted.

Roger



From rpeng at jhsph.edu  Thu Aug 19 04:26:06 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Wed, 18 Aug 2004 22:26:06 -0400
Subject: [R] Getting data loaded
In-Reply-To: <0I2O00C2N2K2XF@mail.uts.edu.au>
References: <0I2O00C2N2K2XF@mail.uts.edu.au>
Message-ID: <41240FBE.8090606@jhsph.edu>

I think you should load data in a .First.lib() function, or if 
you have a namespace, in a .onLoad() function.

-roger

Jim Lemon wrote:
> Hi,
> 
> I have been informed of a bug in the concord package, in that the data files 
> containing the tabulated critical values for Kendall's W are not loaded on 
> the command library(concord).
> 
> I had assumed that the lines in install.R would correspond to the commands to 
> load data in R, e.g.
> 
> data(Wcrit01)
> data(Wcrit05)
> 
> While these work on the command line, I get the errors:
> 
> library(concord)
> Warning messages:
> 1: Data set 'Wcrit01' not found in: data(Wcrit01)
> 2: Data set 'Wcrit05' not found in: data(Wcrit05)
> 
> The section on this in R-exts doesn't seem to have any information on how to 
> write the lines in install.R, nor does Checking and Building Packages. I 
> managed to locate a few install.R files in other packages, but they were all 
> empty. Any hints?
> 
> Jim
> 
> Dr Jim Lemon
> Research Psychologist
> Health Psychology Unit
> University of Technology, Sydney
> 
> Feel free to ignore any garbage beneath this line.
>



From informatics at myhelios.net  Thu Aug 19 04:34:42 2004
From: informatics at myhelios.net (David L. Van Brunt, Ph.D.)
Date: Wed, 18 Aug 2004 21:34:42 -0500
Subject: [R] OS X specific question: help.start() won't launch
In-Reply-To: <25fz6jvpv6.fsf@stanford.edu>
References: <Pine.LNX.4.44.0408182006090.25449-100000@gannet.stats>
	<25fz6jvpv6.fsf@stanford.edu>
Message-ID: <5354AFBD-F188-11D8-90B9-000393B2A94A@myhelios.net>

It's been a while since I used R, and have certainly applied a few 
system patches. Since I installed the latest R.bin, when I type 
"help.start()" nothing happens anymore. It used to launch a browser 
with the R help system.

Anyone know of any issues here, or ways to re-enable this?

Didn't see anything searching the FAQ, just stuff on the java-based 
search. I don't need that, just need to open the help!

Thanks,
	
	Dave VB



From rsong at stat.wisc.edu  Thu Aug 19 05:20:30 2004
From: rsong at stat.wisc.edu (Rui Song)
Date: Wed, 18 Aug 2004 22:20:30 -0500 (CDT)
Subject: [R] A question about external time-dependent covariates in cox model
Message-ID: <Pine.LNX.4.58.0408182135000.1487@chi18.stat.wisc.edu>

Dear Sir or Madam:
I am a graduate student in UW-Madison statistics department. I have a
question about fitting a cox model with external time-dependent
covariates.

Say the original data is in the following format:
Obs Eventtime  Status  Cov(time=5)  Cov(time=8)  Cov(time=10)	Cov(time=12)
1	5	1		2
2	8	0(censored)	2	4
3	10	1		2	4		6
4	12	1		2	4		6		8
....

Notice that the time-dependent covariates are identical at the same
time points for all obs since they are external to the failure process.
process.

Then I organized the data as the following:
obs	start	end	eventtime	status	cov
1	0	5	5		1	2
2	0	5	8		0	2
2	5	8	8		0	4
3	0	5	10		1	2
3	5	8	10		1	4
3	8	10	10		1	6
4	0	5	12		1	2
4	5	8	12		1	4
4	8	10	12		1	6
4	10	12	12		1	8

And fit the model using:

fit<-coxph(Surv(start, end, status)~cov);

When I fit the model to my data set (Which has 89 observations and 81
distinct time points, sort of large.), I always got a message that
"Process R segmentation fault (core dumped)". Would you let me know if it
is due to the matrix sigularity in the computation of the partial
likelihood or something else? And how should I fit a cox model with
external time-dependent covariates?

Thanks a lot for your time and help!

Sincerely,
Rui Song



From ggrothendieck at myway.com  Thu Aug 19 06:13:15 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 19 Aug 2004 04:13:15 +0000 (UTC)
Subject: [R] labeled break statements in R?
References: <258yccz3cv.fsf@stanford.edu>
Message-ID: <loom.20040819T060129-24@post.gmane.org>

Roger Levy <rog <at> stanford.edu> writes:

: 
: Hi,
: 
: Are there labeled break statements in R?  i.e., something along the
: lines of
: 
: TOPLOOP: for(i in 1:m) {
:   for(j in 1:n) {
:     ...
:     if(condition) {
:        break TOPLOOP
:     }
:   }
: }

Assuming that your labelled break is supposed to break out of
both loops, unlike an ordinary break that just breaks out of
the inner loop, this is how it would be done:

a <- matrix(0, nr=3, nc=3)
local({
	for(i in 1:3) 
		for(j in 1:3)
			if (i+j>4) return() else a[i,j] <<- i+j
})
a

Be aware that assigning variables within the local will assign
local copies unless you use <<-, assign(..., ..., parent.frame())
or eval.parent(...).

Of course this style is not recommended for R, and as someone else 
already mentioned, you should try to vectorize your code eliminating 
the loops altogether.  For example, the above could be written 
without loops like this:

a <- matrix(0, nr=3, nc=3)
rc <- row(a) + col(a)
a <- ifelse(rc > 4, a, rc)
a



From ewhi4239 at mail.usyd.edu.au  Thu Aug 19 06:23:23 2004
From: ewhi4239 at mail.usyd.edu.au (Evelyn Hall)
Date: Thu, 19 Aug 2004 14:23:23 +1000
Subject: [R] nlme R vs S plus
Message-ID: <1092889403.41242b3b0e009@www-mail.usyd.edu.au>

Hi all,

I'm a PhD student at sydney uni and am trying to run a non linear mixed
model program to obtain estimates of parameters describing dairy cow
lactation curves. At present, I have been able to get the data to converge
using the S plus (S plus 2000) nlme function.  However, when I put the same
data into R (R 1.9.0), add in the nlme package and run the code, it does
not converge by the max 50 iterations. Is this because the nlme function
is slightly different and if so, is there a way to solve this problem?

The code I am using is below:

W3<-deriv(~A*ti^exp(logB)*exp(-exp(logC)*ti),c("A","logB","logC"),
function(ti,A,logB,logC){})

Lact.nlme<-{nlme(model=MLKYLD~W3(DIM,A,logB,logC),
	fixed=A+logB+logC~1,
	random=A+logB+logC~1|ID,
  
	data=LacData.x0,					start=c(13.41143,log(0.152792),log(0.002494)),			control=nlmeControl(msMaxIter=200),
   	verbose=T)
   	}
Any help or advice would be greatly appreciated,

Evelyn

--
Evelyn Hall
PhD Student
Faculty of Veterinary Science
C01 JL Shute Building Camden
University of Sydney



From ggrothendieck at myway.com  Thu Aug 19 06:49:21 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 19 Aug 2004 04:49:21 +0000 (UTC)
Subject: [R] labeled break statements in R?
References: <258yccz3cv.fsf@stanford.edu>
	<loom.20040819T060129-24@post.gmane.org>
Message-ID: <loom.20040819T064330-457@post.gmane.org>

Gabor Grothendieck <ggrothendieck <at> myway.com> writes:

: 
: Roger Levy <rog <at> stanford.edu> writes:
: 
: : 
: : Hi,
: : 
: : Are there labeled break statements in R?  i.e., something along the
: : lines of
: : 
: : TOPLOOP: for(i in 1:m) {
: :   for(j in 1:n) {
: :     ...
: :     if(condition) {
: :        break TOPLOOP
: :     }
: :   }
: : }
: 
: Assuming that your labelled break is supposed to break out of
: both loops, unlike an ordinary break that just breaks out of
: the inner loop, this is how it would be done:
: 
: a <- matrix(0, nr=3, nc=3)
: local({
: 	for(i in 1:3) 
: 		for(j in 1:3)
: 			if (i+j>4) return() else a[i,j] <<- i+j
: })
: a
: 
: Be aware that assigning variables within the local will assign
: local copies unless you use <<-, assign(..., ..., parent.frame())
: or eval.parent(...).
: 
: Of course this style is not recommended for R, and as someone else 
: already mentioned, you should try to vectorize your code eliminating 
: the loops altogether.  For example, the above could be written 
: without loops like this:
: 
: a <- matrix(0, nr=3, nc=3)
: rc <- row(a) + col(a)
: a <- ifelse(rc > 4, a, rc)
: a
: 
Sorry, the ifelse line should have been:

ifelse( t(matrix(cumsum(rc>4),3,3)), a, rc)



From ggrothendieck at myway.com  Thu Aug 19 07:51:10 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 19 Aug 2004 05:51:10 +0000 (UTC)
Subject: [R] labeled break statements in R?
References: <258yccz3cv.fsf@stanford.edu>
	<loom.20040819T060129-24@post.gmane.org>
	<loom.20040819T064330-457@post.gmane.org>
Message-ID: <loom.20040819T074717-847@post.gmane.org>

Gabor Grothendieck <ggrothendieck <at> myway.com> writes:

: 
: Gabor Grothendieck <ggrothendieck <at> myway.com> writes:
: 
: : 
: : Roger Levy <rog <at> stanford.edu> writes:
: : 
: : : 
: : : Hi,
: : : 
: : : Are there labeled break statements in R?  i.e., something along the
: : : lines of
: : : 
: : : TOPLOOP: for(i in 1:m) {
: : :   for(j in 1:n) {
: : :     ...
: : :     if(condition) {
: : :        break TOPLOOP
: : :     }
: : :   }
: : : }
: : 
: : Assuming that your labelled break is supposed to break out of
: : both loops, unlike an ordinary break that just breaks out of
: : the inner loop, this is how it would be done:
: : 
: : a <- matrix(0, nr=3, nc=3)
: : local({
: : 	for(i in 1:3) 
: : 		for(j in 1:3)
: : 			if (i+j>4) return() else a[i,j] <<- i+j
: : })
: : a
: : 
: : Be aware that assigning variables within the local will assign
: : local copies unless you use <<-, assign(..., ..., parent.frame())
: : or eval.parent(...).
: : 
: : Of course this style is not recommended for R, and as someone else 
: : already mentioned, you should try to vectorize your code eliminating 
: : the loops altogether.  For example, the above could be written 
: : without loops like this:
: : 
: : a <- matrix(0, nr=3, nc=3)
: : rc <- row(a) + col(a)
: : a <- ifelse(rc > 4, a, rc)
: : a
: : 
: Sorry, the ifelse line should have been:
: 
: ifelse( t(matrix(cumsum(rc>4),3,3)), a, rc)

or even simpler:

a <- ifelse( matrix(cumsum(rc>4),3,byrow=T), a, rc)



From almirall at rand.org  Thu Aug 19 08:30:11 2004
From: almirall at rand.org (Almirall, Daniel)
Date: Thu, 19 Aug 2004 02:30:11 -0400
Subject: [R] The 'test.terms' argument in 'regTermTest' in package 'survey'
Message-ID: <5A637F509C50B444BDA096EB2D7BC15842C48C@pghmail2.rand.org>


This is a question regarding the 'regTermTest' function in the 'survey' package.  Imagine Z as a three level factor variable, and code ZB and ZC as the two corresponding dummy variables.  X is a continuous variable.  In a 'glm' of Y on Z and X, say, how do the two test specifications

	test.terms = c("ZB:X","ZC:X")  # and
	test.terms = ~ ZB:X + ZC:X

in 'regTermTest' differ?  I thought that both would return the same joint (Wald) test for the two Z:X interactions.  Why does the second one specify a 1 degree of freedom test?  The code below should help clarify my question.

Thanks much, 
Danny




## I'm currently using: R Version 1.9.1 / Windows 2000 / P4/2.8 Ghz

Z <- as.factor(rep(LETTERS[1:3],20))
Y <- rep(0:1, 30)
X <- rnorm(60)

glm1 <- glm(Y ~ Z + X + Z:X, family=binomial)
summary(glm1)$coeff

regTermTest( model=glm1 , test.terms=~Z:X)

ZB <- ifelse(Z=="B",1,0)
ZC <- ifelse(Z=="C",1,0)

glm2 <- glm(Y ~ ZB + ZC + X + ZB:X + ZC:X, family=binomial)
summary(glm2)$coeff 		## Okay, same as glm1

regTermTest( model=glm2 , test.terms= c("ZB:X","ZC:X"))
regTermTest( model=glm2 , test.terms= ~ ZB:X + ZC:X)



From ripley at stats.ox.ac.uk  Thu Aug 19 08:35:57 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 19 Aug 2004 07:35:57 +0100 (BST)
Subject: [R] glmmPQL in R and S-PLUS 6 - differing results
In-Reply-To: <a06110400bd49b2bcea3c@[150.203.51.113]>
Message-ID: <Pine.LNX.4.44.0408190731340.13976-100000@gannet.stats>

glmmPQL calls lme, and it is that which differs between S-PLUS and R.

These are optimization problems with multiple local maxima, and like any 
complex statistical fitting problem you should not expect all programs to 
give the same answer.  The short answer is to believe all of them, and to 
check the usefulnees of the answer for your criteria.

On Thu, 19 Aug 2004, Simon wrote:

> Greetings R-ers,
> 
> A colleague and I have been exploring the behaviour of glmmPQL in R 
> and S-PLUS 6 and we appear to get different results using the same 
> code and the same data set, which worries us. I have checked the 
> behaviour in R 1.7.1 (MacOS 9.2) and R. 1.9.0 (Windows 2000) and the 
> results are the same, but differ from S-PLUS 6 with the latest Mass 
> and nlme libraries (Windows XP).

...

> Note that R and S-PLUS differ in the number of iterations. Also the 
> logLikelihoods differ considerably too. Pinheiro and Bates argue that 
> a likelihood-ratio test for fixed effects is not reliable 
> ("anticonservative"), but I think that both packages should at least 
> give the same answer!! I checked lmeControl() in R and S-PLUS and the 
> settings for lme look the same on both platforms. Which output (if 
> any) should we believe? Any insight would be greatly appreciated.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jawegelin at ucdavis.edu  Thu Aug 19 08:45:05 2004
From: jawegelin at ucdavis.edu (Jacob Wegelin)
Date: Wed, 18 Aug 2004 23:45:05 -0700 (PDT)
Subject: [R] Is R good for not-professional-statistician,
 un-mathematical clinical researchers?
Message-ID: <Pine.OSX.4.53.0408182313310.3661@biostat5.ucdavis.edu>


Alternate title: How can I persuade my students that R is for them?

Alternate title: Can R replace SAS, SPSS or Stata for clinicians?

I am teaching introductory statistics to twelve physicians and two veterinarians
who have enrolled in a Mentored Clinical Research Training Program.  My course is the
first in a sequence of three.  We (the instructors of this sequence) chose to teach
R rather than some other computing environment.

My (highly motivated) students have never encountered anything like R.  One frankly
asked:

"Do you feel (honestly) that a group of physicians (with two vets) clinicians will
be able to effectively use and actually understand R? If so, I will happily call this
bookstore and order this book [Venables and Ripley] tomorrow."

I am heavily biased toward R/S because I have used it since the first applied statistics
course I took.  But I would love to give these students some kind of objective information
about the usability of R by non-statisticians--not just my own bias.

Could anyone suggest any such information?  Or does anyone on this list use R who is
a clinician and not really mathematically savvy?  For instance, someone who doesn't
remember any math beyond algebra and doesn't think in terms of P(A|B)?

Or have we done a disservice to our students by choosing to make them
learn R, rather than making ourselves learn SAS, Stata or SPSS?

Thank you for any ideas

Jake Wegelin



From Roger.Bivand at nhh.no  Thu Aug 19 09:36:53 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 19 Aug 2004 09:36:53 +0200 (CEST)
Subject: [R] Do you know if you can map a large minimum spanning tree   
	in R?
In-Reply-To: <61411576E951D211AF330008C7245DD916900166@ntmsg0005.corpmail.telstra.c
	om.au>
References: <61411576E951D211AF330008C7245DD916900166@ntmsg0005.corpmail.telstra.com.au>
Message-ID: <63549.129.177.44.229.1092901013.squirrel@webmail.nhh.no>

>
> Thanks Mike.
> My data has longitude and latitude coords and I used distAB {clim.pact}
then mst {ape} to calculate my minimum spanning tree.  The nodes are
telecoms sites from all over Australia.  My goal is to determine the
minimum cost of linking them via cabling, and I'm starting by
> calculating the distance "as the crow flies", but will probably
> eventually need to calculate the rectilinear distances also.
> I am a very newbie user of R, but have had experience with other
stats/programming software such as SAS, however no longer have access to
SAS so I've turned to R.  I also have tried using MapInfo with the data
exported from R, but have found that not so intuitive to learn on the
fly.
> Back to R - I'm using W2K, and have managed to graph the tree using
plot(mdist,graph="nsca") where mdist is the output matrix from my mst
command, however this is not terribly map-like, so I'm looking for a
better display that can be embedded in a document.
> Any assistance gratefully received!

One possibility is to use the fact that mst() function returns a 0/1
matrix exactly like spatial weights matrices used in the spdep package:

> X <- matrix(runif(n*50), 50, n)
> d <- dist(X)
> M <- mst(d)
> library(spdep)
> M.nb <- mat2listw(M)$neighbours
> plot(M.nb, X)

The plot.nb() function then plots the graph by joining points defined as
neighbours, use the add=TRUE argument to overplot on a base map. The nb
object can also be manipulated a little, like subsetting, if that is any
use. The idea is to convert the 0/1 matrix to a list with vectors of
neighbours' IDs for each point, rather than store the whole matrix.
Contact me off-list if you need more details. (nbdists() retrieves the
distances on the graph too).


> Fiona.
>
>> Fiona Wade
>> Project Manager
>> MARA
>> F&A
>> Telstra Corporation Limited
>> Tel: 03 9634 5674
>> Fax: 03 9634 2874
>> Email: fiona.m.wade at team.telstra.com
>> The information contained in this e-mail message may be confidential.
> If you are not the intended recipient, any use of, interference with,
disclosure or copying of this material is unauthorised and prohibited.
If you have received this message in error, please notify me by reply
e-mail and then delete the message.
>
>
> -----Original Message-----
> From: Michael Sumner [mailto:mdsumner at utas.edu.au]
> Sent: Thursday, 19 August 2004 10:18 AM
> To: Briggs, Meredith M; r-help at stat.math.ethz.ch
> Cc: Wade, Fiona M
> Subject: Re: [R] Do you know if you can map a large minmum spanning tree
in R?
>
>
> At 09:47 AM 8/19/2004, Briggs, Meredith M wrote:
>
>
>
>>         Do you know
if you can map in R?
>>         I have my minimum spanning tree, but as there are 1371 nodes
> (all
>> over Australia) I'd like to be able to "graph" them as they actually
would be on the map.
>>Do you know if this is possible?
>
> You can certainly "map" in R.  Depending on the coordinate system of your
> data . . .
> but, e.g. - if it's lat/lon - perhaps the easiest way is to install the
"maps" package and you can add the continental outlines to an existing
plot:
>
> ## display nodes code here . . .
> library(maps)
> map('world',add=T,xlim=c(109,157),ylim=c(-47,-7))
>
> There are plenty of other options, if you have your own map data (or want
> to use another source).  Feel free to provide more detail about your
current plotting methods and coordinate system.
>
> Also, the package "mapdata" contains a high reso
> lution continental
> dataset
> -"worldHires"
>
> Hope that helps, Mike.
>
>
>
>
>
> ###############################################
>
> Michael Sumner - PhD. candidate
> Maths and Physics (ACE CRC & IASOS) and Zoology (AWRU)
> University of Tasmania
> Private Bag 77, Hobart, Tas 7001, Australia
> Phone: 6226 1752
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html


-- 
Roger Bivand
NHH, Breiviksveien 40, N-5045 Bergen, Norway



From Roger.Bivand at nhh.no  Thu Aug 19 09:50:31 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 19 Aug 2004 09:50:31 +0200 (CEST)
Subject: [R] Do you know if you can map a large minimum spanning tree   
	in R?
In-Reply-To: <63549.129.177.44.229.1092901014.squirrel@webmail.nhh.no>
References: <61411576E951D211AF330008C7245DD916900166@ntmsg0005.corpmail.telstra.com.au>
	<63549.129.177.44.229.1092901014.squirrel@webmail.nhh.no>
Message-ID: <63594.129.177.44.229.1092901831.squirrel@webmail.nhh.no>


>>
>> Thanks Mike.
>> My data has longitude and latitude coords and I used distAB {clim.pact}
> then mst {ape} to calculate my minimum spanning tree.  The nodes are
> telecoms sites from all over Australia.  My goal is to determine the
> minimum cost of linking them via cabling, and I'm starting by
>> calculating the distance "as the crow flies", but will probably
>> eventually need to calculate the rectilinear distances also.
>> I am a very newbie user of R, but have had experience with other
> stats/programming software such as SAS, however no longer have access to
> SAS so I've turned to R.  I also have tried using MapInfo with the data
> exported from R, but have found that not so intuitive to learn on the
> fly.
>> Back to R - I'm using W2K, and have managed to graph the tree using
> plot(mdist,graph="nsca") where mdist is the output matrix from my mst
> command, however this is not terribly map-like, so I'm looking for a
> better display that can be embedded in a document.
>> Any assistance gratefully received!
>
> One possibility is to use the fact that mst() function returns a 0/1
> matrix exactly like spatial weights matrices used in the spdep package:
>

Sorry, must be 2D:

> n <- 2

otherwise plot.nb() will not like X, the matrix of coordinates. In your
case that's obvious, but not in general.

>> X <- matrix(runif(n*50), 50, n)
>> d <- dist(X)
>> M <- mst(d)
>> library(spdep)
>> M.nb <- mat2listw(M)$neighbours
>> plot(M.nb, X)
>
> The plot.nb() function then plots the graph by joining points defined as
> neighbours, use the add=TRUE argument to overplot on a base map. The nb
> object can also be manipulated a little, like subsetting, if that is any
> use. The idea is to convert the 0/1 matrix to a list with vectors of
> neighbours' IDs for each point, rather than store the whole matrix.
> Contact me off-list if you need more details. (nbdists() retrieves the
> distances on the graph too).
>
>
>> Fiona.
>>
>>> Fiona Wade
>>> Project Manager
>>> MARA
>>> F&A
>>> Telstra Corporation Limited
>>> Tel: 03 9634 5674
>>> Fax: 03 9634 2874
>>> Email: fiona.m.wade at team.telstra.com
>>> The information contained in this e-mail message may be confidential.
>> If you are not the intended recipient, any use of, interference with,
> disclosure or copying of this material is unauthorised and prohibited.
> If you have received this message in error, please notify me by reply
> e-mail and then delete the message.
>>
>>
>> -----Original Message-----
>> From: Michael Sumner [mailto:mdsumner at utas.edu.au]
>> Sent: Thursday, 19 August 2004 10:18 AM
>> To: Briggs, Meredith M; r-help at stat.math.ethz.ch
>> Cc: Wade, Fiona M
>> Subject: Re: [R] Do you know if you can map a large minmum spanning tree
> in R?
>>
>>
>> At 09:47 AM 8/19/2004, Briggs, Meredith M wrote:
>>
>>
>>
>>>         Do you know
> if you can map in R?
>>>
> I have my minimum spanning tree, but as there are 1371 nodes
>> (all
>>> over Australia) I'd like to be able to "graph" them as they actually
> would be on the map.
>>>Do you know if this is possible?
>>
>> You can certainly "map" in R.  Depending on the coordinate system of
>> your
>> data . . .
>> but, e.g. - if it's lat/lon - perhaps the easiest way is to install the
> "maps" package and you can add the continental outlines to an existing
> plot:
>>
>> ## display nodes code here . . .
>> library(maps)
>> map('world',add=T,xlim=c(109,157),ylim=c(-47,-7))
>>
>> There are plenty of other options, if you have your own map data (or
>> want
>> to use another source).  Feel free to provide more detail about your
> current plotting methods and coordinate system.
>>
>> Also, the package "mapdata" contains a high reso
>> lution continental
>> dataset
>> -"worldHires"
>>
>> Hope that helps, Mike.
>>
>>
>>
>>
>>
>> ###############################################
>>
>> Michael Sumner - PhD. candidate
>> Maths and Physics (ACE CRC & IASOS) and Zoology (AWRU)
>> University of Tasmania
>> Private Bag 77, Hobart, Tas 7001, Australia
>> Phone: 6226 1752
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>
>
> --
> Roger Bivand
> NHH, Breiviksveien 40, N-5045 Bergen, Norway
>
>


-- 
Roger Bivand
NHH, Breiviksveien 40, N-5045 Bergen, Norway



From i.visser at uva.nl  Thu Aug 19 10:27:47 2004
From: i.visser at uva.nl (Ingmar Visser)
Date: Thu, 19 Aug 2004 10:27:47 +0200
Subject: [R] OS X specific question: help.start() won't launch
In-Reply-To: <5354AFBD-F188-11D8-90B9-000393B2A94A@myhelios.net>
Message-ID: <BD4A3123.62EF%i.visser@uva.nl>

Hi Dave,
This has come up a couple of times recently.
I don't remember by heart but searching the archives should help you find
the answer. It's best to try R-SIG-MAC list archive.
Bye, ingmar

On 8/19/04 4:34 AM, "David L. Van Brunt, Ph.D." <informatics at myhelios.net>
wrote:

> It's been a while since I used R, and have certainly applied a few
> system patches. Since I installed the latest R.bin, when I type
> "help.start()" nothing happens anymore. It used to launch a browser
> with the R help system.
> 
> Anyone know of any issues here, or ways to re-enable this?
> 
> Didn't see anything searching the FAQ, just stuff on the java-based
> search. I don't need that, just need to open the help!
> 
> Thanks,
> 
> Dave VB
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ernesto at ipimar.pt  Thu Aug 19 11:08:44 2004
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Thu, 19 Aug 2004 10:08:44 +0100
Subject: [R] Is R good for not-professional-statistician,
	un-mathematical clinical researchers?
In-Reply-To: <Pine.OSX.4.53.0408182313310.3661@biostat5.ucdavis.edu>
References: <Pine.OSX.4.53.0408182313310.3661@biostat5.ucdavis.edu>
Message-ID: <1092906524.11860.9.camel@gandalf.local>

On Thu, 2004-08-19 at 07:45, Jacob Wegelin wrote:
> Alternate title: How can I persuade my students that R is for them?
> 
> Alternate title: Can R replace SAS, SPSS or Stata for clinicians?
> 
> I am teaching introductory statistics to twelve physicians and two veterinarians
> who have enrolled in a Mentored Clinical Research Training Program.  My course is the
> first in a sequence of three.  We (the instructors of this sequence) chose to teach
> R rather than some other computing environment.
> 
> My (highly motivated) students have never encountered anything like R.  One frankly
> asked:
> 
> "Do you feel (honestly) that a group of physicians (with two vets) clinicians will
> be able to effectively use and actually understand R? If so, I will happily call this
> bookstore and order this book [Venables and Ripley] tomorrow."
> 
> I am heavily biased toward R/S because I have used it since the first applied statistics
> course I took.  But I would love to give these students some kind of objective information
> about the usability of R by non-statisticians--not just my own bias.
> 
> Could anyone suggest any such information?  Or does anyone on this list use R who is
> a clinician and not really mathematically savvy?  For instance, someone who doesn't
> remember any math beyond algebra and doesn't think in terms of P(A|B)?
> 
> Or have we done a disservice to our students by choosing to make them
> learn R, rather than making ourselves learn SAS, Stata or SPSS?
> 
> Thank you for any ideas
> 
> Jake Wegelin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Hi Jake,

I can give you my experience on this subject. First of all I am not a
Stats/Maths professional, I'm a Marine Biologist working in fisheries,
although with a MSc in Stats ... but I've started using R before I did
the MSc.

In the Fisheries Scientists Community R is becoming broadly used and
most scientists have backgrounds in Biology or similars.

I think the most important question is "are they afraid of the command
line ?" if yes forget about R, otherwise I guess they will love it when
they'll find how much they can do with it.

Hope it helps

Best regards

EJ



From sivana1 at techunix.technion.ac.il  Thu Aug 19 11:05:20 2004
From: sivana1 at techunix.technion.ac.il (Sivan Aldor)
Date: Thu, 19 Aug 2004 12:05:20 +0300 (IDT)
Subject: [R] how to extract data from excel sheet?
Message-ID: <Pine.GSO.4.44_heb2.09.0408191203060.9285-100000@techunix.technion.ac.il>

i want to upload different sheets from one excel file.
does anyone know of a way to do so? (i use read.csv to load the excel but
i would like to load form the sheets themselves.)
thanks
sivan



From rossini at blindglobe.net  Thu Aug 19 11:04:13 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Thu, 19 Aug 2004 02:04:13 -0700
Subject: [R] Is R good for not-professional-statistician,
	un-mathematical clinical researchers?
In-Reply-To: <Pine.OSX.4.53.0408182313310.3661@biostat5.ucdavis.edu> (Jacob
	Wegelin's message of "Wed, 18 Aug 2004 23:45:05 -0700 (PDT)")
References: <Pine.OSX.4.53.0408182313310.3661@biostat5.ucdavis.edu>
Message-ID: <85isbffqia.fsf@servant.blindglobe.net>


One thought -- the first course I took, I taught physicians
XLispStat.  They appreciated it since it allowed them to do logistic
regression without spending large $$$.....

Now, that isn't quite true any more.  So might depend on how times
have changed.



Jacob Wegelin <jawegelin at ucdavis.edu> writes:

> Alternate title: How can I persuade my students that R is for them?
>
> Alternate title: Can R replace SAS, SPSS or Stata for clinicians?
>
> I am teaching introductory statistics to twelve physicians and two veterinarians
> who have enrolled in a Mentored Clinical Research Training Program.  My course is the
> first in a sequence of three.  We (the instructors of this sequence) chose to teach
> R rather than some other computing environment.
>
> My (highly motivated) students have never encountered anything like R.  One frankly
> asked:
>
> "Do you feel (honestly) that a group of physicians (with two vets) clinicians will
> be able to effectively use and actually understand R? If so, I will happily call this
> bookstore and order this book [Venables and Ripley] tomorrow."
>
> I am heavily biased toward R/S because I have used it since the first applied statistics
> course I took.  But I would love to give these students some kind of objective information
> about the usability of R by non-statisticians--not just my own bias.
>
> Could anyone suggest any such information?  Or does anyone on this list use R who is
> a clinician and not really mathematically savvy?  For instance, someone who doesn't
> remember any math beyond algebra and doesn't think in terms of P(A|B)?
>
> Or have we done a disservice to our students by choosing to make them
> learn R, rather than making ourselves learn SAS, Stata or SPSS?
>
> Thank you for any ideas
>
> Jake Wegelin
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Anthony Rossini			    Research Associate Professor
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From ligges at statistik.uni-dortmund.de  Thu Aug 19 11:41:32 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 19 Aug 2004 11:41:32 +0200
Subject: [R] how to extract data from excel sheet?
In-Reply-To: <Pine.GSO.4.44_heb2.09.0408191203060.9285-100000@techunix.technion.ac.il>
References: <Pine.GSO.4.44_heb2.09.0408191203060.9285-100000@techunix.technion.ac.il>
Message-ID: <412475CC.70204@statistik.uni-dortmund.de>

Sivan Aldor wrote:

> i want to upload different sheets from one excel file.
> does anyone know of a way to do so? 

Yes, the "R Data Import/Export" manual knows.

Uwe Ligges



 > (i use read.csv to load the excel but
> i would like to load form the sheets themselves.)
> thanks
> sivan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From wia at zhwin.ch  Thu Aug 19 11:45:10 2004
From: wia at zhwin.ch (Wildi Marc, wia)
Date: Thu, 19 Aug 2004 11:45:10 +0200
Subject: [R] sample selection problem, inverse mills ratio (Heckman, Lewbel,
	...)
Message-ID: <53A181E56FB0694ABFD212F8AEDA7F6F1460E2@langouste.zhwin.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040819/b0437bfe/attachment.pl

From nilsas at web.de  Thu Aug 19 11:50:36 2004
From: nilsas at web.de (Nils Aschenbruck)
Date: Thu, 19 Aug 2004 11:50:36 +0200
Subject: [R] Erlang distribution
Message-ID: <1228984282@web.de>

Content-Type: text/plain; charset="iso-8859-1"
Content-Transfer-Encoding: 7bit
Received-SPF: none (hypatia: domain of nilsas at web.de does not designate permitted sender hosts)
X-Virus-Scanned: by amavisd-new at stat.math.ethz.ch
X-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on hypatia.math.ethz.ch
X-Spam-Level: 
X-Spam-Status: No, hits=0.0 required=5.0 tests=BAYES_50 autolearn=no version=2.63

Hello,

is there a packet that supports the Erlang distribution?

I want to use this distribution for tests against empirical data. Thus, is there a packet that also supports "fitdistr" (Maximum-likelihood fitting) for this distribution?

Thanks in advance, best regards
Nils
-- 
Nils Aschenbruck - nilsas at web.de



From Kevin.Wang at maths.anu.edu.au  Thu Aug 19 11:53:38 2004
From: Kevin.Wang at maths.anu.edu.au (Kevin Wang)
Date: Thu, 19 Aug 2004 19:53:38 +1000 (EST)
Subject: [R] how to extract data from excel sheet?
In-Reply-To: <Pine.GSO.4.44_heb2.09.0408191203060.9285-100000@techunix.technion.ac.il>
References: <Pine.GSO.4.44_heb2.09.0408191203060.9285-100000@techunix.technion.ac.il>
Message-ID: <Pine.GSO.4.58.0408191953140.18797@yin>

Hi,

Take a look at read.xls() from gregmisc package.

HTH,

Kevin
On Thu, 19 Aug 2004, Sivan Aldor wrote:

> i want to upload different sheets from one excel file.
> does anyone know of a way to do so? (i use read.csv to load the excel but
> i would like to load form the sheets themselves.)
> thanks
> sivan
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

--------------------------------
Ko-Kang Kevin Wang
PhD Student
Centre for Mathematics and its Applications
Building 27, Room 1004
Mathematical Sciences Institute (MSI)
Australian National University
Canberra, ACT 0200
Australia

Homepage: http://wwwmaths.anu.edu.au/~wangk/
Ph (W): +61-2-6125-2431
Ph (H): +61-2-6125-7407
Ph (M): +61-40-451-8301



From cbrunsdo at glam.ac.uk  Thu Aug 19 11:56:06 2004
From: cbrunsdo at glam.ac.uk (Brunsdon C (Comp))
Date: Thu, 19 Aug 2004 10:56:06 +0100
Subject: [R] OS X specific question: help.start() won't launch
Message-ID: <EF1C49A3F569D41186C900508B6DDC9915FA1DE6@ems3.glam.ac.uk>

I've got the same problem with help.start().  It seems to be a problem with
the 'open'
command when executed through R's 'system' function -

ie system('open something.html')

gives an error message,  whilst opening a new xterm (or console)
and entering 

open something.html

works fine (here 'something.html' is the same in both cases.)

My current work-around is to enter
debug(browseURL)

- this function is called by help.start -
and then to 'catch' the name of the url that is meant to be opened
by stepping through the function until the final 'system' command and
looking at
the variable 'quotedURL'.  I then open a new xterm and enter the open
command
with the value of quotedURL. Once this is done,  I make sure I don't 
close the help window for the rest of my R session!

I'm sure there must be a better way than this - perhaps somebody out there
can help.
Some other bits of info may be useful -

1) This only started happening when I upgraded to OS X 10.3.5
2) The error message I get when I try to execute 'open' from the system
function
   is as follows 
   dyld: /usr/bin/open version mismatch for library.
/usr/local/lib/libxml2.2.dylib
   (compatibility version of user: 9.0.0 greater than library's version
8.0.0)
3) I'm using R 1.9.1


Chris


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of David 
> L. Van Brunt, Ph.D.
> Sent: 19 August 2004 03:35
> To: R-help
> Subject: [R] OS X specific question: help.start() won't launch
> 
> 
> It's been a while since I used R, and have certainly applied a few 
> system patches. Since I installed the latest R.bin, when I type 
> "help.start()" nothing happens anymore. It used to launch a browser 
> with the R help system.
> 
> Anyone know of any issues here, or ways to re-enable this?
> 
> Didn't see anything searching the FAQ, just stuff on the java-based 
> search. I don't need that, just need to open the help!
> 
> Thanks,
> 	
> 	Dave VB
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From dtrenkler at nts6.oec.uni-osnabrueck.de  Thu Aug 19 12:29:32 2004
From: dtrenkler at nts6.oec.uni-osnabrueck.de (Trenkler, Dietrich)
Date: Thu, 19 Aug 2004 12:29:32 +0200
Subject: [R] Erlang distribution
Message-ID: <FB75CFC167F3D311B11D00A0CC20FB0E88555B@nts7.oec.Uni-Osnabrueck.DE>



> -----Original Message-----
> From:	Nils Aschenbruck 
> Sent:	Thursday, August 19, 2004 11:51 AM
> To:	r-help at stat.math.ethz.ch
> Subject:	[R] Erlang distribution
> 
> 
> Hello,
> 
> is there a packet that supports the Erlang distribution?
> 
> I want to use this distribution for tests against empirical data. Thus, is
> there a packet that also supports "fitdistr" (Maximum-likelihood fitting)
> for this distribution?
> 
	[Dietrich Trenkler]  Hi Nils,

	you do not need a special package because the Erlang is a special
gamma distribution.
	For instance

	"derlang" <- function(x, k, l = 1) {
	    f <- dgamma(x, k, l)
	    f
	}

	delivers the density of the Erlang(k,1) distribution.

	HTH

	D. Trenkler



From ccleland at optonline.net  Thu Aug 19 13:08:35 2004
From: ccleland at optonline.net (Chuck Cleland)
Date: Thu, 19 Aug 2004 07:08:35 -0400
Subject: [R] how to extract data from excel sheet?
In-Reply-To: <Pine.GSO.4.44_heb2.09.0408191203060.9285-100000@techunix.technion.ac.il>
References: <Pine.GSO.4.44_heb2.09.0408191203060.9285-100000@techunix.technion.ac.il>
Message-ID: <41248A33.9070607@optonline.net>

   Here is one way to read the "mysheet1" worksheet from the 
"mydata.xls" file:

library(RODBC)
          z <- odbcConnectExcel("c:\\mydata.xls")
    myframe <- sqlFetch(z, "mysheet1")
    close(z)

   Your data will be in myframe.  Thanks to Michael Lapsley 
and Brian Ripley for the RODBC package.  See the "R Data 
Import/Export" manual for more information.

Sivan Aldor wrote:
> i want to upload different sheets from one excel file.
> does anyone know of a way to do so? (i use read.csv to load the excel but
> i would like to load form the sheets themselves.)

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From fm3a004 at math.uni-hamburg.de  Thu Aug 19 13:18:59 2004
From: fm3a004 at math.uni-hamburg.de (Christian Hennig)
Date: Thu, 19 Aug 2004 13:18:59 +0200 (MEST)
Subject: [R] distance to cluster center
In-Reply-To: <830D8D4719112B418ABBC3A0EBA9581272EAD9@webmail.scitegic.com>
Message-ID: <Pine.GSO.3.95q.1040819131501.7808E-100000@sun11.math.uni-hamburg.de>

On Wed, 18 Aug 2004, Moises Hassan wrote:

> Is there a way to get the distance between each point and the center of
> the cluster it was assigned to in cluster methods such as agnes, pam,
> and clara.

I don't think that it's directly available, but for pam and clara you can
easily get it from your dissimilarity matrix using the information
given in the components clustering and
medoids of the output object (see ?pam.object, ?clara.object).

For hierarchical clustering methods as implemented in agnes, there is no
such thing as a well defined cluster center.

Christian


***********************************************************************
Christian Hennig
Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
#######################################################################
ich empfehle www.boag-online.de



From helga.neidlinger at zkrd.de  Thu Aug 19 13:26:41 2004
From: helga.neidlinger at zkrd.de (Helga Neidlinger)
Date: Thu, 19 Aug 2004 13:26:41 +0200
Subject: [R] Use Font Frutiger in PDF-File
Message-ID: <4124AA91.16208.13F9098@localhost>

Hi,

I'd like to use the font "Frutiger" in my pdf files generated by R. 
but I only manage this if I create a postscript file with 
postscript() and the option family = c( "LT_14361.afm", 
"LT_14363.afm", "LT_14373.afm", "LT_14375.afm" ) and convert this 
postscript file to pdf.
If I specify the option 'family' like above ( = c("...afm", ..., ... 
... ) ) in the function pdf(), I'll get the error message: 
"length(family)' differs between new and previous!".

Can I specify 'family' in pdf() only as "Helvetica" etc. and not as a 
vector (like above) ?
Is there no better and shorter way to get my favourite font into a 
pdf file?

Thanks,
Helga Neidlinger
-- 
Helga Neidlinger              Zentrales Knochenmarkspender-Register
Assistenz Gesch??ftsf??hrung    fuer die Bundesrepublik Deutschland
                              ZKRD gGmbH, Postfach 4244,    89032 Ulm
Tel.: (0731) 1507-12                      Helmholtzstr. 10, 89081 Ulm
Fax : (0731) 1507-51          http://www.zkrd.de



From petr.pikal at precheza.cz  Thu Aug 19 13:34:00 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 19 Aug 2004 13:34:00 +0200
Subject: [R] how to extract data from excel sheet?
In-Reply-To: <Pine.GSO.4.44_heb2.09.0408191203060.9285-100000@techunix.technion.ac.il>
Message-ID: <4124AC48.16229.147968C@localhost>



On 19 Aug 2004 at 12:05, Sivan Aldor wrote:

> i want to upload different sheets from one excel file.
> does anyone know of a way to do so? (i use read.csv to load the excel
> but i would like to load form the sheets themselves.) thanks sivan

Hi

Open Excel, go to the sheet, select rectangular area with your data 
(preferably with abbreviated names in collumn headings), press 
Ctrl-C, open R, go to command window, type:

mysheet <- read.delim("clipboard")

read suggested documentation and help pages.

Cheers
Petr

> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From m.g.walker at massey.ac.nz  Thu Aug 19 13:38:31 2004
From: m.g.walker at massey.ac.nz (Matthew Walker)
Date: Thu, 19 Aug 2004 23:38:31 +1200
Subject: [R] Clipping of display in Lattice graphics
Message-ID: <41249137.5060207@massey.ac.nz>

I'm baffled as to how the Lattice package achieves clipping.  Would 
someone mind explaining this to me?

Firstly, my attempt using "just" the grid package:

x<-seq(0,3,by=0.3)/2.8
y<-seq(0,1,by=0.1)

grid.newpage()
grid.rect(gp=gpar(fill="pink"))

vp<-viewport(width=0.8, height=0.8)
pushViewport(vp)

grid.rect(gp=gpar(fill="white"))
grid.xaxis()
grid.yaxis()

grid.points(x,y)
grid.line(x,y)

If you run this you will see a datapoint and line extend into the pink area.

However, if xyplot is used, it doesn't extend past its borders:

xyplot(y~x, xlim=c(0,1), ylim=c(0,1), type=c("p","l"))

 From what I understand so far:
xyplot calls panel.xyplot
panel.xyplot calls lpoints and llines
neither lpoints nor llines do much except call xy.coords and then call 
lplot.xy
lplot.xy calls grid.points and grid.lines

Where in this code does the clipping occur?  I'm quite baffled really.

Thank you for considering this,

Matthew



From ripley at stats.ox.ac.uk  Thu Aug 19 13:44:32 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 19 Aug 2004 12:44:32 +0100 (BST)
Subject: [R] Use Font Frutiger in PDF-File
In-Reply-To: <4124AA91.16208.13F9098@localhost>
Message-ID: <Pine.LNX.4.44.0408191238290.3928-100000@gannet.stats>

Have you read the help file?  It says (R 1.9.1)

  family: the font family to be used, one of '"AvantGarde"',
          '"Bookman"', '"Courier"', '"Helvetica"',
          '"Helvetica-Narrow"', '"NewCenturySchoolbook"', '"Palatino"'
          or '"Times"'. 

Now, it would seem if you can get away with specifying a list of *five* 
afms, but not four as in postscript.  Try

family = c("LT_14361.afm", "LT_14363.afm", "LT_14373.afm", 
           "LT_14375.afm", "sy______.afm") 

but if it does not work, remember it is undocumented.

On Thu, 19 Aug 2004, Helga Neidlinger wrote:

> Hi,
> 
> I'd like to use the font "Frutiger" in my pdf files generated by R. 
> but I only manage this if I create a postscript file with 
> postscript() and the option family = c( "LT_14361.afm", 
> "LT_14363.afm", "LT_14373.afm", "LT_14375.afm" ) and convert this 
> postscript file to pdf.
> If I specify the option 'family' like above ( = c("...afm", ..., ... 
> ... ) ) in the function pdf(), I'll get the error message: 
> "length(family)' differs between new and previous!".
> 
> Can I specify 'family' in pdf() only as "Helvetica" etc. and not as a 
> vector (like above) ?
> Is there no better and shorter way to get my favourite font into a 
> pdf file?
> 
> Thanks,
> Helga Neidlinger
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andreas.krause at pharma.novartis.com  Thu Aug 19 13:47:34 2004
From: andreas.krause at pharma.novartis.com (andreas.krause@pharma.novartis.com)
Date: Thu, 19 Aug 2004 13:47:34 +0200
Subject: [R] sorting character vectors
Message-ID: <OF71C8A8AE.AA2E5840-ONC1256EF5.003FFDA4-C1256EF5.0040C7DD@EU.novartis.net>

The following is not what I expected in sorting characters (single letters 
and the same letters with preceding spaces).
Can someone enlighten me as to why the following might be a correct result 
for sorting?

; x <- c(LETTERS[1:3], paste(" ", LETTERS[1:3], sep=""))
; x
[1] "A"  "B"  "C"  " A" " B" " C"
; sort(x)
[1] "A"  " A" "B"  " B" "C"  " C"
; sort(x, method="shell")
[1] "A"  " A" "B"  " B" "C"  " C"
; sort(x, method="quick")
[1] "A"  " A" "B"  " B" "C"  " C"

I would expect the result to be " A" " B" " C" "A"  "B"  "C" instead, 
going by ASCII codes (and a quick check with S-Plus 6.2 shows that this is 
what S-Plus thinks the sorted sequence is).

Thanks,

        Andreas Krause

PS. Version specs:

; version
         _ 
platform i686-pc-linux-gnu
arch     i686 
os       linux-gnu 
system   i686, linux-gnu 
status 
major    1 
minor    9.1 
year     2004 
month    06 
day      21 
language R



From petr.pikal at precheza.cz  Thu Aug 19 13:50:16 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 19 Aug 2004 13:50:16 +0200
Subject: [R] pdf font encoding
Message-ID: <4124B018.23080.156782E@localhost>

Dear all.

This is probably more PDF then R issue but anyway, maybe 
somebody can help me.

I try to persuade pdf device to use eastern european (Czech) 
encoding but I was not successful yet. Working on W 2000, R 
1.9.0, lattice plots.

pdf("redprov.pdf", width = 5, height = 5, bg="white", 
family="Times", encoding="WinAnsi")

The characters are OK when using other devices (screen, png, ...) 
but in case of pdf, some characters are not correctly shown nor 
printed.

After reading help pages and some archives I understand I shall 
change encoding and/or font but after I used several combinations 
without making any progress I gave up. Shall I install some other 
encoding and/or fonts?

Any help is greatly appreciated.

Best regards



Petr Pikal
petr.pikal at precheza.cz



From ripley at stats.ox.ac.uk  Thu Aug 19 14:10:32 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 19 Aug 2004 13:10:32 +0100 (BST)
Subject: [R] sorting character vectors
In-Reply-To: <OF71C8A8AE.AA2E5840-ONC1256EF5.003FFDA4-C1256EF5.0040C7DD@EU.novartis.net>
Message-ID: <Pine.LNX.4.44.0408191302440.3968-100000@gannet.stats>

It is documented to depend on your locale.  I get

>  sort(x)
[1] " A" " B" " C" "A"  "B"  "C"

in the C locale.  The help page does say so:

     The sort order for character vectors will depend on the collating
     sequence of the locale in use: see 'Comparison'.

The default collation sequences for standard locales in Linux distros are
quite unintuitive (and are not character-by-character either).  If you 
want ASCII, ask for it by LC_COLLATE=C.


On Thu, 19 Aug 2004 andreas.krause at pharma.novartis.com wrote:

> The following is not what I expected in sorting characters (single letters 
> and the same letters with preceding spaces).
> Can someone enlighten me as to why the following might be a correct result 
> for sorting?
> 
> ; x <- c(LETTERS[1:3], paste(" ", LETTERS[1:3], sep=""))
> ; x
> [1] "A"  "B"  "C"  " A" " B" " C"
> ; sort(x)
> [1] "A"  " A" "B"  " B" "C"  " C"
> ; sort(x, method="shell")
> [1] "A"  " A" "B"  " B" "C"  " C"
> ; sort(x, method="quick")
> [1] "A"  " A" "B"  " B" "C"  " C"
> 
> I would expect the result to be " A" " B" " C" "A"  "B"  "C" instead, 
> going by ASCII codes (and a quick check with S-Plus 6.2 shows that this is 
> what S-Plus thinks the sorted sequence is).

That explicitly says it uses ASCII.  I believe that is a deficiency they 
plan to correct.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rpeng at jhsph.edu  Thu Aug 19 14:16:23 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 19 Aug 2004 08:16:23 -0400
Subject: [R] sorting character vectors
In-Reply-To: <OF71C8A8AE.AA2E5840-ONC1256EF5.003FFDA4-C1256EF5.0040C7DD@EU.novartis.net>
References: <OF71C8A8AE.AA2E5840-ONC1256EF5.003FFDA4-C1256EF5.0040C7DD@EU.novartis.net>
Message-ID: <41249A17.1040409@jhsph.edu>

Sorting depends on the locale.  For example, I get

 > x <- c(LETTERS[1:3], paste(" ", LETTERS[1:3], sep=""))
 > x
[1] "A"  "B"  "C"  " A" " B" " C"
 > sort(x)
[1] " A" " B" " C" "A"  "B"  "C"
 >

On Linux (Fedora Core 1) I set LANG=C, which is necessary for some 
other (non-R) things to work.

-roger

andreas.krause at pharma.novartis.com wrote:
> The following is not what I expected in sorting characters (single letters 
> and the same letters with preceding spaces).
> Can someone enlighten me as to why the following might be a correct result 
> for sorting?
> 
> ; x <- c(LETTERS[1:3], paste(" ", LETTERS[1:3], sep=""))
> ; x
> [1] "A"  "B"  "C"  " A" " B" " C"
> ; sort(x)
> [1] "A"  " A" "B"  " B" "C"  " C"
> ; sort(x, method="shell")
> [1] "A"  " A" "B"  " B" "C"  " C"
> ; sort(x, method="quick")
> [1] "A"  " A" "B"  " B" "C"  " C"
> 
> I would expect the result to be " A" " B" " C" "A"  "B"  "C" instead, 
> going by ASCII codes (and a quick check with S-Plus 6.2 shows that this is 
> what S-Plus thinks the sorted sequence is).
> 
> Thanks,
> 
>         Andreas Krause
> 
> PS. Version specs:
> 
> ; version
>          _ 
> platform i686-pc-linux-gnu
> arch     i686 
> os       linux-gnu 
> system   i686, linux-gnu 
> status 
> major    1 
> minor    9.1 
> year     2004 
> month    06 
> day      21 
> language R
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ben.yip at webmail.ki.se  Thu Aug 19 14:19:17 2004
From: ben.yip at webmail.ki.se (Ben Yip)
Date: Thu, 19 Aug 2004 14:19:17 +0200
Subject: [R] Fortran 90  in R?
Message-ID: <41249AC5.3080109@webmail.ki.se>

Hi there,

I am using Fortran 77 g77 compiler to write Fortran subprogram to R. I 
would like to change and write Fortran 90 code instead (no more 
specification of matrix!).  Does gcc support Fortran 90? And if Yes, 
does it works in R?

Ben



From andy_liaw at merck.com  Thu Aug 19 14:18:33 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 19 Aug 2004 08:18:33 -0400
Subject: [R] sorting character vectors
Message-ID: <3A822319EB35174CA3714066D590DCD504AF8247@usrymx25.merck.com>

I do get (R-1.9.1 on WinXPPro):

> x <- c(LETTERS[1:3], paste(" ", LETTERS[1:3], sep=""))
> sort(x)
[1] " A" " B" " C" "A"  "B"  "C" 

The `right' sequence is locale dependent, and I wonder if that's the
discrepancy here.

Andy


> From: andreas.krause at pharma.novartis.com
> 
> The following is not what I expected in sorting characters 
> (single letters 
> and the same letters with preceding spaces).
> Can someone enlighten me as to why the following might be a 
> correct result 
> for sorting?
> 
> ; x <- c(LETTERS[1:3], paste(" ", LETTERS[1:3], sep=""))
> ; x
> [1] "A"  "B"  "C"  " A" " B" " C"
> ; sort(x)
> [1] "A"  " A" "B"  " B" "C"  " C"
> ; sort(x, method="shell")
> [1] "A"  " A" "B"  " B" "C"  " C"
> ; sort(x, method="quick")
> [1] "A"  " A" "B"  " B" "C"  " C"
> 
> I would expect the result to be " A" " B" " C" "A"  "B"  "C" instead, 
> going by ASCII codes (and a quick check with S-Plus 6.2 shows 
> that this is 
> what S-Plus thinks the sorted sequence is).
> 
> Thanks,
> 
>         Andreas Krause
> 
> PS. Version specs:
> 
> ; version
>          _ 
> platform i686-pc-linux-gnu
> arch     i686 
> os       linux-gnu 
> system   i686, linux-gnu 
> status 
> major    1 
> minor    9.1 
> year     2004 
> month    06 
> day      21 
> language R
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From HankeA at mar.dfo-mpo.gc.ca  Thu Aug 19 14:36:22 2004
From: HankeA at mar.dfo-mpo.gc.ca (Hanke, Alex)
Date: Thu, 19 Aug 2004 09:36:22 -0300
Subject: [R] A question about external time-dependent covariates in co	x
	model
Message-ID: <E37EEC6DE3A0C5439B7E7B07406C24AE124A08@msgmarsta01.bio.dfo.ca>

Dear Rui,
>From my understanding of time-dependent covariates (not an expert but have
been working on a similar problem), it would appear that the coding of the
status column is not correct. Unless you have observed an event at each
interval you should only have status=1 for the last interval. In your
example I see 3 in total. Also, I think that if "end" is proportional to
your "covariate" you are incorporating a redundant time effect into the
model. The time effect is in the baseline hazard.

Alex
-----Original Message-----
From: Rui Song [mailto:rsong at stat.wisc.edu] 
Sent: August 19, 2004 12:21 AM
To: r-help at stat.math.ethz.ch
Subject: [R] A question about external time-dependent covariates in cox
model


Dear Sir or Madam:
I am a graduate student in UW-Madison statistics department. I have a
question about fitting a cox model with external time-dependent
covariates.

Say the original data is in the following format:
Obs Eventtime  Status  Cov(time=5)  Cov(time=8)  Cov(time=10)	Cov(time=12)
1	5	1		2
2	8	0(censored)	2	4
3	10	1		2	4		6
4	12	1		2	4		6		8
....

Notice that the time-dependent covariates are identical at the same
time points for all obs since they are external to the failure process.
process.

Then I organized the data as the following:
obs	start	end	eventtime	status	cov
1	0	5	5		1	2
2	0	5	8		0	2
2	5	8	8		0	4
3	0	5	10		1	2
3	5	8	10		1	4
3	8	10	10		1	6
4	0	5	12		1	2
4	5	8	12		1	4
4	8	10	12		1	6
4	10	12	12		1	8

And fit the model using:

fit<-coxph(Surv(start, end, status)~cov);

When I fit the model to my data set (Which has 89 observations and 81
distinct time points, sort of large.), I always got a message that
"Process R segmentation fault (core dumped)". Would you let me know if it
is due to the matrix sigularity in the computation of the partial
likelihood or something else? And how should I fit a cox model with
external time-dependent covariates?

Thanks a lot for your time and help!

Sincerely,
Rui Song

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Thu Aug 19 14:47:36 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 19 Aug 2004 12:47:36 +0000 (UTC)
Subject: [R] labeled break statements in R?
References: <258yccz3cv.fsf@stanford.edu>
	<loom.20040819T060129-24@post.gmane.org>
Message-ID: <loom.20040819T144542-598@post.gmane.org>

Gabor Grothendieck <ggrothendieck <at> myway.com> writes:
> Roger Levy <rog <at> stanford.edu> writes:
> : Are there labeled break statements in R?  i.e., something along the
> : lines of
> : 
> : TOPLOOP: for(i in 1:m) {
> :   for(j in 1:n) {
> :     ...
> :     if(condition) {
> :        break TOPLOOP
> :     }
> :   }
> : }
> 
> Assuming that your labelled break is supposed to break out of
> both loops, unlike an ordinary break that just breaks out of
> the inner loop, this is how it would be done:
> 
> a <- matrix(0, nr=3, nc=3)
> local({
> 	for(i in 1:3) 
> 		for(j in 1:3)
> 			if (i+j>4) return() else a[i,j] <<- i+j
> })
> a
> 
> Be aware that assigning variables within the local will assign
> local copies unless you use <<-, assign(..., ..., parent.frame())
> or eval.parent(...).

An improvement over the above is to use evalq as this does not set
up a local environment, thereby avoiding the problems cited in
the last sentence:

a <- matrix(0, nr=3, nc=3)
evalq({
     for(i in 1:3)
          for(j in 1:3)
               if (i+j>4) return() else a[i,j] <- i+j
})
a

Note that we were able to replace the <<- with <-.



From ripley at stats.ox.ac.uk  Thu Aug 19 14:50:47 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 19 Aug 2004 13:50:47 +0100 (BST)
Subject: [R] Fortran 90  in R?
In-Reply-To: <41249AC5.3080109@webmail.ki.se>
Message-ID: <Pine.LNX.4.44.0408191341370.6353-100000@gannet.stats>

On Thu, 19 Aug 2004, Ben Yip wrote:

> I am using Fortran 77 g77 compiler to write Fortran subprogram to R. I 
> would like to change and write Fortran 90 code instead (no more 
> specification of matrix!).  Does gcc support Fortran 90? 

Isn't that a question for a gcc-help list?  Take a look at
http://gcc.gnu.org/fortran/ or do your own Googling.

> And if Yes, does it works in R?

N/A.  Other Fortran 95 compilers do work with R, e.g. on Solaris.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andreas.krause at pharma.novartis.com  Thu Aug 19 14:52:42 2004
From: andreas.krause at pharma.novartis.com (andreas.krause@pharma.novartis.com)
Date: Thu, 19 Aug 2004 14:52:42 +0200
Subject: [R] sorting character vectors
Message-ID: <OF7F4F8AA1.0E612698-ONC1256EF5.0045A7E1-C1256EF5.0046BE40@EU.novartis.net>

Thank you very much to Brian Ripley, Roger Peng, and Andy Liaw. Everyone 
pointed out the same solution.
Setting
LC_COLLATE=C
did it. This default setting is indeed odd to me.

Thanks again!

        Andreas Krause



From ripley at stats.ox.ac.uk  Thu Aug 19 14:56:55 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 19 Aug 2004 13:56:55 +0100 (BST)
Subject: [R] OS X specific question: help.start() won't launch
In-Reply-To: <EF1C49A3F569D41186C900508B6DDC9915FA1DE6@ems3.glam.ac.uk>
Message-ID: <Pine.LNX.4.44.0408191353430.6353-100000@gannet.stats>

You need to find out what put /usr/local/lib/libxml2.2.dylib
there and correct it.  Look at PR#7063 in the R-bugs archive.
Stefano Iacus said there

  Please remove your copy of libxml2 in /usr/local/lib, you no longer
  need it on Panther and the one included in 10.3.4 is incompatible with
  the one in /usr/local/lib 

On Thu, 19 Aug 2004, Brunsdon C (Comp) wrote:

> I've got the same problem with help.start().  It seems to be a problem with
> the 'open'
> command when executed through R's 'system' function -
> 
> ie system('open something.html')
> 
> gives an error message,  whilst opening a new xterm (or console)
> and entering 
> 
> open something.html
> 
> works fine (here 'something.html' is the same in both cases.)
> 
> My current work-around is to enter
> debug(browseURL)
> 
> - this function is called by help.start -
> and then to 'catch' the name of the url that is meant to be opened
> by stepping through the function until the final 'system' command and
> looking at
> the variable 'quotedURL'.  I then open a new xterm and enter the open
> command
> with the value of quotedURL. Once this is done,  I make sure I don't 
> close the help window for the rest of my R session!
> 
> I'm sure there must be a better way than this - perhaps somebody out there
> can help.
> Some other bits of info may be useful -
> 
> 1) This only started happening when I upgraded to OS X 10.3.5
> 2) The error message I get when I try to execute 'open' from the system
> function
>    is as follows 
>    dyld: /usr/bin/open version mismatch for library.
> /usr/local/lib/libxml2.2.dylib
>    (compatibility version of user: 9.0.0 greater than library's version
> 8.0.0)
> 3) I'm using R 1.9.1
> 
> 
> Chris
> 
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of David 
> > L. Van Brunt, Ph.D.
> > Sent: 19 August 2004 03:35
> > To: R-help
> > Subject: [R] OS X specific question: help.start() won't launch
> > 
> > 
> > It's been a while since I used R, and have certainly applied a few 
> > system patches. Since I installed the latest R.bin, when I type 
> > "help.start()" nothing happens anymore. It used to launch a browser 
> > with the R help system.
> > 
> > Anyone know of any issues here, or ways to re-enable this?
> > 
> > Didn't see anything searching the FAQ, just stuff on the java-based 
> > search. I don't need that, just need to open the help!
> > 
> > Thanks,
> > 	
> > 	Dave VB
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read 
> > the posting guide! http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From charles.edwin.white at us.army.mil  Thu Aug 19 14:53:36 2004
From: charles.edwin.white at us.army.mil (White, Charles E WRAIR-Wash DC)
Date: Thu, 19 Aug 2004 08:53:36 -0400
Subject: [R] Is R good for not-professional-statistician,
	un-mathematical clinical researchers?
Message-ID: <12D0D00E1404D511A4820090274CA09C03FBA746@dasmtyjqf010.amedd.army.mil>

I'm not in your target audience but I consult with and teach to your target audience, so I'll chime in anyway. 

1. Look at the "user friendliness" of the alternatives you offered to R. I haven't used SPSS recently or Stata at all but I know SAS is NOT that user friendly. Note that my institute has historically used Minitab for teaching our introductory course in biostatistics. 
2. Look for "user friendly" front ends to R. Professor John Fox has late beta software for teaching introductory statistics (see: http://socserv.mcmaster.ca/jfox/Misc/Rcmdr/). If you would like to add procedures to those provided by the basic front end, Rcmdr is written so that you can. I plan to do that and teach a stand alone seminar on R through Rcmdr in the late fall or early winter.
3. Last year, outside of our biostatistics class, I taught a seminar on installing and basic use of command line R. At first, I was concerned that I wouldn't be able to fill the 20 seats in our computer lab. In response to my advertisement within WRAIR, I actually had to add three more sections. Classes were received enthusiastically and students stayed after each seminar for continued discussion. Unfortunately, the small number of students I consulted with subsequently indicated that they never used R after my seminar. 

Chuck 

Charles E. White, Senior Biostatistician, MS
Walter Reed Army Institute of Research
503 Robert Grant Ave., Room 1w102
Silver Spring, MD 20910-1557
301 319-9781
Personal/Professional Site: http://users.starpower.net/cwhite571/professional/



From ggrothendieck at myway.com  Thu Aug 19 15:17:21 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 19 Aug 2004 13:17:21 +0000 (UTC)
Subject: [R] Suggestion for posting guide
Message-ID: <loom.20040819T151603-570@post.gmane.org>


I have a suggestion for the posting guide.  One problem with some posts is that
they do not provide an example that can be reproduced.   I think that many
people just do not know how to easily specify some data and some technical
assistance should be provided in the posting guide.  If the problem
depends on specific data they should be made aware, in the posting guide, of:

   dput(x) 

since that outputs object x as R code which can then be easily copied from the
post and pasted into a session.  If its not dependent on particular data they
can generate patterned or random data IF THEY KNOW HOW but many might find it
easier to just use one of the included datasets so some guidance should be
provided on the contents of a few of them, e.g.

R comes with built in data sets.  data() will list them, data(iris) will
attach data set iris and ?iris, str(iris), summary(iris), head(iris)
and dput(iris) will give more information on iris (after attaching it).
The following are a few of the datasets that come with R:

   iris - data frame with 4 numeric columns and one 3 level factor
   nhtemp - a ts class time series 
   faithful - data frame with two numeric columns
   warpbreaks - data with a numeric column, a 2-level factor & a 3-level factor

Also letters, LETTERS, month.abb and month.name are built in character vectors
that do not require a data statement to access.



From cts at debian.org  Thu Aug 19 15:17:27 2004
From: cts at debian.org (Christian T. Steigies)
Date: Thu, 19 Aug 2004 15:17:27 +0200
Subject: [R] Fortran 90  in R?
In-Reply-To: <Pine.LNX.4.44.0408191341370.6353-100000@gannet.stats>
References: <41249AC5.3080109@webmail.ki.se>
	<Pine.LNX.4.44.0408191341370.6353-100000@gannet.stats>
Message-ID: <20040819131727.GA12767@skeeve>

On Thu, Aug 19, 2004 at 01:50:47PM +0100, Prof Brian Ripley wrote:
> On Thu, 19 Aug 2004, Ben Yip wrote:
> 
> > I am using Fortran 77 g77 compiler to write Fortran subprogram to R. I 
> > would like to change and write Fortran 90 code instead (no more 
> > specification of matrix!).  Does gcc support Fortran 90? 
> 
> Isn't that a question for a gcc-help list?  Take a look at
> http://gcc.gnu.org/fortran/ or do your own Googling.

man g77
[...]
       -ff90
           Allow certain Fortran-90 constructs.

           This option controls whether certain Fortran 90 constructs are
           recognized.  (Other Fortran 90 constructs might or might not be
           recognized depending on other options such as -fvxt,
           -ff90-intrinsics-enable, and the current level of support for
           Fortran 90.)

Christian



From p.dalgaard at biostat.ku.dk  Thu Aug 19 15:21:06 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 19 Aug 2004 15:21:06 +0200
Subject: [R] sorting character vectors
In-Reply-To: <OF7F4F8AA1.0E612698-ONC1256EF5.0045A7E1-C1256EF5.0046BE40@EU.novartis.net>
References: <OF7F4F8AA1.0E612698-ONC1256EF5.0045A7E1-C1256EF5.0046BE40@EU.novartis.net>
Message-ID: <x2657fuuv1.fsf@biostat.ku.dk>

andreas.krause at pharma.novartis.com writes:

> Thank you very much to Brian Ripley, Roger Peng, and Andy Liaw. Everyone 
> pointed out the same solution.
> Setting
> LC_COLLATE=C
> did it. This default setting is indeed odd to me.

It's odd to everyone, except perhaps to the POSIX locale mafia... Once
upon a time all dotfile sorted to the top of but nowadays "LANG=da_DK
ls -a | less" gives you

...
a2ps
a2ps-4.13-3.i386.rpm
abcd.txt
.AbiSuite
.ab_library
abline.ps
.acrobat
.acrorc
.acrosrch
active.rtf
...

Oddly enough da_DK does sort names with a leading space before
characters, whereas de_DE does not.


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From rolf at math.unb.ca  Thu Aug 19 15:33:22 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Thu, 19 Aug 2004 10:33:22 -0300 (ADT)
Subject: [R] Suggestion for posting guide
Message-ID: <200408191333.i7JDXMm3005686@erdos.math.unb.ca>


Technical point:

Gabor Grothendieck wrote

	``data(iris) will attach data set iris''

This is not true --- data(iris) put ``iris'' into the Global
environment; if one wants it attached, one has to do the follow up by
attach(iris).

			cheers,

				Rolf Turner
				rolf at math.unb.ca



From sokamp at web.de  Thu Aug 19 15:47:09 2004
From: sokamp at web.de (Susanne Osterkamp)
Date: Thu, 19 Aug 2004 15:47:09 +0200
Subject: [R] How to randomize a set of integers in R
Message-ID: <260279960@web.de>

Content-Type: text/plain; charset="iso-8859-1"
Content-Transfer-Encoding: 7bit
Received-SPF: none (hypatia: domain of sokamp at web.de does not designate permitted sender hosts)
X-Virus-Scanned: by amavisd-new at stat.math.ethz.ch
X-Spam-Checker-Version: SpamAssassin 2.63 (2004-01-11) on hypatia.math.ethz.ch
X-Spam-Level: ***
X-Spam-Status: No, hits=3.2 required=5.0 tests=AWL,BAYES_60,RCVD_IN_BL_SPAMCOP_NET autolearn=no version=2.63

Hello,

I want  to validate my data for working with CART.
I need to now how to create a randomized column of some 6800 intgers within R . I did not understand the manual concerning the possibilities to random data, sorry.
Thanks a lot for your help!

Susanne Osterkamp


From almirall at rand.org  Thu Aug 19 15:53:58 2004
From: almirall at rand.org (Almirall, Daniel)
Date: Thu, 19 Aug 2004 09:53:58 -0400
Subject: [R] Ques about the 'test.terms' argument in 'regTermTest'
Message-ID: <5A637F509C50B444BDA096EB2D7BC15842C48E@pghmail2.rand.org>

Hi,

Z is a three level factor variables and ZB and ZC are its corresponding binary dummy variables.  In a 'glm' of Y on Z and X, say, how do the two test specifications

	test.terms = c("ZB:X","ZC:X")  # and
	test.terms = ~ ZB:X + ZC:X

in 'regTermTest' differ?  I thought that both would return the same joint (Wald) test for the two Z:X interactions.  Why does the second one specify a 1 degree of freedom test?  The code below should help clarify my question.

Thanks much, 
Danny




## I'm currently using: R Version 1.9.1 / Windows 2000 / P4/2.8 Ghz

Z <- as.factor(rep(LETTERS[1:3],20))
Y <- rep(0:1, 30)
X <- rnorm(60)

glm1 <- glm(Y ~ Z + X + Z:X, family=binomial)
summary(glm1)$coeff

regTermTest( model=glm1 , test.terms=~Z:X)

ZB <- ifelse(Z=="B",1,0)
ZC <- ifelse(Z=="C",1,0)

glm2 <- glm(Y ~ ZB + ZC + X + ZB:X + ZC:X, family=binomial)
summary(glm2)$coeff 		## Okay, same as glm1

regTermTest( model=glm2 , test.terms= c("ZB:X","ZC:X"))
regTermTest( model=glm2 , test.terms= ~ ZB:X + ZC:X)



From olafm at tako.de  Thu Aug 19 15:59:28 2004
From: olafm at tako.de (Olaf Mersmann)
Date: Thu, 19 Aug 2004 15:59:28 +0200
Subject: [R] How to randomize a set of integers in R
In-Reply-To: <260279960@web.de>
References: <260279960@web.de>
Message-ID: <20040819135928.GA13202@kimberly.tako.de>

Hi Susanne,
* Susanne Osterkamp <sokamp at web.de> [040819 15:52]:
> Hello,
> 
> I want  to validate my data for working with CART.
> I need to now how to create a randomized column of some 6800 intgers
> within R. 

Do you want to sample 6800 random integers? In that case try ?runif
(for uniform distribution) 

If you want to resample 6800 integers try ?sample

HTH
Olaf Mersmann



From gb at stat.umu.se  Thu Aug 19 21:56:34 2004
From: gb at stat.umu.se (Gran Brostrm)
Date: Thu, 19 Aug 2004 15:56:34 -0400
Subject: [R] A question about external time-dependent covariates in co	x
	model
In-Reply-To: <E37EEC6DE3A0C5439B7E7B07406C24AE124A08@msgmarsta01.bio.dfo.ca>
References: <E37EEC6DE3A0C5439B7E7B07406C24AE124A08@msgmarsta01.bio.dfo.ca>
Message-ID: <20040819195634.GA12139@stat.umu.se>

On Thu, Aug 19, 2004 at 09:36:22AM -0300, Hanke, Alex wrote:
> Dear Rui,
> >From my understanding of time-dependent covariates (not an expert but have
> been working on a similar problem), it would appear that the coding of the
> status column is not correct. Unless you have observed an event at each
> interval you should only have status=1 for the last interval. In your
> example I see 3 in total. Also, I think that if "end" is proportional to
> your "covariate" you are incorporating a redundant time effect into the
> model. The time effect is in the baseline hazard.

Right, the 'splitting' was made incorrectly, but 'coxph' shouldn't
segfault anyway. The error seems to be (caught) in 'coxph_wtest.c',
line 29, which may be of interest to the R maintainer of 'survival', 
Thomas L.

G??ran

> 
> Alex
> -----Original Message-----
> From: Rui Song [mailto:rsong at stat.wisc.edu] 
> Sent: August 19, 2004 12:21 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] A question about external time-dependent covariates in cox
> model
> 
> 
> Dear Sir or Madam:
> I am a graduate student in UW-Madison statistics department. I have a
> question about fitting a cox model with external time-dependent
> covariates.
> 
> Say the original data is in the following format:
> Obs Eventtime  Status  Cov(time=5)  Cov(time=8)  Cov(time=10)	Cov(time=12)
> 1	5	1		2
> 2	8	0(censored)	2	4
> 3	10	1		2	4		6
> 4	12	1		2	4		6		8
> ....
> 
> Notice that the time-dependent covariates are identical at the same
> time points for all obs since they are external to the failure process.
> process.
> 
> Then I organized the data as the following:
> obs	start	end	eventtime	status	cov
> 1	0	5	5		1	2
> 2	0	5	8		0	2
> 2	5	8	8		0	4
> 3	0	5	10		1	2
> 3	5	8	10		1	4
> 3	8	10	10		1	6
> 4	0	5	12		1	2
> 4	5	8	12		1	4
> 4	8	10	12		1	6
> 4	10	12	12		1	8
> 
> And fit the model using:
> 
> fit<-coxph(Surv(start, end, status)~cov);
> 
> When I fit the model to my data set (Which has 89 observations and 81
> distinct time points, sort of large.), I always got a message that
> "Process R segmentation fault (core dumped)". Would you let me know if it
> is due to the matrix sigularity in the computation of the partial
> likelihood or something else? And how should I fit a cox model with
> external time-dependent covariates?
> 
> Thanks a lot for your time and help!
> 
> Sincerely,
> Rui Song
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
 G??ran Brostr??m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume?? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume??, Sweden             e-mail: gb at stat.umu.se



From rogboone at yahoo.com  Thu Aug 19 16:01:20 2004
From: rogboone at yahoo.com (Roggie Boone)
Date: Thu, 19 Aug 2004 07:01:20 -0700 (PDT)
Subject: [R] RMySQL, Solaris and getopt????
Message-ID: <20040819140120.31160.qmail@web60504.mail.yahoo.com>

Hi,

I've been struggling with RMySQL installation
for several days.  I'm running Solaris 2.9
and MySQL 4.0. RMySQL (v0.4.3) seems to install
OK, but when I load it in R I get the message:
-------
> library(RMySQL)
Error in dyn.load(x, as.logical(local),
as.logical(now)) :
        unable to load shared library
"/local/rboone/SOFTWARE/R-1.9.1/library/RMySQL/libs/RMySQL.so":
  ld.so.1: /local/rboone/SOFTWARE/R-1.9.1/bin/R.bin:
fatal: relocation error: file
/local/rboone/SOFTWARE/R-1.9.1/library/RMySQL/libs/RMySQL.so:
symbol getopt_long: referenced symbol not found
Error in library(RMySQL) : .First.lib failed
-------

It appears that getopt_long is not linked in.
It does not appear to be in the standard libs
on Solaris (I am on a campus network and use
our network version of gcc compiler).
In the RMySQL distribution, I 
see source code for getopt.c and getopt.h, but
I don't understand what to do with them.  I tried
"gcc -c getopt.c -o getopt.o" but it produced
a screenfull of parse errors, etc.

Anyone have suggestions on what to try?
Thanks,
Roggie Boone



From baron at psych.upenn.edu  Thu Aug 19 16:03:26 2004
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Thu, 19 Aug 2004 10:03:26 -0400
Subject: [R] Is R good for not-professional-statistician,
	un-mathematical clinical researchers?
In-Reply-To: <12D0D00E1404D511A4820090274CA09C03FBA746@dasmtyjqf010.amedd.army.mil>
References: <12D0D00E1404D511A4820090274CA09C03FBA746@dasmtyjqf010.amedd.army.mil>
Message-ID: <20040819140326.GA2764@psych>

A few comments.

1. What students use.

I've gotten a few undergraduate psychology students to use R to
analyze their data for independent study projects.  They do fine.
I prepare the data for them and show them how to do t tests and
correlations, and how to define new variables from old ones.
They write down what I showed them, and they do it.  They would
have to do the same thing with any data analysis program.

Some of these students will go to graduate school.  Then, when
they analyze data, they will do with we advise all our grad
students to do, "Use whatever statistics program your advisor
uses."  In psychology, that is sometimes R/S, but not often.

But it is often SAS or Matlab.  (Perception people seem to use
Matlab for everything.)  SAS, Matlab, and R all require writing
scripts and looking up things in manuals.  I see no difference in
difficulty, and of course I think R is better for most of what
psychologists do.  For grad students, the way to their hearts is
through their advisors.

Most undergrads do the same thing, using what their advisor uses,
and that is usually Systat or SPSS.  (The faculty who use R, SAS,
and Matlab as not as popular as undergrad research advisors as
those who use Systat and SPSS.)  Whatever they use, it will be a
one-time thing, unless they go to grad school, and then it will
be pretty unlikely that they will wind up using the same program.

My bottom line is that teaching R, as opposed to other options,
does very little harm and has very little long-term benefit, but
perhaps more benefit, because ...

2. The bigger issue.

This is all part of what I see as a larger issue concerning how
research is done.  In the 60s, when I got my formal education, it
was normal for bigshot professors to build their own apparatus,
run their own subjects, and analyze their own data.  I watches my
advisor wire relay racks for operant experiments.  When I got to
do it myself, I found it fun, and I still do everything myself
(except pay my subjects, which the business office won't let me
do).

By contrast, today's young researchers have the attitude that
their role in life is to write grant proposals.  When they hit
the jackpot (which often takes many tries), they then hire people
to do the real work.  They hire programmers to develop their
experiments and statisticians to analyze their data.  Granting
agencies seem to encourage this.  I've seen reviews suggesting
that a statistical consultant be added to a proposal because the
researchers proposed doing t tests (which were all that were
needed).  This attitude percolates down to graduate students and
even undergraduates.

The upshot is that fewer and fewer students are learning the nuts
and bolts of research.  (The one essential thing they all learn
is how to use PowerPoint.)  It isn't just statistics.  It is also
computer programming, electrical engineering, and so on.

The students (and faculty) who take to R are the old fashioned
ones, the ones who also write computer programs for their own
experiments, and like it.  The ones who roll up their sleeves.
Perhaps the downturn in the software industry will encourage some
of these students to stay in acadmic fields rather than saying to
themselves, "Why I am in grad school when I could be making 5
times my income in software?"  Indeed, we are now starting to see
grad-school applications from ex-software-engineers (along with
the steady stream from ex-lawyers).

It would be nice if grant reviewers would encourage this sort of
thing, by questioning big staff budgets rather than asking that
they be bigger.  Social science research could benefit from more,
but smaller, grants.

Teaching R is part of this "battle," since it conveys an attitude
as well as specific knowledge.  I guess that is the main reason I
plan to keep trying to do it.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron
R search page: http://finzi.psych.upenn.edu/



From deepayan at stat.wisc.edu  Thu Aug 19 16:14:29 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 19 Aug 2004 09:14:29 -0500
Subject: [R] Clipping of display in Lattice graphics
In-Reply-To: <41249137.5060207@massey.ac.nz>
References: <41249137.5060207@massey.ac.nz>
Message-ID: <200408190914.30212.deepayan@stat.wisc.edu>

On Thursday 19 August 2004 06:38, Matthew Walker wrote:
> I'm baffled as to how the Lattice package achieves clipping.  Would
> someone mind explaining this to me?
>
> Firstly, my attempt using "just" the grid package:
>
> x<-seq(0,3,by=0.3)/2.8
> y<-seq(0,1,by=0.1)
>
> grid.newpage()
> grid.rect(gp=gpar(fill="pink"))
>
> vp<-viewport(width=0.8, height=0.8)

You need to add 'clip="on"' here. Clipping is a property of viewports 
(not individual 'grob'-s as produced by grid.points and grid.lines),  
controlled by the 'clip' argument to viewport(). Details (and caveats) 
in ?viewport.

Hth,

Deepayan



From gudrunj at math.su.se  Thu Aug 19 16:15:03 2004
From: gudrunj at math.su.se (Gudrun Jonasdottir)
Date: Thu, 19 Aug 2004 16:15:03 +0200 (CEST)
Subject: [R] convert strings to object names
Message-ID: <4032.193.10.23.33.1092924903.squirrel@webmail.math.su.se>

Dear R-Help list,

I have a problem with convertions of strings. I want to use the function
"paste()" to create an object name and then use that character string to
call on that object. So, for example:

dat99 <- matrix(rbind(1,1,2),3,3)
no <- 99
dat <- paste("dat",no,sep="")
dat
[1] "dat99"

What should I do to get the output

dat
     [,1] [,2] [,3]
[1,]    1    1    1
[2,]    1    1    1
[3,]    2    2    2

Cheers,
Gudrun



-- 
Gudrun Jonasdottir, M.Sc.
Matematiska institutionen
Stockholms Universitet
SE- 106 91 Stockholm

Work: +46 (0)8 16 45 56
Mobile: +46 (0)709 779 800



From andy_liaw at merck.com  Thu Aug 19 16:19:28 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 19 Aug 2004 10:19:28 -0400
Subject: [R] How to randomize a set of integers in R
Message-ID: <3A822319EB35174CA3714066D590DCD504AF8248@usrymx25.merck.com>

Use sample() in either case:

## randomly shuffle the elements of the vector `x':
rx <- sample(x, replace=FALSE)

## randomly draw 6800 integers between 1 and 6800:
x <- sample(6800, 6800, replace=TRUE)

Andy

> From: Olaf Mersmann
> 
> Hi Susanne,
> * Susanne Osterkamp <sokamp at web.de> [040819 15:52]:
> > Hello,
> > 
> > I want  to validate my data for working with CART.
> > I need to now how to create a randomized column of some 6800 intgers
> > within R. 
> 
> Do you want to sample 6800 random integers? In that case try ?runif
> (for uniform distribution) 
> 
> If you want to resample 6800 integers try ?sample
> 
> HTH
> Olaf Mersmann
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From andy_liaw at merck.com  Thu Aug 19 16:31:54 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 19 Aug 2004 10:31:54 -0400
Subject: [R] convert strings to object names
Message-ID: <3A822319EB35174CA3714066D590DCD504AF824A@usrymx25.merck.com>

See ?get.

Andy

> From: Gudrun Jonasdottir
> 
> Dear R-Help list,
> 
> I have a problem with convertions of strings. I want to use 
> the function
> "paste()" to create an object name and then use that 
> character string to
> call on that object. So, for example:
> 
> dat99 <- matrix(rbind(1,1,2),3,3)
> no <- 99
> dat <- paste("dat",no,sep="")
> dat
> [1] "dat99"
> 
> What should I do to get the output
> 
> dat
>      [,1] [,2] [,3]
> [1,]    1    1    1
> [2,]    1    1    1
> [3,]    2    2    2
> 
> Cheers,
> Gudrun
> 
> 
> 
> -- 
> Gudrun Jonasdottir, M.Sc.
> Matematiska institutionen
> Stockholms Universitet
> SE- 106 91 Stockholm
> 
> Work: +46 (0)8 16 45 56
> Mobile: +46 (0)709 779 800
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From MSchwartz at MedAnalytics.com  Thu Aug 19 16:33:02 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 19 Aug 2004 09:33:02 -0500
Subject: [R] Is R good for not-professional-statistician,
	un-mathematical clinical researchers?
In-Reply-To: <Pine.OSX.4.53.0408182313310.3661@biostat5.ucdavis.edu>
References: <Pine.OSX.4.53.0408182313310.3661@biostat5.ucdavis.edu>
Message-ID: <1092925982.30114.49.camel@localhost.localdomain>

On Thu, 2004-08-19 at 01:45, Jacob Wegelin wrote:
> Alternate title: How can I persuade my students that R is for them?
> 
> Alternate title: Can R replace SAS, SPSS or Stata for clinicians?
> 
> I am teaching introductory statistics to twelve physicians and two veterinarians
> who have enrolled in a Mentored Clinical Research Training Program.  My course is the
> first in a sequence of three.  We (the instructors of this sequence) chose to teach
> R rather than some other computing environment.
> 
> My (highly motivated) students have never encountered anything like R.  One frankly
> asked:
> 
> "Do you feel (honestly) that a group of physicians (with two vets) clinicians will
> be able to effectively use and actually understand R? If so, I will happily call this
> bookstore and order this book [Venables and Ripley] tomorrow."
> 
> I am heavily biased toward R/S because I have used it since the first applied statistics
> course I took.  But I would love to give these students some kind of objective information
> about the usability of R by non-statisticians--not just my own bias.
> 
> Could anyone suggest any such information?  Or does anyone on this list use R who is
> a clinician and not really mathematically savvy?  For instance, someone who doesn't
> remember any math beyond algebra and doesn't think in terms of P(A|B)?
> 
> Or have we done a disservice to our students by choosing to make them
> learn R, rather than making ourselves learn SAS, Stata or SPSS?
> 
> Thank you for any ideas
> 
> Jake Wegelin


A couple of questions:

1. What is the intended goal of the series of classes?

2. What are the expectations of the clinicians for themselves and what
is their likely career path?


Possible answers to the questions:

1. Provide the clinicians a reasonable (and perhaps broad) foundation of
statistical knowledge.

2. To be able to have a reasonable comprehension of statistical concepts
and methods so that in the future, as they are busy with patients
(animals for the vets) in a clinical practice, they can intelligently
interact with formally trained statisticians when engaged in clinical
research in a multi-disciplinary team environment.


If the above is close to reality, then let me suggest that you consider
Peter's book "Introductory Statistics with R" rather than MASS, at least
for the first class in the series. I cannot think of a more gentle,
broad and competent way to introduce clinicians to both statistics and R
at the same time.

If these clinicians are likely to move on to busy clinical practices, in
my experience having come out of the clinical environment, they will not
have the time to sit at a computer and grind out analyses, much less
maintain their proficiency with a programming language (R, Stata or SAS)
or the broad range of statistical methodologies that they would likely
encounter over their careers.

They will however, need to be able to sit and interact with
statisticians, bringing the significant value of their clinical training
and knowledge, to the process of designing clinical research projects
and effectively comprehend the multitude of issues in that endeavor.
They will need to have an understanding of the complex processes by
which data are collected, managed, manipulated and analyzed in the
course of obtaining the resultant analyses. 

In other words, it is important that they realize that it is more than
just a "point and click" process where voila, you have logistic
regression model. They need to appreciate both the subtleties and
complexities of dealing with real world research, incomplete data, etc.
Many clinicians do not and this results in mis-matched expectations in
the future as they deal with real world situations.

There are certainly physicians who have made the decision to focus their
careers on the statistical part of the research process, forsaking any
significant clinical patient care role. They are far and few between, to
my experience, though two or three immediately come to mind. They have
also generally made the commitment to formal graduate level education in
math/statistics securing advanced degrees.

Short of that, there is typically a future dependence upon trained
statisticians, either within an academic medical environment or via
contracted services.

The above is based upon my own experience, which is largely in
sub-specialty clinical areas. Others may and perhaps will differ, based
upon their own bias.

HTH,

Marc Schwartz



From dimitris.rizopoulos at med.kuleuven.ac.be  Thu Aug 19 16:35:55 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Thu, 19 Aug 2004 16:35:55 +0200
Subject: [R] convert strings to object names
References: <4032.193.10.23.33.1092924903.squirrel@webmail.math.su.se>
Message-ID: <007d01c485f9$d6e37bc0$ad133a86@www.domain>

Hi Gudrun,

try to use

eval(parse(text=dat))



Best,
Dimitris

----
Dimitris Rizopoulos
Doctoral Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Gudrun Jonasdottir" <gudrunj at math.su.se>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, August 19, 2004 4:15 PM
Subject: [R] convert strings to object names


> Dear R-Help list,
>
> I have a problem with convertions of strings. I want to use the
function
> "paste()" to create an object name and then use that character
string to
> call on that object. So, for example:
>
> dat99 <- matrix(rbind(1,1,2),3,3)
> no <- 99
> dat <- paste("dat",no,sep="")
> dat
> [1] "dat99"
>
> What should I do to get the output
>
> dat
>      [,1] [,2] [,3]
> [1,]    1    1    1
> [2,]    1    1    1
> [3,]    2    2    2
>
> Cheers,
> Gudrun
>
>
>
> -- 
> Gudrun Jonasdottir, M.Sc.
> Matematiska institutionen
> Stockholms Universitet
> SE- 106 91 Stockholm
>
> Work: +46 (0)8 16 45 56
> Mobile: +46 (0)709 779 800
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Thu Aug 19 16:40:08 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 19 Aug 2004 16:40:08 +0200
Subject: [R] convert strings to object names
In-Reply-To: <4032.193.10.23.33.1092924903.squirrel@webmail.math.su.se>
References: <4032.193.10.23.33.1092924903.squirrel@webmail.math.su.se>
Message-ID: <4124BBC8.7090906@statistik.uni-dortmund.de>

Gudrun Jonasdottir wrote:

> Dear R-Help list,
> 
> I have a problem with convertions of strings. I want to use the function
> "paste()" to create an object name and then use that character string to
> call on that object. So, for example:
> 
> dat99 <- matrix(rbind(1,1,2),3,3)
> no <- 99
> dat <- paste("dat",no,sep="")
> dat
> [1] "dat99"
> 
> What should I do to get the output

get(dat)

Uwe Ligges


> dat
>      [,1] [,2] [,3]
> [1,]    1    1    1
> [2,]    1    1    1
> [3,]    2    2    2
> 
> Cheers,
> Gudrun
> 
> 
>



From ramasamy at cancer.org.uk  Thu Aug 19 16:47:23 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Thu, 19 Aug 2004 15:47:23 +0100
Subject: [R] convert strings to object names
In-Reply-To: <4032.193.10.23.33.1092924903.squirrel@webmail.math.su.se>
References: <4032.193.10.23.33.1092924903.squirrel@webmail.math.su.se>
Message-ID: <1092926842.6010.3.camel@localhost.localdomain>

 assign("dat", get(paste("dat", no, sep="")))

or simply 

 dat <- get(paste("dat", no, sep=""))


On Thu, 2004-08-19 at 15:15, Gudrun Jonasdottir wrote:
> Dear R-Help list,
> 
> I have a problem with convertions of strings. I want to use the function
> "paste()" to create an object name and then use that character string to
> call on that object. So, for example:
> 
> dat99 <- matrix(rbind(1,1,2),3,3)
> no <- 99
> dat <- paste("dat",no,sep="")
> dat
> [1] "dat99"
> 
> What should I do to get the output
> 
> dat
>      [,1] [,2] [,3]
> [1,]    1    1    1
> [2,]    1    1    1
> [3,]    2    2    2
> 
> Cheers,
> Gudrun
> 
>



From tlumley at u.washington.edu  Thu Aug 19 17:15:17 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 19 Aug 2004 08:15:17 -0700 (PDT)
Subject: [R] convert strings to object names
In-Reply-To: <1092926842.6010.3.camel@localhost.localdomain>
References: <4032.193.10.23.33.1092924903.squirrel@webmail.math.su.se>
	<1092926842.6010.3.camel@localhost.localdomain>
Message-ID: <Pine.A41.4.58.0408190814430.234646@homer11.u.washington.edu>


And this is all in the FAQ.

	-thomas


On Thu, 19 Aug 2004, Adaikalavan Ramasamy wrote:

>  assign("dat", get(paste("dat", no, sep="")))
>
> or simply
>
>  dat <- get(paste("dat", no, sep=""))
>
>
> On Thu, 2004-08-19 at 15:15, Gudrun Jonasdottir wrote:
> > Dear R-Help list,
> >
> > I have a problem with convertions of strings. I want to use the function
> > "paste()" to create an object name and then use that character string to
> > call on that object. So, for example:
> >
> > dat99 <- matrix(rbind(1,1,2),3,3)
> > no <- 99
> > dat <- paste("dat",no,sep="")
> > dat
> > [1] "dat99"
> >
> > What should I do to get the output
> >
> > dat
> >      [,1] [,2] [,3]
> > [1,]    1    1    1
> > [2,]    1    1    1
> > [3,]    2    2    2
> >
> > Cheers,
> > Gudrun
> >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From tlumley at u.washington.edu  Thu Aug 19 17:19:49 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 19 Aug 2004 08:19:49 -0700 (PDT)
Subject: [R] A question about external time-dependent covariates in co
	x model
In-Reply-To: <20040819195634.GA12139@stat.umu.se>
References: <E37EEC6DE3A0C5439B7E7B07406C24AE124A08@msgmarsta01.bio.dfo.ca>
	<20040819195634.GA12139@stat.umu.se>
Message-ID: <Pine.A41.4.58.0408190818270.234646@homer11.u.washington.edu>

On Thu, 19 Aug 2004, G??ran Brostr??m wrote:

> On Thu, Aug 19, 2004 at 09:36:22AM -0300, Hanke, Alex wrote:
> > Dear Rui,
> > >From my understanding of time-dependent covariates (not an expert but have
> > been working on a similar problem), it would appear that the coding of the
> > status column is not correct. Unless you have observed an event at each
> > interval you should only have status=1 for the last interval. In your
> > example I see 3 in total. Also, I think that if "end" is proportional to
> > your "covariate" you are incorporating a redundant time effect into the
> > model. The time effect is in the baseline hazard.
>
> Right, the 'splitting' was made incorrectly, but 'coxph' shouldn't
> segfault anyway. The error seems to be (caught) in 'coxph_wtest.c',
> line 29, which may be of interest to the R maintainer of 'survival',
> Thomas L.
>

It's already fixed.  coxph_wtest segfaulted on  covariance matrices of rank 0.

	-thomas



From tlumley at u.washington.edu  Thu Aug 19 17:40:12 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 19 Aug 2004 08:40:12 -0700 (PDT)
Subject: [R] The 'test.terms' argument in 'regTermTest' in package 'survey'
In-Reply-To: <5A637F509C50B444BDA096EB2D7BC15842C48C@pghmail2.rand.org>
References: <5A637F509C50B444BDA096EB2D7BC15842C48C@pghmail2.rand.org>
Message-ID: <Pine.A41.4.58.0408190830400.234646@homer11.u.washington.edu>

On Thu, 19 Aug 2004, Almirall, Daniel wrote:

>
> This is a question regarding the 'regTermTest' function in the 'survey' package.  Imagine Z as a three level factor variable, and code ZB and ZC as the two corresponding dummy variables.  X is a continuous variable.  In a 'glm' of Y on Z and X, say, how do the two test specifications
>
> 	test.terms = c("ZB:X","ZC:X")  # and
> 	test.terms = ~ ZB:X + ZC:X
>
> in 'regTermTest' differ?  I thought that both would return the same joint (Wald) test for the two Z:X interactions.  Why does the second one specify a 1 degree of freedom test?  The code below should help clarify my question.

What's happening is that the terms() function is reordering the variables.
The version with ~ZB:X +ZC:X uses attr(terms(~ZB:X+ZC:X),"term.labels"),
which turns out to be c("ZB:X","X:ZC").  This will cause problems when you
have more than one interaction term listed. Ugh.

A workaround in simple cases like this is to take advantage of R's
ability to make indicator variables.  You don't really have two
interactions, just two terms describing one interaction.

   glm3<-glm(Y~factor(Z)*X,family=binomial)
   regTermTest(glm3, ~factor(Z):X)

	-thomas

>
> Thanks much,
> Danny
>
>
>
>
> ## I'm currently using: R Version 1.9.1 / Windows 2000 / P4/2.8 Ghz
>
> Z <- as.factor(rep(LETTERS[1:3],20))
> Y <- rep(0:1, 30)
> X <- rnorm(60)
>
> glm1 <- glm(Y ~ Z + X + Z:X, family=binomial)
> summary(glm1)$coeff
>
> regTermTest( model=glm1 , test.terms=~Z:X)
>
> ZB <- ifelse(Z=="B",1,0)
> ZC <- ifelse(Z=="C",1,0)
>
> glm2 <- glm(Y ~ ZB + ZC + X + ZB:X + ZC:X, family=binomial)
> summary(glm2)$coeff 		## Okay, same as glm1
>
> regTermTest( model=glm2 , test.terms= c("ZB:X","ZC:X"))
> regTermTest( model=glm2 , test.terms= ~ ZB:X + ZC:X)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From rsong at stat.wisc.edu  Thu Aug 19 17:47:19 2004
From: rsong at stat.wisc.edu (Rui Song)
Date: Thu, 19 Aug 2004 10:47:19 -0500 (CDT)
Subject: [R] A question about external time-dependent covariates in co
	x model
In-Reply-To: <E37EEC6DE3A0C5439B7E7B07406C24AE124A08@msgmarsta01.bio.dfo.ca>
References: <E37EEC6DE3A0C5439B7E7B07406C24AE124A08@msgmarsta01.bio.dfo.ca>
Message-ID: <Pine.LNX.4.58.0408191012160.8991@chi18.stat.wisc.edu>

Dear Alex,

Thanks a lot for your help! The "end" is not proportional to the
covariate in my data set. I recoded the status as you suggested, but the
problem still exists. I tried to fit the model with partial data, it has
no problem when I use the first 25 rows (from the data in the form
"start", "end", "status", "cov"), with very huge standard error. And the
problem exists if I cooperated more data. It is kind of weird...

Rui

On Thu, 19 Aug 2004, Hanke, Alex wrote:

> Dear Rui,
> >From my understanding of time-dependent covariates (not an expert but have
> been working on a similar problem), it would appear that the coding of the
> status column is not correct. Unless you have observed an event at each
> interval you should only have status=1 for the last interval. In your
> example I see 3 in total. Also, I think that if "end" is proportional to
> your "covariate" you are incorporating a redundant time effect into the
> model. The time effect is in the baseline hazard.
>
> Alex
> -----Original Message-----
> From: Rui Song [mailto:rsong at stat.wisc.edu]
> Sent: August 19, 2004 12:21 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] A question about external time-dependent covariates in cox
> model
>
>
> Dear Sir or Madam:
> I am a graduate student in UW-Madison statistics department. I have a
> question about fitting a cox model with external time-dependent
> covariates.
>
> Say the original data is in the following format:
> Obs Eventtime  Status  Cov(time=5)  Cov(time=8)  Cov(time=10)	Cov(time=12)
> 1	5	1		2
> 2	8	0(censored)	2	4
> 3	10	1		2	4		6
> 4	12	1		2	4		6		8
> ....
>
> Notice that the time-dependent covariates are identical at the same
> time points for all obs since they are external to the failure process.
> process.
>
> Then I organized the data as the following:
> obs	start	end	eventtime	status	cov
> 1	0	5	5		1	2
> 2	0	5	8		0	2
> 2	5	8	8		0	4
> 3	0	5	10		1	2
> 3	5	8	10		1	4
> 3	8	10	10		1	6
> 4	0	5	12		1	2
> 4	5	8	12		1	4
> 4	8	10	12		1	6
> 4	10	12	12		1	8
>
> And fit the model using:
>
> fit<-coxph(Surv(start, end, status)~cov);
>
> When I fit the model to my data set (Which has 89 observations and 81
> distinct time points, sort of large.), I always got a message that
> "Process R segmentation fault (core dumped)". Would you let me know if it
> is due to the matrix sigularity in the computation of the partial
> likelihood or something else? And how should I fit a cox model with
> external time-dependent covariates?
>
> Thanks a lot for your time and help!
>
> Sincerely,
> Rui Song
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From mapdpl at bath.ac.uk  Thu Aug 19 17:53:44 2004
From: mapdpl at bath.ac.uk (Duncan Lee)
Date: Thu, 19 Aug 2004 16:53:44 +0100
Subject: [R] GEEs for time series data
Message-ID: <001001c48604$b4f7b3e0$0961268a@mapc0028>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040819/52d9fb09/attachment.pl

From blh at mssl.ucl.ac.uk  Thu Aug 19 18:05:14 2004
From: blh at mssl.ucl.ac.uk (Benjamin Lloyd-Hughes)
Date: Thu, 19 Aug 2004 17:05:14 +0100
Subject: [R] rgdal under windows?
Message-ID: <000201c48606$504bb0c0$434e2880@geol.ucl.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040819/88c2e33b/attachment.pl

From FWS4 at CDRH.FDA.GOV  Thu Aug 19 18:11:00 2004
From: FWS4 at CDRH.FDA.GOV (Samuelson, Frank*)
Date: Thu, 19 Aug 2004 12:11:00 -0400
Subject: [R] precision problems in testing with Intel compilers
Message-ID: <644D9337A02FC24689647BF9E48EC39E08ABB86B@drm556>

I compiled the 1.9.1 src.rpm with the standard gnu tools and it works.
I tried compiling the 1.9.1 src.rpm with the Intel 8 C and FORTRAN
compilers and it bombs out during the testing phase:

    comparing 'd-p-q-r-tests.Rout' to './d-p-q-r-tests.Rout.save' ...267c267
    < df = 0.5[1] "Mean relative  difference: 5.001647e-10"
    ---
    > df = 0.5[1] TRUE
    make[3]: *** [d-p-q-r-tests.Rout] Error 1
    make[3]: Leaving directory `/usr/src/redhat/BUILD/R-1.9.1/tests'
    make[2]: *** [test-Specific] Error 2
    make[2]: Leaving directory `/usr/src/redhat/BUILD/R-1.9.1/tests'
    make[1]: *** [test-all-basics] Error 1
    make[1]: Leaving directory `/usr/src/redhat/BUILD/R-1.9.1/tests'
    make: *** [check-all] Error 2
    error: Bad exit status from /var/tmp/rpm-tmp.63044 (%build)

looking at the differences between the failed file and the standard, I get:

    fws wolf tests] diff  d-p-q-r-tests.Rout.save d-p-q-r-tests.Rout.fail 
    3c3
    < Version 1.9.0 Patched (2004-04-19), ISBN 3-900051-00-3
    ---
    > Version 1.9.1  (2004-06-21), ISBN 3-900051-00-3
    281c281
    < df = 0.5[1] TRUE
    ---
    > df = 0.5[1] "Mean relative  difference: 5.001647e-10"
    935c935
    < Time elapsed:  7.83 0.04 16.1 0 0 
    ---
    > Time elapsed:  2.49 0.01 2.55 0 0 

Besides being 3 times faster, it's stopping on the following code:

   for(df in c(0.1, 0.5, 1.5, 4.7, 10, 20,50,100)) {
     cat("df =", formatC(df, wid=3))
     xx <- c(10^-(5:1), .9, 1.2, df + c(3,7,20,30,35,38))
     pp <- pchisq(xx, df=df, ncp = 1) #print(pp)
     dtol <- 1e-12 *(if(2 < df && df <= 50) 64 else if(df > 50) 20000 else
500)
     print(all.equal(xx, qchisq(pp, df=df, ncp=1), tol = dtol))# TRUE
     ##or print(mapply(rErr, xx, qchisq(pp, df=df,ncp=1)), digits = 3)
   }

Where dtol used by all.equal is set to be 5e-10,
which the intel compiler misses by 1.6e-13.
This tolerance value seems a bit arbitrary.
The gcc compiled version's passes the test with a 9.3e-11 error.


I am using the -mp option for the intel compilers, which was recommended
on this mailing list previously and would make sense given the docs:

   Floating Point Optimization Options
       -mp    Maintain floating-point  precision  (disables  some
              optimizations).  The -mp option restricts optimiza-
              tion to maintain declared precision and  to  ensure
              that   floating-point   arithmetic   conforms  more
              closely to the ANSI and IEEE  standards.  For  most
              programs,  specifying this option adversely affects
              performance. If  you  are  not  sure  whether  your
              application  needs  this  option, try compiling and
              running your program both with and  without  it  to
              evaluate the effects on both performance and preci-
              sion.


Has anyone else encountered this?  

-Frank



From biocperi at yahoo.com  Thu Aug 19 18:20:41 2004
From: biocperi at yahoo.com (S Peri)
Date: Thu, 19 Aug 2004 09:20:41 -0700 (PDT)
Subject: [R] How to convert a vector into a list
In-Reply-To: <20040819195634.GA12139@stat.umu.se>
Message-ID: <20040819162041.79746.qmail@web50002.mail.yahoo.com>

Dear group, 
Apologies for asking the most chomped FAQ.

I have a file with list of gene names(genes.txt):
EGF
EGFR
PTPN6
TIEG2
MAPK1


I have another object in R, I do not know the data
type for that object that looks like this
("lidnames"):
"RABGGTA"   "MAPK3"     "TIE" "CYP2C19" 


> lidnames[1:10]
 100_g_at   1000_at   1001_at 1002_f_at 1003_s_at    
"RABGGTA"   "MAPK3"     "TIE" "CYP2C19"    "BLR1" 


I want to pick list of genes from lidnames object that
are in genes.txt.  I am using %in% function.

>mygenes<-read.table("genes.txt")

> mygenes[mygenes %in% lidnames]
NULL data frame with 164 rows
> 

I am unable to pullout genes from lidnames object.

Is it because that lidnames is as a list type and
mygenes object is as a vector/matrix type.

How can I convert mygenes to list type where I can
have the elements:

"EGF","EGFR","PTPN6","TIEG2","MAPK1".

I think in this way I can pull out the names from
lidnames object or any other complex matrix. 

Please help me. 

Thank you
SP



From kkthird at yahoo.com  Thu Aug 19 18:25:53 2004
From: kkthird at yahoo.com (KKThird@Yahoo.Com)
Date: Thu, 19 Aug 2004 09:25:53 -0700 (PDT)
Subject: [R] NLME: Holding constant the across group correlational structure
	of the fixed effects in nlme
Message-ID: <20040819162553.56502.qmail@web52504.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040819/bdfd732b/attachment.pl

From johannesson1 at llnl.gov  Thu Aug 19 18:39:41 2004
From: johannesson1 at llnl.gov (Gardar Johannesson)
Date: Thu, 19 Aug 2004 09:39:41 -0700
Subject: [R] convert strings to object names
In-Reply-To: <4032.193.10.23.33.1092924903.squirrel@webmail.math.su.se>
Message-ID: <5.2.0.9.2.20040819093739.00ac6118@poptop.llnl.gov>

Use the get() function:

 > dat99 <- matrix(c(1,1,2),3,3)
 > no <- 99
 > dat.name <- paste("dat",no,sep="")
 > get(dat.name)
      [,1] [,2] [,3]
[1,]    1    1    1
[2,]    1    1    1
[3,]    2    2    2
 >


At 04:15 PM 8/19/2004 +0200, Gudrun Jonasdottir wrote:
>Dear R-Help list,
>
>I have a problem with convertions of strings. I want to use the function
>"paste()" to create an object name and then use that character string to
>call on that object. So, for example:
>
>dat99 <- matrix(rbind(1,1,2),3,3)
>no <- 99
>dat <- paste("dat",no,sep="")
>dat
>[1] "dat99"
>
>What should I do to get the output
>
>dat
>      [,1] [,2] [,3]
>[1,]    1    1    1
>[2,]    1    1    1
>[3,]    2    2    2
>
>Cheers,
>Gudrun
>
>
>
>--
>Gudrun Jonasdottir, M.Sc.
>Matematiska institutionen
>Stockholms Universitet
>SE- 106 91 Stockholm
>
>Work: +46 (0)8 16 45 56
>Mobile: +46 (0)709 779 800
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Thu Aug 19 18:39:30 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 19 Aug 2004 12:39:30 -0400
Subject: [R] How to convert a vector into a list
Message-ID: <3A822319EB35174CA3714066D590DCD504AF8250@usrymx25.merck.com>

1.  When you don't know (or are not sure) what an object is, str() is your
friend.

2.  My guess is that `lidnames' is a character vector with names.

3.  If genes.txt has only only column, you might as well use:

     mygenes <- scan("genes.txt", what="")

    which reads the data into mygenes as a character vector.  Then your
command should work.

[read.table() returns a data frame.  You index mygenes, a data frame, with a
logical vector, which means selecting columns that are TRUE.  I suspect
mygenes, as you have it, is a data frame of only one column, so the
subsetting won't work.]

HTH,
Andy

> From: S Peri
> 
> Dear group, 
> Apologies for asking the most chomped FAQ.
> 
> I have a file with list of gene names(genes.txt):
> EGF
> EGFR
> PTPN6
> TIEG2
> MAPK1
> 
> 
> I have another object in R, I do not know the data
> type for that object that looks like this
> ("lidnames"):
> "RABGGTA"   "MAPK3"     "TIE" "CYP2C19" 
> 
> 
> > lidnames[1:10]
>  100_g_at   1000_at   1001_at 1002_f_at 1003_s_at    
> "RABGGTA"   "MAPK3"     "TIE" "CYP2C19"    "BLR1" 
> 
> 
> I want to pick list of genes from lidnames object that
> are in genes.txt.  I am using %in% function.
> 
> >mygenes<-read.table("genes.txt")
> 
> > mygenes[mygenes %in% lidnames]
> NULL data frame with 164 rows
> > 
> 
> I am unable to pullout genes from lidnames object.
> 
> Is it because that lidnames is as a list type and
> mygenes object is as a vector/matrix type.
> 
> How can I convert mygenes to list type where I can
> have the elements:
> 
> "EGF","EGFR","PTPN6","TIEG2","MAPK1".
> 
> I think in this way I can pull out the names from
> lidnames object or any other complex matrix. 
> 
> Please help me. 
> 
> Thank you
> SP
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From f.harrell at vanderbilt.edu  Thu Aug 19 18:42:04 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 19 Aug 2004 11:42:04 -0500
Subject: [R] Is R good for not-professional-statistician, , un-mathematical
	clinical researchers?
Message-ID: <4124D85C.1040205@vanderbilt.edu>

We are finding more and more clinical researchers interested in learning 
R and are starting to teach R to clinicians.  We have put some teaching 
material on our site: http://biostat.mc.vanderbilt.edu
-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From ligges at statistik.uni-dortmund.de  Thu Aug 19 18:43:01 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 19 Aug 2004 18:43:01 +0200
Subject: [R] How to convert a vector into a list
In-Reply-To: <20040819162041.79746.qmail@web50002.mail.yahoo.com>
References: <20040819162041.79746.qmail@web50002.mail.yahoo.com>
Message-ID: <4124D895.4000306@statistik.uni-dortmund.de>

S Peri wrote:

> Dear group, 
> Apologies for asking the most chomped FAQ.
> 
> I have a file with list of gene names(genes.txt):
> EGF
> EGFR
> PTPN6
> TIEG2
> MAPK1
> 
> 
> I have another object in R, I do not know the data
> type for that object that looks like this
> ("lidnames"):
> "RABGGTA"   "MAPK3"     "TIE" "CYP2C19" 
> 
> 
> 
>>lidnames[1:10]
> 
>  100_g_at   1000_at   1001_at 1002_f_at 1003_s_at    
> "RABGGTA"   "MAPK3"     "TIE" "CYP2C19"    "BLR1" 
> 
> 
> I want to pick list of genes from lidnames object that
> are in genes.txt.  I am using %in% function.
> 
> 
>>mygenes<-read.table("genes.txt")
> 
> 
>>mygenes[mygenes %in% lidnames]
> 
> NULL data frame with 164 rows
> 
> 
> I am unable to pullout genes from lidnames object.
> 
> Is it because that lidnames is as a list type and
> mygenes object is as a vector/matrix type.

Well, mygenes is a data frame, and lidnames a vector.
E.g., try to read in mygenes as a vector directly:
  mygenes <- scan("genes.txt", what = "character")

Uwe Ligges

> How can I convert mygenes to list type where I can
> have the elements:
> 
> "EGF","EGFR","PTPN6","TIEG2","MAPK1".
> 
> I think in this way I can pull out the names from
> lidnames object or any other complex matrix. 
> 
> Please help me. 
> 
> Thank you
> SP
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From rolf at math.unb.ca  Thu Aug 19 18:42:21 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Thu, 19 Aug 2004 13:42:21 -0300 (ADT)
Subject: [R] paired t-test vs pairwise t-test
Message-ID: <200408191642.i7JGgLY1014243@erdos.math.unb.ca>


You wrote:

> What's the difference between t.test(x, y) and pairwise.t.test()? Is
> it just that the former takes two vectors, whereas the latter takes a
> vector and a factor?

	No.  The pairwise.t.test() function (according to the help
	file) does a multiplicity of t-tests, on more than two
	samples, adjusting the p-value to compensate for the
	multiplicity by various methods.

	IMHO the name of this function is bad, because to me it
	suggests doing ***paired*** t-tests, which would trip up the
	naive user, who probably wouldn't notice or would ignore the
	"t tests with pooled SD" message in the output.  As one of
	the Ripley fortunes says ``It really is hard to anticipate
	just how silly users can be.''  But why go out of the way to
	give them a chance to be silly?


				cheers,

					Rolf Turner
					rolf at math.unb.ca



From ripley at stats.ox.ac.uk  Thu Aug 19 19:10:31 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 19 Aug 2004 18:10:31 +0100 (BST)
Subject: [R] rgdal under windows?
In-Reply-To: <000201c48606$504bb0c0$434e2880@geol.ucl.ac.uk>
Message-ID: <Pine.LNX.4.44.0408191713511.24875-100000@gannet.stats>

On Thu, 19 Aug 2004, Benjamin Lloyd-Hughes wrote:

> Has anyone had any joy getting the rgdal package to compile under windows?

First you need to meet the system requirements:

SystemRequirements: GDAL library from 
http://www.remotesensing.org/gdal/download.html

Since that has build instructions which say

  GDAL can be built on Windows using MS VC++ 6.x and MS Visual Studio .NET 
  (C++) at the DOS command line.

you will first need to get a copy of DOS and those compilers.  (I suspect
they mean the Windows command prompt, but they don't seem to know the
difference which is not confidence-inspiring.  I also suspect they mean
`or' not `and'.)  Then you will have to fathom out how to link against a
VC++ DLL which since this C++ is very unlikely to work with MinGW.  So you
probably need to make the package's rgdal.dll under VC++ -- see
README.packages.

I have tried and failed to make GDAL with MinGW.

> I've been trying with MinGW using Duncan Murdoch's Rtools but to no avail.

Do read README.packages and follow the advice at the top.  You don't have 
the (mis-attributed) tools in your path.  But that is probably the least 
of your potential problems.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mkondrin at hppi.troitsk.ru  Fri Aug 20 22:37:58 2004
From: mkondrin at hppi.troitsk.ru (M.Kondrin)
Date: Fri, 20 Aug 2004 13:37:58 -0700
Subject: [R] pdf font encoding
In-Reply-To: <4124B018.23080.156782E@localhost>
References: <4124B018.23080.156782E@localhost>
Message-ID: <41266126.6060604@hppi.troitsk.ru>

Petr Pikal wrote:

>Dear all.
>
>This is probably more PDF then R issue but anyway, maybe 
>somebody can help me.
>
>I try to persuade pdf device to use eastern european (Czech) 
>encoding but I was not successful yet. Working on W 2000, R 
>1.9.0, lattice plots.
>
>pdf("redprov.pdf", width = 5, height = 5, bg="white", 
>family="Times", encoding="WinAnsi")
>
>The characters are OK when using other devices (screen, png, ...) 
>but in case of pdf, some characters are not correctly shown nor 
>printed.
>
>After reading help pages and some archives I understand I shall 
>change encoding and/or font but after I used several combinations 
>without making any progress I gave up. Shall I install some other 
>encoding and/or fonts?
>
>Any help is greatly appreciated.
>
>Best regards
>
>
>
>Petr Pikal
>petr.pikal at precheza.cz
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>
If you are working under Linux I would suggest using 
bitmap(type="pdfwrite") instead of pdf(). Bitmap function is a shortcut 
for postscript() and ps2pdf command from ghostscript (so as far as you 
manage to install national fonts into ghostscript it will be OK with non 
ascii symbols). As I have heard Windows has some problems with 
postscript, so do not know how it will work under Windows.



From biocperi at yahoo.com  Thu Aug 19 20:13:49 2004
From: biocperi at yahoo.com (S Peri)
Date: Thu, 19 Aug 2004 11:13:49 -0700 (PDT)
Subject: [R]  IDE or an Editor for R
In-Reply-To: <41266126.6060604@hppi.troitsk.ru>
Message-ID: <20040819181349.67668.qmail@web50005.mail.yahoo.com>

Hi, 
 Is there any IDE or any editor or any pice of code
for .vimrc is available?

Please let me know.

Thanks
SP


		
_______________________________


From Roger.Bivand at nhh.no  Thu Aug 19 20:27:03 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 19 Aug 2004 20:27:03 +0200 (CEST)
Subject: [R] rgdal under windows?
In-Reply-To: <Pine.LNX.4.44.0408191713511.24875-100000@gannet.stats>
Message-ID: <Pine.LNX.4.44.0408191957500.3940-100000@reclus.nhh.no>

On Thu, 19 Aug 2004, Prof Brian Ripley wrote:

> On Thu, 19 Aug 2004, Benjamin Lloyd-Hughes wrote:
> 
> > Has anyone had any joy getting the rgdal package to compile under windows?
> 
> First you need to meet the system requirements:
> 
> SystemRequirements: GDAL library from 
> http://www.remotesensing.org/gdal/download.html
> 
> Since that has build instructions which say
> 
>   GDAL can be built on Windows using MS VC++ 6.x and MS Visual Studio .NET 
>   (C++) at the DOS command line.
> 
> you will first need to get a copy of DOS and those compilers.  (I suspect
> they mean the Windows command prompt, but they don't seem to know the
> difference which is not confidence-inspiring.  I also suspect they mean
> `or' not `and'.)  Then you will have to fathom out how to link against a
> VC++ DLL which since this C++ is very unlikely to work with MinGW.  So you
> probably need to make the package's rgdal.dll under VC++ -- see
> README.packages.
> 
> I have tried and failed to make GDAL with MinGW.
> 
> > I've been trying with MinGW using Duncan Murdoch's Rtools but to no avail.
> 
> Do read README.packages and follow the advice at the top.  You don't have 
> the (mis-attributed) tools in your path.  But that is probably the least 
> of your potential problems.
> 

Exactly. The closest anyone has got so far is Hisaji Ono, who used MSYS 
(http://www.mingw.org/) to build PROJ.4 and GDAL (GDAL depends on PROJ.4, 
PROJ.4 needs a PATH to metadata files for projection and transformation), 
and then hand-pasted the paths to the GDAL headers and library into 
src/Makevars, running Rcmd INSTALL rgdal at the Windows command prompt as 
usual. All of this can be repeated, but is not portable, and does not suit 
the very valuable standard binary package build system for Windows. 
Roughly:

1. Download everything you need to build source packages under Windows and 
make sure it works;

2. Download MSYS and make sure it works;

3. Download the GDAL and PROJ.4 source tarballs, and possibly other
libraries you want to use with GDAL, and *within MSYS* untar, ./configure
with the appropriate arguments, at least make but maybe also make install
- now leave MSYS;

4. Download the rgdal source package, and *at the Windows command prompt*
untar it, change the name of configure to something else, create
src/Makevars manually from src/Makevars.in, insert the correct values of:
PKG_CPPFLAGS as -I<path to the directory with the GDAL headers>, PKG_LIBS
as -L< ... GDAL libraries> -lgdal, run Rcmd INSTALL rgdal (better first 
Rcmd check rgdal), and repeat until all the problems are resolved.

5. The installation should work locally, but some paths are compiled into 
the resulting *.dll, so it will, most likely, not be portable.

Both Hisaji and I were surprised that this actually seemed to work (as of
late Autumn last year for the versions of MinGW and MSYS available then;  
further fragility is introduced by much of GDAL being written in C++); it
is not wholly impossible that it could be made available portably as a
Windows binary with an installer, but not through the regular R binary
package repositories.

> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From p.dalgaard at biostat.ku.dk  Thu Aug 19 20:39:18 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 19 Aug 2004 20:39:18 +0200
Subject: [R]  IDE or an Editor for R
In-Reply-To: <20040819181349.67668.qmail@web50005.mail.yahoo.com>
References: <20040819181349.67668.qmail@web50005.mail.yahoo.com>
Message-ID: <x23c2jx99l.fsf@biostat.ku.dk>

S Peri <biocperi at yahoo.com> writes:

> Hi, 
>  Is there any IDE or any editor or any pice of code
> for .vimrc is available?

What do you want to put into .vimrc? Vim does syntax highlighting for
R out of the box...

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ihok at hotmail.com  Thu Aug 19 20:48:38 2004
From: ihok at hotmail.com (Jack Tanner)
Date: Thu, 19 Aug 2004 14:48:38 -0400
Subject: [R] paired t-test vs pairwise t-test
Message-ID: <BAY22-F24SK94rhwtCq0002f8c5@hotmail.com>

From: Rolf Turner <rolf at math.unb.ca>
>No.  The pairwise.t.test() function (according to the help
>file) does a multiplicity of t-tests, on more than two
>samples, adjusting the p-value to compensate for the
>multiplicity by various methods.

Thank you, Rolf, that's helpful. So pairwise.t.test() is used for comparison 
of more than two means. I did, indeed, confuse it with t.test(paired=TRUE). 
But the documentation, which I valiantly tried to make sense of BEFORE 
asking my stupid question, is not clear enough for this particular idiot. 
Might I suggest that the documentation be altered? It could use an example 
(as in, real-life applied statistical problem) of when pairwise.t.test() 
ought to be used, and why t.test(paired=TRUE) would be inappropriate in that 
context; it could also use a reference to some published paper, website or 
some such that explains the rationale and correct procedure for using this 
test.

Thanks again for your clairvoyance.



From murdoch at stats.uwo.ca  Thu Aug 19 21:22:12 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 19 Aug 2004 15:22:12 -0400
Subject: [R] paired t-test vs pairwise t-test
In-Reply-To: <200408191642.i7JGgLY1014243@erdos.math.unb.ca>
References: <200408191642.i7JGgLY1014243@erdos.math.unb.ca>
Message-ID: <s7v9i0dhhd9seljeik85k3pong8t03qgvh@4ax.com>

On Thu, 19 Aug 2004 13:42:21 -0300 (ADT), Rolf Turner
<rolf at math.unb.ca> wrote :

>
>You wrote:
>
>> What's the difference between t.test(x, y) and pairwise.t.test()? Is
>> it just that the former takes two vectors, whereas the latter takes a
>> vector and a factor?
>
>	No.  The pairwise.t.test() function (according to the help
>	file) does a multiplicity of t-tests, on more than two
>	samples, adjusting the p-value to compensate for the
>	multiplicity by various methods.
>
>	IMHO the name of this function is bad, because to me it
>	suggests doing ***paired*** t-tests, which would trip up the
>	naive user, who probably wouldn't notice or would ignore the
>	"t tests with pooled SD" message in the output.  As one of
>	the Ripley fortunes says ``It really is hard to anticipate
>	just how silly users can be.''  But why go out of the way to
>	give them a chance to be silly?

And Jack wrote:

>But the documentation, which I valiantly tried to make sense of BEFORE 
>asking my stupid question, is not clear enough for this particular idiot. 
>Might I suggest that the documentation be altered? It could use an example 
>(as in, real-life applied statistical problem) of when pairwise.t.test() 
>ought to be used, and why t.test(paired=TRUE) would be inappropriate in that 
>context; it could also use a reference to some published paper, website or 
>some such that explains the rationale and correct procedure for using this 
>test.

I think it's unlikely that we would rename the function; it's been
around a while with its current name so that's a bad idea.  On the
other hand, clearer documentation is always a plus:  why not submit
some?

Duncan Murdoch



From HankeA at mar.dfo-mpo.gc.ca  Thu Aug 19 21:21:53 2004
From: HankeA at mar.dfo-mpo.gc.ca (Hanke, Alex)
Date: Thu, 19 Aug 2004 16:21:53 -0300
Subject: [R] Clustering and the test for proportional hazards
Message-ID: <E37EEC6DE3A0C5439B7E7B07406C24AE124A0D@msgmarsta01.bio.dfo.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040819/75fd4aa1/attachment.pl

From andy_liaw at merck.com  Thu Aug 19 21:42:49 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 19 Aug 2004 15:42:49 -0400
Subject: [R] paired t-test vs pairwise t-test
Message-ID: <3A822319EB35174CA3714066D590DCD504AF8258@usrymx25.merck.com>

> From: Duncan Murdoch
> 
> On Thu, 19 Aug 2004 13:42:21 -0300 (ADT), Rolf Turner
> <rolf at math.unb.ca> wrote :
> 
> >
> >You wrote:
> >
> >> What's the difference between t.test(x, y) and 
> pairwise.t.test()? Is
> >> it just that the former takes two vectors, whereas the 
> latter takes a
> >> vector and a factor?
> >
> >	No.  The pairwise.t.test() function (according to the help
> >	file) does a multiplicity of t-tests, on more than two
> >	samples, adjusting the p-value to compensate for the
> >	multiplicity by various methods.
> >
> >	IMHO the name of this function is bad, because to me it
> >	suggests doing ***paired*** t-tests, which would trip up the
> >	naive user, who probably wouldn't notice or would ignore the
> >	"t tests with pooled SD" message in the output.  As one of
> >	the Ripley fortunes says ``It really is hard to anticipate
> >	just how silly users can be.''  But why go out of the way to
> >	give them a chance to be silly?
> 
> And Jack wrote:
> 
> >But the documentation, which I valiantly tried to make sense 
> of BEFORE 
> >asking my stupid question, is not clear enough for this 
> particular idiot. 
> >Might I suggest that the documentation be altered? It could 
> use an example 
> >(as in, real-life applied statistical problem) of when 
> pairwise.t.test() 
> >ought to be used, and why t.test(paired=TRUE) would be 
> inappropriate in that 
> >context; it could also use a reference to some published 
> paper, website or 
> >some such that explains the rationale and correct procedure 
> for using this 
> >test.
> 
> I think it's unlikely that we would rename the function; it's been
> around a while with its current name so that's a bad idea.  On the
> other hand, clearer documentation is always a plus:  why not submit
> some?

I guess this is sort of related to the thread on whether R is good for
non-statisticians...  The help pages in R are sort of like *nix man pages.
They give the technical information about the topic, but not necessarily the
background.  E.g., the man page for `chmod' does not explain file
permissions in detail: the user is expected to learn that elsewhere.

Perhaps other stat packages do it differently?  Does SPSS manuals detail
what its t-test procedure does, including which t-test(s) it does and when
it's appropriate?  That might make it easier on users, but I still think the
users should learn the appropriate use of statistical procedures
elsewhere...

Best,
Andy



> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From hjaffee at jhmi.edu  Thu Aug 19 22:15:59 2004
From: hjaffee at jhmi.edu (Harris A. Jaffee)
Date: Thu, 19 Aug 2004 16:15:59 -0400 (EDT)
Subject: [R] save() & load()
Message-ID: <Pine.GSO.4.44.0408191611160.556-100000@isis.jhmi.edu>

Is there a way to list the contents of a "save" file,
i.e. the variable names, without load'ing from it first?



From p.dalgaard at biostat.ku.dk  Thu Aug 19 22:14:38 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 19 Aug 2004 22:14:38 +0200
Subject: [R] paired t-test vs pairwise t-test
In-Reply-To: <BAY22-F24SK94rhwtCq0002f8c5@hotmail.com>
References: <BAY22-F24SK94rhwtCq0002f8c5@hotmail.com>
Message-ID: <x2oel6holt.fsf@biostat.ku.dk>

"Jack Tanner" <ihok at hotmail.com> writes:

> From: Rolf Turner <rolf at math.unb.ca>
> >No.  The pairwise.t.test() function (according to the help
> >file) does a multiplicity of t-tests, on more than two
> >samples, adjusting the p-value to compensate for the
> >multiplicity by various methods.
> 
> Thank you, Rolf, that's helpful. So pairwise.t.test() is used for
> comparison of more than two means. I did, indeed, confuse it with
> t.test(paired=TRUE). But the documentation, which I valiantly tried to
> make sense of BEFORE asking my stupid question, is not clear enough
> for this particular idiot. Might I suggest that the documentation be
> altered? It could use an example (as in, real-life applied statistical
> problem) of when pairwise.t.test() ought to be used, and why
> t.test(paired=TRUE) would be inappropriate in that context; it could
> also use a reference to some published paper, website or some such
> that explains the rationale and correct procedure for using this test.
> 
> Thanks again for your clairvoyance.

I didn't suggest looking at the example section of the help pages
without a reason...

It should be pretty clear example(pairwise.t.test) does give output
that pretty clearly indicates that all pairwise comparisons are being
performed.

We do have a bug though: in 1.9.1 print.pairwise.htest seems to have
gone AWOL, so that the pretty-printing is not as nice as it might have
been. (And it might also be called a buglet that there's no example
with a paired t test.)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From morzarialuna at wisc.edu  Thu Aug 19 22:20:00 2004
From: morzarialuna at wisc.edu (Hem Nalini Morzaria-Luna)
Date: Thu, 19 Aug 2004 15:20:00 -0500
Subject: [R] Question on TukeyHSD
Message-ID: <000001c48629$ea136fe0$e7626880@ad.botany.wisc.edu>

Hi,
	I am running a ANOVA on a factorial design, and using TukeyHSD
for post hoc comparisons. I have 2 factors with three levels each:
		Factor B
Factor A    1   2   3
	1
	2
	3


When I look at the Tukey output (on the interaction of the factors) the
comparisons come out numbered 1-36. 

e.g.
$"A:B"
             diff         lwr      upr
[1,]  49.1666667 -160.041022 258.3744
[2,]  50.0000000 -159.207689 259.2077
....

[35,]  84.3333333 -124.874355 293.5410
[36,]  57.6666667 -151.541022 266.8744

Does anyone know how the function numbers the comparisons or an easy way
of figuring out the order without having to go back and calculate the
means between each pair and then going back to compare them with the
output?

Thank you in advance

Hem Nalini Morzaria-Luna
Department of Botany
UW-Madison
341 Birge Hall
430 Lincoln Drive
Madison, WI 53706
(608) 265 9722



From FWS4 at CDRH.FDA.GOV  Thu Aug 19 22:22:11 2004
From: FWS4 at CDRH.FDA.GOV (Samuelson, Frank*)
Date: Thu, 19 Aug 2004 16:22:11 -0400
Subject: [R] More precision problems in testing with Intel compilers
Message-ID: <644D9337A02FC24689647BF9E48EC39E08ABB86D@drm556>

The Intel compiled version also fails the below test:

> ###------------ Very big and very small
> umach <- unlist(.Machine)[paste("double.x", c("min","max"), sep='')]
> xmin <- umach[1]
> xmax <- umach[2]
> tx <- unique(outer(-1:1,c(.1,1e-3,1e-7)))# 7 values  (out of 9)
> tx <- unique(sort(c(outer(umach,1+tx))))# 11 values  (out of 14)
> tx <- tx[is.finite(tx)] #-- all kept
> (txp <- tx[tx >= 1])#-- Positive exponent -- 4 values
[1] 1.617924e+308 1.795895e+308 1.797693e+308 1.797693e+308
> (txn <- tx[tx <        1])#-- Negative exponent -- 7 values
[1] 2.002566e-308 2.222849e-308 2.225074e-308 2.225074e-308 2.225074e-308
2.227299e-308 2.447581e-308


Does anyone really care about being correct to 1 unit of machine precision?
If you do, you have a bad algorithm.



-----Original Message-----
From: Samuelson, Frank* [mailto:FWS4 at cdrh.fda.gov] 
Sent: Thursday, August 19, 2004 12:11 PM
To: 'r-help at stat.math.ethz.ch '
Subject: [R] precision problems in testing with Intel compilers


I compiled the 1.9.1 src.rpm with the standard gnu tools and it works.
I tried compiling the 1.9.1 src.rpm with the Intel 8 C and FORTRAN
compilers and it bombs out during the testing phase:

    comparing 'd-p-q-r-tests.Rout' to './d-p-q-r-tests.Rout.save' ...267c267
    < df = 0.5[1] "Mean relative  difference: 5.001647e-10"
    ---
    > df = 0.5[1] TRUE
    make[3]: *** [d-p-q-r-tests.Rout] Error 1
    make[3]: Leaving directory `/usr/src/redhat/BUILD/R-1.9.1/tests'
    make[2]: *** [test-Specific] Error 2
    make[2]: Leaving directory `/usr/src/redhat/BUILD/R-1.9.1/tests'
    make[1]: *** [test-all-basics] Error 1
    make[1]: Leaving directory `/usr/src/redhat/BUILD/R-1.9.1/tests'
    make: *** [check-all] Error 2
    error: Bad exit status from /var/tmp/rpm-tmp.63044 (%build)
...



From savano at superig.com.br  Thu Aug 19 22:30:06 2004
From: savano at superig.com.br (Savano)
Date: Thu, 19 Aug 2004 17:30:06 -0300
Subject: [R] Package rmutil
Message-ID: <5.1.0.14.2.20040819172814.00b575b0@pop.superig.com.br>

Users,

Where I can to download rmutil package?

thanks



From ripley at stats.ox.ac.uk  Thu Aug 19 22:29:47 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 19 Aug 2004 21:29:47 +0100 (BST)
Subject: [R] save() & load()
In-Reply-To: <Pine.GSO.4.44.0408191611160.556-100000@isis.jhmi.edu>
Message-ID: <Pine.LNX.4.44.0408192126400.2187-100000@gannet.stats>

On Thu, 19 Aug 2004, Harris A. Jaffee wrote:

> Is there a way to list the contents of a "save" file,
> i.e. the variable names, without load'ing from it first?

I believe not.  But that's not too bad:

> x <- system.file(package="MASS", "data", "hills.rda")
> print(load(x , new.env()))
[1] "hills"

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ramasamy at cancer.org.uk  Thu Aug 19 22:34:01 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Thu, 19 Aug 2004 21:34:01 +0100
Subject: [R] How to convert a vector into a list
In-Reply-To: <20040819162041.79746.qmail@web50002.mail.yahoo.com>
References: <20040819162041.79746.qmail@web50002.mail.yahoo.com>
Message-ID: <1092947641.3120.104.camel@localhost.localdomain>

Peri, 

Please stop posting to both R-help and BioConductor simultaneously. 

Please read the posting guide and decide which list is more appropriate.
http://www.R-project.org/posting-guide.html





On Thu, 2004-08-19 at 17:20, S Peri wrote:
> Dear group, 
> Apologies for asking the most chomped FAQ.
> 
> I have a file with list of gene names(genes.txt):
> EGF
> EGFR
> PTPN6
> TIEG2
> MAPK1
> 
> 
> I have another object in R, I do not know the data
> type for that object that looks like this
> ("lidnames"):
> "RABGGTA"   "MAPK3"     "TIE" "CYP2C19" 
> 
> 
> > lidnames[1:10]
>  100_g_at   1000_at   1001_at 1002_f_at 1003_s_at    
> "RABGGTA"   "MAPK3"     "TIE" "CYP2C19"    "BLR1" 
> 
> 
> I want to pick list of genes from lidnames object that
> are in genes.txt.  I am using %in% function.
> 
> >mygenes<-read.table("genes.txt")
> 
> > mygenes[mygenes %in% lidnames]
> NULL data frame with 164 rows
> > 
> 
> I am unable to pullout genes from lidnames object.
> 
> Is it because that lidnames is as a list type and
> mygenes object is as a vector/matrix type.
> 
> How can I convert mygenes to list type where I can
> have the elements:
> 
> "EGF","EGFR","PTPN6","TIEG2","MAPK1".
> 
> I think in this way I can pull out the names from
> lidnames object or any other complex matrix. 
> 
> Please help me. 
> 
> Thank you
> SP
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Thu Aug 19 22:35:42 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 19 Aug 2004 21:35:42 +0100 (BST)
Subject: [R] Question on TukeyHSD
In-Reply-To: <000001c48629$ea136fe0$e7626880@ad.botany.wisc.edu>
Message-ID: <Pine.LNX.4.44.0408192130510.2187-100000@gannet.stats>

On Thu, 19 Aug 2004, Hem Nalini Morzaria-Luna wrote:

> Hi,
> 	I am running a ANOVA on a factorial design, and using TukeyHSD
> for post hoc comparisons. I have 2 factors with three levels each:
> 		Factor B
> Factor A    1   2   3
> 	1
> 	2
> 	3
> 
> 
> When I look at the Tukey output (on the interaction of the factors) the
> comparisons come out numbered 1-36. 

Rather, they come out as unlabelled and this is a print of a matrix 
without rownames.

> e.g.
> $"A:B"
>              diff         lwr      upr
> [1,]  49.1666667 -160.041022 258.3744
> [2,]  50.0000000 -159.207689 259.2077
> ....
> 
> [35,]  84.3333333 -124.874355 293.5410
> [36,]  57.6666667 -151.541022 266.8744
> 
> Does anyone know how the function numbers the comparisons 

It doesn't.   However, this is standard R order, that is down columns
(1:1, 2:1, 3:1 etc).

> or an easy way of figuring out the order without having to go back and
> calculate the means between each pair and then going back to compare
> them with the output?

It should be easy to add labels for the next release.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ramasamy at cancer.org.uk  Thu Aug 19 22:40:37 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Thu, 19 Aug 2004 21:40:37 +0100
Subject: [R]  IDE or an Editor for R
In-Reply-To: <20040819181349.67668.qmail@web50005.mail.yahoo.com>
References: <20040819181349.67668.qmail@web50005.mail.yahoo.com>
Message-ID: <1092948037.3120.110.camel@localhost.localdomain>

I am not sure what does an IDE consist besides syntax highlighting and
ability to parse lines from script into console. Nor am I familiar with
VIM but IMHO emacs with ESS is the best for R.


On Thu, 2004-08-19 at 19:13, S Peri wrote:
> Hi, 
>  Is there any IDE or any editor or any pice of code
> for .vimrc is available?
> 
> Please let me know.
> 
> Thanks
> SP
> 
> 
> 		
> _______________________________
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From jgentry at jimmy.harvard.edu  Thu Aug 19 22:42:01 2004
From: jgentry at jimmy.harvard.edu (Jeff Gentry)
Date: Thu, 19 Aug 2004 16:42:01 -0400 (EDT)
Subject: [R] Package rmutil
In-Reply-To: <5.1.0.14.2.20040819172814.00b575b0@pop.superig.com.br>
Message-ID: <Pine.SOL.4.20.0408191641210.26332-100000@santiam.dfci.harvard.edu>

> Where I can to download rmutil package?

You can use the reposTools package from Bioconductor:

install.packages2("rmutil")

There's also the website:
http://popgen0146uns50.unimaas.nl/~jlindsey/rcode.html



From ripley at stats.ox.ac.uk  Thu Aug 19 22:51:46 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 19 Aug 2004 21:51:46 +0100 (BST)
Subject: [R] Package rmutil
In-Reply-To: <5.1.0.14.2.20040819172814.00b575b0@pop.superig.com.br>
Message-ID: <Pine.LNX.4.44.0408192148500.2187-100000@gannet.stats>

On Thu, 19 Aug 2004, Savano wrote:

> Where I can to download rmutil package?

See the FAQ 5.1.5.

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
 
-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From MSchwartz at MedAnalytics.com  Thu Aug 19 22:53:18 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 19 Aug 2004 15:53:18 -0500
Subject: [R] paired t-test vs pairwise t-test
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF8258@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF8258@usrymx25.merck.com>
Message-ID: <1092948798.30114.87.camel@localhost.localdomain>

On Thu, 2004-08-19 at 14:42, Liaw, Andy wrote:
> > From: Duncan Murdoch
> > 
> > On Thu, 19 Aug 2004 13:42:21 -0300 (ADT), Rolf Turner
> > <rolf at math.unb.ca> wrote :
> > 
> > >
> > >You wrote:
> > >
> > >> What's the difference between t.test(x, y) and 
> > pairwise.t.test()? Is
> > >> it just that the former takes two vectors, whereas the 
> > latter takes a
> > >> vector and a factor?
> > >
> > >	No.  The pairwise.t.test() function (according to the help
> > >	file) does a multiplicity of t-tests, on more than two
> > >	samples, adjusting the p-value to compensate for the
> > >	multiplicity by various methods.
> > >
> > >	IMHO the name of this function is bad, because to me it
> > >	suggests doing ***paired*** t-tests, which would trip up the
> > >	naive user, who probably wouldn't notice or would ignore the
> > >	"t tests with pooled SD" message in the output.  As one of
> > >	the Ripley fortunes says ``It really is hard to anticipate
> > >	just how silly users can be.''  But why go out of the way to
> > >	give them a chance to be silly?
> > 
> > And Jack wrote:
> > 
> > >But the documentation, which I valiantly tried to make sense 
> > of BEFORE 
> > >asking my stupid question, is not clear enough for this 
> > particular idiot. 
> > >Might I suggest that the documentation be altered? It could 
> > use an example 
> > >(as in, real-life applied statistical problem) of when 
> > pairwise.t.test() 
> > >ought to be used, and why t.test(paired=TRUE) would be 
> > inappropriate in that 
> > >context; it could also use a reference to some published 
> > paper, website or 
> > >some such that explains the rationale and correct procedure 
> > for using this 
> > >test.
> > 
> > I think it's unlikely that we would rename the function; it's been
> > around a while with its current name so that's a bad idea.  On the
> > other hand, clearer documentation is always a plus:  why not submit
> > some?
> 
> I guess this is sort of related to the thread on whether R is good for
> non-statisticians...  The help pages in R are sort of like *nix man pages.
> They give the technical information about the topic, but not necessarily the
> background.  E.g., the man page for `chmod' does not explain file
> permissions in detail: the user is expected to learn that elsewhere.
> 
> Perhaps other stat packages do it differently?  Does SPSS manuals detail
> what its t-test procedure does, including which t-test(s) it does and when
> it's appropriate?  That might make it easier on users, but I still think the
> users should learn the appropriate use of statistical procedures
> elsewhere...
> 
> Best,
> Andy


Andy,

I don't know about SPSS, but SAS' documentation is available online at:

http://support.sas.com/91doc/docMainpage.jsp

The documentation specifically for PROC TTEST is at:

http://support.sas.com/91doc/getDoc/statug.hlp/ttest_index.htm

and the documentation for PROC MULTTEST is at:

http://support.sas.com/91doc/getDoc/statug.hlp/multtest_index.htm

Of course, to go along with the standard SAS documentation, there is the
line of "Books by Users", which parallels in a fashion, the increasing
number of books on R, authored by members of this community.

Best regards,

Marc



From nair at sdsc.edu  Thu Aug 19 23:14:44 2004
From: nair at sdsc.edu (T. Murlidharan Nair)
Date: Thu, 19 Aug 2004 14:14:44 -0700
Subject: [R] Coagulation data by Box/Hunter/Hunter
Message-ID: <41251844.2050305@sdsc.edu>

Hi!!
Has anyone used the coagulation data for statistical analysis ?  I 
managed to
get the data from the web but unsure of the way its supposed to read. I am
new to R so trying to gets myself familiarized with the statistical 
tools using
available data.  I am appending the data I got of the web. If anyone is 
aware of
its use I would welcome their input.
Cheers always!!
Murli

=======================================================================
THIS IS DATAPLOT DATA FILE   BOXBLOOD.DAT
DIET EFFECT ON BLOOD COAGULATION
BOX, HUNTER & HUNTER (1978)
STATISTIC FOR EXPERIMENTERS
WILEY, PAGE 165-197 (MAIN EXAMPLE OF CHAPTER 6)
COMPLETELY RANDOMIZED DESIGN
NUMBER OF OBSERVATIONS = 24
TOTAL NUMBER OF VARIABLES PER LINE IMAGE = 3
   RESPONSE VARIABLE = BLOOD COAGULATION TIME
   FACTOR 1 = DIET (4 LEVELS)
   FACTOR 2 = RUN SEQUENCE
TO READ THIS FILE INTO DATAPLOT (AND ANALYZE)--
   SKIP 25
   READ BOXBLOOD.DAT Y X1 RUNSEQ
   CHAR X ALL
   PLOT Y X1 X1
   .
   ER; ANOVA Y X1
   .
   PLOT RES X1 X1
   PLOT RES PRED PRED
   PLOT RES RUNSEQ
   NORMAL PROBABILITY PLOT RES
 Y   X1  RUNSEQ
-----------------
62    1    20
60    1     2
63    1    11
59    1    10
63    2    12
67    2     9
71    2    15
64    2    14
65    2     4
66    2     8
68    3    16
66    3     7
71    3     1
67    3    17
68    3    13
68    3    21
56    4    23
62    4     3
60    4     6
61    4    18
63    4    22
64    4    19
63    4     5
59    4    24



From lauraholt_983 at hotmail.com  Thu Aug 19 22:43:37 2004
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Thu, 19 Aug 2004 15:43:37 -0500
Subject: [R] ROracle and vector elements
Message-ID: <BAY12-F32SXRu4eoqsD000bbc90@hotmail.com>

Hi there!

Is ROracle available for Windows, please?

I found a download site, but it's really for UNIX/Linux.

Here is a "thought question", please:  Why do the vector elements start at 
location 1 rather than zero, as C does?

Thanks in advance!

R Version 1.9.1 Windows
Sincerely,
Laura Holt
mailto: lauraholt_983 at hotmail.com



From ikshan at ucla.edu  Thu Aug 19 23:23:56 2004
From: ikshan at ucla.edu (Stephanie Tsung)
Date: Thu, 19 Aug 2004 14:23:56 -0700
Subject: [R] A question about memory size
Message-ID: <00e801c48632$d5eca8c0$1107a8c0@youru0jglaxfze>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040819/b3d052ac/attachment.pl

From charles.edwin.white at us.army.mil  Thu Aug 19 23:17:06 2004
From: charles.edwin.white at us.army.mil (White, Charles E WRAIR-Wash DC)
Date: Thu, 19 Aug 2004 17:17:06 -0400
Subject: [R] List dimention labels to plots of components
Message-ID: <12D0D00E1404D511A4820090274CA09C03FBA749@dasmtyjqf010.amedd.army.mil>

It is frustrating to see the labels I want in the dimensions of a list but not be able to extract those labels into titles for plots generated from component objects. If someone could set me straight, I would appreciate it. For your amusement, I have provided an example of the Byzantine code I am currently using to avoid loops:

# Simulate ANOVA type test data
sex<-c(rep(1,8),rep(0,8))
dose<-c(rep(1,4),rep(0,4))
treatment<-c(rep(1,2),rep(0,2))
fix<-sex+dose+treatment
Response<-fix+rnorm(16)
Sex<-rep("Male",16)
Sex[sex==0]<-"Female"
Dose<-rep("High",16)
Dose[dose==0]<-"Low"
Treatment<-rep("A",16)
Treatment[treatment==0]<-"B"
dat<-data.frame(Sex,Dose,Treatment,fix,Response)

# Redundant Transfer and Execution of Dimension ID?
mymod<-function(x){
  model<-lm(Response~Sex,data=x)
  list(model,Dose=x$Dose[1],Treatment=x$Treatment[1])}
myplt<-function(x){
  plot(x[[1]],main=paste(x$Dose,"/",x$Treatment,sep=""))}

# Generate list of Model Estimates 
dat.lm<-by(dat,list(Dose=dat$Dose,Treatment=dat$Treatment),mymod)

# Execute plots with labels
pdf(file="junk.pdf",height=7.5,width=10)
par(mfrow=c(2,2))
bitbucket<-lapply(dat.lm,myplt)
dev.off()

Charles E. White, Senior Biostatistician, MS
Walter Reed Army Institute of Research
503 Robert Grant Ave., Room 1w102
Silver Spring, MD 20910-1557
301 319-9781
Personal/Professional Site: http://users.starpower.net/cwhite571/professional/



From ripley at stats.ox.ac.uk  Thu Aug 19 23:42:50 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 19 Aug 2004 22:42:50 +0100 (BST)
Subject: [R] A question about memory size
In-Reply-To: <00e801c48632$d5eca8c0$1107a8c0@youru0jglaxfze>
Message-ID: <Pine.LNX.4.44.0408192241560.2411-100000@gannet.stats>

Please read the rw-FAQ, as the posting guide asks.  The answer is there.

On Thu, 19 Aug 2004, Stephanie Tsung wrote:

> I have a question about how to increase my memory size, could someone
> answer it for me??
> 
> I am using Bioconductor in R to calculate gene expression values with
> mas5, dchip, and mas4. I have only 18 samples, all from Affymetrix U133A
> Plus 2 arrays, which has ~54,000 genes. My machine equipments are: CPU
> P4 3.0GHz, and 1GM RAM. Somehow when I was running mas5 in R, it always
> showed the error message: "Cannot allocate vector of size 211702 Kb." I
> believed that it would be a memory setting problem, so I tried to use
> the command: memory.size(TRUE) or memory.limit(size=NA) to increase the
> memory. But it still didn't work!! Can someone tell me what's going on
> and how can I run it?? I really appreciate it. Thanks.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Thu Aug 19 23:39:48 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 19 Aug 2004 23:39:48 +0200
Subject: [R] ROracle and vector elements
In-Reply-To: <BAY12-F32SXRu4eoqsD000bbc90@hotmail.com>
References: <BAY12-F32SXRu4eoqsD000bbc90@hotmail.com>
Message-ID: <x2hdqykdsr.fsf@biostat.ku.dk>

"Laura Holt" <lauraholt_983 at hotmail.com> writes:

> Hi there!
> 
> Is ROracle available for Windows, please?
> 
> I found a download site, but it's really for UNIX/Linux.

As I understand it, a Windows version can easily (?) be built, but to
do so you need Oracle libraries and header files, which you can't have
without having Oracle. The people who build binary packages do not
have Oracle...
 
> Here is a "thought question", please:  Why do the vector elements
> start at location 1 rather than zero, as C does?

Negative indexing is a fairly good argument. C doesn't have that.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Meredith.Briggs at team.telstra.com  Fri Aug 20 00:58:11 2004
From: Meredith.Briggs at team.telstra.com (Briggs, Meredith M)
Date: Fri, 20 Aug 2004 08:58:11 +1000
Subject: [R] How do you read in a table with numeric and dates fields
	without it defaulting to reading in the dates as factors?
Message-ID: <3B5823541A25D311B3B90008C7F9056410E3578B@ntmsg0092.corpmail.telstra.com.au>



From andrewr at uidaho.edu  Fri Aug 20 01:05:46 2004
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Fri, 20 Aug 2004 09:05:46 +1000
Subject: [R] Is R good for not-professional-statistician,
	un-mathematical clinical researchers?
Message-ID: <1fd2051fbe71.1fbe711fd205@uidaho.edu>

Jake,

this is a great question.

I have experience teaching R to forestry and natural resources graduate and undergraduate students.  These students are amongst the least numerically comfortable students I've taught.  I have found that the numerically comfortable among them leap into the challenge, and the remainder reject it (and usually me) as irrelevant and deliberately obtuse.  I am heavily biased towards R because every once in a while I find myself writing code with a big happy smile.  It's just fun.  There are some things that it does, that it does just beautifully.  Oh, and I'm addicted to Sweave :).

R, by its nature, exposes the seamy underbelly of statistical reasoning, and this creates various degrees of empowerment and fear, depending on the student.  R requires us to create the syntax of our commands, and to think through the process of analysis before its execution, at least enough to compose a coherent statement.  Of course, this is educationally a very good thing, but also very challenging.  

I would suggest that it's just as plausible to teach R in a pre-packaged way as it is to teach the other applications in a pre-packaged way.  The problem arises when the students have to do their own analyses.  R has many elements that require confidence and experience to overcome efficiently.  

For example, let's take error reporting: one really has to have at least a grasp of matrix algebra to know what singularities are.  Yet, singularities will be reported in errors for tools that, prima facie, have no necessary obvious relation with matrices.  Or another example is: we don't need to know qr-decomposition or svd in order to be able to understand the statistical elements of a multiple regression.  But, sometimes (NOT in those cases, I hasten to add, but they're a good example of the kind of thing I mean) these details bleed through.  So, our error messages sometimes, lack obvious statistical relation to the problem at hand for the neophyte.  They can seem cryptic and obscure.

Ok, this is inevitable, in a community-generated product like R, but it is a hurdle that students will find frustrating, and will take them a long time to overcome, regardless of their good intentions.  Many of the help files include references to further reading, which is excellent and essential, but some do not.  

Now, I argue that student will certainly benefit from adopting the hacker-style can-do attitude necessary to plough forwards.  But they rightly ask: is this the most appropriate medium for that approach to be encouraged, for us?  A problem is that for success, they not only require the hacker-style can-do attitude, they also require technical background, which they do not intend to develop.  Googling alone is not the answer, nor is R-help.  So it depends on the student.  

In general, though, I don't think that R by itself can be considered adequate for not-professional-statistician, un-mathematical clinical researchers.  I don't think that it wants to be, or that we want it to be, enough.  So, increasingly, I do think that we should learn another language, and offer such students the option of a more unified approach.

Thanks for a very thought-provoking question.

Andrew

----- Original Message -----
From: Jacob Wegelin <jawegelin at ucdavis.edu>
Date: Thursday, August 19, 2004 4:45 pm
Subject: [R] Is R good for not-professional-statistician, un-mathematical clinical researchers?

> 
> Alternate title: How can I persuade my students that R is for them?
> 
> Alternate title: Can R replace SAS, SPSS or Stata for clinicians?
> 
> I am teaching introductory statistics to twelve physicians and two 
> veterinarianswho have enrolled in a Mentored Clinical Research 
> Training Program.  My course is the
> first in a sequence of three.  We (the instructors of this 
> sequence) chose to teach
> R rather than some other computing environment.
> 
> My (highly motivated) students have never encountered anything 
> like R.  One frankly  asked:
> 
> "Do you feel (honestly) that a group of physicians (with two vets) 
> clinicians will
> be able to effectively use and actually understand R? If so, I 
> will happily call this
> bookstore and order this book [Venables and Ripley] tomorrow."
> 
> I am heavily biased toward R/S because I have used it since the 
> first applied statistics
> course I took.  But I would love to give these students some kind 
> of objective information
> about the usability of R by non-statisticians--not just my own bias.
> 
> Could anyone suggest any such information?  Or does anyone on this 
> list use R who is
> a clinician and not really mathematically savvy?  For instance, 
> someone who doesn't
> remember any math beyond algebra and doesn't think in terms of P(A|B)?
> 
> Or have we done a disservice to our students by choosing to make them
> learn R, rather than making ourselves learn SAS, Stata or SPSS?
> 
> Thank you for any ideas
> 
> Jake Wegelin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From p.murrell at auckland.ac.nz  Fri Aug 20 01:19:06 2004
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Fri, 20 Aug 2004 11:19:06 +1200
Subject: [R] Clipping of display in Lattice graphics
References: <41249137.5060207@massey.ac.nz>
	<200408190914.30212.deepayan@stat.wisc.edu>
Message-ID: <4125356A.2080604@stat.auckland.ac.nz>

Hi


Deepayan Sarkar wrote:
> On Thursday 19 August 2004 06:38, Matthew Walker wrote:
> 
>>I'm baffled as to how the Lattice package achieves clipping.  Would
>>someone mind explaining this to me?
>>
>>Firstly, my attempt using "just" the grid package:
>>
>>x<-seq(0,3,by=0.3)/2.8
>>y<-seq(0,1,by=0.1)
>>
>>grid.newpage()
>>grid.rect(gp=gpar(fill="pink"))
>>
>>vp<-viewport(width=0.8, height=0.8)
> 
> 
> You need to add 'clip="on"' here. Clipping is a property of viewports 
> (not individual 'grob'-s as produced by grid.points and grid.lines),  
> controlled by the 'clip' argument to viewport(). Details (and caveats) 
> in ?viewport.


If your aim is to draw outside the lattice panel, one way is to write a 
panel function that pushes a viewport with clipping turned off.  The 
following gratuitous example also shows how you can recreate the x- and 
y-axis ranges in the new viewport.

library(grid)
data(quakes)
Depth <- equal.count(quakes$depth, number=8, overlap=.1)
# extra lines clipped to panel
xyplot(lat ~ long | Depth, data = quakes,
        panel=function(x, y, ...) {
          panel.xyplot(x, y, ...)
          grid.lines(c(155, 200), c(-40, -10),
                     gp=gpar(lwd=3),
                     default.units="native")
          })
# extra lines extend beyond panel
xyplot(lat ~ long | Depth, data = quakes,
        panel=function(x, y, ...) {
          panel.xyplot(x, y, ...)
          xscale <- convertX(unit(0:1, "npc"), "native", valueOnly=TRUE)
          yscale <- convertY(unit(0:1, "npc"), "native", valueOnly=TRUE)
          pushViewport(viewport(clip="off",
                                xscale=xscale, yscale=yscale))
          grid.lines(c(155, 200), c(-40, -10),
                     gp=gpar(lwd=3),
                     default.units="native")
          popViewport()
          })

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From louize99 at yahoo.co.uk  Fri Aug 20 01:26:15 2004
From: louize99 at yahoo.co.uk (Louize Hill)
Date: Fri, 20 Aug 2004 00:26:15 +0100
Subject: [R] column names in data.frame
Message-ID: <004e01c48643$ec783c10$bbc98c52@Louisept>

Dear R-help,
Please can someone explain how to put a column name on an output data.frame.

##Starting with a data.frame with 3 columns (d$Year, d$NoIndiv, d$wtd_tl)

yr_ind <- split (d$NoIndiv, d$Year)
yr_tl <- split (d$wtd_tl, d$Year)

ann_ind <- sapply (yr_ind, sum)
ann_tl <- sapply (yr_tl, sum)

av_tl <- ann_tl/ann_ind

d2<- data.frame (av_tl)

##This gives me a data.frame with columns, the second of which has a column
name (av_tl)
I have tried
> d2<- data.frame (year = x, av_tl)
> d2<- data.frame (x="year", av_tl)
> d2<- data.frame (x="year", av_tl, check.names = TRUE)
> d2<- data.frame (year, av_tl)

as well as several combinations with cbind, as.matrix, etc...
I cannot relate the examples given in ?data.frame with my problem or find a
similar problem in the archives.
Thanks
Louize



From andrewr at uidaho.edu  Fri Aug 20 01:27:55 2004
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Fri, 20 Aug 2004 09:27:55 +1000
Subject: [R] glmmPQL in R and S-PLUS 6 - differing results
Message-ID: <200daa2024c6.2024c6200daa@uidaho.edu>

Professor Ripley,

> These are optimization problems with multiple local maxima, and 
> like any complex statistical fitting problem you should not expect all 
> programs to give the same answer.  

Examining the log-likelihoods from the anova statements, while fit1 and m1 are similar, the maximum for fit2 (due to R) is higher than the maximum for m2 (due to S-plus).

R:

>  anova(fit, fit2)
      Model df      AIC      BIC    logLik   Test  L.Ratio p-value
fit      1  4 87.79085 94.12493 -39.89543                       
fit2     2  3 86.41927 91.16983 -40.20964 1 vs 2 0.628421  0.4279

S-PLUS:

>  anova(m1, m2)
   Model df      AIC      BIC    logLik   Test L.Ratio p-value
m1     1  4 87.00400 93.33808 -39.50200                     
m2     2  3 91.75447 96.50503 -42.87724 1 vs 2 6.75047  0.0094


Can we interpret that in any way? For example, is it reasonable to infer that in this case the R results, suggesting no significant difference between the model, are preferable?

Andrew

Associate Professor, Forest Biometrics
University of Idaho
Moscow ID 83843



From supton at referentia.com  Fri Aug 20 01:35:47 2004
From: supton at referentia.com (Steve Upton)
Date: Thu, 19 Aug 2004 19:35:47 -0400
Subject: [R] column names in data.frame
In-Reply-To: <004e01c48643$ec783c10$bbc98c52@Louisept>
Message-ID: <20040819233541.KEWY4212.imta06a2.registeredsite.com@FLORIDA>

Is names(d2) <- c("year","av_t1") what you're looking for?

steve

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of Louize Hill
> Sent: Thursday, August 19, 2004 7:26 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] column names in data.frame
> 
> Dear R-help,
> Please can someone explain how to put a column name on an output
> data.frame.
> 
> ##Starting with a data.frame with 3 columns (d$Year, d$NoIndiv, d$wtd_tl)
> 
> yr_ind <- split (d$NoIndiv, d$Year)
> yr_tl <- split (d$wtd_tl, d$Year)
> 
> ann_ind <- sapply (yr_ind, sum)
> ann_tl <- sapply (yr_tl, sum)
> 
> av_tl <- ann_tl/ann_ind
> 
> d2<- data.frame (av_tl)
> 
> ##This gives me a data.frame with columns, the second of which has a
> column
> name (av_tl)
> I have tried
> > d2<- data.frame (year = x, av_tl)
> > d2<- data.frame (x="year", av_tl)
> > d2<- data.frame (x="year", av_tl, check.names = TRUE)
> > d2<- data.frame (year, av_tl)
> 
> as well as several combinations with cbind, as.matrix, etc...
> I cannot relate the examples given in ?data.frame with my problem or find
> a
> similar problem in the archives.
> Thanks
> Louize
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From lebouton at msu.edu  Fri Aug 20 01:56:41 2004
From: lebouton at msu.edu (Joseph LeBouton)
Date: Thu, 19 Aug 2004 19:56:41 -0400
Subject: [R] probability histogram question
Message-ID: <41253E39.2000300@msu.edu>

Hello, all;

I get an unexpected result when trying to plot a probability histogram
with R1.9.1 on windows xp:

#with the following code:

> x <- runif(100,0,1)
> hist(x)
> hist(x, freq=F)
> h <- hist(x, freq=F)
> summary(h)

#            Length Class  Mode
#breaks      11     -none- numeric
#counts      10     -none- numeric
#intensities 10     -none- numeric
#density     10     -none- numeric
#mids        10     -none- numeric
#xname        1     -none- character
#equidist     1     -none- logical

# The help file says that <h$density>  holds the values
#	plotted in the probability histogram.  If that's the
#	case, I'd expect that the sum of h$density for a histogram
	where freq=F would equal 1.0 ...  However:

> sum(h$density)

#returns the value :
#[1] 10

I would really like to plot values in the probablility histogram that
sum to 1, not 10.  Is there a switch in the hist(x) command that I'm
missing?  Or, is there another function that can do this?

Thanks in advance,

jlb

-- 
************************************
Joseph P. LeBouton
Forest Ecology PhD Candidate
Department of Forestry
Michigan State University
East Lansing, Michigan 48824

Office phone: 517-355-7744
email: lebouton at msu.edu



From louize99 at yahoo.co.uk  Fri Aug 20 02:10:20 2004
From: louize99 at yahoo.co.uk (Louize Hill)
Date: Fri, 20 Aug 2004 01:10:20 +0100
Subject: [R] column names in data.frame
References: <20040819233541.KEWY4212.imta06a2.registeredsite.com@FLORIDA>
Message-ID: <000e01c4864a$14f9beb0$3bc98c52@Louisept>

apparently not
that gives me the following error:

Error in "names<-.default"(`*tmp*`, value = c("year", "av_t1")) :
        names attribute [2] must be the same length as the vector [1]

presumably because the 2nd column (av_tl) already has a column name, and
therefore seems longer?


----- Original Message ----- 
From: "Steve Upton" <supton at referentia.com>
To: "'Louize Hill'" <lhill at ipimar.pt>; <r-help at stat.math.ethz.ch>
Sent: Friday, August 20, 2004 12:35 AM
Subject: RE: [R] column names in data.frame


> Is names(d2) <- c("year","av_t1") what you're looking for?
>
> steve
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> > bounces at stat.math.ethz.ch] On Behalf Of Louize Hill
> > Sent: Thursday, August 19, 2004 7:26 PM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] column names in data.frame
> >
> > Dear R-help,
> > Please can someone explain how to put a column name on an output
> > data.frame.
> >
> > ##Starting with a data.frame with 3 columns (d$Year, d$NoIndiv,
d$wtd_tl)
> >
> > yr_ind <- split (d$NoIndiv, d$Year)
> > yr_tl <- split (d$wtd_tl, d$Year)
> >
> > ann_ind <- sapply (yr_ind, sum)
> > ann_tl <- sapply (yr_tl, sum)
> >
> > av_tl <- ann_tl/ann_ind
> >
> > d2<- data.frame (av_tl)
> >
> > ##This gives me a data.frame with columns, the second of which has a
> > column
> > name (av_tl)
> > I have tried
> > > d2<- data.frame (year = x, av_tl)
> > > d2<- data.frame (x="year", av_tl)
> > > d2<- data.frame (x="year", av_tl, check.names = TRUE)
> > > d2<- data.frame (year, av_tl)
> >
> > as well as several combinations with cbind, as.matrix, etc...
> > I cannot relate the examples given in ?data.frame with my problem or
find
> > a
> > similar problem in the archives.
> > Thanks
> > Louize
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-
> > guide.html
>
>



From astephen at efs.mq.edu.au  Fri Aug 20 02:14:28 2004
From: astephen at efs.mq.edu.au (Alec Stephenson)
Date: Fri, 20 Aug 2004 10:14:28 +1000
Subject: [R] probability histogram question
Message-ID: <s125cf16.078@efs04.efs.mq.edu.au>



Alec Stephenson                                               
Department of Statistics
Macquarie University
NSW 2109, Australia 

>>> Joseph LeBouton <lebouton at msu.edu> 08/20/04 09:56am >>>
Hello, all;

I get an unexpected result when trying to plot a probability histogram
with R1.9.1 on windows xp:

#with the following code:

> x <- runif(100,0,1)
> hist(x)
> hist(x, freq=F)
> h <- hist(x, freq=F)
> summary(h)

#            Length Class  Mode
#breaks      11     -none- numeric
#counts      10     -none- numeric
#intensities 10     -none- numeric
#density     10     -none- numeric
#mids        10     -none- numeric
#xname        1     -none- character
#equidist     1     -none- logical

# The help file says that <h$density>  holds the values
#	plotted in the probability histogram.  If that's the
#	case, I'd expect that the sum of h$density for a histogram
	where freq=F would equal 1.0 ...  However:

> sum(h$density)

#returns the value :
#[1] 10

I would really like to plot values in the probablility histogram that
sum to 1, not 10.  Is there a switch in the hist(x) command that I'm
missing?  Or, is there another function that can do this?

The width of each bar is 0.1, so you need
sum(h$density) * 0.1
or in general
sum(h$density * diff(h$breaks))

Thanks,
Alec



From lebouton at msu.edu  Fri Aug 20 02:28:47 2004
From: lebouton at msu.edu (Joseph LeBouton)
Date: Thu, 19 Aug 2004 20:28:47 -0400
Subject: [R] probability histogram question
In-Reply-To: <s125cf16.076@efs04.efs.mq.edu.au>
References: <s125cf16.076@efs04.efs.mq.edu.au>
Message-ID: <412545BF.9010906@msu.edu>

Alec,

Thanks for your reply.  I guess what I'm getting at is that I to plot 
the histogram such that the HEIGHT of each bar represents the proportion 
of that class in the sample.  From your reply I gather that the AREA of 
each bar is currently representing the proportion.

My current work-around is to not plot the histogram immediately; I set 
it up (with plot=F), divide h$density by 10, then plot h;

x <- runif(100,0,1)
h <- hist(x, freq=F, plot=F)
h$density <- h$density/10
plot(h, freq=F)

while this is up to my normal hacking modus operandi, it's not terribly 
efficient.  Is there another way to do that?  Or is what I'm trying to 
do a perceptually and/or statistically incorrect way to think about 
histograms?

Thanks again,

jlb

Alec Stephenson wrote:

> 
> Alec Stephenson                                               
> Department of Statistics
> Macquarie University
> NSW 2109, Australia 
> 
> 
>>>>Joseph LeBouton <lebouton at msu.edu> 08/20/04 09:56am >>>
> 
> Hello, all;
> 
> I get an unexpected result when trying to plot a probability histogram
> with R1.9.1 on windows xp:
> 
> #with the following code:
> 
> 
>>x <- runif(100,0,1)
>>hist(x)
>>hist(x, freq=F)
>>h <- hist(x, freq=F)
>>summary(h)
> 
> 
> #            Length Class  Mode
> #breaks      11     -none- numeric
> #counts      10     -none- numeric
> #intensities 10     -none- numeric
> #density     10     -none- numeric
> #mids        10     -none- numeric
> #xname        1     -none- character
> #equidist     1     -none- logical
> 
> # The help file says that <h$density>  holds the values
> #	plotted in the probability histogram.  If that's the
> #	case, I'd expect that the sum of h$density for a histogram
> 	where freq=F would equal 1.0 ...  However:
> 
> 
>>sum(h$density)
> 
> 
> #returns the value :
> #[1] 10
> 
> I would really like to plot values in the probablility histogram that
> sum to 1, not 10.  Is there a switch in the hist(x) command that I'm
> missing?  Or, is there another function that can do this?
> 
> The width of each bar is 0.1, so you need
> sum(h$density) * 0.1
> or in general
> sum(h$density * diff(h$breaks))
> 
> Thanks,
> Alec
> 
> 
> 
> 

-- 
************************************
Joseph P. LeBouton
Forest Ecology PhD Candidate
Department of Forestry
Michigan State University
East Lansing, Michigan 48824

Office phone: 517-355-7744
email: lebouton at msu.edu



From supton at referentia.com  Fri Aug 20 02:33:45 2004
From: supton at referentia.com (Steve Upton)
Date: Thu, 19 Aug 2004 20:33:45 -0400
Subject: [R] column names in data.frame
In-Reply-To: <000e01c4864a$14f9beb0$3bc98c52@Louisept>
Message-ID: <20040820003341.YXHS4801.imta01a2.registeredsite.com@FLORIDA>

Hi Louize,

Sorry to lead you astray, but I misinterpreted what you were doing (I could
still be leading you astray :-(. You should be able to make a names
attribute for d2 by names(d2) <- c("year", "col2namewhatyouwant",
"col3namewhatyouwant", ...) to the length of av_t1. When you created the
data frame there are no names associated with av_t1 (at least I don't think
so) other than the "av_t1" name from the variable. Does that make sense?

This may be the blind helping the blind, so please excuse me ...

steve

> -----Original Message-----
> From: Louize Hill [mailto:louize99 at yahoo.co.uk]
> Sent: Thursday, August 19, 2004 8:10 PM
> To: Steve Upton; r-help at stat.math.ethz.ch
> Subject: Re: [R] column names in data.frame
> 
> apparently not
> that gives me the following error:
> 
> Error in "names<-.default"(`*tmp*`, value = c("year", "av_t1")) :
>         names attribute [2] must be the same length as the vector [1]
> 
> presumably because the 2nd column (av_tl) already has a column name, and
> therefore seems longer?
> 
> 
> ----- Original Message -----
> From: "Steve Upton" <supton at referentia.com>
> To: "'Louize Hill'" <lhill at ipimar.pt>; <r-help at stat.math.ethz.ch>
> Sent: Friday, August 20, 2004 12:35 AM
> Subject: RE: [R] column names in data.frame
> 
> 
> > Is names(d2) <- c("year","av_t1") what you're looking for?
> >
> > steve
> >
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> > > bounces at stat.math.ethz.ch] On Behalf Of Louize Hill
> > > Sent: Thursday, August 19, 2004 7:26 PM
> > > To: r-help at stat.math.ethz.ch
> > > Subject: [R] column names in data.frame
> > >
> > > Dear R-help,
> > > Please can someone explain how to put a column name on an output
> > > data.frame.
> > >
> > > ##Starting with a data.frame with 3 columns (d$Year, d$NoIndiv,
> d$wtd_tl)
> > >
> > > yr_ind <- split (d$NoIndiv, d$Year)
> > > yr_tl <- split (d$wtd_tl, d$Year)
> > >
> > > ann_ind <- sapply (yr_ind, sum)
> > > ann_tl <- sapply (yr_tl, sum)
> > >
> > > av_tl <- ann_tl/ann_ind
> > >
> > > d2<- data.frame (av_tl)
> > >
> > > ##This gives me a data.frame with columns, the second of which has a
> > > column
> > > name (av_tl)
> > > I have tried
> > > > d2<- data.frame (year = x, av_tl)
> > > > d2<- data.frame (x="year", av_tl)
> > > > d2<- data.frame (x="year", av_tl, check.names = TRUE)
> > > > d2<- data.frame (year, av_tl)
> > >
> > > as well as several combinations with cbind, as.matrix, etc...
> > > I cannot relate the examples given in ?data.frame with my problem or
> find
> > > a
> > > similar problem in the archives.
> > > Thanks
> > > Louize
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! http://www.R-project.org/posting-
> > > guide.html
> >
> >
>



From astephen at efs.mq.edu.au  Fri Aug 20 02:36:49 2004
From: astephen at efs.mq.edu.au (Alec Stephenson)
Date: Fri, 20 Aug 2004 10:36:49 +1000
Subject: [R] column names in data.frame
Message-ID: <s125d469.060@efs04.efs.mq.edu.au>

I suspect you are interpreting the column names as a column, whereas in
fact you only have a single column data frame, so using
names(d2) <- "whatever"
will work.

Thanks,
Alec



Alec Stephenson                                               
Department of Statistics
Macquarie University
NSW 2109, Australia 

>>> louize99 at yahoo.co.uk 08/20/04 10:10am >>>
apparently not
that gives me the following error:

Error in "names<-.default"(`*tmp*`, value = c("year", "av_t1")) :
        names attribute [2] must be the same length as the vector [1]

presumably because the 2nd column (av_tl) already has a column name,
and
therefore seems longer?


----- Original Message ----- 
From: "Steve Upton" <supton at referentia.com>
To: "'Louize Hill'" <lhill at ipimar.pt>; <r-help at stat.math.ethz.ch>
Sent: Friday, August 20, 2004 12:35 AM
Subject: RE: [R] column names in data.frame


> Is names(d2) <- c("year","av_t1") what you're looking for?
>
> steve
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> > bounces at stat.math.ethz.ch] On Behalf Of Louize Hill
> > Sent: Thursday, August 19, 2004 7:26 PM
> > To: r-help at stat.math.ethz.ch 
> > Subject: [R] column names in data.frame
> >
> > Dear R-help,
> > Please can someone explain how to put a column name on an output
> > data.frame.
> >
> > ##Starting with a data.frame with 3 columns (d$Year, d$NoIndiv,
d$wtd_tl)
> >
> > yr_ind <- split (d$NoIndiv, d$Year)
> > yr_tl <- split (d$wtd_tl, d$Year)
> >
> > ann_ind <- sapply (yr_ind, sum)
> > ann_tl <- sapply (yr_tl, sum)
> >
> > av_tl <- ann_tl/ann_ind
> >
> > d2<- data.frame (av_tl)
> >
> > ##This gives me a data.frame with columns, the second of which has
a
> > column
> > name (av_tl)
> > I have tried
> > > d2<- data.frame (year = x, av_tl)
> > > d2<- data.frame (x="year", av_tl)
> > > d2<- data.frame (x="year", av_tl, check.names = TRUE)
> > > d2<- data.frame (year, av_tl)
> >
> > as well as several combinations with cbind, as.matrix, etc...
> > I cannot relate the examples given in ?data.frame with my problem
or
find
> > a
> > similar problem in the archives.
> > Thanks
> > Louize
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help 
> > PLEASE do read the posting guide! http://www.R-project.org/posting-

> > guide.html
>
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From louize99 at yahoo.co.uk  Fri Aug 20 02:43:05 2004
From: louize99 at yahoo.co.uk (Louize Hill)
Date: Fri, 20 Aug 2004 01:43:05 +0100
Subject: [R] column names in data.frame
References: <s125d469.058@efs04.efs.mq.edu.au>
Message-ID: <003001c4864e$a82fc310$3bc98c52@Louisept>

Hi Steve and Alec,

Thanks for your help - the 2nd column does have a column name ...
see below:

> d2
        av_tl
1979 3.520333
1980 3.513684
etc ...

or maybe I only have 1 column, as I can change "av_tl" to "year",
"whatever"... but then what is the list of dates (labels)?
I need to put a name to this column for subsequent analysis.
Thanks
Louize



----- Original Message ----- 
From: "Alec Stephenson" <astephen at efs.mq.edu.au>
To: <lhill at ipimar.pt>; <supton at referentia.com>; <r-help at stat.math.ethz.ch>;
<louize99 at yahoo.co.uk>
Sent: Friday, August 20, 2004 1:36 AM
Subject: Re: [R] column names in data.frame


> I suspect you are interpreting the column names as a column, whereas in
> fact you only have a single column data frame, so using
> names(d2) <- "whatever"
> will work.
>
> Thanks,
> Alec
>
>
>
> Alec Stephenson
> Department of Statistics
> Macquarie University
> NSW 2109, Australia
>
> >>> louize99 at yahoo.co.uk 08/20/04 10:10am >>>
> apparently not
> that gives me the following error:
>
> Error in "names<-.default"(`*tmp*`, value = c("year", "av_t1")) :
>         names attribute [2] must be the same length as the vector [1]
>
> presumably because the 2nd column (av_tl) already has a column name,
> and
> therefore seems longer?
>
>
> ----- Original Message ----- 
> From: "Steve Upton" <supton at referentia.com>
> To: "'Louize Hill'" <lhill at ipimar.pt>; <r-help at stat.math.ethz.ch>
> Sent: Friday, August 20, 2004 12:35 AM
> Subject: RE: [R] column names in data.frame
>
>
> > Is names(d2) <- c("year","av_t1") what you're looking for?
> >
> > steve
> >
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> > > bounces at stat.math.ethz.ch] On Behalf Of Louize Hill
> > > Sent: Thursday, August 19, 2004 7:26 PM
> > > To: r-help at stat.math.ethz.ch
> > > Subject: [R] column names in data.frame
> > >
> > > Dear R-help,
> > > Please can someone explain how to put a column name on an output
> > > data.frame.
> > >
> > > ##Starting with a data.frame with 3 columns (d$Year, d$NoIndiv,
> d$wtd_tl)
> > >
> > > yr_ind <- split (d$NoIndiv, d$Year)
> > > yr_tl <- split (d$wtd_tl, d$Year)
> > >
> > > ann_ind <- sapply (yr_ind, sum)
> > > ann_tl <- sapply (yr_tl, sum)
> > >
> > > av_tl <- ann_tl/ann_ind
> > >
> > > d2<- data.frame (av_tl)
> > >
> > > ##This gives me a data.frame with columns, the second of which has
> a
> > > column
> > > name (av_tl)
> > > I have tried
> > > > d2<- data.frame (year = x, av_tl)
> > > > d2<- data.frame (x="year", av_tl)
> > > > d2<- data.frame (x="year", av_tl, check.names = TRUE)
> > > > d2<- data.frame (year, av_tl)
> > >
> > > as well as several combinations with cbind, as.matrix, etc...
> > > I cannot relate the examples given in ?data.frame with my problem
> or
> find
> > > a
> > > similar problem in the archives.
> > > Thanks
> > > Louize
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! http://www.R-project.org/posting-
>
> > > guide.html
> >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From supton at referentia.com  Fri Aug 20 02:48:40 2004
From: supton at referentia.com (Steve Upton)
Date: Thu, 19 Aug 2004 20:48:40 -0400
Subject: [R] column names in data.frame
In-Reply-To: <003001c4864e$a82fc310$3bc98c52@Louisept>
Message-ID: <20040820004834.VJXV7559.imta02a2.registeredsite.com@FLORIDA>

Hi Louize,

What does names(d2) and str(d2) give you? As Alec has suggested your first
(column) may be just the row names of av_t1, which is a single column.

steve

> -----Original Message-----
> From: Louize Hill [mailto:louize99 at yahoo.co.uk]
> Sent: Thursday, August 19, 2004 8:43 PM
> To: Alec Stephenson; supton at referentia.com; r-help at stat.math.ethz.ch
> Subject: Re: [R] column names in data.frame
> 
> Hi Steve and Alec,
> 
> Thanks for your help - the 2nd column does have a column name ...
> see below:
> 
> > d2
>         av_tl
> 1979 3.520333
> 1980 3.513684
> etc ...
> 
> or maybe I only have 1 column, as I can change "av_tl" to "year",
> "whatever"... but then what is the list of dates (labels)?
> I need to put a name to this column for subsequent analysis.
> Thanks
> Louize
> 
> 
> 
> ----- Original Message -----
> From: "Alec Stephenson" <astephen at efs.mq.edu.au>
> To: <lhill at ipimar.pt>; <supton at referentia.com>; <r-
> help at stat.math.ethz.ch>;
> <louize99 at yahoo.co.uk>
> Sent: Friday, August 20, 2004 1:36 AM
> Subject: Re: [R] column names in data.frame
> 
> 
> > I suspect you are interpreting the column names as a column, whereas in
> > fact you only have a single column data frame, so using
> > names(d2) <- "whatever"
> > will work.
> >
> > Thanks,
> > Alec
> >
> >
> >
> > Alec Stephenson
> > Department of Statistics
> > Macquarie University
> > NSW 2109, Australia
> >
> > >>> louize99 at yahoo.co.uk 08/20/04 10:10am >>>
> > apparently not
> > that gives me the following error:
> >
> > Error in "names<-.default"(`*tmp*`, value = c("year", "av_t1")) :
> >         names attribute [2] must be the same length as the vector [1]
> >
> > presumably because the 2nd column (av_tl) already has a column name,
> > and
> > therefore seems longer?
> >
> >
> > ----- Original Message -----
> > From: "Steve Upton" <supton at referentia.com>
> > To: "'Louize Hill'" <lhill at ipimar.pt>; <r-help at stat.math.ethz.ch>
> > Sent: Friday, August 20, 2004 12:35 AM
> > Subject: RE: [R] column names in data.frame
> >
> >
> > > Is names(d2) <- c("year","av_t1") what you're looking for?
> > >
> > > steve
> > >
> > > > -----Original Message-----
> > > > From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> > > > bounces at stat.math.ethz.ch] On Behalf Of Louize Hill
> > > > Sent: Thursday, August 19, 2004 7:26 PM
> > > > To: r-help at stat.math.ethz.ch
> > > > Subject: [R] column names in data.frame
> > > >
> > > > Dear R-help,
> > > > Please can someone explain how to put a column name on an output
> > > > data.frame.
> > > >
> > > > ##Starting with a data.frame with 3 columns (d$Year, d$NoIndiv,
> > d$wtd_tl)
> > > >
> > > > yr_ind <- split (d$NoIndiv, d$Year)
> > > > yr_tl <- split (d$wtd_tl, d$Year)
> > > >
> > > > ann_ind <- sapply (yr_ind, sum)
> > > > ann_tl <- sapply (yr_tl, sum)
> > > >
> > > > av_tl <- ann_tl/ann_ind
> > > >
> > > > d2<- data.frame (av_tl)
> > > >
> > > > ##This gives me a data.frame with columns, the second of which has
> > a
> > > > column
> > > > name (av_tl)
> > > > I have tried
> > > > > d2<- data.frame (year = x, av_tl)
> > > > > d2<- data.frame (x="year", av_tl)
> > > > > d2<- data.frame (x="year", av_tl, check.names = TRUE)
> > > > > d2<- data.frame (year, av_tl)
> > > >
> > > > as well as several combinations with cbind, as.matrix, etc...
> > > > I cannot relate the examples given in ?data.frame with my problem
> > or
> > find
> > > > a
> > > > similar problem in the archives.
> > > > Thanks
> > > > Louize
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide! http://www.R-project.org/posting-
> >
> > > > guide.html
> > >
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >



From astephen at efs.mq.edu.au  Fri Aug 20 02:51:07 2004
From: astephen at efs.mq.edu.au (Alec Stephenson)
Date: Fri, 20 Aug 2004 10:51:07 +1000
Subject: [R] probability histogram question
Message-ID: <s125d7a7.079@efs04.efs.mq.edu.au>



Alec Stephenson                                               
Department of Statistics
Macquarie University
NSW 2109, Australia 

>>> Joseph LeBouton <lebouton at msu.edu> 08/20/04 10:28am >>>
Alec,

Thanks for your reply.  I guess what I'm getting at is that I to plot 
the histogram such that the HEIGHT of each bar represents the
proportion 
of that class in the sample.  From your reply I gather that the AREA of

each bar is currently representing the proportion.

My current work-around is to not plot the histogram immediately; I set

it up (with plot=F), divide h$density by 10, then plot h;

x <- runif(100,0,1)
h <- hist(x, freq=F, plot=F)
h$density <- h$density/10
plot(h, freq=F)

while this is up to my normal hacking modus operandi, it's not terribly

efficient.  Is there another way to do that?  Or is what I'm trying to

do a perceptually and/or statistically incorrect way to think about 
histograms?

Quick responses to each sentence in this paragraph:
1) Seems perfectly fine to me.
2) Not that I know of.
3) A histogram can be thought of as a density estimate. If you change
the scale on the x-axis, you are no longer plotting a histogram.

Yours,
Alec



From astephen at efs.mq.edu.au  Fri Aug 20 02:59:13 2004
From: astephen at efs.mq.edu.au (Alec Stephenson)
Date: Fri, 20 Aug 2004 10:59:13 +1000
Subject: [R] column names in data.frame
Message-ID: <s125d994.003@efs04.efs.mq.edu.au>

Your years are your row names; if you want to use them in an analysis,
it may be easiest to create a new column of years using cbind.



Alec Stephenson                                               
Department of Statistics
Macquarie University
NSW 2109, Australia 

>>> louize99 at yahoo.co.uk 08/20/04 10:43am >>>
Hi Steve and Alec,

Thanks for your help - the 2nd column does have a column name ...
see below:

> d2
        av_tl
1979 3.520333
1980 3.513684
etc ...

or maybe I only have 1 column, as I can change "av_tl" to "year",
"whatever"... but then what is the list of dates (labels)?
I need to put a name to this column for subsequent analysis.
Thanks
Louize



----- Original Message ----- 
From: "Alec Stephenson" <astephen at efs.mq.edu.au>
To: <lhill at ipimar.pt>; <supton at referentia.com>;
<r-help at stat.math.ethz.ch>;
<louize99 at yahoo.co.uk>
Sent: Friday, August 20, 2004 1:36 AM
Subject: Re: [R] column names in data.frame


> I suspect you are interpreting the column names as a column, whereas
in
> fact you only have a single column data frame, so using
> names(d2) <- "whatever"
> will work.
>
> Thanks,
> Alec
>
>
>
> Alec Stephenson
> Department of Statistics
> Macquarie University
> NSW 2109, Australia
>
> >>> louize99 at yahoo.co.uk 08/20/04 10:10am >>>
> apparently not
> that gives me the following error:
>
> Error in "names<-.default"(`*tmp*`, value = c("year", "av_t1")) :
>         names attribute [2] must be the same length as the vector
[1]
>
> presumably because the 2nd column (av_tl) already has a column name,
> and
> therefore seems longer?
>
>
> ----- Original Message ----- 
> From: "Steve Upton" <supton at referentia.com>
> To: "'Louize Hill'" <lhill at ipimar.pt>; <r-help at stat.math.ethz.ch>
> Sent: Friday, August 20, 2004 12:35 AM
> Subject: RE: [R] column names in data.frame
>
>
> > Is names(d2) <- c("year","av_t1") what you're looking for?
> >
> > steve
> >
> > > -----Original Message-----
> > > From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> > > bounces at stat.math.ethz.ch] On Behalf Of Louize Hill
> > > Sent: Thursday, August 19, 2004 7:26 PM
> > > To: r-help at stat.math.ethz.ch 
> > > Subject: [R] column names in data.frame
> > >
> > > Dear R-help,
> > > Please can someone explain how to put a column name on an output
> > > data.frame.
> > >
> > > ##Starting with a data.frame with 3 columns (d$Year, d$NoIndiv,
> d$wtd_tl)
> > >
> > > yr_ind <- split (d$NoIndiv, d$Year)
> > > yr_tl <- split (d$wtd_tl, d$Year)
> > >
> > > ann_ind <- sapply (yr_ind, sum)
> > > ann_tl <- sapply (yr_tl, sum)
> > >
> > > av_tl <- ann_tl/ann_ind
> > >
> > > d2<- data.frame (av_tl)
> > >
> > > ##This gives me a data.frame with columns, the second of which
has
> a
> > > column
> > > name (av_tl)
> > > I have tried
> > > > d2<- data.frame (year = x, av_tl)
> > > > d2<- data.frame (x="year", av_tl)
> > > > d2<- data.frame (x="year", av_tl, check.names = TRUE)
> > > > d2<- data.frame (year, av_tl)
> > >
> > > as well as several combinations with cbind, as.matrix, etc...
> > > I cannot relate the examples given in ?data.frame with my
problem
> or
> find
> > > a
> > > similar problem in the archives.
> > > Thanks
> > > Louize
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help 
> > > PLEASE do read the posting guide!
http://www.R-project.org/posting- 
>
> > > guide.html
> >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html 
>



From maustin at amgen.com  Fri Aug 20 03:27:34 2004
From: maustin at amgen.com (Austin, Matt)
Date: Thu, 19 Aug 2004 18:27:34 -0700
Subject: [R] column names in data.frame
Message-ID: <E7D5AB4811D20B489622AABA9C53859101F1115D@teal-exch.amgen.com>

Note that split() creates a list with each component named by the year.
When you use the sapply function the names are retained in the attribute
"names", you can print this out to see or use str() or look at the output
from the attribute() function.

Your division also maintains the names and then when you coerce av.tl into a
dataframe they are translated to row names.  Check out the output from
row.names(d2).  If you want these as part of your dataframe you could so
something like

data.frame(d2, year=row.names(d2))
or 
data.frame(av.tl, year=names(av.tl))

Hope this helps,

--Matt

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Louize Hill
Sent: Thursday, August 19, 2004 16:26 PM
To: r-help at stat.math.ethz.ch
Subject: [R] column names in data.frame


Dear R-help,
Please can someone explain how to put a column name on an output data.frame.

##Starting with a data.frame with 3 columns (d$Year, d$NoIndiv, d$wtd_tl)

yr_ind <- split (d$NoIndiv, d$Year)
yr_tl <- split (d$wtd_tl, d$Year)

ann_ind <- sapply (yr_ind, sum)
ann_tl <- sapply (yr_tl, sum)

av_tl <- ann_tl/ann_ind

d2<- data.frame (av_tl)

##This gives me a data.frame with columns, the second of which has a column
name (av_tl)
I have tried
> d2<- data.frame (year = x, av_tl)
> d2<- data.frame (x="year", av_tl)
> d2<- data.frame (x="year", av_tl, check.names = TRUE)
> d2<- data.frame (year, av_tl)

as well as several combinations with cbind, as.matrix, etc...
I cannot relate the examples given in ?data.frame with my problem or find a
similar problem in the archives.
Thanks
Louize

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From lauraholt_983 at hotmail.com  Fri Aug 20 04:53:29 2004
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Thu, 19 Aug 2004 21:53:29 -0500
Subject: [R] legends on the outside of the "box"
Message-ID: <BAY12-F15lYJwZZBuc600033190@hotmail.com>

Hi R People:

Here is a simple set of commands:

>x1 <- 1:10
>x2 <- sqrt(x1)
>plot(x1)
>points(x2,col="red",pch=3)
>legend(2,8,legend=c("First","Second"),col=c("black","red"),pch=c(1,3))
>
Fine.
Now, I would like to put the legend box on the outside of the plot itself, 
perhaps in the lower
left hand corner.

I've been messing with mtext, but to no avail.

Any help would be much appreciated.

R Version 1.9.1 Windows
Thanks,
Laura Holt
mailto: lauraholt_983 at hotmail.com



From aragon at berkeley.edu  Fri Aug 20 04:59:58 2004
From: aragon at berkeley.edu (Tomas Aragon)
Date: Thu, 19 Aug 2004 19:59:58 -0700 (PDT)
Subject: [R] Is R good for not-professional-statistician,
	un-mathematical clinical researchers?
In-Reply-To: <Pine.OSX.4.53.0408182313310.3661@biostat5.ucdavis.edu>
Message-ID: <20040820025958.29090.qmail@web80108.mail.yahoo.com>

I am a clinician turned epidemiologist. I just taught R in an intro epi
course. Here are some tips:

- encourage them to use R as their calculator
- encourage them to use R as their spreadsheet
- provide them with exercises to work this functionality
- we started a Yahoo help group for beginner questions (see
http://www.ucbcidp.org/epitools.html)

I have some exercises at http://www.medepi.net/epitools/lab/

If you only show how to do things that can be done is any statistical
package, then they'll choose the "user-friendly" statistical package
(e.g., stata).

For any additional question, please contact me.
Good luck!

Tomas Aragon
http://www.idready.org

 
--- Jacob Wegelin <jawegelin at ucdavis.edu> wrote:

> 
> Alternate title: How can I persuade my students that R is for them?
> 
> Alternate title: Can R replace SAS, SPSS or Stata for clinicians?
> 
> I am teaching introductory statistics to twelve physicians and two
> veterinarians
> who have enrolled in a Mentored Clinical Research Training Program. 
> My course is the
> first in a sequence of three.  We (the instructors of this sequence)
> chose to teach
> R rather than some other computing environment.
> 
> My (highly motivated) students have never encountered anything like
> R.  One frankly
> asked:
> 
> "Do you feel (honestly) that a group of physicians (with two vets)
> clinicians will
> be able to effectively use and actually understand R? If so, I will
> happily call this
> bookstore and order this book [Venables and Ripley] tomorrow."
> 
> I am heavily biased toward R/S because I have used it since the first
> applied statistics
> course I took.  But I would love to give these students some kind of
> objective information
> about the usability of R by non-statisticians--not just my own bias.
> 
> Could anyone suggest any such information?  Or does anyone on this
> list use R who is
> a clinician and not really mathematically savvy?  For instance,
> someone who doesn't
> remember any math beyond algebra and doesn't think in terms of
> P(A|B)?
> 
> Or have we done a disservice to our students by choosing to make them
> learn R, rather than making ourselves learn SAS, Stata or SPSS?
> 
> Thank you for any ideas
> 
> Jake Wegelin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 


=====
Tomas Aragon, MD, DrPH, Director
Center for Infectious Disease Preparedness
UC Berkeley School of Public Health
1918 University Ave., 4th Fl., MC-7350
Berkeley, CA 94720-7350
http://www.idready.org



From ggrothendieck at myway.com  Fri Aug 20 05:07:47 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 20 Aug 2004 03:07:47 +0000 (UTC)
Subject: [R] legends on the outside of the "box"
References: <BAY12-F15lYJwZZBuc600033190@hotmail.com>
Message-ID: <loom.20040820T050641-361@post.gmane.org>

Laura Holt <lauraholt_983 <at> hotmail.com> writes:

: 
: Hi R People:
: 
: Here is a simple set of commands:
: 
: >x1 <- 1:10
: >x2 <- sqrt(x1)
: >plot(x1)
: >points(x2,col="red",pch=3)
: >legend(2,8,legend=c("First","Second"),col=c("black","red"),pch=c(1,3))
: >
: Fine.
: Now, I would like to put the legend box on the outside of the plot itself, 
: perhaps in the lower
: left hand corner.
: 
: I've been messing with mtext, but to no avail.
: 
: Any help would be much appreciated.

Admittedly this is not what you have asked for but smartlegend in
package gregmisc makes it very easy to position the legend in the
plot area.



From chiaming.chen at jcu.edu.au  Fri Aug 20 06:51:20 2004
From: chiaming.chen at jcu.edu.au (Carla Chen)
Date: Fri, 20 Aug 2004 14:51:20 +1000
Subject: [R] Is there an alternative to subplot() of splus 
Message-ID: <000301c48671$55e0aa70$f428db89@elf.jcu.edu.au>


Dear R-gurus,

Is there any function in R does the same thing as function subplot()in
Splus?

I am trying to post a pie chart onto the top right hand corner of
existing bar chart.

Also, Does anyone know if I can use paste() to paste the graph to
another graph?

Cheers,

Carla Chen



From anafava at uiuc.edu  Fri Aug 20 06:53:32 2004
From: anafava at uiuc.edu (anafava@uiuc.edu)
Date: Thu, 19 Aug 2004 23:53:32 -0500
Subject: [R] Instrumental variables
Message-ID: <a2af9f04.72a72f80.8233d00@expms2.cites.uiuc.edu>

May someone help me to find out how to use intruments in R?

Thanks
Ana



From ozric at web.de  Fri Aug 20 08:25:13 2004
From: ozric at web.de (Christian Schulz)
Date: Fri, 20 Aug 2004 08:25:13 +0200
Subject: [R] Package rmutil
In-Reply-To: <5.1.0.14.2.20040819172814.00b575b0@pop.superig.com.br>
References: <5.1.0.14.2.20040819172814.00b575b0@pop.superig.com.br>
Message-ID: <200408200825.14140.ozric@web.de>

http://www.luc.ac.be/~jlindsey/rcode.html

christian

Am Donnerstag, 19. August 2004 22:30 schrieb Savano:
> Users,
>
> Where I can to download rmutil package?
>
> thanks
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri Aug 20 09:02:31 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 20 Aug 2004 09:02:31 +0200
Subject: [R] legends on the outside of the "box"
In-Reply-To: <BAY12-F15lYJwZZBuc600033190@hotmail.com>
References: <BAY12-F15lYJwZZBuc600033190@hotmail.com>
Message-ID: <4125A207.2070802@statistik.uni-dortmund.de>

Laura Holt wrote:

> Hi R People:
> 
> Here is a simple set of commands:
> 
>> x1 <- 1:10
>> x2 <- sqrt(x1)
>> plot(x1)
>> points(x2,col="red",pch=3)
>> legend(2,8,legend=c("First","Second"),col=c("black","red"),pch=c(1,3))
>>
> Fine.
> Now, I would like to put the legend box on the outside of the plot 
> itself, perhaps in the lower
> left hand corner.

Set par(xpd=NA) before trying to place it there. See ?par for details.

Uwe Ligges


> I've been messing with mtext, but to no avail.
> 
> Any help would be much appreciated.
> 
> R Version 1.9.1 Windows
> Thanks,
> Laura Holt
> mailto: lauraholt_983 at hotmail.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From Fiona.M.Wade at team.telstra.com  Fri Aug 20 09:16:16 2004
From: Fiona.M.Wade at team.telstra.com (Wade, Fiona M)
Date: Fri, 20 Aug 2004 17:16:16 +1000
Subject: [R] Do you know if you can map a large minimum spanning tree in R?
Message-ID: <61411576E951D211AF330008C7245DD916900174@ntmsg0005.corpmail.telstra.com.au>


Thanks Roger and Mike.
I spent most of today fiddling as Mike's suggestion got me the nodes on the map, but I had trouble with the coordinates, however I eventually got it to work with the following :

	...
	code to input site data, calculate distances, calculate minimum spanning tree in mdist
	...

	coords = array(0,c(NumSites,2))

	for( i in 1:NumSites )
	{
	   coords[i,1] = sites[i,3];
	   coords[i,2] = sites[i,2];
	}  #sites contains site name, lat, long

	library(spdep)
	mdist.nb <- mat2listw(mdist)$neighbours
	plot(mdist.nb, coords)

	library(maps)
	map('world',add=T,xlim=c(110,155),ylim=c(-45,-10))
	map(add=T,xlim=c(110,155),ylim=c(-45,-10))

Now I just have to pretty up the result for the boss!
Thanks again!
Fiona.


-----Original Message-----
From: Roger Bivand [mailto:Roger.Bivand at nhh.no]
Sent: Thursday, 19 August 2004 5:51 PM
To: r-help at stat.math.ethz.ch
Cc: fiona.m.wade at team.telstra.com
Subject: RE: [R] Do you know if you can map a large minimum spanning
tree in R?



>>
>> Thanks Mike.
>> My data has longitude and latitude coords and I used distAB {clim.pact}
> then mst {ape} to calculate my minimum spanning tree.  The nodes are
> telecoms sites from all over Australia.  My goal is to determine the
> minimum cost of linking them via cabling, and I'm starting by
>> calculating the distance "as the crow flies", but will probably
>> eventually need to calculate the rectilinear distances also.
>> I am a very newbie user of R, but have had experience with other
> stats/programming software such as SAS, however no longer have access to
> SAS so I've turned to R.  I also have tried using MapInfo with the data
> exported from R, but have found that not so intuitive to learn on the
> fly.
>> Back to R - I'm using W2K, and have managed to graph the tree using
> plot(mdist,graph="nsca") where mdist is the output matrix from my mst
> command, however this is not terribly map-like, so I'm looking for a
> better display that can be embedded in a document.
>> Any assistance gratefully received!
>
> One possibility is to use the fact that mst() function returns a 0/1
> matrix exactly like spatial weights matrices used in the spdep package:
>

Sorry, must be 2D:

> n <- 2

otherwise plot.nb() will not like X, the matrix of coordinates. In your
case that's obvious, but not in general.

>> X <- matrix(runif(n*50), 50, n)
>> d <- dist(X)
>> M <- mst(d)
>> library(spdep)
>> M.nb <- mat2listw(M)$neighbours
>> plot(M.nb, X)
>
> The plot.nb() function then plots the graph by joining points defined as
> neighbours, use the add=TRUE argument to overplot on a base map. The nb
> object can also be manipulated a little, like subsetting, if that is any
> use. The idea is to convert the 0/1 matrix to a list with vectors of
> neighbours' IDs for each point, rather than store the whole matrix.
> Contact me off-list if you need more details. (nbdists() retrieves the
> distances on the graph too).
>
>
>> Fiona.
>>
>>> Fiona Wade
>>> Project Manager
>>> MARA
>>> F&A
>>> Telstra Corporation Limited
>>> Tel: 03 9634 5674
>>> Fax: 03 9634 2874
>>> Email: fiona.m.wade at team.telstra.com
>>> The information contained in this e-mail message may be confidential.
>> If you are not the intended recipient, any use of, interference with,
> disclosure or copying of this material is unauthorised and prohibited.
> If you have received this message in error, please notify me by reply
> e-mail and then delete the message.
>>
>>
>> -----Original Message-----
>> From: Michael Sumner [mailto:mdsumner at utas.edu.au]
>> Sent: Thursday, 19 August 2004 10:18 AM
>> To: Briggs, Meredith M; r-help at stat.math.ethz.ch
>> Cc: Wade, Fiona M
>> Subject: Re: [R] Do you know if you can map a large minmum spanning tree
> in R?
>>
>>
>> At 09:47 AM 8/19/2004, Briggs, Meredith M wrote:
>>
>>
>>
>>>         Do you know
> if you can map in R?
>>>
> I have my minimum spanning tree, but as there are 1371 nodes
>> (all
>>> over Australia) I'd like to be able to "graph" them as they actually
> would be on the map.
>>>Do you know if this is possible?
>>
>> You can certainly "map" in R.  Depending on the coordinate system of
>> your
>> data . . .
>> but, e.g. - if it's lat/lon - perhaps the easiest way is to install the
> "maps" package and you can add the continental outlines to an existing
> plot:
>>
>> ## display nodes code here . . .
>> library(maps)
>> map('world',add=T,xlim=c(109,157),ylim=c(-47,-7))
>>
>> There are plenty of other options, if you have your own map data (or
>> want
>> to use another source).  Feel free to provide more detail about your
> current plotting methods and coordinate system.
>>
>> Also, the package "mapdata" contains a high reso
>> lution continental
>> dataset
>> -"worldHires"
>>
>> Hope that helps, Mike.
>>
>>
>>
>>
>>
>> ###############################################
>>
>> Michael Sumner - PhD. candidate
>> Maths and Physics (ACE CRC & IASOS) and Zoology (AWRU)
>> University of Tasmania
>> Private Bag 77, Hobart, Tas 7001, Australia
>> Phone: 6226 1752
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>
>
> --
> Roger Bivand
> NHH, Breiviksveien 40, N-5045 Bergen, Norway
>
>


-- 
Roger Bivand
NHH, Breiviksveien 40, N-5045 Bergen, Norway

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri Aug 20 09:35:19 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 20 Aug 2004 09:35:19 +0200
Subject: [R] Is there an alternative to subplot() of splus
In-Reply-To: <000301c48671$55e0aa70$f428db89@elf.jcu.edu.au>
References: <000301c48671$55e0aa70$f428db89@elf.jcu.edu.au>
Message-ID: <4125A9B7.8030804@statistik.uni-dortmund.de>

Carla Chen wrote:

> Dear R-gurus,
> 
> Is there any function in R does the same thing as function subplot()in
> Splus?
> 
> I am trying to post a pie chart onto the top right hand corner of
> existing bar chart.
> 
> Also, Does anyone know if I can use paste() to paste the graph to
> another graph?
> 
> Cheers,
> 
> Carla Chen

How to do it is explained in the article

@Article{Rnews:Murrell:2003,
  author  = {Paul Murrell},
  title	 = {Integrating grid Graphics Output with Base Graphics Output},
  journal = {R News},
  year	 = 2003,
  volume  = 3,
  number  = 2,
  pages	 = {7--12},
  month	 = {October},
  url	 = {http://CRAN.R-project.org/doc/Rnews/}
}

See also Paul's slides from the useR!2004 meeting.

Uwe Ligges



> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri Aug 20 09:36:37 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 20 Aug 2004 09:36:37 +0200
Subject: [R] Instrumental variables
In-Reply-To: <a2af9f04.72a72f80.8233d00@expms2.cites.uiuc.edu>
References: <a2af9f04.72a72f80.8233d00@expms2.cites.uiuc.edu>
Message-ID: <4125AA05.5060709@statistik.uni-dortmund.de>

anafava at uiuc.edu wrote:

> May someone help me to find out how to use intruments in R?
> 
> Thanks
> Ana
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Please do what the sentence above tells to do. In particular, please 
read the manual "An Introduction to R"!

Uwe Ligges



From ligges at statistik.uni-dortmund.de  Fri Aug 20 09:50:36 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 20 Aug 2004 09:50:36 +0200
Subject: [R] List dimention labels to plots of components
In-Reply-To: <12D0D00E1404D511A4820090274CA09C03FBA749@dasmtyjqf010.amedd.army.mil>
References: <12D0D00E1404D511A4820090274CA09C03FBA749@dasmtyjqf010.amedd.army.mil>
Message-ID: <4125AD4C.4050509@statistik.uni-dortmund.de>

White, Charles E WRAIR-Wash DC wrote:

> It is frustrating to see the labels I want in the dimensions of a list but not be able to extract those labels into titles for plots generated from component objects. If someone could set me straight, I would appreciate it. For your amusement, I have provided an example of the Byzantine code I am currently using to avoid loops:

The code below works perfectly. I don't get the point what you are 
looking for ....

Uwe Ligges


> # Simulate ANOVA type test data
> sex<-c(rep(1,8),rep(0,8))
> dose<-c(rep(1,4),rep(0,4))
> treatment<-c(rep(1,2),rep(0,2))
> fix<-sex+dose+treatment
> Response<-fix+rnorm(16)
> Sex<-rep("Male",16)
> Sex[sex==0]<-"Female"
> Dose<-rep("High",16)
> Dose[dose==0]<-"Low"
> Treatment<-rep("A",16)
> Treatment[treatment==0]<-"B"
> dat<-data.frame(Sex,Dose,Treatment,fix,Response)
> 
> # Redundant Transfer and Execution of Dimension ID?
> mymod<-function(x){
>   model<-lm(Response~Sex,data=x)
>   list(model,Dose=x$Dose[1],Treatment=x$Treatment[1])}
> myplt<-function(x){
>   plot(x[[1]],main=paste(x$Dose,"/",x$Treatment,sep=""))}
> 
> # Generate list of Model Estimates 
> dat.lm<-by(dat,list(Dose=dat$Dose,Treatment=dat$Treatment),mymod)
> 
> # Execute plots with labels
> pdf(file="junk.pdf",height=7.5,width=10)
> par(mfrow=c(2,2))
> bitbucket<-lapply(dat.lm,myplt)
> dev.off()
> 
> Charles E. White, Senior Biostatistician, MS
> Walter Reed Army Institute of Research
> 503 Robert Grant Ave., Room 1w102
> Silver Spring, MD 20910-1557
> 301 319-9781
> Personal/Professional Site: http://users.starpower.net/cwhite571/professional/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Fri Aug 20 10:22:29 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Aug 2004 10:22:29 +0200
Subject: [R] Instrumental variables
In-Reply-To: <4125AA05.5060709@statistik.uni-dortmund.de>
References: <a2af9f04.72a72f80.8233d00@expms2.cites.uiuc.edu>
	<4125AA05.5060709@statistik.uni-dortmund.de>
Message-ID: <x24qmynrqy.fsf@biostat.ku.dk>

Uwe Ligges <ligges at statistik.uni-dortmund.de> writes:

> anafava at uiuc.edu wrote:
> 
> > May someone help me to find out how to use intruments in R?
> > Thanks
> > Ana
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> Please do what the sentence above tells to do. In particular, please
> read the manual "An Introduction to R"!

And in particular, make sure that your query is unambiguous. Was that
instrumental variables as in errors-in-variables theory, or as in
blood pressure recording apparatus?

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ahenningsen at email.uni-kiel.de  Fri Aug 20 10:48:58 2004
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Fri, 20 Aug 2004 10:48:58 +0200
Subject: [R] Instrumental variables
In-Reply-To: <a2af9f04.72a72f80.8233d00@expms2.cites.uiuc.edu>
References: <a2af9f04.72a72f80.8233d00@expms2.cites.uiuc.edu>
Message-ID: <200408201048.58636.ahenningsen@email.uni-kiel.de>

The package systemfit offers instrumental variables (IV) estimation (2SLS + 
3SLS). Though this package is built to estimate equation systems, people told 
me that they use it also for single equation IV estimations (using an 
equation "system" that contains only one equation).

Arne

On Friday 20 August 2004 06:53, anafava at uiuc.edu wrote:
> May someone help me to find out how to use intruments in R?
>
> Thanks
> Ana
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From B.Rowlingson at lancaster.ac.uk  Fri Aug 20 10:53:27 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 20 Aug 2004 09:53:27 +0100
Subject: [R] rgdal under windows?
In-Reply-To: <Pine.LNX.4.44.0408191957500.3940-100000@reclus.nhh.no>
References: <Pine.LNX.4.44.0408191957500.3940-100000@reclus.nhh.no>
Message-ID: <4125BC07.2010603@lancaster.ac.uk>

Roger Bivand wrote:

>>>Has anyone had any joy getting the rgdal package to compile under windows?

> Exactly. The closest anyone has got so far is Hisaji Ono, who used MSYS 
> (http://www.mingw.org/) to build PROJ.4 and GDAL (GDAL depends on PROJ.4, 
> PROJ.4 needs a PATH to metadata files for projection and transformation), 
> and then hand-pasted the paths to the GDAL headers and library into 
> src/Makevars, running Rcmd INSTALL rgdal at the Windows command prompt as 
> usual. All of this can be repeated, but is not portable, and does not suit 
> the very valuable standard binary package build system for Windows. 
> Roughly:

> [points 1 to 5 etc omitted]

  At some point the complexity of installing things like this for 
Windows will cross the complexity of installing Linux...

Baz

PS excepting live-Linux installs like Knoppix.



From joehl at gmx.de  Fri Aug 20 10:55:30 2004
From: joehl at gmx.de (=?ISO-8859-1?Q?=22Jens_Oehlschl=E4gel=22?=)
Date: Fri, 20 Aug 2004 10:55:30 +0200 (MEST)
Subject: [R] Survey analysis of repeated relationships?
Message-ID: <29370.1092992130@www21.gmx.net>


I just discovered the great piece of software that is available with the
survey package. Many thanks and 'Hats off' to Thomas Lumley.

While package survey covers analysis of features of objects sampled (in
clusters, strata) I could not find analysis of features of repeated
relationsships between sampled objects (in clusters, strata). My
understanding is that it is not adequate to treat relationships as sampled
by themselves, because correlations between relationships introduced by
repeated involvement of the same objects would underestimate variability.

Can anyone point me to software/literature/people dealing with estimating
variance / estimating sample size of such surveys?

Best regards


Jens


P.S. two examples follow

Professional Talks
==================
Strata:                  Professions
Sampled Objects:         Professionals
Repeated Relationships:  Make some of them talk to each other such that 
                         one Professional is involved in several talks 
                         (to different other professionals and possibly 
                         several times to the same)
Features:                Binary evaluation of talk
Analysis target:         Fraction of 'good' talks 
                         - for each combination of professions
                         - in overall population


TCP Network Time
================
Strata:                  Geographic regions
Sampled Clusters:        Towns
Sampled Objects:         Routers
Features:                package travel times (or hop numbers)
Analysis target:         Average travel times
                         - for each combination of towns
                         - for each combination of geographic regions
                         - in the overall population


-- 
NEU: Bis zu 10 GB Speicher fr e-mails & Dateien!
1 GB bereits bei GMX FreeMail http://www.gmx.net/de/go/mail



From ahenningsen at email.uni-kiel.de  Fri Aug 20 11:04:42 2004
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Fri, 20 Aug 2004 11:04:42 +0200
Subject: [R]  IDE or an Editor for R
In-Reply-To: <20040819181349.67668.qmail@web50005.mail.yahoo.com>
References: <20040819181349.67668.qmail@web50005.mail.yahoo.com>
Message-ID: <200408201104.42730.ahenningsen@email.uni-kiel.de>

Hi SP,

I prefer to use kate (KDE advanced text editor), because it is easier to learn 
than emacs or vim and IMHO it is also very convenient to use. I have the kate 
window split into 3 parts: a) the file selector, b) the editor window, and c) 
a command line with "R". I can either source() whole files or select some 
lines in the editor and execute them by a middle-click in the command line 
window. You can download the latest version of the syntax highlighting 
definitions for R from my homepage (scroll to the very bottom): 
http://www.uni-kiel.de/agrarpol/ahenningsen/index-e.html

All the best,
Arne

On Thursday 19 August 2004 20:13, S Peri wrote:
> Hi,
>  Is there any IDE or any editor or any pice of code
> for .vimrc is available?
>
> Please let me know.
>
> Thanks
> SP
>
>
>
> _______________________________
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From pburns at pburns.seanet.com  Fri Aug 20 11:38:14 2004
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Fri, 20 Aug 2004 10:38:14 +0100
Subject: [R] Is R good for not-professional-statistician, un-mathematical
	clinical researchers?
In-Reply-To: <20040820025958.29090.qmail@web80108.mail.yahoo.com>
References: <20040820025958.29090.qmail@web80108.mail.yahoo.com>
Message-ID: <4125C686.4000304@pburns.seanet.com>

I definitely agree that focussing on what R does better than
other options is the right approach.  One thing that Tomas
does not mention is graphics.  Two possible selling points along
this line are:

*)  R is good for understanding your data with graphics.
*)  R is good for producing graphics for publication.

This discussion reminds me of something my wife says: classes
are for learning what people don't want to do -- if someone is
happy to do something, they don't need a class to force them to
do it.


Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Tomas Aragon wrote:

>I am a clinician turned epidemiologist. I just taught R in an intro epi
>course. Here are some tips:
>
>- encourage them to use R as their calculator
>- encourage them to use R as their spreadsheet
>- provide them with exercises to work this functionality
>- we started a Yahoo help group for beginner questions (see
>http://www.ucbcidp.org/epitools.html)
>
>I have some exercises at http://www.medepi.net/epitools/lab/
>
>If you only show how to do things that can be done is any statistical
>package, then they'll choose the "user-friendly" statistical package
>(e.g., stata).
>
>For any additional question, please contact me.
>Good luck!
>
>Tomas Aragon
>http://www.idready.org
>
> 
>--- Jacob Wegelin <jawegelin at ucdavis.edu> wrote:
>
>  
>
>>Alternate title: How can I persuade my students that R is for them?
>>
>>Alternate title: Can R replace SAS, SPSS or Stata for clinicians?
>>
>>I am teaching introductory statistics to twelve physicians and two
>>veterinarians
>>who have enrolled in a Mentored Clinical Research Training Program. 
>>My course is the
>>first in a sequence of three.  We (the instructors of this sequence)
>>chose to teach
>>R rather than some other computing environment.
>>
>>My (highly motivated) students have never encountered anything like
>>R.  One frankly
>>asked:
>>
>>"Do you feel (honestly) that a group of physicians (with two vets)
>>clinicians will
>>be able to effectively use and actually understand R? If so, I will
>>happily call this
>>bookstore and order this book [Venables and Ripley] tomorrow."
>>
>>I am heavily biased toward R/S because I have used it since the first
>>applied statistics
>>course I took.  But I would love to give these students some kind of
>>objective information
>>about the usability of R by non-statisticians--not just my own bias.
>>
>>Could anyone suggest any such information?  Or does anyone on this
>>list use R who is
>>a clinician and not really mathematically savvy?  For instance,
>>someone who doesn't
>>remember any math beyond algebra and doesn't think in terms of
>>P(A|B)?
>>
>>Or have we done a disservice to our students by choosing to make them
>>learn R, rather than making ourselves learn SAS, Stata or SPSS?
>>
>>Thank you for any ideas
>>
>>Jake Wegelin
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>>http://www.R-project.org/posting-guide.html
>>
>>    
>>
>
>
>=====
>Tomas Aragon, MD, DrPH, Director
>Center for Infectious Disease Preparedness
>UC Berkeley School of Public Health
>1918 University Ave., 4th Fl., MC-7350
>Berkeley, CA 94720-7350
>http://www.idready.org
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>



From helga.neidlinger at zkrd.de  Fri Aug 20 11:39:46 2004
From: helga.neidlinger at zkrd.de (Helga Neidlinger)
Date: Fri, 20 Aug 2004 11:39:46 +0200
Subject: [R] Position of corporate logo beneath diagram
Message-ID: <4125E302.1509.EDDAFD@localhost>

Hi,

I'd like to position our corporate logo (gif) beneath a graph. Is 
there a function to position images?

Thanks,
Helga Neidlinger-- 
Helga Neidlinger              Zentrales Knochenmarkspender-Register
Assistenz Gesch??ftsf??hrung    fuer die Bundesrepublik Deutschland
                              ZKRD gGmbH, Postfach 4244,    89032 Ulm
Tel.: (0731) 1507-12                      Helmholtzstr. 10, 89081 Ulm
Fax : (0731) 1507-51          http://www.zkrd.de



From ripley at stats.ox.ac.uk  Fri Aug 20 12:32:21 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 20 Aug 2004 11:32:21 +0100 (BST)
Subject: [R] Position of corporate logo beneath diagram
In-Reply-To: <4125E302.1509.EDDAFD@localhost>
Message-ID: <Pine.LNX.4.44.0408201101140.24398-100000@gannet.stats>

Look in package pixmap which has

addlogo-methods         Methods for Adding a Pixmap Logo to a Plot

Not that it will accept gif, but you can surely convert that format
by e.g. ImageMagick or PhotoShop.

On Fri, 20 Aug 2004, Helga Neidlinger wrote:

> I'd like to position our corporate logo (gif) beneath a graph. Is 
> there a function to position images?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Luisr at frs.fo  Fri Aug 20 12:47:17 2004
From: Luisr at frs.fo (Luis Rideau Cruz)
Date: Fri, 20 Aug 2004 11:47:17 +0100
Subject: [R] removing last element from a character string
Message-ID: <s125e4d0.025@ffdata.setur.fo>

R-help,

Any function to remove last element (or any specific elemnet in the sequence) from a character string?

t<-c("aaab","qqqc")

>function(t)  ### remove last character (b and c respectively)

> aaa   qqq

Thanks

`??._ .??  `??. _ .??  `??._ .??  `??. _ .??  `??._ .??  `??. _ .??  `??..
Luis Ridao Cruz
Fiskiranns??knarstovan
N??at??n 1
P.O. Box 3051
FR-110 T??rshavn
Faroe Islands
Phone:             +298 353900
Phone(direct): +298 353912
Mobile:             +298 580800
Fax:                 +298 353901
E-mail:              luisr at frs.fo
Web:                www.frs.fo

`??._ .??  `??. _ .??  `??._ .??  `??. _ .??  `??._ .??  `??. _ .??  `??..



From ligges at statistik.uni-dortmund.de  Fri Aug 20 12:56:28 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 20 Aug 2004 12:56:28 +0200
Subject: [R] removing last element from a character string
In-Reply-To: <s125e4d0.025@ffdata.setur.fo>
References: <s125e4d0.025@ffdata.setur.fo>
Message-ID: <4125D8DC.8030803@statistik.uni-dortmund.de>

Luis Rideau Cruz wrote:
> R-help,
> 
> Any function to remove last element (or any specific elemnet in the sequence) from a character string?
> 
> t<-c("aaab","qqqc")
> 
> 
>>function(t)  ### remove last character (b and c respectively)
> 
> 
>>aaa   qqq
> 


   substr(t, 1, nchar(t)-1)

Uwe Ligges


> Thanks
> 
> `??._ .??  `??. _ .??  `??._ .??  `??. _ .??  `??._ .??  `??. _ .??  `??..
> Luis Ridao Cruz
> Fiskiranns??knarstovan
> N??at??n 1
> P.O. Box 3051
> FR-110 T??rshavn
> Faroe Islands
> Phone:             +298 353900
> Phone(direct): +298 353912
> Mobile:             +298 580800
> Fax:                 +298 353901
> E-mail:              luisr at frs.fo
> Web:                www.frs.fo
> 
> `??._ .??  `??. _ .??  `??._ .??  `??. _ .??  `??._ .??  `??. _ .??  `??..
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Ted.Harding at nessie.mcc.ac.uk  Fri Aug 20 12:50:11 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 20 Aug 2004 11:50:11 +0100 (BST)
Subject: [R] Coagulation data by Box/Hunter/Hunter
In-Reply-To: <41251844.2050305@sdsc.edu>
Message-ID: <XFMail.040820115011.Ted.Harding@nessie.mcc.ac.uk>

On 19-Aug-04 T. Murlidharan Nair wrote:
> Hi!!
> Has anyone used the coagulation data for statistical analysis?
> I managed to get the data from the web but unsure of the way its
> supposed to read. I am new to R so trying to gets myself familiarized
> with the statistical tools using available data. I am appending the
> data I got of the web. If anyone is aware of its use I would welcome
> their input.
> Cheers always!!
> Murli
> 
> =======================================================================
[Original dataset omitted]

Hi Murli,

There are several approaches to getting started with this sort of
situation in R. The approach I usually adopt is generally as follows.

First, copy the original data to a file, say "blood.csv", and then
edit this file to remove all except the line which lists the variables
and the lines of data themselves; then edit these lines to replace
the spaces separating data items on a line with a single comma.
In this way you obtain

Y,X1,RUNSEQ
62,1,20
60,1,2
63,1,11
59,1,10
63,2,12
67,2,9
71,2,15
64,2,14
65,2,4
66,2,8
68,3,16
66,3,7
71,3,1
67,3,17
68,3,13
68,3,21
56,4,23
62,4,3
60,4,6
61,4,18
63,4,22
64,4,19
63,4,5
59,4,24

Then start R, and enter commands like

D <- read.csv("blood.csv")
Y <- D$Y; X1 <- D$X1; RUNSEQ <- D$RUNSEQ

Now R has all the data, both combined into a dataframe D and
also as separate variables. From this point on, you can do
what you like.

Some examples:

  plot(X1,Y)

  ix<-(X1==1);  plot(RUNSEQ[ix],Y[ix],pch=1,xlim=c(0,25),ylim=c(40,80))
  ix<-(X1==2);points(RUNSEQ[ix],Y[ix],pch=2,col="red")
  ix<-(X1==3);points(RUNSEQ[ix],Y[ix],pch=3,col="green")
  ix<-(X1==4);points(RUNSEQ[ix],Y[ix],pch=4,col="blue")

  X1.f <- factor(X1); D.lm <- lm(Y ~ X1.f)
  summary(D.lm)

  plot(RUNSEQ,D.lm$res)

and so on.

Good hunting!
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 20-Aug-04                                       Time: 11:50:11
------------------------------ XFMail ------------------------------



From partha_bagchi at HGSI.COM  Fri Aug 20 13:52:11 2004
From: partha_bagchi at HGSI.COM (partha_bagchi@HGSI.COM)
Date: Fri, 20 Aug 2004 07:52:11 -0400
Subject: [R] legends on the outside of the "box"
Message-ID: <OFCD487DF7.F90E0401-ON85256EF6.00412000-85256EF6.00413463@hgsi.com>

How about:

par(omi = c(1, 1, 1, 1), xpd = NA)
x1 <- 1:10
x2 <- sqrt(x1)
plot(x1)
points(x2,col="red",pch=3)
legend(par("usr")[1],par("usr")[3] - 
1.5,legend=c("First","Second"),col=c("black","red"),pch=c(1,3))

?

Note that you will have to play with the -1.5 to get it right in general.

Partha





"Laura Holt" <lauraholt_983 at hotmail.com>
Sent by: r-help-bounces at stat.math.ethz.ch
08/19/2004 10:53 PM

 
        To:     r-help at stat.math.ethz.ch
        cc: 
        Subject:        [R] legends on the outside of the "box"


Hi R People:

Here is a simple set of commands:

>x1 <- 1:10
>x2 <- sqrt(x1)
>plot(x1)
>points(x2,col="red",pch=3)
>legend(2,8,legend=c("First","Second"),col=c("black","red"),pch=c(1,3))
>
Fine.
Now, I would like to put the legend box on the outside of the plot itself,
perhaps in the lower
left hand corner.

I've been messing with mtext, but to no avail.

Any help would be much appreciated.

R Version 1.9.1 Windows
Thanks,
Laura Holt
mailto: lauraholt_983 at hotmail.com

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From partha_bagchi at hgsi.com  Fri Aug 20 13:55:57 2004
From: partha_bagchi at hgsi.com (partha_bagchi@hgsi.com)
Date: Fri, 20 Aug 2004 07:55:57 -0400
Subject: [R]  IDE or an Editor for R
Message-ID: <OF1C8DC22F.ABD554A8-ON85256EF6.00415D4C-85256EF6.00418C9B@hgsi.com>

A similar approach is available for windows user with the Crimson Editor - 
(http://www.crimsoneditor.com/)
 




Arne Henningsen <ahenningsen at email.uni-kiel.de>
Sent by: r-help-bounces at stat.math.ethz.ch
08/20/2004 05:04 AM

 
        To:     r-help at stat.math.ethz.ch
        cc: 
        Subject:        Re: [R]  IDE or an Editor for R


Hi SP,

I prefer to use kate (KDE advanced text editor), because it is easier to 
learn
than emacs or vim and IMHO it is also very convenient to use. I have the 
kate
window split into 3 parts: a) the file selector, b) the editor window, and 
c)
a command line with "R". I can either source() whole files or select some
lines in the editor and execute them by a middle-click in the command line
window. You can download the latest version of the syntax highlighting
definitions for R from my homepage (scroll to the very bottom):
http://www.uni-kiel.de/agrarpol/ahenningsen/index-e.html

All the best,
Arne

On Thursday 19 August 2004 20:13, S Peri wrote:
> Hi,
>  Is there any IDE or any editor or any pice of code
> for .vimrc is available?
>
> Please let me know.
>
> Thanks
> SP
>
>
>
> _______________________________
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From kjetil at acelerate.com  Fri Aug 20 14:12:01 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Fri, 20 Aug 2004 08:12:01 -0400
Subject: [R] Principal surfaces in R
Message-ID: <4125EA91.4070703@acelerate.com>

Hola!

Anybody knows about principal surfaces in R? I found
pcurve and princurve, but both seem to have only
principal curves.

Kjetil halvorsen



From maechler at stat.math.ethz.ch  Fri Aug 20 14:20:43 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 20 Aug 2004 14:20:43 +0200
Subject: [R] do not reply for a new topic!!!
In-Reply-To: <20040819162041.79746.qmail@web50002.mail.yahoo.com>
References: <20040819162041.79746.qmail@web50002.mail.yahoo.com>
Message-ID: <16677.60571.622859.50711@gargle.gargle.HOWL>

Dear S,

when posting something to R-help or bioconductor,
please do *NOT* reply to an existing message and add a new
subject!!!  {it breaks threads for most e-mail system that use
	     threads, including the threaded archives, see, e.g.
   https://stat.ethz.ch/pipermail/bioconductor/2004-August/thread.html
   (and look for "S Peri").

This is clearly said in the posting guide, see below

>>>>> "S" == S Peri <biocperi at yahoo.com>
>>>>>     on Thu, 19 Aug 2004 09:20:41 -0700 (PDT) writes:

    S> Dear group, 
    
  <..........>

    S> ______________________________________________
    S> R-help at stat.math.ethz.ch mailing list
    S> https://stat.ethz.ch/mailman/listinfo/r-help
    S> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
      
PLEASE do read it!

Martin Maechler



From rpeng at jhsph.edu  Fri Aug 20 14:22:31 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Fri, 20 Aug 2004 08:22:31 -0400
Subject: [R] How do you read in a table with numeric and dates fields
	without it defaulting to reading in the dates as factors?
In-Reply-To: <3B5823541A25D311B3B90008C7F9056410E3578B@ntmsg0092.corpmail.telstra.com.au>
References: <3B5823541A25D311B3B90008C7F9056410E3578B@ntmsg0092.corpmail.telstra.com.au>
Message-ID: <4125ED07.4090806@jhsph.edu>

Does setting `as.is = TRUE' in read.table() do what you want?

-roger

Briggs, Meredith M wrote:
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From maechler at stat.math.ethz.ch  Fri Aug 20 15:52:27 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 20 Aug 2004 15:52:27 +0200
Subject: [R] do not reply for a new topic!!!
In-Reply-To: <Pine.GSO.4.58.0408202236410.22318@yin>
References: <20040819162041.79746.qmail@web50002.mail.yahoo.com>
	<16677.60571.622859.50711@gargle.gargle.HOWL>
	<Pine.GSO.4.58.0408202236410.22318@yin>
Message-ID: <16678.539.461996.200360@gargle.gargle.HOWL>

Hi Kevin,
 {and R-help readers:  Kevin's (privatae) question is a quite
  relevant one to this sidetrack topic of "mailing list netiquette"}

>>>>> "Kevin" == Kevin Wang <Kevin.Wang at maths.anu.edu.au>
>>>>>     on Fri, 20 Aug 2004 22:37:39 +1000 (EST) writes:

    Kevin> Just out of interest, what about those posts with (no
    Kevin> subject) as the subject?  Sometimes I would change
    Kevin> the subject so it fits the body...is it recommended?

yes, I'd recommend it --- IFF --- your e-mail system is good
enough to set / augment the (usually invisible) headers
"References:" and "In-Reply-To:".

AFAIK, by official standards, these are used for threading
(if available) rather than the subjects.

Martin

    Kevin> On Fri, 20 Aug 2004, Martin Maechler wrote:

    >> Dear S,
    >> 
    >> when posting something to R-help or bioconductor,
    >> please do *NOT* reply to an existing message and add a new
    >> subject!!!  {it breaks threads for most e-mail system that use
    >> threads, including the threaded archives, see, e.g.
    >> https://stat.ethz.ch/pipermail/bioconductor/2004-August/thread.html



From buser at stat.math.ethz.ch  Fri Aug 20 16:00:29 2004
From: buser at stat.math.ethz.ch (Christoph Buser)
Date: Fri, 20 Aug 2004 16:00:29 +0200
Subject: [R] drop1 with contr.treatment
Message-ID: <16678.1021.376913.203251@gargle.gargle.HOWL>

Dear R Core Team

I've a proposal to improve drop1(). The function should change the
contrast from the default ("treatment") to "sum". If you fit a
model with an interaction (which ist not signifikant) and you
display the main effect with

drop1(   , scope = .~., test = "F")

If you remove the interaction, then everything's okay. There is
no way to fit a model, let an non signifikant interaction in
the model and get interpretable main effects with the
"treatment" contrast.

One solution would be to change automatically the contrast to
the "sum" contrast.
Another possibility is to produce a warning when you use drop1
with the scope argument to get main effects in the presence of 
an interaction (even not signifikant) with "treatment"
contrast. 

An example:

library(MASS)
##- Data "genotype"
names(genotype)
##- > [1] "Litter" "Mother" "Wt"

##- to be sure the contrasts are "treatment"
options(contrasts = c("contr.treatment", "contr.poly" ))

##- model with interaction
gen.int <- aov(Wt ~ Litter*Mother, data = genotype)
drop1(gen.int, scope = .~., test = "F")
##- Model:
##- Wt ~ Litter * Mother
##-               Df Sum of Sq    RSS    AIC F value   Pr(F)  
##- <none>                     2440.8  257.0                  
##- Litter         3     591.7 3032.5  264.3  3.6362 0.01968 *
##- Mother         3     582.3 3023.1  264.1  3.5782 0.02099 *
##- Litter:Mother  9     824.1 3264.9  256.8  1.6881 0.12005  
##- ---
##- Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 

##- the interaction is not signifikant, Litter is signifikant,
##- but is that really true?

##- main effect model
gen.main <- aov(Wt ~ Litter + Mother, data = genotype)
drop1(gen.main, scope = .~., test = "F")
##- Model:
##- Wt ~ Litter + Mother
##-        Df Sum of Sq    RSS    AIC F value   Pr(F)   
##- <none>              3264.9  256.8                   
##- Litter  3      63.6 3328.5  252.0  0.3508 0.78870   
##- Mother  3     775.1 4040.0  263.8  4.2732 0.00886 **
##- ---
##- Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 

## Dramatic change of the sum of squares for the factor Litter

## With contrast = "sum" the whole thing looks better
options(contrasts = c("contr.sum", "contr.poly" ))

##- model with interaction
gen.int1 <- aov(Wt ~ Litter*Mother, data = genotype)
drop1(gen.int1, scope = .~., test = "F")
##- Model:
##- Wt ~ Litter * Mother
##-               Df Sum of Sq    RSS    AIC F value   Pr(F)  
##- <none>                     2440.8  257.0                  
##- Litter         3      27.7 2468.5  251.7  0.1700 0.91612  
##- Mother         3     671.7 3112.6  265.9  4.1282 0.01142 *
##- Litter:Mother  9     824.1 3264.9  256.8  1.6881 0.12005  
##- ---
##- Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 

##- the interaction is not signifikant, Litter is NOT! signifikant.

gen.main1 <- aov(Wt ~ Litter + Mother, data = genotype)
drop1(gen.main1, scope = .~., test = "F")
##- Wt ~ Litter + Mother
##-        Df Sum of Sq    RSS    AIC F value   Pr(F)   
##- <none>              3264.9  256.8                   
##- Litter  3      63.6 3328.5  252.0  0.3508 0.78870   
##- Mother  3     775.1 4040.0  263.8  4.2732 0.00886 **
##- ---
##- Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 

##- Litter stays not signifikant.


Best regards

Christoph Buser

-- 
Christoph Buser <buser at stat.math.ethz.ch>
Seminar fuer Statistik, LEO D6
ETH (Federal Inst. Technology)	8092 Zurich	 SWITZERLAND
phone: x-41-1-632-3505		fax: 632-1228
http://stat.ethz.ch/~buser/



From charles.edwin.white at us.army.mil  Fri Aug 20 15:52:14 2004
From: charles.edwin.white at us.army.mil (White, Charles E WRAIR-Wash DC)
Date: Fri, 20 Aug 2004 09:52:14 -0400
Subject: [R] List dimention labels to plots of components
Message-ID: <12D0D00E1404D511A4820090274CA09C03FBA74B@dasmtyjqf010.amedd.army.mil>

My goal is more efficient code for something I anticipate doing a lot.
My example code from my first message works because I insert a seemingly
redundant recording of Dose & Treatment in the list generated in the
"by" command. Since Dose & Treatment are already recorded in the
dimensions of the list, I would like to pass those dimensions into the
function executed by lapply. That may or may not be possible.

# Append the following code to my message of 8/19
# Dose & Treatment go to list without separate user code to insert them
dat.lm2<-by(dat,list(Dose=dat$Dose,Treatment=dat$Treatment),
            function(x) lm(Response~Sex,data=x))
dimnames(dat.lm2)
dat.lm2["Low","B"]

# lapply uses dimnames to select objects but does not appear to pass 
# dimnames with object
lapply(dat.lm2,dimnames)
lapply(dat.lm2,names)
lapply(dat.lm2,function(x) print(x)[1:13])

Thanks for your help. 

Chuck



From ligges at statistik.uni-dortmund.de  Fri Aug 20 16:27:46 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 20 Aug 2004 16:27:46 +0200
Subject: [R] List dimention labels to plots of components
In-Reply-To: <12D0D00E1404D511A4820090274CA09C03FBA74B@dasmtyjqf010.amedd.army.mil>
References: <12D0D00E1404D511A4820090274CA09C03FBA74B@dasmtyjqf010.amedd.army.mil>
Message-ID: <41260A62.2040206@statistik.uni-dortmund.de>

White, Charles E WRAIR-Wash DC wrote:

> My goal is more efficient code for something I anticipate doing a lot.
> My example code from my first message works because I insert a seemingly
> redundant recording of Dose & Treatment in the list generated in the
> "by" command. Since Dose & Treatment are already recorded in the
> dimensions of the list, I would like to pass those dimensions into the
> function executed by lapply. That may or may not be possible.
> 
> # Append the following code to my message of 8/19
> # Dose & Treatment go to list without separate user code to insert them
> dat.lm2<-by(dat,list(Dose=dat$Dose,Treatment=dat$Treatment),
>             function(x) lm(Response~Sex,data=x))
> dimnames(dat.lm2)
> dat.lm2["Low","B"]
> 
> # lapply uses dimnames to select objects but does not appear to pass 
> # dimnames with object
> lapply(dat.lm2,dimnames)
> lapply(dat.lm2,names)
> lapply(dat.lm2,function(x) print(x)[1:13])
> 
> Thanks for your help. 
> 
> Chuck 


Just one example:

mapply(function(x, y) plot(x[[1]], main = y),
        dat.lm2,
        outer(rownames(dat.lm2), colnames(dat.lm2), paste, sep="/"))


Uwe Ligges



From ramasamy at cancer.org.uk  Fri Aug 20 16:33:57 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Fri, 20 Aug 2004 15:33:57 +0100
Subject: [R] do not reply for a new topic!!!
In-Reply-To: <16678.539.461996.200360@gargle.gargle.HOWL>
References: <20040819162041.79746.qmail@web50002.mail.yahoo.com>
	<16677.60571.622859.50711@gargle.gargle.HOWL>
	<Pine.GSO.4.58.0408202236410.22318@yin>
	<16678.539.461996.200360@gargle.gargle.HOWL>
Message-ID: <1093012436.4081.33.camel@vpn202001.lif.icnet.uk>

How about those e-mail which specify the problem in the subject line
only or does not use a subject line at all.

Is it practicable and desirable to simply bounce back an e-mail to the
sender with a message to read the posting guide if either the subject
line is empty or body is empty. Or this simply rude ?



On Fri, 2004-08-20 at 14:52, Martin Maechler wrote:
> Hi Kevin,
>  {and R-help readers:  Kevin's (privatae) question is a quite
>   relevant one to this sidetrack topic of "mailing list netiquette"}
> 
> >>>>> "Kevin" == Kevin Wang <Kevin.Wang at maths.anu.edu.au>
> >>>>>     on Fri, 20 Aug 2004 22:37:39 +1000 (EST) writes:
> 
>     Kevin> Just out of interest, what about those posts with (no
>     Kevin> subject) as the subject?  Sometimes I would change
>     Kevin> the subject so it fits the body...is it recommended?
> 
> yes, I'd recommend it --- IFF --- your e-mail system is good
> enough to set / augment the (usually invisible) headers
> "References:" and "In-Reply-To:".
> 
> AFAIK, by official standards, these are used for threading
> (if available) rather than the subjects.
> 
> Martin
> 
>     Kevin> On Fri, 20 Aug 2004, Martin Maechler wrote:
> 
>     >> Dear S,
>     >> 
>     >> when posting something to R-help or bioconductor,
>     >> please do *NOT* reply to an existing message and add a new
>     >> subject!!!  {it breaks threads for most e-mail system that use
>     >> threads, including the threaded archives, see, e.g.
>     >> https://stat.ethz.ch/pipermail/bioconductor/2004-August/thread.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Fri Aug 20 16:41:06 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 20 Aug 2004 15:41:06 +0100 (BST)
Subject: [R] drop1 with contr.treatment
In-Reply-To: <16678.1021.376913.203251@gargle.gargle.HOWL>
Message-ID: <Pine.LNX.4.44.0408201536470.14043-100000@gannet.stats>

Since this was addressed to R-Core (please do read the posting guide!) 
discussion will happen elsewhere.

R-help is not the right place for suggestions of changes to R: R-devel may
be.  As with bugs, please ensure you understand why the existing
behaviour is not both correct and intentional.  In a case like this, see
what the White Book says it does (as referred to on the help page) and
check it against S if you can.

On Fri, 20 Aug 2004, Christoph Buser wrote:

> Dear R Core Team
> 
> I've a proposal to improve drop1(). The function should change the
> contrast from the default ("treatment") to "sum". If you fit a
> model with an interaction (which ist not signifikant) and you
> display the main effect with
> 
> drop1(   , scope = .~., test = "F")
> 
> If you remove the interaction, then everything's okay. There is
> no way to fit a model, let an non signifikant interaction in
> the model and get interpretable main effects with the
> "treatment" contrast.
> 
> One solution would be to change automatically the contrast to
> the "sum" contrast.
> Another possibility is to produce a warning when you use drop1
> with the scope argument to get main effects in the presence of 
> an interaction (even not signifikant) with "treatment"
> contrast. 

[...]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dethlef at math.aau.dk  Fri Aug 20 14:20:25 2004
From: dethlef at math.aau.dk (Claus Dethlefsen)
Date: Fri, 20 Aug 2004 14:20:25 +0200
Subject: [R] [R-pkgs] Package "deal" version 1.2-17
In-Reply-To: <200408131021.i7DAK5cI027245@hypatia.math.ethz.ch>
Message-ID: <002a01c486b0$127e2700$7b360ad4@math.auc.dk>

A new version of the package "deal" is now available on CRAN. The package is
for learning (parameters and structure) of Bayesian networks and provide an
interface to Hugin. In the new version there is an interface to the package
dynamicGraph which allows for editing and callbacks of graphs in the
displayed window.

Try
> install.packages(c("dynamicGraph","deal"))
> library(deal)
> demo(reinis)

Best,
Claus

---------------------------------------------------------------------------
Assistant Prof. Claus Dethlefsen, PhD
Department of Mathematical Sciences, Aalborg University 
Fredrik Bajers Vej 7G, DK-9220 Aalborg, Denmark
dethlef at math.aau.dk; www.math.aau.dk/~dethlef
Ph: +45 9635 8878; Fax: +45 9815 8129
 

---

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From ivo_welch-rstat8783 at mailblocks.com  Fri Aug 20 17:24:52 2004
From: ivo_welch-rstat8783 at mailblocks.com (ivo_welch-rstat8783@mailblocks.com)
Date: Fri, 20 Aug 2004 08:24:52 -0700
Subject: [R] R on gentoo amd64 (gcc 3.3.3) is unstable
Message-ID: <200408201524.i7KFOrp9015320@hypatia.math.ethz.ch>


dear wizards:

  FYI: gentoo is a linux meta distribution, which compiles all packages. 
  Once running, gentoo is stable on most applications.  (it has some 
problems with system tools, such as grub.)  the compiler is gcc 3.3.3.  
I do not expect anyone to track down for me why R fails on the gentoo 
amd64+gcc3.3.3 system, but I thought that it would be good to put it on 
the record to save others some time.

The resulting from-source-compiled R is not stable in this combination. 
  R starts up ok, but a couple of commands (a simple linear regression) 
usually result in a Segfault.  this applies both to the R provided by 
gentoo (an old version of 1.9.0) or the latest version 1.9.1 which I 
just pulled down.  in case someone is wondering, the debugger backtrace 
provides

#0  0x000000000052ac16 in dnrm2_ ()
#1  0x00000000005185e8 in dqrdc2_ ()
#2  0x0000000000518db1 in dqrls_ ()
#3  0x000000000045d5d5 in do_dotCode ()
#4  0x0000000000472b6b in Rf_eval ()
...

(if any R wizard were to find it useful to try it out for himself, I 
could arrange for machine access, or provide other relevant 
information.  if anyone has compiled R 1.9.1 under amd64 with gcc 3.3.3 
and found it to be stable, please let me know, too.)

sincerely,

/ivo welch

---
ivo welch
professor of finance and economics
brown / nber / yale



From yuleih at umich.edu  Fri Aug 20 17:44:42 2004
From: yuleih at umich.edu (Yulei He)
Date: Fri, 20 Aug 2004 11:44:42 -0400 (EDT)
Subject: [R] how to read a series of data set
Message-ID: <Pine.SOL.4.58.0408201142001.26313@zektor.gpcc.itd.umich.edu>

Hi, there.

I want to input and output a bunch of data set. Suppose I want to read
data set m1.dat, m2.dat, m3.dat,... m100.dat and output c1.dat, c2.dat, ..
and c100.dat. Notice that the index of data set is from 1 to 100, How can
I put them into a loop? The code I am thinking is
that

for (i in 1:100)
{
 test=read.table(file="?");
 write.table(test, file="?");
}

Any help will be greatly appreicated.

Yulei



$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
Yulei He
1586 Murfin Ave. Apt 37
Ann Arbor, MI 48105-3135
yuleih at umich.edu
734-647-0305(H)
734-763-0421(O)
734-763-0427(O)
734-764-8263(fax)
$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$



From mmccall at mail.nih.gov  Fri Aug 20 18:06:55 2004
From: mmccall at mail.nih.gov (Matt McCall)
Date: Fri, 20 Aug 2004 12:06:55 -0400
Subject: [R] how to read a series of data set
In-Reply-To: <Pine.SOL.4.58.0408201142001.26313@zektor.gpcc.itd.umich.edu>
References: <Pine.SOL.4.58.0408201142001.26313@zektor.gpcc.itd.umich.edu>
Message-ID: <F53B93F3-F2C2-11D8-8884-000A95AFE190@mail.nih.gov>

try this maybe?

for (i in 1:100){
test <- read.table(file=paste("m",i,".dat", sep=""))
write.table(test, file=paste("c",i, ".dat", sep=""))
}

GO BLUE!
Matt



On Aug 20, 2004, at 11:44 AM, Yulei He wrote:

> Hi, there.
>
> I want to input and output a bunch of data set. Suppose I want to read
> data set m1.dat, m2.dat, m3.dat,... m100.dat and output c1.dat, 
> c2.dat, ..
> and c100.dat. Notice that the index of data set is from 1 to 100, How 
> can
> I put them into a loop? The code I am thinking is
> that
>
> for (i in 1:100)
> {
>  test=read.table(file="?");
>  write.table(test, file="?");
> }
>
> Any help will be greatly appreicated.
>
> Yulei
>
>
>
> $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
> Yulei He
> 1586 Murfin Ave. Apt 37
> Ann Arbor, MI 48105-3135
> yuleih at umich.edu
> 734-647-0305(H)
> 734-763-0421(O)
> 734-763-0427(O)
> 734-764-8263(fax)
> $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From tplate at blackmesacapital.com  Fri Aug 20 18:05:28 2004
From: tplate at blackmesacapital.com (Tony Plate)
Date: Fri, 20 Aug 2004 10:05:28 -0600
Subject: [R] Suggestion for posting guide
In-Reply-To: <loom.20040819T151603-570@post.gmane.org>
References: <loom.20040819T151603-570@post.gmane.org>
Message-ID: <6.1.0.6.2.20040820094859.06421ac0@mailhost.blackmesacapital.com>

When I originally compiled the posting guide many people felt that it 
should be kept as concise as possible, so that its length would not 
discourage people from reading it.  (It probably ended up too long 
anyway.)  So I wouldn't really recommend adding a section of this length 
too it.

That said, a question posted with a good example that can be cut and pasted 
directly into R is far easier to respond to, so it does seem like a good 
idea to help people create such things.  If someone (Gabor?) wanted to 
create a page on how to provide good examples in posts, the people who 
control what gets put on the R-project site might be willing to put it up 
there, and a link to it from the posting guide would seem like a good idea.

Tony Plate

At Thursday 07:17 AM 8/19/2004, Gabor Grothendieck wrote:

>I have a suggestion for the posting guide.  One problem with some posts is 
>that
>they do not provide an example that can be reproduced.   I think that many
>people just do not know how to easily specify some data and some technical
>assistance should be provided in the posting guide.  If the problem
>depends on specific data they should be made aware, in the posting guide, of:
>
>    dput(x)
>
>since that outputs object x as R code which can then be easily copied from the
>post and pasted into a session.  If its not dependent on particular data they
>can generate patterned or random data IF THEY KNOW HOW but many might find it
>easier to just use one of the included datasets so some guidance should be
>provided on the contents of a few of them, e.g.
>
>R comes with built in data sets.  data() will list them, data(iris) will
>attach data set iris and ?iris, str(iris), summary(iris), head(iris)
>and dput(iris) will give more information on iris (after attaching it).
>The following are a few of the datasets that come with R:
>
>    iris - data frame with 4 numeric columns and one 3 level factor
>    nhtemp - a ts class time series
>    faithful - data frame with two numeric columns
>    warpbreaks - data with a numeric column, a 2-level factor & a 3-level 
> factor
>
>Also letters, LETTERS, month.abb and month.name are built in character vectors
>that do not require a data statement to access.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jonathan_lees at unc.edu  Fri Aug 20 18:23:05 2004
From: jonathan_lees at unc.edu (Jonathan M. Lees)
Date: Fri, 20 Aug 2004 12:23:05 -0400
Subject: [R] problem with R start up MASS and family?
Message-ID: <41262569.80202@unc.edu>

Dear fellow R-users:

Is this a BUG?

R : Copyright 2004, The R Foundation for Statistical Computing
Version 1.9.1  (2004-06-21), ISBN 3-900051-00-3
 
R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.
 
R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R in publications.
 
Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for a HTML browser interface to help.
Type 'q()' to quit R.
 
Error: object 'family' not found whilst loading namespace 'MASS'
Fatal error: unable to restore saved data in .RData


it goes away if I move .RData to something else and then load it up again.
What gives?
 

 > R.Version()
$platform
[1] "i686-redhat-linux-gnu"
 
$arch
[1] "i686"
 
$os
[1] "linux-gnu"
 
$system
[1] "i686, linux-gnu"
 
$status
[1] ""
 
$major
[1] "1"
 
$minor
[1] "9.1"
 
$year
[1] "2004"
 
$month
[1] "06"
 
$day
[1] "21"
 
$language
[1] "R"
 
 >


-- 
==========================================
Prof. Jonathan M. Lees
Department of Geological Sciences
CB #3315, Mitchell Hall
University of North Carolina
Chapel Hill, NC  27599-3315
(919) 962-0695
FAX (919) 966-4519

jonathan_lees at unc.edu
http://www.unc.edu/~leesj



From lauraholt_983 at hotmail.com  Fri Aug 20 18:27:26 2004
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Fri, 20 Aug 2004 11:27:26 -0500
Subject: [R] apply on a data frame
Message-ID: <BAY12-F22ipZgCzfjmX0003c632@hotmail.com>

Hi R People:

There are 2 data sets of the iris data: one is iris3, which is a 3-d array, 
and the other is iris, which
is a data frame with 150 rows and 6 variables.

Getting means is straightforward from the 3-day iris3 set:
>apply(iris3,c(2,3),mean)
         Setosa Versicolor Virginica
Sepal L.  5.006      5.936     6.588
Sepal W.  3.428      2.770     2.974
Petal L.  1.462      4.260     5.552
Petal W.  0.246      1.326     2.026
>

Is there is similar way to obtain those values from the iris data frame, 
please?

thanks in advance
R Version 1.9.1 Windows.

Sincerely,
Laura Holt
mailto: lauraholt_983 at hotmail.com



From lauraholt_983 at hotmail.com  Fri Aug 20 18:30:50 2004
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Fri, 20 Aug 2004 11:30:50 -0500
Subject: [R] apply and data frames
Message-ID: <BAY12-F27ZM2F34Ywx4000c6013@hotmail.com>

Whoops!

Just found it:

>by(iris[,1:4],Species,mean)

Sorry for the inconvenience.

Laura.


Security. http://clinic.mcafee.com/clinic/ibuy/campaign.asp?cid=3963



From kjetil at acelerate.com  Fri Aug 20 18:33:55 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Fri, 20 Aug 2004 12:33:55 -0400
Subject: [R] Coagulation data by Box/Hunter/Hunter
In-Reply-To: <41251844.2050305@sdsc.edu>
References: <41251844.2050305@sdsc.edu>
Message-ID: <412627F3.90901@acelerate.com>

The coagulation dataset is from chapter 6 of Box, Hunter&Hunter, and is 
used to
introduce comparison of more than two treatments (ANOVA). It is supposedly
coagulation times of blood extracted from animals, after  randomizing 
the animals
to four groups and giving four different diets.

Some use in R:

coag <- matrix( scan(), 24, 3, byrow=TRUE)
   # cutting/pasting from your post into the prompt from scan()

colnames(coag) <- c("time","diet","order")
 > coag <- as.data.frame(coag)
 > coag$diet <- as.factor(coag$diet)
 > oneway.test(time ~  diet, data=coag, var.eq=TRUE)

        One-way analysis of means

data:  time and diet
F = 13.5714, num df = 3, denom df = 20, p-value = 4.658e-05

with(coag, stripchart(time ~ diet))

Kjetil Halvorsen



T. Murlidharan Nair wrote:

> Hi!!
> Has anyone used the coagulation data for statistical analysis ?  I 
> managed to
> get the data from the web but unsure of the way its supposed to read. 
> I am
> new to R so trying to gets myself familiarized with the statistical 
> tools using
> available data.  I am appending the data I got of the web. If anyone 
> is aware of
> its use I would welcome their input.
> Cheers always!!
> Murli
>
> =======================================================================
> THIS IS DATAPLOT DATA FILE   BOXBLOOD.DAT
> DIET EFFECT ON BLOOD COAGULATION
> BOX, HUNTER & HUNTER (1978)
> STATISTIC FOR EXPERIMENTERS
> WILEY, PAGE 165-197 (MAIN EXAMPLE OF CHAPTER 6)
> COMPLETELY RANDOMIZED DESIGN
> NUMBER OF OBSERVATIONS = 24
> TOTAL NUMBER OF VARIABLES PER LINE IMAGE = 3
>   RESPONSE VARIABLE = BLOOD COAGULATION TIME
>   FACTOR 1 = DIET (4 LEVELS)
>   FACTOR 2 = RUN SEQUENCE
> TO READ THIS FILE INTO DATAPLOT (AND ANALYZE)--
>   SKIP 25
>   READ BOXBLOOD.DAT Y X1 RUNSEQ
>   CHAR X ALL
>   PLOT Y X1 X1
>   .
>   ER; ANOVA Y X1
>   .
>   PLOT RES X1 X1
>   PLOT RES PRED PRED
>   PLOT RES RUNSEQ
>   NORMAL PROBABILITY PLOT RES
> Y   X1  RUNSEQ
> -----------------
> 62    1    20
> 60    1     2
> 63    1    11
> 59    1    10
> 63    2    12
> 67    2     9
> 71    2    15
> 64    2    14
> 65    2     4
> 66    2     8
> 68    3    16
> 66    3     7
> 71    3     1
> 67    3    17
> 68    3    13
> 68    3    21
> 56    4    23
> 62    4     3
> 60    4     6
> 61    4    18
> 63    4    22
> 64    4    19
> 63    4     5
> 59    4    24
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>



From rpeng at jhsph.edu  Fri Aug 20 18:59:32 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Fri, 20 Aug 2004 12:59:32 -0400
Subject: [R] problem with R start up MASS and family?
In-Reply-To: <41262569.80202@unc.edu>
References: <41262569.80202@unc.edu>
Message-ID: <41262DF4.4020809@jhsph.edu>

What version of the VR package are you using?  Do you see the problem 
after running update.packages()?

-roger

Jonathan M. Lees wrote:
> Dear fellow R-users:
> 
> Is this a BUG?
> 
> R : Copyright 2004, The R Foundation for Statistical Computing
> Version 1.9.1  (2004-06-21), ISBN 3-900051-00-3
> 
> R is free software and comes with ABSOLUTELY NO WARRANTY.
> You are welcome to redistribute it under certain conditions.
> Type 'license()' or 'licence()' for distribution details.
> 
> R is a collaborative project with many contributors.
> Type 'contributors()' for more information and
> 'citation()' on how to cite R in publications.
> 
> Type 'demo()' for some demos, 'help()' for on-line help, or
> 'help.start()' for a HTML browser interface to help.
> Type 'q()' to quit R.
> 
> Error: object 'family' not found whilst loading namespace 'MASS'
> Fatal error: unable to restore saved data in .RData
> 
> 
> it goes away if I move .RData to something else and then load it up again.
> What gives?
> 
> 
>  > R.Version()
> $platform
> [1] "i686-redhat-linux-gnu"
> 
> $arch
> [1] "i686"
> 
> $os
> [1] "linux-gnu"
> 
> $system
> [1] "i686, linux-gnu"
> 
> $status
> [1] ""
> 
> $major
> [1] "1"
> 
> $minor
> [1] "9.1"
> 
> $year
> [1] "2004"
> 
> $month
> [1] "06"
> 
> $day
> [1] "21"
> 
> $language
> [1] "R"
> 
>  >
> 
>



From tlumley at u.washington.edu  Fri Aug 20 18:59:41 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 20 Aug 2004 09:59:41 -0700 (PDT)
Subject: [R] probability histogram question
In-Reply-To: <412545BF.9010906@msu.edu>
References: <s125cf16.076@efs04.efs.mq.edu.au> <412545BF.9010906@msu.edu>
Message-ID: <Pine.A41.4.58.0408200959240.127282@homer06.u.washington.edu>

On Thu, 19 Aug 2004, Joseph LeBouton wrote:

> Alec,
>
> Thanks for your reply.  I guess what I'm getting at is that I to plot
> the histogram such that the HEIGHT of each bar represents the proportion
> of that class in the sample.  From your reply I gather that the AREA of
> each bar is currently representing the proportion.

That is a bar plot, not a histogram. The barplot() function draws bar
plots.

	-thomas

>
> My current work-around is to not plot the histogram immediately; I set
> it up (with plot=F), divide h$density by 10, then plot h;
>
> x <- runif(100,0,1)
> h <- hist(x, freq=F, plot=F)
> h$density <- h$density/10
> plot(h, freq=F)
>
> while this is up to my normal hacking modus operandi, it's not terribly
> efficient.  Is there another way to do that?  Or is what I'm trying to
> do a perceptually and/or statistically incorrect way to think about
> histograms?
>
> Thanks again,
>
> jlb
>
> Alec Stephenson wrote:
>
> >
> > Alec Stephenson
> > Department of Statistics
> > Macquarie University
> > NSW 2109, Australia
> >
> >
> >>>>Joseph LeBouton <lebouton at msu.edu> 08/20/04 09:56am >>>
> >
> > Hello, all;
> >
> > I get an unexpected result when trying to plot a probability histogram
> > with R1.9.1 on windows xp:
> >
> > #with the following code:
> >
> >
> >>x <- runif(100,0,1)
> >>hist(x)
> >>hist(x, freq=F)
> >>h <- hist(x, freq=F)
> >>summary(h)
> >
> >
> > #            Length Class  Mode
> > #breaks      11     -none- numeric
> > #counts      10     -none- numeric
> > #intensities 10     -none- numeric
> > #density     10     -none- numeric
> > #mids        10     -none- numeric
> > #xname        1     -none- character
> > #equidist     1     -none- logical
> >
> > # The help file says that <h$density>  holds the values
> > #	plotted in the probability histogram.  If that's the
> > #	case, I'd expect that the sum of h$density for a histogram
> > 	where freq=F would equal 1.0 ...  However:
> >
> >
> >>sum(h$density)
> >
> >
> > #returns the value :
> > #[1] 10
> >
> > I would really like to plot values in the probablility histogram that
> > sum to 1, not 10.  Is there a switch in the hist(x) command that I'm
> > missing?  Or, is there another function that can do this?
> >
> > The width of each bar is 0.1, so you need
> > sum(h$density) * 0.1
> > or in general
> > sum(h$density * diff(h$breaks))
> >
> > Thanks,
> > Alec
> >
> >
> >
> >
>
> --
> ************************************
> Joseph P. LeBouton
> Forest Ecology PhD Candidate
> Department of Forestry
> Michigan State University
> East Lansing, Michigan 48824
>
> Office phone: 517-355-7744
> email: lebouton at msu.edu
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From tlumley at u.washington.edu  Fri Aug 20 19:06:58 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 20 Aug 2004 10:06:58 -0700 (PDT)
Subject: [R] GEEs for time series data
In-Reply-To: <001001c48604$b4f7b3e0$0961268a@mapc0028>
References: <001001c48604$b4f7b3e0$0961268a@mapc0028>
Message-ID: <Pine.A41.4.58.0408200959510.127282@homer06.u.washington.edu>

On Thu, 19 Aug 2004, Duncan Lee wrote:

> I want to run a GEE for a time series of counts. The data are daily
> respiratory mortality counts  and so there aren't any 'clusters' in the
> longitudinal sense. Neither the gee or geese packages work. The gee one
> wont run at all and the geese one produces NaNs or just runs
> indefinitely depending on how long the time series is. Any ideas how to
> make these work of any other packages that might do the trick?
>


There are sandwich estimators for time series, but they are slightly more
complicated than for GEE, because there are no simple replicates.  The
"sandwich" package provides some of these.

One of these, the Newey-West estimator, is closely related to an ad hoc
estimator that lots of people have invented.  If you break your time
series into chunks that are long enough to be roughly independent you
could just use GEE.  The Newey-West estimator does this, but averages over
different places you could have put the break points (eg with month-long
chunks should they start at the 1st of the month, the 2nd, the 15th,...).
These estimators are also closely related to the block bootstrap and the
subsampling estimators for time series.  One place to find descriptions of
this is
  Lumley T, Heagerty PJ, (1999) "Weighted Empirical Adaptive Variance
  Estimators for Correlated Data Regression" Journal of the Royal
  Statistical Society, Series B.61:459-477

Econometricians will point out (with some justification) that they
invented all this stuff decades ago. On the other hand, AFAICS they
didn't point out to anyone that you could do this for generalized linear
models as well.


	-thomas



From tlumley at u.washington.edu  Fri Aug 20 19:28:45 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 20 Aug 2004 10:28:45 -0700 (PDT)
Subject: [R] Survey analysis of repeated relationships?
In-Reply-To: <29370.1092992130@www21.gmx.net>
References: <29370.1092992130@www21.gmx.net>
Message-ID: <Pine.A41.4.58.0408201012310.127282@homer06.u.washington.edu>

On Fri, 20 Aug 2004, [ISO-8859-1] "Jens Oehlschlgel" wrote:

>
> I just discovered the great piece of software that is available with the
> survey package. Many thanks and 'Hats off' to Thomas Lumley.
>
> While package survey covers analysis of features of objects sampled (in
> clusters, strata) I could not find analysis of features of repeated
> relationsships between sampled objects (in clusters, strata). My
> understanding is that it is not adequate to treat relationships as sampled
> by themselves, because correlations between relationships introduced by
> repeated involvement of the same objects would underestimate variability.

At first I thought you were just interested in longitudinal survey data,
which would be straightforward.  This doesn't look straightforward.

I don't know what standard methods there are in survey statistics for
this, but I have looked at this problem in non-survey contexts, and the
linearisation and reweighting arguments seem to transfer in most cases,
so this might be helpful if you don't get anything more focused.

The basic units of analysis are binary relationships between some sampled
individuals, and your data are measurements on these relationships, rather
than being the presence or absence of such relationships.  Now, if
individuals are independent then two relationships will be independent as
long as they share no individuals.   Arguments remarkably similar to those
for time series in my previous message this morning say that a "sandwich"
variance estimator can be obtained in three parts. Using your example of
professional talks

   variance using speaker as PSU + variance using listener as PSU -
variance using speaker:listener combination as PSU

This uses an estimate of variance based on summing individual cross
products of residuals over all pairs of observations not known to be
independent.  There is a good heuristic case for leaving out the third
term at least in situations where the speaker and listener factors are not
too far from orthogonal.

The sampling weights for a pair are needed. If individuals are sampled
independently the weight for a pair is the product of the individual
weights.


The sandwich estimator is simple, but even in model-based terms the theory
for this is not entirely straightforward.  Details are at
http://www.bepress.com/uwbiostat/paper207/

	-thomas

>
> Can anyone point me to software/literature/people dealing with estimating
> variance / estimating sample size of such surveys?
>
> Best regards
>
>
> Jens
>
>
> P.S. two examples follow
>
> Professional Talks
> ==================
> Strata:                  Professions
> Sampled Objects:         Professionals
> Repeated Relationships:  Make some of them talk to each other such that
>                          one Professional is involved in several talks
>                          (to different other professionals and possibly
>                          several times to the same)
> Features:                Binary evaluation of talk
> Analysis target:         Fraction of 'good' talks
>                          - for each combination of professions
>                          - in overall population
>
>
> TCP Network Time
> ================
> Strata:                  Geographic regions
> Sampled Clusters:        Towns
> Sampled Objects:         Routers
> Features:                package travel times (or hop numbers)
> Analysis target:         Average travel times
>                          - for each combination of towns
>                          - for each combination of geographic regions
>                          - in the overall population
>
>
> --
> NEU: Bis zu 10 GB Speicher fr e-mails & Dateien!
> 1 GB bereits bei GMX FreeMail http://www.gmx.net/de/go/mail
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From aragon at berkeley.edu  Fri Aug 20 19:29:37 2004
From: aragon at berkeley.edu (Tomas Aragon)
Date: Fri, 20 Aug 2004 10:29:37 -0700 (PDT)
Subject: [R] Is R good for not-professional-statistician,
	un-mathematical clinical researchers?
In-Reply-To: <4125C686.4000304@pburns.seanet.com>
Message-ID: <20040820172937.15698.qmail@web80103.mail.yahoo.com>

--- Patrick Burns <pburns at pburns.seanet.com> wrote:

> I definitely agree that focussing on what R does better than
> other options is the right approach.  One thing that Tomas
> does not mention is graphics.  Two possible selling points along
> this line are:
> 
> *)  R is good for understanding your data with graphics.
> *)  R is good for producing graphics for publication.
> 
> This discussion reminds me of something my wife says: classes
> are for learning what people don't want to do -- if someone is
> happy to do something, they don't need a class to force them to
> do it.
> 
> 
> Patrick Burns
> 
> Burns Statistics
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
> 
Thanks! I agree. I have a web page demonstrating plotting West Nile
virus infections in California at
http://www.medepi.net/data/wnv/index.html

Tomas

=====
Tomas Aragon, MD, DrPH, Director
Center for Infectious Disease Preparedness
UC Berkeley School of Public Health
1918 University Ave., 4th Fl., MC-7350
Berkeley, CA 94720-7350
http://www.idready.org



From maechler at stat.math.ethz.ch  Fri Aug 20 19:45:57 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 20 Aug 2004 19:45:57 +0200
Subject: [R] Suggestion for posting guide
In-Reply-To: <6.1.0.6.2.20040820094859.06421ac0@mailhost.blackmesacapital.com>
References: <loom.20040819T151603-570@post.gmane.org>
	<6.1.0.6.2.20040820094859.06421ac0@mailhost.blackmesacapital.com>
Message-ID: <16678.14549.68383.586906@gargle.gargle.HOWL>

>>>>> "Tony" == Tony Plate <tplate at blackmesacapital.com>
>>>>>     on Fri, 20 Aug 2004 10:05:28 -0600 writes:

    Tony> When I originally compiled the posting guide many
    Tony> people felt that it should be kept as concise as
    Tony> possible, so that its length would not discourage
    Tony> people from reading it.  (It probably ended up too
    Tony> long anyway.)  So I wouldn't really recommend adding a
    Tony> section of this length too it.

I agree

    Tony> That said, a question posted with a good example that
    Tony> can be cut and pasted directly into R is far easier to
    Tony> respond to, so it does seem like a good idea to help
    Tony> people create such things.  If someone (Gabor?) wanted
    Tony> to create a page on how to provide good examples in
    Tony> posts, the people who control what gets put on the
    Tony> R-project site might be willing to put it up there,
    Tony> and a link to it from the posting guide would seem
    Tony> like a good idea.

indeed!
    Tony> Tony Plate

read on :

    Tony> At Thursday 07:17 AM 8/19/2004, Gabor Grothendieck wrote:

    >> I have a suggestion for the posting guide.  One problem
    >> with some posts is that they do not provide an example
    >> that can be reproduced.  I think that many people just do
    >> not know how to easily specify some data and some
    >> technical assistance should be provided in the posting
    >> guide.  If the problem depends on specific data they
    >> should be made aware, in the posting guide, of:

    >> dput(x)

Rereading the posting guide (which *really* is rather too long
already for beginners), I see that we already have an 'Examples:' section.
[Have you seen that, Gabor, and not found useful enough?]

And just below that, we say

  PGuide> When providing examples, it is best to give an R command that
  PGuide> constructs the data, as in the matrix() expression above. For
  PGuide> more complicated data structures, dump("x", file=stdout())
  PGuide> will print an expression that will recreate the object x.

I tend to think that
	dump("x", file=stdout())
should probably be replaced with 
        dput(x)

even though they are not quite the same now (and even more
different in R-devel i.e. 2.0.0-to-be).

    >> since that outputs object x as R code which can then be
    >> easily copied from the post and pasted into a session.

(yes,  but that's already in there!).

    >> If its not dependent on particular data they can generate
    >> patterned or random data IF THEY KNOW HOW but many might
    >> find it easier to just use one of the included datasets
    >> so some guidance should be provided on the contents of a
    >> few of them, e.g.
    >> 
    >> R comes with built in data sets.  data() will list them, data(iris) will
    >> attach data set iris and ?iris, str(iris), summary(iris), head(iris)
    >> and dput(iris) will give more information on iris (after attaching it).
    >> The following are a few of the datasets that come with R:
    >> 
    >> iris - data frame with 4 numeric columns and one 3 level factor
    >> nhtemp - a ts class time series
    >> faithful - data frame with two numeric columns
    >> warpbreaks - data with a numeric column, a 2-level factor & a 3-level 
    >> factor
    >> 
    >> Also letters, LETTERS, month.abb and month.name are built
    >> in character vectors that do not require a data statement
    >> to access.

Thank you for the suggestions, Gabor.

If you have proposals for improvement, I'd mostly appreciate to
get a "diff -u" output from the current and your modified
posting-guide.html files -- 
probably only sent to Tony Plate and me (unless other people
chime in).

Martin



From gabriel.serendip at hipernet.com.br  Fri Aug 20 19:46:22 2004
From: gabriel.serendip at hipernet.com.br (Gabriel Erbano)
Date: Fri, 20 Aug 2004 14:46:22 -0300
Subject: [R] Mutual information
Message-ID: <BD4BBF3E.6CEC%gabriel.serendip@hipernet.com.br>

Greetings,

I wish to know if any R package contains a function to calculate the mutual
information statistic. I already searched the site, but came with my hands
empty. Can anybody help me here?

Thanks,

Gabriel Erbano



From minhan.science at gmail.com  Fri Aug 20 19:46:51 2004
From: minhan.science at gmail.com (Min-Han Tan)
Date: Fri, 20 Aug 2004 13:46:51 -0400
Subject: [R] Loss of rownames and colnames
Message-ID: <7902152a040820104675f54690@mail.gmail.com>

Hi,

I am working on some microarray data, and have some problems with
writing iterations.

In essence, the problem is that objects with three dimensions don't
have rownames and colnames. These colnames and rownames would
otherwise still be there in 2 dimensional objects.

I need to generate multiple iterations of a 2 means-clustering
algorithm, and these objects thus probably need 3 dimensions.

My scripts are all written with heavy references to matching of
colnames and rownames, so I am running into some problems here.
(colnames = sample ids, and rownames = gene ids)

My bad workaround solution so far has been to generate objects tagged
with ".2", and have multiple blocks of code.

e.g. 

test.1 <- ...

test.2 <- ...

test.x <- ..

The obvious problem with this solution is that there does not seem to
be an easy way of manipulating all these objects together without
typing out their names individually.

Thanks for any advice.

Regards,
Min-Han



From maechler at stat.math.ethz.ch  Fri Aug 20 19:52:50 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 20 Aug 2004 19:52:50 +0200
Subject: [R] do not reply for a new topic!!!
In-Reply-To: <1093012436.4081.33.camel@vpn202001.lif.icnet.uk>
References: <20040819162041.79746.qmail@web50002.mail.yahoo.com>
	<16677.60571.622859.50711@gargle.gargle.HOWL>
	<Pine.GSO.4.58.0408202236410.22318@yin>
	<16678.539.461996.200360@gargle.gargle.HOWL>
	<1093012436.4081.33.camel@vpn202001.lif.icnet.uk>
Message-ID: <16678.14962.981826.468108@gargle.gargle.HOWL>

>>>>> "Adaikalavan" == Adaikalavan Ramasamy <ramasamy at cancer.org.uk>
>>>>>     on Fri, 20 Aug 2004 15:33:57 +0100 writes:

    Adaikalavan> How about those e-mail which specify the
    Adaikalavan> problem in the subject line only or does not
    Adaikalavan> use a subject line at all.

    Adaikalavan> Is it practicable and desirable to simply
    Adaikalavan> bounce back an e-mail to the sender with a
    Adaikalavan> message to read the posting guide if either the
    Adaikalavan> subject line is empty or body is empty. Or this
    Adaikalavan> simply rude ?

This has crossed my mind more than once,
however (independent of etiquette/rudeness), 
we do stick to our policy as to not bounce back anything in
almost all cases :

If you start bouncing to another mail machine which likes to do
so as well, you quickly get into high-speed Ping-Pong ... ;-)

Martin

    Adaikalavan> On Fri, 2004-08-20 at 14:52, Martin Maechler wrote:
    >> Hi Kevin,
    >> {and R-help readers:  Kevin's (privatae) question is a quite
    >> relevant one to this sidetrack topic of "mailing list netiquette"}
    >> 
    >> >>>>> "Kevin" == Kevin Wang <Kevin.Wang at maths.anu.edu.au>
    >> >>>>>     on Fri, 20 Aug 2004 22:37:39 +1000 (EST) writes:
    >> 
    Kevin> Just out of interest, what about those posts with (no
    Kevin> subject) as the subject?  Sometimes I would change
    Kevin> the subject so it fits the body...is it recommended?
    >> 
    >> yes, I'd recommend it --- IFF --- your e-mail system is good
    >> enough to set / augment the (usually invisible) headers
    >> "References:" and "In-Reply-To:".
    >> 
    >> AFAIK, by official standards, these are used for threading
    >> (if available) rather than the subjects.
    >> 
    >> Martin
    >> 
    Kevin> On Fri, 20 Aug 2004, Martin Maechler wrote:
    >> 
    >> >> Dear S,
    >> >> 
    >> >> when posting something to R-help or bioconductor,
    >> >> please do *NOT* reply to an existing message and add a new
    >> >> subject!!!  {it breaks threads for most e-mail system that use
    >> >> threads, including the threaded archives, see, e.g.
    >> >> https://stat.ethz.ch/pipermail/bioconductor/2004-August/thread.html



From p.dalgaard at biostat.ku.dk  Fri Aug 20 20:05:11 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Aug 2004 20:05:11 +0200
Subject: [R] R on gentoo amd64 (gcc 3.3.3) is unstable
In-Reply-To: <200408201524.i7KFOrp9015320@hypatia.math.ethz.ch>
References: <200408201524.i7KFOrp9015320@hypatia.math.ethz.ch>
Message-ID: <x2u0uxfzxk.fsf@biostat.ku.dk>

<ivo_welch-rstat8783 at mailblocks.com> writes:

> dear wizards:
> 
>   FYI: gentoo is a linux meta distribution, which compiles all
> packages. Once running, gentoo is stable on most applications.  (it
> has some problems with system tools, such as grub.)  the compiler is
> gcc 3.3.3.  I do not expect anyone to track down for me why R fails on
> the gentoo amd64+gcc3.3.3 system, but I thought that it would be good
> to put it on the record to save others some time.
> 
> The resulting from-source-compiled R is not stable in this
> combination. R starts up ok, but a couple of commands (a simple linear
> regression) usually result in a Segfault.  this applies both to the R
> provided by gentoo (an old version of 1.9.0) or the latest version
> 1.9.1 which I just pulled down.  in case someone is wondering, the
> debugger backtrace provides
> 
> #0  0x000000000052ac16 in dnrm2_ ()
> #1  0x00000000005185e8 in dqrdc2_ ()
> #2  0x0000000000518db1 in dqrls_ ()
> #3  0x000000000045d5d5 in do_dotCode ()
> #4  0x0000000000472b6b in Rf_eval ()
> ...
> 
> (if any R wizard were to find it useful to try it out for himself, I
> could arrange for machine access, or provide other relevant
> information.  if anyone has compiled R 1.9.1 under amd64 with gcc
> 3.3.3 and found it to be stable, please let me know, too.)
> 

SuSE 9.1 appears quite happy.

> gcc -v
Reading specs from /usr/lib64/gcc-lib/x86_64-suse-linux/3.3.3/specs
Configured with: ../configure --enable-threads=posix --prefix=/usr
--with-local-prefix=/usr/local --infodir=/usr/share/info
--mandir=/usr/share/man --enable-languages=c,c++,f77,objc,java,ada
--disable-checking --libdir=/usr/lib64 --enable-libgcj
--with-gxx-include-dir=/usr/include/g++ --with-slibdir=/lib64
--with-system-zlib --enable-shared --enable-__cxa_atexit
x86_64-suse-linux
Thread model: posix
gcc version 3.3.3 (SuSE Linux)

I'd suspect that your blas libs are broken rather than R itself.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From sdavis2 at mail.nih.gov  Fri Aug 20 20:13:45 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Fri, 20 Aug 2004 14:13:45 -0400
Subject: [R] Loss of rownames and colnames
References: <7902152a040820104675f54690@mail.gmail.com>
Message-ID: <000c01c486e1$71abe1f0$04653744@WATSON>

Min-Han,

Have you considered using lists of matrices?  You can make a list of
matrices by:

mymatlist <- list()
for (i in 1:10) {a <- matrix(rnorm(100),nrow=10)
  mymatlist[[i]] <- a
}

Now, mymatlist[[1]] is the first matrix (with rownames and colnames if there
were any), mymatlist[[2]] the second, etc.

Sean

----- Original Message -----
From: "Min-Han Tan" <minhan.science at gmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, August 20, 2004 1:46 PM
Subject: [R] Loss of rownames and colnames


> Hi,
>
> I am working on some microarray data, and have some problems with
> writing iterations.
>
> In essence, the problem is that objects with three dimensions don't
> have rownames and colnames. These colnames and rownames would
> otherwise still be there in 2 dimensional objects.
>
> I need to generate multiple iterations of a 2 means-clustering
> algorithm, and these objects thus probably need 3 dimensions.
>
> My scripts are all written with heavy references to matching of
> colnames and rownames, so I am running into some problems here.
> (colnames = sample ids, and rownames = gene ids)
>
> My bad workaround solution so far has been to generate objects tagged
> with ".2", and have multiple blocks of code.
>
> e.g.
>
> test.1 <- ...
>
> test.2 <- ...
>
> test.x <- ..
>
> The obvious problem with this solution is that there does not seem to
> be an easy way of manipulating all these objects together without
> typing out their names individually.
>
> Thanks for any advice.
>
> Regards,
> Min-Han
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>



From f.duan at yale.edu  Fri Aug 20 20:32:48 2004
From: f.duan at yale.edu (F Duan)
Date: Fri, 20 Aug 2004 14:32:48 -0400
Subject: [R] Could anyone tell me how to open .RNW file?
Message-ID: <1093026768.412643d0ed8f1@webmail.med.yale.edu>


Dear R people,

I find some of help files under doc directory in R are RNW extensions. Could 
anyone explain to me what they mean and which program I should use to open them?

I tried to open them by Rgui.exe but failed. Currently I use wordpad (or other 
txt editors) to open them though the opened files are not very readable.

Thank you.

Best regards,

Frank



From ross at biostat.ucsf.edu  Fri Aug 20 20:35:24 2004
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Fri, 20 Aug 2004 11:35:24 -0700
Subject: [R] Error messages and C
Message-ID: <1093026924.30108.65.camel@iron.libaux.ucsf.edu>

I am calling a C (C++ really) function via the .C interface.
Sometimes when things go wrong I want to return an error message.

1.  R provides C functions error and warning which look about right. 
But exactly how does this exit, and in particular what happens with
cleaning up, calling C++ destructors, and unwinding the stack?  Will I
get memory leaks?

2.  Before I discovered those functions, I looked at passing in a
character vector as an argument, char ** p in the C code.  Exactly how
do I use these things?  Am I supposed to allocate a string and stuff the
pointer in the function argument?  Or should I assume *p points to valid
space (how much?) and fill it in?

Thanks.
-- 
Ross Boylan                                      wk:  (415) 502-4031
530 Parnassus Avenue (Library) rm 115-4          ross at biostat.ucsf.edu
Dept of Epidemiology and Biostatistics           fax: (415) 476-9856
University of California, San Francisco
San Francisco, CA 94143-0840                     hm:  (415) 550-1062



From tplate at blackmesacapital.com  Fri Aug 20 20:42:00 2004
From: tplate at blackmesacapital.com (Tony Plate)
Date: Fri, 20 Aug 2004 12:42:00 -0600
Subject: [R] Loss of rownames and colnames
In-Reply-To: <7902152a040820104675f54690@mail.gmail.com>
References: <7902152a040820104675f54690@mail.gmail.com>
Message-ID: <6.1.0.6.2.20040820122802.0643f6f0@mailhost.blackmesacapital.com>

At Friday 11:46 AM 8/20/2004, Min-Han Tan wrote:
>Hi,
>
>I am working on some microarray data, and have some problems with
>writing iterations.
>
>In essence, the problem is that objects with three dimensions don't
>have rownames and colnames. These colnames and rownames would
>otherwise still be there in 2 dimensional objects.
>
>I need to generate multiple iterations of a 2 means-clustering
>algorithm, and these objects thus probably need 3 dimensions.

What objects are you using that are three dimensional but don't have 
dimension names?  Ordinary arrays have dimension names, and rownames() and 
colnames() extract the names on the first two dimensions:

 > x <- 
array(1:12,dim=c(2,3,2),dimnames=list(letters[1:2],LETTERS[24:26],letters[20:21]))
 > x
, , t

   X Y Z
a 1 3 5
b 2 4 6

, , u

   X  Y  Z
a 7  9 11
b 8 10 12

 > dimnames(x)[[1]]
[1] "a" "b"
 > dimnames(x)[[2]]
[1] "X" "Y" "Z"
 > x[,"Y",]
   t  u
a 3  9
b 4 10
 > rownames(x)
[1] "a" "b"
 > colnames(x)
[1] "X" "Y" "Z"
 >

If you need a convenient way to construct three dimensional objects, you 
can use the abind() package, e.g.:

 > library(abind) # you will have to install the package from CRAN first
 > x1 <- matrix(1:6,nrow=2,dimnames=list(letters[1:2],LETTERS[24:26]))
 > x2 <- matrix(7:12,nrow=2,dimnames=list(letters[1:2],LETTERS[24:26]))
 > abind(list(t=x1, u=x2), along=3)
, , t

   X Y Z
a 1 3 5
b 2 4 6

, , u

   X  Y  Z
a 7  9 11
b 8 10 12

 >

(The objects to be bound don't have to be given to abind() in a list, but 
this manner of invocation is convenient when one happens to have a list of 
objects to be bound together, as one might get in the result from lapply().)


>My scripts are all written with heavy references to matching of
>colnames and rownames, so I am running into some problems here.
>(colnames = sample ids, and rownames = gene ids)

Last time I looked, subscripting matrices and arrays with strings was very 
slow (for large objects), so if you are using character subscripts and 
you're having problems with slowness, consider doing the indexing yourself 
using match(), e.g.:

 > x <- matrix(rnorm(26^4), ncol=26, 
dimnames=list(paste(rep(letters,each=26^2),rep(letters,each=26),letters,sep=""), 
LETTERS))
 > dim(x)
[1] 17576    26
 > xr <- sample(rownames(x), 10000)
 > length(xr)
[1] 10000
 > system.time(y <- x[xr, ])
[1] 2.22 0.00 2.30   NA   NA
 > system.time(y <- x[match(xr, rownames(x)), ])
[1] 0.09 0.00 0.09   NA   NA
 >

HTH

-- Tony Plate

>My bad workaround solution so far has been to generate objects tagged
>with ".2", and have multiple blocks of code.
>
>e.g.
>
>test.1 <- ...
>
>test.2 <- ...
>
>test.x <- ..
>
>The obvious problem with this solution is that there does not seem to
>be an easy way of manipulating all these objects together without
>typing out their names individually.
>
>Thanks for any advice.
>
>Regards,
>Min-Han
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From tlumley at u.washington.edu  Fri Aug 20 21:04:47 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 20 Aug 2004 12:04:47 -0700 (PDT)
Subject: [R] Error messages and C
In-Reply-To: <1093026924.30108.65.camel@iron.libaux.ucsf.edu>
References: <1093026924.30108.65.camel@iron.libaux.ucsf.edu>
Message-ID: <Pine.A41.4.58.0408201138370.127282@homer06.u.washington.edu>

On Fri, 20 Aug 2004, Ross Boylan wrote:

> I am calling a C (C++ really) function via the .C interface.
> Sometimes when things go wrong I want to return an error message.
>
> 1.  R provides C functions error and warning which look about right.
> But exactly how does this exit, and in particular what happens with
> cleaning up, calling C++ destructors, and unwinding the stack?  Will I
> get memory leaks?

Memory handled by R will be reclaimed properly (ie R_alloc, but not
Calloc). C++ destructors will not be called -- you have to do that
yourself either before calling error() or in subsequent cleanup code that
you call from R (perhaps triggered by on.exit()).

R does have a finalizer mechanism that you could use. I have never tried
this but there are some notes at
http://www.stat.uiowa.edu/~luke/R/references/weakfinex.html
This allows C or R functions to be called by the garbage collector when an
object is disposed of.  You can then put an object on the R heap so that
when R tidies this object up it will call your C++ destructors.


> 2.  Before I discovered those functions, I looked at passing in a
> character vector as an argument, char ** p in the C code.  Exactly how
> do I use these things?  Am I supposed to allocate a string and stuff the
> pointer in the function argument?  Or should I assume *p points to valid
> space (how much?) and fill it in?

If you think of p as a vector of strings (ie char *p[]) then it
corresponds to whatever strings you passed in from R.  You can modify
these in place, possibly making them shorter. So if you need 80 characters
of error message, pass in a string of length at least 80.

If you want to allocate more memory you need to do this through R (either
R_alloc or the Rinternals functions).  Using malloc() and stuffing the
result in, say, p[1], will cause a memory leak since you won't be able to
find it and free it.

	-thomas



From robert.kruus at utoronto.ca  Fri Aug 20 21:13:39 2004
From: robert.kruus at utoronto.ca (Robert Kruus)
Date: Fri, 20 Aug 2004 15:13:39 -0400
Subject: [R] R-devel and gcc 3.4
Message-ID: <20040820151339.7e257ff0@kruuslt.forestry.utoronto.ca>

	I tried compiling R-devel from the subversion sources using gcc 3.4.1
and it fails in make check at d-p-q-r-tests.  It compiles and works
using gcc 3.3.4.  Can post more details if needed (don't remember the
exact error off the  top of my head).  Linux-i686.


-- 
robert.kruus at utoronto.ca
linux: the choice of a GNU generation
(ksh at cis.ufl.edu put this on Tshirts in '93)



From myao at ou.edu  Fri Aug 20 21:49:30 2004
From: myao at ou.edu (Yao, Minghua)
Date: Fri, 20 Aug 2004 14:49:30 -0500
Subject: [R] How generate "A01", "A02", ..., "A99"?
Message-ID: <89944065A099FB4AB1296358DD02877A1FD717@XMAIL.sooner.net.ou.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040820/f3cb4051/attachment.pl

From sundar.dorai-raj at PDF.COM  Fri Aug 20 21:59:07 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Fri, 20 Aug 2004 14:59:07 -0500
Subject: [R] How generate "A01", "A02", ..., "A99"?
In-Reply-To: <89944065A099FB4AB1296358DD02877A1FD717@XMAIL.sooner.net.ou.edu>
References: <89944065A099FB4AB1296358DD02877A1FD717@XMAIL.sooner.net.ou.edu>
Message-ID: <4126580B.5070102@pdf.com>



Yao, Minghua wrote:

> Hi,
>  
> Anyone can tell me how to generate "A01", "A02", ..., "A99"?
>  
> paste("A", 1:99, sep="") generates "A1", "A2",..., "A99". This is not  what I want.
>  
> Thanks for the help.
>  
> -MY
> 
> 	[[alternative HTML version deleted]]
> 

How about?

sapply(1:99, function(i) sprintf("A%02d", i))

--sundar



From rossini at blindglobe.net  Fri Aug 20 21:58:46 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Fri, 20 Aug 2004 12:58:46 -0700
Subject: [R] How generate "A01", "A02", ..., "A99"?
In-Reply-To: <89944065A099FB4AB1296358DD02877A1FD717@XMAIL.sooner.net.ou.edu>
	("Yao, Minghua"'s message of "Fri, 20 Aug 2004 14:49:30 -0500")
References: <89944065A099FB4AB1296358DD02877A1FD717@XMAIL.sooner.net.ou.edu>
Message-ID: <85d61loa2x.fsf@servant.blindglobe.net>

"Yao, Minghua" <myao at ou.edu> writes:

> Hi,
>  
> Anyone can tell me how to generate "A01", "A02", ..., "A99"?
>  
> paste("A", 1:99, sep="") generates "A1", "A2",..., "A99". This is not  what I want.

c(paste("A0",1:9,sep=""),paste("A",10:99,sep=""))

?

-- 
Anthony Rossini			    Research Associate Professor
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From vograno at evafunds.com  Fri Aug 20 22:09:46 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Fri, 20 Aug 2004 13:09:46 -0700
Subject: [R] Error messages and C
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A563D491@phost015.EVAFUNDS.intermedia.net>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Ross Boylan
> Sent: Friday, August 20, 2004 11:35 AM
> To: r-help
> Subject: [R] Error messages and C
> 
> I am calling a C (C++ really) function via the .C interface.
> Sometimes when things go wrong I want to return an error message.
> 
> 1.  R provides C functions error and warning which look about right. 
> But exactly how does this exit, and in particular what 
> happens with cleaning up, calling C++ destructors, and 
> unwinding the stack?  Will I get memory leaks?
> 

I've run across this issue and I couldn't find a satisfactory solution
to the problem. error() is effectively a long jump. Moreover, if your
C++ function calls an R API function, e.g allocVector(), and the latter
calls error() ... you already know what will happen. Same for
interrupts.



From FWS4 at CDRH.FDA.GOV  Fri Aug 20 22:17:48 2004
From: FWS4 at CDRH.FDA.GOV (Samuelson, Frank*)
Date: Fri, 20 Aug 2004 16:17:48 -0400
Subject: [R] R-devel and gcc 3.4
Message-ID: <644D9337A02FC24689647BF9E48EC39E08ABB870@drm556>

I encountered a similar problem with 
1. Intel compilers.
2. gcc when I turn off optimizations (no -Ox)


-----Original Message-----
From: Robert Kruus [mailto:robert.kruus at utoronto.ca] 
Sent: Friday, August 20, 2004 3:14 PM
To: r-help at stat.math.ethz.ch
Subject: [R] R-devel and gcc 3.4


	I tried compiling R-devel from the subversion sources using gcc
3.4.1
and it fails in make check at d-p-q-r-tests.  It compiles and works
using gcc 3.3.4.  Can post more details if needed (don't remember the
exact error off the  top of my head).  Linux-i686.


-- 
robert.kruus at utoronto.ca
linux: the choice of a GNU generation
(ksh at cis.ufl.edu put this on Tshirts in '93)

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Fri Aug 20 22:15:47 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Aug 2004 22:15:47 +0200
Subject: [R] How generate "A01", "A02", ..., "A99"?
In-Reply-To: <4126580B.5070102@pdf.com>
References: <89944065A099FB4AB1296358DD02877A1FD717@XMAIL.sooner.net.ou.edu>
	<4126580B.5070102@pdf.com>
Message-ID: <x24qmxy39o.fsf@biostat.ku.dk>

Sundar Dorai-Raj <sundar.dorai-raj at pdf.com> writes:

> Yao, Minghua wrote:
> 
> > Hi,
> >  Anyone can tell me how to generate "A01", "A02", ..., "A99"?
> >  paste("A", 1:99, sep="") generates "A1", "A2",..., "A99". This is
> > not  what I want.
> >  Thanks for the help.
> >  -MY
> > 	[[alternative HTML version deleted]]
> >
> 
> How about?
> 
> sapply(1:99, function(i) sprintf("A%02d", i))

or just 

sapply(1:99,sprintf,fmt="A%02d")

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Fri Aug 20 22:23:30 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Aug 2004 22:23:30 +0200
Subject: [R] R-devel and gcc 3.4
In-Reply-To: <644D9337A02FC24689647BF9E48EC39E08ABB870@drm556>
References: <644D9337A02FC24689647BF9E48EC39E08ABB870@drm556>
Message-ID: <x2zn4pwocd.fsf@biostat.ku.dk>

"Samuelson, Frank*" <FWS4 at cdrh.fda.gov> writes:

> I encountered a similar problem with 
> 1. Intel compilers.
> 2. gcc when I turn off optimizations (no -Ox)

Ooops. I didn't notice #2 the first time around. If we're failing the
checks with no optimization, then we surely have set the tolerances
too tight.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From robert.kruus at utoronto.ca  Fri Aug 20 22:33:33 2004
From: robert.kruus at utoronto.ca (Robert Kruus)
Date: Fri, 20 Aug 2004 16:33:33 -0400
Subject: [R] R-devel and gcc 3.4
In-Reply-To: <x2zn4pwocd.fsf@biostat.ku.dk>
References: <644D9337A02FC24689647BF9E48EC39E08ABB870@drm556>
	<x2zn4pwocd.fsf@biostat.ku.dk>
Message-ID: <20040820163333.03dcf14c@kruuslt.forestry.utoronto.ca>

Here:
CFLAGS=-Os -march=athlon-xp -pipe -fomit-frame-pointer -ftracer

Will try some other flags to see if that is an issue.

comparing 'd-p-q-r-tests.Rout' to './d-p-q-r-tests.Rout.save'
...196,227d195< [1] "Mean scaled  difference: 230.9933"
< [1] "Mean scaled  difference: 16.72081"
< [1] "Mean scaled  difference: 60.41034"
< [1] "Mean scaled  difference: 60.41034"
< [1] "Mean scaled  difference: 22472.41"
< [1] "Mean scaled  difference: 230.9933"
< [1] "Mean scaled  difference: 450.4906"
< [1] "Mean scaled  difference: 117.9663"
< [1] "Mean scaled  difference: 11833.79"
< [1] "Mean scaled  difference: 117.9663"
< [1] "Mean scaled  difference: 874.8262"
< [1] "Mean scaled  difference: 117.9663"
< [1] "Mean scaled  difference: 31.44371"
< [1] "Mean scaled  difference: 60.41034"
< [1] "Mean scaled  difference: 230.9933"
< [1] "Mean scaled  difference: 1689.909"
< [1] "Mean scaled  difference: 450.4906"
< [1] "Mean scaled  difference: 1689.909"
< [1] "Mean scaled  difference: 31.44371"
< [1] "Mean scaled  difference: 230.9933"
< [1] "Mean scaled  difference: 230.9933"
< [1] "Mean scaled  difference: 7.480203"
< [1] "Mean scaled  difference: 3245.883"
< [1] "Mean scaled  difference: 450.4906"
< [1] "Mean scaled  difference: 60.41034"
< [1] "Mean scaled  difference: 80399.63"
< [1] "Mean scaled  difference: 11833.79"
< [1] "Mean scaled  difference: 230.9933"
< [1] "Mean scaled  difference: 450.4906"
< [1] "Mean scaled  difference: 450.4906"
< [1] "Mean scaled  difference: 874.8262"
< [1] "Mean scaled  difference: 230.9933"
240,314d207
< [1] "Mean scaled  difference: 158.0424"
< [1] "Mean scaled  difference: 136.1327"
< [1] "Mean scaled  difference: 203.5565"
< [1] "Mean scaled  difference: 158.0424"
< [1] "Mean scaled  difference: 158.0424"
< [1] "Mean scaled  difference: 114.8797"
< [1] "Mean scaled  difference: 158.0424"
< [1] "Mean scaled  difference: 114.8797"
< [1] "Mean scaled  difference: 227.0455"
< [1] "Mean scaled  difference: 114.8797"
< [1] "Mean scaled  difference: 158.0424"
< [1] "Mean scaled  difference: 94.3997"
< [1] "Mean scaled  difference: 56.3034"
< [1] "Mean scaled  difference: 350.2388"
< [1] "Mean scaled  difference: 275.2651"
< [1] "Mean scaled  difference: 397.2022"
< [1] "Mean scaled  difference: 483.6942"
< [1] "Mean scaled  difference: 197.2793"
< [1] "Mean scaled  difference: 197.2793"
< [1] "Mean scaled  difference: 161.1166"
< [1] "Mean scaled  difference: 126.6956"
< [1] "Mean scaled  difference: 397.2022"
< [1] "Mean scaled  difference: 483.6942"
< [1] "Mean scaled  difference: 161.1166"
< [1] "Mean scaled  difference: 273.9165"
< [1] "Mean scaled  difference: 273.9165"
< [1] "Mean scaled  difference: 273.9165"
< [1] "Mean scaled  difference: 440.0712"
< [1] "Mean scaled  difference: 197.2793"
< [1] "Mean scaled  difference: 161.1166"
< [1] "Mean scaled  difference: 528.008"
< [1] "Mean scaled  difference: 94.3997"
< [1] "Mean scaled  difference: 197.2793"
< [1] "Mean scaled  difference: 483.6942"
< [1] "Mean scaled  difference: 234.9492"
< [1] "Mean scaled  difference: 64.668"
< [1] "Mean scaled  difference: 197.2793"
< [1] "Mean scaled  difference: 126.6956"
< [1] "Mean scaled  difference: 273.9165"
< [1] "Mean scaled  difference: 197.2793"
< [1] "Mean scaled  difference: 355.1587"
< [1] "Mean scaled  difference: 94.3997"
< [1] "Mean scaled  difference: 314.0298"
< [1] "Mean scaled  difference: 234.9492"
< [1] "Mean scaled  difference: 314.0298"
< [1] "Mean scaled  difference: 158.0424"
< [1] "Mean scaled  difference: 180.5405"
< [1] "Mean scaled  difference: 74.79829"
< [1] "Mean scaled  difference: 114.8797"
< [1] "Mean scaled  difference: 56.3034"
< [1] "Mean scaled  difference: 250.9591"
< [1] "Mean scaled  difference: 158.0424"
< [1] "Mean scaled  difference: 180.5405"
< [1] "Mean scaled  difference: 74.79829"
< [1] "Mean scaled  difference: 136.1327"
< [1] "Mean scaled  difference: 203.5565"
< [1] "Mean scaled  difference: 203.5565"
< [1] "Mean scaled  difference: 158.0424"
< [1] "Mean scaled  difference: 158.0424"
< [1] "Mean scaled  difference: 94.3997"
< [1] "Mean scaled  difference: 403.9424"
< [1] "Mean scaled  difference: 337.4193"
< [1] "Mean scaled  difference: 273.9165"
< [1] "Mean scaled  difference: 337.4193"
< [1] "Mean scaled  difference: 213.9039"
< [1] "Mean scaled  difference: 403.9424"
< [1] "Mean scaled  difference: 62.57874"
< [1] "Mean scaled  difference: 618.1512"
< [1] "Mean scaled  difference: 770.7226"
< [1] "Mean scaled  difference: 337.4193"
< [1] "Mean scaled  difference: 618.1512"
< [1] "Mean scaled  difference: 213.9039"
< [1] "Mean scaled  difference: 273.9165"
< [1] "Mean scaled  difference: 618.1512"
< [1] "Mean scaled  difference: 213.9039"
766,768c659,662
<  [1] -5.576862 -6.208975 -6.008831 -7.604174 -5.757453 -5.719751
-5.477951<  [8] -6.328233 -5.505609 -6.041780 -6.075742 -5.528914
-5.503182 -6.471671< [15] -6.899387 -5.477143 -6.626737 -5.646456
-7.929201 -5.727071---
>  [1] 0.0037844241 0.0020112978 0.0024569596 0.0004983669 0.0031591482
>  [6] 0.0032805288 0.0041778794 0.0017851857 0.0040639142 0.0023773231
> [11] 0.0022979400 0.0039702993 0.0040737886 0.0015466398 0.0010084031
> [16] 0.0041812581 0.0013244780 0.0035300054 0.0003600738 0.0032566009
783,785c677,680
<  [1] -4.528263 -4.154630 -4.154630 -6.125949 -4.143093 -4.113462
-4.281849<  [8] -4.167814 -4.154630 -4.124981 -4.108528 -4.391428
-4.785674 -4.306719< [15] -5.049338 -5.874622 -4.333257 -5.359233
-4.785674 -4.258675---
>  [1] 0.010799421 0.015691594 0.015691594 0.002185415 0.015873676
>  0.016351073[7] 0.013817085 0.015486075 0.015691594 0.016163798
>  0.016431951 0.012383032
> [13] 0.008348499 0.013477697 0.006413577 0.002809857 0.013124724
> 0.004704515[19] 0.008348499 0.014141025
make[2]: *** [d-p-q-r-tests.Rout] Error 1


-- 
robert.kruus at utoronto.ca
I am what you will be; I was what you are.


On 20 Aug 2004 22:23:30 +0200 it is rumored
that Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:

> "Samuelson, Frank*" <FWS4 at cdrh.fda.gov> writes:
> 
> > I encountered a similar problem with 
> > 1. Intel compilers.
> > 2. gcc when I turn off optimizations (no -Ox)
> 
> Ooops. I didn't notice #2 the first time around. If we're failing the
> checks with no optimization, then we surely have set the tolerances
> too tight.
> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45)
>  35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45)
> 35327907
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From ihok at hotmail.com  Fri Aug 20 22:33:46 2004
From: ihok at hotmail.com (Jack Tanner)
Date: Fri, 20 Aug 2004 16:33:46 -0400
Subject: [R] paired t-test vs pairwise t-test
Message-ID: <BAY22-F32RoVPWnOIwQ0007b560@hotmail.com>

On Thu, 2004-08-19 at 14:42, Liaw, Andy wrote:
 > Perhaps other stat packages do it differently?  Does SPSS manuals detail
 > what its t-test procedure does, including which t-test(s) it does and 
when
 > it's appropriate?

SAS documentation, which Marc pointed out, is much more detailed than SPSS. 
For completeness, here's SPSS's:

username Guest, password Guest (case sensitive):
http://support.spss.com/tech/stat/algorithms/algorithms.htm

specifically, for the t-test, see
http://support.spss.com/tech/stat/Algorithms/12.0/t_test.pdf

 > That might make it easier on users, but I still think the
 > users should learn the appropriate use of statistical procedures
 > elsewhere...

Hell, I took my share of stats classes, but it's not like the terminology is 
always standard, anyway. I much prefer a walked-through example to an 
abstract explanation like "Calculate pairwise comparisons between group 
levels with corrections for multiple testing." That seems terse to me. It 
sorta makes sense only after the fact.



From MSchwartz at MedAnalytics.com  Fri Aug 20 22:37:58 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 20 Aug 2004 15:37:58 -0500
Subject: [R] How generate "A01", "A02", ..., "A99"?
In-Reply-To: <x24qmxy39o.fsf@biostat.ku.dk>
References: <89944065A099FB4AB1296358DD02877A1FD717@XMAIL.sooner.net.ou.edu>
	<4126580B.5070102@pdf.com>  <x24qmxy39o.fsf@biostat.ku.dk>
Message-ID: <1093034278.3716.31.camel@localhost.localdomain>

On Fri, 2004-08-20 at 15:15, Peter Dalgaard wrote:
> Sundar Dorai-Raj <sundar.dorai-raj at pdf.com> writes:
> 
> > Yao, Minghua wrote:
> > 
> > > Hi,
> > >  Anyone can tell me how to generate "A01", "A02", ..., "A99"?
> > >  paste("A", 1:99, sep="") generates "A1", "A2",..., "A99". This is
> > > not  what I want.
> > >  Thanks for the help.
> > >  -MY
> > > 	[[alternative HTML version deleted]]
> > >
> > 
> > How about?
> > 
> > sapply(1:99, function(i) sprintf("A%02d", i))
> 
> or just 
> 
> sapply(1:99,sprintf,fmt="A%02d")


or yet another variation:

paste("A", formatC(1:99, width = 2, format = "d", flag = "0"), 
      sep = "")


HTH,

Marc Schwartz



From ligges at statistik.uni-dortmund.de  Fri Aug 20 22:50:06 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 20 Aug 2004 22:50:06 +0200
Subject: [R] Could anyone tell me how to open .RNW file?
In-Reply-To: <1093026768.412643d0ed8f1@webmail.med.yale.edu>
References: <1093026768.412643d0ed8f1@webmail.med.yale.edu>
Message-ID: <412663FE.6040706@statistik.uni-dortmund.de>

F Duan wrote:
> Dear R people,
> 
> I find some of help files under doc directory in R are RNW extensions. Could 
> anyone explain to me what they mean and which program I should use to open them?
> 
> I tried to open them by Rgui.exe but failed. Currently I use wordpad (or other 
> txt editors) to open them though the opened files are not very readable.

Rnw stands for R NoWeb files. They have to be processed by Sweave i.e. 
code chunks are processed with R and results inluded into a LaTeX file 
that itself has to be processed by LaTeX oder PDFLaTeX.
For more details on Sweave goggle for it and its author, Friedrich 
Leisch. There sre at least two corresponding articles in R News.

Either in R and in contributed packages, so-called vignettes should have 
been preprocessed and already available in form of pdf files, if a 
package has been build correctly.

Uwe Ligges



> Thank you.
> 
> Best regards,
> 
> Frank
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ihok at hotmail.com  Fri Aug 20 22:48:35 2004
From: ihok at hotmail.com (Jack Tanner)
Date: Fri, 20 Aug 2004 16:48:35 -0400
Subject: [R] paired t-test vs pairwise t-test
Message-ID: <BAY22-F32rGVquEZvuU0007b72b@hotmail.com>

From: Peter Dalgaard <p.dalgaard at biostat.ku.dk>
>I didn't suggest looking at the example section of the help pages
>without a reason...
>It should be pretty clear example(pairwise.t.test) does give output
>that pretty clearly indicates that all pairwise comparisons are being
>performed.

It's only clear if you know what you're looking for. Worse: the 
pairwise.t.test help page tells me that the value is an "Object of class 
'pairwise.htest'". Fine, but help.search("pairwise.htest") produces nothing 
at all. Sorry, but I'm just a newbie having a tough time interpreting the 
pairwise.t.test function and its output.

P.S. Try googling for "pairwise t-test". Ironically enough, the first hit 
that correctly explains what pairwise.t.test() does is a SAS help page 
that's at number 10 for me; the preceding 9 hits confuse paired t-tests and 
pairwise t-tests.
http://support.sas.com/documentation/onlinedoc/v7/whatsnew/insight/sect3.htm



From maechler at stat.math.ethz.ch  Fri Aug 20 23:02:32 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 20 Aug 2004 23:02:32 +0200
Subject: [R] R-devel and gcc 3.4
In-Reply-To: <20040820163333.03dcf14c@kruuslt.forestry.utoronto.ca>
References: <644D9337A02FC24689647BF9E48EC39E08ABB870@drm556>
	<x2zn4pwocd.fsf@biostat.ku.dk>
	<20040820163333.03dcf14c@kruuslt.forestry.utoronto.ca>
Message-ID: <16678.26344.885797.789141@gargle.gargle.HOWL>

>>>>> "Robert" == Robert Kruus <robert.kruus at utoronto.ca>
>>>>>     on Fri, 20 Aug 2004 16:33:33 -0400 writes:

    Robert> Here: CFLAGS=-Os -march=athlon-xp -pipe
    Robert> -fomit-frame-pointer -ftracer

is this now gcc (w/o -O) or the intel one?

In any case, your numeric results seem completely off target,
not just slightly inaccurate..

    Robert> Will try some other flags to see if that is an
    Robert> issue.

  comparing 'd-p-q-r-tests.Rout' to './d-p-q-r-tests.Rout.save'
  ...196,227d195< [1] "Mean scaled  difference: 230.9933"
  < [1] "Mean scaled  difference: 16.72081"
  < [1] "Mean scaled  difference: 60.41034"
  < [1] "Mean scaled  difference: 60.41034"
  < [1] "Mean scaled  difference: 22472.41"
  < [1] "Mean scaled  difference: 230.9933"
  < [1] "Mean scaled  difference: 450.4906"
  < [1] "Mean scaled  difference: 117.9663"
  < [1] "Mean scaled  difference: 11833.79"
  < [1] "Mean scaled  difference: 117.9663"
  < [1] "Mean scaled  difference: 874.8262"
  < [1] "Mean scaled  difference: 117.9663"
  < [1] "Mean scaled  difference: 31.44371"
  < [1] "Mean scaled  difference: 60.41034"
  < [1] "Mean scaled  difference: 230.9933"
  < [1] "Mean scaled  difference: 1689.909"
  < [1] "Mean scaled  difference: 450.4906"
  < [1] "Mean scaled  difference: 1689.909"
  < [1] "Mean scaled  difference: 31.44371"
  < [1] "Mean scaled  difference: 230.9933"
  < [1] "Mean scaled  difference: 230.9933"
  < [1] "Mean scaled  difference: 7.480203"
  < [1] "Mean scaled  difference: 3245.883"
  < [1] "Mean scaled  difference: 450.4906"
  < [1] "Mean scaled  difference: 60.41034"
  < [1] "Mean scaled  difference: 80399.63"
  < [1] "Mean scaled  difference: 11833.79"
  < [1] "Mean scaled  difference: 230.9933"
  < [1] "Mean scaled  difference: 450.4906"
  < [1] "Mean scaled  difference: 450.4906"
  < [1] "Mean scaled  difference: 874.8262"
  < [1] "Mean scaled  difference: 230.9933"
  240,314d207
  < [1] "Mean scaled  difference: 158.0424"
  < [1] "Mean scaled  difference: 136.1327"
  < [1] "Mean scaled  difference: 203.5565"
  < [1] "Mean scaled  difference: 158.0424"
  < [1] "Mean scaled  difference: 158.0424"
  < [1] "Mean scaled  difference: 114.8797"
  < [1] "Mean scaled  difference: 158.0424"
  < [1] "Mean scaled  difference: 114.8797"
  < [1] "Mean scaled  difference: 227.0455"
  < [1] "Mean scaled  difference: 114.8797"
  < [1] "Mean scaled  difference: 158.0424"
  < [1] "Mean scaled  difference: 94.3997"
  < [1] "Mean scaled  difference: 56.3034"
  < [1] "Mean scaled  difference: 350.2388"
  < [1] "Mean scaled  difference: 275.2651"
  < [1] "Mean scaled  difference: 397.2022"
  < [1] "Mean scaled  difference: 483.6942"
  < [1] "Mean scaled  difference: 197.2793"
  < [1] "Mean scaled  difference: 197.2793"
  < [1] "Mean scaled  difference: 161.1166"
  < [1] "Mean scaled  difference: 126.6956"
  < [1] "Mean scaled  difference: 397.2022"
  < [1] "Mean scaled  difference: 483.6942"
  < [1] "Mean scaled  difference: 161.1166"
  < [1] "Mean scaled  difference: 273.9165"
  < [1] "Mean scaled  difference: 273.9165"
  < [1] "Mean scaled  difference: 273.9165"
  < [1] "Mean scaled  difference: 440.0712"
  < [1] "Mean scaled  difference: 197.2793"
  < [1] "Mean scaled  difference: 161.1166"
  < [1] "Mean scaled  difference: 528.008"
  < [1] "Mean scaled  difference: 94.3997"
  < [1] "Mean scaled  difference: 197.2793"
  < [1] "Mean scaled  difference: 483.6942"
  < [1] "Mean scaled  difference: 234.9492"
  < [1] "Mean scaled  difference: 64.668"
  < [1] "Mean scaled  difference: 197.2793"
  < [1] "Mean scaled  difference: 126.6956"
  < [1] "Mean scaled  difference: 273.9165"
  < [1] "Mean scaled  difference: 197.2793"
  < [1] "Mean scaled  difference: 355.1587"
  < [1] "Mean scaled  difference: 94.3997"
  < [1] "Mean scaled  difference: 314.0298"
  < [1] "Mean scaled  difference: 234.9492"
  < [1] "Mean scaled  difference: 314.0298"
  < [1] "Mean scaled  difference: 158.0424"
  < [1] "Mean scaled  difference: 180.5405"
  < [1] "Mean scaled  difference: 74.79829"
  < [1] "Mean scaled  difference: 114.8797"
  < [1] "Mean scaled  difference: 56.3034"
  < [1] "Mean scaled  difference: 250.9591"
  < [1] "Mean scaled  difference: 158.0424"
  < [1] "Mean scaled  difference: 180.5405"
  < [1] "Mean scaled  difference: 74.79829"
  < [1] "Mean scaled  difference: 136.1327"
  < [1] "Mean scaled  difference: 203.5565"
  < [1] "Mean scaled  difference: 203.5565"
  < [1] "Mean scaled  difference: 158.0424"
  < [1] "Mean scaled  difference: 158.0424"
  < [1] "Mean scaled  difference: 94.3997"
  < [1] "Mean scaled  difference: 403.9424"
  < [1] "Mean scaled  difference: 337.4193"
  < [1] "Mean scaled  difference: 273.9165"
  < [1] "Mean scaled  difference: 337.4193"
  < [1] "Mean scaled  difference: 213.9039"
  < [1] "Mean scaled  difference: 403.9424"
  < [1] "Mean scaled  difference: 62.57874"
  < [1] "Mean scaled  difference: 618.1512"
  < [1] "Mean scaled  difference: 770.7226"
  < [1] "Mean scaled  difference: 337.4193"
  < [1] "Mean scaled  difference: 618.1512"
  < [1] "Mean scaled  difference: 213.9039"
  < [1] "Mean scaled  difference: 273.9165"
  < [1] "Mean scaled  difference: 618.1512"
  < [1] "Mean scaled  difference: 213.9039"
  766,768c659,662
  <  [1] -5.576862 -6.208975 -6.008831 -7.604174 -5.757453 -5.719751
  -5.477951<  [8] -6.328233 -5.505609 -6.041780 -6.075742 -5.528914
  -5.503182 -6.471671< [15] -6.899387 -5.477143 -6.626737 -5.646456
  -7.929201 -5.727071---
  >  [1] 0.0037844241 0.0020112978 0.0024569596 0.0004983669 0.0031591482
  >  [6] 0.0032805288 0.0041778794 0.0017851857 0.0040639142 0.0023773231
  > [11] 0.0022979400 0.0039702993 0.0040737886 0.0015466398 0.0010084031
  > [16] 0.0041812581 0.0013244780 0.0035300054 0.0003600738 0.0032566009
  783,785c677,680
  <  [1] -4.528263 -4.154630 -4.154630 -6.125949 -4.143093 -4.113462
  -4.281849<  [8] -4.167814 -4.154630 -4.124981 -4.108528 -4.391428
  -4.785674 -4.306719< [15] -5.049338 -5.874622 -4.333257 -5.359233
  -4.785674 -4.258675---
  >  [1] 0.010799421 0.015691594 0.015691594 0.002185415 0.015873676
  >  0.016351073[7] 0.013817085 0.015486075 0.015691594 0.016163798
  >  0.016431951 0.012383032
  > [13] 0.008348499 0.013477697 0.006413577 0.002809857 0.013124724
  > 0.004704515[19] 0.008348499 0.014141025
  make[2]: *** [d-p-q-r-tests.Rout] Error 1


    Robert> -- robert.kruus at utoronto.ca I am what you will be; I
    Robert> was what you are.


    Robert> On 20 Aug 2004 22:23:30 +0200 it is rumored that
    Robert> Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
 
    PD> "Samuelson, Frank*" <FWS4 at cdrh.fda.gov> writes:
    PD> 
    PD> > I encountered a similar problem with > 1. Intel
    PD> compilers.  > 2. gcc when I turn off optimizations
    PD> (no -Ox)
    PD> 
    PD> Ooops. I didn't notice #2 the first time around. If
    PD> we're failing the checks with no optimization, then
    PD> we surely have set the tolerances too tight.

well, not in this case.
The above numbers are *way* off.

Yes, Robert, do try other compilere switches.



From gunter.berton at gene.com  Fri Aug 20 23:16:03 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Fri, 20 Aug 2004 14:16:03 -0700
Subject: [R] paired t-test vs pairwise t-test
References: <BAY22-F32rGVquEZvuU0007b72b@hotmail.com>
Message-ID: <41266A13.4F40566C@gene.com>

The fact is that, while certainly desirable,  it is very difficult and
time-consuming to write the sort of extensively exampled, instructional Help
files that you desire. Given the voluntary nature of R authors/developers, the
Help files as they exist are remarkable, while still admittedly being uneven. SAS
charges all that money and sells all those manuals for a reason: the revenue
stream allows them to employ an army of documenters to presumably do the sorts of
things you'd like. If that's really important to you -- and this is no criticism
-- you should pay the money and use SAS. Those of us who use R feel that it's
many strengths outweigh this weakness.

One addendum to this, already extensively discussed in this thread: There are
more and more extra R documentation and instructional sources (Peter Dalgaard's
book being a recent example) that can help. Search the thread and CRAN for more.

-- Bert

Bert Gunter

Non-Clinical Biostatistics
Genentech
MS: 240B
Phone: 650-467-7374


"The business of the statistician is to catalyze the scientific learning
process."

 -- George E.P. Box



From richard_raubertas at merck.com  Fri Aug 20 23:17:09 2004
From: richard_raubertas at merck.com (Raubertas, Richard)
Date: Fri, 20 Aug 2004 17:17:09 -0400
Subject: [R] How generate "A01", "A02", ..., "A99"?
Message-ID: <B88F4BCF37DD0847937C1C98255291FB0175C3CD@uswsmx05.merck.com>

And yet another way:

x <- as.character(101:199)
substr(x,1,1) <- "A"


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Marc Schwartz
> Sent: Friday, August 20, 2004 4:38 PM
> To: Peter Dalgaard
> Cc: R Help; sundar.dorai-raj at pdf.com
> Subject: Re: [R] How generate "A01", "A02", ..., "A99"?
> 
> 
> On Fri, 2004-08-20 at 15:15, Peter Dalgaard wrote:
> > Sundar Dorai-Raj <sundar.dorai-raj at pdf.com> writes:
> > 
> > > Yao, Minghua wrote:
> > > 
> > > > Hi,
> > > >  Anyone can tell me how to generate "A01", "A02", ..., "A99"?
> > > >  paste("A", 1:99, sep="") generates "A1", "A2",..., 
> "A99". This is
> > > > not  what I want.
> > > >  Thanks for the help.
> > > >  -MY
> > > > 	[[alternative HTML version deleted]]
> > > >
> > > 
> > > How about?
> > > 
> > > sapply(1:99, function(i) sprintf("A%02d", i))
> > 
> > or just 
> > 
> > sapply(1:99,sprintf,fmt="A%02d")
> 
> 
> or yet another variation:
> 
> paste("A", formatC(1:99, width = 2, format = "d", flag = "0"), 
>       sep = "")
> 
> 
> HTH,
> 
> Marc Schwartz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ross at biostat.ucsf.edu  Fri Aug 20 23:39:21 2004
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Fri, 20 Aug 2004 14:39:21 -0700
Subject: [R] Error messages and C
In-Reply-To: <Pine.A41.4.58.0408201138370.127282@homer06.u.washington.edu>
References: <1093026924.30108.65.camel@iron.libaux.ucsf.edu>
	<Pine.A41.4.58.0408201138370.127282@homer06.u.washington.edu>
Message-ID: <1093037961.30105.80.camel@iron.libaux.ucsf.edu>

On Fri, 2004-08-20 at 12:04, Thomas Lumley wrote:
> On Fri, 20 Aug 2004, Ross Boylan wrote:
> 
> > I am calling a C (C++ really) function via the .C interface.
> > Sometimes when things go wrong I want to return an error message.
> >
> > 1.  R provides C functions error and warning which look about right.
> > But exactly how does this exit, and in particular what happens with
> > cleaning up, calling C++ destructors, and unwinding the stack?  Will I
> > get memory leaks?
> 
> Memory handled by R will be reclaimed properly (ie R_alloc, but not
> Calloc). C++ destructors will not be called -- you have to do that
> yourself either before calling error() 
That's not hard in my case, so that's what I'm doing.
> or in subsequent cleanup code that
> you call from R (perhaps triggered by on.exit()).

That would be tricky, since the location of the objects to be destroyed
is only known inside my original function.  Of course, I could use
global variables...

> 
> R does have a finalizer mechanism that you could use. I have never tried
> this but there are some notes at
> http://www.stat.uiowa.edu/~luke/R/references/weakfinex.html
> This allows C or R functions to be called by the garbage collector when an
> object is disposed of.  You can then put an object on the R heap so that
> when R tidies this object up it will call your C++ destructors.
> 
The weak references cleaned up are R objects, and my stuff is mostly
non-R objects.  I see two possible ways to get this to work:
1) Create some kind of dummy R object with a finalizer that cleans up my
C++ objects.
2) Hook the C++ object creation facilities (via new) into R.
Either approach is complex, possibly not doable, and probably
inefficient.  Fortunately, I don't need this.
> 
> > 2.  Before I discovered those functions, I looked at passing in a
> > character vector as an argument, char ** p in the C code.  Exactly how
> > do I use these things?  Am I supposed to allocate a string and stuff the
> > pointer in the function argument?  Or should I assume *p points to valid
> > space (how much?) and fill it in?
> 
> If you think of p as a vector of strings (ie char *p[]) then it
> corresponds to whatever strings you passed in from R.  You can modify
> these in place, possibly making them shorter. So if you need 80 characters
> of error message, pass in a string of length at least 80.

If space permits, is this just an ordinary, C-style null terminated
string?  Or does R have some notion of the string length which is
unaffected by what I stuff in it?

> If you want to allocate more memory you need to do this through R (either
> R_alloc or the Rinternals functions).  Using malloc() and stuffing the
> result in, say, p[1], will cause a memory leak since you won't be able to
> find it and free it.
> 
So if I overwrite p[0], for example, the R garbage collector will know
to clean up the old string that was pointed to from that location?

> 	-thomas

Thanks for the info.
-- 
Ross Boylan                                      wk:  (415) 502-4031
530 Parnassus Avenue (Library) rm 115-4          ross at biostat.ucsf.edu
Dept of Epidemiology and Biostatistics           fax: (415) 476-9856
University of California, San Francisco
San Francisco, CA 94143-0840                     hm:  (415) 550-1062



From guano at usp.br  Fri Aug 20 23:48:42 2004
From: guano at usp.br (Carlos Henrique Grohmann)
Date: Fri, 20 Aug 2004 18:48:42 -0300
Subject: [R] slackware packages
Message-ID: <1093038522.412671baba72b@webmail.usp.br>

Hello all, 
I'm new in this list and I'd like to know if someone knows about R packages for
slackware linux. 

thanks all.


-- 
+-------------------------------------------------+
        Carlos Henrique Grohmann - Guano  
  Geologist M.Sc - PhD Student at IGc-USP - Brazil
     Linux User #89721  - guano at usp dot br -
+-------------------------------------------------+



From lschaffe at gnf.org  Fri Aug 20 23:49:51 2004
From: lschaffe at gnf.org (Lana Schaffer)
Date: Fri, 20 Aug 2004 14:49:51 -0700
Subject: [R] Partial Least Squares
Message-ID: <833E32F61B9F8746878F2A1865BECE6001455460@EXCHCLUSTER01.lj.gnf.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040820/44b3cccc/attachment.pl

From lschaffe at gnf.org  Fri Aug 20 23:51:44 2004
From: lschaffe at gnf.org (Lana Schaffer)
Date: Fri, 20 Aug 2004 14:51:44 -0700
Subject: [R] Sorting matrices
Message-ID: <833E32F61B9F8746878F2A1865BECE6001455461@EXCHCLUSTER01.lj.gnf.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040820/8e44f530/attachment.pl

From sbchapman at highstream.net  Fri Aug 20 23:55:50 2004
From: sbchapman at highstream.net (Sam Chapman)
Date: Fri, 20 Aug 2004 17:55:50 -0400
Subject: [R] The "Green" Book?
Message-ID: <1093038950.41267366c5c4b@webmail.highstream.net>

I am new to R and am currently gathering reference books. In 'An Introduction to
R' I have read the following:

"For R, the basic reference is 'The New S Language: A Programming Environment
for Data Analysis and Graphics' by Richard A. Becker, John M. Chambers and Allan
R. Wilks. The new features of the 1991 release of S (S version 3) are covered in
'Statistical Models in S' edited by John M. Chambers and Trevor J. Hastie."

There is no mention of 'Programming with Data: A Guide to the S Language' by
John M. Chambers. Is this newest ("Green") book also suitable as a reference
for R? Thank you for your time and attention!

Sincerely,

Sam Chapman



From minhan.science at gmail.com  Fri Aug 20 23:57:21 2004
From: minhan.science at gmail.com (Min-Han Tan)
Date: Fri, 20 Aug 2004 17:57:21 -0400
Subject: [R] Loss of rownames and colnames
In-Reply-To: <6.1.0.6.2.20040820122802.0643f6f0@mailhost.blackmesacapital.com>
References: <7902152a040820104675f54690@mail.gmail.com>
	<6.1.0.6.2.20040820122802.0643f6f0@mailhost.blackmesacapital.com>
Message-ID: <7902152a040820145740792d8e@mail.gmail.com>

Thank you all very much.

I have solved the problem using lists of matrices.

But yes, I will certainly look into the issue of the names for
higher-dimensional arrays.

Regards,
Min-han



From sundar.dorai-raj at PDF.COM  Sat Aug 21 00:02:56 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Fri, 20 Aug 2004 17:02:56 -0500
Subject: [R] Sorting matrices
In-Reply-To: <833E32F61B9F8746878F2A1865BECE6001455461@EXCHCLUSTER01.lj.gnf.org>
References: <833E32F61B9F8746878F2A1865BECE6001455461@EXCHCLUSTER01.lj.gnf.org>
Message-ID: <41267510.7080601@pdf.com>



Lana Schaffer wrote:

> Hi,
> I have been sorting matrices by sorting each column one at a time.
> However,
> Many times I want to sort the whole matrix by sorting one column.  Are
> there
> Ways to do this in R?
> Lana
> 

Lana,
   See ?order:

x[order(x[, 1]), ]

--sundar



From robert.kruus at utoronto.ca  Sat Aug 21 00:24:18 2004
From: robert.kruus at utoronto.ca (Robert Kruus)
Date: Fri, 20 Aug 2004 18:24:18 -0400
Subject: [R] R-devel and gcc 3.4
In-Reply-To: <16678.26344.885797.789141@gargle.gargle.HOWL>
References: <644D9337A02FC24689647BF9E48EC39E08ABB870@drm556>
	<x2zn4pwocd.fsf@biostat.ku.dk>
	<20040820163333.03dcf14c@kruuslt.forestry.utoronto.ca>
	<16678.26344.885797.789141@gargle.gargle.HOWL>
Message-ID: <20040820182418.5cf09ce1@kruuslt.forestry.utoronto.ca>

gcc.  Sorry for the double post.

On Fri, 20 Aug 2004 23:02:32 +0200 it is rumored
that Martin Maechler <maechler at stat.math.ethz.ch> wrote:

> >>>>> "Robert" == Robert Kruus <robert.kruus at utoronto.ca>
> >>>>>     on Fri, 20 Aug 2004 16:33:33 -0400 writes:
> 
>     Robert> Here: CFLAGS=-Os -march=athlon-xp -pipe
>     Robert> -fomit-frame-pointer -ftracer
> 
> is this now gcc (w/o -O) or the intel one?
> 
> In any case, your numeric results seem completely off target,
> not just slightly inaccurate..
> 
>     Robert> Will try some other flags to see if that is an
>     Robert> issue.
> 
<---SNIP--->

-- 
robert.kruus at utoronto.ca
If little else, the brain is an educational toy.
		-- Tom Robbins



From maechler at stat.math.ethz.ch  Sat Aug 21 00:39:25 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 21 Aug 2004 00:39:25 +0200
Subject: [R] More precision problems in testing with Intel compilers
In-Reply-To: <644D9337A02FC24689647BF9E48EC39E08ABB86D@drm556>
References: <644D9337A02FC24689647BF9E48EC39E08ABB86D@drm556>
Message-ID: <16678.32157.647169.330756@gargle.gargle.HOWL>

>>>>> "FrankSa" == Samuelson, Frank* <Samuelson>
>>>>>     on Thu, 19 Aug 2004 16:22:11 -0400 writes:

    FrankSa> The Intel compiled version also fails the below test:

here you give the desired output.
What does your 'Intel compiled R' return instead?

    >> ###------------ Very big and very small
    >> umach <- unlist(.Machine)[paste("double.x", c("min","max"), sep='')]
    >> xmin <- umach[1]
    >> xmax <- umach[2]
    >> tx <- unique(outer(-1:1,c(.1,1e-3,1e-7)))# 7 values  (out of 9)
    >> tx <- unique(sort(c(outer(umach,1+tx))))# 11 values  (out of 14)
    >> tx <- tx[is.finite(tx)] #-- all kept
    >> (txp <- tx[tx >= 1])#-- Positive exponent -- 4 values
     [1] 1.617924e+308 1.795895e+308 1.797693e+308 1.797693e+308
    >> (txn <- tx[tx <        1])#-- Negative exponent -- 7 values
    [1] 2.002566e-308 2.222849e-308 2.225074e-308 2.225074e-308 2.225074e-308 2.227299e-308 2.447581e-308

    FrankSa> Does anyone really care about being correct to 1
    FrankSa> unit of machine precision?  If you do, you have a
    FrankSa> bad algorithm.  ??

We have had these tests there for a long time now and haven't
heard of failures before..  so this is interesting.
DIG(7) makes us only look at 7 digits which is less than half machine
precision, but then there's cancellation of another 7 digits in
some of those above which gets in the region of machine precision,
(but still leaves a factor of ~= 45).

Can you upload the full print-test.Rout file somewhere?

Regards,
Martin


    FrankSa> -----Original Message-----
    FrankSa> From: Samuelson, Frank* [mailto:FWS4 at cdrh.fda.gov] 
    FrankSa> Sent: Thursday, August 19, 2004 12:11 PM
    FrankSa> To: 'r-help at stat.math.ethz.ch '
    FrankSa> Subject: [R] precision problems in testing with Intel compilers


    FrankSa> I compiled the 1.9.1 src.rpm with the standard gnu tools and it works.
    FrankSa> I tried compiling the 1.9.1 src.rpm with the Intel 8 C and FORTRAN
    FrankSa> compilers and it bombs out during the testing phase:

    FrankSa> comparing 'd-p-q-r-tests.Rout' to './d-p-q-r-tests.Rout.save' ...267c267
    FrankSa> < df = 0.5[1] "Mean relative  difference: 5.001647e-10"
    FrankSa> ---
    >> df = 0.5[1] TRUE
    FrankSa> make[3]: *** [d-p-q-r-tests.Rout] Error 1
    FrankSa> make[3]: Leaving directory `/usr/src/redhat/BUILD/R-1.9.1/tests'
    FrankSa> make[2]: *** [test-Specific] Error 2
    FrankSa> make[2]: Leaving directory `/usr/src/redhat/BUILD/R-1.9.1/tests'
    FrankSa> make[1]: *** [test-all-basics] Error 1
    FrankSa> make[1]: Leaving directory `/usr/src/redhat/BUILD/R-1.9.1/tests'
    FrankSa> make: *** [check-all] Error 2
    FrankSa> error: Bad exit status from /var/tmp/rpm-tmp.63044 (%build)
    FrankSa> ...



From maechler at stat.math.ethz.ch  Sat Aug 21 00:49:32 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 21 Aug 2004 00:49:32 +0200
Subject: [R] precision problems in testing with Intel compilers
In-Reply-To: <644D9337A02FC24689647BF9E48EC39E08ABB86B@drm556>
References: <644D9337A02FC24689647BF9E48EC39E08ABB86B@drm556>
Message-ID: <16678.32764.850686.997868@gargle.gargle.HOWL>

Thank you for the report.
Your points are quite valid (see below).

Note that these precision tests were a bit stringent on purpose
because pchisq() eventually needs some accuracy improvements
which I had been investigating (when I did some minor
improvements).

>>>>> "FrankSa" == Samuelson, Frank* <Samuelson>
>>>>>     on Thu, 19 Aug 2004 12:11:00 -0400 writes:

    FrankSa> I compiled the 1.9.1 src.rpm with the standard gnu tools and it works.
    FrankSa> I tried compiling the 1.9.1 src.rpm with the Intel 8 C and FORTRAN
    FrankSa> compilers and it bombs out during the testing phase:

    FrankSa> comparing 'd-p-q-r-tests.Rout' to './d-p-q-r-tests.Rout.save' ...267c267
    FrankSa> < df = 0.5[1] "Mean relative  difference: 5.001647e-10"
    FrankSa> ---
    >> df = 0.5[1] TRUE
    FrankSa> make[3]: *** [d-p-q-r-tests.Rout] Error 1
    FrankSa> make[3]: Leaving directory `/usr/src/redhat/BUILD/R-1.9.1/tests'
    FrankSa> make[2]: *** [test-Specific] Error 2
    FrankSa> make[2]: Leaving directory `/usr/src/redhat/BUILD/R-1.9.1/tests'
    FrankSa> make[1]: *** [test-all-basics] Error 1
    FrankSa> make[1]: Leaving directory `/usr/src/redhat/BUILD/R-1.9.1/tests'
    FrankSa> make: *** [check-all] Error 2
    FrankSa> error: Bad exit status from /var/tmp/rpm-tmp.63044 (%build)

    FrankSa> looking at the differences between the failed file and the standard, I get:

    FrankSa> fws wolf tests] diff  d-p-q-r-tests.Rout.save d-p-q-r-tests.Rout.fail 
    FrankSa> 3c3
    FrankSa> < Version 1.9.0 Patched (2004-04-19), ISBN 3-900051-00-3
    FrankSa> ---
    >> Version 1.9.1  (2004-06-21), ISBN 3-900051-00-3
    FrankSa> 281c281
    FrankSa> < df = 0.5[1] TRUE
    FrankSa> ---
    >> df = 0.5[1] "Mean relative  difference: 5.001647e-10"
    FrankSa> 935c935
    FrankSa> < Time elapsed:  7.83 0.04 16.1 0 0 
    FrankSa> ---
    >> Time elapsed:  2.49 0.01 2.55 0 0 

    FrankSa> Besides being 3 times faster, it's stopping on the following code:

(intel's new C compiler,  what hardware?)

    FrankSa> for(df in c(0.1, 0.5, 1.5, 4.7, 10, 20,50,100)) {
    FrankSa>  cat("df =", formatC(df, wid=3))
    FrankSa>  xx <- c(10^-(5:1), .9, 1.2, df + c(3,7,20,30,35,38))
    FrankSa>  pp <- pchisq(xx, df=df, ncp = 1) #print(pp)
    FrankSa>  dtol <- 1e-12 *(if(2 < df && df <= 50) 64 else if(df > 50) 20000 else 500)
    FrankSa>  print(all.equal(xx, qchisq(pp, df=df, ncp=1), tol = dtol))# TRUE
    FrankSa>  ##or print(mapply(rErr, xx, qchisq(pp, df=df,ncp=1)), digits = 3)
    FrankSa> }

    FrankSa> Where dtol used by all.equal is set to be 5e-10,
    FrankSa> which the intel compiler misses by 1.6e-13.
    FrankSa> This tolerance value seems a bit arbitrary.

indeed.  Replacing 500 by 501 above would already let pass your setup

    FrankSa> The gcc compiled version's passes the test with a 9.3e-11 error.


    FrankSa> I am using the -mp option for the intel compilers, which was recommended
    FrankSa> on this mailing list previously and would make sense given the docs:

(still be interested to hear what happens with the test when you
 do *not* set "-mp")

    FrankSa> Floating Point Optimization Options
    FrankSa> -mp    Maintain floating-point  precision  (disables  some
    FrankSa> optimizations).  The -mp option restricts optimiza-
    FrankSa> tion to maintain declared precision and  to  ensure
    FrankSa> that   floating-point   arithmetic   conforms  more
    FrankSa> closely to the ANSI and IEEE  standards.  For  most
    FrankSa> programs,  specifying this option adversely affects
    FrankSa> performance. If  you  are  not  sure  whether  your
    FrankSa> application  needs  this  option, try compiling and
    FrankSa> running your program both with and  without  it  to
    FrankSa> evaluate the effects on both performance and preci-
    FrankSa> sion.


    FrankSa> Has anyone else encountered this?



From vograno at evafunds.com  Sat Aug 21 00:58:53 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Fri, 20 Aug 2004 15:58:53 -0700
Subject: [R] More precision problems in testing with Intel compilers
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A563D4B1@phost015.EVAFUNDS.intermedia.net>

For what it's worth. I had this very problem, i.e. the diff, this
morning (I reported it to r-devel). I was using gcc, but because my
$CFLAGS env variable was set to some value, the compilation flags were
different from the ones presumably used to produce the Rout. Once I
unset CFLAGS it worked w/o a hitch (thanks to Peter Dalgaard)

The compiler options that lead to the failure were (note that
optimization id turned off):
gcc -D__NO_MATH_INLINES -mieee-fp -DNO_PURE -Wchar-subscripts -Wformat
-Wimplicit -Wreturn-type -Wswitch -Wreorder -Wwrite-strings
-Woverloaded-virtual -Wshadow -Wno-ctor-dtor-privacy -m486 -fPIC
-DOSRELMAJOR=2 -DOSRELMINOR=4

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Martin Maechler
> Sent: Friday, August 20, 2004 3:39 PM
> To: Samuelson, Frank*
> Cc: 'r-help at stat.math.ethz.ch '
> Subject: RE: [R] More precision problems in testing with 
> Intel compilers
> 
> >>>>> "FrankSa" == Samuelson, Frank* <Samuelson>
> >>>>>     on Thu, 19 Aug 2004 16:22:11 -0400 writes:
> 
>     FrankSa> The Intel compiled version also fails the below test:
> 
> here you give the desired output.
> What does your 'Intel compiled R' return instead?
> 
>     >> ###------------ Very big and very small
>     >> umach <- unlist(.Machine)[paste("double.x", 
> c("min","max"), sep='')]
>     >> xmin <- umach[1]
>     >> xmax <- umach[2]
>     >> tx <- unique(outer(-1:1,c(.1,1e-3,1e-7)))# 7 values  (out of 9)
>     >> tx <- unique(sort(c(outer(umach,1+tx))))# 11 values  
> (out of 14)
>     >> tx <- tx[is.finite(tx)] #-- all kept
>     >> (txp <- tx[tx >= 1])#-- Positive exponent -- 4 values
>      [1] 1.617924e+308 1.795895e+308 1.797693e+308 1.797693e+308
>     >> (txn <- tx[tx <        1])#-- Negative exponent -- 7 values
>     [1] 2.002566e-308 2.222849e-308 2.225074e-308 
> 2.225074e-308 2.225074e-308 2.227299e-308 2.447581e-308
> 
>     FrankSa> Does anyone really care about being correct to 1
>     FrankSa> unit of machine precision?  If you do, you have a
>     FrankSa> bad algorithm.  ??
> 
> We have had these tests there for a long time now and haven't 
> heard of failures before..  so this is interesting.
> DIG(7) makes us only look at 7 digits which is less than half 
> machine precision, but then there's cancellation of another 7 
> digits in some of those above which gets in the region of 
> machine precision, (but still leaves a factor of ~= 45).
> 
> Can you upload the full print-test.Rout file somewhere?
> 
> Regards,
> Martin
> 
> 
>     FrankSa> -----Original Message-----
>     FrankSa> From: Samuelson, Frank* [mailto:FWS4 at cdrh.fda.gov] 
>     FrankSa> Sent: Thursday, August 19, 2004 12:11 PM
>     FrankSa> To: 'r-help at stat.math.ethz.ch '
>     FrankSa> Subject: [R] precision problems in testing with 
> Intel compilers
> 
> 
>     FrankSa> I compiled the 1.9.1 src.rpm with the standard 
> gnu tools and it works.
>     FrankSa> I tried compiling the 1.9.1 src.rpm with the 
> Intel 8 C and FORTRAN
>     FrankSa> compilers and it bombs out during the testing phase:
> 
>     FrankSa> comparing 'd-p-q-r-tests.Rout' to 
> './d-p-q-r-tests.Rout.save' ...267c267
>     FrankSa> < df = 0.5[1] "Mean relative  difference: 5.001647e-10"
>     FrankSa> ---
>     >> df = 0.5[1] TRUE
>     FrankSa> make[3]: *** [d-p-q-r-tests.Rout] Error 1
>     FrankSa> make[3]: Leaving directory 
> `/usr/src/redhat/BUILD/R-1.9.1/tests'
>     FrankSa> make[2]: *** [test-Specific] Error 2
>     FrankSa> make[2]: Leaving directory 
> `/usr/src/redhat/BUILD/R-1.9.1/tests'
>     FrankSa> make[1]: *** [test-all-basics] Error 1
>     FrankSa> make[1]: Leaving directory 
> `/usr/src/redhat/BUILD/R-1.9.1/tests'
>     FrankSa> make: *** [check-all] Error 2
>     FrankSa> error: Bad exit status from 
> /var/tmp/rpm-tmp.63044 (%build)
>     FrankSa> ...
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From tlumley at u.washington.edu  Sat Aug 21 01:39:31 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 20 Aug 2004 16:39:31 -0700 (PDT)
Subject: [R] Error messages and C
In-Reply-To: <1093037961.30105.80.camel@iron.libaux.ucsf.edu>
References: <1093026924.30108.65.camel@iron.libaux.ucsf.edu> 
	<Pine.A41.4.58.0408201138370.127282@homer06.u.washington.edu>
	<1093037961.30105.80.camel@iron.libaux.ucsf.edu>
Message-ID: <Pine.A41.4.58.0408201633110.174930@homer10.u.washington.edu>

On Fri, 20 Aug 2004, Ross Boylan wrote:
> The weak references cleaned up are R objects, and my stuff is mostly
> non-R objects.  I see two possible ways to get this to work:
> 1) Create some kind of dummy R object with a finalizer that cleans up my
> C++ objects.

That's the approach used in that web page.

> > If you think of p as a vector of strings (ie char *p[]) then it
> > corresponds to whatever strings you passed in from R.  You can modify
> > these in place, possibly making them shorter. So if you need 80 characters
> > of error message, pass in a string of length at least 80.
>
> If space permits, is this just an ordinary, C-style null terminated
> string?  Or does R have some notion of the string length which is
> unaffected by what I stuff in it?

For an R object there is a notion of length, but the things passed to .C
are copied back with strcpy and the R length is set to the strlen of the C
string.  So they are just null-terminated C strings.

This would cause problems with DUP=FALSE, which is why you aren't allowed
to pass strings with DUP=FALSE.


> > If you want to allocate more memory you need to do this through R (either
> > R_alloc or the Rinternals functions).  Using malloc() and stuffing the
> > result in, say, p[1], will cause a memory leak since you won't be able to
> > find it and free it.
> >
> So if I overwrite p[0], for example, the R garbage collector will know
> to clean up the old string that was pointed to from that location?
>

Yes. In fact the strings are just temporary versions that are copied
back to new R objects and destroyed when you return.

	-thomas



From tlumley at u.washington.edu  Sat Aug 21 01:43:00 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 20 Aug 2004 16:43:00 -0700 (PDT)
Subject: [R] The "Green" Book?
In-Reply-To: <1093038950.41267366c5c4b@webmail.highstream.net>
References: <1093038950.41267366c5c4b@webmail.highstream.net>
Message-ID: <Pine.A41.4.58.0408201639510.174930@homer10.u.washington.edu>

On Fri, 20 Aug 2004, Sam Chapman wrote:

>
> There is no mention of 'Programming with Data: A Guide to the S Language' by
> John M. Chambers. Is this newest ("Green") book also suitable as a reference
> for R? Thank you for your time and attention!
>

Yes. The system implemented in the "methods" package is not identical to
that in the Green Book, but it's pretty similar.

	-thomas



From ross at biostat.ucsf.edu  Sat Aug 21 01:47:47 2004
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Fri, 20 Aug 2004 16:47:47 -0700
Subject: [R] R CMD check testing environment
Message-ID: <1093045667.30107.104.camel@iron.libaux.ucsf.edu>

I can't tell from the docs  ("Writing R Extensions" 1.9.1) exactly what
environment the tests, examples, and vignettes that R CMD check tries to
run are in.

In particular:
1) how do I get the package loaded?
2) how do I access data in the data/ directory?
3) where is the material in the other directories?  (e.g., has inst/
material been installed?  where?)

Apparently (section 1.3) stuff in demo/ is not checked, which seems odd.

By inspecting some other packages, it seems the answer to 1) is that the
package is already loaded, so I don't need to say library(...).  In
particular, I don't need to figure out what lib.loc is.

I have some C code as part of the package, so that (well the .so file)
needs to be loaded too.

2) Others seem to just say data(..), but this doesn't work for me.
I created the data with
save(gold, e2, q2, file="mspath/data/inputs", compress=TRUE)
and later renamed the file to inputs.RData.  (check didn't think the
file counted without the extension).

I have tried to access it in my test script (under tests/) with both
load and data (e.g., data("inputs"), data("inputs.RData"),
data(inputs)).  I get
> data(inputs)
Warning message: 
Data set 'inputs' not found in: data(inputs) 

For that matter, my assumed answer to 1) doesn't seem to be working out,
because when I try to access one of my functions it tells me it can't
find it.  The function name is the same as the package name.

Perhaps the problem is I have inferred answers from \example{}, and the
story for tests/ is different.

Although I'm currently focussed on running a script in tests/, I'd like
to know what the story is for \examples in documentation or vignettes.

Thanks.

P.S. Is there a typical way to produce the .Rout.save file used in
tests/?  What I'm doing is a slightly awkward 2-step process.
-- 
Ross Boylan                                      wk:  (415) 502-4031
530 Parnassus Avenue (Library) rm 115-4          ross at biostat.ucsf.edu
Dept of Epidemiology and Biostatistics           fax: (415) 476-9856
University of California, San Francisco
San Francisco, CA 94143-0840                     hm:  (415) 550-1062



From kjetil at acelerate.com  Sat Aug 21 00:17:48 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Fri, 20 Aug 2004 18:17:48 -0400
Subject: [R] removing last element from a character string
In-Reply-To: <s125e4d0.025@ffdata.setur.fo>
References: <s125e4d0.025@ffdata.setur.fo>
Message-ID: <4126788C.5020908@acelerate.com>

 > test <- "xyz"
 > nchar(test)
[1] 3
 > n <- nchar(test)
 > n <- n-1
 > substr(test,1,n)
[1] "xy"

Kjetil Halvorsen

Luis Rideau Cruz wrote:

>R-help,
>
>Any function to remove last element (or any specific elemnet in the sequence) from a character string?
>
>t<-c("aaab","qqqc")
>
>  
>
>>function(t)  ### remove last character (b and c respectively)
>>    
>>
>
>  
>
>>aaa   qqq
>>    
>>
>
>Thanks
>
>`??._ .??  `??. _ .??  `??._ .??  `??. _ .??  `??._ .??  `??. _ .??  `??..
>Luis Ridao Cruz
>Fiskiranns??knarstovan
>N??at??n 1
>P.O. Box 3051
>FR-110 T??rshavn
>Faroe Islands
>Phone:             +298 353900
>Phone(direct): +298 353912
>Mobile:             +298 580800
>Fax:                 +298 353901
>E-mail:              luisr at frs.fo
>Web:                www.frs.fo
>
>`??._ .??  `??. _ .??  `??._ .??  `??. _ .??  `??._ .??  `??. _ .??  `??..
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>



From kjetil at acelerate.com  Sat Aug 21 00:11:24 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Fri, 20 Aug 2004 18:11:24 -0400
Subject: [R] rgdal under windows?
In-Reply-To: <4125BC07.2010603@lancaster.ac.uk>
References: <Pine.LNX.4.44.0408191957500.3940-100000@reclus.nhh.no>
	<4125BC07.2010603@lancaster.ac.uk>
Message-ID: <4126770C.50308@acelerate.com>

Barry Rowlingson wrote:

> Roger Bivand wrote:
>
>>>> Has anyone had any joy getting the rgdal package to compile under 
>>>> windows?
>>>
>
>> Exactly. The closest anyone has got so far is Hisaji Ono, who used 
>> MSYS (http://www.mingw.org/) to build PROJ.4 and GDAL (GDAL depends 
>> on PROJ.4, PROJ.4 needs a PATH to metadata files for projection and 
>> transformation), and then hand-pasted the paths to the GDAL headers 
>> and library into src/Makevars, running Rcmd INSTALL rgdal at the 
>> Windows command prompt as usual. All of this can be repeated, but is 
>> not portable, and does not suit the very valuable standard binary 
>> package build system for Windows. Roughly:
>
>
>> [points 1 to 5 etc omitted]
>
>
>  At some point the complexity of installing things like this for 
> Windows will cross the complexity of installing Linux...
>
Except it will not cross the complexity of getting linux CD-roms to 
install. Based in a peripherical
country, with no internet connection anywhere close fast enough  to 
actually download linux, have to buy the
CD-roms. Trying to get Suse 9.1 (since I have suse 7.1 on an old 
computer). Amazon
doses'nt send outside north america, so we tried Canadian camelot. They 
covered from the
credit card more than a month ago, but did'nt actually send anything, 
and dos'snt anymore
respond to (angry) emails.

Anybody know something more serious than Camelot?

Kjetil halvorsen

> Baz
>
> PS excepting live-Linux installs like Knoppix.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>



From robert.kruus at utoronto.ca  Sat Aug 21 02:15:16 2004
From: robert.kruus at utoronto.ca (Robert Kruus)
Date: Fri, 20 Aug 2004 20:15:16 -0400
Subject: [R] R-devel and gcc 3.4
In-Reply-To: <20040820182418.5cf09ce1@kruuslt.forestry.utoronto.ca>
References: <644D9337A02FC24689647BF9E48EC39E08ABB870@drm556>
	<x2zn4pwocd.fsf@biostat.ku.dk>
	<20040820163333.03dcf14c@kruuslt.forestry.utoronto.ca>
	<16678.26344.885797.789141@gargle.gargle.HOWL>
	<20040820182418.5cf09ce1@kruuslt.forestry.utoronto.ca>
Message-ID: <20040820201516.5f9d3a9d@kruuslt.forestry.utoronto.ca>

Seems to be something with the Os flag.

Will investigate further.


On Fri, 20 Aug 2004 18:24:18 -0400
Robert Kruus <robert.kruus at utoronto.ca> wrote:

> gcc.  Sorry for the double post.
> 
> On Fri, 20 Aug 2004 23:02:32 +0200 it is rumored
> that Martin Maechler <maechler at stat.math.ethz.ch> wrote:
> 
> > >>>>> "Robert" == Robert Kruus <robert.kruus at utoronto.ca>
> > >>>>>     on Fri, 20 Aug 2004 16:33:33 -0400 writes:
> > 
> >     Robert> Here: CFLAGS=-Os -march=athlon-xp -pipe
> >     Robert> -fomit-frame-pointer -ftracer
> > 
> > is this now gcc (w/o -O) or the intel one?
> > 
> > In any case, your numeric results seem completely off target,
> > not just slightly inaccurate..
> > 
> >     Robert> Will try some other flags to see if that is an
> >     Robert> issue.
> > 
> <---SNIP--->
> 
> -- 
> robert.kruus at utoronto.ca
> If little else, the brain is an educational toy.
> 		-- Tom Robbins
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From spencer.graves at pdf.com  Sat Aug 21 02:47:42 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 20 Aug 2004 20:47:42 -0400
Subject: [R] Partial Least Squares
In-Reply-To: <833E32F61B9F8746878F2A1865BECE6001455460@EXCHCLUSTER01.lj.gnf.org>
References: <833E32F61B9F8746878F2A1865BECE6001455460@EXCHCLUSTER01.lj.gnf.org>
Message-ID: <41269BAE.3030709@pdf.com>

      Did you "read the posting guide! 
http://www.R-project.org/posting-guide.html"?  In particular, did you 
try www.r-project.org -> search -> "R site search" for "partial least 
squares"?  I just got 65 hits for "partial least squares".  Some were 
not relevant, but many were.  I also got 9 hits for "nipals", all of 
which seemed relevant. 

      Also, have you considered "sem" (structural equation modeling)?  
 From what I've heard, partial least squares started out as a solution 
without a clear problem statement, i.e., an algorithm claiming to solve 
a problem but without a clear statement of a probability model for which 
their algorithm produced the (approximate) maximum likelihood or 
Bayesian posterior mode.  Structural equation modeling, by contrast, 
provides the model and problem statement that partial least squares 
seems to try to solve.  I got this impression from reading the PLS 
article in the Encyclopedia of Statistical Sciences. 

      hope this helps.  spencer graves

Lana Schaffer wrote:

>Friends,
>Is there a Partial Least Squares package implemented in R?
>Thanks,
>Lana 
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From edgar at cs.uprm.edu  Sat Aug 21 03:19:27 2004
From: edgar at cs.uprm.edu (Edgar Acuna)
Date: Fri, 20 Aug 2004 21:19:27 -0400 (EDT)
Subject: [R] Partial Least Squares
In-Reply-To: <833E32F61B9F8746878F2A1865BECE6001455460@EXCHCLUSTER01.lj.gnf.org>
Message-ID: <Pine.GSO.4.33.0408202117030.21068-100000@cs.uprm.edu>

The package pls.pcr performs multivariate regression by PLS and PCR
Good luck!
Edgar Acuna
UPR-Mayaguez,Puerto Rico
On Fri, 20 Aug 2004, Lana Schaffer wrote:

> Friends,
> Is there a Partial Least Squares package implemented in R?
> Thanks,
> Lana
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ggrothendieck at myway.com  Sat Aug 21 03:24:32 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 21 Aug 2004 01:24:32 +0000 (UTC)
Subject: [R] Suggestion for posting guide
References: <loom.20040819T151603-570@post.gmane.org>
	<6.1.0.6.2.20040820094859.06421ac0@mailhost.blackmesacapital.com>
	<16678.14549.68383.586906@gargle.gargle.HOWL>
Message-ID: <loom.20040821T031226-33@post.gmane.org>

Martin Maechler <maechler <at> stat.math.ethz.ch> writes:

>     >> dput(x)
> 
> Rereading the posting guide (which *really* is rather too long
> already for beginners), I see that we already have an 'Examples:' section.
> [Have you seen that, Gabor, and not found useful enough?]
> 
> And just below that, we say
> 
>   PGuide> When providing examples, it is best to give an R command that
>   PGuide> constructs the data, as in the matrix() expression above. For
>   PGuide> more complicated data structures, dump("x", file=stdout())
>   PGuide> will print an expression that will recreate the object x.
> 
> I tend to think that
> 	dump("x", file=stdout())
> should probably be replaced with 
>         dput(x)
> 

I plead guilty to missing this but in my defense I think everyone else 
missed it too since I can't ever remember anyone using dump or dput in a 
post.

Also I don't see too many people using the built in data sets for their
data either although perhaps that's changing since I noticed a post with
the iris data set just today or yesterday.  The nice thing about using the
built in data sets for examples is that it makes it easy for the person
asking the question to include data and it makes the post easy to
understand since it replaces a potentially complex expression to generate 
data with something as simple as:

   data(iris)

reducing the mental load of the post.

At any rate, I will think about this some more and I agree with you and
Tony that its important to keep the guide short and its nice that we have
a posting guide so can have discussions like this.



From andy_liaw at merck.com  Sat Aug 21 04:21:40 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 20 Aug 2004 22:21:40 -0400
Subject: [R] Partial Least Squares
Message-ID: <3A822319EB35174CA3714066D590DCD504AF826C@usrymx25.merck.com>

> From: Spencer Graves
> 
>       Did you "read the posting guide! 
> http://www.R-project.org/posting-guide.html"?  In particular, did you 
> try www.r-project.org -> search -> "R site search" for "partial least 
> squares"?  I just got 65 hits for "partial least squares".  Some were 
> not relevant, but many were.  I also got 9 hits for "nipals", all of 
> which seemed relevant. 
> 
>       Also, have you considered "sem" (structural equation 
> modeling)?  
>  From what I've heard, partial least squares started out as a 
> solution 
> without a clear problem statement, i.e., an algorithm 
> claiming to solve 
> a problem but without a clear statement of a probability 
> model for which 
> their algorithm produced the (approximate) maximum likelihood or 
> Bayesian posterior mode.  Structural equation modeling, by contrast, 
> provides the model and problem statement that partial least squares 
> seems to try to solve.  I got this impression from reading the PLS 
> article in the Encyclopedia of Statistical Sciences. 

AFAIU the problem statement for PLS is not that different than that of
canonical correlations.  In fact some had criticized PLS as re-invention of
the canonical correlations.  It can also be seen as a regularized (thus
biased) regression fitting method.

As has been pointed out, the pls.pcr package on CRAN offers PLS (both De
Jong's SIMPLS and the `kernel PLS' algorithms).  There's also the gpls
package that combines PLS and the IWLS iterations in glm.

Best,
Andy
 
>       hope this helps.  spencer graves
> 
> Lana Schaffer wrote:
> 
> >Friends,
> >Is there a Partial Least Squares package implemented in R?
> >Thanks,
> >Lana 
> >
> >	[[alternative HTML version deleted]]
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> >  
> >
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ramasamy at cancer.org.uk  Sat Aug 21 04:49:21 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Sat, 21 Aug 2004 03:49:21 +0100
Subject: [R] Suggestion for posting guide
In-Reply-To: <loom.20040821T031226-33@post.gmane.org>
References: <loom.20040819T151603-570@post.gmane.org>
	<6.1.0.6.2.20040820094859.06421ac0@mailhost.blackmesacapital.com>
	<16678.14549.68383.586906@gargle.gargle.HOWL>
	<loom.20040821T031226-33@post.gmane.org>
Message-ID: <1093056519.3081.38.camel@localhost.localdomain>

One suggestion is to begin the posting guide with a few summary lines
(table of contents).

Here is a good example with an informative summary
http://archive.ncsa.uiuc.edu/General/Internet/WWW/HTMLPrimerAll.html

For R, we can have something like this (modified from the posting guide
according to my order of importance)

General information
* purpose of this posting guide
* more about the list
* acceptable types of question
* selecting an appropriate mailing list
* further resources

Do your homework before posting
* Search the mailing archives (http://maths.newcastle.edu.au/~rking/R/)
* How to find help within R
* Read the FAQ and relevant manuals

When posting a question
* use a meaningful subject line and do not leave the body empty
* use a reproducible example to illustrate your problem
* send your mail as plain text
* start a new thread properly
* ensure your bug is really a bug

Each of the starred can be link that jumps to a more detailed section
below. For example if someone clicks on "How to find help within R",
they might get something like

There are at least 3 ways of getting help in R, all at the R-prompt. If
you know the exact function name, type help("functionname"). Run the
examples in the help section to see if that helps in your understanding.
If you do not know the function name but know some keywords, try
help.search("keyword") and gradually narrowing it down. Typing
help.start() will bring up a help page in an internet browser. Please
note that you will need javascript and java installed and enabled to use
the search features.





On Sat, 2004-08-21 at 02:24, Gabor Grothendieck wrote:
> Martin Maechler <maechler <at> stat.math.ethz.ch> writes:
> 
> >     >> dput(x)
> > 
> > Rereading the posting guide (which *really* is rather too long
> > already for beginners), I see that we already have an 'Examples:' section.
> > [Have you seen that, Gabor, and not found useful enough?]
> > 
> > And just below that, we say
> > 
> >   PGuide> When providing examples, it is best to give an R command that
> >   PGuide> constructs the data, as in the matrix() expression above. For
> >   PGuide> more complicated data structures, dump("x", file=stdout())
> >   PGuide> will print an expression that will recreate the object x.
> > 
> > I tend to think that
> > 	dump("x", file=stdout())
> > should probably be replaced with 
> >         dput(x)
> > 
> 
> I plead guilty to missing this but in my defense I think everyone else 
> missed it too since I can't ever remember anyone using dump or dput in a 
> post.
> 
> Also I don't see too many people using the built in data sets for their
> data either although perhaps that's changing since I noticed a post with
> the iris data set just today or yesterday.  The nice thing about using the
> built in data sets for examples is that it makes it easy for the person
> asking the question to include data and it makes the post easy to
> understand since it replaces a potentially complex expression to generate 
> data with something as simple as:
> 
>    data(iris)
> 
> reducing the mental load of the post.
> 
> At any rate, I will think about this some more and I agree with you and
> Tony that its important to keep the guide short and its nice that we have
> a posting guide so can have discussions like this.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Sat Aug 21 04:51:31 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 20 Aug 2004 22:51:31 -0400
Subject: [R] rgdal under windows?
Message-ID: <3A822319EB35174CA3714066D590DCD504AF826E@usrymx25.merck.com>

> From: Kjetil Brinchmann Halvorsen
>
> Except it will not cross the complexity of getting linux CD-roms to 
> install. Based in a peripherical
> country, with no internet connection anywhere close fast enough  to 
> actually download linux, have to buy the
> CD-roms. Trying to get Suse 9.1 (since I have suse 7.1 on an old 
> computer). Amazon
> doses'nt send outside north america, so we tried Canadian 
> camelot. They 
> covered from the
> credit card more than a month ago, but did'nt actually send anything, 
> and dos'snt anymore
> respond to (angry) emails.
> 
> Anybody know something more serious than Camelot?

Why not try Quantian?  Cheapbytes.com sells one for $4.99US, and will ship
outside US.  The version they have is a bit outdated, but is the last
version that fits on a CD-ROM...

Best,
Andy
 
> Kjetil halvorsen
>



From jc at or.psychology.dal.ca  Sat Aug 21 04:52:34 2004
From: jc at or.psychology.dal.ca (John Christie)
Date: Fri, 20 Aug 2004 23:52:34 -0300
Subject: [R] paired t-test vs pairwise t-test
In-Reply-To: <41266A13.4F40566C@gene.com>
References: <BAY22-F32rGVquEZvuU0007b72b@hotmail.com>
	<41266A13.4F40566C@gene.com>
Message-ID: <26EFB786-F31D-11D8-B9A5-000D93AEDA56@or.psychology.dal.ca>


On Aug 20, 2004, at 6:16 PM, Berton Gunter wrote:

> The fact is that, while certainly desirable,  it is very difficult and
> time-consuming to write the sort of extensively exampled, 
> instructional Help
> files that you desire.

It would be great if one could easily submit a modified help file for 
inclusion in the default distribution.  In fact, maybe we could review 
them on the mailing list and they could be submitted with subjects like
"proposed help for pairwise.t.test" or some such.  Then the community 
could also do the editing and the discourse would be available for 
search.

Maybe I'm missing something but I don't see an easy way to help with 
this ad hoc.  Perhaps someone (hey, maybe me) could volunteer as a 
filter for ad hoc help modifications.  Although, I certainly do not 
feel qualified.



From ripley at stats.ox.ac.uk  Sat Aug 21 06:37:17 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 21 Aug 2004 05:37:17 +0100 (BST)
Subject: [R] R-devel and gcc 3.4
In-Reply-To: <x2zn4pwocd.fsf@biostat.ku.dk>
Message-ID: <Pine.LNX.4.44.0408210535340.29237-100000@gannet.stats>

On 20 Aug 2004, Peter Dalgaard wrote:

> "Samuelson, Frank*" <FWS4 at cdrh.fda.gov> writes:
> 
> > I encountered a similar problem with 
> > 1. Intel compilers.
> > 2. gcc when I turn off optimizations (no -Ox)
> 
> Ooops. I didn't notice #2 the first time around. If we're failing the
> checks with no optimization, then we surely have set the tolerances
> too tight.

He may be, but I am not (i686, x86_64, Solaris 32- and 64-bit, Windows), 
all gcc 3.4.1 with -O2 and without (in most cases).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From lauraholt_983 at hotmail.com  Sat Aug 21 06:40:16 2004
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Fri, 20 Aug 2004 23:40:16 -0500
Subject: [R] more on apply on data frame
Message-ID: <BAY12-F5hss5W3xNbMr0004454b@hotmail.com>

Hi R People:

Several of you pointed out that using "tapply" on a data frame will work on 
the iris data frame.

I'm still having a problem.

The iris data frame has 150 rows, 5 variables.  The first 4 are numeric, 
while the last is a factor, which has the Species names.

I can use tapply for 1 variable at a time:
>tapply(iris[,1],iris[,5],mean)
    setosa versicolor  virginica
     5.006      5.936      6.588
>
but if I try to use this for all of the first 4, I get an error:
>tapply(iris[,1:4],iris[,5],mean)
Error in tapply(iris[, 1:4], iris[, 5], mean) :
        arguments must have same length
>
Any ideas of what I'm doing wrong, please?

Thanks,
Laura Holt
mailto: lauraholt_983 at hotmail.com
R Version 1.9.1
Windows



From ripley at stats.ox.ac.uk  Sat Aug 21 06:58:02 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 21 Aug 2004 05:58:02 +0100 (BST)
Subject: [R] The "Green" Book?
In-Reply-To: <Pine.A41.4.58.0408201639510.174930@homer10.u.washington.edu>
Message-ID: <Pine.LNX.4.44.0408210552170.29237-100000@gannet.stats>

On Fri, 20 Aug 2004, Thomas Lumley wrote:

> On Fri, 20 Aug 2004, Sam Chapman wrote:
> 

[A quote from `An Introduction to R' has been excised here]

> > There is no mention of 'Programming with Data: A Guide to the S Language' by
> > John M. Chambers. Is this newest ("Green") book also suitable as a reference
> > for R? Thank you for your time and attention!
> >
> 
> Yes. The system implemented in the "methods" package is not identical to
> that in the Green Book, but it's pretty similar.

Well, it is suitable as reference for programmers using the "methods"  
package in R, not quite the question asked.  At the level of `An
Introduction to R' it is not really a suitable reference as it has limited
coverage at that level.  (The Green Book itself recommends other books for
end users.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sat Aug 21 07:33:58 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 21 Aug 2004 06:33:58 +0100 (BST)
Subject: [R] suggesting documentation (was paired t-test vs pairwise
	t-test)
In-Reply-To: <26EFB786-F31D-11D8-B9A5-000D93AEDA56@or.psychology.dal.ca>
Message-ID: <Pine.LNX.4.44.0408210606420.29237-100000@gannet.stats>

On Fri, 20 Aug 2004, John Christie wrote:

> On Aug 20, 2004, at 6:16 PM, Berton Gunter wrote:
> 
> > The fact is that, while certainly desirable,  it is very difficult and
> > time-consuming to write the sort of extensively exampled, 
> > instructional Help
> > files that you desire.
> 
> It would be great if one could easily submit a modified help file for 
> inclusion in the default distribution.

You can (for _consideration_ for inclusion, that is).  You send a report
to R-bugs of a documentation problem with either a patch or a complete new
.Rd file.

*However* if we are going to start having help files which attempt to
teach statistics we will need an expert moderator to review them and that
will slow (or even halt) their takeup.  That's not to discourage people
suggesting factual information, in this case a description of the class
"pairwise.htest" and some references.

BTW, at least this help page does describe the value: many do not.

>   In fact, maybe we could review them on the mailing list and they could
> be submitted with subjects like "proposed help for pairwise.t.test" or
> some such.  Then the community could also do the editing and the
> discourse would be available for search.

Not on _this_ mailing list, please (this would be better on R-devel, and
most likely this would need a separate mailing list again).  And please
keep the subject line in track with the subject, folks.

> Maybe I'm missing something but I don't see an easy way to help with 
> this ad hoc.  Perhaps someone (hey, maybe me) could volunteer as a 
> filter for ad hoc help modifications.  Although, I certainly do not 
> feel qualified.

See above.

Behavioural psychology does come into this.  This thread has been
extremely discouraging to those few of us who do work on the
documentation.  Since this it seems unappreciated (and some of the
comments have been a good deal less than appreciative), it will go further
down the list of enjoyable tasks to do for R.  Do remember R is a
volunteer project and the volunteers are in the audience.

Finally, a radical suggestion.  Many respondents are from commercial 
addresses.  Perhaps a group of you would like to put forward a proposal to 
the R Foundation to sponsor an R documentation project.

BDR

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Sat Aug 21 10:46:20 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 21 Aug 2004 10:46:20 +0200
Subject: [R] R-devel and gcc 3.4
In-Reply-To: <Pine.LNX.4.44.0408210535340.29237-100000@gannet.stats>
References: <Pine.LNX.4.44.0408210535340.29237-100000@gannet.stats>
Message-ID: <x2hdqwkher.fsf@biostat.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> On 20 Aug 2004, Peter Dalgaard wrote:
> 
> > "Samuelson, Frank*" <FWS4 at cdrh.fda.gov> writes:
> > 
> > > I encountered a similar problem with 
> > > 1. Intel compilers.
> > > 2. gcc when I turn off optimizations (no -Ox)
> > 
> > Ooops. I didn't notice #2 the first time around. If we're failing the
> > checks with no optimization, then we surely have set the tolerances
> > too tight.
> 
> He may be, but I am not (i686, x86_64, Solaris 32- and 64-bit, Windows), 
> all gcc 3.4.1 with -O2 and without (in most cases).

Yes, I couldn't reproduce it on x86_64 either. What was the exact
platform again? 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From gb at stat.umu.se  Sat Aug 21 16:48:53 2004
From: gb at stat.umu.se (Gran Brostrm)
Date: Sat, 21 Aug 2004 10:48:53 -0400
Subject: [R] more on apply on data frame
In-Reply-To: <BAY12-F5hss5W3xNbMr0004454b@hotmail.com>
References: <BAY12-F5hss5W3xNbMr0004454b@hotmail.com>
Message-ID: <20040821144853.GA21617@stat.umu.se>

On Fri, Aug 20, 2004 at 11:40:16PM -0500, Laura Holt wrote:
> Hi R People:
> 
> Several of you pointed out that using "tapply" on a data frame will work on 
> the iris data frame.
> 
> I'm still having a problem.
> 
> The iris data frame has 150 rows, 5 variables.  The first 4 are numeric, 
> while the last is a factor, which has the Species names.
> 
> I can use tapply for 1 variable at a time:
> >tapply(iris[,1],iris[,5],mean)
>    setosa versicolor  virginica
>     5.006      5.936      6.588
> >
> but if I try to use this for all of the first 4, I get an error:
> >tapply(iris[,1:4],iris[,5],mean)
> Error in tapply(iris[, 1:4], iris[, 5], mean) :
>        arguments must have same length
> >
> Any ideas of what I'm doing wrong, please?

You are not reading the help page:

Usage:

     tapply(X, INDEX, FUN = NULL, ..., simplify = TRUE)

Arguments:

       X: an atomic object, typically a vector.

iris[, 1:4] is a data frame of length 4; iris[, 5] is a vector of length 150.

You probably need to loop over the four first columns and apply tapply(!)
four times, but I'm sure there is a smarter way. Others will tell you.

G??ran
-- 
 G??ran Brostr??m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume?? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume??, Sweden             e-mail: gb at stat.umu.se



From ozric at web.de  Sat Aug 21 11:04:26 2004
From: ozric at web.de (Christian Schulz)
Date: Sat, 21 Aug 2004 11:04:26 +0200
Subject: [R] sweave and post "rpart"
Message-ID: <200408211104.26576.ozric@web.de>

Hi,

have anybody positive experience how 
it is possible get the post output from a
rpart-object in a large sweave file or is only the 
less nicely plot function instead of post possible???

\begin{figure}[htbp]
  \begin{center}
<<fig=true,eps=T>>=
post(dtreeEB, title="Tree",digits=getOption("digits") - 0,use.n=TRUE)
@
    \caption{Tree1}
  \end{center}
\end{figure}


(1) if the rpart object result contain only the root node, i get this
error:
Error:  chunk 21 
Error in if (is.leaf[lson]) left <- list(left = x[lson], right = x[lson],  : 
	missing value where TRUE/FALSE needed

(2) if the rpart object contains further nodes sweave finished fine, but 
the plot-part contains no tree and "corupted" the tex-document, so
a tex2pdf isn't possible.

Many Thanks,
Christian



From bjg at network-theory.co.uk  Sat Aug 21 12:00:43 2004
From: bjg at network-theory.co.uk (Brian Gough)
Date: 21 Aug 2004 11:00:43 +0100
Subject: [R] paired t-test vs pairwise t-test
In-Reply-To: John Christie's message of "Fri, 20 Aug 2004 23:52:34 -0300"
References: <BAY22-F32rGVquEZvuU0007b72b@hotmail.com>
	<41266A13.4F40566C@gene.com>
	<26EFB786-F31D-11D8-B9A5-000D93AEDA56@or.psychology.dal.ca>
Message-ID: <87fz6golo4.fsf@network-theory.co.uk>

John Christie <jc at or.psychology.dal.ca> writes:

> Maybe I'm missing something but I don't see an easy way to help with 
> this ad hoc.  Perhaps someone (hey, maybe me) could volunteer as a 
> filter for ad hoc help modifications.  Although, I certainly do not 
> feel qualified.

I collect any errors reported in the printed versions of the manuals
and will periodically submit suitable patches for them to r-bugs.

If someone finds a section of the manual that needs more description
they should let me know -- I will either submit a patch myself or by
hiring an R expert (as sales permit).

-- 
Brian Gough

Network Theory Ltd,
Publishing the R Reference Manuals --- http://www.network-theory.co.uk/R/



From ripley at stats.ox.ac.uk  Sat Aug 21 12:22:52 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 21 Aug 2004 11:22:52 +0100 (BST)
Subject: [R] sweave and post "rpart"
In-Reply-To: <200408211104.26576.ozric@web.de>
Message-ID: <Pine.GSO.4.31.0408211110120.572-100000@toucan.stats>

On Sat, 21 Aug 2004, Christian Schulz wrote:

> Hi,
>
> have anybody positive experience how
> it is possible get the post output from a
> rpart-object in a large sweave file or is only the
> less nicely plot function instead of post possible???
>
> \begin{figure}[htbp]
>   \begin{center}
> <<fig=true,eps=T>>=
> post(dtreeEB, title="Tree",digits=getOption("digits") - 0,use.n=TRUE)
> @
>     \caption{Tree1}
>   \end{center}
> \end{figure}
>
>
> (1) if the rpart object result contain only the root node, i get this

That's not a tree so an error is probably correct if not so unfriendly a
one.  Perhaps you would like to submit a patch to the *authors*, cc the
maintainer.

> error:
> Error:  chunk 21
> Error in if (is.leaf[lson]) left <- list(left = x[lson], right = x[lson],  :
> 	missing value where TRUE/FALSE needed
>
> (2) if the rpart object contains further nodes sweave finished fine, but
> the plot-part contains no tree and "corupted" the tex-document, so
> a tex2pdf isn't possible.

post.rpart() does not produce graphics output on the open device.
If you trouble to read its help page, you will see the solution in the
Details: section.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Christoph.Scherber at uni-jena.de  Sat Aug 21 12:38:05 2004
From: Christoph.Scherber at uni-jena.de (Christoph Scherber)
Date: Sat, 21 Aug 2004 12:38:05 +0200
Subject: [R] error bars
Message-ID: <4127260D.6020402@uni-jena.de>

Dear all,

is there an easy way to create error bars for the following types of plots:

a) barplots
b) interaction plots

Many other statistics packages (e.g. Statistica) offer very nice 
interaction plots with error bars, and I??d like to be able to do the 
same in R.

Best regards
Christoph



From santosh at igidr.ac.in  Sat Aug 21 12:37:29 2004
From: santosh at igidr.ac.in (Santosh Kumar)
Date: Sat, 21 Aug 2004 16:07:29 +0530 (IST)
Subject: [R] (no subject)
In-Reply-To: <Pine.GSO.4.31.0408211110120.572-100000@toucan.stats>
Message-ID: <Pine.OSF.4.30.0408211604300.115584-100000@brahma.igidr.ac.in>

Hi!

I would like to unsubscribe this mailing list, but I do not know how to do
that. It is my humble request to mailing list administrator to delete my
email address from the list.

Thanks a lot.

with regards;
Santosh



From ligges at statistik.uni-dortmund.de  Sat Aug 21 12:55:52 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 21 Aug 2004 12:55:52 +0200
Subject: [R] slackware packages
In-Reply-To: <1093038522.412671baba72b@webmail.usp.br>
References: <1093038522.412671baba72b@webmail.usp.br>
Message-ID: <41272A38.4070905@statistik.uni-dortmund.de>

Carlos Henrique Grohmann wrote:

> Hello all, 
> I'm new in this list and I'd like to know if someone knows about R packages for
> slackware linux. 

I don't know slackware linux, but what about simply installing the 
packages using install.packages()?

Uwe Ligges

> thanks all.
> 
>



From ligges at statistik.uni-dortmund.de  Sat Aug 21 12:59:42 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 21 Aug 2004 12:59:42 +0200
Subject: [R] unsubscribing R-help (no subject)
In-Reply-To: <Pine.OSF.4.30.0408211604300.115584-100000@brahma.igidr.ac.in>
References: <Pine.OSF.4.30.0408211604300.115584-100000@brahma.igidr.ac.in>
Message-ID: <41272B1E.2040307@statistik.uni-dortmund.de>

Santosh Kumar wrote:

> Hi!
> 
> I would like to unsubscribe this mailing list, but I do not know how to do
> that. It is my humble request to mailing list administrator to delete my
> email address from the list.
> 
> Thanks a lot.
> 
> with regards;
> Santosh

Arrgh! If you were on this list before, have you never noticed the lines 
below all the messages:

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

The second one tells you how to unsubscribe (please use that way in 
order to reduce the load of work of the list administrator - thanks 
again to Martin!).
The third one tells you not to post questions like this on the list, as 
well as using a sensible subject line!

Uwe Ligges



From ggrothendieck at myway.com  Sat Aug 21 14:32:58 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 21 Aug 2004 12:32:58 +0000 (UTC)
Subject: [R] more on apply on data frame
References: <BAY12-F5hss5W3xNbMr0004454b@hotmail.com>
Message-ID: <loom.20040821T142648-561@post.gmane.org>

Laura Holt <lauraholt_983 <at> hotmail.com> writes:

> 
> Hi R People:
> 
> Several of you pointed out that using "tapply" on a data frame will work on 
> the iris data frame.
> 
> I'm still having a problem.
> 
> The iris data frame has 150 rows, 5 variables.  The first 4 are numeric, 
> while the last is a factor, which has the Species names.
> 
> I can use tapply for 1 variable at a time:
> >tapply(iris[,1],iris[,5],mean)
>     setosa versicolor  virginica
>      5.006      5.936      6.588
> >
> but if I try to use this for all of the first 4, I get an error:
> >tapply(iris[,1:4],iris[,5],mean)
> Error in tapply(iris[, 1:4], iris[, 5], mean) :
>         arguments must have same length


This is a job for aggregate:

R> data(iris)
R> aggregate(iris[,1:4], list(Species = iris[,5]), mean)

     Species Sepal.Length Sepal.Width Petal.Length Petal.Width
1     setosa        5.006       3.428        1.462       0.246
2 versicolor        5.936       2.770        4.260       1.326
3  virginica        6.588       2.974        5.552       2.026


The by command would also work using colMeans:

R> by(iris[,1:4], list(Species = iris[,5]), colMeans)

Species: setosa
Sepal.Length  Sepal.Width Petal.Length  Petal.Width 
       5.006        3.428        1.462        0.246 
------------------------------------------------------------ 
Species: versicolor
Sepal.Length  Sepal.Width Petal.Length  Petal.Width 
       5.936        2.770        4.260        1.326 
------------------------------------------------------------ 
Species: virginica
Sepal.Length  Sepal.Width Petal.Length  Petal.Width 
       6.588        2.974        5.552        2.026



From Kevin.Wang at maths.anu.edu.au  Sat Aug 21 14:37:02 2004
From: Kevin.Wang at maths.anu.edu.au (Kevin Wang)
Date: Sat, 21 Aug 2004 22:37:02 +1000 (EST)
Subject: [R] error bars
In-Reply-To: <4127260D.6020402@uni-jena.de>
References: <4127260D.6020402@uni-jena.de>
Message-ID: <Pine.GSO.4.58.0408212235520.26671@yin>

Hi,

On Sat, 21 Aug 2004, Christoph Scherber wrote:

> Dear all,
>
> is there an easy way to create error bars for the following types of plots:
>
> a) barplots
> b) interaction plots

You might want to take a look at Marc Schwartz's article in R NEWS 3/2.

HTH,

Kevin


--------------------------------
Ko-Kang Kevin Wang
PhD Student
Centre for Mathematics and its Applications
Building 27, Room 1004
Mathematical Sciences Institute (MSI)
Australian National University
Canberra, ACT 0200
Australia

Homepage: http://wwwmaths.anu.edu.au/~wangk/
Ph (W): +61-2-6125-2431
Ph (H): +61-2-6125-7407
Ph (M): +61-40-451-8301



From ggrothendieck at myway.com  Sat Aug 21 14:39:31 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 21 Aug 2004 12:39:31 +0000 (UTC)
Subject: [R] paired t-test vs pairwise t-test
References: <BAY22-F32rGVquEZvuU0007b72b@hotmail.com>
	<41266A13.4F40566C@gene.com>
	<26EFB786-F31D-11D8-B9A5-000D93AEDA56@or.psychology.dal.ca>
Message-ID: <loom.20040821T143408-721@post.gmane.org>

John Christie <jc <at> or.psychology.dal.ca> writes:

: 
: On Aug 20, 2004, at 6:16 PM, Berton Gunter wrote:
: 
: > The fact is that, while certainly desirable,  it is very difficult and
: > time-consuming to write the sort of extensively exampled, 
: > instructional Help
: > files that you desire.
: 
: It would be great if one could easily submit a modified help file for 
: inclusion in the default distribution.  

I had mentioned this some time ago but another possibility would be 
to turn the whole help system into a wiki or have a link from each
help page to a supplementary wiki page that users could add additional
comments to.  

This would not require any coordination and could
harness the growing R community making it easy for more people to
contribute.

There is already an unused wiki at:

   http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl?RwikiHome

but having the links might provide a more structured approach and
further encouragement to use the wiki.



From minhan.science at gmail.com  Sat Aug 21 16:44:58 2004
From: minhan.science at gmail.com (Min-Han Tan)
Date: Sat, 21 Aug 2004 10:44:58 -0400
Subject: [R] A troubled state of freedom: generalized linear models where
	number of parameters > number of samples
Message-ID: <7902152a0408210744151a379c@mail.gmail.com>

Good morning,

Thank you all for your help so far. I really appreciate it.

The crux of my problem is that I am generating a generalized linear
model with 1 dependent variable, approximately 50 training samples and
100 parameters (gene levels).

Essentially, if I have 100 genes and 50 samples, this results in
coefficients for the first 49 samples, and NAs for the rest, with an
ultra low residual deviance (usually approx. 10^-27). This seems to
have something to do with the number of degrees of freedom (since as
the number of genes increases up to 49, the number of residual degrees
of freedom drops to 0)

What kind of methods can I use to make sense of this? 

I have a subsequent set of samples to work on to validate the results
of this glm, so I am not sure if overfitting is really a problem.

Background: this is a microarray study, where I have divided the
samples in the training set into 2 groups, and generated a number of
genes to differentiate between both groups. I am going to use the GLM
in a subsequent regression analysis to determine survival. For this
purpose, I need to generate some kind of score for each individual
case using the coefficients of each gene level * gene expression
level.

I am not a statistician (but a clinician) - many apologies if I am not
conveying myself very clearly here!

Thanks. 

Min-Han Tan



From steffen.katzner at mail.gwdg.de  Sat Aug 21 19:10:56 2004
From: steffen.katzner at mail.gwdg.de (Steffen Katzner)
Date: Sat, 21 Aug 2004 19:10:56 +0200
Subject: [R] relative frequencies for hist()
Message-ID: <41278220.7010403@mail.gwdg.de>


I have problems getting a histogram with relative frequencies on the y-axis.

Here is an example data set:

 > a <- c(4.626, 4.627, 4.627, 4.628, 4.629, 4.629, 4.630, 4.631, 4.632, 
             4.632)
 > d = hist(a,freq=F)
 > d$density
[1] 299.9999 100.0000 200.0000 100.0000 100.0000 200.0000

The obtained densities are given by counts/(total n * bin width), with 
bin width being 0.001 in this case.

Is there any way to get a histogram with relative frequencies 
irrespective of bin width, i.e. counts/total n ? It doesn't seem to work 
for truehist() either.

Thanks.
Steffen



From wolski at molgen.mpg.de  Sat Aug 21 20:00:15 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Sat, 21 Aug 2004 20:00:15 +0200
Subject: [R] relative frequencies for hist()
References: <41278220.7010403@mail.gwdg.de>
	<200408211959450561.0287642D@mail.math.fu-berlin.de>
Message-ID: <200408212000150635.0287D9A6@mail.math.fu-berlin.de>

Hi!

d$counts/length(a)


And of course you can if it is what you want.

d$density<-d$counts/length(a)
plot(d,freq=F)


Sincerely Eryk


*********** REPLY SEPARATOR  ***********

On 8/21/2004 at 7:10 PM Steffen Katzner wrote:

>I have problems getting a histogram with relative frequencies on the
>y-axis.
>
>Here is an example data set:
>
> > a <- c(4.626, 4.627, 4.627, 4.628, 4.629, 4.629, 4.630, 4.631, 4.632, 
>             4.632)
> > d = hist(a,freq=F)
> > d$density
>[1] 299.9999 100.0000 200.0000 100.0000 100.0000 200.0000
>
>The obtained densities are given by counts/(total n * bin width), with 
>bin width being 0.001 in this case.
>
>Is there any way to get a histogram with relative frequencies 
>irrespective of bin width, i.e. counts/total n ? It doesn't seem to work 
>for truehist() either.
>
>Thanks.
>Steffen
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



Dipl. bio-chem. Eryk Witold Wolski    @    MPI-Moleculare Genetic   
Ihnestrasse 63-73 14195 Berlin       'v'    
tel: 0049-30-83875219               /   \    
mail: wolski at molgen.mpg.de        ---W-W----    http://www.molgen.mpg.de/~wolski



From msvika at mscc.huji.ac.il  Sat Aug 21 23:51:48 2004
From: msvika at mscc.huji.ac.il (Victoria Landsman)
Date: Sat, 21 Aug 2004 23:51:48 +0200
Subject: [R] Convergence code in nlm function 
Message-ID: <000e01c487c9$0f319360$8600a8c0@home2>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040821/e1d91576/attachment.pl

From ivo_welch-rstat8783 at mailblocks.com  Sat Aug 21 23:50:28 2004
From: ivo_welch-rstat8783 at mailblocks.com (ivo_welch-rstat8783@mailblocks.com)
Date: Sat, 21 Aug 2004 14:50:28 -0700
Subject: [R] loadhistory() in .Rprofile ?
In-Reply-To: <200408211008.i7LA4MSx020306@hypatia.math.ethz.ch>
References: <200408211008.i7LA4MSx020306@hypatia.math.ethz.ch>
Message-ID: <200408212150.i7LLoYt5017944@hypatia.math.ethz.ch>


dear wizards:  my .Rprofile has just one command for testing,
  loadhistory("~/.Rhistory")
but this gives me an error on R startup:
  Error: couldn't find function "loadhistory"
Invoking loadhistory() as the first interactive command works fine; 
incidentally, I believe loadhistory() in the .Rprofile worked in 
earlier or other platform R releases, too.

Is the .Rprofile now loaded before loadhistory()?  if so, this seems to 
make it impossible to set up one big history file across directories 
(save history prior to exit, then reload automatically on entry).  if 
not, I probably screwed something up---again.

regards, /iaw
---
ivo welch

PS: my earlier amd64 gentoo problem was caused by my use of f2c, which 
apparently is not happy on amd64 platforms.  thanks to those who let me 
know the problem.



From s.mcclatchie at iinet.net.au  Sun Aug 22 01:00:11 2004
From: s.mcclatchie at iinet.net.au (Sam McClatchie & Elena Turin)
Date: Sun, 22 Aug 2004 08:30:11 +0930
Subject: [R] review by statistician/  Heiberger and Holland
Message-ID: <41285993.5518.7E2F437@localhost>

System Info:
OS: linux Mandrake 9.1
R Version 1.8.1 (2003-11-21),
GNU Emacs 21.2.93.1 
Browser Mozilla firefox
-----------------

Colleagues

Has anyone written a review of this book? 

Statistical Analysis and Data Display: An Intermediate Course with 
Examples in S-PLUS, R, and SAS.
Richard M. Heiberger and Burt Holland
<http://www.insightful.com/news_events/webcasts/pharm04/heiberg
er.asp>

I can't find a review on the web yet, except for publisher type 
reviews, e.g. 
<http://bookwebpro.kinokuniya.co.jp/booksea.cgi?ISBN=038740270
5&USID=>. 
I'm really interested in a review by a statistician.


I'm particularly interested with how it compares with 
Statistical Computing: An Introduction to Data Analysis using S-Plus
Michael J. Crawley

I've found MJ Crawleys book very useful for both myself and grad 
students, and got our library 
<http://www.sardi.sa.gov.au/aquatic/index.html> to buy it.

Best fishes

Sam
 --
Sam McClatchie & Elena Turin
22 McKenzie St, Coromandel Valley,
Adelaide, South Australia 5051
email <s.mcclatchie at iinet.net.au>
Telephone: (61-8) 8270 1022
cell. 0431 304 497
Research home page 
<http://www.members.iinet.net.au/~s.mcclatchie/>
 
                    /\
       ...>><xX(?> 
                 //// \\\\
                    <?)Xx><<
               /////  \\\\\\
                         ><(((?> 
   >><(((?>   ...>><xX(?>O<?)Xx><<



From ripley at stats.ox.ac.uk  Sun Aug 22 05:48:52 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 22 Aug 2004 04:48:52 +0100 (BST)
Subject: [R] loadhistory() in .Rprofile ?
In-Reply-To: <200408212150.i7LLoYt5017944@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.44.0408220446140.15675-100000@gannet.stats>

> find("loadhistory")
[1] "package:utils"

so see the comments at the top of the NEWS for 1.9.0.  You will need to 
use utils::loadhistory() in .Rprofile, or a loadhook.

On Sat, 21 Aug 2004 ivo_welch-rstat8783 at mailblocks.com wrote:

> 
> dear wizards:  my .Rprofile has just one command for testing,
>   loadhistory("~/.Rhistory")
> but this gives me an error on R startup:
>   Error: couldn't find function "loadhistory"
> Invoking loadhistory() as the first interactive command works fine; 
> incidentally, I believe loadhistory() in the .Rprofile worked in 
> earlier or other platform R releases, too.
> 
> Is the .Rprofile now loaded before loadhistory()?  if so, this seems to 
> make it impossible to set up one big history file across directories 
> (save history prior to exit, then reload automatically on entry).  if 
> not, I probably screwed something up---again.
> 
> regards, /iaw
> ---
> ivo welch
> 
> PS: my earlier amd64 gentoo problem was caused by my use of f2c, which 
> apparently is not happy on amd64 platforms.  thanks to those who let me 
> know the problem.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rn001 at cebas.csic.es  Wed Aug 18 11:20:01 2004
From: rn001 at cebas.csic.es (javier garcia - CEBAS)
Date: Wed, 18 Aug 2004 11:20:01 +0200
Subject: [R] Fwd: strptime() problem? - Resolved
In-Reply-To: <20040817180157.407E5A7AC4@cebas.csic.es>
References: <20040817180157.407E5A7AC4@cebas.csic.es>
Message-ID: <20040822105221.82B2CA7AC3@cebas.csic.es>

Hi Gabor and everybody;

Thanks Gabor, with the alternative step you've told me the problem is 
resolved. Comparing the two procedures:

Extract from the source 'character' data:

> rain$ts[2039:2046]
[1] "25/03/2000 22:00:00 UTC" "25/03/2000 23:00:00 UTC"
[3] "26/03/2000 00:00:00 UTC" "26/03/2000 01:00:00 UTC"
[5] "26/03/2000 02:00:00 UTC" "26/03/2000 03:00:00 UTC"
[7] "26/03/2000 04:00:00 UTC" "26/03/2000 05:00:00 UTC"

Proc 1. The 5th el. of the obtained POSIXct serie goes out of itself
---------------------------------------------------------------------------------
> rain.strptime <- strptime(rain$ts, format="%d/%m/%Y %H:%M:%S")
> rain.strptime.ct <- as.POSIXct(rain.strptime,tz="GMT")
> rain.strptime.ct[2039:2046]
[1] "2000-03-25 23:00:00 CET"  "2000-03-26 00:00:00 CET"
[3] "2000-03-26 01:00:00 CET"  "2000-03-26 03:00:00 CEST"
[5] "2000-03-26 05:00:00 CEST" "2000-03-26 05:00:00 CEST"
[7] "2000-03-26 06:00:00 CEST" "2000-03-26 07:00:00 CEST"
> format(rain.strptime.ct[2039:2046],tz="GMT",usetz=TRUE)
[1] "2000-03-25 22:00:00 GMT" "2000-03-25 23:00:00 GMT"
[3] "2000-03-26 00:00:00 GMT" "2000-03-26 01:00:00 GMT"
[5] "2000-03-26 03:00:00 GMT" "2000-03-26 03:00:00 GMT"
[7] "2000-03-26 04:00:00 GMT" "2000-03-26 05:00:00 GMT"
> as.numeric(rain.strptime.ct[2039:2046])
[1] 954021600 954025200 954028800 954032400 954039600 954039600 954043200
[8] 954046800

Proc 2. The obtained POSIXct serie is continuous, and it seems OK for me.
---------------------------------------------------------------------------------
rain.chron <- 
chron(substring(rain$ts,1,10),substring(rain$ts,12,19),format=c("d/m/y","h:m:s"))
rain.chron.ct <- as.POSIXct(rain.chron,tz="GMT")
> rain.chron.ct[2039:2046]
[1] "2000-03-25 23:00:00 CET"  "2000-03-26 00:00:00 CET"
[3] "2000-03-26 01:00:00 CET"  "2000-03-26 03:00:00 CEST"
[5] "2000-03-26 04:00:00 CEST" "2000-03-26 05:00:00 CEST"
[7] "2000-03-26 06:00:00 CEST" "2000-03-26 07:00:00 CEST"
> format(lluvia.chron.ct[2039:2046],tz="GMT",usetz=TRUE)
[1] "2000-03-25 22:00:00 GMT" "2000-03-25 23:00:00 GMT"
[3] "2000-03-26 00:00:00 GMT" "2000-03-26 01:00:00 GMT"
[5] "2000-03-26 02:00:00 GMT" "2000-03-26 03:00:00 GMT"
[7] "2000-03-26 04:00:00 GMT" "2000-03-26 05:00:00 GMT"
> as.numeric(rain.chron.ct[2039:2046])
[1] 954021600 954025200 954028800 954032400 954036000 954039600 954043200
[8] 954046800


For me the problem is resolved by mean of package 'chron'. And it's as direct 
as the use of the first procedure. Just as a comment, I think that for a 
proper behaviour, the first procedure should give the same result. Shouldn't 
it?

Thanks all and best regards.
-------
-------

El Mar 17 Ago 2004 20:02, javier garcia - CEBAS escribi??:
> ----------  Mensaje reenviado  ----------
>
> Subject: RE: [R] Fwd: strptime() problem?
> Date: Tue, 17 Aug 2004 11:57:46 -0400 (EDT)
> From: "Gabor Grothendieck" <ggrothendieck at myway.com>
> To: rn001 at cebas.csic.es, ripley at stats.ox.ac.uk, r-help at stat.math.ethz.ch
>
> I am in a different time zone, EDT, on Windows XP and can't
> replicate this but you might try reading the latest R News
> article on dates and times for some ideas, viz. page 32 of:
>
>    http://cran.r-project.org/doc/Rnews/Rnews_2004-1.pd
>
> In particular, try converting them to chron and then doing
> your manipulations in chron or else convert them from chron to
> POSIXct:
>
>    require(chron)
>    r.asc <- raincida$ts
>    r.chron <- chron(substring(r.asc, 1, 10),
>              substring(r.asc, 12, 19), format = c("d/m/y", "h:m:s"))
>
>    r.ct <- as.POSIXct(r.chron)
>    format(r.ct, tz="GMT")   # display POSIXct in GMT
>
> Date:   	Tue, 17 Aug 2004 16:25:12 +0200
> From:   	javier garcia - CEBAS <rn001 at cebas.csic.es>
> To:   	Prof Brian Ripley <ripley at stats.ox.ac.uk>,
> <r-help at stat.math.ethz.ch> Subject:   	[R] Fwd: strptime() problem?
>
> Hi all;
> I've already send a similar e-mail to the list and Prof. Brian Ripley
> answered me but my doubts remain unresolved. Thanks for the clarification,
> but perhaps I wasn't clear enough in posting my questions.
>
> I've got a postgres database which I read into R. The first column is
> Timestamp with timezone, and my data are already in UTC format. An
> 'printed' extract of R character column, resulting from the timestamptz
> field is:
>
> raincida$ts:
>
> [2039] "25/03/2000 22:00:00 UTC" "25/03/2000 23:00:00 UTC"
> [2041] "26/03/2000 00:00:00 UTC" "26/03/2000 01:00:00 UTC"
> [2043] "26/03/2000 02:00:00 UTC" "26/03/2000 03:00:00 UTC"
> [2045] "26/03/2000 04:00:00 UTC" "26/03/2000 05:00:00 UTC"
>
> #And I need to convert this character column into POSIXct, for eventual
> work. #As I can see in the documentation, the process is to use strptime(),
> what #creates an object POSIXlt and doesn't allow to specify that the time
> zone of #the data is already UTC; followed by as.POSIXct()
>
> > lluvia.strptime <- strptime(raincida$ts, format="%d/%m/%Y %H:%M:%S")
> > lluvia.strptime.POSIXct <- as.POSIXct(lluvia.strptime,tz="GMT")
>
> A "printed" extract is:
>
> [2039] "2000-03-25 22:00:00 GMT" "2000-03-25 23:00:00 GMT"
> [2041] "2000-03-26 00:00:00 GMT" "2000-03-26 01:00:00 GMT"
> [2043] "2000-03-26 03:00:00 GMT" "2000-03-26 03:00:00 GMT"
> [2045] "2000-03-26 04:00:00 GMT" "2000-03-26 05:00:00 GMT"
>
> As we can see, elements [2043] differ. Shouldn't they be similar as the
> rest of the other shown elements? I thought this was a bug, but it seems
> that I've got and conceptual error.(?). This happens several times in my
> data, and produces eventual errors.
>
> Please, how could I resolved this?
>
> Thanks all, and best regards,
>
> Javier G.
>
>
>
> _______________________________________________
> No banners. No pop-ups. No kidding.

>
> -------------------------------------------------------



From rpeng at jhsph.edu  Sun Aug 22 15:49:28 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Sun, 22 Aug 2004 09:49:28 -0400
Subject: [R] loadhistory() in .Rprofile ?
In-Reply-To: <200408212150.i7LLoYt5017944@hypatia.math.ethz.ch>
References: <200408211008.i7LA4MSx020306@hypatia.math.ethz.ch>
	<200408212150.i7LLoYt5017944@hypatia.math.ethz.ch>
Message-ID: <4128A468.1060504@jhsph.edu>

Take a look at ?setHook.

-roger

ivo_welch-rstat8783 at mailblocks.com wrote:
> 
> dear wizards:  my .Rprofile has just one command for testing,
>  loadhistory("~/.Rhistory")
> but this gives me an error on R startup:
>  Error: couldn't find function "loadhistory"
> Invoking loadhistory() as the first interactive command works fine; 
> incidentally, I believe loadhistory() in the .Rprofile worked in earlier 
> or other platform R releases, too.
> 
> Is the .Rprofile now loaded before loadhistory()?  if so, this seems to 
> make it impossible to set up one big history file across directories 
> (save history prior to exit, then reload automatically on entry).  if 
> not, I probably screwed something up---again.
> 
> regards, /iaw
> ---
> ivo welch
> 
> PS: my earlier amd64 gentoo problem was caused by my use of f2c, which 
> apparently is not happy on amd64 platforms.  thanks to those who let me 
> know the problem.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ggrothendieck at myway.com  Sun Aug 22 17:08:19 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 22 Aug 2004 11:08:19 -0400 (EDT)
Subject: [R] Fwd: strptime() problem? - Resolved
Message-ID: <20040822150819.8EE6839BB@mprdmxin.myway.com>


Hi,

Unfortunately, in my time zone I cannot reproduce your problem.
For example,

> rain
[1] "25/03/2000 22:00:00 UTC" "25/03/2000 23:00:00 UTC"
[3] "26/03/2000 00:00:00 UTC" "26/03/2000 01:00:00 UTC"
[5] "26/03/2000 02:00:00 UTC" "26/03/2000 03:00:00 UTC"
[7] "26/03/2000 04:00:00 UTC" "26/03/2000 05:00:00 UTC"
> str(rain)
 chr [1:8] "25/03/2000 22:00:00 UTC" "25/03/2000 23:00:00 UTC" ...
> rain.lt <- strptime(rain, format="%d/%m/%Y %H:%M:%S")
> rain.lt
[1] "2000-03-25 22:00:00" "2000-03-25 23:00:00" "2000-03-26 00:00:00"
[4] "2000-03-26 01:00:00" "2000-03-26 02:00:00" "2000-03-26 03:00:00"
[7] "2000-03-26 04:00:00" "2000-03-26 05:00:00"
> rain.ct <- as.POSIXct(rain.lt,tz="GMT")
> rain.ct
[1] "2000-03-25 22:00:00 GMT" "2000-03-25 23:00:00 GMT"
[3] "2000-03-26 00:00:00 GMT" "2000-03-26 01:00:00 GMT"
[5] "2000-03-26 02:00:00 GMT" "2000-03-26 03:00:00 GMT"
[7] "2000-03-26 04:00:00 GMT" "2000-03-26 05:00:00 GMT"
> R.version.string  # Windows XP
[1] "R version 1.9.1, 2004-08-03"

Without being able to reproduce it, its difficult for me to 
figure out what is wrong.  It seems to be ignoring the tz=
on the conversion to POSIXct.  I mentioned that I noticed
that it sometimes seems to ignore this parameter in my
recent R News article but have never attempted to track down
this behavior further.

What I can say is that, in gereral,
I have found that I wasted a lot of time on subtle aspects
related to time zones even when my underlying problem actually
had nothing to do with time zones so in order to avoid all
those difficulties I converted all my software from POSIXt 
to chron (which does not use time zones in the first place
and so cannot run into suchh problems) and I provided a table 
in the latest R News showing the translation of some idioms.  
This solved everything for me.


Date:   	Wed, 18 Aug 2004 11:20:01 +0200
From:   	javier garcia - CEBAS <rn001 at cebas.csic.es>
To:   	Gabor Grothendieck <ggrothendieck at myway.com>, <r-help at stat.math.ethz.ch>
Subject:   	RE: [R] Fwd: strptime() problem? - Resolved

Hi Gabor and everybody;

Thanks Gabor, with the alternative step you've told me the problem is
resolved. Comparing the two procedures:

Extract from the source 'character' data:

> rain$ts[2039:2046]
[1] "25/03/2000 22:00:00 UTC" "25/03/2000 23:00:00 UTC"
[3] "26/03/2000 00:00:00 UTC" "26/03/2000 01:00:00 UTC"
[5] "26/03/2000 02:00:00 UTC" "26/03/2000 03:00:00 UTC"
[7] "26/03/2000 04:00:00 UTC" "26/03/2000 05:00:00 UTC"

Proc 1. The 5th el. of the obtained POSIXct serie goes out of itself
---------------------------------------------------------------------------------
> rain.strptime <- strptime(rain$ts, format="%d/%m/%Y %H:%M:%S")
> rain.strptime.ct <- as.POSIXct(rain.strptime,tz="GMT")
> rain.strptime.ct[2039:2046]
[1] "2000-03-25 23:00:00 CET" "2000-03-26 00:00:00 CET"
[3] "2000-03-26 01:00:00 CET" "2000-03-26 03:00:00 CEST"
[5] "2000-03-26 05:00:00 CEST" "2000-03-26 05:00:00 CEST"
[7] "2000-03-26 06:00:00 CEST" "2000-03-26 07:00:00 CEST"
> format(rain.strptime.ct[2039:2046],tz="GMT",usetz=TRUE)
[1] "2000-03-25 22:00:00 GMT" "2000-03-25 23:00:00 GMT"
[3] "2000-03-26 00:00:00 GMT" "2000-03-26 01:00:00 GMT"
[5] "2000-03-26 03:00:00 GMT" "2000-03-26 03:00:00 GMT"
[7] "2000-03-26 04:00:00 GMT" "2000-03-26 05:00:00 GMT"
> as.numeric(rain.strptime.ct[2039:2046])
[1] 954021600 954025200 954028800 954032400 954039600 954039600 954043200
[8] 954046800

Proc 2. The obtained POSIXct serie is continuous, and it seems OK for me.
---------------------------------------------------------------------------------
rain.chron <-
chron(substring(rain$ts,1,10),substring(rain$ts,12,19),format=c("d/m/y","h:m:s"))
rain.chron.ct <- as.POSIXct(rain.chron,tz="GMT")
> rain.chron.ct[2039:2046]
[1] "2000-03-25 23:00:00 CET" "2000-03-26 00:00:00 CET"
[3] "2000-03-26 01:00:00 CET" "2000-03-26 03:00:00 CEST"
[5] "2000-03-26 04:00:00 CEST" "2000-03-26 05:00:00 CEST"
[7] "2000-03-26 06:00:00 CEST" "2000-03-26 07:00:00 CEST"
> format(lluvia.chron.ct[2039:2046],tz="GMT",usetz=TRUE)
[1] "2000-03-25 22:00:00 GMT" "2000-03-25 23:00:00 GMT"
[3] "2000-03-26 00:00:00 GMT" "2000-03-26 01:00:00 GMT"
[5] "2000-03-26 02:00:00 GMT" "2000-03-26 03:00:00 GMT"
[7] "2000-03-26 04:00:00 GMT" "2000-03-26 05:00:00 GMT"
> as.numeric(rain.chron.ct[2039:2046])
[1] 954021600 954025200 954028800 954032400 954036000 954039600 954043200
[8] 954046800


For me the problem is resolved by mean of package 'chron'. And it's as direct
as the use of the first procedure. Just as a comment, I think that for a
proper behaviour, the first procedure should give the same result. Shouldn't
it?

Thanks all and best regards.
-------
-------

El Mar 17 Ago 2004 20:02, javier garcia - CEBAS escribi:
> ---------- Mensaje reenviado ----------
>
> Subject: RE: [R] Fwd: strptime() problem?
> Date: Tue, 17 Aug 2004 11:57:46 -0400 (EDT)
> From: "Gabor Grothendieck" <ggrothendieck at myway.com>
> To: rn001 at cebas.csic.es, ripley at stats.ox.ac.uk, r-help at stat.math.ethz.ch
>
> I am in a different time zone, EDT, on Windows XP and can't
> replicate this but you might try reading the latest R News
> article on dates and times for some ideas, viz. page 32 of:
>
> http://cran.r-project.org/doc/Rnews/Rnews_2004-1.pd
>
> In particular, try converting them to chron and then doing
> your manipulations in chron or else convert them from chron to
> POSIXct:
>
> require(chron)
> r.asc <- raincida$ts
> r.chron <- chron(substring(r.asc, 1, 10),
> substring(r.asc, 12, 19), format = c("d/m/y", "h:m:s"))
>
> r.ct <- as.POSIXct(r.chron)
> format(r.ct, tz="GMT") # display POSIXct in GMT
>
> Date:      Tue, 17 Aug 2004 16:25:12 +0200
> From:      javier garcia - CEBAS <rn001 at cebas.csic.es>
> To:      Prof Brian Ripley <ripley at stats.ox.ac.uk>,
> <r-help at stat.math.ethz.ch> Subject:      [R] Fwd: strptime() problem?
>
> Hi all;
> I've already send a similar e-mail to the list and Prof. Brian Ripley
> answered me but my doubts remain unresolved. Thanks for the clarification,
> but perhaps I wasn't clear enough in posting my questions.
>
> I've got a postgres database which I read into R. The first column is
> Timestamp with timezone, and my data are already in UTC format. An
> 'printed' extract of R character column, resulting from the timestamptz
> field is:
>
> raincida$ts:
>
> [2039] "25/03/2000 22:00:00 UTC" "25/03/2000 23:00:00 UTC"
> [2041] "26/03/2000 00:00:00 UTC" "26/03/2000 01:00:00 UTC"
> [2043] "26/03/2000 02:00:00 UTC" "26/03/2000 03:00:00 UTC"
> [2045] "26/03/2000 04:00:00 UTC" "26/03/2000 05:00:00 UTC"
>
> #And I need to convert this character column into POSIXct, for eventual
> work. #As I can see in the documentation, the process is to use strptime(),
> what #creates an object POSIXlt and doesn't allow to specify that the time
> zone of #the data is already UTC; followed by as.POSIXct()
>
> > lluvia.strptime <- strptime(raincida$ts, format="%d/%m/%Y %H:%M:%S")
> > lluvia.strptime.POSIXct <- as.POSIXct(lluvia.strptime,tz="GMT")
>
> A "printed" extract is:
>
> [2039] "2000-03-25 22:00:00 GMT" "2000-03-25 23:00:00 GMT"
> [2041] "2000-03-26 00:00:00 GMT" "2000-03-26 01:00:00 GMT"
> [2043] "2000-03-26 03:00:00 GMT" "2000-03-26 03:00:00 GMT"
> [2045] "2000-03-26 04:00:00 GMT" "2000-03-26 05:00:00 GMT"
>
> As we can see, elements [2043] differ. Shouldn't they be similar as the
> rest of the other shown elements? I thought this was a bug, but it seems
> that I've got and conceptual error.(?). This happens several times in my
> data, and produces eventual errors.
>
> Please, how could I resolved this?
>
> Thanks all, and best regards,
>
> Javier G.



From wdmccoy at geo.umass.edu  Sun Aug 22 18:49:12 2004
From: wdmccoy at geo.umass.edu (William D. McCoy)
Date: Sun, 22 Aug 2004 12:49:12 -0400 (EDT)
Subject: [R] no response from rsync.r-project.org
Message-ID: <200408221649.MAA01921@aeolus.geo.umass.edu>

Last night and again today I am getting no response from
rsync.r-project.org (an alias for bates4.stat.wisc.edu according to
host) when I try to update my R source.  Is there an alternate host
from which I can update using rsync?

-- 
William D. McCoy
Geosciences
University of Massachusetts
Amherst, MA  01003



From ligges at statistik.uni-dortmund.de  Sun Aug 22 18:58:56 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 22 Aug 2004 18:58:56 +0200
Subject: [R] R CMD check testing environment
In-Reply-To: <1093045667.30107.104.camel@iron.libaux.ucsf.edu>
References: <1093045667.30107.104.camel@iron.libaux.ucsf.edu>
Message-ID: <4128D0D0.8040304@statistik.uni-dortmund.de>

Ross Boylan wrote:
> I can't tell from the docs  ("Writing R Extensions" 1.9.1) exactly what
> environment the tests, examples, and vignettes that R CMD check tries to
> run are in.
> 
> In particular:
> 1) how do I get the package loaded?
> 2) how do I access data in the data/ directory?
> 3) where is the material in the other directories?  (e.g., has inst/
> material been installed?  where?)
> 
> Apparently (section 1.3) stuff in demo/ is not checked, which seems odd.
> 
> By inspecting some other packages, it seems the answer to 1) is that the
> package is already loaded, so I don't need to say library(...).  In
> particular, I don't need to figure out what lib.loc is.
> 
> I have some C code as part of the package, so that (well the .so file)
> needs to be loaded too.

Yes. You don't need library() in the the help pages' exmaples, but you 
need it in tests.


> 2) Others seem to just say data(..), but this doesn't work for me.
> I created the data with
> save(gold, e2, q2, file="mspath/data/inputs", compress=TRUE)
> and later renamed the file to inputs.RData.  (check didn't think the
> file counted without the extension).

Indeed, the manuals tells us that saved images have to be called either
*.RData (note the capitalization) or *.rda.


> I have tried to access it in my test script (under tests/) with both
> load and data (e.g., data("inputs"), data("inputs.RData"),
> data(inputs)).  I get
> 
>>data(inputs)
> 
> Warning message: 
> Data set 'inputs' not found in: data(inputs) 

Works for me as "Mytests.R" in "Mypackage":

  library(Mypackage)
  data(MydataInMypackage)
  print(MydataInMypackage)



> For that matter, my assumed answer to 1) doesn't seem to be working out,
> because when I try to access one of my functions it tells me it can't
> find it.  The function name is the same as the package name.
> 
> Perhaps the problem is I have inferred answers from \example{}, and the
> story for tests/ is different.

Right, see above.


> Although I'm currently focussed on running a script in tests/, I'd like
> to know what the story is for \examples in documentation or vignettes.

You won't need library() in the examples ...


> Thanks.
> 
> P.S. Is there a typical way to produce the .Rout.save file used in
> tests/?  What I'm doing is a slightly awkward 2-step process.


Running R CMD check once, looking whether the Rout looks fine (this 
should be the step that takes most of the time, and R can't do it for 
you), rename the file it, finished.


Uwe Ligges



From ligges at statistik.uni-dortmund.de  Sun Aug 22 19:14:08 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 22 Aug 2004 19:14:08 +0200
Subject: [R] no response from rsync.r-project.org
In-Reply-To: <200408221649.MAA01921@aeolus.geo.umass.edu>
References: <200408221649.MAA01921@aeolus.geo.umass.edu>
Message-ID: <4128D460.10300@statistik.uni-dortmund.de>

William D. McCoy wrote:

> Last night and again today I am getting no response from
> rsync.r-project.org (an alias for bates4.stat.wisc.edu according to
> host) when I try to update my R source.  Is there an alternate host
> from which I can update using rsync?
> 

See Brian Ripley's message from yesterday on r-devel:
https://stat.ethz.ch/pipermail/r-devel/2004-August/030450.html

Uwe Ligges



From mi2kelgrum at yahoo.com  Sun Aug 22 19:58:04 2004
From: mi2kelgrum at yahoo.com (Mikkel Grum)
Date: Sun, 22 Aug 2004 10:58:04 -0700 (PDT)
Subject: [R] data files for packages
Message-ID: <20040822175804.89992.qmail@web60210.mail.yahoo.com>

Dear R-helpers,

This must be really simple and clearly expained
somewhere, but I can't find it. I'm writing an
R-package and want to create data sets for the
examples.  How do I save them in the format needed for
a package?

cheers,
Mikkel



From mi2kelgrum at yahoo.com  Sun Aug 22 19:59:58 2004
From: mi2kelgrum at yahoo.com (Mikkel Grum)
Date: Sun, 22 Aug 2004 10:59:58 -0700 (PDT)
Subject: [R] latitude longitude data
Message-ID: <20040822175958.15733.qmail@web60205.mail.yahoo.com>

Dear R-helpers,

I get GPS readings with bug counts (bugs meaning
insects in this case) made along rows in crop fields
and use these to make maps of bug distribution.  The
GPS readings are not quite accurate enough for my
purpose, so since I know what row each reading is made
in, I adjust the latitudinal coordinate using:

    grd<-lm(lat~lon+Row,data)
    data$lat<-predict(grd[,c("lon","Row")])

which adjusts the latitude pretty well when the rows
run East-West, but not at all when the rows run
North-South, and it doesn't adjust the longitude at
all.

Is there a better approach I could use to adjust both
longitude and latitude onto the nearest point in the
row, whatever the direction of the rows? In other
words, move the point onto the row in a direction that
is perpendicular to the row?

cheers,
Mikkel



From ggrothendieck at myway.com  Sun Aug 22 21:01:22 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 22 Aug 2004 19:01:22 +0000 (UTC)
Subject: [R] latitude longitude data
References: <20040822175958.15733.qmail@web60205.mail.yahoo.com>
Message-ID: <loom.20040822T205206-630@post.gmane.org>

Mikkel Grum <mi2kelgrum <at> yahoo.com> writes:

: 
: Dear R-helpers,
: 
: I get GPS readings with bug counts (bugs meaning
: insects in this case) made along rows in crop fields
: and use these to make maps of bug distribution.  The
: GPS readings are not quite accurate enough for my
: purpose, so since I know what row each reading is made
: in, I adjust the latitudinal coordinate using:
: 
:     grd<-lm(lat~lon+Row,data)
:     data$lat<-predict(grd[,c("lon","Row")])
: 
: which adjusts the latitude pretty well when the rows
: run East-West, but not at all when the rows run
: North-South, and it doesn't adjust the longitude at
: all.
: 
: Is there a better approach I could use to adjust both
: longitude and latitude onto the nearest point in the
: row, whatever the direction of the rows? In other
: words, move the point onto the row in a direction that
: is perpendicular to the row?

If you have the lonlats spaced along the row in a matrix called actual
and the gps lonlats in matrix called gps then its just a matter of finding
the nearest actual row to each gps row.  rdist.earth from fields will do the 
lonlat distance calculation:

require(fields)

# test data
gps <- ozone$lon.lat[seq(3),]
actual <- ozone$lon.lat[-seq(3),]

# use rdist.earth from fields to get distance from each gps to each actual
# and find least
f <- function(i) {
	dst <- rdist.earth(gps[i,,drop=F], actual)
	actual[order(dst)[1],]
}
t(sapply(1:nrow(gps), f))



From ripley at stats.ox.ac.uk  Sun Aug 22 21:34:28 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 22 Aug 2004 20:34:28 +0100 (BST)
Subject: [R] data files for packages
In-Reply-To: <20040822175804.89992.qmail@web60210.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0408222030280.3603-100000@gannet.stats>

On Sun, 22 Aug 2004, Mikkel Grum wrote:

> This must be really simple and clearly expained
> somewhere, but I can't find it. I'm writing an
> R-package and want to create data sets for the
> examples.  How do I save them in the format needed for
> a package?

It's in `Writing R Extensions' (in the subsection entitled `Package
subdirectories') and also in ?data.  There are several possible formats.

If the question was how to produce .rda files, use for dataset foo

    save(foo, file="foo.rda", compress=TRUE)


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tlumley at u.washington.edu  Sun Aug 22 22:10:38 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Sun, 22 Aug 2004 13:10:38 -0700 (PDT)
Subject: [R] Partial Least Squares
In-Reply-To: <41269BAE.3030709@pdf.com>
References: <833E32F61B9F8746878F2A1865BECE6001455460@EXCHCLUSTER01.lj.gnf.org>
	<41269BAE.3030709@pdf.com>
Message-ID: <Pine.A41.4.58.0408221302160.238618@homer09.u.washington.edu>

On Fri, 20 Aug 2004, Spencer Graves wrote:
>       Also, have you considered "sem" (structural equation modeling)?
>  From what I've heard, partial least squares started out as a solution
> without a clear problem statement, i.e., an algorithm claiming to solve
> a problem but without a clear statement of a probability model for which
> their algorithm produced the (approximate) maximum likelihood or
> Bayesian posterior mode.  Structural equation modeling, by contrast,
> provides the model and problem statement that partial least squares
> seems to try to solve.  I got this impression from reading the PLS
> article in the Encyclopedia of Statistical Sciences.
>

I don't think that's quite right.  If you look at how PLS is used and who
uses it, it is in situations where maximum likelihood estimation often is
not possible or not reliable because of the dimension of the predictor
variables.  It is a regularised version of SEM, which can be useful even
if the penalty term is not clearly specified.

	-thomas



From tlumley at u.washington.edu  Sun Aug 22 22:17:20 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Sun, 22 Aug 2004 13:17:20 -0700 (PDT)
Subject: [R] loadhistory() in .Rprofile ?
In-Reply-To: <200408212150.i7LLoYt5017944@hypatia.math.ethz.ch>
References: <200408211008.i7LA4MSx020306@hypatia.math.ethz.ch>
	<200408212150.i7LLoYt5017944@hypatia.math.ethz.ch>
Message-ID: <Pine.A41.4.58.0408221316580.238618@homer09.u.washington.edu>


This has recently been added to the FAQ.

	-thomas

On Sat, 21 Aug 2004 ivo_welch-rstat8783 at mailblocks.com wrote:

>
> dear wizards:  my .Rprofile has just one command for testing,
>   loadhistory("~/.Rhistory")
> but this gives me an error on R startup:
>   Error: couldn't find function "loadhistory"
> Invoking loadhistory() as the first interactive command works fine;
> incidentally, I believe loadhistory() in the .Rprofile worked in
> earlier or other platform R releases, too.
>
> Is the .Rprofile now loaded before loadhistory()?  if so, this seems to
> make it impossible to set up one big history file across directories
> (save history prior to exit, then reload automatically on entry).  if
> not, I probably screwed something up---again.
>
> regards, /iaw
> ---
> ivo welch
>
> PS: my earlier amd64 gentoo problem was caused by my use of f2c, which
> apparently is not happy on amd64 platforms.  thanks to those who let me
> know the problem.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From ivo_welch at mailblocks.com  Sun Aug 22 22:35:03 2004
From: ivo_welch at mailblocks.com (ivo welch)
Date: Sun, 22 Aug 2004 16:35:03 -0400
Subject: [R] loadhistory() in .Rprofile ?
In-Reply-To: <Pine.LNX.4.44.0408220446140.15675-100000@gannet.stats>
References: <Pine.LNX.4.44.0408220446140.15675-100000@gannet.stats>
Message-ID: <41290377.2030006@mailblocks.com>



Prof Brian Ripley wrote:

>>find("loadhistory")
>>    
>>
>[1] "package:utils"
>
>so see the comments at the top of the NEWS for 1.9.0.  You will need to 
>use utils::loadhistory() in .Rprofile, or a loadhook.
>  
>
thank you all.  sheesh, it broke in 1.9.0, not 1.9.1---I looked through 
the 1.9.1 release notes, and then tried to google for it, too.  it had 
not occurred to me that I had skipped a release.  thank you also for 
adding it to the faq.

regards,

/ivo



From spencer.graves at pdf.com  Sun Aug 22 23:28:29 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 22 Aug 2004 17:28:29 -0400
Subject: [R] Partial Least Squares
In-Reply-To: <Pine.A41.4.58.0408221302160.238618@homer09.u.washington.edu>
References: <833E32F61B9F8746878F2A1865BECE6001455460@EXCHCLUSTER01.lj.gnf.org>
	<41269BAE.3030709@pdf.com>
	<Pine.A41.4.58.0408221302160.238618@homer09.u.washington.edu>
Message-ID: <41290FFD.9030205@pdf.com>

Hi, Thomas:  Thanks for the clarification.  spencer graves

Thomas Lumley wrote:

>On Fri, 20 Aug 2004, Spencer Graves wrote:
>  
>
>>      Also, have you considered "sem" (structural equation modeling)?
>> From what I've heard, partial least squares started out as a solution
>>without a clear problem statement, i.e., an algorithm claiming to solve
>>a problem but without a clear statement of a probability model for which
>>their algorithm produced the (approximate) maximum likelihood or
>>Bayesian posterior mode.  Structural equation modeling, by contrast,
>>provides the model and problem statement that partial least squares
>>seems to try to solve.  I got this impression from reading the PLS
>>article in the Encyclopedia of Statistical Sciences.
>>
>>    
>>
>
>I don't think that's quite right.  If you look at how PLS is used and who
>uses it, it is in situations where maximum likelihood estimation often is
>not possible or not reliable because of the dimension of the predictor
>variables.  It is a regularised version of SEM, which can be useful even
>if the penalty term is not clearly specified.
>
>	-thomas
>  
>



From ross at biostat.ucsf.edu  Mon Aug 23 01:55:01 2004
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Sun, 22 Aug 2004 16:55:01 -0700
Subject: [R] R CMD check testing environment
In-Reply-To: <4128D0D0.8040304@statistik.uni-dortmund.de>
References: <1093045667.30107.104.camel@iron.libaux.ucsf.edu>
	<4128D0D0.8040304@statistik.uni-dortmund.de>
Message-ID: <20040822235501.GC2203@wheat.boylan.org>

On Sun, Aug 22, 2004 at 06:58:56PM +0200, Uwe Ligges wrote:
> Ross Boylan wrote:
> >I can't tell from the docs  ("Writing R Extensions" 1.9.1) exactly what
> >environment the tests, examples, and vignettes that R CMD check tries to
> >run are in.
> >
> >In particular:
> >1) how do I get the package loaded?
> >2) how do I access data in the data/ directory?
> >3) where is the material in the other directories?  (e.g., has inst/
> >material been installed?  where?)
> >
> >Apparently (section 1.3) stuff in demo/ is not checked, which seems odd.
> >
> >By inspecting some other packages, it seems the answer to 1) is that the
> >package is already loaded, so I don't need to say library(...).  In
> >particular, I don't need to figure out what lib.loc is.
> >
> >I have some C code as part of the package, so that (well the .so file)
> >needs to be loaded too.
> 
> Yes. You don't need library() in the the help pages' exmaples, but you 
> need it in tests.
> 

So what should I use for lib.loc?

> 
> >2) Others seem to just say data(..), but this doesn't work for me.
> >I created the data with
> >save(gold, e2, q2, file="mspath/data/inputs", compress=TRUE)
> >and later renamed the file to inputs.RData.  (check didn't think the
> >file counted without the extension).
> 
> Indeed, the manuals tells us that saved images have to be called either
> *.RData (note the capitalization) or *.rda.
> 
> 
> >I have tried to access it in my test script (under tests/) with both
> >load and data (e.g., data("inputs"), data("inputs.RData"),
> >data(inputs)).  I get
> >
> >>data(inputs)
> >
> >Warning message: 
> >Data set 'inputs' not found in: data(inputs) 
> 
> Works for me as "Mytests.R" in "Mypackage":
> 
>  library(Mypackage)
>  data(MydataInMypackage)
>  print(MydataInMypackage)

Is this with MydataInMyPackage in the same directory as Mytests.R, or
in data/.  I'm attempting the latter.

> 
> >For that matter, my assumed answer to 1) doesn't seem to be working out,
> >because when I try to access one of my functions it tells me it can't
> >find it.  The function name is the same as the package name.
> >
> >Perhaps the problem is I have inferred answers from \example{}, and the
> >story for tests/ is different.
> 
> Right, see above.
> 
> 
> >Although I'm currently focussed on running a script in tests/, I'd like
> >to know what the story is for \examples in documentation or vignettes.
> 
> You won't need library() in the examples ...

And it's also unnecessary for vignettes?

> 
> 
> >Thanks.
> >
> >P.S. Is there a typical way to produce the .Rout.save file used in
> >tests/?  What I'm doing is a slightly awkward 2-step process.
> 
> 
> Running R CMD check once, looking whether the Rout looks fine (this 
> should be the step that takes most of the time, and R can't do it for 
> you), rename the file it, finished.
OK, that's what I was doing.
> 
> 
> Uwe Ligges

Thanks.



From andy_liaw at merck.com  Mon Aug 23 02:16:24 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sun, 22 Aug 2004 20:16:24 -0400
Subject: [R] A troubled state of freedom: generalized linear models
	wh ere number of parameters > number of samples
Message-ID: <3A822319EB35174CA3714066D590DCD504AF826F@usrymx25.merck.com>

Check out the gpls package on CRAN. 

HTH,
Andy

> From: Min-Han Tan
> 
> Good morning,
> 
> Thank you all for your help so far. I really appreciate it.
> 
> The crux of my problem is that I am generating a generalized linear
> model with 1 dependent variable, approximately 50 training samples and
> 100 parameters (gene levels).
> 
> Essentially, if I have 100 genes and 50 samples, this results in
> coefficients for the first 49 samples, and NAs for the rest, with an
> ultra low residual deviance (usually approx. 10^-27). This seems to
> have something to do with the number of degrees of freedom (since as
> the number of genes increases up to 49, the number of residual degrees
> of freedom drops to 0)
> 
> What kind of methods can I use to make sense of this? 
> 
> I have a subsequent set of samples to work on to validate the results
> of this glm, so I am not sure if overfitting is really a problem.
> 
> Background: this is a microarray study, where I have divided the
> samples in the training set into 2 groups, and generated a number of
> genes to differentiate between both groups. I am going to use the GLM
> in a subsequent regression analysis to determine survival. For this
> purpose, I need to generate some kind of score for each individual
> case using the coefficients of each gene level * gene expression
> level.
> 
> I am not a statistician (but a clinician) - many apologies if I am not
> conveying myself very clearly here!
> 
> Thanks. 
> 
> Min-Han Tan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From doriaba2 at snu.ac.kr  Mon Aug 23 04:09:26 2004
From: doriaba2 at snu.ac.kr (Dong H. Oh)
Date: Mon, 23 Aug 2004 11:09:26 +0900
Subject: [R] Stochastic Frontier Analysis with R
Message-ID: <200408230209.i7N29Vie008760@hypatia.math.ethz.ch>

Hi, all,

Is there a way to analyze data with Stochastic Fronteir Analysis with R?
I'd like to know the packages or source code of SFA developed by Jondrow et
al. (1982).

Cheers,



From treebc at telus.net  Mon Aug 23 04:14:17 2004
From: treebc at telus.net (bcatton)
Date: Sun, 22 Aug 2004 19:14:17 -0700
Subject: [R] GEE - test for overdispersion and scale adjustment
Message-ID: <000001c488b6$e50daa80$c5c65289@forestry.ubc.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040822/0bd38d25/attachment.pl

From ajayshah at mayin.org  Sat Aug 21 17:47:29 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Sat, 21 Aug 2004 21:17:29 +0530
Subject: [R] Puzzled at lm() and time-series
Message-ID: <20040821154729.GA3312@igidr.ac.in>

I tried toy problems and there doesn't seem to be a basic problem
between lm() and ts objects:

   X = data.frame(x=c(1,2,7,9), y=c(7,2,3,1))
   lm(y ~ x, X)
   X <- lapply(X, function(x) ts(x, frequency=12, start=c(1994,7)))
   lm(y ~ x, X)

and this works fine - whether you do an lm() before or after making ts
objects, it's okay. 

But I have a situation where things aren't okay. I have two happy
time-series objects in a data frame:

> M$g.cpi.iw
       Jan   Feb   Mar   Apr   May   Jun   Jul   Aug   Sep   Oct   Nov   Dec
1994                                     11.07 10.94 11.20 10.31  9.81  9.47
1995  9.89  9.81  9.74  9.67 10.29 10.47 11.39 10.92 10.07 10.38 10.31  9.69
... (deleted)
2004  4.35  4.13  3.49  2.23  2.83  3.02    NA                              

> M$g.wpi
       Jan   Feb   Mar   Apr   May   Jun   Jul   Aug   Sep   Oct   Nov   Dec
1994                                     11.14 11.83 11.88 12.66 13.20 14.54
1995 16.18 16.88 16.88 11.02 10.94  9.70  9.61  8.94  8.92  8.47  8.24  6.62
... (deleted)
2004  6.45  6.14  4.79  4.52  4.99  6.14  6.86                              

But I can't get an OLS going:

> lm(g.cpi.iw ~ g.wpi, M)
Error in "storage.mode<-"(`*tmp*`, value = "double") : 
        invalid time series parameters specified

Any idea why? I think both objects are quite conformable (except for
an NA, but that should get dropped by lm() by default).

Here's a block of R code which creates M, in case you'd like to toy
with this:

M = structure(list(g.wpi = structure(c(11.14, 11.83, 11.88, 12.66,
13.2, 14.54, 16.18, 16.88, 16.88, 11.02, 10.94, 9.7, 9.61, 8.94, 8.92,
8.47, 8.24, 6.62, 5.04, 4.51, 4.53, 3.67, 3.58, 3.61, 4.28, 4.9, 5.08,
4.54, 4.48, 5.25, 5.12, 5.47, 5.43, 5.86, 5.09, 5.07, 3.6, 3.26, 3.83,
4.38, 3.94, 4.01, 5.11, 4.17, 4.32, 4.57, 5.64, 6.43, 7.1, 6.56, 5.89,
6.5, 7.15, 6.32, 4.48, 5.4, 5.41, 3.98, 3.31, 2.49, 1.97, 2.79, 3.23,
3.41, 3.1, 2.79, 3.58, 3.5, 5.54, 6.56, 6.35, 6.5, 6.51, 6.12, 6.47,
7.54, 7.61, 8.31, 8.69, 8.35, 6.45, 5.42, 5.6, 5.35, 5.27, 5.38, 4.48,
2.89, 2.59, 2.26, 1.53, 1.35, 1.72, 1.5, 1.5, 2.4, 2.79, 3.35, 3.55,
3.08, 3.39, 3.31, 4.21, 5.38, 6, 6.62, 6.57, 5.39, 4.68, 3.96, 4.9,
5.15, 5.42, 5.79, 6.45, 6.14, 4.79, 4.52, 4.99, 6.14, 6.86), .Tsp =
c(1994.5, 2004.5, 12), class = "ts"), g.cpi.iw = structure(c(11.07,
10.94, 11.2, 10.31, 9.81, 9.47, 9.89, 9.81, 9.74, 9.67, 10.29, 10.47,
11.39, 10.92, 10.07, 10.38, 10.31, 9.69, 9, 8.59, 8.87, 9.83, 9.33,
8.82, 8.31, 8.89, 8.52, 8.46, 8.72, 10.41, 11.11, 10.76, 10.03, 9.26,
7.32, 6.61, 5.6, 4.66, 4.94, 5.49, 4.87, 6.29, 9.71, 9.14, 8.26, 8.19,
10.51, 12.39, 14.8, 15.04, 16.34, 18.63, 19.67, 15.32, 9.38, 8.64,
8.95, 8.36, 7.71, 5.26, 3.16, 3.15, 2.14, 0.92, 0, 0.47, 2.62, 3.61,
4.83, 5.54, 5.01, 5.24, 4.95, 3.99, 3.5, 2.75, 2.74, 3.48, 3.25, 3.02,
2.53, 2.28, 2.5, 3.39, 4.04, 5.19, 4.73, 4.23, 4.89, 5.16, 4.94, 5.19,
5.17, 4.69, 4.66, 4.16, 3.89, 3.86, 4.3, 4.06, 3.6, 3.2, 3.43, 3.86,
4.06, 5.12, 4.66, 4.41, 4.16, 3.1, 2.89, 3.29, 3.07, 3.72, 4.35, 4.13,
3.49, 2.23, 2.83, 3.02, NA), .Tsp = c(1994.5, 2004.5, 12), class =
"ts")), .Names = c("g.wpi", "g.cpi.iw"), row.names = c("1", "2", "3",
"4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15",
"16", "17", "18", "19", "20", "21", "22", "23", "24", "25", "26",
"27", "28", "29", "30", "31", "32", "33", "34", "35", "36", "37",
"38", "39", "40", "41", "42", "43", "44", "45", "46", "47", "48",
"49", "50", "51", "52", "53", "54", "55", "56", "57", "58", "59",
"60", "61", "62", "63", "64", "65", "66", "67", "68", "69", "70",
"71", "72", "73", "74", "75", "76", "77", "78", "79", "80", "81",
"82", "83", "84", "85", "86", "87", "88", "89", "90", "91", "92",
"93", "94", "95", "96", "97", "98", "99", "100", "101", "102", "103",
"104", "105", "106", "107", "108", "109", "110", "111", "112", "113",
"114", "115", "116", "117", "118", "119", "120", "121"), class =
"data.frame")

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From ggrothendieck at myway.com  Mon Aug 23 07:52:33 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 23 Aug 2004 05:52:33 +0000 (UTC)
Subject: [R] Puzzled at lm() and time-series
References: <20040821154729.GA3312@igidr.ac.in>
Message-ID: <loom.20040823T074718-597@post.gmane.org>

Ajay Shah <ajayshah <at> mayin.org> writes:
: 
: But I can't get an OLS going:
: 
: > lm(g.cpi.iw ~ g.wpi, M)
: Error in "storage.mode<-"(`*tmp*`, value = "double") : 
:         invalid time series parameters specified
: 

: 
: M = structure(list(g.wpi = structure(c(11.14, 11.83, 11.88, 12.66,
: 13.2, 14.54, 16.18, 16.88, 16.88, 11.02, 10.94, 9.7, 9.61, 8.94, 8.92,
: 8.47, 8.24, 6.62, 5.04, 4.51, 4.53, 3.67, 3.58, 3.61, 4.28, 4.9, 5.08,
: 4.54, 4.48, 5.25, 5.12, 5.47, 5.43, 5.86, 5.09, 5.07, 3.6, 3.26, 3.83,
: 4.38, 3.94, 4.01, 5.11, 4.17, 4.32, 4.57, 5.64, 6.43, 7.1, 6.56, 5.89,
: 6.5, 7.15, 6.32, 4.48, 5.4, 5.41, 3.98, 3.31, 2.49, 1.97, 2.79, 3.23,
: 3.41, 3.1, 2.79, 3.58, 3.5, 5.54, 6.56, 6.35, 6.5, 6.51, 6.12, 6.47,
: 7.54, 7.61, 8.31, 8.69, 8.35, 6.45, 5.42, 5.6, 5.35, 5.27, 5.38, 4.48,
: 2.89, 2.59, 2.26, 1.53, 1.35, 1.72, 1.5, 1.5, 2.4, 2.79, 3.35, 3.55,
: 3.08, 3.39, 3.31, 4.21, 5.38, 6, 6.62, 6.57, 5.39, 4.68, 3.96, 4.9,
: 5.15, 5.42, 5.79, 6.45, 6.14, 4.79, 4.52, 4.99, 6.14, 6.86), .Tsp =
: c(1994.5, 2004.5, 12), class = "ts"), g.cpi.iw = structure(c(11.07,
: 10.94, 11.2, 10.31, 9.81, 9.47, 9.89, 9.81, 9.74, 9.67, 10.29, 10.47,
: 11.39, 10.92, 10.07, 10.38, 10.31, 9.69, 9, 8.59, 8.87, 9.83, 9.33,
: 8.82, 8.31, 8.89, 8.52, 8.46, 8.72, 10.41, 11.11, 10.76, 10.03, 9.26,
: 7.32, 6.61, 5.6, 4.66, 4.94, 5.49, 4.87, 6.29, 9.71, 9.14, 8.26, 8.19,
: 10.51, 12.39, 14.8, 15.04, 16.34, 18.63, 19.67, 15.32, 9.38, 8.64,
: 8.95, 8.36, 7.71, 5.26, 3.16, 3.15, 2.14, 0.92, 0, 0.47, 2.62, 3.61,
: 4.83, 5.54, 5.01, 5.24, 4.95, 3.99, 3.5, 2.75, 2.74, 3.48, 3.25, 3.02,
: 2.53, 2.28, 2.5, 3.39, 4.04, 5.19, 4.73, 4.23, 4.89, 5.16, 4.94, 5.19,
: 5.17, 4.69, 4.66, 4.16, 3.89, 3.86, 4.3, 4.06, 3.6, 3.2, 3.43, 3.86,
: 4.06, 5.12, 4.66, 4.41, 4.16, 3.1, 2.89, 3.29, 3.07, 3.72, 4.35, 4.13,
: 3.49, 2.23, 2.83, 3.02, NA), .Tsp = c(1994.5, 2004.5, 12), class =
: "ts")), .Names = c("g.wpi", "g.cpi.iw"), row.names = c("1", "2", "3",
: "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15",
: "16", "17", "18", "19", "20", "21", "22", "23", "24", "25", "26",
: "27", "28", "29", "30", "31", "32", "33", "34", "35", "36", "37",
: "38", "39", "40", "41", "42", "43", "44", "45", "46", "47", "48",
: "49", "50", "51", "52", "53", "54", "55", "56", "57", "58", "59",
: "60", "61", "62", "63", "64", "65", "66", "67", "68", "69", "70",
: "71", "72", "73", "74", "75", "76", "77", "78", "79", "80", "81",
: "82", "83", "84", "85", "86", "87", "88", "89", "90", "91", "92",
: "93", "94", "95", "96", "97", "98", "99", "100", "101", "102", "103",
: "104", "105", "106", "107", "108", "109", "110", "111", "112", "113",
: "114", "115", "116", "117", "118", "119", "120", "121"), class =
: "data.frame")
: 

>From the above we see there is an NA in the last observation.
If we delete that observation then it runs to completion:

R> lm(g.cpi.iw ~ g.wpi, M, subset = -121)

Call:
lm(formula = g.cpi.iw ~ g.wpi, data = M, subset = -121)

Coefficients:
(Intercept)        g.wpi  
     3.7952       0.5083



From ligges at statistik.uni-dortmund.de  Mon Aug 23 08:44:28 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 23 Aug 2004 08:44:28 +0200
Subject: [R] R CMD check testing environment
In-Reply-To: <20040822235501.GC2203@wheat.boylan.org>
References: <1093045667.30107.104.camel@iron.libaux.ucsf.edu>
	<4128D0D0.8040304@statistik.uni-dortmund.de>
	<20040822235501.GC2203@wheat.boylan.org>
Message-ID: <4129924C.8070904@statistik.uni-dortmund.de>

Ross Boylan wrote:
> On Sun, Aug 22, 2004 at 06:58:56PM +0200, Uwe Ligges wrote:
> 
>>Ross Boylan wrote:
>>
>>>I can't tell from the docs  ("Writing R Extensions" 1.9.1) exactly what
>>>environment the tests, examples, and vignettes that R CMD check tries to
>>>run are in.
>>>
>>>In particular:
>>>1) how do I get the package loaded?
>>>2) how do I access data in the data/ directory?
>>>3) where is the material in the other directories?  (e.g., has inst/
>>>material been installed?  where?)
>>>
>>>Apparently (section 1.3) stuff in demo/ is not checked, which seems odd.
>>>
>>>By inspecting some other packages, it seems the answer to 1) is that the
>>>package is already loaded, so I don't need to say library(...).  In
>>>particular, I don't need to figure out what lib.loc is.
>>>
>>>I have some C code as part of the package, so that (well the .so file)
>>>needs to be loaded too.
>>
>>Yes. You don't need library() in the the help pages' exmaples, but you 
>>need it in tests.
>>
> 
> 
> So what should I use for lib.loc?

The default (i.e. you do not need to set it).


>>>2) Others seem to just say data(..), but this doesn't work for me.
>>>I created the data with
>>>save(gold, e2, q2, file="mspath/data/inputs", compress=TRUE)
>>>and later renamed the file to inputs.RData.  (check didn't think the
>>>file counted without the extension).
>>
>>Indeed, the manuals tells us that saved images have to be called either
>>*.RData (note the capitalization) or *.rda.
>>
>>
>>
>>>I have tried to access it in my test script (under tests/) with both
>>>load and data (e.g., data("inputs"), data("inputs.RData"),
>>>data(inputs)).  I get
>>>
>>>
>>>>data(inputs)
>>>
>>>Warning message: 
>>>Data set 'inputs' not found in: data(inputs) 
>>
>>Works for me as "Mytests.R" in "Mypackage":
>>
>> library(Mypackage)
>> data(MydataInMypackage)
>> print(MydataInMypackage)
> 
> 
> Is this with MydataInMyPackage in the same directory as Mytests.R, or
> in data/.  I'm attempting the latter.


In ./data, or do you want to use the data for the tests *only*?



> 
>>>For that matter, my assumed answer to 1) doesn't seem to be working out,
>>>because when I try to access one of my functions it tells me it can't
>>>find it.  The function name is the same as the package name.
>>>
>>>Perhaps the problem is I have inferred answers from \example{}, and the
>>>story for tests/ is different.
>>
>>Right, see above.
>>
>>
>>
>>>Although I'm currently focussed on running a script in tests/, I'd like
>>>to know what the story is for \examples in documentation or vignettes.
>>
>>You won't need library() in the examples ...
> 
> 
> And it's also unnecessary for vignettes?

I haven't tried it out - why are you not trying it out yourself?
Other people are using library() in vignettes, so I'd just do it as well.

Uwe Ligges


>>
>>>Thanks.
>>>
>>>P.S. Is there a typical way to produce the .Rout.save file used in
>>>tests/?  What I'm doing is a slightly awkward 2-step process.
>>
>>
>>Running R CMD check once, looking whether the Rout looks fine (this 
>>should be the step that takes most of the time, and R can't do it for 
>>you), rename the file it, finished.
> 
> OK, that's what I was doing.
> 
>>
>>Uwe Ligges
> 
> 
> Thanks.



From ripley at stats.ox.ac.uk  Mon Aug 23 08:53:11 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 23 Aug 2004 07:53:11 +0100 (BST)
Subject: [R] Puzzled at lm() and time-series
In-Reply-To: <20040821154729.GA3312@igidr.ac.in>
Message-ID: <Pine.LNX.4.44.0408230733480.4477-100000@gannet.stats>

On Sat, 21 Aug 2004, Ajay Shah wrote:

> I tried toy problems and there doesn't seem to be a basic problem
> between lm() and ts objects:
> 
>    X = data.frame(x=c(1,2,7,9), y=c(7,2,3,1))
>    lm(y ~ x, X)
>    X <- lapply(X, function(x) ts(x, frequency=12, start=c(1994,7)))
>    lm(y ~ x, X)
> 
> and this works fine - whether you do an lm() before or after making ts
> objects, it's okay. 
> 
> But I have a situation where things aren't okay. I have two happy
> time-series objects in a data frame:
> 
> > M$g.cpi.iw
>        Jan   Feb   Mar   Apr   May   Jun   Jul   Aug   Sep   Oct   Nov   Dec
> 1994                                     11.07 10.94 11.20 10.31  9.81  9.47
> 1995  9.89  9.81  9.74  9.67 10.29 10.47 11.39 10.92 10.07 10.38 10.31  9.69
> ... (deleted)
> 2004  4.35  4.13  3.49  2.23  2.83  3.02    NA                              
> 
> > M$g.wpi
>        Jan   Feb   Mar   Apr   May   Jun   Jul   Aug   Sep   Oct   Nov   Dec
> 1994                                     11.14 11.83 11.88 12.66 13.20 14.54
> 1995 16.18 16.88 16.88 11.02 10.94  9.70  9.61  8.94  8.92  8.47  8.24  6.62
> ... (deleted)
> 2004  6.45  6.14  4.79  4.52  4.99  6.14  6.86                              
> 
> But I can't get an OLS going:
> 
> > lm(g.cpi.iw ~ g.wpi, M)
> Error in "storage.mode<-"(`*tmp*`, value = "double") : 
>         invalid time series parameters specified
> 
> Any idea why? I think both objects are quite conformable (except for
> an NA, but that should get dropped by lm() by default).

That's the problem: the row with a NA gets dropped but the tsp atribute
does not get adjusted.

BTW, try traceback() when you get an error.

lm(g.cpi.iw ~ g.wpi, data = na.omit(M))

will work, since an explicit call to na.omit does the right thing re 
attributes.  The difference is in the internal code which says

	/* need to transfer _all but dim_ attributes, possibly lost
	   by subsetting in na.action.  */
	for ( i = length(ans) ; i-- ; )
	  	copyMostAttrib(VECTOR_ELT(data, i),VECTOR_ELT(ans, i));

That's wrong in this case.

I think it was a leap to assume that you could fit linear models to time 
series via lm.  ts objects are not mentioned on the help page for lm, are 
they?  Another trap is to assume that diff() will be respected by lm.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ajayshah at mayin.org  Mon Aug 23 09:43:29 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Mon, 23 Aug 2004 13:13:29 +0530
Subject: [R] Puzzled at lm() and time-series
In-Reply-To: <Pine.LNX.4.44.0408230733480.4477-100000@gannet.stats>
References: <20040821154729.GA3312@igidr.ac.in>
	<Pine.LNX.4.44.0408230733480.4477-100000@gannet.stats>
Message-ID: <20040823074329.GB713@igidr.ac.in>

> > Any idea why? I think both objects are quite conformable (except for
> > an NA, but that should get dropped by lm() by default).
> 
> That's the problem: the row with a NA gets dropped but the tsp atribute
> does not get adjusted.

Thanks! :-)

> BTW, try traceback() when you get an error.
> 
> lm(g.cpi.iw ~ g.wpi, data = na.omit(M))

Will do.

> I think it was a leap to assume that you could fit linear models to time 
> series via lm.  ts objects are not mentioned on the help page for lm, are 
> they?  Another trap is to assume that diff() will be respected by lm.

My general attitude in R is "if it sounds reasonable, then it ought to
work". :-) I did do a toy example (which was there in the previous
post) and that worked fine, so I jumped to the conclusion that the
internals are all conformable.

If OLS regressions involving ts objects is not 'officially' supported,
what is the recommended way of going about them? Is there an "unts()"
operator which will take me back to raw data? I like to call things
ts() because it gives me prettier graphs.

On diff() also, I have an instinctive confidence that you guys will
have touched up everything perfectly, so if
        x = ts()
        y = ts()
        lm(y ~ x)
works, then
        dx = diff(x,1)
        dy = diff(y,1)
        lm(dy ~ dx)
should also work perfectly. :-)

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From ripley at stats.ox.ac.uk  Mon Aug 23 10:07:51 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 23 Aug 2004 09:07:51 +0100 (BST)
Subject: [R] Puzzled at lm() and time-series
In-Reply-To: <20040823074329.GB713@igidr.ac.in>
Message-ID: <Pine.LNX.4.44.0408230849030.4743-100000@gannet.stats>

On Mon, 23 Aug 2004, Ajay Shah wrote:

> > > Any idea why? I think both objects are quite conformable (except for
> > > an NA, but that should get dropped by lm() by default).
> > 
> > That's the problem: the row with a NA gets dropped but the tsp atribute
> > does not get adjusted.
> 
> Thanks! :-)
> 
> > BTW, try traceback() when you get an error.
> > 
> > lm(g.cpi.iw ~ g.wpi, data = na.omit(M))
> 
> Will do.
> 
> > I think it was a leap to assume that you could fit linear models to time 
> > series via lm.  ts objects are not mentioned on the help page for lm, are 
> > they?  Another trap is to assume that diff() will be respected by lm.
> 
> My general attitude in R is "if it sounds reasonable, then it ought to
> work". :-) 

You get what you pay for, don't forget.  Telling the donor what a free
gift `ought' or `should' do is discourteous at best.


If you want to use lm on time series, do a ts.intersect to create a 
suitable data frame first, then call na.omit.

If you get an error you don't understand, R's debugging facilities will 
help you find what you didn't understand.  Please learn to use them.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dtrenkler at nts6.oec.uni-osnabrueck.de  Mon Aug 23 13:00:16 2004
From: dtrenkler at nts6.oec.uni-osnabrueck.de (Trenkler, Dietrich)
Date: Mon, 23 Aug 2004 13:00:16 +0200
Subject: [R] Two factor ANOVA with lm()
Message-ID: <FB75CFC167F3D311B11D00A0CC20FB0E885565@nts7.oec.Uni-Osnabrueck.DE>

The following is a data frame


> "jjd" <- structure(list(Observations = c(6.8, 6.6, 5.3, 6.1,
      7.5, 7.4, 7.2, 6.5, 7.8, 9.1, 8.8, 9.1), LevelA = structure(c(1,
      1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3), .Label = c("A1", "A2",
      "A3"), class = "factor"), LevelB = structure(c(1, 1, 2, 2,
      1, 1, 2, 2, 1, 1, 2, 2), .Label = c("B1", "B2"), class = "factor")),
      .Names = c("Observations", "LevelA", "LevelB"), row.names = c("1",
          "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12"),
      class = "data.frame")

representing data from


@BOOK{Dobson02,
  author = {Annette J. Dobson},
  year = 2002,
  title = {An Introduction to Generalized Linear Models},
  edition = {2.},
  publisher = {Chapman \& Hall/CRC},
  address = {Boca Raton, Florida, 33431}
}

page 101. To reproduce the estimates c(6.7,0.75,1.75,-1.0,0.4,1.5)
given on page 103 in a two factor ANOVA  entering


> jja1 <- lm(Observations~LevelA*LevelB,data=jjd)
> summary(jja1)

I get


Call:
lm(formula = Observations ~ LevelA * LevelB, data = jjd)

Residuals:
       Min         1Q     Median         3Q        Max
-6.500e-01 -2.000e-01 -3.469e-17  2.000e-01  6.500e-01

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)
(Intercept)         6.7000     0.3512  19.078 1.34e-06 ***
LevelAA2            0.7500     0.4967   1.510   0.1818
LevelAA3            1.7500     0.4967   3.524   0.0125 *
LevelBB2           -1.0000     0.4967  -2.013   0.0907 .
LevelAA2:LevelBB2   0.4000     0.7024   0.569   0.5897
LevelAA3:LevelBB2   1.5000     0.7024   2.136   0.0766 .
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1

Residual standard error: 0.4967 on 6 degrees of freedom
Multiple R-Squared: 0.9065,     Adjusted R-squared: 0.8286
F-statistic: 11.64 on 5 and 6 DF,  p-value: 0.00481


This is fine. But why do I get these estimates?


Entering

> model.matrix(jja1)

delivers


   (Intercept) LevelAA2 LevelAA3 LevelBB2 LevelAA2:LevelBB2
LevelAA3:LevelBB2
1            1        0        0        0                 0
0
2            1        0        0        0                 0
0
3            1        0        0        1                 0
0
4            1        0        0        1                 0
0
5            1        1        0        0                 0
0
6            1        1        0        0                 0
0
7            1        1        0        1                 1
0
8            1        1        0        1                 1
0
9            1        0        1        0                 0
0
10           1        0        1        0                 0
0
11           1        0        1        1                 0
1
12           1        0        1        1                 0
1
attr(,"assign")
[1] 0 1 1 2 3 3
attr(,"contrasts")
attr(,"contrasts")$LevelA
[1] "contr.treatment"

attr(,"contrasts")$LevelB
[1] "contr.treatment"


which shows that internally lm() seems to use corner point constraints
of the form

\[\alpha_1=\beta_1=(\alpha\beta)_{11}=
(\alpha\beta)_{12}=(\alpha\beta)_{12}=(\alpha\beta)_{31}=0\]


in the model $E[Y_{jkl}]=\mu+\alpha_j+\beta_k+(\alpha\beta)_{jk}$
$j=1,2,3$, $k=1,2$, $l=1,2$, Dobson, page 102.


My question is:  how can I incorporate restrictions like
$\alpha_1+\alpha_2+\alpha_3=0$, $\beta_1+\beta_2=0$,
$(\alpha\beta)_{21}+\alpha\beta)_{22}=0$,
$(\alpha\beta)_{31}+(\alpha\beta)_{32}=0$ and
$(\alpha\beta)_{11}+(\alpha\beta)_{21}+(\alpha\beta)_{31}=0$ from the
outset?  Or put another way:  Why is it that lm() uses the corner point
constraints by default?  Where can I find a documentation for this
behavior?

I know that I can use something like lm(y~X) where y <- c(6.8, 6.6,
5.3, 6.1, 7.5, 7.4, 7.2, 6.5, 7.8, 9.1, 8.8, 9.1) and X is an
appropriate design matrix.  But I wonder if there is a more direct way.


Many thanks in advance.

D. Trenkler                                                                 


--
Dietrich Trenkler   Universit??t Osnabr??ck                                  
FB Wirtschaftswissenschaften           
Rolandstr.8              D-49069 Osnabr??ck

dtrenkler at nts6.oec.uni-osnabrueck.de



From Mike.Firth at astrazeneca.com  Mon Aug 23 13:00:48 2004
From: Mike.Firth at astrazeneca.com (Firth, Mike A (ALDP))
Date: Mon, 23 Aug 2004 12:00:48 +0100
Subject: [R] Installing R on DEC Alpha - problems with dynamic loading
Message-ID: <BAF5C0B1B1FE5A41B28C71CC977422E904964776@ukapphresmsx01.ukapd.astrazeneca.net>

I have recently installed R on DEC Alpha OSF 5.1.

The first regression test (reg-test-1.R) failed with an unresolved symbol in
lapack.so:

2246839:/data/h1/bifdev/apps/wga/R-1.9.1/bin/R.bin: /sbin/loader: Fatal
Error: call to unresolved symbol from
/data/h1/bifdev/apps/wga/R-1.9.1/modules/lapack.so (pc=0x3ffbfe22b60)

Further investigation has revealed that there is NO problem with lapack.so.
The problem seems to lie in the use of 'unloadNamespace' earlier in the
script.

On our machine the following line:

	unloadNamespace(splines)

seem to really confuse the dynamic linking/symbol resolution. I have a very
small example to demonstrate:

	data(iris)
	library(splines)
	unloadNamespace(splines)
	data(iris)

works fine, but:

	library(splines)
	unloadNamespace(splines)
	data(iris)

falls over with:

Error in dyn.load(x, as.logical(local), as.logical(now)) : 
        unable to load shared library
"/data/h1/bifdev/apps/wga/R-1.9.1/library/tools/libs/tools.so":
  dlopen: /data/h1/bifdev/apps/wga/R-1.9.1/library/stats/libs/stats.so:
symbol "bdrsplerr_" unresolved
Execution halted

The unloadNamespace has effectively unloaded a symbol from a completely
different shared object.

Can anybody shed any light on this problem?

Thanks.
-----
Mike Firth
Senior Technical Specialist
AstraZeneca Pharmaceuticals
Mereside
Alderley Park
Macclesfield
Cheshire
SK10 3LT

Tel: 01625 513919



From dtrenkler at nts6.oec.uni-osnabrueck.de  Mon Aug 23 13:39:28 2004
From: dtrenkler at nts6.oec.uni-osnabrueck.de (Trenkler, Dietrich)
Date: Mon, 23 Aug 2004 13:39:28 +0200
Subject: [R] Two factor ANOVA with lm()
Message-ID: <FB75CFC167F3D311B11D00A0CC20FB0E885566@nts7.oec.Uni-Osnabrueck.DE>



> -----Original Message-----
> From:	Prof Brian Ripley 
> Sent:	Monday, August 23, 2004 1:15 PM
> To:	Trenkler, Dietrich
> Subject:	Re: [R] Two factor ANOVA with lm()
> 
> On Mon, 23 Aug 2004, Trenkler, Dietrich wrote:
> 
> [...]
> 
> > outset?  Or put another way:  Why is it that lm() uses the corner point
> > constraints by default?  Where can I find a documentation for this
> > behavior?
> 
> In almost any piece of documentation on linear models in R, including the
> FAQ and `An Introduction to R', which says
> 
>   The main reason for mentioning this is that R and S have different
>   defaults for unordered factors, S using Helmert contrasts.  So if you 
>   need to compare your results to those of a textbook or paper which used
>   S-PLUS, you will need to set
> 
>     options(contrasts = c("contr.helmert", "contr.poly"))
> 
>   This is a deliberate difference, as treatment contrasts (R's default)
>   are thought easier for newcomers to interpret.
> 
> Now, what does the posting guide say about doing your homework?
	 
	[Dietrich Trenkler]  I swear I didn't need it for a homework.
	I just overlooked the self-evident... (blush)

	Thank you.

	D. Trenkler



From gudrunj at math.su.se  Mon Aug 23 14:59:36 2004
From: gudrunj at math.su.se (Gudrun Jonasdottir)
Date: Mon, 23 Aug 2004 14:59:36 +0200 (CEST)
Subject: [R] convert strings to object names: SUMMARY
In-Reply-To: <5.2.0.9.2.20040819093739.00ac6118@poptop.llnl.gov>
References: <4032.193.10.23.33.1092924903.squirrel@webmail.math.su.se>
	<5.2.0.9.2.20040819093739.00ac6118@poptop.llnl.gov>
Message-ID: <4755.193.10.23.31.1093265976.squirrel@webmail.math.su.se>

Dear R-list,

Apologies for replying late. Thanks to Andy, Dimitris, James, Uwe, Olaf,
Adai, Gardar and Patrick. (Apologies if I forgot someone)

In summary: The quickest way to convert a string to an object name, seems
to be by using get(dat), but it seems to work equally well if you use any
of

eval(parse(text=dat))
do.call("print", list(as.name(dat)))
eval(substitute(print(x), list(x=as.name(dat))))
assign("dat", get(paste("dat", no, sep="")))

Thanks again,
Gudrun

Gardar Johannessonwrote:
> Use the get() function:
>
>  > dat99 <- matrix(c(1,1,2),3,3)
>  > no <- 99
>  > dat.name <- paste("dat",no,sep="")
>  > get(dat.name)
>       [,1] [,2] [,3]
> [1,]    1    1    1
> [2,]    1    1    1
> [3,]    2    2    2
>  >
>
>
> At 04:15 PM 8/19/2004 +0200, Gudrun Jonasdottir wrote:
>>Dear R-Help list,
>>
>>I have a problem with convertions of strings. I want to use the function
>>"paste()" to create an object name and then use that character string to
>>call on that object. So, for example:
>>
>>dat99 <- matrix(rbind(1,1,2),3,3)
>>no <- 99
>>dat <- paste("dat",no,sep="")
>>dat
>>[1] "dat99"
>>
>>What should I do to get the output
>>
>>dat
>>      [,1] [,2] [,3]
>>[1,]    1    1    1
>>[2,]    1    1    1
>>[3,]    2    2    2
>>
>>Cheers,
>>Gudrun
>>
>>
>>
>>--
>>Gudrun Jonasdottir, M.Sc.
>>Matematiska institutionen
>>Stockholms Universitet
>>SE- 106 91 Stockholm
>>
>>Work: +46 (0)8 16 45 56
>>Mobile: +46 (0)709 779 800
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>
>



From oren.kaminer at intel.com  Mon Aug 23 15:14:28 2004
From: oren.kaminer at intel.com (Kaminer, Oren)
Date: Mon, 23 Aug 2004 16:14:28 +0300
Subject: [R] Statistical help about backets analisys
Message-ID: <92C157534C58E44C9A5D517F0F3D733E013B7C4F@hasmsx404.ger.corp.intel.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040823/d4b3dc28/attachment.pl

From v_bill_pikounis at merck.com  Mon Aug 23 15:46:20 2004
From: v_bill_pikounis at merck.com (Pikounis, Bill)
Date: Mon, 23 Aug 2004 09:46:20 -0400
Subject: [R] [Job Ad] Position at Merck Research Laboratories, NJ USA
Message-ID: <CFBD404F5E0C9547B4E10B7BDC3DFA2F0415653B@usrymx18.merck.com>

Please accept my apologies for cross-posting to those subscribed to both
R-help and S-news.

Also, please direct *all* inquiries to Vladimir Svetnik, the hiring manager.
His contact information is below.

Thanks,
Bill

################################################################

Job description:  Computational statistician/biometrician   

The Biometrics Research Department at Merck Research Laboratories, Merck &
Co., Inc. in Rahway, NJ, is seeking a highly motivated statistician/data
analyst to work in its basic research and drug discovery area.  The
applicant should have broad expertise in statistical computing.   Experience
and/or education relevant to signal processing, image processing, pattern
recognition and machine learning are preferred.  The position will involve
providing statistical, mathematical, and software development support for
one or more of following areas: medical imaging, biological signal analysis,
and computational chemistry.   We are looking for a Ph.D. with a background
and/or post-doctoral experience in at least one of the following fields:
Statistics, Electrical/Computer or Biomedical Engineering, Computer Science,
Applied Mathematics, or Physics.   Advanced computer programming skills
(including, but not limited to R, Splus, Matlab, C/C++) and excellent
communication skills are essential. An ability to lead statistical analysis
efforts within a multidisciplinary team is required.   The position may also
involve general statistical consulting and training.

Our dedication to delivering quality medicines in innovative ways and our
commitment to bringing out the best in our people are just some of the
reasons why we're ranked among Fortune magazine's "100 Best Companies to
Work for in America."  We offer a competitive salary, an outstanding
benefits package, and a professional work environment with a company known
for scientific excellence.  To apply, please forward your CV or resume and
cover letter to

ATTENTION: Open Position  
Vladimir Svetnik, Ph.D.  
Biometrics Research Dept.  
Merck Research Laboratories
RY33-300
126 E. Lincoln Avenue
Rahway, NJ 07065-0900 
vladimir_svetnik at merck.com



From heiko.schaefer at swissrisk.ch  Mon Aug 23 16:11:26 2004
From: heiko.schaefer at swissrisk.ch (Heiko Schaefer)
Date: Mon, 23 Aug 2004 16:11:26 +0200
Subject: [R] C++ classes as opaque types in R
Message-ID: <20040823141126.GA12907@heikster.swissrisk.com>

Dear all,

suppose I would have an abstract class in C++ representing some
complex type. Is it possible to expose a selected C++ interface
to R while retaining all the gory details within C++. Other 
scripting languages have the concept of an opaque pointer and
constructor/destructor hooks, so that the scripting language's
garbage collection can clean up (e.g. Python). Is there anything
like this in R?

Thanks a lot,
Heiko



From sbchapman at highstream.net  Mon Aug 23 16:16:09 2004
From: sbchapman at highstream.net (Sam Chapman)
Date: Mon, 23 Aug 2004 10:16:09 -0400
Subject: [R] The "Green" Book?
In-Reply-To: <Pine.LNX.4.44.0408210552170.29237-100000@gannet.stats>
References: <Pine.LNX.4.44.0408210552170.29237-100000@gannet.stats>
Message-ID: <1093270569.4129fc2910872@webmail.highstream.net>

Thank you for your responses. I should have mentioned that I am new to R, but
not to programming. Nevertheless, the insights are valued and appreciated!


Quoting Prof Brian Ripley <ripley at stats.ox.ac.uk>:

> On Fri, 20 Aug 2004, Thomas Lumley wrote:
>
> > On Fri, 20 Aug 2004, Sam Chapman wrote:
> >
>
> [A quote from `An Introduction to R' has been excised here]
>
> > > There is no mention of 'Programming with Data: A Guide to the S Language'
> by
> > > John M. Chambers. Is this newest ("Green") book also suitable as a
> reference
> > > for R? Thank you for your time and attention!
> > >
> >
> > Yes. The system implemented in the "methods" package is not identical to
> > that in the Green Book, but it's pretty similar.
>
> Well, it is suitable as reference for programmers using the "methods"
> package in R, not quite the question asked.  At the level of `An
> Introduction to R' it is not really a suitable reference as it has limited
> coverage at that level.  (The Green Book itself recommends other books for
> end users.)
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>


Sincerely,

Sam Chapman



From Mark.Betten at vai.org  Mon Aug 23 16:26:18 2004
From: Mark.Betten at vai.org (Betten, Mark)
Date: Mon, 23 Aug 2004 10:26:18 -0400
Subject: [R] unsubscribe to R-help
Message-ID: <CEA39A213F7F2E44A0DED9210BCD352F0778D5@VAIEXCH04.vai.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040823/8ad6f12a/attachment.pl

From m_nica at hotmail.com  Mon Aug 23 16:52:41 2004
From: m_nica at hotmail.com (Mihai Nica)
Date: Mon, 23 Aug 2004 09:52:41 -0500
Subject: [R] Reading GAL file
Message-ID: <BAY18-DAV117tdMUc5f0000fa8c@hotmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040823/f1b5d2c4/attachment.pl

From p.dalgaard at biostat.ku.dk  Mon Aug 23 17:01:29 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 23 Aug 2004 17:01:29 +0200
Subject: [R] unsubscribe to R-help
In-Reply-To: <CEA39A213F7F2E44A0DED9210BCD352F0778D5@VAIEXCH04.vai.org>
References: <CEA39A213F7F2E44A0DED9210BCD352F0778D5@VAIEXCH04.vai.org>
Message-ID: <x2u0utzynq.fsf@biostat.ku.dk>

"Betten, Mark" <Mark.Betten at vai.org> writes:

> I would like to unsubscribe to the R help list so that my email box
> doesn't get so full.  I've tried several times to unsubscribe at
> r-project.org, but I keep getting emails.  How do I successfully
> unsubscribe?  Thanks
...
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help

Did you visit that page? (not r-project.org...)

Did you receive a confirmation request? And reply to it? (I think you
need that. I don't often unsubscribe myself...)

Did you wait long enough? If you unsubscribe, you will get removed
from the list of recipients for any new messages, but things that are
already in outgoing mail folders will not be actively removed from the
server.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From hedderich at medinfo.uni-kiel.de  Mon Aug 23 17:18:28 2004
From: hedderich at medinfo.uni-kiel.de (=?ISO-8859-1?Q?J=FCrgen_Hedderich?=)
Date: Mon, 23 Aug 2004 17:18:28 +0200
Subject: [R] Siegel-Tukey test
Message-ID: <412A0AC4.90408@medinfo.uni-kiel.de>

Hello R friends,

I am looking for a function to perform the Siegel-Tukey test in R.

Can anyvbody help me? Thanks so much!

J. Hedderich

-- 
---------------------------------------------------------------
Dipl. Inform. J. Hedderich
Institut f??r Medizinische Informatik und
Statistik im Klinikum S.H. - Campus Kiel -
                                            Phone : 0431/5973182
Brunswiker Str. 10                           Fax : 0431/5973193
24105 Kiel                E-mail: hedderich at medinfo.uni-kiel.de



From almirall at rand.org  Mon Aug 23 17:28:50 2004
From: almirall at rand.org (Almirall, Daniel)
Date: Mon, 23 Aug 2004 11:28:50 -0400
Subject: [R] unsubscribe to R-help
Message-ID: <5A637F509C50B444BDA096EB2D7BC158427FA8@pghmail2.rand.org>

R-help is a pretty incredible service.  Are you sure you would like to unsubscribe?

If you have concerns about your inbox getting so full, you may want to try a FILTER of some sort--Pine, Outlook, Eudora, etc... all of these have this option.  You can have all emails with "[R]" in the subject line routed to your very own "R-help Folder" (which you can create).  Then visit that folder whenever you want to get in on a good convo or learn new tricks.

CheeRs, Danny




-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Peter Dalgaard
Sent: Monday, August 23, 2004 11:01 AM
To: Betten, Mark
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] unsubscribe to R-help


"Betten, Mark" <Mark.Betten at vai.org> writes:

> I would like to unsubscribe to the R help list so that my email box
> doesn't get so full.  I've tried several times to unsubscribe at
> r-project.org, but I keep getting emails.  How do I successfully
> unsubscribe?  Thanks
.....
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help

Did you visit that page? (not r-project.org...)

Did you receive a confirmation request? And reply to it? (I think you
need that. I don't often unsubscribe myself...)

Did you wait long enough? If you unsubscribe, you will get removed
from the list of recipients for any new messages, but things that are
already in outgoing mail folders will not be actively removed from the
server.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From rpeng at jhsph.edu  Mon Aug 23 17:37:57 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Mon, 23 Aug 2004 11:37:57 -0400
Subject: [R] unsubscribe to R-help
In-Reply-To: <5A637F509C50B444BDA096EB2D7BC158427FA8@pghmail2.rand.org>
References: <5A637F509C50B444BDA096EB2D7BC158427FA8@pghmail2.rand.org>
Message-ID: <412A0F55.8070807@jhsph.edu>

There is also a digest option which is only mailed once daily.

-roger

Almirall, Daniel wrote:

> R-help is a pretty incredible service.  Are you sure you would like to unsubscribe?
> 
> If you have concerns about your inbox getting so full, you may want to try a FILTER of some sort--Pine, Outlook, Eudora, etc... all of these have this option.  You can have all emails with "[R]" in the subject line routed to your very own "R-help Folder" (which you can create).  Then visit that folder whenever you want to get in on a good convo or learn new tricks.
> 
> CheeRs, Danny
> 
> 
> 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Peter Dalgaard
> Sent: Monday, August 23, 2004 11:01 AM
> To: Betten, Mark
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] unsubscribe to R-help
> 
> 
> "Betten, Mark" <Mark.Betten at vai.org> writes:
> 
> 
>>I would like to unsubscribe to the R help list so that my email box
>>doesn't get so full.  I've tried several times to unsubscribe at
>>r-project.org, but I keep getting emails.  How do I successfully
>>unsubscribe?  Thanks
> 
> .....
> 
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
> 
> 
> Did you visit that page? (not r-project.org...)
> 
> Did you receive a confirmation request? And reply to it? (I think you
> need that. I don't often unsubscribe myself...)
> 
> Did you wait long enough? If you unsubscribe, you will get removed
> from the list of recipients for any new messages, but things that are
> already in outgoing mail folders will not be actively removed from the
> server.
>



From maechler at stat.math.ethz.ch  Mon Aug 23 17:57:26 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 23 Aug 2004 17:57:26 +0200
Subject: [R] unsubscribe to R-help
In-Reply-To: <412A0F55.8070807@jhsph.edu>
References: <5A637F509C50B444BDA096EB2D7BC158427FA8@pghmail2.rand.org>
	<412A0F55.8070807@jhsph.edu>
Message-ID: <16682.5094.303120.594547@gargle.gargle.HOWL>

>>>>> "Roger" == Roger D Peng <rpeng at jhsph.edu>
>>>>>     on Mon, 23 Aug 2004 11:37:57 -0400 writes:

    Roger> There is also a digest option which is only mailed once daily.
    Roger> -roger

and there's an option to 
    "don't get mail for the time being"
which I really recommend for people going away for a while who
don't even want to receive daily digests.

You can unset that option when you come back
(more easily than subscribe anew) and it gives a bit better
statistics about the number of R-help subscribers ;-)

-- and BTW: messages like the original shouldn't even make it to
   be posted the list; I do try trashing them before they waste everyone's
   time. At least subscribers should find out where to send such
   mails.  But this one slipped through my filters ...

Martin Maechler {Mailing list maintainer}


    Roger> Almirall, Daniel wrote:

    >> R-help is a pretty incredible service.  Are you sure you
    >> would like to unsubscribe?

    >> If you have concerns about your inbox getting so full, you may want to try a FILTER of some sort--Pine, Outlook, Eudora, etc... all of these have this option.  You can have all emails with "[R]" in the subject line routed to your very own "R-help Folder" (which you can create).  Then visit that folder whenever you want to get in on a good convo or learn new tricks.
    >> 
    >> CheeRs, Danny
    >> 
    >> 
    >> 
    >> 
    >> -----Original Message-----
    >> From: r-help-bounces at stat.math.ethz.ch
    >> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Peter Dalgaard
    >> Sent: Monday, August 23, 2004 11:01 AM
    >> To: Betten, Mark
    >> Cc: r-help at stat.math.ethz.ch
    >> Subject: Re: [R] unsubscribe to R-help
    >> 
    >> 
    >> "Betten, Mark" <Mark.Betten at vai.org> writes:
    >> 
    >> 
    >>> I would like to unsubscribe to the R help list so that my email box
    >>> doesn't get so full.  I've tried several times to unsubscribe at
    >>> r-project.org, but I keep getting emails.  How do I successfully
    >>> unsubscribe?  Thanks
    >> 
    >> .....
    >> 
    >>> R-help at stat.math.ethz.ch mailing list
    >>> https://stat.ethz.ch/mailman/listinfo/r-help
    >> 
    >> 
    >> Did you visit that page? (not r-project.org...)
    >> 
    >> Did you receive a confirmation request? And reply to it? (I think you
    >> need that. I don't often unsubscribe myself...)
    >> 
    >> Did you wait long enough? If you unsubscribe, you will get removed
    >> from the list of recipients for any new messages, but things that are
    >> already in outgoing mail folders will not be actively removed from the
    >> server.



From ahenningsen at email.uni-kiel.de  Mon Aug 23 18:01:39 2004
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Mon, 23 Aug 2004 18:01:39 +0200
Subject: [R] sample selection problem, inverse mills ratio (Heckman, Lewbel,
	...)
In-Reply-To: <53A181E56FB0694ABFD212F8AEDA7F6F1460E2@langouste.zhwin.ch>
References: <53A181E56FB0694ABFD212F8AEDA7F6F1460E2@langouste.zhwin.ch>
Message-ID: <200408231801.39266.ahenningsen@email.uni-kiel.de>

Hi,

I think you have to do these estimations "by hand". However, this shouldn't be 
too difficult. For instance the first step of the 2-step Heckman estimation 
is a probit estimation that can by done in R by 
   glm(  ... , family=binomial(link=probit))
(see ?glm). And the second step is a simple OLS regression.

All the best,
Arne


On Thursday 19 August 2004 11:45, Wildi Marc, wia wrote:
>  -----Urspr??ngliche Nachricht-----
>  Von: Wildi Marc, wia
>  Gesendet: Mittwoch, 18. August 2004 10:11
>  An: r-help at lists.R-project.org
>  Betreff:
>
>
>  Hi
>
>  Does anybody know from an R-package devoted to sample selection problems
> (Heckman's lambda, Lewbel, ...)?
>
>  Thanks and best regards
>
>  Marc Wildi
>
>
>
>
>  [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From Roger.Bivand at nhh.no  Mon Aug 23 18:19:28 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 23 Aug 2004 18:19:28 +0200 (CEST)
Subject: [R] Reading GAL file
In-Reply-To: <BAY18-DAV117tdMUc5f0000fa8c@hotmail.com>
Message-ID: <Pine.LNX.4.44.0408231814090.17611-100000@reclus.nhh.no>

On Mon, 23 Aug 2004, Mihai Nica wrote:

> Greetings:
> 
> I am trying to work with spdep (everything is "brand new" downloaded
> this morning). OS = Windows 2000 (also up to date). The code I am using
> follows:
> 
> #example
> 
> gal.county=read.geoda("lnpilnd.GAL", row.names=NULL, skip=0) 

Well, if you look at the help page for read.geoda() and read.gal(), you 
may see that read.gal() is what you use for reading the GAL file, and 
read.geoda() is just there as an easy way of using read.csv() for data 
files on the CSISS server. I can see that the help page could be clearer, 
it will be in the next release.

> summary.nb(gal.county)
> Error in summary.nb(gal.county) : Not a neighbours list
> 
> #end
> 
> The gal file works just fine in GeoDa (also up to date). 
> 
> Thanks, I am really stuck here.
> 
> Mihai Nica
> Jackson State University
> 155 B Parkhurst Dr.
> Jackson, MS 39202
> 601 969 5423
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From m_nica at hotmail.com  Mon Aug 23 18:25:44 2004
From: m_nica at hotmail.com (Mihai Nica)
Date: Mon, 23 Aug 2004 11:25:44 -0500
Subject: [R] Reading GAL file
References: <Pine.LNX.4.44.0408231814090.17611-100000@reclus.nhh.no>
Message-ID: <BAY18-DAV7A7JIaO8sW00011cd0@hotmail.com>

Oh, that was easy, THANKS!

Mihai Nica
Jackson State University
155 B Parkhurst Dr.
Jackson, MS 39202
601 969 5423
----- Original Message ----- 
From: "Roger Bivand" <Roger.Bivand at nhh.no>
To: "Mihai Nica" <m_nica at hotmail.com>
Cc: <r-help at stat.math.ethz.ch>
Sent: Monday, August 23, 2004 11:19 AM
Subject: Re: [R] Reading GAL file


> On Mon, 23 Aug 2004, Mihai Nica wrote:
>
> > Greetings:
> >
> > I am trying to work with spdep (everything is "brand new" downloaded
> > this morning). OS = Windows 2000 (also up to date). The code I am using
> > follows:
> >
> > #example
> >
> > gal.county=read.geoda("lnpilnd.GAL", row.names=NULL, skip=0)
>
> Well, if you look at the help page for read.geoda() and read.gal(), you
> may see that read.gal() is what you use for reading the GAL file, and
> read.geoda() is just there as an easy way of using read.csv() for data
> files on the CSISS server. I can see that the help page could be clearer,
> it will be in the next release.
>
> > summary.nb(gal.county)
> > Error in summary.nb(gal.county) : Not a neighbours list
> >
> > #end
> >
> > The gal file works just fine in GeoDa (also up to date).
> >
> > Thanks, I am really stuck here.
> >
> > Mihai Nica
> > Jackson State University
> > 155 B Parkhurst Dr.
> > Jackson, MS 39202
> > 601 969 5423
> > [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
> >
>
> -- 
> Roger Bivand
> Economic Geography Section, Department of Economics, Norwegian School of
> Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
> Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
> e-mail: Roger.Bivand at nhh.no
>
>
>



From FWS4 at CDRH.FDA.GOV  Mon Aug 23 20:09:29 2004
From: FWS4 at CDRH.FDA.GOV (Samuelson, Frank*)
Date: Mon, 23 Aug 2004 14:09:29 -0400
Subject: [R] More precision problems in testing with Intel compilers
Message-ID: <644D9337A02FC24689647BF9E48EC39E08ABB878@drm556>

I've put up the test problems I had with 'alternate' compilers at
http://www.merrill-samuelson.com/tmp

One is the d-p-q-r test output, and one is the complete print-tests.Rout
file.

I also put up a couple of failures from the reg-tests-1 file.
Even if you don't change anything else, one of these _needs_ changing.
There's a == comparison from the output of 2 different glm calls that
fails because the operands differ by
1/2 unit of machine precision.  (I added some digits to the print 
statements for more info.)  I'd recommend an all.equal() here.

And last, all the < 100*.Machine$double.eps comparisons in nafns.R appear to
fail by a factor of about 2.5.  There are some examples with extra prints.

-Frank


-----Original Message-----
From: Martin Maechler [mailto:maechler at stat.math.ethz.ch] 
Sent: Friday, August 20, 2004 6:39 PM
To: Samuelson, Frank*
Cc: 'r-help at stat.math.ethz.ch '
Subject: RE: [R] More precision problems in testing with Intel compilers


>>>>> "FrankSa" == Samuelson, Frank* <Samuelson>
>>>>>     on Thu, 19 Aug 2004 16:22:11 -0400 writes:

    FrankSa> The Intel compiled version also fails the below test:

here you give the desired output.
What does your 'Intel compiled R' return instead?

    >> ###------------ Very big and very small
    >> umach <- unlist(.Machine)[paste("double.x", c("min","max"), sep='')]
    >> xmin <- umach[1]
    >> xmax <- umach[2]
    >> tx <- unique(outer(-1:1,c(.1,1e-3,1e-7)))# 7 values  (out of 9)
    >> tx <- unique(sort(c(outer(umach,1+tx))))# 11 values  (out of 14)
    >> tx <- tx[is.finite(tx)] #-- all kept
    >> (txp <- tx[tx >= 1])#-- Positive exponent -- 4 values
     [1] 1.617924e+308 1.795895e+308 1.797693e+308 1.797693e+308
    >> (txn <- tx[tx <        1])#-- Negative exponent -- 7 values
    [1] 2.002566e-308 2.222849e-308 2.225074e-308 2.225074e-308
2.225074e-308 2.227299e-308 2.447581e-308

    FrankSa> Does anyone really care about being correct to 1
    FrankSa> unit of machine precision?  If you do, you have a
    FrankSa> bad algorithm.  ??

We have had these tests there for a long time now and haven't
heard of failures before..  so this is interesting.
DIG(7) makes us only look at 7 digits which is less than half machine
precision, but then there's cancellation of another 7 digits in
some of those above which gets in the region of machine precision,
(but still leaves a factor of ~= 45).

Can you upload the full print-test.Rout file somewhere?

Regards,
Martin


    FrankSa> -----Original Message-----
    FrankSa> From: Samuelson, Frank* [mailto:FWS4 at cdrh.fda.gov] 
    FrankSa> Sent: Thursday, August 19, 2004 12:11 PM
    FrankSa> To: 'r-help at stat.math.ethz.ch '
    FrankSa> Subject: [R] precision problems in testing with Intel compilers


    FrankSa> I compiled the 1.9.1 src.rpm with the standard gnu tools and it
works.
    FrankSa> I tried compiling the 1.9.1 src.rpm with the Intel 8 C and
FORTRAN
    FrankSa> compilers and it bombs out during the testing phase:

    FrankSa> comparing 'd-p-q-r-tests.Rout' to './d-p-q-r-tests.Rout.save'
...267c267
    FrankSa> < df = 0.5[1] "Mean relative  difference: 5.001647e-10"
    FrankSa> ---
    >> df = 0.5[1] TRUE
    FrankSa> make[3]: *** [d-p-q-r-tests.Rout] Error 1
    FrankSa> make[3]: Leaving directory
`/usr/src/redhat/BUILD/R-1.9.1/tests'
    FrankSa> make[2]: *** [test-Specific] Error 2
    FrankSa> make[2]: Leaving directory
`/usr/src/redhat/BUILD/R-1.9.1/tests'
    FrankSa> make[1]: *** [test-all-basics] Error 1
    FrankSa> make[1]: Leaving directory
`/usr/src/redhat/BUILD/R-1.9.1/tests'
    FrankSa> make: *** [check-all] Error 2
    FrankSa> error: Bad exit status from /var/tmp/rpm-tmp.63044 (%build)
    FrankSa> ...



From jfox at mcmaster.ca  Mon Aug 23 20:53:09 2004
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 23 Aug 2004 14:53:09 -0400
Subject: [R] Re: Giving a first good impression of R to Social Scientists
Message-ID: <20040823185310.OUBX4758.tomts13-srv.bellnexxia.net@JohnDesktop8300>

I apologize for joining this discussion late, but I was out of town and just
reviewed postings to R-help. I noticed that there were a couple of other
threads that also touched on similar issues.

Bob Andersen and I have written a paper on using R to teach social
statistics; a copy is at
<http://socserv.socsci.mcmaster.ca/jfox/Teaching-with-R.pdf>. It would be
tedious to rehash the paper in detail here, but I'll make a few brief
points:

(1) I believe that, except in special circumstances, GUIs to statistical
software are best for casual or infrequent use. Even users of SPSS who
intend to use the software seriously would be well advised to learn to write
commands. 

(2) The GUI in the Rcmdr package covers what's typically taught in first and
second social-statistics classes.

(3) The difficulty of moving from one statistical package (or computing
environment) to another shouldn't be confused with learning to use a
particular package (or environment) as a novice. I don't believe that it's
any more difficult to write R commands than, say, SAS or SPSS commands.

(4) As others have pointed out, the principal advantage of R (or another
statistical computing environment, as opposed to a statistical package) is
its programmability. Nevertheless, R is not difficult to teach and learn
even to those who will not (initially) be writing programs.

John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox



From harry at biochemistry.ucl.ac.uk  Mon Aug 23 23:03:35 2004
From: harry at biochemistry.ucl.ac.uk (Andrew Harrison)
Date: Mon, 23 Aug 2004 22:03:35 +0100 (BST)
Subject: [R] R on windows problem
Message-ID: <Pine.LNX.4.44.0408232202310.9088-100000@bsmlx01>

Hi,

	I've installed several versions of R on a number of Window 
systems. I get the following error message (which I don't get on linux)

> dev2bitmap(file="test.jpeg",type="jpeg")
Error in system(paste(gsexe, "-help"), intern = TRUE, invisible = TRUE) : 
        gswin32c.exe not found

Any idea how to fix this? It seems common to all windows systems I've 
tried. Is there a package I can install that will correct this?

	Best wishes,
		Harry


-- 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Dr Andrew Harrison                               Tel: 44 (0) 207 679 3890   
Biomolecular Structure and Modelling Unit        Fax: 44 (0) 207 679 7193
Biochemistry and Molecular Biology Dept.     
University College London
Gower Street                       	   Email: harry at biochem.ucl.ac.uk
London, WC1E 6BT, UK                  http://www.biochem.ucl.ac.uk/~harry
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



From gunter.berton at gene.com  Mon Aug 23 23:17:16 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 23 Aug 2004 14:17:16 -0700
Subject: [R] R on windows problem
References: <Pine.LNX.4.44.0408232202310.9088-100000@bsmlx01>
Message-ID: <412A5EDC.C5D8CCE2@gene.com>

Please read the Help file for dev2bitmap where it will it tell you to install and
how to set the required GhostScript path!


--

Bert Gunter

Non-Clinical Biostatistics
Genentech
MS: 240B
Phone: 650-467-7374


"The business of the statistician is to catalyze the scientific learning
process."

 -- George E.P. Box


Andrew Harrison wrote:

> Hi,
>
>         I've installed several versions of R on a number of Window
> systems. I get the following error message (which I don't get on linux)
>
> > dev2bitmap(file="test.jpeg",type="jpeg")
> Error in system(paste(gsexe, "-help"), intern = TRUE, invisible = TRUE) :
>         gswin32c.exe not found
>
> Any idea how to fix this? It seems common to all windows systems I've
> tried. Is there a package I can install that will correct this?
>
>         Best wishes,
>                 Harry
>
> --
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> Dr Andrew Harrison                               Tel: 44 (0) 207 679 3890
> Biomolecular Structure and Modelling Unit        Fax: 44 (0) 207 679 7193
> Biochemistry and Molecular Biology Dept.
> University College London
> Gower Street                               Email: harry at biochem.ucl.ac.uk
> London, WC1E 6BT, UK                  http://www.biochem.ucl.ac.uk/~harry
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Mon Aug 23 23:19:31 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 23 Aug 2004 22:19:31 +0100 (BST)
Subject: [R] R on windows problem
In-Reply-To: <Pine.LNX.4.44.0408232202310.9088-100000@bsmlx01>
Message-ID: <Pine.LNX.4.44.0408232214070.5551-100000@gannet.stats>

Please do as the posting guide asks and use a sensible subject line.

On Mon, 23 Aug 2004, Andrew Harrison wrote:

> 	I've installed several versions of R on a number of Window 
> systems. I get the following error message (which I don't get on linux)
> 
> > dev2bitmap(file="test.jpeg",type="jpeg")
> Error in system(paste(gsexe, "-help"), intern = TRUE, invisible = TRUE) : 
>         gswin32c.exe not found
> 
> Any idea how to fix this? It seems common to all windows systems I've 
> tried. Is there a package I can install that will correct this?

You:

1) Read the help page, on Windows.

     You will need a version of 'ghostscript' (5.10 and later have been
     tested): the full path to the executable can be set by the
     environment variable 'R_GSCMD'.

2) Install 'ghostscript' as it says.  If googling for that is beyond you,
http://www.cs.wisc.edu/~ghost/ should get you started.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From 0034058 at fudan.edu.cn  Tue Aug 24 07:59:38 2004
From: 0034058 at fudan.edu.cn (ronggui wong)
Date: Tue, 24 Aug 2004 13:59:38 +0800
Subject: [R] do linux-version r has gui?
In-Reply-To: <20040821154729.GA3312@igidr.ac.in>
References: <20040821154729.GA3312@igidr.ac.in>
Message-ID: <200408241355.35405.0034058@fudan.edu.cn>

it seems only has command line interface,is not it?
and i only have konqure as my browser, when i use help.start(),the help file 
does not appear.so waht should i do as to make it work ?

thank you!



From Paul.Boutros at utoronto.ca  Tue Aug 24 08:12:25 2004
From: Paul.Boutros at utoronto.ca (Paul Boutros)
Date: Tue, 24 Aug 2004 02:12:25 -0400
Subject: [R] Boxplot across levels of a factor
Message-ID: <CPEAKHBKLBNIKJDIELLCMEJGCLAA.Paul.Boutros@utoronto.ca>

Hello,

I have a data-frame in which one-column is a factor:

> str(data);
`data.frame':   194 obs. of  8 variables:
 $ Type         : Factor w/ 3 levels "Nuclear-Rec..",..: 1 2 2 2 2 2 2 2 2 2
...
 $ Locus        : num  0.000571 0.004000 0.001429 0.004857 0.007429 ...

And I'd like to make a boxplot of the data$Locus values, where each level of
the factor gets its own box-and-whiskers plot.  I'm weak in R, but I thought
there might be some shortcut to automating this instead of just creating a
new data-structure with all the separate values?

Any suggestions much appreciated!
Paul



From ripley at stats.ox.ac.uk  Tue Aug 24 08:27:57 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 24 Aug 2004 07:27:57 +0100 (BST)
Subject: [R] Boxplot across levels of a factor
In-Reply-To: <CPEAKHBKLBNIKJDIELLCMEJGCLAA.Paul.Boutros@utoronto.ca>
Message-ID: <Pine.LNX.4.44.0408240723440.15021-100000@gannet.stats>

On Tue, 24 Aug 2004, Paul Boutros wrote:

> Hello,
> 
> I have a data-frame in which one-column is a factor:
> 
> > str(data);
> `data.frame':   194 obs. of  8 variables:
>  $ Type         : Factor w/ 3 levels "Nuclear-Rec..",..: 1 2 2 2 2 2 2 2 2 2
> ...
>  $ Locus        : num  0.000571 0.004000 0.001429 0.004857 0.007429 ...
> 
> And I'd like to make a boxplot of the data$Locus values, where each level of
> the factor gets its own box-and-whiskers plot.  I'm weak in R, but I thought
> there might be some shortcut to automating this instead of just creating a
> new data-structure with all the separate values?

There are two.  The simpler is

	boxplot(Locus ~ Type, data=data)

and you can also use

	with(data, boxplot(split(Locus, Type)))

(split() does automate the construction of a suitable data structure.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Ivar.Herfindal at bio.ntnu.no  Tue Aug 24 08:41:14 2004
From: Ivar.Herfindal at bio.ntnu.no (Ivar Herfindal)
Date: Tue, 24 Aug 2004 08:41:14 +0200
Subject: [R] Boxplot across levels of a factor
In-Reply-To: <Pine.LNX.4.44.0408240723440.15021-100000@gannet.stats>
References: <Pine.LNX.4.44.0408240723440.15021-100000@gannet.stats>
Message-ID: <opsc74u0f8ndboo6@mail.bio.ntnu.no>

I think Paul wanted one plot for each box, not all boxes in one plot (sorry 
if I misunderstand).

One way to solve this can be like this:

par(mfrow=c(1,3))
with(data, by(1:nrow(data), Type, function(x) boxplot(Locus[x]~data[x]))

Hope this works for you.

Ivar

On Tue, 24 Aug 2004 07:27:57 +0100 (BST), Prof Brian Ripley 
<ripley at stats.ox.ac.uk> wrote:

> On Tue, 24 Aug 2004, Paul Boutros wrote:
>
>> Hello,
>>
>> I have a data-frame in which one-column is a factor:
>>
>> > str(data);
>> `data.frame':   194 obs. of  8 variables:
>> $ Type         : Factor w/ 3 levels "Nuclear-Rec..",..: 1 2 2 2 2 2 2 2 
>> 2 2
>> ...
>> $ Locus        : num  0.000571 0.004000 0.001429 0.004857 0.007429 ...
>>
>> And I'd like to make a boxplot of the data$Locus values, where each 
>> level of
>> the factor gets its own box-and-whiskers plot.  I'm weak in R, but I 
>> thought
>> there might be some shortcut to automating this instead of just creating 
>> a
>> new data-structure with all the separate values?
>
> There are two.  The simpler is
>
> 	boxplot(Locus ~ Type, data=data)
>
> and you can also use
>
> 	with(data, boxplot(split(Locus, Type)))
>
> (split() does automate the construction of a suitable data structure.)
>



From petr.pikal at precheza.cz  Tue Aug 24 08:51:57 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 24 Aug 2004 08:51:57 +0200
Subject: [R] Boxplot across levels of a factor
In-Reply-To: <CPEAKHBKLBNIKJDIELLCMEJGCLAA.Paul.Boutros@utoronto.ca>
Message-ID: <412B01AD.24327.3BD6E8@localhost>



On 24 Aug 2004 at 2:12, Paul Boutros wrote:

> Hello,
> 
> I have a data-frame in which one-column is a factor:
> 
> > str(data);
> `data.frame':   194 obs. of  8 variables:
>  $ Type         : Factor w/ 3 levels "Nuclear-Rec..",..: 1 2 2 2 2 2 2
>  2 2 2
> ...
>  $ Locus        : num  0.000571 0.004000 0.001429 0.004857 0.007429
>  ...
> 
> And I'd like to make a boxplot of the data$Locus values, where each
> level of the factor gets its own box-and-whiskers plot.  I'm weak in
> R, but I thought there might be some shortcut to automating this
> instead of just creating a new data-structure with all the separate
> values?
Hi

You have at least 2 options:

boxplot(data$Locus~data$Type)
boxplot(split(data$Locus,data$Type))

See ?boxplot for further options.

Cheers
Petr




> 
> Any suggestions much appreciated!
> Paul
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From ripley at stats.ox.ac.uk  Tue Aug 24 09:03:58 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 24 Aug 2004 08:03:58 +0100 (BST)
Subject: [R] Boxplot across levels of a factor
In-Reply-To: <opsc74u0f8ndboo6@mail.bio.ntnu.no>
Message-ID: <Pine.LNX.4.44.0408240745180.15071-100000@gannet.stats>

On Tue, 24 Aug 2004, Ivar Herfindal wrote:

> I think Paul wanted one plot for each box, not all boxes in one plot (sorry 
> if I misunderstand).
> 
> One way to solve this can be like this:
> 
> par(mfrow=c(1,3))
> with(data, by(1:nrow(data), Type, function(x) boxplot(Locus[x]~data[x]))

Actually, that does not come close to working.  Did you try it?
(Please read the documentation for by().)  The following does work

by(data, Type, function(x) boxplot(x$Locus, 
                                   main=paste("Type", unique(x$Type))))

but that would be a very unusual plot, and one in which the three plots
had different scalings so the boxplots could not be compared.

For something like that, lattice's bwplot is a better option.

bwplot(Locus ~ Type, data=data)  1 panel
bwplot(Locus ~ factor(rep(1, 182)) | Type, data=data) 3 panels

> 
> Hope this works for you.
> 
> Ivar
> 
> On Tue, 24 Aug 2004 07:27:57 +0100 (BST), Prof Brian Ripley 
> <ripley at stats.ox.ac.uk> wrote:
> 
> > On Tue, 24 Aug 2004, Paul Boutros wrote:
> >
> >> Hello,
> >>
> >> I have a data-frame in which one-column is a factor:
> >>
> >> > str(data);
> >> `data.frame':   194 obs. of  8 variables:
> >> $ Type         : Factor w/ 3 levels "Nuclear-Rec..",..: 1 2 2 2 2 2 2 2 
> >> 2 2
> >> ...
> >> $ Locus        : num  0.000571 0.004000 0.001429 0.004857 0.007429 ...
> >>
> >> And I'd like to make a boxplot of the data$Locus values, where each 
> >> level of
> >> the factor gets its own box-and-whiskers plot.  I'm weak in R, but I 
> >> thought
> >> there might be some shortcut to automating this instead of just creating 
> >> a
> >> new data-structure with all the separate values?
> >
> > There are two.  The simpler is
> >
> > 	boxplot(Locus ~ Type, data=data)
> >
> > and you can also use
> >
> > 	with(data, boxplot(split(Locus, Type)))
> >
> > (split() does automate the construction of a suitable data structure.)
> >
> 
> 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Ramzi_Nahhas at sil.org  Tue Aug 24 09:46:02 2004
From: Ramzi_Nahhas at sil.org (Ramzi Nahhas)
Date: Tue, 24 Aug 2004 14:46:02 +0700
Subject: [R] stem() bug?
Message-ID: <auto-000046747870@mail.link77.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040824/3987281e/attachment.pl

From asemeria at cramont.it  Tue Aug 24 09:53:36 2004
From: asemeria at cramont.it (asemeria@cramont.it)
Date: Tue, 24 Aug 2004 09:53:36 +0200
Subject: [R] do linux-version r has gui?
Message-ID: <OF784D9B44.5F2F88C9-ONC1256EFA.002B5C1C@tomware.it>

R-cmdr package is nice R-gui.
Help.start() work if your browser support java script,
then I think you haven't a java-plugin installed on your browser
Best regards!

A.S.

----------------------------

Alessandro Semeria
Models and Simulations Laboratory
Montecatini Environmental Research Center (Edison Group),
Via Ciro Menotti 48,
48023 Marina di Ravenna (RA), Italy
Tel. +39 544 536811
Fax. +39 544 538663
E-mail: alessandro.semeria at cramont.it



From ripley at stats.ox.ac.uk  Tue Aug 24 10:17:08 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 24 Aug 2004 09:17:08 +0100 (BST)
Subject: [R] stem() bug?
In-Reply-To: <auto-000046747870@mail.link77.net>
Message-ID: <Pine.LNX.4.44.0408240903460.21322-100000@gannet.stats>

It is as intended.  The answer is correct but your interpretation of it is 
not, and the help page is woefully lacking.

Note that the bins in your problem are [20, 40), [40, 60), [60, 80) and 
[80, 100) and they are correctly labelled.  In such bins e.g. 20 and 30 
are both represented in the same way.

Does stem(seq(30, 92, 5)) help you see the pattern?

Please would one of those bemoaning R's documentation recently submit to
R-bugs an enhanced help page for stem?

On Tue, 24 Aug 2004, Ramzi Nahhas wrote:

> 
> Is the following a bug with stem() or is there something else that I am
> missing? I ran stem() on the vector x below and got stem(x-10) instead of
> stem(x). If I subtract 1 from x, I get a correct answer. If I add 1 to x, I
> still get a wrong answer. If I add 10 to x, I get a correct answer. I'm not
> sure what to make of this, other than to think it is a bug.
> 
> Can anyone tell me if this is a bug?
> 
> I am running R 1.9.1 on Windows XP (SP2).
> 
> Sincerely,
> 	Ramzi Nahhas
> 
> PS In your reply, please copy me at Ramzi_Nahhas at sil.org as I am not
> subscribed to this list. Thank you.
> 
> > x
>  [1] 30 70 90 75 70 95 75 70 60 55
> > stem(x)
> 
>   The decimal point is 1 digit(s) to the right of the |
> 
>   2 | 0
>   4 | 5
>   6 | 000055
>   8 | 05
> 
> > stem(x-1)
> 
>   The decimal point is 1 digit(s) to the right of the |
> 
>   2 | 9
>   4 | 49
>   6 | 99944
>   8 | 94
> 
> > stem(x+1)
> 
>   The decimal point is 1 digit(s) to the right of the |
> 
>   2 | 1
>   4 | 6
>   6 | 111166
>   8 | 16
> 
> > stem(x+10)
> 
>   The decimal point is 1 digit(s) to the right of the |
> 
>    4 | 0
>    6 | 50
>    8 | 00055
>   10 | 05
> 
> ---
> 
> 
> 
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From l.houdusse at cerep.fr  Tue Aug 24 10:58:06 2004
From: l.houdusse at cerep.fr (Laurent Houdusse)
Date: Tue, 24 Aug 2004 10:58:06 +0200
Subject: [R] How to get Dunnett's table value?
Message-ID: <BA420EFAAC96D311A7A0006097D37BDB04515D22@EOLE>

Hi,

I want to retrieve a value in the Dunnett's tables.
I know the comparisons type, the percent of level,the number of traitments
and the degrees of freedom.
Is there a function to retrieve this value with this?



Laurent Houdusse 
Analyste Programmeur



From blh at mssl.ucl.ac.uk  Tue Aug 24 11:11:18 2004
From: blh at mssl.ucl.ac.uk (Benjamin Lloyd-Hughes)
Date: Tue, 24 Aug 2004 10:11:18 +0100
Subject: [R] rgdal under windows?
In-Reply-To: <Pine.LNX.4.44.0408191957500.3940-100000@reclus.nhh.no>
Message-ID: <000201c489ba$535aabb0$434e2880@geol.ucl.ac.uk>

Success!

I'm pleased to inform that Roger's recipe worked a treat for compiling
rgdal_0.2-7 in conjunction with gdal-1.2.1 and proj-4.4.8.  Using
MinGW-3.1.0 with MYS-1.0.8 on an xp pro box (SP1) Proj-4 compiled straight
out of the box. Gdal needed a couple of minor tweaks wrt how the boolean
type is defined.  I suspect that gcc is being strict about name spaces.  My
hack was an explicit typedef in jpeglib.h: typedef unsigned char boolean;
and to comment out the definition in jmorecfg.h
After that it was plain sailing except for the curious behaviour of the
MinGW dlltool.exe which refused to to pick up R.dll unless it was given the
full path _irrespective_ of what was placed in $PATH.

Big thanks to Roger and Brian for their helpful advice.  Complicated? Not if
you learn to read!

Cheers, Ben

-----Original Message-----
From: Roger Bivand [mailto:Roger.Bivand at nhh.no] 
Sent: 19 August 2004 19:27
To: Prof Brian Ripley
Cc: Benjamin Lloyd-Hughes; r-help at stat.math.ethz.ch
Subject: Re: [R] rgdal under windows?

On Thu, 19 Aug 2004, Prof Brian Ripley wrote:

> On Thu, 19 Aug 2004, Benjamin Lloyd-Hughes wrote:
> 
> > Has anyone had any joy getting the rgdal package to compile under
windows?
> 
> First you need to meet the system requirements:
> 
> SystemRequirements: GDAL library from 
> http://www.remotesensing.org/gdal/download.html
> 
> Since that has build instructions which say
> 
>   GDAL can be built on Windows using MS VC++ 6.x and MS Visual Studio .NET

>   (C++) at the DOS command line.
> 
> you will first need to get a copy of DOS and those compilers.  (I suspect
> they mean the Windows command prompt, but they don't seem to know the
> difference which is not confidence-inspiring.  I also suspect they mean
> `or' not `and'.)  Then you will have to fathom out how to link against a
> VC++ DLL which since this C++ is very unlikely to work with MinGW.  So you
> probably need to make the package's rgdal.dll under VC++ -- see
> README.packages.
> 
> I have tried and failed to make GDAL with MinGW.
> 
> > I've been trying with MinGW using Duncan Murdoch's Rtools but to no
avail.
> 
> Do read README.packages and follow the advice at the top.  You don't have 
> the (mis-attributed) tools in your path.  But that is probably the least 
> of your potential problems.
> 

Exactly. The closest anyone has got so far is Hisaji Ono, who used MSYS 
(http://www.mingw.org/) to build PROJ.4 and GDAL (GDAL depends on PROJ.4, 
PROJ.4 needs a PATH to metadata files for projection and transformation), 
and then hand-pasted the paths to the GDAL headers and library into 
src/Makevars, running Rcmd INSTALL rgdal at the Windows command prompt as 
usual. All of this can be repeated, but is not portable, and does not suit 
the very valuable standard binary package build system for Windows. 
Roughly:

1. Download everything you need to build source packages under Windows and 
make sure it works;

2. Download MSYS and make sure it works;

3. Download the GDAL and PROJ.4 source tarballs, and possibly other
libraries you want to use with GDAL, and *within MSYS* untar, ./configure
with the appropriate arguments, at least make but maybe also make install
- now leave MSYS;

4. Download the rgdal source package, and *at the Windows command prompt*
untar it, change the name of configure to something else, create
src/Makevars manually from src/Makevars.in, insert the correct values of:
PKG_CPPFLAGS as -I<path to the directory with the GDAL headers>, PKG_LIBS
as -L< ... GDAL libraries> -lgdal, run Rcmd INSTALL rgdal (better first 
Rcmd check rgdal), and repeat until all the problems are resolved.

5. The installation should work locally, but some paths are compiled into 
the resulting *.dll, so it will, most likely, not be portable.

Both Hisaji and I were surprised that this actually seemed to work (as of
late Autumn last year for the versions of MinGW and MSYS available then;  
further fragility is introduced by much of GDAL being written in C++); it
is not wholly impossible that it could be made available portably as a
Windows binary with an installer, but not through the regular R binary
package repositories.

> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From vikas at mail.jnu.ac.in  Tue Aug 24 11:35:59 2004
From: vikas at mail.jnu.ac.in (Vikas Rawal)
Date: Tue, 24 Aug 2004 15:05:59 +0530
Subject: [R] Test of significance in estimation of correlation coefficients
Message-ID: <412B0BFF.2010102@mail.jnu.ac.in>

I estimated spearman's correlation coefficient using cor(). How do I 
test for significance?

Vikas



From baron at psych.upenn.edu  Tue Aug 24 11:43:41 2004
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Tue, 24 Aug 2004 05:43:41 -0400
Subject: [R] Test of significance in estimation of correlation coefficients
In-Reply-To: <412B0BFF.2010102@mail.jnu.ac.in>
References: <412B0BFF.2010102@mail.jnu.ac.in>
Message-ID: <20040824094341.GA16546@psych>

On 08/24/04 15:05, Vikas Rawal wrote:
>I estimated spearman's correlation coefficient using cor(). How do I
>test for significance?

cor.test

You could find this many ways, but one is to look at the help
document for cor.  cor.test is listed is at the bottom.  In
general, it is a good idea to look at help before posting here.

BTW, there is a small bug in the documentation for cor: cor.test
is no longer in the ctest package, but in stats.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron
R search page: http://finzi.psych.upenn.edu/



From ripley at stats.ox.ac.uk  Tue Aug 24 11:55:59 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 24 Aug 2004 10:55:59 +0100 (BST)
Subject: [R] Test of significance in estimation of correlation coefficients
In-Reply-To: <20040824094341.GA16546@psych>
Message-ID: <Pine.LNX.4.44.0408241052200.4870-100000@gannet.stats>

On Tue, 24 Aug 2004, Jonathan Baron wrote:

> BTW, there is a small bug in the documentation for cor: cor.test
> is no longer in the ctest package, but in stats.

Your information is not up to date: that page has been changed in
R-patched and R-devel. 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From fredrik.karlsson at ling.umu.se  Tue Aug 24 12:51:23 2004
From: fredrik.karlsson at ling.umu.se (Fredrik Karlsson)
Date: Tue, 24 Aug 2004 12:51:23 +0200
Subject: [R] Maxima/minima of loess line?
Message-ID: <200408241251.24043.fredrik.karlsson@ling.umu.se>

Dear list,

I've produced a loess line that I would like to investigate in terms of 
local/global maxima and minima. How would you do this?

Thank you in advance.

/Fredrik Karlsson



From purmann at mi.fu-berlin.de  Tue Aug 24 12:56:13 2004
From: purmann at mi.fu-berlin.de (purmann@mi.fu-berlin.de)
Date: Tue, 24 Aug 2004 12:56:13 +0200
Subject: [R] Run function in BATCH mode?
Message-ID: <1093344973.412b1ecd47e31@webmail.mi.fu-berlin.de>

Hi,

I am working on a shell script where i need to start a R function defined in a
script. I know how to run the script; with 
R CMD BATCH script 
which would be equal to 
source("script") in R. 
But how do I run a function(param) of that script in the BATCH mode? 
Thanks in advance.



From purmann at mi.fu-berlin.de  Tue Aug 24 12:56:08 2004
From: purmann at mi.fu-berlin.de (purmann@mi.fu-berlin.de)
Date: Tue, 24 Aug 2004 12:56:08 +0200
Subject: [R] Run function in BATCH mode?
Message-ID: <1093344968.412b1ec8b346a@webmail.mi.fu-berlin.de>

Hi,

I am working on a shell script where i need to start a R function defined in a
script. I know how to run the script; with 
R CMD BATCH script 
which would be equal to 
source("script") in R. 
But how do I run a function(param) of that script in the BATCH mode? 
Thanks in advance.



From ligges at statistik.uni-dortmund.de  Tue Aug 24 13:10:41 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 24 Aug 2004 13:10:41 +0200
Subject: [R] Run function in BATCH mode?
In-Reply-To: <1093344973.412b1ecd47e31@webmail.mi.fu-berlin.de>
References: <1093344973.412b1ecd47e31@webmail.mi.fu-berlin.de>
Message-ID: <412B2231.4050106@statistik.uni-dortmund.de>

purmann at mi.fu-berlin.de wrote:

> Hi,
> 
> I am working on a shell script where i need to start a R function defined in a
> script. I know how to run the script; with 
> R CMD BATCH script 
> which would be equal to 
> source("script") in R. 
> But how do I run a function(param) of that script in the BATCH mode? 
> Thanks in advance.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Say you have a function MyFoo defined in foo.R then you create a file 
call.R that sources foo.r (source("foo.R")) and calls your function by, 
e.g., MyFoo(a, b, x).

Then just say "R CMD BATCH call.R" ...

Uwe Ligges



From martinol at ensam.inra.fr  Tue Aug 24 13:57:57 2004
From: martinol at ensam.inra.fr (Martin Olivier)
Date: Tue, 24 Aug 2004 13:57:57 +0200
Subject: [R] help with knn from class library
Message-ID: <412B2D45.8020909@ensam.inra.fr>

Hi all,

I made some computations with the knn function from the class library.
If I execute this function several times (with the same parameters k, 
training set and test set), I obtain different results.
I don't understand why the results for my test set are different. Could 
you give me some explanations?
Is the solution for  a k-nearest-neighbor classifier  unique?

Best regards.



From andy_liaw at merck.com  Tue Aug 24 14:22:35 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 24 Aug 2004 08:22:35 -0400
Subject: [R] help with knn from class library
Message-ID: <3A822319EB35174CA3714066D590DCD504AF8287@usrymx25.merck.com>

> From: Martin Olivier
> 
> Hi all,
> 
> I made some computations with the knn function from the class library.
> If I execute this function several times (with the same parameters k, 
> training set and test set), I obtain different results.
> I don't understand why the results for my test set are 
> different. Could 
> you give me some explanations?
> Is the solution for  a k-nearest-neighbor classifier  unique?

Could it be that you have ties in the data?  There's a `use.all' argument:

 use.all: controls handling of ties. If true, all distances equal to
          the 'k'th largest are included. If false, a random selection
          of distances equal to the 'k'th is chosen to use exactly 'k'
          neighbours. 

So you may want to try that.  The other thing to try is to set.seed() before
calling knn().

Andy

 
> Best regards.



From andy_liaw at merck.com  Tue Aug 24 14:23:49 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 24 Aug 2004 08:23:49 -0400
Subject: [R] Maxima/minima of loess line?
Message-ID: <3A822319EB35174CA3714066D590DCD504AF8288@usrymx25.merck.com>

Just take range() of the fitted loess values.

Andy

> From: Fredrik Karlsson
> 
> Dear list,
> 
> I've produced a loess line that I would like to investigate 
> in terms of 
> local/global maxima and minima. How would you do this?
> 
> Thank you in advance.
> 
> /Fredrik Karlsson



From jarioksa at sun3.oulu.fi  Tue Aug 24 15:38:57 2004
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: 24 Aug 2004 16:38:57 +0300
Subject: [R] Maxima/minima of loess line?
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF8288@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF8288@usrymx25.merck.com>
Message-ID: <1093354737.8056.10.camel@biol102145.oulu.fi>

On Tue, 2004-08-24 at 15:23, Liaw, Andy wrote:
> Just take range() of the fitted loess values.
> 
Or if you really want to investigate the *line* instead of some random
*points*, you may need something like:

> optimize(function(x, mod) predict(mod, data.frame(speed=x)), c(0,20),
maximum=TRUE, mod=cars.lo)
$maximum
[1] 19.99995
 
$objective
[1] 56.44498

This elaborates the ?loess example with the result object cars.lo (and,
of course is a bad example since the fit is monotone and solutions is
forced to the margin). Use maximum=FALSE for *a* minimum.

If you have several predictors, you either need to supply constant
values for those in optimize, or for simultaneous search in all use
optim or nlm.

cheers, jari oksanen
> 
> > From: Fredrik Karlsson
> > 
> > Dear list,
> > 
> > I've produced a loess line that I would like to investigate 
> > in terms of 
> > local/global maxima and minima. How would you do this?
> > 
> > Thank you in advance.
> > 
> > /Fredrik Karlsson
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From szhan at uoguelph.ca  Tue Aug 24 16:21:11 2004
From: szhan at uoguelph.ca (szhan@uoguelph.ca)
Date: Tue, 24 Aug 2004 10:21:11 -0400
Subject: [R] how to in XML on windows XP for R 1.9.1
Message-ID: <1093357271.412b4ed72fe9d@webmail.uoguelph.ca>

Dear R experts:
I tried two ways to install Package:XML on windows xp for R 1.9.1, all failed.
Messages are given as bellows:
1> download from CRAN
> install.packages("XML", CRAN = getOption("CRAN"),
+ contriburl = contrib.url("http://cran.r-project.org"),
+ available = NULL, destdir = NULL,
+ installWithVers = FALSE)
trying URL `http://cran.r-project.org/bin/windows/contrib/1.9/PACKAGES'
Content type `text/plain; charset=iso-8859-1' length 20065 bytes
opened URL
downloaded 19Kb

Warning message:
No package "XML" on CRAN. in: download.packages(pkgs, destdir = tmpd, available
= available,

2> Download binary code XML_0.97.0.zip from http://www.omegahat.org/RSXML/
unzipt to goXML directory:
> install.packages("C:/goXML/XML")
trying URL `http://cran.r-project.org/bin/windows/contrib/1.9/PACKAGES'
Content type `text/plain; charset=iso-8859-1' length 20065 bytes
opened URL
downloaded 19Kb

Warning message:
No package "C:/goXML/XML" on CRAN. in: download.packages(pkgs, destdir = tmpd,
available = available,

Could you please help me install it sucessfully?
Thank you for your hellp!



From ripley at stats.ox.ac.uk  Tue Aug 24 16:32:54 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 24 Aug 2004 15:32:54 +0100 (BST)
Subject: [R] how to in XML on windows XP for R 1.9.1
In-Reply-To: <1093357271.412b4ed72fe9d@webmail.uoguelph.ca>
Message-ID: <Pine.LNX.4.44.0408241531430.2102-100000@gannet.stats>

On Tue, 24 Aug 2004 szhan at uoguelph.ca wrote:

> I tried two ways to install Package:XML on windows xp for R 1.9.1, all failed.

But you did not read the ReadMe's.

Do read the ReadMe at http://cran.r-project.org/bin/windows/contrib/1.9.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From xliu5 at uky.edu  Tue Aug 24 16:42:11 2004
From: xliu5 at uky.edu (Xun Liu)
Date: Tue, 24 Aug 2004 10:42:11 -0400
Subject: [R] rgl installation problems
Message-ID: <200408241444.i7OEiE209029@mr1.uky.edu>

It is indeed the following problem. I used the RPM installation as of July 20, 2004 and had the same problem installing rgl. After fixing the following line in the Makeconf file ("-share" is missing), the installation of rgl went successfully.

>This is a little strange.  I'm now building RPMS for older Red Hat versions on FC2 using a tool called Mach.  There is a possibility that
>there is some configuration problem, but I can't see it. As Brian has pointed out, you are missing the crucial "-shared" flag when building
>the shared library for rgl. This comes from the line
>
>SHLIB_CXXLDFLAGS = -shared
>
>in the file /usr/lib/R/etc/Makeconf.  This is present in my latest RPM for Red Hat ( R-1.9.1-0.fdr.2.rh90.i386.rpm ) so I don't know why it
>isn't working for you.

>>Here's what should happen on RH9 (with the latest libpng and gcc in /usr/local/lib) 
>>
>>g++ -shared -L/usr/local/lib -o rgl.so x11lib.o x11gui.o types.o math.o fps.o pixmap.o gui.o api.o device.o devicemanager.o rglview.o scene.o 
>>glgui.o -L/usr/X11R6/lib -L/usr/local/lib -Wl,-rpath,/usr/local/lib -lpng12 -lz -lm -lstdc++ -lX11 -lXext -lGL -lGLU -lpng12 -lz -lm 

>>Note that he had no --shared but did have crt1.o, that is was trying to build a standalone executable and not a shared object. 

>>Something is wrong with the R installation's rules to make shared libraries. 

>>On Fri, 25 Jun 2004, Duncan Murdoch wrote: 

>> On Fri, 25 Jun 2004 17:36:30 +0000, "E GCP" <egcp at hotmail.com> wrote : 
>> 
>> >Hi! 
>> > 
>> >I'm new to R, but have worked with Splus before. I installed several 
>> >packages in R (R-1.9.1) without problems, but when I try to install rgl 
>> >(rgl_0.64-13.tar.gz). I get the following, and the package does not install. 
>> >Any help would be greatly appreciated. I'm running R in redhat 9. 
>> 
>> The missing reference R_InputHandlers is declared in the 
>> $RHOME/src/include/R_ext/eventloop.h file, and I believe is compiled 
>> into the library R_X11 (with some extension). You don't seem to have 
>> that in the list of libraries: 
>> 
>> >g++ -L/usr/local/lib -o rgl.so x11lib.o x11gui.o types.o math.o fps.o 
>> >pixmap.o 
>> >gui.o api.o device.o devicemanager.o rglview.o scene.o glgui.o 
>> >-L/usr/X11R6/lib 
>> >-L/usr/lib -lstdc++ -lX11 -lXext -lGL -lGLU -lpng 
>> >/usr/lib/gcc-lib/i386-redhat-linux/3.2.2/../../../crt1.o(.text+0x18): In 
>> >functio 
>> >n `_start': 
>> >../sysdeps/i386/elf/start.S:77: undefined reference to `main' 
>> >x11lib.o(.text+0x84): In function `set_R_handler': 
>> >/tmp/R.INSTALL.8663/rgl/src/x11gui.h:33: undefined reference to 
>> >`R_InputHandlers 
>> 
>> I don't know what you'll need to do to fix this, since I'm using 
>> Windows, so none of this stuff happens there, and I could be 
>> completely wrong about it.



From dj at research.bell-labs.com  Tue Aug 24 16:43:16 2004
From: dj at research.bell-labs.com (David James)
Date: Tue, 24 Aug 2004 10:43:16 -0400
Subject: [R] ROracle and vector elements
In-Reply-To: <BAY12-F32SXRu4eoqsD000bbc90@hotmail.com>;
	from lauraholt_983@hotmail.com on Thu, Aug 19, 2004 at 03:43:37PM
	-0500
References: <BAY12-F32SXRu4eoqsD000bbc90@hotmail.com>
Message-ID: <20040824104316.C21684@jessie.research.bell-labs.com>

Hi Laura,

Sorry for the delay, but I was away and I'm finally catching up...

I do have a windows binary at
   http://stat.bell-labs.com/RS-DBI/download/
but note that you need to have the Oracle's client software.

Hope this helps,

--
David

Laura Holt wrote:
> Hi there!
> 
> Is ROracle available for Windows, please?
> 
> I found a download site, but it's really for UNIX/Linux.
> 
> Here is a "thought question", please:  Why do the vector elements start at 
> location 1 rather than zero, as C does?
> 
> Thanks in advance!
> 
> R Version 1.9.1 Windows
> Sincerely,
> Laura Holt
> mailto: lauraholt_983 at hotmail.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
David A. James
Statistics Research, Room 2C-253            Phone:  (908) 582-3082       
Bell Labs, Lucent Technologies              Fax:    (908) 582-3340
Murray Hill, NJ 09794-0636



From lichi at physics.uoguelph.ca  Tue Aug 24 17:31:15 2004
From: lichi at physics.uoguelph.ca (lichi@physics.uoguelph.ca)
Date: Tue, 24 Aug 2004 11:31:15 -0400 (EDT)
Subject: [R] how to set the number format to pure numeric?
Message-ID: <1117.131.104.152.222.1093361475.squirrel@131.104.152.222>

Hello,

I want to export a numeric matrix in pure numeric format, i.e. I want
0.0001 to appear as "0.0001". But it seems the default setting for
write.table is scientific notation, i.e. it will appear as "1e-04". how to
set the number format to pure numeric? Thank you very much for your help.

cheers,
lichi shi



From szhan at uoguelph.ca  Tue Aug 24 18:07:46 2004
From: szhan at uoguelph.ca (szhan@uoguelph.ca)
Date: Tue, 24 Aug 2004 12:07:46 -0400
Subject: [R] how to in XML on windows XP for R 1.9.1
In-Reply-To: <412B53E0.1030002@statistik.uni-dortmund.de>
References: <Pine.LNX.4.44.0408241531430.2102-100000@gannet.stats>
	<412B53E0.1030002@statistik.uni-dortmund.de>
Message-ID: <1093363665.412b67d2022f7@webmail.uoguelph.ca>

Hello, Uwe,
Thank you all for your reply!
I installed binary packages XML_0.97.0.zip by "Install package(s) from local zip
file" and followed instruction in the doc:
 you will have to add the XML/libs
directory to your path.  Putting them into the
directory which contains Rgui.exe and Rterm.exe for this
version of R is probably best.
I then put add the XML/libs directory to director rw1091/bin, but it still gave
messagae like this:
> library("XML")
Error in dyn.load(x, as.logical(local), as.logical(now)) :
        unable to load shared library
"C:/PROGRA~1/R/rw1091/library/XML/libs/XML.dll":
  LoadLibrary failure:  The specified module could not be found.
Error in library("XML") : package/namespace load failed
What should I do now?
Josh
Quoting Uwe Ligges <ligges at statistik.uni-dortmund.de>:

> Prof Brian Ripley wrote:
> > On Tue, 24 Aug 2004 szhan at uoguelph.ca wrote:
> >
> >
> >>I tried two ways to install Package:XML on windows xp for R 1.9.1, all
> failed.
> >
> >
> > But you did not read the ReadMe's.
> >
> > Do read the ReadMe at http://cran.r-project.org/bin/windows/contrib/1.9.
>
> Additionally, after you have got the right package, read the docs how to
> install binary packages from the local hard disc.
>
> Uwe Ligges
>



From ripley at stats.ox.ac.uk  Tue Aug 24 18:09:26 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 24 Aug 2004 17:09:26 +0100 (BST)
Subject: [R] how to set the number format to pure numeric?
In-Reply-To: <1117.131.104.152.222.1093361475.squirrel@131.104.152.222>
Message-ID: <Pine.LNX.4.44.0408241705300.2242-100000@gannet.stats>

On Tue, 24 Aug 2004 lichi at physics.uoguelph.ca wrote:

> I want to export a numeric matrix in pure numeric format, i.e. I want
> 0.0001 to appear as "0.0001". But it seems the default setting for
> write.table is scientific notation, i.e. it will appear as "1e-04". how to
> set the number format to pure numeric? Thank you very much for your help.

It's not the default, BTW.  Try 0.002, say.
See ?options and look at `scipen' to alter the rules.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Tue Aug 24 18:15:10 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 24 Aug 2004 18:15:10 +0200
Subject: [R] how to in XML on windows XP for R 1.9.1
In-Reply-To: <1093363665.412b67d2022f7@webmail.uoguelph.ca>
References: <Pine.LNX.4.44.0408241531430.2102-100000@gannet.stats>
	<412B53E0.1030002@statistik.uni-dortmund.de>
	<1093363665.412b67d2022f7@webmail.uoguelph.ca>
Message-ID: <412B698E.2090400@statistik.uni-dortmund.de>

szhan at uoguelph.ca wrote:

> Hello, Uwe,
> Thank you all for your reply!
> I installed binary packages XML_0.97.0.zip 

Hear, hear. To my knowledge, there are the versions
0.94-1 and 0.95-6 of XML available at the URL given in the ReadMe cited 
by Brian Ripley. Note that this ReadMe points to 
http://www.stats.ox.ac.uk/pub/RWin/ !!!

Uwe Ligges



 > by "Install package(s) from local zip
> file" and followed instruction in the doc:
>  you will have to add the XML/libs
> directory to your path.  Putting them into the
> directory which contains Rgui.exe and Rterm.exe for this
> version of R is probably best.
> I then put add the XML/libs directory to director rw1091/bin, but it still gave
> messagae like this:
> 
>>library("XML")
> 
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>         unable to load shared library
> "C:/PROGRA~1/R/rw1091/library/XML/libs/XML.dll":
>   LoadLibrary failure:  The specified module could not be found.
> Error in library("XML") : package/namespace load failed
> What should I do now?
> Josh
> Quoting Uwe Ligges <ligges at statistik.uni-dortmund.de>:
> 
> 
>>Prof Brian Ripley wrote:
>>
>>>On Tue, 24 Aug 2004 szhan at uoguelph.ca wrote:
>>>
>>>
>>>
>>>>I tried two ways to install Package:XML on windows xp for R 1.9.1, all
>>
>>failed.
>>
>>>
>>>But you did not read the ReadMe's.
>>>
>>>Do read the ReadMe at http://cran.r-project.org/bin/windows/contrib/1.9.
>>
>>Additionally, after you have got the right package, read the docs how to
>>install binary packages from the local hard disc.
>>
>>Uwe Ligges
>>
> 
>



From sanguissola at rcsi.ie  Tue Aug 24 18:14:18 2004
From: sanguissola at rcsi.ie (Sergio Anguissola)
Date: Tue, 24 Aug 2004 17:14:18 +0100
Subject: [R] firewall or proxy problems
Message-ID: <3DBACD0C44A1CF429431C0C6DC3E8169385198@crc_exchange.rcsi-internal.ie>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040824/059fcf32/attachment.pl

From ripley at stats.ox.ac.uk  Tue Aug 24 18:31:18 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 24 Aug 2004 17:31:18 +0100 (BST)
Subject: [R] how to in XML on windows XP for R 1.9.1
In-Reply-To: <1093363665.412b67d2022f7@webmail.uoguelph.ca>
Message-ID: <Pine.LNX.4.44.0408241714540.2287-100000@gannet.stats>

On Tue, 24 Aug 2004 szhan at uoguelph.ca wrote:

> Hello, Uwe,
> Thank you all for your reply!
> I installed binary packages XML_0.97.0.zip by "Install package(s) from local zip
> file" and followed instruction in the doc:

That's not the instructions in the README!

>  you will have to add the XML/libs
> directory to your path.  Putting them into the
> directory which contains Rgui.exe and Rterm.exe for this
> version of R is probably best.
> I then put add the XML/libs directory to director rw1091/bin, but it still gave
> messagae like this:

What it actually says is

but to use it in R, you will have to add the XML/libs
directory to your path or copy the files
  libxml2.dll zlib.dll iconv.dll
to a directory in your path.  Putting them into the
directory which contains Rgui.exe and Rterm.exe for this
version of R is probably best.

and there are no such files in the distribution.

> > library("XML")
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>         unable to load shared library
> "C:/PROGRA~1/R/rw1091/library/XML/libs/XML.dll":
>   LoadLibrary failure:  The specified module could not be found.
> Error in library("XML") : package/namespace load failed
> What should I do now?

Asking Uwe questions about a package his ReadMe explicitly says he does
not support is unfair.  

Since XML_0.97-0 is not a CRAN package, please ask its author 
directly.  It depends on a libxml2.dll (etc) he has not included.

[This is part of the reason why I statically link libxml into the port of
XML that I make available.  Another is that with a dynamic link the 
internet connection possibilities do not work correctly, at least when I 
build it.

Another issue is that you need different builds for different versions of
R, and www.omegahat.org/RSXML does not say which it was built for.]

> Josh
> Quoting Uwe Ligges <ligges at statistik.uni-dortmund.de>:
> 
> > Prof Brian Ripley wrote:
> > > On Tue, 24 Aug 2004 szhan at uoguelph.ca wrote:
> > >
> > >
> > >>I tried two ways to install Package:XML on windows xp for R 1.9.1, all
> > failed.
> > >
> > >
> > > But you did not read the ReadMe's.
> > >
> > > Do read the ReadMe at http://cran.r-project.org/bin/windows/contrib/1.9.
> >
> > Additionally, after you have got the right package, read the docs how to
> > install binary packages from the local hard disc.
> >
> > Uwe Ligges
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Tue Aug 24 18:39:07 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 24 Aug 2004 18:39:07 +0200
Subject: [R] firewall or proxy problems
In-Reply-To: <3DBACD0C44A1CF429431C0C6DC3E8169385198@crc_exchange.rcsi-internal.ie>
References: <3DBACD0C44A1CF429431C0C6DC3E8169385198@crc_exchange.rcsi-internal.ie>
Message-ID: <412B6F2B.2040902@statistik.uni-dortmund.de>

Sergio Anguissola wrote:

> Hi, I right now installed the R and then I tried to download and install the
> get bioC package, but I receive an error message saying that the software
> couldn't connect trough the port 80. I went to the FAQ and I found that I
> have to type somewhere -internet 2

So you are on Windows?
You are going to *invoke* R the way mentioned there. Just change the 
link that calls R in the described manner (BTW: it says "--internet2"!). 
If you don't know how to change a link, please contact local Windows 
support.

Uwe Ligges


> , but I couldn't understand when and
> where, could you help me?
> 
> Thanks.
> 
> Sergio.
> 
>  
> 
> Sergio Anguissola Ph.D 
> Dept. of Physiology 
> RCSI, Education & Research Center 
> Smurfit Building, Beaumont Hospital 
> Dublin 9 
> Republic of Ireland 
> Tel: 01-809-3861 
> Fax: 01-809-3837 
> Email: sanguissola at rcsi.ie
> 
>  
> 
> --------------------------------------------------------------------------------------------------------------------
> This email and any files transmitted with it are confidentia...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From White.Denis at epamail.epa.gov  Tue Aug 24 18:42:58 2004
From: White.Denis at epamail.epa.gov (White.Denis@epamail.epa.gov)
Date: Tue, 24 Aug 2004 09:42:58 -0700
Subject: [R] apply ( , , table)
Message-ID: <OFB614788D.32106E7F-ON88256EFA.005AD44B-88256EFA.005BC943@epamail.epa.gov>





a <- matrix (c(
    7, 1, 1, 2, 6,
    3, 4, 0, 1, 4,
    5, 1, 8, 4, 4,
    6, 1, 1, 2, 5), nrow=4, byrow=TRUE)

b <- apply (a, 1, table)

"apply" documentation says clearly that if the rows of the result of FUN
are the same length, then an array will be returned.  And column-major
would be the appropriate order in R.  But "b" above is pretty opaque
compared to what one would expect, and what one would get from "apply (
, , table)" if the rows were not of equal length.  One needs to do
something like

n <- matrix (apply (a, 1, function (x) unique (sort (x))), nrow=nrow(a))

to get the corresponding "names" of "b" to figure out the counts.

Denis White



From paolo at directwave.com.br  Tue Aug 24 18:50:34 2004
From: paolo at directwave.com.br (Paolo Tommasini)
Date: Tue, 24 Aug 2004 13:50:34 -0300
Subject: [R] (no subject)
Message-ID: <412B71DA.8060209@directwave.com.br>

Hi my name is Paolo Tommasini does anyone know how to compute a "mode"  
( most frequent element ) for a distribution ?

thanks



From baron at psych.upenn.edu  Tue Aug 24 19:01:19 2004
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Tue, 24 Aug 2004 13:01:19 -0400
Subject: [R] (no subject)
In-Reply-To: <412B71DA.8060209@directwave.com.br>
References: <412B71DA.8060209@directwave.com.br>
Message-ID: <20040824170119.GA18952@psych>

On 08/24/04 13:50, Paolo Tommasini wrote:
>Hi my name is Paolo Tommasini does anyone know how to compute a "mode"
>( most frequent element ) for a distribution ?

which.max

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron
R search page: http://finzi.psych.upenn.edu/



From sbchapman at highstream.net  Tue Aug 24 19:05:32 2004
From: sbchapman at highstream.net (Sam Chapman)
Date: Tue, 24 Aug 2004 13:05:32 -0400
Subject: [R] The "Green" Book?
Message-ID: <1093367132.412b755c67fba@webmail.highstream.net>

Thank you for your perspective . . . I see what you mean, and that's actually
more or less how I had interpreted Professor Ripley's remarks. I'll have to
investigate further the advantages of using S4 classes in my work. I suspect
they'll eventually prove their worth. Thanks again for the clarification. SC

Quoting "Liaw, Andy" <andy_liaw at merck.com>:

> Sam,
>
> What BDR meant (I believe) is that it depends on _how_ you intend to use R,
> not what your background is.  If you are going to develop code using the new
> S4 classes, the green book will be relevant.  If you are going to use R for
> data analysis, there's probably little to gain by reading the green book.
>
> Best,
> Andy
>
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Sam Chapman
> > Sent: Monday, August 23, 2004 10:16 AM
> > To: Prof Brian Ripley
> > Cc: Thomas Lumley; r-help at stat.math.ethz.ch
> > Subject: Re: [R] The "Green" Book?
> >
> >
> > Thank you for your responses. I should have mentioned that I
> > am new to R, but
> > not to programming. Nevertheless, the insights are valued and
> > appreciated!
> >
> >
> > Quoting Prof Brian Ripley <ripley at stats.ox.ac.uk>:
> >
> > > On Fri, 20 Aug 2004, Thomas Lumley wrote:
> > >
> > > > On Fri, 20 Aug 2004, Sam Chapman wrote:
> > > >
> > >
> > > [A quote from `An Introduction to R' has been excised here]
> > >
> > > > > There is no mention of 'Programming with Data: A Guide
> > to the S Language'
> > > by
> > > > > John M. Chambers. Is this newest ("Green") book also
> > suitable as a
> > > reference
> > > > > for R? Thank you for your time and attention!
> > > > >
> > > >
> > > > Yes. The system implemented in the "methods" package is
> > not identical to
> > > > that in the Green Book, but it's pretty similar.
> > >
> > > Well, it is suitable as reference for programmers using the
> > "methods"
> > > package in R, not quite the question asked.  At the level of `An
> > > Introduction to R' it is not really a suitable reference as
> > it has limited
> > > coverage at that level.  (The Green Book itself recommends
> > other books for
> > > end users.)
> > >
> > > --
> > > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > > University of Oxford,             Tel:  +44 1865 272861 (self)
> > > 1 South Parks Road,                     +44 1865 272866 (PA)
> > > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> > >
> > >
> >
> >
> > Sincerely,
> >
> > Sam Chapman
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
>
>
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachments, contains
> information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New
> Jersey, USA 08889), and/or its affiliates (which may be known outside the
> United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as
> Banyu) that may be confidential, proprietary copyrighted and/or legally
> privileged. It is intended solely for the use of the individual or entity
> named on this message.  If you are not the intended recipient, and have
> received this message in error, please notify us immediately by reply e-mail
> and then delete it from your system.
> ------------------------------------------------------------------------------
>


Sincerely,

Sam Chapman



From gunter.berton at gene.com  Tue Aug 24 19:17:25 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 24 Aug 2004 10:17:25 -0700
Subject: [R] (no subject)
In-Reply-To: <20040824170119.GA18952@psych>
Message-ID: <200408241717.i7OHHPD3011643@hertz.gene.com>

No, this is not quite right: 

which.max

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron
R search page: http://finzi.psych.upenn.edu/

You need to be more careful 

1. For a continuous distribution, the mode is ** not ** the most frequent
value (this has no meaning) -- it is the maximum of the density function. So
you must fit a density function and then estimate the mode, perhaps by using
which.max() on a discretization of the fitted density. Or, if you have the
fitted density in closed form (and it is unimodal) you can use calculus.

2. For a discrete distribution, which could be the sample empirical
distribution,use which.max(table()). 

-- Bert Gunter



From Charles.Annis at StatisticalEngineering.com  Tue Aug 24 19:18:50 2004
From: Charles.Annis at StatisticalEngineering.com (Charles Annis, P.E.)
Date: Tue, 24 Aug 2004 13:18:50 -0400
Subject: [R]  mode of a distribution -  was (no subject)
In-Reply-To: <20040824170119.GA18952@psych>
Message-ID: <200408241718.i7OHInap020949@hypatia.math.ethz.ch>

Perhaps a time out might be helpful.

?which.max will produce the location (index) of the maximum of a numeric
vector, but Paolo doesn't want that.  He wants the location of the most
frequent observation which would be the maximum of the probability density,
which he doesn't have.  

It would be useful to estimate what kind of density represents the data and
then, given this, determine its interesting statistics, including the mode.


Charles Annis, P.E.
 
Charles.Annis at StatisticalEngineering.com
phone: 561-352-9699
eFax:  503-217-5849
http://www.StatisticalEngineering.com

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jonathan Baron
Sent: Tuesday, August 24, 2004 1:01 PM
To: Paolo Tommasini
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] (no subject)

On 08/24/04 13:50, Paolo Tommasini wrote:
>Hi my name is Paolo Tommasini does anyone know how to compute a "mode"
>( most frequent element ) for a distribution ?

which.max

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron
R search page: http://finzi.psych.upenn.edu/

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From tplate at blackmesacapital.com  Tue Aug 24 19:20:03 2004
From: tplate at blackmesacapital.com (Tony Plate)
Date: Tue, 24 Aug 2004 11:20:03 -0600
Subject: [R] apply ( , , table)
In-Reply-To: <OFB614788D.32106E7F-ON88256EFA.005AD44B-88256EFA.005BC943@
	epamail.epa.gov>
References: <OFB614788D.32106E7F-ON88256EFA.005AD44B-88256EFA.005BC943@epamail.epa.gov>
Message-ID: <6.1.0.6.2.20040824111213.06523a80@mailhost.blackmesacapital.com>

apply() tries to be a bit smart about what it does (sometimes maybe too 
smart), but it actually is pretty useful a lot of the time.  It's extremely 
widely used, so changing the behavior is not an option -- changing the 
behavior would break a lot of existing code.  (Personally, I'd prefer it if 
apply() put its dimensions back together in a slightly more intelligent 
way, i.e., if apply(x, 1, c) and apply(x, 2, c) returned the same thing, 
but apply is how it is.)

In situations where you don't want apply() to try to construct a matrix 
from your results, you can wrap the results in a list, to force apply() to 
return just a list of results, e.g. (the outer "lapply()" strips off an 
unnecessary level of list depth):

 > b2 <- lapply(apply (a, 1, function(x) list(table(x))), "[[", 1)
 > length(b2)
[1] 4
 > b2[[1]]
x
1 2 6 7
2 1 1 1
 > attributes(b2[[1]])
$dim
[1] 4

$dimnames
$dimnames$x
[1] "1" "2" "6" "7"


$class
[1] "table"

Your particular case might benefit from more information given to table, 
which allows it to provide results in a more uniform format, e.g.:

 > b1 <- apply (a, 1, function(x) table(factor(x, levels=0:9)))
 > b1
   [,1] [,2] [,3] [,4]
0    0    1    0    0
1    2    1    1    2
2    1    0    0    1
3    0    1    0    0
4    0    2    2    0
5    0    0    1    1
6    1    0    0    1
7    1    0    0    0
8    0    0    1    0
9    0    0    0    0
 >

hope this helps,

Tony Plate

At Tuesday 10:42 AM 8/24/2004, White.Denis at epamail.epa.gov wrote:




>a <- matrix (c(
>     7, 1, 1, 2, 6,
>     3, 4, 0, 1, 4,
>     5, 1, 8, 4, 4,
>     6, 1, 1, 2, 5), nrow=4, byrow=TRUE)
>
>b <- apply (a, 1, table)
>
>"apply" documentation says clearly that if the rows of the result of FUN
>are the same length, then an array will be returned.  And column-major
>would be the appropriate order in R.  But "b" above is pretty opaque
>compared to what one would expect, and what one would get from "apply (
>, , table)" if the rows were not of equal length.  One needs to do
>something like
>
>n <- matrix (apply (a, 1, function (x) unique (sort (x))), nrow=nrow(a))
>
>to get the corresponding "names" of "b" to figure out the counts.
>
>Denis White
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From brahm at alum.mit.edu  Tue Aug 24 19:35:29 2004
From: brahm at alum.mit.edu (David Brahm)
Date: Tue, 24 Aug 2004 13:35:29 -0400
Subject: [R] how to set the number format to pure numeric?
Message-ID: <16683.31841.292104.22018@arbres1a.fmr.com>

lichi shi <lichi at physics.uoguelph.ca> wrote:
> I want to export a numeric matrix in pure numeric format, i.e. I want
> 0.0001 to appear as "0.0001". But it seems the default setting for
> write.table is scientific notation, i.e. it will appear as "1e-04". how
> to set the number format to pure numeric?

Try:
R> options(scipen=99)
which sets a very high SCIentific notation PENalty.
-- 
                              -- David Brahm (brahm at alum.mit.edu)



From tplate at blackmesacapital.com  Tue Aug 24 19:43:42 2004
From: tplate at blackmesacapital.com (Tony Plate)
Date: Tue, 24 Aug 2004 11:43:42 -0600
Subject: [R] (no subject)
In-Reply-To: <20040824170119.GA18952@psych>
References: <412B71DA.8060209@directwave.com.br> <20040824170119.GA18952@psych>
Message-ID: <6.1.0.6.2.20040824114125.06556380@mailhost.blackmesacapital.com>

Looks like there might have been some truncation of Jonathon Baron's message.

Here's one way of computing the sample mode of a vector.

 > set.seed(1)
 > x <- sample(1:5,20,rep=T)
 > x
  [1] 2 2 3 5 2 5 5 4 4 1 2 1 4 2 4 3 4 5 2 4
 > table(x)
x
1 2 3 4 5
2 6 2 6 4
 > names(which.max(table(x)))
[1] "2"
 >

Note that this method returns the first max value in the case of ties.

hope this helps,

Tony Plate

At Tuesday 11:01 AM 8/24/2004, Jonathan Baron wrote:
>On 08/24/04 13:50, Paolo Tommasini wrote:
> >Hi my name is Paolo Tommasini does anyone know how to compute a "mode"
> >( most frequent element ) for a distribution ?
>
>which.max
>
>--
>Jonathan Baron, Professor of Psychology, University of Pennsylvania
>Home page: http://www.sas.upenn.edu/~baron
>R search page: http://finzi.psych.upenn.edu/
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From lichi at physics.uoguelph.ca  Tue Aug 24 20:16:12 2004
From: lichi at physics.uoguelph.ca (lichi@physics.uoguelph.ca)
Date: Tue, 24 Aug 2004 14:16:12 -0400 (EDT)
Subject: [R] "how to set the number format to pure numeric?" was SOLVED
Message-ID: <1293.131.104.152.222.1093371372.squirrel@131.104.152.222>

Thank you for your kind help. I am done now.

regards,
lichi



From elvis at xlsolutions-corp.com  Tue Aug 24 20:35:16 2004
From: elvis at xlsolutions-corp.com (elvis@xlsolutions-corp.com)
Date: Tue, 24 Aug 2004 11:35:16 -0700
Subject: [R] R/S-plus Course***September,
	2004***R/Splus Fundamentals and Programming Techniques
Message-ID: <20040824183516.27217.qmail@webmail03.mesa1.secureserver.net>

XLSolutions Corporation (www.xlsolutions-corp.com) is proud 
to announce August 2004 2-day "R/S-plus Fundamentals and 
Programming Techniques". 

****Washington, DC --------------------------> September, 23-24
****Chicago, IL        --------------------------> September, 16-17

Interested in our R/Splus Advanced Programming course? Please email 
us! 
Reserve your seat now at the early bird rates! Payment due AFTER the 
class. 


Course Description: 
This two-day beginner to intermediate R/S-plus course focuses 
on a broad spectrum of topics, 
from reading raw data to a comparison of R and S. We will learn 
the essentials of data manipulation, graphical visualization 
and R/S-plus programming. We will explore statistical data analysis 
tools,including graphics with data sets. How to enhance your plots. 
We will perform basic statistics and fit linear regression models. 
Participants are encouraged to bring data for interactive sessions 


With the following outline: 
- An Overview of R and S 
- Data Manipulation and Graphics 
- Using Lattice Graphics 
- A Comparison of R and S-Plus 
- How can R Complement SAS? 
- Writing Functions 
- Avoiding Loops 
- Vectorization 
- Statistical Modeling 
- Project Management 
- Techniques for Effective use of R and S 
- Enhancing Plots 
- Using High-level Plotting Functions 
- Building and Distributing Packages (libraries) 


Email us for group discounts. 
Email Sue Turner: sue at xlsolutions-corp.com 

Visit us: www.xlsolutions-corp.com/training.htm 
Please let us know if you and your colleagues are interested in this 
classto take advantage of group discount. Register now to secure your 
seat! 
Interested in R/Splus Advanced course? email us. 


Cheers, 
Elvis Miller, PhD 
Manager Training. 
XLSolutions Corporation 
206 686 1578 
www.xlsolutions-corp.com 
elvis at xlsolutions-corp.com



From morey at banta.psyc.missouri.edu  Tue Aug 24 20:42:53 2004
From: morey at banta.psyc.missouri.edu (Richard Morey)
Date: Tue, 24 Aug 2004 13:42:53 -0500 (CDT)
Subject: [R] bug or no?
Message-ID: <Pine.LNX.4.44.0408241341370.18254-100000@banta.psyc.missouri.edu>

The following code prints [1] 2, as it should

temp<-function(ab,...){
print(ab)
}
temp(2,s=3)

However, this code prints [1] 3:

temp<-function(sb,...){
print(sb)
}
temp(2,s=3)

It should still print [1] 2. It appears
that if a variable in ... begins with the same letter as another variable,
the value in the variable in ... overwrites the value in the variable with
the same first letter.

I didn't see this bug reported elsewhere.

Richard Morey

Is this a bug or am I missing something?

output of R.version
platform i686-pc-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status
major    1
minor    9.1
year     2004
month    06
day      21
language R


-- 
Richard Morey
Graduate Research Assistant, Cognition and Neuroscience
University of Missouri-Columbia



From sundar.dorai-raj at PDF.COM  Tue Aug 24 20:56:59 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Tue, 24 Aug 2004 11:56:59 -0700
Subject: [R] bug or no?
In-Reply-To: <Pine.LNX.4.44.0408241341370.18254-100000@banta.psyc.missouri.edu>
References: <Pine.LNX.4.44.0408241341370.18254-100000@banta.psyc.missouri.edu>
Message-ID: <412B8F7B.5030502@pdf.com>



Richard Morey wrote:

> The following code prints [1] 2, as it should
> 
> temp<-function(ab,...){
> print(ab)
> }
> temp(2,s=3)
> 
> However, this code prints [1] 3:
> 
> temp<-function(sb,...){
> print(sb)
> }
> temp(2,s=3)
> 
> It should still print [1] 2. It appears
> that if a variable in ... begins with the same letter as another variable,
> the value in the variable in ... overwrites the value in the variable with
> the same first letter.
> 
> I didn't see this bug reported elsewhere.
> 
> Richard Morey
> 
> Is this a bug or am I missing something?
> 

You are missing something. Mainly, reading section 4.3 of "R Language 
Definition", which should come with your distribution or is accessible 
from CRAN.

http://cran.r-project.org/doc/manuals/R-lang.pdf

--sundar



From ccleland at optonline.net  Tue Aug 24 21:08:37 2004
From: ccleland at optonline.net (Chuck Cleland)
Date: Tue, 24 Aug 2004 15:08:37 -0400
Subject: [R] bug or no?
In-Reply-To: <Pine.LNX.4.44.0408241341370.18254-100000@banta.psyc.missouri.edu>
References: <Pine.LNX.4.44.0408241341370.18254-100000@banta.psyc.missouri.edu>
Message-ID: <412B9235.5090901@optonline.net>

   I don't think it's a bug, it's partial matching of 
argument names.  To avoid it, use exact argument names in 
your call to the function:

temp(sb=2,s=3)
[1] 2

   From the R language definition:

"Argument matching: Formal arguments are matched to supplied 
arguments first by exact matching on tags, then by partial 
matching on tags, and finally by positional matching."

http://cran.r-project.org/doc/contrib/R_language.pdf

Richard Morey wrote:
> The following code prints [1] 2, as it should
> 
> temp<-function(ab,...){
> print(ab)
> }
> temp(2,s=3)
> 
> However, this code prints [1] 3:
> 
> temp<-function(sb,...){
> print(sb)
> }
> temp(2,s=3)
> 
> It should still print [1] 2. It appears
> that if a variable in ... begins with the same letter as another variable,
> the value in the variable in ... overwrites the value in the variable with
> the same first letter.
> 
> I didn't see this bug reported elsewhere.
> 
> Richard Morey
> 
> Is this a bug or am I missing something?
> 
> output of R.version
> platform i686-pc-linux-gnu
> arch     i686
> os       linux-gnu
> system   i686, linux-gnu
> status
> major    1
> minor    9.1
> year     2004
> month    06
> day      21
> language R
> 
> 


-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From ggrothendieck at myway.com  Tue Aug 24 21:22:31 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 24 Aug 2004 19:22:31 +0000 (UTC)
Subject: [R] bug or no?
References: <Pine.LNX.4.44.0408241341370.18254-100000@banta.psyc.missouri.edu>
	<412B9235.5090901@optonline.net>
Message-ID: <loom.20040824T211809-292@post.gmane.org>


Note that R requires that anything placed AFTER the ... 
to be matched exactly so one can force exact matching
on sb by putting it after the ... like this:

R> temp<-function(...,sb) print(sb)
R> temp(2,s=3)
Error in print(sb) : Argument "sb" is missing, with no default



Chuck Cleland <ccleland <at> optonline.net> writes:

: 
: I don't think it's a bug, it's partial matching of 
: argument names.  To avoid it, use exact argument names in 
: your call to the function:
: 
: temp(sb=2,s=3)
: [1] 2
: 
:    From the R language definition:
: 
: "Argument matching: Formal arguments are matched to supplied 
: arguments first by exact matching on tags, then by partial 
: matching on tags, and finally by positional matching."
: 
: http://cran.r-project.org/doc/contrib/R_language.pdf
: 
: Richard Morey wrote:
: > The following code prints [1] 2, as it should
: > 
: > temp<-function(ab,...){
: > print(ab)
: > }
: > temp(2,s=3)
: > 
: > However, this code prints [1] 3:
: > 
: > temp<-function(sb,...){
: > print(sb)
: > }
: > temp(2,s=3)
: > 
: > It should still print [1] 2. It appears
: > that if a variable in ... begins with the same letter as another variable,
: > the value in the variable in ... overwrites the value in the variable with
: > the same first letter.
: > 
: > I didn't see this bug reported elsewhere.
: > 
: > Richard Morey
: > 
: > Is this a bug or am I missing something?
: > 
: > output of R.version
: > platform i686-pc-linux-gnu
: > arch     i686
: > os       linux-gnu
: > system   i686, linux-gnu
: > status
: > major    1
: > minor    9.1
: > year     2004
: > month    06
: > day      21
: > language R
: > 
: > 
:



From von-hippel.1 at osu.edu  Tue Aug 24 23:47:15 2004
From: von-hippel.1 at osu.edu (Paul von Hippel)
Date: Tue, 24 Aug 2004 17:47:15 -0400
Subject: [R] analyzing cluster sample
Message-ID: <5.2.0.9.2.20040824173529.01b22690@mail.sociology.ohio-state.edu>

I am analyzing a survey where ~20,000 cases were sampled in ~1000 clusters. 
I would like to analyze the data using, for example, gam. What is the best 
way to account for the clustering? I've tried including the cluster ID as a 
factor in the model formula, but the default response is to try and 
estimate the unique effect of each cluster, which given 1000 clusters is 
impossibly time consuming. What I want instead is an estimate of the 
variance due to clusters, or perhaps an intraclass correlation, and 
cluster-adjusted standard errors for the effects of other variables in the 
model.

I expect I can account for clustering by using lme with clusters as a 
random effect, but then I can't use the flexible smooths available in gam.

If it's not possible to get both clustering and smooths, I may use gam and 
adjust the standard errors using an estimate of the design effect.

Many thanks for any advice,
Paul

Paul von Hippel
Department of Sociology / Initiative in Population Research
Ohio State University



From ihok at hotmail.com  Wed Aug 25 01:11:50 2004
From: ihok at hotmail.com (Jack Tanner)
Date: Tue, 24 Aug 2004 19:11:50 -0400
Subject: [R] stem() bug?
Message-ID: <BAY22-F5WkDCFMcrKpW00024e01@hotmail.com>

<insert obligatory bemoan>

First, I'm curious. Is there any reason why the leaves are squished together 
with no delimeters?

Second, the crucial point that seems to be missing from the stem help page 
(and one I can't glean without going through the code) is how bins are 
determined. If that can be explained to the reader, the explanation of scale 
follows naturally.

Third, feel free to incorporate the following text into the help page as 
necessary; I've tried to make it helpful (but clarity is obviously in the 
eye of the beholder).

stem produces a stem-and-leaf plot of the elements in x. Each element is 
placed in a bin ranging [low, high); that is, if y is an element, it is 
placed in the bin iff low <= element < high. The size of the bin is 
determined by... .

The stem that is listed for the bin is not necessarily the stem of all (or 
any) of the leaves in the bin; the true stems must be equal to or higher 
than the listed stems, but they will all be smaller than the  lowerst stem 
of the next highest bin. (Observe how the bin for the stem 4 changes with 
the two examples below.)

Leaves are ordered within a bin. If leaves seem unordered (e.g., 4 | 0306), 
then the scale of the plot may be too compressed: that bin contains leaves 
from multiple different stems. The parameter scale can be used to expand the 
scale of the plot.

Arguments
x 	a numeric vector.
scale 	This controls the number of bins. A value of scale=2 will cause the 
plot to be twice as long as the default by using roughly twice as many bins.
width 	The desired width of the plot, in characters.
atom 	a tolerance [This needs better text.]

Examples
stem(c(30, 32, 50, 52, 60, 62, 70, 72, 80, 82))
stem(c(30, 32, 50, 52, 60, 62, 70, 72, 80, 82), scale=2)

----Original Message Follows----
From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
To: Ramzi Nahhas <Ramzi_Nahhas at sil.org>
CC: ihok at hotmail.com, <jc at or.psychology.dal.ca>, <r-help at stat.math.ethz.ch>
Subject: Re: [R] stem() bug?
Date: Tue, 24 Aug 2004 09:17:08 +0100 (BST)

It is as intended.  The answer is correct but your interpretation of it is
not, and the help page is woefully lacking.

Note that the bins in your problem are [20, 40), [40, 60), [60, 80) and
[80, 100) and they are correctly labelled.  In such bins e.g. 20 and 30
are both represented in the same way.

Does stem(seq(30, 92, 5)) help you see the pattern?

Please would one of those bemoaning R's documentation recently submit to
R-bugs an enhanced help page for stem?

On Tue, 24 Aug 2004, Ramzi Nahhas wrote:

 >
 > Is the following a bug with stem() or is there something else that I am
 > missing? I ran stem() on the vector x below and got stem(x-10) instead of
 > stem(x). If I subtract 1 from x, I get a correct answer. If I add 1 to x, 
I
 > still get a wrong answer. If I add 10 to x, I get a correct answer. I'm 
not
 > sure what to make of this, other than to think it is a bug.
 >
 > Can anyone tell me if this is a bug?
 >
 > I am running R 1.9.1 on Windows XP (SP2).
 >
 > Sincerely,
 > 	Ramzi Nahhas
 >
 > PS In your reply, please copy me at Ramzi_Nahhas at sil.org as I am not
 > subscribed to this list. Thank you.
 >
 > > x
 >  [1] 30 70 90 75 70 95 75 70 60 55
 > > stem(x)
 >
 >   The decimal point is 1 digit(s) to the right of the |
 >
 >   2 | 0
 >   4 | 5
 >   6 | 000055
 >   8 | 05
 >
 > > stem(x-1)
 >
 >   The decimal point is 1 digit(s) to the right of the |
 >
 >   2 | 9
 >   4 | 49
 >   6 | 99944
 >   8 | 94
 >
 > > stem(x+1)
 >
 >   The decimal point is 1 digit(s) to the right of the |
 >
 >   2 | 1
 >   4 | 6
 >   6 | 111166
 >   8 | 16
 >
 > > stem(x+10)
 >
 >   The decimal point is 1 digit(s) to the right of the |
 >
 >    4 | 0
 >    6 | 50
 >    8 | 00055
 >   10 | 05
 >
 > ---
 >
 >
 >
 >
 >
 > 	[[alternative HTML version deleted]]
 >
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://stat.ethz.ch/mailman/listinfo/r-help
 > PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html
 >
 >

--
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Paul.Boutros at utoronto.ca  Wed Aug 25 06:58:26 2004
From: Paul.Boutros at utoronto.ca (Paul Boutros)
Date: Wed, 25 Aug 2004 00:58:26 -0400
Subject: [R] Boxplot across levels of a factor
In-Reply-To: <Pine.LNX.4.44.0408240723440.15021-100000@gannet.stats>
Message-ID: <CPEAKHBKLBNIKJDIELLCOEJLCLAA.Paul.Boutros@utoronto.ca>

Thank you: works like a charm.
Paul

> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Sent: Tuesday, August 24, 2004 2:28 AM
> To: Paul Boutros
> Cc: R-Help
> Subject: Re: [R] Boxplot across levels of a factor
> 
> 
> On Tue, 24 Aug 2004, Paul Boutros wrote:
> 
> > Hello,
> > 
> > I have a data-frame in which one-column is a factor:
> > 
> > > str(data);
> > `data.frame':   194 obs. of  8 variables:
> >  $ Type         : Factor w/ 3 levels "Nuclear-Rec..",..: 1 2 2 
> 2 2 2 2 2 2 2
> > ...
> >  $ Locus        : num  0.000571 0.004000 0.001429 0.004857 0.007429 ...
> > 
> > And I'd like to make a boxplot of the data$Locus values, where 
> each level of
> > the factor gets its own box-and-whiskers plot.  I'm weak in R, 
> but I thought
> > there might be some shortcut to automating this instead of just 
> creating a
> > new data-structure with all the separate values?
> 
> There are two.  The simpler is
> 
> 	boxplot(Locus ~ Type, data=data)
> 
> and you can also use
> 
> 	with(data, boxplot(split(Locus, Type)))
> 
> (split() does automate the construction of a suitable data structure.)
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From nleonard at tartarus.uwa.edu.au  Wed Aug 25 06:59:52 2004
From: nleonard at tartarus.uwa.edu.au (Neil Leonard)
Date: Wed, 25 Aug 2004 12:59:52 +0800
Subject: [R] Adding labels to variables
Message-ID: <9987DFF6-F653-11D8-8EA2-003065D5B8EC@tartarus.uwa.edu.au>

Hi,

Is it possible to add labels to variables in R (so as to have a better 
description of what the variables represent)?


Thanks,
Neil



From maustin at amgen.com  Wed Aug 25 07:15:15 2004
From: maustin at amgen.com (Austin, Matt)
Date: Tue, 24 Aug 2004 22:15:15 -0700
Subject: [R] Adding labels to variables
Message-ID: <E7D5AB4811D20B489622AABA9C53859101F11190@teal-exch.amgen.com>

Check out the Hmisc library.

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Neil Leonard
Sent: Tuesday, August 24, 2004 22:0 PM
To: R-help at stat.math.ethz.ch
Subject: [R] Adding labels to variables


Hi,

Is it possible to add labels to variables in R (so as to have a better 
description of what the variables represent)?


Thanks,
Neil

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Wed Aug 25 08:20:27 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 25 Aug 2004 06:20:27 +0000 (UTC)
Subject: [R] Adding labels to variables
References: <9987DFF6-F653-11D8-8EA2-003065D5B8EC@tartarus.uwa.edu.au>
Message-ID: <loom.20040825T081606-120@post.gmane.org>

Neil Leonard <nleonard <at> tartarus.uwa.edu.au> writes:

: Is it possible to add labels to variables in R (so as to have a better 
: description of what the variables represent)?

One can add arbitrarily named attributes to objects.  For example,
we can add an attribute which we shall call memo to object z:

R> z <- 3
R> attr(z,"memo") <- "very important number"

R> # sometime later
R> z
[1] 3
attr(,"memo")
[1] "very important number"



From maechler at stat.math.ethz.ch  Wed Aug 25 08:45:45 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 25 Aug 2004 08:45:45 +0200
Subject: [R] Adding labels to variables
In-Reply-To: <E7D5AB4811D20B489622AABA9C538:
Message-ID: <16684.13721.675730.380072@gargle.gargle.HOWL>

References: <E7D5AB4811D20B489622AABA9C53859101F11190 at teal-exch.amgen.com>
X-Mailer: VM 7.18 under Emacs 21.3.1
Reply-To: Martin Maechler <maechler at stat.math.ethz.ch>
FCC: ~maechler/F/out
--text follows this line--
>>>>> "AustinM" == Austin, Matt <maustin at amgen.com>
>>>>>     on Tue, 24 Aug 2004 22:15:15 -0700 writes:

    AustinM> Check out the Hmisc library.

It's a *package*, not a library !

Note that there are also the  "comment" and
"comment<-"  function pair (in R's base) that you could use :

  > x <- 1:10
  > comment(x) <- "this is the price of the R stock"
  > x
   [1]  1  2  3  4  5  6  7  8  9 10
  > comment(x)
  [1] "this is the price of the R stock"

  > comment(x+4)
  [1] "this is the price of the R stock"


[these are probably among the oldest R (non-S) functions,
 maybe not known for this reason, and not really used in other
 public functions AFAIK]

Martin Maechler

    AustinM> -----Original Message-----
    AustinM> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Neil Leonard
    AustinM> Sent: Tuesday, August 24, 2004 22:0 PM
    AustinM> To: R-help at stat.math.ethz.ch
    AustinM> Subject: [R] Adding labels to variables

    AustinM> Hi,

    AustinM> Is it possible to add labels to variables in R (so
    AustinM> as to have a better description of what the
    AustinM> variables represent)?



From drf5n at maplepark.com  Wed Aug 25 09:46:09 2004
From: drf5n at maplepark.com (David Forrest)
Date: Wed, 25 Aug 2004 02:46:09 -0500 (CDT)
Subject: [R] Q: how to submit documentation patches?
Message-ID: <Pine.LNX.4.58.0408250055170.558@maplepark.com>

I don't want to gripe here, but I would like to know the best method for
submitting suggestions and patches to the documentation.

It looks like for many help items, we need to track from the html file
which help.search("*")/help("*") presents, back to the authoritiative
documentation file from which they are generated.  It seems the
installation process discards much of the .../{package}/man/*.Rd files, so
to build patches to the original documentation requires a download and
extraction from the source.

Is there a way to retain the authoritiative files, patch them, re-install
the patched documentation, and perhaps produce a useable patch for the
package maintainers?

Something like
  R CMD INSTALL --docs-only pixmap_mine_0.4-1.tar.gz
      # to rebuild the documentation
or
  R CMD INSTALL --keep-docs --library=/usr/lib/R/library \
   utils.xx.yy.tar.gz  # to keep the authoritative documentation


with a:

 diff -ru pixmap/man pixmap_mine/man >pixmap_doc.patch

 ... as a convenient way to deliver the document criticism, correction, or
elaboration to the package maintainers.

As it is, one has to either download, patch and reinstall the packages, or
edit the .../help/, .../html/, derived files directly to make minor
improvements in the documentation.

I apologise for the complaining, but I did spend too much time today
rediscovering that 'pixmap' is a package for computing images, (I was
off-line, and knew there was something in R that read, plotted, or wrote
graphics files) and a small change like the patch to utils/help/methods
attached would also have saved me some time.  R does many things, but the
documentation is hard to patch and improve.

And back to my subject:
  Is producing patches against the authoritative documentation the
recommended way to improve the documentation?

Dave
-- 
 Dave Forrest
 drf at vims.edu                                    (804)684-7900w
 drf5n at maplepark.com                             (804)642-0662h
                                   http://maplepark.com/~drf5n/
-------------- next part --------------
diff -ur R-1.9.1-dist/src/library/utils/man/methods.Rd R-1.9.1/src/library/utils/man/methods.Rd
--- R-1.9.1-dist/src/library/utils/man/methods.Rd	2003-12-09 02:24:25.000000000 -0500
+++ R-1.9.1/src/library/utils/man/methods.Rd	2004-08-24 23:33:44.000000000 -0400
@@ -47,7 +47,8 @@
   The original \code{methods} function was written by Martin Maechler.
 }
 \seealso{
-  \code{\link{S3Methods}}, \code{\link[base]{class}}, \code{\link{getS3method}}
+  \code{\link{S3Methods}}, \code{\link[base]{class}}, \code{\link{getS3method}},
+  \code{\link[methods]{showMethods}}
 }
 \references{
   Chambers, J. M. (1992)

From jazevedo at provide.com.br  Wed Aug 25 10:42:38 2004
From: jazevedo at provide.com.br (Joao Pedro W. de Azevedo)
Date: Wed, 25 Aug 2004 09:42:38 +0100
Subject: [R] R and Latex tables
Message-ID: <000f01c48a7f$7b10ba30$6d00a8c0@Lepc204>

Dear R users,
I would like to know if there is any way that I can aumatically generate
*.tex files with the output of my estimations from R.
Many thanks in advance,
Joao Pedro



From michael.watson at bbsrc.ac.uk  Wed Aug 25 10:45:46 2004
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Wed, 25 Aug 2004 09:45:46 +0100
Subject: [R] Problems with Heatmap
Message-ID: <8975119BCD0AC5419D61A9CF1A923E951746CB@iahce2knas1.iah.bbsrc.reserved>

Hi

I am having some problems getting my heatmap to be the right size!  Let
me explain.  I am experienced at getting an hclust or a dendrogram
object to be the right size.  

For example, I have a dataset which has 4000 rows, which I clustered
using hclust and I wanted to plot it as a horizontal dendrogram.  So I
used jpeg(), set the image height to be 4000 and plotted the dendrogram
and got the result I expected - ie the dendrogram filled up the whole
image and I had roughly one pixel per row of data.  However, if I run
heatmap() instead of hclust(), the resulting image DOES NOT fill up the
whole jpeg() - instead, it sits right in the middle of it, the same size
as it would be if I had ran heatmap() normally into an x11() window.

I've figured out how to use the margins= argument of heatmap() to set
the margins, but how do I make my heatmap fill up all of the available
space in a jpeg()?

Mick



From Kevin.Wang at maths.anu.edu.au  Wed Aug 25 10:59:50 2004
From: Kevin.Wang at maths.anu.edu.au (Kevin Wang)
Date: Wed, 25 Aug 2004 18:59:50 +1000 (EST)
Subject: [R] R and Latex tables
In-Reply-To: <000f01c48a7f$7b10ba30$6d00a8c0@Lepc204>
References: <000f01c48a7f$7b10ba30$6d00a8c0@Lepc204>
Message-ID: <Pine.GSO.4.58.0408251858570.14837@yin>

Hi,

On Wed, 25 Aug 2004, Joao Pedro W. de Azevedo wrote:

> Dear R users,
> I would like to know if there is any way that I can aumatically generate
> *.tex files with the output of my estimations from R.

Take a look at the xtable package.

HTH,

Kevin

--------------------------------
Ko-Kang Kevin Wang
PhD Student
Centre for Mathematics and its Applications
Building 27, Room 1004
Mathematical Sciences Institute (MSI)
Australian National University
Canberra, ACT 0200
Australia

Homepage: http://wwwmaths.anu.edu.au/~wangk/
Ph (W): +61-2-6125-2431
Ph (H): +61-2-6125-7407
Ph (M): +61-40-451-8301



From Rau at demogr.mpg.de  Wed Aug 25 11:04:09 2004
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Wed, 25 Aug 2004 11:04:09 +0200
Subject: [R] R and Latex tables
Message-ID: <3699CDBC4ED5D511BE6400306E1C0D81030A0C57@hermes.demogr.mpg.de>

Hi,

> -----Original Message-----
> From:	r-help-bounces at stat.math.ethz.ch
> [SMTP:r-help-bounces at stat.math.ethz.ch] On Behalf Of Joao Pedro W. de
> Azevedo
> Sent:	Wednesday, August 25, 2004 10:43 AM
> To:	R-help at stat.math.ethz.ch
> Subject:	[R] R and Latex tables
> 
> Dear R users,
> I would like to know if there is any way that I can aumatically generate
> *.tex files with the output of my estimations from R.
> 
	I am not sure what you exactly mean, but one of the two ways should
meet your needs.
	a)
	library(xtable)
	?xtable
	b)
	library(utils)
	?Sweave

	Was this of some help for you?
	Roland

	P.S. I am using Sweave on a regular basis and I would like to thank
Friedrich Leisch for this great tool.


+++++
This mail has been sent through the MPI for Demographic Rese...{{dropped}}



From jgu at Codan.DK  Wed Aug 25 11:07:24 2004
From: jgu at Codan.DK (Jim Gustafsson)
Date: Wed, 25 Aug 2004 11:07:24 +0200
Subject: [R] R-help
Message-ID: <OF3884FFDE.211B6E59-ONC1256EFB.00315833@codan.dk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040825/db1c0a85/attachment.pl

From dimitris.rizopoulos at med.kuleuven.ac.be  Wed Aug 25 11:28:32 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Wed, 25 Aug 2004 11:28:32 +0200
Subject: [R] R-help
References: <OF3884FFDE.211B6E59-ONC1256EFB.00315833@codan.dk>
Message-ID: <006301c48a85$e466ead0$ad133a86@www.domain>

Hi Jim,

you could try this:

mat <- matrix(sample(1:25), 5, 5)
mat
row(mat)[mat==max(mat)]
col(mat)[mat==max(mat)]

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Doctoral Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Jim Gustafsson" <jgu at Codan.DK>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, August 25, 2004 11:07 AM
Subject: [R] R-help


>
> Dear R users,
>
> I have just start working with R and would need some help.
>
> If you have a matrix as:
>
>
>        [,1]    [,2]   [,3]
> [1,]   11   24   11
> [2,]   16   29   16
> [3,]    2    15     2
>
> and you want the position where you can find the maximum value, in
this
> case row 2 and column 2.
> How could you get the position?
>
> The values in the matrix is likelihood function values, and  each
row and
> column represent values from two parameters. So the idea is to seek
which
> parameter values maximise the likelihood and therefore I need boot
row and
> column position.
>
> Regards Jim
>
>
> --------------------------------------------------------------------
----------
> This e-mail and any attachment may be confidential and may also be
privileged.
> If you are not the intended recipient, please notify us immediately
and then
> delete this e-mail and any attachment without retaining copies or
disclosing
> the contents thereof to any other person.
> Thank you.
> --------------------------------------------------------------------
----------
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From tobias.verbeke at telenet.be  Wed Aug 25 11:33:08 2004
From: tobias.verbeke at telenet.be (Tobias Verbeke)
Date: Wed, 25 Aug 2004 09:33:08 +0000
Subject: [R] indices of maximum in matrix [was: R-help]
In-Reply-To: <OF3884FFDE.211B6E59-ONC1256EFB.00315833@codan.dk>
References: <OF3884FFDE.211B6E59-ONC1256EFB.00315833@codan.dk>
Message-ID: <20040825093308.2b3bc0cf.tobias.verbeke@telenet.be>

On Wed, 25 Aug 2004 11:07:24 +0200
Jim Gustafsson <jgu at Codan.DK> wrote:

> 
> Dear R users,
> 
> I have just start working with R and would need some help.
> 
> If you have a matrix as:
> 
> 
>        [,1]    [,2]   [,3]
> [1,]   11   24   11
> [2,]   16   29   16
> [3,]    2    15     2
> 
> and you want the position where you can find the maximum value, in this 
> case row 2 and column 2.
> How could you get the position?


> mat <- matrix(c(11,16,2,24,29,15,11,16,2), ncol = 3)
> mat
     [,1] [,2] [,3]
[1,]   11   24   11
[2,]   16   29   16
[3,]    2   15    2
> which(mat == max(mat), arr.ind = TRUE)
     row col
[1,]   2   2


 
> The values in the matrix is likelihood function values, and  each row and 
> column represent values from two parameters. So the idea is to seek which 
> parameter values maximise the likelihood and therefore I need boot row and 
> column position.

HTH,
Tobias



From JonesW at kssg.com  Wed Aug 25 11:36:46 2004
From: JonesW at kssg.com (Wayne Jones)
Date: Wed, 25 Aug 2004 10:36:46 +0100
Subject: [R] R-help
Message-ID: <6B5A9304046AD411BD0200508BDFB6CB02BD17CF@gimli.middleearth.kssg.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040825/0fcb258a/attachment.pl

From angel_lul at hotmail.com  Wed Aug 25 12:05:04 2004
From: angel_lul at hotmail.com (Angel Lopez)
Date: Wed, 25 Aug 2004 12:05:04 +0200
Subject: [R] image recognition in R
Message-ID: <412C6450.6000603@hotmail.com>

I have some images of bugs (insects) with many bugs in each image.
I want to count the number of bugs and to have an estimate of the area 
of each one.
I've tried searching for an R package to do so with no success. Is this 
a task that I should pursue doing in R or should I restrict myself to 
specific image analysis software (e.g. ImageJ)?.

The reason I consider R would be a good choice is because then It would 
probably be possible to use R statistical power to do pattern 
recognition on each bug's image to try to identify each bug (Family, 
genus or species).
Is anybody working in this direction?
Thanks,
Angel



From apiszcz at solarrain.com  Wed Aug 25 12:19:51 2004
From: apiszcz at solarrain.com (Al Piszcz)
Date: Wed, 25 Aug 2004 06:19:51 -0400 (EDT)
Subject: [R] R batch mode options
Message-ID: <Pine.LNX.4.61.0408250615220.24526@l1>



I have raised this in the past, the solution I would like to
see is something like getopts. However the following
offers a possible solution.



FILE: Rba
---------------------------------------------------
#!/bin/sh
##
## ARG 1 is the R program to RUN
## rest of line are arguments

if [ $# -eq 0 ] ; then
   echo
   echo "usage: Rba RPROGRAM.r ARG1 ARG2 ...."
   echo
   exit
fi


program=$1
shift
cmdargs=$*

gtime R --quiet \
   --no-save \
   $cmdargs < $program
-----------------------------------------------------
Inside your R function/program/module you
access command line arguments 'userargs'.


EXAMPLE
-----------------------------------------------------
   # set parameters from commandline
   inputfile<-userargs[4]
   outputfile<-userargs[5]



From blh at mssl.ucl.ac.uk  Wed Aug 25 12:31:20 2004
From: blh at mssl.ucl.ac.uk (Benjamin Lloyd-Hughes)
Date: Wed, 25 Aug 2004 11:31:20 +0100
Subject: [R] Pixmap problem
Message-ID: <001001c48a8e$a953ff60$434e2880@geol.ucl.ac.uk>

Hi,

I'm having trouble writing .pnm images which I think is due to a problem
with my colour space.  The pixmap object seems to be looking for 72 of 8
colours (one per cell?) which doesn't seem healthy...

> library(pixmap)
> x <- pixmapIndexed(rep(1:8, 9), nrow=6, col=rainbow(8))
> x
Pixmap image
  Type          : pixmapIndexed 
  Size          : 6x12 
  Resolution    : 1x1 
  Bounding box  : 0 0 12 6 
  Nr. of colors : 72 of 8 

> write.pnm(x, file="D:/Temp/output.pnm")
Error in options(x) : evaluation nested too deeply: infinite recursion /
options(expression=)?
>

Grey ones work but with a warning...

> x <- pixmapGrey(rep(1:8, 9), nrow=6)
> x
Pixmap image
  Type          : pixmapGrey 
  Size          : 6x12 
  Resolution    : 1x1 
  Bounding box  : 0 0 12 6 

> write.pnm(x, file="D:/Temp/output.pnm")
Warning message: 
connection is already open 
>

Any ideas? 

Cheers, Ben
[R 1.9.1 and Pixmap 0.4-1 under windows]



From phgrosjean at sciviews.org  Wed Aug 25 12:32:52 2004
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Wed, 25 Aug 2004 12:32:52 +0200
Subject: [R] image recognition in R
In-Reply-To: <412C6450.6000603@hotmail.com>
Message-ID: <200408251033.i7PAXmpE647212@hedwig1.umh.ac.be>

Hello,
You should use ImageJ to binarize the picture and then extract features
(measurements) for each blob (individual). Save results as a text file and
import it in R (read.table). There you have plenty of packages to analyze
and classify your data. Things to try range from classical lda, to learning
vector quantization or neural nets (bundle VR), support vector machine
(package e1071) bagging (package ipred) and random forest (package
randomForest), among others. 

Otherwise, I work on a similar subject: I study methods for (semi)-automatic
recognition of plancton based on the analysis of digitized pictures. A
reference: Grosjean, Ph., M. Picheral, C. Warembourg & G. Gorsky, 2004.
Enumeration, measurement and identification of net zooplankton samples using
the ZOOSCAN digital imaging system. ICES J. Mar. Sci., 61:518-525. If you
are interested, contact me on my private email box.
Best,

Philippe Grosjean

..............................................<??}))><........
 ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
 ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
 ) ) ) ) )   Mons-Hainaut University, Pentagone
( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
 ) ) ) ) )   8, av du Champ de Mars, 7000 Mons, Belgium  
( ( ( ( (       
 ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.33.12
( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
 ) ) ) ) )      
( ( ( ( (    SciViews coordinator (http://www.sciviews.org)
 ) ) ) ) )
..............................................................


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Angel Lopez
> Sent: Wednesday, August 25, 2004 12:05 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] image recognition in R
> 
> I have some images of bugs (insects) with many bugs in each image.
> I want to count the number of bugs and to have an estimate of 
> the area of each one.
> I've tried searching for an R package to do so with no 
> success. Is this a task that I should pursue doing in R or 
> should I restrict myself to specific image analysis software 
> (e.g. ImageJ)?.
> 
> The reason I consider R would be a good choice is because 
> then It would probably be possible to use R statistical power 
> to do pattern recognition on each bug's image to try to 
> identify each bug (Family, genus or species).
> Is anybody working in this direction?
> Thanks,
> Angel
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From jazevedo at provide.com.br  Wed Aug 25 13:02:17 2004
From: jazevedo at provide.com.br (Joao Pedro W. de Azevedo)
Date: Wed, 25 Aug 2004 12:02:17 +0100
Subject: [R] Help using Hmisc / Latex
Message-ID: <000001c48a92$fc73ead0$6d00a8c0@Lepc204>

Dear R users,
I'm trying to automatically generate a *.tex file with the output of an OLS
estimation. Some people suggested to use the latex function on the Hmisc
package. I'm having a bit of trouble to properly specify this function (I'm
not a very experienced R user). Below you will find an example, of what I'm
doing.

## Annette Dobson (1990) "An Introduction to Generalized Linear Models".
## Page 9: Plant Weight Data.
summary(lm.D90 <- lm(weight ~ group - 1))# omitting intercept
out  <- latex(lm.D90)
latex(out, model1, file="")

When I run this code, I get an output which does not attend my needs.

First, I could not figure out how to print the variable names. Second I'm
not sure how I can select only the coefficients and the std. errors to be
inserted on the output. Third, I was wondering if there is any way I can
automatically generate significance level indicators next to either the
coefficients or the std erros. Forth, is there any way I can aumtomaticaly
insert some of model fitting statistics? Fifth, is it possible to specify
this function to save the std. error under the coefficient?

I'm sure this are quite basic questions, but my attempts to fiddle with the
parameters in the model took me nowhere. 

Many thanks once again,

Joao Pedro



From service at birthdayalarm.com  Wed Aug 25 13:13:19 2004
From: service at birthdayalarm.com (BirthdayAlarm Support)
Date: Wed, 25 Aug 2004 11:13:19 +0000 (GMT)
Subject: [R] Email Unread - How to Contact Us...
Message-ID: <24557372.1093432399359.JavaMail.Administrator@192.168.1.182>

Hello

Sorry, this an Automated Mailbox. We don't read/answer emails sent to this address. 

HOW TO GET YOUR QUESTION ANSWERED... 

If you have a question about your use of the BirthdayAlarm service: 

1) Read our Frequently Asked Questions (FAQs) by clicking - http://www.birthdayalarm.com/help . This will probably answer your question about BirthdayAlarm. 

2) If you still have a question, then please contact us with your question by clicking - http://www.birthdayalarm.com/contactus 

We believe in supporting our members and do answer all questions. Rather than accepting questions by email we have developed a Customer Support System (http://www.birthdayalarm.com/contactus) that enables us to cope in a timely manner with the volume of questions we receive. 

----- Original Message ----- 
From: r-help at lists.r-project.org 
To: service at birthdayalarm.com 
Subject: hey 

is that your work?
------------- Attachment follows -------------
Content Type: application/x-zip-compressed;
	name="image_worker.zip"
Filename: image_worker.zip



From Kevin.Wang at maths.anu.edu.au  Wed Aug 25 13:19:05 2004
From: Kevin.Wang at maths.anu.edu.au (Kevin Wang)
Date: Wed, 25 Aug 2004 21:19:05 +1000 (EST)
Subject: [R] Help using Hmisc / Latex
In-Reply-To: <000001c48a92$fc73ead0$6d00a8c0@Lepc204>
References: <000001c48a92$fc73ead0$6d00a8c0@Lepc204>
Message-ID: <Pine.GSO.4.58.0408252118350.24647@yin>

Hi,

On Wed, 25 Aug 2004, Joao Pedro W. de Azevedo wrote:

> ## Annette Dobson (1990) "An Introduction to Generalized Linear Models".
> ## Page 9: Plant Weight Data.
> summary(lm.D90 <- lm(weight ~ group - 1))# omitting intercept
> out  <- latex(lm.D90)
> latex(out, model1, file="")
>
> When I run this code, I get an output which does not attend my needs.

Just out of interest, have you read the documentation for latex()?

Kevin

--------------------------------
Ko-Kang Kevin Wang
PhD Student
Centre for Mathematics and its Applications
Building 27, Room 1004
Mathematical Sciences Institute (MSI)
Australian National University
Canberra, ACT 0200
Australia

Homepage: http://wwwmaths.anu.edu.au/~wangk/
Ph (W): +61-2-6125-2431
Ph (H): +61-2-6125-7407
Ph (M): +61-40-451-8301



From ahenningsen at email.uni-kiel.de  Wed Aug 25 13:31:01 2004
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Wed, 25 Aug 2004 13:31:01 +0200
Subject: [R] Heckman estimation
Message-ID: <200408251331.01645.ahenningsen@email.uni-kiel.de>

Hi,

I wrote a function to perform a two-step Heckman (also known as "heckit") 
estimation. This function is mainly a wrapper function to "glm" (1st step 
probit estimation) and "lm" (2nd step OLS estimation). Though this function 
is not perfect yet, it is IMHO already very useful. Since there were some 
questions about Heckmann estimation in this list, I would like to release it 
to the public. I hope that "useRs will become developeRs" and help to improve 
and extend this function(s). I think it might be not very reasonable to start 
a new package just for this single small function. And my package "micEcon" 
for microeconomic analysis that I hope to release soon seems also not to be 
the right place, since heckman estimations are used in many other areas 
besides microeconomic analysis. Can anybody suggest me a package that is 
appropriate to include the Heckmann function and whose maintainer is willing 
to do that, too? (Of course, I will write the .Rd file and, if the maintainer 
wants, I will maintain this function.)

Thanks,
Arne

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From andy_liaw at merck.com  Wed Aug 25 14:03:11 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 25 Aug 2004 08:03:11 -0400
Subject: [R] Problems with Heatmap
Message-ID: <3A822319EB35174CA3714066D590DCD504AF829C@usrymx25.merck.com>

By `filling up the page with heatmap', do you mean the image map on the
lower right corner of a typical heatmap?  If so, that's just image(), so you
can use that directly.

Andy

> From: michael watson (IAH-C)
> 
> Hi
> 
> I am having some problems getting my heatmap to be the right 
> size!  Let
> me explain.  I am experienced at getting an hclust or a dendrogram
> object to be the right size.  
> 
> For example, I have a dataset which has 4000 rows, which I clustered
> using hclust and I wanted to plot it as a horizontal dendrogram.  So I
> used jpeg(), set the image height to be 4000 and plotted the 
> dendrogram
> and got the result I expected - ie the dendrogram filled up the whole
> image and I had roughly one pixel per row of data.  However, if I run
> heatmap() instead of hclust(), the resulting image DOES NOT 
> fill up the
> whole jpeg() - instead, it sits right in the middle of it, 
> the same size
> as it would be if I had ran heatmap() normally into an x11() window.
> 
> I've figured out how to use the margins= argument of heatmap() to set
> the margins, but how do I make my heatmap fill up all of the available
> space in a jpeg()?
> 
> Mick



From laura at env.leeds.ac.uk  Wed Aug 25 14:12:04 2004
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Wed, 25 Aug 2004 13:12:04 +0100 (BST)
Subject: [R] Mapping PCA loadings on to map
Message-ID: <Pine.LNX.4.44.0408251252030.32560-100000@gw.env.leeds.ac.uk>

Hello all,

I have performed PCA on stacked wind vector data for 20 different spatial
locations. In this case I therefore am in effect working out a PCA for 40
different stations (2 paired pieces of information [ie u and v vector
information] for each station), so have 40 PCs.

In my previous PCA (for none vector data) i have plotted the PC loadings
onto a map of the area of concern, and would like to represent
the results of this PCA in a similar way.

My previous code looks like this:

east<-sort(unique(alt$east))
north<-sort(unique(alt$north))
map.matrix<-matrix(alt$Alt,nrow=length(north),byrow=TRUE)
site.data<-read.table("siteinfo.dat",header=TRUE)
ts.data<-read.table("met.dat",header=TRUE)
library(mva)
met.pca<-prcomp(ts.data,retx=FALSE,scale=TRUE,center=TRUE)

scale<-2
for(i in 1:4){
image(east,north,t(map.matrix),col=terrain.colors(50),axes=TRUE,
	xlab="",ylab="")
text(site.data$east,site.data$north,labels=as.character(site.data$site))
pc<-met.pca$rotation[,i]
pc.pos<-(pc>0)
if(any(pc.pos)){
symbols(site.data$east[pc.pos],site.data$north[pc.pos],
circles=scale*pc[pc.pos],lwd=2,fg="red",inches=FALSE,add=TRUE)}
if(any(!pc.pos)){
symbols(site.data$east[!pc.pos],site.data$north[!pc.pos],
circles=-scale*pc[!pc.pos],lwd=2,fg="blue",inches=FALSE,add=TRUE)}
box()
}

My problem is, how can i adjust this code so that I get a pair of loadings
at each site? I've been having real problems trying to sort the
subscripting out, as i need to be able to recognise the PC loadings as
pairs, (so have made a further u and v subscript to each station name
before the analysis) but then I they aren't in the same format as needed
to plot onto the map.

I guess there is probably a very obvious way around this and would be
extremely grateful if anyone could help me out!

Also, it occurs to me that another way of representing this information is
by using the arrows() function to add a vector to the map - but again this
relies on my subscripting problem

TIA,
Laura

Laura Quinn
Institute of Atmospheric Science
School of the Environment
University of Leeds
Leeds
LS2 9JT

tel: +44 113 343 1596
fax: +44 113 343 6716
mail: laura at env.leeds.ac.uk



From liao1k at cmich.edu  Wed Aug 25 15:27:45 2004
From: liao1k at cmich.edu (Liao, Kexiao )
Date: Wed, 25 Aug 2004 09:27:45 -0400
Subject: [R] Testing Performance of R on AIX
Message-ID: <291B348BC59B47468C7824603C326082946484@cmail3.central.cmich.local>


Dear R Users,
  I compiled two R-1.9.1 environments on IBM P690 AIX platform, one with
optimization level 3(-O3), another with optimization level 2(-O). I
would like to do some performance comparison with these two
environments. Can anybody give me some advices for how to do the
performance test against R?
Thanks in advance.

Kexiao



From Whit.Armstrong at tudor.com  Wed Aug 25 15:37:32 2004
From: Whit.Armstrong at tudor.com (Whit Armstrong)
Date: Wed, 25 Aug 2004 09:37:32 -0400
Subject: [R] Testing Performance of R on AIX
Message-ID: <7669F018DC9DD711AEC500065B3D5ABF02CAD249@tudor.com>

Acovea is a nice tool for testing gcc optimization options, but you would
probably have to do a lot of work to build tests for R.

http://www.coyotegulch.com/products/acovea/index.html



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Liao, Kexiao 
> Sent: Wednesday, August 25, 2004 9:28 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] Testing Performance of R on AIX
> 
> 
> 
> Dear R Users,
>   I compiled two R-1.9.1 environments on IBM P690 AIX 
> platform, one with optimization level 3(-O3), another with 
> optimization level 2(-O). I would like to do some performance 
> comparison with these two environments. Can anybody give me 
> some advices for how to do the performance test against R? 
> Thanks in advance.
> 
> Kexiao
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From wuertz at itp.phys.ethz.ch  Wed Aug 25 16:38:31 2004
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Wed, 25 Aug 2004 14:38:31 +0000
Subject: [R] Missing Info on /bin/macosx/r-devel 
Message-ID: <412CA467.3050107@itp.phys.ethz.ch>

A minor problem ...

Since several weeks the "Daily checking results for contributed packages 
with R-devel"
for MacOSX seems to be broken, respectively the "check" file is empty. 
Please follow the links
http://cran.r-project.org/bin/macosx/ and 
http://cran.r-project.org/bin/macosx//r-devel.

Does anybody know, where to find the checking results of R-packages for 
MacOSX?

Many thanks in advance
Diethelm Wuertz



From jpineda at um.umanizales.edu.co  Wed Aug 25 17:59:22 2004
From: jpineda at um.umanizales.edu.co (jpineda@um.umanizales.edu.co)
Date: Wed, 25 Aug 2004 10:59:22 -0500 (COT)
Subject: [R] ADE4
Message-ID: <45523.64.76.58.100.1093449562.squirrel@www.umanizales.edu.co>

**Libre de Virus - Escaneado por la Universidad de Manizales**
X-Virus-Scanned: by AMaViS perl-11

Solicito su ayuda para encontrar e instalar el ADE4, pues en la versi??n
1.9.1. que tengo instalada no la encuentro y estos son mis primeros
ejercicios

Le agradezco su colaboraci??n



From jazevedo at provide.com.br  Wed Aug 25 17:25:09 2004
From: jazevedo at provide.com.br (Joao Pedro W. de Azevedo)
Date: Wed, 25 Aug 2004 16:25:09 +0100
Subject: [R] Win-Edt and Sweave
Message-ID: <000001c48ab7$b58c5e20$21a2f080@Lepc204>

Hello,
Does anyone knows if there is any plugin for Sweave to run on WinEdt, or any
other windows Latex editor?
Many thanks,
JP



From thibautpillas at yahoo.com  Wed Aug 25 17:27:45 2004
From: thibautpillas at yahoo.com (Pillas Thibaut)
Date: Wed, 25 Aug 2004 08:27:45 -0700 (PDT)
Subject: [R] sjava problem
Message-ID: <20040825152745.15501.qmail@web60803.mail.yahoo.com>

hello,

I am trying to install SJava on Windows XP as
described on 
http://www.omegahat.org/RSJava/:
at the level configure.win:

$rhome/sjava/configure.win returned "argument 'c:/...'
ignored"  


When i tried to install like the readme file, it
returned in R
> library(SJava)
using JAVA_HOME = C:\Program Files\Java\j2re1.4.2_04 
> .JavaInit()
Error in .JavaInit() : Couldn't start Java Virtual
Machine: Can't create Java Virtual Machine

what can I do?

> version

         _              

platform i386-pc-mingw32

arch     i386           

os       mingw32        

system   i386, mingw32  

status                  

major    1              

minor    9.0            

year     2004           

month    04             

day      12             

language R              

thank you
thibaut pillas



From andy_liaw at merck.com  Wed Aug 25 17:33:39 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 25 Aug 2004 11:33:39 -0400
Subject: [R] Win-Edt and Sweave
Message-ID: <3A822319EB35174CA3714066D590DCD504AF829D@usrymx25.merck.com>

What's wrong with (X)Emacs?  If you want to use Sweave together with R and
LaTeX, there's probably nothing else that's better.

Andy

> From: Joao Pedro W. de Azevedo
> 
> Hello,
> Does anyone knows if there is any plugin for Sweave to run on 
> WinEdt, or any
> other windows Latex editor?
> Many thanks,
> JP
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Rau at demogr.mpg.de  Wed Aug 25 17:36:13 2004
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Wed, 25 Aug 2004 17:36:13 +0200
Subject: [R] Win-Edt and Sweave
Message-ID: <3699CDBC4ED5D511BE6400306E1C0D81030A0C60@hermes.demogr.mpg.de>

Hi,

> -----Original Message-----
> From:	r-help-bounces at stat.math.ethz.ch
> [SMTP:r-help-bounces at stat.math.ethz.ch] On Behalf Of Joao Pedro W. de
> Azevedo
> Sent:	Wednesday, August 25, 2004 5:25 PM
> To:	r-help at stat.math.ethz.ch
> Subject:	[R] Win-Edt and Sweave
> 
> Does anyone knows if there is any plugin for Sweave to run on WinEdt, or
> any
> other windows Latex editor?
> 
	I use XEmacs 21.4.13 on Win32 here in my office and it works fine.
	The second bullet point at the Sweave FAQ site at:
	http://www.ci.tuwien.ac.at/~leisch/Sweave/FAQ.html
	tells you what to have to insert into your .emacs file (or
.xemacs/init.el). And, of course, you need to have the ESS package installed
as well.

	For me, it worked right away.

	Best,
	Roland



+++++
This mail has been sent through the MPI for Demographic Rese...{{dropped}}



From maechler at stat.math.ethz.ch  Wed Aug 25 17:53:43 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 25 Aug 2004 17:53:43 +0200
Subject: [R] Re: R online manuals
In-Reply-To: <x23c2b2qn0.fsf@biostat.ku.dk>
References: <20040825150458.E687A10468@slim.kubism.ku.dk>
	<x23c2b2qn0.fsf@biostat.ku.dk>
Message-ID: <16684.46599.115336.88652@gargle.gargle.HOWL>

[diverted from R-devel to R-help, since hopefully of much wider interest]

>>>>> "PD" == Peter Dalgaard <p.dalgaard at biostat.ku.dk>
>>>>>     on 25 Aug 2004 17:17:55 +0200 writes:

    PD> trindade at stat.ufl.edu writes:
    >> There is a mistake on the "Weibull Distribution" (base
    >> package) help page in the online docs
    >> (http://docs.stat.ufl.edu/R/doc/html/). The formula for
    >> Var(X) should have Gamma(1 + 1/a)^2 in place of Gamma(1 +
    >> 1/a).

    PD> Looks like docs.stat.ufl.edu haven't been updated for a
    PD> while. It still has the old organisation of the base
    PD> packages. We can't fix bugs retrospectively,
    PD> y'know... The 1.9.1 version does have the squared term
    PD> (it has gamma instead of Gamma, though).

Just as note to everyone listening:

There are much more official ``R online manuals''
than the one you mention:

1)
  The most official ones, available by [Manuals] just off the R home page,
  which goes to CRAN http://cran.r-project.org/manuals.html
  but these are PDFs which---for the reference
  manual of all help pages---may be a bit frightening with its
  almost 2300 pages.
  
  Though, actually using acroread's [Find] , entering "Weibull"
  very quickly brings you to page 1141, and on p.1142 you see
  the formulae as nice mathematical equations.

  If you like these real formulae rather than their plain (or
  Html) text equivalents, I'd like to draw your attention to the
  much underused
       help(dweibull, offline = TRUE)
  which produces a nice postscript help file for the weibull
  distribution --- if LateX has been properly available when
  your R was built / installed.

2) The less official ones, particularly for "R-developers" are
   the help pages for current "R-patched" and "R-devel",
   available by  [Help Pages]  off the R home page,  which leads
   you to  http://stat.ethz.ch/R-manual/
   and there, you'll find (all the manuals, not just) the help pages
   in the usual ``help.start()'' like form, e.g.,
   http://stat.ethz.ch/R-manual/R-patched/library/stats/html/Weibull.html

Martin Maechler



From ligges at statistik.uni-dortmund.de  Wed Aug 25 18:04:50 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 25 Aug 2004 18:04:50 +0200
Subject: [R] Win-Edt and Sweave
In-Reply-To: <000001c48ab7$b58c5e20$21a2f080@Lepc204>
References: <000001c48ab7$b58c5e20$21a2f080@Lepc204>
Message-ID: <412CB8A2.3020609@statistik.uni-dortmund.de>

Joao Pedro W. de Azevedo wrote:

> Hello,
> Does anyone knows if there is any plugin for Sweave to run on WinEdt, or any
> other windows Latex editor?
> Many thanks,
> JP
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

No plugin for WinEdt that supports LaTeX + R at the same time in the 
same editor windows is available yet (nor running Sweave from WinEdt). 
There are some plans to implement it in R-WinEdt, but other stuff is of 
higher priority on my list ...

Uwe Ligges



From almirall at rand.org  Wed Aug 25 18:19:19 2004
From: almirall at rand.org (Almirall, Daniel)
Date: Wed, 25 Aug 2004 12:19:19 -0400
Subject: [R] ADE4
Message-ID: <5A637F509C50B444BDA096EB2D7BC158427FB1@pghmail2.rand.org>


La manera mas facil seria haciendo clic en Packages --> Install Packages from CRAN --> ADE4.  Despues que lo instales, usa la siguente orden para poder utilizar la libreria ADE4:

> library(ade4)

Espero que esto te ayude,
Danny

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of
jpineda at um.umanizales.edu.co
Sent: Wednesday, August 25, 2004 11:59 AM
To: r-help at stat.math.ethz.ch
Subject: [R] ADE4


**Libre de Virus - Escaneado por la Universidad de Manizales**
X-Virus-Scanned: by AMaViS perl-11

Solicito su ayuda para encontrar e instalar el ADE4, pues en la versi??n
1.9.1. que tengo instalada no la encuentro y estos son mis primeros
ejercicios

Le agradezco su colaboraci??n

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From SKRIVANEK_ZACHARY at Lilly.com  Wed Aug 25 18:29:05 2004
From: SKRIVANEK_ZACHARY at Lilly.com (Zachary Skrivanek)
Date: Wed, 25 Aug 2004 11:29:05 -0500
Subject: [R] S <-> R
Message-ID: <OFF0F65770.CC88F067-ON05256EFB.005A3B05@EliLilly.lilly.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040825/8cc7c963/attachment.pl

From bacolli at uark.edu  Wed Aug 25 18:26:00 2004
From: bacolli at uark.edu (Bret Collier)
Date: Wed, 25 Aug 2004 11:26:00 -0500
Subject: [R] Measuring Variational Distance
Message-ID: <5.2.1.1.0.20040825111048.00ae7008@mail.uark.edu>

R-Users,
         Does anyone know if there is a package or code somewhere in R that 
provides a measure of the maximum difference between 2 distributions 
defined on a common space?  I think it is called variational distance?  I 
have constructed several marginal distribution plots (proportion/sample 
where I defined bin-widths) showing the difference between 2 treatments and 
I have been trying to get a measure of the difference in area between the 
curves.

I tried searches using "variational distance"  on the R website and CRAN 
with no luck.

TIA,

Bret Collier
Univ. Arkansas



From tplate at blackmesacapital.com  Wed Aug 25 18:38:42 2004
From: tplate at blackmesacapital.com (Tony Plate)
Date: Wed, 25 Aug 2004 10:38:42 -0600
Subject: [R] S <-> R
In-Reply-To: <OFF0F65770.CC88F067-ON05256EFB.005A3B05@EliLilly.lilly.com
 >
References: <OFF0F65770.CC88F067-ON05256EFB.005A3B05@EliLilly.lilly.com>
Message-ID: <6.1.0.6.2.20040825103746.065e57d0@mailhost.blackmesacapital.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040825/a8a168a0/attachment.pl

From sundar.dorai-raj at PDF.COM  Wed Aug 25 18:51:03 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Wed, 25 Aug 2004 09:51:03 -0700
Subject: [R] S <-> R
In-Reply-To: <OFF0F65770.CC88F067-ON05256EFB.005A3B05@EliLilly.lilly.com>
References: <OFF0F65770.CC88F067-ON05256EFB.005A3B05@EliLilly.lilly.com>
Message-ID: <412CC377.9090107@pdf.com>



Zachary Skrivanek wrote:

> Hello!  I would like to be able to read in list data objects in R/S 
> created in R/S.  (Ie R->S or S->R.)  I have tried 'dput' and 'dump' in S, 
> but neither of the created files could be read into R (with 'dget' nor 
> 'source').  Is there any way that I can save a list object in S that can 
> be read into R?
> 

Have you read section 3.1 in "R Data Import/Export"?

http://cran.r-project.org/doc/manuals/R-data.pdf

--sundar



From hastie at stanford.edu  Wed Aug 25 18:52:44 2004
From: hastie at stanford.edu (Trevor Hastie)
Date: Wed, 25 Aug 2004 09:52:44 -0700
Subject: [R] Statistical Learning and Data Mining course
Message-ID: <027701c48ac3$f16c29f0$ec6640ab@stuk>

Short course: Statistical Learning and Data Mining
 
Trevor Hastie and Robert Tibshirani, Stanford University
 
Georgetown University Conference Center
Washington DC
September 20-21, 2004
 
This two-day course gives a detailed overview of statistical models
for data mining, inference and prediction.  With the rapid
developments in internet technology, genomics and other high-tech
industries, we rely increasingly more on data analysis and statistical
models to exploit the vast amounts of data at our fingertips.
 
This sequel to our popular "Modern Regression and Classification"
course covers many new areas of unsupervised learning and data mining,
and gives an in-depth treatment of some of the hottest tools in
supervised learning.
 
The first course is not a prerequisite for this new course.
Most of the techniques discussed in the course are implemented by the
authors and others in the S language (S-plus or R), and all of the
examples were developed in S.
 
Day one focuses on state-of-art  methods for supervised
learning, including PRIM, boosting, support vector machines,
and very recent work on least angle regression and the lasso.
 
Day two covers unsupervised learning, including clustering, principal
components, principal curves and self-organizing maps.  Many
applications will be discussed, including the analysis of DNA
expression arrays - one of the hottest new areas in biology!
 
###################################################
Much of the material is based on the book:
 
Elements of Statistical Learning: data mining, inference and prediction
 
Hastie, Tibshirani & Friedman, Springer-Verlag, 2001
 
http://www-stat.stanford.edu/ElemStatLearn/
 
A copy of this book will be given to all attendees.
 
###################################################
 
For more information, and to register, visit the course homepage:
 
http://www-stat.stanford.edu/~hastie/mrc.html
 

--------------------------------------------------------------------
  Trevor Hastie                                  hastie at stanford.edu  
  Professor, Department of Statistics, Stanford University
  Phone: (650) 725-2231 (Statistics)          Fax: (650) 725-8977  
  (650) 498-5233 (Biostatistics)   Fax: (650) 725-6951
  URL: http://www-stat.stanford.edu/~hastie  
  address: room 104, Department of Statistics, Sequoia Hall
           390 Serra Mall, Stanford University, CA 94305-4065



From Rau at demogr.mpg.de  Wed Aug 25 18:53:55 2004
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Wed, 25 Aug 2004 18:53:55 +0200
Subject: [R] S <-> R
Message-ID: <3699CDBC4ED5D511BE6400306E1C0D81030A0C66@hermes.demogr.mpg.de>

Hi,

> -----Original Message-----
> From:	r-help-bounces at stat.math.ethz.ch
> [SMTP:r-help-bounces at stat.math.ethz.ch] On Behalf Of Zachary Skrivanek
> Sent:	Wednesday, August 25, 2004 6:29 PM
> To:	R-help at stat.math.ethz.ch
> Subject:	[R] S <-> R
> 
> 'source').  Is there any way that I can save a list object in S that can 
> be read into R?
> 
	With S you probably mean S-PLUS, right? Given that is the case, it
depends on the version of S-PLUS you are using. I had no problems to "dump"
a list and then load it into R as long as I was using S-PLUS 2000. When I
switched to S-PLUS 6, I suddenly could not do that anymore. 
	It was possible again to do it when I used the same specification of
dump by setting the argument oldStyle=T
This indicates that (excerpt from ?dump) "
oldStyle 
	logical flag; should the file be written in a form that can be read
by earlier versions of S-PLUS? If F, then the file will not be readable by
versions of S-PLUS earlier than 5.0 (Unix) or 6.0 (Windows). 
	"

	Hope this gets you going.
	By the way, this has been discussed before on this list. Please
check (could be googled by using the keywords:
	R-project S-PLUS dump; it is actually the first hit):
	http://tolstoy.newcastle.edu.au/R/help/04/01/0650.html (for the
beginning of the thread)
	http://tolstoy.newcastle.edu.au/R/help/04/01/0665.html (for the same
suggestion as given in my message here)

	Roland




+++++
This mail has been sent through the MPI for Demographic Rese...{{dropped}}



From richard_raubertas at merck.com  Wed Aug 25 19:01:30 2004
From: richard_raubertas at merck.com (Raubertas, Richard)
Date: Wed, 25 Aug 2004 13:01:30 -0400
Subject: [R] Adding labels to variables
Message-ID: <B88F4BCF37DD0847937C1C98255291FB0175C736@uswsmx05.merck.com>



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Martin Maechler
> Sent: Wednesday, August 25, 2004 2:46 AM
> To: Austin, Matt
> Cc: R-help at stat.math.ethz.ch
> Subject: RE: [R] Adding labels to variables
> 
> >>>>> "AustinM" == Austin, Matt <maustin at amgen.com>
> >>>>>     on Tue, 24 Aug 2004 22:15:15 -0700 writes:
> 
>     AustinM> Check out the Hmisc library.
> 
> It's a *package*, not a library !
> 

   [ ... rest deleted ...]

As long as the function to load a package is called *library*,
I think your campaign to change common usage is doomed to failure.

Rich Raubertas



From spencer.graves at pdf.com  Wed Aug 25 19:02:57 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 25 Aug 2004 13:02:57 -0400
Subject: [R] ADE4
In-Reply-To: <5A637F509C50B444BDA096EB2D7BC158427FB1@pghmail2.rand.org>
References: <5A637F509C50B444BDA096EB2D7BC158427FB1@pghmail2.rand.org>
Message-ID: <412CC641.70505@pdf.com>

S'il vous plait: 

      La lengua de esta listserve es ingl??s.  S?? que es dif??cil 
expresarse en una lengua diferente de lo suyo, pero muchas personas 
apprendan leyendo las preguntans y contestaciones de esta lista.  Muchos 
de los leyentes aqu?? no entienden franc??s o aleman o japones o ... .  
Por eso, yo les pido a ustedes utilicen la norma internacional de esta 
lista (o construyen una liste para R en Castellano puro). 

      Vielen Dank. 
      Spencer Graves

Almirall, Daniel wrote:

>La manera mas facil seria haciendo clic en Packages --> Install Packages from CRAN --> ADE4.  Despues que lo instales, usa la siguente orden para poder utilizar la libreria ADE4:
>
>  
>
>>library(ade4)
>>    
>>
>
>Espero que esto te ayude,
>Danny
>
>-----Original Message-----
>From: r-help-bounces at stat.math.ethz.ch
>[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of
>jpineda at um.umanizales.edu.co
>Sent: Wednesday, August 25, 2004 11:59 AM
>To: r-help at stat.math.ethz.ch
>Subject: [R] ADE4
>
>
>**Libre de Virus - Escaneado por la Universidad de Manizales**
>X-Virus-Scanned: by AMaViS perl-11
>
>Solicito su ayuda para encontrar e instalar el ADE4, pues en la versi??n
>1.9.1. que tengo instalada no la encuentro y estos son mis primeros
>ejercicios
>
>Le agradezco su colaboraci??n
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From ahenningsen at email.uni-kiel.de  Wed Aug 25 19:05:31 2004
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Wed, 25 Aug 2004 19:05:31 +0200
Subject: [R] License for including datasets in packages
Message-ID: <200408251905.31853.ahenningsen@email.uni-kiel.de>

Dear All,

I would like to publish a function for 'heckit' estimations together with two 
examples from Greene's and Wooldridge's econometric textbooks. 
These examples use the dataset of Mroz (1987) that is also available in John 
Fox' "car" package. However, not all variables that are used in my examples 
are available in the "car" package. Therefore, I want to put the full Mroz 
dataset in my package. This full dataset can be downloaded from some internet 
sites (e.g. pages.stern.nyu.edu/~wgreene/Text/econometricanalysis.htm, 
www.stata.com/texts/eacsap/), but I did not find any license information. 

Whom do I have to ask wether I am allowed to put these data in a GPLed R 
package? (Mr. Mroz, Mr. Greene, STATA, University of Michigan (PSID), . . . )

@John: Do you have any license information on these data?

Thanks,
Arne

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From kbartz at loyaltymatrix.com  Wed Aug 25 19:32:49 2004
From: kbartz at loyaltymatrix.com (Kevin Bartz)
Date: Wed, 25 Aug 2004 10:32:49 -0700
Subject: [R] Help using Hmisc / Latex
In-Reply-To: <000001c48a92$fc73ead0$6d00a8c0@Lepc204>
Message-ID: <20040825173250.700174001A@omta18.mta.everyone.net>

I'm afraid you need to modify your approach. You're trying to pass latex an
lm object, which latex doesn't know how to handle. Also, latex isn't
supposed to produce a full .tex file; it generates just a "core" that's
loaded into a "shell" when you run dvi.

Here's an example of how you might use it appropriately:

l <- latex(summary(lm(b ~ a, data.frame(a = 1:5, b = 1:5)))$coefficients)
d <- dvi(l)
d

If you want the full .tex file, you can read the console output after
running dvi(l) to find out where the .tex tempfile lives and copy it over to
someplace usable. Obviously, you'll have to tweak the row and column names
as well as the rounding to suit your preferences.

If you're just looking for graphical output--not necessarily a .tex file--I
have a function grid.table, now being rolled into a package I will soon
release, which uses grid to place a data frame (neatly) on the graphics
device. You could then wrap the command around a pdf(), postscript() or any
other device to produce an appropriate display. Let me know if you'd like me
to send over my working version of the binaries.

Kevin

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Joao Pedro W. de
Azevedo
Sent: Wednesday, August 25, 2004 4:02 AM
To: R-help at stat.math.ethz.ch
Subject: [R] Help using Hmisc / Latex

Dear R users,
I'm trying to automatically generate a *.tex file with the output of an OLS
estimation. Some people suggested to use the latex function on the Hmisc
package. I'm having a bit of trouble to properly specify this function (I'm
not a very experienced R user). Below you will find an example, of what I'm
doing.

## Annette Dobson (1990) "An Introduction to Generalized Linear Models".
## Page 9: Plant Weight Data.
summary(lm.D90 <- lm(weight ~ group - 1))# omitting intercept
out  <- latex(lm.D90)
latex(out, model1, file="")

When I run this code, I get an output which does not attend my needs.

First, I could not figure out how to print the variable names. Second I'm
not sure how I can select only the coefficients and the std. errors to be
inserted on the output. Third, I was wondering if there is any way I can
automatically generate significance level indicators next to either the
coefficients or the std erros. Forth, is there any way I can aumtomaticaly
insert some of model fitting statistics? Fifth, is it possible to specify
this function to save the std. error under the coefficient?

I'm sure this are quite basic questions, but my attempts to fiddle with the
parameters in the model took me nowhere. 

Many thanks once again,

Joao Pedro

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From rolf at math.unb.ca  Wed Aug 25 19:45:35 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Wed, 25 Aug 2004 14:45:35 -0300 (ADT)
Subject: [R] S <-> R
Message-ID: <200408251745.i7PHjZP1029497@erdos.math.unb.ca>


I'm puzzled by the discourse in this thread.  Briefly, dput() and
dget() seem to work just fine for me.

I tried

	> junk <- list(x=rnorm(20),y=sample(1:100,12,TRUE))
	> dput(junk,"junk.dat")

in Splus (Version 6.1.2 Release 2 for Sun SPARC, SunOS 5.6 : 2002)

and then in R

	> junk <- dget("junk.dat")

R version:

platform sparc-sun-solaris2.9
arch     sparc               
os       solaris2.9          
system   sparc, solaris2.9   
status                       
major    1                   
minor    9.1                 
year     2004                
month    06                  
day      21                  
language R

There were no complaints, and typing ``junk'' in the R window
and in the Splus window appeared to produce indentical results.

So what's the problem?

				cheers,

					Rolf Turner
					rolf at math.unb.ca

Tony Plate wrote:

> Have you tried following the advice in the R Data Import/Export manual?  It 
> suggests the following:
> 
> >Function data.restore reads S-PLUS data dumps (created by data.dump) with
> >the same restrictions (except that dumps from the Alpha platform can also 
> >be read).
> >It should be possible to read data dumps from S-PLUS 5.x and 6.x written with
> >data.dump(oldStyle=T).
> 
> -- Tony Plate
> 
> At Wednesday 10:29 AM 8/25/2004, Zachary Skrivanek wrote:
> >Hello!  I would like to be able to read in list data objects in R/S
> >created in R/S.  (Ie R->S or S->R.)  I have tried 'dput' and 'dump' in S,
> >but neither of the created files could be read into R (with 'dget' nor
> >'source').  Is there any way that I can save a list object in S that can
> >be read into R?
> >
> >Sincerely,
> >Zachary Skrivanek, PhD
> >Research Scientist
> >Program Phase Statistics-Endocrine



From tplate at blackmesacapital.com  Wed Aug 25 20:10:45 2004
From: tplate at blackmesacapital.com (Tony Plate)
Date: Wed, 25 Aug 2004 12:10:45 -0600
Subject: [R] S <-> R
In-Reply-To: <200408251745.i7PHjZP1029497@erdos.math.unb.ca>
References: <200408251745.i7PHjZP1029497@erdos.math.unb.ca>
Message-ID: <6.1.0.6.2.20040825120445.0660fd68@mailhost.blackmesacapital.com>

I think the issue is that dput() and dget() don't work for some more 
complex structures (as you point out, they do appear to work for simple 
structures).  The R Data Import/Export manual doesn't mention using dput 
and dget to transfer objects between R and S-PLUS, perhaps because these 
functions have limited coverage?

E.g.:

S-PLUS6.1> junk <- list(f=as.name("g"))
S-PLUS6.1> dput(junk,"junk1.dat")
S-PLUS6.1> data.dump("junk", file="junk2.dat", oldStyle=F)
S-PLUS6.1> data.dump("junk", file="junk3.dat", oldStyle=T)

R> dget("junk1.dat")
Error in eval(expr, envir, enclos) : Object "g" not found
R> # with package "foreign" loaded
R> data.restore("junk2.dat")
Error in ReadSdump(TRUE, " ") : S mode "junk" (near byte offset 45) not 
supported
In addition: Warning message:
NAs introduced by coercion
R> data.restore("junk3.dat")
[1] "junk3.dat"
R> junk
$f
g

 >

-- Tony Plate

At Wednesday 11:45 AM 8/25/2004, Rolf Turner wrote:

>I'm puzzled by the discourse in this thread.  Briefly, dput() and
>dget() seem to work just fine for me.
>
>I tried
>
>         > junk <- list(x=rnorm(20),y=sample(1:100,12,TRUE))
>         > dput(junk,"junk.dat")
>
>in Splus (Version 6.1.2 Release 2 for Sun SPARC, SunOS 5.6 : 2002)
>
>and then in R
>
>         > junk <- dget("junk.dat")
>
>R version:
>
>platform sparc-sun-solaris2.9
>arch     sparc
>os       solaris2.9
>system   sparc, solaris2.9
>status
>major    1
>minor    9.1
>year     2004
>month    06
>day      21
>language R
>
>There were no complaints, and typing ``junk'' in the R window
>and in the Splus window appeared to produce indentical results.
>
>So what's the problem?
>
>                                 cheers,
>
>                                         Rolf Turner
>                                         rolf at math.unb.ca
>
>Tony Plate wrote:
>
> > Have you tried following the advice in the R Data Import/Export 
> manual?  It
> > suggests the following:
> >
> > >Function data.restore reads S-PLUS data dumps (created by data.dump) with
> > >the same restrictions (except that dumps from the Alpha platform can also
> > >be read).
> > >It should be possible to read data dumps from S-PLUS 5.x and 6.x 
> written with
> > >data.dump(oldStyle=T).
> >
> > -- Tony Plate
> >
> > At Wednesday 10:29 AM 8/25/2004, Zachary Skrivanek wrote:
> > >Hello!  I would like to be able to read in list data objects in R/S
> > >created in R/S.  (Ie R->S or S->R.)  I have tried 'dput' and 'dump' in S,
> > >but neither of the created files could be read into R (with 'dget' nor
> > >'source').  Is there any way that I can save a list object in S that can
> > >be read into R?
> > >
> > >Sincerely,
> > >Zachary Skrivanek, PhD
> > >Research Scientist
> > >Program Phase Statistics-Endocrine
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From rolf at math.unb.ca  Wed Aug 25 20:27:17 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Wed, 25 Aug 2004 15:27:17 -0300 (ADT)
Subject: [R] S <-> R
Message-ID: <200408251827.i7PIRHvd002851@erdos.math.unb.ca>

Thanks Tony.  But the Mr. Skrivanek initially said he couldn't get
dput() and dget() to work with ***lists***.  It seems to turn out
that lists, as such, are not the issue.  The problem stems from
having relatively weird components in the lists.

Also, in your example, the issue is not an R <--> S incompatibility
problem.  The example causes an error even if one stays entirely
within Splus (or within R):

===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===
> junk <- list(f=as.name("g"))
> dput(junk,"junk.dat")
> crap <- dget("junk.dat")
Problem in dget("junk.dat"): Object "g" not found 
Use traceback() to see the call stack
===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===

And the same happens in R (except that the error message is phrased
differently:

===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===
Error in structure(list(f = g), .Names = "f") : 
        Object "g" not found
===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===+===

					cheers,

						Rolf Turner
						rolf at math.unb.ca



From jeff at edc.uri.edu  Wed Aug 25 20:26:42 2004
From: jeff at edc.uri.edu (Jeff Hollister)
Date: Wed, 25 Aug 2004 14:26:42 -0400
Subject: [R] Newbie Question:  Spatial Autocorrelation with R Tutorial?
Message-ID: <9056D760A8B1EA4F91CB9CE194C5210A3BDBEF@sentinel.edc.uri.edu>

Howdy All,

I am looking for some good tutorials (books, websites, whatever) for calculating/testing for Spatial Autocorrelation using R.

Specifically, I am wanting to test for autocorrelation of a number of variables measured at a set of discrete locations.

Up to this point I have been exploring the "spdep" package and I can get "moran.test" to work, but I am concerned that somewhere along the line I may not be doing things correctly.  Hence my request for a tutorial so that I may brush up on my autocorrelation basics, specifically autocorrelation with R, and reassure myself that the results I am getting aren't bogus.

Thanks in advance for any suggestions!

Jeff Hollister
  
*****************************************************
Jeffrey William Hollister                          
Ph.D. Candidate                                      
Environmental Data Center                       
Department of Natural Resources Science  
University of Rhode Island                           
office: (401) 874 5054
fax: (401) 874 4561
cell: (401)556 4087
http://www.edc.uri.edu/personal/jeff/home/jwh_cv_full.htm



From peterson at heritage.nv.gov  Wed Aug 25 20:57:59 2004
From: peterson at heritage.nv.gov (Eric Peterson)
Date: Wed, 25 Aug 2004 11:57:59 -0700
Subject: [R] Censored (Tobit) Regression method
Message-ID: <DOEBIKIFOPFCBHBFCKOKOEODCIAA.peterson@heritage.nv.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040825/906b9d59/attachment.pl

From myao at ou.edu  Wed Aug 25 21:20:52 2004
From: myao at ou.edu (Yao, Minghua)
Date: Wed, 25 Aug 2004 14:20:52 -0500
Subject: [R] How to Arrange character vector in alphabetic order
Message-ID: <89944065A099FB4AB1296358DD02877A1FD71B@XMAIL.sooner.net.ou.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040825/30c20b58/attachment.pl

From jfox at mcmaster.ca  Wed Aug 25 21:24:15 2004
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 25 Aug 2004 15:24:15 -0400
Subject: [R] License for including datasets in packages
In-Reply-To: <200408251905.31853.ahenningsen@email.uni-kiel.de>
Message-ID: <20040825192419.RNWX25796.tomts36-srv.bellnexxia.net@JohnDesktop8300>

Dear Arne,

I'm not a lawyer, but I believe that data per se are not copyrightable.

Regards,
 John 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Arne Henningsen
> Sent: Wednesday, August 25, 2004 12:06 PM
> To: r-help at stat.math.ethz.ch
> Cc: jfox at mcmaster.ca
> Subject: [R] License for including datasets in packages
> 
> Dear All,
> 
> I would like to publish a function for 'heckit' estimations 
> together with two examples from Greene's and Wooldridge's 
> econometric textbooks. 
> These examples use the dataset of Mroz (1987) that is also 
> available in John Fox' "car" package. However, not all 
> variables that are used in my examples are available in the 
> "car" package. Therefore, I want to put the full Mroz dataset 
> in my package. This full dataset can be downloaded from some 
> internet sites (e.g. 
> pages.stern.nyu.edu/~wgreene/Text/econometricanalysis.htm,
> www.stata.com/texts/eacsap/), but I did not find any license 
> information. 
> 
> Whom do I have to ask wether I am allowed to put these data 
> in a GPLed R package? (Mr. Mroz, Mr. Greene, STATA, 
> University of Michigan (PSID), . . . )
> 
> @John: Do you have any license information on these data?
> 
> Thanks,
> Arne
> 
> --
> Arne Henningsen
> Department of Agricultural Economics
> University of Kiel
> Olshausenstr. 40
> D-24098 Kiel (Germany)
> Tel: +49-431-880 4445
> Fax: +49-431-880 1397
> ahenningsen at agric-econ.uni-kiel.de
> http://www.uni-kiel.de/agrarpol/ahenningsen/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Wed Aug 25 21:32:59 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 25 Aug 2004 15:32:59 -0400
Subject: [R] How to Arrange character vector in alphabetic order
Message-ID: <3A822319EB35174CA3714066D590DCD504AF82A1@usrymx25.merck.com>

You mean as in:

> x <- c("aaa", "abc", "ab", "d")
> sort(x)
[1] "aaa" "ab"  "abc" "d"  

??

Andy

> From: Yao, Minghua
> 
> Hi,
>  
> Is there any function that can arrange a character in 
> alphabetic order? Thanks for answer
>  
> -MY



From jgentry at jimmy.harvard.edu  Wed Aug 25 21:36:13 2004
From: jgentry at jimmy.harvard.edu (Jeff Gentry)
Date: Wed, 25 Aug 2004 15:36:13 -0400 (EDT)
Subject: [R] How to Arrange character vector in alphabetic order
In-Reply-To: <89944065A099FB4AB1296358DD02877A1FD71B@XMAIL.sooner.net.ou.edu>
Message-ID: <Pine.SOL.4.20.0408251535210.777-100000@santiam.dfci.harvard.edu>

> Is there any function that can arrange a character in alphabetic order? Thanks for answer

Check out sort().

> vv <- c(letters[5:10], letters[20:11], letters[21:26], letters[4:1])
> vv
 [1]
"e" "f" "g" "h" "i" "j" "t" "s" "r" "q" "p" "o" "n" "m" "l" "k" "u" "v" "w"[20]
"x" "y" "z" "d" "c" "b" "a"
> sort(vv)
 [1]
"a" "b" "c" "d" "e" "f" "g" "h" "i" "j" "k" "l" "m" "n" "o" "p" "q" "r" "s"[20]
"t" "u" "v" "w" "x" "y" "z"



From sundar.dorai-raj at PDF.COM  Wed Aug 25 21:38:19 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Wed, 25 Aug 2004 12:38:19 -0700
Subject: [R] How to Arrange character vector in alphabetic order
In-Reply-To: <89944065A099FB4AB1296358DD02877A1FD71B@XMAIL.sooner.net.ou.edu>
References: <89944065A099FB4AB1296358DD02877A1FD71B@XMAIL.sooner.net.ou.edu>
Message-ID: <412CEAAB.7030308@pdf.com>



Yao, Minghua wrote:

> Hi,
>  
> Is there any function that can arrange a character in alphabetic order? Thanks for answer
>  
> -MY 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

?sort, ?order, etc....

--sundar



From myao at ou.edu  Wed Aug 25 21:38:48 2004
From: myao at ou.edu (Yao, Minghua)
Date: Wed, 25 Aug 2004 14:38:48 -0500
Subject: [R] RE: How to Arrange character vector in alphabetic order
Message-ID: <89944065A099FB4AB1296358DD02877A1FD71C@XMAIL.sooner.net.ou.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040825/dc4db6f7/attachment.pl

From Roger.Bivand at nhh.no  Wed Aug 25 21:42:55 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 25 Aug 2004 21:42:55 +0200 (CEST)
Subject: [R] Pixmap problem
In-Reply-To: <001001c48a8e$a953ff60$434e2880@geol.ucl.ac.uk>
Message-ID: <Pine.LNX.4.44.0408252058240.19494-100000@reclus.nhh.no>

On Wed, 25 Aug 2004, Benjamin Lloyd-Hughes wrote:

> Hi,
> 
> I'm having trouble writing .pnm images which I think is due to a problem
> with my colour space.  The pixmap object seems to be looking for 72 of 8
> colours (one per cell?) which doesn't seem healthy...
> 
> > library(pixmap)
> > x <- pixmapIndexed(rep(1:8, 9), nrow=6, col=rainbow(8))
> > x
> Pixmap image
>   Type          : pixmapIndexed 
>   Size          : 6x12 
>   Resolution    : 1x1 
>   Bounding box  : 0 0 12 6 
>   Nr. of colors : 72 of 8 
> 
> > write.pnm(x, file="D:/Temp/output.pnm")
> Error in options(x) : evaluation nested too deeply: infinite recursion /
> options(expression=)?

This one is in:

> x <- pixmapIndexed(rep(1:8, 9), nrow=6, col=rainbow(8))
> as(x, "pixmapRGB")
Error in options(x) : evaluation nested too deeply: infinite recursion / 
options(expression=)?

and as(x, "pixmapRGB") is called to make the indexed pixmap 
an RGB pixmap, and hits:

setAs("pixmapIndexed", "pixmapRGB",
function(from, to){
    z = as(from, "pixmapRGB")

in setAs("pixmapIndexed", "pixmapRGB", ...

with traceback() saying, quite logically:

> traceback()
140: options(x)
139: getOption("topLevelEnvironment")
138: identical(envir, matchThisEnv)
137: topenv(parent.frame())
136: exists(classMetaName(Class), where)
135: isClass(class1)
134: possibleExtends(cl, class2)
133: is(fdef, "genericMethods")
132: getMethodsForDispatch(f, fdef)
131: getMethods("coerce")
130: .quickCoerceSelect(thisClass, Class)
129: as(from, "pixmapRGB")
128: asMethod(object)
...
5: as(from, "pixmapRGB")
4: asMethod(object)
3: as(from, "pixmapRGB")
2: asMethod(object)
1: as(x, "pixmap")

for me:

> version
         _                           
platform i686-pc-linux-gnu           
arch     i686                        
os       linux-gnu                   
system   i686, linux-gnu             
status   Under development (unstable)
major    2                           
minor    0.0                         
year     2004                        
month    08                          
day      06                          
language R                           

Until this is fixed, is it feasible for you to use pixmapRGB, not
pixmapIndexed, directly? "pixmapRGB" objects seem to pass through 
write.pnm() without trouble.

> >
> 
> Grey ones work but with a warning...
> 
> > x <- pixmapGrey(rep(1:8, 9), nrow=6)
> > x
> Pixmap image
>   Type          : pixmapGrey 
>   Size          : 6x12 
>   Resolution    : 1x1 
>   Bounding box  : 0 0 12 6 
> 
> > write.pnm(x, file="D:/Temp/output.pnm")
> Warning message: 
> connection is already open 
> >

> showConnections(TRUE)
  description class      mode text   isopen   can read can write
0 "stdin"     "terminal" "r"  "text" "opened" "yes"    "no"     
1 "stdout"    "terminal" "w"  "text" "opened" "no"     "yes"    
2 "stderr"    "terminal" "w"  "text" "opened" "no"     "yes"    

suggests that this is caused by a coding infelicity:

        con <- file(file, open="w")
        open(con, open="w")

in write.pnm() - the open() is unneeded, and will be removed in the next 
release.

> 
> Any ideas? 
> 
> Cheers, Ben
> [R 1.9.1 and Pixmap 0.4-1 under windows]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From Mike.Prager at noaa.gov  Wed Aug 25 21:42:44 2004
From: Mike.Prager at noaa.gov (Mike Prager)
Date: Wed, 25 Aug 2004 15:42:44 -0400
Subject: [R] Adding labels to variables
In-Reply-To: <B88F4BCF37DD0847937C1C98255291FB0175C736@uswsmx05.merck.co
 m>
References: <B88F4BCF37DD0847937C1C98255291FB0175C736@uswsmx05.merck.com>
Message-ID: <6.1.2.0.0.20040825154112.01b0c0c0@hermes.nos.noaa.gov>

At 01:01 PM 08/25/2004, Raubertas, Richard wrote:
>As long as the function to load a package is called *library*,
>I think your campaign to change common usage is doomed to failure.

Yes, and it's also annoyingly hard to remember -- at least for those of us 
who don't eat, sleep, and breathe R.

MHP



-- 
Michael Prager, Ph.D.                <Mike.Prager at noaa.gov>
NOAA Center for Coastal Fisheries and Habitat Research
Beaufort, North Carolina  28516
http://shrimp.ccfhrb.noaa.gov/~mprager/

NOTE: Opinions expressed are personal, not official.  No government 
endorsement of
any product is expressed or implied.



From sundar.dorai-raj at PDF.COM  Wed Aug 25 22:04:22 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Wed, 25 Aug 2004 13:04:22 -0700
Subject: [R] RE: How to Arrange character vector in alphabetic order
In-Reply-To: <89944065A099FB4AB1296358DD02877A1FD71C@XMAIL.sooner.net.ou.edu>
References: <89944065A099FB4AB1296358DD02877A1FD71C@XMAIL.sooner.net.ou.edu>
Message-ID: <412CF0C6.8000308@pdf.com>



Yao, Minghua wrote:
> Sorry for not asking the question clearly. The elements of the character vector may consist of numbers not just letter. How to do that. Thanks.
>  
> -MY
> 

Why won't ?sort do what you want? Please provide an example and the 
output you desire.

--sundar



From chrysopa at insecta.ufv.br  Wed Aug 25 22:01:40 2004
From: chrysopa at insecta.ufv.br (Ronaldo Reis Jr.)
Date: Wed, 25 Aug 2004 17:01:40 -0300
Subject: [R] integrate function
Message-ID: <200408251701.41005.chrysopa@insecta.ufv.br>

Is possible to integrate this diferential equation:

dN/dt = Nr(1-(N/K))

in R using the integrate() function?

Or any other diferential equation?

If yes, how?

If no, anybody know any software on linux that make this?

Inte
Ronaldo
-- 

O problema de ter os dois p??s bem firmes no ch??o ?? que voc?? n??o vai conseguir 
tirar as cal??as.

--An??nimo
--
|>   // | \\   [***********************************]
|   ( ??   ?? )  [Ronaldo Reis J??nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36571-000 Vi??osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-2532                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From andy_liaw at merck.com  Wed Aug 25 22:17:24 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 25 Aug 2004 16:17:24 -0400
Subject: [R] RE: How to Arrange character vector in alphabetic order
Message-ID: <3A822319EB35174CA3714066D590DCD504AF82A3@usrymx25.merck.com>

Please do read the posting guide, which suggests that you provide a simple
example of what you want to achieve, instead of leaving people guessing what
you have in mind.

Andy

> From: Yao, Minghua
> 
> Sorry for not asking the question clearly. The elements of 
> the character vector may consist of numbers not just letter. 
> How to do that. Thanks.
>  
> -MY
> 
> ________________________________
> 
> From: Yao, Minghua
> Sent: Wed 8/25/2004 2:20 PM
> To: R Help
> Subject: How to Arrange character vector in alphabetic order
> 
> 
> Hi,
>  
> Is there any function that can arrange a character in 
> alphabetic order? Thanks for answer
>  
> -MY 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From p.dalgaard at biostat.ku.dk  Wed Aug 25 22:36:43 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 25 Aug 2004 22:36:43 +0200
Subject: [R] Adding labels to variables
In-Reply-To: <6.1.2.0.0.20040825154112.01b0c0c0@hermes.nos.noaa.gov>
References: <B88F4BCF37DD0847937C1C98255291FB0175C736@uswsmx05.merck.com>
	<6.1.2.0.0.20040825154112.01b0c0c0@hermes.nos.noaa.gov>
Message-ID: <x2y8k3lztw.fsf@biostat.ku.dk>

"Mike Prager" <Mike.Prager at noaa.gov> writes:

> At 01:01 PM 08/25/2004, Raubertas, Richard wrote:
> >As long as the function to load a package is called *library*,
> >I think your campaign to change common usage is doomed to failure.

Who says it's common usage? It is of course a common fallacy to think
that everyone else makes the same mistakes as you do... 
 
> Yes, and it's also annoyingly hard to remember -- at least for those
> of us who don't eat, sleep, and breathe R.

There's a good chance that R-2.x will introduce usePackage() and
eventually remove library() as a tool for loading packages. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From roebuck at odin.mdacc.tmc.edu  Wed Aug 25 22:42:36 2004
From: roebuck at odin.mdacc.tmc.edu (Paul Roebuck)
Date: Wed, 25 Aug 2004 15:42:36 -0500 (CDT)
Subject: [R] [Q] Apply Function Over Multiple Array Margins
Message-ID: <Pine.OSF.4.58.0408251514120.503716@odin.mdacc.tmc.edu>

Is there a canonical means to apply a function
over multiple arrays simultaneously? For example,
if I wanted to plot each line with a different color?
Or is a for loop conversion my best option?


x <- seq(0, 8, by = 2)
y <- matrix(1:15, nrow = 5, byrow = TRUE)
my.colors <- heat.colors(3)

drawLines <- function(row) {
    lines(x, row)    # want corresponding 'my.colors' here
}

plotData <- function(x, y) {
    plot(x, type='n', axes = FALSE,
         xlim = c(min(x), max(x)),
         ylim = c(0, max(y)));
    box();
    axis(1, min(x):max(x))
    axis(2, seq(0, max(y), by = 5))
    axis(3, labels = FALSE)
    axis(4, labels = FALSE)
    apply(y, 1, drawLines)
}

plotData(x, t(y))

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From p.dalgaard at biostat.ku.dk  Wed Aug 25 22:44:09 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 25 Aug 2004 22:44:09 +0200
Subject: [R] integrate function
In-Reply-To: <200408251701.41005.chrysopa@insecta.ufv.br>
References: <200408251701.41005.chrysopa@insecta.ufv.br>
Message-ID: <x2u0urlzhi.fsf@biostat.ku.dk>

"Ronaldo Reis Jr." <chrysopa at insecta.ufv.br> writes:

> Is possible to integrate this diferential equation:
> 
> dN/dt = Nr(1-(N/K))
> 
> in R using the integrate() function?

No.
 
> Or any other diferential equation?

Only if the right hand side does not depend on the dependent variable
(N in this case) -- just plain integrals.

> If yes, how?
> 
> If no, anybody know any software on linux that make this?

The "odesolve" package in R. 

Possibly, there's also stuff in Octave but I haven't looked lately. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Wed Aug 25 22:51:34 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 25 Aug 2004 22:51:34 +0200
Subject: [R] [Q] Apply Function Over Multiple Array Margins
In-Reply-To: <Pine.OSF.4.58.0408251514120.503716@odin.mdacc.tmc.edu>
References: <Pine.OSF.4.58.0408251514120.503716@odin.mdacc.tmc.edu>
Message-ID: <x2oekzlz55.fsf@biostat.ku.dk>

Paul Roebuck <roebuck at odin.mdacc.tmc.edu> writes:

> Is there a canonical means to apply a function
> over multiple arrays simultaneously? For example,
> if I wanted to plot each line with a different color?
> Or is a for loop conversion my best option?

Try looking at matplot() and friends...
 
> 
> x <- seq(0, 8, by = 2)
> y <- matrix(1:15, nrow = 5, byrow = TRUE)
> my.colors <- heat.colors(3)
> 
> drawLines <- function(row) {
>     lines(x, row)    # want corresponding 'my.colors' here
> }
> 
> plotData <- function(x, y) {
>     plot(x, type='n', axes = FALSE,
>          xlim = c(min(x), max(x)),
>          ylim = c(0, max(y)));
>     box();
>     axis(1, min(x):max(x))
>     axis(2, seq(0, max(y), by = 5))
>     axis(3, labels = FALSE)
>     axis(4, labels = FALSE)
>     apply(y, 1, drawLines)
> }
> 
> plotData(x, t(y))
> 
> ----------------------------------------------------------
> SIGSIG -- signature too long (core dumped)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jgentry at jimmy.harvard.edu  Wed Aug 25 22:55:20 2004
From: jgentry at jimmy.harvard.edu (Jeff Gentry)
Date: Wed, 25 Aug 2004 16:55:20 -0400 (EDT)
Subject: [R] [Q] Apply Function Over Multiple Array Margins
In-Reply-To: <Pine.OSF.4.58.0408251514120.503716@odin.mdacc.tmc.edu>
Message-ID: <Pine.SOL.4.20.0408251654570.777-100000@santiam.dfci.harvard.edu>

> Is there a canonical means to apply a function
> over multiple arrays simultaneously? For example,

Would mapply() get you what you're looking for?



From von-hippel.1 at osu.edu  Wed Aug 25 23:39:39 2004
From: von-hippel.1 at osu.edu (Paul von Hippel)
Date: Wed, 25 Aug 2004 17:39:39 -0400
Subject: [R] gam plots
Message-ID: <5.2.0.9.2.20040825173809.043b91f8@mail.sociology.ohio-state.edu>

When smooths fitted by the gam package are plotted, what are the units of 
the vertical axis? Is there a simple way to change these units to units of 
the dependent variable?

Thanks for any suggestions!
Paul von Hippel

Paul von Hippel
Department of Sociology / Initiative in Population Research
Ohio State University
300 Bricker Hall
190 N. Oval Mall
Columbus OH 43210
614 688-3768
Office hours M-Th 3-5pm



From minhan.science at gmail.com  Thu Aug 26 00:40:38 2004
From: minhan.science at gmail.com (Min-Han Tan)
Date: Wed, 25 Aug 2004 18:40:38 -0400
Subject: [R] brlr function
Message-ID: <7902152a04082515406b14c3ed@mail.gmail.com>

Hi,

I'm trying the brlr function in a penalized logistic regression function. 

However, I am not sure why I am encountering errors. I hope to seek
your advice here. (output below)

Thank you! Your help is truly appreciated.

Min-Han


#No error here, the glm seems to work fine
> genes.cox1.glm1<-glm(as.formula(paste(paste('as.integer(sim.cv.cox.yhat1)~'),paste('sim.dat.tst[',genes.cox1.rows,',]',sep="",collapse='+'))))

#Something happened here ... I only substituted brlr for glm
 > genes.cox1.glm1<-brlr(as.formula(paste(paste('as.integer(sim.cv.cox.yhat1)~'),paste('sim.dat.tst[',genes.cox1.rows,',]',sep="",collapse='+'))))
Error in eval(expr, envir, enclos) : y values must be 0 <= y <= 1

#using the test data in brlr gives the same error if the dependent is
reduced to one column, is it a problem with my dependent? It works
fine as per the original vignette with
brlr(cbind(grahami,opalinus)~height ...)
> brlr(grahami~height+diameter,data=lizards)
Error in eval(expr, envir, enclos) : y values must be 0 <= y <= 1
 
#This is how the objects look
> as.integer(sim.cv.cox.yhat1)
 [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 2 2 2 1 1 2 2 1 1 1 2 1 2 2 2
1 2 1 1 2 1 1 2 1 1 1 2 2 2 2

> as.formula(paste(paste('as.integer(sim.cv.cox.yhat1)~'),paste('sim.dat.tst[',genes.cox1.rows,',]',sep="",collapse='+')))
as.integer(sim.cv.cox.yhat1) ~ sim.dat.tst[27, ] + sim.dat.tst[35, 
    ] + sim.dat.tst[17, ] + sim.dat.tst[41, ] + sim.dat.tst[38, 
    ] + sim.dat.tst[31, ] + sim.dat.tst[3, ] + sim.dat.tst[32, 
    ] + sim.dat.tst[16, ] + sim.dat.tst[48, ] + sim.dat.tst[13, 
    ] + sim.dat.tst[28, ]
 
> sim.dat.tst[27,]
Sample 1  Sample 2  Sample 3  Sample 4  Sample 5  Sample 6  Sample 7 
Sample 8  Sample 9 Sample 10 Sample 11 Sample 12 Sample 13 Sample 14
Sample 15
     5.79      6.72      6.11      5.58      6.32      6.50      6.45 
    5.77      7.46      6.32      5.64      5.77      5.44      5.83  
   5.57
Sample 16 Sample 17 Sample 18 Sample 19 Sample 20 Sample 21 Sample 22
Sample 23 Sample 24 Sample 25 Sample 26 Sample 27 Sample 28 Sample 29
Sample 30
     5.70      5.67      5.72      5.50      6.34      5.98      6.10 
    6.25      6.13      7.40      7.28      8.37      5.73      6.83  
   5.72
Sample 31 Sample 32 Sample 33 Sample 34 Sample 35 Sample 36 Sample 37
Sample 38 Sample 39 Sample 40 Sample 41 Sample 42 Sample 43 Sample 44
Sample 45
     6.12      6.74      6.64      6.13      7.74      6.70      7.37 
    6.54      6.49      5.75      6.18      6.41      7.68      5.36  
   6.34
Sample 46 Sample 47 Sample 48 
     5.86      6.64      6.69



From richard_raubertas at merck.com  Thu Aug 26 00:50:38 2004
From: richard_raubertas at merck.com (Raubertas, Richard)
Date: Wed, 25 Aug 2004 18:50:38 -0400
Subject: [R] Adding labels to variables
Message-ID: <B88F4BCF37DD0847937C1C98255291FB0175C787@uswsmx05.merck.com>

Sigh.  I thought I was making a small innocuous observation,
but it has apparently been taken as some kind of challenge.
Martin's efforts to correct this usage seem (to me) frequent 
enough to justify calling the mistake 'common', but I guess
others will disagree.

And what grounds do you have for your implication that it is 
a mistake I make?

Rich Raubertas

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Peter Dalgaard
> Sent: Wednesday, August 25, 2004 4:37 PM
> To: Mike Prager
> Cc: R-help at stat.math.ethz.ch
> Subject: Re: [R] Adding labels to variables
> 
> 
> "Mike Prager" <Mike.Prager at noaa.gov> writes:
> 
> > At 01:01 PM 08/25/2004, Raubertas, Richard wrote:
> > >As long as the function to load a package is called *library*,
> > >I think your campaign to change common usage is doomed to failure.
> 
> Who says it's common usage? It is of course a common fallacy to think
> that everyone else makes the same mistakes as you do... 
>  
> > Yes, and it's also annoyingly hard to remember -- at least for those
> > of us who don't eat, sleep, and breathe R.
> 
> There's a good chance that R-2.x will introduce usePackage() and
> eventually remove library() as a tool for loading packages. 
> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: 
> (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: 
> (+45) 35327907
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From johannes_graumann at web.de  Thu Aug 26 01:07:32 2004
From: johannes_graumann at web.de (Johannes Graumann)
Date: Wed, 25 Aug 2004 16:07:32 -0700
Subject: [R] Beginners Question: Make nlm work
Message-ID: <20040825160732.47cdd152@localhost>

Hello,

I'm new to this and am trying to teach myself some R by plotting
biological data. The growth curve in question is supposed to be fitted
to the Verhulst equation, which may be transcribed as follows:
f(x)=a/(1+((a-0.008)/0.008)*exp(-(b*x)))
- for a known population density (0.008) at t(0).

I am trying to rework the example from "An Introduction to R" (p. 72)
for my case and am failing miserably. Could somebody glance over the
code below and nudge me into the right direction - I must have some
major conceptual problem which can't be solved by me staring at it ...
Since I'm repeating something I have done with gnuplot I know that 3 and
4e-3 as starting values for the fit are appropriate ...

Thanks for any hint,

Joh

setwd("~/Biology/R_versuch")
mydata<-read.table("YJG45-7_Growth.dat")
x<-mydata$V1
y<-mydata$V2
VH <- function(p) y ~ p[1]/(1+((p[1]-0.008)/0.008)*exp(-(p[2]*x)))
plot(x, y, xlab="Time (h)",ylab=expression(OD[600][~nm]),las=1)
out <- nlm(VH, p = c(3, 4e-3), hessian = TRUE)



From gunter.berton at gene.com  Thu Aug 26 01:45:46 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 25 Aug 2004 16:45:46 -0700
Subject: [R] Adding labels to variables
In-Reply-To: <B88F4BCF37DD0847937C1C98255291FB0175C787@uswsmx05.merck.com>
Message-ID: <200408252345.i7PNjkMA016917@compton.gene.com>

May I also point out the following section title in the R1091 FAQ for
Windows:

3.1 Can I install packages (libraries) in this version?  

So I think that R's own documentation supports Rich's (gentle) comment.

-- Bert Gunter
Genentech

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Raubertas, Richard
Sent: Wednesday, August 25, 2004 3:51 PM
To: 'Peter Dalgaard'
Cc: 'R-help at stat.math.ethz.ch'
Subject: RE: [R] Adding labels to variables

Sigh.  I thought I was making a small innocuous observation,
but it has apparently been taken as some kind of challenge.
Martin's efforts to correct this usage seem (to me) frequent 
enough to justify calling the mistake 'common', but I guess
others will disagree.

And what grounds do you have for your implication that it is 
a mistake I make?

Rich Raubertas



From louize99 at yahoo.co.uk  Thu Aug 26 02:34:15 2004
From: louize99 at yahoo.co.uk (Louize Hill)
Date: Thu, 26 Aug 2004 01:34:15 +0100
Subject: [R] labels on secondary y axis
Message-ID: <009c01c48b04$6aeddcc0$6bc98c52@Louisept>

Hi,
I have plotted a graph and used a secondary axis as follows:

plot (d$Year, d$HrFishing, type='l', col='1', xlab='Year', ylab='Effort
(hours/fishing)')
par(new=TRUE)
plot (d$Year, d$Landings, type='l', col='3', xlab='', ylab='')
axis(4)

However, if I put a label in the 2nd ylab='' it is placed over the 1st ylab,
on the left hand side.
Clearly label in the axis command is not what I want as that erases my tick
marks, and ylab generates errors here!
Please can someone give me a pointer on what I should be using.
Thanks
Louize


PS - how long do postings take to get in the archives - I remember seeing
something about scale values (pure numeric?) this week, I erased it and now
I discover I need to follow the advice!



From jfbrennan at rogers.com  Thu Aug 26 03:06:24 2004
From: jfbrennan at rogers.com (Jim Brennan)
Date: Wed, 25 Aug 2004 21:06:24 -0400
Subject: [R] Beginners Question: Make nlm work
References: <20040825160732.47cdd152@localhost>
Message-ID: <00f901c48b08$e9d3fd40$3b8ac445@slnt.phub.net.cable.rogers.com>


----- Original Message -----
From: "Johannes Graumann" <johannes_graumann at web.de>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, August 25, 2004 7:07 PM
Subject: [R] Beginners Question: Make nlm work


> Hello,
>
> I'm new to this and am trying to teach myself some R by plotting
> biological data. The growth curve in question is supposed to be fitted
> to the Verhulst equation, which may be transcribed as follows:
> f(x)=a/(1+((a-0.008)/0.008)*exp(-(b*x)))
> - for a known population density (0.008) at t(0).
>
> I am trying to rework the example from "An Introduction to R" (p. 72)
> for my case and am failing miserably. Could somebody glance over the
> code below and nudge me into the right direction - I must have some
> major conceptual problem which can't be solved by me staring at it ...
> Since I'm repeating something I have done with gnuplot I know that 3 and
> 4e-3 as starting values for the fit are appropriate ...
>
> Thanks for any hint,
>
> Joh
>
> setwd("~/Biology/R_versuch")
> mydata<-read.table("YJG45-7_Growth.dat")
> x<-mydata$V1
> y<-mydata$V2
> VH <- function(p) y ~ p[1]/(1+((p[1]-0.008)/0.008)*exp(-(p[2]*x)))

Shouldn't this be
VH <- function(p) sum((y - p[1]/(1+((p[1]-0.008)/0.008)*exp(-(p[2]*x))))^2)
Where you are minimizing the squared deviations from what your given
equation is when you pass it to nlm?

Maybe if you send some data that would make it more clear.

Jim


> plot(x, y, xlab="Time (h)",ylab=expression(OD[600][~nm]),las=1)
> out <- nlm(VH, p = c(3, 4e-3), hessian = TRUE)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From p.connolly at hortresearch.co.nz  Thu Aug 26 03:26:17 2004
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Thu, 26 Aug 2004 13:26:17 +1200
Subject: [R] labels on secondary y axis
In-Reply-To: <009c01c48b04$6aeddcc0$6bc98c52@Louisept>; from 
	louize99@yahoo.co.uk on Thu, Aug 26, 2004 at 01:34:15AM +0100
References: <009c01c48b04$6aeddcc0$6bc98c52@Louisept>
Message-ID: <20040826132617.R2422@hortresearch.co.nz>

On Thu, 26-Aug-2004 at 01:34AM +0100, Louize Hill wrote:

|> Hi,
|> I have plotted a graph and used a secondary axis as follows:
|> 
|> plot (d$Year, d$HrFishing, type='l', col='1', xlab='Year', ylab='Effort
|> (hours/fishing)')
|> par(new=TRUE)
|> plot (d$Year, d$Landings, type='l', col='3', xlab='', ylab='')
|> axis(4)
|> 
|> However, if I put a label in the 2nd ylab='' it is placed over the 1st ylab,
|> on the left hand side.
|> Clearly label in the axis command is not what I want as that erases my tick
|> marks, and ylab generates errors here!
|> Please can someone give me a pointer on what I should be using.

mtext is your friend.



-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From minhan.science at gmail.com  Thu Aug 26 03:31:43 2004
From: minhan.science at gmail.com (Min-Han Tan)
Date: Wed, 25 Aug 2004 21:31:43 -0400
Subject: [R] Re: brlr function
In-Reply-To: <7902152a04082515406b14c3ed@mail.gmail.com>
References: <7902152a04082515406b14c3ed@mail.gmail.com>
Message-ID: <7902152a04082518315e148070@mail.gmail.com>

Thank you all. 

The reason was because the dependent should have been 0 and 1, rather
than 1 and 2!

Apologies for any trouble.

Min-Han


On Wed, 25 Aug 2004 18:40:38 -0400, Min-Han Tan
<minhan.science at gmail.com> wrote:
> Hi,
> 
> I'm trying the brlr function in a penalized logistic regression function.
> 
> However, I am not sure why I am encountering errors. I hope to seek
> your advice here. (output below)
> 
> Thank you! Your help is truly appreciated.
> 
> Min-Han
> 
> #No error here, the glm seems to work fine
> > genes.cox1.glm1<-glm(as.formula(paste(paste('as.integer(sim.cv.cox.yhat1)~'),paste('sim.dat.tst[',genes.cox1.rows,',]',sep="",collapse='+'))))
> 
> #Something happened here ... I only substituted brlr for glm
> > genes.cox1.glm1<-brlr(as.formula(paste(paste('as.integer(sim.cv.cox.yhat1)~'),paste('sim.dat.tst[',genes.cox1.rows,',]',sep="",collapse='+'))))
> Error in eval(expr, envir, enclos) : y values must be 0 <= y <= 1
> 
> #using the test data in brlr gives the same error if the dependent is
> reduced to one column, is it a problem with my dependent? It works
> fine as per the original vignette with
> brlr(cbind(grahami,opalinus)~height ...)
> > brlr(grahami~height+diameter,data=lizards)
> Error in eval(expr, envir, enclos) : y values must be 0 <= y <= 1
> 
> #This is how the objects look
> > as.integer(sim.cv.cox.yhat1)
> [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 2 2 2 1 1 2 2 1 1 1 2 1 2 2 2
> 1 2 1 1 2 1 1 2 1 1 1 2 2 2 2
> 
> > as.formula(paste(paste('as.integer(sim.cv.cox.yhat1)~'),paste('sim.dat.tst[',genes.cox1.rows,',]',sep="",collapse='+')))
> as.integer(sim.cv.cox.yhat1) ~ sim.dat.tst[27, ] + sim.dat.tst[35,
>    ] + sim.dat.tst[17, ] + sim.dat.tst[41, ] + sim.dat.tst[38,
>    ] + sim.dat.tst[31, ] + sim.dat.tst[3, ] + sim.dat.tst[32,
>    ] + sim.dat.tst[16, ] + sim.dat.tst[48, ] + sim.dat.tst[13,
>    ] + sim.dat.tst[28, ]
> 
> > sim.dat.tst[27,]
> Sample 1  Sample 2  Sample 3  Sample 4  Sample 5  Sample 6  Sample 7
> Sample 8  Sample 9 Sample 10 Sample 11 Sample 12 Sample 13 Sample 14
> Sample 15
>     5.79      6.72      6.11      5.58      6.32      6.50      6.45
>    5.77      7.46      6.32      5.64      5.77      5.44      5.83
>   5.57
> Sample 16 Sample 17 Sample 18 Sample 19 Sample 20 Sample 21 Sample 22
> Sample 23 Sample 24 Sample 25 Sample 26 Sample 27 Sample 28 Sample 29
> Sample 30
>     5.70      5.67      5.72      5.50      6.34      5.98      6.10
>    6.25      6.13      7.40      7.28      8.37      5.73      6.83
>   5.72
> Sample 31 Sample 32 Sample 33 Sample 34 Sample 35 Sample 36 Sample 37
> Sample 38 Sample 39 Sample 40 Sample 41 Sample 42 Sample 43 Sample 44
> Sample 45
>     6.12      6.74      6.64      6.13      7.74      6.70      7.37
>    6.54      6.49      5.75      6.18      6.41      7.68      5.36
>   6.34
> Sample 46 Sample 47 Sample 48
>     5.86      6.64      6.69
>



From graumann at its.caltech.edu  Thu Aug 26 03:57:55 2004
From: graumann at its.caltech.edu (Johannes Graumann)
Date: Wed, 25 Aug 2004 18:57:55 -0700
Subject: [R] Beginners Question: Make nlm work
In-Reply-To: <00f901c48b08$e9d3fd40$3b8ac445@slnt.phub.net.cable.rogers.com>
References: <20040825160732.47cdd152@localhost>
	<00f901c48b08$e9d3fd40$3b8ac445@slnt.phub.net.cable.rogers.com>
Message-ID: <20040825185755.27721b2b@localhost>

Aaaahhhh - there was my conceptual problem ... Thanks!
... but 'Oh, God!'! R sucks at fitting (when directed to do so by the
simple minded)! Or am I really that off?
I attach a plot that contains my data as well as two modeled curves:
- the nice one fitted by gnuplot (nonlinear least-squares (NLLS)
Marquardt-Levenberg)
- the nasty one fitted by the code below.

Both algorithms were started with the same values for p[1] and p[2].

Comments?

Is there any way to access the 2 elements of 'out$estimate' from the
program?

Joh

setwd("~/Biology/R_versuch")
mydata<-read.table("YJG45-7_Growth.dat")
#plot(mydata$V1,mydata$V2,xlab="Time
(h)",ylab=expression(OD[600][~nm]),las=1) x<-mydata$V1
y<-mydata$V2
VH <- function(p) sum(
	(y - p[1]/(1+((p[1]-0.008)/0.008)*exp(-(p[2]*x))))^2) 
plot(x, y,
	axes=FALSE,
	type="p",
	xlab="Time (min)",
	ylab=expression(OD[600][~nm]),
	las=1,
	pch=20,
	tck=0.015,
	mgp=c(1.5,0.25,0),
)
axis(1,
	at=NULL,
	tick=TRUE,
	tck=0.015,
	mgp=c(1.5,0.25,0)
)
axis(2,
	at=NULL,
	tick=TRUE,
	tck=0.015,
	mgp=c(1.5,0.25,0)
)
axis(3,
	at=c(300,600,900,1200,1500,1800,2100),
	labels=c(5,10,15,20,25,30,35),
	tick=TRUE,
	tck=0.015,
	mgp=c(1.5,0.25,0))
axis(4,
	labels=FALSE,
	at=NULL,
	tick=TRUE,
	tck=0.015,
	mgp=c(1.5,0.25,0)
)
box() 
out <- nlm(VH, p = c(3, 4e-3), hessian = TRUE)
#Plot Results from Gnuplot fit
yfit1 <- 4.21632/(1+((4.21632-0.008)/0.008)*exp(-(0.00414403*x)))
lines(spline(x, yfit1))
#Plot Results from 'out'
yfit2 <-
3.000002050/(1+((3.000002050-0.008)/0.008)*exp(-(0.004587924*x)))
lines(spline(x,yfit2))

On Wed, 25 Aug 2004 21:06:24 -0400
"Jim Brennan" <jfbrennan at rogers.com> wrote:

> 
> ----- Original Message -----
> From: "Johannes Graumann" <johannes_graumann at web.de>
> To: <r-help at stat.math.ethz.ch>
> Sent: Wednesday, August 25, 2004 7:07 PM
> Subject: [R] Beginners Question: Make nlm work
> 
> 
> > Hello,
> >
> > I'm new to this and am trying to teach myself some R by plotting
> > biological data. The growth curve in question is supposed to be
> > fitted to the Verhulst equation, which may be transcribed as
> > follows: f(x)=a/(1+((a-0.008)/0.008)*exp(-(b*x)))
> > - for a known population density (0.008) at t(0).
> >
> > I am trying to rework the example from "An Introduction to R" (p.
> > 72) for my case and am failing miserably. Could somebody glance over
> > the code below and nudge me into the right direction - I must have
> > some major conceptual problem which can't be solved by me staring at
> > it ... Since I'm repeating something I have done with gnuplot I know
> > that 3 and 4e-3 as starting values for the fit are appropriate ...
> >
> > Thanks for any hint,
> >
> > Joh
> >
> > setwd("~/Biology/R_versuch")
> > mydata<-read.table("YJG45-7_Growth.dat")
> > x<-mydata$V1
> > y<-mydata$V2
> > VH <- function(p) y ~ p[1]/(1+((p[1]-0.008)/0.008)*exp(-(p[2]*x)))
> 
> Shouldn't this be
> VH <- function(p) sum((y -
> p[1]/(1+((p[1]-0.008)/0.008)*exp(-(p[2]*x))))^2) Where you are
> minimizing the squared deviations from what your given equation is
> when you pass it to nlm?
> 
> Maybe if you send some data that would make it more clear.
> 
> Jim
> 
> 
> > plot(x, y, xlab="Time (h)",ylab=expression(OD[600][~nm]),las=1)
> > out <- nlm(VH, p = c(3, 4e-3), hessian = TRUE)
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
-------------- next part --------------
A non-text attachment was scrubbed...
Name: plot1.eps
Type: application/postscript
Size: 6160 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20040825/acb8a8b9/plot1.eps

From spencer.graves at pdf.com  Thu Aug 26 04:03:04 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 25 Aug 2004 22:03:04 -0400
Subject: [R] Adding labels to variables
In-Reply-To: <x2y8k3lztw.fsf@biostat.ku.dk>
References: <B88F4BCF37DD0847937C1C98255291FB0175C736@uswsmx05.merck.com>	<6.1.2.0.0.20040825154112.01b0c0c0@hermes.nos.noaa.gov>
	<x2y8k3lztw.fsf@biostat.ku.dk>
Message-ID: <412D44D8.3040006@pdf.com>

      Mon Dieu! 

      I realize that I'm not part of r-devel, but for what it's worth, I 
vastly prefer commands and programming styles that produce code that can 
run without change in both S-Plus and R.  Any move to obsolete in R a 
command commonly used in S-Plus would just make life more difficult for 
those of us who have to live in both the S-Plus and R worlds.  In my 
judgment, where S-Plus and R currently differ, there usually seems to be 
a good reason, and R is usually superior.  One example is the "log" 
options in the probability functions. 

      The suggestion to introduce "usePackage" and obsolete "library" 
reminds me of a 6 week delay in the introduction in France of a US 
automobile, because the French Royal Academy failed to produce without a 
substantial delay an official French word for a new technical term.  The 
automobile could not be sold without the User's Manual in French, and it 
could not be printed without the approval of the French Royal Academy 
for new words. 

      just my 2e-12 cents.  spencer graves

Peter Dalgaard wrote:

>"Mike Prager" <Mike.Prager at noaa.gov> writes:
>
>  
>
>>At 01:01 PM 08/25/2004, Raubertas, Richard wrote:
>>    
>>
>>>As long as the function to load a package is called *library*,
>>>I think your campaign to change common usage is doomed to failure.
>>>      
>>>
>
>Who says it's common usage? It is of course a common fallacy to think
>that everyone else makes the same mistakes as you do... 
> 
>  
>
>>Yes, and it's also annoyingly hard to remember -- at least for those
>>of us who don't eat, sleep, and breathe R.
>>    
>>
>
>There's a good chance that R-2.x will introduce usePackage() and
>eventually remove library() as a tool for loading packages. 
>
>  
>



From jfox at mcmaster.ca  Thu Aug 26 05:28:22 2004
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 25 Aug 2004 23:28:22 -0400
Subject: [R] Adding labels to variables
In-Reply-To: <412D44D8.3040006@pdf.com>
Message-ID: <20040826032823.XNAD29920.tomts10-srv.bellnexxia.net@JohnDesktop8300>

Dear Spencer et al.,

I agree with Spencer's point: It's one thing to introduce usePackage() and
encourage its use, another to remove library().

Regards,
 John

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Spencer Graves
> Sent: Wednesday, August 25, 2004 9:03 PM
> To: Peter Dalgaard
> Cc: Mike Prager; R-help at stat.math.ethz.ch
> Subject: Re: [R] Adding labels to variables
> 
>       Mon Dieu! 
> 
>       I realize that I'm not part of r-devel, but for what 
> it's worth, I vastly prefer commands and programming styles 
> that produce code that can run without change in both S-Plus 
> and R.  Any move to obsolete in R a command commonly used in 
> S-Plus would just make life more difficult for those of us 
> who have to live in both the S-Plus and R worlds.  In my 
> judgment, where S-Plus and R currently differ, there usually 
> seems to be a good reason, and R is usually superior.  One 
> example is the "log" 
> options in the probability functions. 
> 
>       The suggestion to introduce "usePackage" and obsolete "library" 
> reminds me of a 6 week delay in the introduction in France of 
> a US automobile, because the French Royal Academy failed to 
> produce without a substantial delay an official French word 
> for a new technical term.  The automobile could not be sold 
> without the User's Manual in French, and it could not be 
> printed without the approval of the French Royal Academy for 
> new words. 
> 
>       just my 2e-12 cents.  spencer graves
> 
> Peter Dalgaard wrote:
> 
> >"Mike Prager" <Mike.Prager at noaa.gov> writes:
> >
> >  
> >
> >>At 01:01 PM 08/25/2004, Raubertas, Richard wrote:
> >>    
> >>
> >>>As long as the function to load a package is called *library*, I 
> >>>think your campaign to change common usage is doomed to failure.
> >>>      
> >>>
> >
> >Who says it's common usage? It is of course a common fallacy 
> to think 
> >that everyone else makes the same mistakes as you do...
> > 
> >  
> >
> >>Yes, and it's also annoyingly hard to remember -- at least 
> for those 
> >>of us who don't eat, sleep, and breathe R.
> >>    
> >>
> >
> >There's a good chance that R-2.x will introduce usePackage() and 
> >eventually remove library() as a tool for loading packages.
> >
> >  
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Thu Aug 26 05:57:09 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 26 Aug 2004 03:57:09 +0000 (UTC)
Subject: [R] Adding labels to variables
References: <B88F4BCF37DD0847937C1C98255291FB0175C736@uswsmx05.merck.com>
	<6.1.2.0.0.20040825154112.01b0c0c0@hermes.nos.noaa.gov>
	<x2y8k3lztw.fsf@biostat.ku.dk>
Message-ID: <loom.20040826T055518-825@post.gmane.org>

Peter Dalgaard <p.dalgaard <at> biostat.ku.dk> writes:

> There's a good chance that R-2.x will introduce usePackage() and
> eventually remove library() as a tool for loading packages. 

We already have require().



From ajayshah at mayin.org  Wed Aug 25 18:29:01 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Wed, 25 Aug 2004 21:59:01 +0530
Subject: [R] Difficulties in starting up with its package
Message-ID: <20040825162901.GA18796@igidr.ac.in>

Folks,

I'm trying to learn `its' and am stuck on many basics. Could you
please help? I am on R 1.9.1 (2004-06-21) on Linux 2.4.17 #2. My its
version says
  Packaged: Tue Apr 27 13:38:25 2004; HeywoodG
  Built: R 1.9.0; ; 2004-04-28 15:03:13; unix

This part flows fine --
  library(its)
  x1 <- newIts(start="2000-01-01", end="2000-01-10", 1:30, ncol=3)
  print(x1)
  x2 <- newIts(start="2000-01-01", end="2000-01-10", 1:30, ncol=3,
    extract=T, weekday=T)
  print(x2)

Now I get into trouble --
  str(x1)    # works
  str(x2)    # BREAKS

For the str(x2) it says "Error in object[1:ile] : subscript out of bounds"
Is this a bug?


I try to go on.
  # Let's try writing and reading --
  writecsvIts(x1, filename="/tmp/try.1")
  y <- its(readcsvIts(filename="/tmp/try.1"))
  print(y)      # looks fine
  print(x1)     # looks fine
  y-x1       # BREAKS

For the "y-x1" command, he says "Error in y - x1 : dates must match".
But when I look at y and x1, the dates do seem to match.


I read in two of my own data files like this --
  nifty.its <- readcsvIts(filename="nifty.d", header=F, sep=" ",
                          col.names=c("date", "r.nifty"),
                          informat=its.format("%Y%m%d"),
                          outformat=its.format("%Y-%m-%d"))
  inrusd.its <- readcsvIts(filename="inrusd.d", header=F, sep=" ",
                          col.names=c("date", "r.inrusd"),
                          informat=its.format("%Y%m%d"),
                          outformat=its.format("%Y-%m-%d"))

These seem to be fine. To help you diagnose my problem, I have put the
results of dput() for both these at the end. Now when I say:

  > union(nifty.its, inrusd.its)
    [1] 1.0149431 1.0093692 1.0126526 0.9845319 0.9804095 0.9846924 1.0105904
    [8] 1.0311819 0.9986636 0.9950831 0.9922442 0.9510527 0.9833306 1.0131162
    [15] 0.9966518 0.9970885 1.0000000 1.0065139 1.0093729 0.9991156 0.9975658
    [22] 1.0042147 1.0039761 0.9988999 0.9955947 0.9953540 1.0046677 0.9993363
    [29] 1.0015497 1.0022104 0.9984561

I find this odd: I thought union() was supposed to give me an its
object.

  > intersect(nifty.its, inrusd.its)
    numeric(0)

I find this odd: the two series have plenty of dates in common. And, I
thought intersect() would produce an its object.

Here's the dput results for nifty.its and inrusd.its --

> dput(nifty.its)
structure(c(1.01494311428086, 1.00936924878702, 1.01265263274214, 
0.984531863814928, 0.980409543074067, 0.98469241317728, 1.01059040709914, 
1.03118189975644, 0.998663641731672, 0.995083089562457, 0.992244183137353, 
0.951052697932426, 0.983330571665286), .Dim = as.integer(c(13, 
1)), .Dimnames = list(c("2004-05-04", "2004-05-05", "2004-05-06", 
"2004-05-07", "2004-05-10", "2004-05-20", "2004-05-21", "2004-05-24", 
"2004-05-25", "2004-05-26", "2004-05-27", "2004-05-28", "2004-05-31"),
"r.nifty"))
> dput(inrusd.its)
structure(c(1.01311623699683, 0.996651785714286, 0.997088465845465, 
1, 1.00651392632525, 1.00937290783307, 0.999115631218218, 0.997565833148927, 
1.00421472937001, 1.00397614314115, 0.998899889989, 0.995594713656388, 
0.995353982300885, 1.00466770393421, 0.99933628318584, 1, 1.00154970112907, 
1.00221043324492, 0.998456109395677), .Dim = as.integer(c(19, 
1)), .Dimnames = list(c("2004-05-03", "2004-05-05", "2004-05-06", 
"2004-05-07", "2004-05-10", "2004-05-11", "2004-05-12", "2004-05-13", 
"2004-05-14", "2004-05-17", "2004-05-18", "2004-05-19", "2004-05-20", 
"2004-05-21", "2004-05-24", "2004-05-25", "2004-05-26", "2004-05-27", 
"2004-05-28"), "r.inrusd"))

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From d.scott at auckland.ac.nz  Thu Aug 26 07:13:05 2004
From: d.scott at auckland.ac.nz (David Scott)
Date: Thu, 26 Aug 2004 17:13:05 +1200 (NZST)
Subject: [R] Plotting groupedData objects
Message-ID: <Pine.LNX.4.44.0408261547420.27848-100000@hydra.stat.auckland.ac.nz>


I am trying to create a plot similar to Figure 3.2 in Bates and Pinheiro.

I have repeated measurements on about 80 subjects from 2 treatment groups.  
I would like to have the panels for the two treatment groups in separate
groups and within those groups have the panels ordered on maximum value
(as is the default).

I am ok with getting plots similar to Figs 3.1 and 3.2, but can't see how 
to change the ordering of the panels to what I want.

Here is the definition of my groupedData object

RAWlmeData <- groupedData(RAW~Elapsed|ID,
              data=RAWData,
              labels=list(x="Elapsed time",y="Airways resistance"),
              units=list(x="(hours)",y="cm H20/L/sec"))

I guess I could just plot the two treatment groups separately in turn but 
I feel there is something I am missing.

I am using 1.9.1 on Windows XP.

David Scott


_________________________________________________________________
David Scott	Department of Statistics, Tamaki Campus
		The University of Auckland, PB 92019
		Auckland	NEW ZEALAND
Phone: +64 9 373 7599 ext 86830		Fax: +64 9 373 7000
Email:	d.scott at auckland.ac.nz 


Graduate Officer, Department of Statistics



From ggrothendieck at myway.com  Thu Aug 26 07:28:17 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 26 Aug 2004 05:28:17 +0000 (UTC)
Subject: [R] Difficulties in starting up with its package
References: <20040825162901.GA18796@igidr.ac.in>
Message-ID: <loom.20040826T072121-958@post.gmane.org>

Ajay Shah <ajayshah <at> mayin.org> writes:

: 
: Folks,
: 
: I'm trying to learn `its' and am stuck on many basics. 


Not sure about those issues but I am familiar with zoo and you could do
the above in zoo like this. 

require(zoo)

d <- seq(as.Date("2000-01-01"), as.Date("2000-01-10"), by = "day")
x1 <- zoo(matrix(1:30,ncol=3), d)

dw <- d[as.numeric(format(d,"%w")) %in% 1:5]
x2 <- zoo(matrix(1:30,ncol=3), dw)
print(x2)

str(x1)
str(x2)

write.table(data.frame(time = time(x1), unclass(x1)), file = "/mytest.txt")
y <- read.table("/mytest.txt", header = T, as.is = T)
y <- zoo(data.matrix(y[,-1]),as.Date(y[,1]))
print(y)
print(x1)

# this works since y and x1 same times and are conformable but in general 
# you need to do a merge first to align them
y-x1 

nifty.its <-
structure(c(1.01494311428086, 1.00936924878702, 1.01265263274214, 
0.984531863814928, 0.980409543074067, 0.98469241317728, 1.01059040709914, 
1.03118189975644, 0.998663641731672, 0.995083089562457, 0.992244183137353, 
0.951052697932426, 0.983330571665286), .Dim = as.integer(c(13, 
1)), .Dimnames = list(c("2004-05-04", "2004-05-05", "2004-05-06", 
"2004-05-07", "2004-05-10", "2004-05-20", "2004-05-21", "2004-05-24", 
"2004-05-25", "2004-05-26", "2004-05-27", "2004-05-28", "2004-05-31"),
"r.nifty"))

inrusd.its <-
structure(c(1.01311623699683, 0.996651785714286, 0.997088465845465, 
1, 1.00651392632525, 1.00937290783307, 0.999115631218218, 0.997565833148927, 
1.00421472937001, 1.00397614314115, 0.998899889989, 0.995594713656388, 
0.995353982300885, 1.00466770393421, 0.99933628318584, 1, 1.00154970112907, 
1.00221043324492, 0.998456109395677), .Dim = as.integer(c(19, 
1)), .Dimnames = list(c("2004-05-03", "2004-05-05", "2004-05-06", 
"2004-05-07", "2004-05-10", "2004-05-11", "2004-05-12", "2004-05-13", 
"2004-05-14", "2004-05-17", "2004-05-18", "2004-05-19", "2004-05-20", 
"2004-05-21", "2004-05-24", "2004-05-25", "2004-05-26", "2004-05-27", 
"2004-05-28"), "r.inrusd"))

nifty.zoo <- zoo(nifty.its, as.Date(rownames(nifty.its)))
rownames(nifty.zoo) <- NULL

inrusd.zoo <- zoo(inrusd.its, as.Date(rownames(inrusd.its)))
rownames(inrusd.zoo) <- NULL

# merge.zoo is similar to union in its
merge(nifty.zoo, inrusd.zoo)

# merge.zoo with all = FALSE is similar to intersect in its
merge(nifty.zoo, inrusd.zoo, all = FALSE)



From Kevin.Wang at maths.anu.edu.au  Thu Aug 26 07:46:16 2004
From: Kevin.Wang at maths.anu.edu.au (Kevin Wang)
Date: Thu, 26 Aug 2004 15:46:16 +1000
Subject: [R] Order with scientific notations
In-Reply-To: <loom.20040826T072121-958@post.gmane.org>
References: <20040825162901.GA18796@igidr.ac.in>
	<loom.20040826T072121-958@post.gmane.org>
Message-ID: <412D7928.4030708@maths.anu.edu.au>

Hi,

If a value like 8e-04 is in a data frame, is the following behaviour normal?

   > final.df
      Chr P.values
   1    1   0.0379
   2    6    0.068
   3    2   0.0025
   4   13    8e-04
   5   14   0.0244
   6    3   0.0279
   7    4   0.1561
   8    5   0.9261
   9    7   0.0011
   10   9   0.5125
   11  10   0.2196
   12  11   0.6457
   13  18   0.9272

   > final.sorted <- data.frame(final.df[order(final.df$P.values), ],
   +                            row.names = 1:13)
   > final.sorted
      Chr P.values
   1    7   0.0011
   2    2   0.0025
   3   14   0.0244
   4    3   0.0279
   5    1   0.0379
   6    6    0.068
   7    4   0.1561
   8   10   0.2196
   9    9   0.5125
   10  11   0.6457
   11   5   0.9261
   12  18   0.9272
   13  13    8e-04

I'd think 8e-04 should be the smallest value...?  It's R 1.9.1 on 
Windows XP if that helps.

Cheers,

Kevin

-- 
Ko-Kang Kevin Wang
PhD Student
Centre for Mathematics and its Applications
Building 27, Room 1004
Mathematical Sciences Institute (MSI)
Australian National University
Canberra, ACT 0200
Australia

Homepage: http://wwwmaths.anu.edu.au/~wangk/
Ph (W): +61-2-6125-2431
Ph (H): +61-2-6125-7407
Ph (M): +61-40-451-8301



From ripley at stats.ox.ac.uk  Thu Aug 26 07:52:56 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 26 Aug 2004 06:52:56 +0100 (BST)
Subject: [R] library() (was Adding labels to variables)
In-Reply-To: <20040826032823.XNAD29920.tomts10-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <Pine.LNX.4.44.0408260640140.1822-100000@gannet.stats>

Can I remind people to use a relevant subject line -- this has gone way 
off the original topic.

It is not proposed to just rename library() to usePackage() or similar.
The need is for a new interface to the loading of packages, one that
returns useful information in an object.  We also need to sort out the
mess of require() vs library(), especially as `require' does not actually
require its argument (and almost no one tests the return value).  I don't 
think it is going to make 2.0.0, though.

Incidentally, the section heading Bert Gunter quoted does not indicate 
that `library' is a synonym for `package', but that packages are installed 
into libraries.  In S, `library sections' are installed into libraries.

Spencer should be very happy that data() will be almost completely 
optional come 2.0.0: that's the R/S difference that most hurts writing 
common documentation in my experience.


On Wed, 25 Aug 2004, John Fox wrote:

> Dear Spencer et al.,
> 
> I agree with Spencer's point: It's one thing to introduce usePackage() and
> encourage its use, another to remove library().
> 
> Regards,
>  John
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Spencer Graves
> > Sent: Wednesday, August 25, 2004 9:03 PM
> > To: Peter Dalgaard
> > Cc: Mike Prager; R-help at stat.math.ethz.ch
> > Subject: Re: [R] Adding labels to variables
> > 
> >       Mon Dieu! 
> > 
> >       I realize that I'm not part of r-devel, but for what 
> > it's worth, I vastly prefer commands and programming styles 
> > that produce code that can run without change in both S-Plus 
> > and R.  Any move to obsolete in R a command commonly used in 
> > S-Plus would just make life more difficult for those of us 
> > who have to live in both the S-Plus and R worlds.  In my 
> > judgment, where S-Plus and R currently differ, there usually 
> > seems to be a good reason, and R is usually superior.  One 
> > example is the "log" 
> > options in the probability functions. 
> > 
> >       The suggestion to introduce "usePackage" and obsolete "library" 
> > reminds me of a 6 week delay in the introduction in France of 
> > a US automobile, because the French Royal Academy failed to 
> > produce without a substantial delay an official French word 
> > for a new technical term.  The automobile could not be sold 
> > without the User's Manual in French, and it could not be 
> > printed without the approval of the French Royal Academy for 
> > new words. 
> > 
> >       just my 2e-12 cents.  spencer graves
> > 
> > Peter Dalgaard wrote:
> > 
> > >"Mike Prager" <Mike.Prager at noaa.gov> writes:
> > >
> > >  
> > >
> > >>At 01:01 PM 08/25/2004, Raubertas, Richard wrote:
> > >>    
> > >>
> > >>>As long as the function to load a package is called *library*, I 
> > >>>think your campaign to change common usage is doomed to failure.
> > >>>      
> > >>>
> > >
> > >Who says it's common usage? It is of course a common fallacy 
> > to think 
> > >that everyone else makes the same mistakes as you do...
> > > 
> > >  
> > >
> > >>Yes, and it's also annoyingly hard to remember -- at least 
> > for those 
> > >>of us who don't eat, sleep, and breathe R.
> > >>    
> > >>
> > >
> > >There's a good chance that R-2.x will introduce usePackage() and 
> > >eventually remove library() as a tool for loading packages.
> > >
> > >  
> > >
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at myway.com  Thu Aug 26 07:58:22 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 26 Aug 2004 05:58:22 +0000 (UTC)
Subject: [R] Difficulties in starting up with its package
References: <20040825162901.GA18796@igidr.ac.in>
	<loom.20040826T072121-958@post.gmane.org>
Message-ID: <loom.20040826T075436-858@post.gmane.org>


In looking at this again I don't think my x2 corresponds to yours.
It should be:

x2 <- zoo(matrix(1:30,ncol=3),d)[as.numeric(format(d,"%w")) %in% 1:5,]

Gabor Grothendieck <ggrothendieck <at> myway.com> writes:

: 
: Ajay Shah <ajayshah <at> mayin.org> writes:
: 
: : 
: : Folks,
: : 
: : I'm trying to learn `its' and am stuck on many basics. 
: 
: Not sure about those issues but I am familiar with zoo and you could do
: the above in zoo like this. 
: 
: require(zoo)
: 
: d <- seq(as.Date("2000-01-01"), as.Date("2000-01-10"), by = "day")
: x1 <- zoo(matrix(1:30,ncol=3), d)
: 
: dw <- d[as.numeric(format(d,"%w")) %in% 1:5]
: x2 <- zoo(matrix(1:30,ncol=3), dw)
: print(x2)
: 
: str(x1)
: str(x2)
: 
: write.table(data.frame(time = time(x1), unclass(x1)), file = "/mytest.txt")
: y <- read.table("/mytest.txt", header = T, as.is = T)
: y <- zoo(data.matrix(y[,-1]),as.Date(y[,1]))
: print(y)
: print(x1)
: 
: # this works since y and x1 same times and are conformable but in general 
: # you need to do a merge first to align them
: y-x1 
: 
: nifty.its <-
: structure(c(1.01494311428086, 1.00936924878702, 1.01265263274214, 
: 0.984531863814928, 0.980409543074067, 0.98469241317728, 1.01059040709914, 
: 1.03118189975644, 0.998663641731672, 0.995083089562457, 0.992244183137353, 
: 0.951052697932426, 0.983330571665286), .Dim = as.integer(c(13, 
: 1)), .Dimnames = list(c("2004-05-04", "2004-05-05", "2004-05-06", 
: "2004-05-07", "2004-05-10", "2004-05-20", "2004-05-21", "2004-05-24", 
: "2004-05-25", "2004-05-26", "2004-05-27", "2004-05-28", "2004-05-31"),
: "r.nifty"))
: 
: inrusd.its <-
: structure(c(1.01311623699683, 0.996651785714286, 0.997088465845465, 
: 1, 1.00651392632525, 1.00937290783307, 0.999115631218218, 0.997565833148927, 
: 1.00421472937001, 1.00397614314115, 0.998899889989, 0.995594713656388, 
: 0.995353982300885, 1.00466770393421, 0.99933628318584, 1, 1.00154970112907, 
: 1.00221043324492, 0.998456109395677), .Dim = as.integer(c(19, 
: 1)), .Dimnames = list(c("2004-05-03", "2004-05-05", "2004-05-06", 
: "2004-05-07", "2004-05-10", "2004-05-11", "2004-05-12", "2004-05-13", 
: "2004-05-14", "2004-05-17", "2004-05-18", "2004-05-19", "2004-05-20", 
: "2004-05-21", "2004-05-24", "2004-05-25", "2004-05-26", "2004-05-27", 
: "2004-05-28"), "r.inrusd"))
: 
: nifty.zoo <- zoo(nifty.its, as.Date(rownames(nifty.its)))
: rownames(nifty.zoo) <- NULL
: 
: inrusd.zoo <- zoo(inrusd.its, as.Date(rownames(inrusd.its)))
: rownames(inrusd.zoo) <- NULL
: 
: # merge.zoo is similar to union in its
: merge(nifty.zoo, inrusd.zoo)
: 
: # merge.zoo with all = FALSE is similar to intersect in its
: merge(nifty.zoo, inrusd.zoo, all = FALSE)
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://stat.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
: 
:



From ripley at stats.ox.ac.uk  Thu Aug 26 08:03:14 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 26 Aug 2004 07:03:14 +0100 (BST)
Subject: [R] Order with scientific notation
In-Reply-To: <412D7928.4030708@maths.anu.edu.au>
Message-ID: <Pine.LNX.4.44.0408260654510.1822-100000@gannet.stats>

Your column of P.values is not numeric.  If it were, it would be left
justified and not printed with trailing zeroes suppressed. As in

> (X <- data.frame(P.values=c(0.0379, 0.068, 0.0025, 8e-04)))
  P.values
1   0.0379
2   0.0680
3   0.0025
4   0.0008

So I guess it is factor or character, as in

> X$P.values <- as.character(X$P.values)
> X
  P.values
1   0.0379
2    0.068
3   0.0025
4    8e-04

> sort(X$P.values)
[1] "0.0025" "0.0379" "0.068"  "8e-04"

str() is usually your friend in problems like this.


On Thu, 26 Aug 2004, Kevin Wang wrote:

> Hi,
> 
> If a value like 8e-04 is in a data frame, is the following behaviour normal?
> 
>    > final.df
>       Chr P.values
>    1    1   0.0379
>    2    6    0.068
>    3    2   0.0025
>    4   13    8e-04
>    5   14   0.0244
>    6    3   0.0279
>    7    4   0.1561
>    8    5   0.9261
>    9    7   0.0011
>    10   9   0.5125
>    11  10   0.2196
>    12  11   0.6457
>    13  18   0.9272
> 
>    > final.sorted <- data.frame(final.df[order(final.df$P.values), ],
>    +                            row.names = 1:13)
>    > final.sorted
>       Chr P.values
>    1    7   0.0011
>    2    2   0.0025
>    3   14   0.0244
>    4    3   0.0279
>    5    1   0.0379
>    6    6    0.068
>    7    4   0.1561
>    8   10   0.2196
>    9    9   0.5125
>    10  11   0.6457
>    11   5   0.9261
>    12  18   0.9272
>    13  13    8e-04
> 
> I'd think 8e-04 should be the smallest value...?  It's R 1.9.1 on 
> Windows XP if that helps.
> 
> Cheers,
> 
> Kevin
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From deepayan at cs.wisc.edu  Thu Aug 26 08:32:00 2004
From: deepayan at cs.wisc.edu (Deepayan Sarkar)
Date: Thu, 26 Aug 2004 01:32:00 -0500
Subject: [R] Plotting groupedData objects
In-Reply-To: <Pine.LNX.4.44.0408261547420.27848-100000@hydra.stat.auckland.ac.nz>
References: <Pine.LNX.4.44.0408261547420.27848-100000@hydra.stat.auckland.ac.nz>
Message-ID: <1093501920.412d83e0d9891@www-auth.cs.wisc.edu>

Quoting David Scott <d.scott at auckland.ac.nz>:

> 
> I am trying to create a plot similar to Figure 3.2 in Bates and Pinheiro.
> 
> I have repeated measurements on about 80 subjects from 2 treatment groups.  
> I would like to have the panels for the two treatment groups in separate
> groups and within those groups have the panels ordered on maximum value
> (as is the default).
> 
> I am ok with getting plots similar to Figs 3.1 and 3.2, but can't see how 
> to change the ordering of the panels to what I want.
> 
> Here is the definition of my groupedData object
> 
> RAWlmeData <- groupedData(RAW~Elapsed|ID,
>               data=RAWData,
>               labels=list(x="Elapsed time",y="Airways resistance"),
>               units=list(x="(hours)",y="cm H20/L/sec"))
> 
> I guess I could just plot the two treatment groups separately in turn but 
> I feel there is something I am missing.

My reading of page 105 suggests that you need to specify 
'outer=<whatever your grouping factor is>'.
Have you tried that?

Deepayan



From jhpark at artsci.wustl.edu  Thu Aug 26 09:43:24 2004
From: jhpark at artsci.wustl.edu (Jong Hee Park)
Date: Thu, 26 Aug 2004 02:43:24 -0500 (CDT)
Subject: [R] A bug in ternaryplot (vcd)
Message-ID: <24479957.1093506204067.JavaMail.root@ascc>


Does anyone know how to make multiple ternaryplots?

My first figure in the following command appears in the bottom row and then 
the next figure goes to the next screen. 
> par(mfrow=c(2,1))
>    ternaryplot(list[,1:3], grid=F, pch="."); 
>    ternaryplot(list[,4:6], grid=F, pch=".")

I can't put these two figures in the same page. 

Does anyone have a good idea about this?

Jong Hee Park
Department of Political Science
Washington University in St.Louis
Campus Box 1063
One Brookings Drive
St. Louis, MO 63130
Office Phone: (314) 935-6764



From maechler at stat.math.ethz.ch  Thu Aug 26 09:49:22 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 26 Aug 2004 09:49:22 +0200
Subject: [R] postings and archives
In-Reply-To: <009c01c48b04$6aeddcc0$6bc98c52@Louisept>
References: <009c01c48b04$6aeddcc0$6bc98c52@Louisept>
Message-ID: <16685.38402.903958.172589@gargle.gargle.HOWL>

>>>>> "Louize" == Louize Hill <louize99 at yahoo.co.uk>
>>>>>     on Thu, 26 Aug 2004 01:34:15 +0100 writes:

    <....>

    Louize> PS - how long do postings take to get in the
    Louize> archives - I remember seeing something about scale
    Louize> values (pure numeric?) this week, I erased it and
    Louize> now I discover I need to follow the advice!

well, there are several (somewhat) independent archives.
The official "cheap" ones are directly available from the
mailing list page (URL at the end of every message!), and things
get in there typically faster than they hit your e-mailbox.

Then, there are quite a few other archives
the most prominent ones mentioned on
    http://cran.r-project.org/search.html

Martin Maechler



From ahenningsen at email.uni-kiel.de  Thu Aug 26 10:13:06 2004
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Thu, 26 Aug 2004 10:13:06 +0200
Subject: [R] Beginners Question: Make nlm work
In-Reply-To: <20040825160732.47cdd152@localhost>
References: <20040825160732.47cdd152@localhost>
Message-ID: <200408261013.06446.ahenningsen@email.uni-kiel.de>

Hi,

did you try "nls" (nonlinear least squares)? "?nlm" says: 
"For nonlinear regression, 'nls' may be better".
Another option would be "optim".

Best wishes,
Arne

On Thursday 26 August 2004 01:07, Johannes Graumann wrote:
> Hello,
>
> I'm new to this and am trying to teach myself some R by plotting
> biological data. The growth curve in question is supposed to be fitted
> to the Verhulst equation, which may be transcribed as follows:
> f(x)=a/(1+((a-0.008)/0.008)*exp(-(b*x)))
> - for a known population density (0.008) at t(0).
>
> I am trying to rework the example from "An Introduction to R" (p. 72)
> for my case and am failing miserably. Could somebody glance over the
> code below and nudge me into the right direction - I must have some
> major conceptual problem which can't be solved by me staring at it ...
> Since I'm repeating something I have done with gnuplot I know that 3 and
> 4e-3 as starting values for the fit are appropriate ...
>
> Thanks for any hint,
>
> Joh
>
> setwd("~/Biology/R_versuch")
> mydata<-read.table("YJG45-7_Growth.dat")
> x<-mydata$V1
> y<-mydata$V2
> VH <- function(p) y ~ p[1]/(1+((p[1]-0.008)/0.008)*exp(-(p[2]*x)))
> plot(x, y, xlab="Time (h)",ylab=expression(OD[600][~nm]),las=1)
> out <- nlm(VH, p = c(3, 4e-3), hessian = TRUE)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From michael.watson at bbsrc.ac.uk  Thu Aug 26 10:33:04 2004
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Thu, 26 Aug 2004 09:33:04 +0100
Subject: [R] Problems with par() and labels with boxplot
Message-ID: <8975119BCD0AC5419D61A9CF1A923E957C2950@iahce2knas1.iah.bbsrc.reserved>

Quite a simple one really!

When I run boxplot(), the labels on the X axis are horizontal, and I
want them vertical.  So I did:

par(las=3)
boxplot(...)

And my labels just aren't there anymore....

Any help???



From bavorak at klobouk.fsv.cvut.cz  Thu Aug 26 10:34:42 2004
From: bavorak at klobouk.fsv.cvut.cz (bavorak@klobouk.fsv.cvut.cz)
Date: Thu, 26 Aug 2004 10:34:42 +0200 (CEST)
Subject: [R] "o" bty with different axes
Message-ID: <Pine.LNX.4.43.0408261024110.30806-100000@klobouk.fsv.cvut.cz>

Is it possible to create plot bty=o with different scale of left- and
right x-axis? In my documents, by typesetting, bty=u looks disruptively.

At 2nd: how to frame the whole box (including title and axes labels) and
resize to landscape-oriented rectangle?

Tomas Bayer



From p.dalgaard at biostat.ku.dk  Thu Aug 26 10:32:08 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 26 Aug 2004 10:32:08 +0200
Subject: [R] Beginners Question: Make nlm work
In-Reply-To: <200408261013.06446.ahenningsen@email.uni-kiel.de>
References: <20040825160732.47cdd152@localhost>
	<200408261013.06446.ahenningsen@email.uni-kiel.de>
Message-ID: <x2ekluxptj.fsf@biostat.ku.dk>

Arne Henningsen <ahenningsen at email.uni-kiel.de> writes:

> Hi,
> 
> did you try "nls" (nonlinear least squares)? "?nlm" says: 
> "For nonlinear regression, 'nls' may be better".
> Another option would be "optim".

Actually, just using nlm  according to specs might help:

> > setwd("~/Biology/R_versuch")
> > mydata<-read.table("YJG45-7_Growth.dat")
> > x<-mydata$V1
> > y<-mydata$V2
> > VH <- function(p) y ~ p[1]/(1+((p[1]-0.008)/0.008)*exp(-(p[2]*x)))

Make that  

VH <- function(p) sum((y - p[1]/(1+((p[1]-0.008)/0.008)*exp(-(p[2]*x))))^2)

> > plot(x, y, xlab="Time (h)",ylab=expression(OD[600][~nm]),las=1)
> > out <- nlm(VH, p = c(3, 4e-3), hessian = TRUE)

I'm quite surprised that the other thing gives a result at all!

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ligges at statistik.uni-dortmund.de  Thu Aug 26 10:47:39 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 26 Aug 2004 10:47:39 +0200
Subject: [R] Problems with par() and labels with boxplot
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E957C2950@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E957C2950@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <412DA3AB.8070505@statistik.uni-dortmund.de>

michael watson (IAH-C) wrote:

> Quite a simple one really!
> 
> When I run boxplot(), the labels on the X axis are horizontal, and I
> want them vertical.  So I did:
> 
> par(las=3)
> boxplot(...)
> 
> And my labels just aren't there anymore....
> 
> Any help???

I guess they get clipped and you have to enlarge the margins, but you 
haven't given sufficient details such as
version of R, OS, device you are using, a reproducible example, ...

Please read the posting guide (see citation below).

Uwe Ligges


> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Ted.Harding at nessie.mcc.ac.uk  Thu Aug 26 10:43:10 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 26 Aug 2004 09:43:10 +0100 (BST)
Subject: [R] Order with scientific notations
In-Reply-To: <412D7928.4030708@maths.anu.edu.au>
Message-ID: <XFMail.040826094310.Ted.Harding@nessie.mcc.ac.uk>

On 26-Aug-04 Kevin Wang wrote:
> Hi,
> 
> If a value like 8e-04 is in a data frame, is the following behaviour
> normal?
> 
>    > final.df
>       Chr P.values
>    1    1   0.0379
>    2    6    0.068
>    3    2   0.0025
>    4   13    8e-04
>    5   14   0.0244
>    6    3   0.0279
>    7    4   0.1561
>    8    5   0.9261
>    9    7   0.0011
>    10   9   0.5125
>    11  10   0.2196
>    12  11   0.6457
>    13  18   0.9272
> 
>    > final.sorted <- data.frame(final.df[order(final.df$P.values), ],
>    +                            row.names = 1:13)
>    > final.sorted
>       Chr P.values
>    1    7   0.0011
>    2    2   0.0025
>    3   14   0.0244
>    4    3   0.0279
>    5    1   0.0379
>    6    6    0.068
>    7    4   0.1561
>    8   10   0.2196
>    9    9   0.5125
>    10  11   0.6457
>    11   5   0.9261
>    12  18   0.9272
>    13  13    8e-04
> 
> I'd think 8e-04 should be the smallest value...?  It's R 1.9.1 on 
> Windows XP if that helps.

It looks to me as though you have managed somehow to get the
"numerical" data into the dataframe as character strings, which
will be sorted alphabetically rather than numerically by 'order'.

The first clue is that if you read in a (e.g.) CSV file with your
original data as displayed above, then print it out with 'final.df',
then the value "8e-04" shows as "0.008". Sorting this DF with your
command shows "0.008" in its proper place, at the top of the list.

The next clue is that if you then assign final final.df[4,2] with

  final.df[4,2]<-"8e-04"

and then sort the DF in the same way, then "8e-04" comes out where
you found it, i.e. at the bottom of the list. This is consistent
with alphabetical sorting, since "8" follows "0" (and in any case
"e" follows ".").

So I think you should have a look at how the data got into final.df
in the first place.

Good luck,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 26-Aug-04                                       Time: 09:43:10
------------------------------ XFMail ------------------------------



From ligges at statistik.uni-dortmund.de  Thu Aug 26 10:53:30 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 26 Aug 2004 10:53:30 +0200
Subject: [R] "o" bty with different axes
In-Reply-To: <Pine.LNX.4.43.0408261024110.30806-100000@klobouk.fsv.cvut.cz>
References: <Pine.LNX.4.43.0408261024110.30806-100000@klobouk.fsv.cvut.cz>
Message-ID: <412DA50A.6030605@statistik.uni-dortmund.de>

bavorak at klobouk.fsv.cvut.cz wrote:

> Is it possible to create plot bty=o with different scale of left- and
> right x-axis? 

See the mailing list archives, this has been discussed several times 
during the last years.
Why is this related to bty="o" (which is the default)???

> In my documents, by typesetting, bty=u looks disruptively.

I don't see why using different scalings is related to bty settings???


> At 2nd: how to frame the whole box (including title and axes labels) and
> resize to landscape-oriented rectangle?

a) Please read the help page! ?box tells us something about an argument 
called "which" ...

b) Orientation is controlled by arguments to the function that starts 
the device (e.g. argument "horizontal" in postscript()).

Uwe Ligges


> Tomas Bayer
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Thu Aug 26 10:51:04 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 26 Aug 2004 10:51:04 +0200
Subject: [R] Problems with par() and labels with boxplot
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E957C2950@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E957C2950@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <x26576xoxz.fsf@biostat.ku.dk>

"michael watson (IAH-C)" <michael.watson at bbsrc.ac.uk> writes:

> Quite a simple one really!
> 
> When I run boxplot(), the labels on the X axis are horizontal, and I
> want them vertical.  So I did:
> 
> par(las=3)
> boxplot(...)
> 
> And my labels just aren't there anymore....
> 
> Any help???

You may need to make space for them. par(mar=....)

E.g.:

par(las=2,mar=c(10.1,4.1,4.1,2.1))
boxplot(list(foobarbazbletch=rnorm(10),snafu=rnorm(20)))


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From michael.watson at bbsrc.ac.uk  Thu Aug 26 11:08:00 2004
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Thu, 26 Aug 2004 10:08:00 +0100
Subject: [R] Problems with par() and labels with boxplot
Message-ID: <8975119BCD0AC5419D61A9CF1A923E951746D1@iahce2knas1.iah.bbsrc.reserved>

Hello

I thought it was such a simple problem it didn't warrant too much
detail!  Sorry :-)

OS is Suse Linux 8.2, R version is 1.9.0, device is x11.
Example reproducible code:

data(InsectSprays)
subset <- InsectSprays[1:24,]

# this creates normal horizontal text
boxplot(count ~ spray, data = subset)

# lets try vertical text
par(las = 3)
boxplot(count ~ spray, data = subset)

# lets try it with long names
par(las=3)
boxplot(count ~ spray, data = subset, names = c('a really, really,
really, really, really long name','a really, really, really, really,
really long name'))

# lets try adjusting the marjins
par(las=3, mar=c(15,4,4,2))
boxplot(count ~ spray, data = subset, names = c('a really, really,
really, really, really long name','a really, really, really, really,
really long name'))

I should state at this point that on Windows, R does as would be
expected ie it draws the labels, but they are truncated if they are too
long.  However, on SuSe Linux, running R 1.9.0, it just doesn't draw the
labels at all if they are too long (it does draw them if they are small
enough to fit on the page)

Thanks
Mick


-----Original Message-----
From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de] 
Sent: 26 August 2004 09:48
To: michael watson (IAH-C)
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Problems with par() and labels with boxplot


michael watson (IAH-C) wrote:

> Quite a simple one really!
> 
> When I run boxplot(), the labels on the X axis are horizontal, and I 
> want them vertical.  So I did:
> 
> par(las=3)
> boxplot(...)
> 
> And my labels just aren't there anymore....
> 
> Any help???

I guess they get clipped and you have to enlarge the margins, but you 
haven't given sufficient details such as
version of R, OS, device you are using, a reproducible example, ...

Please read the posting guide (see citation below).

Uwe Ligges


> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From michael.watson at bbsrc.ac.uk  Thu Aug 26 11:10:01 2004
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Thu, 26 Aug 2004 10:10:01 +0100
Subject: [R] Problems with par() and labels with boxplot
Message-ID: <8975119BCD0AC5419D61A9CF1A923E957C2952@iahce2knas1.iah.bbsrc.reserved>

Yes, I thought of that, but when one is creating lots of images
automatically, one doesn't always know how long one's labels are in
advance.

So I guess I need to check how long my labels are and truncate them if
they are too large, or adjust the margins if they are just slightly
bigger than normal.

The different behaviour between Windows and Linux is a bit odd though
(see my previous post)

Mick

-----Original Message-----
From: Peter Dalgaard [mailto:p.dalgaard at biostat.ku.dk] 
Sent: 26 August 2004 09:51
To: michael watson (IAH-C)
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Problems with par() and labels with boxplot


"michael watson (IAH-C)" <michael.watson at bbsrc.ac.uk> writes:

> Quite a simple one really!
> 
> When I run boxplot(), the labels on the X axis are horizontal, and I 
> want them vertical.  So I did:
> 
> par(las=3)
> boxplot(...)
> 
> And my labels just aren't there anymore....
> 
> Any help???

You may need to make space for them. par(mar=....)

E.g.:

par(las=2,mar=c(10.1,4.1,4.1,2.1))
boxplot(list(foobarbazbletch=rnorm(10),snafu=rnorm(20)))


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ligges at statistik.uni-dortmund.de  Thu Aug 26 11:14:51 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 26 Aug 2004 11:14:51 +0200
Subject: [R] Problems with par() and labels with boxplot
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E951746D1@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E951746D1@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <412DAA0B.3020406@statistik.uni-dortmund.de>

michael watson (IAH-C) wrote:

> Hello
> 
> I thought it was such a simple problem it didn't warrant too much
> detail!  Sorry :-)
> 
> OS is Suse Linux 8.2, R version is 1.9.0, device is x11.
> Example reproducible code:
> 
> data(InsectSprays)
> subset <- InsectSprays[1:24,]
> 
> # this creates normal horizontal text
> boxplot(count ~ spray, data = subset)
> 
> # lets try vertical text
> par(las = 3)
> boxplot(count ~ spray, data = subset)
> 
> # lets try it with long names
> par(las=3)
> boxplot(count ~ spray, data = subset, names = c('a really, really,
> really, really, really long name','a really, really, really, really,
> really long name'))
> 
> # lets try adjusting the marjins
> par(las=3, mar=c(15,4,4,2))
> boxplot(count ~ spray, data = subset, names = c('a really, really,
> really, really, really long name','a really, really, really, really,
> really long name'))
> 
> I should state at this point that on Windows, R does as would be
> expected ie it draws the labels, but they are truncated if they are too
> long.  However, on SuSe Linux, running R 1.9.0, it just doesn't draw the
> labels at all if they are too long (it does draw them if they are small
> enough to fit on the page)
> 
> Thanks
> Mick
> 
> 
> -----Original Message-----
> From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de] 
> Sent: 26 August 2004 09:48
> To: michael watson (IAH-C)
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Problems with par() and labels with boxplot
> 
> 
> michael watson (IAH-C) wrote:
> 
> 
>>Quite a simple one really!
>>
>>When I run boxplot(), the labels on the X axis are horizontal, and I 
>>want them vertical.  So I did:
>>
>>par(las=3)
>>boxplot(...)
>>
>>And my labels just aren't there anymore....
>>
>>Any help???
> 
> 
> I guess they get clipped and you have to enlarge the margins, but you 
> haven't given sufficient details such as
> version of R, OS, device you are using, a reproducible example, ...
> 
> Please read the posting guide (see citation below).
> 
> Uwe Ligges
> 
> 
> 
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list 
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
> 
> 

So my guess was right and it works for you after enlarging the margins 
using par(mar = ...), isn't it?

Uwe Ligges



From michael.watson at bbsrc.ac.uk  Thu Aug 26 11:26:49 2004
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Thu, 26 Aug 2004 10:26:49 +0100
Subject: [R] Problems with par() and labels with boxplot
Message-ID: <8975119BCD0AC5419D61A9CF1A923E957C2954@iahce2knas1.iah.bbsrc.reserved>

Nope.

Even after par(las=3, mar=c(15,4,4,2)), it doesn't draw the labels, and
at those margins the plot os all skwished up.

I guess if I keep enlarging the margins then it maybe will draw my
labels, but what use is that?  What I would like it to do is simply try
and squeeze the labels into the available space.  It looks like what it
is doing (on linux only, not on windows) is deciding there isn't enough
space and therefore not drawing the labels.

-----Original Message-----
From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de] 
Sent: 26 August 2004 10:15
To: michael watson (IAH-C)
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Problems with par() and labels with boxplot


michael watson (IAH-C) wrote:

> Hello
> 
> I thought it was such a simple problem it didn't warrant too much 
> detail!  Sorry :-)
> 
> OS is Suse Linux 8.2, R version is 1.9.0, device is x11. Example 
> reproducible code:
> 
> data(InsectSprays)
> subset <- InsectSprays[1:24,]
> 
> # this creates normal horizontal text
> boxplot(count ~ spray, data = subset)
> 
> # lets try vertical text
> par(las = 3)
> boxplot(count ~ spray, data = subset)
> 
> # lets try it with long names
> par(las=3)
> boxplot(count ~ spray, data = subset, names = c('a really, really, 
> really, really, really long name','a really, really, really, really, 
> really long name'))
> 
> # lets try adjusting the marjins
> par(las=3, mar=c(15,4,4,2))
> boxplot(count ~ spray, data = subset, names = c('a really, really, 
> really, really, really long name','a really, really, really, really, 
> really long name'))
> 
> I should state at this point that on Windows, R does as would be 
> expected ie it draws the labels, but they are truncated if they are 
> too long.  However, on SuSe Linux, running R 1.9.0, it just doesn't 
> draw the labels at all if they are too long (it does draw them if they

> are small enough to fit on the page)
> 
> Thanks
> Mick
> 
> 
> -----Original Message-----
> From: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de]
> Sent: 26 August 2004 09:48
> To: michael watson (IAH-C)
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Problems with par() and labels with boxplot
> 
> 
> michael watson (IAH-C) wrote:
> 
> 
>>Quite a simple one really!
>>
>>When I run boxplot(), the labels on the X axis are horizontal, and I
>>want them vertical.  So I did:
>>
>>par(las=3)
>>boxplot(...)
>>
>>And my labels just aren't there anymore....
>>
>>Any help???
> 
> 
> I guess they get clipped and you have to enlarge the margins, but you
> haven't given sufficient details such as
> version of R, OS, device you are using, a reproducible example, ...
> 
> Please read the posting guide (see citation below).
> 
> Uwe Ligges
> 
> 
> 
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
> 
> 

So my guess was right and it works for you after enlarging the margins 
using par(mar = ...), isn't it?

Uwe Ligges



From michael.watson at bbsrc.ac.uk  Thu Aug 26 11:31:26 2004
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Thu, 26 Aug 2004 10:31:26 +0100
Subject: [R] Column names of a data frame
Message-ID: <8975119BCD0AC5419D61A9CF1A923E957C2955@iahce2knas1.iah.bbsrc.reserved>

I'm coming out with all the classic problems today ;-)

Can anyone tell me how to change the column names of an already existing
data frame?  I've read the docs for ?data.frame and ?as.data.frame but
can't figure it out.

I want to make a new data.frame from some of the columns of an existing
data.frame, but when I do that, the new data.frame columns have the same
names as the old data.frame's columns, and I want them to be
different....

Sorry for the lame question....



From ripley at stats.ox.ac.uk  Thu Aug 26 11:35:49 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 26 Aug 2004 10:35:49 +0100 (BST)
Subject: [R] Problems with par() and labels with boxplot
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E957C2952@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <Pine.LNX.4.44.0408261026300.12966-100000@gannet.stats>

On Thu, 26 Aug 2004, michael watson (IAH-C) wrote:

> The different behaviour between Windows and Linux is a bit odd though
> (see my previous post)

No, it's not.  It is the graphics device, not the OS which matters.  How a
graphics display system responds to a request to partially clip text is
not really under R's control.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Aug 26 11:52:04 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 26 Aug 2004 10:52:04 +0100 (BST)
Subject: [R] Column names of a data frame
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E957C2955@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <Pine.LNX.4.44.0408261050150.13044-100000@gannet.stats>

?names

data frames have names, rather column names (they are lists with extra 
structure).

On Thu, 26 Aug 2004, michael watson (IAH-C) wrote:

> I'm coming out with all the classic problems today ;-)
> 
> Can anyone tell me how to change the column names of an already existing
> data frame?  I've read the docs for ?data.frame and ?as.data.frame but
> can't figure it out.
> 
> I want to make a new data.frame from some of the columns of an existing
> data.frame, but when I do that, the new data.frame columns have the same
> names as the old data.frame's columns, and I want them to be
> different....
> 
> Sorry for the lame question....

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sdavis2 at mail.nih.gov  Thu Aug 26 11:57:22 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 26 Aug 2004 05:57:22 -0400
Subject: [R] Column names of a data frame
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E957C2955@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E957C2955@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <536F71A2-F746-11D8-9891-000A95D7BA10@mail.nih.gov>

Michael,

See ?colnames.

Sean

On Aug 26, 2004, at 5:31 AM, michael watson (IAH-C) wrote:

> I'm coming out with all the classic problems today ;-)
>
> Can anyone tell me how to change the column names of an already 
> existing
> data frame?  I've read the docs for ?data.frame and ?as.data.frame but
> can't figure it out.
>
> I want to make a new data.frame from some of the columns of an existing
> data.frame, but when I do that, the new data.frame columns have the 
> same
> names as the old data.frame's columns, and I want them to be
> different....
>
> Sorry for the lame question....
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From jarioksa at sun3.oulu.fi  Thu Aug 26 11:55:49 2004
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: 26 Aug 2004 12:55:49 +0300
Subject: [R] integrate function
In-Reply-To: <x2u0urlzhi.fsf@biostat.ku.dk>
References: <200408251701.41005.chrysopa@insecta.ufv.br>
	<x2u0urlzhi.fsf@biostat.ku.dk>
Message-ID: <1093514149.18675.26.camel@biol102145.oulu.fi>

On Wed, 2004-08-25 at 23:44, Peter Dalgaard wrote:
> "Ronaldo Reis Jr." <chrysopa at insecta.ufv.br> writes:
> 
> > Is possible to integrate this diferential equation:
> > 
> > dN/dt = Nr(1-(N/K))
> > 
> > in R using the integrate() function?
> 
> No.
>  
However,  you could use 

N = K/(1 + exp(log((K-N0)/K) -r*t)),

where N0 is the population size at t=0 (that you must fix or estimate).

Causton has a long discussion about integrating this funcition on in his
"Mathematics for Biologists" (or something like that). Apart from that
MuPad may be free for Linux, and you can buy many other alternatives for
symbolic mathematics (Maple is available for Linux, at least). It may be
that you still have to work to get the solution you need, even with
snappy tools like that.

cheers, jari oksanen
-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From ahenningsen at email.uni-kiel.de  Thu Aug 26 12:03:03 2004
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Thu, 26 Aug 2004 12:03:03 +0200
Subject: [R] Column names of a data frame
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E957C2955@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E957C2955@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <200408261203.03522.ahenningsen@email.uni-kiel.de>

On Thursday 26 August 2004 11:31, michael watson (IAH-C) wrote:
> I'm coming out with all the classic problems today ;-)
>
> Can anyone tell me how to change the column names of an already existing
> data frame?  I've read the docs for ?data.frame and ?as.data.frame but
> can't figure it out.

This is a sample data frame:
> myData <- data.frame( col1 = 1:3, col2 = 2:4, col3 = 3:5 )
> myData
  col1 col2 col3
1    1    2    3
2    2    3    4
3    3    4    5

You can change all names by:
> names( myData )<- c( "newcol1", "newcol2", "newcol3" )
> myData
  newcol1 newcol2 newcol3
1       1       2       3
2       2       3       4
3       3       4       5

Or a single name by:
> names( myData )[ 2 ] <- "newcol2"
> myData
  col1 newcol2 col3
1    1       2    3
2    2       3    4
3    3       4    5

Or if you know the name, but not the column number:
> names( myData )[ which( names( myData ) == "newcol2" ) ] <- "verynewcol2"
> myData
  col1 verynewcol2 col3
1    1           2    3
2    2           3    4
3    3           4    5

All the best,
Arne

> I want to make a new data.frame from some of the columns of an existing
> data.frame, but when I do that, the new data.frame columns have the same
> names as the old data.frame's columns, and I want them to be
> different....
>
> Sorry for the lame question....
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From mrufino at ipimar.ualg.pt  Thu Aug 26 12:25:47 2004
From: mrufino at ipimar.ualg.pt (Marta Rufino)
Date: Thu, 26 Aug 2004 11:25:47 +0100
Subject: [R] Column names of a data frame
References: <8975119BCD0AC5419D61A9CF1A923E957C2955@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <011c01c48b57$10945a30$0b1a0e0a@PORTATILMARTA>

Hello,

> Can anyone tell me how to change the column names of an already existing
> data frame?  I've read the docs for ?data.frame and ?as.data.frame but
> can't figure it out.

names(object.name)=c("col.name1", "col.name2", "col.name3", etc.)
 # note that the number of names as to the the same as the number of columns
in your data frame

# Another alternative would be:
labels(object.name)[[2]]

> I want to make a new data.frame from some of the columns of an existing
> data.frame, but when I do that, the new data.frame columns have the same
> names as the old data.frame's columns, and I want them to be
> different....

# For exaple, if you want to create a new data frame called new.dataframe,
with columns 1 and 2 from the dataframe1, and # colnm 3 and 10 from
dataframe2, you do like this:
new.dataframe=data.frame(data.frame1[,1:2], data.frame1[,c(3,10)])

# If you want to call a different name to the col, for example:
new.dataframe=data.frame("new name"=data.frame1[,1], "new name2" =
data.frame2[,3])

# or, once you created you rename the names, as:
names(object.name)=c("col.name1", "col.name2", "col.name3", etc.)

the [row numbers, col numbers] is for subsetting the dataframes (or whatever
objects).

hope this helps,

All the best
Marta Rufino



From p.dalgaard at biostat.ku.dk  Thu Aug 26 12:22:40 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 26 Aug 2004 12:22:40 +0200
Subject: [R] Beginners Question: Make nlm work
In-Reply-To: <x2ekluxptj.fsf@biostat.ku.dk>
References: <20040825160732.47cdd152@localhost>
	<200408261013.06446.ahenningsen@email.uni-kiel.de>
	<x2ekluxptj.fsf@biostat.ku.dk>
Message-ID: <x21xhuxkpb.fsf@biostat.ku.dk>

Peter Dalgaard <p.dalgaard at biostat.ku.dk> writes:

> Arne Henningsen <ahenningsen at email.uni-kiel.de> writes:
> 
> > Hi,
> > 
> > did you try "nls" (nonlinear least squares)? "?nlm" says: 
> > "For nonlinear regression, 'nls' may be better".
> > Another option would be "optim".
> 
> Actually, just using nlm  according to specs might help:
> 
> > > setwd("~/Biology/R_versuch")
> > > mydata<-read.table("YJG45-7_Growth.dat")
> > > x<-mydata$V1
> > > y<-mydata$V2
> > > VH <- function(p) y ~ p[1]/(1+((p[1]-0.008)/0.008)*exp(-(p[2]*x)))
> 
> Make that  
> 
> VH <- function(p) sum((y - p[1]/(1+((p[1]-0.008)/0.008)*exp(-(p[2]*x))))^2)
> 
> > > plot(x, y, xlab="Time (h)",ylab=expression(OD[600][~nm]),las=1)
> > > out <- nlm(VH, p = c(3, 4e-3), hessian = TRUE)
> 
> I'm quite surprised that the other thing gives a result at all!

Ah, sorry. That was fixed in another branch of this thread. So much
for skimming emails in the morning... 

It would be good to see that actual output inside "out". However, the
real problem seems to be that the nlm algorithm is quite sensitive to
getting the typsize argument approximately right. nlm(VH, p=c(3,4e-3),
typsize=c(1,1e-3), ....) appears to fare considerably better.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From wilks at dial.pipex.com  Thu Aug 26 12:42:22 2004
From: wilks at dial.pipex.com (John Wilkinson)
Date: Thu, 26 Aug 2004 11:42:22 +0100
Subject: [R] Re:how to compute a "mode" of a distribution
Message-ID: <005401c48b59$7d4153c0$ad54e195@EvoN200>

On 08/24/04 13:50, Paolo Tommasini wrote:

>Hi my name is Paolo Tommasini does anyone know how to compute a "mode"
>( most frequent element ) for a distribution ?

try

median(density(my.vector))


John

John Wilknson

[wilks at dial.pipex.com]



From adrian at maths.uwa.edu.au  Thu Aug 26 12:58:48 2004
From: adrian at maths.uwa.edu.au (Adrian Baddeley)
Date: Thu, 26 Aug 2004 18:58:48 +0800
Subject: [R] Image recognition in R
Message-ID: <16685.49768.784049.901702@maths.uwa.edu.au>


Angel Lopez writes:
> I have some images of bugs (insects) with many bugs in each image.
> I want to count the number of bugs and to have an estimate of the area 
> of each one.

See also the book
    A. Lawson & D. Denison
    Spatial cluster modelling
    Chapman & Hall
where chapters 4-6 describe different statistical strategies for recognising
objects in an image. The authors of these three chapters 
each have some code, but it's probably not wrapped up in an R package yet.
And also 
  A.J. Baddeley and M.N.M. van Lieshout.
  Stochastic geometry models in high-level vision.
  In K.V. Mardia (ed.) Statistics and Images, volume 1,
  Abingdon: Carfax, 1993. pp. 233--258.



From S.J.Eglen at damtp.cam.ac.uk  Thu Aug 26 13:07:30 2004
From: S.J.Eglen at damtp.cam.ac.uk (Stephen Eglen)
Date: Thu, 26 Aug 2004 12:07:30 +0100
Subject: [R] image recognition in R
In-Reply-To: <200408251033.i7PAXmpE647212@hedwig1.umh.ac.be>
References: <412C6450.6000603@hotmail.com>
	<200408251033.i7PAXmpE647212@hedwig1.umh.ac.be>
Message-ID: <16685.50290.679127.499748@notch.amtp.cam.ac.uk>



Philippe wrote:
 > You should use ImageJ to binarize the picture and then extract features
 > (measurements) for each blob (individual). Save results as a text file and
 > import it in R (read.table). There you have plenty of packages to analyze
 > and classify your data. Things to try range from classical lda, to learning
 > vector quantization or neural nets (bundle VR), support vector machine
 > (package e1071) bagging (package ipred) and random forest (package
 > randomForest), among others. 
 > 
 > Otherwise, I work on a similar subject: I study methods for (semi)-automatic
 > recognition of plancton based on the analysis of digitized pictures. A
 > reference: Grosjean, Ph., M. Picheral, C. Warembourg & G. Gorsky, 2004.
 > Enumeration, measurement and identification of net zooplankton samples using
 > the ZOOSCAN digital imaging system. ICES J. Mar. Sci., 61:518-525. If you
 > are interested, contact me on my private email box.

Angel wrote:
 > > I have some images of bugs (insects) with many bugs in each image.
 > > I want to count the number of bugs and to have an estimate of 
 > > the area of each one.
 > > I've tried searching for an R package to do so with no 
 > > success. Is this a task that I should pursue doing in R or 
 > > should I restrict myself to specific image analysis software 
 > > (e.g. ImageJ)?.
 > > 
 > > The reason I consider R would be a good choice is because 
 > > then It would probably be possible to use R statistical power 
 > > to do pattern recognition on each bug's image to try to 
 > > identify each bug (Family, genus or species).
 > > Is anybody working in this direction?

I wrote some code recently to do simple image analysis in R, not going
as far as Image J.  But you can see the process at:
  http://www.anc.ed.ac.uk/~stephen/data/mosaics/w81_scan/w81s1.html

e.g. In the following image:

http://www.anc.ed.ac.uk/~stephen/data/mosaics/w81_scan/w81s1_check2.png

each "blob" is a neural cell, and I've used simple image processing to
find each region in a binarized image, find its centre and area; the
cell is then approximated as a circle of equivalent radius (R has
drawn the red/green circles on top of the original image, the colour
has been hand selected by me, not the the program.).

This uses the pixmap package to read in PGM files and then some C code
to find the regions.  You could build on this by extracting other
features from each region, but if ImageJ already does most of the hard
work, you might well stick with that rather than take this approach.

My code is not yet clean, but if others wanted to see it, I can pass
it on.  (I also have some 2-d convolution code since we are on the
topic of image processing).  Contact me off-list.

best wishes,
Stephen



From B.Rowlingson at lancaster.ac.uk  Thu Aug 26 13:17:30 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Thu, 26 Aug 2004 12:17:30 +0100
Subject: [R] image recognition in R
In-Reply-To: <16685.50290.679127.499748@notch.amtp.cam.ac.uk>
References: <412C6450.6000603@hotmail.com>	<200408251033.i7PAXmpE647212@hedwig1.umh.ac.be>
	<16685.50290.679127.499748@notch.amtp.cam.ac.uk>
Message-ID: <412DC6CA.8050806@lancaster.ac.uk>


> Angel wrote:
>  > > I have some images of bugs (insects) with many bugs in each image.
>  > > I want to count the number of bugs and to have an estimate of 
>  > > the area of each one.
>  > > I've tried searching for an R package to do so with no 
>  > > success. Is this a task that I should pursue doing in R or 
>  > > should I restrict myself to specific image analysis software 
>  > > (e.g. ImageJ)?.
>  > > 
>  > > The reason I consider R would be a good choice is because 
>  > > then It would probably be possible to use R statistical power 
>  > > to do pattern recognition on each bug's image to try to 
>  > > identify each bug (Family, genus or species).
>  > > Is anybody working in this direction?

I see people have mentioned ImageJ, but I recently discovered the 
UTHSCSA Image Tool (http://ddsdx.uthscsa.edu/dig/itdesc.html) which does 
lots of image processing things. Its free, the source code is available 
(it says, the ftp link is bad though), and one of the examples is in 
classifying black 'blobs' in an image.

  It has the power to export various quantities for each identified 
object (major/minor axis, size, perimeter, that sort of thing) so you 
could import that into R for more discrimination. Not seamless, but 
workable.

  Beware though, it did crash _lots_ when were doing object identification.

Barry



From cullens at tcd.ie  Thu Aug 26 13:25:50 2004
From: cullens at tcd.ie (Simon Cullen)
Date: Thu, 26 Aug 2004 12:25:50 +0100
Subject: [R] Label using equivalent of \mathbb{R}
Message-ID: <opsdb7dcgp1pelvz@smtp.tcd.ie>

Hi,

I'm trying to label the horizontal axis of a plot with a symbol that is  
the equivalent of \mathbb{R} in LaTeX. I've had a look through the help  
pages for plotmath and for Hershey and haven't found the symbol. Could  
someone give me a pointer, please?

Using R 1.9.1 on Win32.

-- 
SC



From P.Lemmens at nici.kun.nl  Thu Aug 26 13:30:10 2004
From: P.Lemmens at nici.kun.nl (Paul Lemmens)
Date: Thu, 26 Aug 2004 13:30:10 +0200
Subject: [R] library(car) Anova() and Error-term in aov()
Message-ID: <367E13CEB2DEA06EF9C39623@lemmens.socsci.kun.nl>

Dear all,

Type III SS time again. This case trying to reproduce some SPSS (type III) 
data in R for a repeated measures anova with a betwSS factor included. As I 
understand this list etc, if I want type III then I can do

library(car)
Anova(lm.obj, type="III")

But for the repeated measures anova, I need to include an Error-term in the 
aov() call (Psychology-guide from Jonathan Baron) which results in multiple 
lm() calls. Anova() does not seem capable to handle this situation. Or am I 
tackling Type III calculation, in this case with Error(), the wrong way 
(besides ignoring advice concerning Type I vs III)??

For instance,

dat <- rnorm(12)
pp <- factor(c(rep(1:3,2), rep(4:6,2)))
betw <- gl(2,6)
A <- factor(rep(c(rep('a',3),rep('b',3)), 2))
taov <- aov(dat~betw*A+Error(pp/A))
Anova(taov, type="III") # Goes wrong with following error.
#Error in Anova(taov, type = "III") : no applicable method for "Anova"

Phrased differently, ?Anova says "Calculates type-II or type-III 
analysis-of-variance tables for model objects produced by 'lm' and 'glm'", 
so it's not suitable for the aovlist that aov() with Error()-term returns. 
How can I compute Type III SS for such objects?

kind regards,
Paul




-- 
Paul Lemmens
NICI, University of Nijmegen              ASCII Ribbon Campaign /"\
Montessorilaan 3 (B.01.05)                    Against HTML Mail \ /
NL-6525 HR Nijmegen                                              X
The Netherlands                                                 / \
Phonenumber    +31-24-3612648
Fax            +31-24-3616066



From phgrosjean at sciviews.org  Thu Aug 26 13:45:00 2004
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Thu, 26 Aug 2004 13:45:00 +0200
Subject: [R] image recognition in R
In-Reply-To: <412DC6CA.8050806@lancaster.ac.uk>
Message-ID: <200408261146.i7QBjr7b762034@hedwig1.umh.ac.be>

Hello,

Yes, UTHSCSA Image Tools is a good package too, but it is less powerful than
ImageJ and it is not updated since quite a long time. There is an active
development of ImageJ, plenty of plugins, and an easier interface to make
its own plugins and scripts than in image tools. So, unless someone decide
to continue the development of Image Tools, ImageJ is probably a better
choice.

Also, taxonomic classification of bugs (as in the initial request) or
plancton, is a rather difficult tasks that deserves all the power and
flexibility you can get in R. So, I think Angel Lopez is in a better way in
importing data to R and working there. Automatic classification in Image
Tools and ImageJ could be useful in some situations, but it is not powerful
in this case.

Best,

Philippe Grosjean

..............................................<??}))><........
 ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
 ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
 ) ) ) ) )   Mons-Hainaut University, Pentagone
( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
 ) ) ) ) )   8, av du Champ de Mars, 7000 Mons, Belgium  
( ( ( ( (       
 ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.33.12
( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
 ) ) ) ) )      
( ( ( ( (    SciViews coordinator (http://www.sciviews.org)
 ) ) ) ) )
..............................................................
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Barry 
> Rowlingson
> Sent: Thursday, August 26, 2004 1:18 PM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] image recognition in R
> 
> 
> > Angel wrote:
> >  > > I have some images of bugs (insects) with many bugs in 
> each image.
> >  > > I want to count the number of bugs and to have an 
> estimate of  > 
> > > the area of each one.
> >  > > I've tried searching for an R package to do so with no  > > 
> > success. Is this a task that I should pursue doing in R or  
> > > should 
> > I restrict myself to specific image analysis software  > > (e.g. 
> > ImageJ)?.
> >  > >
> >  > > The reason I consider R would be a good choice is because  > > 
> > then It would probably be possible to use R statistical 
> power  > > to 
> > do pattern recognition on each bug's image to try to  > > identify 
> > each bug (Family, genus or species).
> >  > > Is anybody working in this direction?
> 
> I see people have mentioned ImageJ, but I recently discovered 
> the UTHSCSA Image Tool 
> (http://ddsdx.uthscsa.edu/dig/itdesc.html) which does lots of 
> image processing things. Its free, the source code is 
> available (it says, the ftp link is bad though), and one of 
> the examples is in classifying black 'blobs' in an image.
> 
>   It has the power to export various quantities for each 
> identified object (major/minor axis, size, perimeter, that 
> sort of thing) so you could import that into R for more 
> discrimination. Not seamless, but workable.
> 
>   Beware though, it did crash _lots_ when were doing object 
> identification.
> 
> Barry
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From p.dalgaard at biostat.ku.dk  Thu Aug 26 14:08:18 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 26 Aug 2004 14:08:18 +0200
Subject: [R] library(car) Anova() and Error-term in aov()
In-Reply-To: <367E13CEB2DEA06EF9C39623@lemmens.socsci.kun.nl>
References: <367E13CEB2DEA06EF9C39623@lemmens.socsci.kun.nl>
Message-ID: <x2wtzmw18t.fsf@biostat.ku.dk>

Paul Lemmens <P.Lemmens at nici.kun.nl> writes:

> Dear all,
> 
> Type III SS time again. This case trying to reproduce some SPSS (type
> III) data in R for a repeated measures anova with a betwSS factor
> included. As I understand this list etc, if I want type III then I can
> do
> 
> library(car)
> Anova(lm.obj, type="III")
> 
> But for the repeated measures anova, I need to include an Error-term
> in the aov() call (Psychology-guide from Jonathan Baron) which results
> in multiple lm() calls. Anova() does not seem capable to handle this
> situation. Or am I tackling Type III calculation, in this case with
> Error(), the wrong way (besides ignoring advice concerning Type I vs
> III)??
> 
> For instance,
> 
> dat <- rnorm(12)
> pp <- factor(c(rep(1:3,2), rep(4:6,2)))
> betw <- gl(2,6)
> A <- factor(rep(c(rep('a',3),rep('b',3)), 2))
> taov <- aov(dat~betw*A+Error(pp/A))
> Anova(taov, type="III") # Goes wrong with following error.
> #Error in Anova(taov, type = "III") : no applicable method for "Anova"
> 
> Phrased differently, ?Anova says "Calculates type-II or type-III
> analysis-of-variance tables for model objects produced by 'lm' and
> 'glm'", so it's not suitable for the aovlist that aov() with
> Error()-term returns. How can I compute Type III SS for such objects?

Well, ...

In a balanced design you don't need Type III SS (because they are all
the same) --  summary(taov) will do.

In an unbalanced design, you don't want to use aov() with an Error
term. (Slightly overstated, but you certainly get to think very
closely if the unbalance is in the Error model).

I'm not actually sure what SPSS does in the case of unbalanced designs
(complete-case analysis?).

In principle, with a balanced Error model, you should be able to
extract, say, taov[[2]] and do an Anova() or drop1() on that, but it
doesn't work because the object is not really an "lm" object, even
though

> class(taov[[2]])
[1] "aov" "lm"

but we get things like

> model.frame(as(taov[[2]],"lm"))
$method
lm


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From kjetil at acelerate.com  Thu Aug 26 14:49:46 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Thu, 26 Aug 2004 08:49:46 -0400
Subject: [R] Re:how to compute a "mode" of a distribution
In-Reply-To: <005401c48b59$7d4153c0$ad54e195@EvoN200>
References: <005401c48b59$7d4153c0$ad54e195@EvoN200>
Message-ID: <412DDC6A.1060607@acelerate.com>

John Wilkinson wrote:

>On 08/24/04 13:50, Paolo Tommasini wrote:
>
>  
>
>>Hi my name is Paolo Tommasini does anyone know how to compute a "mode"
>>( most frequent element ) for a distribution ?
>>    
>>
>
>try
>
>median(density(my.vector))
>
>  
>
That cannot be right. Try:

 > test <- rnorm(1000)
 > d <- density(test)
 > median(d)
Error in median(d) : need numeric data
 > str(d)
List of 7
 $ x        : num [1:512] -3.64 -3.63 -3.62 -3.60 -3.59 ...
 $ y        : num [1:512] 2.16e-05 2.60e-05 3.11e-05 3.71e-05 4.43e-05 ...
 $ bw       : num 0.221
 $ n        : int 1000
 $ call     : language density(x = test)
 $ data.name: chr "test"
 $ has.na   : logi FALSE
 - attr(*, "class")= chr "density"
 > i <- which.max(d$y)
 > d$x[i]
[1] 0.05625893


Kjetil halvorsen



>John
>
>John Wilknson
>
>[wilks at dial.pipex.com]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>



From galvan at cenpat.edu.ar  Thu Aug 26 15:08:49 2004
From: galvan at cenpat.edu.ar (David Galvan)
Date: Thu, 26 Aug 2004 10:08:49 -0300
Subject: [R] PLS and categorical variables
Message-ID: <412DB6B1.23579.B893B@localhost>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040826/f10e8f1d/attachment.pl

From dtrenkler at nts6.oec.uni-osnabrueck.de  Thu Aug 26 15:24:33 2004
From: dtrenkler at nts6.oec.uni-osnabrueck.de (Trenkler, Dietrich)
Date: Thu, 26 Aug 2004 15:24:33 +0200
Subject: [R] Label using equivalent of \mathbb{R}
Message-ID: <FB75CFC167F3D311B11D00A0CC20FB0E88556C@nts7.oec.Uni-Osnabrueck.DE>



> -----Original Message-----
> From:	Simon Cullen 
> Sent:	Thursday, August 26, 2004 1:26 PM
> To:	r-help at stat.math.ethz.ch
> Subject:	[R] Label using equivalent of \mathbb{R}
> 
> Hi,
> 
> I'm trying to label the horizontal axis of a plot with a symbol that is  
> the equivalent of \mathbb{R} in LaTeX. I've had a look through the help  
> pages for plotmath and for Hershey and haven't found the symbol. Could  
> someone give me a pointer, please?
> 
	[Dietrich Trenkler]  

	> plot(rnorm(10),xlab=expression(bold(x)),ylab=expression(bold(y)))



From JonesW at kssg.com  Thu Aug 26 15:17:15 2004
From: JonesW at kssg.com (Wayne Jones)
Date: Thu, 26 Aug 2004 14:17:15 +0100
Subject: [R] integrate function
Message-ID: <6B5A9304046AD411BD0200508BDFB6CB02BD1838@gimli.middleearth.kssg.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040826/38d07c1c/attachment.pl

From armin at xss.de  Thu Aug 26 15:25:23 2004
From: armin at xss.de (Armin Roehrl)
Date: Thu, 26 Aug 2004 15:25:23 +0200
Subject: [R] introduction slides for beginners
Message-ID: <412DE4C3.3000906@xss.de>

Hi all,

    does anybody have slides for a 1 to 3 hour crash course into R,
that I would be allowed to recycle to show a few people the light?

On the R-website http://cran.r-project.org/other-docs.html
I found several additional books, but no slides.

It might be nice to add  a page with slides, too.

Thank you,
    -Armin



From jfox at mcmaster.ca  Thu Aug 26 15:40:21 2004
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 26 Aug 2004 09:40:21 -0400
Subject: [R] library(car) Anova() and Error-term in aov()
In-Reply-To: <x2wtzmw18t.fsf@biostat.ku.dk>
Message-ID: <20040826134021.ICWO29920.tomts10-srv.bellnexxia.net@JohnDesktop8300>

Dear Peter and Paul,

As Paul discovered, Anova() doesn't handle aovlist objects. 

As a general matter, one should be careful with "type-III" tests, since it's
easy to test hypotheses that aren't sensible (e.g., tests ostensibly of main
effects that aren't reasonably interpretable as tests of main effects). For
example, SAS (and I assume SPSS) produce type-III tests in analysis of
covariance that aren't generally sensible. I haven't thought about whether
there's a similar trap in unbalanced repeated-measures ANOVA. By the way,
sequential (or "type-I") tests are rarely sensible in my opinion.

Regards,
 John

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Peter Dalgaard
> Sent: Thursday, August 26, 2004 7:08 AM
> To: Paul Lemmens
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] library(car) Anova() and Error-term in aov()
> 
> Paul Lemmens <P.Lemmens at nici.kun.nl> writes:
> 
> > Dear all,
> > 
> > Type III SS time again. This case trying to reproduce some 
> SPSS (type
> > III) data in R for a repeated measures anova with a betwSS factor 
> > included. As I understand this list etc, if I want type III 
> then I can 
> > do
> > 
> > library(car)
> > Anova(lm.obj, type="III")
> > 
> > But for the repeated measures anova, I need to include an 
> Error-term 
> > in the aov() call (Psychology-guide from Jonathan Baron) 
> which results 
> > in multiple lm() calls. Anova() does not seem capable to 
> handle this 
> > situation. Or am I tackling Type III calculation, in this case with 
> > Error(), the wrong way (besides ignoring advice concerning 
> Type I vs 
> > III)??
> > 
> > For instance,
> > 
> > dat <- rnorm(12)
> > pp <- factor(c(rep(1:3,2), rep(4:6,2))) betw <- gl(2,6) A <- 
> > factor(rep(c(rep('a',3),rep('b',3)), 2)) taov <- 
> > aov(dat~betw*A+Error(pp/A)) Anova(taov, type="III") # Goes 
> wrong with 
> > following error.
> > #Error in Anova(taov, type = "III") : no applicable method 
> for "Anova"
> > 
> > Phrased differently, ?Anova says "Calculates type-II or type-III 
> > analysis-of-variance tables for model objects produced by 'lm' and 
> > 'glm'", so it's not suitable for the aovlist that aov() with 
> > Error()-term returns. How can I compute Type III SS for 
> such objects?
> 
> Well, ...
> 
> In a balanced design you don't need Type III SS (because they 
> are all the same) --  summary(taov) will do.
> 
> In an unbalanced design, you don't want to use aov() with an 
> Error term. (Slightly overstated, but you certainly get to 
> think very closely if the unbalance is in the Error model).
> 
> I'm not actually sure what SPSS does in the case of 
> unbalanced designs (complete-case analysis?).
> 
> In principle, with a balanced Error model, you should be able 
> to extract, say, taov[[2]] and do an Anova() or drop1() on 
> that, but it doesn't work because the object is not really an 
> "lm" object, even though
> 
> > class(taov[[2]])
> [1] "aov" "lm"
> 
> but we get things like
> 
> > model.frame(as(taov[[2]],"lm"))
> $method
> lm
> 
> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: 
> (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: 
> (+45) 35327907
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From jfox at mcmaster.ca  Thu Aug 26 15:44:27 2004
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 26 Aug 2004 09:44:27 -0400
Subject: [R] introduction slides for beginners
In-Reply-To: <412DE4C3.3000906@xss.de>
Message-ID: <20040826134427.VQQB15743.tomts22-srv.bellnexxia.net@JohnDesktop8300>

Dear Armin,

I have a variety of materials at
<http://socserv.socsci.mcmaster.ca/jfox/Courses/S-course/index.html>, though
the slides are minimal; most of the material is in the script files. The
introductory lecture (of seven lectures) takes me two hours.

I hope that this helps,
 John 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Armin Roehrl
> Sent: Thursday, August 26, 2004 8:25 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] introduction slides for beginners
> 
> Hi all,
> 
>     does anybody have slides for a 1 to 3 hour crash course 
> into R, that I would be allowed to recycle to show a few 
> people the light?
> 
> On the R-website http://cran.r-project.org/other-docs.html
> I found several additional books, but no slides.
> 
> It might be nice to add  a page with slides, too.
>



From deepayan at stat.wisc.edu  Thu Aug 26 15:44:56 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 26 Aug 2004 08:44:56 -0500 (CDT)
Subject: [R] Problems with par() and labels with boxplot
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E957C2952@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E957C2952@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <Pine.LNX.4.58.0408260842580.28412@public02.stat.wisc.edu>

On Thu, 26 Aug 2004, michael watson (IAH-C) wrote:

> Yes, I thought of that, but when one is creating lots of images
> automatically, one doesn't always know how long one's labels are in
> advance.

bwplot in the lattice package will try to do that for you, which IMO is
one of the big perks of Trellis graphics.

Deepayan



From tk at tariqkhan.org  Thu Aug 26 15:58:56 2004
From: tk at tariqkhan.org (tk@tariqkhan.org)
Date: Thu, 26 Aug 2004 09:58:56 -0400
Subject: [R] EM norm package (NA/NaN/Inf in foreign function call (arg 2))
Message-ID: <191410-22004842613585694@M2W068.mail2web.com>

Greetings!

I am bootstrapping and I am using EM in the norm package to fill in missing
data for a financial time series with each step of the loop. For the most
part EM works fine for me, but the following error message is guaranteed
before I hit the 200th scenario:

Iterations of EM:
1...2...3........348...349...Error: NA/NaN/Inf in foreign function call
(arg 2)

The following code should replicate the error by downloading the dataset
from the internet (it is not too big):

library(norm)
df<-download.file("http://www.tariqkhan.org/R/DataFromExcel.csv",
"C:/Program Files/R/d.csv")
mat<-as.matrix(read.table("C:/Program Files/R/d.csv", sep = ","))
s<-prelim.norm(mat)
rngseed(1234567)
thetahat<-em.norm(s, maxits = 1000, criterion = 0.0035)

Iterations of EM:
1...2...3........348...349...Error: NA/NaN/Inf in foreign function call
(arg 2)

Someone else on the list found that using scale() helped with em.norm, but
for me it only increased the number of iterations before giving the same
error.

I dont get it. Insights into what I can do to solve this would be much
appreciated!

Details:
Norm Package version 1.0.9; 
R version 1.9.0;
Windows XP Pro 2002 SP1; 
384MB RAM, Pentium 4 CPU 2.40 GHz

--------------------------------------------------------------------
mail2web - Check your email from the web at
http://mail2web.com/ .



From f.harrell at vanderbilt.edu  Thu Aug 26 16:08:39 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 26 Aug 2004 09:08:39 -0500
Subject: [R] Help using Hmisc / Latex
Message-ID: <412DEEE7.3070809@vanderbilt.edu>

Sorry about non-indenting of quoted text - I haven't figured out a good 
way to answer mail when receiving messages in digest mode -FH
--------------------------------------------------------------------

I'm afraid you need to modify your approach. You're trying to pass latex an
lm object, which latex doesn't know how to handle. Also, latex isn't
supposed to produce a full .tex file; it generates just a "core" that's
loaded into a "shell" when you run dvi.

Here's an example of how you might use it appropriately:

l <- latex(summary(lm(b ~ a, data.frame(a = 1:5, b = 1:5)))$coefficients)
d <- dvi(l)
d

If you want the full .tex file, you can read the console output after
running dvi(l) to find out where the .tex tempfile lives and copy it over to
someplace usable. Obviously, you'll have to tweak the row and column names
as well as the rounding to suit your preferences.

If you're just looking for graphical output--not necessarily a .tex file--I
have a function grid.table, now being rolled into a package I will soon
release, which uses grid to place a data frame (neatly) on the graphics
device. You could then wrap the command around a pdf(), postscript() or any
other device to produce an appropriate display. Let me know if you'd like me
to send over my working version of the binaries.

Kevin

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Joao Pedro W. de
Azevedo
Sent: Wednesday, August 25, 2004 4:02 AM
To: R-help at stat.math.ethz.ch
Subject: [R] Help using Hmisc / Latex

Dear R users,
I'm trying to automatically generate a *.tex file with the output of an OLS
estimation. Some people suggested to use the latex function on the Hmisc
package. I'm having a bit of trouble to properly specify this function (I'm
not a very experienced R user). Below you will find an example, of what I'm
doing.

## Annette Dobson (1990) "An Introduction to Generalized Linear Models".
## Page 9: Plant Weight Data.
summary(lm.D90 <- lm(weight ~ group - 1))# omitting intercept
out  <- latex(lm.D90)
latex(out, model1, file="")

When I run this code, I get an output which does not attend my needs.

First, I could not figure out how to print the variable names. Second I'm
not sure how I can select only the coefficients and the std. errors to be
inserted on the output. Third, I was wondering if there is any way I can
automatically generate significance level indicators next to either the
coefficients or the std erros. Forth, is there any way I can aumtomaticaly
insert some of model fitting statistics? Fifth, is it possible to specify
this function to save the std. error under the coefficient?

I'm sure this are quite basic questions, but my attempts to fiddle with the
parameters in the model took me nowhere.

Many thanks once again,

Joao Pedro

-------------------------------------------------------------------------

Good comments Kevin - thanks.  Joao to get full latex support you can 
use library(Design)'s ols function in place of lm.  This is for getting 
the fitted model in algebraic form, with factoring out of interaction 
terms, simplification of natural splines, etc.

In linux I use a little shell script to view LaTeX code fragments:
~/bin harrelfe:cat latexfxdvi
echo -e 
"\\\documentclass{article}\n\\\usepackage{setspace,relsize,ctable,longtable}\n\\\begin{document}\n" 
| cat - $1.tex > /tmp/$$.tex
echo "\\end{document}" > /tmp/$$.tmp
pushd /tmp
cat $$.tex $$.tmp > $$2.tex
latex "\scrollmode\input" $$2
xdvi $$2 &
popd

This assumes you have setspace, relsize, ctable installed. These are the 
most frequently used non-standard latex packages called for by Hmisc.
-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From cullens at tcd.ie  Thu Aug 26 16:30:06 2004
From: cullens at tcd.ie (Simon Cullen)
Date: Thu, 26 Aug 2004 15:30:06 +0100
Subject: [R] Label using equivalent of \mathbb{R}
In-Reply-To: <FB75CFC167F3D311B11D00A0CC20FB0E88556C@nts7.oec.Uni-Osnabrueck.DE>
References: <FB75CFC167F3D311B11D00A0CC20FB0E88556C@nts7.oec.Uni-Osnabrueck.DE>
Message-ID: <opsdcfwgp21pelvz@smtp.tcd.ie>

On Thu, 26 Aug 2004 15:24:33 +0200, Trenkler, Dietrich  
<dtrenkler at nts6.oec.uni-osnabrueck.de> wrote:

> 	[Dietrich Trenkler]
>
> 	> plot(rnorm(10),xlab=expression(bold(x)),ylab=expression(bold(y)))

Not quite what I am looking for, I'm afraid. \mathbb gives "blackboard"  
fonts - the capitals with two vertical parallel lines in them that are  
used for the Reals, Complex numbers etc.

-- 
SC



From deepayan at stat.wisc.edu  Thu Aug 26 16:26:35 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 26 Aug 2004 09:26:35 -0500 (CDT)
Subject: [R] introduction slides for beginners
In-Reply-To: <412DE4C3.3000906@xss.de>
References: <412DE4C3.3000906@xss.de>
Message-ID: <Pine.LNX.4.58.0408260859370.28463@public02.stat.wisc.edu>

On Thu, 26 Aug 2004, Armin Roehrl wrote:

> Hi all,
>
>     does anybody have slides for a 1 to 3 hour crash course into R,
> that I would be allowed to recycle to show a few people the light?

I have some stuff I used for 5 x 1 hour presentations at

http://www.stat.wisc.edu/~deepayan/SIBS/slides/

meant to complement the first chapter of 'An Introduction to R'. It was
aimed at undergrad students without any Statistics background (they were
supposed to get some in the rest of the course), so it might not be
appropriate for you.

Deepayan



From ggrothendieck at myway.com  Thu Aug 26 16:38:12 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 26 Aug 2004 14:38:12 +0000 (UTC)
Subject: [R] Help using Hmisc / Latex
References: <412DEEE7.3070809@vanderbilt.edu>
Message-ID: <loom.20040826T163633-731@post.gmane.org>

Frank E Harrell Jr <f.harrell <at> vanderbilt.edu> writes:

> Sorry about non-indenting of quoted text - I haven't figured out a good 
> way to answer mail when receiving messages in digest mode -FH

What I do is post through

    http://news.gmane.org/gmane.comp.lang.r.general



From diego.rubolini at unipv.it  Thu Aug 26 16:50:34 2004
From: diego.rubolini at unipv.it (Diego Rubolini)
Date: Thu, 26 Aug 2004 16:50:34 +0200
Subject: [R] glmm in R
In-Reply-To: <200408261146.i7QBjr7b762034@hedwig1.umh.ac.be>
References: <412DC6CA.8050806@lancaster.ac.uk>
	<200408261146.i7QBjr7b762034@hedwig1.umh.ac.be>
Message-ID: <6.1.1.1.0.20040826135525.025878c0@pop.unipv.it>

Dear all,

I'm new to R and to the list, and I have a problem which I'm unable to solve.

Consider the following simple simulated data frame:

simul<-data.frame(Group=factor(c(rep(1,4),rep(2,2),rep(3,6),rep(4,8),rep(5,4))),Pair=factor(rep(1:12,rep(2,12))),Y=rep(c(0,1),12),X1=c(9,10,8,9,1,5,2,7,0,3,7,9,5,4,6,8,4,8,10,4,1,6,2,3),X2=c(1,3,2,5,2,4,0,1,3,7,8,10,3,4,5,5,4,8,10,12,0,3,3,4),Day=(c(1,1,2,2,2,2,1,1,2,2,3,3,2,2,4,4,5,5,7,7,1,1,2,2)))

I'd like to fit a logistic mixed model with two random effects, one is a 
subject level random effect (Group) and another is a pair-level random 
effect (Pair), which is nested within Group. Furthermore, I have a Day 
variable (a time variable), which has the same value for each Y pair. As 
far as I could understand, the syntax for the nested random effect should 
be as follows (but I'm not sure about this):

random = ~ 1|Pair/Group

I used GLMM (from lme4) and glmPQL (from MASS) functions using the 
following model specifications:

fit <- GLMM(Y ~ X1+X2+Day+X1*Day+X2*Day, family = binomial, data = 
simul,  random = ~ 1|Pair/Group)
summary(fit)

fit1 <- glmmPQL(Y ~ X1+X2+Day+X1*Day+X2*Day, family = binomial, data = 
simul,  random = ~ 1|Pair/Group)
summary(fit1)

My questions are (in order of importance):

1) is the formulation correct for the nested random effect?

2) why do the results of the two packages give different std. errors and 
P-values for parameter estimates, which are indeed very similar? glmmPQL 
tend to give greater errors and higher P's than GLMM. Further, it takes 
longer to produce the output. They both implement PQL estimation, I 
believe. Which function is best? I believe it should be that with smaller se's.

3) I wish also to model Day as a random intercept in Group (but not in Pair 
nested in Group), but I do not know if this is possible. Does anybody have 
any suggestion?

4) X2 and Day are correlated by nature. Is this giving any problem for 
parameter estimates?

I have already bothered other statisticians about this, but haven't still 
resolved my case (you know who you are :-)). I've also been told that PQL 
estimates are flawed, and that I should better turn to other software (e.g. 
SAS PROC NLMIXED).

Thank you all for your attention,

Diego


_________________________________________________

  Diego Rubolini
  Dipartimento di Biologia Animale
  Universit?? degli Studi di Pavia
  Piazza Botta 9
  I - 27100 Pavia
  Italy
  Email: diego.rubolini at unipv.it

  URL: http://www.unipv.it/webbio/labweb/ecoeto/



From cullens at tcd.ie  Thu Aug 26 17:03:37 2004
From: cullens at tcd.ie (Simon Cullen)
Date: Thu, 26 Aug 2004 16:03:37 +0100
Subject: [R] Label using equivalent of \mathbb{R}
In-Reply-To: <FB75CFC167F3D311B11D00A0CC20FB0E885571@nts7.oec.Uni-Osnabrueck.DE>
References: <FB75CFC167F3D311B11D00A0CC20FB0E885571@nts7.oec.Uni-Osnabrueck.DE>
Message-ID: <opsdchgbkl1pelvz@smtp.tcd.ie>

On Thu, 26 Aug 2004 16:40:09 +0200, Trenkler, Dietrich  
<dtrenkler at nts6.oec.uni-osnabrueck.de> wrote:

> 	[Dietrich Trenkler]  Oh yes, I see. I was too overhasty.
> 	Maybe using  psfrag
> 	http://www.ctan.org/tex-archive/macros/latex/contrib/psfrag/
> 	 is a way out.

Thanks! That is exactly what I wanted.

-- 
SC



From ernesto at ipimar.pt  Thu Aug 26 17:14:21 2004
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Thu, 26 Aug 2004 16:14:21 +0100
Subject: [R] How to change pch in plot.factor ?
Message-ID: <1093533261.7270.2.camel@linux.site>

Hi,

I want to change the symbol in plot.factor but using pch is not working.
It keeps plotting the horizontal lines ...

How can I do it ?

Thanks 

EJ



From wilks at dial.pipex.com  Thu Aug 26 17:13:04 2004
From: wilks at dial.pipex.com (John Wilkinson)
Date: Thu, 26 Aug 2004 16:13:04 +0100
Subject: [R] Re:how to compute a "mode" of a distribution
References: <005401c48b59$7d4153c0$ad54e195@EvoN200>
	<412DDC6A.1060607@acelerate.com>
Message-ID: <003b01c48b7f$52725ab0$a888e195@EvoN200>

you are quite right  ,I forgot to insert $x,

I should have written

median(density(my.vector)$x)

that gives a median result but it is still  not the proper result.

density(my.vector)$y)

gives the probabilites y of each x value
The correst answer would be the value of x corresponding to

y=max(density(my.vector)$y)

see
summary(density(my.vector)

of course a

plot(density(my.vector))

would reveal the mode easily by inspection.

John





----- Original Message ----- 
From: "Kjetil Brinchmann Halvorsen" <kjetil at acelerate.com>
To: "John Wilkinson" <wilks at dial.pipex.com>
Cc: "Paolo Tommasini" <paolo at directwave.com.br>; <r-help at stat.math.ethz.ch>
Sent: Thursday, August 26, 2004 1:49 PM
Subject: Re: [R] Re:how to compute a "mode" of a distribution


> John Wilkinson wrote:
>
> >On 08/24/04 13:50, Paolo Tommasini wrote:
> >
> >
> >
> >>Hi my name is Paolo Tommasini does anyone know how to compute a "mode"
> >>( most frequent element ) for a distribution ?
> >>
> >>
> >
> >try
> >
> >median(density(my.vector))
> >
> >
> >
> That cannot be right. Try:
>
>  > test <- rnorm(1000)
>  > d <- density(test)
>  > median(d)
> Error in median(d) : need numeric data
>  > str(d)
> List of 7
>  $ x        : num [1:512] -3.64 -3.63 -3.62 -3.60 -3.59 ...
>  $ y        : num [1:512] 2.16e-05 2.60e-05 3.11e-05 3.71e-05 4.43e-05 ...
>  $ bw       : num 0.221
>  $ n        : int 1000
>  $ call     : language density(x = test)
>  $ data.name: chr "test"
>  $ has.na   : logi FALSE
>  - attr(*, "class")= chr "density"
>  > i <- which.max(d$y)
>  > d$x[i]
> [1] 0.05625893
>
>
> Kjetil halvorsen
>
>
>
> >John
> >
> >John Wilknson
> >
> >[wilks at dial.pipex.com]
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
> >
> >
> >
> >
>
>
>



From bvy9 at cdc.gov  Thu Aug 26 17:13:25 2004
From: bvy9 at cdc.gov (Bossarte, Robert)
Date: Thu, 26 Aug 2004 11:13:25 -0400
Subject: [R] GLMM
Message-ID: <AE3AD514E378164DBAC0CDE14210E19B05BF198F@m-ncipc-1.ncipc.cdc.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040826/2f6586bb/attachment.pl

From Mike.Prager at noaa.gov  Thu Aug 26 17:19:54 2004
From: Mike.Prager at noaa.gov (Mike Prager)
Date: Thu, 26 Aug 2004 11:19:54 -0400
Subject: [R] Adding labels to variables
In-Reply-To: <x2y8k3lztw.fsf@biostat.ku.dk>
References: <B88F4BCF37DD0847937C1C98255291FB0175C736@uswsmx05.merck.com>
	<6.1.2.0.0.20040825154112.01b0c0c0@hermes.nos.noaa.gov>
	<x2y8k3lztw.fsf@biostat.ku.dk>
Message-ID: <6.1.2.0.0.20040826111855.01bfb4a0@hermes.nos.noaa.gov>

At 04:36 PM 08/25/2004, Peter Dalgaard wrote:

>There's a good chance that R-2.x will introduce usePackage() and
>eventually remove library() as a tool for loading packages.

That would be a welcome instance of consistency.


-- 
Michael Prager, Ph.D.                <Mike.Prager at noaa.gov>
NOAA Beaufort Laboratory
Beaufort, North Carolina  28516
http://shrimp.ccfhrb.noaa.gov/~mprager/
***



From filippobiscarini at anafi.it  Thu Aug 26 17:06:45 2004
From: filippobiscarini at anafi.it (Filippo Biscarini)
Date: Thu, 26 Aug 2004 17:06:45 +0200
Subject: [R] plot.new?
Message-ID: <NHBBLJIIHCNEKPHLLJMOKEPOCBAA.filippobiscarini@anafi.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040826/d5484ffb/attachment.pl

From bavorak at klobouk.fsv.cvut.cz  Thu Aug 26 17:31:01 2004
From: bavorak at klobouk.fsv.cvut.cz (bavorak@klobouk.fsv.cvut.cz)
Date: Thu, 26 Aug 2004 17:31:01 +0200 (CEST)
Subject: [R] "o" bty with different axes
In-Reply-To: <412DA50A.6030605@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.43.0408261724580.26360-100000@klobouk.fsv.cvut.cz>

>I don't see why using different scalings is related to bty settings???

Really, after shifting the box type from "u" to "o" I received an error
message that it can't plot with different scaling on the left and right
ordinate. How to fix it?

Tomas Bayer



From ligges at statistik.uni-dortmund.de  Thu Aug 26 17:38:00 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 26 Aug 2004 17:38:00 +0200
Subject: [R] "o" bty with different axes
In-Reply-To: <Pine.LNX.4.43.0408261724580.26360-100000@klobouk.fsv.cvut.cz>
References: <Pine.LNX.4.43.0408261724580.26360-100000@klobouk.fsv.cvut.cz>
Message-ID: <412E03D8.2000302@statistik.uni-dortmund.de>

bavorak at klobouk.fsv.cvut.cz wrote:

>>I don't see why using different scalings is related to bty settings???
> 
> 
> Really, after shifting the box type from "u" to "o" I received an error
> message that it can't plot with different scaling on the left and right
> ordinate. How to fix it?
> 
> Tomas Bayer
> 

Please specify an example!

Uwe Ligges



From jgoebel at diw.de  Thu Aug 26 17:44:34 2004
From: jgoebel at diw.de (Jan Goebel)
Date: Thu, 26 Aug 2004 17:44:34 +0200
Subject: [R] plot.new?
In-Reply-To: <NHBBLJIIHCNEKPHLLJMOKEPOCBAA.filippobiscarini@anafi.it>
References: <NHBBLJIIHCNEKPHLLJMOKEPOCBAA.filippobiscarini@anafi.it>
Message-ID: <20040826154434.GA11134@diw138134.diw-berlin.de>

>From help(abline):

Examples:

     data(cars)
     z <- lm(dist ~ speed, data = cars)
     plot(cars)
     abline(z)


You first have to create a plot to which you can add a line.

jan

On Thu, 26 Aug 2004, Filippo Biscarini wrote:

> Hi all,
> 
> I am just beginning to use R and encountered a problem in trying to draw a
> regression line using the command "abline(coef(var)" and similar ones but
> got the error "plot.new  has not been called yet". Does anybody know how can
> I call plot.new? Is this an additional package to be found somewhere on the
> web? Is it a functionality that I just have to "activate" somehow?
> 
> I am using R both on windows and on Linux and get the same error message
> when using those drawing functions.
> 
> Thank you,
> 
> Filippo Biscarini
>   ****************************
>   Filippo Biscarini,
>   Research and Development
>   ANAFI (Italian Holstein Association)
>   Via Bergamo, 292
>   26100, Cremona, Italy
> 
>   tel: +39.0372.474234
> 
>   *****************************
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
+-----------------------------------------
 Jan Goebel 
 j g o e b e l @ d i w . d e

 DIW Berlin 
 German Socio-Economic Panel Study (GSOEP) 
 K??nigin-Luise-Str. 5
 D-14195 Berlin -- Germany --
 phone: 49 30 89789-377
+-----------------------------------------



From ligges at statistik.uni-dortmund.de  Thu Aug 26 17:42:18 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 26 Aug 2004 17:42:18 +0200
Subject: [R] How to change pch in plot.factor ?
In-Reply-To: <1093533261.7270.2.camel@linux.site>
References: <1093533261.7270.2.camel@linux.site>
Message-ID: <412E04DA.1080906@statistik.uni-dortmund.de>

Ernesto Jardim wrote:

> Hi,
> 
> I want to change the symbol in plot.factor but using pch is not working.
> It keeps plotting the horizontal lines ...

Have you read ?plot.factor.
What kind of a symbol do you expect in a barplot or boxplot (instead of 
lines)? It would not work for boxplot and barplot themselves.


> How can I do it ?

Using another kind of plot?

Uwe Ligges


> Thanks 
> 
> EJ
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Thu Aug 26 17:44:50 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 26 Aug 2004 17:44:50 +0200
Subject: [R] plot.new?
In-Reply-To: <NHBBLJIIHCNEKPHLLJMOKEPOCBAA.filippobiscarini@anafi.it>
References: <NHBBLJIIHCNEKPHLLJMOKEPOCBAA.filippobiscarini@anafi.it>
Message-ID: <412E0572.4080305@statistik.uni-dortmund.de>

Filippo Biscarini wrote:

> Hi all,
> 
> I am just beginning to use R and encountered a problem in trying to draw a
> regression line using the command "abline(coef(var)" and similar ones but
> got the error "plot.new  has not been called yet". Does anybody know how can
> I call plot.new? Is this an additional package to be found somewhere on the
> web? Is it a functionality that I just have to "activate" somehow?

?abline: "This function adds one or more straight lines through the 
current plot.", i.e. the already existing *current plot*.

So plot your data (e.g. with plot(x, y)) before adding a regression line.

Uwe Ligges



> I am using R both on windows and on Linux and get the same error message
> when using those drawing functions.
> 
> Thank you,
> 
> Filippo Biscarini
>   ****************************
>   Filippo Biscarini,
>   Research and Development
>   ANAFI (Italian Holstein Association)
>   Via Bergamo, 292
>   26100, Cremona, Italy
> 
>   tel: +39.0372.474234
> 
>   *****************************
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From mrufino at ipimar.ualg.pt  Thu Aug 26 17:45:03 2004
From: mrufino at ipimar.ualg.pt (Marta Rufino)
Date: Thu, 26 Aug 2004 16:45:03 +0100
Subject: [R] plot.new?
References: <NHBBLJIIHCNEKPHLLJMOKEPOCBAA.filippobiscarini@anafi.it>
Message-ID: <01cb01c48b83$a731ec60$0b1a0e0a@PORTATILMARTA>

Hello,

Probably the only probem is that you have to do the plot first, than you put
the line in.
so, first:
plot(x,y)
# then you do the abline command.
abline(coef..., col=2)

Marta



From gaby.aguilera at gmail.com  Thu Aug 26 17:47:10 2004
From: gaby.aguilera at gmail.com (Gaby Aguilera)
Date: Thu, 26 Aug 2004 09:47:10 -0600
Subject: [R] newbie question: how to read a file into a matrix
Message-ID: <ebcee77004082608476b8b84b5@mail.gmail.com>

I am trying to read a file with 8 columns and about 200 rows into a
matrix.  I tried something like:
x<-matrix(data="data.txt", nrow=200, ncol=8) but this produces a
matrix with "data.txt" in it.  Is there a way to read the data in?

Any pointers in the right direction would be appreciated.  Thank you!



From ernesto at ipimar.pt  Thu Aug 26 18:06:27 2004
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Thu, 26 Aug 2004 17:06:27 +0100
Subject: [R] How to change pch in plot.factor ?
In-Reply-To: <412E04DA.1080906@statistik.uni-dortmund.de>
References: <1093533261.7270.2.camel@linux.site>
	<412E04DA.1080906@statistik.uni-dortmund.de>
Message-ID: <1093536387.7270.11.camel@linux.site>

On Thu, 2004-08-26 at 16:42, Uwe Ligges wrote:
> Ernesto Jardim wrote:
> 
> > Hi,
> > 
> > I want to change the symbol in plot.factor but using pch is not working.
> > It keeps plotting the horizontal lines ...
> 
> Have you read ?plot.factor.
> What kind of a symbol do you expect in a barplot or boxplot (instead of 
> lines)? It would not work for boxplot and barplot themselves.
> 

Yes, I've read it and it's not that clear that it uses _allways_ boxplot
or barplot. It says that boxplot os barplot will be used "when
appropriate". Also it has the argument "...: Further arguments to
'plot', see also 'par'."

And I've searched the mailing list archives looking for more information
before I posted to the mailing list.

> 
> > How can I do it ?
> 
> Using another kind of plot?
> 

Thanks for your suggestion.

> Uwe Ligges
> 
> 
> > Thanks 
> > 
> > EJ
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From thpe at hhbio.wasser.tu-dresden.de  Thu Aug 26 18:03:38 2004
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Thu, 26 Aug 2004 18:03:38 +0200
Subject: [R] Modalwert
In-Reply-To: <000501c42a1e$21eb49f0$fe79a8c0@amd26>
References: <000501c42a1e$21eb49f0$fe79a8c0@amd26>
Message-ID: <412E09DA.5010406@hhbio.wasser.tu-dresden.de>

Sonja Dornieden wrote:

> Hai -
> kann mir jemand sagen, wie ich den Modalwert in R berechne?! IRgendwie finde
> ich den Befehl nicht....
> greetz und herzlichen Dank
> Sonja

Hi Sonja,

this question seems to appear in regular intervals and you find
something about it in the R-Help archives, e.g.:

https://www.stat.math.ethz.ch/pipermail/r-help/2004-February/045014.html

or

https://www.stat.math.ethz.ch/pipermail/r-help/2004-February/045010.html

Thomas P.



From sundar.dorai-raj at PDF.COM  Thu Aug 26 18:18:42 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Thu, 26 Aug 2004 09:18:42 -0700
Subject: [R] newbie question: how to read a file into a matrix
In-Reply-To: <ebcee77004082608476b8b84b5@mail.gmail.com>
References: <ebcee77004082608476b8b84b5@mail.gmail.com>
Message-ID: <412E0D62.10004@pdf.com>



Gaby Aguilera wrote:

> I am trying to read a file with 8 columns and about 200 rows into a
> matrix.  I tried something like:
> x<-matrix(data="data.txt", nrow=200, ncol=8) but this produces a
> matrix with "data.txt" in it.  Is there a way to read the data in?
> 
> Any pointers in the right direction would be appreciated.  Thank you!
> 

You should read "R Data Import/Export" (and the posting guide).

http://cran.r-project.org/doc/manuals/R-data.pdf

What you are looking for is ?read.table or ?scan. Also read ?matrix 
while you are at it.

x <- matrix(scan("data.txt"), nrow = 200, ncol = 8)

--sundar



From ernesto at ipimar.pt  Thu Aug 26 18:31:48 2004
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Thu, 26 Aug 2004 17:31:48 +0100
Subject: [R] newbie question: how to read a file into a matrix
In-Reply-To: <ebcee77004082608476b8b84b5@mail.gmail.com>
References: <ebcee77004082608476b8b84b5@mail.gmail.com>
Message-ID: <1093537907.18313.4.camel@linux.site>


On Thu, 2004-08-26 at 16:47, Gaby Aguilera wrote:
> I am trying to read a file with 8 columns and about 200 rows into a
> matrix.  I tried something like:
> x<-matrix(data="data.txt", nrow=200, ncol=8) but this produces a
> matrix with "data.txt" in it.  Is there a way to read the data in?
> 
> Any pointers in the right direction would be appreciated.  Thank you!
> 

Hi,

Read the R manuals ... and use function "read.table" or similar.

EJ



From Rau at demogr.mpg.de  Thu Aug 26 18:25:53 2004
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Thu, 26 Aug 2004 18:25:53 +0200
Subject: [R] newbie question: how to read a file into a matrix
Message-ID: <3699CDBC4ED5D511BE6400306E1C0D81030A0C6E@hermes.demogr.mpg.de>

Hi,

	-----Original Message-----
	From:	r-help-bounces at stat.math.ethz.ch
[SMTP:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gaby Aguilera
	Sent:	Thursday, August 26, 2004 5:47 PM
	To:	r-help at stat.math.ethz.ch
	Subject:	[R] newbie question: how to read a file into a
matrix

> matrix with "data.txt" in it.  Is there a way to read the data in?
> 
> Any pointers in the right direction would be appreciated.  Thank you!
> 
	The pointer in the right direction is the bottom of each message:
read the posting guide. And there it says you should at least read the
relevant section in "An Introduction to R" which is shipped with every
distribution of R. So please read section 7 there.
	What you probably want to do is:
	### assuming you are working on Windows
	x <- read.table("c:\\mydatafolder\\data.txt", sep=",", header=TRUE)
	# depending on your separator and whether you have variable names 

	For some more information you may also check the "R Data
Import/Export Manual" also included in every distribution of R.

	Best,
	Roland




+++++
This mail has been sent through the MPI for Demographic Rese...{{dropped}}



From ahenningsen at email.uni-kiel.de  Thu Aug 26 18:28:05 2004
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Thu, 26 Aug 2004 18:28:05 +0200
Subject: [R] newbie question: how to read a file into a matrix
In-Reply-To: <ebcee77004082608476b8b84b5@mail.gmail.com>
References: <ebcee77004082608476b8b84b5@mail.gmail.com>
Message-ID: <200408261828.05154.ahenningsen@email.uni-kiel.de>

Please read the "R Data Import/Export" manual. 
It should be on your harddisk and it is also available from  
http://cran.r-project.org/doc/manuals/R-data.pdf

Arne

On Thursday 26 August 2004 17:47, Gaby Aguilera wrote:
> I am trying to read a file with 8 columns and about 200 rows into a
> matrix.  I tried something like:
> x<-matrix(data="data.txt", nrow=200, ncol=8) but this produces a
> matrix with "data.txt" in it.  Is there a way to read the data in?
>
> Any pointers in the right direction would be appreciated.  Thank you!
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From asemeria at cramont.it  Thu Aug 26 18:42:26 2004
From: asemeria at cramont.it (asemeria@cramont.it)
Date: Thu, 26 Aug 2004 18:42:26 +0200
Subject: [R] newbie question: how to read a file into a matrix
Message-ID: <OF3966F7C6.D978BA97-ONC1256EFC.005BC646@tomware.it>

I think was better you read something on "R data Import/Export"
(base documentation available for all R distribution).

A.S.

----------------------------

Alessandro Semeria
Models and Simulations Laboratory
Montecatini Environmental Research Center (Edison Group),
Via Ciro Menotti 48,
48023 Marina di Ravenna (RA), Italy
Tel. +39 544 536811
Fax. +39 544 538663
E-mail: alessandro.semeria at cramont.it



From maechler at stat.math.ethz.ch  Thu Aug 26 18:41:04 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 26 Aug 2004 18:41:04 +0200
Subject: [R] Label using equivalent of \mathbb{R}
In-Reply-To: <opsdcfwgp21pelvz@smtp.tcd.ie>
References: <FB75CFC167F3D311B11D00A0CC20FB0E88556C@nts7.oec.Uni-Osnabrueck.DE>
	<opsdcfwgp21pelvz@smtp.tcd.ie>
Message-ID: <16686.4768.62622.442505@gargle.gargle.HOWL>

>>>>> "Simon" == Simon Cullen <cullens at tcd.ie>
>>>>>     on Thu, 26 Aug 2004 15:30:06 +0100 writes:

    Simon> On Thu, 26 Aug 2004 15:24:33 +0200, Trenkler,
    Simon> Dietrich <dtrenkler at nts6.oec.uni-osnabrueck.de>
    Simon> wrote:

    >> [Dietrich Trenkler]
    >> 
    >> >
    >> plot(rnorm(10),xlab=expression(bold(x)),ylab=expression(bold(y)))

    Simon> Not quite what I am looking for, I'm afraid. \mathbb
    Simon> gives "blackboard" fonts - the capitals with two
    Simon> vertical parallel lines in them that are used for the
    Simon> Reals, Complex numbers etc.

yes (and Dieter confused \mathbb{} with \mathbf{}).

R's builtin plotmath utilities can't do this directly.
A workaround would be similar to what we used to do in LaTeX 2.09
times (before \mathbb{} was standardly available):

      { "I", <negative space>, "R" }

Whereas with (La)TeX, a macro definition was relatively
easy for the above, it might be a bit harder / more ugly with
current R builtins, for one because I don't think ``plotmath''
has a concept of <negative space>.
1)  A really ugly hack would work with  text() -- not quite useful
   if you'd like these in 'xlab' etc.

2)  A potentially much nicer scheme might be to use  "grid" instead of
   oldstyle  "graphics" :  There one could construct a  "grob"
   (graphical object) for the '|R' symbol that one could should be
   pass to other functions -- though it may need some tweaking
   before a function like (pkg lattice's) xyplot() would accept
   a grob for xlab [instead of only a character or expression].

Well, Paul & Deepayan ?

Regards,
Martin Maechler



From bklick at jhsph.edu  Thu Aug 26 19:08:20 2004
From: bklick at jhsph.edu (Brendan A. Klick)
Date: Thu, 26 Aug 2004 13:08:20 -0400
Subject: [R] gls: Newton-Raphson or EM?
Message-ID: <E619BDBD99B4F74D9DCA32F43BE92671611785@XCH-VN02.sph.ad.jhsph.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040826/29b52136/attachment.pl

From christian_mora at vtr.net  Thu Aug 26 19:18:43 2004
From: christian_mora at vtr.net (christian_mora@vtr.net)
Date: Thu, 26 Aug 2004 13:18:43 -0400
Subject: [R] plot.lm basic question
Message-ID: <4121350600027D07@hudson.vtr.net>

Hi all,

Does anyone know how can I change the color of the 'captions' and 'point
labels' in the plots obtained from plot.lm? 

Thanks for any hint

Christian



From dimitris.rizopoulos at med.kuleuven.ac.be  Thu Aug 26 19:20:06 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Thu, 26 Aug 2004 19:20:06 +0200
Subject: [R] gls: Newton-Raphson or EM?
References: <E619BDBD99B4F74D9DCA32F43BE92671611785@XCH-VN02.sph.ad.jhsph.edu>
Message-ID: <00e101c48b90$ef60b360$ad133a86@www.domain>

Hi Brendan,

according to

@Book{pinheiro.bates:00,
  author    = {J. Pinheiro and D. Bates},
  title     = {Mixed-Effects Models in S and S-PLUS},
  year      = {2000},
  address   = {New York},
  publisher = {Springer-Verlag}
}

Section 2.2.8, the optimization procedure for lme (so I suspect also
for gls) is a hybrid algorithm which starts as EM for 25 iterations
and then switches to Newton-Raphson.

Best,
Dimitris

----
Dimitris Rizopoulos
Doctoral Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Brendan A. Klick" <bklick at jhsph.edu>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, August 26, 2004 7:08 PM
Subject: [R] gls: Newton-Raphson or EM?


> Hello,
>
> Does anyone know whether the gls function in the nlme library uses
the Newton-Raphson or EM algorithm to find the restricted
log-likelihood or maximum log-likelihood estimates?
>
> Brendan Klick
> bklick at jhsph.edu
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From deepayan at stat.wisc.edu  Thu Aug 26 19:29:01 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 26 Aug 2004 12:29:01 -0500
Subject: [R] Label using equivalent of \mathbb{R}
In-Reply-To: <16686.4768.62622.442505@gargle.gargle.HOWL>
References: <FB75CFC167F3D311B11D00A0CC20FB0E88556C@nts7.oec.Uni-Osnabrueck.DE>
	<opsdcfwgp21pelvz@smtp.tcd.ie>
	<16686.4768.62622.442505@gargle.gargle.HOWL>
Message-ID: <200408261229.01753.deepayan@stat.wisc.edu>

On Thursday 26 August 2004 11:41, Martin Maechler wrote:
> >>>>> "Simon" == Simon Cullen <cullens at tcd.ie>
> >>>>>     on Thu, 26 Aug 2004 15:30:06 +0100 writes:
>
>     Simon> On Thu, 26 Aug 2004 15:24:33 +0200, Trenkler,
>     Simon> Dietrich <dtrenkler at nts6.oec.uni-osnabrueck.de>
>
>     Simon> wrote:
>     >> [Dietrich Trenkler]
>     >>
>     >>
>     >> plot(rnorm(10),xlab=expression(bold(x)),ylab=expression(bold(y)))
>
>     Simon> Not quite what I am looking for, I'm afraid. \mathbb
>     Simon> gives "blackboard" fonts - the capitals with two
>     Simon> vertical parallel lines in them that are used for the
>     Simon> Reals, Complex numbers etc.
>
> yes (and Dieter confused \mathbb{} with \mathbf{}).
>
> R's builtin plotmath utilities can't do this directly.
> A workaround would be similar to what we used to do in LaTeX 2.09
> times (before \mathbb{} was standardly available):
>
>       { "I", <negative space>, "R" }
>
> Whereas with (La)TeX, a macro definition was relatively
> easy for the above, it might be a bit harder / more ugly with
> current R builtins, for one because I don't think ``plotmath''
> has a concept of <negative space>.
> 1)  A really ugly hack would work with  text() -- not quite useful
>    if you'd like these in 'xlab' etc.
>
> 2)  A potentially much nicer scheme might be to use  "grid" instead of
>    oldstyle  "graphics" :  There one could construct a  "grob"
>    (graphical object) for the '|R' symbol that one could should be
>    pass to other functions -- though it may need some tweaking
>    before a function like (pkg lattice's) xyplot() would accept
>    a grob for xlab [instead of only a character or expression].

Interesting thought. Should be almost trivial to implement.

Currently, legends (sort of generalized key-s in the Trellis context) can be 
grobs, e.g.

bbR <- textGrob(c("I", "R"), x = 0.7 * c(-1, 1), 
                default.units = "mm", 
                vp = viewport(h=0, w=0))

xyplot(1 ~ 1, xlab = NULL, legend = list(bottom = list(fun = bbR)))

Deepayan



From maechler at stat.math.ethz.ch  Thu Aug 26 19:36:01 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 26 Aug 2004 19:36:01 +0200
Subject: [R] Replying to digested R-help messages --> get MIME digests!
In-Reply-To: <412DEEE7.3070809@vanderbilt.edu>
References: <412DEEE7.3070809@vanderbilt.edu>
Message-ID: <16686.8065.789068.554644@gargle.gargle.HOWL>

Hi Frank,

>>>>> "Frank" == Frank E Harrell <f.harrell at vanderbilt.edu>
>>>>>     on Thu, 26 Aug 2004 09:08:39 -0500 writes:

    Frank> Sorry about non-indenting of quoted text - I haven't
    Frank> figured out a good way to answer mail when receiving
    Frank> messages in digest mode -FH

use a mail client that can "undigest"  (Emacs RMAIL, VM or GNUS all do) ;-)

No, more seriously: 

I'm sure your Mozilla mail reader is capable deal with "MIME
Attachments".  Now there's an option to receive digests either in "traditional
digest format" (called "plain text") (which good ol' 'mailers
*can* undigestify!) or in MIME format: after the table of
contents, the whole bunch is "multipart/digest" and insidere
there, every message is a (nicely encapsulated) 
MIME entity of "Content-Type: message/rfc822"

For traditional reasons, the default digest format is
"plain", since the MIME digests really look horrendous in
non-MIME-capable e-mail clients.
But I do recommend to also choose "MIME" when you choose
"digest".  With the web interface to mailman, you go to your
personal option page (you need the your mailman password for
that which you should get once per month and can get sent again
by clicking somewhere) and the third entry in the option table
is

>> Get MIME or Plain Text Digests?
>> 
>> Your mail reader may or may not support MIME digests. In general
>> MIME digests are preferred, but if you have a problem reading
>> them, select plain text digests. 

There, you can not only select MIME, but also select "[Set globally]"
which sets this for all mailing lists on stat.ethz.ch to which
you are subscribed.

------------

Note that *instead* of the Web / GUI interface to the mailing
 list, you can use email 
 'To: R-help-request at ...'
	    ^^^^^^^^ 
	    (please!)

and on unix like systems, you can even do the following in the shell:

    echo password | mail r-help-request at R-project.org

and you'll get your passwor by e-mail within a few minutes
and then send the two lines
    set authenticate <<your_password>>
    set digest mime
to the same address, or in one ugly shell line (which will
probably be wrapped around before you read it!) :

   (echo set authenticate <<password>> ; echo set digest mime) | mail r-help-request at R-project.org
	 
and to turn off "digest" and revert to regular replace 'mime' by
'off' in the above.

--
Hoping this is of general help,
Martin Maechler



From john at pitney.org  Thu Aug 26 19:54:12 2004
From: john at pitney.org (John Pitney)
Date: Thu, 26 Aug 2004 12:54:12 -0500 (CDT)
Subject: [R] Why terms are dropping out of an lm() model
Message-ID: <10056.167.170.98.10.1093542852.squirrel@www.pitney.org>

Hi all!

I'm fairly new to R and not too experienced with regression.  Because
of one or both of those traits, I'm not seeing why some terms are being
dropped from my model when doing a regression using lm().

I am trying to do a regression on some experimental data d, which has
two numeric predictors, p1 and p2, and one numeric response, r.  The aim
is to compare polynomial models in p1 and p2 up to third order.  I don't
understand why lm() doesn't return coefficients for the p1^3 and p2^3
terms.  Similar loss of terms happened when I tried orthonormal
polynomials to third order.

I'm satisfied with the second-order regression, by the way, but I'd
still like to understand why the third-order regression doesn't work
like I'd expect.

Can anyone offer a pointer to help me understand this?

Here's what I'm seeing in R 1.9.1 for Windows.  Note the NA's for p1^3
and p2^3 in the last summary.

> d <- read.csv("DOE_data.csv")
> d$p1
 [1] 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 0 0 0
[34] 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0
[67] 0 0 0 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2
> d$p2
 [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1
[34] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2
[67] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
> summary(d$r)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
  18.68   19.88   21.94   21.48   22.64   24.36
> summary(d.lm1 <- lm(r ~ p1 + p2, data=d))

Call:
lm(formula = r ~ p1 + p2, data = d)

Residuals:
     Min       1Q   Median       3Q      Max
-0.58107 -0.09248  0.02492  0.26061  0.49617

Coefficients:
            Estimate Std. Error t value Pr(>|t|)
(Intercept) 18.66417    0.06591  283.17   <2e-16 ***
p1           1.96145    0.04036   48.60   <2e-16 ***
p2           0.85801    0.04036   21.26   <2e-16 ***
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1

Residual standard error: 0.3126 on 87 degrees of freedom
Multiple R-Squared:  0.97,      Adjusted R-squared: 0.9693
F-statistic:  1407 on 2 and 87 DF,  p-value: < 2.2e-16

> summary(d.lm2 <- update(d.lm1, . ~ . + I(p1^2) + I(p2^2) + I(p1 * p2)))

Call:
lm(formula = r ~ p1 + p2 + I(p1^2) + I(p2^2) + I(p1 * p2), data = d)

Residuals:
      Min        1Q    Median        3Q       Max
-0.106813 -0.021568  0.003214  0.025083  0.084687

Coefficients:
             Estimate Std. Error t value Pr(>|t|)
(Intercept) 18.701098   0.011265 1660.14   <2e-16 ***
p1           2.674525   0.019511  137.08   <2e-16 ***
p2           0.984765   0.019511   50.47   <2e-16 ***
I(p1^2)     -0.489210   0.008875  -55.12   <2e-16 ***
I(p2^2)     -0.196050   0.008875  -22.09   <2e-16 ***
I(p1 * p2)   0.265345   0.006275   42.28   <2e-16 ***
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1

Residual standard error: 0.03969 on 84 degrees of freedom
Multiple R-Squared: 0.9995,     Adjusted R-squared: 0.9995
F-statistic: 3.598e+04 on 5 and 84 DF,  p-value: < 2.2e-16

> summary(d.lm3 <- update(d.lm2, . ~ . + I(p1^3) + I(p2^3) + I(p1^2*p2) +
I(p1*p2^2)))

Call:
lm(formula = r ~ p1 + p2 + I(p1^2) + I(p2^2) + I(p1 * p2) + I(p1^3) +
    I(p2^3) + I(p1^2 * p2) + I(p1 * p2^2), data = d)

Residuals:
      Min        1Q    Median        3Q       Max
-0.089823 -0.017707  0.001952  0.020820  0.059302

Coefficients: (2 not defined because of singularities)
              Estimate Std. Error  t value Pr(>|t|)
(Intercept)  18.728958   0.009657 1939.365  < 2e-16 ***
p1            2.604190   0.022970  113.376  < 2e-16 ***
p2            0.860080   0.022970   37.444  < 2e-16 ***
I(p1^2)      -0.463725   0.010950  -42.348  < 2e-16 ***
I(p2^2)      -0.137955   0.010950  -12.598  < 2e-16 ***
I(p1 * p2)    0.432505   0.024486   17.664  < 2e-16 ***
I(p1^3)             NA         NA       NA       NA
I(p2^3)             NA         NA       NA       NA
I(p1^2 * p2) -0.025485   0.008482   -3.005  0.00353 **
I(p1 * p2^2) -0.058095   0.008482   -6.849 1.26e-09 ***
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1

Residual standard error: 0.03097 on 82 degrees of freedom
Multiple R-Squared: 0.9997,     Adjusted R-squared: 0.9997
F-statistic: 4.221e+04 on 7 and 82 DF,  p-value: < 2.2e-16

Thanks and best regards,
John



From sundar.dorai-raj at PDF.COM  Thu Aug 26 20:10:47 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Thu, 26 Aug 2004 11:10:47 -0700
Subject: [R] Why terms are dropping out of an lm() model
In-Reply-To: <10056.167.170.98.10.1093542852.squirrel@www.pitney.org>
References: <10056.167.170.98.10.1093542852.squirrel@www.pitney.org>
Message-ID: <412E27A7.6020404@pdf.com>



John Pitney wrote:
> Hi all!
> 
> I'm fairly new to R and not too experienced with regression.  Because
> of one or both of those traits, I'm not seeing why some terms are being
> dropped from my model when doing a regression using lm().
> 
> I am trying to do a regression on some experimental data d, which has
> two numeric predictors, p1 and p2, and one numeric response, r.  The aim
> is to compare polynomial models in p1 and p2 up to third order.  I don't
> understand why lm() doesn't return coefficients for the p1^3 and p2^3
> terms.  Similar loss of terms happened when I tried orthonormal
> polynomials to third order.
> 
> I'm satisfied with the second-order regression, by the way, but I'd
> still like to understand why the third-order regression doesn't work
> like I'd expect.
> 
> Can anyone offer a pointer to help me understand this?
> 
> Here's what I'm seeing in R 1.9.1 for Windows.  Note the NA's for p1^3
> and p2^3 in the last summary.
> 
> 
>>d <- read.csv("DOE_data.csv")
>>d$p1
> 
>  [1] 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 0 0 0
> [34] 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0
> [67] 0 0 0 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2
> 
>>d$p2
> 
>  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1
> [34] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2
> [67] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
> 
>>summary(d$r)
> 
>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
>   18.68   19.88   21.94   21.48   22.64   24.36
> 
>>summary(d.lm1 <- lm(r ~ p1 + p2, data=d))
> 
> 
> Call:
> lm(formula = r ~ p1 + p2, data = d)
> 
> Residuals:
>      Min       1Q   Median       3Q      Max
> -0.58107 -0.09248  0.02492  0.26061  0.49617
> 
> Coefficients:
>             Estimate Std. Error t value Pr(>|t|)
> (Intercept) 18.66417    0.06591  283.17   <2e-16 ***
> p1           1.96145    0.04036   48.60   <2e-16 ***
> p2           0.85801    0.04036   21.26   <2e-16 ***
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
> 
> Residual standard error: 0.3126 on 87 degrees of freedom
> Multiple R-Squared:  0.97,      Adjusted R-squared: 0.9693
> F-statistic:  1407 on 2 and 87 DF,  p-value: < 2.2e-16
> 
> 
>>summary(d.lm2 <- update(d.lm1, . ~ . + I(p1^2) + I(p2^2) + I(p1 * p2)))
> 
> 
> Call:
> lm(formula = r ~ p1 + p2 + I(p1^2) + I(p2^2) + I(p1 * p2), data = d)
> 
> Residuals:
>       Min        1Q    Median        3Q       Max
> -0.106813 -0.021568  0.003214  0.025083  0.084687
> 
> Coefficients:
>              Estimate Std. Error t value Pr(>|t|)
> (Intercept) 18.701098   0.011265 1660.14   <2e-16 ***
> p1           2.674525   0.019511  137.08   <2e-16 ***
> p2           0.984765   0.019511   50.47   <2e-16 ***
> I(p1^2)     -0.489210   0.008875  -55.12   <2e-16 ***
> I(p2^2)     -0.196050   0.008875  -22.09   <2e-16 ***
> I(p1 * p2)   0.265345   0.006275   42.28   <2e-16 ***
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
> 
> Residual standard error: 0.03969 on 84 degrees of freedom
> Multiple R-Squared: 0.9995,     Adjusted R-squared: 0.9995
> F-statistic: 3.598e+04 on 5 and 84 DF,  p-value: < 2.2e-16
> 
> 
>>summary(d.lm3 <- update(d.lm2, . ~ . + I(p1^3) + I(p2^3) + I(p1^2*p2) +
> 
> I(p1*p2^2)))
> 
> Call:
> lm(formula = r ~ p1 + p2 + I(p1^2) + I(p2^2) + I(p1 * p2) + I(p1^3) +
>     I(p2^3) + I(p1^2 * p2) + I(p1 * p2^2), data = d)
> 
> Residuals:
>       Min        1Q    Median        3Q       Max
> -0.089823 -0.017707  0.001952  0.020820  0.059302
> 
> Coefficients: (2 not defined because of singularities)

Did you miss reading the above line? Seems you supplied a singular model 
to `lm' and since the default for `lm' is `singular.ok = TRUE,' it just 
pivoted these columns out in the QR-decomposition.


--sundar


>               Estimate Std. Error  t value Pr(>|t|)
> (Intercept)  18.728958   0.009657 1939.365  < 2e-16 ***
> p1            2.604190   0.022970  113.376  < 2e-16 ***
> p2            0.860080   0.022970   37.444  < 2e-16 ***
> I(p1^2)      -0.463725   0.010950  -42.348  < 2e-16 ***
> I(p2^2)      -0.137955   0.010950  -12.598  < 2e-16 ***
> I(p1 * p2)    0.432505   0.024486   17.664  < 2e-16 ***
> I(p1^3)             NA         NA       NA       NA
> I(p2^3)             NA         NA       NA       NA
> I(p1^2 * p2) -0.025485   0.008482   -3.005  0.00353 **
> I(p1 * p2^2) -0.058095   0.008482   -6.849 1.26e-09 ***
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
> 
> Residual standard error: 0.03097 on 82 degrees of freedom
> Multiple R-Squared: 0.9997,     Adjusted R-squared: 0.9997
> F-statistic: 4.221e+04 on 7 and 82 DF,  p-value: < 2.2e-16
>



From Ted.Harding at nessie.mcc.ac.uk  Thu Aug 26 20:03:53 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 26 Aug 2004 19:03:53 +0100 (BST)
Subject: [R] EM norm package (NA/NaN/Inf in foreign function call (ar
In-Reply-To: <191410-22004842613585694@M2W068.mail2web.com>
Message-ID: <XFMail.040826190353.Ted.Harding@nessie.mcc.ac.uk>

On 26-Aug-04 tk at tariqkhan.org wrote:
> The following code should replicate the error by downloading
> the dataset from the internet (it is not too big):
> 
> library(norm)
> df<-download.file("http://www.tariqkhan.org/R/DataFromExcel.csv",
> "C:/Program Files/R/d.csv")
> mat<-as.matrix(read.table("C:/Program Files/R/d.csv", sep = ","))

I downloaded the dataset in my own way: 51x26 matrix with 166
missing values, right? -- and then:

  mat <- as.matrix(read.csv("DataFromExcel.csv"))

> s<-prelim.norm(mat)
> rngseed(1234567)

You don't need to set rngseed at this stage, since em.norm does
not require it; but never mind, it is needed if you go on to do
imputations.

> thetahat<-em.norm(s, maxits = 1000, criterion = 0.0035)
> 
> Iterations of EM:
> 1...2...3........348...349...Error: NA/NaN/Inf in foreign function call
> (arg 2)

I did not get this result: using the same command, em.norm terminated
normally after 82 iterations.

You can get your error message when a [nearly] singular matrix is
generated in the course of em.norm, since it has to invert a matrix
to compute the expected values of the missing components of the
sufficient statistics.

Having set rngseed as above, I then did

  mat.imp<-imp.norm(s,thetahat,mat)

after which

  svd(mat.imp)$d
   [1] 8.343633e+04 2.321644e-01 1.751089e-01 1.275187e-01
   [5] 1.116023e-01 8.807676e-02 8.006840e-02 6.198593e-02
   [9] 6.002220e-02 5.918019e-02 5.617467e-02 4.797701e-02
  [13] 4.631037e-02 4.239089e-02 3.917043e-02 3.786447e-02
  [17] 3.007310e-02 2.704916e-02 2.397084e-02 2.025846e-02
  [21] 1.681492e-02 1.336568e-02 9.161890e-03 6.042817e-03
  [25] 4.795948e-03 6.187377e-10

shows that the imputed matrix is close to 1-dimensional
and very nearly singular:
  the largest singular value is 8e+04,
  the next 10 are O(0.1),
  the next 14 are O(0.01),
  and the last one is O(1e-09).

so there is the potential for singularity problems, However,
as I say, I did not encounter any, so the behaviour you observe
is a bit puzzling.

I observe that if I set "criterion = 0.000699" or greater
(compared with your 0.0035), then em.norm terminates normally
in 479 cycles of fewer, while for "criterion = 0.000698" or
less it goes the full 1000 cycles. But still no error message.
However, this does suggest that the maximum is not too well defined.

I'm using norm version 1.0-9, like you, with R version 1.8.1 on
Linux (so I did dos2unix on DataFromExcel.csv as well, but that
shouldn't matter).

Apart from the version of R, the only difference between us is
that you're running on Windows rather than Linux, but hopefully
that shouldn't matter either.

Hmmm.
Best wishes,
Ted.

> Someone else on the list found that using scale() helped with
> em.norm, but for me it only increased the number of iterations
> before giving the same error.
> 
> I dont get it. Insights into what I can do to solve this would
> be much appreciated!
> 
> Details:
> Norm Package version 1.0.9; 
> R version 1.9.0;
> Windows XP Pro 2002 SP1; 
> 384MB RAM, Pentium 4 CPU 2.40 GHz


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 26-Aug-04                                       Time: 19:03:53
------------------------------ XFMail ------------------------------



From graumann at its.caltech.edu  Thu Aug 26 20:18:12 2004
From: graumann at its.caltech.edu (Johannes Graumann)
Date: Thu, 26 Aug 2004 11:18:12 -0700
Subject: [R] Beginners Question: Make nlm work
In-Reply-To: <000e01c48b15$e356b0e0$3b8ac445@slnt.phub.net.cable.rogers.com>
References: <20040825160732.47cdd152@localhost>
	<00f901c48b08$e9d3fd40$3b8ac445@slnt.phub.net.cable.rogers.com>
	<20040825185755.27721b2b@localhost>
	<000e01c48b15$e356b0e0$3b8ac445@slnt.phub.net.cable.rogers.com>
Message-ID: <20040826111812.721192d8@localhost>

Due to the combined nudging of the newsgroup I get it done VERY nicely
now - thanks guys!

The one problem remaining (similar to what Jim explained earlier) is:
how to get to the fitted parameters?
For the code below (were the nls was assigned to 'fit'), 'coef(fit)'
gives me:
> coef(fit)
          a           b 
4.216478980 0.004143935

and 'coef(fit)[1]' results in:
> coef(fit)[1]
       a 
4.216479

How now to extract just '4.216479'?

Thanks for the help again, everybody!

Joh

Here is the code:
setwd("~/Biology/R_versuch")
mydata<-read.table("YJG45-7_Growth.dat")
x<-mydata$V1
y<-mydata$V2
plot(x, y,
	axes=FALSE, 
	type="p",
	xlab="Time (min)",
	ylab=expression(OD[600][~nm]),
	las=1,
	pch=20,
#	log="y",
	tck=0.015,
	mgp=c(1.5,0.25,0),
)
axis(1,at=NULL,tick=TRUE,tck=0.015,mgp=c(1.5,0.25,0))
axis(2,at=NULL,tick=TRUE,tck=0.015,las=1,mgp=c(1.5,0.5,0))
axis(3,at=c(300,600,900,1200,1500,1800,2100),labels=c(5,10,15,20,25,30,
35),tick=TRUE,tck=0.015,mgp=c(1.5,0.25,0))
axis(4,labels=FALSE,at=NULL,tick=TRUE,tck=0.015,mgp=c(1.5,0.25,0)) box()
fit <- nls(y ~ a/(1+((a-0.008)/0.008)*exp(-(b*x))),
	start = list(a = 3, b = 4e-3),
#	trace = TRUE
	)
yfit1 <- 4.216479/(1+((4.216479-0.008)/0.008)*exp(-(0.004143935*x)))
lines(spline(x, yfit1))

And the data:
# YJG45, YJG46, YJG47 Growth curves 08/12/-08/x/04
# Inoculation with an over night culture to OD600=0.008
# Inoculums:
#       YJG45 (A): 2.67 --> use 74.9 myl in 25 ml
#       YJG46 (B): 2.65 --> use 75.5 myl in 25 ml
#       YJG47 (C): 2.26 --> use 88.5 myl in 25 ml
#
# t     OD600
# (min) YJG45   YJG46   YJG47
#       A       B       C
#0      0.008   0.008   0.008
94      0.011   0.013   0.012
188     0.013   0.016   0.018   
281     0.018   0.02    0.023
376     0.027   0.026   0.028
454     0.037   0.041   0.039
599     0.078   0.076   0.075
695     0.113   0.121   0.113
814     0.204   0.210   0.197
914     0.3     0.33    0.3
1009    0.47    0.51    0.48
1136    0.75    0.8     0.77
1265    1.18    1.28    1.16
1394    1.78    1.74    1.73
1525    2.13    2.16    2.20
1652    2.64    2.64    2.67
1771    3.08    3.01    2.99
1897    3.38    3.43    3.41
2036    3.78    3.76    3.72
2164    4.1     4.14    4.17

On Wed, 25 Aug 2004 22:39:12 -0400
"Jim Brennan" <jfbrennan at rogers.com> wrote:

> I can't open your file it comes up missing or damaged.
> Maybe you could send me your data as I am curious now to see the
> result. :-) probably something still wrong if it is not fitting the
> points properly...
> 
> Jim
> ----- Original Message -----
> From: "Johannes Graumann" <graumann at its.caltech.edu>
> To: "Jim Brennan" <jfbrennan at rogers.com>
> Cc: <r-help at stat.math.ethz.ch>
> Sent: Wednesday, August 25, 2004 9:57 PM
> Subject: Re: [R] Beginners Question: Make nlm work
> 
> 
> > Aaaahhhh - there was my conceptual problem ... Thanks!
> > ... but 'Oh, God!'! R sucks at fitting (when directed to do so by
> > the simple minded)! Or am I really that off?
> > I attach a plot that contains my data as well as two modeled curves:
> > - the nice one fitted by gnuplot (nonlinear least-squares (NLLS)
> > Marquardt-Levenberg)
> > - the nasty one fitted by the code below.
> >
> > Both algorithms were started with the same values for p[1] and p[2].
> >
> > Comments?
> >
> > Is there any way to access the 2 elements of 'out$estimate' from the
> > program?
> >
> > Joh
> >
> > setwd("~/Biology/R_versuch")
> > mydata<-read.table("YJG45-7_Growth.dat")
> > #plot(mydata$V1,mydata$V2,xlab="Time
> > (h)",ylab=expression(OD[600][~nm]),las=1) x<-mydata$V1
> > y<-mydata$V2
> > VH <- function(p) sum(
> > (y - p[1]/(1+((p[1]-0.008)/0.008)*exp(-(p[2]*x))))^2)
> > plot(x, y,
> > axes=FALSE,
> > type="p",
> > xlab="Time (min)",
> > ylab=expression(OD[600][~nm]),
> > las=1,
> > pch=20,
> > tck=0.015,
> > mgp=c(1.5,0.25,0),
> > )
> > axis(1,
> > at=NULL,
> > tick=TRUE,
> > tck=0.015,
> > mgp=c(1.5,0.25,0)
> > )
> > axis(2,
> > at=NULL,
> > tick=TRUE,
> > tck=0.015,
> > mgp=c(1.5,0.25,0)
> > )
> > axis(3,
> > at=c(300,600,900,1200,1500,1800,2100),
> > labels=c(5,10,15,20,25,30,35),
> > tick=TRUE,
> > tck=0.015,
> > mgp=c(1.5,0.25,0))
> > axis(4,
> > labels=FALSE,
> > at=NULL,
> > tick=TRUE,
> > tck=0.015,
> > mgp=c(1.5,0.25,0)
> > )
> > box()
> > out <- nlm(VH, p = c(3, 4e-3), hessian = TRUE)
> > #Plot Results from Gnuplot fit
> > yfit1 <- 4.21632/(1+((4.21632-0.008)/0.008)*exp(-(0.00414403*x)))
> > lines(spline(x, yfit1))
> > #Plot Results from 'out'
> > yfit2 <-
> > 3.000002050/(1+((3.000002050-0.008)/0.008)*exp(-(0.004587924*x)))
> > lines(spline(x,yfit2))
> >
> > On Wed, 25 Aug 2004 21:06:24 -0400
> > "Jim Brennan" <jfbrennan at rogers.com> wrote:
> >
> > >
> > > ----- Original Message -----
> > > From: "Johannes Graumann" <johannes_graumann at web.de>
> > > To: <r-help at stat.math.ethz.ch>
> > > Sent: Wednesday, August 25, 2004 7:07 PM
> > > Subject: [R] Beginners Question: Make nlm work
> > >
> > >
> > > > Hello,
> > > >
> > > > I'm new to this and am trying to teach myself some R by plotting
> > > > biological data. The growth curve in question is supposed to be
> > > > fitted to the Verhulst equation, which may be transcribed as
> > > > follows: f(x)=a/(1+((a-0.008)/0.008)*exp(-(b*x)))
> > > > - for a known population density (0.008) at t(0).
> > > >
> > > > I am trying to rework the example from "An Introduction to R"
> > > > (p. 72) for my case and am failing miserably. Could somebody
> > > > glance over the code below and nudge me into the right direction
> > > > - I must have some major conceptual problem which can't be
> > > > solved by me staring at it ... Since I'm repeating something I
> > > > have done with gnuplot I know that 3 and 4e-3 as starting values
> > > > for the fit are appropriate ...
> > > >
> > > > Thanks for any hint,
> > > >
> > > > Joh
> > > >
> > > > setwd("~/Biology/R_versuch")
> > > > mydata<-read.table("YJG45-7_Growth.dat")
> > > > x<-mydata$V1
> > > > y<-mydata$V2
> > > > VH <- function(p) y ~
> > > > p[1]/(1+((p[1]-0.008)/0.008)*exp(-(p[2]*x)))
> > >
> > > Shouldn't this be
> > > VH <- function(p) sum((y -
> > > p[1]/(1+((p[1]-0.008)/0.008)*exp(-(p[2]*x))))^2) Where you are
> > > minimizing the squared deviations from what your given equation is
> > > when you pass it to nlm?
> > >
> > > Maybe if you send some data that would make it more clear.
> > >
> > > Jim
> > >
> > >
> > > > plot(x, y, xlab="Time (h)",ylab=expression(OD[600][~nm]),las=1)
> > > > out <- nlm(VH, p = c(3, 4e-3), hessian = TRUE)
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > >
> >
> 
>



From gs234-r-help at srcf.ucam.org  Thu Aug 26 20:36:09 2004
From: gs234-r-help at srcf.ucam.org (Gaute B Strokkenes)
Date: Thu, 26 Aug 2004 20:36:09 +0200
Subject: [R] Beginner troubles.
Message-ID: <87brgx21d2.fsf@srcf.ucam.org>

I'm new to R.  Having read through a good chunk of "An Introduction to
R" I'm trying to use R to do some relatively straight-forward
(mathematically speaking) stuff.

I have a file which contains the coordinates of a set of points in
R^3, with one point per line (i.e. three real numbers).  The aim is to
read that file in and produce the set of all the distances between all
possible different pairs of points in that file.  So I try:

> a <- read.table("FL_BR14.xyz")

> dim(a)
[1] 329   3

> A <- function(i) a[i, ]

> A(1)
         V1       V2       V3
1 -1558.364 16.97055 1004.381

> d <- function(x, y) sqrt(sum((x - y) * (x - y)))

> outer(1:length(a[, 1]), 1:length(a[, 1]), function(u, 
    v) d(A(u), A(v)))
Error in outer(1:length(a[, 1]), 1:length(a[, 1]), function(u, v) d(A(u),  : 
	dim<- : dims [product 108241] do not match the length of object [1]

This last error has me stumped.  Could anyone give me a pointer in the
right direction?

-- 
Gaute Strokkenes                        http://www.srcf.ucam.org/~gs234/
I'm CONTROLLED by the CIA!!  EVERYONE is controlled by the CIA!!



From stchang at umich.edu  Thu Aug 26 20:37:46 2004
From: stchang at umich.edu (Stewart T Chang)
Date: Thu, 26 Aug 2004 14:37:46 -0400 (EDT)
Subject: [R] predict.mvr error message
In-Reply-To: <XFMail.040826190353.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.040826190353.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.SOL.4.58.0408261421520.11388@tetris.gpcc.itd.umich.edu>

Greetings,

I've encountered an error message while using the pls.pcr package
that's left me scratching my head for a couple of hours, and I thought I
might post it here in the hopes that someone else uses this package.
Using a data set of 1350 observations (each observation comprises 180
predictor variables and 1 output variable), I trained a model using
pls.pcr.  (Other details: ncomp=2, method="SIMPLS",validation="none".  I
might also add that each predictor variable has a value of either 0 or 1,
though the output variable is a real number.)
'
When I tried to use this model to make predictions (on 312 sets of 180
predictor variables), I received the following error:

Error in X %*% object$training$B[, , index, drop = TRUE] :
        requires numeric matrix/vector arguments

Normally I'd ask a friend of mine in Biostatistics here to help me out,
but he's currently away!  Have any of you used pls.pcr and gotten this
kind of message?  Thanks!

Stewart



From dhirm001 at student.ucr.edu  Thu Aug 26 20:42:16 2004
From: dhirm001 at student.ucr.edu (dhirm001@student.ucr.edu)
Date: Thu, 26 Aug 2004 11:42:16 -0700
Subject: [R] Adding 3D points
Message-ID: <f07913ef.760a408a.829ba00@webmail.ucr.edu>

I would like to add individual points and lines to a persp() 
plot that I generated with the geoR package. It is a kriged 
surface map of a field plot and I'd like to overlay my 
sampling points on it but am having some trouble. I loaded 
the scatterplot3d package, but I can't seem to use the add 
parameter with it. Any suggestions?



From gunter.berton at gene.com  Thu Aug 26 20:44:05 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 26 Aug 2004 11:44:05 -0700
Subject: [R] Beginner troubles.
In-Reply-To: <87brgx21d2.fsf@srcf.ucam.org>
Message-ID: <200408261844.i7QIi57p012853@ohm.gene.com>

?dist

Rule 1: Do not try to do it by yourself. Assume that a function already
exists to do what you want unless you know otherwise. Then use R's search
and help capabilities to find it (it may be in an external package on CRAN).

In this case, help.search("distance") would have given you dist.

-- Bert Gunter
Genentech



From rpeng at jhsph.edu  Thu Aug 26 20:46:48 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 26 Aug 2004 14:46:48 -0400
Subject: [R] Beginner troubles.
In-Reply-To: <87brgx21d2.fsf@srcf.ucam.org>
References: <87brgx21d2.fsf@srcf.ucam.org>
Message-ID: <412E3018.2060809@jhsph.edu>

Your function is not vectorized.  See the FAQ 
(http://cran.r-project.org/doc/FAQ/R-FAQ.html), question 7.19.

-roger

Gaute B Strokkenes wrote:
> I'm new to R.  Having read through a good chunk of "An Introduction to
> R" I'm trying to use R to do some relatively straight-forward
> (mathematically speaking) stuff.
> 
> I have a file which contains the coordinates of a set of points in
> R^3, with one point per line (i.e. three real numbers).  The aim is to
> read that file in and produce the set of all the distances between all
> possible different pairs of points in that file.  So I try:
> 
> 
>>a <- read.table("FL_BR14.xyz")
> 
> 
>>dim(a)
> 
> [1] 329   3
> 
> 
>>A <- function(i) a[i, ]
> 
> 
>>A(1)
> 
>          V1       V2       V3
> 1 -1558.364 16.97055 1004.381
> 
> 
>>d <- function(x, y) sqrt(sum((x - y) * (x - y)))
> 
> 
>>outer(1:length(a[, 1]), 1:length(a[, 1]), function(u, 
> 
>     v) d(A(u), A(v)))
> Error in outer(1:length(a[, 1]), 1:length(a[, 1]), function(u, v) d(A(u),  : 
> 	dim<- : dims [product 108241] do not match the length of object [1]
> 
> This last error has me stumped.  Could anyone give me a pointer in the
> right direction?
>



From brahm at alum.mit.edu  Thu Aug 26 21:02:10 2004
From: brahm at alum.mit.edu (David Brahm)
Date: Thu, 26 Aug 2004 15:02:10 -0400
Subject: [R] Adding 3D points
Message-ID: <16686.13234.741285.563715@lwtc3.fmr.com>

<dhirm001 at student.ucr.edu> wrote:

> I would like to add individual points and lines to a persp() plot that I
> generated with the geoR package.

See example (2) in ?persp.  You must define this function:
  trans3d <- function(x,y,z, pmat) {
    tr <- cbind(x,y,z,1) %*% pmat
    list(x = tr[,1]/tr[,4], y= tr[,2]/tr[,4])
  }

Then you must assign the result of your persp() call to "pmat", e.g.:
R> x <- y <- seq(-10, 10, length = 50)
R> f <- function(x, y) {r <- sqrt(x^2+y^2); 10 * ifelse(r==0, 1, sin(r)/r)}
R> z <- outer(x, y, f)
R> pmat <- persp(x, y, z, theta=30, phi=30, expand=.5, col="lightblue",
+          xlab="X", ylab="Y", zlab="Z", ticktype="detailed")

And then you can add points and lines:
R> points(trans3d(0,0,f(0,0),pmat))
R> z2 <- sapply(1:length(x),function(n)f(x[n],y[n]))
R> lines(trans3d(x,y,z2,pmat),col="red",lwd=2)
R> lines(trans3d(c(-10,10,10,-10,-10),c(-10,-10,10,10,-10),c(2,2,8,8,2), pmat),
+        col="blue")

All hail to Ben Bolker, who kindly taught me this in March 2002.
-- 
                              -- David Brahm (brahm at alum.mit.edu)



From Roger.Bivand at nhh.no  Thu Aug 26 21:01:48 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 26 Aug 2004 21:01:48 +0200 (CEST)
Subject: [R] Beginner troubles.
In-Reply-To: <87brgx21d2.fsf@srcf.ucam.org>
Message-ID: <Pine.LNX.4.44.0408262059230.20050-100000@reclus.nhh.no>

On Thu, 26 Aug 2004, Gaute B Strokkenes wrote:

> I'm new to R.  Having read through a good chunk of "An Introduction to
> R" I'm trying to use R to do some relatively straight-forward
> (mathematically speaking) stuff.
> 
> I have a file which contains the coordinates of a set of points in
> R^3, with one point per line (i.e. three real numbers).  The aim is to
> read that file in and produce the set of all the distances between all
> possible different pairs of points in that file.  So I try:

outer() is maybe more difficult than need be, unless this isn't the real 
question. Perhaps look at dist() first - it solves your declared problem 
directly in a single command, but that makes me think your question was 
actually about outer, not distances?

Roger

> 
> > a <- read.table("FL_BR14.xyz")
> 
> > dim(a)
> [1] 329   3
> 
> > A <- function(i) a[i, ]
> 
> > A(1)
>          V1       V2       V3
> 1 -1558.364 16.97055 1004.381
> 
> > d <- function(x, y) sqrt(sum((x - y) * (x - y)))
> 
> > outer(1:length(a[, 1]), 1:length(a[, 1]), function(u, 
>     v) d(A(u), A(v)))
> Error in outer(1:length(a[, 1]), 1:length(a[, 1]), function(u, v) d(A(u),  : 
> 	dim<- : dims [product 108241] do not match the length of object [1]
> 
> This last error has me stumped.  Could anyone give me a pointer in the
> right direction?
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From p.dalgaard at biostat.ku.dk  Thu Aug 26 21:28:56 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 26 Aug 2004 21:28:56 +0200
Subject: [R] Beginners Question: Make nlm work
In-Reply-To: <20040826111812.721192d8@localhost>
References: <20040825160732.47cdd152@localhost>
	<00f901c48b08$e9d3fd40$3b8ac445@slnt.phub.net.cable.rogers.com>
	<20040825185755.27721b2b@localhost>
	<000e01c48b15$e356b0e0$3b8ac445@slnt.phub.net.cable.rogers.com>
	<20040826111812.721192d8@localhost>
Message-ID: <x2zn4hog07.fsf@biostat.ku.dk>

Johannes Graumann <graumann at its.caltech.edu> writes:

> > coef(fit)[1]
>        a 
> 4.216479
> 
> How now to extract just '4.216479'?

That's what you have, it's just that is has a names attribute. Doesn't
usually matter, but you can feed it to as.vector() if you want to get
rid of it.


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From graumann at its.caltech.edu  Thu Aug 26 22:00:15 2004
From: graumann at its.caltech.edu (Johannes Graumann)
Date: Thu, 26 Aug 2004 13:00:15 -0700
Subject: [R] Beginners Question: Make nlm work
In-Reply-To: <x2zn4hog07.fsf@biostat.ku.dk>
References: <20040825160732.47cdd152@localhost>
	<00f901c48b08$e9d3fd40$3b8ac445@slnt.phub.net.cable.rogers.com>
	<20040825185755.27721b2b@localhost>
	<000e01c48b15$e356b0e0$3b8ac445@slnt.phub.net.cable.rogers.com>
	<20040826111812.721192d8@localhost> <x2zn4hog07.fsf@biostat.ku.dk>
Message-ID: <20040826130015.7e3a011f@localhost>

Thanks.

Joh

On 26 Aug 2004 21:28:56 +0200
Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:

> Johannes Graumann <graumann at its.caltech.edu> writes:
> 
> > > coef(fit)[1]
> >        a 
> > 4.216479
> > 
> > How now to extract just '4.216479'?
> 
> That's what you have, it's just that is has a names attribute. Doesn't
> usually matter, but you can feed it to as.vector() if you want to get
> rid of it.
> 
> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45)
>  35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45)
> 35327907
>



From EFG at Stowers-Institute.org  Thu Aug 26 22:15:26 2004
From: EFG at Stowers-Institute.org (Glynn, Earl)
Date: Thu, 26 Aug 2004 15:15:26 -0500
Subject: [R] Surprise when mapping matrix to image
Message-ID: <200408262015.i7QKFVMm029395@hypatia.math.ethz.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040826/e9297324/attachment.pl

From vograno at evafunds.com  Thu Aug 26 22:22:54 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Thu, 26 Aug 2004 13:22:54 -0700
Subject: [R] how to efficiently bootstrap mgcv::gam
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A563D67D@phost015.EVAFUNDS.intermedia.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040826/848ad160/attachment.pl

From kjetil at acelerate.com  Thu Aug 26 22:25:24 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Thu, 26 Aug 2004 16:25:24 -0400
Subject: [R] library(car) Anova() and Error-term in aov()
In-Reply-To: <20040826134021.ICWO29920.tomts10-srv.bellnexxia.net@JohnDesktop8300>
References: <20040826134021.ICWO29920.tomts10-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <412E4734.6030105@acelerate.com>

John Fox wrote:

>Dear Peter and Paul,
>
>As Paul discovered, Anova() doesn't handle aovlist objects. 
>
>As a general matter, one should be careful with "type-III" tests, since it's
>easy to test hypotheses that aren't sensible (e.g., tests ostensibly of main
>effects that aren't reasonably interpretable as tests of main effects). For
>example, SAS (and I assume SPSS) produce type-III tests in analysis of
>covariance that aren't generally sensible. I haven't thought about whether
>there's a similar trap in unbalanced repeated-measures ANOVA. By the way,
>sequential (or "type-I") tests are rarely sensible in my opinion. 
>  
>

 With the computing power of today, are there any reason not to do the 
easy and sensible thing,
to test the hypothesis which interest YOU, by estimating to models, 
large and small, and test the
small within the large?

Kjetil Halvorsen

>Regards,
> John
>
>  
>
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Peter Dalgaard
>>Sent: Thursday, August 26, 2004 7:08 AM
>>To: Paul Lemmens
>>Cc: r-help at stat.math.ethz.ch
>>Subject: Re: [R] library(car) Anova() and Error-term in aov()
>>
>>Paul Lemmens <P.Lemmens at nici.kun.nl> writes:
>>
>>    
>>
>>>Dear all,
>>>
>>>Type III SS time again. This case trying to reproduce some 
>>>      
>>>
>>SPSS (type
>>    
>>
>>>III) data in R for a repeated measures anova with a betwSS factor 
>>>included. As I understand this list etc, if I want type III 
>>>      
>>>
>>then I can 
>>    
>>
>>>do
>>>
>>>library(car)
>>>Anova(lm.obj, type="III")
>>>
>>>But for the repeated measures anova, I need to include an 
>>>      
>>>
>>Error-term 
>>    
>>
>>>in the aov() call (Psychology-guide from Jonathan Baron) 
>>>      
>>>
>>which results 
>>    
>>
>>>in multiple lm() calls. Anova() does not seem capable to 
>>>      
>>>
>>handle this 
>>    
>>
>>>situation. Or am I tackling Type III calculation, in this case with 
>>>Error(), the wrong way (besides ignoring advice concerning 
>>>      
>>>
>>Type I vs 
>>    
>>
>>>III)??
>>>
>>>For instance,
>>>
>>>dat <- rnorm(12)
>>>pp <- factor(c(rep(1:3,2), rep(4:6,2))) betw <- gl(2,6) A <- 
>>>factor(rep(c(rep('a',3),rep('b',3)), 2)) taov <- 
>>>aov(dat~betw*A+Error(pp/A)) Anova(taov, type="III") # Goes 
>>>      
>>>
>>wrong with 
>>    
>>
>>>following error.
>>>#Error in Anova(taov, type = "III") : no applicable method 
>>>      
>>>
>>for "Anova"
>>    
>>
>>>Phrased differently, ?Anova says "Calculates type-II or type-III 
>>>analysis-of-variance tables for model objects produced by 'lm' and 
>>>'glm'", so it's not suitable for the aovlist that aov() with 
>>>Error()-term returns. How can I compute Type III SS for 
>>>      
>>>
>>such objects?
>>
>>Well, ...
>>
>>In a balanced design you don't need Type III SS (because they 
>>are all the same) --  summary(taov) will do.
>>
>>In an unbalanced design, you don't want to use aov() with an 
>>Error term. (Slightly overstated, but you certainly get to 
>>think very closely if the unbalance is in the Error model).
>>
>>I'm not actually sure what SPSS does in the case of 
>>unbalanced designs (complete-case analysis?).
>>
>>In principle, with a balanced Error model, you should be able 
>>to extract, say, taov[[2]] and do an Anova() or drop1() on 
>>that, but it doesn't work because the object is not really an 
>>"lm" object, even though
>>
>>    
>>
>>>class(taov[[2]])
>>>      
>>>
>>[1] "aov" "lm"
>>
>>but we get things like
>>
>>    
>>
>>>model.frame(as(taov[[2]],"lm"))
>>>      
>>>
>>$method
>>lm
>>
>>
>>-- 
>>   O__  ---- Peter Dalgaard             Blegdamsvej 3  
>>  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>> (*) \(*) -- University of Copenhagen   Denmark      Ph: 
>>(+45) 35327918
>>~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: 
>>(+45) 35327907
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>



From ripley at stats.ox.ac.uk  Thu Aug 26 22:43:50 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 26 Aug 2004 21:43:50 +0100 (BST)
Subject: [R] Surprise when mapping matrix to image
In-Reply-To: <200408262015.i7QKFVMm029395@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.44.0408262130440.29831-100000@gannet.stats>

On Thu, 26 Aug 2004, Glynn, Earl wrote:

> Start with:
> 
> > x <- c(1:7,1)
> > dim(x) <- c(2,4)
> > x
>      [,1] [,2] [,3] [,4]
> [1,]    1    3    5    7
> [2,]    2    4    6    1
> 
> 2 Rows of 4 Columns.  Upper-left and lower-right elements of the matrix
> are the same.
> 
> All to this point makes good sense.  

It's pure convention: see below.

> > image(x)
> 
> However, this image shows 2 columns of 4 rows.  The lower-left and
> upper-right elements are the same.   This does not make sense to me.
> Did I miss some simple parameter to "fix" all of this naturally?  Why
> would the numeric matrix of "x" and the image of "x" have such a
> different geometry?

Did you try reading the help for image?  You don't seem to understand it
if you actually did.  It seems you are looking for

	image(t(x)[ncol(x):1, ])

Easy!

Mathematical conventions are just that, conventions.  They differ by field 
of mathematics.  Don't ask us why matrix rows are numbered down but graphs 
are numbered up the y axis, nor why x comes before y but row before 
column.  But the matrix layout has always seemed illogical to me.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From graumann at its.caltech.edu  Thu Aug 26 22:47:32 2004
From: graumann at its.caltech.edu (Johannes Graumann)
Date: Thu, 26 Aug 2004 13:47:32 -0700
Subject: [R] how to make lines() meet axis in autoscaled coordinate system?
Message-ID: <20040826134732.66b382e6@localhost>

Hello,

I have an auto-scaled coordinate system and would like to add some
clarifying lines to the graph - which ares supposed to meet the axis.

>lines(c(0,time1,time1), c(1,1,0),lty=3)

does what I want, BUT the second leg does not touch the x-axis since the
auto-scaling of the y-axis does not start at'0' but slightly negative.
I could now adjust the line length by 'trial and error' to suit my
needs, but I'd prefer a generalizable solution like

>lines(c(<some_macro_for_y-axis_position>,time1,time1),
+	c(1,1,<some_macro_for_x-axis_position>),lty=3)

Can somebody help me out with this?

Thanks,

Joh



From rolf at math.unb.ca  Thu Aug 26 23:15:44 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Thu, 26 Aug 2004 18:15:44 -0300 (ADT)
Subject: [R] how to make lines() meet axis in autoscaled coordinate system?
Message-ID: <200408262115.i7QLFiGe025939@erdos.math.unb.ca>


Johannes Graumann wrote:

> I have an auto-scaled coordinate system and would like to add some
> clarifying lines to the graph - which ares supposed to meet the axis.
> 
> >lines(c(0,time1,time1), c(1,1,0),lty=3)
> 
> does what I want, BUT the second leg does not touch the x-axis since the
> auto-scaling of the y-axis does not start at'0' but slightly negative.
> I could now adjust the line length by 'trial and error' to suit my
> needs, but I'd prefer a generalizable solution like
> 
> >lines(c(<some_macro_for_y-axis_position>,time1,time1),
> +	c(1,1,<some_macro_for_x-axis_position>),lty=3)

	lines(c(par()$usr[1],time1,time1), c(1,1,par()$usr[3]),lty=3)

				cheers,

					Rolf Turner
					rolf at math.unb.ca



From EFG at Stowers-Institute.org  Thu Aug 26 23:47:16 2004
From: EFG at Stowers-Institute.org (Glynn, Earl)
Date: Thu, 26 Aug 2004 16:47:16 -0500
Subject: [R] Surprise when mapping matrix to image
Message-ID: <200408262147.i7QLlJBt017379@hypatia.math.ethz.ch>

Prof Ripley:

Thank you for your prompt reply.

> It's pure convention: see below.
> 
> Did you try reading the help for image?  You don't seem to 
> understand it
> if you actually did.  It seems you are looking for
> 
> 	image(t(x)[ncol(x):1, ])

I think you guys are too close to "R" to understand how hard it is to
use sometimes.  What may be blatantly obvious to you is quite a problem
especially to beginners.  Some of us may be beginners to R, but we know
math, science, programming, and how to solve problems with other tools
and languages.    

I re-read the guidelines before posting fearing condemnation.

Before posting I searched the online R-help Google interface with
keywords "image", "flip", "rotate".  A discussion from 1998 touched on
this issue but I was hoping that this was deemed a "bug" at some point
and fixed -- or had an easy workaround, like some parameter I was
missing.

I read the "?image" help before posting. Was the part I didn't
understand buried in this "note"?

     "Based on a function by Thomas Lumley tlumley at u.washington.edu."

I asked the question because I did not find the answer under "?image".
Please quote the portion that I missed that you think explains this.  I
was tempted to post the whole "?image" section here to prove my point.
There is no entry on that page with transpose, "t(", or "ncol" the way
you explained above. I'm not sure what you think I missed, but I did my
homework on this problem before posting.
 
> Easy!
> 
> Mathematical conventions are just that, conventions.  They 
> differ by field 
> of mathematics.  Don't ask us why matrix rows are numbered 
> down but graphs 
> are numbered up the y axis, nor why x comes before y but row before 
> column.  But the matrix layout has always seemed illogical to me.

My 30+ years of software experience says this is a design flaw.  A good
design would reconcile the differences between conventions.  Coordinate
transformations are not that hard, but I wouldn't burden the end-user
with them needlessly.

I'll go back to lurking in the daily R-Helps and not ask any more
questions until I've read all the old R-help messages.  I'm working on
December 1998 right now and reading forward.  Perhaps by next year I'll
will have read all the old R-help postings and I'll dare ask another
question then.

efg



From jnb21 at cam.ac.uk  Fri Aug 27 00:04:10 2004
From: jnb21 at cam.ac.uk (Jennie Bee)
Date: Thu, 26 Aug 2004 23:04:10 +0100
Subject: [R] AIC to compare glm models with Poisson errors?
Message-ID: <025301c48bb8$9d5d3070$07256f83@plantsci.cam.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040826/bdd4f515/attachment.pl

From d.scott at auckland.ac.nz  Fri Aug 27 00:10:20 2004
From: d.scott at auckland.ac.nz (David Scott)
Date: Fri, 27 Aug 2004 10:10:20 +1200 (NZST)
Subject: [R] Plotting groupedData objects
In-Reply-To: <1093501920.412d83e0d9891@www-auth.cs.wisc.edu>
Message-ID: <Pine.LNX.4.44.0408271003320.31281-100000@hydra.stat.auckland.ac.nz>

On Thu, 26 Aug 2004, Deepayan Sarkar wrote:

> Quoting David Scott <d.scott at auckland.ac.nz>:
> 
> > 
> > I am trying to create a plot similar to Figure 3.2 in Bates and Pinheiro.
> > 
> > I have repeated measurements on about 80 subjects from 2 treatment groups.  
> > I would like to have the panels for the two treatment groups in separate
> > groups and within those groups have the panels ordered on maximum value
> > (as is the default).
> > 
> > I am ok with getting plots similar to Figs 3.1 and 3.2, but can't see how 
> > to change the ordering of the panels to what I want.
> > 
> > Here is the definition of my groupedData object
> > 
> > RAWlmeData <- groupedData(RAW~Elapsed|ID,
> >               data=RAWData,
> >               labels=list(x="Elapsed time",y="Airways resistance"),
> >               units=list(x="(hours)",y="cm H20/L/sec"))
> > 
> > I guess I could just plot the two treatment groups separately in turn but 
> > I feel there is something I am missing.
> 
> My reading of page 105 suggests that you need to specify 
> 'outer=<whatever your grouping factor is>'.
> Have you tried that?
> 
I did try that. It produces Fig 3.3 with multiple lines giving the results 
for each subject by treatment. With around 40 subjects per treatment 
group, that isn't very attractive in my case.

I should also mention that I looked for on-line complements to Pinheiro 
and Bates, and found MEMSS.tar.gz on 
http://cm.bell-labs.com/cm/ms/departments/sia/NLME/MEMSS/index.html
which promised scripts for the examples in the book but turned out to have 
only chapters 1 and 2, and for S-PLUS rather than R (going on the .q 
extension).

David Scott

_________________________________________________________________
David Scott	Department of Statistics, Tamaki Campus
		The University of Auckland, PB 92019
		Auckland	NEW ZEALAND
Phone: +64 9 373 7599 ext 86830		Fax: +64 9 373 7000
Email:	d.scott at auckland.ac.nz 


Graduate Officer, Department of Statistics



From graumann at its.caltech.edu  Fri Aug 27 00:11:36 2004
From: graumann at its.caltech.edu (Johannes Graumann)
Date: Thu, 26 Aug 2004 15:11:36 -0700
Subject: [R] how to make lines() meet axis in autoscaled coordinate system?
In-Reply-To: <200408262115.i7QLFiGe025939@erdos.math.unb.ca>
References: <200408262115.i7QLFiGe025939@erdos.math.unb.ca>
Message-ID: <20040826151136.15e13b5c@localhost>

Grand!

Thanks so much!

Joh

On Thu, 26 Aug 2004 18:15:44 -0300 (ADT)
Rolf Turner <rolf at math.unb.ca> wrote:

> 
> Johannes Graumann wrote:
> 
> > I have an auto-scaled coordinate system and would like to add some
> > clarifying lines to the graph - which ares supposed to meet the
> > axis.
> > 
> > >lines(c(0,time1,time1), c(1,1,0),lty=3)
> > 
> > does what I want, BUT the second leg does not touch the x-axis since
> > the auto-scaling of the y-axis does not start at'0' but slightly
> > negative. I could now adjust the line length by 'trial and error' to
> > suit my needs, but I'd prefer a generalizable solution like
> > 
> > >lines(c(<some_macro_for_y-axis_position>,time1,time1),
> > +	c(1,1,<some_macro_for_x-axis_position>),lty=3)
> 
> 	lines(c(par()$usr[1],time1,time1), c(1,1,par()$usr[3]),lty=3)
> 
> 				cheers,
> 
> 					Rolf Turner
> 					rolf at math.unb.ca
>



From bates at stat.wisc.edu  Fri Aug 27 00:14:01 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 26 Aug 2004 17:14:01 -0500
Subject: [R] GLMM
In-Reply-To: <AE3AD514E378164DBAC0CDE14210E19B05BF198F@m-ncipc-1.ncipc.cdc.gov>
References: <AE3AD514E378164DBAC0CDE14210E19B05BF198F@m-ncipc-1.ncipc.cdc.gov>
Message-ID: <412E60A9.1070808@stat.wisc.edu>

Bossarte, Robert wrote:
> I am trying to use the LME package to run a multilevel logistic model
> using the following code:
> 
>  
> 
> ------------------------------------------------------------------------
> -------------------------------------------
> 
>  
> 
> Model1 = GLMM(WEAP ~ TSRAT2 , random = ~1 | GROUP , family = binomial,
> na.action = na.omit )
> 
>  
> 
> ------------------------------------------------------------------------
> -------------------------------------------
> 
>  
> 
> Where WEAP is a dichotomous outcome measure, TSRAT2 is a group level
> covariate, and GROUP is the grouping variable.
> 
>  
> 
> I do not have significant missing data and the dataset is over 15000
> cases.
> 
>  
> 
> I have tried changing the outcome measure(s) and predictor(s) with the
> same result. Each time R fails and returns the following message:
> 
>  
> 
> ------------------------------------------------------------------------
> ------------------------------------------------
> 
>  
> 
> structure(list( : flist must be a non-empty list of factors
> 
>  
> 
> ------------------------------------------------------------------------
> ------------------------------------------------
> 
>  
> 
> This appears to be problem with the grouping variable, yet there are no
> missing values and the data were sorted on this variable prior to being
> read into R.

Is the grouping variable a factor?  Check with

str(GROUP)

If it isn't a factor then convert it a factor using

GROUP <- factor(GROUP)

> 
>  
> 
> Any suggestions would be greatly appreciated.



From john at pitney.org  Fri Aug 27 00:17:24 2004
From: john at pitney.org (John Pitney)
Date: Thu, 26 Aug 2004 17:17:24 -0500 (CDT)
Subject: [R] Why terms are dropping out of an lm() model
In-Reply-To: <412E27A7.6020404@pdf.com>
References: <10056.167.170.98.10.1093542852.squirrel@www.pitney.org>
	<412E27A7.6020404@pdf.com>
Message-ID: <14541.167.170.98.10.1093558644.squirrel@www.pitney.org>

>
>
> John Pitney wrote:
>> Hi all!
>>
>> I'm fairly new to R and not too experienced with regression.  Because
>> of one or both of those traits, I'm not seeing why some terms are being
>> dropped from my model when doing a regression using lm().
>>
>> I am trying to do a regression on some experimental data d, which has
>> two numeric predictors, p1 and p2, and one numeric response, r.  The aim
>> is to compare polynomial models in p1 and p2 up to third order.  I don't
>> understand why lm() doesn't return coefficients for the p1^3 and p2^3
>> terms.  Similar loss of terms happened when I tried orthonormal
>> polynomials to third order.
>>
>> I'm satisfied with the second-order regression, by the way, but I'd
>> still like to understand why the third-order regression doesn't work
>> like I'd expect.
>>
>> Can anyone offer a pointer to help me understand this?
>>
>> Here's what I'm seeing in R 1.9.1 for Windows.  Note the NA's for p1^3
>> and p2^3 in the last summary.
>>
>> [stuff deleted]
>>
>> -0.089823 -0.017707  0.001952  0.020820  0.059302
>>
>> Coefficients: (2 not defined because of singularities)
>
> Did you miss reading the above line? Seems you supplied a singular model
> to `lm' and since the default for `lm' is `singular.ok = TRUE,' it just
> pivoted these columns out in the QR-decomposition.

Yes, I missed that line.  The model matrix is indeed singular.

Thanks for the quick and helpful response, and sorry for posting before
thinking carefully enough!

John



From deepayan at cs.wisc.edu  Fri Aug 27 00:22:41 2004
From: deepayan at cs.wisc.edu (Deepayan Sarkar)
Date: Thu, 26 Aug 2004 17:22:41 -0500
Subject: [R] Plotting groupedData objects
In-Reply-To: <Pine.LNX.4.44.0408271003320.31281-100000@hydra.stat.auckland.ac.nz>
References: <Pine.LNX.4.44.0408271003320.31281-100000@hydra.stat.auckland.ac.nz>
Message-ID: <1093558961.412e62b1c674e@www-auth.cs.wisc.edu>

Quoting David Scott <d.scott at auckland.ac.nz>:

> On Thu, 26 Aug 2004, Deepayan Sarkar wrote:
> 
> > Quoting David Scott <d.scott at auckland.ac.nz>:
> > 
> > > 
> > > I am trying to create a plot similar to Figure 3.2 in Bates and
> Pinheiro.
> > > 
> > > I have repeated measurements on about 80 subjects from 2 treatment
> groups.  
> > > I would like to have the panels for the two treatment groups in separate
> > > groups and within those groups have the panels ordered on maximum value
> > > (as is the default).
> > > 
> > > I am ok with getting plots similar to Figs 3.1 and 3.2, but can't see how
> 
> > > to change the ordering of the panels to what I want.
> > > 
> > > Here is the definition of my groupedData object
> > > 
> > > RAWlmeData <- groupedData(RAW~Elapsed|ID,
> > >               data=RAWData,
> > >               labels=list(x="Elapsed time",y="Airways resistance"),
> > >               units=list(x="(hours)",y="cm H20/L/sec"))
> > > 
> > > I guess I could just plot the two treatment groups separately in turn but
> 
> > > I feel there is something I am missing.
> > 
> > My reading of page 105 suggests that you need to specify 
> > 'outer=<whatever your grouping factor is>'.
> > Have you tried that?
> > 
> I did try that. It produces Fig 3.3 with multiple lines giving the results 
> for each subject by treatment. With around 40 subjects per treatment 
> group, that isn't very attractive in my case.

I should have been more specific, but I thought this was clear enough in the
book. I was talking about specifying outer in the groupedData() constructor,
not in the plot() call. Have you tried _that_?

> I should also mention that I looked for on-line complements to Pinheiro 
> and Bates, and found MEMSS.tar.gz on 
> http://cm.bell-labs.com/cm/ms/departments/sia/NLME/MEMSS/index.html
> which promised scripts for the examples in the book but turned out to have 
> only chapters 1 and 2, and for S-PLUS rather than R (going on the .q 
> extension).

The R scripts are, naturally enough, bundled with the R version of the package.
They should be in the scripts/ subdirectory of your nlme installation.

Deepayan



From deepayan at cs.wisc.edu  Fri Aug 27 00:36:00 2004
From: deepayan at cs.wisc.edu (Deepayan Sarkar)
Date: Thu, 26 Aug 2004 17:36:00 -0500
Subject: [R] Surprise when mapping matrix to image
In-Reply-To: <200408262147.i7QLlJBt017379@hypatia.math.ethz.ch>
References: <200408262147.i7QLlJBt017379@hypatia.math.ethz.ch>
Message-ID: <1093559760.412e65d0a85ee@www-auth.cs.wisc.edu>

Quoting "Glynn, Earl" <EFG at Stowers-Institute.org>:

> Prof Ripley:
> 
> Thank you for your prompt reply.
> 
> > It's pure convention: see below.
> > 
> > Did you try reading the help for image?  You don't seem to 
> > understand it
> > if you actually did.  It seems you are looking for
> > 
> >  image(t(x)[ncol(x):1, ])
> 
> I think you guys are too close to "R" to understand how hard it is to
> use sometimes.  What may be blatantly obvious to you is quite a problem
> especially to beginners.  Some of us may be beginners to R, but we know
> math, science, programming, and how to solve problems with other tools
> and languages.    
> 
> I re-read the guidelines before posting fearing condemnation.
> 
> Before posting I searched the online R-help Google interface with
> keywords "image", "flip", "rotate".  A discussion from 1998 touched on
> this issue but I was hoping that this was deemed a "bug" at some point
> and fixed -- or had an easy workaround, like some parameter I was
> missing.
> 
> I read the "?image" help before posting. Was the part I didn't
> understand buried in this "note"?
> 
>      "Based on a function by Thomas Lumley tlumley at u.washington.edu."

You seem to be thinking that Prof Ripley's solution had something to do with
image(). It doesn't, it has to do with manipulating a matrix. image()
visualizes a matrix in a particular and well-defined way. You want your matrix
to be shown in a different way, and one (simple) way of doing that is to
convert your matrix into a different matrix, on which calling image would give
you what you want. Why would this be explained in ?image ? This is basic R.

More generally, I think your frustration is caused by your expectation that a
matrix object should behave like a bitmap image. It doesn't. If you want work
with images, use the pixmap package.

Deepayan



From nair at sdsc.edu  Fri Aug 27 00:50:43 2004
From: nair at sdsc.edu (T. Murlidharan Nair)
Date: Thu, 26 Aug 2004 15:50:43 -0700
Subject: [R] Error TukeyHSD
Message-ID: <412E6943.6020307@sdsc.edu>

I am running the following code on the coagulation data and I am getting 
an error. Please let me know
if I am missing anything from my code.

coag<- matrix( scan("//Samba3/nair/R/blood.dat", sep=","), 24, 3, 
byrow=TRUE)
colnames(coag) <- c("time","diet","order")
coag <- as.data.frame(coag)
oneway.test(time ~  diet, data=coag, var.eq=TRUE)
coag.mod <- aov(time ~ diet, data=coag)
options(scipen=5)
options(digits=5)
TukeyHSD( coag.mod )


I get the following error when I run the above code
Read 72 items
Error in rep.int(n, length(means)) : Unimplemented feature in rep
In addition: Warning message:
non-factors ignored: diet in: replications(paste("~", 
paste(names(tables), collapse = "+")),

The data is here ..

62,1,20
60,1,2
63,1,11
59,1,10
63,2,12
67,2,9
71,2,15
64,2,14
65,2,4
66,2,8
68,3,16
66,3,7
71,3,1
67,3,17
68,3,13
68,3,21
56,4,23
62,4,3
60,4,6
61,4,18
63,4,22
64,4,19
63,4,5
59,4,24


Thanks ../ Murli



From m_nica at hotmail.com  Fri Aug 27 01:08:47 2004
From: m_nica at hotmail.com (Mihai Nica)
Date: Thu, 26 Aug 2004 18:08:47 -0500
Subject: [R] coplot and par
Message-ID: <BAY18-F11XCpU6q5ijb0001c543@hotmail.com>

R 1.9.1 on Win2000 or Win98SE.

I am using coplot as follows:

coplot(AVG~LRPI| REGION)

the output seems normal but I get:

"Warning message:
calling par(new=) with no plot"

This is the only explanation that I have for being unable to use par() with 
coplot for changing the way the xlab and ylab appears. From within coplot I 
can change the text itself but not the font, fontsize, etc, but par() before 
coplot has absolutely no effect wahtsoever (maybe it isn't supposed to?).

I guess the question is: how can one change the way the xlab and ylab appear 
on a coplot?

Thanks,

Mihai
JSU



From rossini at blindglobe.net  Fri Aug 27 01:11:04 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Thu, 26 Aug 2004 16:11:04 -0700
Subject: [R] Surprise when mapping matrix to image
In-Reply-To: <1093559760.412e65d0a85ee@www-auth.cs.wisc.edu> (Deepayan
	Sarkar's message of "Thu, 26 Aug 2004 17:36:00 -0500")
References: <200408262147.i7QLlJBt017379@hypatia.math.ethz.ch>
	<1093559760.412e65d0a85ee@www-auth.cs.wisc.edu>
Message-ID: <85k6vlebqv.fsf@servant.blindglobe.net>


I think I'd have to respectfully disagree with both Brian and
Deepayan, as to whether it should be obvious.  It is reasonable
(principle of least suprise) to expect orientation of the plot to
match the print order of the matrix.  I would have expected Brian's
one-liner to be in the help page, with a notice.  It's a not-so-rare
activity, being a general matrix visualization that is commonly used
in certain areas of science (whether it ought to be commonly used is a
separate question).

While "heatmap" might've been perhaps a better pointer, but it doesn't
seem to do the "right" thing, either.  I.e. 

myTemp <- matrix(c(1,2,3,3,2,3),nrow=2)
heatmap(myTemp,Rowv=NA,Colv=NA)

doesn't look right to me (R Version 1.9.1  (2004-06-21))

I see the "pixmap/bitmap" issue as a bit of a red herring, in this
case. 

best,
-tony

p.s. I seem to get bit by this about once a year for the last few,
hence why I'm speaking up.








Deepayan Sarkar <deepayan at cs.wisc.edu> writes:

> Quoting "Glynn, Earl" <EFG at Stowers-Institute.org>:
>
>> Prof Ripley:
>> 
>> Thank you for your prompt reply.
>> 
>> > It's pure convention: see below.
>> > 
>> > Did you try reading the help for image?  You don't seem to 
>> > understand it
>> > if you actually did.  It seems you are looking for
>> > 
>> >  image(t(x)[ncol(x):1, ])
>> 
>> I think you guys are too close to "R" to understand how hard it is to
>> use sometimes.  What may be blatantly obvious to you is quite a problem
>> especially to beginners.  Some of us may be beginners to R, but we know
>> math, science, programming, and how to solve problems with other tools
>> and languages.    
>> 
>> I re-read the guidelines before posting fearing condemnation.
>> 
>> Before posting I searched the online R-help Google interface with
>> keywords "image", "flip", "rotate".  A discussion from 1998 touched on
>> this issue but I was hoping that this was deemed a "bug" at some point
>> and fixed -- or had an easy workaround, like some parameter I was
>> missing.
>> 
>> I read the "?image" help before posting. Was the part I didn't
>> understand buried in this "note"?
>> 
>>      "Based on a function by Thomas Lumley tlumley at u.washington.edu."
>
> You seem to be thinking that Prof Ripley's solution had something to do with
> image(). It doesn't, it has to do with manipulating a matrix. image()
> visualizes a matrix in a particular and well-defined way. You want your matrix
> to be shown in a different way, and one (simple) way of doing that is to
> convert your matrix into a different matrix, on which calling image would give
> you what you want. Why would this be explained in ?image ? This is basic R.
>
> More generally, I think your frustration is caused by your expectation that a
> matrix object should behave like a bitmap image. It doesn't. If you want work
> with images, use the pixmap package.
>
> Deepayan
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
Anthony Rossini			    Research Associate Professor
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From graumann at its.caltech.edu  Fri Aug 27 01:29:40 2004
From: graumann at its.caltech.edu (Johannes Graumann)
Date: Thu, 26 Aug 2004 16:29:40 -0700
Subject: [R] text() with text, variables and math HOWTO?
Message-ID: <20040826162940.5ef5149d@localhost>

Hello,

One more question from the 'abusing R for blotting - particularly
anally' department:
How can I in the expression below make the '%~~%' show up as the
aprrox-sign I want it to be?

Thanks for any hint,

Joh

text(
	500,1.5,
	cex=0.75,
	substitute(
		paste(
			OD[600][~nm],
			" of 1 at ",
			time1,
			" min ",
			"%~~%",
			time1h,
			"h"
		),
		list(
			time1=round(time1,digits=0),
			time1h=round(time1/60,digits=1)
		)
	)
)



From avelox12 at yahoo.com  Fri Aug 27 01:13:19 2004
From: avelox12 at yahoo.com (franc Li)
Date: Thu, 26 Aug 2004 16:13:19 -0700 (PDT)
Subject: [R] multiple regression with dummy variables
Message-ID: <20040826231319.9247.qmail@web51810.mail.yahoo.com>

Hi I'm a beginner in R, please help...

I'm trying to do regression analysis on a categorical
data to find b's of each Xi.  Essentially, f(x) is the
output, and two independent variables Xd (days of week
(1,2,...,7) and Xwk (week of month
(1,2,...,6)including partial wks).  

So the data looks like this:
f(x) Xd  Xwk
0.03  1   2
0.06  3   1
:
:

I use factor() and contrasts() to produce I.matrix as
such
  2 3 4 5 6 7
1 0 0 0 0 0 0
2 1 0 0 0 0 0
3 0 1 0 0 0 0
4 0 0 1 0 0 0
5 0 0 0 1 0 0
6 0 0 0 0 1 0
7 0 0 0 0 0 1

  2 3 4 5 6
1 0 0 0 0 0
2 1 0 0 0 0
3 0 1 0 0 0
4 0 0 1 0 0
5 0 0 0 1 0
6 0 0 0 0 1

However, when I tried to regress with:

summary(lm(log(y1)~ I(daydum==1) + I(weekdum==1)),
contrast=T)

Error in model.frame(formula, rownames, variables,
varnames, extras, extranames,  : 
        variable lengths differ 

I'm trying to find pairwise regression in order to
find each bi.  Can anyone hlep?  Thanks so much in
advance.

franc



From jfbrennan at rogers.com  Fri Aug 27 01:42:35 2004
From: jfbrennan at rogers.com (Jim Brennan)
Date: Thu, 26 Aug 2004 19:42:35 -0400
Subject: [R] Surprise when mapping matrix to image
References: <200408262147.i7QLlJBt017379@hypatia.math.ethz.ch><1093559760.412e65d0a85ee@www-auth.cs.wisc.edu>
	<85k6vlebqv.fsf@servant.blindglobe.net>
Message-ID: <014201c48bc6$5e03a860$3b8ac445@slnt.phub.net.cable.rogers.com>

I was also surprised by the image orientation this summer and used the
"easy" fix of matrix manipulation. There is however another issue and that
is when you start flipping around the matrix, orders etc. and for the case
you want to have sensibly labeled axes, you may have to use the axis
commands etc. which is also easy, but accumulation of easy fixes can be
tedious so there is perhaps some argument for yet another new documentation
submission. Maybe a method for easily changing the orientation in image
could be added.

Jim
----- Original Message -----
From: "A.J. Rossini" <rossini at blindglobe.net>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, August 26, 2004 7:11 PM
Subject: Re: [R] Surprise when mapping matrix to image


>
> I think I'd have to respectfully disagree with both Brian and
> Deepayan, as to whether it should be obvious.  It is reasonable
> (principle of least suprise) to expect orientation of the plot to
> match the print order of the matrix.  I would have expected Brian's
> one-liner to be in the help page, with a notice.  It's a not-so-rare
> activity, being a general matrix visualization that is commonly used
> in certain areas of science (whether it ought to be commonly used is a
> separate question).
>
> While "heatmap" might've been perhaps a better pointer, but it doesn't
> seem to do the "right" thing, either.  I.e.
>
> myTemp <- matrix(c(1,2,3,3,2,3),nrow=2)
> heatmap(myTemp,Rowv=NA,Colv=NA)
>
> doesn't look right to me (R Version 1.9.1  (2004-06-21))
>
> I see the "pixmap/bitmap" issue as a bit of a red herring, in this
> case.
>
> best,
> -tony
>
> p.s. I seem to get bit by this about once a year for the last few,
> hence why I'm speaking up.
>
>
>
>
>
>
>
>
> Deepayan Sarkar <deepayan at cs.wisc.edu> writes:
>
> > Quoting "Glynn, Earl" <EFG at Stowers-Institute.org>:
> >
> >> Prof Ripley:
> >>
> >> Thank you for your prompt reply.
> >>
> >> > It's pure convention: see below.
> >> >
> >> > Did you try reading the help for image?  You don't seem to
> >> > understand it
> >> > if you actually did.  It seems you are looking for
> >> >
> >> >  image(t(x)[ncol(x):1, ])
> >>
> >> I think you guys are too close to "R" to understand how hard it is to
> >> use sometimes.  What may be blatantly obvious to you is quite a problem
> >> especially to beginners.  Some of us may be beginners to R, but we know
> >> math, science, programming, and how to solve problems with other tools
> >> and languages.
> >>
> >> I re-read the guidelines before posting fearing condemnation.
> >>
> >> Before posting I searched the online R-help Google interface with
> >> keywords "image", "flip", "rotate".  A discussion from 1998 touched on
> >> this issue but I was hoping that this was deemed a "bug" at some point
> >> and fixed -- or had an easy workaround, like some parameter I was
> >> missing.
> >>
> >> I read the "?image" help before posting. Was the part I didn't
> >> understand buried in this "note"?
> >>
> >>      "Based on a function by Thomas Lumley tlumley at u.washington.edu."
> >
> > You seem to be thinking that Prof Ripley's solution had something to do
with
> > image(). It doesn't, it has to do with manipulating a matrix. image()
> > visualizes a matrix in a particular and well-defined way. You want your
matrix
> > to be shown in a different way, and one (simple) way of doing that is to
> > convert your matrix into a different matrix, on which calling image
would give
> > you what you want. Why would this be explained in ?image ? This is basic
R.
> >
> > More generally, I think your frustration is caused by your expectation
that a
> > matrix object should behave like a bitmap image. It doesn't. If you want
work
> > with images, use the pixmap package.
> >
> > Deepayan
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
> >
>
> --
> Anthony Rossini     Research Associate Professor
> rossini at u.washington.edu            http://www.analytics.washington.edu/
> Biomedical and Health Informatics   University of Washington
> Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
> UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
> FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email
>
> CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From hodgess at gator.uhd.edu  Fri Aug 27 01:55:28 2004
From: hodgess at gator.uhd.edu (Erin Hodgess)
Date: Thu, 26 Aug 2004 18:55:28 -0500
Subject: [R] expressions/approximate text
Message-ID: <200408262355.i7QNtSh18436@gator.dt.uh.edu>

Please forgive the general posting.

To get the x approximately = y,
please try
text(6,2,expression(x %~~% y))

HTH.

Sincerely,
Erin Hodgess
mailto: hodgess at gator.uhd.edu



From p.murrell at auckland.ac.nz  Fri Aug 27 02:05:26 2004
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Fri, 27 Aug 2004 12:05:26 +1200
Subject: [R] Label using equivalent of \mathbb{R}
References: <FB75CFC167F3D311B11D00A0CC20FB0E88556C@nts7.oec.Uni-Osnabrueck.DE>	<opsdcfwgp21pelvz@smtp.tcd.ie>	<16686.4768.62622.442505@gargle.gargle.HOWL>
	<200408261229.01753.deepayan@stat.wisc.edu>
Message-ID: <412E7AC6.3060807@stat.auckland.ac.nz>

Hi


Deepayan Sarkar wrote:
> On Thursday 26 August 2004 11:41, Martin Maechler wrote:
> 
>>>>>>>"Simon" == Simon Cullen <cullens at tcd.ie>
>>>>>>>    on Thu, 26 Aug 2004 15:30:06 +0100 writes:
>>>>>>
>>    Simon> On Thu, 26 Aug 2004 15:24:33 +0200, Trenkler,
>>    Simon> Dietrich <dtrenkler at nts6.oec.uni-osnabrueck.de>
>>
>>    Simon> wrote:
>>    >> [Dietrich Trenkler]
>>    >>
>>    >>
>>    >> plot(rnorm(10),xlab=expression(bold(x)),ylab=expression(bold(y)))
>>
>>    Simon> Not quite what I am looking for, I'm afraid. \mathbb
>>    Simon> gives "blackboard" fonts - the capitals with two
>>    Simon> vertical parallel lines in them that are used for the
>>    Simon> Reals, Complex numbers etc.
>>
>>yes (and Dieter confused \mathbb{} with \mathbf{}).
>>
>>R's builtin plotmath utilities can't do this directly.
>>A workaround would be similar to what we used to do in LaTeX 2.09
>>times (before \mathbb{} was standardly available):
>>
>>      { "I", <negative space>, "R" }
>>
>>Whereas with (La)TeX, a macro definition was relatively
>>easy for the above, it might be a bit harder / more ugly with
>>current R builtins, for one because I don't think ``plotmath''
>>has a concept of <negative space>.
>>1)  A really ugly hack would work with  text() -- not quite useful
>>   if you'd like these in 'xlab' etc.
>>
>>2)  A potentially much nicer scheme might be to use  "grid" instead of
>>   oldstyle  "graphics" :  There one could construct a  "grob"
>>   (graphical object) for the '|R' symbol that one could should be
>>   pass to other functions -- though it may need some tweaking
>>   before a function like (pkg lattice's) xyplot() would accept
>>   a grob for xlab [instead of only a character or expression].
> 
> 
> Interesting thought. Should be almost trivial to implement.
> 
> Currently, legends (sort of generalized key-s in the Trellis context) can be 
> grobs, e.g.
> 
> bbR <- textGrob(c("I", "R"), x = 0.7 * c(-1, 1), 
>                 default.units = "mm", 
>                 vp = viewport(h=0, w=0))
> 
> xyplot(1 ~ 1, xlab = NULL, legend = list(bottom = list(fun = bbR)))


Cool :)  An alternative hack would be to treat this as an annotation of 
the lattice panel.  For example ...

xyplot(1 ~ 1, xlab = "") # NOTE "" not NULL
downViewport("panel.1")
pushViewport(viewport(clip="off")) # to be able to draw outside panel
grid.text(c("I", "R"), y=unit(-2, "lines"),
           x=unit(0.5, "npc") - 0.5*stringWidth(c("I", "")),
           just="left")
upViewport(0)

...  (using stringWidth makes the hack scale with font size).

In this case though, I think the ideal solution would be just to use a 
font that has the desired character.  On Windows this should be doable 
via the file RHOME\etc\Rdevga and something like mtext(..., font=<some 
large number>) [assuming that the appropriate font can be found and 
installed].

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From gerifalte28 at hotmail.com  Fri Aug 27 02:30:06 2004
From: gerifalte28 at hotmail.com (F Z)
Date: Fri, 27 Aug 2004 00:30:06 +0000
Subject: [R] FIML in lme
Message-ID: <BAY2-F35FHLqbp3eccm000238e7@hotmail.com>

Hi

I was asked if lme can use FIML (Full Information Maximum Likelihood) 
instead of REML or ML but I don't know the answer.  Does anybody know if 
this is implemented in R?

Thanks

Francisco



From hb at maths.lth.se  Fri Aug 27 02:47:19 2004
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Fri, 27 Aug 2004 02:47:19 +0200
Subject: [R] Surprise when mapping matrix to image
In-Reply-To: <014201c48bc6$5e03a860$3b8ac445@slnt.phub.net.cable.rogers.com>
Message-ID: <001201c48bcf$6e51c400$8e0040d5@hblaptop>

I've got some simple code example at http://www.maths.lth.se/help/R/image/,
or just try:

 source("http://www.maths.lth.se/help/R/image/image.R")

Best wishes

Henrik Bengtsson


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jim Brennan
> Sent: Friday, August 27, 2004 1:43 AM
> To: rossini at u.washington.edu; r-help at stat.math.ethz.ch
> Subject: Re: [R] Surprise when mapping matrix to image
> 
> 
> I was also surprised by the image orientation this summer and 
> used the "easy" fix of matrix manipulation. There is however 
> another issue and that is when you start flipping around the 
> matrix, orders etc. and for the case you want to have 
> sensibly labeled axes, you may have to use the axis commands 
> etc. which is also easy, but accumulation of easy fixes can 
> be tedious so there is perhaps some argument for yet another 
> new documentation submission. Maybe a method for easily 
> changing the orientation in image could be added.
> 
> Jim
> ----- Original Message -----
> From: "A.J. Rossini" <rossini at blindglobe.net>
> To: <r-help at stat.math.ethz.ch>
> Sent: Thursday, August 26, 2004 7:11 PM
> Subject: Re: [R] Surprise when mapping matrix to image
> 
> 
> >
> > I think I'd have to respectfully disagree with both Brian and 
> > Deepayan, as to whether it should be obvious.  It is reasonable 
> > (principle of least suprise) to expect orientation of the plot to 
> > match the print order of the matrix.  I would have expected Brian's 
> > one-liner to be in the help page, with a notice.  It's a 
> not-so-rare 
> > activity, being a general matrix visualization that is 
> commonly used 
> > in certain areas of science (whether it ought to be 
> commonly used is a 
> > separate question).
> >
> > While "heatmap" might've been perhaps a better pointer, but 
> it doesn't 
> > seem to do the "right" thing, either.  I.e.
> >
> > myTemp <- matrix(c(1,2,3,3,2,3),nrow=2)
> > heatmap(myTemp,Rowv=NA,Colv=NA)
> >
> > doesn't look right to me (R Version 1.9.1  (2004-06-21))
> >
> > I see the "pixmap/bitmap" issue as a bit of a red herring, in this 
> > case.
> >
> > best,
> > -tony
> >
> > p.s. I seem to get bit by this about once a year for the last few, 
> > hence why I'm speaking up.
> >
> >
> >
> >
> >
> >
> >
> >
> > Deepayan Sarkar <deepayan at cs.wisc.edu> writes:
> >
> > > Quoting "Glynn, Earl" <EFG at Stowers-Institute.org>:
> > >
> > >> Prof Ripley:
> > >>
> > >> Thank you for your prompt reply.
> > >>
> > >> > It's pure convention: see below.
> > >> >
> > >> > Did you try reading the help for image?  You don't seem to 
> > >> > understand it if you actually did.  It seems you are 
> looking for
> > >> >
> > >> >  image(t(x)[ncol(x):1, ])
> > >>
> > >> I think you guys are too close to "R" to understand how 
> hard it is 
> > >> to use sometimes.  What may be blatantly obvious to you 
> is quite a 
> > >> problem especially to beginners.  Some of us may be 
> beginners to R, 
> > >> but we know math, science, programming, and how to solve 
> problems 
> > >> with other tools and languages.
> > >>
> > >> I re-read the guidelines before posting fearing condemnation.
> > >>
> > >> Before posting I searched the online R-help Google 
> interface with 
> > >> keywords "image", "flip", "rotate".  A discussion from 
> 1998 touched 
> > >> on this issue but I was hoping that this was deemed a 
> "bug" at some 
> > >> point and fixed -- or had an easy workaround, like some 
> parameter I 
> > >> was missing.
> > >>
> > >> I read the "?image" help before posting. Was the part I didn't 
> > >> understand buried in this "note"?
> > >>
> > >>      "Based on a function by Thomas Lumley 
> > >> tlumley at u.washington.edu."
> > >
> > > You seem to be thinking that Prof Ripley's solution had 
> something to 
> > > do
> with
> > > image(). It doesn't, it has to do with manipulating a matrix. 
> > > image() visualizes a matrix in a particular and well-defined way. 
> > > You want your
> matrix
> > > to be shown in a different way, and one (simple) way of 
> doing that 
> > > is to convert your matrix into a different matrix, on 
> which calling 
> > > image
> would give
> > > you what you want. Why would this be explained in ?image 
> ? This is 
> > > basic
> R.
> > >
> > > More generally, I think your frustration is caused by your 
> > > expectation
> that a
> > > matrix object should behave like a bitmap image. It 
> doesn't. If you 
> > > want
> work
> > > with images, use the pixmap package.
> > >
> > > Deepayan
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list 
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> > >
> >
> > --
> > Anthony Rossini     Research Associate Professor
> > rossini at u.washington.edu            
> http://www.analytics.washington.edu/
> > Biomedical and Health Informatics   University of Washington
> > Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer 
> Research Center
> > UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is 
> unreliable 
> > FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email
> >
> > CONFIDENTIALITY NOTICE: This e-mail message and any 
> > attachme...{{dropped}}
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From jeroschh at ohsu.edu  Fri Aug 27 02:55:17 2004
From: jeroschh at ohsu.edu (Michael Jerosch-Herold)
Date: Thu, 26 Aug 2004 17:55:17 -0700
Subject: [R] read.spss warning: unrecognized record type??
Message-ID: <s12e2412.032@ohsu.edu>

When using read.spss (library: 'foreign') I get the following warning
message:

Warning message: 
E:/R4win/mesamri.sav: Unrecognized record type 7, subtype 13
encountered in system file. 

I don't see anything wrong with record #7 in the database I am trying
to read in, but I suspect that the warning message does not refer to a
specific record, but a "variable" type. Is this correct? And what does
"subtype 13" mean?

I basically have a data table with records (cases) in rows, and various
variables for each record/case in the columns. Again, I suspect that I
am using "record" in a different sense then meant by the above warning
message.

Based on this warning, is "record type 7" discarded when the data are
read in?

Thank you in advance for shedding some light on this!

Michael Jerosch-Herold



From d.scott at auckland.ac.nz  Fri Aug 27 03:19:15 2004
From: d.scott at auckland.ac.nz (David Scott)
Date: Fri, 27 Aug 2004 13:19:15 +1200 (NZST)
Subject: [R] Plotting groupedData objects
In-Reply-To: <1093558961.412e62b1c674e@www-auth.cs.wisc.edu>
Message-ID: <Pine.LNX.4.44.0408271305540.31281-100000@hydra.stat.auckland.ac.nz>

On Thu, 26 Aug 2004, Deepayan Sarkar wrote:

> Quoting David Scott <d.scott at auckland.ac.nz>:
> 
> > On Thu, 26 Aug 2004, Deepayan Sarkar wrote:
> > 
> > > Quoting David Scott <d.scott at auckland.ac.nz>:
> > > 
> > > > 
> > > > I am trying to create a plot similar to Figure 3.2 in Bates and
> > Pinheiro.
> > > > 
> > > > I have repeated measurements on about 80 subjects from 2 treatment
> > groups.  
> > > > I would like to have the panels for the two treatment groups in separate
> > > > groups and within those groups have the panels ordered on maximum value
> > > > (as is the default).
> > > > 
> > > > I am ok with getting plots similar to Figs 3.1 and 3.2, but can't see how
> > 
> > > > to change the ordering of the panels to what I want.
> > > > 
> > > > Here is the definition of my groupedData object
> > > > 
> > > > RAWlmeData <- groupedData(RAW~Elapsed|ID,
> > > >               data=RAWData,
> > > >               labels=list(x="Elapsed time",y="Airways resistance"),
> > > >               units=list(x="(hours)",y="cm H20/L/sec"))
> > > > 
> > > > I guess I could just plot the two treatment groups separately in turn but
> > 
> > > > I feel there is something I am missing.
> > > 
> > > My reading of page 105 suggests that you need to specify 
> > > 'outer=<whatever your grouping factor is>'.
> > > Have you tried that?
> > > 
> > I did try that. It produces Fig 3.3 with multiple lines giving the results 
> > for each subject by treatment. With around 40 subjects per treatment 
> > group, that isn't very attractive in my case.
> 
> I should have been more specific, but I thought this was clear enough in the
> book. I was talking about specifying outer in the groupedData() constructor,
> not in the plot() call. Have you tried _that_?
> 
> > I should also mention that I looked for on-line complements to Pinheiro 
> > and Bates, and found MEMSS.tar.gz on 
> > http://cm.bell-labs.com/cm/ms/departments/sia/NLME/MEMSS/index.html
> > which promised scripts for the examples in the book but turned out to have 
> > only chapters 1 and 2, and for S-PLUS rather than R (going on the .q 
> > extension).
> 
> The R scripts are, naturally enough, bundled with the R version of the package.
> They should be in the scripts/ subdirectory of your nlme installation.
> 
Thanks Deepayan. I didn't read that section closely enough. Defining an 
outer factor in the constructor did what I wanted.

I also found the scripts as you suggested although they didn't include 
code for the production of Fig 3.2 as far as I could see.


David Scott









_________________________________________________________________
David Scott	Department of Statistics, Tamaki Campus
		The University of Auckland, PB 92019
		Auckland	NEW ZEALAND
Phone: +64 9 373 7599 ext 86830		Fax: +64 9 373 7000
Email:	d.scott at auckland.ac.nz 


Graduate Officer, Department of Statistics



From deepayan at cs.wisc.edu  Fri Aug 27 03:26:53 2004
From: deepayan at cs.wisc.edu (Deepayan Sarkar)
Date: Thu, 26 Aug 2004 20:26:53 -0500
Subject: [R] introduction slides for beginners
In-Reply-To: <Pine.LNX.4.58.0408260859370.28463@public02.stat.wisc.edu>
References: <412DE4C3.3000906@xss.de>
	<Pine.LNX.4.58.0408260859370.28463@public02.stat.wisc.edu>
Message-ID: <1093570013.412e8ddd1ac2b@www-auth.cs.wisc.edu>

Quoting Deepayan Sarkar <deepayan at cs.wisc.edu>:

> On Thu, 26 Aug 2004, Armin Roehrl wrote:
> 
> > Hi all,
> >
> >     does anybody have slides for a 1 to 3 hour crash course into R,
> > that I would be allowed to recycle to show a few people the light?
> 
> I have some stuff I used for 5 x 1 hour presentations at
> 
> http://www.stat.wisc.edu/~deepayan/SIBS/slides/
> 
> meant to complement the first chapter of 'An Introduction to R'. 

Umm, for the record, I meant to say 'Introductory Statistics with R' here.

Deepayan



From ok at cs.otago.ac.nz  Fri Aug 27 04:19:27 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Fri, 27 Aug 2004 14:19:27 +1200 (NZST)
Subject: [R] Surprise when mapping matrix to image
Message-ID: <200408270219.i7R2JRQN413645@atlas.otago.ac.nz>

Deepayan Sarkar <deepayan at cs.wisc.edu> wrote:
	You seem to be thinking that Prof Ripley's solution had
	something to do with image().

"Glynn, Earl" <EFG at Stowers-Institute.org>
could certainly be excused for thinking so, because
what Prof Ripley wrote included this:

	> > Did you try reading the help for image?  You don't seem to 
	> > understand it if you actually did.

That certainly sounds to me as though the answer should have been
obvious from ?image

Now, the image() function is trying to be extremely helpful and offers
you lots of ways of providing the data, and that is explained in ?image.

In fact a key piece of information *IS* in ?image, so it really DOES
have something to do with image():

    Details:

     The length of 'x' should be equal to the 'nrow(z)+1' or 'nrow(z)'.
     In the first case 'x' specifies the boundaries between the cells:
     in the second case 'x' specifies the midpoints of the cells. 

This tells us that the "x" direction of the image goes along the *rows*
of the matrix and by implication that the "y" direction goes along the
*columns* of the matrix, and is precisely what you need to figure out
that image(t(x)) is the thing to use.

One does have to be a little subtle at reading between the lines to see
the relevance of details about a parameter one is not passing, but the
information _is_ there and it _is_ 'something to do with image()', not
something to do with matrices.

The 'volcano' example from ?image also points the same way.

The basic problem here is that the standard visual layout of
matrices sets up such a strong expectation that it's difficult for
any text to override it.

I don't see anything that explicitly discusses the direction of the
axes, I don't see anything explicit, but I took it that ?image would
follow the usual "horizontal axis increases from left to right,
vertical axis increases from bottom to top" convention, which means
"horizontal gives ROW number increasing left to right,
 vertical gives COLUMN number increasing bottom to top"
so when I do image(t(x)) I see my columns as vertical strips but
they are "upside down", so the last step, to
    image(t(x)[ncol(x):1,])
does make sense, provided you read ?image as *implying* without actually
stating "follows the usual graphical conventions for axes".  But again,
this really is a fact about image(), not about matrices.

As for the claim that
	It doesn't, it has to do with manipulating a matrix. image()
	visualizes a matrix in a particular and well-defined way.

Well yes, it is a particular way, and I suppose you could say that the
code is defined well, but it really isn't, when you get right down to
it, and *EXPLICITLY* documented way.  You really have to work at it.

This is NOT a criticism of R documentation.  Of all the open-source
programs I use, I reckon R is by far the best documented, and its
on-line documentation can more than withstand comparison against most
of the commercial programs I've seen.  The real problem is that R is
just so big and there are only so many people writing documentation for
it.

Given the wide range of ways that the x, y, z arguments can be passed
to image(), it would actually make sense to have some kind of flip and/or
mirror operations specified via an argument to image().
The source code of image is available (image.default) so it wouldn't be
a lot of work for someone who wants it to produce such a thing.



From andy_liaw at merck.com  Fri Aug 27 04:39:43 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 26 Aug 2004 22:39:43 -0400
Subject: [R] predict.mvr error message
Message-ID: <3A822319EB35174CA3714066D590DCD504AF82AA@usrymx25.merck.com>

What version of R, what version of pls.pcr, and on what OS?  Have you
checked whether your versions of software are up to date?  I get:

> n <- 1350
> p <- 180
> y <- rnorm(n)
> x <- matrix(sample(0:1, n*p, replace=TRUE), n, p)
> fit <- mvr(x, y, method="SIMPLS", validat="none", ncomp=2)
> xt <- matrix(sample(0:1, 312*p, replace=TRUE), 312, p)
> yt <- predict(fit, xt)


Andy

> From: Stewart T Chang
> 
> Greetings,
> 
> I've encountered an error message while using the pls.pcr package
> that's left me scratching my head for a couple of hours, and 
> I thought I
> might post it here in the hopes that someone else uses this package.
> Using a data set of 1350 observations (each observation comprises 180
> predictor variables and 1 output variable), I trained a model using
> pls.pcr.  (Other details: ncomp=2, 
> method="SIMPLS",validation="none".  I
> might also add that each predictor variable has a value of 
> either 0 or 1,
> though the output variable is a real number.)
> '
> When I tried to use this model to make predictions (on 312 sets of 180
> predictor variables), I received the following error:
> 
> Error in X %*% object$training$B[, , index, drop = TRUE] :
>         requires numeric matrix/vector arguments
> 
> Normally I'd ask a friend of mine in Biostatistics here to 
> help me out,
> but he's currently away!  Have any of you used pls.pcr and gotten this
> kind of message?  Thanks!
> 
> Stewart
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From stchang at umich.edu  Fri Aug 27 04:50:56 2004
From: stchang at umich.edu (Stewart T Chang)
Date: Thu, 26 Aug 2004 22:50:56 -0400 (EDT)
Subject: [R] predict.mvr error message
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF82AA@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF82AA@usrymx25.merck.com>
Message-ID: <Pine.SOL.4.58.0408262241430.18266@timepilot.gpcc.itd.umich.edu>

Well, I figured it out on my own.  It turns out, even though my test set
of data appeared to be a matrix, it wasn't.  Adding "as.matrix" to the
front of the name of the data set solved the problem.  (Maybe there's some
alternative way of doing it, such as yours?)  Thanks for your suggestion.

Stewart



On Thu, 26 Aug 2004, Liaw, Andy wrote:

> What version of R, what version of pls.pcr, and on what OS?  Have you
> checked whether your versions of software are up to date?  I get:
>
> > n <- 1350
> > p <- 180
> > y <- rnorm(n)
> > x <- matrix(sample(0:1, n*p, replace=TRUE), n, p)
> > fit <- mvr(x, y, method="SIMPLS", validat="none", ncomp=2)
> > xt <- matrix(sample(0:1, 312*p, replace=TRUE), 312, p)
> > yt <- predict(fit, xt)
>
>
> Andy
>
> > From: Stewart T Chang
> >
> > Greetings,
> >
> > I've encountered an error message while using the pls.pcr package
> > that's left me scratching my head for a couple of hours, and
> > I thought I
> > might post it here in the hopes that someone else uses this package.
> > Using a data set of 1350 observations (each observation comprises 180
> > predictor variables and 1 output variable), I trained a model using
> > pls.pcr.  (Other details: ncomp=2,
> > method="SIMPLS",validation="none".  I
> > might also add that each predictor variable has a value of
> > either 0 or 1,
> > though the output variable is a real number.)
> > '
> > When I tried to use this model to make predictions (on 312 sets of 180
> > predictor variables), I received the following error:
> >
> > Error in X %*% object$training$B[, , index, drop = TRUE] :
> >         requires numeric matrix/vector arguments
> >
> > Normally I'd ask a friend of mine in Biostatistics here to
> > help me out,
> > but he's currently away!  Have any of you used pls.pcr and gotten this
> > kind of message?  Thanks!
> >
> > Stewart
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
>
>
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachments, contains information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New Jersey, USA 08889), and/or its affiliates (which may be known outside the United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as Banyu) that may be confidential, proprietary copyrighted and/or legally privileged. It is intended solely for the use of the individual or entity named on this message.  If you are not the intended recipient, and have received this message in error, please notify us immediately by reply e-mail and then delete it from your system.
> ------------------------------------------------------------------------------
>
>
>



From andy_liaw at merck.com  Fri Aug 27 05:08:07 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 26 Aug 2004 23:08:07 -0400
Subject: [R] Surprise when mapping matrix to image
Message-ID: <3A822319EB35174CA3714066D590DCD504AF82AB@usrymx25.merck.com>

> From: Richard A. O'Keefe
[snip]
> Given the wide range of ways that the x, y, z arguments can be passed
> to image(), it would actually make sense to have some kind of 
> flip and/or
> mirror operations specified via an argument to image().
> The source code of image is available (image.default) so it 
> wouldn't be
> a lot of work for someone who wants it to produce such a thing.

I'd suggest adding an `as.matrix' argument to image.default.

Best,
Andy



From deepayan at cs.wisc.edu  Fri Aug 27 05:16:23 2004
From: deepayan at cs.wisc.edu (Deepayan Sarkar)
Date: Thu, 26 Aug 2004 22:16:23 -0500
Subject: [R] Surprise when mapping matrix to image
In-Reply-To: <200408270219.i7R2JRQN413645@atlas.otago.ac.nz>
References: <200408270219.i7R2JRQN413645@atlas.otago.ac.nz>
Message-ID: <1093576583.412ea7876ee02@www-auth.cs.wisc.edu>

Quoting "Richard A. O'Keefe" <ok at cs.otago.ac.nz>:

> Deepayan Sarkar <deepayan at cs.wisc.edu> wrote:
>  You seem to be thinking that Prof Ripley's solution had
>  something to do with image().
> 
> "Glynn, Earl" <EFG at Stowers-Institute.org>
> could certainly be excused for thinking so, because
> what Prof Ripley wrote included this:
> 
>  > > Did you try reading the help for image?  You don't seem to 
>  > > understand it if you actually did.
> 
> That certainly sounds to me as though the answer should have been
> obvious from ?image

I agree with you that the issues are related, but your (edited) quoting is
misleading. I interpret Prof Ripley's comment as implying that reading ?image
would tell the user what happens when he calls image(x). This is distinct from
how to manipulate a matrix, which is what t(x)[ncol(x):1, ] is doing. It
doesn't seem obvious to me that this information should be in ?image, and I
still don't see how Prof Ripley's comment suggested that it was. Anything but
the most trivial use of S involves combining different ideas. One can hardly
expect all possible interactions to be documented.

Of course, documentation can always use improvements (and the rest of your
comments on ?image are all valid), and it would be nice to anticipate
frequently faced problems and offer solutions preemptively, but that's a
different issue.

Deepayan



From ok at cs.otago.ac.nz  Fri Aug 27 06:22:15 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Fri, 27 Aug 2004 16:22:15 +1200 (NZST)
Subject: [R] Surprise when mapping matrix to image
Message-ID: <200408270422.i7R4MFF7409020@atlas.otago.ac.nz>

Deepayan Sarkar <deepayan at cs.wisc.edu> wrote:
	I agree with you that the issues are related, but your (edited) quoting is
	misleading.

My quotes were selective but not otherwise edited.

	I interpret Prof Ripley's comment as implying that reading ?image
	would tell the user what happens when he calls image(x).

And that's how I interpreted it too.

	This is distinct from how to manipulate a matrix, which is what
	t(x)[ncol(x):1, ] is doing.  It doesn't seem obvious to me that this
	information should be in ?image,

Nobody whatsoever has suggested that the matrix manipulation code *SHOULD*
be in ?image.   Nobody.  Not me.  Not anyone else.  The original poster's
problem was not "how do I manipulate a matrix" but "why is the image the
wrong way around".

	and I still don't see how Prof Ripley's
	comment suggested that it was.

It didn't.  NOBODY has made any such suggestion.

	One can hardly expect all possible interactions to be documented.
	
Nobody has suggested that either.

All I was saying is this:
(1) The original poster's problem was a problem about image(),
    NOT a problem about matrix manipulation.
(2) We have no reason whatsoever to assert, hint, believe, or opine
    that the original poster was any less competent at matrix manipulation
    per se than anyone else reading this mail list.  He didn't have a
    matrix problem.
(3) The problem basically is precisely where Prof Ripley located it:
    a clash between
    (a) an extremely strong "cultural" expectation that the elements
        of a matrix will be presented with rows running from 1 at top
        to m at bottom and columns running from 1 at left to n at right, and
    (b) an extremely strong "cultural" expectation that the vertical
        axis of a plot runs from low at the bottom to high at the top, and
    (c) the fact that image() maps the "row" axis of a matrix to the
        horizontal axis of a plot and the "column" axis of a matrix to
        the vertical axis of a plot.
    Expectations (a) and (b) are not only part of general computing,
    they are strongly supported by R itself.  As for (c), it doesn't
    seem to me that it _had_ to be that way.
(4) Point (c) is a sufficiently strange quirk of image() that it is
    a stumbling block waiting to trip people up.  I've stumbled over
    it myself, which is how come I knew where to look in ?image.
    That makes it *essential* that it should be documented.
(5) And it *IS* documented.  It really is an image() problem, not a
    matrix problem, and the essential part of it *IS* documented.

Pointing out that a obvious nasty interaction *is* documented is
hardly a demand that all possible interactions should be documented.

How about this addition to the documentation?

    "The x axis corresponds to the rows of the matrix (first on the left
     to last on the right).  The y axis corresponds to the columns of the
     matrix (first at the bottom to last at the top)."

Once you know what image() is _intended_ to do with a matrix,
you can figure out the transposing and reversing you need for any
other view.  It's knowing you need to that's the problem.



From rossini at blindglobe.net  Fri Aug 27 06:59:21 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Thu, 26 Aug 2004 21:59:21 -0700
Subject: [R] Surprise when mapping matrix to image
In-Reply-To: <200408270422.i7R4MFF7409020@atlas.otago.ac.nz> (Richard A.
	O'Keefe's message of "Fri, 27 Aug 2004 16:22:15 +1200 (NZST)")
References: <200408270422.i7R4MFF7409020@atlas.otago.ac.nz>
Message-ID: <85llg1tbva.fsf@servant.blindglobe.net>

"Richard A. O'Keefe" <ok at cs.otago.ac.nz> writes:

> Once you know what image() is _intended_ to do with a matrix,
> you can figure out the transposing and reversing you need for any
> other view.  It's knowing you need to that's the problem.

This is a much better statement of my point, and precisely why it
keeps (though only occassionally) burning me.

best,
-tony

-- 
Anthony Rossini			    Research Associate Professor
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From nleonard at tartarus.uwa.edu.au  Fri Aug 27 07:03:45 2004
From: nleonard at tartarus.uwa.edu.au (Neil Leonard)
Date: Fri, 27 Aug 2004 13:03:45 +0800
Subject: [R] Keyboard input into functions
In-Reply-To: <opsdchgbkl1pelvz@smtp.tcd.ie>
References: <FB75CFC167F3D311B11D00A0CC20FB0E885571@nts7.oec.Uni-Osnabrueck.DE>
	<opsdchgbkl1pelvz@smtp.tcd.ie>
Message-ID: <797B336F-F7E6-11D8-8BC2-003065D5B8EC@tartarus.uwa.edu.au>

Hi,

Does anybody know if it is possible to have keyboard input into 
functions? Kind of like 'scanf()' in C.

I want to write a function that asks me what to put in certain 
variables.


Thanks,
Neil



From Wanzare at HCJP.com  Fri Aug 27 07:11:14 2004
From: Wanzare at HCJP.com (Manoj - Hachibushu Capital)
Date: Fri, 27 Aug 2004 14:11:14 +0900
Subject: [R] Keyboard input into functions
Message-ID: <1CBA12F2D414914989C723D196B287DC26BCD1@jp-svr-ex1.hcjp.com>

?readline

HTH
-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Neil Leonard
Sent: Friday, August 27, 2004 2:04 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Keyboard input into functions

Hi,

Does anybody know if it is possible to have keyboard input into 
functions? Kind of like 'scanf()' in C.

I want to write a function that asks me what to put in certain 
variables.


Thanks,
Neil

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Fri Aug 27 08:50:59 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 27 Aug 2004 07:50:59 +0100 (BST)
Subject: [R] Error TukeyHSD
In-Reply-To: <412E6943.6020307@sdsc.edu>
Message-ID: <Pine.LNX.4.44.0408270747580.30532-100000@gannet.stats>

You are trying to apply TukeyHSD to a linear regression.  I think you want
`diet' to be a factor, but it is not.  (The warning message told you
that.) The model needs to be

coag.mod <- aov(time ~ factor(diet), data=coag)

Function aov happily accepts continuous covariates as it can also do
ANCOVA.

On Thu, 26 Aug 2004, T. Murlidharan Nair wrote:

> I am running the following code on the coagulation data and I am getting 
> an error. Please let me know
> if I am missing anything from my code.
> 
> coag<- matrix( scan("//Samba3/nair/R/blood.dat", sep=","), 24, 3, 
> byrow=TRUE)
> colnames(coag) <- c("time","diet","order")
> coag <- as.data.frame(coag)
> oneway.test(time ~  diet, data=coag, var.eq=TRUE)
> coag.mod <- aov(time ~ diet, data=coag)
> options(scipen=5)
> options(digits=5)
> TukeyHSD( coag.mod )
> 
> 
> I get the following error when I run the above code
> Read 72 items
> Error in rep.int(n, length(means)) : Unimplemented feature in rep
> In addition: Warning message:
> non-factors ignored: diet in: replications(paste("~", 
> paste(names(tables), collapse = "+")),

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Fri Aug 27 08:46:57 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 27 Aug 2004 08:46:57 +0200
Subject: [R] Surprise when mapping matrix to image
In-Reply-To: <200408270422.i7R4MFF7409020@atlas.otago.ac.nz>
References: <200408270422.i7R4MFF7409020@atlas.otago.ac.nz>
Message-ID: <x2pt5d13j2.fsf@biostat.ku.dk>

"Richard A. O'Keefe" <ok at cs.otago.ac.nz> writes:

> How about this addition to the documentation?
> 
>     "The x axis corresponds to the rows of the matrix (first on the left
>      to last on the right).  The y axis corresponds to the columns of the
>      matrix (first at the bottom to last at the top)."
> 
> Once you know what image() is _intended_ to do with a matrix,
> you can figure out the transposing and reversing you need for any
> other view.  It's knowing you need to that's the problem.

Good idea, but perhaps not phrased sharply enough to catch the user's
eye. How about something like this:

"Notice that image() interprets a matrix as a table of f(x_i, y_j), so
the x axis corresponds to row number and the y axis to column number
(bottom to top)."

and then use Brian's one-liner in the examples section?

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Fri Aug 27 08:55:46 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 27 Aug 2004 07:55:46 +0100 (BST)
Subject: [R] coplot and par
In-Reply-To: <BAY18-F11XCpU6q5ijb0001c543@hotmail.com>
Message-ID: <Pine.LNX.4.44.0408270751220.30532-100000@gannet.stats>

On Thu, 26 Aug 2004, Mihai Nica wrote:

> R 1.9.1 on Win2000 or Win98SE.
> 
> I am using coplot as follows:
> 
> coplot(AVG~LRPI| REGION)
> 
> the output seems normal but I get:
> 
> "Warning message:
> calling par(new=) with no plot"

>From the NEWS file for R-patched:

    o	coplot(..) doesn't give an extraneous warning anymore when called
	on a fresh device.

so it is a bug that is fixed in the latest version (which you cna find on 
CRAN).
> 
> This is the only explanation that I have for being unable to use par() with 
> coplot for changing the way the xlab and ylab appears. 

xlab and ylab are not part of par -- see its help page.

> From within coplot I can change the text itself but not the font,
> fontsize, etc, but par() before coplot has absolutely no effect
> wahtsoever (maybe it isn't supposed to?).
> 
> I guess the question is: how can one change the way the xlab and ylab appear 
> on a coplot?

They are listed as arguments to coplot, and work for me when given there.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Aug 27 09:07:52 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 27 Aug 2004 08:07:52 +0100 (BST)
Subject: [R] Keyboard input into functions
In-Reply-To: <1CBA12F2D414914989C723D196B287DC26BCD1@jp-svr-ex1.hcjp.com>
Message-ID: <Pine.LNX.4.44.0408270806140.30532-100000@gannet.stats>

On Fri, 27 Aug 2004, Manoj - Hachibushu Capital wrote:

> ?readline

Yes, but a nearer equivalent to scanf in C is scan() which reads from the 
keyboard by default.

> -----Original Message-----
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Neil Leonard
> Sent: Friday, August 27, 2004 2:04 PM
> 
> Does anybody know if it is possible to have keyboard input into 
> functions? Kind of like 'scanf()' in C.
> 
> I want to write a function that asks me what to put in certain 
> variables.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From petr.pikal at precheza.cz  Fri Aug 27 09:21:39 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 27 Aug 2004 09:21:39 +0200
Subject: [R] Surprise when mapping matrix to image
In-Reply-To: <200408262147.i7QLlJBt017379@hypatia.math.ethz.ch>
Message-ID: <412EFD23.249.3B1DE9@localhost>



On 26 Aug 2004 at 16:47, Glynn, Earl wrote:

<snip>

> I'll go back to lurking in the daily R-Helps and not ask any more
> questions until I've read all the old R-help messages.  I'm working on
> December 1998 right now and reading forward.  Perhaps by next year
> I'll will have read all the old R-help postings and I'll dare ask
> another question then.

Hi

When I do not find something in help page and a function does not 
do what I want I usually consult Paul Johnson's Rtips (or 
StatsRUs). Its a first hit by Google "statsrus". There you can find 
impresive collection of hints neatly arranged. And probably after a 
while you will collect some usefull hints yourself (as I myself 
collected).

Cheers.
Petr

> 
> efg
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From p.dalgaard at biostat.ku.dk  Fri Aug 27 09:17:45 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 27 Aug 2004 09:17:45 +0200
Subject: [R] read.spss warning: unrecognized record type??
In-Reply-To: <s12e2412.032@ohsu.edu>
References: <s12e2412.032@ohsu.edu>
Message-ID: <x2llg1123q.fsf@biostat.ku.dk>

"Michael Jerosch-Herold" <jeroschh at ohsu.edu> writes:

> When using read.spss (library: 'foreign') I get the following warning
> message:
> 
> Warning message: 
> E:/R4win/mesamri.sav: Unrecognized record type 7, subtype 13
> encountered in system file. 
> 
> I don't see anything wrong with record #7 in the database I am trying
> to read in, but I suspect that the warning message does not refer to a
> specific record, but a "variable" type. Is this correct? And what does
> "subtype 13" mean?

Well foreign doesn't know either, and that's the problem... Most
likely you have one of your SPSS variables coded in an unusual manner
(do they all come though alright?), or it is something else in the
meta-info that foreign cannot interpret.
 
> I basically have a data table with records (cases) in rows, and various
> variables for each record/case in the columns. Again, I suspect that I
> am using "record" in a different sense then meant by the above warning
> message.

Yes. System files are typically organised in "records" which are not
the actual data records but pieces of meta-information. As in (I'm
just making this up, read the sources for the exact definitions)

type=1 subtype=1 length=22 data1="My very important data"

which is compactly stored as (<..> meaning "binary coding of")

<1><1><22>My very...

so that on reading, the program sees the first two bytes and then
knows that this is a "main header" and that it will be followed by a
2-byte length and then as many characters as the length indicates.

There are other record types for individual variable names, variable
types, variable labels, value labels for categorical data, etc...

 
> Based on this warning, is "record type 7" discarded when the data are
> read in?

Possibly, but you have the data to check and we don't....
 
> Thank you in advance for shedding some light on this!

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Fri Aug 27 09:33:25 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 27 Aug 2004 08:33:25 +0100 (BST)
Subject: [R] coplot and par
In-Reply-To: <Pine.LNX.4.44.0408270751220.30532-100000@gannet.stats>
Message-ID: <Pine.LNX.4.44.0408270822590.30632-100000@gannet.stats>

On Fri, 27 Aug 2004, Prof Brian Ripley wrote:

> On Thu, 26 Aug 2004, Mihai Nica wrote:

[...]

> > This is the only explanation that I have for being unable to use par() with 
> > coplot for changing the way the xlab and ylab appears. 
> 
> xlab and ylab are not part of par -- see its help page.
> 
> > From within coplot I can change the text itself but not the font,
> > fontsize, etc, but par() before coplot has absolutely no effect
> > wahtsoever (maybe it isn't supposed to?).
> > 
> > I guess the question is: how can one change the way the xlab and ylab appear 
> > on a coplot?
> 
> They are listed as arguments to coplot, and work for me when given there.

Although you didn't mention them at all, did you mean specifying cex.lab 
and font.lab?   They do not work as the xlab and ylab in coplot are not
`x and y labels' in the sense used in ?par: they are placed by mtext() not 
title().

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Fri Aug 27 09:37:12 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 27 Aug 2004 09:37:12 +0200
Subject: [R] text() with text, variables and math HOWTO?
In-Reply-To: <20040826162940.5ef5149d@localhost>
References: <20040826162940.5ef5149d@localhost>
Message-ID: <412EE4A8.2090503@statistik.uni-dortmund.de>

Johannes Graumann wrote:

> Hello,
> 
> One more question from the 'abusing R for blotting - particularly
> anally' department:
> How can I in the expression below make the '%~~%' show up as the
> aprrox-sign I want it to be?
> 
> Thanks for any hint,
> 
> Joh
> 
> text(
> 	500,1.5,
> 	cex=0.75,
> 	substitute(
> 		paste(
> 			OD[600][~nm],
> 			" of 1 at ",
> 			time1,
> 			" min ",
> 			"%~~%",
> 			time1h,
> 			"h"
> 		),
> 		list(
> 			time1=round(time1,digits=0),
> 			time1h=round(time1/60,digits=1)
> 		)
> 	)
> )


   substitute(OD[600][~nm] * " of 1 at " *
     time1 * " min" %~~% time1h * h,
       list(time1=round(time1,digits=0),
            time1h=round(time1/60,digits=1))
   )



Uwe Ligges



> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From rasch at med1.med.tum.de  Fri Aug 27 09:35:07 2004
From: rasch at med1.med.tum.de (Raphael Schneider)
Date: Fri, 27 Aug 2004 09:35:07 +0200
Subject: [R] read.spss warning: unrecognized record type??
In-Reply-To: <s12e2412.032@ohsu.edu>
References: <s12e2412.032@ohsu.edu>
Message-ID: <200408270935.07786.rasch@med1.med.tum.de>

On Friday 27 August 2004 02:55, Michael Jerosch-Herold wrote:
> When using read.spss (library: 'foreign') I get the following warning
> message:
>
> Warning message:
> E:/R4win/mesamri.sav: Unrecognized record type 7, subtype 13
> encountered in system file.
>
> I don't see anything wrong with record #7 in the database I am trying
> to read in, but I suspect that the warning message does not refer to a
> specific record, but a "variable" type. Is this correct? And what does
> "subtype 13" mean?
I get the same message but it seems that all data is read correctly. My 
impression is that this message shows up only reading datasets stored with 
SPSS12. At the moment I have no dataset where I know exactly this was stored 
using a SPSS version below 12.

Raphael



From thpe at hhbio.wasser.tu-dresden.de  Fri Aug 27 09:53:00 2004
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Fri, 27 Aug 2004 09:53:00 +0200
Subject: [R] text() with text, variables and math HOWTO?
In-Reply-To: <20040826162940.5ef5149d@localhost>
References: <20040826162940.5ef5149d@localhost>
Message-ID: <412EE85C.9020009@hhbio.wasser.tu-dresden.de>

Johannes Graumann wrote:
> Hello,
> 
> One more question from the 'abusing R for blotting - particularly
> anally' department:
> How can I in the expression below make the '%~~%' show up as the
> aprrox-sign I want it to be?
> 
> Thanks for any hint,

Your code does not work because "%~~%" is a character string and %~~% is 
an invalid mathematical expression. The operator (e.g. %~~%) must be 
between something, so it works if you use phantom(0)%~~%phantom(0), and 
I found that phantom() without 0 works too:

See the following:


plot(0, xlim=c(0,1000), ylim=c(0,2), type="n")

time1=99

text(
	500,1.5,
	cex=0.75,
	substitute(
		paste(
			OD[600][~nm],
			" of 1 at ",
			time1,
			" min ",
			phantom()%~~%phantom(),
			time1h,
			"h"
		),
		list(
			time1=round(time1,digits=0),
			time1h=round(time1/60,digits=1)
		)
       )
)

Hope it helps

Thomas P.



From Roger.Bivand at nhh.no  Fri Aug 27 09:58:43 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 27 Aug 2004 09:58:43 +0200 (CEST)
Subject: [R] Newbie Question:  Spatial Autocorrelation with R Tutorial?
In-Reply-To: <9056D760A8B1EA4F91CB9CE194C5210A3BDBEF@sentinel.edc.uri.edu>
Message-ID: <Pine.LNX.4.44.0408270933110.20757-100000@reclus.nhh.no>

On Wed, 25 Aug 2004, Jeff Hollister wrote:

> Howdy All,
> 
> I am looking for some good tutorials (books, websites, whatever) for
> calculating/testing for Spatial Autocorrelation using R.
> 
> Specifically, I am wanting to test for autocorrelation of a number of
> variables measured at a set of discrete locations.
> 

>From your signature line, "Environmental Data", spatial autocorrelation 
could mean a number of things, depending on whether the variables could be 
measurements of a continuous surface of values at your discrete locations, 
or whether the discrete locations are "spatial entities" formed as areal 
aggregations of some kind. Since you mention spdep below, I'm assuming 
that the data you are working on refer to "spatial entities", for which 
Moran's I would be a reasonable choice of test. If the variable of 
interest isn't of this form, then other packages are more relevant (see R 
spatial projects link below).

> Up to this point I have been exploring the "spdep" package and I can get
> "moran.test" to work, but I am concerned that somewhere along the line I
> may not be doing things correctly.  Hence my request for a tutorial so
> that I may brush up on my autocorrelation basics, specifically
> autocorrelation with R, and reassure myself that the results I am
> getting aren't bogus.

Admittedly, the help page for moran.test() simply refers to Cliff, A. D., 
Ord, J. K. 1981 Spatial processes, Pion, p. 21 as the original source, and 
the "sids" vignette (see the foot of the output of help(package=spdep) to 
locate it on your system) is incomplete. My guess is however that if your 
data are for "spatial entities", theb constructing a sensible 
neighbour weights is at least 75% of the work - you will also see this in 
Virgilio G??mez-Rubio's "DCluster" package, and the existing "sids" 
vignette does cover that a little. Completing and improving this vignette 
is on my TODO list.

If you are unsure of the result, and want to stay within the R framework, 
consider calculating Moran's I using DCluster, or gearymoran() in "ade4". 
Beyond that, you could access the GeoDa software (Windows, not R) and 
documentation at http://sal.agecon.uiuc.edu/csiss/geoda.html, the site 
also housing the R spatial projects web pages:

http://agec221.agecon.uiuc.edu/csiss/Rgeo/

Please contact me off-list, or on the R-sig-geo list if you feel that 
would help.

Best wishes,

Roger Bivand

> 
> Thanks in advance for any suggestions!
> 
> Jeff Hollister
>   
> *****************************************************
> Jeffrey William Hollister                          
> Ph.D. Candidate                                      
> Environmental Data Center                       
> Department of Natural Resources Science  
> University of Rhode Island                           
> office: (401) 874 5054
> fax: (401) 874 4561
> cell: (401)556 4087
> http://www.edc.uri.edu/personal/jeff/home/jwh_cv_full.htm
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From pburns at pburns.seanet.com  Fri Aug 27 11:32:46 2004
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Fri, 27 Aug 2004 10:32:46 +0100
Subject: [R] Surprise when mapping matrix to image
In-Reply-To: <x2pt5d13j2.fsf@biostat.ku.dk>
References: <200408270422.i7R4MFF7409020@atlas.otago.ac.nz>
	<x2pt5d13j2.fsf@biostat.ku.dk>
Message-ID: <412EFFBE.6030703@pburns.seanet.com>



Peter Dalgaard wrote:

>"Richard A. O'Keefe" <ok at cs.otago.ac.nz> writes:
>  
>
[snip]

>  
>
>
>Good idea, but perhaps not phrased sharply enough to catch the user's
>eye. How about something like this:
>
>"Notice that image() interprets a matrix as a table of f(x_i, y_j), so
>the x axis corresponds to row number and the y axis to column number
>(bottom to top)."
>
>and then use Brian's one-liner in the examples section?
>  
>

Though I think the added text is a good idea, the addition of the example
is probably 102.58 times more useful than any text that could be inserted.

The examples in help files are probably the main reason that people think
of R having better documentation than other programs.  Like Unix man
pages, for example, which would be wonderful if only they had examples
so that you could figure out what the sentences mean when you understand
every word.

Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")



From p.dalgaard at biostat.ku.dk  Fri Aug 27 11:35:14 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 27 Aug 2004 11:35:14 +0200
Subject: [R] Surprise when mapping matrix to image
In-Reply-To: <412EFFBE.6030703@pburns.seanet.com>
References: <200408270422.i7R4MFF7409020@atlas.otago.ac.nz>
	<x2pt5d13j2.fsf@biostat.ku.dk> <412EFFBE.6030703@pburns.seanet.com>
Message-ID: <x2pt5crkj1.fsf@biostat.ku.dk>

Patrick Burns <pburns at pburns.seanet.com> writes:

> Peter Dalgaard wrote:
> 
> >"Richard A. O'Keefe" <ok at cs.otago.ac.nz> writes:
> >
> [snip]
> 
> >  Good idea, but perhaps not phrased sharply enough to catch the
> > user's
> >eye. How about something like this:
> >
> >"Notice that image() interprets a matrix as a table of f(x_i, y_j), so
> >the x axis corresponds to row number and the y axis to column number
> >(bottom to top)."
> >
> >and then use Brian's one-liner in the examples section?
> >
> 
> Though I think the added text is a good idea, the addition of the example
> is probably 102.58 times more useful than any text that could be inserted.
> 
> The examples in help files are probably the main reason that people think
> of R having better documentation than other programs.  Like Unix man
> pages, for example, which would be wonderful if only they had examples
> so that you could figure out what the sentences mean when you understand
> every word.

There are limits, I think. If you put too many variations into the
example section, it tends to become a grey blob of code. Anyways, I'm
going to add both.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ajayshah at mayin.org  Fri Aug 27 12:38:40 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Fri, 27 Aug 2004 16:08:40 +0530
Subject: [R] `its' questions
Message-ID: <20040827103840.GA26861@igidr.ac.in>

Problem 1: Often, when I'm dealing with its, str() breaks. Here's a
bug demo. The first statements work fine --

  library(its)

  # Make a series of all dates from 1/1/2000 to 10/1/2000; fill this up
  # with integers from 1 to 30
  x1 <- newIts(start="2000-01-01", end="2000-01-10", 1:30, ncol=3)
  print(x1)
  # Do the same, but restrict yourself to weekdays only
  x2 <- newIts(start="2000-01-01", end="2000-01-10", 1:30, ncol=3,
    extract=T, weekday=T)
  print(x2)

  str(x1)    # works

But here it breaks --
  > str(x2)
  Error in object[1:ile] : subscript out of bounds



Problem 2: I try to write an its out and read it back in, but the two
don't seem to be conformable for matrix subtraction.

  # Let's try writing and reading --
  writecsvIts(x1, filename="/tmp/try.1")
  y <- its(readcsvIts(filename="/tmp/try.1"))
  print(y)
  print(x1)

But this breaks --
  > y-x1
  Error in y - x1 : dates must match

(But they are identical, as the print commands show. x1 was written
out and read back into y).


Problem 3: How would I convert an its object into an ordinary
  numerical matrix or data frame? I have succeeded with statements
  like x <- as.numeric(itsobject[,2]) which extracts the 2nd
  column. But this 'wastes' the dates. How do I make a data frame out
  of the its object, where one of the columns is the dates? How do I
  make a matrix out of the its object, so that I don't have to attack
  one vector at a time using as.numeric() as shown above?


Problem 4: I tried to do plot() and then lines() but that does not
  seem to work. How does one superpose multiple curves on one picture,
  where each of them is an its object?


I will be most happy if someone can point me to more knowledge on
its. It seems like a fascinating library but I'm not yet able to learn
it using the standard docs.

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From tapake.kishore at hp.com  Fri Aug 27 13:06:03 2004
From: tapake.kishore at hp.com (Kishore, Tapake)
Date: Fri, 27 Aug 2004 16:36:03 +0530
Subject: [R] About passing parameter to  '.R' script file
Message-ID: <7A68D3FBF13DB64AA5D0645D1FF8267D85C910@bgeexc01.asiapacific.cpqcorp.net>

Hi,

I am trying the 'R' application for generating the data for the uploaded '.gpr' file. I have written script file named 'test.R'. Currently i have hardcoded the path of uploaded '.gpr' file in the script itself. 

I would like to know how to pass a command line parameter to the 'test.R' script file, so that i dont have to hardcode the path and filename of the '.gpr' files.
Also, need to access these parameters inside the script file to use the path of the uploaded file.
I am using below command to invoke the 'R' application by using the 'test.R' script file.

e.g.
c:\\R\\rw1091\\bin\\R.exe CMD BATCH c:\\R\\rw1091\\bin\\test.R

Expecting your suggestion on this.

Thanks in advance.

Regards,
Kishore



From Roger.Bivand at nhh.no  Fri Aug 27 13:22:10 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 27 Aug 2004 13:22:10 +0200 (CEST)
Subject: [R] About passing parameter to  '.R' script file
In-Reply-To: <7A68D3FBF13DB64AA5D0645D1FF8267D85C910@bgeexc01.asiapacific.cp
	q	corp.net>
Message-ID: <Pine.LNX.4.44.0408271315100.20864-100000@reclus.nhh.no>

On Fri, 27 Aug 2004, Kishore, Tapake wrote:

> Hi,
> 
> I am trying the 'R' application for generating the data for the uploaded
> '.gpr' file. I have written script file named 'test.R'. Currently i have
> hardcoded the path of uploaded '.gpr' file in the script itself.
> 

Typically, you can write a batch file that sets environment variables, and
then calls R with your script file, accessing the environment variables
from your R script using Sys.getenv(). This means that the values are not
hard-coded in your script - just the environment variable names - and that
the values inserted into the environment variables in your batch file are
accessible within the script as values using the function mentioned.


> I would like to know how to pass a command line parameter to the
> 'test.R' script file, so that i dont have to hardcode the path and
> filename of the '.gpr' files. Also, need to access these parameters
> inside the script file to use the path of the uploaded file. I am using
> below command to invoke the 'R' application by using the 'test.R' script
> file.
> 
> e.g.
> c:\\R\\rw1091\\bin\\R.exe CMD BATCH c:\\R\\rw1091\\bin\\test.R
> 
> Expecting your suggestion on this.
> 
> Thanks in advance.
> 
> Regards,
> Kishore
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From wolski at molgen.mpg.de  Fri Aug 27 13:22:29 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Fri, 27 Aug 2004 13:22:29 +0200
Subject: [R] About passing parameter to  '.R' script file
In-Reply-To: <7A68D3FBF13DB64AA5D0645D1FF8267D85C910@bgeexc01.asiapacific.cpqcorp.net>
References: <7A68D3FBF13DB64AA5D0645D1FF8267D85C910@bgeexc01.asiapacific.cpqcorp.net>
Message-ID: <200408271322290111.00FE2B8B@mail.math.fu-berlin.de>

Hi!

You can do it indirectly e.g within a batch file.

a) set an envirovment variable (the path to the file)
b) access it from within you *.R script using ?Sys.getenv

Hope it helps.
Eryk

*********** REPLY SEPARATOR  ***********

On 8/27/2004 at 4:36 PM Kishore, Tapake wrote:

>>>Hi,
>>>
>>>I am trying the 'R' application for generating the data for the uploaded
>>>'.gpr' file. I have written script file named 'test.R'. Currently i have
>>>hardcoded the path of uploaded '.gpr' file in the script itself. 
>>>
>>>I would like to know how to pass a command line parameter to the
>>>'test.R' script file, so that i dont have to hardcode the path and
>>>filename of the '.gpr' files.
>>>Also, need to access these parameters inside the script file to use the
>>>path of the uploaded file.
>>>I am using below command to invoke the 'R' application by using the
>>>'test.R' script file.
>>>
>>>e.g.
>>>c:\\R\\rw1091\\bin\\R.exe CMD BATCH c:\\R\\rw1091\\bin\\test.R
>>>
>>>Expecting your suggestion on this.
>>>
>>>Thanks in advance.
>>>
>>>Regards,
>>>Kishore
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



Dipl. bio-chem. Eryk Witold Wolski    @    MPI-Moleculare Genetic   
Ihnestrasse 63-73 14195 Berlin       'v'    
tel: 0049-30-83875219               /   \    
mail: wolski at molgen.mpg.de        ---W-W----    http://www.molgen.mpg.de/~wolski



From edd at debian.org  Fri Aug 27 13:51:58 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 27 Aug 2004 06:51:58 -0500
Subject: [R] `its' questions
In-Reply-To: <20040827103840.GA26861@igidr.ac.in>
References: <20040827103840.GA26861@igidr.ac.in>
Message-ID: <20040827115157.GB1744@sonny.eddelbuettel.com>

On Fri, Aug 27, 2004 at 04:08:40PM +0530, Ajay Shah wrote:
> Problem 1: Often, when I'm dealing with its, str() breaks. Here's a
> bug demo. The first statements work fine --
[...]
> Problem 2: I try to write an its out and read it back in, but the two
> don't seem to be conformable for matrix subtraction.
[...]

Those two seem like genuine its bugs to me.

> Problem 3: How would I convert an its object into an ordinary
>   numerical matrix or data frame? I have succeeded with statements

Use    itsAsDf <- core(itsObject)   to turn 'itsObject' into a data.frame,
as.matrix and friends take it from there.

> Problem 4: I tried to do plot() and then lines() but that does not
>   seem to work. How does one superpose multiple curves on one picture,
>   where each of them is an its object?

The par(new=TRUE) command allows you do that generically. I.e. to add a second
object to a plot, and have it use the right axis, this works

   plot(Data)	      	      	  # plot something, sets up plot, axes, ...
   par(new=TRUE)		  # tell R we need more
   plot(moreData, axes=FALSE)	  # plot more, don't overplot x axis
   axis(4)	  		  # add 2nd y-axis on right


Hth, Dirk

-- 
Those are my principles, and if you don't like them... well, I have others.
                                                -- Groucho Marx



From gladys at mat.ua.pt  Fri Aug 27 13:52:32 2004
From: gladys at mat.ua.pt (=?iso-8859-1?Q?Gladys_Castillo_Jord=E1n?=)
Date: Fri, 27 Aug 2004 12:52:32 +0100
Subject: [R] rbind for similar data frames
Message-ID: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAAmnUf/J783kGORwaN3kXMCcKAAAAQAAAAUlbhTZwxLUqEfFavPCfIAwEAAAAA@mat.ua.pt>

Hi all:
I have a problem when I try to concatenate two similar data frames with
different number of rows using rbind.  I did something like this:

> d<-data.frame(a=1:10,b=2:11,c=3:12)
> e<-data.frame(a=101:105,b=102:106,c=103:107)
> data=rbind(d,e)

The resulting row enumeration is not sequentially ordered as I expected:
 
> data
     a   b   c
1    1   2   3
2    2   3   4
3    3   4   5
4    4   5   6
5    5   6   7
6    6   7   8
7    7   8   9
8    8   9  10
9    9  10  11
10  10  11  12
11 101 102 103
22 102 103 104
33 103 104 105
44 104 105 106
55 105 106 107

Any suggestion?
  Regard,
   Gladys

------------------------------------------------
Gladys Castillo Jordan
Departamento de Matem??tica
Universidade de Aveiro
Campus Universitario de Santiago
3810-193 Aveiro, Portugal
Phone: (351) 234-370359
Fax: (351) 234-382014
email: gladys at mat.ua.pt
HTTP: www.mat.ua.pt/gladys



From ripley at stats.ox.ac.uk  Fri Aug 27 13:55:05 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 27 Aug 2004 12:55:05 +0100 (BST)
Subject: [R] About passing parameter to  '.R' script file
In-Reply-To: <Pine.LNX.4.44.0408271315100.20864-100000@reclus.nhh.no>
Message-ID: <Pine.LNX.4.44.0408271230330.848-100000@gannet.stats>

There is another solution that the helpers might like to be aware of.
Anything on the command line after --args is ignored but can be retrieved 
by commandArgs.  So suppose I have a script

quote.R:
commandArgs()[-(1:3)]
q()

and a batch file quote.bat:
Rterm.exe --slave --args %* < quote.R

Then

[c:/R]% quote.bat foo bar bah
[1] "foo" "bar" "bah"

If people would cooperate (an identical message was sent to R-bugs this
morning, PR#7201), we might feel inclined to add such things to the
documentation.

On Fri, 27 Aug 2004, Roger Bivand wrote:

> On Fri, 27 Aug 2004, Kishore, Tapake wrote:
> 
> > Hi,
> > 
> > I am trying the 'R' application for generating the data for the uploaded
> > '.gpr' file. I have written script file named 'test.R'. Currently i have
> > hardcoded the path of uploaded '.gpr' file in the script itself.
> > 
> 
> Typically, you can write a batch file that sets environment variables, and
> then calls R with your script file, accessing the environment variables
> from your R script using Sys.getenv(). This means that the values are not
> hard-coded in your script - just the environment variable names - and that
> the values inserted into the environment variables in your batch file are
> accessible within the script as values using the function mentioned.
> 
> 
> > I would like to know how to pass a command line parameter to the
> > 'test.R' script file, so that i dont have to hardcode the path and
> > filename of the '.gpr' files. Also, need to access these parameters
> > inside the script file to use the path of the uploaded file. I am using
> > below command to invoke the 'R' application by using the 'test.R' script
> > file.
> > 
> > e.g.
> > c:\\R\\rw1091\\bin\\R.exe CMD BATCH c:\\R\\rw1091\\bin\\test.R
> > 
> > Expecting your suggestion on this.
> > 
> > Thanks in advance.
> > 
> > Regards,
> > Kishore

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tapake.kishore at hp.com  Fri Aug 27 14:11:49 2004
From: tapake.kishore at hp.com (Kishore, Tapake)
Date: Fri, 27 Aug 2004 17:41:49 +0530
Subject: [R] About passing parameter to  '.R' script file
Message-ID: <7A68D3FBF13DB64AA5D0645D1FF8267D85C934@bgeexc01.asiapacific.cpqcorp.net>

Hi,

Thanks a lot for an instant reply.
Actualy i am invoking the 'R' application from JAVA programme. I am using 'test.R' script file.

Is the below command a right one or is there any other command option available.

c:\\R\\rw1091\\bin\\R.exe CMD BATCH c:\\R\\rw1091\\bin\\test.R

I need to create a batch file for setting the environment variable and call it using commandArgs() or ?Sys.getenv.

Thanks & Regards,
Kishore

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
Sent: Friday, August 27, 2004 5:25 PM
To: Roger Bivand
Cc: Kishore, Tapake; r-help at r-project.org
Subject: Re: [R] About passing parameter to '.R' script file


There is another solution that the helpers might like to be aware of.
Anything on the command line after --args is ignored but can be retrieved 
by commandArgs.  So suppose I have a script

quote.R:
commandArgs()[-(1:3)]
q()

and a batch file quote.bat:
Rterm.exe --slave --args %* < quote.R

Then

[c:/R]% quote.bat foo bar bah
[1] "foo" "bar" "bah"

If people would cooperate (an identical message was sent to R-bugs this
morning, PR#7201), we might feel inclined to add such things to the
documentation.

On Fri, 27 Aug 2004, Roger Bivand wrote:

> On Fri, 27 Aug 2004, Kishore, Tapake wrote:
> 
> > Hi,
> > 
> > I am trying the 'R' application for generating the data for the uploaded
> > '.gpr' file. I have written script file named 'test.R'. Currently i have
> > hardcoded the path of uploaded '.gpr' file in the script itself.
> > 
> 
> Typically, you can write a batch file that sets environment variables, and
> then calls R with your script file, accessing the environment variables
> from your R script using Sys.getenv(). This means that the values are not
> hard-coded in your script - just the environment variable names - and that
> the values inserted into the environment variables in your batch file are
> accessible within the script as values using the function mentioned.
> 
> 
> > I would like to know how to pass a command line parameter to the
> > 'test.R' script file, so that i dont have to hardcode the path and
> > filename of the '.gpr' files. Also, need to access these parameters
> > inside the script file to use the path of the uploaded file. I am using
> > below command to invoke the 'R' application by using the 'test.R' script
> > file.
> > 
> > e.g.
> > c:\\R\\rw1091\\bin\\R.exe CMD BATCH c:\\R\\rw1091\\bin\\test.R
> > 
> > Expecting your suggestion on this.
> > 
> > Thanks in advance.
> > 
> > Regards,
> > Kishore

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From thpe at hhbio.wasser.tu-dresden.de  Fri Aug 27 14:15:17 2004
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Fri, 27 Aug 2004 14:15:17 +0200
Subject: [R] rbind for similar data frames
In-Reply-To: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAAmnUf/J783kGORwaN3kXMCcKAAAAQAAAAUlbhTZwxLUqEfFavPCfIAwEAAAAA@mat.ua.pt>
References: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAAmnUf/J783kGORwaN3kXMCcKAAAAQAAAAUlbhTZwxLUqEfFavPCfIAwEAAAAA@mat.ua.pt>
Message-ID: <412F25D5.6090808@hhbio.wasser.tu-dresden.de>

Gladys Castillo Jord??n wrote:

> Hi all:
> I have a problem when I try to concatenate two similar data frames with
> different number of rows using rbind.  I did something like this:
> 
> 
>>d<-data.frame(a=1:10,b=2:11,c=3:12)
>>e<-data.frame(a=101:105,b=102:106,c=103:107)
>>data=rbind(d,e)
> 
> 
> The resulting row enumeration is not sequentially ordered as I expected:

[..]

> Any suggestion?

row.names(data)  <- 1:nrow(data)

Hope it helps

Thomas P.



From p.b.pynsent at bham.ac.uk  Fri Aug 27 14:27:02 2004
From: p.b.pynsent at bham.ac.uk (P. B. Pynsent)
Date: Fri, 27 Aug 2004 13:27:02 +0100
Subject: [R] gsub, backslash and xtable
Message-ID: <662E59F8-F824-11D8-B569-000A95B0CE8A@bham.ac.uk>

R Version 1.9.1  (2004-06-21)
Mac OS X.3.5 Dual 2GHz PowerPC G5
GUI = "AQUA"


I have a data.frame comprising percentiles with the column headings 
containing % characters, e.g.
 > (pp <- colnames(temp2))
[1] "5%"  "10%" "25%" "50%" "75%" "90%" "95%"
I use xtable to convert the data.frame to Latex but I want to protect 
these % signs from Latex using a backslash in the normal way before 
calling xtable.
I have tried using gsub as follows;
 > gsub("\%","\\%",pp)
[1] "5%"  "10%" "25%" "50%" "75%" "90%" "95%"
also
 > gsub("%","\134%",pp) #octal for backslash
[1] "5%"  "10%" "25%" "50%" "75%" "90%" "95%"
Both of which fail to provide what I need.

I verified  my 'regexps' using awk under Darwin thus;
$ cat fred
"5%"  "10%" "25%" "50%" "75%" "90%" "95%"
$ awk '{gsub(/%/,"\\%"); print}' <fred
"5\%"  "10\%" "25\%" "50\%" "75\%" "90\%" "95\%"
and
$ awk '{gsub(/%/,"\134%"); print}' <fred
"5\%"  "10\%" "25\%" "50\%" "75\%" "90\%" "95\%"

As a possble 'work around', I noticed that,
 > chartr("z","\",gsub("%","z%",pp))
Error: syntax error
 > chartr("z","\\",gsub("%","z%",pp))
[1] "5\\%"  "10\\%" "25\\%" "50\\%" "75\\%" "90\\%" "95\\%"
 > chartr("z","\134",gsub("%","z%",pp))
[1] "5\\%"  "10\\%" "25\\%" "50\\%" "75\\%" "90\\%" "95\\%"

As the xtable is then 'catted' to a file and read back (vide infra) I 
actually end up with what I want using the latter example.
However I am very much left with the feeling that R is in control of me 
rather than vise versa.



Secondly, as I am building up a character vector of sentences, tables 
and figures, I wanted to convert my xtable output to a character vector 
with newline
separators. I have only able to accomplish this by printing to a 
temporary file thus,

theTx <- "\\documentclass[A4paper,10pt]{article}"
.
.
theTx <- paste(theTx, paste_xtable(temp2,"Percentiles for scores"), sep 
= "")
.
theTx <- paste(theTx,"\n" ,"\\end{document}","\n", sep = "")
.
cat(theTx) #into a file for Latex

############## with my past_xtable function being ##########
paste_xtable <- function(a_table, cap) {
sink(file = "levzz", append = FALSE, type = "output")
print(xtable(a_table,caption=cap))
sink()
#read it  back
temp <- readLines("levzz", n=-1) #note / get doubled automaticaly
unlink("levzz") #delete file
a <- "\n"
for (i in 1:length(temp)) {
	a <- paste(a,temp[i],"\n",sep = "")
	}
return(a)
}

I should be most grateful for a  more elegant solutions to both these 
issues or a pointer to the documentation.

Paul



From ripley at stats.ox.ac.uk  Fri Aug 27 15:03:43 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 27 Aug 2004 14:03:43 +0100 (BST)
Subject: [R] gsub, backslash and xtable
In-Reply-To: <662E59F8-F824-11D8-B569-000A95B0CE8A@bham.ac.uk>
Message-ID: <Pine.LNX.4.44.0408271400400.1100-100000@gannet.stats>

On Fri, 27 Aug 2004, P. B. Pynsent wrote:

> R Version 1.9.1  (2004-06-21)
> Mac OS X.3.5 Dual 2GHz PowerPC G5
> GUI = "AQUA"
> 
> 
> I have a data.frame comprising percentiles with the column headings 
> containing % characters, e.g.
>  > (pp <- colnames(temp2))
> [1] "5%"  "10%" "25%" "50%" "75%" "90%" "95%"
> I use xtable to convert the data.frame to Latex but I want to protect 
> these % signs from Latex using a backslash in the normal way before 
> calling xtable.
> I have tried using gsub as follows;
>  > gsub("\%","\\%",pp)
> [1] "5%"  "10%" "25%" "50%" "75%" "90%" "95%"
> also
>  > gsub("%","\134%",pp) #octal for backslash
> [1] "5%"  "10%" "25%" "50%" "75%" "90%" "95%"
> Both of which fail to provide what I need.

Remember you need to double \ in R character strings (in the FAQ, for 
example, and in ?regex):

>  gsub("%","\\\\%",pp)
[1] "5\\%"  "10\\%" "25\\%" "50\\%" "75\\%" "90\\%" "95\\%"
> cat(gsub("%","\\\\%",pp), "\n")
5\% 10\% 25\% 50\% 75\% 90\% 95\% 



-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Fri Aug 27 15:02:01 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 27 Aug 2004 15:02:01 +0200
Subject: [R] gsub, backslash and xtable
In-Reply-To: <662E59F8-F824-11D8-B569-000A95B0CE8A@bham.ac.uk>
References: <662E59F8-F824-11D8-B569-000A95B0CE8A@bham.ac.uk>
Message-ID: <x2hdqoraye.fsf@biostat.ku.dk>

"P. B. Pynsent" <p.b.pynsent at bham.ac.uk> writes:

> I have a data.frame comprising percentiles with the column headings
> containing % characters, e.g.
>  > (pp <- colnames(temp2))
> [1] "5%"  "10%" "25%" "50%" "75%" "90%" "95%"
> I use xtable to convert the data.frame to Latex but I want to protect
> these % signs from Latex using a backslash in the normal way before
> calling xtable.
> I have tried using gsub as follows;
>  > gsub("\%","\\%",pp)
> [1] "5%"  "10%" "25%" "50%" "75%" "90%" "95%"
> also
...
> However I am very much left with the feeling that R is in control of
> me rather than vise versa.


The generic rule for backslashes is that you need twice as many as you
thought:

> p
[1] "25%"
> gsub("%","\\\\%",p)
[1] "25\\%"
> cat(gsub("%","\\\\%",p),"\n")
25\%

The thing that people usually forget is that you have two levels of
escaping, one in R's string parser and another one in the regexp
machinery. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Luisr at frs.fo  Fri Aug 27 15:11:31 2004
From: Luisr at frs.fo (Luis Rideau Cruz)
Date: Fri, 27 Aug 2004 14:11:31 +0100
Subject: [R] for (i in ...) { df[[i]]<- .....}
Message-ID: <s12f411b.025@ffdata.setur.fo>

R-help,

In the following loop :

for(i in 1:8)
{
cc[[i]]<-tapply(test[,i+6],list(puntar=test$puntar),sum)/tapply(test[,5],list(puntar=test$puntar),sum)

cbind.data.frame(cc[[1]],cc[[2]],cc[[3]],cc[[4]],
cc[[5]],c[[6]],cc[[7]],cc[[8]])

}

Is there anyway I can 'cbind.data.frame' the objects cc[[ i ]] without
actually writing every single element??
In this case there are 8 but what if it is a large number??


Thank you



From Luisr at frs.fo  Fri Aug 27 15:15:32 2004
From: Luisr at frs.fo (Luis Rideau Cruz)
Date: Fri, 27 Aug 2004 14:15:32 +0100
Subject: [R] R-help,
Message-ID: <s12f420b.027@ffdata.setur.fo>

R-help,



From rolf at math.unb.ca  Fri Aug 27 15:17:41 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Fri, 27 Aug 2004 10:17:41 -0300 (ADT)
Subject: [R] gsub, backslash and xtable
Message-ID: <200408271317.i7RDHfQL021090@erdos.math.unb.ca>


Peter Dalgaard wrote:

> ``The generic rule for backslashes is that you need twice as many
>   as you thought''

	And you have to apply that rule recursively! :-)

				cheers,

					Rolf Turner
					rolf at math.unb.ca



From Luisr at frs.fo  Fri Aug 27 15:18:27 2004
From: Luisr at frs.fo (Luis Rideau Cruz)
Date: Fri, 27 Aug 2004 14:18:27 +0100
Subject: [R] R-help,
Message-ID: <s12f42b8.028@ffdata.setur.fo>

R-help,



From Luisr at frs.fo  Fri Aug 27 15:22:40 2004
From: Luisr at frs.fo (Luis Rideau Cruz)
Date: Fri, 27 Aug 2004 14:22:40 +0100
Subject: [R] R-help,
Message-ID: <s12f43ba.029@ffdata.setur.fo>

R-help,



From Luisr at frs.fo  Fri Aug 27 15:23:10 2004
From: Luisr at frs.fo (Luis Rideau Cruz)
Date: Fri, 27 Aug 2004 14:23:10 +0100
Subject: [R] R-help,
Message-ID: <s12f43d4.031@ffdata.setur.fo>

R-help,



From Luisr at frs.fo  Fri Aug 27 15:23:10 2004
From: Luisr at frs.fo (Luis Rideau Cruz)
Date: Fri, 27 Aug 2004 14:23:10 +0100
Subject: [R] R-help,
Message-ID: <s12f43d4.030@ffdata.setur.fo>

R-help,



From Luisr at frs.fo  Fri Aug 27 15:23:10 2004
From: Luisr at frs.fo (Luis Rideau Cruz)
Date: Fri, 27 Aug 2004 14:23:10 +0100
Subject: [R] R-help,
Message-ID: <s12f43d4.033@ffdata.setur.fo>

R-help,



From Luisr at frs.fo  Fri Aug 27 15:23:10 2004
From: Luisr at frs.fo (Luis Rideau Cruz)
Date: Fri, 27 Aug 2004 14:23:10 +0100
Subject: [R] R-help,
Message-ID: <s12f43d4.034@ffdata.setur.fo>

R-help,



From Luisr at frs.fo  Fri Aug 27 15:23:10 2004
From: Luisr at frs.fo (Luis Rideau Cruz)
Date: Fri, 27 Aug 2004 14:23:10 +0100
Subject: [R] R-help,
Message-ID: <s12f43d4.032@ffdata.setur.fo>

R-help,



From matthew_wiener at merck.com  Fri Aug 27 15:27:49 2004
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Fri, 27 Aug 2004 09:27:49 -0400
Subject: [R] for (i in ...) { df[[i]]<- .....}
Message-ID: <45AAE6FD142DCB43A38C00A11FF5DF3E02225E8C@uswsmx03.merck.com>

Take a look at "do.call".  

In your case, 'do.call("cbind", cc)' should work.

Hope this helps,

Matt Wiener

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Luis Rideau Cruz
Sent: Friday, August 27, 2004 9:12 AM
To: r-help at stat.math.ethz.ch
Subject: [R] for (i in ...) { df[[i]]<- .....}


R-help,

In the following loop :

for(i in 1:8)
{
cc[[i]]<-tapply(test[,i+6],list(puntar=test$puntar),sum)/tapply(test[,5],lis
t(puntar=test$puntar),sum)

cbind.data.frame(cc[[1]],cc[[2]],cc[[3]],cc[[4]],
cc[[5]],c[[6]],cc[[7]],cc[[8]])

}

Is there anyway I can 'cbind.data.frame' the objects cc[[ i ]] without
actually writing every single element??
In this case there are 8 but what if it is a large number??


Thank you

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From wolski at molgen.mpg.de  Fri Aug 27 15:29:55 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Fri, 27 Aug 2004 15:29:55 +0200
Subject: [R] for (i in ...) { df[[i]]<- .....}
In-Reply-To: <s12f411b.025@ffdata.setur.fo>
References: <s12f411b.025@ffdata.setur.fo>
Message-ID: <200408271529550727.0172D756@mail.math.fu-berlin.de>

?do.call

do.call(cc,"cbind")

Hope it helps

Eryk



*********** REPLY SEPARATOR  ***********

On 8/27/2004 at 2:11 PM Luis Rideau Cruz wrote:

>>>R-help,
>>>
>>>In the following loop :
>>>
>>>for(i in 1:8)
>>>{
>>>cc[[i]]<-tapply(test[,i+6],list(puntar=test$puntar),sum)/tapply(test[,5],list(puntar=test$puntar),sum)
>>>
>>>cbind.data.frame(cc[[1]],cc[[2]],cc[[3]],cc[[4]],
>>>cc[[5]],c[[6]],cc[[7]],cc[[8]])
>>>
>>>}
>>>
>>>Is there anyway I can 'cbind.data.frame' the objects cc[[ i ]] without
>>>actually writing every single element??
>>>In this case there are 8 but what if it is a large number??
>>>
>>>
>>>Thank you
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



Dipl. bio-chem. Eryk Witold Wolski    @    MPI-Moleculare Genetic   
Ihnestrasse 63-73 14195 Berlin       'v'    
tel: 0049-30-83875219               /   \    
mail: wolski at molgen.mpg.de        ---W-W----    http://www.molgen.mpg.de/~wolski



From ripley at stats.ox.ac.uk  Fri Aug 27 15:31:44 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 27 Aug 2004 14:31:44 +0100 (BST)
Subject: [R] for (i in ...) { df[[i]]<- .....}
In-Reply-To: <s12f411b.025@ffdata.setur.fo>
Message-ID: <Pine.LNX.4.44.0408271427010.1248-100000@gannet.stats>

PLEASE don't call methods explicitly.

I think

	do.call("cbind", cc)

is what you are looking for, although probably you should be calling 
data.frame not cbind here.  (Your lines are too long and contain no spaces 
so I am not going to try to fathom out what cc[[i]] really is.  But I am 
sure you do not intend the cbind to be inside the loop.)

On Fri, 27 Aug 2004, Luis Rideau Cruz wrote:

> In the following loop :
> 
> for(i in 1:8)
> {
> cc[[i]]<-tapply(test[,i+6],list(puntar=test$puntar),sum)/tapply(test[,5],list(puntar=test$puntar),sum)
> 
> cbind.data.frame(cc[[1]],cc[[2]],cc[[3]],cc[[4]],
> cc[[5]],c[[6]],cc[[7]],cc[[8]])
> 
> }
> 
> Is there anyway I can 'cbind.data.frame' the objects cc[[ i ]] without
> actually writing every single element??
> In this case there are 8 but what if it is a large number??

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bates at stat.wisc.edu  Fri Aug 27 15:27:59 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 27 Aug 2004 08:27:59 -0500
Subject: [R] FIML in lme
In-Reply-To: <BAY2-F35FHLqbp3eccm000238e7@hotmail.com>
References: <BAY2-F35FHLqbp3eccm000238e7@hotmail.com>
Message-ID: <412F36DF.80203@stat.wisc.edu>

F Z wrote:
> Hi
> 
> I was asked if lme can use FIML (Full Information Maximum Likelihood) 
> instead of REML or ML but I don't know the answer.  Does anybody know if 
> this is implemented in R?

To the best of my knowledge, FIML is ML so the answer is yes.

For example, the phrase "Full Information Maximum Likelihood" is used in 
Singer and Willett (2004) "Applied Longitudinal Data Analysis" (Oxford 
University Press) as a synonym for maximum likelihood.



From p.dalgaard at biostat.ku.dk  Fri Aug 27 15:35:49 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 27 Aug 2004 15:35:49 +0200
Subject: [R] for (i in ...) { df[[i]]<- .....}
In-Reply-To: <s12f411b.025@ffdata.setur.fo>
References: <s12f411b.025@ffdata.setur.fo>
Message-ID: <x28yc0r9e2.fsf@biostat.ku.dk>

"Luis Rideau Cruz" <Luisr at frs.fo> writes:

> R-help,
> 
> In the following loop :
> 
> for(i in 1:8)
> {
> cc[[i]]<-tapply(test[,i+6],list(puntar=test$puntar),sum)/tapply(test[,5],list(puntar=test$puntar),sum)
> 
> cbind.data.frame(cc[[1]],cc[[2]],cc[[3]],cc[[4]],
> cc[[5]],c[[6]],cc[[7]],cc[[8]])
> 
> }
> 
> Is there anyway I can 'cbind.data.frame' the objects cc[[ i ]] without
> actually writing every single element??
> In this case there are 8 but what if it is a large number??

Whatever did as.data.frame(cc) do to you to deserve getting overlooked
that badly? (and I suspect that there's a much neater solution
along the lines of

dd <- lapply(test[5:14],tapply,test$puntar,sum)
dd <- as.data.frame(lapply(dd,as.vector))
dd <- dd[-1]/dd[[1]]

)

As for the generic question, have a look at do.call().

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From maechler at stat.math.ethz.ch  Fri Aug 27 15:51:26 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 27 Aug 2004 15:51:26 +0200
Subject: [R] `its' questions
In-Reply-To: <20040827103840.GA26861@igidr.ac.in>
References: <20040827103840.GA26861@igidr.ac.in>
Message-ID: <16687.15454.575871.505294@gargle.gargle.HOWL>

>>>>> "Ajay" == Ajay Shah <ajayshah at mayin.org>
>>>>>     on Fri, 27 Aug 2004 16:08:40 +0530 writes:

    Ajay> Problem 1: Often, when I'm dealing with its, str() breaks. Here's a
    Ajay> bug demo. The first statements work fine --

    Ajay> library(its)

    Ajay> # Make a series of all dates from 1/1/2000 to 10/1/2000; fill this up
    Ajay> # with integers from 1 to 30
    Ajay> x1 <- newIts(start="2000-01-01", end="2000-01-10", 1:30, ncol=3)
    Ajay> print(x1)
    Ajay> # Do the same, but restrict yourself to weekdays only
    Ajay> x2 <- newIts(start="2000-01-01", end="2000-01-10", 1:30, ncol=3,
    Ajay>    extract=T, weekday=T)
    Ajay> print(x2)

    Ajay> str(x1)    # works

    Ajay> But here it breaks --
    >> str(x2)
    Ajay> Error in object[1:ile] : subscript out of bounds

be it an its bug or not,  str() shouldn't "break".

-- and it doesn't anymore in "R-devel" (aka "R 2.0.0 unstable").
There, str() has been enhanced (several weeks ago) to nicely
display S4 class objects, and for the above it returns

 > str(x2)
 Formal class 'its' [package "its"] with 3 slots
   ..@ dim     : int [1:2] 6 3
   ..@ dimnames:List of 2
   .. ..$ : chr [1:6] "2000-01-03" "2000-01-04" "2000-01-05" "2000-01-06" ...
   .. ..$ : chr [1:3] "1" "2" "3"
   ..@ dates   :`POSIXct', format: chr [1:6] "2000-01-03 01:00:00" "2000-01-04 01:00:00" "2000-01-05 01:00:00" "2000-01-06 01:00:00" ...

---

Martin Maechler



From hans at vanwalen.com  Fri Aug 27 15:54:53 2004
From: hans at vanwalen.com (Hans van Walen)
Date: Fri, 27 Aug 2004 15:54:53 +0200
Subject: [R] Running R from CD?
Message-ID: <000901c48c3d$777beb00$9b99153e@espresso>

At work I have no permission to install R. So, would anyone know whether it
is possible to create a CD with a running R-installation for a windows(XP)
pc? And of course, how to?

Thank you for your help,
Hans van Walen



From tlumley at u.washington.edu  Fri Aug 27 16:30:39 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 27 Aug 2004 07:30:39 -0700 (PDT)
Subject: [R] read.spss warning: unrecognized record type??
In-Reply-To: <200408270935.07786.rasch@med1.med.tum.de>
References: <s12e2412.032@ohsu.edu> <200408270935.07786.rasch@med1.med.tum.de>
Message-ID: <Pine.A41.4.58.0408270723170.269116@homer11.u.washington.edu>

On Fri, 27 Aug 2004, Raphael Schneider wrote:

> On Friday 27 August 2004 02:55, Michael Jerosch-Herold wrote:
> > When using read.spss (library: 'foreign') I get the following warning
> > message:
> >
> > Warning message:
> > E:/R4win/mesamri.sav: Unrecognized record type 7, subtype 13
> > encountered in system file.
> >
> > I don't see anything wrong with record #7 in the database I am trying
> > to read in, but I suspect that the warning message does not refer to a
> > specific record, but a "variable" type. Is this correct? And what does
> > "subtype 13" mean?
> I get the same message but it seems that all data is read correctly. My
> impression is that this message shows up only reading datasets stored with
> SPSS12. At the moment I have no dataset where I know exactly this was stored
> using a SPSS version below 12.

I have had the same experience. Since the computer where I use read.spss
doesn't have any development tools on it I haven't got around to looking
at what is actually happening.

The difficulty with read.spss is that the format is not publically
documented (in fact, most of the formats read by foreign are not
documented or are incompletely or inaccurately documented -- Stata gets an
honorable mention as one of the exceptions). All read.spss can do is
ignore anything it doesn't understand, and check that the rest of the file
makes sense.


	-thomas



From asemeria at cramont.it  Fri Aug 27 16:55:40 2004
From: asemeria at cramont.it (asemeria@cramont.it)
Date: Fri, 27 Aug 2004 16:55:40 +0200
Subject: [R] Running R from CD?
Message-ID: <OFC9B3B2AB.7D076834-ONC1256EFD.0052003E@tomware.it>

You can simply start a Linux live distribution
including R (for example bio-knoppix) from CD, but
you have to restart pc with cd inside.
A.S.

----------------------------

Alessandro Semeria
Models and Simulations Laboratory
Montecatini Environmental Research Center (Edison Group),
Via Ciro Menotti 48,
48023 Marina di Ravenna (RA), Italy
Tel. +39 544 536811
Fax. +39 544 538663
E-mail: alessandro.semeria at cramont.it



From ripley at stats.ox.ac.uk  Fri Aug 27 16:51:36 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 27 Aug 2004 15:51:36 +0100 (BST)
Subject: [R] Running R from CD?
In-Reply-To: <000901c48c3d$777beb00$9b99153e@espresso>
Message-ID: <Pine.LNX.4.44.0408271547060.6624-100000@gannet.stats>

On Fri, 27 Aug 2004, Hans van Walen wrote:

> At work I have no permission to install R. So, would anyone know whether it
> is possible to create a CD with a running R-installation for a windows(XP)
> pc? And of course, how to?

Yes, for a suitable definition of `CD'.  (You need the extensions which
allow long file names and lower-case chars.)

You will need some disc area where you unpack the files from the R 
installer.  Then just write the top-level directory (rw1091 or whatever) 
and all its contents to CD-R.  The Windows installation (but not the Unix 
one) is relocatable as it only uses relative file paths.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From alexandrepatrot at yahoo.com.br  Fri Aug 27 16:57:46 2004
From: alexandrepatrot at yahoo.com.br (=?iso-8859-1?q?Alexandre=20Galv=E3o=20Patriota?=)
Date: Fri, 27 Aug 2004 11:57:46 -0300 (ART)
Subject: [R] degrees of freedom (lme4 and nlme)
Message-ID: <20040827145746.13556.qmail@web52710.mail.yahoo.com>

Hi, I'm having some problems regarding the packages
lme4 and nlme, more specifically in the denominator
degrees of freedom. I used data Orthodont for the two
packages. The commands used are below.

require(nlme)
data(Orthodont)

fm1<-lme(distance~age+ Sex,
data=Orthodont,random=~1|Subject, method="REML")

anova(fm1)

            numDF  DenDF  F-value    p-value
(Intercept)   1      80   4123.156   <.0001
age           1      80    114.838   <.0001
Sex           1      25     9.292    0.0054


The DenDF for each fixed effect is 80, 80 and 25.
Using the package lme4:

require(lme4)
data(Orthodont)

fm2<-lme(distance~age+ Sex,
data=Orthodont,random=~1|Subject, method="REML")

anova(fm2)

    numDF  Sum Sq  Mean Sq  DenDF  F-value    p-value
age  1	   235.356 235.356   105   114.838    <2.2e-16
Sex  1      19.044  19.044   105    9.292     0.002912


In this case the DenDF for each fixed effect is 105
and 105. In this example, the conclusions are still
the same, but it's not the case with another dataset I
analyzed.
I experience the same type of problem when using
glmmPQL of the MASS package and the GLMM of package
lme4. Could anyone give me a hint on why the two
functions are giving incompatible results?
thank you in advance for your help

Alexandre Galv??o Patriota.
	
_______________________________________________________


From wolski at molgen.mpg.de  Fri Aug 27 16:59:46 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Fri, 27 Aug 2004 16:59:46 +0200
Subject: [R] Running R from CD?
In-Reply-To: <000901c48c3d$777beb00$9b99153e@espresso>
References: <000901c48c3d$777beb00$9b99153e@espresso>
Message-ID: <200408271659460516.01C51837@mail.math.fu-berlin.de>

Hi!

Make a isntallation of R on a PC where it is allowed. Install all packages you need. 
Burn the installation folder (rw1091) on the CD.
Start R by clicking on Rgui.exe in the bin folder (rw1091/bin).
Or set a path on the command line to (CD drive letter) :\rw1091\bin
Or create a shortcut.
This should work.

Hope it helps.
Eryk

Ps. If you need to install afterwards package you have to set R_LIBS to a writable drive.



*********** REPLY SEPARATOR  ***********

On 8/27/2004 at 3:54 PM Hans van Walen wrote:

>>>At work I have no permission to install R. So, would anyone know
>>>whether it
>>>is possible to create a CD with a running R-installation for a
>>>windows(XP)
>>>pc? And of course, how to?
>>>
>>>Thank you for your help,
>>>Hans van Walen
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



Dipl. bio-chem. Eryk Witold Wolski    @    MPI-Moleculare Genetic   
Ihnestrasse 63-73 14195 Berlin       'v'    
tel: 0049-30-83875219               /   \    
mail: wolski at molgen.mpg.de        ---W-W----    http://www.molgen.mpg.de/~wolski



From m_nica at hotmail.com  Fri Aug 27 17:00:21 2004
From: m_nica at hotmail.com (Mihai Nica)
Date: Fri, 27 Aug 2004 10:00:21 -0500
Subject: [R] coplot and par
References: <Pine.LNX.4.44.0408270822590.30632-100000@gannet.stats>
Message-ID: <BAY18-DAV8NmxccfxlA0002c51d@hotmail.com>

Yes, I was trying cex.lab(). Now I understand why it didn't work.

Sincerely,

Mihai Nica
Jackson State University
155 B Parkhurst Dr.
Jackson, MS 39202
601 969 5423
----- Original Message ----- 
From: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
To: "Mihai Nica" <m_nica at hotmail.com>
Cc: <r-help at stat.math.ethz.ch>
Sent: Friday, August 27, 2004 2:33 AM
Subject: Re: [R] coplot and par


> On Fri, 27 Aug 2004, Prof Brian Ripley wrote:
>
> > On Thu, 26 Aug 2004, Mihai Nica wrote:
>
> [...]
>
> > > This is the only explanation that I have for being unable to use par()
with
> > > coplot for changing the way the xlab and ylab appears.
> >
> > xlab and ylab are not part of par -- see its help page.
> >
> > > From within coplot I can change the text itself but not the font,
> > > fontsize, etc, but par() before coplot has absolutely no effect
> > > wahtsoever (maybe it isn't supposed to?).
> > >
> > > I guess the question is: how can one change the way the xlab and ylab
appear
> > > on a coplot?
> >
> > They are listed as arguments to coplot, and work for me when given
there.
>
> Although you didn't mention them at all, did you mean specifying cex.lab
> and font.lab?   They do not work as the xlab and ylab in coplot are not
> `x and y labels' in the sense used in ?par: they are placed by mtext() not
> title().
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>
>



From m_nica at hotmail.com  Fri Aug 27 17:09:59 2004
From: m_nica at hotmail.com (Mihai Nica)
Date: Fri, 27 Aug 2004 10:09:59 -0500
Subject: [R] Running R from CD?
References: <000901c48c3d$777beb00$9b99153e@espresso>
Message-ID: <BAY18-DAV12KDkb9pPF0002a8c9@hotmail.com>

Try http://dirk.eddelbuettel.com/quantian.html. It seems it works just fine,
and R is already on it... However I only played with it a little.

Mihai Nica
Jackson State University
155 B Parkhurst Dr.
Jackson, MS 39202
601 969 5423
----- Original Message ----- 
From: "Hans van Walen" <hans at vanwalen.com>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, August 27, 2004 8:54 AM
Subject: [R] Running R from CD?


> At work I have no permission to install R. So, would anyone know whether
it
> is possible to create a CD with a running R-installation for a windows(XP)
> pc? And of course, how to?
>
> Thank you for your help,
> Hans van Walen
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>



From graumann at its.caltech.edu  Fri Aug 27 17:49:49 2004
From: graumann at its.caltech.edu (Johannes Graumann)
Date: Fri, 27 Aug 2004 08:49:49 -0700
Subject: [R] text() with text, variables and math HOWTO?
In-Reply-To: <412EE4A8.2090503@statistik.uni-dortmund.de>
References: <20040826162940.5ef5149d@localhost>
	<412EE4A8.2090503@statistik.uni-dortmund.de>
Message-ID: <20040827084949.48c05607@localhost>

Thanks everybody!

Joh

On Fri, 27 Aug 2004 09:37:12 +0200
Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:

> Johannes Graumann wrote:
> 
> > Hello,
> > 
> > One more question from the 'abusing R for blotting - particularly
> > anally' department:
> > How can I in the expression below make the '%~~%' show up as the
> > aprrox-sign I want it to be?
> > 
> > Thanks for any hint,
> > 
> > Joh
> > 
> > text(
> > 	500,1.5,
> > 	cex=0.75,
> > 	substitute(
> > 		paste(
> > 			OD[600][~nm],
> > 			" of 1 at ",
> > 			time1,
> > 			" min ",
> > 			"%~~%",
> > 			time1h,
> > 			"h"
> > 		),
> > 		list(
> > 			time1=round(time1,digits=0),
> > 			time1h=round(time1/60,digits=1)
> > 		)
> > 	)
> > )
> 
> 
>    substitute(OD[600][~nm] * " of 1 at " *
>      time1 * " min" %~~% time1h * h,
>        list(time1=round(time1,digits=0),
>             time1h=round(time1/60,digits=1))
>    )
> 
> 
> 
> Uwe Ligges
> 
> 
> 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From S.Nyangoma at cs.rug.nl  Fri Aug 27 19:18:19 2004
From: S.Nyangoma at cs.rug.nl (Stephen Nyangoma)
Date: 27 Aug 2004 19:18:19 +0200
Subject: [R] selecting unique columns of a matrix/data frame
In-Reply-To: <200408271008.i7RA4NUb030086@hypatia.math.ethz.ch>
References: <200408271008.i7RA4NUb030086@hypatia.math.ethz.ch>
Message-ID: <1093627099.4141.22.camel@iwi142>

Hi all,
I have a very high dimensional data and apparently there are several
columns that contain similar information (some columns are equal). I
want to form a matrix/data frame consisting of unique columns. Does
anyone have an efficient way of getting out these columns. A small
section of the data frame is given below.

Thanks for helping.

Stephen.


> newdata
      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]
 [1,]  107  125  149  149  165  125  172  172  149   149   107   125  
 [2,]  185  171  150  150  299  171  173  173  150   150   185   171   
 [3,]    0  193  177  177  618  193  343  343  177   177     0   193    
 [4,]    0    0  178  178    0    0  365    0  178   178     0     0    
 [5,]    0    0  223  223    0    0    0    0  223   223     0     0    
 [6,]    0    0  245  245    0    0    0    0  245   245     0     0    
 [7,]    0    0  246  246    0    0    0    0  246   246     0     0    
 [8,]    0    0  466  466    0    0    0    0  466   466     0     0    
 [9,]    0    0  467  467    0    0    0    0  467   467     0     0    
[10,]    0    0  468  468    0    0    0    0  468     0     0     0 

  
##################################################################################


















> > Easy!
> > 
> > Mathematical conventions are just that, conventions.  They 
> > differ by field 
> > of mathematics.  Don't ask us why matrix rows are numbered 
> > down but graphs 
> > are numbered up the y axis, nor why x comes before y but row before 
> > column.  But the matrix layout has always seemed illogical to me.
> 
> My 30+ years of software experience says this is a design flaw.  A good
> design would reconcile the differences between conventions.  Coordinate
> transformations are not that hard, but I wouldn't burden the end-user
> with them needlessly.
> 
> I'll go back to lurking in the daily R-Helps and not ask any more
> questions until I've read all the old R-help messages.  I'm working on
> December 1998 right now and reading forward.  Perhaps by next year I'll
> will have read all the old R-help postings and I'll dare ask another
> question then.
> 
> efg
> 
> 
> 
> ------------------------------
> 
> Message: 72
> Date: Thu, 26 Aug 2004 23:04:10 +0100
> From: "Jennie Bee" <jnb21 at cam.ac.uk>
> Subject: [R] AIC to compare glm models with Poisson errors?
> To: <r-help at stat.math.ethz.ch>
> Message-ID: <025301c48bb8$9d5d3070$07256f83 at plantsci.cam.ac.uk>
> Content-Type: text/plain
> 
> 
> I have used the glm function to fit a series of models using a poisson error structure.
> e.g:
> Model 1: Y is a function of a + bX
> Model 2: Y is a function of a
> 
> I have tried to compare models using AIC, but do not get sensible results (lower AICs for the null, intercept only, model despite the alternate model containing highly significant parameters).  
> 
> 
> 
> I found the following explanation in the online R manual, that seemed to be relevant:
> 
> "There is a potential problem in using glm fits with a variable scale, as in that case the deviance is not simply related to the maximized log-likelihood. The "glm" method for function extractAIC makes the appropriate adjustment for a gaussian family, but may need to be amended for other cases. (The binomial and poisson families have fixed scale by default and do not correspond to a particular maximum-likelihood problem for variable scale.) "
> 
> My question is, how do you amend the function for the poisson family?
> Should I be using AIC, or is there a better information criterion? - I want a method that has the flexibility to compare alternative models that (unlike in my example) are not simply nested families of additive variables.
> 
> Many thanks,
> Jennie Bee
> 
> 
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> Jennie Bee
> 
> Conservation and Community Ecology Group
> Department of Plant Sciences
> University of Cambridge
> Downing Street 
> Cambridge
> CB2 3EA
> 
> Tel: +44 (0)1223 330213 (office); 07890 971 374 (mobile)
> 	[[alternative HTML version deleted]]
> 
> 
> 
> ------------------------------
> 
> Message: 73
> Date: Fri, 27 Aug 2004 10:10:20 +1200 (NZST)
> From: David Scott <d.scott at auckland.ac.nz>
> Subject: Re: [R] Plotting groupedData objects
> To: Deepayan Sarkar <deepayan at cs.wisc.edu>
> Cc: r-help at stat.math.ethz.ch
> Message-ID:
> 	<Pine.LNX.4.44.0408271003320.31281-100000 at hydra.stat.auckland.ac.nz>
> Content-Type: TEXT/PLAIN; charset=US-ASCII
> 
> On Thu, 26 Aug 2004, Deepayan Sarkar wrote:
> 
> > Quoting David Scott <d.scott at auckland.ac.nz>:
> > 
> > > 
> > > I am trying to create a plot similar to Figure 3.2 in Bates and Pinheiro.
> > > 
> > > I have repeated measurements on about 80 subjects from 2 treatment groups.  
> > > I would like to have the panels for the two treatment groups in separate
> > > groups and within those groups have the panels ordered on maximum value
> > > (as is the default).
> > > 
> > > I am ok with getting plots similar to Figs 3.1 and 3.2, but can't see how 
> > > to change the ordering of the panels to what I want.
> > > 
> > > Here is the definition of my groupedData object
> > > 
> > > RAWlmeData <- groupedData(RAW~Elapsed|ID,
> > >               data=RAWData,
> > >               labels=list(x="Elapsed time",y="Airways resistance"),
> > >               units=list(x="(hours)",y="cm H20/L/sec"))
> > > 
> > > I guess I could just plot the two treatment groups separately in turn but 
> > > I feel there is something I am missing.
> > 
> > My reading of page 105 suggests that you need to specify 
> > 'outer=<whatever your grouping factor is>'.
> > Have you tried that?
> > 
> I did try that. It produces Fig 3.3 with multiple lines giving the results 
> for each subject by treatment. With around 40 subjects per treatment 
> group, that isn't very attractive in my case.
> 
> I should also mention that I looked for on-line complements to Pinheiro 
> and Bates, and found MEMSS.tar.gz on 
> http://cm.bell-labs.com/cm/ms/departments/sia/NLME/MEMSS/index.html
> which promised scripts for the examples in the book but turned out to have 
> only chapters 1 and 2, and for S-PLUS rather than R (going on the .q 
> extension).
> 
> David Scott
> 
> _________________________________________________________________
> David Scott	Department of Statistics, Tamaki Campus
> 		The University of Auckland, PB 92019
> 		Auckland	NEW ZEALAND
> Phone: +64 9 373 7599 ext 86830		Fax: +64 9 373 7000
> Email:	d.scott at auckland.ac.nz 
> 
> 
> Graduate Officer, Department of Statistics
> 
> 
> 
> ------------------------------
> 
> Message: 74
> Date: Thu, 26 Aug 2004 15:11:36 -0700
> From: Johannes Graumann <graumann at its.caltech.edu>
> Subject: Re: [R] how to make lines() meet axis in autoscaled
> 	coordinate system?
> To: Rolf Turner <rolf at math.unb.ca>
> Cc: r-help at stat.math.ethz.ch
> Message-ID: <20040826151136.15e13b5c at localhost>
> Content-Type: text/plain; charset=US-ASCII
> 
> Grand!
> 
> Thanks so much!
> 
> Joh
> 
> On Thu, 26 Aug 2004 18:15:44 -0300 (ADT)
> Rolf Turner <rolf at math.unb.ca> wrote:
> 
> > 
> > Johannes Graumann wrote:
> > 
> > > I have an auto-scaled coordinate system and would like to add some
> > > clarifying lines to the graph - which ares supposed to meet the
> > > axis.
> > > 
> > > >lines(c(0,time1,time1), c(1,1,0),lty=3)
> > > 
> > > does what I want, BUT the second leg does not touch the x-axis since
> > > the auto-scaling of the y-axis does not start at'0' but slightly
> > > negative. I could now adjust the line length by 'trial and error' to
> > > suit my needs, but I'd prefer a generalizable solution like
> > > 
> > > >lines(c(<some_macro_for_y-axis_position>,time1,time1),
> > > +	c(1,1,<some_macro_for_x-axis_position>),lty=3)
> > 
> > 	lines(c(par()$usr[1],time1,time1), c(1,1,par()$usr[3]),lty=3)
> > 
> > 				cheers,
> > 
> > 					Rolf Turner
> > 					rolf at math.unb.ca
> >
> 
> 
> 
> ------------------------------
> 
> Message: 75
> Date: Thu, 26 Aug 2004 17:14:01 -0500
> From: Douglas Bates <bates at stat.wisc.edu>
> Subject: Re: [R] GLMM
> To: "Bossarte, Robert" <bvy9 at cdc.gov>
> Cc: r-help at stat.math.ethz.ch
> Message-ID: <412E60A9.1070808 at stat.wisc.edu>
> Content-Type: text/plain; charset=us-ascii; format=flowed
> 
> Bossarte, Robert wrote:
> > I am trying to use the LME package to run a multilevel logistic model
> > using the following code:
> > 
> >  
> > 
> > ------------------------------------------------------------------------
> > -------------------------------------------
> > 
> >  
> > 
> > Model1 = GLMM(WEAP ~ TSRAT2 , random = ~1 | GROUP , family = binomial,
> > na.action = na.omit )
> > 
> >  
> > 
> > ------------------------------------------------------------------------
> > -------------------------------------------
> > 
> >  
> > 
> > Where WEAP is a dichotomous outcome measure, TSRAT2 is a group level
> > covariate, and GROUP is the grouping variable.
> > 
> >  
> > 
> > I do not have significant missing data and the dataset is over 15000
> > cases.
> > 
> >  
> > 
> > I have tried changing the outcome measure(s) and predictor(s) with the
> > same result. Each time R fails and returns the following message:
> > 
> >  
> > 
> > ------------------------------------------------------------------------
> > ------------------------------------------------
> > 
> >  
> > 
> > structure(list( : flist must be a non-empty list of factors
> > 
> >  
> > 
> > ------------------------------------------------------------------------
> > ------------------------------------------------
> > 
> >  
> > 
> > This appears to be problem with the grouping variable, yet there are no
> > missing values and the data were sorted on this variable prior to being
> > read into R.
> 
> Is the grouping variable a factor?  Check with
> 
> str(GROUP)
> 
> If it isn't a factor then convert it a factor using
> 
> GROUP <- factor(GROUP)
> 
> > 
> >  
> > 
> > Any suggestions would be greatly appreciated.
> 
> 
> 
> ------------------------------
> 
> Message: 76
> Date: Thu, 26 Aug 2004 17:17:24 -0500 (CDT)
> From: "John Pitney" <john at pitney.org>
> Subject: Re: [R] Why terms are dropping out of an lm() model
> To: sundar.dorai-raj at PDF.COM
> Cc: r-help at stat.math.ethz.ch
> Message-ID: <14541.167.170.98.10.1093558644.squirrel at www.pitney.org>
> Content-Type: text/plain;charset=iso-8859-1
> 
> >
> >
> > John Pitney wrote:
> >> Hi all!
> >>
> >> I'm fairly new to R and not too experienced with regression.  Because
> >> of one or both of those traits, I'm not seeing why some terms are being
> >> dropped from my model when doing a regression using lm().
> >>
> >> I am trying to do a regression on some experimental data d, which has
> >> two numeric predictors, p1 and p2, and one numeric response, r.  The aim
> >> is to compare polynomial models in p1 and p2 up to third order.  I don't
> >> understand why lm() doesn't return coefficients for the p1^3 and p2^3
> >> terms.  Similar loss of terms happened when I tried orthonormal
> >> polynomials to third order.
> >>
> >> I'm satisfied with the second-order regression, by the way, but I'd
> >> still like to understand why the third-order regression doesn't work
> >> like I'd expect.
> >>
> >> Can anyone offer a pointer to help me understand this?
> >>
> >> Here's what I'm seeing in R 1.9.1 for Windows.  Note the NA's for p1^3
> >> and p2^3 in the last summary.
> >>
> >> [stuff deleted]
> >>
> >> -0.089823 -0.017707  0.001952  0.020820  0.059302
> >>
> >> Coefficients: (2 not defined because of singularities)
> >
> > Did you miss reading the above line? Seems you supplied a singular model
> > to `lm' and since the default for `lm' is `singular.ok = TRUE,' it just
> > pivoted these columns out in the QR-decomposition.
> 
> Yes, I missed that line.  The model matrix is indeed singular.
> 
> Thanks for the quick and helpful response, and sorry for posting before
> thinking carefully enough!
> 
> John
> 
> 
> 
> ------------------------------
> 
> Message: 77
> Date: Thu, 26 Aug 2004 17:22:41 -0500
> From: Deepayan Sarkar <deepayan at cs.wisc.edu>
> Subject: Re: [R] Plotting groupedData objects
> To: David Scott <d.scott at auckland.ac.nz>
> Cc: r-help at stat.math.ethz.ch
> Message-ID: <1093558961.412e62b1c674e at www-auth.cs.wisc.edu>
> Content-Type: text/plain; charset=utf-8
> 
> Quoting David Scott <d.scott at auckland.ac.nz>:
> 
> > On Thu, 26 Aug 2004, Deepayan Sarkar wrote:
> > 
> > > Quoting David Scott <d.scott at auckland.ac.nz>:
> > > 
> > > > 
> > > > I am trying to create a plot similar to Figure 3.2 in Bates and
> > Pinheiro.
> > > > 
> > > > I have repeated measurements on about 80 subjects from 2 treatment
> > groups.  
> > > > I would like to have the panels for the two treatment groups in separate
> > > > groups and within those groups have the panels ordered on maximum value
> > > > (as is the default).
> > > > 
> > > > I am ok with getting plots similar to Figs 3.1 and 3.2, but can't see how
> > 
> > > > to change the ordering of the panels to what I want.
> > > > 
> > > > Here is the definition of my groupedData object
> > > > 
> > > > RAWlmeData <- groupedData(RAW~Elapsed|ID,
> > > >               data=RAWData,
> > > >               labels=list(x="Elapsed time",y="Airways resistance"),
> > > >               units=list(x="(hours)",y="cm H20/L/sec"))
> > > > 
> > > > I guess I could just plot the two treatment groups separately in turn but
> > 
> > > > I feel there is something I am missing.
> > > 
> > > My reading of page 105 suggests that you need to specify 
> > > 'outer=<whatever your grouping factor is>'.
> > > Have you tried that?
> > > 
> > I did try that. It produces Fig 3.3 with multiple lines giving the results 
> > for each subject by treatment. With around 40 subjects per treatment 
> > group, that isn't very attractive in my case.
> 
> I should have been more specific, but I thought this was clear enough in the
> book. I was talking about specifying outer in the groupedData() constructor,
> not in the plot() call. Have you tried _that_?
> 
> > I should also mention that I looked for on-line complements to Pinheiro 
> > and Bates, and found MEMSS.tar.gz on 
> > http://cm.bell-labs.com/cm/ms/departments/sia/NLME/MEMSS/index.html
> > which promised scripts for the examples in the book but turned out to have 
> > only chapters 1 and 2, and for S-PLUS rather than R (going on the .q 
> > extension).
> 
> The R scripts are, naturally enough, bundled with the R version of the package.
> They should be in the scripts/ subdirectory of your nlme installation.
> 
> Deepayan
> 
> 
> 
> ------------------------------
> 
> Message: 78
> Date: Thu, 26 Aug 2004 17:36:00 -0500
> From: Deepayan Sarkar <deepayan at cs.wisc.edu>
> Subject: RE: [R] Surprise when mapping matrix to image
> To: "Glynn, Earl" <EFG at Stowers-Institute.org>
> Cc: Prof Brian Ripley <ripley at stats.ox.ac.uk>,
> 	r-help at stat.math.ethz.ch
> Message-ID: <1093559760.412e65d0a85ee at www-auth.cs.wisc.edu>
> Content-Type: text/plain; charset=utf-8
> 
> Quoting "Glynn, Earl" <EFG at Stowers-Institute.org>:
> 
> > Prof Ripley:
> > 
> > Thank you for your prompt reply.
> > 
> > > It's pure convention: see below.
> > > 
> > > Did you try reading the help for image?  You don't seem to 
> > > understand it
> > > if you actually did.  It seems you are looking for
> > > 
> > >  image(t(x)[ncol(x):1, ])
> > 
> > I think you guys are too close to "R" to understand how hard it is to
> > use sometimes.  What may be blatantly obvious to you is quite a problem
> > especially to beginners.  Some of us may be beginners to R, but we know
> > math, science, programming, and how to solve problems with other tools
> > and languages.    
> > 
> > I re-read the guidelines before posting fearing condemnation.
> > 
> > Before posting I searched the online R-help Google interface with
> > keywords "image", "flip", "rotate".  A discussion from 1998 touched on
> > this issue but I was hoping that this was deemed a "bug" at some point
> > and fixed -- or had an easy workaround, like some parameter I was
> > missing.
> > 
> > I read the "?image" help before posting. Was the part I didn't
> > understand buried in this "note"?
> > 
> >      "Based on a function by Thomas Lumley tlumley at u.washington.edu."
> 
> You seem to be thinking that Prof Ripley's solution had something to do with
> image(). It doesn't, it has to do with manipulating a matrix. image()
> visualizes a matrix in a particular and well-defined way. You want your matrix
> to be shown in a different way, and one (simple) way of doing that is to
> convert your matrix into a different matrix, on which calling image would give
> you what you want. Why would this be explained in ?image ? This is basic R.
> 
> More generally, I think your frustration is caused by your expectation that a
> matrix object should behave like a bitmap image. It doesn't. If you want work
> with images, use the pixmap package.
> 
> Deepayan
> 
> 
> 
> ------------------------------
> 
> Message: 79
> Date: Thu, 26 Aug 2004 15:50:43 -0700
> From: "T. Murlidharan Nair" <nair at sdsc.edu>
> Subject: [R] Error TukeyHSD
> To: r-help at stat.math.ethz.ch
> Message-ID: <412E6943.6020307 at sdsc.edu>
> Content-Type: text/plain; charset=us-ascii; format=flowed
> 
> I am running the following code on the coagulation data and I am getting 
> an error. Please let me know
> if I am missing anything from my code.
> 
> coag<- matrix( scan("//Samba3/nair/R/blood.dat", sep=","), 24, 3, 
> byrow=TRUE)
> colnames(coag) <- c("time","diet","order")
> coag <- as.data.frame(coag)
> oneway.test(time ~  diet, data=coag, var.eq=TRUE)
> coag.mod <- aov(time ~ diet, data=coag)
> options(scipen=5)
> options(digits=5)
> TukeyHSD( coag.mod )
> 
> 
> I get the following error when I run the above code
> Read 72 items
> Error in rep.int(n, length(means)) : Unimplemented feature in rep
> In addition: Warning message:
> non-factors ignored: diet in: replications(paste("~", 
> paste(names(tables), collapse = "+")),
> 
> The data is here ..
> 
> 62,1,20
> 60,1,2
> 63,1,11
> 59,1,10
> 63,2,12
> 67,2,9
> 71,2,15
> 64,2,14
> 65,2,4
> 66,2,8
> 68,3,16
> 66,3,7
> 71,3,1
> 67,3,17
> 68,3,13
> 68,3,21
> 56,4,23
> 62,4,3
> 60,4,6
> 61,4,18
> 63,4,22
> 64,4,19
> 63,4,5
> 59,4,24
> 
> 
> Thanks ../ Murli
> 
> 
> 
> ------------------------------
> 
> Message: 80
> Date: Thu, 26 Aug 2004 18:08:47 -0500
> From: "Mihai Nica" <m_nica at hotmail.com>
> Subject: [R] coplot and par
> To: r-help at stat.math.ethz.ch
> Message-ID: <BAY18-F11XCpU6q5ijb0001c543 at hotmail.com>
> Content-Type: text/plain; format=flowed
> 
> R 1.9.1 on Win2000 or Win98SE.
> 
> I am using coplot as follows:
> 
> coplot(AVG~LRPI| REGION)
> 
> the output seems normal but I get:
> 
> "Warning message:
> calling par(new=) with no plot"
> 
> This is the only explanation that I have for being unable to use par() with 
> coplot for changing the way the xlab and ylab appears. From within coplot I 
> can change the text itself but not the font, fontsize, etc, but par() before 
> coplot has absolutely no effect wahtsoever (maybe it isn't supposed to?).
> 
> I guess the question is: how can one change the way the xlab and ylab appear 
> on a coplot?
> 
> Thanks,
> 
> Mihai
> JSU
> 
> 
> 
> ------------------------------
> 
> Message: 81
> Date: Thu, 26 Aug 2004 16:11:04 -0700
> From: rossini at blindglobe.net (A.J. Rossini)
> Subject: Re: [R] Surprise when mapping matrix to image
> To: r-help at stat.math.ethz.ch
> Message-ID: <85k6vlebqv.fsf at servant.blindglobe.net>
> Content-Type: text/plain; charset=us-ascii
> 
> 
> I think I'd have to respectfully disagree with both Brian and
> Deepayan, as to whether it should be obvious.  It is reasonable
> (principle of least suprise) to expect orientation of the plot to
> match the print order of the matrix.  I would have expected Brian's
> one-liner to be in the help page, with a notice.  It's a not-so-rare
> activity, being a general matrix visualization that is commonly used
> in certain areas of science (whether it ought to be commonly used is a
> separate question).
> 
> While "heatmap" might've been perhaps a better pointer, but it doesn't
> seem to do the "right" thing, either.  I.e. 
> 
> myTemp <- matrix(c(1,2,3,3,2,3),nrow=2)
> heatmap(myTemp,Rowv=NA,Colv=NA)
> 
> doesn't look right to me (R Version 1.9.1  (2004-06-21))
> 
> I see the "pixmap/bitmap" issue as a bit of a red herring, in this
> case. 
> 
> best,
> -tony
> 
> p.s. I seem to get bit by this about once a year for the last few,
> hence why I'm speaking up.
> 
> 
> 
> 
> 
> 
> 
> 
> Deepayan Sarkar <deepayan at cs.wisc.edu> writes:
> 
> > Quoting "Glynn, Earl" <EFG at Stowers-Institute.org>:
> >
> >> Prof Ripley:
> >> 
> >> Thank you for your prompt reply.
> >> 
> >> > It's pure convention: see below.
> >> > 
> >> > Did you try reading the help for image?  You don't seem to 
> >> > understand it
> >> > if you actually did.  It seems you are looking for
> >> > 
> >> >  image(t(x)[ncol(x):1, ])
> >> 
> >> I think you guys are too close to "R" to understand how hard it is to
> >> use sometimes.  What may be blatantly obvious to you is quite a problem
> >> especially to beginners.  Some of us may be beginners to R, but we know
> >> math, science, programming, and how to solve problems with other tools
> >> and languages.    
> >> 
> >> I re-read the guidelines before posting fearing condemnation.
> >> 
> >> Before posting I searched the online R-help Google interface with
> >> keywords "image", "flip", "rotate".  A discussion from 1998 touched on
> >> this issue but I was hoping that this was deemed a "bug" at some point
> >> and fixed -- or had an easy workaround, like some parameter I was
> >> missing.
> >> 
> >> I read the "?image" help before posting. Was the part I didn't
> >> understand buried in this "note"?
> >> 
> >>      "Based on a function by Thomas Lumley tlumley at u.washington.edu."
> >
> > You seem to be thinking that Prof Ripley's solution had something to do with
> > image(). It doesn't, it has to do with manipulating a matrix. image()
> > visualizes a matrix in a particular and well-defined way. You want your matrix
> > to be shown in a different way, and one (simple) way of doing that is to
> > convert your matrix into a different matrix, on which calling image would give
> > you what you want. Why would this be explained in ?image ? This is basic R.
> >
> > More generally, I think your frustration is caused by your expectation that a
> > matrix object should behave like a bitmap image. It doesn't. If you want work
> > with images, use the pixmap package.
> >
> > Deepayan
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> 
> -- 
> Anthony Rossini			    Research Associate Professor
> rossini at u.washington.edu            http://www.analytics.washington.edu/ 
> Biomedical and Health Informatics   University of Washington
> Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
> UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
> FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email
> 
> CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}
> 
> 
> 
> ------------------------------
> 
> Message: 82
> Date: Thu, 26 Aug 2004 16:29:40 -0700
> From: Johannes Graumann <graumann at its.caltech.edu>
> Subject: [R] text() with text, variables and math HOWTO?
> To: r-help at stat.math.ethz.ch
> Message-ID: <20040826162940.5ef5149d at localhost>
> Content-Type: text/plain; charset=US-ASCII
> 
> Hello,
> 
> One more question from the 'abusing R for blotting - particularly
> anally' department:
> How can I in the expression below make the '%~~%' show up as the
> aprrox-sign I want it to be?
> 
> Thanks for any hint,
> 
> Joh
> 
> text(
> 	500,1.5,
> 	cex=0.75,
> 	substitute(
> 		paste(
> 			OD[600][~nm],
> 			" of 1 at ",
> 			time1,
> 			" min ",
> 			"%~~%",
> 			time1h,
> 			"h"
> 		),
> 		list(
> 			time1=round(time1,digits=0),
> 			time1h=round(time1/60,digits=1)
> 		)
> 	)
> )
> 
> 
> 
> ------------------------------
> 
> Message: 83
> Date: Thu, 26 Aug 2004 16:13:19 -0700 (PDT)
> From: franc Li <avelox12 at yahoo.com>
> Subject: [R] multiple regression with dummy variables
> To: r-help at stat.math.ethz.ch
> Message-ID: <20040826231319.9247.qmail at web51810.mail.yahoo.com>
> Content-Type: text/plain; charset=us-ascii
> 
> Hi I'm a beginner in R, please help...
> 
> I'm trying to do regression analysis on a categorical
> data to find b's of each Xi.  Essentially, f(x) is the
> output, and two independent variables Xd (days of week
> (1,2,...,7) and Xwk (week of month
> (1,2,...,6)including partial wks).  
> 
> So the data looks like this:
> f(x) Xd  Xwk
> 0.03  1   2
> 0.06  3   1
> :
> :
> 
> I use factor() and contrasts() to produce I.matrix as
> such
>   2 3 4 5 6 7
> 1 0 0 0 0 0 0
> 2 1 0 0 0 0 0
> 3 0 1 0 0 0 0
> 4 0 0 1 0 0 0
> 5 0 0 0 1 0 0
> 6 0 0 0 0 1 0
> 7 0 0 0 0 0 1
> 
>   2 3 4 5 6
> 1 0 0 0 0 0
> 2 1 0 0 0 0
> 3 0 1 0 0 0
> 4 0 0 1 0 0
> 5 0 0 0 1 0
> 6 0 0 0 0 1
> 
> However, when I tried to regress with:
> 
> summary(lm(log(y1)~ I(daydum==1) + I(weekdum==1)),
> contrast=T)
> 
> Error in model.frame(formula, rownames, variables,
> varnames, extras, extranames,  : 
>         variable lengths differ 
> 
> I'm trying to find pairwise regression in order to
> find each bi.  Can anyone hlep?  Thanks so much in
> advance.
> 
> franc
> 
> 
> 
> ------------------------------
> 
> Message: 84
> Date: Thu, 26 Aug 2004 19:42:35 -0400
> From: "Jim Brennan" <jfbrennan at rogers.com>
> Subject: Re: [R] Surprise when mapping matrix to image
> To: <rossini at u.washington.edu>, <r-help at stat.math.ethz.ch>
> Message-ID:
> 	<014201c48bc6$5e03a860$3b8ac445 at slnt.phub.net.cable.rogers.com>
> Content-Type: text/plain;	charset="iso-8859-1"
> 
> I was also surprised by the image orientation this summer and used the
> "easy" fix of matrix manipulation. There is however another issue and that
> is when you start flipping around the matrix, orders etc. and for the case
> you want to have sensibly labeled axes, you may have to use the axis
> commands etc. which is also easy, but accumulation of easy fixes can be
> tedious so there is perhaps some argument for yet another new documentation
> submission. Maybe a method for easily changing the orientation in image
> could be added.
> 
> Jim
> ----- Original Message -----
> From: "A.J. Rossini" <rossini at blindglobe.net>
> To: <r-help at stat.math.ethz.ch>
> Sent: Thursday, August 26, 2004 7:11 PM
> Subject: Re: [R] Surprise when mapping matrix to image
> 
> 
> >
> > I think I'd have to respectfully disagree with both Brian and
> > Deepayan, as to whether it should be obvious.  It is reasonable
> > (principle of least suprise) to expect orientation of the plot to
> > match the print order of the matrix.  I would have expected Brian's
> > one-liner to be in the help page, with a notice.  It's a not-so-rare
> > activity, being a general matrix visualization that is commonly used
> > in certain areas of science (whether it ought to be commonly used is a
> > separate question).
> >
> > While "heatmap" might've been perhaps a better pointer, but it doesn't
> > seem to do the "right" thing, either.  I.e.
> >
> > myTemp <- matrix(c(1,2,3,3,2,3),nrow=2)
> > heatmap(myTemp,Rowv=NA,Colv=NA)
> >
> > doesn't look right to me (R Version 1.9.1  (2004-06-21))
> >
> > I see the "pixmap/bitmap" issue as a bit of a red herring, in this
> > case.
> >
> > best,
> > -tony
> >
> > p.s. I seem to get bit by this about once a year for the last few,
> > hence why I'm speaking up.
> >
> >
> >
> >
> >
> >
> >
> >
> > Deepayan Sarkar <deepayan at cs.wisc.edu> writes:
> >
> > > Quoting "Glynn, Earl" <EFG at Stowers-Institute.org>:
> > >
> > >> Prof Ripley:
> > >>
> > >> Thank you for your prompt reply.
> > >>
> > >> > It's pure convention: see below.
> > >> >
> > >> > Did you try reading the help for image?  You don't seem to
> > >> > understand it
> > >> > if you actually did.  It seems you are looking for
> > >> >
> > >> >  image(t(x)[ncol(x):1, ])
> > >>
> > >> I think you guys are too close to "R" to understand how hard it is to
> > >> use sometimes.  What may be blatantly obvious to you is quite a problem
> > >> especially to beginners.  Some of us may be beginners to R, but we know
> > >> math, science, programming, and how to solve problems with other tools
> > >> and languages.
> > >>
> > >> I re-read the guidelines before posting fearing condemnation.
> > >>
> > >> Before posting I searched the online R-help Google interface with
> > >> keywords "image", "flip", "rotate".  A discussion from 1998 touched on
> > >> this issue but I was hoping that this was deemed a "bug" at some point
> > >> and fixed -- or had an easy workaround, like some parameter I was
> > >> missing.
> > >>
> > >> I read the "?image" help before posting. Was the part I didn't
> > >> understand buried in this "note"?
> > >>
> > >>      "Based on a function by Thomas Lumley tlumley at u.washington.edu."
> > >
> > > You seem to be thinking that Prof Ripley's solution had something to do
> with
> > > image(). It doesn't, it has to do with manipulating a matrix. image()
> > > visualizes a matrix in a particular and well-defined way. You want your
> matrix
> > > to be shown in a different way, and one (simple) way of doing that is to
> > > convert your matrix into a different matrix, on which calling image
> would give
> > > you what you want. Why would this be explained in ?image ? This is basic
> R.
> > >
> > > More generally, I think your frustration is caused by your expectation
> that a
> > > matrix object should behave like a bitmap image. It doesn't. If you want
> work
> > > with images, use the pixmap package.
> > >
> > > Deepayan
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> > >
> >
> > --
> > Anthony Rossini     Research Associate Professor
> > rossini at u.washington.edu            http://www.analytics.washington.edu/
> > Biomedical and Health Informatics   University of Washington
> > Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
> > UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
> > FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email
> >
> > CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 
> 
> ------------------------------
> 
> Message: 85
> Date: Thu, 26 Aug 2004 18:55:28 -0500
> From: Erin Hodgess <hodgess at gator.uhd.edu>
> Subject: [R] expressions/approximate text
> To: r-help at stat.math.ethz.ch
> Message-ID: <200408262355.i7QNtSh18436 at gator.dt.uh.edu>
> 
> Please forgive the general posting.
> 
> To get the x approximately = y,
> please try
> text(6,2,expression(x %~~% y))
> 
> HTH.
> 
> Sincerely,
> Erin Hodgess
> mailto: hodgess at gator.uhd.edu
> 
> 
> 
> ------------------------------
> 
> Message: 86
> Date: Fri, 27 Aug 2004 12:05:26 +1200
> From: Paul Murrell <p.murrell at auckland.ac.nz>
> Subject: Re: [R] Label using equivalent of \mathbb{R}
> To: Deepayan Sarkar <deepayan at stat.wisc.edu>
> Cc: r-help at stat.math.ethz.ch, Martin Maechler
> 	<maechler at stat.math.ethz.ch>
> Message-ID: <412E7AC6.3060807 at stat.auckland.ac.nz>
> Content-Type: text/plain; charset=us-ascii; format=flowed
> 
> Hi
> 
> 
> Deepayan Sarkar wrote:
> > On Thursday 26 August 2004 11:41, Martin Maechler wrote:
> > 
> >>>>>>>"Simon" == Simon Cullen <cullens at tcd.ie>
> >>>>>>>    on Thu, 26 Aug 2004 15:30:06 +0100 writes:
> >>>>>>
> >>    Simon> On Thu, 26 Aug 2004 15:24:33 +0200, Trenkler,
> >>    Simon> Dietrich <dtrenkler at nts6.oec.uni-osnabrueck.de>
> >>
> >>    Simon> wrote:
> >>    >> [Dietrich Trenkler]
> >>    >>
> >>    >>
> >>    >> plot(rnorm(10),xlab=expression(bold(x)),ylab=expression(bold(y)))
> >>
> >>    Simon> Not quite what I am looking for, I'm afraid. \mathbb
> >>    Simon> gives "blackboard" fonts - the capitals with two
> >>    Simon> vertical parallel lines in them that are used for the
> >>    Simon> Reals, Complex numbers etc.
> >>
> >>yes (and Dieter confused \mathbb{} with \mathbf{}).
> >>
> >>R's builtin plotmath utilities can't do this directly.
> >>A workaround would be similar to what we used to do in LaTeX 2.09
> >>times (before \mathbb{} was standardly available):
> >>
> >>      { "I", <negative space>, "R" }
> >>
> >>Whereas with (La)TeX, a macro definition was relatively
> >>easy for the above, it might be a bit harder / more ugly with
> >>current R builtins, for one because I don't think ``plotmath''
> >>has a concept of <negative space>.
> >>1)  A really ugly hack would work with  text() -- not quite useful
> >>   if you'd like these in 'xlab' etc.
> >>
> >>2)  A potentially much nicer scheme might be to use  "grid" instead of
> >>   oldstyle  "graphics" :  There one could construct a  "grob"
> >>   (graphical object) for the '|R' symbol that one could should be
> >>   pass to other functions -- though it may need some tweaking
> >>   before a function like (pkg lattice's) xyplot() would accept
> >>   a grob for xlab [instead of only a character or expression].
> > 
> > 
> > Interesting thought. Should be almost trivial to implement.
> > 
> > Currently, legends (sort of generalized key-s in the Trellis context) can be 
> > grobs, e.g.
> > 
> > bbR <- textGrob(c("I", "R"), x = 0.7 * c(-1, 1), 
> >                 default.units = "mm", 
> >                 vp = viewport(h=0, w=0))
> > 
> > xyplot(1 ~ 1, xlab = NULL, legend = list(bottom = list(fun = bbR)))
> 
> 
> Cool :)  An alternative hack would be to treat this as an annotation of 
> the lattice panel.  For example ...
> 
> xyplot(1 ~ 1, xlab = "") # NOTE "" not NULL
> downViewport("panel.1")
> pushViewport(viewport(clip="off")) # to be able to draw outside panel
> grid.text(c("I", "R"), y=unit(-2, "lines"),
>            x=unit(0.5, "npc") - 0.5*stringWidth(c("I", "")),
>            just="left")
> upViewport(0)
> 
> ...  (using stringWidth makes the hack scale with font size).
> 
> In this case though, I think the ideal solution would be just to use a 
> font that has the desired character.  On Windows this should be doable 
> via the file RHOME\etc\Rdevga and something like mtext(..., font=<some 
> large number>) [assuming that the appropriate font can be found and 
> installed].
> 
> Paul
> -- 
> Dr Paul Murrell
> Department of Statistics
> The University of Auckland
> Private Bag 92019
> Auckland
> New Zealand
> 64 9 3737599 x85392
> paul at stat.auckland.ac.nz
> http://www.stat.auckland.ac.nz/~paul/
> 
> 
> 
> ------------------------------
> 
> Message: 87
> Date: Fri, 27 Aug 2004 00:30:06 +0000
> From: "F Z" <gerifalte28 at hotmail.com>
> Subject: [R] FIML in lme
> To: R-help at stat.math.ethz.ch
> Message-ID: <BAY2-F35FHLqbp3eccm000238e7 at hotmail.com>
> Content-Type: text/plain; format=flowed
> 
> Hi
> 
> I was asked if lme can use FIML (Full Information Maximum Likelihood) 
> instead of REML or ML but I don't know the answer.  Does anybody know if 
> this is implemented in R?
> 
> Thanks
> 
> Francisco
> 
> 
> 
> ------------------------------
> 
> Message: 88
> Date: Fri, 27 Aug 2004 02:47:19 +0200
> From: "Henrik Bengtsson" <hb at maths.lth.se>
> Subject: RE: [R] Surprise when mapping matrix to image
> To: "'Jim Brennan'" <jfbrennan at rogers.com>,
> 	<rossini at u.washington.edu>,	<r-help at stat.math.ethz.ch>
> Message-ID: <001201c48bcf$6e51c400$8e0040d5 at hblaptop>
> Content-Type: text/plain;	charset="us-ascii"
> 
> I've got some simple code example at http://www.maths.lth.se/help/R/image/,
> or just try:
> 
>  source("http://www.maths.lth.se/help/R/image/image.R")
> 
> Best wishes
> 
> Henrik Bengtsson
> 
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jim Brennan
> > Sent: Friday, August 27, 2004 1:43 AM
> > To: rossini at u.washington.edu; r-help at stat.math.ethz.ch
> > Subject: Re: [R] Surprise when mapping matrix to image
> > 
> > 
> > I was also surprised by the image orientation this summer and 
> > used the "easy" fix of matrix manipulation. There is however 
> > another issue and that is when you start flipping around the 
> > matrix, orders etc. and for the case you want to have 
> > sensibly labeled axes, you may have to use the axis commands 
> > etc. which is also easy, but accumulation of easy fixes can 
> > be tedious so there is perhaps some argument for yet another 
> > new documentation submission. Maybe a method for easily 
> > changing the orientation in image could be added.
> > 
> > Jim
> > ----- Original Message -----
> > From: "A.J. Rossini" <rossini at blindglobe.net>
> > To: <r-help at stat.math.ethz.ch>
> > Sent: Thursday, August 26, 2004 7:11 PM
> > Subject: Re: [R] Surprise when mapping matrix to image
> > 
> > 
> > >
> > > I think I'd have to respectfully disagree with both Brian and 
> > > Deepayan, as to whether it should be obvious.  It is reasonable 
> > > (principle of least suprise) to expect orientation of the plot to 
> > > match the print order of the matrix.  I would have expected Brian's 
> > > one-liner to be in the help page, with a notice.  It's a 
> > not-so-rare 
> > > activity, being a general matrix visualization that is 
> > commonly used 
> > > in certain areas of science (whether it ought to be 
> > commonly used is a 
> > > separate question).
> > >
> > > While "heatmap" might've been perhaps a better pointer, but 
> > it doesn't 
> > > seem to do the "right" thing, either.  I.e.
> > >
> > > myTemp <- matrix(c(1,2,3,3,2,3),nrow=2)
> > > heatmap(myTemp,Rowv=NA,Colv=NA)
> > >
> > > doesn't look right to me (R Version 1.9.1  (2004-06-21))
> > >
> > > I see the "pixmap/bitmap" issue as a bit of a red herring, in this 
> > > case.
> > >
> > > best,
> > > -tony
> > >
> > > p.s. I seem to get bit by this about once a year for the last few, 
> > > hence why I'm speaking up.
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > > Deepayan Sarkar <deepayan at cs.wisc.edu> writes:
> > >
> > > > Quoting "Glynn, Earl" <EFG at Stowers-Institute.org>:
> > > >
> > > >> Prof Ripley:
> > > >>
> > > >> Thank you for your prompt reply.
> > > >>
> > > >> > It's pure convention: see below.
> > > >> >
> > > >> > Did you try reading the help for image?  You don't seem to 
> > > >> > understand it if you actually did.  It seems you are 
> > looking for
> > > >> >
> > > >> >  image(t(x)[ncol(x):1, ])
> > > >>
> > > >> I think you guys are too close to "R" to understand how 
> > hard it is 
> > > >> to use sometimes.  What may be blatantly obvious to you 
> > is quite a 
> > > >> problem especially to beginners.  Some of us may be 
> > beginners to R, 
> > > >> but we know math, science, programming, and how to solve 
> > problems 
> > > >> with other tools and languages.
> > > >>
> > > >> I re-read the guidelines before posting fearing condemnation.
> > > >>
> > > >> Before posting I searched the online R-help Google 
> > interface with 
> > > >> keywords "image", "flip", "rotate".  A discussion from 
> > 1998 touched 
> > > >> on this issue but I was hoping that this was deemed a 
> > "bug" at some 
> > > >> point and fixed -- or had an easy workaround, like some 
> > parameter I 
> > > >> was missing.
> > > >>
> > > >> I read the "?image" help before posting. Was the part I didn't 
> > > >> understand buried in this "note"?
> > > >>
> > > >>      "Based on a function by Thomas Lumley 
> > > >> tlumley at u.washington.edu."
> > > >
> > > > You seem to be thinking that Prof Ripley's solution had 
> > something to 
> > > > do
> > with
> > > > image(). It doesn't, it has to do with manipulating a matrix. 
> > > > image() visualizes a matrix in a particular and well-defined way. 
> > > > You want your
> > matrix
> > > > to be shown in a different way, and one (simple) way of 
> > doing that 
> > > > is to convert your matrix into a different matrix, on 
> > which calling 
> > > > image
> > would give
> > > > you what you want. Why would this be explained in ?image 
> > ? This is 
> > > > basic
> > R.
> > > >
> > > > More generally, I think your frustration is caused by your 
> > > > expectation
> > that a
> > > > matrix object should behave like a bitmap image. It 
> > doesn't. If you 
> > > > want
> > work
> > > > with images, use the pixmap package.
> > > >
> > > > Deepayan
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list 
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > > >
> > >
> > > --
> > > Anthony Rossini     Research Associate Professor
> > > rossini at u.washington.edu            
> > http://www.analytics.washington.edu/
> > > Biomedical and Health Informatics   University of Washington
> > > Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer 
> > Research Center
> > > UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is 
> > unreliable 
> > > FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email
> > >
> > > CONFIDENTIALITY NOTICE: This e-mail message and any 
> > > attachme...{{dropped}}
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list 
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> >
> 
> 
> 
> ------------------------------
> 
> Message: 89
> Date: Thu, 26 Aug 2004 17:55:17 -0700
> From: "Michael Jerosch-Herold" <jeroschh at ohsu.edu>
> Subject: [R] read.spss warning: unrecognized record type??
> To: r-help at stat.math.ethz.ch
> Message-ID: <s12e2412.032 at ohsu.edu>
> Content-Type: text/plain; charset=us-ascii
> 
> When using read.spss (library: 'foreign') I get the following warning
> message:
> 
> Warning message: 
> E:/R4win/mesamri.sav: Unrecognized record type 7, subtype 13
> encountered in system file. 
> 
> I don't see anything wrong with record #7 in the database I am trying
> to read in, but I suspect that the warning message does not refer to a
> specific record, but a "variable" type. Is this correct? And what does
> "subtype 13" mean?
> 
> I basically have a data table with records (cases) in rows, and various
> variables for each record/case in the columns. Again, I suspect that I
> am using "record" in a different sense then meant by the above warning
> message.
> 
> Based on this warning, is "record type 7" discarded when the data are
> read in?
> 
> Thank you in advance for shedding some light on this!
> 
> Michael Jerosch-Herold
> 
> 
> 
> ------------------------------
> 
> Message: 90
> Date: Fri, 27 Aug 2004 13:19:15 +1200 (NZST)
> From: David Scott <d.scott at auckland.ac.nz>
> Subject: Re: [R] Plotting groupedData objects
> To: Deepayan Sarkar <deepayan at cs.wisc.edu>
> Cc: r-help at stat.math.ethz.ch, David Scott <d.scott at auckland.ac.nz>
> Message-ID:
> 	<Pine.LNX.4.44.0408271305540.31281-100000 at hydra.stat.auckland.ac.nz>
> Content-Type: TEXT/PLAIN; charset=US-ASCII
> 
> On Thu, 26 Aug 2004, Deepayan Sarkar wrote:
> 
> > Quoting David Scott <d.scott at auckland.ac.nz>:
> > 
> > > On Thu, 26 Aug 2004, Deepayan Sarkar wrote:
> > > 
> > > > Quoting David Scott <d.scott at auckland.ac.nz>:
> > > > 
> > > > > 
> > > > > I am trying to create a plot similar to Figure 3.2 in Bates and
> > > Pinheiro.
> > > > > 
> > > > > I have repeated measurements on about 80 subjects from 2 treatment
> > > groups.  
> > > > > I would like to have the panels for the two treatment groups in separate
> > > > > groups and within those groups have the panels ordered on maximum value
> > > > > (as is the default).
> > > > > 
> > > > > I am ok with getting plots similar to Figs 3.1 and 3.2, but can't see how
> > > 
> > > > > to change the ordering of the panels to what I want.
> > > > > 
> > > > > Here is the definition of my groupedData object
> > > > > 
> > > > > RAWlmeData <- groupedData(RAW~Elapsed|ID,
> > > > >               data=RAWData,
> > > > >               labels=list(x="Elapsed time",y="Airways resistance"),
> > > > >               units=list(x="(hours)",y="cm H20/L/sec"))
> > > > > 
> > > > > I guess I could just plot the two treatment groups separately in turn but
> > > 
> > > > > I feel there is something I am missing.
> > > > 
> > > > My reading of page 105 suggests that you need to specify 
> > > > 'outer=<whatever your grouping factor is>'.
> > > > Have you tried that?
> > > > 
> > > I did try that. It produces Fig 3.3 with multiple lines giving the results 
> > > for each subject by treatment. With around 40 subjects per treatment 
> > > group, that isn't very attractive in my case.
> > 
> > I should have been more specific, but I thought this was clear enough in the
> > book. I was talking about specifying outer in the groupedData() constructor,
> > not in the plot() call. Have you tried _that_?
> > 
> > > I should also mention that I looked for on-line complements to Pinheiro 
> > > and Bates, and found MEMSS.tar.gz on 
> > > http://cm.bell-labs.com/cm/ms/departments/sia/NLME/MEMSS/index.html
> > > which promised scripts for the examples in the book but turned out to have 
> > > only chapters 1 and 2, and for S-PLUS rather than R (going on the .q 
> > > extension).
> > 
> > The R scripts are, naturally enough, bundled with the R version of the package.
> > They should be in the scripts/ subdirectory of your nlme installation.
> > 
> Thanks Deepayan. I didn't read that section closely enough. Defining an 
> outer factor in the constructor did what I wanted.
> 
> I also found the scripts as you suggested although they didn't include 
> code for the production of Fig 3.2 as far as I could see.
> 
> 
> David Scott
> 
> 
> 
> 
> 
> 
> 
> 
> 
> _________________________________________________________________
> David Scott	Department of Statistics, Tamaki Campus
> 		The University of Auckland, PB 92019
> 		Auckland	NEW ZEALAND
> Phone: +64 9 373 7599 ext 86830		Fax: +64 9 373 7000
> Email:	d.scott at auckland.ac.nz 
> 
> 
> Graduate Officer, Department of Statistics
> 
> 
> 
> ------------------------------
> 
> Message: 91
> Date: Thu, 26 Aug 2004 20:26:53 -0500
> From: Deepayan Sarkar <deepayan at cs.wisc.edu>
> Subject: Re: [R] introduction slides for beginners
> To: Deepayan Sarkar <deepayan at cs.wisc.edu>
> Cc: Armin Roehrl <armin at xss.de>, r-help at stat.math.ethz.ch
> Message-ID: <1093570013.412e8ddd1ac2b at www-auth.cs.wisc.edu>
> Content-Type: text/plain; charset=utf-8
> 
> Quoting Deepayan Sarkar <deepayan at cs.wisc.edu>:
> 
> > On Thu, 26 Aug 2004, Armin Roehrl wrote:
> > 
> > > Hi all,
> > >
> > >     does anybody have slides for a 1 to 3 hour crash course into R,
> > > that I would be allowed to recycle to show a few people the light?
> > 
> > I have some stuff I used for 5 x 1 hour presentations at
> > 
> > http://www.stat.wisc.edu/~deepayan/SIBS/slides/
> > 
> > meant to complement the first chapter of 'An Introduction to R'. 
> 
> Umm, for the record, I meant to say 'Introductory Statistics with R' here.
> 
> Deepayan
> 
> 
> 
> ------------------------------
> 
> Message: 92
> Date: Fri, 27 Aug 2004 14:19:27 +1200 (NZST)
> From: "Richard A. O'Keefe" <ok at cs.otago.ac.nz>
> Subject: RE: [R] Surprise when mapping matrix to image
> To: r-help at stat.math.ethz.ch
> Message-ID: <200408270219.i7R2JRQN413645 at atlas.otago.ac.nz>
> 
> Deepayan Sarkar <deepayan at cs.wisc.edu> wrote:
> 	You seem to be thinking that Prof Ripley's solution had
> 	something to do with image().
> 
> "Glynn, Earl" <EFG at Stowers-Institute.org>
> could certainly be excused for thinking so, because
> what Prof Ripley wrote included this:
> 
> 	> > Did you try reading the help for image?  You don't seem to 
> 	> > understand it if you actually did.
> 
> That certainly sounds to me as though the answer should have been
> obvious from ?image
> 
> Now, the image() function is trying to be extremely helpful and offers
> you lots of ways of providing the data, and that is explained in ?image.
> 
> In fact a key piece of information *IS* in ?image, so it really DOES
> have something to do with image():
> 
>     Details:
> 
>      The length of 'x' should be equal to the 'nrow(z)+1' or 'nrow(z)'.
>      In the first case 'x' specifies the boundaries between the cells:
>      in the second case 'x' specifies the midpoints of the cells. 
> 
> This tells us that the "x" direction of the image goes along the *rows*
> of the matrix and by implication that the "y" direction goes along the
> *columns* of the matrix, and is precisely what you need to figure out
> that image(t(x)) is the thing to use.
> 
> One does have to be a little subtle at reading between the lines to see
> the relevance of details about a parameter one is not passing, but the
> information _is_ there and it _is_ 'something to do with image()', not
> something to do with matrices.
> 
> The 'volcano' example from ?image also points the same way.
> 
> The basic problem here is that the standard visual layout of
> matrices sets up such a strong expectation that it's difficult for
> any text to override it.
> 
> I don't see anything that explicitly discusses the direction of the
> axes, I don't see anything explicit, but I took it that ?image would
> follow the usual "horizontal axis increases from left to right,
> vertical axis increases from bottom to top" convention, which means
> "horizontal gives ROW number increasing left to right,
>  vertical gives COLUMN number increasing bottom to top"
> so when I do image(t(x)) I see my columns as vertical strips but
> they are "upside down", so the last step, to
>     image(t(x)[ncol(x):1,])
> does make sense, provided you read ?image as *implying* without actually
> stating "follows the usual graphical conventions for axes".  But again,
> this really is a fact about image(), not about matrices.
> 
> As for the claim that
> 	It doesn't, it has to do with manipulating a matrix. image()
> 	visualizes a matrix in a particular and well-defined way.
> 
> Well yes, it is a particular way, and I suppose you could say that the
> code is defined well, but it really isn't, when you get right down to
> it, and *EXPLICITLY* documented way.  You really have to work at it.
> 
> This is NOT a criticism of R documentation.  Of all the open-source
> programs I use, I reckon R is by far the best documented, and its
> on-line documentation can more than withstand comparison against most
> of the commercial programs I've seen.  The real problem is that R is
> just so big and there are only so many people writing documentation for
> it.
> 
> Given the wide range of ways that the x, y, z arguments can be passed
> to image(), it would actually make sense to have some kind of flip and/or
> mirror operations specified via an argument to image().
> The source code of image is available (image.default) so it wouldn't be
> a lot of work for someone who wants it to produce such a thing.
> 
> 
> 
> ------------------------------
> 
> Message: 93
> Date: Thu, 26 Aug 2004 22:39:43 -0400
> From: "Liaw, Andy" <andy_liaw at merck.com>
> Subject: RE: [R] predict.mvr error message
> To: "'Stewart T Chang'" <stchang at umich.edu>, r-help at stat.math.ethz.ch
> Message-ID:
> 	<3A822319EB35174CA3714066D590DCD504AF82AA at usrymx25.merck.com>
> Content-Type: text/plain
> 
> What version of R, what version of pls.pcr, and on what OS?  Have you
> checked whether your versions of software are up to date?  I get:
> 
> > n <- 1350
> > p <- 180
> > y <- rnorm(n)
> > x <- matrix(sample(0:1, n*p, replace=TRUE), n, p)
> > fit <- mvr(x, y, method="SIMPLS", validat="none", ncomp=2)
> > xt <- matrix(sample(0:1, 312*p, replace=TRUE), 312, p)
> > yt <- predict(fit, xt)
> 
> 
> Andy
> 
> > From: Stewart T Chang
> > 
> > Greetings,
> > 
> > I've encountered an error message while using the pls.pcr package
> > that's left me scratching my head for a couple of hours, and 
> > I thought I
> > might post it here in the hopes that someone else uses this package.
> > Using a data set of 1350 observations (each observation comprises 180
> > predictor variables and 1 output variable), I trained a model using
> > pls.pcr.  (Other details: ncomp=2, 
> > method="SIMPLS",validation="none".  I
> > might also add that each predictor variable has a value of 
> > either 0 or 1,
> > though the output variable is a real number.)
> > '
> > When I tried to use this model to make predictions (on 312 sets of 180
> > predictor variables), I received the following error:
> > 
> > Error in X %*% object$training$B[, , index, drop = TRUE] :
> >         requires numeric matrix/vector arguments
> > 
> > Normally I'd ask a friend of mine in Biostatistics here to 
> > help me out,
> > but he's currently away!  Have any of you used pls.pcr and gotten this
> > kind of message?  Thanks!
> > 
> > Stewart
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> >
> 
> 
> 
> ------------------------------
> 
> Message: 94
> Date: Thu, 26 Aug 2004 22:50:56 -0400 (EDT)
> From: Stewart T Chang <stchang at umich.edu>
> Subject: RE: [R] predict.mvr error message
> To: "Liaw, Andy" <andy_liaw at merck.com>
> Cc: r-help at stat.math.ethz.ch
> Message-ID:
> 	<Pine.SOL.4.58.0408262241430.18266 at timepilot.gpcc.itd.umich.edu>
> Content-Type: TEXT/PLAIN; charset=US-ASCII
> 
> Well, I figured it out on my own.  It turns out, even though my test set
> of data appeared to be a matrix, it wasn't.  Adding "as.matrix" to the
> front of the name of the data set solved the problem.  (Maybe there's some
> alternative way of doing it, such as yours?)  Thanks for your suggestion.
> 
> Stewart
> 
> 
> 
> On Thu, 26 Aug 2004, Liaw, Andy wrote:
> 
> > What version of R, what version of pls.pcr, and on what OS?  Have you
> > checked whether your versions of software are up to date?  I get:
> >
> > > n <- 1350
> > > p <- 180
> > > y <- rnorm(n)
> > > x <- matrix(sample(0:1, n*p, replace=TRUE), n, p)
> > > fit <- mvr(x, y, method="SIMPLS", validat="none", ncomp=2)
> > > xt <- matrix(sample(0:1, 312*p, replace=TRUE), 312, p)
> > > yt <- predict(fit, xt)
> >
> >
> > Andy
> >
> > > From: Stewart T Chang
> > >
> > > Greetings,
> > >
> > > I've encountered an error message while using the pls.pcr package
> > > that's left me scratching my head for a couple of hours, and
> > > I thought I
> > > might post it here in the hopes that someone else uses this package.
> > > Using a data set of 1350 observations (each observation comprises 180
> > > predictor variables and 1 output variable), I trained a model using
> > > pls.pcr.  (Other details: ncomp=2,
> > > method="SIMPLS",validation="none".  I
> > > might also add that each predictor variable has a value of
> > > either 0 or 1,
> > > though the output variable is a real number.)
> > > '
> > > When I tried to use this model to make predictions (on 312 sets of 180
> > > predictor variables), I received the following error:
> > >
> > > Error in X %*% object$training$B[, , index, drop = TRUE] :
> > >         requires numeric matrix/vector arguments
> > >
> > > Normally I'd ask a friend of mine in Biostatistics here to
> > > help me out,
> > > but he's currently away!  Have any of you used pls.pcr and gotten this
> > > kind of message?  Thanks!
> > >
> > > Stewart
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > >
> > >
> >
> >
> > ------------------------------------------------------------------------------
> > Notice:  This e-mail message, together with any attachments, contains information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New Jersey, USA 08889), and/or its affiliates (which may be known outside the United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as Banyu) that may be confidential, proprietary copyrighted and/or legally privileged. It is intended solely for the use of the individual or entity named on this message.  If you are not the intended recipient, and have received this message in error, please notify us immediately by reply e-mail and then delete it from your system.
> > ------------------------------------------------------------------------------
> >
> >
> >
> 
> 
> 
> ------------------------------
> 
> Message: 95
> Date: Thu, 26 Aug 2004 23:08:07 -0400
> From: "Liaw, Andy" <andy_liaw at merck.com>
> Subject: RE: [R] Surprise when mapping matrix to image
> To: "'Richard A. O'Keefe'" <ok at cs.otago.ac.nz>,
> 	r-help at stat.math.ethz.ch
> Message-ID:
> 	<3A822319EB35174CA3714066D590DCD504AF82AB at usrymx25.merck.com>
> Content-Type: text/plain
> 
> > From: Richard A. O'Keefe
> [snip]
> > Given the wide range of ways that the x, y, z arguments can be passed
> > to image(), it would actually make sense to have some kind of 
> > flip and/or
> > mirror operations specified via an argument to image().
> > The source code of image is available (image.default) so it 
> > wouldn't be
> > a lot of work for someone who wants it to produce such a thing.
> 
> I'd suggest adding an `as.matrix' argument to image.default.
> 
> Best,
> Andy
> 
> 
> 
> ------------------------------
> 
> Message: 96
> Date: Thu, 26 Aug 2004 22:16:23 -0500
> From: Deepayan Sarkar <deepayan at cs.wisc.edu>
> Subject: RE: [R] Surprise when mapping matrix to image
> To: "Richard A. O'Keefe" <ok at cs.otago.ac.nz>
> Cc: r-help at stat.math.ethz.ch
> Message-ID: <1093576583.412ea7876ee02 at www-auth.cs.wisc.edu>
> Content-Type: text/plain; charset=utf-8
> 
> Quoting "Richard A. O'Keefe" <ok at cs.otago.ac.nz>:
> 
> > Deepayan Sarkar <deepayan at cs.wisc.edu> wrote:
> >  You seem to be thinking that Prof Ripley's solution had
> >  something to do with image().
> > 
> > "Glynn, Earl" <EFG at Stowers-Institute.org>
> > could certainly be excused for thinking so, because
> > what Prof Ripley wrote included this:
> > 
> >  > > Did you try reading the help for image?  You don't seem to 
> >  > > understand it if you actually did.
> > 
> > That certainly sounds to me as though the answer should have been
> > obvious from ?image
> 
> I agree with you that the issues are related, but your (edited) quoting is
> misleading. I interpret Prof Ripley's comment as implying that reading ?image
> would tell the user what happens when he calls image(x). This is distinct from
> how to manipulate a matrix, which is what t(x)[ncol(x):1, ] is doing. It
> doesn't seem obvious to me that this information should be in ?image, and I
> still don't see how Prof Ripley's comment suggested that it was. Anything but
> the most trivial use of S involves combining different ideas. One can hardly
> expect all possible interactions to be documented.
> 
> Of course, documentation can always use improvements (and the rest of your
> comments on ?image are all valid), and it would be nice to anticipate
> frequently faced problems and offer solutions preemptively, but that's a
> different issue.
> 
> Deepayan
> 
> 
> 
> ------------------------------
> 
> Message: 97
> Date: Fri, 27 Aug 2004 16:22:15 +1200 (NZST)
> From: "Richard A. O'Keefe" <ok at cs.otago.ac.nz>
> Subject: RE: [R] Surprise when mapping matrix to image
> To: r-help at stat.math.ethz.ch
> Message-ID: <200408270422.i7R4MFF7409020 at atlas.otago.ac.nz>
> 
> Deepayan Sarkar <deepayan at cs.wisc.edu> wrote:
> 	I agree with you that the issues are related, but your (edited) quoting is
> 	misleading.
> 
> My quotes were selective but not otherwise edited.
> 
> 	I interpret Prof Ripley's comment as implying that reading ?image
> 	would tell the user what happens when he calls image(x).
> 
> And that's how I interpreted it too.
> 
> 	This is distinct from how to manipulate a matrix, which is what
> 	t(x)[ncol(x):1, ] is doing.  It doesn't seem obvious to me that this
> 	information should be in ?image,
> 
> Nobody whatsoever has suggested that the matrix manipulation code *SHOULD*
> be in ?image.   Nobody.  Not me.  Not anyone else.  The original poster's
> problem was not "how do I manipulate a matrix" but "why is the image the
> wrong way around".
> 
> 	and I still don't see how Prof Ripley's
> 	comment suggested that it was.
> 
> It didn't.  NOBODY has made any such suggestion.
> 
> 	One can hardly expect all possible interactions to be documented.
> 	
> Nobody has suggested that either.
> 
> All I was saying is this:
> (1) The original poster's problem was a problem about image(),
>     NOT a problem about matrix manipulation.
> (2) We have no reason whatsoever to assert, hint, believe, or opine
>     that the original poster was any less competent at matrix manipulation
>     per se than anyone else reading this mail list.  He didn't have a
>     matrix problem.
> (3) The problem basically is precisely where Prof Ripley located it:
>     a clash between
>     (a) an extremely strong "cultural" expectation that the elements
>         of a matrix will be presented with rows running from 1 at top
>         to m at bottom and columns running from 1 at left to n at right, and
>     (b) an extremely strong "cultural" expectation that the vertical
>         axis of a plot runs from low at the bottom to high at the top, and
>     (c) the fact that image() maps the "row" axis of a matrix to the
>         horizontal axis of a plot and the "column" axis of a matrix to
>         the vertical axis of a plot.
>     Expectations (a) and (b) are not only part of general computing,
>     they are strongly supported by R itself.  As for (c), it doesn't
>     seem to me that it _had_ to be that way.
> (4) Point (c) is a sufficiently strange quirk of image() that it is
>     a stumbling block waiting to trip people up.  I've stumbled over
>     it myself, which is how come I knew where to look in ?image.
>     That makes it *essential* that it should be documented.
> (5) And it *IS* documented.  It really is an image() problem, not a
>     matrix problem, and the essential part of it *IS* documented.
> 
> Pointing out that a obvious nasty interaction *is* documented is
> hardly a demand that all possible interactions should be documented.
> 
> How about this addition to the documentation?
> 
>     "The x axis corresponds to the rows of the matrix (first on the left
>      to last on the right).  The y axis corresponds to the columns of the
>      matrix (first at the bottom to last at the top)."
> 
> Once you know what image() is _intended_ to do with a matrix,
> you can figure out the transposing and reversing you need for any
> other view.  It's knowing you need to that's the problem.
> 
> 
> 
> ------------------------------
> 
> Message: 98
> Date: Thu, 26 Aug 2004 21:59:21 -0700
> From: rossini at blindglobe.net (A.J. Rossini)
> Subject: Re: [R] Surprise when mapping matrix to image
> To: "Richard A. O'Keefe" <ok at cs.otago.ac.nz>
> Cc: r-help at stat.math.ethz.ch
> Message-ID: <85llg1tbva.fsf at servant.blindglobe.net>
> Content-Type: text/plain; charset=us-ascii
> 
> "Richard A. O'Keefe" <ok at cs.otago.ac.nz> writes:
> 
> > Once you know what image() is _intended_ to do with a matrix,
> > you can figure out the transposing and reversing you need for any
> > other view.  It's knowing you need to that's the problem.
> 
> This is a much better statement of my point, and precisely why it
> keeps (though only occassionally) burning me.
> 
> best,
> -tony
> 
> -- 
> Anthony Rossini			    Research Associate Professor
> rossini at u.washington.edu            http://www.analytics.washington.edu/ 
> Biomedical and Health Informatics   University of Washington
> Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
> UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
> FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email
> 
> CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}
> 
> 
> 
> ------------------------------
> 
> Message: 99
> Date: Fri, 27 Aug 2004 13:03:45 +0800
> From: Neil Leonard <nleonard at tartarus.uwa.edu.au>
> Subject: [R] Keyboard input into functions
> To: r-help at stat.math.ethz.ch
> Message-ID: <797B336F-F7E6-11D8-8BC2-003065D5B8EC at tartarus.uwa.edu.au>
> Content-Type: text/plain; charset=US-ASCII; format=flowed
> 
> Hi,
> 
> Does anybody know if it is possible to have keyboard input into 
> functions? Kind of like 'scanf()' in C.
> 
> I want to write a function that asks me what to put in certain 
> variables.
> 
> 
> Thanks,
> Neil
> 
> 
> 
> ------------------------------
> 
> Message: 100
> Date: Fri, 27 Aug 2004 14:11:14 +0900
> From: "Manoj - Hachibushu Capital" <Wanzare at HCJP.com>
> Subject: RE: [R] Keyboard input into functions
> To: "Neil Leonard" <nleonard at tartarus.uwa.edu.au>,
> 	<r-help at stat.math.ethz.ch>
> Message-ID:
> 	<1CBA12F2D414914989C723D196B287DC26BCD1 at jp-svr-ex1.hcjp.com>
> Content-Type: text/plain;	charset="us-ascii"
> 
> ?readline
> 
> HTH
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Neil Leonard
> Sent: Friday, August 27, 2004 2:04 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Keyboard input into functions
> 
> Hi,
> 
> Does anybody know if it is possible to have keyboard input into 
> functions? Kind of like 'scanf()' in C.
> 
> I want to write a function that asks me what to put in certain 
> variables.
> 
> 
> Thanks,
> Neil
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 
> 
> ------------------------------
> 
> Message: 101
> Date: Fri, 27 Aug 2004 07:50:59 +0100 (BST)
> From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
> Subject: Re: [R] Error TukeyHSD
> To: "T. Murlidharan Nair" <nair at sdsc.edu>
> Cc: r-help at stat.math.ethz.ch
> Message-ID: <Pine.LNX.4.44.0408270747580.30532-100000 at gannet.stats>
> Content-Type: TEXT/PLAIN; charset=US-ASCII
> 
> You are trying to apply TukeyHSD to a linear regression.  I think you want
> `diet' to be a factor, but it is not.  (The warning message told you
> that.) The model needs to be
> 
> coag.mod <- aov(time ~ factor(diet), data=coag)
> 
> Function aov happily accepts continuous covariates as it can also do
> ANCOVA.
> 
> On Thu, 26 Aug 2004, T. Murlidharan Nair wrote:
> 
> > I am running the following code on the coagulation data and I am getting 
> > an error. Please let me know
> > if I am missing anything from my code.
> > 
> > coag<- matrix( scan("//Samba3/nair/R/blood.dat", sep=","), 24, 3, 
> > byrow=TRUE)
> > colnames(coag) <- c("time","diet","order")
> > coag <- as.data.frame(coag)
> > oneway.test(time ~  diet, data=coag, var.eq=TRUE)
> > coag.mod <- aov(time ~ diet, data=coag)
> > options(scipen=5)
> > options(digits=5)
> > TukeyHSD( coag.mod )
> > 
> > 
> > I get the following error when I run the above code
> > Read 72 items
> > Error in rep.int(n, length(means)) : Unimplemented feature in rep
> > In addition: Warning message:
> > non-factors ignored: diet in: replications(paste("~", 
> > paste(names(tables), collapse = "+")),
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> 
> 
> ------------------------------
> 
> Message: 102
> Date: 27 Aug 2004 08:46:57 +0200
> From: Peter Dalgaard <p.dalgaard at biostat.ku.dk>
> Subject: Re: [R] Surprise when mapping matrix to image
> To: "Richard A. O'Keefe" <ok at cs.otago.ac.nz>
> Cc: r-help at stat.math.ethz.ch
> Message-ID: <x2pt5d13j2.fsf at biostat.ku.dk>
> Content-Type: text/plain; charset=us-ascii
> 
> "Richard A. O'Keefe" <ok at cs.otago.ac.nz> writes:
> 
> > How about this addition to the documentation?
> > 
> >     "The x axis corresponds to the rows of the matrix (first on the left
> >      to last on the right).  The y axis corresponds to the columns of the
> >      matrix (first at the bottom to last at the top)."
> > 
> > Once you know what image() is _intended_ to do with a matrix,
> > you can figure out the transposing and reversing you need for any
> > other view.  It's knowing you need to that's the problem.
> 
> Good idea, but perhaps not phrased sharply enough to catch the user's
> eye. How about something like this:
> 
> "Notice that image() interprets a matrix as a table of f(x_i, y_j), so
> the x axis corresponds to row number and the y axis to column number
> (bottom to top)."
> 
> and then use Brian's one-liner in the examples section?
> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907
> 
> 
> 
> ------------------------------
> 
> Message: 103
> Date: Fri, 27 Aug 2004 07:55:46 +0100 (BST)
> From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
> Subject: Re: [R] coplot and par
> To: Mihai Nica <m_nica at hotmail.com>
> Cc: r-help at stat.math.ethz.ch
> Message-ID: <Pine.LNX.4.44.0408270751220.30532-100000 at gannet.stats>
> Content-Type: TEXT/PLAIN; charset=US-ASCII
> 
> On Thu, 26 Aug 2004, Mihai Nica wrote:
> 
> > R 1.9.1 on Win2000 or Win98SE.
> > 
> > I am using coplot as follows:
> > 
> > coplot(AVG~LRPI| REGION)
> > 
> > the output seems normal but I get:
> > 
> > "Warning message:
> > calling par(new=) with no plot"
> 
> >From the NEWS file for R-patched:
> 
>     o	coplot(..) doesn't give an extraneous warning anymore when called
> 	on a fresh device.
> 
> so it is a bug that is fixed in the latest version (which you cna find on 
> CRAN).
> > 
> > This is the only explanation that I have for being unable to use par() with 
> > coplot for changing the way the xlab and ylab appears. 
> 
> xlab and ylab are not part of par -- see its help page.
> 
> > From within coplot I can change the text itself but not the font,
> > fontsize, etc, but par() before coplot has absolutely no effect
> > wahtsoever (maybe it isn't supposed to?).
> > 
> > I guess the question is: how can one change the way the xlab and ylab appear 
> > on a coplot?
> 
> They are listed as arguments to coplot, and work for me when given there.
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> 
> 
> ------------------------------
> 
> Message: 104
> Date: Fri, 27 Aug 2004 08:07:52 +0100 (BST)
> From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
> Subject: RE: [R] Keyboard input into functions
> To: Manoj - Hachibushu Capital <Wanzare at hcjp.com>
> Cc: r-help at stat.math.ethz.ch
> Message-ID: <Pine.LNX.4.44.0408270806140.30532-100000 at gannet.stats>
> Content-Type: TEXT/PLAIN; charset=US-ASCII
> 
> On Fri, 27 Aug 2004, Manoj - Hachibushu Capital wrote:
> 
> > ?readline
> 
> Yes, but a nearer equivalent to scanf in C is scan() which reads from the 
> keyboard by default.
> 
> > -----Original Message-----
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Neil Leonard
> > Sent: Friday, August 27, 2004 2:04 PM
> > 
> > Does anybody know if it is possible to have keyboard input into 
> > functions? Kind of like 'scanf()' in C.
> > 
> > I want to write a function that asks me what to put in certain 
> > variables.
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> 
> 
> ------------------------------
> 
> Message: 105
> Date: Fri, 27 Aug 2004 09:21:39 +0200
> From: "Petr Pikal" <petr.pikal at precheza.cz>
> Subject: RE: [R] Surprise when mapping matrix to image
> To: "Glynn, Earl" <EFG at Stowers-Institute.org>
> Cc: r-help at stat.math.ethz.ch
> Message-ID: <412EFD23.249.3B1DE9 at localhost>
> Content-Type: text/plain; charset=US-ASCII
> 
> 
> 
> On 26 Aug 2004 at 16:47, Glynn, Earl wrote:
> 
> <snip>
> 
> > I'll go back to lurking in the daily R-Helps and not ask any more
> > questions until I've read all the old R-help messages.  I'm working on
> > December 1998 right now and reading forward.  Perhaps by next year
> > I'll will have read all the old R-help postings and I'll dare ask
> > another question then.
> 
> Hi
> 
> When I do not find something in help page and a function does not 
> do what I want I usually consult Paul Johnson's Rtips (or 
> StatsRUs). Its a first hit by Google "statsrus". There you can find 
> impresive collection of hints neatly arranged. And probably after a 
> while you will collect some usefull hints yourself (as I myself 
> collected).
> 
> Cheers.
> Petr
> 
> > 
> > efg
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 
> Petr Pikal
> petr.pikal at precheza.cz
> 
> 
> 
> ------------------------------
> 
> Message: 106
> Date: 27 Aug 2004 09:17:45 +0200
> From: Peter Dalgaard <p.dalgaard at biostat.ku.dk>
> Subject: Re: [R] read.spss warning: unrecognized record type??
> To: "Michael Jerosch-Herold" <jeroschh at ohsu.edu>
> Cc: r-help at stat.math.ethz.ch
> Message-ID: <x2llg1123q.fsf at biostat.ku.dk>
> Content-Type: text/plain; charset=us-ascii
> 
> "Michael Jerosch-Herold" <jeroschh at ohsu.edu> writes:
> 
> > When using read.spss (library: 'foreign') I get the following warning
> > message:
> > 
> > Warning message: 
> > E:/R4win/mesamri.sav: Unrecognized record type 7, subtype 13
> > encountered in system file. 
> > 
> > I don't see anything wrong with record #7 in the database I am trying
> > to read in, but I suspect that the warning message does not refer to a
> > specific record, but a "variable" type. Is this correct? And what does
> > "subtype 13" mean?
> 
> Well foreign doesn't know either, and that's the problem... Most
> likely you have one of your SPSS variables coded in an unusual manner
> (do they all come though alright?), or it is something else in the
> meta-info that foreign cannot interpret.
>  
> > I basically have a data table with records (cases) in rows, and various
> > variables for each record/case in the columns. Again, I suspect that I
> > am using "record" in a different sense then meant by the above warning
> > message.
> 
> Yes. System files are typically organised in "records" which are not
> the actual data records but pieces of meta-information. As in (I'm
> just making this up, read the sources for the exact definitions)
> 
> type=1 subtype=1 length=22 data1="My very important data"
> 
> which is compactly stored as (<..> meaning "binary coding of")
> 
> <1><1><22>My very...
> 
> so that on reading, the program sees the first two bytes and then
> knows that this is a "main header" and that it will be followed by a
> 2-byte length and then as many characters as the length indicates.
> 
> There are other record types for individual variable names, variable
> types, variable labels, value labels for categorical data, etc...
> 
>  
> > Based on this warning, is "record type 7" discarded when the data are
> > read in?
> 
> Possibly, but you have the data to check and we don't....
>  
> > Thank you in advance for shedding some light on this!
> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907
> 
> 
> 
> ------------------------------
> 
> Message: 107
> Date: Fri, 27 Aug 2004 08:33:25 +0100 (BST)
> From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
> Subject: Re: [R] coplot and par
> To: Mihai Nica <m_nica at hotmail.com>
> Cc: r-help at stat.math.ethz.ch
> Message-ID: <Pine.LNX.4.44.0408270822590.30632-100000 at gannet.stats>
> Content-Type: TEXT/PLAIN; charset=US-ASCII
> 
> On Fri, 27 Aug 2004, Prof Brian Ripley wrote:
> 
> > On Thu, 26 Aug 2004, Mihai Nica wrote:
> 
> [...]
> 
> > > This is the only explanation that I have for being unable to use par() with 
> > > coplot for changing the way the xlab and ylab appears. 
> > 
> > xlab and ylab are not part of par -- see its help page.
> > 
> > > From within coplot I can change the text itself but not the font,
> > > fontsize, etc, but par() before coplot has absolutely no effect
> > > wahtsoever (maybe it isn't supposed to?).
> > > 
> > > I guess the question is: how can one change the way the xlab and ylab appear 
> > > on a coplot?
> > 
> > They are listed as arguments to coplot, and work for me when given there.
> 
> Although you didn't mention them at all, did you mean specifying cex.lab 
> and font.lab?   They do not work as the xlab and ylab in coplot are not
> `x and y labels' in the sense used in ?par: they are placed by mtext() not 
> title().
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> 
> 
> ------------------------------
> 
> Message: 108
> Date: Fri, 27 Aug 2004 09:37:12 +0200
> From: Uwe Ligges <ligges at statistik.uni-dortmund.de>
> Subject: Re: [R] text() with text, variables and math HOWTO?
> To: Johannes Graumann <graumann at its.caltech.edu>
> Cc: r-help at stat.math.ethz.ch
> Message-ID: <412EE4A8.2090503 at statistik.uni-dortmund.de>
> Content-Type: text/plain; charset=us-ascii; format=flowed
> 
> Johannes Graumann wrote:
> 
> > Hello,
> > 
> > One more question from the 'abusing R for blotting - particularly
> > anally' department:
> > How can I in the expression below make the '%~~%' show up as the
> > aprrox-sign I want it to be?
> > 
> > Thanks for any hint,
> > 
> > Joh
> > 
> > text(
> > 	500,1.5,
> > 	cex=0.75,
> > 	substitute(
> > 		paste(
> > 			OD[600][~nm],
> > 			" of 1 at ",
> > 			time1,
> > 			" min ",
> > 			"%~~%",
> > 			time1h,
> > 			"h"
> > 		),
> > 		list(
> > 			time1=round(time1,digits=0),
> > 			time1h=round(time1/60,digits=1)
> > 		)
> > 	)
> > )
> 
> 
>    substitute(OD[600][~nm] * " of 1 at " *
>      time1 * " min" %~~% time1h * h,
>        list(time1=round(time1,digits=0),
>             time1h=round(time1/60,digits=1))
>    )
> 
> 
> 
> Uwe Ligges
> 
> 
> 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 
> 
> ------------------------------
> 
> Message: 109
> Date: Fri, 27 Aug 2004 09:35:07 +0200
> From: Raphael Schneider <rasch at med1.med.tum.de>
> Subject: Re: [R] read.spss warning: unrecognized record type??
> To: r-help at stat.math.ethz.ch
> Message-ID: <200408270935.07786.rasch at med1.med.tum.de>
> Content-Type: text/plain;  charset="iso-8859-1"
> 
> On Friday 27 August 2004 02:55, Michael Jerosch-Herold wrote:
> > When using read.spss (library: 'foreign') I get the following warning
> > message:
> >
> > Warning message:
> > E:/R4win/mesamri.sav: Unrecognized record type 7, subtype 13
> > encountered in system file.
> >
> > I don't see anything wrong with record #7 in the database I am trying
> > to read in, but I suspect that the warning message does not refer to a
> > specific record, but a "variable" type. Is this correct? And what does
> > "subtype 13" mean?
> I get the same message but it seems that all data is read correctly. My 
> impression is that this message shows up only reading datasets stored with 
> SPSS12. At the moment I have no dataset where I know exactly this was stored 
> using a SPSS version below 12.
> 
> Raphael
> 
> 
> 
> ------------------------------
> 
> Message: 110
> Date: Fri, 27 Aug 2004 09:53:00 +0200
> From: Thomas Petzoldt <thpe at hhbio.wasser.tu-dresden.de>
> Subject: Re: [R] text() with text, variables and math HOWTO?
> To: Johannes Graumann <graumann at its.caltech.edu>
> Cc: r-help at stat.math.ethz.ch
> Message-ID: <412EE85C.9020009 at hhbio.wasser.tu-dresden.de>
> Content-Type: text/plain; charset=us-ascii; format=flowed
> 
> Johannes Graumann wrote:
> > Hello,
> > 
> > One more question from the 'abusing R for blotting - particularly
> > anally' department:
> > How can I in the expression below make the '%~~%' show up as the
> > aprrox-sign I want it to be?
> > 
> > Thanks for any hint,
> 
> Your code does not work because "%~~%" is a character string and %~~% is 
> an invalid mathematical expression. The operator (e.g. %~~%) must be 
> between something, so it works if you use phantom(0)%~~%phantom(0), and 
> I found that phantom() without 0 works too:
> 
> See the following:
> 
> 
> plot(0, xlim=c(0,1000), ylim=c(0,2), type="n")
> 
> time1=99
> 
> text(
> 	500,1.5,
> 	cex=0.75,
> 	substitute(
> 		paste(
> 			OD[600][~nm],
> 			" of 1 at ",
> 			time1,
> 			" min ",
> 			phantom()%~~%phantom(),
> 			time1h,
> 			"h"
> 		),
> 		list(
> 			time1=round(time1,digits=0),
> 			time1h=round(time1/60,digits=1)
> 		)
>        )
> )
> 
> Hope it helps
> 
> Thomas P.
> 
> 
> 
> ------------------------------
> 
> Message: 111
> Date: Fri, 27 Aug 2004 09:58:43 +0200 (CEST)
> From: Roger Bivand <Roger.Bivand at nhh.no>
> Subject: Re: [R] Newbie Question:  Spatial Autocorrelation with R
> 	Tutorial?
> To: Jeff Hollister <jeff at edc.uri.edu>
> Cc: r-help at stat.math.ethz.ch
> Message-ID: <Pine.LNX.4.44.0408270933110.20757-100000 at reclus.nhh.no>
> Content-Type: TEXT/PLAIN;	charset=ISO-8859-1
> 
> On Wed, 25 Aug 2004, Jeff Hollister wrote:
> 
> > Howdy All,
> > 
> > I am looking for some good tutorials (books, websites, whatever) for
> > calculating/testing for Spatial Autocorrelation using R.
> > 
> > Specifically, I am wanting to test for autocorrelation of a number of
> > variables measured at a set of discrete locations.
> > 
> 
> >From your signature line, "Environmental Data", spatial autocorrelation 
> could mean a number of things, depending on whether the variables could be 
> measurements of a continuous surface of values at your discrete locations, 
> or whether the discrete locations are "spatial entities" formed as areal 
> aggregations of some kind. Since you mention spdep below, I'm assuming 
> that the data you are working on refer to "spatial entities", for which 
> Moran's I would be a reasonable choice of test. If the variable of 
> interest isn't of this form, then other packages are more relevant (see R 
> spatial projects link below).
> 
> > Up to this point I have been exploring the "spdep" package and I can get
> > "moran.test" to work, but I am concerned that somewhere along the line I
> > may not be doing things correctly.  Hence my request for a tutorial so
> > that I may brush up on my autocorrelation basics, specifically
> > autocorrelation with R, and reassure myself that the results I am
> > getting aren't bogus.
> 
> Admittedly, the help page for moran.test() simply refers to Cliff, A. D., 
> Ord, J. K. 1981 Spatial processes, Pion, p. 21 as the original source, and 
> the "sids" vignette (see the foot of the output of help(package=spdep) to 
> locate it on your system) is incomplete. My guess is however that if your 
> data are for "spatial entities", theb constructing a sensible 
> neighbour weights is at least 75% of the work - you will also see this in 
> Virgilio G??mez-Rubio's "DCluster" package, and the existing "sids" 
> vignette does cover that a little. Completing and improving this vignette 
> is on my TODO list.
> 
> If you are unsure of the result, and want to stay within the R framework, 
> consider calculating Moran's I using DCluster, or gearymoran() in "ade4". 
> Beyond that, you could access the GeoDa software (Windows, not R) and 
> documentation at http://sal.agecon.uiuc.edu/csiss/geoda.html, the site 
> also housing the R spatial projects web pages:
> 
> http://agec221.agecon.uiuc.edu/csiss/Rgeo/
> 
> Please contact me off-list, or on the R-sig-geo list if you feel that 
> would help.
> 
> Best wishes,
> 
> Roger Bivand
> 
> > 
> > Thanks in advance for any suggestions!
> > 
> > Jeff Hollister
> >   
> > *****************************************************
> > Jeffrey William Hollister                          
> > Ph.D. Candidate                                      
> > Environmental Data Center                       
> > Department of Natural Resources Science  
> > University of Rhode Island                           
> > office: (401) 874 5054
> > fax: (401) 874 4561
> > cell: (401)556 4087
> > http://www.edc.uri.edu/personal/jeff/home/jwh_cv_full.htm
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> > 
> 
> -- 
> Roger Bivand
> Economic Geography Section, Department of Economics, Norwegian School of
> Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
> Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
> e-mail: Roger.Bivand at nhh.no
> 
> 
> 
> ------------------------------
> 
> Message: 112
> Date: Fri, 27 Aug 2004 10:32:46 +0100
> From: Patrick Burns <pburns at pburns.seanet.com>
> Subject: Re: [R] Surprise when mapping matrix to image
> To: Peter Dalgaard <p.dalgaard at biostat.ku.dk>
> Cc: r-help at stat.math.ethz.ch
> Message-ID: <412EFFBE.6030703 at pburns.seanet.com>
> Content-Type: text/plain; charset=ISO-8859-1; format=flowed
> 
> 
> 
> Peter Dalgaard wrote:
> 
> >"Richard A. O'Keefe" <ok at cs.otago.ac.nz> writes:
> >  
> >
> [snip]
> 
> >  
> >
> >
> >Good idea, but perhaps not phrased sharply enough to catch the user's
> >eye. How about something like this:
> >
> >"Notice that image() interprets a matrix as a table of f(x_i, y_j), so
> >the x axis corresponds to row number and the y axis to column number
> >(bottom to top)."
> >
> >and then use Brian's one-liner in the examples section?
> >  
> >
> 
> Though I think the added text is a good idea, the addition of the example
> is probably 102.58 times more useful than any text that could be inserted.
> 
> The examples in help files are probably the main reason that people think
> of R having better documentation than other programs.  Like Unix man
> pages, for example, which would be wonderful if only they had examples
> so that you could figure out what the sentences mean when you understand
> every word.
> 
> Patrick Burns
> 
> Burns Statistics
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
> 
> 
> 
> ------------------------------
> 
> Message: 113
> Date: 27 Aug 2004 11:35:14 +0200
> From: Peter Dalgaard <p.dalgaard at biostat.ku.dk>
> Subject: Re: [R] Surprise when mapping matrix to image
> To: Patrick Burns <pburns at pburns.seanet.com>
> Cc: r-help at stat.math.ethz.ch, Peter Dalgaard
> 	<p.dalgaard at biostat.ku.dk>
> Message-ID: <x2pt5crkj1.fsf at biostat.ku.dk>
> Content-Type: text/plain; charset=us-ascii
> 
> Patrick Burns <pburns at pburns.seanet.com> writes:
> 
> > Peter Dalgaard wrote:
> > 
> > >"Richard A. O'Keefe" <ok at cs.otago.ac.nz> writes:
> > >
> > [snip]
> > 
> > >  Good idea, but perhaps not phrased sharply enough to catch the
> > > user's
> > >eye. How about something like this:
> > >
> > >"Notice that image() interprets a matrix as a table of f(x_i, y_j), so
> > >the x axis corresponds to row number and the y axis to column number
> > >(bottom to top)."
> > >
> > >and then use Brian's one-liner in the examples section?
> > >
> > 
> > Though I think the added text is a good idea, the addition of the example
> > is probably 102.58 times more useful than any text that could be inserted.
> > 
> > The examples in help files are probably the main reason that people think
> > of R having better documentation than other programs.  Like Unix man
> > pages, for example, which would be wonderful if only they had examples
> > so that you could figure out what the sentences mean when you understand
> > every word.
> 
> There are limits, I think. If you put too many variations into the
> example section, it tends to become a grey blob of code. Anyways, I'm
> going to add both.
> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907
> 
> 
> 
> ------------------------------
> 
> _______________________________________________
> R-help at stat.math.ethz.ch mailing list  
> https://stat.ethz.ch/mailman/listinfo/r-help 
> PLEASE read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 
> End of R-help Digest, Vol 18, Issue 27
> **************************************



From jmacdon at med.umich.edu  Fri Aug 27 19:32:00 2004
From: jmacdon at med.umich.edu (James W. MacDonald)
Date: Fri, 27 Aug 2004 13:32:00 -0400
Subject: [R] selecting unique columns of a matrix/data frame
In-Reply-To: <1093627099.4141.22.camel@iwi142>
References: <200408271008.i7RA4NUb030086@hypatia.math.ethz.ch>
	<1093627099.4141.22.camel@iwi142>
Message-ID: <412F7010.3040005@med.umich.edu>

Stephen Nyangoma wrote:

> Hi all,
> I have a very high dimensional data and apparently there are several
> columns that contain similar information (some columns are equal). I
> want to form a matrix/data frame consisting of unique columns. Does
> anyone have an efficient way of getting out these columns. A small
> section of the data frame is given below.
> 
> Thanks for helping.
> 
> Stephen.
?unique
-- 
James W. MacDonald
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109



From sorenh at agrsci.dk  Fri Aug 27 19:45:33 2004
From: sorenh at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Fri, 27 Aug 2004 19:45:33 +0200
Subject: [R] Reading SAS data into R
Message-ID: <001801c48c5d$abcbe4d0$ba055b53@djf.agrsci.dk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040827/86b2049b/attachment.pl

From sundar.dorai-raj at PDF.COM  Fri Aug 27 19:59:02 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Fri, 27 Aug 2004 10:59:02 -0700
Subject: [R] Reading SAS data into R
In-Reply-To: <001801c48c5d$abcbe4d0$ba055b53@djf.agrsci.dk>
References: <001801c48c5d$abcbe4d0$ba055b53@djf.agrsci.dk>
Message-ID: <412F7666.8000107@pdf.com>


S??ren H??jsgaard wrote:

> Dear all,
> One of my students (whom I am trying to convince to use R) wants to get a fairly large SAS dataset into R (about 150mB). An obvious and simple thing she tried was to write the dataset as a .csv-file and then read that into R, but that takes forever (or something close to that..). The dataset is so large, that exporting it as an Excel file from SAS is not feasible (more than 65000 lines). I am reluctant to ask her to go through all the data base steps (then she'll just stick to SAS...). Can anyone help me out on that one? 
> Thanks in advance
> S??ren H??jsgaard
> 
> 

See ?read.ssd in package:foreign or ?sas.get in package:Hmisc. Both 
require you have SAS installed and in your PATH.

--sundar



From drf5n at maplepark.com  Fri Aug 27 21:52:15 2004
From: drf5n at maplepark.com (David Forrest)
Date: Fri, 27 Aug 2004 14:52:15 -0500 (CDT)
Subject: [R] Reading SAS data into R
In-Reply-To: <412F7666.8000107@pdf.com>
References: <001801c48c5d$abcbe4d0$ba055b53@djf.agrsci.dk>
	<412F7666.8000107@pdf.com>
Message-ID: <Pine.LNX.4.58.0408271436540.558@maplepark.com>

On Fri, 27 Aug 2004, Sundar Dorai-Raj wrote:

>
> S??ren H??jsgaard wrote:
>
> > Dear all,
> > One of my students (whom I am trying to convince to use R) wants to get a fairly large SAS dataset into R (about 150mB). An obvious and simple thing she tried was to write the dataset as a .csv-file and then read that into R, but that takes forever (or something close to that..). The dataset is so large, that exporting it as an Excel file from SAS is not feasible (more than 65000 lines). I am reluctant to ask her to go through all the data base steps (then she'll just stick to SAS...). Can anyone help me out on that one?
> > Thanks in advance
> > S??ren H??jsgaard
> >
> >
>
> See ?read.ssd in package:foreign or ?sas.get in package:Hmisc. Both
> require you have SAS installed and in your PATH.

Those routines use the SAS XPORT procedure, which is a good
semi-non-proprietary way to save your data anyway.  I just fought with
this a long while and found that XPORT needs to write short variable
names, but cannot accept the longer table names.  The short stub of SAS I
needed was:


Given a sas file '/junk/data.sas7bdat' files, the short stub of SAS code should
extract the data to '/junk/rd.xport' (The paths here may be all messed
up, but maybe the options below will save you the hours I lost.)


 libname src2rd '/junk/data'
 libname rd xport '/junk/rd'
 proc copy in=src2rd out=rd;
 select data;

However it may fail if there are long variable names.  The SAS below works
around that:

 options VALIDVARNAME=V7; /* enable longnames of {V6|V7|UPCASE|ANY} */
 data shrtnm;
    set verylonggtablename;
 run;

 However, when longnames are enabled, the XPORT procedure fails on long
variable names, so the option must be set to truncate the long variable
names into shortnames:


 options VALIDVARNAME=V6; /* enable xport varname truncation */
 proc copy in=work out=rd1 memtype=data;
    select shrtnm ;
 run;

Once this is done, the data may be imported into R using read.xport
Library(foreign) :

 x<-read.xport("/junk/x.xport")


R is sooo much better than SAS.

Dave
-- 
 Dave Forrest
 drf at vims.edu                                    (804)684-7900w
 drf5n at maplepark.com                             (804)642-0662h
                                   http://maplepark.com/~drf5n/



From oliver at imcs.marine.rutgers.edu  Fri Aug 27 23:10:11 2004
From: oliver at imcs.marine.rutgers.edu (Matt Oliver)
Date: Fri, 27 Aug 2004 17:10:11 -0400
Subject: [R] ANCOVA
Message-ID: <5.0.0.25.2.20040827163747.024dc758@imcs.rutgers.edu>

Dear R-help list,

I am attempting to understand the proper formulation of ANCOVA's in R. I 
would like to test both parallelism and intercept equality for some data 
sets, so I have generated an artificial data set to ease my understanding.

This is what I have done

#Limits of random error added to vectors
min <- -0.1
max <- 0.1

x <- c(c(1:10), c(1:10))+runif(20, min, max)
x1 <- c(c(1:10), c(1:10))+runif(20, min, max)
y <- c(c(1:10), c(10:1))+runif(20, min, max)
z <- c(c(1:10), c(11:20))+runif(20, min, max)
g <- as.factor(c(rep(1, 10), rep(2, 10)))

#x and x1 have similar slopes and have the similar intercepts,
#x and y have different slopes and different intercepts
#x and z have similar slopes with different intercepts

#These are my full effects models

fitx1x <- lm(x1~g+x+x:g)
fityx <- lm(y~g+x+x:g)
fitzx <- lm(z~g+x+x:g)


anova(fitx1x)

Analysis of Variance Table

Response: x1
                 Df       Sum Sq      Mean Sq         F 
value           Pr(>F)
g               1         0.002           0.002              0.3348 
0.5709
x               1         163.927      163.927          23456.8319 
<2e-16 ***
g:x            1          0.002          0.002               0.2671 
0.6123
Residuals 16          0.112          0.007
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1

These results confirm that x and x1 do not have significantly different 
means with respect to g;
There is a significant linear relationship between x and x1 independent of g
There is no evidence that the slopes of x and x1 in the two g groups is 
different

anova(fityx)

Analysis of Variance Table

Response: y
                     Df       Sum Sq      Mean Sq         F 
value      Pr(>F)
g                   1           0.012          0.012            1.7344 
0.2064
x                   1           0.003          0.003            0.4399 
0.5166
g:x                 1          164.947       164.947      24274.4246 <2e-16 ***
Residuals       16            0.109        0.007
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1

These results confirm that x and y do not have significantly different 
means with respect to g;
There is a not a significant linear relationship between x and y 
independent of g
There is evidence that the slopes of x and y in the two g groups is 
significantly different.


anova(fitzx)

Analysis of Variance Table

Response: z
                    Df        Sum Sq      Mean Sq      F value      Pr(>F)
g                  1          501.07        501.07    52709.9073  <2e-16 ***
x                  1          165.39        165.39    17398.4057  <2e-16 ***
g:x                1            0.02          0.02         1.7472 
0.2048
Residuals     16            0.15          0.01
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1

These results confirm that x and z  have significantly different means with 
respect to g;
There is a a significant linear relationship between x and z independent of g
There is no evidence that the slopes of x and z in the two g groups is 
significantly different.


What I don't understand is how to formulate the model so that I can tell if 
the intercepts between the g groups are different.
Also, how would I formulate an ANCOVA if I am dealing with Model II 
regressions?


Any help would be greatly appreciated.

Matt



==============================
When you reach an equilibrium in biology,
you're dead. - A. Mandell
==============================
Matthew J. Oliver
Institute of Marine and Coastal Sciences
71 Dudley Road, New Brunswick
New Jersey, 08901
http://marine.rutgers.edu/cool/



From drf5n at maplepark.com  Fri Aug 27 23:06:41 2004
From: drf5n at maplepark.com (David Forrest)
Date: Fri, 27 Aug 2004 16:06:41 -0500 (CDT)
Subject: [R] Plotting irregular grid as image or persp
Message-ID: <Pine.LNX.4.58.0408271527040.558@maplepark.com>

Hi,
   I have an array of 2d node locations and an array triangles, and would
like to plot something like a image or persp.


An example of doing it with rgl is:

  library(ncdf)
  library(rgl)
  # wget http://www.maplepark.com/~drf5n/extras/teapot.nc
  teapot<-open.ncdf("teapot.nc")
  z<-get.var.ncdf(teapot,"tris")
  l<-get.var.ncdf(teapot,"locations")
  z<-as.vector(z)

 demo.teapot<-function(x=1){
  rgl.triangles(l[1,z],l[2,z],l[3,z])
  x<-readline("Hit any key to continue")
  rgl.pop()}

 demo.teapot() # produces a 3d rgl plot of the teapot

I wrote some rgl.* helper functions (
http://www.maplepark.com/~drf5n/cgi-bin/wiki.cgi?ElcircVisualization )
to drive rgl.points(), rgl.lines(), rgl.triangles(), rgl.quads() such that
I can call :

rgl.drf5n.polys(verts=l,conns=z) # plot points, wireframe, triangles,
# quads, depending on shape of conns)

but I'd really like to be able to plot an irregular grid with the
known triangles as specified colors, and I'm not sure
what would be the best package.

library(fields) extrapolates the image beyond the data
library(tripack) refits and plots its own data

What should I use? Suggestions?

Dave
-- 
 Dave Forrest
 drf at vims.edu                                    (804)684-7900w
 drf5n at maplepark.com                             (804)642-0662h
                                   http://maplepark.com/~drf5n/



From sgilpin at doubleclick.net  Sat Aug 28 00:40:20 2004
From: sgilpin at doubleclick.net (Gilpin, Scott)
Date: Fri, 27 Aug 2004 16:40:20 -0600
Subject: [R] Reading SAS data into R
Message-ID: <897DB3A3E951174E8CBB850E96AD500E014A1542@THN-EXCLS1.dc1.doubleclick.corp>



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
> bounces at stat.math.ethz.ch] On Behalf Of S??ren H??jsgaard
> Sent: Friday, August 27, 2004 11:46 AM
> To: r-help at stat.math.ethz.ch
> Cc: S??ren H??jsgaard
> Subject: [R] Reading SAS data into R
> 
> Dear all,
> One of my students (whom I am trying to convince to use R) wants to get a
> fairly large SAS dataset into R (about 150mB). An obvious and simple thing
> she tried was to write the dataset as a .csv-file and then read that into
> R, but that takes forever (or something close to that..). The dataset is
> so large, that exporting it as an Excel file from SAS is not feasible
> (more than 65000 lines). I am reluctant to ask her to go through all the
> data base steps (then she'll just stick to SAS...). Can anyone help me out
> on that one?

What platform are you on, and how much memory do you have?  150mb isn't *that* large - but it will depend on your system.  See the FAQs regarding memory issues, as well as ?mem.limits and ?gc

It also depends on the type of data you're working with (integers take half the space of numerics) and what type of analysis you want to do.  The CSV approach should work fine - but you'll want to use scan instead of read.table.  You can use scan to read data in chunks (using skip and nlines), do something useful with this chunk of data, run gc(), and then read in another chunk.

In general, users I've seen who try to go from SAS to R don't seem to realize that R is not a data manipulation language, and hence just try to shove their entire dataset into R and manipulate it (which is what they would do in SAS).  Perl, awk, cut, etc. (not to mention DBMSes) are all very useful for processing data before putting it into R.


> Thanks in advance
> S??ren H??jsgaard
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From chris at lordsutch.com  Sat Aug 28 09:29:08 2004
From: chris at lordsutch.com (Chris Lawrence)
Date: Sat, 28 Aug 2004 02:29:08 -0500
Subject: [R] FIML in lme
In-Reply-To: <412F36DF.80203@stat.wisc.edu>
References: <BAY2-F35FHLqbp3eccm000238e7@hotmail.com>
	<412F36DF.80203@stat.wisc.edu>
Message-ID: <20040828072908.GA17117@bellsouth.net>

On Aug 27, Douglas Bates wrote:
> F Z wrote:
> >I was asked if lme can use FIML (Full Information Maximum Likelihood) 
> >instead of REML or ML but I don't know the answer.  Does anybody know if 
> >this is implemented in R?
> 
> To the best of my knowledge, FIML is ML so the answer is yes.
> 
> For example, the phrase "Full Information Maximum Likelihood" is used in 
> Singer and Willett (2004) "Applied Longitudinal Data Analysis" (Oxford 
> University Press) as a synonym for maximum likelihood.

I have seen FIML used to refer to a type of ML estimation where a
missing data treatment is included in the estimation procedure
(parameter estimates are derived from incomplete cases for only the
variables present in the case, rather than simply discarding the
cases), at least in the latent-variable SEM context, specifically in
AMOS.  This may be what Francisco is getting at.

To my knowledge, no R packages implement this sort of "FIML", for any
class of models, although there are other available missing data
treatments (EM, MCMC estimation).


Chris
-- 
Christopher N. Lawrence, Ph.D.
Visiting Assistant Professor of Political Science
Millsaps College
1701 N. State St
Jackson, MS 39210
(601) 974-1438 / lawrecn at millsaps.edu



From f.harrell at vanderbilt.edu  Sat Aug 28 14:58:16 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sat, 28 Aug 2004 07:58:16 -0500
Subject: [R] Reading SAS data into R
Message-ID: <41308168.6070307@vanderbilt.edu>

------- Begin quoted text
From: "Gilpin, Scott" <sgilpin at doubleclick.net>
 >-----Original Message-----
 >> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-
 >> bounces at stat.math.ethz.ch] On Behalf Of S??ren H??jsgaard
 >> Sent: Friday, August 27, 2004 11:46 AM
 >> To: r-help at stat.math.ethz.ch
 >> Cc: S??ren H??jsgaard
 >> Subject: [R] Reading SAS data into R
 >>
 >> Dear all,
 >> One of my students (whom I am trying to convince to use R) wants to 
get a
 >> fairly large SAS dataset into R (about 150mB). An obvious and simple 
thing
 >> she tried was to write the dataset as a .csv-file and then read that 
into
 >> R, but that takes forever (or something close to that..). The dataset is
 >> so large, that exporting it as an Excel file from SAS is not feasible
 >> (more than 65000 lines). I am reluctant to ask her to go through all the
 >> data base steps (then she'll just stick to SAS...). Can anyone help 
me out
 >> on that one?


What platform are you on, and how much memory do you have?  150mb isn't 
*that* large - but it will depend on your system.  See the FAQs 
regarding memory issues, as well as ?mem.limits and ?gc

It also depends on the type of data you're working with (integers take 
half the space of numerics) and what type of analysis you want to do. 
The CSV approach should work fine - but you'll want to use scan instead 
of read.table.  You can use scan to read data in chunks (using skip and 
nlines), do something useful with this chunk of data, run gc(), and then 
read in another chunk.

In general, users I've seen who try to go from SAS to R don't seem to 
realize that R is not a data manipulation language, and hence just try 
to shove their entire dataset into R and manipulate it (which is what 
they would do in SAS).  Perl, awk, cut, etc. (not to mention DBMSes) are 
all very useful for processing data before putting it into R.

-----------End quoted text

R is also a data manipulation language and once you get used to it it is 
better than SAS at data manipulation, for non-huge datasets.  We have 
many examples of data manipulation with R on our web site 
http://biostat.mc.vanderbilt.edu (see especially the Alzola and Harrell 
text).

What we do for importing SAS datasets is described at 
http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/SASexportHowto . 
This will preserve labels and value labels, handle dates, times, 
date/times, and get around problems we've faced in importing SAS V5 
transport files using the foreign package, by having SAS run PROC EXPORT 
to create csv files.

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From nov_tao at yahoo.com  Sat Aug 28 22:15:14 2004
From: nov_tao at yahoo.com (Y Tao)
Date: Sat, 28 Aug 2004 13:15:14 -0700 (PDT)
Subject: [R] Handling of special characters by xtable
Message-ID: <20040828201514.28325.qmail@web53509.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040828/6451c007/attachment.pl

From mhassan at scitegic.com  Sun Aug 29 01:25:22 2004
From: mhassan at scitegic.com (Moises Hassan)
Date: Sat, 28 Aug 2004 16:25:22 -0700
Subject: [R] removing invariant columns from a matrix
Message-ID: <830D8D4719112B418ABBC3A0EBA9581272EAEA@webmail.scitegic.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040828/2c68539b/attachment.pl

From andy_liaw at merck.com  Sun Aug 29 01:39:43 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 28 Aug 2004 19:39:43 -0400
Subject: [R] removing invariant columns from a matrix
Message-ID: <3A822319EB35174CA3714066D590DCD504AF82B4@usrymx25.merck.com>

Something like:

keep <- apply(myData, 2, function(x) diff(range(x)) > 0)
newData <- myData[, keep]

Andy

> From: Moises Hassan
> 
> I'm looking for an efficient way of removing zero-variance 
> columns from
> a large matrix.
> 
> Any suggestions?
> 
> Thanks,
> 
>    - Moises
>



From p.dalgaard at biostat.ku.dk  Sun Aug 29 01:37:15 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 29 Aug 2004 01:37:15 +0200
Subject: [R] removing invariant columns from a matrix
In-Reply-To: <830D8D4719112B418ABBC3A0EBA9581272EAEA@webmail.scitegic.com>
References: <830D8D4719112B418ABBC3A0EBA9581272EAEA@webmail.scitegic.com>
Message-ID: <x24qmmq1g4.fsf@biostat.ku.dk>

"Moises Hassan" <mhassan at scitegic.com> writes:

> I'm looking for an efficient way of removing zero-variance columns from
> a large matrix.
> 
> Any suggestions?

A[,apply(A,2,var)>0]

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jfox at mcmaster.ca  Sun Aug 29 01:44:38 2004
From: jfox at mcmaster.ca (John Fox)
Date: Sat, 28 Aug 2004 19:44:38 -0400
Subject: [R] ANCOVA
In-Reply-To: <5.0.0.25.2.20040827163747.024dc758@imcs.rutgers.edu>
Message-ID: <20040828234437.FKAH14082.tomts16-srv.bellnexxia.net@JohnDesktop8300>

Dear Matt,

The sequential sums of squares produced by anova() test for g ignoring x
(and the interaction), x after g (and ignoring the interaction), and the x:g
interaction after g and x. The second and third test are generally sensible,
but the first doesn't adjust for x, which is probably not what you want in
general (although you've constructed x to be independent of g, which is
typically not the case). Note that you would not usually want to test for
equal intercepts in the model that doesn't constrain the slopes to be equal
(though you haven't done this): Among other things, 0 is outside the range
of x.

The Anova() function in the car package will produce so-called type-II
tests.

I hope that this helps.
 John

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Matt Oliver
> Sent: Friday, August 27, 2004 4:10 PM
> To: R-help at stat.math.ethz.ch
> Subject: [R] ANCOVA
> 
> Dear R-help list,
> 
> I am attempting to understand the proper formulation of 
> ANCOVA's in R. I would like to test both parallelism and 
> intercept equality for some data sets, so I have generated an 
> artificial data set to ease my understanding.
> 
> This is what I have done
> 
> #Limits of random error added to vectors min <- -0.1 max <- 0.1
> 
> x <- c(c(1:10), c(1:10))+runif(20, min, max)
> x1 <- c(c(1:10), c(1:10))+runif(20, min, max) y <- c(c(1:10), 
> c(10:1))+runif(20, min, max) z <- c(c(1:10), 
> c(11:20))+runif(20, min, max) g <- as.factor(c(rep(1, 10), 
> rep(2, 10)))
> 
> #x and x1 have similar slopes and have the similar 
> intercepts, #x and y have different slopes and different 
> intercepts #x and z have similar slopes with different intercepts
> 
> #These are my full effects models
> 
> fitx1x <- lm(x1~g+x+x:g)
> fityx <- lm(y~g+x+x:g)
> fitzx <- lm(z~g+x+x:g)
> 
> 
> anova(fitx1x)
> 
> Analysis of Variance Table
> 
> Response: x1
>                  Df       Sum Sq      Mean Sq         F 
> value           Pr(>F)
> g               1         0.002           0.002              0.3348 
> 0.5709
> x               1         163.927      163.927          23456.8319 
> <2e-16 ***
> g:x            1          0.002          0.002               0.2671 
> 0.6123
> Residuals 16          0.112          0.007
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
> 
> These results confirm that x and x1 do not have significantly 
> different means with respect to g; There is a significant 
> linear relationship between x and x1 independent of g There 
> is no evidence that the slopes of x and x1 in the two g 
> groups is different
> 
> anova(fityx)
> 
> Analysis of Variance Table
> 
> Response: y
>                      Df       Sum Sq      Mean Sq         F 
> value      Pr(>F)
> g                   1           0.012          0.012          
>   1.7344 
> 0.2064
> x                   1           0.003          0.003          
>   0.4399 
> 0.5166
> g:x                 1          164.947       164.947      
> 24274.4246 <2e-16 ***
> Residuals       16            0.109        0.007
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
> 
> These results confirm that x and y do not have significantly 
> different means with respect to g; There is a not a 
> significant linear relationship between x and y independent 
> of g There is evidence that the slopes of x and y in the two 
> g groups is significantly different.
> 
> 
> anova(fitzx)
> 
> Analysis of Variance Table
> 
> Response: z
>                     Df        Sum Sq      Mean Sq      F 
> value      Pr(>F)
> g                  1          501.07        501.07    
> 52709.9073  <2e-16 ***
> x                  1          165.39        165.39    
> 17398.4057  <2e-16 ***
> g:x                1            0.02          0.02         1.7472 
> 0.2048
> Residuals     16            0.15          0.01
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
> 
> These results confirm that x and z  have significantly 
> different means with respect to g; There is a a significant 
> linear relationship between x and z independent of g There is 
> no evidence that the slopes of x and z in the two g groups is 
> significantly different.
> 
> 
> What I don't understand is how to formulate the model so that 
> I can tell if the intercepts between the g groups are different.
> Also, how would I formulate an ANCOVA if I am dealing with 
> Model II regressions?
> 
> 
> Any help would be greatly appreciated.
> 
> Matt
> 
> 
> 
> ==============================
> When you reach an equilibrium in biology, you're dead. - A. 
> Mandell ============================== Matthew J. Oliver 
> Institute of Marine and Coastal Sciences
> 71 Dudley Road, New Brunswick
> New Jersey, 08901
> http://marine.rutgers.edu/cool/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Sun Aug 29 05:25:32 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sat, 28 Aug 2004 20:25:32 -0700
Subject: [R] removing invariant columns from a matrix
In-Reply-To: <x24qmmq1g4.fsf@biostat.ku.dk>
References: <830D8D4719112B418ABBC3A0EBA9581272EAEA@webmail.scitegic.com>
	<x24qmmq1g4.fsf@biostat.ku.dk>
Message-ID: <41314CAC.20904@pdf.com>

      Both the previous solutions seem to assume a numeric matrix.  How 
about the following: 

A <- array(letters[c(rep(1, 13), rep(2, 13), 1:26)], dim=c(13, 5))
A[, apply(A, 2, function(x)any(x[-1] != x[-length(x)]))]
      A[, apply(A, 2, function(x)any(x[-1] != x[-length(x)])]

      enjoy.  spencer graves

Peter Dalgaard wrote:

>"Moises Hassan" <mhassan at scitegic.com> writes:
>
>  
>
>>I'm looking for an efficient way of removing zero-variance columns from
>>a large matrix.
>>
>>Any suggestions?
>>    
>>
>
>A[,apply(A,2,var)>0]
>
>  
>



From ligges at statistik.uni-dortmund.de  Sun Aug 29 12:22:05 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 29 Aug 2004 12:22:05 +0200
Subject: [R] Handling of special characters by xtable
In-Reply-To: <20040828201514.28325.qmail@web53509.mail.yahoo.com>
References: <20040828201514.28325.qmail@web53509.mail.yahoo.com>
Message-ID: <4131AE4D.6070901@statistik.uni-dortmund.de>

Y Tao wrote:

> It seems that xtable does not escape special characters such as % (which indicates a comment line in LaTeX).
>

Please send your feature request (probably with R code contribution) to 
the package maintainer, David Dahl (in CC).

Uwe Ligges


> Try these few lines for example:
> 
>>library(xtable)
>>q<-data.frame(quantile(rnorm(100)))
>>xtable(q)
> 
>  
> This produces:
> 
> % latex table generated in R 1.9.1 by xtable 1.2-3 package
> % Sat Aug 28 16:11:05 2004
> \begin{table}[ht]
> \begin{center}
> \begin{tabular}{rr}
> \hline
>  & quantile.rnorm.100.. \\
> \hline
> 0% & $-$2.02 \\
> 25% & $-$0.70 \\
> 50% & $-$0.16 \\
> 75% & 0.54 \\
> 100% & 2.16 \\
> \hline
> \end{tabular}
> \end{center}
> \end{table}
> 
> 
> 
> 
> 		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From remigijus.lapinskas at mif.vu.lt  Sun Aug 29 13:21:04 2004
From: remigijus.lapinskas at mif.vu.lt (Remigijus Lapinskas)
Date: Sun, 29 Aug 2004 14:21:04 +0300
Subject: [R] predict(arima)
Message-ID: <18217271421.20040829142104@mif.vu.lt>

Dear All,

R 1.9.1, Windows

When copying and pasting a few lines from the 'predict.Arima' help, I
get an error message:

> data(lh)
> predict(arima(lh, order = c(3,0,0)), n.ahead = 12)
Error in eval(expr, envir, enclos) : Object "xreg" not found

On the other hand, the following is OK:

> data(lh)
> predict(arima0(lh, order = c(3,0,0)), n.ahead = 12)
$pred
Time Series:
Start = 49 
End = 60 
Frequency = 1 
 [1] 2.460173 2.270829 2.198597 2.260696 2.346933 2.414479 2.438918 2.431440 2.410223 2.391645 2.382653 2.382697

$se
Time Series:
Start = 49 
End = 60 
Frequency = 1 
 [1] 0.4226823 0.5029332 0.5245256 0.5247161 0.5305499 0.5369159 0.5388045 0.5388448 0.5391043 0.5395174 0.5396991 0.5397140

So, what is wrong with arima?

Thanking you in advance,
Rem



From ripley at stats.ox.ac.uk  Sun Aug 29 15:43:03 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 29 Aug 2004 14:43:03 +0100 (BST)
Subject: [R] predict(arima)
In-Reply-To: <18217271421.20040829142104@mif.vu.lt>
Message-ID: <Pine.LNX.4.44.0408291440050.2379-100000@gannet.stats>

Nothing is wrong with arima: those commands are part of make check, so 
your copy of R has been corrupted.  Search for masked functions, and try 
starting R with --vanilla.

On Sun, 29 Aug 2004, Remigijus Lapinskas wrote:

> Dear All,
> 
> R 1.9.1, Windows
> 
> When copying and pasting a few lines from the 'predict.Arima' help, I
> get an error message:
> 
> > data(lh)
> > predict(arima(lh, order = c(3,0,0)), n.ahead = 12)
> Error in eval(expr, envir, enclos) : Object "xreg" not found
> 
> On the other hand, the following is OK:
> 
> > data(lh)
> > predict(arima0(lh, order = c(3,0,0)), n.ahead = 12)
> $pred
> Time Series:
> Start = 49 
> End = 60 
> Frequency = 1 
>  [1] 2.460173 2.270829 2.198597 2.260696 2.346933 2.414479 2.438918 2.431440 2.410223 2.391645 2.382653 2.382697
> 
> $se
> Time Series:
> Start = 49 
> End = 60 
> Frequency = 1 
>  [1] 0.4226823 0.5029332 0.5245256 0.5247161 0.5305499 0.5369159 0.5388045 0.5388448 0.5391043 0.5395174 0.5396991 0.5397140
> 
> So, what is wrong with arima?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From economia at adinet.com.uy  Sun Aug 29 18:51:03 2004
From: economia at adinet.com.uy (Gerardo Prieto Blanco)
Date: Sun, 29 Aug 2004 13:51:03 -0300
Subject: [R] importing data in excel
Message-ID: <41320976.69645EB2@adinet.com.uy>

Hello, I need to care excel data to be used
in R,..., how do I make it?
Thank you and greetings, Gerardo Prieto



From sdavis2 at mail.nih.gov  Sun Aug 29 19:46:54 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Sun, 29 Aug 2004 13:46:54 -0400
Subject: [R] R and PostGresQL
Message-ID: <BD578ECE.281D%sdavis2@mail.nih.gov>

Sorry for the simple question:

What is the "standard" package for interfacing with PostGreSQL (most
up-to-date)?  There appear to be at least three with different names:

Rdbi.PgSQL
RdbiPgSQL (available as part of BioConductor)
RPgSQL (On the home page, no longer maintained in favor or Rdbi)

The R and database developer page (http://developer.r-project.org/db/) was
last updated in 2001, it appears.  The R Import-export manual addresses
RPgSQL, but not the other flavors mentioned above.

I am sure I have missed some key page stating the obvious.... Thanks in
advance for clarification.

Sean



From ligges at statistik.uni-dortmund.de  Sun Aug 29 20:04:42 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 29 Aug 2004 20:04:42 +0200
Subject: [R] importing data in excel
In-Reply-To: <41320976.69645EB2@adinet.com.uy>
References: <41320976.69645EB2@adinet.com.uy>
Message-ID: <41321ABA.30700@statistik.uni-dortmund.de>

Gerardo Prieto Blanco wrote:

> Hello, I need to care excel data to be used
> in R,..., how do I make it?
> Thank you and greetings, Gerardo Prieto

Please read the R Data Import/Export manual!

Uwe Ligges


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From mhassan at scitegic.com  Mon Aug 30 00:00:50 2004
From: mhassan at scitegic.com (Moises Hassan)
Date: Sun, 29 Aug 2004 15:00:50 -0700
Subject: [R] removing invariant columns from a matrix
Message-ID: <830D8D4719112B418ABBC3A0EBA9581272EAEB@webmail.scitegic.com>

It works great, except that in the case where only one column is left,
it returns a vector and the column name is lost. How can you avoid that
behavior?

Thanks,
   - Moises


-----Original Message-----
From: Spencer Graves [mailto:spencer.graves at pdf.com] 
Sent: Saturday, August 28, 2004 8:26 PM
To: Peter Dalgaard
Cc: Moises Hassan; R Help
Subject: Re: [R] removing invariant columns from a matrix

      Both the previous solutions seem to assume a numeric matrix.  How 
about the following: 

A <- array(letters[c(rep(1, 13), rep(2, 13), 1:26)], dim=c(13, 5))
A[, apply(A, 2, function(x)any(x[-1] != x[-length(x)]))]
      A[, apply(A, 2, function(x)any(x[-1] != x[-length(x)])]

      enjoy.  spencer graves

Peter Dalgaard wrote:

>"Moises Hassan" <mhassan at scitegic.com> writes:
>
>  
>
>>I'm looking for an efficient way of removing zero-variance columns
from
>>a large matrix.
>>
>>Any suggestions?
>>    
>>
>
>A[,apply(A,2,var)>0]
>
>  
>



From rpeng at jhsph.edu  Mon Aug 30 00:07:48 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Sun, 29 Aug 2004 18:07:48 -0400
Subject: [R] removing invariant columns from a matrix
In-Reply-To: <830D8D4719112B418ABBC3A0EBA9581272EAEB@webmail.scitegic.com>
References: <830D8D4719112B418ABBC3A0EBA9581272EAEB@webmail.scitegic.com>
Message-ID: <413253B4.8010508@jhsph.edu>

See question 7.7 in the R FAQ 
(http://cran.r-project.org/doc/FAQ/R-FAQ.html)

-roger

Moises Hassan wrote:
> It works great, except that in the case where only one column is left,
> it returns a vector and the column name is lost. How can you avoid that
> behavior?
> 
> Thanks,
>    - Moises
> 
> 
> -----Original Message-----
> From: Spencer Graves [mailto:spencer.graves at pdf.com] 
> Sent: Saturday, August 28, 2004 8:26 PM
> To: Peter Dalgaard
> Cc: Moises Hassan; R Help
> Subject: Re: [R] removing invariant columns from a matrix
> 
>       Both the previous solutions seem to assume a numeric matrix.  How 
> about the following: 
> 
> A <- array(letters[c(rep(1, 13), rep(2, 13), 1:26)], dim=c(13, 5))
> A[, apply(A, 2, function(x)any(x[-1] != x[-length(x)]))]
>       A[, apply(A, 2, function(x)any(x[-1] != x[-length(x)])]
> 
>       enjoy.  spencer graves
> 
> Peter Dalgaard wrote:
> 
> 
>>"Moises Hassan" <mhassan at scitegic.com> writes:
>>
>> 
>>
>>
>>>I'm looking for an efficient way of removing zero-variance columns
> 
> from
> 
>>>a large matrix.
>>>
>>>Any suggestions?
>>>   
>>>
>>
>>A[,apply(A,2,var)>0]
>>
>> 
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From mhassan at scitegic.com  Mon Aug 30 00:22:41 2004
From: mhassan at scitegic.com (Moises Hassan)
Date: Sun, 29 Aug 2004 15:22:41 -0700
Subject: [R] removing invariant columns from a matrix
Message-ID: <830D8D4719112B418ABBC3A0EBA9581272EAEC@webmail.scitegic.com>

Thanks, that solved the problem!

   - Moises


-----Original Message-----
From: Roger D. Peng [mailto:rpeng at jhsph.edu] 
Sent: Sunday, August 29, 2004 3:08 PM
To: Moises Hassan
Cc: R Help
Subject: Re: [R] removing invariant columns from a matrix

See question 7.7 in the R FAQ 
(http://cran.r-project.org/doc/FAQ/R-FAQ.html)

-roger

Moises Hassan wrote:
> It works great, except that in the case where only one column is left,
> it returns a vector and the column name is lost. How can you avoid
that
> behavior?
> 
> Thanks,
>    - Moises
> 
> 
> -----Original Message-----
> From: Spencer Graves [mailto:spencer.graves at pdf.com] 
> Sent: Saturday, August 28, 2004 8:26 PM
> To: Peter Dalgaard
> Cc: Moises Hassan; R Help
> Subject: Re: [R] removing invariant columns from a matrix
> 
>       Both the previous solutions seem to assume a numeric matrix.
How 
> about the following: 
> 
> A <- array(letters[c(rep(1, 13), rep(2, 13), 1:26)], dim=c(13, 5))
> A[, apply(A, 2, function(x)any(x[-1] != x[-length(x)]))]
>       A[, apply(A, 2, function(x)any(x[-1] != x[-length(x)])]
> 
>       enjoy.  spencer graves
> 
> Peter Dalgaard wrote:
> 
> 
>>"Moises Hassan" <mhassan at scitegic.com> writes:
>>
>> 
>>
>>
>>>I'm looking for an efficient way of removing zero-variance columns
> 
> from
> 
>>>a large matrix.
>>>
>>>Any suggestions?
>>>   
>>>
>>
>>A[,apply(A,2,var)>0]
>>
>> 
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>



From p.dalgaard at biostat.ku.dk  Mon Aug 30 00:18:38 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Aug 2004 00:18:38 +0200
Subject: [R] removing invariant columns from a matrix
In-Reply-To: <830D8D4719112B418ABBC3A0EBA9581272EAEB@webmail.scitegic.com>
References: <830D8D4719112B418ABBC3A0EBA9581272EAEB@webmail.scitegic.com>
Message-ID: <x21xhpmvup.fsf@biostat.ku.dk>

"Moises Hassan" <mhassan at scitegic.com> writes:

> It works great, except that in the case where only one column is left,
> it returns a vector and the column name is lost. How can you avoid that
> behavior?

Just add drop=FALSE to the indexing i.e. A[,cond,drop=FALSE] 


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Michael_Bibo at health.qld.gov.au  Mon Aug 30 03:28:17 2004
From: Michael_Bibo at health.qld.gov.au (Michael Bibo)
Date: Mon, 30 Aug 2004 11:28:17 +1000
Subject: [R] Rcmdr X11 protocol error message
Message-ID: <s1330f65.025@health-es2.health.qld.gov.au>

I am using R 1.9.1 with R Commander GUI under Windows at work,  and under Linux Mandrake 10 OR at home.   I am having no problems running R Commander under windows.  Under Linux, though, the opening and sometimes closing of windows from R Commander produces identical and repetitive error messages (in dialogue boxes and mirrored in the console): 

>"Warning: X11 protocol error: BadWindow (invalid Window parameter)"  X 10-20.

These error messages occur most frequently when graphics windows are opening, but can also occur when dialogue boxes pop up, and sometimes even when results are written to the output window.  However, when the error message is "OK'd", the plot, or whatever, is displayed as it should be.  It is not entirely consistent, and will sometimes stop happening for parts of a session.  Exactly the same commands run directly from the R console do not produce the same problem.  I searched the mailing list archives, and found the following exact description of what I have been experiencing, from November 2003:


> Message-id: <200311221950.36723.korponai at georgikon.hu>
> 
> Hi all,
> I've got this using Rcmdr package only.
> R-cmdr> scatterplot.matrix(~PC1+PC2+PC3+PC4 | Species,
> reg.line=FALSE, smooth=FALSE, span=0.5, diagonal= 'density',
> by.groups=TRUE, data=iris)
> Warning: X11 protocol error: BadWindow (invalid Window parameter)
> Warning: X11 protocol error: BadWindow (invalid Window parameter)
> Warning: X11 protocol error: BadWindow (invalid Window parameter)
> Warning: X11 protocol error: BadWindow (invalid Window parameter)
> Warning: X11 protocol error: BadWindow (invalid Window parameter)
> Warning: X11 protocol error: BadWindow (invalid Window parameter)
> Warning: X11 protocol error: BadWindow (invalid Window parameter)
> Warning: X11 protocol error: BadWindow (invalid Window parameter)
> Warning: X11 protocol error: BadWindow (invalid Window parameter)
> Warning: X11 protocol error: BadWindow (invalid Window parameter)
> Warning: X11 protocol error: BadWindow (invalid Window parameter)
> Warning: X11 protocol error: BadWindow (invalid Window parameter)
> Warning: X11 protocol error: BadWindow (invalid Window parameter)
> 
> What's this? The plot is shown.
 
This is exactly the error message I get, with similar 10-20 line repetition.  And the plots are generated anyway.  Unfortunately, I could not find any reply to this email.
 
My system is running R 1.9.1 on Linux Mandrake 10 OR (installed from source, not RPM), with XFree86 4.3, P4 2.4GH, 1GB Ram.  The problem occurs in several different window managers/desktop environments.  I'm wondering if it is something to do with the interaction of tcl/tk with the particular X11 config, but I've reached the limit of my skills in diagnosing.  If anyone can suggest anything, I would appreciate it.


Regards,

Michael Bibo

Research Officer,
Community Health Services,
West Moreton Health Service District,
Queensland Health.

michael_bibo at health.qld.gov.au 




***********************************************************************************
This email, including any attachments sent with it, is confi...{{dropped}}



From thpe at hhbio.wasser.tu-dresden.de  Mon Aug 30 09:21:00 2004
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Mon, 30 Aug 2004 09:21:00 +0200
Subject: [R] importing data in excel
In-Reply-To: <41321ABA.30700@statistik.uni-dortmund.de>
References: <41320976.69645EB2@adinet.com.uy>
	<41321ABA.30700@statistik.uni-dortmund.de>
Message-ID: <4132D55C.5000905@hhbio.wasser.tu-dresden.de>

Uwe Ligges wrote:
> Gerardo Prieto Blanco wrote:
> 
>> Hello, I need to care excel data to be used
>> in R,..., how do I make it?
>> Thank you and greetings, Gerardo Prieto
> 
> 
> Please read the R Data Import/Export manual!
> 
> Uwe Ligges

Hello Gerardo,

I completely agree with Uwe, and the following simple example may help:

1.) Create a rectangular area in Excel like this:

no	treat	val
1	a	1
2	a	5
3	b	3
4	b	3
5	b	2

2.) Mark and Copy these cells including the first row
=> the data are now in the so-called "Windows Clipboard"

3.) Switch to R and provide the following command (by typing it!, not by
copy and paste, because the clipboard already contains the data):

> dat <- read.table("clipboard", header=TRUE)

Now, the data are in dat and you can inspect them using:

> dat

or better

> str(dat)

Hints: depending on your configuration some additional options may be
necessary in read.table, e.g. sep="\t" (if your data contain empty
cells) or dec="," (on some non-English languages, e.g. German).
Furthermore there are many other ways to import data, see the manual.

Thomas P.



From p.dalgaard at biostat.ku.dk  Mon Aug 30 09:18:24 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Aug 2004 09:18:24 +0200
Subject: [R] Rcmdr X11 protocol error message
In-Reply-To: <s1330f65.025@health-es2.health.qld.gov.au>
References: <s1330f65.025@health-es2.health.qld.gov.au>
Message-ID: <x2n00dozzz.fsf@biostat.ku.dk>

"Michael Bibo" <Michael_Bibo at health.qld.gov.au> writes:

> I am using R 1.9.1 with R Commander GUI under Windows at work, and
> under Linux Mandrake 10 OR at home. I am having no problems running
> R Commander under windows. Under Linux, though, the opening and
> sometimes closing of windows from R Commander produces identical and
> repetitive error messages (in dialogue boxes and mirrored in the
> console):
> 
> >"Warning: X11 protocol error: BadWindow (invalid Window parameter)"  X 10-20.
> 
> These error messages occur most frequently when graphics windows are
> opening, but can also occur when dialogue boxes pop up, and
> sometimes even when results are written to the output window.
> However, when the error message is "OK'd", the plot, or whatever, is
> displayed as it should be. It is not entirely consistent, and will
> sometimes stop happening for parts of a session. Exactly the same
> commands run directly from the R console do not produce the same
> problem. I searched the mailing list archives, and found the
> following exact description of what I have been experiencing, from
> November 2003:

It's a bit hard to debug without actually installing Rcmdr (and all
its dependents), but this sort of message usually means either that a
window was invalid to begin with (a bit unlikely) or that it
disappeared while events were still being generated for it.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Wanzare at HCJP.com  Mon Aug 30 10:04:10 2004
From: Wanzare at HCJP.com (Manoj - Hachibushu Capital)
Date: Mon, 30 Aug 2004 17:04:10 +0900
Subject: [R] Multiple lapply get-around
Message-ID: <1CBA12F2D414914989C723D196B287DC055689@jp-svr-ex1.hcjp.com>

I am faced with a situation wherein I have to use multiple lapply's. The
pseudo-code could be approximated to something as below:

For each X from i=1 to n
	For each Y based on j=1 to m 
		For each F from 1 to f
			Do some calculation based on Fij
			Store Xi,Yj	= Fij
		End For F
	End for Y
End for X

Is there anyway to optimize the processing logic further? I *guess*
using the multiple lapply already optimizes the logic a little bit but
is there anything else that could be done (apart from the obvious
solution of writing a c-code)?

TIA

Manoj



From 0034058 at fudan.edu.cn  Mon Aug 30 10:22:39 2004
From: 0034058 at fudan.edu.cn (ronggui wong)
Date: Mon, 30 Aug 2004 16:22:39 +0800
Subject: [R] importing data in excel
In-Reply-To: <4132D55C.5000905@hhbio.wasser.tu-dresden.de>
References: <41320976.69645EB2@adinet.com.uy>
	<41321ABA.30700@statistik.uni-dortmund.de>
	<4132D55C.5000905@hhbio.wasser.tu-dresden.de>
Message-ID: <200408301622.39308.0034058@fudan.edu.cn>

if under  windows ,the command works well,but under linux ,it does NOT,as 
linux can keep couples of file in clipboar.so i want to know how to do 
similar thing under linux.anyone knows?    

the erroe msg is as follow:
> dat<-read.table('clipboard',header=T)
Error in file(file, "r") : unable to open connection
In addition: Warning message:
cannot open file `clipboard'


 2004  30  15:21Thomas Petzoldt 
> Uwe Ligges wrote:
> > Gerardo Prieto Blanco wrote:
> >> Hello, I need to care excel data to be used
> >> in R,..., how do I make it?
> >> Thank you and greetings, Gerardo Prieto
> >
> > Please read the R Data Import/Export manual!
> >
> > Uwe Ligges
>
> Hello Gerardo,
>
> I completely agree with Uwe, and the following simple example may help:
>
> 1.) Create a rectangular area in Excel like this:
>
> no	treat	val
> 1	a	1
> 2	a	5
> 3	b	3
> 4	b	3
> 5	b	2
>
> 2.) Mark and Copy these cells including the first row
> => the data are now in the so-called "Windows Clipboard"
>
> 3.) Switch to R and provide the following command (by typing it!, not by
>
> copy and paste, because the clipboard already contains the data):
> > dat <- read.table("clipboard", header=TRUE)
>
> Now, the data are in dat and you can inspect them using:
> > dat
>
> or better
>
> > str(dat)
>
> Hints: depending on your configuration some additional options may be
> necessary in read.table, e.g. sep="\t" (if your data contain empty
> cells) or dec="," (on some non-English languages, e.g. German).
> Furthermore there are many other ways to import data, see the manual.
>
> Thomas P.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Mon Aug 30 10:44:22 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 30 Aug 2004 09:44:22 +0100 (BST)
Subject: [R] importing data in excel
In-Reply-To: <200408301622.39308.0034058@fudan.edu.cn>
Message-ID: <Pine.LNX.4.44.0408300940530.17316-100000@gannet.stats>

The correct explanation is that 

	read.table("clipboard",header=TRUE)

(which you have entered differently and non-portably) is only
supported under Windows, as file="clipboard" is Windows-specific.
As ?bug.report and the FAQ ask, please do not report spurious 
`explanations', only facts.

The advice

> > > Please read the R Data Import/Export manual!

applies to you, too.


On Mon, 30 Aug 2004, ronggui wong wrote:

> if under  windows ,the command works well,but under linux ,it does NOT,as 
> linux can keep couples of file in clipboar.so i want to know how to do 
> similar thing under linux.anyone knows?    
> 
> the erroe msg is as follow:
> > dat<-read.table('clipboard',header=T)
> Error in file(file, "r") : unable to open connection
> In addition: Warning message:
> cannot open file `clipboard'
> 
> 
> ???? 2004 ???????? 30 ???????????? 15:21????Thomas Petzoldt ????????????
> > Uwe Ligges wrote:
> > > Gerardo Prieto Blanco wrote:
> > >> Hello, I need to care excel data to be used
> > >> in R,..., how do I make it?
> > >> Thank you and greetings, Gerardo Prieto
> > >
> > > Please read the R Data Import/Export manual!
> > >
> > > Uwe Ligges
> >
> > Hello Gerardo,
> >
> > I completely agree with Uwe, and the following simple example may help:
> >
> > 1.) Create a rectangular area in Excel like this:
> >
> > no	treat	val
> > 1	a	1
> > 2	a	5
> > 3	b	3
> > 4	b	3
> > 5	b	2
> >
> > 2.) Mark and Copy these cells including the first row
> > => the data are now in the so-called "Windows Clipboard"
> >
> > 3.) Switch to R and provide the following command (by typing it!, not by
> >
> > copy and paste, because the clipboard already contains the data):
> > > dat <- read.table("clipboard", header=TRUE)
> >
> > Now, the data are in dat and you can inspect them using:
> > > dat
> >
> > or better
> >
> > > str(dat)
> >
> > Hints: depending on your configuration some additional options may be
> > necessary in read.table, e.g. sep="\t" (if your data contain empty
> > cells) or dec="," (on some non-English languages, e.g. German).
> > Furthermore there are many other ways to import data, see the manual.
> >
> > Thomas P.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From karlknoblich at yahoo.de  Mon Aug 30 11:52:03 2004
From: karlknoblich at yahoo.de (=?iso-8859-1?q?Karl=20Knoblick?=)
Date: Mon, 30 Aug 2004 11:52:03 +0200 (CEST)
Subject: [R] Wrong result with cor(x, y, method="spearman",
	use="complete.obs") with NA's???
Message-ID: <20040830095203.87768.qmail@web52503.mail.yahoo.com>

Hallo!

Is there an error in cor to calculate Spearman
correlation with cor if there are NA's? cor.test gives
the correct result. At least there is a difference.

Or am I doing something wrong???

Does anybody know something about this?

a<-c(2,4,3,NA)
b<-c(4,1,2,3)
cor(a, b, method="spearman", use="complete.obs")
# -0.9819805
cor.test(a, b, method="spearman")
# -1

Without the NA both methods give -1
cor(a[1:3], b[1:3], method="s", use="c")
# -1

Is there another method to calculate a nice table with
correlations like cor(data.frame) is doing? Perhaps
even with p-values or "stars"?

Thanks!
Karl



From elisa_bianchini at yahoo.it  Mon Aug 30 11:53:36 2004
From: elisa_bianchini at yahoo.it (=?iso-8859-1?q?Elisa=20Bianchini?=)
Date: Mon, 30 Aug 2004 11:53:36 +0200 (CEST)
Subject: [R] meta.graph
Message-ID: <20040830095336.18187.qmail@web51407.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040830/9096b7a7/attachment.pl

From Ted.Harding at nessie.mcc.ac.uk  Mon Aug 30 11:42:24 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 30 Aug 2004 10:42:24 +0100 (BST)
Subject: [R] importing data in excel
In-Reply-To: <200408301622.39308.0034058@fudan.edu.cn>
Message-ID: <XFMail.040830104224.Ted.Harding@nessie.mcc.ac.uk>

On 30-Aug-04 ronggui wong wrote:
> if under  windows ,the command works well,but under linux,
> it does NOT,as linux can keep couples of file in clipboar.
> so i want to know how to do similar thing under linux.
> anyone knows?  

Since you're using Linux, maybe the following suggestions will
work for you (it's how I almost always deal with this situation).
This follows on from Thomas Petzoldt's suggestions.

1. Save the spreadsheet out of Excel as a CSV (Comma Separated
   Variables) file. There are appropriate options available
   in the "save as ... " menu.

   Comments. There can be complications if the original .xls file
   was badly prepared. You need to ensure that it comes out with
   exactly the same number of columns as there are variables, and
   exactly the same number of rows as there are rows of data in
   the .xls file (plus 1 for the header row of variable names, if
   present). There should be (k-1) commas in every line if there
   are k columns of data.

   a) I have known spreadsheets where the person entering the data
      has simply entered nothing in "short rows", i.e. where the
      last few variables have "missing values".
   b) Likewise, some rows may have had "space" entered beyond the
      range of data columns, so some rows are too long.
   c) Likewise, some rows below the range of data rows may have
      had blank entered in some cells, so Excel sees these as
      having content.

   For the most part, these complications will be avoided if, as
   Thomas suggests, you "mark" the full rectangular area prior to
   saving the marked area. Of course, you may already know that
   these problems will not arise in your case; but I can assure
   you from experience that if you are receiving your data from
   someone else then you should watch out for them.

   d) Similarly, you may encounter cases where the user has entered
      a "." (or similar) in blank/missing cells, though not necessarily
      in all, and possibly (as in (b) above) where they should not have.

2. Transfer the resulting file (say "mydata.csv") from the Windows
   machine to the Linux machine.

3. Run "dos2unix" on this file:

     dos2unix mydata.csv

   This ensures in particular that the end-of-line terminator is
   a clean unix-compatible LF ("\n") and not the Windows pair CRLF
   ("\r\n") which can cause complications; and also will strip any
   DOS-style "filler bytes" from the end of the file. It also copes
   with files originating on MAC systems (where the EOL is "\r").

4. If the resulting file "mydata.csv" is now nice and clean (which
   normally it should be, unless things like (a-d) above apply),
   then you should be able to bring it into R as a dataframe by
   simply giving the command

     mydata.df <- read.csv("mydata.csv")

   or similar.

5. However, if it has fallen victim to the sort of complication
   described above, then you will need to clean it up before use.

   Unix/Linux happily comes provided with plenty of file utilities
   which assist with such clean-up. These include the programs
   'sed', 'awk', 'grep', 'sort' and the like, and for the most part
   I can get such jobs done just using 'sed' and 'awk'.

   If you find yourself in this kind of situation and need further
   advice, get in touch with me off-list.

Good luck,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 30-Aug-04                                       Time: 10:42:23
------------------------------ XFMail ------------------------------



From christoph.lehmann at gmx.ch  Mon Aug 30 12:14:28 2004
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Mon, 30 Aug 2004 12:14:28 +0200
Subject: [R] after lm-fit: equality of two regression coefficients test
Message-ID: <4132FE04.6010307@gmx.ch>

Hi

Let's assume, we have a multiple linear regression, such as the one 
using the Scottish hills data (MASS, data(hills)):

one dependent variable: time
two independent var (metric): dist, climb

if I am interested, after (!) fitting a lm:

	my. lm <- lm(time ~ dist + climb, data = hills)

in the equivalence (or non-equivalence) of the two predictors "dist" and 
"climb":

	H0: dist = climb

Is there any function in R, which lets me calculate this, in just giving 
the lm-object "my.lm" and e.g. a vector such as c(1, -1), 
operationalizing the hypothesis H0: t(c(1, -1)) %*% c(dist, climb) = 0 ?

many thanks

Cheers!

Christoph



From mrufino at ipimar.ualg.pt  Mon Aug 30 12:32:36 2004
From: mrufino at ipimar.ualg.pt (Marta Rufino)
Date: Mon, 30 Aug 2004 11:32:36 +0100
Subject: [R] importing data in excel
References: <41320976.69645EB2@adinet.com.uy>
Message-ID: <007401c48e7c$aaa02f50$0b1a0e0a@PORTATILMARTA>

What I do, is that I save it as a file separated by tab,
and read it in R using (I save it with the xls extension, because this way
the file is automaticly open by excell always, althought it is a text file):

read.table("file name.xls", head=T, sep="\t")


it might help if you replace all the empty cells of your table (in excell)
by NA

cheers,
Marta

----- Original Message ----- 
From: "Gerardo Prieto Blanco" <economia at adinet.com.uy>
To: <R-help at stat.math.ethz.ch>
Sent: Sunday, August 29, 2004 5:51 PM
Subject: [R] importing data in excel


> Hello, I need to care excel data to be used
> in R,..., how do I make it?
> Thank you and greetings, Gerardo Prieto
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Mon Aug 30 12:47:27 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 30 Aug 2004 11:47:27 +0100 (BST)
Subject: [R] after lm-fit: equality of two regression coefficients test
In-Reply-To: <4132FE04.6010307@gmx.ch>
Message-ID: <Pine.LNX.4.44.0408301144360.22464-100000@gannet.stats>

On Mon, 30 Aug 2004, Christoph Lehmann wrote:

> Hi
> 
> Let's assume, we have a multiple linear regression, such as the one 
> using the Scottish hills data (MASS, data(hills)):
> 
> one dependent variable: time
> two independent var (metric): dist, climb
> 
> if I am interested, after (!) fitting a lm:
> 
> 	my. lm <- lm(time ~ dist + climb, data = hills)
> 
> in the equivalence (or non-equivalence) of the two predictors "dist" and 
> "climb":
> 
> 	H0: dist = climb

I think you intend to ask if the *coefficients* in the fit should be 
equal, which is nonsense in this example of course.

> Is there any function in R, which lets me calculate this, in just giving 
> the lm-object "my.lm" and e.g. a vector such as c(1, -1), 
> operationalizing the hypothesis H0: t(c(1, -1)) %*% c(dist, climb) = 0 ?

library(car)
?linear.hypothesis


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From vito.muggeo at giustizia.it  Mon Aug 30 13:01:25 2004
From: vito.muggeo at giustizia.it (Vito Muggeo)
Date: Mon, 30 Aug 2004 13:01:25 +0200
Subject: R: [R] after lm-fit: equality of two regression coefficients test
References: <4132FE04.6010307@gmx.ch>
Message-ID: <007701c48e81$a7566940$5c13070a@PROCGEN>

Hi Christoph,
If you are interested in testing for b1=b2 in a regression model, say

y=b0+b1*x1+b2*x2+e

you can compare the two models o1 and o2

o1<-lm(y~x1+x2)
o2<-lm(y~I(x1+x2))

best,
vito


----- Original Message -----
From: Christoph Lehmann <christoph.lehmann at gmx.ch>
To: <R-help at stat.math.ethz.ch>
Sent: Monday, August 30, 2004 12:14 PM
Subject: [R] after lm-fit: equality of two regression coefficients test


> Hi
>
> Let's assume, we have a multiple linear regression, such as the one
> using the Scottish hills data (MASS, data(hills)):
>
> one dependent variable: time
> two independent var (metric): dist, climb
>
> if I am interested, after (!) fitting a lm:
>
> my. lm <- lm(time ~ dist + climb, data = hills)
>
> in the equivalence (or non-equivalence) of the two predictors "dist" and
> "climb":
>
> H0: dist = climb
>
> Is there any function in R, which lets me calculate this, in just giving
> the lm-object "my.lm" and e.g. a vector such as c(1, -1),
> operationalizing the hypothesis H0: t(c(1, -1)) %*% c(dist, climb) = 0 ?
>
> many thanks
>
> Cheers!
>
> Christoph
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From dataanalytics at rediffmail.com  Mon Aug 30 13:37:41 2004
From: dataanalytics at rediffmail.com (data Analytics)
Date: 30 Aug 2004 11:37:41 -0000
Subject: [R] While installing Hmisc...
Message-ID: <20040830113741.25508.qmail@webmail17.rediffmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040830/3d23fdc9/attachment.pl

From dataanalytics at rediffmail.com  Mon Aug 30 13:45:24 2004
From: dataanalytics at rediffmail.com (data Analytics)
Date: 30 Aug 2004 11:45:24 -0000
Subject: [R] R version was 1.9.1 (for while installing Hmisc...)
Message-ID: <20040830114524.29695.qmail@webmail32.rediffmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040830/f2f1481b/attachment.pl

From andy_liaw at merck.com  Mon Aug 30 14:06:53 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 30 Aug 2004 08:06:53 -0400
Subject: [R] Multiple lapply get-around
Message-ID: <3A822319EB35174CA3714066D590DCD504AF82B6@usrymx25.merck.com>

> From: Manoj - Hachibushu Capital
> 
> I am faced with a situation wherein I have to use multiple 
> lapply's. The
> pseudo-code could be approximated to something as below:
> 
> For each X from i=1 to n
> 	For each Y based on j=1 to m 
> 		For each F from 1 to f
> 			Do some calculation based on Fij
> 			Store Xi,Yj	= Fij

What does the line above mean?  Are you assigning the value of Fij to _both_
Xi and Yj?  Doesn't quite make sense to me...

Andy

> 		End For F
> 	End for Y
> End for X
> 
> Is there anyway to optimize the processing logic further? I *guess*
> using the multiple lapply already optimizes the logic a little bit but
> is there anything else that could be done (apart from the obvious
> solution of writing a c-code)?
> 
> TIA
> 
> Manoj
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From martin.ploederl at sbg.ac.at  Mon Aug 30 14:16:15 2004
From: martin.ploederl at sbg.ac.at (=?ISO-8859-1?Q?Martin_Pl=F6derl?=)
Date: Mon, 30 Aug 2004 14:16:15 +0200
Subject: [R] Implementing Jeffreys Invariant Prior to GLM
Message-ID: <41331A8F.4010401@sbg.ac.at>

Hello!

In logistic regression I have a rare event as the dependent variable, 
i.e. the "successes" appear with a frequency of ca. 3%.
In a paper by

-- 
Dr. Martin Pl??derl
Universit??t Salzburg
Fachbereich Psychologie
Abteilung Sozialpsychologie
Hellbrunnerstr. 34
5020 Salzburg

Phone: +43 662 8044 5130
martin.ploederl at sbg.ac.at



From martin.ploederl at sbg.ac.at  Mon Aug 30 14:20:45 2004
From: martin.ploederl at sbg.ac.at (=?ISO-8859-1?Q?Martin_Pl=F6derl?=)
Date: Mon, 30 Aug 2004 14:20:45 +0200
Subject: [R] Implementing Jeffreys Invariant Prior to GLM
Message-ID: <41331B9D.1030801@sbg.ac.at>

Hello!

In logistic regression I have a rare event as the dependent variable,
i.e. the "successes" appear with a frequency of ca. 3%.
In a Paper by King & Zeng "Logistic Regression in Rare Events Data" 
(http://gking.harvard.edu/stats.shtml )
a prior-correction method and/or a weighting procedure is suggested to 
reduce the bias resulting from such analysis.

The authors state that using Jeffreys invariant prior also works.
Does anybody know who to implement this in R?

Thank you for your reply!

-- 
Dr. Martin Pl??derl
Universit??t Salzburg
Fachbereich Psychologie
Abteilung Sozialpsychologie
Hellbrunnerstr. 34
5020 Salzburg

Phone: +43 662 8044 5130
martin.ploederl at sbg.ac.at



From ripley at stats.ox.ac.uk  Mon Aug 30 14:43:33 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 30 Aug 2004 13:43:33 +0100 (BST)
Subject: [R] Implementing Jeffreys Invariant Prior to GLM
In-Reply-To: <41331B9D.1030801@sbg.ac.at>
Message-ID: <Pine.LNX.4.44.0408301339470.22739-100000@gannet.stats>

On Mon, 30 Aug 2004, Martin Pl??derl wrote:

> Hello!
> 
> In logistic regression I have a rare event as the dependent variable,
> i.e. the "successes" appear with a frequency of ca. 3%.
> In a Paper by King & Zeng "Logistic Regression in Rare Events Data" 
> (http://gking.harvard.edu/stats.shtml )
> a prior-correction method and/or a weighting procedure is suggested to 
> reduce the bias resulting from such analysis.

Bias of what?  
This is hardly news and is in all good books on classification, for example.

> The authors state that using Jeffreys invariant prior also works.
> Does anybody know who to implement this in R?

Take a look at package brlr (`who' being David Firth) for one approach.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From gregory_r_warnes at groton.pfizer.com  Mon Aug 30 14:46:02 2004
From: gregory_r_warnes at groton.pfizer.com (Warnes, Gregory R)
Date: Mon, 30 Aug 2004 08:46:02 -0400
Subject: [R] importing data in excel
Message-ID: <D7A3CFD7825BD6119B880002A58F06C20C521AAA@groexmb02.pfizer.com>


If you need to load Excel data *into* R, you can use the read.excel()
function in the gregmisc package (note that Perl must be installed) to read
MS-Excel .xls files.  


A better option is to simply save the data out as a tab-delimited text file
and then use

read.table(file="<filename>", sep="\t")

If you want to save R data in a form that MS-Excel will be happy with, use

write.table( <object>, file="<filename>.xls", sep="\t"))

-Greg

 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
To: Gerardo Prieto Blanco
Cc: R-help at stat.math.ethz.ch
Sent: 8/29/04 2:04 PM
Subject: Re: [R] importing data in excel

Gerardo Prieto Blanco wrote:

> Hello, I need to care excel data to be used
> in R,..., how do I make it?
> Thank you and greetings, Gerardo Prieto

Please read the R Data Import/Export manual!

Uwe Ligges


> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}



From jfox at mcmaster.ca  Mon Aug 30 15:07:47 2004
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 30 Aug 2004 09:07:47 -0400
Subject: [R] Rcmdr X11 protocol error message
In-Reply-To: <x2n00dozzz.fsf@biostat.ku.dk>
Message-ID: <20040830130747.YEIY15743.tomts22-srv.bellnexxia.net@JohnDesktop8300>

Dear Peter and Michael,

Peter: Thank you for fielding this question. Michael initially contacted me
directly, and I suggested that he write to the list since I didn't know the
possible source of the problem, had never seen it (either under Windows or
Linux), and never had a report of it before.

Michael: Does this problem occur only with certain Rcmdr dialogs or
generally? 

I'd also be curious to learn whether anyone else has experienced this
problem, and whether anyone has run the Rcmdr package under Mandrake Linux
without a problem.

Regards,
 John

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Peter Dalgaard
> Sent: Monday, August 30, 2004 2:18 AM
> To: Michael Bibo
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Rcmdr X11 protocol error message
> 
> "Michael Bibo" <Michael_Bibo at health.qld.gov.au> writes:
> 
> > I am using R 1.9.1 with R Commander GUI under Windows at work, and 
> > under Linux Mandrake 10 OR at home. I am having no problems 
> running R 
> > Commander under windows. Under Linux, though, the opening and 
> > sometimes closing of windows from R Commander produces 
> identical and 
> > repetitive error messages (in dialogue boxes and mirrored in the
> > console):
> > 
> > >"Warning: X11 protocol error: BadWindow (invalid Window 
> parameter)"  X 10-20.
> > 
> > These error messages occur most frequently when graphics 
> windows are 
> > opening, but can also occur when dialogue boxes pop up, and 
> sometimes 
> > even when results are written to the output window.
> > However, when the error message is "OK'd", the plot, or 
> whatever, is 
> > displayed as it should be. It is not entirely consistent, and will 
> > sometimes stop happening for parts of a session. Exactly the same 
> > commands run directly from the R console do not produce the same 
> > problem. I searched the mailing list archives, and found 
> the following 
> > exact description of what I have been experiencing, from November 
> > 2003:
> 
> It's a bit hard to debug without actually installing Rcmdr 
> (and all its dependents), but this sort of message usually 
> means either that a window was invalid to begin with (a bit 
> unlikely) or that it disappeared while events were still 
> being generated for it.
> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: 
> (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: 
> (+45) 35327907
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From Wanzare at HCJP.com  Mon Aug 30 15:45:25 2004
From: Wanzare at HCJP.com (Manoj - Hachibushu Capital)
Date: Mon, 30 Aug 2004 22:45:25 +0900
Subject: [R] Multiple lapply get-around
Message-ID: <1CBA12F2D414914989C723D196B287DC05568B@jp-svr-ex1.hcjp.com>

>	Do some calculation based on Fij
> 			Store Xi,Yj	= Fij

To clarify the above two lines: Do some processing for each f for each j and i and store the results in a three dimensional matrix format of i,j,f.

As you would have noticed, the outermost loop is the slowest moving one.

HTH

Manoj 

-----Original Message-----
From: Liaw, Andy [mailto:andy_liaw at merck.com]
Sent: Monday, August 30, 2004 9:07 PM
To: Manoj - Hachibushu Capital; r-help at stat.math.ethz.ch
Subject: RE: [R] Multiple lapply get-around


> From: Manoj - Hachibushu Capital
> 
> I am faced with a situation wherein I have to use multiple 
> lapply's. The
> pseudo-code could be approximated to something as below:
> 
> For each X from i=1 to n
> 	For each Y based on j=1 to m 
> 		For each F from 1 to f
> 			Do some calculation based on Fij
> 			Store Xi,Yj	= Fij

What does the line above mean?  Are you assigning the value of Fij to _both_
Xi and Yj?  Doesn't quite make sense to me...

Andy

> 		End For F
> 	End for Y
> End for X
> 
> Is there anyway to optimize the processing logic further? I *guess*
> using the multiple lapply already optimizes the logic a little bit but
> is there anything else that could be done (apart from the obvious
> solution of writing a c-code)?
> 
> TIA
> 
> Manoj
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From h.andersson at nioo.knaw.nl  Mon Aug 30 16:02:39 2004
From: h.andersson at nioo.knaw.nl (Henrik Andersson)
Date: Mon, 30 Aug 2004 16:02:39 +0200
Subject: [R] axes=F with pairs()
Message-ID: <cgvc5s$k90$1@sea.gmane.org>

If I want to fiddle with the axes, I normally plot things like this for 
instance

plot(1:5,axes=F)
axis(2,labels=letters[1:5])

BUT, if I want to do remove the axes from a scatter plot matrix using i.e.

pairs(cbind(runif(10),runif(10)),axes=F)

I get

Error in plot.default(x[, j], x[, i], xlab = "", ylab = "", axes = 
FALSE,  :
	formal argument "axes" matched by multiple actual arguments

How do I remove the axes ?


-------------------------------------------------------------
Henrik Andersson
Netherlands Institute of Ecology -
Centre for Estuarine and Marine Ecology
P.O. Box 140
4400 AC Yerseke
Phone: +31 113 577473
h.andersson at nioo.knaw.nl
http://www.nioo.knaw.nl/ppages/handersson



From tlumley at u.washington.edu  Mon Aug 30 16:08:09 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 30 Aug 2004 07:08:09 -0700 (PDT)
Subject: [R] meta.graph
In-Reply-To: <20040830095336.18187.qmail@web51407.mail.yahoo.com>
References: <20040830095336.18187.qmail@web51407.mail.yahoo.com>
Message-ID: <Pine.A41.4.58.0408300707030.160958@homer11.u.washington.edu>

On Mon, 30 Aug 2004, [iso-8859-1] Elisa Bianchini wrote:

> Hallo, I could do a graph for a metanalysis I try with the command
> "meta.graph", but the software not find this command! Do you say me if
> exist a library for install this command or something else to produce a
> meta graph? elisa
>

If you have some documentation suggesting that this command should exist,
it should tell you where to find it.

If you want to do this in R, you could try the rmeta package.

	-thomas



From ripley at stats.ox.ac.uk  Mon Aug 30 16:23:58 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 30 Aug 2004 15:23:58 +0100 (BST)
Subject: [R] axes=F with pairs()
In-Reply-To: <cgvc5s$k90$1@sea.gmane.org>
Message-ID: <Pine.LNX.4.44.0408301520040.29541-100000@gannet.stats>

On Mon, 30 Aug 2004, Henrik Andersson wrote:

> If I want to fiddle with the axes, I normally plot things like this for 
> instance
> 
> plot(1:5,axes=F)
> axis(2,labels=letters[1:5])
> 
> BUT, if I want to do remove the axes from a scatter plot matrix using i.e.
> 
> pairs(cbind(runif(10),runif(10)),axes=F)
> 
> I get
> 
> Error in plot.default(x[, j], x[, i], xlab = "", ylab = "", axes = 
> FALSE,  :
> 	formal argument "axes" matched by multiple actual arguments

As always, PLEASE read the help page before posting:

      ...: graphical parameters can be given as arguments to 'plot'.

and `axes' is *not* a graphical parameter (see ?par).

> How do I remove the axes ?

Use graphical parameters, as it says.  As in

pairs(~Fertility + Education + Catholic, data = swiss, 
      subset = Education < 20, main = "Swiss data, Education < 20",
      xaxt = "n", yaxt = "n")

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From thibautpillas at yahoo.com  Mon Aug 30 16:36:58 2004
From: thibautpillas at yahoo.com (Pillas Thibaut)
Date: Mon, 30 Aug 2004 07:36:58 -0700 (PDT)
Subject: [R] problem with SJava
Message-ID: <20040830143658.99221.qmail@web60809.mail.yahoo.com>

hello,

I am trying to install SJava on Windows XP as
described on 
http://www.omegahat.org/RSJava/:
at the level configure.win:

$rhome/sjava/configure.win returned "argument 'c:/...'
ignored"  


When i tried to install like the readme file, it
returned in R
> library(SJava)
using JAVA_HOME = C:\Program Files\Java\j2re1.4.2_04 
> .JavaInit()
Error in .JavaInit() : Couldn't start Java Virtual
Machine: Can't create Java Virtual Machine

what can I do?

> version

         _              

platform i386-pc-mingw32

arch     i386           

os       mingw32        

system   i386, mingw32  

status                  

major    1              

minor    9.0            

year     2004           

month    04             

day      12             

language R              

thank you
thibaut pillas



From bwittner at jimmy.harvard.edu  Mon Aug 30 16:37:58 2004
From: bwittner at jimmy.harvard.edu (Ben Wittner)
Date: Mon, 30 Aug 2004 10:37:58 -0400 (EDT)
Subject: [R] suggestions motivated by quest for remainders
Message-ID: <Pine.SOL.4.20.0408301025090.26933-100000@noah.dfci.harvard.edu>

Some time ago I tried to find out how to compute remainders in R.
I now know that it is done with %%, which is documented in help('+'),
but before someone told me that I tried:
  help('remainder'), help.search('remainder'), apropos('remainder')
  help('modulo'), help.search('modulo'), and apropos('modulo')
all of which yielded nothing.
I then tried help.search('mod') which yielded many pages of output, none
of which seemed to be relevant to remainders.

1) Easy to implement small suggestion:
Add 'remainder' and 'modulus' as aliases to the help page for '+'.

2) Hard to implement suggestion:
Add full-text search options to help.search(). If that were the case, 
help.search('mod') would have yielded '+' among its pages of output.
I believe this would be very helpful in various situations (e.g., when
trying to find out how to set xlim).

-Ben



From thibautpillas at yahoo.com  Mon Aug 30 16:42:06 2004
From: thibautpillas at yahoo.com (Pillas Thibaut)
Date: Mon, 30 Aug 2004 07:42:06 -0700 (PDT)
Subject: [R] problem with SJava
Message-ID: <20040830144206.67792.qmail@web60808.mail.yahoo.com>

hello,

I am trying to install SJava on Windows XP as
described on 
http://www.omegahat.org/RSJava/:
at the level configure.win:

$rhome/sjava/configure.win returned "argument 'c:/...'
ignored"  


When i tried to install like the readme file, it
returned in R
> library(SJava)
using JAVA_HOME = C:\Program Files\Java\j2re1.4.2_04 
> .JavaInit()
Error in .JavaInit() : Couldn't start Java Virtual
Machine: Can't create Java Virtual Machine

what can I do?

> version

         _              

platform i386-pc-mingw32

arch     i386           

os       mingw32        

system   i386, mingw32  

status                  

major    1              

minor    9.0            

year     2004           

month    04             

day      12             

language R              

thank you
thibaut pillas



From h.andersson at nioo.knaw.nl  Mon Aug 30 16:42:22 2004
From: h.andersson at nioo.knaw.nl (Henrik Andersson)
Date: Mon, 30 Aug 2004 16:42:22 +0200
Subject: [R] axes=F with pairs()
In-Reply-To: <Pine.LNX.4.44.0408301520040.29541-100000@gannet.stats>
References: <cgvc5s$k90$1@sea.gmane.org>
	<Pine.LNX.4.44.0408301520040.29541-100000@gannet.stats>
Message-ID: <cgvega$rl7$1@sea.gmane.org>

Prof Brian Ripley wrote:
> On Mon, 30 Aug 2004, Henrik Andersson wrote:
> 
> 
>>If I want to fiddle with the axes, I normally plot things like this for 
>>instance
>>
>>plot(1:5,axes=F)
>>axis(2,labels=letters[1:5])
>>
>>BUT, if I want to do remove the axes from a scatter plot matrix using i.e.
>>
>>pairs(cbind(runif(10),runif(10)),axes=F)
>>
>>I get
>>
>>Error in plot.default(x[, j], x[, i], xlab = "", ylab = "", axes = 
>>FALSE,  :
>>	formal argument "axes" matched by multiple actual arguments
> 
> 
> As always, PLEASE read the help page before posting:
> 
>       ...: graphical parameters can be given as arguments to 'plot'.
> 
> and `axes' is *not* a graphical parameter (see ?par).
> 
> 
>>How do I remove the axes ?
> 
> 
> Use graphical parameters, as it says.  As in
> 
> pairs(~Fertility + Education + Catholic, data = swiss, 
>       subset = Education < 20, main = "Swiss data, Education < 20",
>       xaxt = "n", yaxt = "n")
> 

Thanks for the swift reply, I already read the help page for both pairs 
and par. It didn't make me any wiser though. (Is it so obvious that 
'axes' is not a graphical parameter when it obviously does something to 
graphs?)



Henrik



From james.holtman at convergys.com  Mon Aug 30 17:04:58 2004
From: james.holtman at convergys.com (james.holtman@convergys.com)
Date: Mon, 30 Aug 2004 11:04:58 -0400
Subject: [R] Multiple lapply get-around
Message-ID: <OF2EDF40F5.DAE45C01-ON85256F00.0052AFF2@nd.convergys.com>





Try the following using 'lapply' and 'outer':

> myFuncs <- list(sin,sqrt)   # list of functions to evaluate
> lapply(myFuncs, function(func){   # apply to each function
+ outer(1:5, 1:5, function(x,y,func){     # here are the dimensions of x &
y
+ func(x+y)
+ }, func=func)
+ })
[[1]]
           [,1]       [,2]       [,3]       [,4]       [,5]
[1,]  0.9092974  0.1411200 -0.7568025 -0.9589243 -0.2794155
[2,]  0.1411200 -0.7568025 -0.9589243 -0.2794155  0.6569866
[3,] -0.7568025 -0.9589243 -0.2794155  0.6569866  0.9893582
[4,] -0.9589243 -0.2794155  0.6569866  0.9893582  0.4121185
[5,] -0.2794155  0.6569866  0.9893582  0.4121185 -0.5440211

[[2]]
         [,1]     [,2]     [,3]     [,4]     [,5]
[1,] 1.414214 1.732051 2.000000 2.236068 2.449490
[2,] 1.732051 2.000000 2.236068 2.449490 2.645751
[3,] 2.000000 2.236068 2.449490 2.645751 2.828427
[4,] 2.236068 2.449490 2.645751 2.828427 3.000000
[5,] 2.449490 2.645751 2.828427 3.000000 3.162278

__________________________________________________________
James Holtman        "What is the problem you are trying to solve?"
Executive Technical Consultant  --  Office of Technology, Convergys
james.holtman at convergys.com
+1 (513) 723-2929


                                                                                                                                          
                      "Manoj - Hachibushu                                                                                                 
                      Capital"                     To:       "Liaw, Andy" <andy_liaw at merck.com>, <r-help at stat.math.ethz.ch>               
                      <Wanzare at hcjp.com>           cc:                                                                                    
                      Sent by:                     Subject:  RE: [R] Multiple lapply get-around                                           
                      r-help-bounces at stat.m                                                                                               
                      ath.ethz.ch                                                                                                         
                                                                                                                                          
                                                                                                                                          
                      08/30/2004 09:45                                                                                                    
                                                                                                                                          
                                                                                                                                          




>            Do some calculation based on Fij
>                                    Store Xi,Yj             = Fij

To clarify the above two lines: Do some processing for each f for each j
and i and store the results in a three dimensional matrix format of i,j,f.

As you would have noticed, the outermost loop is the slowest moving one.

HTH

Manoj

-----Original Message-----
From: Liaw, Andy [mailto:andy_liaw at merck.com]
Sent: Monday, August 30, 2004 9:07 PM
To: Manoj - Hachibushu Capital; r-help at stat.math.ethz.ch
Subject: RE: [R] Multiple lapply get-around


> From: Manoj - Hachibushu Capital
>
> I am faced with a situation wherein I have to use multiple
> lapply's. The
> pseudo-code could be approximated to something as below:
>
> For each X from i=1 to n
>            For each Y based on j=1 to m
>                        For each F from 1 to f
>                                    Do some calculation based on Fij
>                                    Store Xi,Yj             = Fij

What does the line above mean?  Are you assigning the value of Fij to
_both_
Xi and Yj?  Doesn't quite make sense to me...

Andy

>                        End For F
>            End for Y
> End for X
>
> Is there anyway to optimize the processing logic further? I *guess*
> using the multiple lapply already optimizes the logic a little bit but
> is there anything else that could be done (apart from the obvious
> solution of writing a c-code)?
>
> TIA
>
> Manoj
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Mon Aug 30 17:11:57 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 30 Aug 2004 16:11:57 +0100 (BST)
Subject: [R] axes=F with pairs()
In-Reply-To: <cgvega$rl7$1@sea.gmane.org>
Message-ID: <Pine.LNX.4.44.0408301603560.29625-100000@gannet.stats>

On Mon, 30 Aug 2004, Henrik Andersson wrote:

> Prof Brian Ripley wrote:

[...]

> >       ...: graphical parameters can be given as arguments to 'plot'.
> > 
> > and `axes' is *not* a graphical parameter (see ?par).

[...]


> Thanks for the swift reply, I already read the help page for both pairs 
> and par. It didn't make me any wiser though. (Is it so obvious that 
> 'axes' is not a graphical parameter when it obviously does something to 
> graphs?)

Yes.  I've already told you why: it is not listed in ?pars.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Aug 30 17:16:03 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 30 Aug 2004 16:16:03 +0100 (BST)
Subject: [R] problem with SJava
In-Reply-To: <20040830143658.99221.qmail@web60809.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0408301612180.29625-100000@gannet.stats>

I don't think sending this *twice* more (you have already sent it once) 
is going to make people more inclined to help you.

This is R-help, and you are asking about an Omegahat package.
Wrong place, surely.


On Mon, 30 Aug 2004, Pillas Thibaut wrote:

> hello,
> 
> I am trying to install SJava on Windows XP as
> described on 
> http://www.omegahat.org/RSJava/:
> at the level configure.win:
> 
> $rhome/sjava/configure.win returned "argument 'c:/...'
> ignored"  

...

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From wolski at molgen.mpg.de  Mon Aug 30 17:41:51 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Mon, 30 Aug 2004 17:41:51 +0200
Subject: [R] problem with SJava
In-Reply-To: <20040830144206.67792.qmail@web60808.mail.yahoo.com>
References: <20040830144206.67792.qmail@web60808.mail.yahoo.com>
Message-ID: <200408301741510876.01CFC860@mail.math.fu-berlin.de>

Hi!
A page dedicated to R/java with usefull info:

http://simon.urbanek.info/simon/yawe/research/other.html

Hope it helps.
/E


*********** REPLY SEPARATOR  ***********

On 8/30/2004 at 7:42 AM Pillas Thibaut wrote:

>>>hello,
>>>
>>>I am trying to install SJava on Windows XP as
>>>described on 
>>>http://www.omegahat.org/RSJava/:
>>>at the level configure.win:
>>>
>>>$rhome/sjava/configure.win returned "argument 'c:/...'
>>>ignored"  
>>>
>>>
>>>When i tried to install like the readme file, it
>>>returned in R
>>>> library(SJava)
>>>using JAVA_HOME = C:\Program Files\Java\j2re1.4.2_04 
>>>> .JavaInit()
>>>Error in .JavaInit() : Couldn't start Java Virtual
>>>Machine: Can't create Java Virtual Machine
>>>
>>>what can I do?
>>>
>>>> version
>>>
>>>         _              
>>>
>>>platform i386-pc-mingw32
>>>
>>>arch     i386           
>>>
>>>os       mingw32        
>>>
>>>system   i386, mingw32  
>>>
>>>status                  
>>>
>>>major    1              
>>>
>>>minor    9.0            
>>>
>>>year     2004           
>>>
>>>month    04             
>>>
>>>day      12             
>>>
>>>language R              
>>>
>>>thank you
>>>thibaut pillas
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



Dipl. bio-chem. Witold Eryk Wolski             @         MPI-Moleculare Genetic   
Ihnestrasse 63-73 14195 Berlin                'v'    
tel: 0049-30-83875219                        /   \       
mail: witek96 at users.sourceforge.net        ---W-W----    http://www.molgen.mpg.de/~wolski 
      wolski at molgen.mpg.de



From p.dalgaard at biostat.ku.dk  Mon Aug 30 17:41:10 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Aug 2004 17:41:10 +0200
Subject: [R] Rcmdr X11 protocol error message
In-Reply-To: <20040830130747.YEIY15743.tomts22-srv.bellnexxia.net@JohnDesktop8300>
References: <20040830130747.YEIY15743.tomts22-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <x2acwck50p.fsf@biostat.ku.dk>

"John Fox" <jfox at mcmaster.ca> writes:

> Dear Peter and Michael,
> 
> Peter: Thank you for fielding this question. Michael initially contacted me
> directly, and I suggested that he write to the list since I didn't know the
> possible source of the problem, had never seen it (either under Windows or
> Linux), and never had a report of it before.
> 
> Michael: Does this problem occur only with certain Rcmdr dialogs or
> generally? 
> 
> I'd also be curious to learn whether anyone else has experienced this
> problem, and whether anyone has run the Rcmdr package under Mandrake Linux
> without a problem.

I can't seem to reproduce it on RH8 with R-1.9.x. I see a couple of
other quirks though:

If you bring up a dialog and click "help", you get the help window OK,
but it is not scrollable until you exit the dialog with OK or Cancel
(by which time the point is moot either way, presumably).

The "View data set" feature gets stuck in a tight loop and requires
a Ctl-C in the console to allow it to display the data and process any
events.

        -p

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Mon Aug 30 17:47:59 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 30 Aug 2004 16:47:59 +0100 (BST)
Subject: [R] suggestions motivated by quest for remainders
In-Reply-To: <Pine.SOL.4.20.0408301025090.26933-100000@noah.dfci.harvard.edu>
Message-ID: <Pine.LNX.4.44.0408301636420.29762-100000@gannet.stats>

On Mon, 30 Aug 2004, Ben Wittner wrote:

> Some time ago I tried to find out how to compute remainders in R.
> I now know that it is done with %%, which is documented in help('+'),
> but before someone told me that I tried:
>   help('remainder'), help.search('remainder'), apropos('remainder')
>   help('modulo'), help.search('modulo'), and apropos('modulo')
> all of which yielded nothing.
> I then tried help.search('mod') which yielded many pages of output, none
> of which seemed to be relevant to remainders.
> 
> 1) Easy to implement small suggestion:
> Add 'remainder' and 'modulus' as aliases to the help page for '+'.

We have \concept's for that, and I have added those.  But is hard to know 
what people are going to choose to search on.

> 2) Hard to implement suggestion:
> Add full-text search options to help.search(). If that were the case, 
> help.search('mod') would have yielded '+' among its pages of output.

It's Arithmetic, not +, that comes up.

> I believe this would be very helpful in various situations (e.g., when
> trying to find out how to set xlim).

We do already have full-text-search on Windows CHM help, unfortunately
per-package.  If you try it you will find it does not work for your
suggested keywords, and you would be swamped by help.search('mod').

As ever, 

	R is a collaborative project with many contributors.

and it is open to you to contribute FTS extensions to help.search.
With 2.0.0 about to go into its final phases and needing all packages to 
be re-installed, now would be a very good time to do this.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Camarda at demogr.mpg.de  Mon Aug 30 17:56:13 2004
From: Camarda at demogr.mpg.de (Camarda, Carlo Giovanni)
Date: Mon, 30 Aug 2004 17:56:13 +0200
Subject: [R] Generalized Singular Value Decomposition (GSVD)
Message-ID: <3699CDBC4ED5D511BE6400306E1C0D8106B4102D@hermes.demogr.mpg.de>

Dear R-users,
I couldn't find a function or some help in R-project web about the
Generalized Singular Value Decomposition. In MatLab there is a simple
function for this algebric issue (gsvd). Is there anything like that in R?
And, if not, could you help me to apply this method in R?
Thanks in advance, Giancarlo


+++++
This mail has been sent through the MPI for Demographic Rese...{{dropped}}



From ggrothendieck at myway.com  Mon Aug 30 17:59:47 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 30 Aug 2004 15:59:47 +0000 (UTC)
Subject: [R] suggestions motivated by quest for remainders
References: <Pine.SOL.4.20.0408301025090.26933-100000@noah.dfci.harvard.edu>
Message-ID: <loom.20040830T173848-400@post.gmane.org>

Ben Wittner <bwittner <at> jimmy.harvard.edu> writes:

: Some time ago I tried to find out how to compute remainders in R.
: I now know that it is done with %%, which is documented in help('+'),
: but before someone told me that I tried:
:   help('remainder'), help.search('remainder'), apropos('remainder')
:   help('modulo'), help.search('modulo'), and apropos('modulo')
: all of which yielded nothing.
: I then tried help.search('mod') which yielded many pages of output, none
: of which seemed to be relevant to remainders.

Any of these google searches will find it:

  R modulus
  site:r-project.org modulus
  r-project.org modulus

The first hit on the first two give a thread on r-help which, if followed,
gives the answer.  The first hit on the third points to Jonathan Baron's
reference card which has it.

Also if you go to the R web site, click on search in the left hand pane 
and try some of the R specific site searches -- these will also find it.

Finally, the R language manual, obtainable under windows by going into R
and clicking on Help and then Manual and then R Language Manual or in
doc/manual subdirectory in the R directory on your disk.

None of this is not to say that the help should not point to it, of course.



From wolski at molgen.mpg.de  Mon Aug 30 18:07:41 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Mon, 30 Aug 2004 18:07:41 +0200
Subject: [R] Generalized Singular Value Decomposition (GSVD)
In-Reply-To: <3699CDBC4ED5D511BE6400306E1C0D8106B4102D@hermes.demogr.mpg.de>
References: <3699CDBC4ED5D511BE6400306E1C0D8106B4102D@hermes.demogr.mpg.de>
Message-ID: <200408301807410715.01E76E6F@mail.math.fu-berlin.de>

Hi!

I use
help.search()
in addition.

e.g.

help.search("singular values")

svd(base)               Singular Value Decomposition of a Matrix
fast.prcomp(gregmisc)   Efficient computation of principal components
                        and singular value decompositions.
SVD(Rwave)              Singular Value Decomposition



/E

*********** REPLY SEPARATOR  ***********

On 8/30/2004 at 5:56 PM Camarda, Carlo Giovanni wrote:

>>>Dear R-users,
>>>I couldn't find a function or some help in R-project web about the
>>>Generalized Singular Value Decomposition. In MatLab there is a simple
>>>function for this algebric issue (gsvd). Is there anything like that in
>>>R?
>>>And, if not, could you help me to apply this method in R?
>>>Thanks in advance, Giancarlo
>>>
>>>
>>>+++++
>>>This mail has been sent through the MPI for Demographic
>>>Rese...{{dropped}}
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



Dipl. bio-chem. Witold Eryk Wolski             @         MPI-Moleculare Genetic   
Ihnestrasse 63-73 14195 Berlin                'v'    
tel: 0049-30-83875219                        /   \       
mail: witek96 at users.sourceforge.net        ---W-W----    http://www.molgen.mpg.de/~wolski 
      wolski at molgen.mpg.de



From bournery at mnhn.fr  Mon Aug 30 18:08:04 2004
From: bournery at mnhn.fr (Alexandre Bournery)
Date: Mon, 30 Aug 2004 18:08:04 +0200
Subject: [R] D'agostino test
Message-ID: <5.0.2.1.2.20040830180658.00b04bd0@cimrs1.mnhn.fr>

Hi, Does anyone know if the D'agostino test is available with R ?
Alex



From gpagnon at emory.edu  Mon Aug 30 18:14:08 2004
From: gpagnon at emory.edu (Giuseppe Pagnoni)
Date: Mon, 30 Aug 2004 12:14:08 -0400
Subject: [R] Post-hoc t-tests in 2-way repeated measure ANOVA
Message-ID: <41335250.3020405@emory.edu>

Hi all

I will repost this since it seems to me such a common problem and I 
couldn't find any solution in the mail list...

I am running a 2-way repeated measure anova with 1 between-subjects
factor (Group=treatment, control), and 1 within-subject factor (Time of
measurement: time1, time2).  I extract the results of the anova with:

summary(aov(effect ~ Group*Time + Error(Subj/Time), data=mydata))

following the notes of Jonathan Baron.

Now,is there a way in R to quickly extract all the post-hoc t-tests for 
the simple main effects, corrected of course for multiple comparisons?

Group1 at Time1 vs Group1 at Time2
Group2 at Time1 vs Group2 at Time2

Group1 at Time1 vs Group2 at Time1
Group1 at Time2 vs Group2 at Time2



Also, how can I enter in the model formula a counfounding
covariate (e.g., Age)?

thanks in advance for any suggestions!

    giuseppe

-- 
------------------------
Giuseppe Pagnoni, Ph.D.
Dept. of Psychiatry and Behavioral Sciences
1639 Pierce Drive, Suite 4000
WMB Bldg., Atlanta, GA 30322, U.S.
phone: 404-712-8431
fax: 404-727-3233
e-mail: gpagnon at emory.edu



From thpe at hhbio.wasser.tu-dresden.de  Mon Aug 30 18:25:13 2004
From: thpe at hhbio.wasser.tu-dresden.de (Thomas Petzoldt)
Date: Mon, 30 Aug 2004 18:25:13 +0200
Subject: [R] suggestions motivated by quest for remainders
In-Reply-To: <loom.20040830T173848-400@post.gmane.org>
References: <Pine.SOL.4.20.0408301025090.26933-100000@noah.dfci.harvard.edu>
	<loom.20040830T173848-400@post.gmane.org>
Message-ID: <413354E9.3020802@hhbio.wasser.tu-dresden.de>

Gabor Grothendieck wrote:

> Any of these google searches will find it:
> 
>   R modulus
>   site:r-project.org modulus
>   r-project.org modulus
> 
> The first hit on the first two give a thread on r-help which, if followed,
> gives the answer.  The first hit on the third points to Jonathan Baron's
> reference card which has it.
> 
> Also if you go to the R web site, click on search in the left hand pane 
> and try some of the R specific site searches -- these will also find it.

Two other possibilities I found useful are

1) to do a fulltext search for a keyword in all .html files in the R 
installation directory on my harddisk or

2) (very quick and useful in most cases) siply to do a google search 
with "r-help" as an additional keyword.

Thomas P.



From mail at joeconway.com  Mon Aug 30 18:29:35 2004
From: mail at joeconway.com (Joe Conway)
Date: Mon, 30 Aug 2004 09:29:35 -0700
Subject: [R] R and PostGresQL
In-Reply-To: <BD578ECE.281D%sdavis2@mail.nih.gov>
References: <BD578ECE.281D%sdavis2@mail.nih.gov>
Message-ID: <413355EF.7060306@joeconway.com>

Sean Davis wrote:
> What is the "standard" package for interfacing with PostGreSQL (most
> up-to-date)?  There appear to be at least three with different names:
> 
> Rdbi.PgSQL
> RdbiPgSQL (available as part of BioConductor)
> RPgSQL (On the home page, no longer maintained in favor or Rdbi)

AFAICT, the most recent, and only currently maintained package of the 
three, is the one from BioConductor.

You can also embed R in Postgres using PL/R, found here:
http://www.joeconway.com/plr/

HTH,

Joe



From mbibo at qldnet.com.au  Mon Aug 30 18:29:08 2004
From: mbibo at qldnet.com.au (Michael Bibo)
Date: Mon, 30 Aug 2004 16:29:08 +0000 (UTC)
Subject: [R] Rcmdr X11 protocol error message
References: <x2n00dozzz.fsf@biostat.ku.dk>
	<20040830130747.YEIY15743.tomts22-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <loom.20040830T173120-21@post.gmane.org>

>John Fox <jfox <at> mcmaster.ca> writes:

> 
> Dear Peter and Michael,
> 
> Peter: Thank you for fielding this question. Michael initially contacted me
> directly, and I suggested that he write to the list since I didn't know the
> possible source of the problem, had never seen it (either under Windows or
> Linux), and never had a report of it before.
> 
> Michael: Does this problem occur only with certain Rcmdr dialogs or
> generally? 
> 
> I'd also be curious to learn whether anyone else has experienced this
> problem, and whether anyone has run the Rcmdr package under Mandrake Linux
> without a problem.
> 
> Regards,
>  John


John,

The error messages occur most frequently in relation to plot/graphics windows,
but also with dialogue boxes, and occasionally even just when output is written
to the output window.  They can occur when windows/dialogue boxes are opening or
being closed.  But they do not occur consistently - they do not always occur,
not even with the same plots, dialogues etc.  Exactly the same commands run
directly from the console do not produce the same error messages.

I have been experimenting with drawing plots from random data generated in
Rcmdr, as opposed to imported spss data files etc, but that seems to make little
difference.

I have run R Commander under Linux Mandrake myself prior to this without this
problem (R 1.8.1/1.9.0 & Mandrake 9.1/9.2).  Unfortunately, a new version of
Mandrake, with an updated XFree86, the new version of R (1.9.1) and updates to R
Commander all happened around the same time.  And previously I have installed R
from RPM, while this time I took the plunge and installed from source.  I guess
that's what you would call confounding your variables.

I would like to point out to people, though, that despite these error messages,
R Commander is still working.  And under Windows I have had no such issues.  I
think a GUI such as R Commander has an important role to play in attracting
users to R from other, commercial applications.


Michael Bibo
Queensland Health

michael_bibo at health.qld.gov.au



From gerifalte28 at hotmail.com  Mon Aug 30 18:51:51 2004
From: gerifalte28 at hotmail.com (F Z)
Date: Mon, 30 Aug 2004 16:51:51 +0000
Subject: [R] FIML in lme
Message-ID: <BAY2-F22p9rUdOtfaLq0002ffd7@hotmail.com>

Thanks to Douglas Bates and Christopher Lawrence for their responses.  
Christopher is right, that is what I was asking about.  I guess that there 
is no implementation of FIML in R.  Would this be a worthy method to include 
in R?  I don't really use this method so I would say no but maybe some 
people think in a different way (For example SAS users trying ot move to R)?

Respectfully

Francisco


>From: Chris Lawrence <chris at lordsutch.com>
>To: R-Help <r-help at r-project.org>
>Subject: Re: [R] FIML in lme
>Date: Sat, 28 Aug 2004 02:29:08 -0500
>
>On Aug 27, Douglas Bates wrote:
> > F Z wrote:
> > >I was asked if lme can use FIML (Full Information Maximum Likelihood)
> > >instead of REML or ML but I don't know the answer.  Does anybody know 
>if
> > >this is implemented in R?
> >
> > To the best of my knowledge, FIML is ML so the answer is yes.
> >
> > For example, the phrase "Full Information Maximum Likelihood" is used in
> > Singer and Willett (2004) "Applied Longitudinal Data Analysis" (Oxford
> > University Press) as a synonym for maximum likelihood.
>
>I have seen FIML used to refer to a type of ML estimation where a
>missing data treatment is included in the estimation procedure
>(parameter estimates are derived from incomplete cases for only the
>variables present in the case, rather than simply discarding the
>cases), at least in the latent-variable SEM context, specifically in
>AMOS.  This may be what Francisco is getting at.
>
>To my knowledge, no R packages implement this sort of "FIML", for any
>class of models, although there are other available missing data
>treatments (EM, MCMC estimation).
>
>
>Chris
>--
>Christopher N. Lawrence, Ph.D.
>Visiting Assistant Professor of Political Science
>Millsaps College
>1701 N. State St
>Jackson, MS 39210
>(601) 974-1438 / lawrecn at millsaps.edu
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html


Security. http://clinic.mcafee.com/clinic/ibuy/campaign.asp?cid=3963



From p.dalgaard at biostat.ku.dk  Mon Aug 30 18:50:14 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Aug 2004 18:50:14 +0200
Subject: [R] Generalized Singular Value Decomposition (GSVD)
In-Reply-To: <3699CDBC4ED5D511BE6400306E1C0D8106B4102D@hermes.demogr.mpg.de>
References: <3699CDBC4ED5D511BE6400306E1C0D8106B4102D@hermes.demogr.mpg.de>
Message-ID: <x26570k1tl.fsf@biostat.ku.dk>

"Camarda, Carlo Giovanni" <Camarda at demogr.mpg.de> writes:

> Dear R-users,
> I couldn't find a function or some help in R-project web about the
> Generalized Singular Value Decomposition. In MatLab there is a simple
> function for this algebric issue (gsvd). Is there anything like that in R?
> And, if not, could you help me to apply this method in R?
> Thanks in advance, Giancarlo

We do ship GSVD in the Lapack module (function DDGSVD) so it is "just" a
matter of finding out how to interface to it. Beware that the notation
appears to be somewhat different from that of Matlab.

If one of your A,B matrices is nonsingular, I think you can do it with
ordinary SVD and a bit of elbow grease.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From isung at affinnova.com  Mon Aug 30 19:02:31 2004
From: isung at affinnova.com (Iyue Sung)
Date: Mon, 30 Aug 2004 13:02:31 -0400
Subject: [R] FIML in lme
Message-ID: <90821ABF429A074DA88E56D1DF7A71610130CB34@afifs1.affinnova.com>


You might want to look into multiple imputation methods as an
alternative (see work by Don Rubin and Joe Schafer).

I know Schafer has a library for S-plus, but not sure about R.  

> -----Original Message-----
> From: F Z [mailto:gerifalte28 at hotmail.com]
> Sent: Monday, August 30, 2004 12:52 PM
> To: chris at lordsutch.com; r-help at r-project.org; bates at stat.wisc.edu
> Subject: Re: [R] FIML in lme
> 
> Thanks to Douglas Bates and Christopher Lawrence for their responses.
> Christopher is right, that is what I was asking about.  I guess that
there
> is no implementation of FIML in R.  Would this be a worthy method to
> include
> in R?  I don't really use this method so I would say no but maybe some
> people think in a different way (For example SAS users trying ot move
to
> R)?
> 
> Respectfully
> 
> Francisco
> 
> 
> >From: Chris Lawrence <chris at lordsutch.com>
> >To: R-Help <r-help at r-project.org>
> >Subject: Re: [R] FIML in lme
> >Date: Sat, 28 Aug 2004 02:29:08 -0500
> >
> >On Aug 27, Douglas Bates wrote:
> > > F Z wrote:
> > > >I was asked if lme can use FIML (Full Information Maximum
Likelihood)
> > > >instead of REML or ML but I don't know the answer.  Does anybody
know
> >if
> > > >this is implemented in R?
> > >
> > > To the best of my knowledge, FIML is ML so the answer is yes.
> > >
> > > For example, the phrase "Full Information Maximum Likelihood" is
used
> in
> > > Singer and Willett (2004) "Applied Longitudinal Data Analysis"
(Oxford
> > > University Press) as a synonym for maximum likelihood.
> >
> >I have seen FIML used to refer to a type of ML estimation where a
> >missing data treatment is included in the estimation procedure
> >(parameter estimates are derived from incomplete cases for only the
> >variables present in the case, rather than simply discarding the
> >cases), at least in the latent-variable SEM context, specifically in
> >AMOS.  This may be what Francisco is getting at.
> >
> >To my knowledge, no R packages implement this sort of "FIML", for any
> >class of models, although there are other available missing data
> >treatments (EM, MCMC estimation).
> >
> >
> >Chris
> >--
> >Christopher N. Lawrence, Ph.D.
> >Visiting Assistant Professor of Political Science
> >Millsaps College
> >1701 N. State St
> >Jackson, MS 39210
> >(601) 974-1438 / lawrecn at millsaps.edu
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide!
> >http://www.R-project.org/posting-guide.html
> 
> 
> Security. http://clinic.mcafee.com/clinic/ibuy/campaign.asp?cid=3963
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html



From partha_bagchi at hgsi.com  Mon Aug 30 20:23:25 2004
From: partha_bagchi at hgsi.com (partha_bagchi@hgsi.com)
Date: Mon, 30 Aug 2004 14:23:25 -0400
Subject: [R] SAS ODBC & R
Message-ID: <OF21E95E0B.8E21ED92-ON85256F00.00643CE3-85256F00.006505BF@hgsi.com>

This mornings message regarding SAS to R datasets got me thinking about 
ODBC access to SAS datasets. Has anyone tried to do that?

I have been attempting to connect to a SAS Server (local) using RODBC and 
have met with 0 success.

Basically, I am able to connect to the server and assign a connection. 
However, if I try sqlTables(connect) then I get 0 rows and consequently no 
datasets can be read. 

I maybe missing something here and will be happy to provide more 
information if necessary.

However, note that if I use MS Access to connect, I get the following 
message in the SAS log:

98 SAS server SHR1 started.
30AUG2004:14:13:25.229 User pbagchi(1) has connected to server SHR1.
30AUG2004:14:13:25.229 User pbagchi(1) is executing release 2.1 of the 
Remote Engine Emulator (41) of SAS (r) on SAME architecture
                       host type: .
30AUG2004:14:13:25.229 pbagchi(1) in "ODBCAPPL"(0) has connected to 
SQLVIEW.
30AUG2004:14:13:25.229 Server library USER ('c:\analysis' V8) accessed as 
USER by "ODBCAPPL"(0) of user pbagchi(1).
30AUG2004:14:13:25.299 When pbagchi(1) in "ODBCAPPL"(0) called prepare for 
SELECT Config, nValue FROM MSysConf from SQLVIEW:
                       ERROR: SQL passthru expression contained these 
errors: ERROR: File USER.MSYSCONF.DATA does not exist....  .
30AUG2004:14:13:48.322 Work task SHRWTSK8 has been terminated.

(But I am able to access the SAS dataset and create an access table from 
it).

If I use RODBC to connect, I get the following in the log:

30AUG2004:14:20:18.303 SAS server SHR1 started.
30AUG2004:14:20:18.854 User pbagchi(1) has connected to server SHR1.
30AUG2004:14:20:18.854 User pbagchi(1) is executing release 2.1 of the 
Remote Engine Emulator (41) of SAS (r) on SAME architecture
                       host type: .
30AUG2004:14:20:18.864 pbagchi(1) in "ODBCAPPL"(0) has connected to 
SQLVIEW.
30AUG2004:14:20:18.864 Server library USER ('c:\analysis' V8) accessed as 
USER by "ODBCAPPL"(0) of user pbagchi(1).
30AUG2004:14:20:43.179 Work task SHRWTSK8 has been terminated.



From ma1camoj at uco.es  Mon Aug 30 20:50:59 2004
From: ma1camoj at uco.es (Juan Antonio Caballero)
Date: Mon, 30 Aug 2004 20:50:59 +0200
Subject: [R] ADE4
In-Reply-To: <412CC641.70505@pdf.com>
References: <5A637F509C50B444BDA096EB2D7BC158427FB1@pghmail2.rand.org>
	<412CC641.70505@pdf.com>
Message-ID: <1093891859.2671.17.camel@localhost.localdomain>

Ruego disculpen la tardanza en responder a este correo, pero acabo de
volver de vacaciones.

Llevo bastante a??os suscrito a la lista y, generalmente cuando
participo, respondo en ingles a ella. Por la sencilla razon de que la
mayoria de los e-mail que aparecen lo hacen en ingles. Pero si alguna
vez alguien participa en otro idioma, y puedo responderle o me interesa
su topico, NO me importa hacerlo. Naturalmente: en el supuesto de que
conozca ese idoma.
En resumen: NO veo razon alguna a limitar la lista al uso de un idioma.
Yo creo que solo debe haber dos limitaciones 1) Que el tema sea R (razon
obvia) y 2) Que se haga con educacion y cortesia.
Un cordial saludo,
Juan Antonio Caballero.



El mi??, 25-08-2004 a las 19:02, Spencer Graves escribi??:
> S'il vous plait: 
> 
>       La lengua de esta listserve es ingl??s.  S?? que es dif??cil 
> expresarse en una lengua diferente de lo suyo, pero muchas personas 
> apprendan leyendo las preguntans y contestaciones de esta lista.  Muchos 
> de los leyentes aqu?? no entienden franc??s o aleman o japones o ... .  
> Por eso, yo les pido a ustedes utilicen la norma internacional de esta 
> lista (o construyen una liste para R en Castellano puro). 
> 
>       Vielen Dank. 
>       Spencer Graves
> 
> Almirall, Daniel wrote:
> 
> >La manera mas facil seria haciendo clic en Packages --> Install Packages from CRAN --> ADE4.  Despues que lo instales, usa la siguente orden para poder utilizar la libreria ADE4:
> >
> >  
> >
> >>library(ade4)
> >>    
> >>
> >
> >Espero que esto te ayude,
> >Danny
> >
> >-----Original Message-----
> >From: r-help-bounces at stat.math.ethz.ch
> >[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of
> >jpineda at um.umanizales.edu.co
> >Sent: Wednesday, August 25, 2004 11:59 AM
> >To: r-help at stat.math.ethz.ch
> >Subject: [R] ADE4
> >
> >
> >**Libre de Virus - Escaneado por la Universidad de Manizales**
> >X-Virus-Scanned: by AMaViS perl-11
> >
> >Solicito su ayuda para encontrar e instalar el ADE4, pues en la versi??n
> >1.9.1. que tengo instalada no la encuentro y estos son mis primeros
> >ejercicios
> >
> >Le agradezco su colaboraci??n
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >  
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
-- 
Juan Antonio Caballero Molina   /"\    ASCII Ribbon Campaign
Universidad de C??rdoba          \ /    Respect for open standards
Looking for fine software        X     No HTML/RTF in email
and/or writing?                 / \    No M$ Word docs in email
http://counter.li.org                  Linux user number 346272



From jfox at mcmaster.ca  Mon Aug 30 21:47:39 2004
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 30 Aug 2004 15:47:39 -0400
Subject: [R] Rcmdr X11 protocol error message
In-Reply-To: <x2acwck50p.fsf@biostat.ku.dk>
Message-ID: <web-62962003@cgpsrv2.cis.mcmaster.ca>

Dear Peter,

On 30 Aug 2004 17:41:10 +0200
 Peter Dalgaard <p.dalgaard at biostat.ku.dk> wrote:
> "John Fox" <jfox at mcmaster.ca> writes:
> 
> > Dear Peter and Michael,
> > 
> > Peter: Thank you for fielding this question. Michael initially
> contacted me
> > directly, and I suggested that he write to the list since I didn't
> know the
> > possible source of the problem, had never seen it (either under
> Windows or
> > Linux), and never had a report of it before.
> > 
> > Michael: Does this problem occur only with certain Rcmdr dialogs or
> > generally? 
> > 
> > I'd also be curious to learn whether anyone else has experienced
> this
> > problem, and whether anyone has run the Rcmdr package under
> Mandrake Linux
> > without a problem.
> 
> I can't seem to reproduce it on RH8 with R-1.9.x. I see a couple of
> other quirks though:
> 
> If you bring up a dialog and click "help", you get the help window
> OK,
> but it is not scrollable until you exit the dialog with OK or Cancel
> (by which time the point is moot either way, presumably).
> 

My Linux system badly needs updating, so I can't check this directly
now. Does this problem occur with HTML help? As well, setting the Rcmdr
grab.focus option to FALSE might get things unstuck.

> The "View data set" feature gets stuck in a tight loop and requires
> a Ctl-C in the console to allow it to display the data and process
> any
> events.

Does this problem occur occur when  showData() (from the relimp
package) is called directly, or only when it's called from the Rcmdr?

I'm sorry to impose these questions on you -- in the longer term, I
have to update my Linux system so that I can again perform these tests
myself.

Thank you for your help,
 John
> 
>         -p
> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45)
> 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45)
> 35327907

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada
http://socserv.mcmaster.ca/jfox/



From lauraholt_983 at hotmail.com  Mon Aug 30 22:27:29 2004
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Mon, 30 Aug 2004 15:27:29 -0500
Subject: [R] outer question
Message-ID: <BAY12-F34CMCmHrjJQf00021781@hotmail.com>

Dear R People:

I need to construct a matrix of the order (pn-1) x (pn-1).


The elements can be constructed as
z[i,j] = pn - max(i,j)
for (i,j) in 1:(pn - 1)

I was messing with a toy example in which n=4, p=3 and tried to use outer:
>x <- 1:11
>y <- 1:11
>outer(x,y,function(x,y)12-max(x,y))
Error in outer(x, y, function(x, y) 12 - max(x, y)) :
        dim<- : dims [product 121] do not match the length of object [1]
>

Is there a better way to do this, please?  I would like to avoid loops if 
possible.

Thanks in advance.

Sincerely,
Laura Holt
mailto: lauraholt_983 at hotmail.com
R version 1.9.1 windows



From lauraholt_983 at hotmail.com  Mon Aug 30 22:34:24 2004
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Mon, 30 Aug 2004 15:34:24 -0500
Subject: [R] solution to outer question
Message-ID: <BAY12-F1UQeyGeMF3ij00100b26@hotmail.com>

Hi again!

"pmax" works instead of max.

>outer(x,y,function(x,y)12 - pmax(x,y))
      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11]
[1,]   11   10    9    8    7    6    5    4    3     2     1
[2,]   10   10    9    8    7    6    5    4    3     2     1
[3,]    9    9    9    8    7    6    5    4    3     2     1
[4,]    8    8    8    8    7    6    5    4    3     2     1
[5,]    7    7    7    7    7    6    5    4    3     2     1
[6,]    6    6    6    6    6    6    5    4    3     2     1
[7,]    5    5    5    5    5    5    5    4    3     2     1
[8,]    4    4    4    4    4    4    4    4    3     2     1
[9,]    3    3    3    3    3    3    3    3    3     2     1
[10,]    2    2    2    2    2    2    2    2    2     2     1
[11,]    1    1    1    1    1    1    1    1    1     1     1
>

All is well.

Sorry for the trouble.

Sincerely,
Laura H
mailto: lauraholt_983 at hotmail.com


Security. http://clinic.mcafee.com/clinic/ibuy/campaign.asp?cid=3963



From lauraholt_983 at hotmail.com  Mon Aug 30 22:44:29 2004
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Mon, 30 Aug 2004 15:44:29 -0500
Subject: [R] solution to outer question
Message-ID: <BAY12-F195gfUw3ZPpA00003272@hotmail.com>

Hi again!

"pmax" works instead of max.

>outer(x,y,function(x,y)12 - pmax(x,y))
      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11]
[1,]   11   10    9    8    7    6    5    4    3     2     1
[2,]   10   10    9    8    7    6    5    4    3     2     1
[3,]    9    9    9    8    7    6    5    4    3     2     1
[4,]    8    8    8    8    7    6    5    4    3     2     1
[5,]    7    7    7    7    7    6    5    4    3     2     1
[6,]    6    6    6    6    6    6    5    4    3     2     1
[7,]    5    5    5    5    5    5    5    4    3     2     1
[8,]    4    4    4    4    4    4    4    4    3     2     1
[9,]    3    3    3    3    3    3    3    3    3     2     1
[10,]    2    2    2    2    2    2    2    2    2     2     1
[11,]    1    1    1    1    1    1    1    1    1     1     1
>

All is well.

Sorry for the trouble.

Sincerely,
Laura H
mailto: lauraholt_983 at hotmail.com



From p.dalgaard at biostat.ku.dk  Mon Aug 30 22:41:30 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Aug 2004 22:41:30 +0200
Subject: [R] Rcmdr X11 protocol error message
In-Reply-To: <web-62962003@cgpsrv2.cis.mcmaster.ca>
References: <web-62962003@cgpsrv2.cis.mcmaster.ca>
Message-ID: <x2k6vgtl39.fsf@biostat.ku.dk>

"John Fox" <jfox at mcmaster.ca> writes:

> My Linux system badly needs updating, so I can't check this directly
> now. Does this problem occur with HTML help? As well, setting the Rcmdr
> grab.focus option to FALSE might get things unstuck.

Removing the grab.focus seems to help. Wouldn't expect HTML help to be
affected. 
 
> > The "View data set" feature gets stuck in a tight loop and requires
> > a Ctl-C in the console to allow it to display the data and process
> > any
> > events.
> 
> Does this problem occur occur when  showData() (from the relimp
> package) is called directly, or only when it's called from the Rcmdr?

Both. Even w/o Rcmdr.


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From tlumley at u.washington.edu  Mon Aug 30 23:09:26 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 30 Aug 2004 14:09:26 -0700 (PDT)
Subject: [R] Wrong result with cor(x, y, method="spearman",
	use="complete.obs") with NA's???
In-Reply-To: <20040830095203.87768.qmail@web52503.mail.yahoo.com>
References: <20040830095203.87768.qmail@web52503.mail.yahoo.com>
Message-ID: <Pine.A41.4.58.0408301358000.147222@homer11.u.washington.edu>

On Mon, 30 Aug 2004, [iso-8859-1] Karl Knoblick wrote:

> Hallo!
>
> Is there an error in cor to calculate Spearman
> correlation with cor if there are NA's? cor.test gives
> the correct result. At least there is a difference.
>
> Or am I doing something wrong???

The help for cor() says

		      Notice also that the ranking is (currently) done
     removing only cases that are missing on the variable itself, which
     may not be what you expect if you let 'use' be '"complete.obs"' or
     '"pairwise.complete.obs"'.

>
> Does anybody know something about this?
>
> a<-c(2,4,3,NA)
> b<-c(4,1,2,3)
> cor(a, b, method="spearman", use="complete.obs")
> # -0.9819805

That is, when b is converted to ranks the ranks are c(4,1,2,3), not
c(3,1,2), because b has no missing data. cor() then takes the correlation
of c(2,4,3) and c(3,1,2), which is -0.98..


> cor.test(a, b, method="spearman")
> # -1

cor.test does it the other way around. It first drops all the observations
with NAs on any variable, then does the ranking.

>
> Without the NA both methods give -1
> cor(a[1:3], b[1:3], method="s", use="c")
> # -1
>
> Is there another method to calculate a nice table with
> correlations like cor(data.frame) is doing? Perhaps
> even with p-values or "stars"?

You could use cor(na.omit(data.frame))) to get the same NA behaviour as
cor.test(). No pretty stars, though.


	-thomas



From HDoran at air.org  Mon Aug 30 19:04:23 2004
From: HDoran at air.org (Doran, Harold)
Date: Mon, 30 Aug 2004 13:04:23 -0400
Subject: [R] FIML in lme
Message-ID: <88EAF3512A55DF46B06B1954AEF73F74051F1CD5@dc1ex2.air.org>

I'm not sure you are correct on this. Other texts on multilevel models
(e.g., Raudenbush and Bryk, Kreft and Deeuw, and Singer & Willett) all
use FiML as a synonym for ML. In fact, Kreft and Deleeuw go as far to
even state they are the same thing (see page 131).

When you run a model in HLM selecting "Full Maximum Likelihood" and
method="ML" in lme, the results, including all fixed effects, variance
components, empirical bayes residuals, degrees of freedom are exactly
the same.

So, I think Doug is correct in that ML == FiML. 

Harold

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of F Z
Sent: Monday, August 30, 2004 12:52 PM
To: chris at lordsutch.com; r-help at r-project.org; bates at stat.wisc.edu
Subject: Re: [R] FIML in lme

Thanks to Douglas Bates and Christopher Lawrence for their responses.  
Christopher is right, that is what I was asking about.  I guess that
there 
is no implementation of FIML in R.  Would this be a worthy method to
include 
in R?  I don't really use this method so I would say no but maybe some 
people think in a different way (For example SAS users trying ot move to
R)?

Respectfully

Francisco


>From: Chris Lawrence <chris at lordsutch.com>
>To: R-Help <r-help at r-project.org>
>Subject: Re: [R] FIML in lme
>Date: Sat, 28 Aug 2004 02:29:08 -0500
>
>On Aug 27, Douglas Bates wrote:
> > F Z wrote:
> > >I was asked if lme can use FIML (Full Information Maximum
Likelihood)
> > >instead of REML or ML but I don't know the answer.  Does anybody
know 
>if
> > >this is implemented in R?
> >
> > To the best of my knowledge, FIML is ML so the answer is yes.
> >
> > For example, the phrase "Full Information Maximum Likelihood" is
used in
> > Singer and Willett (2004) "Applied Longitudinal Data Analysis"
(Oxford
> > University Press) as a synonym for maximum likelihood.
>
>I have seen FIML used to refer to a type of ML estimation where a
>missing data treatment is included in the estimation procedure
>(parameter estimates are derived from incomplete cases for only the
>variables present in the case, rather than simply discarding the
>cases), at least in the latent-variable SEM context, specifically in
>AMOS.  This may be what Francisco is getting at.
>
>To my knowledge, no R packages implement this sort of "FIML", for any
>class of models, although there are other available missing data
>treatments (EM, MCMC estimation).
>
>
>Chris
>--
>Christopher N. Lawrence, Ph.D.
>Visiting Assistant Professor of Political Science
>Millsaps College
>1701 N. State St
>Jackson, MS 39210
>(601) 974-1438 / lawrecn at millsaps.edu
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html


Security. http://clinic.mcafee.com/clinic/ibuy/campaign.asp?cid=3963

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From linkpema at muohio.edu  Tue Aug 31 00:08:12 2004
From: linkpema at muohio.edu (Melanie A. Link-Perez)
Date: Mon, 30 Aug 2004 18:08:12 -0400 (EDT)
Subject: [R] initialization error with XEmacs init.el file
Message-ID: <3816.134.53.9.199.1093903692.squirrel@webmail.muohio.edu>

I have successfully used XEmacs on my home PC for several months.  On
8/25/04 I  downloaded R version 1.9.1 (rw1091) from one of the R mirror
sites, XEmacs installer (setup.exe) from
<http://www.xemacs.org/Download/win32/>, and Jim Fox's latest configuration
files (fox-ess-config.zip) from
<http://socserv.socsci.mcmaster.ca/jfox/Books/Companion/ESS/> to my Dell
laptop, which is running MS Windows XP Professional version 2002.  Upon
launching XEmacs, I get the following error:

  (1) (initialization/error) An error has occurred while loading
  c:\.xemacs\init.el:

  Cannot open load file: executable


The init.el file is version 0.5.6.  I have triple-checked path names and
placement of files in directories, but have not been able to track down this
problem.  I also tried replacing the init.el file with my older version
(0.5.4) to no avail.  I have tried to get around this by using Alt-x and
shift-R to launch R, but the following appears below the buffer:

  Searching for program: No such file or directory, C:Program
  FilesR^Mw1091^HinRterm


Certainly, there is no "Mw1091" or "HinRterm" and I don?t know where the
program is getting that info (with the obvious typos).  I am grateful for
any assistance that is offered.

Thanks,
Melanie Link-Perez
Miami University



From jfox at mcmaster.ca  Tue Aug 31 00:19:39 2004
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 30 Aug 2004 18:19:39 -0400
Subject: [R] initialization error with XEmacs init.el file
In-Reply-To: <3816.134.53.9.199.1093903692.squirrel@webmail.muohio.edu>
Message-ID: <20040830221938.DZEB18869.tomts5-srv.bellnexxia.net@JohnDesktop8300>

Dear Melanie,

Are you using forward-slashes (/) or double-back-slashes (\\) to separate
directories, as I believe you should, and placing quotes around the path?

I hope that this helps,
 John 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Melanie A. Link-Perez
> Sent: Monday, August 30, 2004 5:08 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] initialization error with XEmacs init.el file
> 
> I have successfully used XEmacs on my home PC for several months.  On
> 8/25/04 I  downloaded R version 1.9.1 (rw1091) from one of 
> the R mirror sites, XEmacs installer (setup.exe) from 
> <http://www.xemacs.org/Download/win32/>, and Jim Fox's latest 
> configuration files (fox-ess-config.zip) from 
> <http://socserv.socsci.mcmaster.ca/jfox/Books/Companion/ESS/> 
> to my Dell laptop, which is running MS Windows XP 
> Professional version 2002.  Upon launching XEmacs, I get the 
> following error:
> 
>   (1) (initialization/error) An error has occurred while loading
>   c:\.xemacs\init.el:
> 
>   Cannot open load file: executable
> 
> 
> The init.el file is version 0.5.6.  I have triple-checked 
> path names and placement of files in directories, but have 
> not been able to track down this problem.  I also tried 
> replacing the init.el file with my older version
> (0.5.4) to no avail.  I have tried to get around this by 
> using Alt-x and shift-R to launch R, but the following 
> appears below the buffer:
> 
>   Searching for program: No such file or directory, C:Program
>   FilesR^Mw1091^HinRterm
> 
> 
> Certainly, there is no "Mw1091" or "HinRterm" and I don't 
> know where the program is getting that info (with the obvious 
> typos).  I am grateful for any assistance that is offered.
> 
> Thanks,
> Melanie Link-Perez
> Miami University
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From kjetil at acelerate.com  Tue Aug 31 01:59:43 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Mon, 30 Aug 2004 19:59:43 -0400
Subject: [R] Generalized Singular Value Decomposition (GSVD)
In-Reply-To: <3699CDBC4ED5D511BE6400306E1C0D8106B4102D@hermes.demogr.mpg.de>
References: <3699CDBC4ED5D511BE6400306E1C0D8106B4102D@hermes.demogr.mpg.de>
Message-ID: <4133BF6F.3060502@acelerate.com>

Camarda, Carlo Giovanni wrote:

I??m not really sure what you mean by generalized SVD, but have you had a 
look at CRAN package PTAk, which says to have some extensions of the SVD 
(but poosibly not the one you
want...?)

Kjetil Halvorsen

>Dear R-users,
>I couldn't find a function or some help in R-project web about the
>Generalized Singular Value Decomposition. In MatLab there is a simple
>function for this algebric issue (gsvd). Is there anything like that in R?
>And, if not, could you help me to apply this method in R?
>Thanks in advance, Giancarlo
>
>
>+++++
>This mail has been sent through the MPI for Demographic Rese...{{dropped}}
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From chokkang.loke at nus.edu.sg  Tue Aug 31 03:41:53 2004
From: chokkang.loke at nus.edu.sg (Loke Chok Kang)
Date: Tue, 31 Aug 2004 09:41:53 +0800
Subject: [R] (no subject)
Message-ID: <F7260AB4B00A3046BBF7931317428FA61FC052@MBOX23.stu.nus.edu.sg>

Hi,
Sorry, I have a problem that require some help. As I am doing a project with R and this project requires me to do a lot of plotting as I run my simulation, I need R to help me store my plots automatically as the simulation is run. Could anyone advise me on how this can be done? If possible I need all the plots to be written to a file.
 
PS:In all I have over 500 over plots, thus it will not be possible to manually plot all the plots out.
 
>From yours faithfully,
Chok Kang
sci10137 at nus.edu.sg



From maustin at amgen.com  Tue Aug 31 03:55:46 2004
From: maustin at amgen.com (Austin, Matt)
Date: Mon, 30 Aug 2004 18:55:46 -0700
Subject: [R] (no subject)
Message-ID: <E7D5AB4811D20B489622AABA9C53859101F111DF@teal-exch.amgen.com>


You need to use a device to print to such as

postscript(file="/where/to/put/file/filename.ps")
	##your plotting code here
dev.off()

Writing a plot at each iteration of your simulation can impact the runtime
greatly.

Please read the posting guide at the bottom of the e-mail, it can help you
get more helpful . . . help

--Matt

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Loke Chok Kang
Sent: Monday, August 30, 2004 18:42 PM
To: r-help at stat.math.ethz.ch
Subject: [R] (no subject)


Hi,
Sorry, I have a problem that require some help. As I am doing a project with
R and this project requires me to do a lot of plotting as I run my
simulation, I need R to help me store my plots automatically as the
simulation is run. Could anyone advise me on how this can be done? If
possible I need all the plots to be written to a file.
 
PS:In all I have over 500 over plots, thus it will not be possible to
manually plot all the plots out.
 
>From yours faithfully,
Chok Kang
sci10137 at nus.edu.sg

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From maustin at amgen.com  Tue Aug 31 04:07:39 2004
From: maustin at amgen.com (Austin, Matt)
Date: Mon, 30 Aug 2004 19:07:39 -0700
Subject: [R] (no subject)
Message-ID: <E7D5AB4811D20B489622AABA9C53859101F111E1@teal-exch.amgen.com>

A correction.  You either need to open the plotting device prior to the
simulation that includes your plotting commands and close it after the
simulation or have the name change dynamically in your simulation so that
the runs go in separate files.

An example of the first method would be

postscript(file="/where/to/put/file/filename.ps")

## your simulation commands

dev.off()

for the second method more info would be needed to know how you are running
your simulation to help with dynamic naming.

--Matt

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Austin, Matt
Sent: Monday, August 30, 2004 18:56 PM
To: 'Loke Chok Kang'; r-help at stat.math.ethz.ch
Subject: RE: [R] (no subject)



You need to use a device to print to such as

postscript(file="/where/to/put/file/filename.ps")
	##your plotting code here
dev.off()

Writing a plot at each iteration of your simulation can impact the runtime
greatly.

Please read the posting guide at the bottom of the e-mail, it can help you
get more helpful . . . help

--Matt

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Loke Chok Kang
Sent: Monday, August 30, 2004 18:42 PM
To: r-help at stat.math.ethz.ch
Subject: [R] (no subject)


Hi,
Sorry, I have a problem that require some help. As I am doing a project with
R and this project requires me to do a lot of plotting as I run my
simulation, I need R to help me store my plots automatically as the
simulation is run. Could anyone advise me on how this can be done? If
possible I need all the plots to be written to a file.
 
PS:In all I have over 500 over plots, thus it will not be possible to
manually plot all the plots out.
 
>From yours faithfully,
Chok Kang
sci10137 at nus.edu.sg

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From mbibo at qldnet.com.au  Tue Aug 31 06:56:20 2004
From: mbibo at qldnet.com.au (Michael Bibo)
Date: Tue, 31 Aug 2004 04:56:20 +0000 (UTC)
Subject: [R] Rcmdr X11 protocol error message
References: <x2n00dozzz.fsf@biostat.ku.dk>
	<20040830130747.YEIY15743.tomts22-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <loom.20040831T065033-210@post.gmane.org>

John Fox <jfox <at> mcmaster.ca> writes:

> 
> Dear Peter and Michael,
> 
> Peter: Thank you for fielding this question. Michael initially contacted me
> directly, and I suggested that he write to the list since I didn't know the
> possible source of the problem, had never seen it (either under Windows or
> Linux), and never had a report of it before.
> 
> Michael: Does this problem occur only with certain Rcmdr dialogs or
> generally? 
> 
> I'd also be curious to learn whether anyone else has experienced this
> problem, and whether anyone has run the Rcmdr package under Mandrake Linux
> without a problem.
> 
> Regards,
>  John
> 
Might the following offer a clue?

Extract from FAQ for SJava (http://www.omegahat.org/RSJava/FAQ.html)

Event Loops

# When I run the X11() graphics device and a Java GUI, sometimes I get errors
like the following

Xlib: unexpected async reply (sequence 0x1356)!

1: X11 protocol error: BadDrawable (invalid Pixmap or Window parameter) 
2: X11 protocol error: BadDrawable (invalid Pixmap or Window parameter) 

What's happening?
    Both Java and R are reading events from the X11 event loop ``at the same
time''. As a result, R is reading events meant for Java and vice-versa and bad
things happen!

    There are two solutions:

        * use the Java graphics device which uses the Java graphics facilities
to do the rendering;
        * don't run the GUI and the X11 device at the same time. 

    The second solution is clearly not ideal. What we can do is to compile the
X11 device code to be thread-safe. However, the same effect will be seen if we
load another X11 event handler (e.g. the Tcl/Tk library). 

Michael

michael_bibo at health.qld.gov.au



From p.dalgaard at biostat.ku.dk  Tue Aug 31 09:32:27 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 31 Aug 2004 09:32:27 +0200
Subject: [R] Rcmdr X11 protocol error message
In-Reply-To: <loom.20040831T065033-210@post.gmane.org>
References: <x2n00dozzz.fsf@biostat.ku.dk>
	<20040830130747.YEIY15743.tomts22-srv.bellnexxia.net@JohnDesktop8300>
	<loom.20040831T065033-210@post.gmane.org>
Message-ID: <x2r7pn3gqc.fsf@biostat.ku.dk>

Michael Bibo <mbibo at qldnet.com.au> writes:

> What's happening?
>     Both Java and R are reading events from the X11 event loop ``at the same
> time''. As a result, R is reading events meant for Java and vice-versa and bad
> things happen!
> 
>     There are two solutions:
> 
>         * use the Java graphics device which uses the Java graphics facilities
> to do the rendering;
>         * don't run the GUI and the X11 device at the same time. 
> 
>     The second solution is clearly not ideal. What we can do is to compile the
> X11 device code to be thread-safe. However, the same effect will be seen if we
> load another X11 event handler (e.g. the Tcl/Tk library). 

I don't think that is the issue. The X11 device and Tk sit on
different connections to the X device, so they shouldn't be seeing
eachother's events (the event loops can starve eachother though
since they are multiplexed). I'd rather suspect a race condition with
the Tk code itself - sending events to a window that is in the process
of being destroyed and things like that. Which Tk version are we
talking about, by the way?

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Gregor.gawron at rmf.ch  Tue Aug 31 09:50:56 2004
From: Gregor.gawron at rmf.ch (Gregor.gawron@rmf.ch)
Date: Tue, 31 Aug 2004 09:50:56 +0200
Subject: [R] D'agostino test
Message-ID: <A3A7D687D463B240844F0718418B68BA4F6828@michexmb01.maninvestments.ad.man.com>

Alexandre,

I found once a code to calculate Shapiro_Francia and D'Agostino tests of normality written long time ago by Peter B. Mandeville. Below his code and references (It is in spanish, I assume):
Gregor


#--------------------------------------------------------------------------------------
I coded the Shapiro-Francia test which handles from 5 to 5000 observations
from 

	Patrick Royston
	1993 A Pocket-Calculator Algorithm for the Shapiro-Francia Test for
	Non-Normality: An Application to Medicine. Statistics in Medicine vol
	12, 181-184
	1991 Estimating Departure from Normality. Statistics in Medicine.
	vol 10, 1283-1293
	1983 A Simple Method for Evaluating the Shapiro-Francia W' Test of 
	Non-Normality. The Statistician 32 (1983) 297-300

and the D'Agostino tests from

	Ralph B. D'Agostino, Albert Belanger, and Ralph B. D'Agostino, Jr.
	1990 A Suggestion for Using Powerful and Informative Tests of Normality.
	The American Statistician, November 1990, Vol. 44, No. 4

for testing normality in R.    

# Lee 1996:33-34
MOMENTS <- function(data,r) sum((data-mean(data))^r)/length(data)

# Peter
SKEW <- function(data) MOMENTS(data,3)/(MOMENTS(data,2)*sqrt(MOMENTS(data,2)))

# Peter
KURTOSIS <- function(data) MOMENTS(data,4)/(MOMENTS(data,2)*MOMENTS(data,2))

# D'Agostino, Belanger and D'Agostino 1990:316-321
DAGOSTINO <- function(data){
    cat("  PRUEBAS DE D'AGOSTINO\n")
    n <- length(data)
    cat("    SIMETRIA\n")
    cat("      COEFICIENTE DE SIMETRIA:",sqrtb1 <- SKEW(data),"\n")
    if(n>8){
        y <- sqrtb1*sqrt((n+1)*(n+3)/(6*(n-2)))
        beta2 <- 3*(n*n+27*n-70)*(n+1)*(n+3)/((n-2)*(n+5)*(n+7)*(n+9))
        w <- sqrt(-1+sqrt(2*(beta2-1)))
        delta <- 1/sqrt(log(w))
        ALPHA <- sqrt(2/(w*w-1))
        cat("      ZETA CALCULADA:",zb1 <-
delta*log(y/ALPHA+sqrt((y/ALPHA)^2+1)),"\n")
        cat("      PROBABILIDAD:",2*(1-pnorm(abs(zb1))),"\n")
    }else
        cat("    LA PRUEBA DE SESGO REQUIERE POR LO MENOS 9 REPETICIONES\n")
    cat("    KURTOSIS\n")
    cat("      COEFICIENTE DE KURTOSIS:",b2 <- KURTOSIS(data),"\n")
    if(n>19){
       meanb2 <- 3*(n-1)/(n+1)
       varb2 <- 24*n*(n-2)*(n-3)/((n+1)*(n+1)*(n+3)*(n+5))
       x <- (b2-meanb2)/sqrt(varb2)
       moment <-
6*(n*n-5*n+2)/((n+7)*(n+9))*sqrt(6*(n+3)*(n+5)/(n*(n-2)*(n-3)))
       a <- 5+8/moment*(2/moment+sqrt(1+4/(moment*moment)))
       cat("      ZETA CALCULADA:",zb2 <-
(1-2/(9*a)-((1-2/a)/(1+x*sqrt(2/(a-4))))^(1/3))/sqrt(2/(9*a)),"\n")
       cat("      PROBABILIDAD:",2*(1-pnorm(abs(zb2))),"\n")
       cat("    OMNIBUS\n")
       cat("      JI-CUADRADA CALCULADA:",k2 <- zb1*zb1+zb2*zb2,"\n")
       cat("      GRADOS DE LIBERTAD: 2\n")
       cat("      PROBABILIDAD:",probji2 <- 1-pchisq(k2,2),"\n")
    }else
       cat("    LAS PRUEBAS DE KURTOSIS Y OMNIBUS REQUIEREN POR LO MENOS 20 REPETICIONES\n")
}

# Royston 1993:183-184
SHAPIRO.FRANCIA <- function(data){
    cat("  PRUEBA DE NORMALIDAD DE SHAPIRO-FRANCIA\n")
    n <- length(data)
    if(n<5 || n>5000)
        cat("    REQUIERE ENTRE 5 Y 5000 REPETICIONES\n")
    else{
        xbar <- mean(data)
        sdata <- sort(data)
        resid <- sdata-xbar
        uniform <- seq(1,n)
        np <- qnorm((uniform-0.375)/(n+0.25))
        cat("    W':",w <-
(sum(np*sdata))^2/(sum(np*np)*sum(resid*resid)),"\n")
        u <- log(n)
        v <- log(u)
        muy <- -1.2725+1.0521*(v-u)
        sigmay <- 1.0308-0.26758*(v+2/u)
        cat("    ZETA CALCULADA:",zeta <- (log(1-w)-muy)/sigmay,"\n")
        cat("    PROBABILIDAD:",probz <- 1-pnorm(zeta),"\n")
    }
}

Peter B.

Peter B. Mandeville                             mandevip at deimos.tc.uaslp.mx
Jefe del Depto. de Inform??tica y Bioestad??stica rpe1531 at pasteur.fmed.uaslp.mx 
Facultad de Medicine                            Tel: 48 26-23-45 ext. 232
Universidad Aut??noma de San Luis Potos??         Fax: 48 28-23-52
Av. V. Carranza 2405
Col. Los Filtros
Apartado Postal 145
San Luis Potos??, S.L.P.
78210 M??xico

#--------------------------------------------------------------------------------------------------


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Alexandre Bournery
Sent: Montag, 30. August 2004 18:08
To: R-help at stat.math.ethz.ch
Subject: [R] D'agostino test


Hi, Does anyone know if the D'agostino test is available with R ? Alex

______________________________________________
R-help at stat.math.ethz.ch mailing list https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


Any information in this communication is confidential and ma...{{dropped}}



From paolo at directwave.com.br  Tue Aug 31 13:16:25 2004
From: paolo at directwave.com.br (Paolo Tommasini)
Date: Tue, 31 Aug 2004 08:16:25 -0300
Subject: [R] (no subject)
Message-ID: <41345E09.30806@directwave.com.br>

Hi this is a very simple question ! if a have a set of date like
x=c(1,2,3,4,4,5,6,8,7,8,8)

how can I find out the percentile of 7 ?

thanks

Paolo



From wolski at molgen.mpg.de  Tue Aug 31 13:33:26 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Tue, 31 Aug 2004 13:33:26 +0200
Subject: [R] (no subject)
In-Reply-To: <41345E09.30806@directwave.com.br>
References: <41345E09.30806@directwave.com.br>
Message-ID: <200408311333260045.00D9CF1F@mail.math.fu-berlin.de>

sum(x<=7)/length(x)*100


*********** REPLY SEPARATOR  ***********

On 8/31/2004 at 8:16 AM Paolo Tommasini wrote:

>>>Hi this is a very simple question ! if a have a set of date like
>>>x=c(1,2,3,4,4,5,6,8,7,8,8)
>>>
>>>how can I find out the percentile of 7 ?
>>>
>>>thanks
>>>
>>>Paolo
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



Dipl. bio-chem. Witold Eryk Wolski             @         MPI-Moleculare Genetic   
Ihnestrasse 63-73 14195 Berlin                'v'    
tel: 0049-30-83875219                        /   \       
mail: witek96 at users.sourceforge.net        ---W-W----    http://www.molgen.mpg.de/~wolski 
      wolski at molgen.mpg.de



From jfox at mcmaster.ca  Tue Aug 31 13:36:09 2004
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 31 Aug 2004 07:36:09 -0400
Subject: [R] Rcmdr X11 protocol error message
In-Reply-To: <x2r7pn3gqc.fsf@biostat.ku.dk>
Message-ID: <20040831113608.TXMS14082.tomts16-srv.bellnexxia.net@JohnDesktop8300>

Dear Michael,

A question: Do you observe these problems with tcltk in general or just with
the Rcmdr package? Is it possible for you to test a Tcl/Tk program outside
of R?

Regards,
 John

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Peter Dalgaard
> Sent: Tuesday, August 31, 2004 2:32 AM
> To: Michael Bibo
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Rcmdr X11 protocol error message
> 
> Michael Bibo <mbibo at qldnet.com.au> writes:
> 
> > What's happening?
> >     Both Java and R are reading events from the X11 event loop ``at 
> > the same time''. As a result, R is reading events meant for 
> Java and 
> > vice-versa and bad things happen!
> > 
> >     There are two solutions:
> > 
> >         * use the Java graphics device which uses the Java graphics 
> > facilities to do the rendering;
> >         * don't run the GUI and the X11 device at the same time. 
> > 
> >     The second solution is clearly not ideal. What we can do is to 
> > compile the
> > X11 device code to be thread-safe. However, the same effect will be 
> > seen if we load another X11 event handler (e.g. the Tcl/Tk library).
> 
> I don't think that is the issue. The X11 device and Tk sit on 
> different connections to the X device, so they shouldn't be 
> seeing eachother's events (the event loops can starve 
> eachother though since they are multiplexed). I'd rather 
> suspect a race condition with the Tk code itself - sending 
> events to a window that is in the process of being destroyed 
> and things like that. Which Tk version are we talking about, 
> by the way?
> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: 
> (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: 
> (+45) 35327907
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From t.dewez at brgm.fr  Tue Aug 31 14:05:59 2004
From: t.dewez at brgm.fr (Dewez Thomas)
Date: Tue, 31 Aug 2004 14:05:59 +0200
Subject: [R] appending data to a dataframe
Message-ID: <D965434E9D6BD511AE3500306E01C8BE05DD6940@SRV0015>

Dear R users,

I am sorry to ask you such a pathetic newbie question, but how does one
append data at the end of a data frame?
I am working with GRASS/R library, but the question is about R.
I have a data.frame containing the following variables
basinID, distoutlet, drainage_area, slope

These variables are stored for all pixels of Grass Raster objects. For each
drainage basin (basinID), I'd like to find the maximum of distoutlet. How
can I store the pair of info (bv$ID, max(bv$distoutlet)) at each step of the
for loop ?

I presume something like this could do

for (i 1:max(basinID)){
bid <- i
length <- max(distoutlet[which(basinID == I(i))])
}

But how do I handle the output and record bid and length in parallel in an
object???

Any hint is welcome

Thomas
***
Le contenu de cet e-mail et de ses pi??ces jointes est destin...{{dropped}}



From murdoch at stats.uwo.ca  Tue Aug 31 14:13:36 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 31 Aug 2004 08:13:36 -0400
Subject: [R] appending data to a dataframe
In-Reply-To: <D965434E9D6BD511AE3500306E01C8BE05DD6940@SRV0015>
References: <D965434E9D6BD511AE3500306E01C8BE05DD6940@SRV0015>
Message-ID: <ijq8j058kos3lcq3be88gd48rob9jl09m3@4ax.com>

On Tue, 31 Aug 2004 14:05:59 +0200, Dewez Thomas <t.dewez at brgm.fr>
wrote :

>Dear R users,
>
>I am sorry to ask you such a pathetic newbie question, but how does one
>append data at the end of a data frame?

The rbind() function should work:  it adds rows to matrices and
dataframes.

However, I'd do something different for the problem you give:

>I am working with GRASS/R library, but the question is about R.
>I have a data.frame containing the following variables
>basinID, distoutlet, drainage_area, slope
>
>These variables are stored for all pixels of Grass Raster objects. For each
>drainage basin (basinID), I'd like to find the maximum of distoutlet. How
>can I store the pair of info (bv$ID, max(bv$distoutlet)) at each step of the
>for loop ?
>
>I presume something like this could do
>
>for (i 1:max(basinID)){
>bid <- i
>length <- max(distoutlet[which(basinID == I(i))])
>}
>
>But how do I handle the output and record bid and length in parallel in an
>object???

Appending a row at a time is very slow.  You'll find it much faster to
set up a matrix to hold the results in advance:

results <- matrix(NA, max(basinID), 2)
for (i in 1:max(basinID)) {
bid <- i
length <- max(distoutlet[which(basinID == i)])
results[i,] <- c(i, length)
}

Duncan Murdoch



From prechelt at pcpool.mi.fu-berlin.de  Tue Aug 31 14:19:29 2004
From: prechelt at pcpool.mi.fu-berlin.de (Lutz Prechelt)
Date: Tue, 31 Aug 2004 14:19:29 +0200
Subject: [R] blockwise sums
Message-ID: <85D25331FFB7AE4C900EA467D4ADA392045B5B@circle.pcpool.mi.fu-berlin.de>

I am looking for a function like 
  my.blockwisesum(vector, n)
that computes sums of disjoint subsequences of length n from vector
and can work with vector lengths that are not a multiple of n.

It should give me for instance
  my.blockwisesum(1:10, 3) == c(6, 15, 24, 10)

Is there a builtin function that can do this?
One could do it by coercing the vector into a matrix of width n,
and then use apply,
but that is cumbersome if the length is not divisible by n, 
is it not?
Any other ideas?

  Lutz

Prof. Dr. Lutz Prechelt;  prechelt at inf.fu-berlin.de
Institut fuer Informatik; Freie Universitaet Berlin
Takustr. 9; 14195 Berlin; Germany
+49 30 838 75115; http://www.inf.fu-berlin.de/inst/ag-se/



From andy_liaw at merck.com  Tue Aug 31 14:28:11 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 31 Aug 2004 08:28:11 -0400
Subject: [R] blockwise sums
Message-ID: <3A822319EB35174CA3714066D590DCD504AF82CC@usrymx25.merck.com>

If you insist, here's one way:

my.blockwisesum <- function(x, n, ...) {
    tapply(x, seq(1, length(x), by=n), sum, ...)
}

Andy

> From: Lutz Prechelt
> 
> I am looking for a function like 
>   my.blockwisesum(vector, n)
> that computes sums of disjoint subsequences of length n from vector
> and can work with vector lengths that are not a multiple of n.
> 
> It should give me for instance
>   my.blockwisesum(1:10, 3) == c(6, 15, 24, 10)
> 
> Is there a builtin function that can do this?
> One could do it by coercing the vector into a matrix of width n,
> and then use apply,
> but that is cumbersome if the length is not divisible by n, 
> is it not?
> Any other ideas?
> 
>   Lutz
> 
> Prof. Dr. Lutz Prechelt;  prechelt at inf.fu-berlin.de
> Institut fuer Informatik; Freie Universitaet Berlin
> Takustr. 9; 14195 Berlin; Germany
> +49 30 838 75115; http://www.inf.fu-berlin.de/inst/ag-se/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Ted.Harding at nessie.mcc.ac.uk  Tue Aug 31 14:08:58 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 31 Aug 2004 13:08:58 +0100 (BST)
Subject: [R] (no subject)
In-Reply-To: <41345E09.30806@directwave.com.br>
Message-ID: <XFMail.040831130858.Ted.Harding@nessie.mcc.ac.uk>

On 31-Aug-04 Paolo Tommasini wrote:
> Hi this is a very simple question ! if a have a set of date like
> x=c(1,2,3,4,4,5,6,8,7,8,8)
> 
> how can I find out the percentile of 7 ?

To some extent this depends on precisely how you want to define
"percentile", but if you adopt the interpretation as "percentage
of data not exceeding X" then

   100*sum(x<=7)/length(x)

will give it to you.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 31-Aug-04                                       Time: 13:08:58
------------------------------ XFMail ------------------------------



From B.Rowlingson at lancaster.ac.uk  Tue Aug 31 14:36:25 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 31 Aug 2004 13:36:25 +0100
Subject: [R] blockwise sums
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF82CC@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF82CC@usrymx25.merck.com>
Message-ID: <413470C9.2090007@lancaster.ac.uk>

Liaw, Andy wrote:
> If you insist, here's one way:
> 
> my.blockwisesum <- function(x, n, ...) {
>     tapply(x, seq(1, length(x), by=n), sum, ...)
> }
> 

  Did you test that? I get:

 > my.blockwisesum(1:10, 3)
Error in tapply(x, seq(1, length(x), by = n), sum, ...) :
         arguments must have same length


  Here's my solution with tapply and rep() to generate a vector like 
c(1,1,1,2,2,2,3,3,3,4):

baz.blockwisesum=
  function(v,n){tapply(v,rep(1:(1+length(v)/n),each=n)[1:length(v)],sum)}

 > baz.blockwisesum(1:10,3)
  1  2  3  4
  6 15 24 10

  - just ignore the 1 to 4 names, they cant hurt you.

Baz



From wolski at molgen.mpg.de  Tue Aug 31 14:40:06 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Tue, 31 Aug 2004 14:40:06 +0200
Subject: [R] blockwise sums
In-Reply-To: <85D25331FFB7AE4C900EA467D4ADA392045B5B@circle.pcpool.mi.fu-berlin.de>
References: <85D25331FFB7AE4C900EA467D4ADA392045B5B@circle.pcpool.mi.fu-berlin.de>
Message-ID: <200408311440060605.0116D96B@mail.math.fu-berlin.de>

Hi!

ind<-c(sort(rep(1:floor(length(x)/ 3 ) ,  3 )) , floor(length(x)/ 3 )+1)



by(x,ind,sum)

my.blockwisesum<-function(x,n,...)
{
	ind<-c(sort(rep(1:floor(length(x)/ n ) ,  n )) , floor(length(x)/ n )+1)
	return(tapply(x,ind,sum))
}


/Eryk


*********** REPLY SEPARATOR  ***********

On 8/31/2004 at 2:19 PM Lutz Prechelt wrote:

>>>I am looking for a function like 
>>>  my.blockwisesum(vector, n)
>>>that computes sums of disjoint subsequences of length n from vector
>>>and can work with vector lengths that are not a multiple of n.
>>>
>>>It should give me for instance
>>>  my.blockwisesum(1:10, 3) == c(6, 15, 24, 10)
>>>
>>>Is there a builtin function that can do this?
>>>One could do it by coercing the vector into a matrix of width n,
>>>and then use apply,
>>>but that is cumbersome if the length is not divisible by n, 
>>>is it not?
>>>Any other ideas?
>>>
>>>  Lutz
>>>
>>>Prof. Dr. Lutz Prechelt;  prechelt at inf.fu-berlin.de
>>>Institut fuer Informatik; Freie Universitaet Berlin
>>>Takustr. 9; 14195 Berlin; Germany
>>>+49 30 838 75115; http://www.inf.fu-berlin.de/inst/ag-se/
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



Dipl. bio-chem. Witold Eryk Wolski             @         MPI-Moleculare Genetic   
Ihnestrasse 63-73 14195 Berlin                'v'    
tel: 0049-30-83875219                        /   \       
mail: witek96 at users.sourceforge.net        ---W-W----    http://www.molgen.mpg.de/~wolski 
      wolski at molgen.mpg.de



From andy_liaw at merck.com  Tue Aug 31 14:42:16 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 31 Aug 2004 08:42:16 -0400
Subject: [R] blockwise sums
Message-ID: <3A822319EB35174CA3714066D590DCD504AF82D0@usrymx25.merck.com>

> From: Barry Rowlingson 
> 
> Liaw, Andy wrote:
> > If you insist, here's one way:
> > 
> > my.blockwisesum <- function(x, n, ...) {
> >     tapply(x, seq(1, length(x), by=n), sum, ...)
> > }
> > 
> 
>   Did you test that? I get:

Of course not (slap on wrist)!!  My apologies...

Andy
 
>  > my.blockwisesum(1:10, 3)
> Error in tapply(x, seq(1, length(x), by = n), sum, ...) :
>          arguments must have same length
> 
> 
>   Here's my solution with tapply and rep() to generate a vector like 
> c(1,1,1,2,2,2,3,3,3,4):
> 
> baz.blockwisesum=
>   
> function(v,n){tapply(v,rep(1:(1+length(v)/n),each=n)[1:length(
> v)],sum)}
> 
>  > baz.blockwisesum(1:10,3)
>   1  2  3  4
>   6 15 24 10
> 
>   - just ignore the 1 to 4 names, they cant hurt you.
> 
> Baz
> 
>



From dimitris.rizopoulos at med.kuleuven.ac.be  Tue Aug 31 14:51:18 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Tue, 31 Aug 2004 14:51:18 +0200
Subject: [R] blockwise sums
References: <85D25331FFB7AE4C900EA467D4ADA392045B5B@circle.pcpool.mi.fu-berlin.de>
Message-ID: <003e01c48f59$36282b40$ad133a86@www.domain>

Hi Lutz,

you could try the following:

blockwisesum <- function(x, n){
  nx <- length(x)
  if(nx%%n) x. <- c(x, rep(0., n*ceiling(nx/n)-nx)) else x. <- x
  x. <- matrix(x., ncol=n, byrow=TRUE)
  rowSums(x.)
}

blockwisesum(1:10, 3)

I hope this helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Doctoral Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Lutz Prechelt" <prechelt at pcpool.mi.fu-berlin.de>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, August 31, 2004 2:19 PM
Subject: [R] blockwise sums


> I am looking for a function like
>   my.blockwisesum(vector, n)
> that computes sums of disjoint subsequences of length n from vector
> and can work with vector lengths that are not a multiple of n.
>
> It should give me for instance
>   my.blockwisesum(1:10, 3) == c(6, 15, 24, 10)
>
> Is there a builtin function that can do this?
> One could do it by coercing the vector into a matrix of width n,
> and then use apply,
> but that is cumbersome if the length is not divisible by n,
> is it not?
> Any other ideas?
>
>   Lutz
>
> Prof. Dr. Lutz Prechelt;  prechelt at inf.fu-berlin.de
> Institut fuer Informatik; Freie Universitaet Berlin
> Takustr. 9; 14195 Berlin; Germany
> +49 30 838 75115; http://www.inf.fu-berlin.de/inst/ag-se/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Bernhard.Pfaff at drkw.com  Tue Aug 31 15:01:08 2004
From: Bernhard.Pfaff at drkw.com (Pfaff, Bernhard)
Date: Tue, 31 Aug 2004 15:01:08 +0200
Subject: [R] blockwise sums
Message-ID: <18D602BD42B7E24EB810D6454A58DB900A29BB01@ibfftce505.de.ad.drkw.net>

> 
> Liaw, Andy wrote:
> > If you insist, here's one way:
> > 
> > my.blockwisesum <- function(x, n, ...) {
> >     tapply(x, seq(1, length(x), by=n), sum, ...)
> > }
> > 
> 
>   Did you test that? I get:
> 
>  > my.blockwisesum(1:10, 3)
> Error in tapply(x, seq(1, length(x), by = n), sum, ...) :
>          arguments must have same length
> 
> 
>   Here's my solution with tapply and rep() to generate a vector like 
> c(1,1,1,2,2,2,3,3,3,4):
> 
> baz.blockwisesum=
>   
> function(v,n){tapply(v,rep(1:(1+length(v)/n),each=n)[1:length(
> v)],sum)}
> 
>  > baz.blockwisesum(1:10,3)
>   1  2  3  4
>   6 15 24 10
> 
>   - just ignore the 1 to 4 names, they cant hurt you.
> 
> Baz

To complete the picture: here is another one:

my.blockwisesum <- function(vec, n){
  vec <- as.vector(vec)
  n <- as.integer(n)
  total <- length(vec)
  if(total <= n){
    stop("\nn should be smaller than length of vector.\n")
  }
  start <- seq(1, total, n)
  end <- start + n - 1
  end[end > total] <- max(start)
  index <- 1 : length(start)
  return(sapply(index, function(x)sum(test[start[x]:end[x]])))
}

> test <- 1:150
> ptn <- proc.time()
> baz.blockwisesum(test,3)
  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
20 
  6  15  24  33  42  51  60  69  78  87  96 105 114 123 132 141 150 159 168
177 
 21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39
40 
186 195 204 213 222 231 240 249 258 267 276 285 294 303 312 321 330 339 348
357 
 41  42  43  44  45  46  47  48  49  50 
366 375 384 393 402 411 420 429 438 447 
> proc.time()-ptn
[1] 0.00 0.00 0.22   NA   NA
> 
> ptn <- proc.time()
> my.blockwisesum(test,3)
 [1]   6  15  24  33  42  51  60  69  78  87  96 105 114 123 132 141 150 159
168
[20] 177 186 195 204 213 222 231 240 249 258 267 276 285 294 303 312 321 330
339
[39] 348 357 366 375 384 393 402 411 420 429 438 447
> proc.time()-ptn
[1] 0.00 0.00 0.19   NA   NA
> 

HTH,
Bernhard


--------------------------------------------------------------------------------
The information contained herein is confidential and is inte...{{dropped}}



From p.dalgaard at biostat.ku.dk  Tue Aug 31 15:03:14 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 31 Aug 2004 15:03:14 +0200
Subject: [R] blockwise sums
In-Reply-To: <413470C9.2090007@lancaster.ac.uk>
References: <3A822319EB35174CA3714066D590DCD504AF82CC@usrymx25.merck.com>
	<413470C9.2090007@lancaster.ac.uk>
Message-ID: <x2d617a299.fsf@biostat.ku.dk>

Barry Rowlingson <B.Rowlingson at lancaster.ac.uk> writes:

> Liaw, Andy wrote:
> > If you insist, here's one way:
> > my.blockwisesum <- function(x, n, ...) {
> >     tapply(x, seq(1, length(x), by=n), sum, ...)
> > }
> >
> 
>   Did you test that? I get:
> 
>  > my.blockwisesum(1:10, 3)
> Error in tapply(x, seq(1, length(x), by = n), sum, ...) :
>          arguments must have same length
> 
> 
>   Here's my solution with tapply and rep() to generate a vector like
> c(1,1,1,2,2,2,3,3,3,4):
> 
> baz.blockwisesum=
>   function(v,n){tapply(v,rep(1:(1+length(v)/n),each=n)[1:length(v)],sum)}
> 
>  > baz.blockwisesum(1:10,3)
>   1  2  3  4
>   6 15 24 10

Slight variant

pd.blockwisesum <- 
 function(v,n){N <- length(v); tapply(x,gl(ceiling(N/n), n, N), sum)}

> pd.blockwisesum(1:10,3)
 1  2  3  4
 6 15 24 10


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ggrothendieck at myway.com  Tue Aug 31 15:13:57 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 31 Aug 2004 13:13:57 +0000 (UTC)
Subject: [R] blockwise sums
References: <85D25331FFB7AE4C900EA467D4ADA392045B5B@circle.pcpool.mi.fu-berlin.de>
Message-ID: <loom.20040831T150940-830@post.gmane.org>

Lutz Prechelt <prechelt <at> pcpool.mi.fu-berlin.de> writes:

: 
: I am looking for a function like 
:   my.blockwisesum(vector, n)
: that computes sums of disjoint subsequences of length n from vector
: and can work with vector lengths that are not a multiple of n.
: 
: It should give me for instance
:   my.blockwisesum(1:10, 3) == c(6, 15, 24, 10)

tapply(v, (seq(v)-1)%/%n, sum)



From Simon.Fear at synequanon.com  Tue Aug 31 15:13:49 2004
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Tue, 31 Aug 2004 14:13:49 +0100
Subject: [R] I've forgotten, why is box("") the default?
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572F023A4@synequanon01>

I've searched on CRAN for axes, axis, and other terms 
I've already forgotten, without (re)discovering the 
reason for S using "non-joining" axes by default, instead
of box("l").

MASS points me towards Cleveland (1993) but I don't 
have ready access to this any more. Could someone 
give me a one-liner to justify this choice to a sceptic?

It's something to do with not mis-interpreting the axes
intersection as (0,0), isn't it?

TIA,
Simon  
 
Simon Fear 
Senior Statistician 
Syne qua non Ltd 
Tel: +44 (0) 1379 644449 
Fax: +44 (0) 1379 644445 
email: Simon.Fear at synequanon.com 
web: http://www.synequanon.com 
  
Number of attachments included with this message: 0 
 
  
This message (and any associated files) is confidential and\...{{dropped}}



From scofield at bio.indiana.edu  Tue Aug 31 15:29:35 2004
From: scofield at bio.indiana.edu (Douglas G. Scofield)
Date: Tue, 31 Aug 2004 08:29:35 -0500
Subject: [R] D'agostino test
In-Reply-To: <5.0.2.1.2.20040830180658.00b04bd0@cimrs1.mnhn.fr>
Message-ID: <200408311325.i7VDP4nD020725@helix.cgb.indiana.edu>

> -----Original Message-----
> From: Alexandre Bournery [mailto:bournery at mnhn.fr] 
> Sent: Monday, August 30, 2004 11:08 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] D'agostino test
> 
> Hi, Does anyone know if the D'agostino test is available with R ?
> Alex
> 
> 
> 

Hi, 

See below, taken pretty much verbatim from Zar (1999)

Douglas G. Scofield, PhD        Department of Biology 
scofield at bio.indiana.edu        Indiana University
off: (812) 856-0115             1001 E. 3rd St.
fax: (812) 855-6705             Bloomington, IN  47405-3700
cell: (786) 514-9141
 
---------------------------------------------------------------

dagostino.pearson.test <- function(x) {
    # from Zar (1999), implemented by Doug Scofield,
scofield at bio.indiana.edu
    DNAME <- deparse(substitute(x))
    n <- length(x)
    x2 <- x * x
    x3 <- x * x2
    x4 <- x * x3
    # compute Z_g1
    k3 <- ((n*sum(x3)) - (3*sum(x)*sum(x2)) + (2*(sum(x)^3)/n)) /
((n-1)*(n-2))
    g1 <- k3 / sqrt(var(x)^3)
    sqrtb1 <- ((n - 2)*g1) / sqrt(n*(n - 1))
    A <- sqrtb1 * sqrt(((n + 1)*(n + 3)) / (6*(n - 2)))
    B <- (3*(n*n + 27*n - 70)*(n+1)*(n+3)) / ((n-2)*(n+5)*(n+7)*(n+9))
    C <- sqrt(2*(B - 1)) - 1
    D <- sqrt(C)
    E <- 1 / sqrt(log(D))
    F <- A / sqrt(2/(C - 1))
    Zg1 <- E * log(F + sqrt(F*F + 1))
    # compute Z_g2
    G <- (24*n*(n-2)*(n-3)) / (((n+1)^2)*(n+3)*(n+5))
    k4 <- (((n*n*n + n*n)*sum(x4)) - (4*(n*n + n)*sum(x3)*sum(x)) -
(3*(n*n - n)*sum(x2)^2) + (12*n*sum(x2)*sum(x)^2) - (6*sum(x)^4)) /
(n*(n-1)*(n-2)*(n-3))
    g2 <- k4 / var(x)^2
    H <- ((n-2)*(n-3)*abs(g2)) / ((n+1)*(n-1)*sqrt(G))
    J <- ((6*(n*n - 5*n + 2)) / ((n+7)*(n+9))) * sqrt((6*(n+3)*(n+5)) /
(n*(n-2)*(n-3)))
    K <- 6 + (8/J)*(2/J + sqrt(1 + 4/(J*J)))
    L <- (1 - 2/K) / (1 + H*sqrt(2/(K-4)))
    Zg2 <- (1 - 2/(9*K) - (L^(1/3))) / (sqrt(2/(9*K)))
    K2 <- Zg1*Zg1 + Zg2*Zg2
    pk2 <- pchisq(K2, 2, lower.tail=FALSE)
    RVAL <- list(statistic = c(K2 = K2), p.value = pk2, method =
"D'Agostino-Pearson normality test\n\nK2 is distributed as Chi-squared
with df=2", alternative = "distribution is not normal", data.name =
DNAME)
    class(RVAL) <- "htest"
    return(RVAL)
}



From tlumley at u.washington.edu  Tue Aug 31 15:35:24 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 31 Aug 2004 06:35:24 -0700 (PDT)
Subject: [R] (no subject)
In-Reply-To: <E7D5AB4811D20B489622AABA9C53859101F111E1@teal-exch.amgen.com>
References: <E7D5AB4811D20B489622AABA9C53859101F111E1@teal-exch.amgen.com>
Message-ID: <Pine.A41.4.58.0408310634190.114976@homer12.u.washington.edu>

On Mon, 30 Aug 2004, Austin, Matt wrote:

> A correction.  You either need to open the plotting device prior to the
> simulation that includes your plotting commands and close it after the
> simulation or have the name change dynamically in your simulation so that
> the runs go in separate files.
>
> An example of the first method would be
>
> postscript(file="/where/to/put/file/filename.ps")

It's also worth pointing out that many people will find pdf() more
convenient than postscript()

	-thomas


>
> ## your simulation commands
>
> dev.off()
>
> for the second method more info would be needed to know how you are running
> your simulation to help with dynamic naming.
>
> --Matt
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Austin, Matt
> Sent: Monday, August 30, 2004 18:56 PM
> To: 'Loke Chok Kang'; r-help at stat.math.ethz.ch
> Subject: RE: [R] (no subject)
>
>
>
> You need to use a device to print to such as
>
> postscript(file="/where/to/put/file/filename.ps")
> 	##your plotting code here
> dev.off()
>
> Writing a plot at each iteration of your simulation can impact the runtime
> greatly.
>
> Please read the posting guide at the bottom of the e-mail, it can help you
> get more helpful . . . help
>
> --Matt
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Loke Chok Kang
> Sent: Monday, August 30, 2004 18:42 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] (no subject)
>
>
> Hi,
> Sorry, I have a problem that require some help. As I am doing a project with
> R and this project requires me to do a lot of plotting as I run my
> simulation, I need R to help me store my plots automatically as the
> simulation is run. Could anyone advise me on how this can be done? If
> possible I need all the plots to be written to a file.
>
> PS:In all I have over 500 over plots, thus it will not be possible to
> manually plot all the plots out.
>
> >From yours faithfully,
> Chok Kang
> sci10137 at nus.edu.sg
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From tlumley at u.washington.edu  Tue Aug 31 15:40:32 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 31 Aug 2004 06:40:32 -0700 (PDT)
Subject: [R] blockwise sums
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF82CC@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF82CC@usrymx25.merck.com>
Message-ID: <Pine.A41.4.58.0408310639570.114976@homer12.u.washington.edu>

On Tue, 31 Aug 2004, Liaw, Andy wrote:

> If you insist, here's one way:
>
> my.blockwisesum <- function(x, n, ...) {
>     tapply(x, seq(1, length(x), by=n), sum, ...)
> }

If x is large, rowsum() should be faster than tapply().

	-thomas


>
> Andy
>
> > From: Lutz Prechelt
> >
> > I am looking for a function like
> >   my.blockwisesum(vector, n)
> > that computes sums of disjoint subsequences of length n from vector
> > and can work with vector lengths that are not a multiple of n.
> >
> > It should give me for instance
> >   my.blockwisesum(1:10, 3) == c(6, 15, 24, 10)
> >
> > Is there a builtin function that can do this?
> > One could do it by coercing the vector into a matrix of width n,
> > and then use apply,
> > but that is cumbersome if the length is not divisible by n,
> > is it not?
> > Any other ideas?
> >
> >   Lutz
> >
> > Prof. Dr. Lutz Prechelt;  prechelt at inf.fu-berlin.de
> > Institut fuer Informatik; Freie Universitaet Berlin
> > Takustr. 9; 14195 Berlin; Germany
> > +49 30 838 75115; http://www.inf.fu-berlin.de/inst/ag-se/
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From murdoch at stats.uwo.ca  Tue Aug 31 15:44:43 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 31 Aug 2004 09:44:43 -0400
Subject: [R] I've forgotten, why is box("") the default?
In-Reply-To: <6C8A8033ABC1E3468048ABC4F13CE572F023A4@synequanon01>
References: <6C8A8033ABC1E3468048ABC4F13CE572F023A4@synequanon01>
Message-ID: <fsv8j0tu4n3luthet3ojrj873tji9vdmmi@4ax.com>

On Tue, 31 Aug 2004 14:13:49 +0100, "Simon Fear"
<Simon.Fear at synequanon.com> wrote :

>I've searched on CRAN for axes, axis, and other terms 
>I've already forgotten, without (re)discovering the 
>reason for S using "non-joining" axes by default, instead
>of box("l").
>
>MASS points me towards Cleveland (1993) but I don't 
>have ready access to this any more. Could someone 
>give me a one-liner to justify this choice to a sceptic?
>
>It's something to do with not mis-interpreting the axes
>intersection as (0,0), isn't it?

I'm not sure I understand your question:  the default in S is a full
box unless you ask for no box (bty="n"), isn't it?  The justification
for this is "The four scale lines also provide a clearly defined
region where our eyes can search for data.  With just two, data can be
camouflaged by virtue of where they lie" (p. 35).  There's also advice
to keep the data away from the axes and to put the ticks outside the
box to avoid hiding extreme points.

I can't spot a discussion in Cleveland of the reason the axes don't
join when bty="n" is specified.  

Duncan Murdoch



From Simon.Fear at synequanon.com  Tue Aug 31 16:02:13 2004
From: Simon.Fear at synequanon.com (Simon Fear)
Date: Tue, 31 Aug 2004 15:02:13 +0100
Subject: [R] I've forgotten, why is box("") the default?
Message-ID: <6C8A8033ABC1E3468048ABC4F13CE572F023A6@synequanon01>

Not helped by my typo and being in a rush!

It's true that plot.default and surely many others do a box, since 

> par("bty")
[1] "o"

But this isn't what happens if you do things via axis() ...
(see first example in help(axis)).

Anyway, point taken, I couldn't find it because it is NOT what
is recommended!

> -----Original Message-----
> From: Duncan Murdoch [mailto:murdoch at stats.uwo.ca]
> Sent: 31 August 2004 14:45
> To: Simon Fear
> Cc: R-help
> Subject: Re: [R] I've forgotten, why is box("") the default?
> 
> 
> Security Warning: 
> If you are not sure an attachment is safe to open contact 
> Andy on x234. 
> There are 0 attachments with this message. 
> ________________________________________________________________ 
>  
> On Tue, 31 Aug 2004 14:13:49 +0100, "Simon Fear"
> <Simon.Fear at synequanon.com> wrote :
> 
> >I've searched on CRAN for axes, axis, and other terms 
> >I've already forgotten, without (re)discovering the 
> >reason for S using "non-joining" axes by default, instead
> >of box("l").
> >
> >MASS points me towards Cleveland (1993) but I don't 
> >have ready access to this any more. Could someone 
> >give me a one-liner to justify this choice to a sceptic?
> >
> >It's something to do with not mis-interpreting the axes
> >intersection as (0,0), isn't it?
> 
> I'm not sure I understand your question:  the default in S is a full
> box unless you ask for no box (bty="n"), isn't it?  The justification
> for this is "The four scale lines also provide a clearly defined
> region where our eyes can search for data.  With just two, data can be
> camouflaged by virtue of where they lie" (p. 35).  There's also advice
> to keep the data away from the axes and to put the ticks outside the
> box to avoid hiding extreme points.
> 
> I can't spot a discussion in Cleveland of the reason the axes don't
> join when bty="n" is specified.  
> 
> Duncan Murdoch
>  
 
Simon Fear 
Senior Statistician 
Syne qua non Ltd 
Tel: +44 (0) 1379 644449 
Fax: +44 (0) 1379 644445 
email: Simon.Fear at synequanon.com 
web: http://www.synequanon.com 
  
Number of attachments included with this message: 0 
 
  
This message (and any associated files) is confidential and\...{{dropped}}



From Luisr at frs.fo  Tue Aug 31 16:49:31 2004
From: Luisr at frs.fo (Luis Rideau Cruz)
Date: Tue, 31 Aug 2004 15:49:31 +0100
Subject: [R] "Restore" function in R
Message-ID: <s1349e13.020@ffdata.setur.fo>

R-help,

Any function in R that restore an object to its "original state" so
that if not satisfied with the modifications made to it there is no need
to redefine again ( 'undo' action????)

I believe there is something like that in S-plus.

Thank you



From rxg218 at psu.edu  Tue Aug 31 17:32:52 2004
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Tue, 31 Aug 2004 11:32:52 -0400
Subject: [R] N-dimensional delaunay tesselation & voronoi diagrams
Message-ID: <1093966372.29858.8.camel@blue.chem.psu.edu>

Hi,
 I've been looking for functions that can do delaunay tesselation and
generate voronoi cells. I came across deldir and tripack but both seem
to be restricted to 2D points. Are there any packages that can do a
tesselation in N dimensions? I know that Matlab and Mathematica use the
qhull package to provide functions for this. Does anybody know of any R
packages that do this (maybe by calling on qhull)?

Thanks,

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
Q: What do you get when you cross a mosquito with a mountain climber?
A: Nothing. You can't cross a vector with a scaler.



From jack_wang2000 at yahoo.com  Tue Aug 31 18:16:50 2004
From: jack_wang2000 at yahoo.com (JTW)
Date: Tue, 31 Aug 2004 09:16:50 -0700 (PDT)
Subject: [R] More efficient matrix computation
Message-ID: <20040831161650.70715.qmail@web40406.mail.yahoo.com>

I have a 20x3 matrix as follows:

> m <- replicate(3, matrix(rnorm(20),20,1))

I need to compute, say, 95th and 99th percentiles of
each column such that the resulting matrix becomes 2x3
with each row representing the respective percentile. 
My "best effort" is to compute one column at a time as
follows:

> quantile(m[,1], c(0.95, 0.99))

To do the same for columns 2 and 3, I would simply
change the column number accordingly.  Clearly, this
is not very efficient as I may have a large matrix
(e.g., 100,000x500) to work with.  Any help with the
code is appreciated.

Jack Wang



From ligges at statistik.uni-dortmund.de  Tue Aug 31 18:28:36 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 31 Aug 2004 18:28:36 +0200
Subject: [R] More efficient matrix computation
In-Reply-To: <20040831161650.70715.qmail@web40406.mail.yahoo.com>
References: <20040831161650.70715.qmail@web40406.mail.yahoo.com>
Message-ID: <4134A734.6030205@statistik.uni-dortmund.de>

JTW wrote:

> I have a 20x3 matrix as follows:
> 
> 
>>m <- replicate(3, matrix(rnorm(20),20,1))

Why not

   m <- matrix(rnorm(60), 20, 3)


> I need to compute, say, 95th and 99th percentiles of
> each column such that the resulting matrix becomes 2x3
> with each row representing the respective percentile. 
> My "best effort" is to compute one column at a time as
> follows:
> 
> 
>>quantile(m[,1], c(0.95, 0.99))

That's what apply() is made for:

   apply(m, 2, quantile, c(0.95, 0.99))

Uwe Ligges



> 
> To do the same for columns 2 and 3, I would simply
> change the column number accordingly.  Clearly, this
> is not very efficient as I may have a large matrix
> (e.g., 100,000x500) to work with.  Any help with the
> code is appreciated.
> 
> Jack Wang
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From wolski at molgen.mpg.de  Tue Aug 31 18:37:03 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Tue, 31 Aug 2004 18:37:03 +0200
Subject: [R] More efficient matrix computation
In-Reply-To: <20040831161650.70715.qmail@web40406.mail.yahoo.com>
References: <20040831161650.70715.qmail@web40406.mail.yahoo.com>
Message-ID: <200408311837030378.01EFC38A@mail.math.fu-berlin.de>

Hi!
apply(m,2,quantile,c(0.95,0.99))
/Eryk

*********** REPLY SEPARATOR  ***********

On 8/31/2004 at 9:16 AM JTW wrote:

>>>I have a 20x3 matrix as follows:
>>>
>>>> m <- replicate(3, matrix(rnorm(20),20,1))
>>>
>>>I need to compute, say, 95th and 99th percentiles of
>>>each column such that the resulting matrix becomes 2x3
>>>with each row representing the respective percentile. 
>>>My "best effort" is to compute one column at a time as
>>>follows:
>>>
>>>> quantile(m[,1], c(0.95, 0.99))
>>>
>>>To do the same for columns 2 and 3, I would simply
>>>change the column number accordingly.  Clearly, this
>>>is not very efficient as I may have a large matrix
>>>(e.g., 100,000x500) to work with.  Any help with the
>>>code is appreciated.
>>>
>>>Jack Wang
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



Dipl. bio-chem. Witold Eryk Wolski             @         MPI-Moleculare Genetic   
Ihnestrasse 63-73 14195 Berlin                'v'    
tel: 0049-30-83875219                        /   \       
mail: witek96 at users.sourceforge.net        ---W-W----    http://www.molgen.mpg.de/~wolski 
      wolski at molgen.mpg.de



From gunter.berton at gene.com  Tue Aug 31 18:39:09 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 31 Aug 2004 09:39:09 -0700
Subject: [R] blockwise sums
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF82D0@usrymx25.merck.com>
Message-ID: <200408311639.i7VGd9eK004250@compton.gene.com>

This is a cute problem, so how about a cute algorithm? Since all that is
wanted is the sum, matrix multiplication can be used. If n=vector length and
b = blocksize, then the issue comes down to constructing a block diagonal
matrix with nblk=n%/%b columns with b 1's in each block to multiply the
first b*nblk elements of the vector by (the sum of any remaining elements
can be appended to the result). This muliplier matrix is easily constructed
using diag():

matrix(rep(diag(nblk),each=b),ncol=nblk)

There may even be slicker ways to do it. Anyway, as no implicit looping
(tapply) is used, I suspect this would run considerably faster,too, although
this may be unimportant, and I haven't tested it.

Of course, this approach doesn't generalize. But it is a nice example of
some of R's little tricks.

Cheers,

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Liaw, Andy
> Sent: Tuesday, August 31, 2004 5:42 AM
> To: 'Barry Rowlingson'
> Cc: r-help at stat.math.ethz.ch
> Subject: RE: [R] blockwise sums
> 
> > From: Barry Rowlingson 
> > 
> > Liaw, Andy wrote:
> > > If you insist, here's one way:
> > > 
> > > my.blockwisesum <- function(x, n, ...) {
> > >     tapply(x, seq(1, length(x), by=n), sum, ...)
> > > }
> > > 
> > 
> >   Did you test that? I get:
> 
> Of course not (slap on wrist)!!  My apologies...
> 
> Andy
>  
> >  > my.blockwisesum(1:10, 3)
> > Error in tapply(x, seq(1, length(x), by = n), sum, ...) :
> >          arguments must have same length
> > 
> > 
> >   Here's my solution with tapply and rep() to generate a 
> vector like 
> > c(1,1,1,2,2,2,3,3,3,4):
> > 
> > baz.blockwisesum=
> >   
> > function(v,n){tapply(v,rep(1:(1+length(v)/n),each=n)[1:length(
> > v)],sum)}
> > 
> >  > baz.blockwisesum(1:10,3)
> >   1  2  3  4
> >   6 15 24 10
> > 
> >   - just ignore the 1 to 4 names, they cant hurt you.
> > 
> > Baz
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From jack_wang2000 at yahoo.com  Tue Aug 31 18:45:58 2004
From: jack_wang2000 at yahoo.com (JTW)
Date: Tue, 31 Aug 2004 09:45:58 -0700 (PDT)
Subject: [R] More efficient matrix computation
In-Reply-To: <200408311837030378.01EFC38A@mail.math.fu-berlin.de>
Message-ID: <20040831164558.64665.qmail@web40405.mail.yahoo.com>

A million thanks to all for the prompt reply: apply()
works!

Jack Wang



From mbibo at qldnet.com.au  Tue Aug 31 18:58:16 2004
From: mbibo at qldnet.com.au (Michael Bibo)
Date: Tue, 31 Aug 2004 16:58:16 +0000 (UTC)
Subject: [R] Rcmdr X11 protocol error message
References: <x2r7pn3gqc.fsf@biostat.ku.dk>
	<20040831113608.TXMS14082.tomts16-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <loom.20040831T173029-151@post.gmane.org>

John Fox <jfox <at> mcmaster.ca> writes:

> 
> Dear Michael,
> 
> A question: Do you observe these problems with tcltk in general or just with
> the Rcmdr package? Is it possible for you to test a Tcl/Tk program outside
> of R?
> 
And Peter asked which version of Tcl/Tk.

Apparently my system has version 8.4 installed, specifically:
Tcl-8.4.5-3-mdk (tclsh8.4) and Tk-8.4.5-3-mdk (libtk8.4.so).  As I understand
it, these are installed from RPMs on the installation DVD, and I note that they
are mandrake specific.

John - I wasn't sure if I had any other Tcl/Tk applications installed (it's not
always obvious when installing from RPM's).  I have certainly not encountered
these error messages with any other application.  I quickly downloaded "WISH
Supernotepad 1.2.3".  This application requires Tcl/Tk 8.4 or greater.  There
are no graphics window in this application, but plenty of dialogue boxes.  It
gave no errors.  Is this an appropriate test?

If this is a mandrake-configuration-specific problem, it may not be worth
spending too much time investigating, as R Commandr still works.  I can always
try re-installing Tcl/Tk from source when/if I have time.

Regards,

Michael



From alexandrepatrot at yahoo.com.br  Tue Aug 31 19:23:19 2004
From: alexandrepatrot at yahoo.com.br (=?iso-8859-1?q?Alexandre=20Galv=E3o=20Patriota?=)
Date: Tue, 31 Aug 2004 14:23:19 -0300 (ART)
Subject: [R] after lm-fit: equality of two regression coefficients
Message-ID: <20040831172319.28989.qmail@web52705.mail.yahoo.com>

Dear  Christoph, You can use the package gregmisc
require(gregmisc)
C<-c(0,1,-1)
glh.test(my.lm,C)

I hope that helps

Alexandre Galv??o

> Hi
> 
> Let's assume, we have a multiple linear regression,
such as the one 
> using the Scottish hills data (MASS, data(hills)):
> 
> one dependent variable: time
> two independent var (metric): dist, climb
> 
> if I am interested, after (!) fitting a lm:
> 
> 	my. lm <- lm(time ~ dist + climb, data = hills)
> 
> in the equivalence (or non-equivalence) of the two
predictors "dist" 
and 
> "climb":
> 
> 	H0: dist = climb

I think you intend to ask if the *coefficients* in the
fit should be 
equal, which is nonsense in this example of course.

> Is there any function in R, which lets me calculate
this, in just 
giving 
> the lm-object "my.lm" and e.g. a vector such as c(1,
-1), 
> operationalizing the hypothesis H0: t(c(1, -1)) %*%
c(dist, climb) = 
0 ?

library(car)
?linear.hypothesis


 Christoph,
If you are interested in testing for b1=b2 in a
regression model, say

y=b0+b1*x1+b2*x2+e

you can compare the two models o1 and o2

o1<-lm(y~x1+x2)
o2<-lm(y~I(x1+x2))

best,
vito



From DAVID.BICKEL at PIONEER.COM  Tue Aug 31 19:26:00 2004
From: DAVID.BICKEL at PIONEER.COM (Bickel, David)
Date: Tue, 31 Aug 2004 12:26:00 -0500
Subject: [R] enter browser on error
Message-ID: <5F883C17941B9F4E80E5FA8C9F1C5E0E010FBAD7@jhms08.phibred.com>

Is there a way I can get R to automatically enter the browser inside a user-defined function on the generation of an error? Specifically, I'm trying to debug this:

Error in as.double.default(sapply(lis, FUN)) : 
        (list) object cannot be coerced to double
In addition: There were 38 warnings (use warnings() to see them)
> traceback()
8: as.double.default(sapply(lis, FUN))
7: as.numeric(sapply(lis, FUN))
6: numeric.sapply(function(x) {
       x at statistic0
   })

On detection of the error, I would like browser() to be called at the level of numeric.sapply(), so that I can examine x. I'm wondering if this can be done by modifying the default error handling. Using try() with browser() didn't work.

Thanks,
David
_____________________________
David Bickel  http://davidbickel.com
Research Scientist
Pioneer Hi-Bred International
Bioinformatics & Exploratory Research
7250 NW 62nd Ave., PO Box 552
Johnston, Iowa 50131-0552
515-334-4739 Tel
515-334-6634 Fax
david.bickel at pioneer.com, bickel at prueba.info



This communication is for use by the intended recipient and ...{{dropped}}



From ripley at stats.ox.ac.uk  Tue Aug 31 19:36:38 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 31 Aug 2004 18:36:38 +0100 (BST)
Subject: [R] enter browser on error
In-Reply-To: <5F883C17941B9F4E80E5FA8C9F1C5E0E010FBAD7@jhms08.phibred.com>
Message-ID: <Pine.LNX.4.44.0408311834160.12607-100000@gannet.stats>

Have you tried options(error=recover)?

On Tue, 31 Aug 2004, Bickel, David wrote:

> Is there a way I can get R to automatically enter the browser inside a user-defined function on the generation of an error? Specifically, I'm trying to debug this:
> 
> Error in as.double.default(sapply(lis, FUN)) : 
>         (list) object cannot be coerced to double
> In addition: There were 38 warnings (use warnings() to see them)
> > traceback()
> 8: as.double.default(sapply(lis, FUN))
> 7: as.numeric(sapply(lis, FUN))
> 6: numeric.sapply(function(x) {
>        x at statistic0
>    })
> 
> On detection of the error, I would like browser() to be called at the
> level of numeric.sapply(), so that I can examine x. I'm wondering if
> this can be done by modifying the default error handling. Using try()
> with browser() didn't work.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tplate at blackmesacapital.com  Tue Aug 31 19:46:52 2004
From: tplate at blackmesacapital.com (Tony Plate)
Date: Tue, 31 Aug 2004 11:46:52 -0600
Subject: [R] enter browser on error
In-Reply-To: <5F883C17941B9F4E80E5FA8C9F1C5E0E010FBAD7@jhms08.phibred.co
 m>
References: <5F883C17941B9F4E80E5FA8C9F1C5E0E010FBAD7@jhms08.phibred.com>
Message-ID: <6.1.0.6.2.20040831114605.067131b8@mailhost.blackmesacapital.com>

use options(error=recover), e.g.:

 > remove("x")
NULL
Warning message:
remove: variable "x" was not found
 > (function() {x})()
Error in (function() { : Object "x" not found
 > options(error=recover)
 > (function(y=1) {x})(2)
Error in (function(y = 1) { : Object "x" not found

Enter a frame number, or 0 to exit
1:(function(y = 1) {
Selection: 1
Called from: eval(expr, envir, enclos)
Browse[1]> y
[1] 2
Browse[1]>

Enter a frame number, or 0 to exit
1:(function(y = 1) {
Selection: 0
 >

At Tuesday 11:26 AM 8/31/2004, Bickel, David wrote:
>Is there a way I can get R to automatically enter the browser inside a 
>user-defined function on the generation of an error? Specifically, I'm 
>trying to debug this:
>
>Error in as.double.default(sapply(lis, FUN)) :
>         (list) object cannot be coerced to double
>In addition: There were 38 warnings (use warnings() to see them)
> > traceback()
>8: as.double.default(sapply(lis, FUN))
>7: as.numeric(sapply(lis, FUN))
>6: numeric.sapply(function(x) {
>        x at statistic0
>    })
>
>On detection of the error, I would like browser() to be called at the 
>level of numeric.sapply(), so that I can examine x. I'm wondering if this 
>can be done by modifying the default error handling. Using try() with 
>browser() didn't work.
>
>Thanks,
>David
>_____________________________
>David Bickel  http://davidbickel.com
>Research Scientist
>Pioneer Hi-Bred International
>Bioinformatics & Exploratory Research
>7250 NW 62nd Ave., PO Box 552
>Johnston, Iowa 50131-0552
>515-334-4739 Tel
>515-334-6634 Fax
>david.bickel at pioneer.com, bickel at prueba.info
>
>
>
>This communication is for use by the intended recipient and ...{{dropped}}
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From caimiaow at u.washington.edu  Tue Aug 31 19:49:59 2004
From: caimiaow at u.washington.edu (Caimiao Wei)
Date: Tue, 31 Aug 2004 10:49:59 -0700
Subject: [R] sample size for t-tests
Message-ID: <038a01c48f82$ef9b4f70$8800a8c0@BumgarnerLab.local>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040831/26a7109e/attachment.pl

From sundar.dorai-raj at pdf.com  Tue Aug 31 19:02:38 2004
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 31 Aug 2004 10:02:38 -0700
Subject: [R] More efficient matrix computation
In-Reply-To: <20040831161650.70715.qmail@web40406.mail.yahoo.com>
References: <20040831161650.70715.qmail@web40406.mail.yahoo.com>
Message-ID: <4134AF2E.6000609@pdf.com>



JTW wrote:

> I have a 20x3 matrix as follows:
> 
> 
>>m <- replicate(3, matrix(rnorm(20),20,1))
> 
> 
> I need to compute, say, 95th and 99th percentiles of
> each column such that the resulting matrix becomes 2x3
> with each row representing the respective percentile. 
> My "best effort" is to compute one column at a time as
> follows:
> 
> 
>>quantile(m[,1], c(0.95, 0.99))
> 
> 
> To do the same for columns 2 and 3, I would simply
> change the column number accordingly.  Clearly, this
> is not very efficient as I may have a large matrix
> (e.g., 100,000x500) to work with.  Any help with the
> code is appreciated.
> 
> Jack Wang
> 

How about:

apply(m, 2, quantile, c(0.95, 0.99))


--sundar



From tlumley at u.washington.edu  Tue Aug 31 20:49:54 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 31 Aug 2004 11:49:54 -0700 (PDT)
Subject: [R] sample size for t-tests
In-Reply-To: <038a01c48f82$ef9b4f70$8800a8c0@BumgarnerLab.local>
References: <038a01c48f82$ef9b4f70$8800a8c0@BumgarnerLab.local>
Message-ID: <Pine.A41.4.58.0408311143010.75308@homer10.u.washington.edu>

On Tue, 31 Aug 2004, Caimiao Wei wrote:

> Dear all,
>
> Could any one please tell me the exact formula R uses to calculate the
> sample size for one-sample and two-sample t-tests? Thanks,

There isn't a formula in closed form.  The exact procedure is in the code.

In words: the critical value for the t-test comes from a t-distribution.
The distribution of the t-statistic under the alternative is a non-central
t-distribution, and the sample size is adjusted to get the requested
power.

	-thomas



From alex.bach at irta.es  Tue Aug 31 20:52:14 2004
From: alex.bach at irta.es (Alex Bach)
Date: Tue, 31 Aug 2004 20:52:14 +0200
Subject: [R] Obtaining Least Square Means with lme (mixed models)
Message-ID: <DFA9FF64-FB7E-11D8-80FB-000393D55A8C@irta.es>

Hello,

I was wondering how one could obtain LSM for fixed factors in a 
mixed-effect model using lme. In other words, if I have model such as:

model<-lme(yield~period+treatment+period*treatment, 
data=data,random=~1|cow)

how can I get LSM for period, treatment and their interaction?

Thanks a lot!

Alex



From danbebber at forestecology.co.uk  Tue Aug 31 21:09:31 2004
From: danbebber at forestecology.co.uk (Dan Bebber)
Date: Tue, 31 Aug 2004 20:09:31 +0100
Subject: [R] spatial autcorrelation in glmmPQL
Message-ID: <000001c48f8e$0b7bb080$fe8f2e50@plants.ox.ac.uk>

I am unable to specify error spatial autocorrelation structure in glmmPQL:

DATA
x and y coordinates for sample points at which presence/absence of seedlings
and canopy openness were recorded in different forest stands.

QUESTION
Does seedling density increase with canopy openness?

ANALYSIS
Initial analysis using glmmPQL with binomial errors and stand as random
effect
> result <- glmmPQL(seedlings ~ canopy, random = ~1|stand, family=binomial,
data=regen)

Semivariogram of residuals from this fit using geoR showed spatial
autocorrelation with range 34.9 m and relative nugget of 75%. Therefore I
tried to create a corStruct object:
> corel <- corSpher(value = c(34.9,0.75), form = ~x+y, nugget=TRUE)
> corel <- Initialize(corel, data = regen)

And specify the structure in a new glmmPQL
> result2 <- glmmPQL(seedlings ~ canopy, random = ~|stand, family =
binomial, correlation = corel, data=regen)

PROBLEM
I get the error message:
iteration 1
Error in eval(expr, envir, enclos) : Object "x" not found

(Running R 1.9.1 on Windows ME)

I appear to be implementing corSpher and Initialize incorrectly- any help
greatly appreciated.


____________________________
Dr. Daniel P. Bebber
Department of Plant Sciences
University of Oxford
South Parks Road
Oxford
OX1 3RB
Tel. 01865 275060
---



From jonsen at mathstat.dal.ca  Tue Aug 31 21:25:38 2004
From: jonsen at mathstat.dal.ca (Ian Jonsen)
Date: Tue, 31 Aug 2004 16:25:38 -0300 (ADT)
Subject: [R] add single contour line to levelplot
Message-ID: <3963.129.173.33.213.1093980338.squirrel@mail.mathstat.dal.ca>

Hello,

I want to add a single contour line to a levelplot but can't figure out
how to do it 'on-the-fly'. When I include the last line in the code below,
I get the following error:

Error in NextMethod("[") : Argument "subscripts" is missing, with no default

Any tips on how to fix this are greatly appreciated!

Ian Jonsen


levelplot(dens~nu*sigma,data=tsurflikB,at=c(-20,-10,-5,-4,-3,-2,-1),
 col.regions=gray(seq(0.5,0.9,length=14)),region=T,colorkey=F,aspect=1,
 panel=function(x,y,z,...){
      panel.levelplot(x=x,y=y,z=z,...)
      lpoints(x[z==0],y[z==0],pch=16,cex=0.5)
      panel.levelplot(z~x*y,at=-3,contour=T,aspect=1)
 }
)

_______________________________
Ian Jonsen, Postdoctoral Fellow
Dept Biology, Dalhousie University, Halifax, NS, CAN
Phone: 902 494 3910; Fax: 902 494 3736
jonsen at mathstat.dal.ca



From p.dalgaard at biostat.ku.dk  Tue Aug 31 21:37:14 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 31 Aug 2004 21:37:14 +0200
Subject: [R] Rcmdr X11 protocol error message
In-Reply-To: <loom.20040831T173029-151@post.gmane.org>
References: <x2r7pn3gqc.fsf@biostat.ku.dk>
	<20040831113608.TXMS14082.tomts16-srv.bellnexxia.net@JohnDesktop8300>
	<loom.20040831T173029-151@post.gmane.org>
Message-ID: <x2brgr3xqt.fsf@biostat.ku.dk>

Michael Bibo <mbibo at qldnet.com.au> writes:

> John Fox <jfox <at> mcmaster.ca> writes:
> 
> > 
> > Dear Michael,
> > 
> > A question: Do you observe these problems with tcltk in general or just with
> > the Rcmdr package? Is it possible for you to test a Tcl/Tk program outside
> > of R?
> > 
> And Peter asked which version of Tcl/Tk.
> 
> Apparently my system has version 8.4 installed, specifically:
> Tcl-8.4.5-3-mdk (tclsh8.4) and Tk-8.4.5-3-mdk (libtk8.4.so).  As I understand
> it, these are installed from RPMs on the installation DVD, and I note that they
> are mandrake specific.
> 
> John - I wasn't sure if I had any other Tcl/Tk applications installed (it's not
> always obvious when installing from RPM's).  I have certainly not encountered
> these error messages with any other application.  I quickly downloaded "WISH
> Supernotepad 1.2.3".  This application requires Tcl/Tk 8.4 or greater.  There
> are no graphics window in this application, but plenty of dialogue boxes.  It
> gave no errors.  Is this an appropriate test?
> 
> If this is a mandrake-configuration-specific problem, it may not be worth
> spending too much time investigating, as R Commandr still works.  I can always
> try re-installing Tcl/Tk from source when/if I have time.

I don't think we have evidence that it's the Tcl installation,
although it could be (Google suggests that there have been problems
with at least some versions, although most references seem rather
old). I can't seem to reproduce the effect with SUSE's tk-8.4.6-28
either. If it is a bug in Rcmdr, then we'd want to find it and you
have the only handle on it....

BTW, sometimes Tk errors allow you to see a trace of the execution.
Would this happen to be one of those situations?

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From murdoch at stats.uwo.ca  Tue Aug 31 21:45:27 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 31 Aug 2004 15:45:27 -0400
Subject: [R] I've forgotten, why is box("") the default?
In-Reply-To: <6C8A8033ABC1E3468048ABC4F13CE572F023A6@synequanon01>
References: <6C8A8033ABC1E3468048ABC4F13CE572F023A6@synequanon01>
Message-ID: <77l9j09j84205op0he2vtcp9rl2kttiepe@4ax.com>

On Tue, 31 Aug 2004 15:02:13 +0100, "Simon Fear"
<Simon.Fear at synequanon.com> wrote :

>Not helped by my typo and being in a rush!
>
>It's true that plot.default and surely many others do a box, since 
>
>> par("bty")
>[1] "o"
>
>But this isn't what happens if you do things via axis() ...
>(see first example in help(axis)).

I suspect it's just done that way for programming convenience.  Since
it's hard or impossible to delete a line once drawn, axis draws the
minimal sensible one; if you want more, you use box() to draw it.

Duncan Murdoch



From rduarte at unicamp.br  Tue Aug 31 22:08:00 2004
From: rduarte at unicamp.br (Rodrigo Drummond)
Date: Tue, 31 Aug 2004 17:08:00 -0300 (BRT)
Subject: [R] Problem (bug?) with vector indices
In-Reply-To: <200408311011.i7VA4wVu010894@hypatia.math.ethz.ch>
References: <200408311011.i7VA4wVu010894@hypatia.math.ethz.ch>
Message-ID: <1094.143.106.4.184.1093982880.squirrel@143.106.4.184>

Hi all,

I found a problem on R that looks like a bug to me. I am working with R
for windows verson 1.9.0, but the problem also happens on the linux
version 1.9.0. I hope that the problem is on my reasoning, and I would be
grateful if someone could explain me why this happens.
There goes the example:

> x<-seq(-2,2,0.001)
> y<-rep(0,length(x))
> for (i in x){y[1000*i+2001]<-i^2}
> plot(x,y)    # Here I found something wrong
> which(y==0)
 [1]  244  353  357  361  365  369  482  486  490  494  579  583  587  591
 595
[16]  599  603  607  611  615  619  708  712  716  720  724  728  732  736
 740
[31]  744  837  841  845  849  853  857  861  865  869  962  966  970  974
 978
[46]  982  986  990  994  998 2001
> x[244]
[1] -1.757
> i<-x[244]
> 1000*i+2001
[1] 244
> y[1000*i+2001]
[1] 3.087049
> y[244]
[1] 0
> y[243]
[1] 3.087049


What means that the expression 1000*i+2001, which is equal to 244, inside
the brackets is evaluated as 243! The same occurs with every value of x
that results in a zero value of y, the right value is stored in y one
position before it should be, overwriting the previous value at this
position.
The bug is on R or on my mind?

Thanks a lot.
Rodrigo Drummond

____________________________________________
Rodrigo D. Drummond
Laboratorio Genoma Funcional
Centro de Biologia Molecular e Eng. Genetica
Universidade Estadual de Campinas
Caixa Postal 6010
13083-875 - Campinas - SP - Brasil
Tel: xx-19-3788-1119 Fax: xx-19-3788-1089



From p.dalgaard at biostat.ku.dk  Tue Aug 31 22:25:37 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 31 Aug 2004 22:25:37 +0200
Subject: [R] Problem (bug?) with vector indices
In-Reply-To: <1094.143.106.4.184.1093982880.squirrel@143.106.4.184>
References: <200408311011.i7VA4wVu010894@hypatia.math.ethz.ch>
	<1094.143.106.4.184.1093982880.squirrel@143.106.4.184>
Message-ID: <x27jrf3vi6.fsf@biostat.ku.dk>

"Rodrigo Drummond" <rduarte at unicamp.br> writes:

> Hi all,
> 
> I found a problem on R that looks like a bug to me. I am working with R
> for windows verson 1.9.0, but the problem also happens on the linux
> version 1.9.0. I hope that the problem is on my reasoning, and I would be
> grateful if someone could explain me why this happens.
> There goes the example:
> 
> > x<-seq(-2,2,0.001)
> > y<-rep(0,length(x))
> > for (i in x){y[1000*i+2001]<-i^2}
...
> > x[244]
> [1] -1.757
> > i<-x[244]
> > 1000*i+2001
> [1] 244
> > y[1000*i+2001]
> [1] 3.087049
> > y[244]
> [1] 0
> > y[243]
> [1] 3.087049
> 
> 
> What means that the expression 1000*i+2001, which is equal to 244, inside
> the brackets is evaluated as 243! The same occurs with every value of x
> that results in a zero value of y, the right value is stored in y one
> position before it should be, overwriting the previous value at this
> position.
> The bug is on R or on my mind?

The latter: 

Have a look at (1000*x[244]+2001)-244

Beware floating point arithmetic!

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From sundar.dorai-raj at PDF.COM  Tue Aug 31 22:39:01 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Tue, 31 Aug 2004 13:39:01 -0700
Subject: [R] add single contour line to levelplot
In-Reply-To: <3963.129.173.33.213.1093980338.squirrel@mail.mathstat.dal.ca>
References: <3963.129.173.33.213.1093980338.squirrel@mail.mathstat.dal.ca>
Message-ID: <4134E1E5.4010701@pdf.com>



Ian Jonsen wrote:

> Hello,
> 
> I want to add a single contour line to a levelplot but can't figure out
> how to do it 'on-the-fly'. When I include the last line in the code below,
> I get the following error:
> 
> Error in NextMethod("[") : Argument "subscripts" is missing, with no default
> 
> Any tips on how to fix this are greatly appreciated!
> 
> Ian Jonsen
> 
> 
> levelplot(dens~nu*sigma,data=tsurflikB,at=c(-20,-10,-5,-4,-3,-2,-1),
>  col.regions=gray(seq(0.5,0.9,length=14)),region=T,colorkey=F,aspect=1,
>  panel=function(x,y,z,...){
>       panel.levelplot(x=x,y=y,z=z,...)
>       lpoints(x[z==0],y[z==0],pch=16,cex=0.5)
>       panel.levelplot(z~x*y,at=-3,contour=T,aspect=1)
>  }
> )
> 

You have misued panel.levelplot and also forgot (neglected?) to include 
"..." in the second call. I think you want something like:

# from ?levelplot
library(lattice)
x <- seq(pi/4, 5 * pi, length = 100)
y <- seq(pi/4, 5 * pi, length = 100)
r <- as.vector(sqrt(outer(x^2, y^2, "+")))
grid <- expand.grid(x = x, y = y)
grid$z <- cos(r^2) * exp(-r/(pi^3))
levelplot(z ~ x * y, grid, cuts = 50, scales=list(log="e"),
           xlab = "", ylab = "", main = "Weird Function",
           sub = "with log scales",
           panel = function(x, y, z, at, contour, ...) {
             panel.levelplot(x, y, z, at = 0, contour = TRUE, ...)
             lpoints(x[z <= -0.5],y[z <= -0.5], pch = 16, cex = 0.5)
           },
           colorkey = FALSE, region = TRUE)

Note the usage of "...". I also think you do not need two calls to 
panel.levelplot, though I may be misunderstanding your intention. One 
will suffice, as it does here.

--sundar



From dsheuman at rogers.com  Tue Aug 31 23:52:05 2004
From: dsheuman at rogers.com (Danny Heuman)
Date: Tue, 31 Aug 2004 17:52:05 -0400 (EDT)
Subject: [R] Sparse Matrices in R
Message-ID: <20040831215205.20730.qmail@web88011.mail.re2.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040831/61992c96/attachment.pl

From rkoenker at uiuc.edu  Tue Aug 31 23:57:38 2004
From: rkoenker at uiuc.edu (roger koenker)
Date: Tue, 31 Aug 2004 16:57:38 -0500
Subject: [R] Sparse Matrices in R
In-Reply-To: <20040831215205.20730.qmail@web88011.mail.re2.yahoo.com>
References: <20040831215205.20730.qmail@web88011.mail.re2.yahoo.com>
Message-ID: <C62FAFB9-FB98-11D8-88D7-000A95A7E3AA@uiuc.edu>

Take a look at the package SparseM

url:	www.econ.uiuc.edu/~roger        	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820

On Aug 31, 2004, at 4:52 PM, Danny Heuman wrote:

>
> I have data in i,j,r format,
>
>
>
> where r is the value in location A[i,j] for some imaginary matrix A.
>
> I need to build this matrix A, but given the sizes of i and j, I 
> believe that using a sparse format would be most adequate.
>
> Hopefully this will allow me to perform some basic matrix manipulation 
> such as multiplication, addition, rowsums,  transpositions, subsetting 
> etc etc.
>
>
>
> Is there any way to achieve this goal in R?
> Thanks,
>
> Danny
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



