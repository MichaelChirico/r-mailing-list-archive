From nleonard at tartarus.uwa.edu.au  Mon Nov  1 10:33:28 2004
From: nleonard at tartarus.uwa.edu.au (Neil Leonard)
Date: Mon, 1 Nov 2004 17:33:28 +0800
Subject: [R] Survival Analysis Table
Message-ID: <1652ACFE-2BE9-11D9-B848-003065D5B8EC@tartarus.uwa.edu.au>

Hi,

I am trying to compute a table of a Kaplan Meier survival function.

I have created the Kaplan Meier using:

field.KM <-  
survfit(Surv(project_no_1983$entryage3,project_no_1983$age_at_death,proj 
ect_no_1983$death)~1)

and I want to find the survival rate and confidence intervals at  
different times (eg, 1 year, 2 years, 5 years, etc).

Can anybody tell me how to do this?


Thanks,
Neil



From songj at ucalgary.ca  Mon Nov  1 13:28:09 2004
From: songj at ucalgary.ca (songj@ucalgary.ca)
Date: Mon, 01 Nov 2004 05:28:09 -0700
Subject: [R] R
In-Reply-To: <1098465725.3131.22.camel@ramasamy.stats>
Message-ID: <200411011228.iA1CS9V06415@smtp1.ucalgary.ca>

Hi, Adaikalavan Ramasamy:

Thanks your message.
I installed R in our SunRay. It seems it is only commend line, could I run 
RGui in SunRay? Thanks. 

Have a good day.

Yours

Song



> You will need to get R working first before installing BioConductor
> packages.
> 
> Please provide a more useful output (like the error message) if you want
> useful help.
> 
> Have you tried reading the R manual or searching the mail archives [2].
> 
> [1] http://cran.r-project.org/doc/manuals/R-admin.pdf
> [2] http://cran.r-project.org/search.html
> 
> 
> On Fri, 2004-10-22 at 17:58, songj at ucalgary.ca wrote:
> > Hi, Everyone:
> > 
> > We are installing R package in our Sun Solaris 9, and falling in 
trouble. We 
> > downloaded all package from bioconductor, the R can not be installed in 
Sun 
> > Solaris 9. We greatly appreciated if you could give us any suggestions. 
> > Thanks.
> > 
> > Yours
> > 
> > Song
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-
guide.html
> > 
> 
> 



--



From clemens.tilke at web.de  Mon Nov  1 13:20:23 2004
From: clemens.tilke at web.de (clemens.tilke@web.de)
Date: Mon, 01 Nov 2004 13:20:23 +0100
Subject: [R] GLMM
Message-ID: <41863817.20449.AA3657@localhost>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041101/fdeaa6cb/attachment.pl

From ripley at stats.ox.ac.uk  Mon Nov  1 14:03:14 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 1 Nov 2004 13:03:14 +0000 (GMT)
Subject: [R] GLMM
In-Reply-To: <41863817.20449.AA3657@localhost>
Message-ID: <Pine.LNX.4.44.0411011254210.3018-100000@gannet.stats>

On Mon, 1 Nov 2004 clemens.tilke at web.de wrote:

> I have a problem concerning estimation of GLMM. I used methods from 3 different 
> packages (see program). 

You haven't really attributed the functions you use to particular
packages.  If this is glmm() from Jim Lindsey's packages then it was our
experience around the time that MASS was written that is often discordant
with other implementations/approaches.  As a result we have not used it
more recently.

> I would expect similar results for glmm and glmmML. The result differ in
> the estimated standard errors, however. I compared the results to MASS,
> 4th ed., p. 297. The results from glmmML resemble the given result for
> 'Numerical integration', but glmm output differs. For the intercept e.g.
> I have a standard error of 0.4354 from glmm and 0.5338 from glmmML. Any
> idea about this problem??

My main reaction is that the intercept is not an interesting parameter in
that problem and that small differences in standard error are not of any
practical consequence.  So what exactly is `this problem'?

> 
> Thanks in advance
> Clemens Tilke
> 
> 
> 
> library (MASS)
> data (bacteria)
> 
> contrasts (bacteria$trt) <- structure (contr.sdif (3), 
> dimnames = list (NULL, c ("drug", "enc")))
>  
> bacteria.pql <- glmmPQL (y ~ trt + I(week > 2), random = ~ 1 | 
> ID, family = binomial, data = bacteria)
> 
> summary (bacteria.pql)
> 
> library (repeated)
> 
> y1 <- 1 * (bacteria$y == "y")
> 
> bacteria1 <- data.frame (bacteria, y1 = y1)
> 
> bacteria.glmm <- glmm (y1 ~ trt + I(week > 2), nest = ID, data 
> = bacteria1, family = binomial)
> 
> summary (bacteria.glmm)
> 
> library (glmmML)
> 
> bacteria.glmmML <- glmmML (y1 ~ trt + I(week > 2), cluster = 
> bacteria1$ID, data = bacteria1, family = binomial)
> 
> summary (bacteria.glmmML)
> 
> bacteria.glmmML

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ramasamy at cancer.org.uk  Mon Nov  1 14:29:54 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 01 Nov 2004 13:29:54 +0000
Subject: [R] R
In-Reply-To: <200411011228.iA1CS9V06415@smtp1.ucalgary.ca>
References: <200411011228.iA1CS9V06415@smtp1.ucalgary.ca>
Message-ID: <1099315794.3322.4.camel@ndmpc126.ihs.ox.ac.uk>

Rgui ? I think only Windows pre-compiled distribution has 'Rgui.exe'
file but others in the list know better.

I presume SunRay is the Sun Solaris 9.0 you mentioned below. If so,
please try with R pre-compiled for the appropriate platform or build
from the source. See http://www.cran.r-project.org/. You system
administrator should be able to help.

Regards, Adai


On Mon, 2004-11-01 at 12:28, songj at ucalgary.ca wrote:
> Hi, Adaikalavan Ramasamy:
> 
> Thanks your message.
> I installed R in our SunRay. It seems it is only commend line, could I run 
> RGui in SunRay? Thanks. 
> 
> Have a good day.
> 
> Yours
> 
> Song
> 
> 
> 
> > You will need to get R working first before installing BioConductor
> > packages.
> > 
> > Please provide a more useful output (like the error message) if you want
> > useful help.
> > 
> > Have you tried reading the R manual or searching the mail archives [2].
> > 
> > [1] http://cran.r-project.org/doc/manuals/R-admin.pdf
> > [2] http://cran.r-project.org/search.html
> > 
> > 
> > On Fri, 2004-10-22 at 17:58, songj at ucalgary.ca wrote:
> > > Hi, Everyone:
> > > 
> > > We are installing R package in our Sun Solaris 9, and falling in 
> trouble. We 
> > > downloaded all package from bioconductor, the R can not be installed in 
> Sun 
> > > Solaris 9. We greatly appreciated if you could give us any suggestions. 
> > > Thanks.
> > > 
> > > Yours
> > > 
> > > Song
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! http://www.R-project.org/posting-
> guide.html
> > > 
> > 
> > 
> 
>



From MSchwartz at MedAnalytics.com  Mon Nov  1 14:28:49 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon, 01 Nov 2004 07:28:49 -0600
Subject: [R] Survival Analysis Table
In-Reply-To: <1652ACFE-2BE9-11D9-B848-003065D5B8EC@tartarus.uwa.edu.au>
References: <1652ACFE-2BE9-11D9-B848-003065D5B8EC@tartarus.uwa.edu.au>
Message-ID: <1099315728.4836.3.camel@localhost.localdomain>

On Mon, 2004-11-01 at 03:33, Neil Leonard wrote:
> Hi,
> 
> I am trying to compute a table of a Kaplan Meier survival function.
> 
> I have created the Kaplan Meier using:
> 
> field.KM <-  
> survfit(Surv(project_no_1983$entryage3,project_no_1983$age_at_death,proj 
> ect_no_1983$death)~1)
> 
> and I want to find the survival rate and confidence intervals at  
> different times (eg, 1 year, 2 years, 5 years, etc).
> 
> Can anybody tell me how to do this?
> 
> 
> Thanks,
> Neil


See ?summary.survfit and note the 'times' argument.

HTH,

Marc Schwartz



From jfox at mcmaster.ca  Mon Nov  1 14:47:23 2004
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 1 Nov 2004 08:47:23 -0500
Subject: [R] Obtaining fitted model information
In-Reply-To: <418537D1.3000106@cirad.fr>
Message-ID: <20041101134722.CLHB4905.tomts13-srv.bellnexxia.net@JohnDesktop8300>

Dear Renaud,

Thanks -- I forgot about the error variance!

John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: Renaud Lancelot [mailto:renaud.lancelot at cirad.fr] 
> Sent: Sunday, October 31, 2004 2:07 PM
> To: John Fox
> Cc: Thomas.Volscho at uconn.edu; r-help at stat.math.ethz.ch
> Subject: Re: [R] Obtaining fitted model information
> 
> With models estimated with lm, the number of parameters is 
> obtained adding 1 to the rank of the fitted model (to account 
> for the residuals variance). The number of parameters is 
> found in logLik objects:
> 
>  > # example from ?lm
>  > ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)
>  > trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)
>  > group <- gl(2,10,20, labels=c("Ctl","Trt"))  > weight <- 
> c(ctl, trt)  > lm.D9 <- lm(weight ~ group)  >  > # rank of 
> the model  > lm.D9$rank [1] 2  >  > # loglik  > logLik(lm.D9) 
> `log Lik.' -20.08824 (df=3)  >  > # number of parameters in 
> the model  > attr(logLik(lm.D9), "df") [1] 3  >  > # AIC  > 
> AIC(lm.D9) [1] 46.17648  >  > c(- 2 * logLik(lm.D9) + 2 * 
> attr(logLik(lm.D9), "df")) [1] 46.17648  >  > # AICc = AIC + 
> 2 * k * (k + 1)/(n - k - 1)  >  > AICc_lm <- function(x){
> +   n <- length(resid(x))
> +   k <- attr(logLik(lm.D9), "df")
> +   AIC(x) + 2 * k * (k + 1) / (n - k - 1)
> +   }
>  >
>  > AICc_lm(lm.D9)
> [1] 47.67648
> 
> Best regards,
> 
> Renaud
> 
> 
> John Fox a ??crit :
> 
> > Dear Thomas,
> > 
> > To get the number of independent parameters in the lm 
> object mod, you 
> > can use mod$rank, sum(!is.na(coef(mod)), or -- if there are 
> no linear 
> > dependencies among the columns of the model matrix -- 
> length(coef(mod)).
> > 
> > I hope this helps,
> >  John
> > 
> > --------------------------------
> > John Fox
> > Department of Sociology
> > McMaster University
> > Hamilton, Ontario
> > Canada L8S 4M4
> > 905-525-9140x23604
> > http://socserv.mcmaster.ca/jfox
> > --------------------------------
> > 
> > 
> >>-----Original Message-----
> >>From: r-help-bounces at stat.math.ethz.ch 
> >>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Thomas W 
> >>Volscho
> >>Sent: Sunday, October 31, 2004 12:41 PM
> >>To: r-help at stat.math.ethz.ch
> >>Subject: [R] Obtaining fitted model information
> >>
> >>Dear list,
> >>I am brand new to R  and using Dalgaard's (2002) book Introductory 
> >>Statistics with R (thus, some of my terminology may be incorrect).
> >>
> >>I am fitting regression models and I want to use Hurvich and Tsai's 
> >>AICC statistic to examine my regression models.  This 
> penalty can be 
> >>expressed as: 2*npar * (n/(n-npar-1)).
> >>
> >>While you can obtain AIC, BIC, and logLik, I want to impose 
> the AICC 
> >>penalty instead.
> >>
> >>After fitting a model.  Is there a way of obtaining the "npar" and 
> >>then assigning it to a variable?
> >>
> >>Essentially, I want to then write a simple function to add the AICC 
> >>penalty to (-2*logLik).
> >>
> >>Thank you in advance for any help,
> >>Tom Volscho
> >>
> >>************************************        
> >>Thomas W. Volscho
> >>Graduate Student
> >>Dept. of Sociology U-2068
> >>University of Connecticut
> >>Storrs, CT 06269
> >>Phone: (860) 486-3882
> >>http://vm.uconn.edu/~twv00001
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide! 
> >>http://www.R-project.org/posting-guide.html
> > 
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> 
> 
> --
> Dr Renaud Lancelot, v??t??rinaire
> C/0 Ambassade de France - SCAC
> BP 834 Antananarivo 101 - Madagascar
> 
> e-mail: renaud.lancelot at cirad.fr
> tel.:   +261 32 40 165 53 (cell)
>          +261 20 22 665 36 ext. 225 (work)
>          +261 20 22 494 37 (home)



From ripley at stats.ox.ac.uk  Mon Nov  1 14:52:08 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 1 Nov 2004 13:52:08 +0000 (GMT)
Subject: [R] R
In-Reply-To: <1099315794.3322.4.camel@ndmpc126.ihs.ox.ac.uk>
Message-ID: <Pine.LNX.4.44.0411011347220.6404-100000@gannet.stats>

On Mon, 1 Nov 2004, Adaikalavan Ramasamy wrote:

> Rgui ? I think only Windows pre-compiled distribution has 'Rgui.exe'
> file but others in the list know better.

Correct.

> I presume SunRay is the Sun Solaris 9.0 you mentioned below. If so,
> please try with R pre-compiled for the appropriate platform or build
> from the source. See http://www.cran.r-project.org/. You system
> administrator should be able to help.

I don't think R is available pre-compiled for Solaris 9.

There is a console along the same lines as Rgui available for Unix 
versions of R -- see `GNOME' in the `R Installation and Administration 
Manual'.  (Note that it is broken under R-2.0.0 but does work under the 
R-patched versions available via CRAN.  It has at some time in the past 
been run on Solaris, but note what the manual says about its status.)


> Regards, Adai
> 
> 
> On Mon, 2004-11-01 at 12:28, songj at ucalgary.ca wrote:
> > Hi, Adaikalavan Ramasamy:
> > 
> > Thanks your message.
> > I installed R in our SunRay. It seems it is only commend line, could I run 
> > RGui in SunRay? Thanks. 
> > 
> > Have a good day.
> > 
> > Yours
> > 
> > Song
> > 
> > 
> > 
> > > You will need to get R working first before installing BioConductor
> > > packages.
> > > 
> > > Please provide a more useful output (like the error message) if you want
> > > useful help.
> > > 
> > > Have you tried reading the R manual or searching the mail archives [2].
> > > 
> > > [1] http://cran.r-project.org/doc/manuals/R-admin.pdf
> > > [2] http://cran.r-project.org/search.html
> > > 
> > > 
> > > On Fri, 2004-10-22 at 17:58, songj at ucalgary.ca wrote:
> > > > Hi, Everyone:
> > > > 
> > > > We are installing R package in our Sun Solaris 9, and falling in 
> > trouble. We 
> > > > downloaded all package from bioconductor, the R can not be installed in 
> > Sun 
> > > > Solaris 9. We greatly appreciated if you could give us any suggestions. 
> > > > Thanks.

That is nonsense, of course: R has frequently been installed on Solaris 9.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ivo_welch-rstat8783 at mailblocks.com  Mon Nov  1 14:55:31 2004
From: ivo_welch-rstat8783 at mailblocks.com (ivo_welch-rstat8783@mailblocks.com)
Date: Mon, 01 Nov 2004 05:55:31 -0800
Subject: [R] non-linear solve?
In-Reply-To: <200411011112.iA1B5E8T016751@hypatia.math.ethz.ch>
References: <200411011112.iA1B5E8T016751@hypatia.math.ethz.ch>
Message-ID: <200411011355.iA1DtB29028664@hypatia.math.ethz.ch>


hi:  could someone please point me to a function that allows me to 
solve general non-linear functions?

   > irr.in <- function(r, c1, c2, c3 ) {  return(c1+c2/(1+r) + 
c3/(1+r)^2); }
  > solve.nonlinear( irr.in, -100, 60, 70 );
  0.189

If someone has written an irr function, this would be helpful, 
too---though not difficult to write, either.  thanks for any pointers.

Regards,

/iaw
---
ivo welch



From andy_liaw at merck.com  Mon Nov  1 15:53:47 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 1 Nov 2004 09:53:47 -0500
Subject: [R] non-linear solve?
Message-ID: <3A822319EB35174CA3714066D590DCD50994E25E@usrymx25.merck.com>

See ?uniroot:

> uniroot(irr.in, c(-1, 1), c1=-100, c2=60, c3=70)
$root
[1] 0.1888196

$f.root
[1] -1.514979e-05

$iter
[1] 8

$estim.prec
[1] 6.103516e-05

Warning message: 
NA/Inf replaced by maximum positive value 

Andy

> From: ivo_welch-rstat8783 at mailblocks.com
> 
> hi:  could someone please point me to a function that allows me to 
> solve general non-linear functions?
> 
>    > irr.in <- function(r, c1, c2, c3 ) {  return(c1+c2/(1+r) + 
> c3/(1+r)^2); }
>   > solve.nonlinear( irr.in, -100, 60, 70 );
>   0.189
> 
> If someone has written an irr function, this would be helpful, 
> too---though not difficult to write, either.  thanks for any pointers.
> 
> Regards,
> 
> /iaw
> ---
> ivo welch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Timur.Elzhov at jinr.ru  Mon Nov  1 16:17:22 2004
From: Timur.Elzhov at jinr.ru (Timur Elzhov)
Date: Mon, 1 Nov 2004 18:17:22 +0300
Subject: [R] non-linear solve?
Message-ID: <20041101151722.GA7037@nf034.jinr.ru>

From: <ivo_welch-rstat8783 at mailblocks.com>
Date: Tue 02 Nov 2004 - 00:55:31 EST

> hi: could someone please point me to a function that allows me to
> solve general non-linear functions?
>
> > irr.in <- function(r, c1, c2, c3 ) { return(c1+c2/(1+r) +
> c3/(1+r)^2); }
> > solve.nonlinear( irr.in, -100, 60, 70 );
>   0.189
>
>   If someone has written an irr function, this would be helpful,
>   too---though not difficult to write, either. thanks for any
>   pointers.
>
>   Regards,
>
>   /iaw

optim, nlm - for general optimization;
nls, nls.lm from `minpack.lm' package - for solving the least-squares problem.


--
WBR,
Timur



From r.g.brown at cefas.co.uk  Mon Nov  1 16:17:37 2004
From: r.g.brown at cefas.co.uk (Robert Brown FM CEFAS)
Date: Mon, 1 Nov 2004 15:17:37 -0000
Subject: [R] deleting specified NA values
Message-ID: <3589BC4D64C84341AE0C258244F977A2B60B5B@expressa.corp.cefas.co.uk>

I have a data set of about 10000 records which was compiled from several smaller data sets using SPSS. During compilation 88 false records were accidentally introduced which comprise all NA values.  I want to delete these records but not other missing data.  The functions na.exclude and na.omit seem to remove all values of NA? How can I delete just the relevant NA's?  . i.e. I want to delete  all records in the data frame DATA where the field age contains NA values

Regards,

Robert Brown



From andy_liaw at merck.com  Mon Nov  1 16:31:39 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 1 Nov 2004 10:31:39 -0500
Subject: [R] deleting specified NA values
Message-ID: <3A822319EB35174CA3714066D590DCD50994E25F@usrymx25.merck.com>

This sort of things are most likely covered in `An Introduction to R':

newDATA <- DATA[!is.na(DATA$age),]

Andy

> From:  Robert Brown FM CEFAS
> 
> I have a data set of about 10000 records which was compiled 
> from several smaller data sets using SPSS. During compilation 
> 88 false records were accidentally introduced which comprise 
> all NA values.  I want to delete these records but not other 
> missing data.  The functions na.exclude and na.omit seem to 
> remove all values of NA? How can I delete just the relevant 
> NA's?  . i.e. I want to delete  all records in the data frame 
> DATA where the field age contains NA values
> 
> Regards,
> 
> Robert Brown
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From abunn at whrc.org  Mon Nov  1 16:42:49 2004
From: abunn at whrc.org (Andy Bunn)
Date: Mon, 1 Nov 2004 10:42:49 -0500
Subject: [R] deleting specified NA values
In-Reply-To: <3589BC4D64C84341AE0C258244F977A2B60B5B@expressa.corp.cefas.co.uk>
Message-ID: <NEBBIPHDAMMOKDKPOFFIIEGLCLAA.abunn@whrc.org>

I think you want something like so:

# make some data
foo.df <- data.frame(x = 1:100, y = runif(100), age = rnorm(100, 10, 1))
# stick some "real" NAs in all columns
foo.df[c(2,78,32,56),] <- NA
# make some "errant" NAs in the column age
foo.df$age[c(99, 26, 75, 3)] <- NA
# eg
foo.df[1:5,]
# remove the errant NAs with is.na
foo.df <- foo.df[!(is.na(foo.df$age) == T & is.na(foo.df$x) == F),]
foo.df[1:5,]



From bxc at steno.dk  Mon Nov  1 16:44:38 2004
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Mon, 1 Nov 2004 16:44:38 +0100
Subject: [R] deleting specified NA values
Message-ID: <0ABD88905D18E347874E0FB71C0B29E90237CF0C@exdkba022.novo.dk>

How about:

all.nas <- apply( old, 1, function(x) sum( is.na( x ) ) )
new <- old[all.nas < dim( old )[2], ]

----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc
----------------------



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Robert 
> Brown FM CEFAS
> Sent: Monday, November 01, 2004 4:18 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] deleting specified NA values
> 
> 
> I have a data set of about 10000 records which was compiled 
> from several smaller data sets using SPSS. During compilation 
> 88 false records were accidentally introduced which comprise 
> all NA values.  I want to delete these records but not other 
> missing data.  The functions na.exclude and na.omit seem to 
> remove all values of NA? How can I delete just the relevant 
> NA's?  . i.e. I want to delete  all records in the data frame 
> DATA where the field age contains NA values
> 
> Regards,
> 
> Robert Brown
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From Ted.Harding at nessie.mcc.ac.uk  Mon Nov  1 16:03:07 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 01 Nov 2004 16:03:07 +0100 (GMT)
Subject: [R] deleting specified NA values
In-Reply-To: <3589BC4D64C84341AE0C258244F977A2B60B5B@expressa.corp.cefas.co.uk>
Message-ID: <XFMail.041101160307.Ted.Harding@nessie.mcc.ac.uk>

On 01-Nov-04 Robert Brown FM CEFAS wrote:
> I have a data set of about 10000 records which was compiled from
> several smaller data sets using SPSS. During compilation 88 false
> records were accidentally introduced which comprise all NA values.  I
> want to delete these records but not other missing data.  The functions
> na.exclude and na.omit seem to remove all values of NA? How can I
> delete just the relevant NA's?  . i.e. I want to delete  all records in
> the data frame DATA where the field age contains NA values

Hi Robert,
It's not quite clear what your "NA" criterion for deletion really is.

If (as you state first) the false records "comprise all NA values",
this suggests that in such a record every field is "NA".

On the other hand you say you "want to delete  all records in
the data frame DATA where the field age contains NA values", so
it looks as though you can check for deletion on the field "age"
only.

Suppose your dataframe is called DF.

In the second case, which is simpler, you can simply do

  newDF <- DF[!is.na(DF$age),]

In the first case, it's fundamentally the same but you have to
run the check along every element in each row. So define a function

  notallna<-function(x){!all(is.na(x))}

and then

  newDF <- DF[apply(DF,1,notallna),]

This will leave in every record in which not all fields are"NA",
so will include records in which only some fields are "NA".

Hoping this helps,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 01-Nov-04                                       Time: 16:03:07
------------------------------ XFMail ------------------------------



From jes at luretanker.no  Mon Nov  1 17:38:28 2004
From: jes at luretanker.no (Jon Egil Strand)
Date: Mon, 1 Nov 2004 17:38:28 +0100 (CET)
Subject: [R] Compilation error on mgcv_1.1-7 on OS X (10.3)
Message-ID: <Pine.LNX.4.44.0411011722240.323-100000@ani.mywh.net>


Greetings

I run into a compilation error when updating to mgcv_1.1-7 in R 2.0.0 on 
OS X 10.3. Note that other pacakges have compiled nicely. 

Some details are given below, but in short it looks like it's seeking for 
	
	/usr/local/lib/powerpc-apple-darwin6.8/3.4.2/

which I don't have. But I do have 

	/usr/lib/gcc/darwin/3.3

i.e a lower version of GCC in a different directory. More details at end.

I am sorry if this is very easy to configure, but as long as I am unable 
to do it I will be very grateful for any help.

All the best

Jon Egil Strand
M.Sc

-----------------------------------
	Compilation details       
-----------------------------------

gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include  -I/usr/local/include   -fno-common  -g -O2 -c qp.c -o qp.o
gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include  -I/usr/local/include   -fno-common  -g -O2 -c tprs.c -o tprs.o
gcc -bundle -flat_namespace -undefined suppress -L/usr/local/lib -o 
  mgcv.so gcv.o magic.o mat.o matrix.o mgcv.o qp.o tprs.o  -framework 
  vecLib -L/usr/local/lib -L/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2 
  -L/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2/../../.. -lfrtbegin 
  -lg2c -lSystem -lcc_dynamic -framework R

ld: warning -L: directory name (/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2) does not exist
ld: warning -L: directory name (/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2/../../..) does not exist
ld: can't locate file for: -lfrtbegin

make: *** [mgcv.so] Error 1
ERROR: compilation failed for package 'mgcv' 
-- 

Jon Egil Strand
jes at luretanker.no
Phone: +47 45030081



From ggrothendieck at myway.com  Mon Nov  1 17:42:15 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 1 Nov 2004 16:42:15 +0000 (UTC)
Subject: [R] deleting specified NA values
References: <0ABD88905D18E347874E0FB71C0B29E90237CF0C@exdkba022.novo.dk>
Message-ID: <loom.20041101T173759-923@post.gmane.org>



BXC (Bendix Carstensen <bxc <at> steno.dk> writes:
: 
: > From: r-help-bounces <at> stat.math.ethz.ch 
: > [mailto:r-help-bounces <at> stat.math.ethz.ch] On Behalf Of Robert 
: > 
: > I have a data set of about 10000 records which was compiled 
: > from several smaller data sets using SPSS. During compilation 
: > 88 false records were accidentally introduced which comprise 
: > all NA values.  I want to delete these records but not other 
: > missing data.  The functions na.exclude and na.omit seem to 
: > remove all values of NA? How can I delete just the relevant 
: > NA's?  . i.e. I want to delete  all records in the data frame 
: > DATA where the field age contains NA values
: 
: How about:
: 
: all.nas <- apply( old, 1, function(x) sum( is.na( x ) ) )
: new <- old[all.nas < dim( old )[2], ]

A minor simplification of this might be:

	x[rowSums(is.na(x)) < ncol(x),]



From p.dalgaard at biostat.ku.dk  Mon Nov  1 18:09:56 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 01 Nov 2004 18:09:56 +0100
Subject: [R] deleting specified NA values
In-Reply-To: <loom.20041101T173759-923@post.gmane.org>
References: <0ABD88905D18E347874E0FB71C0B29E90237CF0C@exdkba022.novo.dk>
	<loom.20041101T173759-923@post.gmane.org>
Message-ID: <x2sm7tv6l7.fsf@biostat.ku.dk>

Gabor Grothendieck <ggrothendieck at myway.com> writes:

> BXC (Bendix Carstensen <bxc <at> steno.dk> writes:
> : 
> : > From: r-help-bounces <at> stat.math.ethz.ch 
> : > [mailto:r-help-bounces <at> stat.math.ethz.ch] On Behalf Of Robert 
> : > 
> : > I have a data set of about 10000 records which was compiled 
> : > from several smaller data sets using SPSS. During compilation 
> : > 88 false records were accidentally introduced which comprise 
> : > all NA values.  I want to delete these records but not other 
> : > missing data.  The functions na.exclude and na.omit seem to 
> : > remove all values of NA? How can I delete just the relevant 
> : > NA's?  . i.e. I want to delete  all records in the data frame 
> : > DATA where the field age contains NA values
> : 
> : How about:
> : 
> : all.nas <- apply( old, 1, function(x) sum( is.na( x ) ) )
> : new <- old[all.nas < dim( old )[2], ]
> 
> A minor simplification of this might be:
> 
> 	x[rowSums(is.na(x)) < ncol(x),]

If you want to get sneaky, there's also

 x[!!rowSums(!is.na(x)),]

although I think I'd prefer

 x[apply(!is.na(x),1,any),]

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Mon Nov  1 18:12:52 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 1 Nov 2004 17:12:52 +0000 (GMT)
Subject: [R] Compilation error on mgcv_1.1-7 on OS X (10.3)
In-Reply-To: <Pine.LNX.4.44.0411011722240.323-100000@ani.mywh.net>
Message-ID: <Pine.LNX.4.44.0411011705360.16885-100000@gannet.stats>

I expect your version of R was not compiled on your own system, although 
you don't say (please see the posting guide).  (I also presume this is a 
Macintosh but you could have said.)

The solutions are simple:

1) Compile R yourself -- the best choice.
2) Download a pre-compiled binary of that package for your platform.
3) Install the same tools as used to build R in the same places.
4) (Perhaps) edit R_HOME/etc/Makeconf and replace the paths in FLIBS.


On Mon, 1 Nov 2004, Jon Egil Strand wrote:

> I run into a compilation error when updating to mgcv_1.1-7 in R 2.0.0 on 
> OS X 10.3. Note that other pacakges have compiled nicely. 

Packages containing Fortran source code?

> Some details are given below, but in short it looks like it's seeking for 
> 	
> 	/usr/local/lib/powerpc-apple-darwin6.8/3.4.2/
> 
> which I don't have. But I do have 
> 
> 	/usr/lib/gcc/darwin/3.3
> 
> i.e a lower version of GCC in a different directory. More details at end.
> 
> I am sorry if this is very easy to configure, but as long as I am unable 
> to do it I will be very grateful for any help.
> 
> All the best
> 
> Jon Egil Strand
> M.Sc
> 
> -----------------------------------
> 	Compilation details       
> -----------------------------------
> 
> gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include  -I/usr/local/include   -fno-common  -g -O2 -c qp.c -o qp.o
> gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include  -I/usr/local/include   -fno-common  -g -O2 -c tprs.c -o tprs.o
> gcc -bundle -flat_namespace -undefined suppress -L/usr/local/lib -o 
>   mgcv.so gcv.o magic.o mat.o matrix.o mgcv.o qp.o tprs.o  -framework 
>   vecLib -L/usr/local/lib -L/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2 
>   -L/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2/../../.. -lfrtbegin 
>   -lg2c -lSystem -lcc_dynamic -framework R
> 
> ld: warning -L: directory name (/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2) does not exist
> ld: warning -L: directory name (/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2/../../..) does not exist
> ld: can't locate file for: -lfrtbegin
> 
> make: *** [mgcv.so] Error 1
> ERROR: compilation failed for package 'mgcv' 


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From roebuck at odin.mdacc.tmc.edu  Mon Nov  1 18:15:03 2004
From: roebuck at odin.mdacc.tmc.edu (Paul Roebuck)
Date: Mon, 1 Nov 2004 11:15:03 -0600 (CST)
Subject: [R] Compilation error on mgcv_1.1-7 on OS X (10.3)
In-Reply-To: <Pine.LNX.4.44.0411011722240.323-100000@ani.mywh.net>
References: <Pine.LNX.4.44.0411011722240.323-100000@ani.mywh.net>
Message-ID: <Pine.OSF.4.58.0411011059100.355459@odin.mdacc.tmc.edu>

On Mon, 1 Nov 2004, Jon Egil Strand wrote:

> I run into a compilation error when updating to mgcv_1.1-7 in R 2.0.0 on
> OS X 10.3. Note that other pacakges have compiled nicely.
>
> Some details are given below, but in short it looks like it's seeking for
>
> 	/usr/local/lib/powerpc-apple-darwin6.8/3.4.2/
>
> which I don't have. But I do have
>
> 	/usr/lib/gcc/darwin/3.3
>
> i.e a lower version of GCC in a different directory. More details at end.
>
> I am sorry if this is very easy to configure, but as long as I am unable
> to do it I will be very grateful for any help.
>
> -----------------------------------
> 	Compilation details
> -----------------------------------
>
> gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include  -I/usr/local/include   -fno-common  -g -O2 -c qp.c -o qp.o
> gcc -no-cpp-precomp -I/Library/Frameworks/R.framework/Resources/include  -I/usr/local/include   -fno-common  -g -O2 -c tprs.c -o tprs.o
> gcc -bundle -flat_namespace -undefined suppress -L/usr/local/lib -o
>   mgcv.so gcv.o magic.o mat.o matrix.o mgcv.o qp.o tprs.o  -framework
>   vecLib -L/usr/local/lib -L/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2
>   -L/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2/../../.. -lfrtbegin
>   -lg2c -lSystem -lcc_dynamic -framework R
>
> ld: warning -L: directory name (/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2) does not exist
> ld: warning -L: directory name (/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2/../../..) does not exist
> ld: can't locate file for: -lfrtbegin
>
> make: *** [mgcv.so] Error 1
> ERROR: compilation failed for package 'mgcv'
>

My guess would be that this package requires G77 in order
to link correctly as that library is for Fortran.

Install this and try it again.

<http://hpc.sf.net/g77v3.4-bin.tar.gz>

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From simon at stats.gla.ac.uk  Mon Nov  1 18:43:56 2004
From: simon at stats.gla.ac.uk (Simon Wood)
Date: Mon, 1 Nov 2004 17:43:56 +0000 (GMT)
Subject: [R] Compilation error on mgcv_1.1-7 on OS X (10.3)
In-Reply-To: <Pine.OSF.4.58.0411011059100.355459@odin.mdacc.tmc.edu>
References: <Pine.LNX.4.44.0411011722240.323-100000@ani.mywh.net>
	<Pine.OSF.4.58.0411011059100.355459@odin.mdacc.tmc.edu>
Message-ID: <Pine.LNX.4.58.0411011740030.26355@moon.stats.gla.ac.uk>

> > ld: warning -L: directory name (/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2) does not exist
> > ld: warning -L: directory name (/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2/../../..) does not exist
> > ld: can't locate file for: -lfrtbegin
> >
> > make: *** [mgcv.so] Error 1
> > ERROR: compilation failed for package 'mgcv'
> >
> 
> My guess would be that this package requires G77 in order
> to link correctly as that library is for Fortran.
- The source is C, but linpack and lapack routines are called 
directly (see mgcv/src/mat.c) and they're written in F77... 


> 
> Install this and try it again.
> 
> <http://hpc.sf.net/g77v3.4-bin.tar.gz>
> 
> ----------------------------------------------------------
> SIGSIG -- signature too long (core dumped)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From xchen at aligntech.com  Mon Nov  1 19:04:16 2004
From: xchen at aligntech.com (Xiaorong Chen)
Date: Mon, 1 Nov 2004 10:04:16 -0800
Subject: [R] Read in data from Microsoft SQL
Message-ID: <7F7C174CC6868B49B4BB1D0E4B6219F902EB886C@mail4.aligntech.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041101/963220fd/attachment.pl

From cyracules at yahoo.co.uk  Mon Nov  1 19:25:38 2004
From: cyracules at yahoo.co.uk (j lee)
Date: Mon, 1 Nov 2004 18:25:38 +0000 (GMT)
Subject: [R] Reading word by word in a dataset
Message-ID: <20041101182538.11274.qmail@web26301.mail.ukl.yahoo.com>

Hello All,

I'd like to read first words in lines into a new file.
If I have a data file the following, how can I get the
first words: apple, banana, strawberry?

i1-apple        10$   New_York
i2-banana       5$    London
i3-strawberry   7$    Japan

Is there any similar question already posted to the
list? I am a bit new to R, having a few months of
experience now.

Cheers,

John



From ahenningsen at email.uni-kiel.de  Mon Nov  1 19:37:08 2004
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Mon, 1 Nov 2004 19:37:08 +0100
Subject: [R] make apply() return a list
Message-ID: <200411011937.08360.ahenningsen@email.uni-kiel.de>

Hi,

I have a dataframe (say myData) and want to get a list (say myList) that 
contains a matrix for each row of the dataframe myData. These matrices are 
calculated based on the corresponding row of myData. Using a for()-loop to do 
this is very slow. Thus, I tried to use apply(). However, afaik apply() does 
only return a list if the matrices have different dimensions, while my 
matrices have all the same dimension. To get a list I could change the 
dimension of one matrix artificially and restore it after apply():

This a (very much) simplified example of what I did:
> myData <- data.frame( a = c( 1,2,3 ), b = c( 4,5,6 ) )
> myFunction <- function( values ) {
+    myMatrix <- matrix( values, 2, 2 )
+    if( all( values == myData[ 1, ] ) ) {
+       myMatrix <- cbind( myMatrix, rep( 0, 2 ) )
+    }
+    return( myMatrix )
+ }
> myList <- apply( myData, 1, myFunction )
> myList[[ 1 ]] <- myList[[ 1 ]][ 1:2, 1:2 ]
> myList
$"1"
     [,1] [,2]
[1,]    1    1
[2,]    4    4

$"2"
     [,1] [,2]
[1,]    2    2
[2,]    5    5

$"3"
     [,1] [,2]
[1,]    3    3
[2,]    6    6

This exactly does what I want and really speeds up the calculation, but I 
wonder if there is an easier way to make apply() return a list.

Thanks for your help,
Arne

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From king.812 at osu.edu  Mon Nov  1 19:42:40 2004
From: king.812 at osu.edu (WAYNE KING)
Date: Mon, 01 Nov 2004 13:42:40 -0500
Subject: [R] suse 9.2 question
Message-ID: <83695d8375e7.8375e783695d@osu.edu>

Hi all, I checked the archive and didn't see an 
anser to this. I updated to Suse 9.2 with kde 3.3 
and the installed version of R (from the suse 9.1) 
rpms came up with the following failed 
dependencies (after querying the rpm) 
 
error: Failed dependencies: 
        libg2c.so.0 is needed by 
R-base-patched1.9.0-1 
        libreadline.so.4 is needed by 
R-base-patched1.9.0-1 
 
My question is: 
1.) can I just compile from the tar ball, or will 
the same missing lib files be a problem. In other 
words, is it just a rpm problem. 
2.) If compiling from the source will not help, 
can I add these files somehow, or will suse 9.2 
rpms be kindly added soon by some very nice 
person? 
 
thank you, 
wayne



From andy_liaw at merck.com  Mon Nov  1 19:44:12 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 1 Nov 2004 13:44:12 -0500
Subject: [R] Reading word by word in a dataset
Message-ID: <3A822319EB35174CA3714066D590DCD50994E261@usrymx25.merck.com>

Using R-2.0.0 on WinXPPro, cut-and-pasting the data you have:

> read.table("clipboard", colClasses=c("character", "NULL", "NULL"))
             V1
1      i1-apple
2     i2-banana
3 i3-strawberry

HTH,
Andy

> From: j lee
> 
> Hello All,
> 
> I'd like to read first words in lines into a new file.
> If I have a data file the following, how can I get the
> first words: apple, banana, strawberry?
> 
> i1-apple        10$   New_York
> i2-banana       5$    London
> i3-strawberry   7$    Japan
> 
> Is there any similar question already posted to the
> list? I am a bit new to R, having a few months of
> experience now.
> 
> Cheers,
> 
> John
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From tplate at blackmesacapital.com  Mon Nov  1 19:55:20 2004
From: tplate at blackmesacapital.com (Tony Plate)
Date: Mon, 01 Nov 2004 11:55:20 -0700
Subject: [R] make apply() return a list
In-Reply-To: <200411011937.08360.ahenningsen@email.uni-kiel.de>
References: <200411011937.08360.ahenningsen@email.uni-kiel.de>
Message-ID: <6.1.0.6.2.20041101115011.0ba722d8@mailhost.blackmesacapital.com>

for()-loops aren't so bad.  Look inside the code of apply() and see what it 
uses!  The important thing is that you use vectorized functions to 
manipulate vectors.  It's often fine to use for-loops to manipulate the 
rows or columns of a matrix, but once you've extracted a row or a column, 
then use a vectorized function to manipulate that data.

In any case, one way to get apply() to return a list is to wrap the result 
from the subfunction inside a list, e.g.:

 > x <- apply(matrix(1:6,2), 1, function(x) list((c(mean=mean(x), sd=sd(x)))))
 > x
[[1]]
[[1]][[1]]
mean   sd
    3    2


[[2]]
[[2]][[1]]
mean   sd
    4    2


 > # to remove the extra level of listing here, do:
 > lapply(x, "[[", 1)
[[1]]
mean   sd
    3    2

[[2]]
mean   sd
    4    2

 >

At Monday 11:37 AM 11/1/2004, Arne Henningsen wrote:
>Hi,
>
>I have a dataframe (say myData) and want to get a list (say myList) that
>contains a matrix for each row of the dataframe myData. These matrices are
>calculated based on the corresponding row of myData. Using a for()-loop to do
>this is very slow. Thus, I tried to use apply(). However, afaik apply() does
>only return a list if the matrices have different dimensions, while my
>matrices have all the same dimension. To get a list I could change the
>dimension of one matrix artificially and restore it after apply():
>
>This a (very much) simplified example of what I did:
> > myData <- data.frame( a = c( 1,2,3 ), b = c( 4,5,6 ) )
> > myFunction <- function( values ) {
>+    myMatrix <- matrix( values, 2, 2 )
>+    if( all( values == myData[ 1, ] ) ) {
>+       myMatrix <- cbind( myMatrix, rep( 0, 2 ) )
>+    }
>+    return( myMatrix )
>+ }
> > myList <- apply( myData, 1, myFunction )
> > myList[[ 1 ]] <- myList[[ 1 ]][ 1:2, 1:2 ]
> > myList
>$"1"
>      [,1] [,2]
>[1,]    1    1
>[2,]    4    4
>
>$"2"
>      [,1] [,2]
>[1,]    2    2
>[2,]    5    5
>
>$"3"
>      [,1] [,2]
>[1,]    3    3
>[2,]    6    6
>
>This exactly does what I want and really speeds up the calculation, but I
>wonder if there is an easier way to make apply() return a list.
>
>Thanks for your help,
>Arne
>
>--
>Arne Henningsen
>Department of Agricultural Economics
>University of Kiel
>Olshausenstr. 40
>D-24098 Kiel (Germany)
>Tel: +49-431-880 4445
>Fax: +49-431-880 1397
>ahenningsen at agric-econ.uni-kiel.de
>http://www.uni-kiel.de/agrarpol/ahenningsen/
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From mahbub.latif at gmail.com  Mon Nov  1 19:56:07 2004
From: mahbub.latif at gmail.com (Mahbub Latif)
Date: Mon, 1 Nov 2004 19:56:07 +0100
Subject: [R] make apply() return a list
In-Reply-To: <200411011937.08360.ahenningsen@email.uni-kiel.de>
References: <200411011937.08360.ahenningsen@email.uni-kiel.de>
Message-ID: <e6f3169a04110110564b7db8ea@mail.gmail.com>

How about this... 

x = matrix(1:27, ncol=9, byrow=T)
nr= nrow(x)
lapply(1:nr, function(i) matrix(x[i,], nrow=3, byrow=T))

Mahbub.


On Mon, 1 Nov 2004 19:37:08 +0100, Arne Henningsen
<ahenningsen at email.uni-kiel.de> wrote:
> Hi,
> 
> I have a dataframe (say myData) and want to get a list (say myList) that
> contains a matrix for each row of the dataframe myData. These matrices are
> calculated based on the corresponding row of myData. Using a for()-loop to do
> this is very slow. Thus, I tried to use apply(). However, afaik apply() does
> only return a list if the matrices have different dimensions, while my
> matrices have all the same dimension. To get a list I could change the
> dimension of one matrix artificially and restore it after apply():
> 
> This a (very much) simplified example of what I did:
> > myData <- data.frame( a = c( 1,2,3 ), b = c( 4,5,6 ) )
> > myFunction <- function( values ) {
> +    myMatrix <- matrix( values, 2, 2 )
> +    if( all( values == myData[ 1, ] ) ) {
> +       myMatrix <- cbind( myMatrix, rep( 0, 2 ) )
> +    }
> +    return( myMatrix )
> + }
> > myList <- apply( myData, 1, myFunction )
> > myList[[ 1 ]] <- myList[[ 1 ]][ 1:2, 1:2 ]
> > myList
> $"1"
>      [,1] [,2]
> [1,]    1    1
> [2,]    4    4
> 
> $"2"
>      [,1] [,2]
> [1,]    2    2
> [2,]    5    5
> 
> $"3"
>      [,1] [,2]
> [1,]    3    3
> [2,]    6    6
> 
> This exactly does what I want and really speeds up the calculation, but I
> wonder if there is an easier way to make apply() return a list.
> 
> Thanks for your help,
> Arne
> 
> --
> Arne Henningsen
> Department of Agricultural Economics
> University of Kiel
> Olshausenstr. 40
> D-24098 Kiel (Germany)
> Tel: +49-431-880 4445
> Fax: +49-431-880 1397
> ahenningsen at agric-econ.uni-kiel.de
> http://www.uni-kiel.de/agrarpol/ahenningsen/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
A.H.M. Mahbub-ul Latif
PhD Student
Department of Medical Statistics 
University of Goettingen
Germany



From ligges at statistik.uni-dortmund.de  Mon Nov  1 19:57:56 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 01 Nov 2004 19:57:56 +0100
Subject: [R] suse 9.2 question
In-Reply-To: <83695d8375e7.8375e783695d@osu.edu>
References: <83695d8375e7.8375e783695d@osu.edu>
Message-ID: <41868734.5060801@statistik.uni-dortmund.de>

WAYNE KING wrote:

> Hi all, I checked the archive and didn't see an 
> anser to this. I updated to Suse 9.2 with kde 3.3 
> and the installed version of R (from the suse 9.1) 
> rpms came up with the following failed 
> dependencies (after querying the rpm) 
>  
> error: Failed dependencies: 
>         libg2c.so.0 is needed by 
> R-base-patched1.9.0-1 
>         libreadline.so.4 is needed by 
> R-base-patched1.9.0-1 
>  
> My question is: 
> 1.) can I just compile from the tar ball, 

Yes.

> or will the same missing lib files be a problem. 

No.

 > In other words, is it just a rpm problem.

Yes, in a way.

Uwe Ligges


> 2.) If compiling from the source will not help, 
> can I add these files somehow, or will suse 9.2 
> rpms be kindly added soon by some very nice 
> person? 
 >
> thank you, 
> wayne
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Mon Nov  1 19:59:43 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 1 Nov 2004 18:59:43 +0000 (UTC)
Subject: [R] make apply() return a list
References: <200411011937.08360.ahenningsen@email.uni-kiel.de>
Message-ID: <loom.20041101T195558-876@post.gmane.org>

Arne Henningsen <ahenningsen <at> email.uni-kiel.de> writes:

: 
: Hi,
: 
: I have a dataframe (say myData) and want to get a list (say myList) that 
: contains a matrix for each row of the dataframe myData. These matrices are 
: calculated based on the corresponding row of myData. Using a for()-loop to 
do 
: this is very slow. Thus, I tried to use apply(). However, afaik apply() does 
: only return a list if the matrices have different dimensions, while my 
: matrices have all the same dimension. To get a list I could change the 
: dimension of one matrix artificially and restore it after apply():
: 
: This a (very much) simplified example of what I did:
: > myData <- data.frame( a = c( 1,2,3 ), b = c( 4,5,6 ) )
: > myFunction <- function( values ) {
: +    myMatrix <- matrix( values, 2, 2 )
: +    if( all( values == myData[ 1, ] ) ) {
: +       myMatrix <- cbind( myMatrix, rep( 0, 2 ) )
: +    }
: +    return( myMatrix )
: + }
: > myList <- apply( myData, 1, myFunction )
: > myList[[ 1 ]] <- myList[[ 1 ]][ 1:2, 1:2 ]

Try lapplying over the columns of the transpose:

lapply(as.data.frame(t(myData)), matrix, 2, 2)



From ligges at statistik.uni-dortmund.de  Mon Nov  1 20:12:07 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 01 Nov 2004 20:12:07 +0100
Subject: [R] Reading word by word in a dataset
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E261@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E261@usrymx25.merck.com>
Message-ID: <41868A87.5020905@statistik.uni-dortmund.de>

Liaw, Andy wrote:

> Using R-2.0.0 on WinXPPro, cut-and-pasting the data you have:
> 
> 
>>read.table("clipboard", colClasses=c("character", "NULL", "NULL"))
> 
>              V1
> 1      i1-apple
> 2     i2-banana
> 3 i3-strawberry


... and if only the words after "-" are of interest, the statement can 
be followed by

  sapply(strsplit(...., "-"), "[", 2)


Uwe Ligges



> HTH,
> Andy
> 
> 
>>From: j lee
>>
>>Hello All,
>>
>>I'd like to read first words in lines into a new file.
>>If I have a data file the following, how can I get the
>>first words: apple, banana, strawberry?
>>
>>i1-apple        10$   New_York
>>i2-banana       5$    London
>>i3-strawberry   7$    Japan
>>
>>Is there any similar question already posted to the
>>list? I am a bit new to R, having a few months of
>>experience now.
>>
>>Cheers,
>>
>>John
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From york at zipcon.net  Mon Nov  1 20:14:34 2004
From: york at zipcon.net (Anne York)
Date: Mon, 1 Nov 2004 19:14:34 +0000 (UTC)
Subject: [R] ms access --> mysql --> R in Linux
Message-ID: <Pine.LNX.4.60.0411011901310.12532@sasquatch>

I am trying to use some ms access databases in R (version 
1.9.1 or 2.0 on a Debian system).  In searching the net for 
promising software to do this, I found mdbtools. Mdbtools 
claims the ability to convert schemas and tables in MS 
Access to MySQL and other databases.

http://mdbtools.sourceforge.net/

I'm wondering if anyone in the R community has tried using 
this software to use MS-Access databases in R with a Linux 
system. If so, Were you successful? What kind of problems 
did you encounter?

Thank-you for your attention.

Anne York



From abunn at whrc.org  Mon Nov  1 20:17:42 2004
From: abunn at whrc.org (Andy Bunn)
Date: Mon, 1 Nov 2004 14:17:42 -0500
Subject: [R] Reading word by word in a dataset
In-Reply-To: <20041101182538.11274.qmail@web26301.mail.ukl.yahoo.com>
Message-ID: <NEBBIPHDAMMOKDKPOFFIAEHACLAA.abunn@whrc.org>

Something like this should work:

foo <- read.table("text2read.txt", colClasses=c("character", "NULL",
"NULL"))$V1
foo <- gsub("i[0-9]-", "", foo)

HTH, Andy



From ligges at statistik.uni-dortmund.de  Mon Nov  1 20:24:37 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 01 Nov 2004 20:24:37 +0100
Subject: [R] Read in data from Microsoft SQL
In-Reply-To: <7F7C174CC6868B49B4BB1D0E4B6219F902EB886C@mail4.aligntech.com>
References: <7F7C174CC6868B49B4BB1D0E4B6219F902EB886C@mail4.aligntech.com>
Message-ID: <41868D75.5050004@statistik.uni-dortmund.de>

Xiaorong Chen wrote:

> Hi, 
>  
> I am trying to read data from Microsoft SQL. I tried to use odbcConnect
> (in RODBC) but failed. What should the "dsn" be? Does anyone know how to
> do it? I will appreciate it. 

The dsn is that one you have entered in the control panel before.
(there you have to specify data source name and corresponding host, user 
name, and name of the database).

Uwe Ligges



> Thank you!
>  
> Xiaorong
> ________________________________________________________________________
> The preceding e-mail message (including any attachments) contains
> information that may be confidential, be protected by the
> attorney-client or other applicable privileges, or constitute non-public
> information. It is intended to be read only by the individual or entity
> to whom it is addressed or by their designee. If you are not an intended
> recipient of this message, please notify the sender by replying to this
> message and then delete it from your system. You are on notice that
> further use, dissemination, distribution, or reproduction of this
> message is strictly prohibited and may be unlawful.
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From admartin at wustl.edu  Mon Nov  1 20:35:49 2004
From: admartin at wustl.edu (Andrew D. Martin)
Date: Mon, 1 Nov 2004 13:35:49 -0600
Subject: [R] Error Message: MCMCpack and coda
In-Reply-To: <2651.67.23.176.48.1099257304.squirrel@smail.binghamton.edu>
References: <2651.67.23.176.48.1099257304.squirrel@smail.binghamton.edu>
Message-ID: <3C0B1862-2C3D-11D9-950B-000D936BBFCC@wustl.edu>


Robin,

This is a problem with coda's mcmc summary method.  I suspect it is in 
the spectrum0() call therein, but I don't know for sure.

For your immediate purposes you could extract the posterior density 
sample as a matrix from the mcmc object and manually compute quantities 
of interest.

Best,
ADM

On Oct 31, 2004, at 3:15 PM, rbest1 at binghamton.edu wrote:

> Hello All,
>
> I'm trying to run a one-dimenional irt model using the packages MCMC 
> and
> coda on a rather large set of roll-call voting data with many missing
> observations.  Here's a sample of the code:
> Post10<-
> MCMCirt1d (Italy10, burnin = 1000, mcmc=50000, thin=100, verbose=TRUE,
> theta.constraints = list(V549=1, V443=-1))
>
> The MCMCirt1d command seems to work fine, but when I try to summarize 
> the
> output I get the following error message(s):
>
> summary(Post10)
> Error: NA/NaN/Inf in foreign function call (arg 1)
> In addition: Warning message:
> Step size truncated due to divergence
>
> My understanding is that this has something to do with the missing 
> data,
> though I don't know how to address this issue.  Any help would be 
> greatly
> appreciated.
>
> Thank you,
> Robin Best
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>

--
Andrew D. Martin, Ph.D.
Department of Political Science
Washington University
Campus Box 1063
One Brookings Drive
St. Louis, MO 63130
(314) 935-5863 (Office)
(314) 753-8377 (Cell)
(314) 935-5856 (Fax)

Office: Eliot Hall 326
Email: admartin at wustl.edu
WWW:   http://adm.wustl.edu



From ripley at stats.ox.ac.uk  Mon Nov  1 21:05:41 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 1 Nov 2004 20:05:41 +0000 (GMT)
Subject: [R] ms access --> mysql --> R in Linux
In-Reply-To: <Pine.LNX.4.60.0411011901310.12532@sasquatch>
Message-ID: <Pine.LNX.4.44.0411012002040.22503-100000@gannet.stats>

mdbtools claims to have an ODBC driver, so no conversion should be 
necessary, just use RODBC.

Both last time I tried and just now I could not get it to compile. The
problems were in its Gtk front end, but that appears not to be optional.

On Mon, 1 Nov 2004, Anne York wrote:

> I am trying to use some ms access databases in R (version 
> 1.9.1 or 2.0 on a Debian system).  In searching the net for 
> promising software to do this, I found mdbtools. Mdbtools 
> claims the ability to convert schemas and tables in MS 
> Access to MySQL and other databases.
> 
> http://mdbtools.sourceforge.net/
> 
> I'm wondering if anyone in the R community has tried using 
> this software to use MS-Access databases in R with a Linux 
> system. If so, Were you successful? What kind of problems 
> did you encounter?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Mon Nov  1 21:09:15 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 01 Nov 2004 21:09:15 +0100
Subject: [R] suse 9.2 question
In-Reply-To: <41868734.5060801@statistik.uni-dortmund.de>
References: <83695d8375e7.8375e783695d@osu.edu>
	<41868734.5060801@statistik.uni-dortmund.de>
Message-ID: <x2is8p2uxg.fsf@biostat.ku.dk>

Uwe Ligges <ligges at statistik.uni-dortmund.de> writes:

> WAYNE KING wrote:
> 
> > Hi all, I checked the archive and didn't see an anser to this. I
> > updated to Suse 9.2 with kde 3.3 and the installed version of R
> > (from the suse 9.1) rpms came up with the following failed
> > dependencies (after querying the rpm)  error: Failed dependencies:
> > libg2c.so.0 is needed by R-base-patched1.9.0-1
> >         libreadline.so.4 is needed by R-base-patched1.9.0-1
> >  My question is: 1.) can I just compile from the tar ball,
> 
> Yes.
> 
> > or will the same missing lib files be a problem.
> 
> No.

Well, yes, they will. Maybe not a compile problem, but otherwise. Not
sure about libg2c, but without libreadline you definitely won't have
command recall.
 
> > 2.) If compiling from the source will not help, can I add these
> > files somehow, or will suse 9.2 rpms be kindly added soon by some
> > very nice person?

I'd be surprised if the files are not on the SuSE CD somewhere. A bit
strange that they didn't get updated along with the rest of the system
though.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From mateusz at Loskot.net  Mon Nov  1 21:31:17 2004
From: mateusz at Loskot.net (Mateusz Loskot)
Date: Mon, 1 Nov 2004 21:31:17 +0100
Subject: [R] ms access --> mysql --> R in Linux
In-Reply-To: <Pine.LNX.4.60.0411011901310.12532@sasquatch>
References: <Pine.LNX.4.60.0411011901310.12532@sasquatch>
Message-ID: <20041101203117.GA29439@kawai.wl.sggw.waw.pl>

01.11 2004 r., on 19:14 Anne York wrote:
> I am trying to use some ms access databases in R (version 
> 1.9.1 or 2.0 on a Debian system).  In searching the net for 
> promising software to do this, I found mdbtools. Mdbtools 
> claims the ability to convert schemas and tables in MS 
> Access to MySQL and other databases.
> 
> http://mdbtools.sourceforge.net/
> 
> I'm wondering if anyone in the R community has tried using 
> this software to use MS-Access databases in R with a Linux 
> system. If so, Were you successful? What kind of problems 
> did you encounter?
> 

I tried mdbtool on my linux box and I can say that
it supports MDB format quite well but I would
like to say that there is no _good_ software
which provides GUI for basic operations on MDB files
like table management, querying etc.

So, if you want to be sure that your data won't
be damaged I'd not recommend you to use MDB files
on Linux. Personally, I use Linux and I'm its funboy, so
don't call me Microsoft's man ;-) but that is a fact.

I have many databases in MDB files and I converted them
to sqlite or mysql databases on Linux to be able to work with
them on Linux. 
For sqlite (which I strongly recommend) there is no problem, because
you can use ODBC drivers for sqlite and export data
using MS Access. sqlite wors very well on both Linux & Windows,
so you don't have to do any conversion.

MDB to Mysql is a little more complex.
I did it that way
1. install ODBC drivers for mysql on Windows machine
2. configure ODBC (DSN) connection to mysql database on Linux
(I assume you have mysql on linux configured and you have ethernet
network connecting both machines).
3. open Microsoft Access, open ODBC conn to mysql in access,
4. import access databas to that connected remotely from linux (mysql)

Another way is to use freeware software called DBTools for Windows
(search for dbtools by google, use first link).
This software provides export/import functionality
between mysql/postgresql/sqlite/access.
Somtimes I use it and it works well.

I believe it will help you.

Greets

-- 

Mateusz ??oskot
mateusz at loskot dot net



From wang at galton.uchicago.edu  Mon Nov  1 21:43:52 2004
From: wang at galton.uchicago.edu (Yong Wang)
Date: Mon, 1 Nov 2004 14:43:52 -0600 (CST)
Subject: [R] How to plot PDF which is in the form of orthogonal polynomial
In-Reply-To: <200410301009.i9UA8mfQ018958@hypatia.math.ethz.ch>
References: <200410301009.i9UA8mfQ018958@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.61.0411011442050.5905@aitken.uchicago.edu>

Dear all
using the orthogonal polymial on a set of data, I get an approximate 
density
which basically is in the form: exp(-polynomial),
  as you know, the parameters are the converged coeeficients.
obviously, It is hard, if not impossible, to use the inverse CDF method to 
get
a sample and then plot density. then how can I plot the approximated 
density in
order to have a graphical comparision with the real data's histogram.

any hint is appreciated

thanks



From spencer.graves at pdf.com  Mon Nov  1 21:59:16 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 01 Nov 2004 12:59:16 -0800
Subject: [R] Reading word by word in a dataset
In-Reply-To: <41868A87.5020905@statistik.uni-dortmund.de>
References: <3A822319EB35174CA3714066D590DCD50994E261@usrymx25.merck.com>
	<41868A87.5020905@statistik.uni-dortmund.de>
Message-ID: <4186A3A4.3090002@pdf.com>

      Uwe and Andy's solutions are great for many applications but won't 
work if not all rows have the same numbers of fields.  Consider for 
example the following modification of Lee's example: 

i1-apple        10$   New_York
i2-banana
i3-strawberry   7$    Japan

      If I copy this to "clipboard" and run Andy's code, I get the 
following: 

 > read.table("clipboard", colClasses=c("character", "NULL", "NULL"))
Error in scan(file = file, what = what, sep = sep, quote = quote, dec = 
dec,  :
    line 2 did not have 3 elements

      We can get around this using "scan", then splitting things apart 
similar to the way Uwe described: 

 > dat <-
+ scan("clipboard", character(0), sep="\n")
Read 3 items
 > dash <- regexpr("-", dat)
 > dat2 <- substring(dat, pmax(0, dash)+1)
 >
 > blank <- regexpr(" ", dat2)
 > if(any(blank<0))
+   blank[blank<0] <- nchar(dat2[blank<0])
 > substring(dat2, 1, blank)
[1] "apple "      "banana"      "strawberry "

      hope this helps.  spencer graves
    
Uwe Ligges wrote:

> Liaw, Andy wrote:
>
>> Using R-2.0.0 on WinXPPro, cut-and-pasting the data you have:
>>
>>
>>> read.table("clipboard", colClasses=c("character", "NULL", "NULL"))
>>
>>
>>              V1
>> 1      i1-apple
>> 2     i2-banana
>> 3 i3-strawberry
>
>
>
> ... and if only the words after "-" are of interest, the statement can 
> be followed by
>
>  sapply(strsplit(...., "-"), "[", 2)
>
>
> Uwe Ligges
>
>
>
>> HTH,
>> Andy
>>
>>
>>> From: j lee
>>>
>>> Hello All,
>>>
>>> I'd like to read first words in lines into a new file.
>>> If I have a data file the following, how can I get the
>>> first words: apple, banana, strawberry?
>>>
>>> i1-apple        10$   New_York
>>> i2-banana       5$    London
>>> i3-strawberry   7$    Japan
>>>
>>> Is there any similar question already posted to the
>>> list? I am a bit new to R, having a few months of
>>> experience now.
>>>
>>> Cheers,
>>>
>>> John
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>>
>>>
>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html


-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From MSchwartz at MedAnalytics.com  Mon Nov  1 22:01:20 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon, 01 Nov 2004 15:01:20 -0600
Subject: [R] ms access --> mysql --> R in Linux
In-Reply-To: <20041101203117.GA29439@kawai.wl.sggw.waw.pl>
References: <Pine.LNX.4.60.0411011901310.12532@sasquatch>
	<20041101203117.GA29439@kawai.wl.sggw.waw.pl>
Message-ID: <1099342880.22390.20.camel@localhost.localdomain>

On Mon, 2004-11-01 at 14:31, Mateusz Loskot wrote:
> 01.11 2004 r., on 19:14 Anne York wrote:
> > I am trying to use some ms access databases in R (version 
> > 1.9.1 or 2.0 on a Debian system).  In searching the net for 
> > promising software to do this, I found mdbtools. Mdbtools 
> > claims the ability to convert schemas and tables in MS 
> > Access to MySQL and other databases.
> > 
> > http://mdbtools.sourceforge.net/
> > 
> > I'm wondering if anyone in the R community has tried using 
> > this software to use MS-Access databases in R with a Linux 
> > system. If so, Were you successful? What kind of problems 
> > did you encounter?
> > 
> 
> I tried mdbtool on my linux box and I can say that
> it supports MDB format quite well but I would
> like to say that there is no _good_ software
> which provides GUI for basic operations on MDB files
> like table management, querying etc.

Just as an FYI, you might want to review what the OO.org folks are doing
by integrating the mdbtools functionality into OpenOffice:

http://dba.openoffice.org/drivers/mdb/index.html

A goal of the integration would be to provide a GUI-like interface to
Access tables, such as can be done under Windows using ODBC:

http://www.openoffice.org/FAQs/ms-access/ms-access.html

or what has been done with MySQL:

http://www.unixodbc.org/doc/OOoMySQL.pdf

The issue at this point (among many I suspect) is that the mdbtools
package is read-only to Access, though there are longer term plans to
enable write functionality. So for the time being at least, there is no
actual management ability to modify existing tables and queries.

Lastly, you can also get a feel for what the OO.org folks are doing with
their own internal applications in the area of DBMS:

http://dba.openoffice.org/miscellaneous/dba20.html

HTH,

Marc Schwartz



From Carlisle.Thacker at noaa.gov  Mon Nov  1 22:09:27 2004
From: Carlisle.Thacker at noaa.gov (Carlisle Thacker)
Date: Mon, 01 Nov 2004 16:09:27 -0500
Subject: [R] overlapping plots
Message-ID: <4186A607.5090601@noaa.gov>

How to combine three plots so that they partially overlap?  As the data 
on all three are near the left side and top, it would be nice to draw 
the first, shift axes down and to the right and draw the second, and 
shift again to draw the third.

I could shift the data for the 2nd and 3rd plots and construct their 
axes by hand, but it would be great if R has an easier way.



From andy_liaw at merck.com  Mon Nov  1 22:06:14 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 1 Nov 2004 16:06:14 -0500
Subject: [R] Reading word by word in a dataset
Message-ID: <3A822319EB35174CA3714066D590DCD50994E267@usrymx25.merck.com>

Don't give up on read.table() just yet:

> read.table("clipboard", colClasses=c("character", "NULL", "NULL"),
fill=TRUE)
             V1
1      i1-apple
2     i2-banana
3 i3-strawberry

Andy

> From: Spencer Graves
> 
>       Uwe and Andy's solutions are great for many 
> applications but won't 
> work if not all rows have the same numbers of fields.  Consider for 
> example the following modification of Lee's example: 
> 
> i1-apple        10$   New_York
> i2-banana
> i3-strawberry   7$    Japan
> 
>       If I copy this to "clipboard" and run Andy's code, I get the 
> following: 
> 
>  > read.table("clipboard", colClasses=c("character", "NULL", "NULL"))
> Error in scan(file = file, what = what, sep = sep, quote = 
> quote, dec = 
> dec,  :
>     line 2 did not have 3 elements
> 
>       We can get around this using "scan", then splitting 
> things apart 
> similar to the way Uwe described: 
> 
>  > dat <-
> + scan("clipboard", character(0), sep="\n")
> Read 3 items
>  > dash <- regexpr("-", dat)
>  > dat2 <- substring(dat, pmax(0, dash)+1)
>  >
>  > blank <- regexpr(" ", dat2)
>  > if(any(blank<0))
> +   blank[blank<0] <- nchar(dat2[blank<0])
>  > substring(dat2, 1, blank)
> [1] "apple "      "banana"      "strawberry "
> 
>       hope this helps.  spencer graves
>     
> Uwe Ligges wrote:
> 
> > Liaw, Andy wrote:
> >
> >> Using R-2.0.0 on WinXPPro, cut-and-pasting the data you have:
> >>
> >>
> >>> read.table("clipboard", colClasses=c("character", "NULL", "NULL"))
> >>
> >>
> >>              V1
> >> 1      i1-apple
> >> 2     i2-banana
> >> 3 i3-strawberry
> >
> >
> >
> > ... and if only the words after "-" are of interest, the 
> statement can 
> > be followed by
> >
> >  sapply(strsplit(...., "-"), "[", 2)
> >
> >
> > Uwe Ligges
> >
> >
> >
> >> HTH,
> >> Andy
> >>
> >>
> >>> From: j lee
> >>>
> >>> Hello All,
> >>>
> >>> I'd like to read first words in lines into a new file.
> >>> If I have a data file the following, how can I get the
> >>> first words: apple, banana, strawberry?
> >>>
> >>> i1-apple        10$   New_York
> >>> i2-banana       5$    London
> >>> i3-strawberry   7$    Japan
> >>>
> >>> Is there any similar question already posted to the
> >>> list? I am a bit new to R, having a few months of
> >>> experience now.
> >>>
> >>> Cheers,
> >>>
> >>> John
> >>>
> >>> ______________________________________________
> >>> R-help at stat.math.ethz.ch mailing list
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide! 
> >>> http://www.R-project.org/posting-guide.html
> >>>
> >>>
> >>
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide! 
> >> http://www.R-project.org/posting-guide.html
> >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> 
> 
> -- 
> Spencer Graves, PhD, Senior Development Engineer
> O:  (408)938-4420;  mobile:  (408)655-4567
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From tplate at acm.org  Mon Nov  1 22:15:36 2004
From: tplate at acm.org (Tony Plate)
Date: Mon, 01 Nov 2004 14:15:36 -0700
Subject: [R] Reading word by word in a dataset
In-Reply-To: <4186A3A4.3090002@pdf.com>
References: <3A822319EB35174CA3714066D590DCD50994E261@usrymx25.merck.com>
	<41868A87.5020905@statistik.uni-dortmund.de>
	<4186A3A4.3090002@pdf.com>
Message-ID: <6.1.0.6.2.20041101141221.0ba80d18@mailhost.blackmesacapital.com>

Trying to make it work when not all rows have the same numbers of fields 
seems like a good place to use the "flush" argument to scan() (to skip 
everything after the first field on the line):

With the following copied to the clipboard:

i1-apple        10$   New_York
i2-banana
i3-strawberry   7$    Japan

do:

 > scan("clipboard", "", flush=T)
Read 3 items
[1] "i1-apple"      "i2-banana"     "i3-strawberry"
 > sub("^[A-Za-z0-9]*-", "", scan("clipboard", "", flush=T))
Read 3 items
[1] "apple"      "banana"     "strawberry"
 >

-- Tony Plate

At Monday 01:59 PM 11/1/2004, Spencer Graves wrote:
>      Uwe and Andy's solutions are great for many applications but won't 
> work if not all rows have the same numbers of fields.  Consider for 
> example the following modification of Lee's example:
>i1-apple        10$   New_York
>i2-banana
>i3-strawberry   7$    Japan
>
>      If I copy this to "clipboard" and run Andy's code, I get the following:
> > read.table("clipboard", colClasses=c("character", "NULL", "NULL"))
>Error in scan(file = file, what = what, sep = sep, quote = quote, dec = 
>dec,  :
>    line 2 did not have 3 elements
>
>      We can get around this using "scan", then splitting things apart 
> similar to the way Uwe described:
> > dat <-
>+ scan("clipboard", character(0), sep="\n")
>Read 3 items
> > dash <- regexpr("-", dat)
> > dat2 <- substring(dat, pmax(0, dash)+1)
> >
> > blank <- regexpr(" ", dat2)
> > if(any(blank<0))
>+   blank[blank<0] <- nchar(dat2[blank<0])
> > substring(dat2, 1, blank)
>[1] "apple "      "banana"      "strawberry "
>
>      hope this helps.  spencer graves
>
>Uwe Ligges wrote:
>
>>Liaw, Andy wrote:
>>
>>>Using R-2.0.0 on WinXPPro, cut-and-pasting the data you have:
>>>
>>>
>>>>read.table("clipboard", colClasses=c("character", "NULL", "NULL"))
>>>
>>>
>>>              V1
>>>1      i1-apple
>>>2     i2-banana
>>>3 i3-strawberry
>>
>>
>>
>>... and if only the words after "-" are of interest, the statement can be 
>>followed by
>>
>>  sapply(strsplit(...., "-"), "[", 2)
>>
>>
>>Uwe Ligges
>>
>>
>>
>>>HTH,
>>>Andy
>>>
>>>
>>>>From: j lee
>>>>
>>>>Hello All,
>>>>
>>>>I'd like to read first words in lines into a new file.
>>>>If I have a data file the following, how can I get the
>>>>first words: apple, banana, strawberry?
>>>>
>>>>i1-apple        10$   New_York
>>>>i2-banana       5$    London
>>>>i3-strawberry   7$    Japan
>>>>
>>>>Is there any similar question already posted to the
>>>>list? I am a bit new to R, having a few months of
>>>>experience now.
>>>>
>>>>Cheers,
>>>>
>>>>John
>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide! 
>>>>http://www.R-project.org/posting-guide.html
>>>>
>>>
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! 
>>>http://www.R-project.org/posting-guide.html
>>
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>--
>Spencer Graves, PhD, Senior Development Engineer
>O:  (408)938-4420;  mobile:  (408)655-4567
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Mon Nov  1 22:23:33 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 1 Nov 2004 16:23:33 -0500
Subject: [R] overlapping plots
Message-ID: <3A822319EB35174CA3714066D590DCD50994E268@usrymx25.merck.com>

I don't really understand what you want to do, but seems like with the
`grid' package, such things are quite possible.

Andy

> From: Carlisle Thacker
> 
> How to combine three plots so that they partially overlap?  
> As the data 
> on all three are near the left side and top, it would be nice to draw 
> the first, shift axes down and to the right and draw the second, and 
> shift again to draw the third.
> 
> I could shift the data for the 2nd and 3rd plots and construct their 
> axes by hand, but it would be great if R has an easier way.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From br44114 at yahoo.com  Mon Nov  1 22:30:30 2004
From: br44114 at yahoo.com (bogdan romocea)
Date: Mon, 1 Nov 2004 13:30:30 -0800 (PST)
Subject: [R] plot time series / dates (basic)
Message-ID: <20041101213030.247.qmail@web50305.mail.yahoo.com>

Dear R users,

I'm having a hard time with some very simple things. I have a time
series where the dates are in the format 7-Oct-04. I imported the
file with read.csv so the date column is a factor. The series is
rather long and I want to plot it piece by piece. The function below
works fine, except that the labels for date are meaningless (ie
9.47e+08 or 1098000000 - apparently the number of seconds since
whatever). I don't want to convert the data frame to a ts object
because there are missing days and I don't want any interpolation.

1. How do I replace the date labels with something like 'Mar04',
instead of 9.47e+08 / 1098000000?

2. In the PDF file, the space between the two graphs printed pair by
pair is fairly large. Can I remove/reduce the area that seems
reserved for Title and X label so that, on a page, the space between
the graph at the top and the one at the bottom is minimized?

3. Given the function below, I haven't discovered a way to have
"vara" appear as the Title or Y label in graphs.
main=as.character(vara) lists all the values of vara (which is a
column from the data frame d). So, how can I use the name of a vector
as title or label in a plot?

Thank you,
b.


d <- ('data.csv', header = T, sep = ",", quote="", dec=".", 
	fill = T, skip=0)
attach(d)
#function to plot a long time series piece by piece
pl <- function(vara, varb, points)
	{
	date <- as.POSIXct(strptime(as.character(Date), "%d-%b-%y"), tz =
"GMT")
	pr1 <- vector(mode="numeric")
	pr2 <- vector(mode="numeric")
	dat <- vector()
	for (j in 1:(round(length(Vol)/points)+1)) #number of plots
		{
		for (i in ((j-1)*points+1):(j*points)) 
			{
			pr1[i-points*(j-1)] <- vara[i]
			pr2[i-points*(j-1)] <- varb[i]
			dat[i-points*(j-1)] <- date[i]
			}
		par(mfrow=c(2,1)) 
		plot(dat, pr1, type="b")
		plot(dat, pr2, type="b")
		}
	}

pdf("Rplots.pdf")
pl(Vol, atr, 50)
dev.off()





		
__________________________________ 


From david.whiting at ncl.ac.uk  Mon Nov  1 22:35:57 2004
From: david.whiting at ncl.ac.uk (David Whiting)
Date: 01 Nov 2004 21:35:57 +0000
Subject: [R] ms access --> mysql --> R in Linux
In-Reply-To: <Pine.LNX.4.60.0411011901310.12532@sasquatch>
References: <Pine.LNX.4.60.0411011901310.12532@sasquatch>
Message-ID: <m2vfcp9rr6.fsf@ganymede.ammp.or.tz>

Anne York <york at zipcon.net> writes:

> I am trying to use some ms access databases in R (version 1.9.1 or 2.0
> on a Debian system).  In searching the net for promising software to
> do this, I found mdbtools. Mdbtools claims the ability to convert
> schemas and tables in MS Access to MySQL and other databases.
> 
> http://mdbtools.sourceforge.net/
> 
> I'm wondering if anyone in the R community has tried using this
> software to use MS-Access databases in R with a Linux system. If so,
> Were you successful? What kind of problems did you encounter?

I have used it several times. The ODBC driver is not really (at all?)
working but the command line tools seem to work well enough.  I
modified mdb-export and created a script that worked like mysqldump
(to produce CREATE TABLE and INSERT statements) so that I was able to
get the tables into MySQL to use with R.  The latest version is quite
a bit more recent than the one I have been using and I believe it has
more features. 

Here is something that I have just tried and seems to work and avoids
the need to take your data into another database:

x <- read.table(pipe('mdb-export -d "\t" databasename.mdb tableName'), sep="\t", header=TRUE)

mdb-export exports the contents of a table from an Access database
(fields separated with commas by default). The -d option specifies the
delimiter (I prefer to use a tab).  This seems to work well on my
relatively small test database. I guess it would not take much work to
write a little set of functions to get the table names (using
mdb-tables) and do some other useful things. Not as good as having a
working ODBC driver, but quite nice all the same.

I'll have spend a little more time playing with this...

Dave


-- 
David Whiting
University of Newcastle upon Tyne, UK.



From spencer.graves at pdf.com  Mon Nov  1 22:35:25 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 01 Nov 2004 13:35:25 -0800
Subject: [R] How to plot PDF which is in the form of orthogonal polynomial
In-Reply-To: <Pine.LNX.4.61.0411011442050.5905@aitken.uchicago.edu>
References: <200410301009.i9UA8mfQ018958@hypatia.math.ethz.ch>
	<Pine.LNX.4.61.0411011442050.5905@aitken.uchicago.edu>
Message-ID: <4186AC1D.4040004@pdf.com>

      Have you considered "cumsum(exp(-polynomial))"? 

      Consider for example the following: 

library(polynom)
b. <- polynomial(1:5)

x <- seq(-2, 2, length=101)
b.p <- predict(b., x)

plot(x, b.p)
plot(x, exp(-b.p))

cdf <- cumsum(exp(-b.p))
plot(cdf/cdf[101])
  
      hope this helps.  spencer graves

Yong Wang wrote:

> Dear all
> using the orthogonal polymial on a set of data, I get an approximate 
> density
> which basically is in the form: exp(-polynomial),
>  as you know, the parameters are the converged coeeficients.
> obviously, It is hard, if not impossible, to use the inverse CDF 
> method to get
> a sample and then plot density. then how can I plot the approximated 
> density in
> order to have a graphical comparision with the real data's histogram.
>
> any hint is appreciated
>
> thanks
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html


-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From ripley at stats.ox.ac.uk  Mon Nov  1 22:37:46 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 1 Nov 2004 21:37:46 +0000 (GMT)
Subject: [R] suse 9.2 question
In-Reply-To: <x2is8p2uxg.fsf@biostat.ku.dk>
Message-ID: <Pine.LNX.4.44.0411012132540.22718-100000@gannet.stats>

On 1 Nov 2004, Peter Dalgaard wrote:

> Uwe Ligges <ligges at statistik.uni-dortmund.de> writes:
> 
> > WAYNE KING wrote:
> > 
> > > Hi all, I checked the archive and didn't see an anser to this. I
> > > updated to Suse 9.2 with kde 3.3 and the installed version of R
> > > (from the suse 9.1) rpms came up with the following failed
> > > dependencies (after querying the rpm)  error: Failed dependencies:
> > > libg2c.so.0 is needed by R-base-patched1.9.0-1
> > >         libreadline.so.4 is needed by R-base-patched1.9.0-1
> > >  My question is: 1.) can I just compile from the tar ball,
> > 
> > Yes.
> > 
> > > or will the same missing lib files be a problem.
> > 
> > No.
> 
> Well, yes, they will. Maybe not a compile problem, but otherwise. Not
> sure about libg2c, but without libreadline you definitely won't have
> command recall.

But we are on libreadline.so.5 nowadays, so I suspect a very recent system 
like SuSE 9.2 has that and not libreadline.so.4.

OTOH, the absence of libg2c.so.0 suggests that g77 and its support files 
are not installed, and that will cause compilation problems.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From david.whiting at ncl.ac.uk  Mon Nov  1 22:44:30 2004
From: david.whiting at ncl.ac.uk (David Whiting)
Date: 01 Nov 2004 21:44:30 +0000
Subject: [R] ms access --> mysql --> R in Linux
In-Reply-To: <1099342880.22390.20.camel@localhost.localdomain>
References: <Pine.LNX.4.60.0411011901310.12532@sasquatch>
	<20041101203117.GA29439@kawai.wl.sggw.waw.pl>
	<1099342880.22390.20.camel@localhost.localdomain>
Message-ID: <m2r7nd9rcx.fsf@ganymede.ammp.or.tz>

Marc Schwartz <MSchwartz at medanalytics.com> writes:

[...]

> 
> The issue at this point (among many I suspect) is that the mdbtools
> package is read-only to Access, though there are longer term plans to
> enable write functionality. So for the time being at least, there is no
> actual management ability to modify existing tables and queries.
> 

I've just checked some recent mailing list posts and it seems that
write/update support for Jet3 (access 97) databases is underway. At
least parts of it are described as "primitive", but there is hope...


-- 
David Whiting
University of Newcastle upon Tyne, UK.



From Jeremy.Primer at morganstanley.com  Mon Nov  1 22:47:53 2004
From: Jeremy.Primer at morganstanley.com (Primer, Jeremy (FID))
Date: Mon, 1 Nov 2004 16:47:53 -0500
Subject: [R] case-insensitive ZIP
Message-ID: <46E679AFC0427C4B90CF3342E02A538E04985E@NYWEXMB31.msad.ms.com>

A development note:

In the function "install.packages", it would be helpful to those of us
who have atypical installations and install manually from ZIP files to
have

	pkgnames <- sub("\\.zip$", "", pkgnames)

replaced with

	pkgnames <- sub("\\.zip$", "", pkgnames, ignore.case = TRUE)

because the contributed zipfiles are ZIPfiles. The routine did not work
for me out of the box.


Jeremy Primer 
--------------------------------------------------------
 
This is not an offer (or solicitation of an offer) to buy/sell the securities/instruments mentioned or an official confirmation.  Morgan Stanley may deal as principal in or own or act as market maker for securities/instruments mentioned or may advise the issuers.  This is not research and is not from MS Research but it may refer to a research analyst/research report.  Unless indicated, these views are the author's and may differ from those of Morgan Stanley research or others in the Firm.  We do not represent this is accurate or complete and we may not update this.  Past performance is not indicative of future returns.  For additional information, research reports and important disclosures, contact me or see https://secure.ms.com/servlet/cls.  You should not use e-mail to request, authorize or effect the purchase or sale of any security or instrument, to send transfer instructions, or to effect any other transactions.  We cannot guarantee that any such requests received via e-mail will be processed in a timely manner.  This communication is solely for the addressee(s) and may contain confidential information.  We do not waive confidentiality by mistransmission.  Contact me if you do not wish to receive these communications.  In the UK, this communication is directed in the UK to those persons who are market counterparties or intermediate customers (as defined in the UK Financial Services Authority's rules).



From ripley at stats.ox.ac.uk  Mon Nov  1 22:58:10 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 1 Nov 2004 21:58:10 +0000 (GMT)
Subject: [R] plot time series / dates (basic)
In-Reply-To: <20041101213030.247.qmail@web50305.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0411012147560.22833-100000@gannet.stats>

On Mon, 1 Nov 2004, bogdan romocea wrote:

> Dear R users,
> 
> I'm having a hard time with some very simple things. I have a time
> series where the dates are in the format 7-Oct-04. 

So why use as.POSIXct for a date, rather than as.Date?

> I imported the
> file with read.csv so the date column is a factor. The series is
> rather long and I want to plot it piece by piece. The function below
> works fine, except that the labels for date are meaningless (ie
> 9.47e+08 or 1098000000 - apparently the number of seconds since
> whatever). I don't want to convert the data frame to a ts object
> because there are missing days and I don't want any interpolation.
> 
> 1. How do I replace the date labels with something like 'Mar04',
> instead of 9.47e+08 / 1098000000?

Just don't convert them to that format.  You set up

>       dat <- vector()

which is not a dates object.  If you use standard R indexing, it will
work. If you throw the class away, it will not.  Try

       dat <- date[(j-1)*points+1):(j*points)]

etc (no for loop required).

If you want a different format, see ?axis.Date

> 2. In the PDF file, the space between the two graphs printed pair by
> pair is fairly large. Can I remove/reduce the area that seems
> reserved for Title and X label so that, on a page, the space between
> the graph at the top and the one at the bottom is minimized?

There's a whole chapter on this in `An Introduction to R': have you read 
it?

> 3. Given the function below, I haven't discovered a way to have
> "vara" appear as the Title or Y label in graphs.
> main=as.character(vara) lists all the values of vara (which is a
> column from the data frame d). So, how can I use the name of a vector
> as title or label in a plot?

That's almost an FAQ.  Use deparse(substitute(vara))

> d <- ('data.csv', header = T, sep = ",", quote="", dec=".", 
> 	fill = T, skip=0)
> attach(d)
> #function to plot a long time series piece by piece
> pl <- function(vara, varb, points)
> 	{
> 	date <- as.POSIXct(strptime(as.character(Date), "%d-%b-%y"), tz =
> "GMT")
> 	pr1 <- vector(mode="numeric")
> 	pr2 <- vector(mode="numeric")
> 	dat <- vector()
> 	for (j in 1:(round(length(Vol)/points)+1)) #number of plots
> 		{
> 		for (i in ((j-1)*points+1):(j*points)) 
> 			{
> 			pr1[i-points*(j-1)] <- vara[i]
> 			pr2[i-points*(j-1)] <- varb[i]
> 			dat[i-points*(j-1)] <- date[i]
> 			}
> 		par(mfrow=c(2,1)) 
> 		plot(dat, pr1, type="b")
> 		plot(dat, pr2, type="b")
> 		}
> 	}
> 
> pdf("Rplots.pdf")
> pl(Vol, atr, 50)
> dev.off()


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From david.whiting at ncl.ac.uk  Mon Nov  1 23:11:08 2004
From: david.whiting at ncl.ac.uk (David Whiting)
Date: 01 Nov 2004 22:11:08 +0000
Subject: [R] ms access --> mysql --> R in Linux
In-Reply-To: <m2vfcp9rr6.fsf@ganymede.ammp.or.tz>
References: <Pine.LNX.4.60.0411011901310.12532@sasquatch>
	<m2vfcp9rr6.fsf@ganymede.ammp.or.tz>
Message-ID: <m2mzy19q4j.fsf@ganymede.ammp.or.tz>

David Whiting <david.whiting at ncl.ac.uk> writes:

> I'll have spend a little more time playing with this...

Well, here's a simple start.  There's more checking that needs to be
done, but this makes the process a little more straight forward:


mdbTables <- function(dbname) {
  system(paste("mdb-tables -d '\t' -S", dbname), intern=TRUE)
}


mdbExport <- function(dbname,tableName) {
  tableName <- dQuote(tableName)
  read.table(pipe(paste("mdb-export -d '\t' ", dbname,  tableName)), sep="\t", header=TRUE)
}


With these functions you can now do:

mytabs <- mdbTables("myAccessDB.mdb")

to get the list of tables in the DB. The first 5 seem to be
information about the database and the tables seem to start from the
6th element of the vector.  If the table you want is the one named in
the 9th element of the vector:

mytab <- mdbTables("myAccessDB.mdb")[9]

and you want that table in R:

x <- mdbExport("myAccessDB.mdb", mytab)


At the moment you have to specify the extension (.mdb) otherwise
mdb-export cannot find the file.  

-- 
David Whiting
Dar es Salaam, Tanzania



From partha_bagchi at hgsi.com  Mon Nov  1 22:52:31 2004
From: partha_bagchi at hgsi.com (partha_bagchi@hgsi.com)
Date: Mon, 1 Nov 2004 16:52:31 -0500
Subject: [R] overlapping plots
Message-ID: <OFF0AAC724.A760E9FB-ON85256F3F.0077F64B-85256F3F.00782ABC@hgsi.com>

Use a judicious combination of par(mai) and par(new = TRUE). For example:

par(mai = c(3,1,1,1))
x <- runif(50)
plot(x, xlim = c(1, 500), ylim = c( -5, 1), bty = "l")

par(mai = c(2,1.5,2,1))
par(new = TRUE)
x <- runif(50)
plot(x, xlim = c(1, 500), ylim = c( -5, 1), bty = "l")

par(mai = c(1,2,3,1))
par(new = TRUE)
x <- runif(50)
plot(x, xlim = c(1, 500), ylim = c( -5, 1), bty = "l")

Does that help?

Partha






"Carlisle Thacker" <Carlisle.Thacker at noaa.gov>
Sent by: r-help-bounces at stat.math.ethz.ch
11/01/2004 04:09 PM

 
        To:     r-help at stat.math.ethz.ch
        cc: 
        Subject:        [R] overlapping plots


How to combine three plots so that they partially overlap?  As the data
on all three are near the left side and top, it would be nice to draw
the first, shift axes down and to the right and draw the second, and
shift again to draw the third.

I could shift the data for the 2nd and 3rd plots and construct their
axes by hand, but it would be great if R has an easier way.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Mon Nov  1 23:13:33 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 01 Nov 2004 23:13:33 +0100
Subject: [R] suse 9.2 question
In-Reply-To: <Pine.LNX.4.44.0411012132540.22718-100000@gannet.stats>
References: <Pine.LNX.4.44.0411012132540.22718-100000@gannet.stats>
Message-ID: <x2654p2p6a.fsf@biostat.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> > Well, yes, they will. Maybe not a compile problem, but otherwise. Not
> > sure about libg2c, but without libreadline you definitely won't have
> > command recall.
> 
> But we are on libreadline.so.5 nowadays, so I suspect a very recent system 
> like SuSE 9.2 has that and not libreadline.so.4.

Seems so since Wayne said (offline) that a compile from scratch worked
fine. Most likely there's a readline4-compat (or so) RPM for code that
has been linked with libreadline.so.4.
 
> OTOH, the absence of libg2c.so.0 suggests that g77 and its support files 
> are not installed, and that will cause compilation problems.

Hmm... I wonder what it did get linked to. Wayne? 

ldd `R RHOME`/lib/*

should tell you.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From spencer.graves at pdf.com  Mon Nov  1 23:53:18 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 01 Nov 2004 14:53:18 -0800
Subject: [R] Reading word by word in a dataset
In-Reply-To: <6.1.0.6.2.20041101141221.0ba80d18@mailhost.blackmesacapital.com>
References: <3A822319EB35174CA3714066D590DCD50994E261@usrymx25.merck.com>	<41868A87.5020905@statistik.uni-dortmund.de>	<4186A3A4.3090002@pdf.com>
	<6.1.0.6.2.20041101141221.0ba80d18@mailhost.blackmesacapital.com>
Message-ID: <4186BE5E.8000606@pdf.com>

Dear Andy & Tony: 

      That's great.  Unfortunately, I still spend most of my life in the 
S-Plus world, and read.table in S-Plus 6.2 does not have the "fill" 
argument.  However, Tony's solution (and my ugly hack) work in both 
S-Plus 6.2 and R 2.0.0. 

      Thanks again. 
      Spencer Graves

Tony Plate wrote:

> Trying to make it work when not all rows have the same numbers of 
> fields seems like a good place to use the "flush" argument to scan() 
> (to skip everything after the first field on the line):
>
> With the following copied to the clipboard:
>
> i1-apple        10$   New_York
> i2-banana
> i3-strawberry   7$    Japan
>
> do:
>
> > scan("clipboard", "", flush=T)
> Read 3 items
> [1] "i1-apple"      "i2-banana"     "i3-strawberry"
> > sub("^[A-Za-z0-9]*-", "", scan("clipboard", "", flush=T))
> Read 3 items
> [1] "apple"      "banana"     "strawberry"
> >
>
> -- Tony Plate
>
> At Monday 01:59 PM 11/1/2004, Spencer Graves wrote:
>
>>      Uwe and Andy's solutions are great for many applications but 
>> won't work if not all rows have the same numbers of fields.  Consider 
>> for example the following modification of Lee's example:
>> i1-apple        10$   New_York
>> i2-banana
>> i3-strawberry   7$    Japan
>>
>>      If I copy this to "clipboard" and run Andy's code, I get the 
>> following:
>> > read.table("clipboard", colClasses=c("character", "NULL", "NULL"))
>> Error in scan(file = file, what = what, sep = sep, quote = quote, dec 
>> = dec,  :
>>    line 2 did not have 3 elements
>>
>>      We can get around this using "scan", then splitting things apart 
>> similar to the way Uwe described:
>> > dat <-
>> + scan("clipboard", character(0), sep="\n")
>> Read 3 items
>> > dash <- regexpr("-", dat)
>> > dat2 <- substring(dat, pmax(0, dash)+1)
>> >
>> > blank <- regexpr(" ", dat2)
>> > if(any(blank<0))
>> +   blank[blank<0] <- nchar(dat2[blank<0])
>> > substring(dat2, 1, blank)
>> [1] "apple "      "banana"      "strawberry "
>>
>>      hope this helps.  spencer graves
>>
>> Uwe Ligges wrote:
>>
>>> Liaw, Andy wrote:
>>>
>>>> Using R-2.0.0 on WinXPPro, cut-and-pasting the data you have:
>>>>
>>>>
>>>>> read.table("clipboard", colClasses=c("character", "NULL", "NULL"))
>>>>
>>>>
>>>>
>>>>              V1
>>>> 1      i1-apple
>>>> 2     i2-banana
>>>> 3 i3-strawberry
>>>
>>>
>>>
>>>
>>> ... and if only the words after "-" are of interest, the statement 
>>> can be followed by
>>>
>>>  sapply(strsplit(...., "-"), "[", 2)
>>>
>>>
>>> Uwe Ligges
>>>
>>>
>>>
>>>> HTH,
>>>> Andy
>>>>
>>>>
>>>>> From: j lee
>>>>>
>>>>> Hello All,
>>>>>
>>>>> I'd like to read first words in lines into a new file.
>>>>> If I have a data file the following, how can I get the
>>>>> first words: apple, banana, strawberry?
>>>>>
>>>>> i1-apple        10$   New_York
>>>>> i2-banana       5$    London
>>>>> i3-strawberry   7$    Japan
>>>>>
>>>>> Is there any similar question already posted to the
>>>>> list? I am a bit new to R, having a few months of
>>>>> experience now.
>>>>>
>>>>> Cheers,
>>>>>
>>>>> John
>>>>>
>>>>> ______________________________________________
>>>>> R-help at stat.math.ethz.ch mailing list
>>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>>> PLEASE do read the posting guide! 
>>>>> http://www.R-project.org/posting-guide.html
>>>>>
>>>>
>>>>
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide! 
>>>> http://www.R-project.org/posting-guide.html
>>>
>>>
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>
>>
>>
>> -- 
>> Spencer Graves, PhD, Senior Development Engineer
>> O:  (408)938-4420;  mobile:  (408)655-4567
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html


-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From sundar.dorai-raj at pdf.com  Tue Nov  2 00:35:45 2004
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 01 Nov 2004 17:35:45 -0600
Subject: [R] stacking imported data
Message-ID: <4186C851.5030706@pdf.com>

Hi all,
   I have a question that I don't have a good answer for (note the word 
"good"; I have an answer, but I consider it not "good"). Take the 
following data in a single tab-delimited text file:

<text>

A
Labels	Value	SE	2.5%	97.5%
R90	0.231787	1.148044	0.035074	1.531779
R0	0.500861	0.604406	0.185336	1.353552

B
Labels	Value	SE	2.5%	97.5%
(Intercept)	1.367514	0.036431	1.287975	1.451964
</text>

(Note: the <text> tags are not present and are added here only to show 
blank lines.)

I would like to read the data into a single data.frame which looks like

Labels	Value	SE	2.5%	97.5%
A.R90	0.231787	1.148044	0.035074	1.531779
A.R0	0.500861	0.604406	0.185336	1.353552
B.(Intercept)	1.367514	0.036431	1.287975	1.451964

A few rules:

1. the number of rows in "A" and "B" will vary from 1 to ???. Here "A" 
has 1 row (excluding header) and B has 2 rows (excluding header).
2. the number of columns in "A" and "B" will always be the same.
4. the headers for "A" and "B" will always be the same.
3. there is always an empty line at the beginning of the file and in 
between "A" and "B".

My solution involves scan and indexing though it is error prone and not 
flexible if more or less than 5 columns are present in the data. While 
the number of columns is always the same from "A" to "B" it may change 
that "A" and "B" have more or fewer columns.

I hope this makes sense.

Thanks,

--sundar



From jarsuaga at cc.ucsf.edu  Tue Nov  2 00:40:12 2004
From: jarsuaga at cc.ucsf.edu (Javier Arsuaga)
Date: 01 Nov 2004 15:40:12 -0800
Subject: [R] Questions 2.0.0
Message-ID: <1099352416.1717.19.camel@localhost.localdomain>

I have just installed R-2.0.0 in my LINUX redhat GNOME.  have installed 
R in the / (top directory)


I am having two problems:

	1.- My graphics are gone (like x11() will not work) even if I install
the library (grGraphics).

	2.- The arrow keys to repeat commands are not working

Thank you



From spencer.graves at pdf.com  Tue Nov  2 02:00:00 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 01 Nov 2004 17:00:00 -0800
Subject: [R] stacking imported data
In-Reply-To: <4186C851.5030706@pdf.com>
References: <4186C851.5030706@pdf.com>
Message-ID: <4186DC10.40201@pdf.com>

Hi, Sundar: 

      I got something that looks like it might be what you want copying 
your 8 lines to clipboard and using the following: 

DF <- read.table("clipboard",  colClasses=character(0), fill=TRUE)
breaks <- which(DF$V2=="")
nrows <- diff(c(breaks, dim(DF)[1]+1))

files <- as.character(DF[breaks,1])

DF2 <- cbind(rep(files, nrows), DF)

DF. <- DF2[-c(breaks, breaks+1),]
DFnames <- as.matrix(DF[breaks[1]+1, ])
names(DF.) <- c("Files", DFnames)
#################
Result: 
DF

  Files      Labels    Value       SE     2.5%    97.5%
3     A         R90 0.231787 1.148044 0.035074 1.531779
4     A          R0 0.500861 0.604406 0.185336 1.353552
7     B (Intercept) 1.367514 0.036431 1.287975 1.451964

      This uses the "fill" argument in data.frame in R that Andy Liaw 
mentioned earlier today.  (Thus, this solution won't work in S-Plus 6.2, 
where data.frame does not have this argument.) 

      Is this satisfactory? 
      Spencer Graves

Sundar Dorai-Raj wrote:

> Hi all,
>   I have a question that I don't have a good answer for (note the word 
> "good"; I have an answer, but I consider it not "good"). Take the 
> following data in a single tab-delimited text file:
>
> <text>
>
> A
> Labels    Value    SE    2.5%    97.5%
> R90    0.231787    1.148044    0.035074    1.531779
> R0    0.500861    0.604406    0.185336    1.353552
>
> B
> Labels    Value    SE    2.5%    97.5%
> (Intercept)    1.367514    0.036431    1.287975    1.451964
> </text>
>
> (Note: the <text> tags are not present and are added here only to show 
> blank lines.)
>
> I would like to read the data into a single data.frame which looks like
>
> Labels    Value    SE    2.5%    97.5%
> A.R90    0.231787    1.148044    0.035074    1.531779
> A.R0    0.500861    0.604406    0.185336    1.353552
> B.(Intercept)    1.367514    0.036431    1.287975    1.451964
>
> A few rules:
>
> 1. the number of rows in "A" and "B" will vary from 1 to ???. Here "A" 
> has 1 row (excluding header) and B has 2 rows (excluding header).
> 2. the number of columns in "A" and "B" will always be the same.
> 4. the headers for "A" and "B" will always be the same.
> 3. there is always an empty line at the beginning of the file and in 
> between "A" and "B".
>
> My solution involves scan and indexing though it is error prone and 
> not flexible if more or less than 5 columns are present in the data. 
> While the number of columns is always the same from "A" to "B" it may 
> change that "A" and "B" have more or fewer columns.
>
> I hope this makes sense.
>
> Thanks,
>
> --sundar
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html


-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From ggrothendieck at myway.com  Tue Nov  2 02:31:27 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 2 Nov 2004 01:31:27 +0000 (UTC)
Subject: [R] stacking imported data
References: <4186C851.5030706@pdf.com>
Message-ID: <loom.20041102T022927-760@post.gmane.org>

Sundar Dorai-Raj <sundar.dorai-raj <at> pdf.com> writes:

: 
: Hi all,
:    I have a question that I don't have a good answer for (note the word 
: "good"; I have an answer, but I consider it not "good"). Take the 
: following data in a single tab-delimited text file:
: 
: <text>
: 
: A
: Labels	Value	SE	2.5%	97.5%
: R90	0.231787	1.148044	0.035074	1.531779
: R0	0.500861	0.604406	0.185336	1.353552
: 
: B
: Labels	Value	SE	2.5%	97.5%
: (Intercept)	1.367514	0.036431	1.287975	1.451964
: </text>
: 
: (Note: the <text> tags are not present and are added here only to show 
: blank lines.)
: 
: I would like to read the data into a single data.frame which looks like
: 
: Labels	Value	SE	2.5%	97.5%
: A.R90	0.231787	1.148044	0.035074	1.531779
: A.R0	0.500861	0.604406	0.185336	1.353552
: B.(Intercept)	1.367514	0.036431	1.287975	1.451964
: 
: A few rules:
: 
: 1. the number of rows in "A" and "B" will vary from 1 to ???. Here "A" 
: has 1 row (excluding header) and B has 2 rows (excluding header).
: 2. the number of columns in "A" and "B" will always be the same.
: 4. the headers for "A" and "B" will always be the same.
: 3. there is always an empty line at the beginning of the file and in 
: between "A" and "B".
: 

Read the lines into vector z, one line per element.

Define a grouping variable, g, which is 1 for the lines
starting at the first blank line and 2 for the lines
starting at the 2nd.  Define a function f which accepts such
a group of lines and creates the appropriate data frame from
them.  tapply the lines, grouped by g, and bind the rows of
the data frame produced from each group together into one
large data frame.  

z <- readLines("file.dat")

g <- cumsum(nchar(z) == 0)
f <- function(x) {
	x[-(1:3)] <- paste(trim(x[2]), x[-(1:3)], sep = ".")
	read.table(textConnection(x[-(1:2)]), header = TRUE)
}
do.call("rbind", tapply(z, cumsum(nchar(z) == 0), f))


Note: if the blank lines or the A and B lines contain
whitespace trim this off first.  That is, insert these
two lines after the readLines statement:

trim <- function(x) gsub("^[[:space:]]+|[[:space:]]+$", "", x)
z <- trim(z)



From ggrothendieck at myway.com  Tue Nov  2 02:41:34 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 2 Nov 2004 01:41:34 +0000 (UTC)
Subject: [R] stacking imported data
References: <4186C851.5030706@pdf.com>
	<loom.20041102T022927-760@post.gmane.org>
Message-ID: <loom.20041102T023942-642@post.gmane.org>

Gabor Grothendieck <ggrothendieck <at> myway.com> writes:

: 
: Sundar Dorai-Raj <sundar.dorai-raj <at> pdf.com> writes:
: 
: : 
: : Hi all,
: :    I have a question that I don't have a good answer for (note the word 
: : "good"; I have an answer, but I consider it not "good"). Take the 
: : following data in a single tab-delimited text file:
: : 
: : <text>
: : 
: : A
: : Labels	Value	SE	2.5%	97.5%
: : R90	0.231787	1.148044	0.035074	1.531779
: : R0	0.500861	0.604406	0.185336	1.353552
: : 
: : B
: : Labels	Value	SE	2.5%	97.5%
: : (Intercept)	1.367514	0.036431	1.287975	1.451964
: : </text>
: : 
: : (Note: the <text> tags are not present and are added here only to show 
: : blank lines.)
: : 
: : I would like to read the data into a single data.frame which looks like
: : 
: : Labels	Value	SE	2.5%	97.5%
: : A.R90	0.231787	1.148044	0.035074	1.531779
: : A.R0	0.500861	0.604406	0.185336	1.353552
: : B.(Intercept)	1.367514	0.036431	1.287975
	1.451964
: : 
: : A few rules:
: : 
: : 1. the number of rows in "A" and "B" will vary from 1 to ???. Here "A" 
: : has 1 row (excluding header) and B has 2 rows (excluding header).
: : 2. the number of columns in "A" and "B" will always be the same.
: : 4. the headers for "A" and "B" will always be the same.
: : 3. there is always an empty line at the beginning of the file and in 
: : between "A" and "B".
: : 
: 
: Read the lines into vector z, one line per element.
: 
: Define a grouping variable, g, which is 1 for the lines
: starting at the first blank line and 2 for the lines
: starting at the 2nd.  Define a function f which accepts such
: a group of lines and creates the appropriate data frame from
: them.  tapply the lines, grouped by g, and bind the rows of
: the data frame produced from each group together into one
: large data frame.  
: 
: z <- readLines("file.dat")
: 
: g <- cumsum(nchar(z) == 0)
: f <- function(x) {
: 	x[-(1:3)] <- paste(trim(x[2]), x[-(1:3)], sep = ".")
: 	read.table(textConnection(x[-(1:2)]), header = TRUE)
: }
: do.call("rbind", tapply(z, cumsum(nchar(z) == 0), f))

A correction:

 z <- readLines("file.dat")
 
 g <- cumsum(nchar(z) == 0)
 f <- function(x) {
 	x[-(1:3)] <- paste(x[2], x[-(1:3)], sep = ".")
 	read.table(textConnection(x[-(1:2)]), header = TRUE)
 }
 do.call("rbind", tapply(z, cumsum(nchar(z) == 0), f))


: 
: Note: if the blank lines or the A and B lines contain
: whitespace trim this off first.  That is, insert these
: two lines after the readLines statement:
: 
: trim <- function(x) gsub("^[[:space:]]+|[[:space:]]+$", "", x)
: z <- trim(z)
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://stat.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
: 
:



From david at frosted.net  Tue Nov  2 08:17:14 2004
From: david at frosted.net (David H.)
Date: Tue, 02 Nov 2004 08:17:14 +0100
Subject: [R] A general Mathematicasl pointer if you could? Thank you.
Message-ID: <4187347A.4090706@frosted.net>

Greetings.

A friend of mine and I are thinking about implementing the SIQ Protocol,
describe as RFC here:
http://www.milter.info/milter-siq/draft-irtf-asrg-iar-howe-siq-00.txt

as an Apache Module. This would implement the HTTP "connector?? described
in the RFC. While we do not think that this is too complicated, we are
concerned about the whole process that is attached to building the
necessary data.

I would like to know your opinion on SIQ per se and my idea.

Our goal is to minimise User interaction as best as we can, we would
also only train on Error.

An Error is defined as a host that sends/sent Spam. This would put the
burden of clearly identifying a message as SPAM on the submitters
shoulder. The Submitter would send in his corpus of SPAM which is then
processed into the system and a score for the "mail senders" in question
is built.

This built score should decay over a given time, while the score itself
and the amount of decay, as well as the total value of the score should
be determined from the behaviour of the "sending host". Which means a
host that is often reported to send Spam will naturally have a higher
score and the score will reset slowly. A host which sends little Spam or
has a high burst of Spam due to a (fixed) misconfiguration will show a
lower score and the score will reset quickly.

I am no mathematician. This is where I need your help. Could you point
me to a newsgroup (preferred) or Mailing-List that could tell me which
discipline in Math is suited for this? I heard that Survival Analysis
and "Time Series" might be suited to fit my "problem".

Just to clarify once more. The IP address of the "sending" host as well as the "domain" that it tries to identify
itself by are scored. I have to find a way how to do this fairly. Since we expect a huge influx of data, this has to be automated as best as possible. 

This of course should all result in a public service that will be made
available freely. Thank you for listening to my stammering.

-d



From ripley at stats.ox.ac.uk  Tue Nov  2 08:22:08 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 2 Nov 2004 07:22:08 +0000 (GMT)
Subject: [R] Questions 2.0.0
In-Reply-To: <1099352416.1717.19.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.44.0411020718480.23538-100000@gannet.stats>

On 1 Nov 2004, Javier Arsuaga wrote:

> I have just installed R-2.0.0 in my LINUX redhat GNOME.  have installed 
> R in the / (top directory)

Did you install from sources, or from an RPM?

If from sources, please check what the end of the configure step says and
what the R-admin manual says about it.

> I am having two problems:
> 
> 	1.- My graphics are gone (like x11() will not work) even if I install
> the library (grGraphics).

What `library' (there is no package grGraphics)?  What error message?
Most likely you were missing the X11 development files at configure time.

> 	2.- The arrow keys to repeat commands are not working

So libreadline was not found.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Jan_Svatos at eurotel.cz  Tue Nov  2 08:35:08 2004
From: Jan_Svatos at eurotel.cz (Jan_Svatos@eurotel.cz)
Date: Tue, 2 Nov 2004 08:35:08 +0100
Subject: [R] Read in data from Microsoft SQL
Message-ID: <OFA837824D.B5529749-ONC1256F40.00294AC0@eurotel.cz>

Hi,

the DSN should be exactly the same as "name" of the (probably system) DSN 
stated in Windows ODBC settings.
Just keep in mind, that DSN in general often holds information about the 
Default Database, and that switching database os stating database as 
prefix may be necessary.

Jan


- - - Original message: - - -
From: r-help-bounces at stat.math.ethz.ch
Send: 1.11.2004 19:10:47
To: <R-help at stat.math.ethz.ch>
Subject: [R] Read in data from Microsoft SQL

Hi,

I am trying to read data from Microsoft SQL. I tried to use odbcConnect
(in RODBC) but failed. What should the "dsn" be? Does anyone know how to
do it? I will appreciate it.

Thank you!

Xiaorong
________________________________________________________________________
The preceding e-mail message (including any attachments) contains
information that may be confidential, be protected by the
attorney-client or other applicable privileges, or constitute non-public
information. It is intended to be read only by the individual or entity
to whom it is addressed or by their designee. If you are not an intended
recipient of this message, please notify the sender by replying to this
message and then delete it from your system. You are on notice that
further use, dissemination, distribution, or reproduction of this
message is strictly prohibited and may be unlawful.

 [[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Tue Nov  2 09:09:41 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 2 Nov 2004 08:09:41 +0000 (GMT)
Subject: [R] Read in data from Microsoft SQL
In-Reply-To: <OFA837824D.B5529749-ONC1256F40.00294AC0@eurotel.cz>
Message-ID: <Pine.LNX.4.44.0411020805100.27932-100000@gannet.stats>

On Tue, 2 Nov 2004 Jan_Svatos at eurotel.cz wrote:

> Hi,
> 
> the DSN should be exactly the same as "name" of the (probably system) DSN 
> stated in Windows ODBC settings.
> Just keep in mind, that DSN in general often holds information about the 
> Default Database, and that switching database os stating database as 
> prefix may be necessary.

Yes, assuming this _is_ on Windows (unstated).  In that case and under
RGui, just use odbcDriverConnect() and you will get dialog boxes to select
a dsn, or setup a temporary one (via the New button).

> 
> Jan
> 
> 
> - - - Original message: - - -
> From: r-help-bounces at stat.math.ethz.ch
> Send: 1.11.2004 19:10:47
> To: <R-help at stat.math.ethz.ch>
> Subject: [R] Read in data from Microsoft SQL
> 
> Hi,
> 
> I am trying to read data from Microsoft SQL. I tried to use odbcConnect
> (in RODBC) but failed. What should the "dsn" be? Does anyone know how to
> do it? I will appreciate it.
> 
> Thank you!
> 
> Xiaorong


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From r.g.brown at cefas.co.uk  Tue Nov  2 09:19:59 2004
From: r.g.brown at cefas.co.uk (Robert Brown FM CEFAS)
Date: Tue, 2 Nov 2004 08:19:59 -0000
Subject: [R] Loadhistory problems
Message-ID: <3589BC4D64C84341AE0C258244F977A2B60B5C@expressa.corp.cefas.co.uk>

History files saved using nFile Save History do not open when using File Load History. I notice that when they are saved they do not seem to have a file extension and there is no option to chose an extension.  Is this a problem.  Other than this it seems so simple that I can't see what the problem is.    I've tried using loadhistory and savehistory with the same problems.  The syntax guide for load history requires the file name, but doesn't specify if this should include and extension or what this should be.  Where am I going wrong



From asemeria at cramont.it  Tue Nov  2 09:46:23 2004
From: asemeria at cramont.it (asemeria@cramont.it)
Date: Tue, 2 Nov 2004 09:46:23 +0100
Subject: [R] Loadhistory problems
Message-ID: <OFA1B90193.77CDD4BA-ONC1256F40.00303129@tomware.it>

Probably you have an R version >= 1.9.1, then
to make use of loadhistory you have to load the
 'utils' library (type library(utils)
on your .Rprofile).
Best
A.S.

----------------------------

Alessandro Semeria
Models and Simulations Laboratory
Montecatini Environmental Research Center (Edison Group),
Via Ciro Menotti 48,
48023 Marina di Ravenna (RA), Italy
Tel. +39 544 536811
Fax. +39 544 538663
E-mail: alessandro.semeria at cramont.it



From ripley at stats.ox.ac.uk  Tue Nov  2 09:51:37 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 2 Nov 2004 08:51:37 +0000 (GMT)
Subject: [R] Loadhistory problems
In-Reply-To: <3589BC4D64C84341AE0C258244F977A2B60B5C@expressa.corp.cefas.co.uk>
Message-ID: <Pine.LNX.4.44.0411020845450.28169-100000@gannet.stats>

Please read the posting guide, and tell us what platform you are using, R 
version and so on.

On Tue, 2 Nov 2004, Robert Brown FM CEFAS wrote:

> History files saved using nFile Save History do not open when using File
> Load History. I notice that when they are saved they do not seem to have
> a file extension and there is no option to chose an extension.  Is this
> a problem.  Other than this it seems so simple that I can't see what the
> problem is.  I've tried using loadhistory and savehistory with the same
> problems.  The syntax guide for load history requires the file name, but
> doesn't specify if this should include and extension or what this should
> be.  Where am I going wrong

What have file extensions to do with this?  A file name is exactly that, 
the name of a file, the habit of Windows Explorer to hide parts of the 
name notwithstanding.  The default name is .Rhistory, and that is not an 
extension!

This does work as documented in R 2.0.0 under Windows.  If you can
describe what you are doing in enough detail for someone to reproduce it, 
they may be able to spot what is wrong.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Tue Nov  2 09:54:20 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 02 Nov 2004 09:54:20 +0100
Subject: [R] Loadhistory problems
In-Reply-To: <3589BC4D64C84341AE0C258244F977A2B60B5C@expressa.corp.cefas.co.uk>
References: <3589BC4D64C84341AE0C258244F977A2B60B5C@expressa.corp.cefas.co.uk>
Message-ID: <41874B3C.90200@statistik.uni-dortmund.de>

Robert Brown FM CEFAS wrote:

> History files saved using nFile Save History do not open when using
> File Load History. I notice that when they are saved they do not seem
> to have a file extension and there is no option to chose an
> extension.  Is this a problem.  Other than this it seems so simple
> that I can't see what the problem is.    I've tried using loadhistory
> and savehistory with the same problems.  The syntax guide for load
> history requires the file name, but doesn't specify if this should
> include and extension or what this should be.  Where am I going wrong

See ?loadhistory. The extension can be choosen arbitrarily, the complete 
default filename is ".Rhistory".

Works for me:

# Type somethimg in R, then:
  savehistory("c:/temp.txt")
  q("no")
# Start R again
  loadhistory("c:/temp.txt")
# Now use the arrow keys and find that
#  the formerly saved history has been
#  loaded correctly ...


Uwe Ligges



> 
> ______________________________________________ 
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the
> posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Tue Nov  2 09:56:19 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 2 Nov 2004 08:56:19 +0000 (GMT)
Subject: [R] Loadhistory problems
In-Reply-To: <OFA1B90193.77CDD4BA-ONC1256F40.00303129@tomware.it>
Message-ID: <Pine.LNX.4.44.0411020851520.28169-100000@gannet.stats>

On Tue, 2 Nov 2004 asemeria at cramont.it wrote:

[In reply to R.G.Brown, original post not included]

> Probably you have an R version >= 1.9.1, then
> to make use of loadhistory you have to load the
>  'utils' library (type library(utils)
> on your .Rprofile).

But only to make use of loadhistory() in your .Rprofile, when it is
preferable to use utils::loadhistory.  (He appears to be talking about 
using RGui menus.)

But why would you want to do that when the loading of a commands history
is controlled by command-line flags and the name by the R_HISTFILE
environment variable?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ahenningsen at email.uni-kiel.de  Tue Nov  2 11:02:45 2004
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Tue, 2 Nov 2004 11:02:45 +0100
Subject: [R] make apply() return a list
In-Reply-To: <418685F7.2060507@pdf.com>
References: <200411011937.08360.ahenningsen@email.uni-kiel.de>
	<418685F7.2060507@pdf.com>
Message-ID: <200411021102.45894.ahenningsen@email.uni-kiel.de>

Hi,

thank you very much Sundar, Patrick, Tony, Mahub and Gabor for your helpful 
answers! All your examples work great. They are all more straightforeward 
than my example and much faster than the for-loop. 
These are the average elapsed times (in seconds) returned by system.time()[3] 
(applied to my real function and my real data):

my original for-loop: 
5.55 

the example I presented in my previous email (using apply):
2.35 

example suggested by Tony (using apply):
2.34 

example suggested by Gabor (using lapply):
2.50 

examples suggested by Sundar and Mahub (using lapply):
2.68 

Best regards,
Arne


On Monday 01 November 2004 19:52, Sundar Dorai-Raj wrote:
> Arne Henningsen wrote:
> > Hi,
> >
> > I have a dataframe (say myData) and want to get a list (say myList) that
> > contains a matrix for each row of the dataframe myData. These matrices
> > are calculated based on the corresponding row of myData. Using a
> > for()-loop to do this is very slow. Thus, I tried to use apply().
> > However, afaik apply() does only return a list if the matrices have
> > different dimensions, while my matrices have all the same dimension. To
> > get a list I could change the dimension of one matrix artificially and
> > restore it after apply():
> >
> > This a (very much) simplified example of what I did:
> >>myData <- data.frame( a = c( 1,2,3 ), b = c( 4,5,6 ) )
> >>myFunction <- function( values ) {
> >
> > +    myMatrix <- matrix( values, 2, 2 )
> > +    if( all( values == myData[ 1, ] ) ) {
> > +       myMatrix <- cbind( myMatrix, rep( 0, 2 ) )
> > +    }
> > +    return( myMatrix )
> > + }
> >
> >>myList <- apply( myData, 1, myFunction )
> >>myList[[ 1 ]] <- myList[[ 1 ]][ 1:2, 1:2 ]
> >>myList
> >
> > $"1"
> >      [,1] [,2]
> > [1,]    1    1
> > [2,]    4    4
> >
> > $"2"
> >      [,1] [,2]
> > [1,]    2    2
> > [2,]    5    5
> >
> > $"3"
> >      [,1] [,2]
> > [1,]    3    3
> > [2,]    6    6
> >
> > This exactly does what I want and really speeds up the calculation, but I
> > wonder if there is an easier way to make apply() return a list.
> >
> > Thanks for your help,
> > Arne
>
> Hi Arne,
>
> I'm not sure how much faster this will be over using `for' but you can try:
>
> lapply(seq(nrow(myData)), function(i) myFunction(myData[i, ]))
>
> --sundar

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From ludovic.tambour at cirad.fr  Tue Nov  2 11:14:07 2004
From: ludovic.tambour at cirad.fr (Ludovic Tambour)
Date: Tue, 2 Nov 2004 11:14:07 +0100
Subject: [R] genou
Message-ID: <002901c4c0c4$b0042080$47ecc7c2@cirad.fr>

Hello,

I am a new user of R software. I have not found documentation about "genou"
function (optimisation using genetic algorithms). I need know if "genou"
uses normalized values or not. Please, somebody has a response for me ?

Ludo



From ludovic.tambour at cirad.fr  Tue Nov  2 11:17:02 2004
From: ludovic.tambour at cirad.fr (Ludovic Tambour)
Date: Tue, 2 Nov 2004 11:17:02 +0100
Subject: [R] integer
Message-ID: <003101c4c0c5$189dc6a0$47ecc7c2@cirad.fr>

Hello,

I need to use "R" to determine parameters which are integers. How I can do
this, please ?

Ludo



From ligges at statistik.uni-dortmund.de  Tue Nov  2 11:31:17 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 02 Nov 2004 11:31:17 +0100
Subject: [R] genou
In-Reply-To: <002901c4c0c4$b0042080$47ecc7c2@cirad.fr>
References: <002901c4c0c4$b0042080$47ecc7c2@cirad.fr>
Message-ID: <418761F5.7040808@statistik.uni-dortmund.de>

Ludovic Tambour wrote:

> Hello,
> 
> I am a new user of R software. I have not found documentation about "genou"
> function (optimisation using genetic algorithms). I need know if "genou"
> uses normalized values or not. Please, somebody has a response for me ?
> 
> Ludo

[I do not know the answer to your question, but please note my other 
remarks.]


Please specify package and function precisely!
I guess you are talking about function genoud() (not genou()!) in 
package "rgenoud".

There is a help page, try
   library(rgenoud)
   ?genoud
There are also some references given on the help page.

Moreover, if reading that still does not help, you might want to contact 
the package maintainer.

Uwe Ligges



> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Tue Nov  2 11:36:27 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 02 Nov 2004 11:36:27 +0100
Subject: [R] integer
In-Reply-To: <003101c4c0c5$189dc6a0$47ecc7c2@cirad.fr>
References: <003101c4c0c5$189dc6a0$47ecc7c2@cirad.fr>
Message-ID: <4187632B.2020000@statistik.uni-dortmund.de>

Ludovic Tambour wrote:

> Hello,
> 
> I need to use "R" to determine parameters which are integers. How I can do
> this, please ?

What so you mean with "parameters"? In which context?

To check whether a numeric vector "x" contains only integers, you can try

all.equal(as.integer(x), x)

Uwe Ligges




> Ludo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From asemeria at cramont.it  Tue Nov  2 11:51:17 2004
From: asemeria at cramont.it (asemeria@cramont.it)
Date: Tue, 2 Nov 2004 11:51:17 +0100
Subject: [R] Loadhistory problems
Message-ID: <OF89B1DE7F.E9559132-ONC1256F40.003B9E9C@tomware.it>

Sorry, some times, as today, I read e-mails on a very poor
web-Lotus Notes  interface with a browser on a linux box, and the bad
formatting text mislead me about r.g.brown's O.S.
A.S.

----------------------------

Alessandro Semeria
Models and Simulations Laboratory
Montecatini Environmental Research Center (Edison Group),
Via Ciro Menotti 48,
48023 Marina di Ravenna (RA), Italy
Tel. +39 544 536811
Fax. +39 544 538663
E-mail: alessandro.semeria at cramont.it



From jdooley at ugcs.caltech.edu  Tue Nov  2 11:52:06 2004
From: jdooley at ugcs.caltech.edu (James Dooley)
Date: Tue, 2 Nov 2004 02:52:06 -0800
Subject: [R] New email address for jdooley
Message-ID: <200411021052.iA2Aq6DR012628@beg.ugcs.caltech.edu>

Either you sent email to me, or someone spammed it on your behalf.
Just want to let you know - this email address is no longer valid.
All email sent to it is just deleted.

thanks,
-james



From sgrabasa at escet.urjc.es  Tue Nov  2 11:54:26 2004
From: sgrabasa at escet.urjc.es (sgrabasa@escet.urjc.es)
Date: Tue, 2 Nov 2004 11:54:26 +0100 (CET)
Subject: [R] warning message
Message-ID: <6245326178sgrabasa@escet.urjc.es>



Hi, 

I am using the glmmPQL package, and when I run the program I got the 
following warning messages:

1: non-integer #successes in a binomial glm! in: eval(expr, envir, 
enclos) 
2: Singular precision matrix in level -1, block 5 

 What do thy mean?

Thanks



From ripley at stats.ox.ac.uk  Tue Nov  2 12:17:36 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 2 Nov 2004 11:17:36 +0000 (GMT)
Subject: [R] warning message
In-Reply-To: <6245326178sgrabasa@escet.urjc.es>
Message-ID: <Pine.LNX.4.44.0411021114280.11989-100000@gannet.stats>

On Tue, 2 Nov 2004 sgrabasa at escet.urjc.es wrote:

> I am using the glmmPQL package, 

I can find no such package: is this the glmmPQL function in the MASS 
package?

> and when I run the program I got the 
> following warning messages:
> 
> 1: non-integer #successes in a binomial glm! in: eval(expr, envir, 
> enclos) 
> 2: Singular precision matrix in level -1, block 5 
> 
>  What do thy mean?

The first means what it says: you have specified the binomial glm 
incorrectly.  Please read the posting guide and remember to send us useful 
information about what you were doing.

The second means the model fitting process got stuck, and is more likely 
an error message.  Given the first, no great surprise.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Brandon.J.Whitcher at gsk.com  Mon Nov  1 20:28:45 2004
From: Brandon.J.Whitcher at gsk.com (Brandon.J.Whitcher@gsk.com)
Date: Mon, 1 Nov 2004 19:28:45 +0000
Subject: [R] [R-pkgs] updated package waveslim 1.4
Message-ID: <OF520777B8.23C07501-ON80256F3F.0068A07B-80256F3F.006B12A1@sb.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041101/3d89ef48/attachment.pl

From gb at stat.umu.se  Tue Nov  2 14:41:29 2004
From: gb at stat.umu.se (Gran Brostrm)
Date: Tue, 2 Nov 2004 14:41:29 +0100
Subject: [R] integer
In-Reply-To: <4187632B.2020000@statistik.uni-dortmund.de>
References: <003101c4c0c5$189dc6a0$47ecc7c2@cirad.fr>
	<4187632B.2020000@statistik.uni-dortmund.de>
Message-ID: <20041102134129.GA31724@stat.umu.se>

On Tue, Nov 02, 2004 at 11:36:27AM +0100, Uwe Ligges wrote:
> Ludovic Tambour wrote:
> 
> >Hello,
> >
> >I need to use "R" to determine parameters which are integers. How I can do
> >this, please ?
> 
> What so you mean with "parameters"? In which context?
> 
> To check whether a numeric vector "x" contains only integers, you can try
> 
> all.equal(as.integer(x), x)

I don't think so:

> x <- as.double(c(1, 2))
> y <- as.integer(c(1, 2))
> all.equal(x, y)
[1] TRUE

But,

> identical(x, y)
[1] FALSE

On the other hand, why not use 

> is.integer(x)
[1] FALSE
> is.integer(y)
[1] TRUE

because I think that a numeric vector can't have a mix of integer and
non-integer elements. With a list it's a different story.

G??ran
-- 
 G??ran Brostr??m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume?? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume??, Sweden             e-mail: gb at stat.umu.se



From florin.maican at handels.gu.se  Tue Nov  2 14:51:39 2004
From: florin.maican at handels.gu.se (Florin G. Maican)
Date: Tue, 02 Nov 2004 14:51:39 +0100
Subject: [R] Matrix decomposition: orthogonal complement
Message-ID: <418790EB.5090904@handels.gu.se>

Hello,

How I can compute in R the orthogonal complement of  one matrix?

If  A (n x m ) matrix of full column rank (n>m), its orthogonal 
complement is denoted by A_ .

A_  is  n X (n-m) matrix of full column rank and such that A'A_=0.

I  need to compute A_.   How I can compute A_ in R?

Best Regards,
/Florin

-- 
Florin G. Maican
Ph.D. candidate, Department of Economics
School of Economics and Commercial Law
Gothenburg University, Sweden
E-mail: Florin.Maican at handels.gu.se
P.O.Box 640 SE-405 30 Gothenburg, Sweden 
Phone no +46 31 773 4866
Fax no +46 31 773 4154



From aolinto_r at bignet.com.br  Tue Nov  2 15:05:18 2004
From: aolinto_r at bignet.com.br (Antonio Olinto)
Date: Tue,  2 Nov 2004 12:05:18 -0200
Subject: [R] p-value for nonparamentric multiple comparison testing
Message-ID: <1099404318.4187941eb3fb2@webmail2.bignet.com.br>

Dear R users,

I wrote a function to perform a nonparametric multiple comparison test. The
function below solves the example 11.10 (pg. 228) from Zar??s Biostatistical
Analysis (3rd ed.). Nevertheless I couldn??t find a function to get the p-value
associated to Q, so I still have to consult Table B.15, App106 (Critical values
of Q for Nonparamentric Multiple Comparison Testing). Is there any function in R
that returns this value?

Thanks in advance.

Antonio Olinto

==========

multcomp <- function(VarCat,VarNum) {
dat.multcomp <- data.frame(VarCat,VarNum)
names(dat.multcomp) <- c("VarCat","VarNum")
attach(dat.multcomp)
dat.multcomp$Rank <- rank(VarNum)
attach(dat.multcomp)
RankList <- aggregate(Rank,list(Rank=Rank),FUN=length)
t <- length(RankList$Rank)
st <- sum(ifelse(RankList[,2]>1,RankList[,2]^3-RankList[,2], 0))
LevCat <- levels(dat.multcomp$VarCat)
NLevCat <- aggregate(VarCat,list(LevCat=VarCat),FUN=length)
RLevCat <- aggregate(Rank,list(LevCat=VarCat),FUN=sum)
MLevCat <- aggregate(Rank,list(LevCat=VarCat),FUN=mean)
SampleSummary <- data.frame(LevCat,RLevCat[,2],NLevCat[,2],MLevCat[,2])
names(SampleSummary)<-c("Samples","RSum","N","RMean")
SampleSummary <- SampleSummary[order(SampleSummary$RMean,decreasing=T),]
NCat <- length(LevCat)
NComb <- choose(NCat,2)
N <- length(dat.multcomp$VarCat)
Results <- data.frame(rep(NA,NComb),rep(NA,NComb),rep(NA,NComb),rep(NA,NComb))
names(Results) <- c("Comparison","Difference","SE","Q")
l <- 1
for (i in 1:(NCat-1)) {
   for (j in NCat:(i+1)) {
   SE <- sqrt(((N*(N+1)/12)-(st/(12*(N-1))))*((1/SampleSummary[i,3])+
(1/SampleSummary[j,3])))
   Dif <- SampleSummary[i,4]-SampleSummary[j,4]
   Q=Dif/SE
   Results[l,1] <- paste(SampleSummary[i,1],"vs",SampleSummary[j,1])
   Results[l,2] <- round(Dif,4)
   Results[l,3] <- round(SE,4)
   Results[l,4] <- round(Q,4)
   l <-l+1
   }
}
print("Sample summary ranked by mean ranks")
print(SampleSummary)
print("")
print("Table of multiple comparisons")
print(Results)
}

-------------------------------------------------
WebMail Bignet - O seu provedor do litoral
www.bignet.com.br



From Carlisle.Thacker at noaa.gov  Tue Nov  2 15:06:09 2004
From: Carlisle.Thacker at noaa.gov (Carlisle Thacker)
Date: Tue, 02 Nov 2004 09:06:09 -0500
Subject: [R] overlapping plots
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E268@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E268@usrymx25.merck.com>
Message-ID: <41879451.50304@noaa.gov>

Andy,

If I had the three plots on three separate sheets of paper, I could cut 
out plot 2 and paste it over plot one so that the data and axes on both 
plots can be seen.  Then I could cut out plot 3 and past it over plots 1 
and 2 so that data and axes on all three plots can be seen.  For my 
special case, the x-axes would have to appear at the top of each plot. 
If there are boxes around the plots or guide lines, the parts below 
plots 2 and 3 should be obscured.

Thanks,

Carlisle

Liaw, Andy wrote:
> I don't really understand what you want to do, but seems like with the
> `grid' package, such things are quite possible.
> 
> Andy
> 
> 
>>From: Carlisle Thacker
>>
>>How to combine three plots so that they partially overlap?  
>>As the data 
>>on all three are near the left side and top, it would be nice to draw 
>>the first, shift axes down and to the right and draw the second, and 
>>shift again to draw the third.
>>
>>I could shift the data for the 2nd and 3rd plots and construct their 
>>axes by hand, but it would be great if R has an easier way.
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>
> 
> 
> 
> ------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachments, contains information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New Jersey, USA 08889), and/or its affiliates (which may be known outside the United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as Banyu) that may be confidential, proprietary copyrighted and/or legally privileged. It is intended solely for the use of the individual or entity named on this message.  If you are not the intended recipient, and have received this message in error, please notify us immediately by reply e-mail and then delete it from your system.
> ------------------------------------------------------------------------------



From ripley at stats.ox.ac.uk  Tue Nov  2 15:08:20 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 2 Nov 2004 14:08:20 +0000 (GMT)
Subject: [R] Matrix decomposition: orthogonal complement
In-Reply-To: <418790EB.5090904@handels.gu.se>
Message-ID: <Pine.LNX.4.44.0411021406430.24946-100000@gannet.stats>

On Tue, 2 Nov 2004, Florin G. Maican wrote:

> How I can compute in R the orthogonal complement of  one matrix?
> 
> If  A (n x m ) matrix of full column rank (n>m), its orthogonal 
> complement is denoted by A_ .
> 
> A_  is  n X (n-m) matrix of full column rank and such that A'A_=0.

It is far from unique, of course.

> I  need to compute A_.   How I can compute A_ in R?

library(MASS)
?Null


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tang_chalmers at hotmail.com  Tue Nov  2 15:11:32 2004
From: tang_chalmers at hotmail.com (jing tang)
Date: Tue, 02 Nov 2004 15:11:32 +0100
Subject: [R] n-th power of a matrix
Message-ID: <BAY8-F43WtIIZS2TRqf000090f7@hotmail.com>

Hello all,
To calculate the power of a matrix, I used the command "mtx.exp(X, n)", but 
there is an error saying "Error: couldn't find function "mtx.exp"".  How can 
I deal with this problem?
Jing



From simon at stats.gla.ac.uk  Tue Nov  2 15:18:31 2004
From: simon at stats.gla.ac.uk (Simon Wood)
Date: Tue, 2 Nov 2004 14:18:31 +0000 (GMT)
Subject: [R] Matrix decomposition: orthogonal complement
In-Reply-To: <418790EB.5090904@handels.gu.se>
References: <418790EB.5090904@handels.gu.se>
Message-ID: <Pine.LNX.4.58.0411021417400.10559@moon.stats.gla.ac.uk>



> How I can compute in R the orthogonal complement of  one matrix?

use the qr decomposition. For example:

A<-matrix(rnorm(40),10,4)
B <- t(qr.Q(qr(A),complete=TRUE)[,5:10])
B%*%A

best,
Simon


> 
> If  A (n x m ) matrix of full column rank (n>m), its orthogonal complement is
> denoted by A_ .
> 
> A_  is  n X (n-m) matrix of full column rank and such that A'A_=0.
> 
> I  need to compute A_.   How I can compute A_ in R?
> 
> Best Regards,
> /Florin
> 
> -- 
> Florin G. Maican
> Ph.D. candidate, Department of Economics
> School of Economics and Commercial Law
> Gothenburg University, Sweden
> E-mail: Florin.Maican at handels.gu.se
> P.O.Box 640 SE-405 30 Gothenburg, Sweden Phone no +46 31 773 4866
> Fax no +46 31 773 4154
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ggrothendieck at myway.com  Tue Nov  2 15:29:32 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 2 Nov 2004 14:29:32 +0000 (UTC)
Subject: [R] make apply() return a list
References: <200411011937.08360.ahenningsen@email.uni-kiel.de>
	<418685F7.2060507@pdf.com>
	<200411021102.45894.ahenningsen@email.uni-kiel.de>
Message-ID: <loom.20041102T152755-211@post.gmane.org>

Arne Henningsen <ahenningsen <at> email.uni-kiel.de> writes:

: 
: Hi,
: 
: thank you very much Sundar, Patrick, Tony, Mahub and Gabor for your helpful 
: answers! All your examples work great. They are all more straightforeward 
: than my example and much faster than the for-loop. 
: These are the average elapsed times (in seconds) returned by system.time()
[3] 
: (applied to my real function and my real data):
: 
: my original for-loop: 
: 5.55 
: 
: the example I presented in my previous email (using apply):
: 2.35 
: 
: example suggested by Tony (using apply):
: 2.34 
: 
: example suggested by Gabor (using lapply):
: 2.50 
: 
: examples suggested by Sundar and Mahub (using lapply):
: 2.68 
: 


Perhaps any comparison should also include simplicity.  This is
somewhat subjective but just to objectify it I have reworked
each solution to compactify it as much as I could and then
calculated the number of characters in each solution using wc:

AH - 293 characters
TP - 70 characters
ML - 62 characters
GG - 48 characters

The versions I used are below.  

---

# data
myData <- data.frame( a = c( 1,2,3 ), b = c( 4,5,6 ) )

# AH
myFunction <- function( values ) {
   myMatrix <- matrix( values, 2, 2 )
   if( all( values == myData[ 1, ] ) ) {
      myMatrix <- cbind( myMatrix, rep( 0, 2 ) )
   }
   return( myMatrix )
}
myList <- apply( myData, 1, myFunction )
myList[[ 1 ]] <- myList[[ 1 ]][ 1:2, 1:2 ]
myList

# TP
lapply(apply(myData, 1, function(x) list(matrix(x, 2, 2))), "[[", 1)

# ML
lapply(1:nrow(myData), function(i) matrix(myData[i,], 2, 2))

# GG
lapply(as.data.frame(t(myData)), matrix, 2, 2)



From ligges at statistik.uni-dortmund.de  Tue Nov  2 15:44:01 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 02 Nov 2004 15:44:01 +0100
Subject: [R] integer
In-Reply-To: <20041102134129.GA31724@stat.umu.se>
References: <003101c4c0c5$189dc6a0$47ecc7c2@cirad.fr>	<4187632B.2020000@statistik.uni-dortmund.de>
	<20041102134129.GA31724@stat.umu.se>
Message-ID: <41879D31.2090404@statistik.uni-dortmund.de>

G??ran Brostr??m wrote:

> On Tue, Nov 02, 2004 at 11:36:27AM +0100, Uwe Ligges wrote:
> 
>>Ludovic Tambour wrote:
>>
>>
>>>Hello,
>>>
>>>I need to use "R" to determine parameters which are integers. How I can do
>>>this, please ?
>>
>>What so you mean with "parameters"? In which context?
>>
>>To check whether a numeric vector "x" contains only integers, you can try
>>
>>all.equal(as.integer(x), x)
> 
> 
> I don't think so:
> 
> 
>>x <- as.double(c(1, 2))
>>y <- as.integer(c(1, 2))
>>all.equal(x, y)
> 
> [1] TRUE
> 
> But,
> 
> 
>>identical(x, y)
> 
> [1] FALSE

[The story was completely different from the stuff I guessed, so all 
further communication related to this thread is "academic".]

G??ran,

yes, as expected.


> On the other hand, why not use 
> 
> 
>>is.integer(x)
> 
> [1] FALSE
> 
>>is.integer(y)
> 
> [1] TRUE
> 
> because I think that a numeric vector can't have a mix of integer and
> non-integer elements. With a list it's a different story.

Yes.

My guees was that the asker tried to identify integers such as 2, 3 in 
contrast to 2.1, 3.1, ...and you won't know it by looking at R's storage 
mode (my guess was that the asker was not interested in the storage 
mode, but in the nature of the numbers!).
Note that is.integer(1) is FALSE!!!

The given usage of all.equal() helps to identify 1 as an integer, but 
not 1.1...

Uwe

> G??ran



From abunn at whrc.org  Tue Nov  2 15:46:59 2004
From: abunn at whrc.org (Andy Bunn)
Date: Tue, 2 Nov 2004 09:46:59 -0500
Subject: [R] n-th power of a matrix
In-Reply-To: <BAY8-F43WtIIZS2TRqf000090f7@hotmail.com>
Message-ID: <NEBBIPHDAMMOKDKPOFFIMEHJCLAA.abunn@whrc.org>

Load the library first:

> library(Malmig)
> ?mtx.exp

HTH, Andy



From ligges at statistik.uni-dortmund.de  Tue Nov  2 15:53:06 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 02 Nov 2004 15:53:06 +0100
Subject: [R] n-th power of a matrix
In-Reply-To: <BAY8-F43WtIIZS2TRqf000090f7@hotmail.com>
References: <BAY8-F43WtIIZS2TRqf000090f7@hotmail.com>
Message-ID: <41879F52.1000503@statistik.uni-dortmund.de>

jing tang wrote:

> Hello all,
> To calculate the power of a matrix, I used the command "mtx.exp(X, n)", 
> but there is an error saying "Error: couldn't find function "mtx.exp"".  
> How can I deal with this problem?
> Jing


You have not told us in which package mtx.exp() is, and looking for it 
shows us that it is in package "Malmig".

  library(Malmig)
  mtx.exp(X, n)

works for me.


Folks, PLEASE think about the packages you are using!
This is the (n+1)-th e-mail within 7 days not specifying the package you 
are talking about.

Uwe Ligges



From ales.ziberna at guest.arnes.si  Tue Nov  2 15:54:30 2004
From: ales.ziberna at guest.arnes.si (=?iso-8859-2?Q?Ale=B9_=AEiberna?=)
Date: Tue, 2 Nov 2004 15:54:30 +0100
Subject: [R] Using East-european characters in R
Message-ID: <004201c4c0eb$fb796120$0d09f9c2@ales>

Does anybody know how to produce a East-European character "??" - "c with a 
v-shaped hat " in R (in text or plot)?



I do know how to get "??,??" - "s,z, with a v-shaped hat", but not this one.



Thanks in advance for any suggestions,

Ales Ziberna



P.S.: I am using Windows XP and R version 1.9.1 (details below)

$platform

[1] "i386-pc-mingw32"



$arch

[1] "i386"



$os

[1] "mingw32"



$system

[1] "i386, mingw32"



$status

[1] ""



$major

[1] "1"



$minor

[1] "9.1"



$year

[1] "2004"



$month

[1] "06"



$day

[1] "21"



$language

[1] "R"



From maechler at stat.math.ethz.ch  Tue Nov  2 15:56:53 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 2 Nov 2004 15:56:53 +0100
Subject: [R] n-th power of a matrix
In-Reply-To: <BAY8-F43WtIIZS2TRqf000090f7@hotmail.com>
References: <BAY8-F43WtIIZS2TRqf000090f7@hotmail.com>
Message-ID: <16775.41013.877075.481401@gargle.gargle.HOWL>

>>>>> "jing" == jing tang <tang_chalmers at hotmail.com>
>>>>>     on Tue, 02 Nov 2004 15:11:32 +0100 writes:

    jing> Hello all,

    jing> To calculate the power of a matrix, I used the command
    jing> "mtx.exp(X, n)", but there is an error saying "Error:
    jing> couldn't find function "mtx.exp"".  How can I deal
    jing> with this problem?  Jing

## First load the "Malmig" package in which the  mtx.exp()
## function is defined:

library(Malmig)

## If that gives an error as well, you also need.
install.packages("Malmig")

Martin Maechler, ETH Zurich



From thomas.carruthers at imperial.ac.uk  Tue Nov  2 16:01:26 2004
From: thomas.carruthers at imperial.ac.uk (Carruthers, Thomas R)
Date: Tue, 2 Nov 2004 15:01:26 -0000
Subject: [R] A GLM for a parameter of the error distribution.
Message-ID: <F741E3C5AE0B914EAD5FFC59288B0D4313577E@icex1.ic.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041102/52016796/attachment.pl

From ludovic.tambour at cirad.fr  Tue Nov  2 16:21:53 2004
From: ludovic.tambour at cirad.fr (Ludovic Tambour)
Date: Tue, 2 Nov 2004 16:21:53 +0100
Subject: [R] integer
References: <003101c4c0c5$189dc6a0$47ecc7c2@cirad.fr>	<4187632B.2020000@statistik.uni-dortmund.de><20041102134129.GA31724@stat.umu.se>
	<41879D31.2090404@statistik.uni-dortmund.de>
Message-ID: <007501c4c0ef$aec6d700$47ecc7c2@cirad.fr>

Sorry, the formulation of my question is bad. I hope that you have not lost
your time. The problem is not a problem of integer identification.

The problem is :
" I have a numerical function y = f(x1,x2,x3) where x1...x3 are integers. I
would like to determine x1,x2,x3 so that "y" has a minimal value. I know
that
R can determine a minimal value when x1,x2,x3 are real. Is-it possible to do
this when x1, x2, x3 are restricted to be integers ? "

Ludo




----- Original Message -----
From: "Uwe Ligges" <ligges at statistik.uni-dortmund.de>
To: "G??ran Brostr??m" <gb at stat.umu.se>
Cc: <r-help at stat.math.ethz.ch>
Sent: Tuesday, November 02, 2004 3:44 PM
Subject: Re: [R] integer


> G??ran Brostr??m wrote:
>
> > On Tue, Nov 02, 2004 at 11:36:27AM +0100, Uwe Ligges wrote:
> >
> >>Ludovic Tambour wrote:
> >>
> >>
> >>>Hello,
> >>>
> >>>I need to use "R" to determine parameters which are integers. How I can
do
> >>>this, please ?
> >>
> >>What so you mean with "parameters"? In which context?
> >>
> >>To check whether a numeric vector "x" contains only integers, you can
try
> >>
> >>all.equal(as.integer(x), x)
> >
> >
> > I don't think so:
> >
> >
> >>x <- as.double(c(1, 2))
> >>y <- as.integer(c(1, 2))
> >>all.equal(x, y)
> >
> > [1] TRUE
> >
> > But,
> >
> >
> >>identical(x, y)
> >
> > [1] FALSE
>
> [The story was completely different from the stuff I guessed, so all
> further communication related to this thread is "academic".]
>
> G??ran,
>
> yes, as expected.
>
>
> > On the other hand, why not use
> >
> >
> >>is.integer(x)
> >
> > [1] FALSE
> >
> >>is.integer(y)
> >
> > [1] TRUE
> >
> > because I think that a numeric vector can't have a mix of integer and
> > non-integer elements. With a list it's a different story.
>
> Yes.
>
> My guees was that the asker tried to identify integers such as 2, 3 in
> contrast to 2.1, 3.1, ...and you won't know it by looking at R's storage
> mode (my guess was that the asker was not interested in the storage
> mode, but in the nature of the numbers!).
> Note that is.integer(1) is FALSE!!!
>
> The given usage of all.equal() helps to identify 1 as an integer, but
> not 1.1...
>
> Uwe
>
> > G??ran
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ahenningsen at email.uni-kiel.de  Tue Nov  2 16:31:42 2004
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Tue, 2 Nov 2004 16:31:42 +0100
Subject: [R] make apply() return a list
In-Reply-To: <loom.20041102T152755-211@post.gmane.org>
References: <200411011937.08360.ahenningsen@email.uni-kiel.de>
	<200411021102.45894.ahenningsen@email.uni-kiel.de>
	<loom.20041102T152755-211@post.gmane.org>
Message-ID: <200411021631.42799.ahenningsen@email.uni-kiel.de>

On Tuesday 02 November 2004 15:29, Gabor Grothendieck wrote:
> Arne Henningsen <ahenningsen <at> email.uni-kiel.de> writes:
> : Hi,
> :
> : thank you very much Sundar, Patrick, Tony, Mahub and Gabor for your
> : helpful answers! All your examples work great. They are all more
> : straightforeward than my example and much faster than the for-loop.
> : These are the average elapsed times (in seconds) returned by
> : system.time()
>
> [3]
>
> : (applied to my real function and my real data):
> :
> : my original for-loop:
> : 5.55
> :
> : the example I presented in my previous email (using apply):
> : 2.35
> :
> : example suggested by Tony (using apply):
> : 2.34
> :
> : example suggested by Gabor (using lapply):
> : 2.50
> :
> : examples suggested by Sundar and Mahub (using lapply):
> : 2.68
>
> Perhaps any comparison should also include simplicity.  

Yes, you are totally right!

> This is 
> somewhat subjective but just to objectify it I have reworked
> each solution to compactify it as much as I could and then
> calculated the number of characters in each solution using wc:
>
> AH - 293 characters
> TP - 70 characters
> ML - 62 characters
> GG - 48 characters

Thank you for wotking this out. I was also thinking about simplicity when I 
compared the different suggestions. Finally I took Tonys suggestion although 
the code is a bit longer than the others in _your_ comparison. The reason was 
only partially the speed, but compared to ML and GG this code preserves the 
(col)names of the dataframe, which I need in the real "myFunction". 
Circumventing this small problem in ML's and GG's suggestions would make my 
code (a bit) longer than the code based on TP's suggestion.

Best wishes,
Arne

> The versions I used are below.
>
> ---
>
> # data
> myData <- data.frame( a = c( 1,2,3 ), b = c( 4,5,6 ) )
>
> # AH
> myFunction <- function( values ) {
>    myMatrix <- matrix( values, 2, 2 )
>    if( all( values == myData[ 1, ] ) ) {
>       myMatrix <- cbind( myMatrix, rep( 0, 2 ) )
>    }
>    return( myMatrix )
> }
> myList <- apply( myData, 1, myFunction )
> myList[[ 1 ]] <- myList[[ 1 ]][ 1:2, 1:2 ]
> myList
>
> # TP
> lapply(apply(myData, 1, function(x) list(matrix(x, 2, 2))), "[[", 1)
>
> # ML
> lapply(1:nrow(myData), function(i) matrix(myData[i,], 2, 2))
>
> # GG
> lapply(as.data.frame(t(myData)), matrix, 2, 2)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From nleonard at tartarus.uwa.edu.au  Tue Nov  2 16:51:16 2004
From: nleonard at tartarus.uwa.edu.au (Neil Leonard)
Date: Tue, 2 Nov 2004 23:51:16 +0800
Subject: [R] StepAIC with coxph
Message-ID: <07F2C6AA-2CE7-11D9-9726-003065D5B8EC@tartarus.uwa.edu.au>

Hi,

I'm having a bit of trouble with using StepAIC with a coxph model.

Can anybody tell me if there is anything wrong with what I am doing  
here (I've removed a few of the variables for the purpose of this  
email, I had about 20 before):

start<- 
coxph(Surv(entryage2,age_at_death_yr,death)~finih5+prev+magegp+zpluralgp 
,data=project_model1)
fmAIC1 <- stepAIC(start,direction="backward",scope=list(lower = ~  
1,upper = ~ finih5+prev+magegp+zpluralgp,data=project_model1))

This is the error I'm getting after it goes through a few times:

Error in stepAIC(start, direction = "backward", scope = list(lower =  
~1,  :
	number of rows in use has changed: remove missing values?
In addition: There were 24 warnings (use warnings() to see them)

The warning messages are all of the form:
1: X matrix deemed to be singular; variable 11 27 28 29 in:  
coxph(formula = Surv(entryage2, age_at_death_yr, death) ~ prev +   ...

These arise because there are no deaths for a particular variable and  
haven't been a problem when I have been creating my own coxph model.


Thanks,
Neil



From Amer.Siddique at ssa.gov  Tue Nov  2 16:51:17 2004
From: Amer.Siddique at ssa.gov (Siddique, Amer)
Date: Tue, 2 Nov 2004 10:51:17 -0500 
Subject: [R] using R in .NET apps
Message-ID: <03EC0EF2887E0C4D84748557F82F2E5B05B09899@sdf0cd1.ph.ssa.gov>

looking for some thoughts on incorporating R functionality to create
histograms of data stored in an informix db.  im gonna write the app in .Net
and will use a managed provider to access the data. what R libs might I have
to package in the assemblies? (sorry my Q is general as Ive only just looked
at wanting this yet)

Thanks. Amer.



From emathieu at iurc.montp.inserm.fr  Tue Nov  2 16:46:39 2004
From: emathieu at iurc.montp.inserm.fr (Eve Mathieu)
Date: Tue, 02 Nov 2004 16:46:39 +0100
Subject: [R] problem to solve a matrix
Message-ID: <5.0.2.1.2.20041102163447.00ae0b00@193.52.202.5>

Dear R group,

I have to solve a hessian matrix 40*40, called M, in order to obtain the 
standart deviations of estimators.

When I use the function solve(M), I have the following error message: 
"Error in solve.default(M) : Lapack routine dgesv: system is exactly singular"

Do you know an alternative approach which could succeed? I have found some 
information about the function "ginv" (library MASS), has someone already 
used it in such a situation?

Thank you very much for your responses.

Eve MATHIEU.



From wuko at mail.nih.gov  Tue Nov  2 17:19:33 2004
From: wuko at mail.nih.gov (Wu, Kotien (NIH/NCI))
Date: Tue, 2 Nov 2004 11:19:33 -0500 
Subject: [R] how to call function in ../src/main
Message-ID: <4CCA53563257AC478E6F764AC6CD08160105C038@nihexchange11.nih.gov>

Hi, This is Kotien Wu from NIH/NCI.

We want to use R function for our web:
http://cgap.nci.nih.gov

It works for functions in R/R-1.9.1/src/nmath/standalone very well.

We have function GetPvalueForT.c which has

#define MATHLIB_STANDALONE 1
#include <Rmath.h>
 
double GetPvalueForT ( double t, double deg ) {
  return 2 * pt(t, deg, 0, 0);
}

We create GetPvalueForT.i, which has

/* File : GetPvalueForT.i */
%module GetPvalueForT
 
extern double GetPvalueForT ( double t, double deg );

We use 

GetPvalueForT: $(srcdir)/GetPvalueForT.c
        swig -perl5 GetPvalueForT.i
        gcc $(ALL_CPPFLAGS) $(ALL_CFLAGS) -c GetPvalueForT.c
GetPvalueForT_wrap.c -I$(DCDFLIB)
 -I/usr/local/lib/perl5/5.8.2/sun4-solaris-thread-multi/CORE -Im
        ld -G GetPvalueForT.o -Im -L. -lRmath -lm GetPvalueForT_wrap.o -o
GetPvalueForT

in Makefile and we got GetPvalueForT.pm and GetPvalueForT and then we can
call it from perl:

test.pl:

#!/usr/local/bin/perl
use strict;
use GetPvalueForT;
 
sub test {
  my ( $t, $deg ) = @ARGV;
  my $result;
  for( my $i=0; $i<1000000; $i++ ) {
    $result = GetPvalueForT::GetPvalueForT($t, $deg);
  }
  my $temp = sprintf "%6.4f\n", $result;
  printf "result: $temp \n";
}
 
test();

It works fine.

But it looks not for calling functions in R/R-1.9.1/src/main

We want to correlation only, so we copy cov.c to cor.c
where use

cor ( int n, double *x, double *y, double *ans ) {
  Rboolean cor, kendall, pair, na_fail, sd_0;
  int ansmat, method, ncx, ncy;
 
  /* compute correlations if PRIMVAL(op) == 0,
             covariances  if PRIMVAL(op) != 0 */
  cor = 0;
 
  ansmat = 0;
 
  ncx = 1;
  ncy = 1;
  pair = TRUE;
 
  kendall = FALSE;
 
  cov_pairwise2(n, ncx, ncy, x, y, ans,
                          &sd_0, cor, kendall);
}

replace

SEXP do_cov(SEXP call, SEXP op, SEXP args, SEXP env)
{
  ...
}

and we have cor.i which has

/* File : cor.i */
%module cor
 
extern cor ( int n, double *x, double *y, double *ans );

We have

cor: cor.c
        swig -perl5 cor.i
        gcc -c cor.c cor_wrap.c -I. -I../../src/include -I/usr/local/include
-I$
(DCDFLIB) -I/usr/local/lib/perl5/5.8.2/sun4-solaris-thread-multi/CORE -Im
        ld -G cor.o -Im -L. -lRmath -lm cor_wrap.o -o cor

in Makefile and We compiled it and we got cor.pm and cor.

We have test.pl which has 

#!/usr/local/bin/perl
use strict;
use cor;
 
sub test {
  my $result;
  my @a = (1,2,3,4,5);
  my @b = (1,2,3,4,5);
  my $n = 5;
  cor( $n, \@a, \@b, \$result );
  my $temp = sprintf "%6.4f\n", $result;
  printf "result: $temp \n";
}
 
test();

We setup LD_LIBRARY_PASTH and then we run test.pl, we got:

$ test.pl
Can't load './cor' for module cor: ld.so.1: /usr/local/bin/perl: fatal:
relocation error: file ./cor: symbol R_NaReal: referenced symbol not found
at /usr/local/lib/perl5/5.8.2/sun4-solaris-thread-multi/DynaLoader.pm line
229.
 at cor.pm line 7
Compilation failed in require at test.pl line 3.
BEGIN failed--compilation aborted at test.pl line 3.

Do you know where is R_NaReal, what I did is wrong?

Thanks a lot!

Kotien



From ggrothendieck at myway.com  Tue Nov  2 17:25:54 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 2 Nov 2004 16:25:54 +0000 (UTC)
Subject: [R] make apply() return a list
References: <200411011937.08360.ahenningsen@email.uni-kiel.de>
	<200411021102.45894.ahenningsen@email.uni-kiel.de>
	<loom.20041102T152755-211@post.gmane.org>
	<200411021631.42799.ahenningsen@email.uni-kiel.de>
Message-ID: <loom.20041102T172233-741@post.gmane.org>

Arne Henningsen <ahenningsen <at> email.uni-kiel.de> writes:

: 
: On Tuesday 02 November 2004 15:29, Gabor Grothendieck wrote:
: > Arne Henningsen <ahenningsen <at> email.uni-kiel.de> writes:
: > : Hi,
: > :
: > : thank you very much Sundar, Patrick, Tony, Mahub and Gabor for your
: > : helpful answers! All your examples work great. They are all more
: > : straightforeward than my example and much faster than the for-loop.
: > : These are the average elapsed times (in seconds) returned by
: > : system.time()
: >
: > [3]
: >
: > : (applied to my real function and my real data):
: > :
: > : my original for-loop:
: > : 5.55
: > :
: > : the example I presented in my previous email (using apply):
: > : 2.35
: > :
: > : example suggested by Tony (using apply):
: > : 2.34
: > :
: > : example suggested by Gabor (using lapply):
: > : 2.50
: > :
: > : examples suggested by Sundar and Mahub (using lapply):
: > : 2.68
: >
: > Perhaps any comparison should also include simplicity.  
: 
: Yes, you are totally right!
: 
: > This is 
: > somewhat subjective but just to objectify it I have reworked
: > each solution to compactify it as much as I could and then
: > calculated the number of characters in each solution using wc:
: >
: > AH - 293 characters
: > TP - 70 characters
: > ML - 62 characters
: > GG - 48 characters
: 
: Thank you for wotking this out. I was also thinking about simplicity when I 
: compared the different suggestions. Finally I took Tonys suggestion although 
: the code is a bit longer than the others in _your_ comparison. The reason 
was 
: only partially the speed, but compared to ML and GG this code preserves the 
: (col)names of the dataframe, which I need in the real "myFunction". 
: Circumventing this small problem in ML's and GG's suggestions would make my 
: code (a bit) longer than the code based on TP's suggestion.

Note that neither the problem statement nor your example code
showing what the solution should look like (nor my compactified versions 
of the responses) exhibit colnames.

: 
: Best wishes,
: Arne
: 
: > The versions I used are below.
: >
: > ---
: >
: > # data
: > myData <- data.frame( a = c( 1,2,3 ), b = c( 4,5,6 ) )
: >
: > # AH
: > myFunction <- function( values ) {
: >    myMatrix <- matrix( values, 2, 2 )
: >    if( all( values == myData[ 1, ] ) ) {
: >       myMatrix <- cbind( myMatrix, rep( 0, 2 ) )
: >    }
: >    return( myMatrix )
: > }
: > myList <- apply( myData, 1, myFunction )
: > myList[[ 1 ]] <- myList[[ 1 ]][ 1:2, 1:2 ]
: > myList
: >
: > # TP
: > lapply(apply(myData, 1, function(x) list(matrix(x, 2, 2))), "[[", 1)
: >
: > # ML
: > lapply(1:nrow(myData), function(i) matrix(myData[i,], 2, 2))
: >
: > # GG
: > lapply(as.data.frame(t(myData)), matrix, 2, 2)



From ripley at stats.ox.ac.uk  Tue Nov  2 17:45:00 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 2 Nov 2004 16:45:00 +0000 (GMT)
Subject: [R] using R in .NET apps
In-Reply-To: <03EC0EF2887E0C4D84748557F82F2E5B05B09899@sdf0cd1.ph.ssa.gov>
Message-ID: <Pine.LNX.4.44.0411021639510.31024-100000@gannet.stats>

On Tue, 2 Nov 2004, Siddique, Amer wrote:

> looking for some thoughts on incorporating R functionality to create
> histograms of data stored in an informix db.  im gonna write the app in .Net
> and will use a managed provider to access the data. what R libs might I have
> to package in the assemblies? (sorry my Q is general as Ive only just looked
> at wanting this yet)

Could you translate the question into English?  My guess is that you are
writing in a lower-cased version of some Microsoft internal language, but
you don't even mention that this is for Windows (if it is).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Nov  2 17:50:13 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 2 Nov 2004 16:50:13 +0000 (GMT)
Subject: [R] problem to solve a matrix
In-Reply-To: <5.0.2.1.2.20041102163447.00ae0b00@193.52.202.5>
Message-ID: <Pine.LNX.4.44.0411021645200.31024-100000@gannet.stats>

On Tue, 2 Nov 2004, Eve Mathieu wrote:

> Dear R group,
> 
> I have to solve a hessian matrix 40*40, called M, in order to obtain the 
> standart deviations of estimators.
> 
> When I use the function solve(M), I have the following error message: 
> "Error in solve.default(M) : Lapack routine dgesv: system is exactly singular"

Note that a Hessian matrix is symmetric and positive definite, so 
special-purpose methods are appropriate.  But that warning normally means 
exactly what it says: the matrix is singular and hence not invertible.

> Do you know an alternative approach which could succeed? I have found some 
> information about the function "ginv" (library MASS), has someone already 
> used it in such a situation?

Yes, for an exactly singular Hessian, but I knew why it was singular and 
that I wanted to ignore the two singular directions since variation along 
them would be removed at a later stage.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Tobias.Lindenberg at attocube.com  Tue Nov  2 17:52:31 2004
From: Tobias.Lindenberg at attocube.com (Tobias Lindenberg)
Date: Tue, 2 Nov 2004 17:52:31 +0100
Subject: [R] R data editor
Message-ID: <6369798F399F2B45BEA47F00805E7B070BD0C2@acss1.acs.attocube.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041102/ced890fc/attachment.pl

From br44114 at yahoo.com  Tue Nov  2 17:58:46 2004
From: br44114 at yahoo.com (bogdan romocea)
Date: Tue, 2 Nov 2004 08:58:46 -0800 (PST)
Subject: [R] plot time series / dates (basic)
In-Reply-To: <Pine.LNX.4.44.0411012147560.22833-100000@gannet.stats>
Message-ID: <20041102165847.6341.qmail@web50308.mail.yahoo.com>

Thank you for the suggestions. I managed to fix everything except the
first part. 
	dat <- date[(j-1)*points+1):(j*points)]
causes a syntax error. If I do 
	dat <- vector() 
I end up with numbers (which is fine by me - just like SAS dates).
However, after checking a couple of sources I still have no idea how
to format numbers as dates (for plotting/printing). Does anyone have
an example for formatting 12710 (# of days since 1 Jan 1970) as
19-Oct-04 (in the x axis of a plot)?

Regards,
b.


#function to plot a long time series piece by piece
pl <- function(vara, varb, points)
	{
	date <- as.Date(as.character(Date), "%d-%b-%y")
	pr1 <- vector(mode="numeric")
	pr2 <- vector(mode="numeric")
	#dat <- vector()
	dat <- date[(j-1)*points+1):(j*points)]
	for (j in 1:(round(length(Vol)/points)+1)) #number of plots
		{
		for (i in ((j-1)*points+1):(j*points)) 
			{
			pr1[i-points*(j-1)] <- vara[i]
			pr2[i-points*(j-1)] <- varb[i]
			#dat[i-points*(j-1)] <- date[i]
			#dat <- date[i]
			}
		par(mfrow=c(2,1), mai=c(0.4, 0.5, 0.3, 0.1), omi=c(0.2, 0, 0, 0), 
			cex.axis=0.7, cex=1.2, cex.main=0.7, pch="*") 
		plot(dat, pr1, main=deparse(substitute(vara)), type="o")
		#axis.Date(1,dat,format="%b%y") 
		plot(dat, pr2, main=deparse(substitute(varb)), type="o")
		}
	}





--- Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:

> On Mon, 1 Nov 2004, bogdan romocea wrote:
> 
> > Dear R users,
> > 
> > I'm having a hard time with some very simple things. I have a
> time
> > series where the dates are in the format 7-Oct-04. 
> 
> So why use as.POSIXct for a date, rather than as.Date?
> 
> > I imported the
> > file with read.csv so the date column is a factor. The series is
> > rather long and I want to plot it piece by piece. The function
> below
> > works fine, except that the labels for date are meaningless (ie
> > 9.47e+08 or 1098000000 - apparently the number of seconds since
> > whatever). I don't want to convert the data frame to a ts object
> > because there are missing days and I don't want any
> interpolation.
> > 
> > 1. How do I replace the date labels with something like 'Mar04',
> > instead of 9.47e+08 / 1098000000?
> 
> Just don't convert them to that format.  You set up
> 
> >       dat <- vector()
> 
> which is not a dates object.  If you use standard R indexing, it
> will
> work. If you throw the class away, it will not.  Try
> 
>        dat <- date[(j-1)*points+1):(j*points)]
> 
> etc (no for loop required).
> 
> If you want a different format, see ?axis.Date
> 
> > 2. In the PDF file, the space between the two graphs printed pair
> by
> > pair is fairly large. Can I remove/reduce the area that seems
> > reserved for Title and X label so that, on a page, the space
> between
> > the graph at the top and the one at the bottom is minimized?
> 
> There's a whole chapter on this in `An Introduction to R': have you
> read 
> it?
> 
> > 3. Given the function below, I haven't discovered a way to have
> > "vara" appear as the Title or Y label in graphs.
> > main=as.character(vara) lists all the values of vara (which is a
> > column from the data frame d). So, how can I use the name of a
> vector
> > as title or label in a plot?
> 
> That's almost an FAQ.  Use deparse(substitute(vara))
> 
> > d <- ('data.csv', header = T, sep = ",", quote="", dec=".", 
> > 	fill = T, skip=0)
> > attach(d)
> > #function to plot a long time series piece by piece
> > pl <- function(vara, varb, points)
> > 	{
> > 	date <- as.POSIXct(strptime(as.character(Date), "%d-%b-%y"), tz
> =
> > "GMT")
> > 	pr1 <- vector(mode="numeric")
> > 	pr2 <- vector(mode="numeric")
> > 	dat <- vector()
> > 	for (j in 1:(round(length(Vol)/points)+1)) #number of plots
> > 		{
> > 		for (i in ((j-1)*points+1):(j*points)) 
> > 			{
> > 			pr1[i-points*(j-1)] <- vara[i]
> > 			pr2[i-points*(j-1)] <- varb[i]
> > 			dat[i-points*(j-1)] <- date[i]
> > 			}
> > 		par(mfrow=c(2,1)) 
> > 		plot(dat, pr1, type="b")
> > 		plot(dat, pr2, type="b")
> > 		}
> > 	}
> > 
> > pdf("Rplots.pdf")
> > pl(Vol, atr, 50)
> > dev.off()
> 
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics, 
> http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> 



		
__________________________________ 


From pburns at pburns.seanet.com  Tue Nov  2 18:12:55 2004
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Tue, 02 Nov 2004 17:12:55 +0000
Subject: [R] integer
In-Reply-To: <007501c4c0ef$aec6d700$47ecc7c2@cirad.fr>
References: <003101c4c0c5$189dc6a0$47ecc7c2@cirad.fr>	<4187632B.2020000@statistik.uni-dortmund.de><20041102134129.GA31724@stat.umu.se>	<41879D31.2090404@statistik.uni-dortmund.de>
	<007501c4c0ef$aec6d700$47ecc7c2@cirad.fr>
Message-ID: <4187C017.6060303@pburns.seanet.com>

Yes, it is possible -- there are two steps:

1)  Create a suitable function.  This is likely to look something like:

f.wrapper <- function(pars)
{
    ipars <- round(pars)
    f(ipars[1], ipars[2], ipars[3])
}

2) Find a suitable optimizer. 

A genetic algorithm, as you alluded to earlier, is a likely choice. 
Choices include "genopt" from S Poetry (you can extract "genopt"
and "genopt.control" from the shar file).

Another possibility is simulated annealing which is available via
the "optim" function.

Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Ludovic Tambour wrote:

>Sorry, the formulation of my question is bad. I hope that you have not lost
>your time. The problem is not a problem of integer identification.
>
>The problem is :
>" I have a numerical function y = f(x1,x2,x3) where x1...x3 are integers. I
>would like to determine x1,x2,x3 so that "y" has a minimal value. I know
>that
>R can determine a minimal value when x1,x2,x3 are real. Is-it possible to do
>this when x1, x2, x3 are restricted to be integers ? "
>
>Ludo
>
>
>
>
>----- Original Message -----
>From: "Uwe Ligges" <ligges at statistik.uni-dortmund.de>
>To: "G??ran Brostr??m" <gb at stat.umu.se>
>Cc: <r-help at stat.math.ethz.ch>
>Sent: Tuesday, November 02, 2004 3:44 PM
>Subject: Re: [R] integer
>
>
>  
>
>>G??ran Brostr??m wrote:
>>
>>    
>>
>>>On Tue, Nov 02, 2004 at 11:36:27AM +0100, Uwe Ligges wrote:
>>>
>>>      
>>>
>>>>Ludovic Tambour wrote:
>>>>
>>>>
>>>>        
>>>>
>>>>>Hello,
>>>>>
>>>>>I need to use "R" to determine parameters which are integers. How I can
>>>>>          
>>>>>
>do
>  
>
>>>>>this, please ?
>>>>>          
>>>>>
>>>>What so you mean with "parameters"? In which context?
>>>>
>>>>To check whether a numeric vector "x" contains only integers, you can
>>>>        
>>>>
>try
>  
>
>>>>all.equal(as.integer(x), x)
>>>>        
>>>>
>>>I don't think so:
>>>
>>>
>>>      
>>>
>>>>x <- as.double(c(1, 2))
>>>>y <- as.integer(c(1, 2))
>>>>all.equal(x, y)
>>>>        
>>>>
>>>[1] TRUE
>>>
>>>But,
>>>
>>>
>>>      
>>>
>>>>identical(x, y)
>>>>        
>>>>
>>>[1] FALSE
>>>      
>>>
>>[The story was completely different from the stuff I guessed, so all
>>further communication related to this thread is "academic".]
>>
>>G??ran,
>>
>>yes, as expected.
>>
>>
>>    
>>
>>>On the other hand, why not use
>>>
>>>
>>>      
>>>
>>>>is.integer(x)
>>>>        
>>>>
>>>[1] FALSE
>>>
>>>      
>>>
>>>>is.integer(y)
>>>>        
>>>>
>>>[1] TRUE
>>>
>>>because I think that a numeric vector can't have a mix of integer and
>>>non-integer elements. With a list it's a different story.
>>>      
>>>
>>Yes.
>>
>>My guees was that the asker tried to identify integers such as 2, 3 in
>>contrast to 2.1, 3.1, ...and you won't know it by looking at R's storage
>>mode (my guess was that the asker was not interested in the storage
>>mode, but in the nature of the numbers!).
>>Note that is.integer(1) is FALSE!!!
>>
>>The given usage of all.equal() helps to identify 1 as an integer, but
>>not 1.1...
>>
>>Uwe
>>
>>    
>>
>>>G??ran
>>>      
>>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>>    
>>
>http://www.R-project.org/posting-guide.html
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>



From Amer.Siddique at ssa.gov  Tue Nov  2 18:14:46 2004
From: Amer.Siddique at ssa.gov (Siddique, Amer)
Date: Tue, 2 Nov 2004 12:14:46 -0500 
Subject: [R] using R in .NET apps
Message-ID: <03EC0EF2887E0C4D84748557F82F2E5B05B098A2@sdf0cd1.ph.ssa.gov>

I am writing a windows XP app which utilizes the .NET framework. my language
is VB.NET. the Visual basic (VB) compiler will translate VB source code into
microsoft intermediate language (MSIL), thus producing native code on XP.

if I have a multi-tier app with a lower layer datastore (IBM Informix using
ODBC) and a middle tier (for logic and computation), I would like to
incorporate R functionality at this stage and send output to the front end
(display charts, etc.).  

Currently, the .NET IDE incorporates the "Crystal Report" engine for such
needs.  I would like to use R in a similar manner. Perhaps I could expose it
as a windows service (using DCOM?), thus allowing me to programmaticaly
access its abilities within the .NET framework.

HTH. Amer    

> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Sent: Tuesday, November 02, 2004 11:45 AM
> To: Siddique, Amer
> Cc: 'r-help at stat.math.ethz.ch'
> Subject: Re: [R] using R in .NET apps
> 
> On Tue, 2 Nov 2004, Siddique, Amer wrote:
> 
> > looking for some thoughts on incorporating R functionality to create
> > histograms of data stored in an informix db.  im gonna write the app
> in .Net
> > and will use a managed provider to access the data. what R libs might I
> have
> > to package in the assemblies? (sorry my Q is general as Ive only just
> looked
> > at wanting this yet)
> 
> Could you translate the question into English?  My guess is that you are
> writing in a lower-cased version of some Microsoft internal language, but
> you don't even mention that this is for Windows (if it is).
> 
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at myway.com  Tue Nov  2 18:20:36 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 2 Nov 2004 17:20:36 +0000 (UTC)
Subject: [R] integer
References: <003101c4c0c5$189dc6a0$47ecc7c2@cirad.fr>	<4187632B.2020000@statistik.uni-dortmund.de><20041102134129.GA31724@stat.umu.se>
	<41879D31.2090404@statistik.uni-dortmund.de>
	<007501c4c0ef$aec6d700$47ecc7c2@cirad.fr>
Message-ID: <loom.20041102T181946-920@post.gmane.org>


Ludovic Tambour ludovic.tambour at cirad.fr 

: The problem is :
: " I have a numerical function y = f(x1,x2,x3) where x1...x3 are integers. I
: would like to determine x1,x2,x3 so that "y" has a minimal value. I know
: that
: R can determine a minimal value when x1,x2,x3 are real. Is-it possible to do
: this when x1, x2, x3 are restricted to be integers ? "

Will brute force do?

R> g <- expand.grid(x1 = 0:10, x2 = 0:10, x3 = 0:10)
R> f <- function(x) sum(x*x)
R> g[which.min(apply(g, 1, f)),]
  x1 x2 x3
1  0  0  0



From ripley at stats.ox.ac.uk  Tue Nov  2 18:25:00 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 2 Nov 2004 17:25:00 +0000 (GMT)
Subject: [R] using R in .NET apps
In-Reply-To: <03EC0EF2887E0C4D84748557F82F2E5B05B098A2@sdf0cd1.ph.ssa.gov>
Message-ID: <Pine.LNX.4.44.0411021723001.31139-100000@gannet.stats>

On Tue, 2 Nov 2004, Siddique, Amer wrote:

> I am writing a windows XP app which utilizes the .NET framework. my language
> is VB.NET. the Visual basic (VB) compiler will translate VB source code into
> microsoft intermediate language (MSIL), thus producing native code on XP.
> 
> if I have a multi-tier app with a lower layer datastore (IBM Informix using
> ODBC) and a middle tier (for logic and computation), I would like to
> incorporate R functionality at this stage and send output to the front end
> (display charts, etc.).  
> 
> Currently, the .NET IDE incorporates the "Crystal Report" engine for such
> needs.  I would like to use R in a similar manner. Perhaps I could expose it
> as a windows service (using DCOM?), thus allowing me to programmaticaly

Yes, and that is in the appropriate FAQ, Q2.16.  (The R posting guide does
ask you to consult the FAQs.)

> access its abilities within the .NET framework.
> 
> HTH. Amer    
> 
> > -----Original Message-----
> > From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> > Sent: Tuesday, November 02, 2004 11:45 AM
> > To: Siddique, Amer
> > Cc: 'r-help at stat.math.ethz.ch'
> > Subject: Re: [R] using R in .NET apps
> > 
> > On Tue, 2 Nov 2004, Siddique, Amer wrote:
> > 
> > > looking for some thoughts on incorporating R functionality to create
> > > histograms of data stored in an informix db.  im gonna write the app
> > in .Net
> > > and will use a managed provider to access the data. what R libs might I
> > have
> > > to package in the assemblies? (sorry my Q is general as Ive only just
> > looked
> > > at wanting this yet)
> > 
> > Could you translate the question into English?  My guess is that you are
> > writing in a lower-cased version of some Microsoft internal language, but
> > you don't even mention that this is for Windows (if it is).
> > 
> > --
> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865 272861 (self)
> > 1 South Parks Road,                     +44 1865 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at myway.com  Tue Nov  2 19:08:22 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 2 Nov 2004 18:08:22 +0000 (UTC)
Subject: [R] plot time series / dates (basic)
References: <Pine.LNX.4.44.0411012147560.22833-100000@gannet.stats>
	<20041102165847.6341.qmail@web50308.mail.yahoo.com>
Message-ID: <loom.20041102T190227-840@post.gmane.org>

> Thank you for the suggestions. I managed to fix everything except the
: first part. 
: 	dat <- date[(j-1)*points+1):(j*points)]
: causes a syntax error. If I do 

You have unbalanced parentheses.

: 	dat <- vector() 
: I end up with numbers (which is fine by me - just like SAS dates).
: However, after checking a couple of sources I still have no idea how
: to format numbers as dates (for plotting/printing). Does anyone have
: an example for formatting 12710 (# of days since 1 Jan 1970) as
: 19-Oct-04 (in the x axis of a plot)?

You can use chron or Date classes:

	library(chron)
	dd <- chron(12710:12721, out.format = "dd-mmm-yy")
	plot(dd, 1:12)
or
	as.Date.integer <- function(x) structure(x, class = "Date")
	dd <- as.Date(12710:12721)
	plot(dd, 1:12)

If you don't like that labelling you can use axis to set up
your own.  Continuing the last example:

	plot(dd, 1:12, xaxt = "n")
	axis.Date(1, dd, format = "%d-%b-%y", cex.axis = .5)

see ?axis, ?strptime and the Help Desk article in R News 4/1.



From spencer.graves at pdf.com  Tue Nov  2 19:22:02 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 02 Nov 2004 10:22:02 -0800
Subject: [R] integer
In-Reply-To: <007501c4c0ef$aec6d700$47ecc7c2@cirad.fr>
References: <003101c4c0c5$189dc6a0$47ecc7c2@cirad.fr>	<4187632B.2020000@statistik.uni-dortmund.de><20041102134129.GA31724@stat.umu.se>	<41879D31.2090404@statistik.uni-dortmund.de>
	<007501c4c0ef$aec6d700$47ecc7c2@cirad.fr>
Message-ID: <4187D04A.6050506@pdf.com>

      I just went to www.r-project.org -> search -> "R site search" -> 
"integer programming".  This brought 99 hits, the 23rd of which referred 
to function "lp" in package "lpSolve".  This may solve the problem if 
"f" in linear. 

      Otherwise, if f is defined and reasonably well behaved for non 
integer values, one could find the minimum over non integers then search 
all combinations of integers near the non integer minimum.

      hope this helps. 
      spencer graves

Ludovic Tambour wrote:

>Sorry, the formulation of my question is bad. I hope that you have not lost
>your time. The problem is not a problem of integer identification.
>
>The problem is :
>" I have a numerical function y = f(x1,x2,x3) where x1...x3 are integers. I
>would like to determine x1,x2,x3 so that "y" has a minimal value. I know
>that
>R can determine a minimal value when x1,x2,x3 are real. Is-it possible to do
>this when x1, x2, x3 are restricted to be integers ? "
>
>Ludo
>
>
>
>
>----- Original Message -----
>From: "Uwe Ligges" <ligges at statistik.uni-dortmund.de>
>To: "G??ran Brostr??m" <gb at stat.umu.se>
>Cc: <r-help at stat.math.ethz.ch>
>Sent: Tuesday, November 02, 2004 3:44 PM
>Subject: Re: [R] integer
>
>
>  
>
>>G??ran Brostr??m wrote:
>>
>>    
>>
>>>On Tue, Nov 02, 2004 at 11:36:27AM +0100, Uwe Ligges wrote:
>>>
>>>      
>>>
>>>>Ludovic Tambour wrote:
>>>>
>>>>
>>>>        
>>>>
>>>>>Hello,
>>>>>
>>>>>I need to use "R" to determine parameters which are integers. How I can
>>>>>          
>>>>>
>do
>  
>
>>>>>this, please ?
>>>>>          
>>>>>
>>>>What so you mean with "parameters"? In which context?
>>>>
>>>>To check whether a numeric vector "x" contains only integers, you can
>>>>        
>>>>
>try
>  
>
>>>>all.equal(as.integer(x), x)
>>>>        
>>>>
>>>I don't think so:
>>>
>>>
>>>      
>>>
>>>>x <- as.double(c(1, 2))
>>>>y <- as.integer(c(1, 2))
>>>>all.equal(x, y)
>>>>        
>>>>
>>>[1] TRUE
>>>
>>>But,
>>>
>>>
>>>      
>>>
>>>>identical(x, y)
>>>>        
>>>>
>>>[1] FALSE
>>>      
>>>
>>[The story was completely different from the stuff I guessed, so all
>>further communication related to this thread is "academic".]
>>
>>G??ran,
>>
>>yes, as expected.
>>
>>
>>    
>>
>>>On the other hand, why not use
>>>
>>>
>>>      
>>>
>>>>is.integer(x)
>>>>        
>>>>
>>>[1] FALSE
>>>
>>>      
>>>
>>>>is.integer(y)
>>>>        
>>>>
>>>[1] TRUE
>>>
>>>because I think that a numeric vector can't have a mix of integer and
>>>non-integer elements. With a list it's a different story.
>>>      
>>>
>>Yes.
>>
>>My guees was that the asker tried to identify integers such as 2, 3 in
>>contrast to 2.1, 3.1, ...and you won't know it by looking at R's storage
>>mode (my guess was that the asker was not interested in the storage
>>mode, but in the nature of the numbers!).
>>Note that is.integer(1) is FALSE!!!
>>
>>The given usage of all.equal() helps to identify 1 as an integer, but
>>not 1.1...
>>
>>Uwe
>>
>>    
>>
>>>G??ran
>>>      
>>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>>    
>>
>http://www.R-project.org/posting-guide.html
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From das at cshl.edu  Tue Nov  2 19:35:28 2004
From: das at cshl.edu (Rajdeep Das)
Date: Tue, 2 Nov 2004 13:35:28 -0500
Subject: [R] Feature selection
References: <Pine.GSO.4.33.0410252008260.8990-100000@cs.uprm.edu>
Message-ID: <012401c4c10a$b9ef7bd0$6807308f@artney>

Hi Edgar,
Thanks. "dprep" module works fine. However, I have one question. How does 
SFFS or SFS function know the class definition in the data?  Function call 
doesn't require to specify the column that identifies the class type (as 
shown inthe example). Can you eleborate on  this?
Thanks.
Rajdeep
----- Original Message ----- 
From: "Edgar Acuna" <edgar at cs.uprm.edu>
To: "Rajdeep Das" <das at cshl.edu>
Cc: <r-help at stat.math.ethz.ch>
Sent: Monday, October 25, 2004 7:11 PM
Subject: Re: [R] Feature selection


> Raj,
> look at academic.uprm.edu/eacuna/softw.htm, I built a library of R
> functions for data preprocessing tasks including feature selection
> for supervised classification.
> Please send me your comments.
>
> Edgar
>
> On Mon, 25 Oct 2004, Rajdeep Das wrote:
>
>> Hello,
>> I want to do feature selection for classification purpose (using lda). 
>> Can someone point me to any R package or S-plus package for this? 
>> Something like SFS or SFFS method would be useful for me.
>> Thanks.
>> Raj
>> [[alternative HTML version deleted]]
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>



From p.dalgaard at biostat.ku.dk  Tue Nov  2 20:19:07 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 02 Nov 2004 20:19:07 +0100
Subject: [R] plot time series / dates (basic)
In-Reply-To: <20041102165847.6341.qmail@web50308.mail.yahoo.com>
References: <20041102165847.6341.qmail@web50308.mail.yahoo.com>
Message-ID: <x2pt2wf49g.fsf@biostat.ku.dk>

bogdan romocea <br44114 at yahoo.com> writes:

> Thank you for the suggestions. I managed to fix everything except the
> first part. 
> 	dat <- date[(j-1)*points+1):(j*points)]
> causes a syntax error. If I do 
> 	dat <- vector() 
> I end up with...

Why not just fix the syntax error? Can't take that long to spot that
there are more ")" than "(" in that line, so presumably what was meant
was

  dat <- date[((j-1)*points+1):(j*points)]

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From mathdoc2be at yahoo.com  Tue Nov  2 20:23:32 2004
From: mathdoc2be at yahoo.com (Cal Tate)
Date: Tue, 2 Nov 2004 11:23:32 -0800 (PST)
Subject: [R] Problems with Durbin Watson and Partial Residual Plots
Message-ID: <20041102192332.36023.qmail@web50503.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041102/7d2d56f5/attachment.pl

From david.whiting at ncl.ac.uk  Tue Nov  2 20:41:01 2004
From: david.whiting at ncl.ac.uk (David Whiting)
Date: 02 Nov 2004 19:41:01 +0000
Subject: [R] A little more on R, mdbtools and Access databases
Message-ID: <m2hdo89gz6.fsf@ganymede.ammp.or.tz>


Hi again,

I have played a little more with mdbtools and R. I downloaded the
latest version of mdbtools from sourceforge (version 0.6pre1). Quickly
scanning the mailing list suggests that ODBC seems to work with PHP
but I have not been able to get it to work with R. I can make a
connection to the database and when I do a query I get back the names
of the rows but not the data. I must admit I have not spent long
trying to figure it out.

For my own use I am able to work directly with a database file on my
local machine, and to make things easier for myself I have
concentrated on making some simple functions (based on those I posted
yesterday) that use mdbtools to:

i) get table names, 
ii) describe tables, 
iii) read tables into R, and
iv) use the (basic) SQL functionality of mdbtools to perform simple
queries (really only able to select subsets of columns and rows)

Here is an example session using a database I found on the web at
http://www.microsoft-accesssolutions.co.uk/downloads/login.zip


> db <- "/home/dave/tmp/login.mdb"
> mdbTables(db)
[1] "MSysObjects"       "MSysACEs"          "MSysQueries"      
[4] "MSysRelationships" "MSysAccessObjects" "tblEmployees"     
> mytab <- mdbTables(db)[6]
> mytab
[1] "tblEmployees"
> x <- mdbReadTable(db, mytab)
> str(x)
`data.frame':	4 obs. of  4 variables:
 $ lngEmpID      : int  1 2 3 4
 $ strEmpName    : Factor w/ 4 levels "David","Gavin",..: 3 2 4 1
 $ strEmpPassword: Factor w/ 4 levels "david","gavin",..: 3 2 4 1
 $ strAccess     : Factor w/ 2 levels "Admin","User": 1 2 2 2
> head(x)
  lngEmpID strEmpName strEmpPassword strAccess
1        1     Graham         graham     Admin
2        2      Gavin          gavin      User
3        3      Lynne          lynne      User
4        4      David          david      User
> mdbDescribe(db, mytab)
      ColumnName         Type Size
1       lngEmpID Long Integer    4
2     strEmpName         Text   20
3 strEmpPassword         Text   20
4      strAccess         Text   40

> mdbQuery(db, "select lngEmpID, strAccess FROM tblEmployees where lngEmpID < 3")
  lngEmpID strAccess
1        1     Admin
2        2      User
> 









Here are the functions:

### Some quick code to make use of mdb-tools to use MS Access tables in R.
### 2004-11-02
### David Whiting


require(gdata) # for the trim function.

mdbTables <- function(dbname) {
  system(paste("mdb-tables -d '\t' -S", dbname), intern=TRUE)
}


mdbReadTable <- function(dbname,tableName) {
  tableName <- dQuote(tableName)
  read.table(pipe(paste("mdb-export -d '\t' ", dbname,  tableName)), sep="\t", header=TRUE)
}


mdbDescribe <- function(dbname,tableName) {
  tableName <- dQuote(tableName)
  cat("describe table ", tableName, "\ngo", file = "tempR.sql")
  mdesc <- system(paste("mdb-sql -i tempR.sql ", dbname), intern=TRUE)
  mdesc <- strsplit(substring(mdesc[-c(1:3,5, length(mdesc), length(mdesc)-1)], 2), "\\|")
  tabDesc <- rbind(mdesc[[2]])
  for (i in 3:length(mdesc)) {
    tabDesc <- rbind(tabDesc, mdesc[[i]])
  }
  tabDesc <- matrix(trim(tabDesc), ncol=3)
  tabDesc <- data.frame(tabDesc)
  names(tabDesc) <- c("ColumnName", "Type", "Size")
  tabDesc$Size <- as.numeric(levels(tabDesc$Size)[tabDesc$Size])
  system("rm -f tempR.sql")
  tabDesc
}


mdbQuery <- function(dbname, mstatement, header=FALSE, footer=FALSE) {
  cat(mstatement, "\ngo", file = "tempR.sql")
  sqlOptions <- "-p"
  if (!header) sqlOptions <- paste(sqlOptions, "H", sep="")
  if (!footer) sqlOptions <- paste(sqlOptions, "F", sep="")
  sqlStatement <- paste("mdb-sql", sqlOptions)
  tmp <- read.table(pipe(paste(sqlStatement, "-i tempR.sql", dbname)), sep="\t")
  names(tmp) <- trim(unlist(strsplit(substr(mstatement, 7, regexpr(" [Ff][Rr][Oo][Mm]", mstatement)[1]), ",")))
  system("rm -f tempR.sql")
  tmp
}


 


-- 
David Whiting
University of Newcastle upon Tyne, UK



From jfox at mcmaster.ca  Tue Nov  2 21:23:23 2004
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 2 Nov 2004 15:23:23 -0500
Subject: [R] Problems with Durbin Watson and Partial Residual Plots
In-Reply-To: <20041102192332.36023.qmail@web50503.mail.yahoo.com>
Message-ID: <20041102202322.RMII4905.tomts13-srv.bellnexxia.net@JohnDesktop8300>

Dear Cal,

The functions that you mention are in the car package.

The problems that you've encountered seem very odd to me. For example, if
you take a look at durbin.watson.lm(), you'll see that the code producing
the errors is quite straight-forward; it just extracts residuals from the
model and checks for NAs:

    residuals <- residuals(model)
    if (any(is.na(residuals))) 
        stop("residuals include missing values") 

Likewise, what's going on inside of cr.plot.lm() is also pretty simple.

Can you send the data set on which this regression was based?

John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Cal Tate
> Sent: Tuesday, November 02, 2004 2:24 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Problems with Durbin Watson and Partial Residual Plots
> 
> I am trying to evaluate a model by using the commands 
> durbin.watson and cr.plot.
> However, I keep getting errors that I can't figure out. A 
> description follows. Does anyone have a hint as to what may be wrong?
>  
>  
> 1)The Durbin Watson Test. In running the command I kept 
> getting the message "residuals include missing values" when 
> actually this was NOT the case.
>  
> Example:
> >durbin.watson(hw8LM0)
> Error in durbin.watson.lm(hw8LM0) : residuals include missing values
> 
> (2)Partial Residual Plots: Here I kept getting that the 
> variables I choose are not in the model, i.e. "Time is not in 
> the model" when it clearly is.
> 
> Example:
> >cr.plot(hw8LM0,variable="Time")
> Error in cr.plot.lm(hw8LM0, variable = "Time") : 
>         Time is not in the model.
> 
> Here is the model and residuals:
>  summary(hw8LM0)
> Call:
> lm(formula = Sales ~ Time)
> Residuals:
>     Min      1Q  Median      3Q     Max 
> -516.32 -292.95  -29.15  238.48  895.09
> Coefficients:
>             Estimate Std. Error t value Pr(>|t|)    
> (Intercept) 1418.872    122.464   11.59 2.35e-14 ***
> Time          73.278      4.962   14.77  < 2e-16 ***
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 
> 1 Residual standard error: 389.8 on 40 degrees of freedom
> Multiple R-Squared: 0.845,      Adjusted R-squared: 0.8411 
> F-statistic: 218.1 on 1 and 40 DF,  p-value: < 2.2e-16 
> 
> hw8LM0$res
>           1           2           3           4           5   
>         6 
>  242.679535  679.531369  895.093204  442.975038 -237.443127  
> 245.868708 
>           7           8           9          10          11   
>        12 
>   82.540542  -13.347623 -209.325789  161.976046  -96.612120 
> -271.380285 
>          13          14          15          16          17   
>        18 
> -460.888451 -113.606616 -311.494782 -417.352947 -516.321113   
>  1.430722 
>          19          20          21          22          23   
>        24 
>  -18.407443 -328.425609 -476.743774    8.528060   67.849895 
> -298.548271 
>          25          26          27          28          29   
>        30 
> -478.826436  225.895398  110.617233 -226.800933 -487.939098  
> 281.782736 
>          31          32          33          34          35   
>        36 
>  -61.495429 -390.773595 -485.051760  431.670075  477.391909  
> -39.886256 
>          37          38          39          40          41   
>        42 
> -276.164422  732.557413  618.279247  -16.998918 -229.277084  
> 756.444751 
>  
> 
> 			
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From abunn at whrc.org  Tue Nov  2 21:25:59 2004
From: abunn at whrc.org (Andy Bunn)
Date: Tue, 2 Nov 2004 15:25:59 -0500
Subject: [R] install.packages, bundles, pmatch, and Rprofile...
Message-ID: <NEBBIPHDAMMOKDKPOFFIAEIBCLAA.abunn@whrc.org>

Hi,

Somebody asked me to make sure that all the machines running the in our lab
(XP and Linux, both running 2.0) have R installed and that A) All the
packages are installed and B) kept up-to-date.

Obediently, I began to modify a shared Rprofile so that once a week it
checks for new packages and updates to the current version of the installed
packages on CRAN. Sounds simple enough. Plus some general conditioning
statements to make sure that this runs only once a week or so the logic I'm
following is:


myPackages      <- installed.packages()
CRANsPackages   <- CRAN.packages()
missingPackages <- CRANsPackages[is.na(match(CRANsPackages[,1],
myPackages[,1])),1]
install.packages(missingPackages)


Now this might be trivial, but, missingPackages includes bundled packages
which are already installed (e.g., dse, VR) in addition to those packages
that are truly missing. I know I have to match myPackages with the Contains
column in CRANsPackages and probably use pmatch to do it, but the syntax
eludes me.

So, an example:
# Install a bundled package
install.packages("gregmisc")
# Get the installed package matrix
myPackages      <- installed.packages()
# See if gregmisc is really there
myPackages[grep("gregmisc", myPackages[,5]),]
# Get the matrix of all the packages on CRAN
CRANsPackages   <- CRAN.packages()
# Find the missing packages
missingPackages <- CRANsPackages[is.na(match(CRANsPackages[,1],
myPackages[,1])),1]
# Whoops, gregmisc is in there even though its bundles are in myPackages...
missingPackages[grep("gregmisc", missingPackages)]
# Here are the bundles as a string in CRANsPackages...
CRANsPackages[grep("gregmisc", CRANsPackages[,1]),8]


I've looked through R-admin and searched CRAN. What I'm after is a
semi-permanent fix to maintaining R on multiple machines and across
platforms. Other folks must do this? Are there other useful tips?

Off to the polls,
Andy



From jfox at mcmaster.ca  Tue Nov  2 21:26:50 2004
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 2 Nov 2004 15:26:50 -0500
Subject: [R] R data editor
In-Reply-To: <6369798F399F2B45BEA47F00805E7B070BD0C2@acss1.acs.attocube.com>
Message-ID: <20041102202649.JZTW25820.tomts20-srv.bellnexxia.net@JohnDesktop8300>

Dear Tobi,

I don't have a Mac, so I can't check the source of the problem. I assume
from your message that the data editor works fine outside of the Rcmdr.
Although it would be nice to get to the root of the problem, you can also
rename variables via the "Data -> Manage variables in active data set ->
Rename variables" menu.

I hope this helps.
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Tobias 
> Lindenberg
> Sent: Tuesday, November 02, 2004 11:53 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] R data editor
> 
> Hi,
>  
> I have installed R2.0.0 on my mac with the new gui. Almost 
> everything works fine now.
> Except when I run RCommander and want to edit a dataset with 
> the data editor.
> Normally when you click on the variablename on top of a row a 
> small dialog box should open where you can change the name 
> and the between numeric uand character.
> But this dialog box does not open.
>  
> Thanks for any help,
>  
> tobi 

> http://www.R-project.org/posting-guide.html



From nair at sdsc.edu  Tue Nov  2 21:40:04 2004
From: nair at sdsc.edu (T. Murlidharan Nair)
Date: Tue, 02 Nov 2004 12:40:04 -0800
Subject: [R] lda
Message-ID: <4187F0A4.6000609@sdsc.edu>

Hi !!
I am trying to analyze some of my data using  linear discriminant analysis.
I worked out the following example code in Venables and Ripley
It does not seem to be happy with it.
============================
library(MASS)
library(stats)
data(iris3)
ir<-rbind(iris3[,,1],iris3[,,2],iris3[,,3])
ir.species<-factor(c(rep("s",50),rep("c",50),rep("v",50)))
ir.lda<-lda(log(ir),ir.species)
ir.ld<-predict(ir.lda,dimen=2)$x
eqscplot(ir.ld, type="n", xlab = "First linear discriminant", ylab = 
"second linear discriminant")
text(ir.ld, labels= as.character(ir.species[-143]), col =3 
+codes(ir.species),cex =0.8)

======================================


eqscplot does not plot anything and it gives me an error 
saying codes is defunct. Have I missed anything there. 

Thanks../Murli



From ripley at stats.ox.ac.uk  Tue Nov  2 22:08:44 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 2 Nov 2004 21:08:44 +0000 (GMT)
Subject: [R] install.packages, bundles, pmatch, and Rprofile...
In-Reply-To: <NEBBIPHDAMMOKDKPOFFIAEIBCLAA.abunn@whrc.org>
Message-ID: <Pine.LNX.4.44.0411022101090.13818-100000@gannet.stats>

On Tue, 2 Nov 2004, Andy Bunn wrote:

> Hi,
> 
> Somebody asked me to make sure that all the machines running the in our lab
> (XP and Linux, both running 2.0) have R installed and that A) All the
> packages are installed and B) kept up-to-date.
> 
> Obediently, I began to modify a shared Rprofile so that once a week it
> checks for new packages and updates to the current version of the installed
> packages on CRAN. Sounds simple enough. Plus some general conditioning
> statements to make sure that this runs only once a week or so the logic I'm
> following is:
> 
> 
> myPackages      <- installed.packages()
> CRANsPackages   <- CRAN.packages()
> missingPackages <- CRANsPackages[is.na(match(CRANsPackages[,1],
> myPackages[,1])),1]
> install.packages(missingPackages)
> 
> 
> Now this might be trivial, but, missingPackages includes bundled packages
> which are already installed (e.g., dse, VR) in addition to those packages
> that are truly missing. I know I have to match myPackages with the Contains
> column in CRANsPackages and probably use pmatch to do it, but the syntax
> eludes me.

You do want an exact match.

The code you need is in packageStatus(), so you don't need to reinvent 
this particular wheel.  Look at its summary() method (and output) to see 
how to get what you want.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Nov  2 22:16:49 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 2 Nov 2004 21:16:49 +0000 (GMT)
Subject: [R] lda
In-Reply-To: <4187F0A4.6000609@sdsc.edu>
Message-ID: <Pine.LNX.4.44.0411022109140.13818-100000@gannet.stats>

On Tue, 2 Nov 2004, T. Murlidharan Nair wrote:

> Hi !!
> I am trying to analyze some of my data using  linear discriminant analysis.
> I worked out the following example code in Venables and Ripley

> It does not seem to be happy with it.

What is `it'?  If you mean R, which version, and which version of the VR 
bundle?

> ============================
> library(MASS)
> library(stats)

That line is definitely not in `Venables and Ripley'

> data(iris3)
> ir<-rbind(iris3[,,1],iris3[,,2],iris3[,,3])
> ir.species<-factor(c(rep("s",50),rep("c",50),rep("v",50)))
> ir.lda<-lda(log(ir),ir.species)
> ir.ld<-predict(ir.lda,dimen=2)$x
> eqscplot(ir.ld, type="n", xlab = "First linear discriminant", ylab = 
> "second linear discriminant")
> text(ir.ld, labels= as.character(ir.species[-143]), col =3 
> +codes(ir.species),cex =0.8)
> 
> ======================================
> 
> 
> eqscplot does not plot anything and it gives me an error 
> saying codes is defunct. Have I missed anything there. 

I have no idea why eqscplot is misbehaving (your example works up to the
last line for me), but the R scripts which the book refers you to do work.  
See p.12 (and the R posting guide asking you to read the relevant section
of the book).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From scott.waichler at pnl.gov  Tue Nov  2 22:04:48 2004
From: scott.waichler at pnl.gov (Scott Waichler)
Date: Tue, 02 Nov 2004 13:04:48 -0800
Subject: [R] Color schemes that work for people with color-deficient vision
Message-ID: <200411022104.iA2L4mF19027@snow.pnl.gov>

A recent article in the earth science literature cited below and available at
http://geography.uoregon.edu/datagraphics/EOS/
points out that rainbow color schemes and mixtures of green and yellow 
can be troublesome for people with color-deficient vision.  
The authors propose alternative schemes that can be viewed and downloaded
in RGB, HSV, CMYK, and RGB256 formats from 
http://geography.uoregon.edu/datagraphics/color_scales.htm.
I have translated their RGB definitions into the hex color names given below.

Ironically, their article appeared on the same page of one that used
a rainbow color pattern to show climate patterns.  I'm not color-blind but
I still had trouble figuring out which end of the scale some of the colors belonged to.

Light A. and P.J. Bartlein, 2004.  "The end of the rainbow?  Color schemes for improved 
data graphics," EOS Transactions of the American Geophysical Union 85(40):385.

BrowntoBlue.10 <- c("#663000", "#996136", "#CC9B7A", "#D9AF98", "#F2DACE", "#CCFDFF", "#99F8FF", "#66F0FF", "#33E4FF", "#00AACC")
BrowntoBlue.12 <- c("#331A00", "#663000", "#996136", "#CC9B7A", "#D9AF98", "#F2DACE", "#CCFDFF", "#99F8FF", "#66F0FF", "#33E4FF", "#00AACC", "#007A99")
BluetoDarkOrange.12 <- c("#1F8F99", "#52C4CC", "#99FAFF", "#B2FCFF", "#CCFEFF", "#E6FFFF", "#FFE6CC", "#FFCA99", "#FFAD66", "#FF8F33", "#CC5800", "#994000")
BluetoDarkOrange.18 <- c("#006666", "#009999", "#00CCCC", "#00FFFF", "#33FFFF", "#66FFFF", "#99FFFF", "#B2FFFF", "#CCFFFF", "#E6FFFF", "#FFE6CC", "#FFCA99", "#FFAD66", "#FF8F33", "#FF6E00", "#CC5500", "#993D00", "#662700")
DarkRedtoBlue.12 <- c("#2A0BD9", "#264EFF", "#40A1FF", "#73DAFF", "#ABF8FF", "#E0FFFF", "#FFFFBF", "#FFE099", "#FFAD73", "#F76E5E", "#D92632", "#A60021")
DarkRedtoBlue.18 <- c("#2400D9", "#191DF7", "#2957FF", "#3D87FF", "#57B0FF", "#75D3FF", "#99EBFF", "#BDF9FF", "#EBFFFF", "#FFFFEB", "#FFF2BD", "#FFD699", "#FFAC75", "#FF7857", "#FF3D3D", "#F72836", "#D91630", "#A60021")
BluetoGreen.14 <- c("#0000FF", "#3333FF", "#6666FF", "#9999FF", "#B2B2FF", "#CCCCFF", "#E6E6FF", "#E6FFE6", "#CCFFCC", "#B2FFB2", "#99FF99", "#66FF66", "#33FF33", "#00FF00")
BluetoGray.8 <- c("#0099CC", "#66E6FF", "#99FFFF", "#CCFFFF", "#E6E6E6", "#999999", "#666666", "#333333")
BluetoOrangeRed.14 <- c("#085AFF", "#3377FF", "#5991FF", "#8CB2FF", "#BFD4FF", "#E6EEFF", "#F7FAFF", "#FFFFCC", "#FFFF99", "#FFFF00", "#FFCC00", "#FF9900", "#FF6600", "#FF0000")
BluetoOrange.10 <- c("#0055FF", "#3399FF", "#66CCFF", "#99EEFF", "#CCFFFF", "#FFFFCC", "#FFEE99", "#FFCC66", "#FF9933", "#FF5500")
BluetoOrange.12 <- c("#002BFF", "#1A66FF", "#3399FF", "#66CCFF", "#99EEFF", "#CCFFFF", "#FFFFCC", "#FFEE99", "#FFCC66", "#FF9933", "#FF661A", "#FF2B00")
BluetoOrange.8 <- c("#0080FF", "#4CC4FF", "#99EEFF", "#CCFFFF", "#FFFFCC", "#FFEE99", "#FFC44C", "#FF8000")
LightBluetoDarkBlue.10 <- c("#E6FFFF", "#CCFBFF", "#B2F2FF", "#99E6FF", "#80D4FF", "#66BFFF", "#4CA6FF", "#3388FF", "#1A66FF", "#0040FF")
LightBluetoDarkBlue.7 <- c("#FFFFFF", "#CCFDFF", "#99F8FF", "#66F0FF", "#33E4FF", "#00AACC", "#007A99")
Categorical.12 <- c("#FFBF80", "#FF8000", "#FFFF99", "#FFFF33", "#B2FF8C", "#33FF00", "#A6EDFF", "#1AB2FF", "#CCBFFF", "#664CFF", "#FF99BF", "#E61A33")
GreentoMagenta.16 <- c("#005100", "#008600", "#00BC00", "#00F100", "#51FF51", "#86FF86", "#BCFFBC", "#FFFFFF", "#FFF1FF", "#FFBCFF", "#FF86FF", "#FF51FF", "#F100F1", "#BC00BC", "#860086", "#510051")
SteppedSequential.5 <- c("#990F0F", "#B22D2D", "#CC5252", "#E67E7E", "#FFB2B2", "#99700F", "#B28B2D", "#CCA852", "#E6C77E", "#FFE8B2", "#1F990F", "#3CB22D", "#60CC52", "#8AE67E", "#BCFFB2", "#710F99", "#8B2DB2", "#A852CC", "#C77EE6", "#E9B2FF", "#990F20", "#B22D3C", "#CC5260", "#E67E8A", "#FFB2BC")

Scott Waichler
Pacific Northwest National Laboratory
scott.waichler at pnl.gov



From sundar.dorai-raj at pdf.com  Tue Nov  2 22:31:17 2004
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 02 Nov 2004 15:31:17 -0600
Subject: [R] lda
In-Reply-To: <4187F0A4.6000609@sdsc.edu>
References: <4187F0A4.6000609@sdsc.edu>
Message-ID: <4187FCA5.3000707@pdf.com>



T. Murlidharan Nair wrote:

> Hi !!
> I am trying to analyze some of my data using  linear discriminant analysis.
> I worked out the following example code in Venables and Ripley
> It does not seem to be happy with it.
> ============================
> library(MASS)
> library(stats)
> data(iris3)
> ir<-rbind(iris3[,,1],iris3[,,2],iris3[,,3])
> ir.species<-factor(c(rep("s",50),rep("c",50),rep("v",50)))
> ir.lda<-lda(log(ir),ir.species)
> ir.ld<-predict(ir.lda,dimen=2)$x
> eqscplot(ir.ld, type="n", xlab = "First linear discriminant", ylab = 
> "second linear discriminant")
> text(ir.ld, labels= as.character(ir.species[-143]), col =3 
> +codes(ir.species),cex =0.8)
> 
> ======================================
> 
> 
> eqscplot does not plot anything and it gives me an error saying codes is 
> defunct. Have I missed anything there.
> Thanks../Murli

Murli,
eqscplot gives you nothing because you specified `type="n"'. You are not 
plotting anything because your call to "text" never completed.

When I do this I get:

 > R.version.string
[1] "R version 2.0.0, 2004-10-09"
 > text(ir.ld, labels= as.character(ir.species[-143]), col =3 
+codes(ir.species),cex =0.8)
Error: 'codes' is defunct.
See help("Defunct")

This means "codes" is no longer available. Use ?as.integer instead.

--sundar

P.S. Please do read the posting guide and tell us what version of R you 
are using, etc.



From elvis at xlsolutions-corp.com  Tue Nov  2 22:36:00 2004
From: elvis at xlsolutions-corp.com (elvis@xlsolutions-corp.com)
Date: Tue,  2 Nov 2004 14:36:00 -0700
Subject: [R] R/S-plus Course***In San Francisco & Washington,
	DC*** December- 2004
Message-ID: <20041102213600.16224.qmail@webmail03.mesa1.secureserver.net>

XLSolutions Corporation (www.xlsolutions-corp.com) is proud 
to announce our December-2004 2-day R/S-plus courses in San Francisco &
Washington, DC


San Francisco -------------------------------------->  December
16th-17th
Washington, DC ----------------------------------->  December 16th-17th

Reserve your seat now at the early bird rates! Payment due AFTER the 
class. 


R/S-plus Fundamentals and Programming Techniques 
=======================================

Course Description: 
This two-day beginner to intermediate R/S-plus course focuses 
on a broad spectrum of topics, \
from reading raw data to a comparison of R and S. We will learn 
the essentials of data manipulation, graphical visualization 
and R/S-plus programming. We will explore statistical data analysis 
tools,including graphics with data sets. How to enhance your plots. 
We will perform basic statistics and fit linear regression models. 
Participants are encouraged to bring data for interactive sessions 


With the following outline: 
- An Overview of R and S 
- Data Manipulation and Graphics 
- Using Lattice Graphics 
- A Comparison of R and S-Plus 
- How can R Complement SAS? 
- Writing Functions 
- Avoiding Loops 
- Vectorization 
- Statistical Modeling 
- Project Management 
- Techniques for Effective use of R and S 
- Enhancing Plots 
- Using High-level Plotting Functions 
- Building and Distributing Packages (libraries) 


Email us for group discounts. 
Email Sue Turner: sue at xlsolutions-corp.com 
Phone: 206-686-1578 
Visit us: www.xlsolutions-corp.com/training.htm 
Please let us know if you and your colleagues are interested in this 
classto take advantage of group discount. Register now to secure your 
seat! 
Interested in R/Splus Advanced course? email us. 


Cheers, 
Elvis Miller, PhD 
Manager Training. 
XLSolutions Corporation 
206 686 1578 
www.xlsolutions-corp.com 
elvis at xlsolutions-corp.com



From anne at wjh.harvard.edu  Tue Nov  2 23:35:30 2004
From: anne at wjh.harvard.edu (Anne G)
Date: Tue, 2 Nov 2004 17:35:30 -0500 (EST)
Subject: [R] Starting with R
Message-ID: <Pine.SOL.4.30.0411021730160.20241-100000@wjh1.wjh.harvard.edu>

I am studying R and within the first 3 lines of
demo("graphics") I get completely stuck.

could you 1. give me the answer so I can move on 2. tell me
how I might find the answer.

On my mac OSX, the first command is equivalent to
get("quartz")()

now get("quartz") would be a regular function format
but what is get () ()? what kind of beast? does get() return
a function?

I run demo("graphics") and of course I want to stop it right
away, without running through the whole demo, how do I STOP
IT! CANCEL IT? (it gets stuck on hit return to continue
lines)

Then I want to look at the program that generated
demo("graphics") and I find a graphic file, but no demo fx.
where is the demo("graphics") fx physically on my computer?
how do I find it on a Mac?

Then I want to list the program, but I can't fine a fx which
list the code of a function.

Sorry, it must seem very basic. But I searched through the
R intro and through the R lists and I can't find the answer.

Thanks for your help

Anne



From jfox at mcmaster.ca  Wed Nov  3 00:02:52 2004
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 2 Nov 2004 18:02:52 -0500
Subject: [R] Problems with Durbin Watson and Partial Residual Plots
In-Reply-To: <20041102214556.33501.qmail@web50509.mail.yahoo.com>
Message-ID: <20041102230252.QAFE2542.tomts22-srv.bellnexxia.net@JohnDesktop8300>

Dear Cal,

Both functions work fine for me (Windows XP Pro, R 2.0.0 patched, car
1.0-13, though the versions probably aren't significant here):

> mod <- lm(Sales ~ Time, data=Data)
> library(car)
> durbin.watson(mod)
 lag Autocorrelation D-W Statistic p-value
   1       0.3232896      1.249564    0.01
 Alternative hypothesis: rho != 0
> cr.plot(mod, "Time") # makes the plot!

BTW, since there is only one predictor in the model (i.e., Time), the
component+residual plot is pointless -- it's essentially the scatterplot of
Sales vs. Time.

I'm not sure what the problem is. Perhaps you should just try again (or
provide some more information about what you did).

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: Cal Tate [mailto:mathdoc2be at yahoo.com] 
> Sent: Tuesday, November 02, 2004 4:46 PM
> To: John Fox
> Subject: RE: [R] Problems with Durbin Watson and Partial 
> Residual Plots
> 
> John,
>  
> Thanks for your reply. The data set is attached.
> 
> Cal
> 
> John Fox <jfox at mcmaster.ca> wrote:
> 
> 	Dear Cal,
> 	
> 	The functions that you mention are in the car package.
> 	
> 	The problems that you've encountered seem very odd to 
> me. For example, if
> 	you take a look at durbin.watson.lm(), you'll see that 
> the code producing
> 	the errors is quite straight-forward; it just extracts 
> residuals from the
> 	model and checks for NAs:
> 	
> 	residuals <- residuals(model)
> 	if (any(is.na(residuals))) 
> 	stop("residuals include missing values") 
> 	
> 	Likewise, what's going on inside of cr.plot.lm() is 
> also pretty simple.
> 	
> 	Can you send the data set on which this regression was based?
> 	
> 	John
> 	
> 	--------------------------------
> 	John Fox
> 	Department of Sociology
> 	McMaster University
> 	Hamilton, Ontario
> 	Canada L8S 4M4
> 	905-525-9140x23604
> 	http://socserv.mcmaster.ca/jfox 
> 	-------------------------------- 
> 	
> 	> -----Original Message-----> From: 
> r-help-bounces at stat.math.ethz.ch 
> 	> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf 
> Of Cal Tate
> 	> Sent: Tuesday, November 02, 2004 2:24 PM
> 	> To: r-help at stat.math.ethz.ch
> 	> Subject: [R] Problems with Durbin Watson and Partial 
> Residual Plots
> 	> 
> 	> I am trying to evaluate a model by using the commands 
> 	> durbin.watson and cr.plot.
> 	> However, I keep getting errors that I can't figure out. A 
> 	> description follows. Does anyone have a hint as to 
> what may be wrong?
> 	> 
> 	> 
> 	> 1)The Durbin Watson Test. In running the command I kept 
> 	> getting the message "residuals include missing values" when 
> 	> actually this was NOT the case.
> 	> 
> 	> Example:
> 	> >durbin.watson(hw8LM0)
> 	> Error in durbin.watson.lm(hw8LM0) : residuals include 
> missing values
> 	> 
> 	> (2)Partial Residual Plots: Here I kept getting that the 
> 	> variables I choose are not in the model, i.e. "Time! 
> is not in 
> 	> the model" when it clearly is.
> 	> 
> 	> Example:
> 	> >cr.plot(hw8LM0,variable="Time")
> 	> Error in cr.plot.lm(hw8LM0, variable = "Time") : 
> 	> Time is not in the model.
> 	> 
> 	> Here is the model and residuals:
> 	> summary(hw8LM0)
> 	> Call:
> 	> lm(formula = Sales ~ Time)
> 	> Residuals:
> 	> Min 1Q Median 3Q Max 
> 	> -516.32 -292.95 -29.15 238.48 895.09
> 	> Coefficients:
> 	> Estimate Std. Error t value Pr(>|t|) 
> 	> (Intercept) 1418.872 122.464 11.59 2.35e-14 ***
> 	> Time 73.278 4.962 14.77 < 2e-16 ***
> 	> ---
> 	> Signif. codes: 0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 
> 	> 1 Residual standard error: 389.8 on 40 degrees of freedom
> 	> Multiple R-Squared: 0.845, Adjusted R-squared: 0.8411 
> 	> F-statistic: 218.1 on 1 and 40 DF, p-value: < 2.2e-16 
> 	> 
> 	> hw8LM0$res
> 	> 1 2 3 4 5 
> 	> 6 
> 	> 242.679535 679.531369 895.093204 442.975038 -237.4! 43127 
> 	> 245.868708 
> 	> 7 8 9 10 11 
> 	> 12 
> 	> 82.540542 -13.347623 -209.325789 161.976046 -96.612120 
> 	> -271.380285 
> 	> 13 14 15 16 17 
> 	> 18 
> 	> -460.888451 -113.606616 -311.494782 -417.352947 -516.321113 
> 	> 1.430722 
> 	> 19 20 21 22 23 
> 	> 24 
> 	> -18.407443 -328.425609 -476.743774 8.528060 67.849895 
> 	> -298.548271 
> 	> 25 26 27 28 29 
> 	> 30 
> 	> -478.826436 225.895398 110.617233 -226.800933 -487.939098 
> 	> 281.782736 
> 	> 31 32 33 34 35 
> 	> 36 
> 	> -61.495429 -390.773595 -485.051760 431.670075 477.391909 
> 	> -39.886256 
> 	> 37 38 39 40 41 
> 	> 42 
> 	> -276.164422 732.557413 618.279247 -16.998918 -229.277084 
> 	> 756.444751 
> 	> 
> 	> 
> 	> 
> 	> ---------------------------------
> 	> 
> 	> [[alternative HTML version deleted]]
> 	> 
> 	> ______________________________________________
> 	> R-help at stat.math.ethz.ch mailing list
> 	> https://stat.ethz.ch/mailman/listinfo/r-help
> 	> PLEASE do read the posting guide! 
> 	> http://www.R-project.org/posting-guide.html
> 	
> 	
> 
> ________________________________
> 


From nair at sdsc.edu  Wed Nov  3 00:16:07 2004
From: nair at sdsc.edu (T. Murlidharan Nair)
Date: Tue, 02 Nov 2004 15:16:07 -0800
Subject: [R] PCA
Message-ID: <41881537.9040505@sdsc.edu>

When using the biplot how do I supress the column names from appearing 
in the plot.
I am only interested in the plot with the arrows. When I use  xlabs 
=NULL   nothing
gets plotted.
Thanks ../Murli



From p.dalgaard at biostat.ku.dk  Wed Nov  3 00:21:33 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 03 Nov 2004 00:21:33 +0100
Subject: [R] Starting with R
In-Reply-To: <Pine.SOL.4.30.0411021730160.20241-100000@wjh1.wjh.harvard.edu>
References: <Pine.SOL.4.30.0411021730160.20241-100000@wjh1.wjh.harvard.edu>
Message-ID: <x2zn1z96rm.fsf@biostat.ku.dk>

Anne G <anne at wjh.harvard.edu> writes:

> I am studying R and within the first 3 lines of
> demo("graphics") I get completely stuck.
> 
> could you 1. give me the answer so I can move on 2. tell me
> how I might find the answer.

Well, y'know, sometimes obstacles are more easily handled by walking
around them than by climbing over them. Who said you need to
understand demo("graphics") in order to get to know R? It's a demo, a
show-off of what glitzy graphics we can do, not a tutorial. Actually,
it might be sensible to walk through some of its examples, but the
first couple of setup lines can certainly be skipped.
 
> On my mac OSX, the first command is equivalent to
> get("quartz")()
> 
> now get("quartz") would be a regular function format
> but what is get () ()? what kind of beast? does get() return
> a function?

In this case, yes, exactly. In general, get() returns the R object of
the given name, which may or may not be a funtion.

> I run demo("graphics") and of course I want to stop it right
> away, without running through the whole demo, how do I STOP
> IT! CANCEL IT? (it gets stuck on hit return to continue
> lines)

This is OS and GUI dependent. On unix text terminals you can generally
hit Ctrl-C, in the Windows GUI there's a Stop button (I think), and
Aqua seems to have an Abort button.

> Then I want to look at the program that generated
> demo("graphics") and I find a graphic file, but no demo fx.
> where is the demo("graphics") fx physically on my computer?
> how do I find it on a Mac?

Somewhere in the R installation directory. 

system.file("demo/graphics.R",package="graphics")

should tell you exactly, on all systems.


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From miq2711 at yahoo.fr  Wed Nov  3 00:34:27 2004
From: miq2711 at yahoo.fr (Pierre-Henry Miquel)
Date: Wed, 3 Nov 2004 00:34:27 +0100 (CET)
Subject: [R] Confident intervals in multinom
Message-ID: <20041102233427.74301.qmail@web86904.mail.ukl.yahoo.com>

Dear R help list,
i'm using multinom (nnet), the results given to me is
the coefficients and std errors.
Is there a way to obtain directly (or in an export to
latex) the odd-ratio (exp(B)) and it's confident
intervals.

thanks very much for your help

Pierre-Henry Miquel
Universit?? de Lille 2 (M??decine)
France


	

	
		
Vous manquez d??espace pour stocker vos mails ?



From paul_bivand at blueyonder.co.uk  Wed Nov  3 00:34:29 2004
From: paul_bivand at blueyonder.co.uk (Paul Bivand)
Date: Tue, 2 Nov 2004 23:34:29 +0000
Subject: [R] Starting with R
In-Reply-To: <Pine.SOL.4.30.0411021730160.20241-100000@wjh1.wjh.harvard.edu>
References: <Pine.SOL.4.30.0411021730160.20241-100000@wjh1.wjh.harvard.edu>
Message-ID: <200411022334.30155.paul_bivand@blueyonder.co.uk>

in R, the source code for functions can be displayed by typing the function 
name without the brackets. 

Therefore ,

demo

produces the source code

and demo() runs the function - which in this case provides a list of available 
demos.

The source for the graphics demo is a R source file named graphics.R which is 
located in (on my Mandrake linux system, OSX may 
differ) /usr/lib/R/library/graphics/

The source code for functions demonstrated can be similarly displayed, 
although some functions will call C or Fortran modules.

Paul Bivand

On Tuesday 02 Nov 2004 22:35, Anne G wrote:
> I am studying R and within the first 3 lines of
> demo("graphics") I get completely stuck.
>
> could you 1. give me the answer so I can move on 2. tell me
> how I might find the answer.
>
> On my mac OSX, the first command is equivalent to
> get("quartz")()
>
> now get("quartz") would be a regular function format
> but what is get () ()? what kind of beast? does get() return
> a function?
>
> I run demo("graphics") and of course I want to stop it right
> away, without running through the whole demo, how do I STOP
> IT! CANCEL IT? (it gets stuck on hit return to continue
> lines)
>
> Then I want to look at the program that generated
> demo("graphics") and I find a graphic file, but no demo fx.
> where is the demo("graphics") fx physically on my computer?
> how do I find it on a Mac?
>
> Then I want to list the program, but I can't fine a fx which
> list the code of a function.
>
> Sorry, it must seem very basic. But I searched through the
> R intro and through the R lists and I can't find the answer.
>
> Thanks for your help
>
> Anne
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From arrayprofile at yahoo.com  Wed Nov  3 00:41:05 2004
From: arrayprofile at yahoo.com (array chip)
Date: Tue, 2 Nov 2004 15:41:05 -0800 (PST)
Subject: [R] time dependency of Cox regression
Message-ID: <20041102234105.21112.qmail@web40827.mail.yahoo.com>

Hi,

How can I specify a Cox proportional hazards model
with a covariate which i believe its strength on
survival changes/diminishes with time? The value of
the covariate was only recorded once at the beginning
of the study for each individual (e.g. at the
diagnosis of the disease), so I do not have the time
course data of the covariate for any given individual.
For example, I want to state at the end of the
analysis that the hazard ratio of the covariate is 6
at the beginning, decrease to 3 after 2 years and
decrease to 1.5 after 5 years.

Is this co-called time-dependent covariate? I guess
not, because it's really about the influence of the
covariate (which was measured once at the beginning)
on survival changing over time.

Thanks for any input.


		
__________________________________ 


From sundar.dorai-raj at pdf.com  Wed Nov  3 00:49:04 2004
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 02 Nov 2004 17:49:04 -0600
Subject: [R] PCA
In-Reply-To: <41881537.9040505@sdsc.edu>
References: <41881537.9040505@sdsc.edu>
Message-ID: <41881CF0.5090002@pdf.com>



T. Murlidharan Nair wrote:
> When using the biplot how do I supress the column names from appearing 
> in the plot.
> I am only interested in the plot with the arrows. When I use  xlabs 
> =NULL   nothing
> gets plotted.
> Thanks ../Murli
> 

How about:

pc <- princomp(USArrests)
biplot(pc, xlabs = rep("", nrow(USArrests)))

--sundar



From murdoch at stats.uwo.ca  Wed Nov  3 02:23:29 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 02 Nov 2004 20:23:29 -0500
Subject: [R] Color schemes that work for people with color-deficient vision
In-Reply-To: <200411022104.iA2L4mF19027@snow.pnl.gov>
References: <200411022104.iA2L4mF19027@snow.pnl.gov>
Message-ID: <obcgo0lk042qeljgc5jrfcn99s0jplu33m@4ax.com>

On Tue, 02 Nov 2004 13:04:48 -0800, Scott Waichler
<scott.waichler at pnl.gov> wrote:

>A recent article in the earth science literature cited below and available at
>http://geography.uoregon.edu/datagraphics/EOS/
>points out that rainbow color schemes and mixtures of green and yellow 
>can be troublesome for people with color-deficient vision.  
>The authors propose alternative schemes that can be viewed and downloaded
>in RGB, HSV, CMYK, and RGB256 formats from 
>http://geography.uoregon.edu/datagraphics/color_scales.htm.
>I have translated their RGB definitions into the hex color names given below.

Thomas Lumley has just written a couple of functions that make
experimentation with scales like this a lot easier.  They're currently
in R-devel, and will appear in R 2.1.0 next spring.  If you want to
play with them before that, you can get the source and the docs from 

https://svn.r-project.org/R/trunk/src/library/graphics/R/colorRamp.R

and 

https://svn.r-project.org/R/trunk/src/library/graphics/man/colorRamp.Rd

Duncan Murdoch



From bitwrit at ozemail.com.au  Wed Nov  3 02:00:43 2004
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Wed, 3 Nov 2004 11:00:43 +1000
Subject: [R] overlapping plots
In-Reply-To: <41879451.50304@noaa.gov>
References: <3A822319EB35174CA3714066D590DCD50994E268@usrymx25.merck.com>
	<41879451.50304@noaa.gov>
Message-ID: <20041103013037.IQWQ4289.smta01.mail.ozemail.net@there>

Carlisle Thacker wrote:
> Andy,
>
> If I had the three plots on three separate sheets of paper, I could cut
> out plot 2 and paste it over plot one so that the data and axes on both
> plots can be seen.  Then I could cut out plot 3 and past it over plots 1
> and 2 so that data and axes on all three plots can be seen.  For my
> special case, the x-axes would have to appear at the top of each plot.
> If there are boxes around the plots or guide lines, the parts below
> plots 2 and 3 should be obscured.
>
This is pretty rough, but I think it might be basically what you want.

par(mar=c(6,2,2,6))
plot(rnorm(20),rnorm(20),axes=FALSE,xlab="",ylab="")
axis(2)
axis(3)
par(new=TRUE)
par(mar=c(4,4,4,4))
plot(rnorm(10),type="l",col="green",axes=FALSE,xlab="",ylab="")
axis(2,col="green")
axis(3,col="green")
par(new=TRUE)
par(mar=c(2,6,6,2))
plot(rnorm(12),pch=2,col="red",axes=FALSE,xlab="",ylab="")
axis(2,col="red")
axis(3,col="red")

Jim



From f.harrell at vanderbilt.edu  Wed Nov  3 02:55:08 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 02 Nov 2004 19:55:08 -0600
Subject: [R] time dependency of Cox regression
In-Reply-To: <20041102234105.21112.qmail@web40827.mail.yahoo.com>
References: <20041102234105.21112.qmail@web40827.mail.yahoo.com>
Message-ID: <41883A7C.2070508@vanderbilt.edu>

array chip wrote:
> Hi,
> 
> How can I specify a Cox proportional hazards model
> with a covariate which i believe its strength on
> survival changes/diminishes with time? The value of
> the covariate was only recorded once at the beginning
> of the study for each individual (e.g. at the
> diagnosis of the disease), so I do not have the time
> course data of the covariate for any given individual.
> For example, I want to state at the end of the
> analysis that the hazard ratio of the covariate is 6
> at the beginning, decrease to 3 after 2 years and
> decrease to 1.5 after 5 years.
> 
> Is this co-called time-dependent covariate? I guess
> not, because it's really about the influence of the
> covariate (which was measured once at the beginning)
> on survival changing over time.
> 
> Thanks for any input.
You might try a log-normal or log-logistic accelerated failure time 
model.  These models dictate decreasing hazard ratios over time for 
baseline covariates.
-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From minhan.science at gmail.com  Wed Nov  3 03:06:33 2004
From: minhan.science at gmail.com (Min-Han Tan)
Date: Tue, 2 Nov 2004 21:06:33 -0500
Subject: [R] Estimating survival?
Message-ID: <7902152a04110218062993bcdd@mail.gmail.com>

Hi,

Sorry to trouble the list. I have a problem which I'm not sure how to resolve.

I have a Cox model with 1 independent variable with 2 categories (and
thus 2 survival curves on plotting survfit)

How can I get an estimate of survival for each category at a
particular time point, with standard error?
 
Looking through ?cph and ?coxph, I'm not quite sure how to go about
that. I would really appreciate any direction here.

Thanks a million!

Min-Han



From SMTP_Abimaq2 at abimaq.org.br  Wed Nov  3 04:03:47 2004
From: SMTP_Abimaq2 at abimaq.org.br (SMTP_Abimaq2@abimaq.org.br)
Date: Wed, 03 Nov 2004 00:03:47 -0300
Subject: [R] Servidor SMTP Abimaq
Message-ID: <200411030307.iA337HpK028985@hypatia.math.ethz.ch>

Sua mensagem foi bloqueada por conter um ou mais anexos n??o permitidos pela Politica de Seguran??a da ABIMAQ.
Atualize seu Anti Virus.

From: r-help at lists.r-project.org
To: diaeq1 at abimaq.org.br

File(s): application_diaeq1.pif

Matching filename: *.pif



From vito.muggeo at giustizia.it  Wed Nov  3 10:02:16 2004
From: vito.muggeo at giustizia.it (Vito Muggeo)
Date: Wed, 3 Nov 2004 10:02:16 +0100
Subject: R: [R] time dependency of Cox regression
References: <20041102234105.21112.qmail@web40827.mail.yahoo.com>
Message-ID: <006501c4c183$d41d8f00$5c13070a@PROCGEN>

Hi,
in order to fit Cox model with time-dependent coeff, you have to restruct
your dataframe. For instance you
can use the counting process formulation "(start,stop,status)".
Some years ago I wrote a function (reCox() below) to make the job. It seems
to work if there are not ties, but
if there are ties it works but some problem rises (results do not match
exactly). Let me know if you can fix such problem.

Note that in the restructed dataset, a (linear) time varying effect for the
variable X, say, may be included by the "X:stop" term in the model.

Hope this helps,
regards,
vito

#example taken from ?cph in the Hmisc package
n <- 100
set.seed(731)
age <- 50 + 12*rnorm(n)
sex <- factor(sample(c('Male','Female'), n,rep=TRUE, prob=c(.6, .4)))
cens <- 15*runif(n)
h <- .02*exp(.04*(age-50)+.8*(sex=='Female'))
ttime <- -log(runif(n))/h
status <- ifelse(ttime <= cens,1,0)
ttime <- pmin(ttime, cens)
d<-data.frame(ttime=ttime,status=status,age=age,sex=sex)
#restruct the original dataframe
dd<-reCox(d)
#compare fitted model without ties (minor (negligible?) differences)
coxph(Surv(ttime,status) ~ age + sex,data=d)
coxph(Surv(start,stop,status) ~ age + sex,data=dd)

#data with ties (major differences)
d$ttime[d$status==1]<-round(d$ttime[d$status==1],1)
dd<-reCox(d,k=1:2,epss=0.01) #here you have to specify epss>0 to allow
coxph() to work
coxph(Surv(ttime,status) ~ age + sex,data=d)
coxph(Surv(start,stop,status) ~ age + sex,data=dd)


reCox<-function(data,k=1:2,epss=0){
#FUNZIONA SOLO SE NON CI SONO TIES :-(
#(Preliminary) function to reshape dataframe according to the
"counting-process" formulation
#       author: <vito.muggeo at giustizia.it>
#data: the data-frame to be transformed
#k: indices of SurvTime and Status variables in data
#epss: if for any new record start==stop, then stop is incremented by epss
        if(ncol(data)<=3) data[,"tmp"]<-rep(99,nrow(data))
        dati<-data[order(data[,k[1]]),] #order(unique(surv.time))#???
        status<-dati[,k[2]]
        b<-dati[,-k]
        dati[,"start"]<-rep(0,nrow(dati))
        names(dati)[k[1]]<-"stop"
        n<-nrow(dati)
        a<-matrix(-99,(n*(n-1)/2+n),3)
        a[1,]<-c(1,as.numeric(as.matrix(dati)[1,c("start","stop")]))
        colnames(a)<-c('id.new','start','stop')
        a[,"id.new"]<-rep(1:n,1:n)
      for(i in 2:nrow(dati)){

a[a[,1]==i,-1]<-rbind(a[a[,1]==(i-1),2:3],c(dati[(i-1),"stop"],dati[i,"stop"
]))
                             }
        a<-cbind(a,status=rep(0,nrow(a)))
        a[cumsum(1:n),"status"]<-status
        bb<-sapply(b,function(x)rep(x,1:n))#le categorie le trasforma in
numeri....
        #bb<-lapply(b,function(x)rep(x,1:n))
        #bb<-apply(b,2,function(x)rep(x,1:n))NO!!!
        A<-data.frame(cbind(a,bb),row.names=NULL)
        #if(!missing(epss))
A[,"stop"]<-A[,"stop"]+ifelse(A[,"stop"]==A[,"start"],epss,0)
        if(epss>0)
A[,"stop"]<-A[,"stop"]+ifelse(A[,"stop"]==A[,"start"],epss,0)
        A$tmp<-NULL
        return(A)
        }



----- Original Message -----
From: array chip <arrayprofile at yahoo.com>
To: <R-help at stat.math.ethz.ch>
Sent: Wednesday, November 03, 2004 12:41 AM
Subject: [R] time dependency of Cox regression


> Hi,
>
> How can I specify a Cox proportional hazards model
> with a covariate which i believe its strength on
> survival changes/diminishes with time? The value of
> the covariate was only recorded once at the beginning
> of the study for each individual (e.g. at the
> diagnosis of the disease), so I do not have the time
> course data of the covariate for any given individual.
> For example, I want to state at the end of the
> analysis that the hazard ratio of the covariate is 6
> at the beginning, decrease to 3 after 2 years and
> decrease to 1.5 after 5 years.
>
> Is this co-called time-dependent covariate? I guess
> not, because it's really about the influence of the
> covariate (which was measured once at the beginning)
> on survival changing over time.
>
> Thanks for any input.
>
>
>
> __________________________________
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From petr.pikal at precheza.cz  Wed Nov  3 11:03:42 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 03 Nov 2004 11:03:42 +0100
Subject: [R] cut POSIX  results in NA - bug?
Message-ID: <4188BB0E.15342.A788CB@localhost>

Dear all

I try to make hourly average by cut() function, which almost works 
as *I* expected. What puzled me is that if there is only one item at 
the end of your data it results in NA.

Example will explain what I mean

datum<-seq(ISOdate(2004,8,31), ISOdate(2004,9,1), "min")

cut(datum[1370:1381],"hour", labels=F)
 [1]  1  1  1  1  1  1  1  1  1  1  1 NA

cut(datum[1370:1382],"hour", labels=F)
 [1] 1 1 1 1 1 1 1 1 1 1 1 2 2

I do not understand why the last item in first call is NA. I found it 
only when there was a switch from DST to standard time as it 
coused a trouble in one of my functions and I found there is NA 
value where I did not expected it. 

I can make some workaround but can you please explain me why 
first call results in NA value at the end of a vector and if it is 
*intended* behaviour. If yes I can count with it in improvement of 
my function(s), if not I can make some temporary workaround.

Thank you.

Petr Pikal
petr.pikal at precheza.cz



From petr.pikal at precheza.cz  Wed Nov  3 11:37:05 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 03 Nov 2004 11:37:05 +0100
Subject: [R] Using East-european characters in R
In-Reply-To: <004201c4c0eb$fb796120$0d09f9c2@ales>
Message-ID: <4188C2E1.11752.C61961@localhost>

Hi

It is hidden somewhere in docs (probably FAQs) and you can 
maybe find some answers in help archives.

Basically you probably need

- appropriate fonts set in Rconsole and Rdevga files (e.g. Courier 
instead of Courier New)

see attached my versions.

Cheers
Petr


On 2 Nov 2004 at 15:54, Ale? ?iberna wrote:

> Does anybody know how to produce a East-European character "?" - "c
> with a v-shaped hat " in R (in text or plot)?
> 
> 
> 
> I do know how to get "?,?" - "s,z, with a v-shaped hat", but not this
> one.
> 
> 
> 
> Thanks in advance for any suggestions,
> 
> Ales Ziberna
> 
> 
> 
> P.S.: I am using Windows XP and R version 1.9.1 (details below)
> 
> $platform
> 
> [1] "i386-pc-mingw32"
> 
> 
> 
> $arch
> 
> [1] "i386"
> 
> 
> 
> $os
> 
> [1] "mingw32"
> 
> 
> 
> $system
> 
> [1] "i386, mingw32"
> 
> 
> 
> $status
> 
> [1] ""
> 
> 
> 
> $major
> 
> [1] "1"
> 
> 
> 
> $minor
> 
> [1] "9.1"
> 
> 
> 
> $year
> 
> [1] "2004"
> 
> 
> 
> $month
> 
> [1] "06"
> 
> 
> 
> $day
> 
> [1] "21"
> 
> 
> 
> $language
> 
> [1] "R"
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz


-------------- next part --------------
# The system-wide copy is in rwxxxx/etc.
# A user copy can be installed in `R_USER'.

# Format is 
# [TT] face:style
# where style is plain, bold, italic or bold&italic.
# If 'TT' is present, only True Type fonts are searched.
# Remarks:
#   (a) Windows graphics engine can only rotate True Type fonts;
#   (b) Only the first 32 fonts will be used.

TT Arial : plain
TT Arial : bold
TT Arial : italic
TT Arial : bold&italic

# Please, don't change the following definition. The plot math engine
# assumes that font5 contains greek letters and math symbols
TT Symbol: plain

TT Times New Roman : plain
TT Times New Roman : bold
TT Times New Roman : italic
TT Times New Roman : bold&italic
TT Courier New: plain
TT Courier New: bold
TT Courier New: italic
TT Courier New: bold&italic

TT Century Gothic : plain
TT Century Gothic : bold
TT Century Gothic : italic
TT Century Gothic : bold&italic

TT Matisse ITC: plain
TT Wingdings: plain
-------------- next part --------------
# Optional parameters for the console and the pager
# The system-wide copy is in rwxxxx/etc.
# A user copy can be installed in `R_USER'.

## Style
# This can be `yes' (for MDI) or `no' (for SDI).
MDI = yes
# the next two are only relevant for MDI
toolbar = yes
statusbar = yes

## Font.
# Please use only fixed width font.
# If font=FixedFont the system fixed font is used; in this case
# points and style are ignored. If font begins with "TT ", only
# True Type fonts are searched for.
font = TT Courier New
points = 10
style = normal # Style can be normal, bold, italic


# Dimensions (in characters) of the console.
rows = 25
columns = 80
# Dimensions (in characters) of the internal pager.
pgrows = 25
pgcolumns = 80
# should options(width=) be set to the console width?
setwidthonresize = yes

# memory limits for the console scrolling buffer, in bytes and lines
bufbytes = 65000
buflines = 8000

# Initial position of the console (pixels, relative to the workspace for MDI)
# xconsole = 0
# yconsole = 0

# Dimension of MDI frame in pixels
# Format (w*h+xorg+yorg) or use -ve w and h for offsets from right bottom
# This will come up maximized if w==0
# MDIsize = 0*0+0+0
# MDIsize = 1000*800+100+0
# MDIsize = -50*-50+50+50  # 50 pixels space all round

# The internal pager can displays help in a single window
# or in multiple windows (one for each topic)
# pagerstyle can be set to `singlewindow' or `multiplewindows'
pagerstyle = multiplewindows


## Colours for console and pager(s)
# (see rwxxxx/etc/rgb.txt for the known colours).
background = White
normaltext = NavyBlue
usertext = Red
highlight = DarkRed


## Initial position of the graphics window 
## (pixels, <0 values from opposite edge)
xgraphics = -25
ygraphics = 0

From minhan.science at gmail.com  Wed Nov  3 11:46:05 2004
From: minhan.science at gmail.com (Min-Han Tan)
Date: Wed, 3 Nov 2004 05:46:05 -0500
Subject: [R] Re: Estimating survival?
In-Reply-To: <7902152a04110218062993bcdd@mail.gmail.com>
References: <7902152a04110218062993bcdd@mail.gmail.com>
Message-ID: <7902152a04110302463cdfbc8b@mail.gmail.com>

Apologies, I located the function in cph (... time.inc =30, surv = TRUE)
...
$surv.summary


Thanks.
Min-Han

On Tue, 2 Nov 2004 21:06:33 -0500, Min-Han Tan <minhan.science at gmail.com> wrote:
> Hi,
> 
> Sorry to trouble the list. I have a problem which I'm not sure how to resolve.
> 
> I have a Cox model with 1 independent variable with 2 categories (and
> thus 2 survival curves on plotting survfit)
> 
> How can I get an estimate of survival for each category at a
> particular time point, with standard error?
> 
> Looking through ?cph and ?coxph, I'm not quite sure how to go about
> that. I would really appreciate any direction here.
> 
> Thanks a million!
> 
> Min-Han
>



From ripley at stats.ox.ac.uk  Wed Nov  3 12:14:39 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 3 Nov 2004 11:14:39 +0000 (GMT)
Subject: [R] Using East-european characters in R
In-Reply-To: <4188C2E1.11752.C61961@localhost>
Message-ID: <Pine.LNX.4.44.0411031112001.20041-100000@gannet.stats>

On Wed, 3 Nov 2004, Petr Pikal wrote:

> It is hidden somewhere in docs (probably FAQs) and you can
> maybe find some answers in help archives.

Yes, it is in the rw-FAQ, the question headed

 I don't see characters with accents at the R console, for example in ?text.

which seems an obvious enough heading to expect people to look at it.

> Basically you probably need
> 
> - appropriate fonts set in Rconsole and Rdevga files (e.g. Courier
> instead of Courier New)

That's there too.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Nov  3 12:20:57 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 3 Nov 2004 11:20:57 +0000 (GMT)
Subject: [R] cut POSIX  results in NA - bug?
In-Reply-To: <4188BB0E.15342.A788CB@localhost>
Message-ID: <Pine.LNX.4.44.0411031115120.20041-100000@gannet.stats>

On Wed, 3 Nov 2004, Petr Pikal wrote:

> Dear all
> 
> I try to make hourly average by cut() function, which almost works 
> as *I* expected. What puzled me is that if there is only one item at 
> the end of your data it results in NA.
> 
> Example will explain what I mean
> 
> datum<-seq(ISOdate(2004,8,31), ISOdate(2004,9,1), "min")
> 
> cut(datum[1370:1381],"hour", labels=F)
>  [1]  1  1  1  1  1  1  1  1  1  1  1 NA
> 
> cut(datum[1370:1382],"hour", labels=F)
>  [1] 1 1 1 1 1 1 1 1 1 1 1 2 2
> 
> I do not understand why the last item in first call is NA. I found it 
> only when there was a switch from DST to standard time as it 
> coused a trouble in one of my functions and I found there is NA 
> value where I did not expected it. 

cut(datum[1370:1381],"hour", labels=F, include.lowest=T)

is what you need.  See ?cut, in the See Also, which says

include.lowest: logical, indicating if an 'x[i]' equal to the lowest
          (or highest, for 'right = FALSE') 'breaks' value should be
          included.

> I can make some workaround but can you please explain me why 
> first call results in NA value at the end of a vector and if it is 
> *intended* behaviour. 

It is the documented behaviour, for better or for worse.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From kadusei at uni-goettingen.de  Wed Nov  3 13:08:49 2004
From: kadusei at uni-goettingen.de (Kwabena Adusei-Poku)
Date: Wed, 3 Nov 2004 13:08:49 +0100
Subject: [R] Variable bandwidth in kernel density estimation
Message-ID: <000201c4c19d$e0f056d0$aaad4c86@wso1809r>

Hi,

Can anybody tell me how to implement variable bandwidths  for kernel
density estimation in "R". You may also direct me to a package.

Regards,

Kwabena

--------------------------------------------
Kwabena Adusei-Poku
University of Goettingen
Institute of Statistics and Econometrics
Platz der Goettingen Sieben 5
37073 Goettingen
Germany
Tel: +49-(0)551-394794



From kjetil at acelerate.com  Tue Nov  2 23:54:29 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Tue, 02 Nov 2004 18:54:29 -0400
Subject: [R] Robust Poisson regression
Message-ID: <41881025.7070609@acelerate.com>

Hola!

Anybody knows if there exists somewhere in R some implementation of 
robust Poisson regression,
where robust is taken in the sense as usen in rlm(MASS). I found 
something in the package
wle, but only for the Poisson distribution, not for regression.

For the moment I try to use linear models with the square-root 
transformation,
and rlm.

Kjetil

-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From andy_liaw at merck.com  Wed Nov  3 13:31:20 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 3 Nov 2004 07:31:20 -0500
Subject: [R] Variable bandwidth in kernel density estimation
Message-ID: <3A822319EB35174CA3714066D590DCD50994E278@usrymx25.merck.com>

The locfit package can do adaptive bandwidth selection.  The book "local
regression and likelihood" by Prof. Loader describes the software as well as
the theory and computation, etc.

HTH,
Andy

> From: Kwabena Adusei-Poku
> 
> Hi,
> 
> Can anybody tell me how to implement variable bandwidths  for kernel
> density estimation in "R". You may also direct me to a package.
> 
> Regards,
> 
> Kwabena
> 
> --------------------------------------------
> Kwabena Adusei-Poku
> University of Goettingen
> Institute of Statistics and Econometrics
> Platz der Goettingen Sieben 5
> 37073 Goettingen
> Germany
> Tel: +49-(0)551-394794
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From 0034058 at fudan.edu.cn  Wed Nov  3 13:12:02 2004
From: 0034058 at fudan.edu.cn (rongguiwong)
Date: Wed, 03 Nov 2004 20:12:02 +0800
Subject: [R] how to compute condition index?
Message-ID: <200411032012.03041.0034058@fudan.edu.cn>

is there any existing function for computing condition index?
" analysing multivariate data" say that we can use condition index to check 
multicollinearity.saying that we can get it via SVD. The elements of the 
diagnoal matrix are the standard deviations of the uncorrelated vectors. the 
condition index is the ratio of the largest of these numbers to the smallest.
so if i have a data frame a,containg variables x,y,z.
my model is :
model<-lm(y~x+z,data=a)
so use the following to compute the condition index,but it seems wrong.

temp<-svd(model.matrix(model))$d
max(temp)/min(temp)

is it wrong?



From florence.combes at paris7.jussieu.fr  Wed Nov  3 14:12:14 2004
From: florence.combes at paris7.jussieu.fr (COMBES Florence)
Date: Wed, 03 Nov 2004 14:12:14 +0100
Subject: [R] automation / interactivity of plots on S-PLUS
Message-ID: <web-9923509@univ-paris7.fr>

Dear all,

I have 2 questions maybe more for developpers, and about an S-PLUS 
script.
I think that people from R and maybe Bioconductor could also help me.

1--
When producing a graph with S-PLUS (under Windows XP), it is possible 
to select (with the mouse) an area of the graph in order to highlight 
the rows corresponding to the selected plots on the dataset sheet.

In my case I have to plot about 100 graphs, so I wrote a script with a 
loop to do that automatically. Each time you pass through this "for" 
loop the graph is done.
My problem is that with this way to build graphs, I lost the 
possiblity to highlight data in the data set sheet by defining a zone 
with the mouse on the graph sheet.
(actually it is impossible to define a zone on the graph with the 
mouse)

Does anyone already encountered this problem, and would handle a 
solution ?

I saw maybe the brush() function to be applied on the loop, but this 
seems to return only the row indices and I would like a variable more 
informative that is on a column in my data set sheet.

2--
In my script I want to use the menuSubset() command. It works in 
command line, but I would like this command to appear as a graphical 
window.
I can have the window with the following command line:

guiDisplayDialog("Function", "menuSubset")

but the problem is that the script continues to run, and does not wait 
for the user to click on the "OK" button in order to wait for the 
result of the command Subset, which is a matrix needed further on the 
script.
Do you know a way to pass arguments from the data filled in the window 
to the script, or to force S-PLUs to wait ?


Any help gratefully welcome,

Florence Combes.



From lengqvist at evolution.uni-bonn.de  Wed Nov  3 14:38:51 2004
From: lengqvist at evolution.uni-bonn.de (Leif Engqvist)
Date: Wed, 03 Nov 2004 14:38:51 +0100
Subject: [R] Johnson-Neyman-procedure in R
Message-ID: <4188DF6B.7070301@evolution.uni-bonn.de>

Hello,

I was wondering if anyone could please help me with some simple 
questions regarding ANCOVA and the assumption of homogeneity of slopes.

The standard design of ANCOVA assumes the homogeneity of regression 
coefficients of the different groups. This assumption can be tested 
using the factor ?? covariate interaction, which should subsequently be 
removed. However if this assumption is not met the interaction term 
shouldn??t be removed, but then the test for group differences only tests 
for differences in intercept. This is in many cases not what was 
initially intended.

Instead, what one wants is perhaps to determine values of the covariate 
at which the groups differ. I??ve seen a description of the 
Johnson-Neyman procedure (in Huitema (1980)), which allows to determine 
the so called regions of nonsignificance, which sounds a lot like what I 
want. The problem is I have very seldom seen it used (at least in my 
field of work), but unequal slopes is a common problem. (Searching the 
R-help archives, however, didn??t give me a single match.) My first 
question is therefore if the Johnson-Neyman procedure is a recommendable 
technique. My second question is then of course if somebody knows how to 
perform it in R (also for more complex models than a two-groups, one 
covariate model).

Hope someone can help me

Wit regards

<>Leif Engqvist



From john_hendrickx at yahoo.com  Wed Nov  3 14:47:22 2004
From: john_hendrickx at yahoo.com (John Hendrickx)
Date: Wed, 3 Nov 2004 05:47:22 -0800 (PST)
Subject: [R] how to compute condition index?
In-Reply-To: <200411032012.03041.0034058@fudan.edu.cn>
Message-ID: <20041103134722.26419.qmail@web52707.mail.yahoo.com>


--- rongguiwong <0034058 at fudan.edu.cn> wrote:

> is there any existing function for computing condition index?

I've written a function called "colldiag" that calculates condition
indexes and variance decomposition proportions. It's available at
http://www.xs4all.nl/~jhckx/R/perturb/. I plan to add some features
to the "perturb" and "reclassify" functions, then upload the package
to CRAN. Until then, it's only available from my website. 

Good luck,
John Hendrickx



		
__________________________________ 


From f.harrell at vanderbilt.edu  Wed Nov  3 15:45:51 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 03 Nov 2004 08:45:51 -0600
Subject: [R] Estimating survival?
In-Reply-To: <7902152a04110218062993bcdd@mail.gmail.com>
References: <7902152a04110218062993bcdd@mail.gmail.com>
Message-ID: <4188EF1F.2070409@vanderbilt.edu>

Min-Han Tan wrote:
> Hi,
> 
> Sorry to trouble the list. I have a problem which I'm not sure how to resolve.
> 
> I have a Cox model with 1 independent variable with 2 categories (and
> thus 2 survival curves on plotting survfit)
> 
> How can I get an estimate of survival for each category at a
> particular time point, with standard error?
>  
> Looking through ?cph and ?coxph, I'm not quite sure how to go about
> that. I would really appreciate any direction here.
> 
> Thanks a million!
> 
> Min-Han

The survest function in the Design package makes this easy.  You can 
also probably do it without a lot of hassle using standard survival 
package routines. With survest you can easily specify covariate 
combinations for which to obtain predictions.

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From ggrothendieck at myway.com  Wed Nov  3 15:53:38 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 3 Nov 2004 14:53:38 +0000 (UTC)
Subject: [R] cut POSIX  results in NA - bug?
References: <4188BB0E.15342.A788CB@localhost>
Message-ID: <loom.20041103T154451-841@post.gmane.org>

Petr Pikal <petr.pikal <at> precheza.cz> writes:

: 
: Dear all
: 
: I try to make hourly average by cut() function, which almost works 
: as *I* expected. What puzled me is that if there is only one item at 
: the end of your data it results in NA.
: 
: Example will explain what I mean
: 
: datum<-seq(ISOdate(2004,8,31), ISOdate(2004,9,1), "min")
: 
: cut(datum[1370:1381],"hour", labels=F)
:  [1]  1  1  1  1  1  1  1  1  1  1  1 NA
: 
: cut(datum[1370:1382],"hour", labels=F)
:  [1] 1 1 1 1 1 1 1 1 1 1 1 2 2
: 
: I do not understand why the last item in first call is NA. I found it 
: only when there was a switch from DST to standard time as it 
: coused a trouble in one of my functions and I found there is NA 
: value where I did not expected it. 
: 
: I can make some workaround but can you please explain me why 
: first call results in NA value at the end of a vector and if it is 
: *intended* behaviour. If yes I can count with it in improvement of 
: my function(s), if not I can make some temporary workaround.
: 

Your question has already been answered but here is an alternate
approach that avoids cut.  We format the datetimes and truncate each 
string at the hour, make that a factor and then get the integer codes:

R> cutHour <- function(x) as.integer(factor(substring(format(x),1,13)))
R> cutHour(datum[1370:1381])
 [1] 1 1 1 1 1 1 1 1 1 1 1 2
R> cutHour(datum[1370:1382])
 [1] 1 1 1 1 1 1 1 1 1 1 1 2 2



From adelmaas at MUSC.EDU  Wed Nov  3 16:12:53 2004
From: adelmaas at MUSC.EDU (adelmaas@MUSC.EDU)
Date: Wed, 3 Nov 2004 10:12:53 -0500
Subject: [R] Newbie question:  plotting regression models
Message-ID: <D599F609-2DAA-11D9-9761-000A9591E11C@musc.edu>

Greetings.

Is there any way to get R to take a regression model object and draw a 
plot of the regression function?  How about overlaying that plot over a 
scatterplot of the actual data?  Thanks in advance for any help anyone 
can provide.

Aaron

-----
Aaron Solomon?? (??ben Saul Joseph??) ??Adelman
E-mail??:  ??adelmaas at musc.edu
Web site??:  ??http??://??people.musc.edu??/??~adelmaas??/??
AOL Instant Messenger?? & ??Yahoo??! ??Messenger:  ??Hiergargo



From murdoch at stats.uwo.ca  Wed Nov  3 16:30:41 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 03 Nov 2004 10:30:41 -0500
Subject: [R] Newbie question:  plotting regression models
In-Reply-To: <D599F609-2DAA-11D9-9761-000A9591E11C@musc.edu>
References: <D599F609-2DAA-11D9-9761-000A9591E11C@musc.edu>
Message-ID: <14uho0pm6qhns0q7r4o7t2bon5j06nhpo1@4ax.com>

On Wed, 3 Nov 2004 10:12:53 -0500, adelmaas at MUSC.EDU wrote :

>Greetings.
>
>Is there any way to get R to take a regression model object and draw a 
>plot of the regression function?  How about overlaying that plot over a 
>scatterplot of the actual data?  Thanks in advance for any help anyone 
>can provide.

Lots of functions in R can adapt themselves to complex objects in a
sensible way.  (These are called generic functions.)  The usual way to
draw a straight line would be to use abline(), and it can handle
linear model objects:

# fake some data
x <- 1:10
y <- rnorm(10)

# fit it and plot it.

fit <- lm(y ~ x)
plot(x, y)
abline(fit)

If you've fit a more complicated model (e.g. a quadratic), you need a
different method (because abline only works on straight lines).  Then
use

fit <- lm(y ~ x + I(x^2))
plot(x, y)
lines(predict(fit))

(This would work for the original one, too.)

Duncan Murdoch



From sundar.dorai-raj at pdf.com  Wed Nov  3 16:33:09 2004
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 03 Nov 2004 09:33:09 -0600
Subject: [R] Newbie question:  plotting regression models
In-Reply-To: <D599F609-2DAA-11D9-9761-000A9591E11C@musc.edu>
References: <D599F609-2DAA-11D9-9761-000A9591E11C@musc.edu>
Message-ID: <4188FA35.5080801@pdf.com>



adelmaas at MUSC.EDU wrote:

> Greetings.
> 
> Is there any way to get R to take a regression model object and draw a 
> plot of the regression function?  How about overlaying that plot over a 
> scatterplot of the actual data?  Thanks in advance for any help anyone 
> can provide.
> 
> Aaron
> 

Hi Aaron,

What type of "regression function" are your referring to? Linear model? 
Non-linear model? The term "regression" is to ambiguous to really answer 
your question. However, typically you should be able to do something like:

fit <- lm(y ~ x)
yhat <- predict(fit)
plot(x, y, ylim = range(c(y, yhat)))
lines(x, yhat)

If you are not using lm please provide more information than you already 
have. You should read "Introduction to R" or any of the recommended 
texts listed on the R website. Also read the posting guide.

--sundar



From petr.pikal at precheza.cz  Wed Nov  3 16:51:01 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 03 Nov 2004 16:51:01 +0100
Subject: [R] cut POSIX  results in NA - bug?
In-Reply-To: <Pine.LNX.4.44.0411031115120.20041-100000@gannet.stats>
References: <4188BB0E.15342.A788CB@localhost>
Message-ID: <41890C75.18793.1E58064@localhost>

Dear prof. Ripley

Thank you very much for explanation (without  it I would not 
consider include.lowest has something to do with my observation). 
I changed my code to get rid of single final POSIXdates.

BTW there is no mention in cut.POSIXt help page about 
include.lowest and  I think that in case of dates it does something 
what is maybe not so *understandable* (61 minutes in one hour). 

datum<-seq(ISOdate(2004,8,31), ISOdate(2004,9,1), "min")

# part of a datum variable
datum[1379:1381]
[1] "2004-09-01 12:58:00 St??edn\355 Evropa (letn\355 ??as)"   
"2004-09-01 12:59:00 St??edn\355 Evropa (letn\355 ??as)"  
[3] "2004-09-01 13:00:00 St??edn\355 Evropa (letn\355 ??as)"  
>

# the last item seems to me to belong to time from 13:00:00 to 
13:59:00 e.g. it is part of thirteen's hour of a day

cut(datum[1370:1381],"hour", include.lowest=T) 
# it will include it to previous hour

 [1] 2004-09-01 12:00:00 2004-09-01 12:00:00 2004-09-01 
12:00:00 2004-09-01 12:00:00 2004-09-01 12:00:00 2004-09-01 
12:00:00
 [7] 2004-09-01 12:00:00 2004-09-01 12:00:00 2004-09-01 
12:00:00 2004-09-01 12:00:00 2004-09-01 12:00:00 2004-09-01 
12:00:00
Levels: 2004-09-01 12:00:00

 cut(datum[1370:1381],"hour")
# this will drop it from result, correct but unfortunate 

 [1] 2004-09-01 12:00:00 2004-09-01 12:00:00 2004-09-01 
12:00:00 2004-09-01 12:00:00 2004-09-01 12:00:00 2004-09-01 
12:00:00
 [7] 2004-09-01 12:00:00 2004-09-01 12:00:00 2004-09-01 
12:00:00 2004-09-01 12:00:00 2004-09-01 12:00:00 <NA>               
Levels: 2004-09-01 12:00:00

# so as a result an hour can have 61 minutes
levels(cut(datum[1321:1381],"hour", include.lowest=T))
[1] "2004-09-01 12:00:00"

length(cut(datum[1321:1381],"hour", include.lowest=T)) #???
[1] 61

Is it correct?

Thank you again.

Best regards
Petr Pikal


On 3 Nov 2004 at 11:20, Prof Brian Ripley wrote:

> On Wed, 3 Nov 2004, Petr Pikal wrote:
> 
> > Dear all
> > 
> > I try to make hourly average by cut() function, which almost works
> > as *I* expected. What puzled me is that if there is only one item at
> > the end of your data it results in NA.
> > 
> > Example will explain what I mean
> > 
> > datum<-seq(ISOdate(2004,8,31), ISOdate(2004,9,1), "min")
> > 
> > cut(datum[1370:1381],"hour", labels=F)
> >  [1]  1  1  1  1  1  1  1  1  1  1  1 NA
> > 
> > cut(datum[1370:1382],"hour", labels=F)
> >  [1] 1 1 1 1 1 1 1 1 1 1 1 2 2
> > 
> > I do not understand why the last item in first call is NA. I found
> > it only when there was a switch from DST to standard time as it
> > coused a trouble in one of my functions and I found there is NA
> > value where I did not expected it. 
> 
> cut(datum[1370:1381],"hour", labels=F, include.lowest=T)
> 
> is what you need.  See ?cut, in the See Also, which says
> 
> include.lowest: logical, indicating if an 'x[i]' equal to the lowest
>           (or highest, for 'right = FALSE') 'breaks' value should be
>           included.
> 
> > I can make some workaround but can you please explain me why 
> > first call results in NA value at the end of a vector and if it is
> > *intended* behaviour. 
> 
> It is the documented behaviour, for better or for worse.
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self) 1 South
> Parks Road,                     +44 1865 272866 (PA) Oxford OX1 3TG,
> UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From dmb at mrc-dunn.cam.ac.uk  Wed Nov  3 16:55:23 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Wed, 3 Nov 2004 15:55:23 +0000 (GMT)
Subject: [R] Legend placement in barplot?
Message-ID: <Pine.LNX.4.21.0411031552540.18231-100000@mail.mrc-dunn.cam.ac.uk>


This has been asked before, but all the answers are hidiously complex.

The

legend.text=TRUE 

option of barplot is almost exactly what I need, except I need a

legend.placement='tl'

(top left) option. This option would be in contrast to the default
placement which we could call 'tr' (top right).

Anyone know how to edit the barplot code to make this change? Could
someone like me work out how to do this?

Cheers,
Dan.



From abunn at whrc.org  Wed Nov  3 16:56:19 2004
From: abunn at whrc.org (Andy Bunn)
Date: Wed, 3 Nov 2004 10:56:19 -0500
Subject: [R] Newbie question:  plotting regression models
In-Reply-To: <D599F609-2DAA-11D9-9761-000A9591E11C@musc.edu>
Message-ID: <NEBBIPHDAMMOKDKPOFFIEEIJCLAA.abunn@whrc.org>

Welcome to R.

You can start by looking at the predict function for the regression model you are using (e.g., ?predict.lm if you are using a linear model).

Then do something like so:

data(cars)
cars.lm <- lm(dist ~ speed, cars)
dist.pred <- predict(cars.lm)
plot(cars$speed, cars$dist, ylim = c(-2, 120))
lines(cars$speed, dist.pred, col = "red", lwd = 2)


HTH, Andy



From etptupaf at bs.ehu.es  Wed Nov  3 17:06:22 2004
From: etptupaf at bs.ehu.es (F.Tusell)
Date: Wed, 03 Nov 2004 17:06:22 +0100
Subject: [R] Princomp(), prcomp() and loadings()
Message-ID: <1099497982.9822.21.camel@agesi.bs.ehu.es>

In comparing the results of princomp and prcomp I find:

   1. The reported standard deviations are similar but about 1% from
      each other, which seems well above round-off error.
   2. princomp returns what I understand are variances and cumulative
      variances accounted for by each principal component which are
      all equal. "SS loadings" is always 1. 
   3. Same happens after the loadings are varimax-rotated, which in 
      general should alter the proportions of variance accounted by 
      each component.

It looks as if the loadings() function were expecting the eigenvectors
to be normalized to the corresponding eigenvalue.

Transcript and version information follow signature. Thank you for any
clues.

ft.
-- 
Fernando TUSELL                                e-mail:
Departamento de Econometr??a y Estad??stica           etptupaf at bs.ehu.es 
Facultad de CC.EE. y Empresariales             Tel:   (+34)94.601.3733
Avenida Lendakari Aguirre, 83                  Fax:   (+34)94.601.3754
E-48015 BILBAO  (Spain)                        Secr:  (+34)94.601.3740
----------------------------------------------------------------------




> pca.1 <- prcomp(USArrests)
> pca.1
Standard deviations:
[1] 83.732400 14.212402  6.489426  2.482790

Rotation:
                PC1         PC2         PC3         PC4
Murder   0.04170432 -0.04482166  0.07989066 -0.99492173
Assault  0.99522128 -0.05876003 -0.06756974  0.03893830
UrbanPop 0.04633575  0.97685748 -0.20054629 -0.05816914
Rape     0.07515550  0.20071807  0.97408059  0.07232502
> pca.2 <- princomp(USArrests)
> pca.2
Call:
princomp(x = USArrests)

Standard deviations:
   Comp.1    Comp.2    Comp.3    Comp.4 
82.890847 14.069560  6.424204  2.457837 

 4  variables and  50 observations.
> summary(pca.2)
Importance of components:
                           Comp.1      Comp.2      Comp.3       Comp.4
Standard deviation     82.8908472 14.06956001 6.424204055 2.4578367034
Proportion of Variance  0.9655342  0.02781734 0.005799535 0.0008489079
Cumulative Proportion   0.9655342  0.99335156 0.999151092 1.0000000000
> loadings(pca.2)

Loadings:
         Comp.1 Comp.2 Comp.3 Comp.4
Murder                         0.995
Assault  -0.995                     
UrbanPop        -0.977 -0.201       
Rape            -0.201  0.974       

               Comp.1 Comp.2 Comp.3 Comp.4
SS loadings      1.00   1.00   1.00   1.00
Proportion Var   0.25   0.25   0.25   0.25
Cumulative Var   0.25   0.50   0.75   1.00

> varimax(pca.2$loadings[,1:3])
$loadings

Loadings:
         Comp.1 Comp.2 Comp.3
Murder                       
Assault  -0.998              
UrbanPop        -0.997       
Rape                    0.995

               Comp.1 Comp.2 Comp.3
SS loadings      1.00   1.00   1.00
Proportion Var   0.25   0.25   0.25
Cumulative Var   0.25   0.50   0.75

$rotmat
            [,1]       [,2]       [,3]
[1,]  0.99211386 0.03604908 -0.1200439
[2,] -0.05442524 0.98664663 -0.1535132
[3,]  0.11290692 0.15883603  0.9808278

> R.Version()
$platform
[1] "i386-pc-linux-gnu"

$arch
[1] "i386"

$os
[1] "linux-gnu"

$system
[1] "i386, linux-gnu"

$status
[1] ""

$major
[1] "2"

$minor
[1] "0.0"

$year
[1] "2004"

$month
[1] "10"

$day
[1] "04"

$language
[1] "R"



From kadusei at uni-goettingen.de  Wed Nov  3 17:03:53 2004
From: kadusei at uni-goettingen.de (Kwabena Adusei-Poku)
Date: Wed, 3 Nov 2004 17:03:53 +0100
Subject: [R] Kernel Density estimation - locfit package
Message-ID: <000e01c4c1be$b7662c10$aaad4c86@wso1809r>

Hello there,

I am presently using the locfit package in "R" and would appreciate some
help here. Could anyone tell me tell me how I can obtain the various
components (x and y values)of the density estimation after using the "R"
command "locfit"? For example, with the command
"fhat<-density(somename)" I can obtain the x and y values by simply
calling "fhat$x" and "fhat$y". This doesn't seems to work with the
"locfit" command.

Thank you in advance.

Regards,

Kwabena 

--------------------------------------------
Kwabena Adusei-Poku
University of Goettingen
Institute of Statistics and Econometrics
Platz der Goettingen Sieben 5
37073 Goettingen
Germany
Tel: +49-(0)551-394794



From kwright at eskimo.com  Wed Nov  3 17:06:59 2004
From: kwright at eskimo.com (kwright@eskimo.com)
Date: Wed, 3 Nov 2004 08:06:59 -0800 (PST)
Subject: [R] Suggested color schemes for points, not regions?
Message-ID: <3058.66.185.0.209.1099498019.squirrel@66.185.0.209>


I have seen a couple of posts about color schemes like those at the
ColorBrewer site.  Most recently:
http://geography.uoregon.edu/datagraphics/color_scales.htm

These color schemes can work very well for regions (bars, polygons,
images, etc.) but are not very suitable for points and/or lines.

Is anyone aware of research/suggestions for a color scheme to use for
scatter plots?  I've looked at great length and have found little on this
topic.

My current scheme of choice is a set of fairly saturated colors along the
lines of:
navy
brown/orange
black
purple
red
medium green
This is similar to the 'paired' color scheme, but using only the saturated
colors and substituting black for yellow.  Depending on circumstances, I
sometimes use a different glyph for each color.  The hard part about all
this is to make sure that each color/glyph combination has the same
'attention-getting' power.

Any discussion or comments are welcome.

Kevin Wright



From tlumley at u.washington.edu  Wed Nov  3 17:16:07 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 3 Nov 2004 08:16:07 -0800 (PST)
Subject: [R] Color schemes that work for people with color-deficient vision
In-Reply-To: <200411022104.iA2L4mF19027@snow.pnl.gov>
References: <200411022104.iA2L4mF19027@snow.pnl.gov>
Message-ID: <Pine.A41.4.61b.0411030812240.320496@homer04.u.washington.edu>

On Tue, 2 Nov 2004, Scott Waichler wrote:

> A recent article in the earth science literature cited below and available at
> http://geography.uoregon.edu/datagraphics/EOS/
> points out that rainbow color schemes and mixtures of green and yellow
> can be troublesome for people with color-deficient vision.
> The authors propose alternative schemes that can be viewed and downloaded
> in RGB, HSV, CMYK, and RGB256 formats from
> http://geography.uoregon.edu/datagraphics/color_scales.htm.
> I have translated their RGB definitions into the hex color names given below.

I'll add these to the dichromat package if they don't go in anywhere else.

Also note that the ColorBrewer colors scheme at www.colorbrewer.org have 
comments on which ones work for color-deficient viewers and also for 
other problem settings (eg cheap inkjet printers, digital projects, B&W 
photocopies).  These color schemes, though not the metadata, are in the 
RColorBrewer package.

 	-thomas



From tlumley at u.washington.edu  Wed Nov  3 17:24:20 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 3 Nov 2004 08:24:20 -0800 (PST)
Subject: [R] time dependency of Cox regression
In-Reply-To: <41883A7C.2070508@vanderbilt.edu>
References: <20041102234105.21112.qmail@web40827.mail.yahoo.com>
	<41883A7C.2070508@vanderbilt.edu>
Message-ID: <Pine.A41.4.61b.0411030818530.320496@homer04.u.washington.edu>

> array chip wrote:
>> Hi,
>> 
>> How can I specify a Cox proportional hazards model
>> with a covariate which i believe its strength on
>> survival changes/diminishes with time? The value of
>> the covariate was only recorded once at the beginning
>> of the study for each individual (e.g. at the
>> diagnosis of the disease), so I do not have the time
>> course data of the covariate for any given individual.
>> For example, I want to state at the end of the
>> analysis that the hazard ratio of the covariate is 6
>> at the beginning, decrease to 3 after 2 years and
>> decrease to 1.5 after 5 years.


If you fit a Cox model with the fixed covariate, plot(cox.zph(model)) will 
show you an estimate of how the log hazard ratio changes over time, with 
pointwise confidence intervals.

If you want more precise estimates and confidence intervals you can split 
up your covariate into a set of time-dependent covariates.

If you wanted a time period for each year up to 6 years you would make 6 
time dependent covariates, looking like

  x 0 0 0 0 0
  0 x 0 0 0 0
  0 0 x 0 0 0
  0 0 0 x 0 0
  0 0 0 0 x 0
  0 0 0 0 0 x

and have (up to) six records per person.  The survSplit() function in the 
survival package will do the splitting, you then need to set the 
appropriate terms to zero and fit the model.

 	-thomas



From sundar.dorai-raj at pdf.com  Wed Nov  3 17:24:52 2004
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 03 Nov 2004 10:24:52 -0600
Subject: [R] Princomp(), prcomp() and loadings()
In-Reply-To: <1099497982.9822.21.camel@agesi.bs.ehu.es>
References: <1099497982.9822.21.camel@agesi.bs.ehu.es>
Message-ID: <41890654.3080601@pdf.com>



F.Tusell wrote:

> In comparing the results of princomp and prcomp I find:
> 
>    1. The reported standard deviations are similar but about 1% from
>       each other, which seems well above round-off error.
>    2. princomp returns what I understand are variances and cumulative
>       variances accounted for by each principal component which are
>       all equal. "SS loadings" is always 1. 
>    3. Same happens after the loadings are varimax-rotated, which in 
>       general should alter the proportions of variance accounted by 
>       each component.
> 
> It looks as if the loadings() function were expecting the eigenvectors
> to be normalized to the corresponding eigenvalue.
> 
> Transcript and version information follow signature. Thank you for any
> clues.
> 

Did you read the corresponding help files?

from ?prcomp:

<quote>
Details:

      The calculation is done by a singular value decomposition of the
      (centered and possibly scaled) data matrix, not by using 'eigen'
      on the covariance matrix.  This is generally the preferred method
      for numerical accuracy.
</quote>

from ?princomp:

<quote>
Details:

      The calculation is done using 'eigen' on the correlation or
      covariance matrix, as determined by 'cor'.  This is done for
      compatibility with the S-PLUS result.  A preferred method of
      calculation is to use 'svd' on 'x', as is done in 'prcomp'.
</quote>

HTH,

--sundar



From tlumley at u.washington.edu  Wed Nov  3 17:28:17 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 3 Nov 2004 08:28:17 -0800 (PST)
Subject: [R] Estimating survival?
In-Reply-To: <7902152a04110218062993bcdd@mail.gmail.com>
References: <7902152a04110218062993bcdd@mail.gmail.com>
Message-ID: <Pine.A41.4.61b.0411030827060.320496@homer04.u.washington.edu>

On Tue, 2 Nov 2004, Min-Han Tan wrote:

> Hi,
>
> Sorry to trouble the list. I have a problem which I'm not sure how to resolve.
>
> I have a Cox model with 1 independent variable with 2 categories (and
> thus 2 survival curves on plotting survfit)
>
> How can I get an estimate of survival for each category at a
> particular time point, with standard error?
>

Here's an example

>    fit <- coxph( Surv(futime,fustat)~rx,data=ovarian)

>    summary(  survfit( fit, newdata=list(rx=c(1,2))))
Call: survfit.coxph(object = fit, newdata = list(rx = c(1, 2)))

  time n.risk n.event survival1 survival2
    59     26       1     0.952     0.973
   115     25       1     0.903     0.945
   156     24       1     0.855     0.917
   268     23       1     0.806     0.888
   329     22       1     0.758     0.858
   353     21       1     0.710     0.828
   365     20       1     0.663     0.797
   431     17       1     0.613     0.764
   464     15       1     0.560     0.727
   475     14       1     0.509     0.689
   563     12       1     0.454     0.648
   638     11       1     0.403     0.606


 	-thomas



From kjetil at acelerate.com  Wed Nov  3 17:32:08 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Wed, 03 Nov 2004 12:32:08 -0400
Subject: [R] how to compute condition index?
In-Reply-To: <200411032012.03041.0034058@fudan.edu.cn>
References: <200411032012.03041.0034058@fudan.edu.cn>
Message-ID: <41890808.7010504@acelerate.com>

rongguiwong wrote:

>is there any existing function for computing condition index?
>  
>
See
?kappa

Kjetil

>" analysing multivariate data" say that we can use condition index to check 
>multicollinearity.saying that we can get it via SVD. The elements of the 
>diagnoal matrix are the standard deviations of the uncorrelated vectors. the 
>condition index is the ratio of the largest of these numbers to the smallest.
>so if i have a data frame a,containg variables x,y,z.
>my model is :
>model<-lm(y~x+z,data=a)
>so use the following to compute the condition index,but it seems wrong.
>
>temp<-svd(model.matrix(model))$d
>max(temp)/min(temp)
>
>is it wrong?
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From ripley at stats.ox.ac.uk  Wed Nov  3 17:34:57 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 3 Nov 2004 16:34:57 +0000 (GMT)
Subject: [R] Princomp(), prcomp() and loadings()
In-Reply-To: <1099497982.9822.21.camel@agesi.bs.ehu.es>
Message-ID: <Pine.LNX.4.44.0411031625090.23721-100000@gannet.stats>

On Wed, 3 Nov 2004, F.Tusell wrote:

> In comparing the results of princomp and prcomp I find:
> 
>    1. The reported standard deviations are similar but about 1% from
>       each other, which seems well above round-off error.

That is explained on the help page!  E.g.

     Note that the default calculation uses divisor 'N' for the
     covariance matrix.

and there is even an example:

     princomp(USArrests, cor = TRUE) # =^= prcomp(USArrests, scale=TRUE)
     ## Similar, but different:
     ## The standard deviations differ by a factor of sqrt(49/50)

>    2. princomp returns what I understand are variances and cumulative
>       variances accounted for by each principal component which are
>       all equal. "SS loadings" is always 1. 
>    3. Same happens after the loadings are varimax-rotated, which in 
>       general should alter the proportions of variance accounted by 
>       each component.

Hmmm.  Varimax rotation of PCA (not factor analysis) is not supported in
base R, so this is not surprising.  Please do as the posting guide asks,
and read the help page (even its title!) before posting.

> It looks as if the loadings() function were expecting the eigenvectors
> to be normalized to the corresponding eigenvalue.
> 
> Transcript and version information follow signature. Thank you for any
> clues.

The best clue is that the help pages are a very useful resource, but need 
to be read as carefully as they were written.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sundar.dorai-raj at pdf.com  Wed Nov  3 17:41:16 2004
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 03 Nov 2004 10:41:16 -0600
Subject: [R] Kernel Density estimation - locfit package
In-Reply-To: <000e01c4c1be$b7662c10$aaad4c86@wso1809r>
References: <000e01c4c1be$b7662c10$aaad4c86@wso1809r>
Message-ID: <41890A2C.1@pdf.com>



Kwabena Adusei-Poku wrote:

> Hello there,
> 
> I am presently using the locfit package in "R" and would appreciate some
> help here. Could anyone tell me tell me how I can obtain the various
> components (x and y values)of the density estimation after using the "R"
> command "locfit"? For example, with the command
> "fhat<-density(somename)" I can obtain the x and y values by simply
> calling "fhat$x" and "fhat$y". This doesn't seems to work with the
> "locfit" command.
> 
> Thank you in advance.
> 
> Regards,
> 
> Kwabena 

On the ?locfit help page it says:

      An object with class '"locfit"'. A standard set of methods for
      printing, ploting, etc. these objects is provided.

This implies that there should be a fitted.locfit method. And, indeed, 
there is:

data(ethanol)
x <- ethanol$E
fit <- locfit(~ x)
plot(x[order(x)], fitted(fit)[order(x)], type = "n")

HTH,

--sundar



From MSchwartz at MedAnalytics.com  Wed Nov  3 17:45:11 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Wed, 03 Nov 2004 10:45:11 -0600
Subject: [R] Legend placement in barplot?
In-Reply-To: <Pine.LNX.4.21.0411031552540.18231-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0411031552540.18231-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <1099500311.8211.46.camel@localhost.localdomain>

On Wed, 2004-11-03 at 09:55, Dan Bolser wrote:
> This has been asked before, but all the answers are hidiously complex.
> 
> The
> 
> legend.text=TRUE 
> 
> option of barplot is almost exactly what I need, except I need a
> 
> legend.placement='tl'
> 
> (top left) option. This option would be in contrast to the default
> placement which we could call 'tr' (top right).
> 
> Anyone know how to edit the barplot code to make this change? Could
> someone like me work out how to do this?
> 
> Cheers,
> Dan.

Dan,

Do not edit the barplot() code. Use the legend() function instead, which
enables you to specify the x,y coordinates of the upper left hand corner
of the legend box. See ?legend

A fair number of the questions that you have had regarding graphics are
covered in Chapter 12 "Graphical Procedures" in "An Introduction to R":

http://cran.r-project.org/doc/manuals/R-intro.pdf

which is included with the R installation.

Another online resource for some graphics assistance would be R News
Volume 3 Number 2 for October 2003, which has an article on R's base
graphics in the R Help Desk section:

http://cran.r-project.org/doc/Rnews/Rnews_2003-2.pdf

Notwithstanding all of that, searching the r-help archives is yet
another terrific online (and free) resource that you _should_ avail
yourself of.

HTH,

Marc Schwartz



From arrayprofile at yahoo.com  Wed Nov  3 18:08:43 2004
From: arrayprofile at yahoo.com (array chip)
Date: Wed, 3 Nov 2004 09:08:43 -0800 (PST)
Subject: [R] time dependency of Cox regression
In-Reply-To: <Pine.A41.4.61b.0411030818530.320496@homer04.u.washington.edu>
Message-ID: <20041103170843.28857.qmail@web40810.mail.yahoo.com>

Thanks very much for the suggestion. still some
quiestions. In your example of splitting the covariate
into 6 time-dependent covariates (6 records per
person), will the survival time and censored status be
the same for each of the 6 records? If that's the
case, how does the model know that each of the 6
time-dependent covariates corresponds to 6 consecutive
time points? 

I am thinking about create a dummy factor variable
called "time" to indicate which time interval each
patient's survival time is in. For example, if a
patient's survival time is less than 2 years, then the
dummy variable is 2, and so on for each patient. Then
I specify a covariate x time interaction term in the
Cox regression. I would assume the Cox regression will
return a separte hazard ratio for each level of the
dummy factor variable which corresponds to the hazard
ratio of each year. Is this a reasonable way to do it?

Thanks

--- Thomas Lumley <tlumley at u.washington.edu> wrote:

> > array chip wrote:
> >> Hi,
> >> 
> >> How can I specify a Cox proportional hazards
> model
> >> with a covariate which i believe its strength on
> >> survival changes/diminishes with time? The value
> of
> >> the covariate was only recorded once at the
> beginning
> >> of the study for each individual (e.g. at the
> >> diagnosis of the disease), so I do not have the
> time
> >> course data of the covariate for any given
> individual.
> >> For example, I want to state at the end of the
> >> analysis that the hazard ratio of the covariate
> is 6
> >> at the beginning, decrease to 3 after 2 years and
> >> decrease to 1.5 after 5 years.
> 
> 
> If you fit a Cox model with the fixed covariate,
> plot(cox.zph(model)) will 
> show you an estimate of how the log hazard ratio
> changes over time, with 
> pointwise confidence intervals.
> 
> If you want more precise estimates and confidence
> intervals you can split 
> up your covariate into a set of time-dependent
> covariates.
> 
> If you wanted a time period for each year up to 6
> years you would make 6 
> time dependent covariates, looking like
> 
>   x 0 0 0 0 0
>   0 x 0 0 0 0
>   0 0 x 0 0 0
>   0 0 0 x 0 0
>   0 0 0 0 x 0
>   0 0 0 0 0 x
> 
> and have (up to) six records per person.  The
> survSplit() function in the 
> survival package will do the splitting, you then
> need to set the 
> appropriate terms to zero and fit the model.
> 
>  	-thomas
>



From ggrothendieck at myway.com  Wed Nov  3 18:30:23 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 3 Nov 2004 17:30:23 +0000 (UTC)
Subject: [R] Legend placement in barplot?
References: <Pine.LNX.4.21.0411031552540.18231-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <loom.20041103T182921-376@post.gmane.org>

Dan Bolser <dmb <at> mrc-dunn.cam.ac.uk> writes:

: 
: This has been asked before, but all the answers are hidiously complex.
: 
: The
: 
: legend.text=TRUE 
: 
: option of barplot is almost exactly what I need, except I need a
: 
: legend.placement='tl'
: 
: (top left) option. This option would be in contrast to the default
: placement which we could call 'tr' (top right).
: 
: Anyone know how to edit the barplot code to make this change? Could
: someone like me work out how to do this?
: 


In package gplots (in bundle gregmisc in R 2.0.0) there is smartlegend.



From tlumley at u.washington.edu  Wed Nov  3 19:38:48 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 3 Nov 2004 10:38:48 -0800 (PST)
Subject: [R] time dependency of Cox regression
In-Reply-To: <20041103170843.28857.qmail@web40810.mail.yahoo.com>
References: <20041103170843.28857.qmail@web40810.mail.yahoo.com>
Message-ID: <Pine.A41.4.61b.0411031035330.364448@homer10.u.washington.edu>

On Wed, 3 Nov 2004, array chip wrote:

> Thanks very much for the suggestion. still some
> quiestions. In your example of splitting the covariate
> into 6 time-dependent covariates (6 records per
> person), will the survival time and censored status be
> the same for each of the 6 records? If that's the
> case, how does the model know that each of the 6
> time-dependent covariates corresponds to 6 consecutive
> time points?

No, it won't.  This is what survSplit handles.  If someone dies in the 
middle of year 4 they will have records

start stop event
0      1    0
1      2    0
2      3    0
3      4    0
4      4.5  1

and no record for subsequent years

> I am thinking about create a dummy factor variable
> called "time" to indicate which time interval each
> patient's survival time is in. For example, if a
> patient's survival time is less than 2 years, then the
> dummy variable is 2, and so on for each patient. Then
> I specify a covariate x time interaction term in the
> Cox regression. I would assume the Cox regression will
> return a separte hazard ratio for each level of the
> dummy factor variable which corresponds to the hazard
> ratio of each year. Is this a reasonable way to do it?

No. This doesn't work unless you also split the records.  The problem is 
that the person is labelled as dying in year 2 at every time point, but 
you only want to change the risk during year 2.


It is quite possible in principle to allow for arbitrary functional time 
dependence in a Cox model, but the R implementation doesn't.  I have an 
implementation that does, but it's in Xlispstat.

 	-thomas



From ggrothendieck at myway.com  Wed Nov  3 19:48:48 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 3 Nov 2004 18:48:48 +0000 (UTC)
Subject: [R] Legend placement in barplot?
References: <Pine.LNX.4.21.0411031552540.18231-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <loom.20041103T194633-105@post.gmane.org>

Dan Bolser <dmb <at> mrc-dunn.cam.ac.uk> writes:

: 
: This has been asked before, but all the answers are hidiously complex.
: 
: The
: 
: legend.text=TRUE 
: 
: option of barplot is almost exactly what I need, except I need a
: 
: legend.placement='tl'
: 
: (top left) option. This option would be in contrast to the default
: placement which we could call 'tr' (top right).
: 
: Anyone know how to edit the barplot code to make this change? Could
: someone like me work out how to do this?
: 
: Cheers,
: Dan.


Check out gplots::smartlegend (in the R 2.0.0 gregmisc bundle).



From laifang1 at yahoo.com  Wed Nov  3 20:04:49 2004
From: laifang1 at yahoo.com (fang lai)
Date: Wed, 3 Nov 2004 11:04:49 -0800 (PST)
Subject: [R] (no subject)
Message-ID: <20041103190449.60148.qmail@web50408.mail.yahoo.com>

Hi there,
 When I ran the wilcox.exact test, it always shows
"number of items to replace is not a multiple of
replacement length"
 However, according to the help it seems to be able to
handle sample of different size.
What does this message indicate, and will it cause a
problem of the resulting p-value?
 Also, when I run the ks.test(), it shows sometimes
that 
cannot compute correct p-values with ties in:
ks.test(income.perm[(data[, 3] == 59) & (data[, 2] ==
1)], income.perm[(data[,   ...

And sometime it shows results and sometimes it just
return NULL. When it shows the results, does it mean
that it had some kind of continuity correction or
other adjustment so that I could believe the results? 
Many thanks,

Fang 


=====
Lai, Fang

PhD candidate
University of California, Berkeley
Department of Agricultural and Resource Economics
314 Giannini Hall, Berkeley, CA 94720-3310
tel: (510) 643 - 5421(O)
     (510) 847 - 9811(Cell)
fax: (510) 643 - 8911
email: lai at are.berkeley.edu
http://www.are.berkeley.edu/jobmarket/fang.html



From gunter.berton at gene.com  Wed Nov  3 20:21:42 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 3 Nov 2004 11:21:42 -0800
Subject: [R] Help Pages kvetch (was: re Princomp(),
	prcomp() and loadings()  )
In-Reply-To: <Pine.LNX.4.44.0411031625090.23721-100000@gannet.stats>
Message-ID: <200411031921.iA3JLgMl018833@volta.gene.com>

My apologies: This is an R-help kvetch only. 

<kvetch>

I would like to forcefully highlight Brian Ripley's remark:

> The best clue is that the help pages are a very useful 
> resource, but need 
> to be read as carefully as they were written.

This is truly the case (at least for the standard R distribution packages)!!
The help pages are remarkably well written and more often than not include
very informative examples (e.g., plotmath()). Both they and the FAQ's are
valuable resources for "newbies' and longtime R users with porous brains
like me. Hence, I continue to be amazed and disappointed by the number of
questions this list receives that could have been answered by carefully
reading these resources.

So may I make a suggestion: The standard script added to all postings is:
<quote>
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
</quote>

However, this requires the user to take the extra time and effort to
actually peruse the guide, where the admonition to read Help Pages is
contained. Apparently, many fail to do this. So my suggestion is: modify the
above script to:

<quote>
The Help pages provide full,carefully written documentation and numerous
illustrative examples. The FAQ's contain further useful information. PLEASE
consult these resources before posting questions and PLEASE do read the
posting guide, http://www.R-project.org/posting-guide.html, so that the
questions you post can be better answered.
</quote>

I know that this is rather prolix (perhaps others can do better at it), but
maybe it will more directly engage prospective posters and thereby avoid the
need for Brian's and many others frequent reminders. Or do I delude myself
-- again?!

</kvetch>

Cheers,
Bert Gunter



From bates at stat.wisc.edu  Wed Nov  3 21:56:30 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 03 Nov 2004 14:56:30 -0600
Subject: [R] Newbie question:  plotting regression models
In-Reply-To: <NEBBIPHDAMMOKDKPOFFIEEIJCLAA.abunn@whrc.org>
References: <NEBBIPHDAMMOKDKPOFFIEEIJCLAA.abunn@whrc.org>
Message-ID: <418945FE.3050103@stat.wisc.edu>

Regarding plotting a regression fit - for a simple linear regression 
model the abline function adds the fitted line to a plot.

plot(optden ~ carb, Formaldehyde)
abline(fm1 <- lm(optden ~ carb, Formaldehyde))



From s02mjtj at math.ku.dk  Wed Nov  3 22:59:11 2004
From: s02mjtj at math.ku.dk (Mads Jeppe Tarp-Johansen)
Date: Wed, 3 Nov 2004 22:59:11 +0100 (CET)
Subject: [R] fold right - recursive list (vector) operators
Message-ID: <Pine.LNX.4.40.0411032249080.17054-100000@shannon.math.ku.dk>


The programming language mosml comes with foldr that 'accumulates' a
function f over a list [x1,x2,...,xn] with initial value b as follows

foldr f b [x1,x2,...,xn]  =  f(x1,...,f(xn-1,f(xn,b))...)

Observe that "list" should have same elements so in R terminology it would
perhaps be appropriate to say that the accumulation takes place over a
'vector'.

I wonder if R comes with a similar function and in general a library or
package with list (vector) operators. Or is such programming style not
intended in R?

Regards MJ



From david.whiting at ncl.ac.uk  Wed Nov  3 23:08:05 2004
From: david.whiting at ncl.ac.uk (David Whiting)
Date: 03 Nov 2004 22:08:05 +0000
Subject: [R] A little more on R, mdbtools and Access databases
In-Reply-To: <Pine.LNX.4.60.0411031743200.22203@sasquatch>
References: <Pine.LNX.4.60.0411031743200.22203@sasquatch>
Message-ID: <m27jp2a8my.fsf@ganymede.ammp.or.tz>


Anne York <york at zipcon.net> writes:

> Wow, thanks for all the good information and ideas on this topic.
> 
> Have you used mdbtools to convert a data base to something like MySQL
> or other open source data base?  

Yes, that's mostly how I have used mdbtools so far.  I modified
mdb-export and created a script to use my modified mdb-export and
mdb-schema to act like MySQL's mysqldump, i.e. to produce CREATE TABLE
and INSERT statements. That way I was able to get tables straight from
an Access database into MySQL. Here's a link to what I did:

http://sourceforge.net/tracker/?group_id=2294&atid=302294&func=detail&aid=857342

With those changes and the mdbdump script I am able to do the
following to get a table into MySQL:

mdbdump accessDB accessTable | mysql mysqlDB


As it is now mdb-export produces output in delimited format so you
could use mdb-schema to create the table, export the data using
mdb-export and then load it into mysql (or whatever).

[it looks like the INSERT statements option has been added to
mdb-export, but I am not sure about the MySQL backend]

> It seems that if I needed to do
> unions or intersections, I would have to do the conversion and use the
> other database from R since I'd rather work on my Linux machine.

Another alternative would be to use mdbtools with the R functions I
posted earlier to read the tables into R and then use RODBC to save
(sqlSave) them into another database.  This would probably be the
easiest because all you need to do is get the current mdb-tools as it
is now (my changes were made to an old version).  This is now the my
preferred route. BTW, I have now tidied the functions up a little so
they don't create temporary files anymore.

> 
> On the negative side, Brian Ripley reported problems compiling
> mdbtools, and on the Debian website, there were some security alerts
> (overflow problems) about mdbtools. Clearly, you were able to compile
> mdbtools. 

I had to make some symlinks to get it all to play properly (it seemed
to expect libraries in /usr/lib when they were in /usr/local/lib), but
it seemed to compile okay.  The lib problem might have been a problem
on my side though.  

> Do you know anything about the security risks?

Not a sausage. I use on my local machine so I don't think that they
apply to my situation. I could easily be wrong though...

Dave


-- 
David Whiting
University of Newcastle upon Tyne, UK



From jfox at mcmaster.ca  Wed Nov  3 23:18:48 2004
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 03 Nov 2004 17:18:48 -0500
Subject: [R] fold right - recursive list (vector) operators
In-Reply-To: <Pine.LNX.4.40.0411032249080.17054-100000@shannon.math.ku.dk>
Message-ID: <web-71258333@cgpsrv2.cis.mcmaster.ca>

Dear MJ,

If I follow correctly what you want to do, then the following should
work:

foldr <- function(f, x){
      if (length(x) > 1) f(c(x[1], Recall(f, x[-1])))
      else f(x)
      }

For example,

> sum(c(4, sum(c(2, sum(6)))))
[1] 12
> foldr(sum, c(4,2,6))
[1] 12

I hope this helps,
 John

On Wed, 3 Nov 2004 22:59:11 +0100 (CET)
 Mads Jeppe Tarp-Johansen <s02mjtj at math.ku.dk> wrote:
> 
> The programming language mosml comes with foldr that 'accumulates' a
> function f over a list [x1,x2,...,xn] with initial value b as follows
> 
> foldr f b [x1,x2,...,xn]  =  f(x1,...,f(xn-1,f(xn,b))...)
> 
> Observe that "list" should have same elements so in R terminology it
> would
> perhaps be appropriate to say that the accumulation takes place over
> a
> 'vector'.
> 
> I wonder if R comes with a similar function and in general a library
> or
> package with list (vector) operators. Or is such programming style
> not
> intended in R?
> 
> Regards MJ
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada
http://socserv.mcmaster.ca/jfox/



From tlumley at u.washington.edu  Wed Nov  3 23:26:16 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 3 Nov 2004 14:26:16 -0800 (PST)
Subject: [R] fold right - recursive list (vector) operators
In-Reply-To: <Pine.LNX.4.40.0411032249080.17054-100000@shannon.math.ku.dk>
References: <Pine.LNX.4.40.0411032249080.17054-100000@shannon.math.ku.dk>
Message-ID: <Pine.A41.4.61b.0411031420150.364448@homer10.u.washington.edu>

On Wed, 3 Nov 2004, Mads Jeppe Tarp-Johansen wrote:

>
> The programming language mosml comes with foldr that 'accumulates' a
> function f over a list [x1,x2,...,xn] with initial value b as follows
>
> foldr f b [x1,x2,...,xn]  =  f(x1,...,f(xn-1,f(xn,b))...)
>
> Observe that "list" should have same elements so in R terminology it would
> perhaps be appropriate to say that the accumulation takes place over a
> 'vector'.
>
> I wonder if R comes with a similar function and in general a library or
> package with list (vector) operators. Or is such programming style not
> intended in R?
>

Only a few such second-order functions are built in, eg sapply() and 
mapply().

For short vectors it is easy to write recursive implementations of most of 
them, eg your foldr function:

reduce<-function(f,b,x){
         n<-length(x)
 	if (n==1)
  	   f(x,b)
         else
 	   f(x[1],reduce(f,b,x[-1]))
}

For longer vectors you need an iterative implementation.
eg
reduce<-function(f,b,x){
     n<-length(x)
     rval<-f(x[n],b)
     if (n>1){
       for (xn in rev(x[-1]))
   	rval<-f(xn,rval)
       }
     return(rval)
}

 	-thomas



From p.dalgaard at biostat.ku.dk  Wed Nov  3 23:33:03 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 03 Nov 2004 23:33:03 +0100
Subject: [R] fold right - recursive list (vector) operators
In-Reply-To: <Pine.LNX.4.40.0411032249080.17054-100000@shannon.math.ku.dk>
References: <Pine.LNX.4.40.0411032249080.17054-100000@shannon.math.ku.dk>
Message-ID: <x2lldi7ecg.fsf@biostat.ku.dk>

Mads Jeppe Tarp-Johansen <s02mjtj at math.ku.dk> writes:

> The programming language mosml comes with foldr that 'accumulates' a
> function f over a list [x1,x2,...,xn] with initial value b as follows
> 
> foldr f b [x1,x2,...,xn]  =  f(x1,...,f(xn-1,f(xn,b))...)
> 
> Observe that "list" should have same elements so in R terminology it would
> perhaps be appropriate to say that the accumulation takes place over a
> 'vector'.
> 
> I wonder if R comes with a similar function and in general a library or
> package with list (vector) operators. Or is such programming style not
> intended in R?

R does generally encourage abstraction and encapsulation. However
operations like foldr are not common in statistics, I'd say. We have
cumsum and cumprod, which are the same sort of thing, hard coded. It's
easy to implement in R, although possibly not efficiently. Something
like this should work (untested!):

foldr <- function(f, b, x) {
   if (!(n <- length(x))) stop("zero-length x not allowed")
   if (n == 1)
      f(b, x) 
   else
      foldr(f, f(x[n], b), x[-n])
}

or non-recursively (equally untested)

foldr <- function(f, b, x)
{
   if (!(n <- length(x))) stop("zero-length x not allowed")
   while (n) { 
     b <- f(b, x[n])
     n <- n - 1
   }
   b
}



-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From JAROSLAW.W.TUSZYNSKI at saic.com  Wed Nov  3 23:41:28 2004
From: JAROSLAW.W.TUSZYNSKI at saic.com (Tuszynski, Jaroslaw W.)
Date: Wed, 3 Nov 2004 17:41:28 -0500 
Subject: [R] Unexpected results from sort function when partial and index
	are used
Message-ID: <9CC1B717EF3BD511AD98000103D63FC53FA6EF@us-arl-asg.mail.saic.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041103/12e34700/attachment.pl

From bxc at steno.dk  Wed Nov  3 23:56:12 2004
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Wed, 3 Nov 2004 23:56:12 +0100
Subject: [R] Building a package under WIN2000 / rw2.0
Message-ID: <0ABD88905D18E347874E0FB71C0B29E90237CFA2@exdkba022.novo.dk>

I have an odd problem in building a package with only R-code in it.
I have a package mainly used by myself which I last  build under R
1.9.0.
The operation system is Win2000 5.00.2195, Service Pack 3

When I do:

c:\stat\r\rw2000\bin\Rcmd install --docs=normal --build
--library=c:\stat\R\bxc\library c:\stat\R\bxc\library.sources\xx

then after updating help pages I get:

  preparing package xx for lazy loading
  Error in tools:::.read_description(file) :
          file '/DESCRIPTION' does not exist
  Execution halted
  make: *** [lazyload] Error 1
  *** Installation of xx failed ***

(Yes, I have a DESCRIPTION file).
Having made a few changes here and there in some of the functions I
transferred
them one at a time to a new folder and tried to build it there.

The probelm seems to be that once I exceed 5 functions in the package
the above
error appears, with 5 or fewer functions it works OK.

Any ideas?

Bendix Carstensen
----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc



From ramasamy at cancer.org.uk  Thu Nov  4 00:50:28 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Wed, 03 Nov 2004 23:50:28 +0000
Subject: [R] (no subject)
In-Reply-To: <20041103190449.60148.qmail@web50408.mail.yahoo.com>
References: <20041103190449.60148.qmail@web50408.mail.yahoo.com>
Message-ID: <1099525828.3337.55.camel@localhost.localdomain>

Reading the posting guide will tell you (among other things) to :
 a) use a sensible subject line
 b) give a reproducible example when possible

See other comments below.

On Wed, 2004-11-03 at 19:04, fang lai wrote:
> Hi there,
>  When I ran the wilcox.exact test, it always shows
> "number of items to replace is not a multiple of
> replacement length"
>  However, according to the help it seems to be able to
> handle sample of different size.

I think the error refers to incorrect lengths during assignment. See
example below. 

> aaa <- matrix( 1:4, nr=2 )
> aaa
     [,1] [,2]
[1,]    1    3
[2,]    2    4
> aaa[1, ]
[1] 1 3

> aaa[1, ] <- 1:3
Error in "[<-"(`*tmp*`, 1, , value = 1:3) :
    number of items to replace is not a multiple of replacement length

Or you might be trying something like 

> aaa <- matrix( 1:4, nr=2 )
> aaa[1, ] <- wilcox.test( 1:3, 1:10 )
Error in "[<-"(`*tmp*`, 1, , value = wilcox.test(1:3, 1:10)) :
    number of items to replace is not a multiple of replacement length


Wilcoxon and all other two-group test statistics (when pairing is not
involved) can handle groups of different size. 
Try wilcox.test( 1:3, 1:10 ). 
To extract p-value only, do wilcox.test( 1:3, 1:10 )$p.value

> What does this message indicate, and will it cause a
> problem of the resulting p-value?

Not directly. You could be expecting p-values to be in, say the 5th
column, but it might be stored elsewhere. But in your case I think the
results will not be stored anyway.

>  Also, when I run the ks.test(), it shows sometimes
> that 
> cannot compute correct p-values with ties in:
> ks.test(income.perm[(data[, 3] == 59) & (data[, 2] ==
> 1)], income.perm[(data[,   ...

Is this a warning or an error ? There is a distinction.

> And sometime it shows results and sometimes it just
> return NULL. When it shows the results, does it mean
> that it had some kind of continuity correction or
> other adjustment so that I could believe the results? 
> Many thanks,
> 
> Fang 
> 
> 
> =====
> Lai, Fang
> 
> PhD candidate
> University of California, Berkeley
> Department of Agricultural and Resource Economics
> 314 Giannini Hall, Berkeley, CA 94720-3310
> tel: (510) 643 - 5421(O)
>      (510) 847 - 9811(Cell)
> fax: (510) 643 - 8911
> email: lai at are.berkeley.edu
> http://www.are.berkeley.edu/jobmarket/fang.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From sundar.dorai-raj at pdf.com  Thu Nov  4 01:13:45 2004
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 03 Nov 2004 18:13:45 -0600
Subject: [R] Unexpected results from sort function when partial and index
	are used
In-Reply-To: <9CC1B717EF3BD511AD98000103D63FC53FA6EF@us-arl-asg.mail.saic.com>
References: <9CC1B717EF3BD511AD98000103D63FC53FA6EF@us-arl-asg.mail.saic.com>
Message-ID: <41897439.9010804@pdf.com>



Tuszynski, Jaroslaw W. wrote:

> Hi,
> 
> Consider the following example:
> 
> sort(10:1, partial=3)
> ## 1  2  3  7  6  5  4  8  9 10
> 
> sort(10:1, index=T)
> ## $x: 1  2  3  4  5  6  7  8  9 10
> ## $ix: 10  9  8  7  6  5  4  3  2  1
> 
> sort(10:1, partial=3, index=T)
> ##  1  2  3  7  6  5  4  8  9 10
> 
> The first 2 calls gave expected returns; however, the third one did not
> returned an index as requested. I could not find anything about it in
> http://stat.ethz.ch/R-manual/R-patched/library/base/html/sort.html
> <http://stat.ethz.ch/R-manual/R-patched/library/base/html/sort.html> , so it
> seems to be an "undocumented feature". 
> 
> Does any body know how to "convince" sort to return index of partially
> sorted array?
> 
> Thanks
> 
> Jarek

Jarek,

Looking at the code for sort, we see the following:

     if (!is.null(partial)) {
         if (!all(is.finite(partial)))
             stop("non-finite `partial'")
         y <- .Internal(psort(x, partial))
     } else {
        # other sort code
     }

so index.return is ignored if partial is provided. To get the index you 
can use ?match:

z <- rnorm(10)
x <- sort(z, partial = 3)
ix <- match(z, x)

Hopefully, I used ?match correctly. Please verify on your own.

--sundar



From jmanjour at fas.harvard.edu  Thu Nov  4 04:13:24 2004
From: jmanjour at fas.harvard.edu (Justin Manjourides)
Date: Wed,  3 Nov 2004 22:13:24 -0500
Subject: [R] Plotting a linear model object with R 2.0 for Mac OS X
Message-ID: <1099538004.41899e54c02b0@webmail.fas.harvard.edu>

I'm using R for Mac OS X Aqua GUI version 2.0

When using plot() with a linear model object, the plot of the LSR line does not
appear. I get the residual plots, the Normal Q-Q plots and Cook's Differnce
plot, just not the first plot with the fitted line. I just get an empty Quartz
window, with no graph displayed. I've even tried to use plot.lm() and plotting
a glm object, but I get the same results. Has anybody run into this problem?

Justin Manjourides



From ripley at stats.ox.ac.uk  Thu Nov  4 08:21:59 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 4 Nov 2004 07:21:59 +0000 (GMT)
Subject: [R] A little more on R, mdbtools and Access databases
In-Reply-To: <m27jp2a8my.fsf@ganymede.ammp.or.tz>
Message-ID: <Pine.LNX.4.44.0411040715250.1858-100000@gannet.stats>

On 3 Nov 2004, David Whiting wrote:

> Anne York <york at zipcon.net> writes:

> > On the negative side, Brian Ripley reported problems compiling
> > mdbtools, and on the Debian website, there were some security alerts
> > (overflow problems) about mdbtools. Clearly, you were able to compile
> > mdbtools. 
> 
> I had to make some symlinks to get it all to play properly (it seemed
> to expect libraries in /usr/lib when they were in /usr/local/lib), but
> it seemed to compile okay.  The lib problem might have been a problem
> on my side though.  

I was able to compile mdbtools 0.6pre1 (which you will have trouble
finding from their website, so search for the project on sourceforge)  
without any problems.  The public 0.5 gave me so many problems that I gave
up on a 64-bit platform, and gave up on their ODBC module on a 32-bit one.  
However, although the ODBC module makes communications, it reports its
info incorrectly and reports syntax errors with even the simplest SQL, so
it is clearly not ready for use.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Nov  4 08:34:40 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 4 Nov 2004 07:34:40 +0000 (GMT)
Subject: [R] Unexpected results from sort function when partial and index
	are used
In-Reply-To: <9CC1B717EF3BD511AD98000103D63FC53FA6EF@us-arl-asg.mail.saic.com>
Message-ID: <Pine.LNX.4.44.0411040731250.1858-100000@gannet.stats>

On Wed, 3 Nov 2004, Tuszynski, Jaroslaw W. wrote:

> Hi,
> 
> Consider the following example:
> 
> sort(10:1, partial=3)
> ## 1  2  3  7  6  5  4  8  9 10
> 
> sort(10:1, index=T)
> ## $x: 1  2  3  4  5  6  7  8  9 10
> ## $ix: 10  9  8  7  6  5  4  3  2  1
> 
> sort(10:1, partial=3, index=T)
> ##  1  2  3  7  6  5  4  8  9 10
> 
> The first 2 calls gave expected returns; however, the third one did not
> returned an index as requested. I could not find anything about it in
> http://stat.ethz.ch/R-manual/R-patched/library/base/html/sort.html
> <http://stat.ethz.ch/R-manual/R-patched/library/base/html/sort.html> , so it
> seems to be an "undocumented feature". 
> 
> Does any body know how to "convince" sort to return index of partially
> sorted array?

You cannot.  There is no underlying code to do so, and the person who 
added 'index.return' forgot this case.  I was against having it at all -- 
we have sort.list for that purpose.  I've updated the documentation.

Sundar's match() solution will not work if there are duplicate values.  
If you need the index, just do a full sort -- partial sorting is only 
implemented for efficiency reasons, and nowadays full sorting is fast 
enough even on massive vectors.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From etptupaf at bs.ehu.es  Thu Nov  4 09:52:06 2004
From: etptupaf at bs.ehu.es (F.Tusell)
Date: Thu, 04 Nov 2004 09:52:06 +0100
Subject: [R] Princomp(), prcomp() and loadings()
In-Reply-To: <Pine.LNX.4.44.0411031625090.23721-100000@gannet.stats>
References: <Pine.LNX.4.44.0411031625090.23721-100000@gannet.stats>
Message-ID: <1099558326.1536.64.camel@agesi.bs.ehu.es>

El mi??, 03-11-2004 a las 17:34, Prof Brian Ripley escribi??:
> On Wed, 3 Nov 2004, F.Tusell wrote:
> 
> > In comparing the results of princomp and prcomp I find:
> > 
> >    1. The reported standard deviations are similar but about 1% from
> >       each other, which seems well above round-off error.
> 
> That is explained on the help page!  E.g.
> 
>      Note that the default calculation uses divisor 'N' for the
>      covariance matrix.
>      ...

       Mea culpa, even without resort to the example in the help page I 
       should have thought that differences in the divisor for the 
       covariance matrix (or using directly the data matrix, as Sundar 
       Dorai-Raj explained) might account for the small discrepancies.
       My apologies.
       
> 
> >    2. princomp returns what I understand are variances and cumulative
> >       variances accounted for by each principal component which are
> >       all equal. "SS loadings" is always 1. 
> >    3. Same happens after the loadings are varimax-rotated, which in 
> >       general should alter the proportions of variance accounted by 
> >       each component.
> 
> Hmmm.  Varimax rotation of PCA (not factor analysis) is not supported in
> base R, so this is not surprising.  Please do as the posting guide asks,
> and read the help page (even its title!) before posting.
> 
       Well, the title says: "Rotation Methods for Factor Analysis" and
       this was not enough to deter me from using it on PCA generated 
       loadings. "Factor Analysis" is sometimes meant to include PCA,
       even if they are different beasts. Looking at the help page for
       "loadings" it also says: "Print Loadings in Factor Analysis", 
       yet it is meant to be used with PCA loadings as well, as the
       later the Description section goes on to saying.

       But this is a side question. What I asked in point 2) is why
       loadings(princomp(USArrests)) reports as the last four lines

                         Comp.1 Comp.2 Comp.3 Comp.4
          SS loadings      1.00   1.00   1.00   1.00
          Proportion Var   0.25   0.25   0.25   0.25
          Cumulative Var   0.25   0.50   0.75   1.00

       when the eigenvalues are different and so are the amounts of 
       variance explained by each component. I used "varimax" merely 
       to produce a different set of loadings and check that the same
       behaviour recurred. 

> > It looks as if the loadings() function were expecting the eigenvectors
> > to be normalized to the corresponding eigenvalue.
> > 
       The first line reported by loadings, "SS loadings", is right. 
       But the loadings matrix returned by princomp has its columns 
       normalized to 1, while loadings returned by factanal are 
       not. Hence, with the later, the last two lines are what I 
       expected, while with the former they are not.

       Perhaps the loadings matrices returned by princomp and factanal
       should be made of a different class, so loadings (or 
       print.loadings) treats them differently?

       ft.

> > Transcript and version information follow signature. Thank you for any
> > clues.
> 
> The best clue is that the help pages are a very useful resource, but need 
> to be read as carefully as they were written.

-- 
Fernando TUSELL                                e-mail:
Departamento de Econometr??a y Estad??stica           etptupaf at bs.ehu.es 
Facultad de CC.EE. y Empresariales             Tel:   (+34)94.601.3733
Avenida Lendakari Aguirre, 83                  Fax:   (+34)94.601.3754
E-48015 BILBAO  (Spain)                        Secr:  (+34)94.601.3740



From henric.nilsson at statisticon.se  Thu Nov  4 10:00:26 2004
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Thu, 04 Nov 2004 10:00:26 +0100
Subject: [R] Robust Poisson regression
In-Reply-To: <41881025.7070609@acelerate.com>
References: <41881025.7070609@acelerate.com>
Message-ID: <6.1.2.0.0.20041103135807.0500b520@10.0.10.66>

At 18:54 2004-11-02 -0400, you wrote:

>Anybody knows if there exists somewhere in R some implementation of robust 
>Poisson regression,
>where robust is taken in the sense as usen in rlm(MASS). I found something 
>in the package
>wle, but only for the Poisson distribution, not for regression.

Take a look at Eva Cantoni's "Analysis of robust quasi-deviances for 
generalized linear models" paper in JSS 10(4) 
(http://www.jstatsoft.org/v10/i04/).

The tar'ed file contains S-PLUS functions, and IIRC the modifications to 
make `glm.rob' run under R were quite simple. I can't find my modified 
version right now, but I remember making changes to lots of `assign' 
calls... I also remember having trouble with `integrate' calls in the 
`quasi.rob.xxx' functions, but since I didn't have to use these I never 
tried fixing it.

HTH,
Henric



From maechler at stat.math.ethz.ch  Thu Nov  4 10:39:13 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 4 Nov 2004 10:39:13 +0100
Subject: [R] Suggested color schemes for points, not regions?
In-Reply-To: <3058.66.185.0.209.1099498019.squirrel@66.185.0.209>
References: <3058.66.185.0.209.1099498019.squirrel@66.185.0.209>
Message-ID: <16777.63681.822947.232080@gargle.gargle.HOWL>

>>>>> "KevinW" == Kevin Wright <kwright at eskimo.com>
>>>>>     on Wed, 3 Nov 2004 08:06:59 -0800 (PST) writes:

    KevinW> I have seen a couple of posts about color schemes like those at the
    KevinW> ColorBrewer site.  Most recently:
    KevinW> http://geography.uoregon.edu/datagraphics/color_scales.htm

    KevinW> These color schemes can work very well for regions (bars, polygons,
    KevinW> images, etc.) but are not very suitable for points and/or lines.

Are you sure?

Currently, for R users, the most accessible of the "new" schemes is the
ColorBrewer (http://colorbrewer.org) one, since Erich Neuwirth
and others have been providing the RColorBrewer package on CRAN.

 install.packages("RColorBrewer")
 library("RColorBrewer")
 example(brewer.pal) # a 'show'

not only has schemes to be used for images/maps,
but also "Set"s of (pairwise well distinguishable) colors well
suited for scatter plots.   In Cynthia Brewer's terminology
these are the "qualitative" schemes.  

    KevinW> Is anyone aware of research/suggestions for a color
    KevinW> scheme to use for scatter plots?  I've looked at
    KevinW> great length and have found little on this topic.

    KevinW> My current scheme of choice is a set of fairly
    KevinW> saturated colors along the lines of:

    KevinW> navy
    KevinW> brown/orange
    KevinW> black
    KevinW> purple
    KevinW> red
    KevinW> medium green

    KevinW> This is similar to the 'paired' color scheme, but
    KevinW> using only the saturated colors and substituting
    KevinW> black for yellow.  Depending on circumstances, I
    KevinW> sometimes use a different glyph for each color.  The
    KevinW> hard part about all this is to make sure that each
    KevinW> color/glyph combination has the same
    KevinW> 'attention-getting' power.

hmm, from the above I tend to read that you've already looked at
(e.g.) 'Set1' or 'Dark2' colors from ColorBrewer but didn't like
them? 

Something that hasn't been properly considered by statisticians
AFAIK is the situation for color blind people, or at least the
most common one. This has just been brought up here two days
ago, see e.g.
  https://stat.ethz.ch/pipermail/r-help/2004-November/058724.html
which mentions new "color ramp" facilities in the future 
R 2.1.0 (2005-04-0x).
Cynthia Brewer and many others also mention and recommend
http://www.vischeck/com/ which allows checking your image/drawing.
There's a simulator trying show how a given picture is seen by
(even different kinds of) color deficient persons.

The colorbrewer.org site has a very nice tool (based on flash 5
plugin), which shows you the pros and cons of a color scheme you
chose (interactively).  She (Cynthia Brewer) uses 6 criteria
(with 3 levels  "ok", "doubtful/unknown", "not ok"):

 1) Color blind friendly   [the "red-green" deficiency]
 2) Photocopy friendly	   [for B&W photocopying: are differences preserved?]
 3) LCD Projector friendly [pastel colors may be problematic]
 4) Laptop (LCD) friendly
 5) CRT-(screen) friendly
 6) Color Printing friendly

>From the 8 qualitative schemes, 
- 5 (of 8) were "not ok" for color blinded.
- each had at least one "not ok";
  i.e., there's no "optimal color scheme" that works everywhere,
  but you have to change color schemes depending on the intended
  medium.  

    KevinW> Any discussion or comments are welcome.

I'm pretty sure Ross Ihaka will also chime in here.
(http://www.stat.auckland.ac.nz/~ihaka/colour/ is a first start).

As you see, I'm quite interested also.
Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From Kevin.Wang at maths.anu.edu.au  Thu Nov  4 11:14:19 2004
From: Kevin.Wang at maths.anu.edu.au (Kevin Wang)
Date: Thu, 4 Nov 2004 21:14:19 +1100 (EST)
Subject: [R] Finding out the number of times a loop has run
Message-ID: <Pine.GSO.4.58.0411042111080.29039@yin>

Hi,

Suppose I have:
  for(i in 1:100000)
     rnorm(100000)
(just a fake example, as my actual example is too long)

Is it possible to get the loop to print out i each time it has run?
Something like:
  for(i in 1:100000) {
     print(i)
     rnorm(100000)
  }
will only print i after the loop is completed.  But if I want to print out
i whenever it re-enters the loop, is it possible (so I know how many times
the loop has been run)?

Cheers,

Kevin

--------------------------------
Ko-Kang Kevin Wang
PhD Student
Centre for Mathematics and its Applications
Building 27, Room 1004
Mathematical Sciences Institute (MSI)
Australian National University
Canberra, ACT 0200
Australia

Homepage: http://wwwmaths.anu.edu.au/~wangk/
Ph (W): +61-2-6125-2431
Ph (H): +61-2-6125-7407
Ph (M): +61-40-451-8301



From p.dalgaard at biostat.ku.dk  Thu Nov  4 11:29:14 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 04 Nov 2004 11:29:14 +0100
Subject: [R] Finding out the number of times a loop has run
In-Reply-To: <Pine.GSO.4.58.0411042111080.29039@yin>
References: <Pine.GSO.4.58.0411042111080.29039@yin>
Message-ID: <x2r7n96h6t.fsf@biostat.ku.dk>

Kevin Wang <Kevin.Wang at maths.anu.edu.au> writes:

> Hi,
> 
> Suppose I have:
>   for(i in 1:100000)
>      rnorm(100000)
> (just a fake example, as my actual example is too long)
> 
> Is it possible to get the loop to print out i each time it has run?
> Something like:
>   for(i in 1:100000) {
>      print(i)
>      rnorm(100000)
>   }
> will only print i after the loop is completed.  But if I want to print out
> i whenever it re-enters the loop, is it possible (so I know how many times
> the loop has been run)?

Kevin, you must have seen this on the list before! Either use
flush.console() or turn off the output buffering (Windows FAQ 6.3).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Kevin.Wang at maths.anu.edu.au  Thu Nov  4 11:32:27 2004
From: Kevin.Wang at maths.anu.edu.au (Kevin Wang)
Date: Thu, 4 Nov 2004 21:32:27 +1100 (EST)
Subject: [R] Finding out the number of times a loop has run
In-Reply-To: <x2r7n96h6t.fsf@biostat.ku.dk>
References: <Pine.GSO.4.58.0411042111080.29039@yin>
	<x2r7n96h6t.fsf@biostat.ku.dk>
Message-ID: <Pine.GSO.4.58.0411042131520.29039@yin>

Hi,

On Thu, 4 Nov 2004, Peter Dalgaard wrote:

> Kevin, you must have seen this on the list before! Either use
> flush.console() or turn off the output buffering (Windows FAQ 6.3).

Whoopss....sorry, I must be tired....need another cup of coffee.

Kev

--------------------------------
Ko-Kang Kevin Wang
PhD Student
Centre for Mathematics and its Applications
Building 27, Room 1004
Mathematical Sciences Institute (MSI)
Australian National University
Canberra, ACT 0200
Australia

Homepage: http://wwwmaths.anu.edu.au/~wangk/
Ph (W): +61-2-6125-2431
Ph (H): +61-2-6125-7407
Ph (M): +61-40-451-8301



From ligges at statistik.uni-dortmund.de  Thu Nov  4 11:34:43 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 04 Nov 2004 11:34:43 +0100
Subject: [R] Finding out the number of times a loop has run
In-Reply-To: <Pine.GSO.4.58.0411042111080.29039@yin>
References: <Pine.GSO.4.58.0411042111080.29039@yin>
Message-ID: <418A05C3.3080800@statistik.uni-dortmund.de>

Kevin Wang wrote:

> Hi,
> 
> Suppose I have:
>   for(i in 1:100000)
>      rnorm(100000)
> (just a fake example, as my actual example is too long)
> 
> Is it possible to get the loop to print out i each time it has run?
> Something like:
>   for(i in 1:100000) {
>      print(i)
>      rnorm(100000)
>   }
> will only print i after the loop is completed.  But if I want to print out
> i whenever it re-enters the loop, is it possible (so I know how many times
> the loop has been run)?

Kevin,

you know that all people are asked to read the FAQs (in particlular the 
R for Windows FAQs in this case - regarding buffered output!) before 
asking questions on R-help ...

Uwe


>

> Cheers,
> 
> Kevin
> 
> --------------------------------
> Ko-Kang Kevin Wang
> PhD Student
> Centre for Mathematics and its Applications
> Building 27, Room 1004
> Mathematical Sciences Institute (MSI)
> Australian National University
> Canberra, ACT 0200
> Australia
> 
> Homepage: http://wwwmaths.anu.edu.au/~wangk/
> Ph (W): +61-2-6125-2431
> Ph (H): +61-2-6125-7407
> Ph (M): +61-40-451-8301
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Thu Nov  4 11:36:33 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 04 Nov 2004 11:36:33 +0100
Subject: [R] Plotting a linear model object with R 2.0 for Mac OS X
In-Reply-To: <1099538004.41899e54c02b0@webmail.fas.harvard.edu>
References: <1099538004.41899e54c02b0@webmail.fas.harvard.edu>
Message-ID: <418A0631.4090406@statistik.uni-dortmund.de>

Justin Manjourides wrote:

> I'm using R for Mac OS X Aqua GUI version 2.0
> 
> When using plot() with a linear model object, the plot of the LSR line does not
> appear. I get the residual plots, the Normal Q-Q plots and Cook's Differnce
> plot, just not the first plot with the fitted line. I just get an empty Quartz
> window, with no graph displayed. I've even tried to use plot.lm() and plotting
> a glm object, but I get the same results. Has anybody run into this problem?

First plot the data, then add the line. That's mentioned in the docs and 
all good books about R:

  plot(data)
  abline(lmobject)

Uwe Ligges




> Justin Manjourides
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From vaugeoisjc at yahoo.fr  Thu Nov  4 12:06:30 2004
From: vaugeoisjc at yahoo.fr (JC Vaugeois)
Date: Thu, 4 Nov 2004 12:06:30 +0100 (CET)
Subject: [R] keep dimension of a sub matrix
Message-ID: <20041104110630.61210.qmail@web26403.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041104/7718417c/attachment.pl

From Ciprian_Marin at ssga.com  Thu Nov  4 12:41:18 2004
From: Ciprian_Marin at ssga.com (Ciprian_Marin@ssga.com)
Date: Thu, 4 Nov 2004 11:41:18 +0000
Subject: [R] problems with seq.dates
Message-ID: <OF5D466B8E.42E4DD9F-ON80256F42.003FD80D-80256F42.00401EF4@StateStreet.com>





There seem to be a bug in the seq.dates function in the chron package for R
2.0. Please see below:

when the specified frequency is "months", seq.dates does not return the end
of the specified interval all the time:

> seq.dates(from = "05/31/04", to = "12/31/04", by = "months")
[1] 05/31/04 06/30/04 07/31/04 08/31/04 09/30/04 10/31/04 11/30/04

Ciprian


Ciprian V Marin
State Street Global Advisors Limited

Authorised and regulated by the Financial Services Authority
Recipient of the Queen's Award for Enterprise 2003

0207 698 6195 (Direct Line)
0207 698 6333 (Main Fax)

Please visit our Web site at www.ssga.co.uk



From dimitris.rizopoulos at med.kuleuven.ac.be  Thu Nov  4 12:48:31 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Thu, 4 Nov 2004 12:48:31 +0100
Subject: [R] keep dimension of a sub matrix
References: <20041104110630.61210.qmail@web26403.mail.ukl.yahoo.com>
Message-ID: <003f01c4c264$34d88620$0540210a@www.domain>

just use

matrix(1,4,4)[i1:i2,j1:j2, drop=FALSE]

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "JC Vaugeois" <vaugeoisjc at yahoo.fr>
To: <R-help at stat.math.ethz.ch>
Sent: Thursday, November 04, 2004 12:06 PM
Subject: [R] keep dimension of a sub matrix


Hi,
is there any way to keep a sub matrix dimension?

exemple :
i1<-1; i2<-1
j1<-2; j2<-3;
ret <-matrix(1,4,4)[i1:i2,j1:j2] ;

dim(ret) is NULL because the submatrix single col or single row is 
coerce to a vector automaticaly.
How can i bypass this cast  : submatrix->vector   ???????

Thank you.





---------------------------------

[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html



From lecoutre at stat.ucl.ac.be  Thu Nov  4 12:41:57 2004
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Thu, 04 Nov 2004 12:41:57 +0100
Subject: [R] keep dimension of a sub matrix
In-Reply-To: <20041104110630.61210.qmail@web26403.mail.ukl.yahoo.com>
References: <20041104110630.61210.qmail@web26403.mail.ukl.yahoo.com>
Message-ID: <6.0.1.1.2.20041104124055.0393fbe0@stat4ux.stat.ucl.ac.be>



See

 > help("[")

which informs you about the additional argument "drop":

 > ret <-matrix(1,4,4)[i1:i2,j1:j2,drop=FALSE] ;

BTW, you do not need to end lines of code with a ";" ...

Eric


At 12:06 4/11/2004, JC Vaugeois wrote:
>Hi,
>is there any way to keep a sub matrix dimension?
>
>exemple :
>i1<-1; i2<-1
>j1<-2; j2<-3;
>ret <-matrix(1,4,4)[i1:i2,j1:j2] ;
>
>dim(ret) is NULL because the submatrix single col or single row is coerce 
>to a vector automaticaly.
>How can i bypass this cast  : submatrix->vector   ???????
>
>Thank you.
>
>
>
>
>
>---------------------------------
>
>
>
>
>         [[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Eric Lecoutre
UCL /  Institut de Statistique
Voie du Roman Pays, 20
1348 Louvain-la-Neuve
Belgium

tel: (+32)(0)10473050
lecoutre at stat.ucl.ac.be
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre

If the statistics are boring, then you've got the wrong numbers. -Edward 
Tufte



From p.dalgaard at biostat.ku.dk  Thu Nov  4 12:51:40 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 04 Nov 2004 12:51:40 +0100
Subject: [R] keep dimension of a sub matrix
In-Reply-To: <20041104110630.61210.qmail@web26403.mail.ukl.yahoo.com>
References: <20041104110630.61210.qmail@web26403.mail.ukl.yahoo.com>
Message-ID: <x2is8l6ddf.fsf@biostat.ku.dk>

JC Vaugeois <vaugeoisjc at yahoo.fr> writes:

> Hi,
> is there any way to keep a sub matrix dimension?
>  
> exemple :
> i1<-1; i2<-1
> j1<-2; j2<-3;
> ret <-matrix(1,4,4)[i1:i2,j1:j2] ;
>  
> dim(ret) is NULL because the submatrix single col or single row is coerce to a vector automaticaly.
> How can i bypass this cast  : submatrix->vector   ???????

help("[") 

drop=FALSE is what you're looking for

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From N.R.STREET at soton.ac.uk  Thu Nov  4 13:22:33 2004
From: N.R.STREET at soton.ac.uk (Nathaniel Street)
Date: Thu, 4 Nov 2004 12:22:33 +0000 (GMT)
Subject: [R] Specifying error terms in aov and lme
In-Reply-To: <200411041125.iA4BEXdw018719@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.44.0411041218060.28041-100000@red00.iridis.soton.ac.uk>

I need to specify error terms properly in a mixed-effects anova model. I 
know you can add error terms in aov using Error and can specify random 
factors in lme but I am not sure how these get treated.

When making the calculations for fixed and random factors, are the correct 
error terms used and how can you get aov or lme to use different error 
terms for fixed and random effects?

I'm basing the need for this on this table from Zar,

Both fixed factors - both tested against error
Both random factors - both tested against interaction MS
1 fixed - interaction MS
1 random - replicate MS

for random you can use error if the interaction is not significant (I have 
been told). 

I want to make sure that the correct error terms are always being used.

Thanks for your help

-- 
Nathaniel Street
         University of Southampton
Plants and Environment Lab
     School of Biological Sciences
Basset Crescent East
    Southampton
SO16 7PX
    tel: +44 (0) 23 8059 4387
 fax: +44 (0) 23 8059 4459
      n.r.street at soton.ac.uk
  http://popyomics.biol.soton.ac.uk/~nat
     ICQ: 203465793



From gavin.simpson at ucl.ac.uk  Thu Nov  4 13:20:23 2004
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Thu, 04 Nov 2004 12:20:23 +0000
Subject: [R] sub- and superscript in plot labels
Message-ID: <418A1E87.40304@ucl.ac.uk>

Dear List,

I need to add a subscript and a superscript to some of the ions in the 
labels on some plots.

I have got to here but now I'm stuck:

plot(1:10, xlab = expression(paste("nm SO"[4], " ", mu, "eq cm"^{-2}, " 
yr"^{-1})))

Which gives almost  what I require. No matter what I tried, however, I 
could not get bot a sub script *and* a superscript attached to the SO in 
the label.

In LaTeX I would just do $SO_4^{2-}$ but taking that to R produces a 
syntax error:

plot(1:10, xlab = expression(paste("nm SO"[4]^{2-}, " ", mu, "eq 
cm"^{-2}, " yr"^{-1})))

Strangely, I can do this:

plot(1:10, xlab = expression(paste("nm SO"[4]^2, " ", mu, "eq cm"^{-2}, 
" yr"^{-1})))

With almost the desired effect (except I need to add two characters to 
the superscript).

Can any one offer a solution?

Many thanks

Gavin
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpson at ucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From N.R.STREET at soton.ac.uk  Thu Nov  4 13:27:19 2004
From: N.R.STREET at soton.ac.uk (Nathaniel Street)
Date: Thu, 4 Nov 2004 12:27:19 +0000 (GMT)
Subject: [R] Labelling contour lines
In-Reply-To: <200411041125.iA4BEXdw018719@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.44.0411041222360.28041-100000@red00.iridis.soton.ac.uk>

I am using contour to draw contour lines onto a photo (jpeg) of a leaf on 
a white background.

I have two problems:

1) The contour gets plotted at right angles to the jpeg image. I guess 
this is a problem of referencing the start and end points of the image 
matrix but I can't see how to over come this other than loading in a 
second image that has been rotated 90 degrees and plotting the contours on 
the non-rotated one (obviously a bad solution)

2) I want to be able to select a single contour after they have been 
drawn. I know there is contourLines but I can't see how to relate this to 
the contours plotted. In matlab there is a function to label contours so 
that they can be called by their contour label in order to get the 
coordinates of the contour. Can this be done in R.

Sorry if these are easy / stupid questions. I'm new to all this. I have 
searched previos posts but can't find the answers I need and I would 
rather be able to use R than matlab.

Thanks

-- 
Nathaniel Street
         University of Southampton
Plants and Environment Lab
     School of Biological Sciences
Basset Crescent East
    Southampton
SO16 7PX
    tel: +44 (0) 23 8059 4387
 fax: +44 (0) 23 8059 4459
      n.r.street at soton.ac.uk
  http://popyomics.biol.soton.ac.uk/~nat
     ICQ: 203465793



From ramasamy at cancer.org.uk  Thu Nov  4 13:27:07 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Thu, 04 Nov 2004 12:27:07 +0000
Subject: [R] (no subject)-about the waring/errors in ks.test
Message-ID: <1099571226.3051.13.camel@ramasamy.stats>

[In future, please press reply all as there might be others in the
mailing list who can help you]

Without a (short) reproducible example, it would be difficult to say.

BTW, I think you have mistyped the error/warning message. Does it
actually say "cannot compute _correct_ p-value" or "cannot compute
_exact_ p-value". See example below.

> wilcox.test( 1:3, 1:10 )

        Wilcoxon rank sum test with continuity correction

data:  1:3 and 1:10
W = 4.5, p-value = 0.08964
alternative hypothesis: true mu is not equal to 0

Warning message:
Cannot compute exact p-value with ties in: wilcox.test.default(1:3,
1:10)


On Thu, 2004-11-04 at 00:31, fang lai wrote:
> Dear Adaikalavan,
>  Many thanks for your suggestions. The message 
> "cannot compute correct p-values with ties in"
> sometimes shows up as an error (in most cases I have
> small group for the test), and sometimes shows up as
> waring when I typed warning() (in most cases the group
> I am testing is relatively large). But all the data
> are from the same data set. 
>  So what is the distinction between these two?
> Many thanks,
> 
> Fang
> 
> 
> --- Adaikalavan Ramasamy <ramasamy at cancer.org.uk>
> wrote:
> 
> > Reading the posting guide will tell you (among other
> > things) to :
> >  a) use a sensible subject line
> >  b) give a reproducible example when possible
> > 
> > See other comments below.
> > 
> > On Wed, 2004-11-03 at 19:04, fang lai wrote:
> > > Hi there,
> > >  When I ran the wilcox.exact test, it always shows
> > > "number of items to replace is not a multiple of
> > > replacement length"
> > >  However, according to the help it seems to be
> > able to
> > > handle sample of different size.
> > 
> > I think the error refers to incorrect lengths during
> > assignment. See
> > example below. 
> > 
> > > aaa <- matrix( 1:4, nr=2 )
> > > aaa
> >      [,1] [,2]
> > [1,]    1    3
> > [2,]    2    4
> > > aaa[1, ]
> > [1] 1 3
> > 
> > > aaa[1, ] <- 1:3
> > Error in "[<-"(`*tmp*`, 1, , value = 1:3) :
> >     number of items to replace is not a multiple of
> > replacement length
> > 
> > Or you might be trying something like 
> > 
> > > aaa <- matrix( 1:4, nr=2 )
> > > aaa[1, ] <- wilcox.test( 1:3, 1:10 )
> > Error in "[<-"(`*tmp*`, 1, , value =
> > wilcox.test(1:3, 1:10)) :
> >     number of items to replace is not a multiple of
> > replacement length
> > 
> > 
> > Wilcoxon and all other two-group test statistics
> > (when pairing is not
> > involved) can handle groups of different size. 
> > Try wilcox.test( 1:3, 1:10 ). 
> > To extract p-value only, do wilcox.test( 1:3, 1:10
> > )$p.value
> > 
> > > What does this message indicate, and will it cause
> > a
> > > problem of the resulting p-value?
> > 
> > Not directly. You could be expecting p-values to be
> > in, say the 5th
> > column, but it might be stored elsewhere. But in
> > your case I think the
> > results will not be stored anyway.
> > 
> > >  Also, when I run the ks.test(), it shows
> > sometimes
> > > that 
> > > cannot compute correct p-values with ties in:
> > > ks.test(income.perm[(data[, 3] == 59) & (data[, 2]
> > ==
> > > 1)], income.perm[(data[,   ...
> > 
> > Is this a warning or an error ? There is a
> > distinction.
> > 
> > > And sometime it shows results and sometimes it
> > just
> > > return NULL. When it shows the results, does it
> > mean
> > > that it had some kind of continuity correction or
> > > other adjustment so that I could believe the
> > results? 
> > > Many thanks,
> > > 
> > > Fang 
> > > 
> > > 
> > > =====
> > > Lai, Fang
> > > 
> > > PhD candidate
> > > University of California, Berkeley
> > > Department of Agricultural and Resource Economics
> > > 314 Giannini Hall, Berkeley, CA 94720-3310
> > > tel: (510) 643 - 5421(O)
> > >      (510) 847 - 9811(Cell)
> > > fax: (510) 643 - 8911
> > > email: lai at are.berkeley.edu
> > > http://www.are.berkeley.edu/jobmarket/fang.html
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > > 
> > 
> > 
> 
> 
> =====
> Lai, Fang
> 
> PhD candidate
> University of California, Berkeley
> Department of Agricultural and Resource Economics
> 314 Giannini Hall, Berkeley, CA 94720-3310
> tel: (510) 643 - 5421(O)
>      (510) 847 - 9811(Cell)
> fax: (510) 643 - 8911
> email: lai at are.berkeley.edu
> http://www.are.berkeley.edu/jobmarket/fang.html
> 
> __________________________________________________



From dmb at mrc-dunn.cam.ac.uk  Thu Nov  4 13:33:08 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Thu, 4 Nov 2004 12:33:08 +0000 (GMT)
Subject: [R] Legend placement in barplot?
In-Reply-To: <1099500311.8211.46.camel@localhost.localdomain>
Message-ID: <Pine.LNX.4.21.0411041230130.29228-100000@mail.mrc-dunn.cam.ac.uk>

On Wed, 3 Nov 2004, Marc Schwartz wrote:

>On Wed, 2004-11-03 at 09:55, Dan Bolser wrote:
>> This has been asked before, but all the answers are hidiously complex.
>> 
>> The
>> 
>> legend.text=TRUE 
>> 
>> option of barplot is almost exactly what I need, except I need a
>> 
>> legend.placement='tl'
>> 
>> (top left) option. This option would be in contrast to the default
>> placement which we could call 'tr' (top right).
>> 
>> Anyone know how to edit the barplot code to make this change? Could
>> someone like me work out how to do this?
>> 
>> Cheers,
>> Dan.
>
>Dan,
>
>Do not edit the barplot() code. Use the legend() function instead, which
>enables you to specify the x,y coordinates of the upper left hand corner
>of the legend box. See ?legend

Thing is I need to pass legend the correct groups and correct plotting
colors and correct XY position relative to my data. All these things are
already known by the barplot function, and used to draw a beautiful
legend.

The fact that this legend can only appear in the upper right hand corner
is surly a bug worthy of changing the code for?


>A fair number of the questions that you have had regarding graphics are
>covered in Chapter 12 "Graphical Procedures" in "An Introduction to R":
>
>http://cran.r-project.org/doc/manuals/R-intro.pdf
>
>which is included with the R installation.
>
>Another online resource for some graphics assistance would be R News
>Volume 3 Number 2 for October 2003, which has an article on R's base
>graphics in the R Help Desk section:
>
>http://cran.r-project.org/doc/Rnews/Rnews_2003-2.pdf

Thanks for the links

>Notwithstanding all of that, searching the r-help archives is yet
>another terrific online (and free) resource that you _should_ avail
>yourself of.


Quoting me... "This has been asked before, but all the answers are
hidiously complex."



>HTH,
>
>Marc Schwartz
>
>



From ripley at stats.ox.ac.uk  Thu Nov  4 13:42:23 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 4 Nov 2004 12:42:23 +0000 (GMT)
Subject: [R] sub- and superscript in plot labels
In-Reply-To: <418A1E87.40304@ucl.ac.uk>
Message-ID: <Pine.LNX.4.44.0411041237010.11570-100000@gannet.stats>

On Thu, 4 Nov 2004, Gavin Simpson wrote:

> Dear List,
> 
> I need to add a subscript and a superscript to some of the ions in the 
> labels on some plots.
> 
> I have got to here but now I'm stuck:
> 
> plot(1:10, xlab = expression(paste("nm SO"[4], " ", mu, "eq cm"^{-2}, " 
> yr"^{-1})))
> 
> Which gives almost  what I require. No matter what I tried, however, I 
> could not get bot a sub script *and* a superscript attached to the SO in 
> the label.
> 
> In LaTeX I would just do $SO_4^{2-}$ but taking that to R produces a 
> syntax error:
> 
> plot(1:10, xlab = expression(paste("nm SO"[4]^{2-}, " ", mu, "eq 
> cm"^{-2}, " yr"^{-1})))

The problem is 2-.  That's not an R expression.  Using "2-" might give
what you want, but it will use a hyphen rather than a minus. Otherwise

plot(1:10, xlab = expression(paste("nm SO"[4]^{2-phantom()})))

will give a minus.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From gavin.simpson at ucl.ac.uk  Thu Nov  4 13:43:23 2004
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Thu, 04 Nov 2004 12:43:23 +0000
Subject: [R] sub- and superscript in plot labels
In-Reply-To: <418A1E87.40304@ucl.ac.uk>
References: <418A1E87.40304@ucl.ac.uk>
Message-ID: <418A23EB.1050302@ucl.ac.uk>

Gavin Simpson wrote:
> Dear List,
> 
> I need to add a subscript and a superscript to some of the ions in the 
> labels on some plots.
> 
> I have got to here but now I'm stuck:
> 
> plot(1:10, xlab = expression(paste("nm SO"[4], " ", mu, "eq cm"^{-2}, " 
> yr"^{-1})))
> 
> Which gives almost  what I require. No matter what I tried, however, I 
> could not get bot a sub script *and* a superscript attached to the SO in 
> the label.
> 
> In LaTeX I would just do $SO_4^{2-}$ but taking that to R produces a 
> syntax error:
> 
> plot(1:10, xlab = expression(paste("nm SO"[4]^{2-}, " ", mu, "eq 
> cm"^{-2}, " yr"^{-1})))
> 
> Strangely, I can do this:
> 
> plot(1:10, xlab = expression(paste("nm SO"[4]^2, " ", mu, "eq cm"^{-2}, 
> " yr"^{-1})))
> 
> With almost the desired effect (except I need to add two characters to 
> the superscript).
> 
> Can any one offer a solution?
> 
> Many thanks
> 
> Gavin

With further playing around, it would appear that you can sort of 
achieve what I was looking for by changing the ordering of the 
characters in the braces on the superscript:

plot(1:10, xlab = expression(paste("nm SO"[4]^{-2}, " ", mu, "eq
cm"^{-2}, " yr"^{-1})))

which works as required.

Is this intentional behaviour? My original question still stands, 
however, is there a way to have {2-} appear as the superscript?

The forgotten version info (sorry!):

 > version
          _
platform i686-pc-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status   Patched
major    2
minor    0.0
year     2004
month    10
day      25
language R

Running on Fedora Core 2.

Many thanks

Gavin
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpson at ucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From p.dalgaard at biostat.ku.dk  Thu Nov  4 13:52:48 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 04 Nov 2004 13:52:48 +0100
Subject: [R] Specifying error terms in aov and lme
In-Reply-To: <Pine.LNX.4.44.0411041218060.28041-100000@red00.iridis.soton.ac.uk>
References: <Pine.LNX.4.44.0411041218060.28041-100000@red00.iridis.soton.ac.uk>
Message-ID: <x2ekj96ajj.fsf@biostat.ku.dk>

Nathaniel Street <N.R.STREET at soton.ac.uk> writes:

> I need to specify error terms properly in a mixed-effects anova model. I 
> know you can add error terms in aov using Error and can specify random 
> factors in lme but I am not sure how these get treated.
> 
> When making the calculations for fixed and random factors, are the correct 
> error terms used and how can you get aov or lme to use different error 
> terms for fixed and random effects?

In short: Those considerations make sense for programs that give you
only a partitioning of the sum of squares. aov() and lme() should
(mostly) get things right automatically *if* you specify the model
correctly. With aov() you still need some know-how to test
significance of variance components, though.

 
> I'm basing the need for this on this table from Zar,
> 
> Both fixed factors - both tested against error

Now that can actually be horribly wrong if there is significant
interaction. 

> Both random factors - both tested against interaction MS
> 1 fixed - interaction MS
> 1 random - replicate MS

wrong again.....

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From cyracules at yahoo.co.uk  Thu Nov  4 14:00:30 2004
From: cyracules at yahoo.co.uk (John)
Date: Thu, 4 Nov 2004 13:00:30 +0000 (GMT)
Subject: [R] Reading word by word in a dataset
In-Reply-To: <6.1.0.6.2.20041101141221.0ba80d18@mailhost.blackmesacapital.com>
Message-ID: <20041104130030.55607.qmail@web26304.mail.ukl.yahoo.com>

Thanks, Tony.
I got a very good idea of using "flush" in scan() from
your reply, so that I successfully did my little job.
But, my next question arises if I want to extract the
list of the price items only in the 2nd column in my
example.
I did it the following way. Is it the right way to do?
Or do you have a smarter or more efficient way to do
it?

> system("more mtx.ex.1")
i1-apple 10$ New_York
i2-banana 5$ London
i3-strawberry 7$ Japan
>
> scan(file="mtx.ex.1", what=list(NULL,""),
flush=T)[[2]]
Read 3 records
[1] "10$" "5$"  "7$"

Cheers,

John


 --- Tony Plate <tplate at acm.org> wrote: 
> Trying to make it work when not all rows have the
> same numbers of fields 
> seems like a good place to use the "flush" argument
> to scan() (to skip 
> everything after the first field on the line):
> 
> With the following copied to the clipboard:
> 
> i1-apple        10$   New_York
> i2-banana
> i3-strawberry   7$    Japan
> 
> do:
> 
>  > scan("clipboard", "", flush=T)
> Read 3 items
> [1] "i1-apple"      "i2-banana"     "i3-strawberry"
>  > sub("^[A-Za-z0-9]*-", "", scan("clipboard", "",
> flush=T))
> Read 3 items
> [1] "apple"      "banana"     "strawberry"
>  >
> 
> -- Tony Plate
> 
> At Monday 01:59 PM 11/1/2004, Spencer Graves wrote:
> >      Uwe and Andy's solutions are great for many
> applications but won't 
> > work if not all rows have the same numbers of
> fields.  Consider for 
> > example the following modification of Lee's
> example:
> >i1-apple        10$   New_York
> >i2-banana
> >i3-strawberry   7$    Japan
> >
> >      If I copy this to "clipboard" and run Andy's
> code, I get the following:
> > > read.table("clipboard",
> colClasses=c("character", "NULL", "NULL"))
> >Error in scan(file = file, what = what, sep = sep,
> quote = quote, dec = 
> >dec,  :
> >    line 2 did not have 3 elements
> >
> >      We can get around this using "scan", then
> splitting things apart 
> > similar to the way Uwe described:
> > > dat <-
> >+ scan("clipboard", character(0), sep="\n")
> >Read 3 items
> > > dash <- regexpr("-", dat)
> > > dat2 <- substring(dat, pmax(0, dash)+1)
> > >
> > > blank <- regexpr(" ", dat2)
> > > if(any(blank<0))
> >+   blank[blank<0] <- nchar(dat2[blank<0])
> > > substring(dat2, 1, blank)
> >[1] "apple "      "banana"      "strawberry "
> >
> >      hope this helps.  spencer graves
> >
> >Uwe Ligges wrote:
> >
> >>Liaw, Andy wrote:
> >>
> >>>Using R-2.0.0 on WinXPPro, cut-and-pasting the
> data you have:
> >>>
> >>>
> >>>>read.table("clipboard",
> colClasses=c("character", "NULL", "NULL"))
> >>>
> >>>
> >>>              V1
> >>>1      i1-apple
> >>>2     i2-banana
> >>>3 i3-strawberry
> >>
> >>
> >>
> >>... and if only the words after "-" are of
> interest, the statement can be 
> >>followed by
> >>
> >>  sapply(strsplit(...., "-"), "[", 2)
> >>
> >>
> >>Uwe Ligges
> >>
> >>
> >>
> >>>HTH,
> >>>Andy
> >>>
> >>>
> >>>>From: j lee
> >>>>
> >>>>Hello All,
> >>>>
> >>>>I'd like to read first words in lines into a new
> file.
> >>>>If I have a data file the following, how can I
> get the
> >>>>first words: apple, banana, strawberry?
> >>>>
> >>>>i1-apple        10$   New_York
> >>>>i2-banana       5$    London
> >>>>i3-strawberry   7$    Japan
> >>>>
> >>>>Is there any similar question already posted to
> the
> >>>>list? I am a bit new to R, having a few months
> of
> >>>>experience now.
> >>>>
> >>>>Cheers,
> >>>>
> >>>>John
> >>>>
> >>>>______________________________________________
> >>>>R-help at stat.math.ethz.ch mailing list
> >>>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>>PLEASE do read the posting guide! 
> >>>>http://www.R-project.org/posting-guide.html
> >>>>
> >>>
> >>>
> >>>______________________________________________
> >>>R-help at stat.math.ethz.ch mailing list
> >>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>PLEASE do read the posting guide! 
> >>>http://www.R-project.org/posting-guide.html
> >>
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> >
> >
> >--
> >Spencer Graves, PhD, Senior Development Engineer
> >O:  (408)938-4420;  mobile:  (408)655-4567
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
>



From cyracules at yahoo.co.uk  Thu Nov  4 14:02:03 2004
From: cyracules at yahoo.co.uk (John)
Date: Thu, 4 Nov 2004 13:02:03 +0000 (GMT)
Subject: [R] Reading word by word in a dataset
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E267@usrymx25.merck.com>
Message-ID: <20041104130203.53690.qmail@web26303.mail.ukl.yahoo.com>

Dear Andy,
Why does my 'read.table()' NOT work in this example?
I have the error, "subscript out of bounds", as you
see below. My R version is 1.9.0.

> system("more mtx.ex.1")
i1-apple 10$ New_York
i2-banana 5$ London
i3-strawberry 7$ Japan
>
> read.table("mtx.ex.1",
colClasses=c("character","NULL","NULL"), fill=T)
Error in data[[i]] : subscript out of bounds
>
> read.table("mtx.ex.1", colClasses=c("character",
NULL, NULL), fill=T)
             V1  V2       V3
1      i1-apple 10$ New_York
2     i2-banana  5$   London
3 i3-strawberry  7$    Japan
>
> read.table("mtx.ex.1", colClasses=c("character",
NULL, NULL), fill=T)[,1]
[1] "i1-apple"      "i2-banana"     "i3-strawberry"
>

Cheers,

John


 --- "Liaw, Andy" <andy_liaw at merck.com> wrote: 
> Don't give up on read.table() just yet:
> 
> > read.table("clipboard", colClasses=c("character",
> "NULL", "NULL"),
> fill=TRUE)
>              V1
> 1      i1-apple
> 2     i2-banana
> 3 i3-strawberry
> 
> Andy
> 
> > From: Spencer Graves
> > 
> >       Uwe and Andy's solutions are great for many 
> > applications but won't 
> > work if not all rows have the same numbers of
> fields.  Consider for 
> > example the following modification of Lee's
> example: 
> > 
> > i1-apple        10$   New_York
> > i2-banana
> > i3-strawberry   7$    Japan
> > 
> >       If I copy this to "clipboard" and run Andy's
> code, I get the 
> > following: 
> > 
> >  > read.table("clipboard",
> colClasses=c("character", "NULL", "NULL"))
> > Error in scan(file = file, what = what, sep = sep,
> quote = 
> > quote, dec = 
> > dec,  :
> >     line 2 did not have 3 elements
> > 
> >       We can get around this using "scan", then
> splitting 
> > things apart 
> > similar to the way Uwe described: 
> > 
> >  > dat <-
> > + scan("clipboard", character(0), sep="\n")
> > Read 3 items
> >  > dash <- regexpr("-", dat)
> >  > dat2 <- substring(dat, pmax(0, dash)+1)
> >  >
> >  > blank <- regexpr(" ", dat2)
> >  > if(any(blank<0))
> > +   blank[blank<0] <- nchar(dat2[blank<0])
> >  > substring(dat2, 1, blank)
> > [1] "apple "      "banana"      "strawberry "
> > 
> >       hope this helps.  spencer graves
> >     
> > Uwe Ligges wrote:
> > 
> > > Liaw, Andy wrote:
> > >
> > >> Using R-2.0.0 on WinXPPro, cut-and-pasting the
> data you have:
> > >>
> > >>
> > >>> read.table("clipboard",
> colClasses=c("character", "NULL", "NULL"))
> > >>
> > >>
> > >>              V1
> > >> 1      i1-apple
> > >> 2     i2-banana
> > >> 3 i3-strawberry
> > >
> > >
> > >
> > > ... and if only the words after "-" are of
> interest, the 
> > statement can 
> > > be followed by
> > >
> > >  sapply(strsplit(...., "-"), "[", 2)
> > >
> > >
> > > Uwe Ligges
> > >
> > >
> > >
> > >> HTH,
> > >> Andy
> > >>
> > >>
> > >>> From: j lee
> > >>>
> > >>> Hello All,
> > >>>
> > >>> I'd like to read first words in lines into a
> new file.
> > >>> If I have a data file the following, how can I
> get the
> > >>> first words: apple, banana, strawberry?
> > >>>
> > >>> i1-apple        10$   New_York
> > >>> i2-banana       5$    London
> > >>> i3-strawberry   7$    Japan
> > >>>
> > >>> Is there any similar question already posted
> to the
> > >>> list? I am a bit new to R, having a few months
> of
> > >>> experience now.
> > >>>
> > >>> Cheers,
> > >>>
> > >>> John
> > >>>
> > >>> ______________________________________________
> > >>> R-help at stat.math.ethz.ch mailing list
> > >>> https://stat.ethz.ch/mailman/listinfo/r-help
> > >>> PLEASE do read the posting guide! 
> > >>> http://www.R-project.org/posting-guide.html
> > >>>
> > >>>
> > >>
> > >>
> > >> ______________________________________________
> > >> R-help at stat.math.ethz.ch mailing list
> > >> https://stat.ethz.ch/mailman/listinfo/r-help
> > >> PLEASE do read the posting guide! 
> > >> http://www.R-project.org/posting-guide.html
> > >
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! 
> > > http://www.R-project.org/posting-guide.html
> > 
> > 
> > -- 
> > Spencer Graves, PhD, Senior Development Engineer
> > O:  (408)938-4420;  mobile:  (408)655-4567
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> > 
> 
> 
>
------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any
> attachments, contains information of Merck & Co.,
> Inc. (One Merck Drive, Whitehouse Station, New
> Jersey, USA 08889), and/or its affiliates (which may
> be known outside the United States as Merck Frosst,
> Merck Sharp & Dohme or MSD and in Japan, as Banyu)
> that may be confidential, proprietary copyrighted
> and/or legally privileged. It is intended solely for
> the use of the individual or entity named on this
> message.  If you are not the intended recipient, and
> have received this message in error, please notify
> us immediately by reply e-mail and then delete it
> from your system.
>
------------------------------------------------------------------------------
>



From bxc at steno.dk  Thu Nov  4 14:02:53 2004
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Thu, 4 Nov 2004 14:02:53 +0100
Subject: [R] sub- and superscript in plot labels
Message-ID: <0ABD88905D18E347874E0FB71C0B29E90237CFD1@exdkba022.novo.dk>

try:

plot(1:10, xlab = substitute( expression(paste("nm SO"[4]^{2-x}, " ", 
                              mu, "eq cm"^{-2}, " yr"^{-1})), 
                              list(x="") ) )

I have no understanding of why it works, formula fidgeting usually
requires use of subtitute().

Btw. I started out trying:
plot(1:10, xlab = substitute( expression(paste("nm SO"[4]^x, " ", 
                              mu, "eq cm"^{-2}, " yr"^{-1})), 
                              list(x="2-") ) )
but that gives a hyphen and not a minus trailing the superscript "2".

Bendix Carstensen

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gavin Simpson
> Sent: Thursday, November 04, 2004 1:20 PM
> To: R-help at stat.math.ethz.ch
> Subject: [R] sub- and superscript in plot labels
> 
> 
> Dear List,
> 
> I need to add a subscript and a superscript to some of the 
> ions in the 
> labels on some plots.
> 
> I have got to here but now I'm stuck:
> 
> plot(1:10, xlab = expression(paste("nm SO"[4], " ", mu, "eq 
> cm"^{-2}, " 
> yr"^{-1})))
> 
> Which gives almost  what I require. No matter what I tried, 
> however, I 
> could not get bot a sub script *and* a superscript attached 
> to the SO in 
> the label.
> 
> In LaTeX I would just do $SO_4^{2-}$ but taking that to R produces a 
> syntax error:
> 
> plot(1:10, xlab = expression(paste("nm SO"[4]^{2-}, " ", mu, "eq 
> cm"^{-2}, " yr"^{-1})))
> 
> Strangely, I can do this:
> 
> plot(1:10, xlab = expression(paste("nm SO"[4]^2, " ", mu, "eq 
> cm"^{-2}, 
> " yr"^{-1})))
> 
> With almost the desired effect (except I need to add two 
> characters to 
> the superscript).
> 
> Can any one offer a solution?
> 
> Many thanks
> 
> Gavin
> -- 
> %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~
> %~%~%~%~%
> Gavin Simpson                     [T] +44 (0)20 7679 5522
> ENSIS Research Fellow             [F] +44 (0)20 7679 7565
> ENSIS Ltd. & ECRC                 [E] gavin.simpson at ucl.ac.uk
> UCL Department of Geography       [W] 
> http://www.ucl.ac.uk/~ucfagls/cv/
> 26 Bedford Way              
>       [W] http://www.ucl.ac.uk/~ucfagls/
> London.  WC1H 0AP. 
> %~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~
> %~%~%~%~%
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From puetz at mpipsykl.mpg.de  Thu Nov  4 14:20:21 2004
From: puetz at mpipsykl.mpg.de (=?ISO-8859-1?Q?Benno_P=FCtz?=)
Date: Thu, 04 Nov 2004 14:20:21 +0100
Subject: [R] Sweave and transparent PDF?
In-Reply-To: <200408091043.34409.deepayan@stat.wisc.edu>
References: <D15343265276D31197BC00A024A6C110C79337@EXS_BDC>
	<200408091043.34409.deepayan@stat.wisc.edu>
Message-ID: <418A2C95.10109@mpipsykl.mpg.de>

Hello,

I have run across the following problem:

Creating PDF files manually by using

pdf(version=1.4)

I can make graphs using the new transparency feature of R2.0. If, 
however, I try to create the same graphs with Sweave, all transparent 
stuff is gone. This os likely due to the default version parameter in in 
pdf().

How can I tell Sweave to use the newer PDF version? The parameter seems 
to be hard-coded (not via some option) and I could not come up with some 
kind of hook ...

Thank you

    Benno



From dmb at mrc-dunn.cam.ac.uk  Thu Nov  4 14:24:11 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Thu, 4 Nov 2004 13:24:11 +0000 (GMT)
Subject: [R] Legend placement in barplot?
In-Reply-To: <loom.20041103T194633-105@post.gmane.org>
Message-ID: <Pine.LNX.4.21.0411041315270.29228-100000@mail.mrc-dunn.cam.ac.uk>

On Wed, 3 Nov 2004, Gabor Grothendieck wrote:

>Dan Bolser <dmb <at> mrc-dunn.cam.ac.uk> writes:
>
>: 
>: This has been asked before, but all the answers are hidiously complex.
>: 
>: The
>: 
>: legend.text=TRUE 
>: 
>: option of barplot is almost exactly what I need, except I need a
>: 
>: legend.placement='tl'
>: 
>: (top left) option. This option would be in contrast to the default
>: placement which we could call 'tr' (top right).
>: 
>: Anyone know how to edit the barplot code to make this change? Could
>: someone like me work out how to do this?
>: 
>: Cheers,
>: Dan.
>
>
>Check out gplots::smartlegend (in the R 2.0.0 gregmisc bundle).


This works great, but like the (smart)legend function, fill=true appears
to be giving me only black boxes.

Here is what I add..

smartlegend(x="left",y="top",
            c("PDB","MSD"),
            fill=TRUE,
            col=c("red","blue")
           )

The result is two black boxes! I tried swapping the order of the color and
fill options, but to the same effect.

I got round the problem by using...

smartlegend(x="left",y="top",
            c("PDB","MSD"),
            col=c("red","blue"),
            lwd=5
            )

Not quite the same, but good enough.


One other thing (while I am generally complaining), the legend dosn't
scale correctly as I change the image size with the mouse. All the other
aspects of the barplot scale correctly. If I redraw the legend after
changing the size it is scaled correctly, suggesting that this problem
isn't fundamental, but is a bug in the implementation of legend.



>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ramasamy at cancer.org.uk  Thu Nov  4 14:46:03 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Thu, 04 Nov 2004 13:46:03 +0000
Subject: [R] list files ignoring the case option
Message-ID: <1099575962.3232.20.camel@ramasamy.stats>

Sorry if this is a question more on regular expressions. I am dealing
with several files which have been badly named. For example the files
are given either the extensions txt, TXT or Txt. I wish to select all
those files ending with 'txt' ignoring case.

Here is how I would do it in bash (Redhat FC2) :

   touch  a1.txt  a2.TXT  a3.Txt  txt.control  TXT.control
   ls -1 | grep -i "txt$"


Here is how I am currently doing it in R

   a <- list.files(all.files=T)
   grep( "txt$", a, ignore.case=T, value=T )


Is it possible for me to modify the following line to include ignore
case option ?

   a <- list.files( pattern="txt$" )

Thank you.

Regards, Adai



From ripley at stats.ox.ac.uk  Thu Nov  4 14:49:52 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 4 Nov 2004 13:49:52 +0000 (GMT)
Subject: [R] Reading word by word in a dataset
In-Reply-To: <20041104130203.53690.qmail@web26303.mail.ukl.yahoo.com>
Message-ID: <Pine.LNX.4.44.0411041344490.26686-100000@gannet.stats>

On Thu, 4 Nov 2004, John wrote:

> Dear Andy,
> Why does my 'read.table()' NOT work in this example?
> I have the error, "subscript out of bounds", as you
> see below. My R version is 1.9.0.
                             ^^^^^

That is your problem. It works in the current version of R, 2.0.0. Using
colClasses=NULL was not documented in 1.9.0, and was not intended to work.

What does the posting guide say about this?


> > system("more mtx.ex.1")
> i1-apple 10$ New_York
> i2-banana 5$ London
> i3-strawberry 7$ Japan
> >
> > read.table("mtx.ex.1",
> colClasses=c("character","NULL","NULL"), fill=T)
> Error in data[[i]] : subscript out of bounds
> >
> > read.table("mtx.ex.1", colClasses=c("character",
> NULL, NULL), fill=T)
>              V1  V2       V3
> 1      i1-apple 10$ New_York
> 2     i2-banana  5$   London
> 3 i3-strawberry  7$    Japan
> >
> > read.table("mtx.ex.1", colClasses=c("character",
> NULL, NULL), fill=T)[,1]
> [1] "i1-apple"      "i2-banana"     "i3-strawberry"
> >
> 
> Cheers,
> 
> John
> 
> 
>  --- "Liaw, Andy" <andy_liaw at merck.com> wrote: 
> > Don't give up on read.table() just yet:
> > 
> > > read.table("clipboard", colClasses=c("character",
> > "NULL", "NULL"),
> > fill=TRUE)
> >              V1
> > 1      i1-apple
> > 2     i2-banana
> > 3 i3-strawberry
> > 
> > Andy
> > 
> > > From: Spencer Graves
> > > 
> > >       Uwe and Andy's solutions are great for many 
> > > applications but won't 
> > > work if not all rows have the same numbers of
> > fields.  Consider for 
> > > example the following modification of Lee's
> > example: 
> > > 
> > > i1-apple        10$   New_York
> > > i2-banana
> > > i3-strawberry   7$    Japan
> > > 
> > >       If I copy this to "clipboard" and run Andy's
> > code, I get the 
> > > following: 
> > > 
> > >  > read.table("clipboard",
> > colClasses=c("character", "NULL", "NULL"))
> > > Error in scan(file = file, what = what, sep = sep,
> > quote = 
> > > quote, dec = 
> > > dec,  :
> > >     line 2 did not have 3 elements
> > > 
> > >       We can get around this using "scan", then
> > splitting 
> > > things apart 
> > > similar to the way Uwe described: 
> > > 
> > >  > dat <-
> > > + scan("clipboard", character(0), sep="\n")
> > > Read 3 items
> > >  > dash <- regexpr("-", dat)
> > >  > dat2 <- substring(dat, pmax(0, dash)+1)
> > >  >
> > >  > blank <- regexpr(" ", dat2)
> > >  > if(any(blank<0))
> > > +   blank[blank<0] <- nchar(dat2[blank<0])
> > >  > substring(dat2, 1, blank)
> > > [1] "apple "      "banana"      "strawberry "
> > > 
> > >       hope this helps.  spencer graves
> > >     
> > > Uwe Ligges wrote:
> > > 
> > > > Liaw, Andy wrote:
> > > >
> > > >> Using R-2.0.0 on WinXPPro, cut-and-pasting the
> > data you have:
> > > >>
> > > >>
> > > >>> read.table("clipboard",
> > colClasses=c("character", "NULL", "NULL"))
> > > >>
> > > >>
> > > >>              V1
> > > >> 1      i1-apple
> > > >> 2     i2-banana
> > > >> 3 i3-strawberry
> > > >
> > > >
> > > >
> > > > ... and if only the words after "-" are of
> > interest, the 
> > > statement can 
> > > > be followed by
> > > >
> > > >  sapply(strsplit(...., "-"), "[", 2)
> > > >
> > > >
> > > > Uwe Ligges
> > > >
> > > >
> > > >
> > > >> HTH,
> > > >> Andy
> > > >>
> > > >>
> > > >>> From: j lee
> > > >>>
> > > >>> Hello All,
> > > >>>
> > > >>> I'd like to read first words in lines into a
> > new file.
> > > >>> If I have a data file the following, how can I
> > get the
> > > >>> first words: apple, banana, strawberry?
> > > >>>
> > > >>> i1-apple        10$   New_York
> > > >>> i2-banana       5$    London
> > > >>> i3-strawberry   7$    Japan
> > > >>>
> > > >>> Is there any similar question already posted
> > to the
> > > >>> list? I am a bit new to R, having a few months
> > of
> > > >>> experience now.
> > > >>>
> > > >>> Cheers,
> > > >>>
> > > >>> John

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From gavin.simpson at ucl.ac.uk  Thu Nov  4 14:59:36 2004
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Thu, 04 Nov 2004 13:59:36 +0000
Subject: [R] sub- and superscript in plot labels
In-Reply-To: <Pine.LNX.4.44.0411041237010.11570-100000@gannet.stats>
References: <Pine.LNX.4.44.0411041237010.11570-100000@gannet.stats>
Message-ID: <418A35C8.4040602@ucl.ac.uk>

Prof Brian Ripley wrote:
> On Thu, 4 Nov 2004, Gavin Simpson wrote:
> 
> 
>>Dear List,
>>
>>I need to add a subscript and a superscript to some of the ions in the 
>>labels on some plots.
>>
>>I have got to here but now I'm stuck:
>>
>>plot(1:10, xlab = expression(paste("nm SO"[4], " ", mu, "eq cm"^{-2}, " 
>>yr"^{-1})))
>>
>>Which gives almost  what I require. No matter what I tried, however, I 
>>could not get bot a sub script *and* a superscript attached to the SO in 
>>the label.
>>
>>In LaTeX I would just do $SO_4^{2-}$ but taking that to R produces a 
>>syntax error:
>>
>>plot(1:10, xlab = expression(paste("nm SO"[4]^{2-}, " ", mu, "eq 
>>cm"^{-2}, " yr"^{-1})))
> 
> 
> The problem is 2-.  That's not an R expression.  Using "2-" might give
> what you want, but it will use a hyphen rather than a minus. Otherwise
> 
> plot(1:10, xlab = expression(paste("nm SO"[4]^{2-phantom()})))
> 
> will give a minus.
> 

Many thanks to Brian Ripley and Bendix Carstensen for your replies.

Both of the above options produce what I was after. One quick follow-up 
question regarding the use of phantom(). Looking at ?plotmath phantom 
leaves space for a character passed as an argument to phantom(), but 
does not plot it.

In the example above we are leaving space for "nothing". I don't 
understand why this is a valid R expression. I guess phantom() is 
returning something that makes 2-<returned_val> a valid expression, but 
I couldn't find the help for ?phantom so I couldn't check on this in the 
documentation.

Also as an aside, phantom() appears, visually, to be a function, but it 
is not visible to the user as a function. i.e. typing phantom at the 
prompt yields: Error: Object "phantom" not found. getAnywhere(phantom) 
yields nothing either. What is phantom() in R parlance?

All the best,

Gavin

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpson at ucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From alexandersokol at ofir.dk  Thu Nov  4 15:00:21 2004
From: alexandersokol at ofir.dk (Alexander Sokol)
Date: Thu, 4 Nov 2004 15:00:21 +0100
Subject: [R] Conversion of strings to expressions
Message-ID: <41A7214E@webmail2.ofir.dk>

Hello,

I'm not sure how to state my question in a technically accurate manner, so 
I'll use a short example to clarify my problem:

Say I have a vector,

vec<-c(1,2,3,2)

I would like to be able to "reference" the vector by using the string 
containing the name of the vector - that is, I would like to know of some 
function which is able to convert the string "vec" into the vector vec itself. 
The purpose of this is to be able to do something like

>somefunc("vec")==2
[1] FALSE TRUE FALSE TRUE

I hope it is clear from this what the problem is. It seems to me that the 
function desired is akin to a sort of "inverse" to the quote() function, but I 
have been unable to find such a function.

I am using R 1.9.1 on Windows 2000 SP4. Does anyone have a suggestion of how 
to solve the problem?

Thanks,
 Alexander



From henric.nilsson at statisticon.se  Thu Nov  4 15:19:58 2004
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Thu, 04 Nov 2004 15:19:58 +0100
Subject: [R] Legend placement in barplot?
In-Reply-To: <Pine.LNX.4.21.0411041315270.29228-100000@mail.mrc-dunn.cam
	.ac.uk>
References: <loom.20041103T194633-105@post.gmane.org>
	<Pine.LNX.4.21.0411041315270.29228-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <6.1.2.0.0.20041104151435.04aa3720@10.0.10.66>

At 13:24 2004-11-04 +0000, Dan Bolser wrote:

> >Check out gplots::smartlegend (in the R 2.0.0 gregmisc bundle).
>This works great, but like the (smart)legend function, fill=true appears
>to be giving me only black boxes.
>Here is what I add..
>smartlegend(x="left",y="top",
>             c("PDB","MSD"),
>             fill=TRUE,
>             col=c("red","blue")
>            )

For coloured boxes, use

smartlegend(x="left",
                    y="top",
                    c("PDB","MSD"),
                  fill=c("red","blue"))

HTH,
Henric



From o.villani at utanet.at  Thu Nov  4 14:58:34 2004
From: o.villani at utanet.at (Oskar Villani)
Date: Thu, 4 Nov 2004 13:58:34 +0000 (UTC)
Subject: [R] calling a var by name in another var
Message-ID: <loom.20041104T144526-955@post.gmane.org>

Hello list,

I'd like to use a variable (or a column of a data frame) by using its name as a
string. E.g.:

Data2003 <- c(150,200,120)
Data2004 <- c(145,211,110)

myvar1 <- "Data2003"
myvar2 <- "Data2004"

# now I'd like do do this

total <- Data2003 + Data2004

# in any way like
# total <- ???(myvar1, myvar2)
# or
# total <- ???(myvar1) + ???(myvar2)
# or something like that

Is there a possibility to do this in R - can't find a solution!

Thanks a lot



From sundar.dorai-raj at pdf.com  Thu Nov  4 15:38:39 2004
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 04 Nov 2004 08:38:39 -0600
Subject: [R] list files ignoring the case option
In-Reply-To: <1099575962.3232.20.camel@ramasamy.stats>
References: <1099575962.3232.20.camel@ramasamy.stats>
Message-ID: <418A3EEF.2060604@pdf.com>



Adaikalavan Ramasamy wrote:

> Sorry if this is a question more on regular expressions. I am dealing
> with several files which have been badly named. For example the files
> are given either the extensions txt, TXT or Txt. I wish to select all
> those files ending with 'txt' ignoring case.
> 
> Here is how I would do it in bash (Redhat FC2) :
> 
>    touch  a1.txt  a2.TXT  a3.Txt  txt.control  TXT.control
>    ls -1 | grep -i "txt$"
> 
> 
> Here is how I am currently doing it in R
> 
>    a <- list.files(all.files=T)
>    grep( "txt$", a, ignore.case=T, value=T )
> 
> 
> Is it possible for me to modify the following line to include ignore
> case option ?
> 
>    a <- list.files( pattern="txt$" )
> 
> Thank you.
> 
> Regards, Adai
> 

Not much of a regexpr guy myself, but the following should work:

list.files(pattern = "[tT][xX][tT]$")

There's probably a better answer though.

--sundar



From dimitris.rizopoulos at med.kuleuven.ac.be  Thu Nov  4 15:41:16 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Thu, 4 Nov 2004 15:41:16 +0100
Subject: [R] Conversion of strings to expressions
References: <41A7214E@webmail2.ofir.dk>
Message-ID: <00cf01c4c27c$56d6d6b0$0540210a@www.domain>

Hi Alexander,

I'm not quite sure of what you want, but maybe this will help:

vec <- c(1,2,3,2)
get("vec")==2

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Alexander Sokol" <alexandersokol at ofir.dk>
To: <R-help at stat.math.ethz.ch>
Sent: Thursday, November 04, 2004 3:00 PM
Subject: [R] Conversion of strings to expressions


> Hello,
>
> I'm not sure how to state my question in a technically accurate 
> manner, so
> I'll use a short example to clarify my problem:
>
> Say I have a vector,
>
> vec<-c(1,2,3,2)
>
> I would like to be able to "reference" the vector by using the 
> string
> containing the name of the vector - that is, I would like to know of 
> some
> function which is able to convert the string "vec" into the vector 
> vec itself.
> The purpose of this is to be able to do something like
>
>>somefunc("vec")==2
> [1] FALSE TRUE FALSE TRUE
>
> I hope it is clear from this what the problem is. It seems to me 
> that the
> function desired is akin to a sort of "inverse" to the quote() 
> function, but I
> have been unable to find such a function.
>
> I am using R 1.9.1 on Windows 2000 SP4. Does anyone have a 
> suggestion of how
> to solve the problem?
>
> Thanks,
> Alexander
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From sundar.dorai-raj at pdf.com  Thu Nov  4 15:41:36 2004
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 04 Nov 2004 08:41:36 -0600
Subject: [R] Conversion of strings to expressions
In-Reply-To: <41A7214E@webmail2.ofir.dk>
References: <41A7214E@webmail2.ofir.dk>
Message-ID: <418A3FA0.4090007@pdf.com>



Alexander Sokol wrote:

> Hello,
> 
> I'm not sure how to state my question in a technically accurate manner, so 
> I'll use a short example to clarify my problem:
> 
> Say I have a vector,
> 
> vec<-c(1,2,3,2)
> 
> I would like to be able to "reference" the vector by using the string 
> containing the name of the vector - that is, I would like to know of some 
> function which is able to convert the string "vec" into the vector vec itself. 
> The purpose of this is to be able to do something like
> 
> 
>>somefunc("vec")==2
> 
> [1] FALSE TRUE FALSE TRUE
> 
> I hope it is clear from this what the problem is. It seems to me that the 
> function desired is akin to a sort of "inverse" to the quote() function, but I 
> have been unable to find such a function.
> 
> I am using R 1.9.1 on Windows 2000 SP4. Does anyone have a suggestion of how 
> to solve the problem?
> 
> Thanks,
>  Alexander
> 

I think you want ?get. This is FAQ 7.21.

--sundar



From ripley at stats.ox.ac.uk  Thu Nov  4 16:03:11 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 4 Nov 2004 15:03:11 +0000 (GMT)
Subject: [R] sub- and superscript in plot labels
In-Reply-To: <418A35C8.4040602@ucl.ac.uk>
Message-ID: <Pine.LNX.4.44.0411041458040.31970-100000@gannet.stats>

On Thu, 4 Nov 2004, Gavin Simpson wrote:

> Prof Brian Ripley wrote:
> > On Thu, 4 Nov 2004, Gavin Simpson wrote:
> > 
> > 
> >>Dear List,
> >>
> >>I need to add a subscript and a superscript to some of the ions in the 
> >>labels on some plots.
> >>
> >>I have got to here but now I'm stuck:
> >>
> >>plot(1:10, xlab = expression(paste("nm SO"[4], " ", mu, "eq cm"^{-2}, " 
> >>yr"^{-1})))
> >>
> >>Which gives almost  what I require. No matter what I tried, however, I 
> >>could not get bot a sub script *and* a superscript attached to the SO in 
> >>the label.
> >>
> >>In LaTeX I would just do $SO_4^{2-}$ but taking that to R produces a 
> >>syntax error:
> >>
> >>plot(1:10, xlab = expression(paste("nm SO"[4]^{2-}, " ", mu, "eq 
> >>cm"^{-2}, " yr"^{-1})))
> > 
> > 
> > The problem is 2-.  That's not an R expression.  Using "2-" might give
> > what you want, but it will use a hyphen rather than a minus. Otherwise
> > 
> > plot(1:10, xlab = expression(paste("nm SO"[4]^{2-phantom()})))
> > 
> > will give a minus.
> > 
> 
> Many thanks to Brian Ripley and Bendix Carstensen for your replies.
> 
> Both of the above options produce what I was after. One quick follow-up 
> question regarding the use of phantom(). Looking at ?plotmath phantom 
> leaves space for a character passed as an argument to phantom(), but 
> does not plot it.
> 
> In the example above we are leaving space for "nothing". I don't 
> understand why this is a valid R expression. I guess phantom() is 
> returning something that makes 2-<returned_val> a valid expression, but 
> I couldn't find the help for ?phantom so I couldn't check on this in the 
> documentation.

Yes, that is a valid formal expression, so R's parser is happy.

> Also as an aside, phantom() appears, visually, to be a function, but it 
> is not visible to the user as a function. i.e. typing phantom at the 
> prompt yields: Error: Object "phantom" not found. getAnywhere(phantom) 
> yields nothing either. What is phantom() in R parlance?

It's part of the language of formal expressions that plotmath accepts.  
It is not part of R per se.  Think of it as a private function to 
plotmath's internal code (and there are quite a few others in plotmath).

It is an analogue of TeX's \hphantom and \vphantom, and like them used as 
a placeholder.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Nov  4 16:10:37 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 4 Nov 2004 15:10:37 +0000 (GMT)
Subject: [R] list files ignoring the case option
In-Reply-To: <1099575962.3232.20.camel@ramasamy.stats>
Message-ID: <Pine.LNX.4.44.0411041503510.31970-100000@gannet.stats>

On Thu, 4 Nov 2004, Adaikalavan Ramasamy wrote:

> Sorry if this is a question more on regular expressions. I am dealing
> with several files which have been badly named. For example the files
> are given either the extensions txt, TXT or Txt. I wish to select all
> those files ending with 'txt' ignoring case.
> 
> Here is how I would do it in bash (Redhat FC2) :
> 
>    touch  a1.txt  a2.TXT  a3.Txt  txt.control  TXT.control
>    ls -1 | grep -i "txt$"
> 
> 
> Here is how I am currently doing it in R
> 
>    a <- list.files(all.files=T)
>    grep( "txt$", a, ignore.case=T, value=T )

I'd write that in one line, but it seems as good a way as any.

> Is it possible for me to modify the following line to include ignore
> case option ?
> 
>    a <- list.files( pattern="txt$" )

Not as such.

First, I think you want "\\.txt$" there if you do mean file extensions.

You can use a regexp that ignores case, though, e.g. "\\.[Tt]{Xx][Tt]".

But I would just use your original idea, which is essentially what ls() is 
doing internally and is self-documenting.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tlumley at u.washington.edu  Thu Nov  4 16:13:33 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 4 Nov 2004 07:13:33 -0800 (PST)
Subject: [R] Plotting a linear model object with R 2.0 for Mac OS X
In-Reply-To: <1099538004.41899e54c02b0@webmail.fas.harvard.edu>
References: <1099538004.41899e54c02b0@webmail.fas.harvard.edu>
Message-ID: <Pine.A41.4.61b.0411040711100.207912@homer03.u.washington.edu>

On Wed, 3 Nov 2004, Justin Manjourides wrote:

> I'm using R for Mac OS X Aqua GUI version 2.0
>
> When using plot() with a linear model object, the plot of the LSR line does not
> appear. I get the residual plots, the Normal Q-Q plots and Cook's Differnce
> plot, just not the first plot with the fitted line. I just get an empty Quartz
> window, with no graph displayed. I've even tried to use plot.lm() and plotting
> a glm object, but I get the same results. Has anybody run into this problem?
>

There is no 'first plot with the fitted line' in the documentation, 
because it would not be possible with more than one predictor.  The first 
plot should be residuals vs fitted values.

To overlay a fitted regression line on a scatterplot of the data use 
abline(your.model).

 	-thomas



From tlumley at u.washington.edu  Thu Nov  4 16:24:13 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 4 Nov 2004 07:24:13 -0800 (PST)
Subject: [R] sub- and superscript in plot labels
In-Reply-To: <418A35C8.4040602@ucl.ac.uk>
References: <Pine.LNX.4.44.0411041237010.11570-100000@gannet.stats>
	<418A35C8.4040602@ucl.ac.uk>
Message-ID: <Pine.A41.4.61b.0411040717490.207912@homer03.u.washington.edu>

On Thu, 4 Nov 2004, Gavin Simpson wrote:
>
> Both of the above options produce what I was after. One quick follow-up 
> question regarding the use of phantom(). Looking at ?plotmath phantom leaves 
> space for a character passed as an argument to phantom(), but does not plot 
> it.
>
> In the example above we are leaving space for "nothing". I don't understand 
> why this is a valid R expression. I guess phantom() is returning something 
> that makes 2-<returned_val> a valid expression, but I couldn't find the help 
> for ?phantom so I couldn't check on this in the documentation.
>
> Also as an aside, phantom() appears, visually, to be a function, but it is 
> not visible to the user as a function. i.e. typing phantom at the prompt 
> yields: Error: Object "phantom" not found. getAnywhere(phantom) yields 
> nothing either. What is phantom() in R parlance?

phantom is not part of the R language, just part of the language that 
plotmath understands (like frac and scriptstyle).  The relevant help page 
is ?plotmath.

As to why it works: phantom() returns an empty box large enough to hold 
its argument.  When given no argument, it returns a very small empty box.

In this example the importance of phantom() is that 2-phantom() is parsed 
as a binary operation, indicating that the - character should be typeset 
as a minus sign.   Writing 2- is a parse error, and writing "2-" indicates 
that the - character is ordinary text and should be typeset as a 
hyphen.

 	-thomas



From tlumley at u.washington.edu  Thu Nov  4 16:25:47 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 4 Nov 2004 07:25:47 -0800 (PST)
Subject: [R] Conversion of strings to expressions
In-Reply-To: <41A7214E@webmail2.ofir.dk>
References: <41A7214E@webmail2.ofir.dk>
Message-ID: <Pine.A41.4.61b.0411040724180.207912@homer03.u.washington.edu>

On Thu, 4 Nov 2004, Alexander Sokol wrote:
> Say I have a vector,
>
> vec<-c(1,2,3,2)
>
> I would like to be able to "reference" the vector by using the string
> containing the name of the vector - that is, I would like to know of some
> function which is able to convert the string "vec" into the vector vec itself.
> The purpose of this is to be able to do something like
>
>> somefunc("vec")==2
> [1] FALSE TRUE FALSE TRUE
>

A) This is a FAQ

B) Many people who think they want to do this are mistaken, and they 
actually want to, for example, work with a list, one of whose elements is 
called "vec".

 	-thomas



From ottorino-luca.pantani at unifi.it  Thu Nov  4 16:29:19 2004
From: ottorino-luca.pantani at unifi.it (8rino-Luca Pantani)
Date: Thu, 04 Nov 2004 16:29:19 +0100
Subject: [R] a coloured band within each panel of a lattice bwplot
Message-ID: <DNEELNJCLGBOLHCFLMHBOENGCHAA.OLPantani@unifi.it>

Hi all,
I would like to add to each panel of a bwplot a coloured central band,
centered on the mean of the values, being its width +- 2% of the mean
itself.

I know how to add lines, i.e. something like

bwplot(X ~ Y|FACTOR
       data=my.df,
       panel= function(x, y){
         panel.bwplot(x, y)
         panel.abline(v = mean(x, na.rm = T) - mean(x, na.rm = T) * 0.02
         panel.abline(v = mean(x, na.rm = T) + mean(x, na.rm = T) * 0.02
       })

but I cannot figure out how to shade the region included between the two
lines.

Is it possible, or I'm pretending too much?

Thanks to all.

Ottorino-Luca Pantani, Universit?? di Firenze
Dip. Scienza del Suolo e Nutrizione della Pianta



From jfox at mcmaster.ca  Thu Nov  4 16:34:12 2004
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 04 Nov 2004 10:34:12 -0500
Subject: [R] Conversion of strings to expressions
In-Reply-To: <41A7214E@webmail2.ofir.dk>
Message-ID: <web-71346033@cgpsrv2.cis.mcmaster.ca>

Dear Alexander,

You can use get("vec").

I hope this helps,
 John

On Thu, 4 Nov 2004 15:00:21 +0100
 Alexander Sokol <alexandersokol at ofir.dk> wrote:
> Hello,
> 
> I'm not sure how to state my question in a technically accurate
> manner, so 
> I'll use a short example to clarify my problem:
> 
> Say I have a vector,
> 
> vec<-c(1,2,3,2)
> 
> I would like to be able to "reference" the vector by using the string
> 
> containing the name of the vector - that is, I would like to know of
> some 
> function which is able to convert the string "vec" into the vector
> vec itself. 
> The purpose of this is to be able to do something like
> 
> >somefunc("vec")==2
> [1] FALSE TRUE FALSE TRUE
> 
> I hope it is clear from this what the problem is. It seems to me that
> the 
> function desired is akin to a sort of "inverse" to the quote()
> function, but I 
> have been unable to find such a function.
> 
> I am using R 1.9.1 on Windows 2000 SP4. Does anyone have a suggestion
> of how 
> to solve the problem?
> 
> Thanks,
>  Alexander
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada
http://socserv.mcmaster.ca/jfox/



From ripley at stats.ox.ac.uk  Thu Nov  4 16:42:16 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 4 Nov 2004 15:42:16 +0000 (GMT)
Subject: [R] Conversion of strings to expressions
In-Reply-To: <41A7214E@webmail2.ofir.dk>
Message-ID: <Pine.LNX.4.44.0411041539100.14037-100000@gannet.stats>

?get

> vec<-c(1,2,3,2)
> get("vec") == 2
[1] FALSE  TRUE FALSE  TRUE

Note that this is converting a string to an object, not to an expression:
the latter is done by

> parse(text="vec")
expression(vec)


On Thu, 4 Nov 2004, Alexander Sokol wrote:

> I'm not sure how to state my question in a technically accurate manner, so 
> I'll use a short example to clarify my problem:
> 
> Say I have a vector,
> 
> vec<-c(1,2,3,2)
> 
> I would like to be able to "reference" the vector by using the string 
> containing the name of the vector - that is, I would like to know of some 
> function which is able to convert the string "vec" into the vector vec itself. 
> The purpose of this is to be able to do something like
> 
> >somefunc("vec")==2
> [1] FALSE TRUE FALSE TRUE
> 
> I hope it is clear from this what the problem is. It seems to me that the 
> function desired is akin to a sort of "inverse" to the quote() function, but I 
> have been unable to find such a function.
> 
> I am using R 1.9.1 on Windows 2000 SP4. Does anyone have a suggestion of how 
> to solve the problem?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dimitris.rizopoulos at med.kuleuven.ac.be  Thu Nov  4 17:01:27 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Thu, 4 Nov 2004 17:01:27 +0100
Subject: [R] calling a var by name in another var
References: <loom.20041104T144526-955@post.gmane.org>
Message-ID: <012b01c4c287$8a5f4750$0540210a@www.domain>

look at ?get, this is also in R-FAQ 7.21

get(myvar1) + get(myvar2)


Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Oskar Villani" <o.villani at utanet.at>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, November 04, 2004 2:58 PM
Subject: [R] calling a var by name in another var


> Hello list,
>
> I'd like to use a variable (or a column of a data frame) by using 
> its name as a
> string. E.g.:
>
> Data2003 <- c(150,200,120)
> Data2004 <- c(145,211,110)
>
> myvar1 <- "Data2003"
> myvar2 <- "Data2004"
>
> # now I'd like do do this
>
> total <- Data2003 + Data2004
>
> # in any way like
> # total <- ???(myvar1, myvar2)
> # or
> # total <- ???(myvar1) + ???(myvar2)
> # or something like that
>
> Is there a possibility to do this in R - can't find a solution!
>
> Thanks a lot
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From gunter.berton at gene.com  Thu Nov  4 17:10:51 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 4 Nov 2004 08:10:51 -0800
Subject: [R] list files ignoring the case option
In-Reply-To: <418A3EEF.2060604@pdf.com>
Message-ID: <200411041610.iA4GAp4g004073@faraday.gene.com>


Simpler:First get uniform case. ?toupper or ?tolower


-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Sundar 
> Dorai-Raj
> Sent: Thursday, November 04, 2004 6:39 AM
> To: ramasamy at cancer.org.uk
> Cc: R-help
> Subject: Re: [R] list files ignoring the case option
> 
> 
> 
> Adaikalavan Ramasamy wrote:
> 
> > Sorry if this is a question more on regular expressions. I 
> am dealing
> > with several files which have been badly named. For example 
> the files
> > are given either the extensions txt, TXT or Txt. I wish to 
> select all
> > those files ending with 'txt' ignoring case.
> > 
> > Here is how I would do it in bash (Redhat FC2) :
> > 
> >    touch  a1.txt  a2.TXT  a3.Txt  txt.control  TXT.control
> >    ls -1 | grep -i "txt$"
> > 
> > 
> > Here is how I am currently doing it in R
> > 
> >    a <- list.files(all.files=T)
> >    grep( "txt$", a, ignore.case=T, value=T )
> > 
> > 
> > Is it possible for me to modify the following line to include ignore
> > case option ?
> > 
> >    a <- list.files( pattern="txt$" )
> > 
> > Thank you.
> > 
> > Regards, Adai
> > 
> 
> Not much of a regexpr guy myself, but the following should work:
> 
> list.files(pattern = "[tT][xX][tT]$")
> 
> There's probably a better answer though.
> 
> --sundar
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From gkaravas at ee.duth.gr  Thu Nov  4 17:20:12 2004
From: gkaravas at ee.duth.gr (KARAVASILIS GEORGE)
Date: Thu, 04 Nov 2004 18:20:12 +0200
Subject: [R] differences between vectors
Message-ID: <418A56BC.4010405@ee.duth.gr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041104/550edc02/attachment.pl

From SBist at LordAbbett.com  Thu Nov  4 17:23:07 2004
From: SBist at LordAbbett.com (Bist, Sandip)
Date: Thu, 4 Nov 2004 11:23:07 -0500 
Subject: [R] Problems with Rpart
Message-ID: <BCB019F368866A479696C25C8F0B3AEB039769B7@la-mail3.lordabbett.com>


Hi,

Anyone had this problem.

I run a test Data aginst my fitted Rpart Object
  
pruned.prediction <- predict( pruned.modeltree, newdata = evaluation, type =
"prob",na.action= na.omit )
Warning message: 
'newdata' had 2019 rows but variable(s) found have 5865 rows 


My new data: evaluation has 2019 rows but the number of observation in my
rpart Object is 5865. ( This is the same number of rows in the DataSet I use
to build the rpart Object).

I had tried with explicitly removing all "NA" from DataSets but no help.


Thanks
Sandip



From tlumley at u.washington.edu  Thu Nov  4 17:38:49 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 4 Nov 2004 08:38:49 -0800 (PST)
Subject: [R] calling a var by name in another var
In-Reply-To: <loom.20041104T144526-955@post.gmane.org>
References: <loom.20041104T144526-955@post.gmane.org>
Message-ID: <Pine.A41.4.61b.0411040837430.301366@homer06.u.washington.edu>

On Thu, 4 Nov 2004, Oskar Villani wrote:

> Hello list,
>
> I'd like to use a variable (or a column of a data frame) by using its name as a
> string. E.g.:
>
> Data2003 <- c(150,200,120)
> Data2004 <- c(145,211,110)
>
> myvar1 <- "Data2003"
> myvar2 <- "Data2004"
>
> # now I'd like do do this
>
> total <- Data2003 + Data2004
>
> # in any way like
> # total <- ???(myvar1, myvar2)
> # or
> # total <- ???(myvar1) + ???(myvar2)
> # or something like that
>

A) This is a FAQ

B) Many people who think they want to do this would be better off putting 
the variables into a list.


 	-thomas



From jes at luretanker.no  Thu Nov  4 18:02:36 2004
From: jes at luretanker.no (Jon Egil Strand)
Date: Thu, 4 Nov 2004 18:02:36 +0100 (CET)
Subject: [R] Compilation error on mgcv_1.1-7 on OS X (10.3)
In-Reply-To: <Pine.LNX.4.44.0411011705360.16885-100000@gannet.stats>
Message-ID: <Pine.LNX.4.44.0411041754040.1974-100000@ani.mywh.net>


> I expect your version of R was not compiled on your own system, although 
> you don't say (please see the posting guide).  (I also presume this is a 
> Macintosh but you could have said.)

Prof Ripley, thank you for your answer. 

Indeed you are correct in that I was using precompiled R-2.0 on a 
Macintosh. 

> > I run into a compilation error when updating to mgcv_1.1-7 in R 2.0.0 on 
> > OS X 10.3. Note that other pacakges have compiled nicely. 
> 
> Packages containing Fortran source code?

Following your suggestion of building R from source I ran into problems 
due to a lack of a fortran compiler (or f2c). It turns out half a gigabyte 
of compiler tools from the Xcode package wasn't enough, Apple left out any 
fortran tools. Hmpf. I transformed the iBook to a pure gentoo linux 
machine, and everything compiles like a charm. 

All the best

Jon Egil Strand



From ggrothendieck at myway.com  Thu Nov  4 18:27:14 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 4 Nov 2004 17:27:14 +0000 (UTC)
Subject: [R] problems with seq.dates
References: <OF5D466B8E.42E4DD9F-ON80256F42.003FD80D-80256F42.00401EF4@StateStreet.com>
Message-ID: <loom.20041104T174642-615@post.gmane.org>

 <Ciprian_Marin <at> ssga.com> writes:

: 
: There seem to be a bug in the seq.dates function in the chron package for R
: 2.0. Please see below:
: 
: when the specified frequency is "months", seq.dates does not return the end
: of the specified interval all the time:
: 
: > seq.dates(from = "05/31/04", to = "12/31/04", by = "months")
: [1] 05/31/04 06/30/04 07/31/04 08/31/04 09/30/04 10/31/04 11/30/04
: 


This is a bug.  A workaround is to place the to= date after the 
last date you want in your sequence rather than right at it:

seq.dates(from = "05/31/04", to = chron("12/31/04")+1, by = "months")


You can fix your copy of seq.dates by inserting, after these 3 lines:

    if (by == "months") {
        nxt.day <- month.day.year(as.numeric(from + 1))$month
        end.of.the.month <- frm.mdy$month != nxt.day

the following line:

        if (end.of.the.month) x <- c(x, x[length(x)]+1)



From dmb at mrc-dunn.cam.ac.uk  Thu Nov  4 18:40:36 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Thu, 4 Nov 2004 17:40:36 +0000 (GMT)
Subject: [R] highly biased PCA data?
Message-ID: <Pine.LNX.4.21.0411041733420.29228-100000@mail.mrc-dunn.cam.ac.uk>


Hello, supposing that I have two or three clear categories for my data,
lets say pet preferece across fish, cat, dog. Lets say most people rate
their preference as being mostly one of the categories.

I want to do pca on the data to see three 'groups' of people, one group
for fish, one for cat and one for dog. I would like to see the odd person
who likes both or all three in the (appropriate) middle of the other main
groups.

Will my data be affected by the fact that I have interviewed 1000 dog
owners, 100 cat owners and 10 fish owners? (assuming that each scale of
preference has an equal range). 

Cheers,
dan.



From gunter.berton at gene.com  Thu Nov  4 19:08:38 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Thu, 4 Nov 2004 10:08:38 -0800
Subject: [R] highly biased PCA data?
In-Reply-To: <Pine.LNX.4.21.0411041733420.29228-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <200411041808.iA4I8coP005080@compton.gene.com>


Dan:


1) There is no guarantee that PCA will show separate groups, of course, as
that is not its purpose, although it is frequently a side effect.

2) If you were to use a classification method of some sort (discriminant
analysis, neural nets, SVM's, model=based classification,  ...), my
understanding is that yes, indeed, severely unbalanced group membership
would, indeed, affect results. A guess is that Bayesian or other methods
that could explicitly model the prior membership probabilities would do
better. To make it clear why, suppose that there was a 99.9% preference of
"dog" and .05% each of the others. Than your datasets would have almost no
information on how covariates could distinguish the classes and the best
classifier would be to call everything a "dog" no matter what values the
covariates had.

I presume experts will have more and better to say about this.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dan Bolser
> Sent: Thursday, November 04, 2004 9:41 AM
> To: R mailing list
> Subject: [R] highly biased PCA data?
> 
> 
> Hello, supposing that I have two or three clear categories 
> for my data,
> lets say pet preferece across fish, cat, dog. Lets say most 
> people rate
> their preference as being mostly one of the categories.
> 
> I want to do pca on the data to see three 'groups' of people, 
> one group
> for fish, one for cat and one for dog. I would like to see 
> the odd person
> who likes both or all three in the (appropriate) middle of 
> the other main
> groups.
> 
> Will my data be affected by the fact that I have interviewed 1000 dog
> owners, 100 cat owners and 10 fish owners? (assuming that 
> each scale of
> preference has an equal range). 
> 
> Cheers,
> dan.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ramasamy at cancer.org.uk  Thu Nov  4 19:22:43 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Thu, 04 Nov 2004 18:22:43 +0000
Subject: [R] list files ignoring the case option
In-Reply-To: <Pine.LNX.4.44.0411041503510.31970-100000@gannet.stats>
References: <Pine.LNX.4.44.0411041503510.31970-100000@gannet.stats>
Message-ID: <1099592562.3275.6.camel@ramasamy.stats>

Thanks to Sundar Dorai-Raj, Prof. Ripley and Berton Gunter for the
solution. I think I will take Prof. Ripley's suggestion and stick with
my initial solution for code readability but I am sure the regexp stuff
will come handy next time.

On Thu, 2004-11-04 at 15:10, Prof Brian Ripley wrote:
> On Thu, 4 Nov 2004, Adaikalavan Ramasamy wrote:
> 
> > Sorry if this is a question more on regular expressions. I am dealing
> > with several files which have been badly named. For example the files
> > are given either the extensions txt, TXT or Txt. I wish to select all
> > those files ending with 'txt' ignoring case.
> > 
> > Here is how I would do it in bash (Redhat FC2) :
> > 
> >    touch  a1.txt  a2.TXT  a3.Txt  txt.control  TXT.control
> >    ls -1 | grep -i "txt$"
> > 
> > 
> > Here is how I am currently doing it in R
> > 
> >    a <- list.files(all.files=T)
> >    grep( "txt$", a, ignore.case=T, value=T )
> 
> I'd write that in one line, but it seems as good a way as any.
> 
> > Is it possible for me to modify the following line to include ignore
> > case option ?
> > 
> >    a <- list.files( pattern="txt$" )
> 
> Not as such.
> 
> First, I think you want "\\.txt$" there if you do mean file extensions.
> 
> You can use a regexp that ignores case, though, e.g. "\\.[Tt]{Xx][Tt]".
> 
> But I would just use your original idea, which is essentially what ls() is 
> doing internally and is self-documenting.



From dmb at mrc-dunn.cam.ac.uk  Thu Nov  4 19:26:54 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Thu, 4 Nov 2004 18:26:54 +0000 (GMT)
Subject: [R] highly biased PCA data?
In-Reply-To: <200411041808.iA4I8coP005080@compton.gene.com>
Message-ID: <Pine.LNX.4.21.0411041821300.29228-100000@mail.mrc-dunn.cam.ac.uk>

On Thu, 4 Nov 2004, Berton Gunter wrote:

>
>Dan:
>
>
>1) There is no guarantee that PCA will show separate groups, of course, as
>that is not its purpose, although it is frequently a side effect.
>
>2) If you were to use a classification method of some sort (discriminant
>analysis, neural nets, SVM's, model=based classification,  ...), my
>understanding is that yes, indeed, severely unbalanced group membership
>would, indeed, affect results. A guess is that Bayesian or other methods
>that could explicitly model the prior membership probabilities would do
>better. To make it clear why, suppose that there was a 99.9% preference of
>"dog" and .05% each of the others. Than your datasets would have almost no
>information on how covariates could distinguish the classes and the best
>classifier would be to call everything a "dog" no matter what values the
>covariates had.
>
>I presume experts will have more and better to say about this.

Sounds interesting. Thanks very much for the input. Just out of curiosity,
given that I can make my data more uniform (less biased), how could I best
generate a 2d plot to encapsulate the clusters (and inter cluster
relationships)?

Actually I am thinking of a 2d density.


>
>-- Bert Gunter
>Genentech Non-Clinical Statistics
>South San Francisco, CA
> 
>"The business of the statistician is to catalyze the scientific learning
>process."  - George E. P. Box
> 
> 
>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch 
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dan Bolser
>> Sent: Thursday, November 04, 2004 9:41 AM
>> To: R mailing list
>> Subject: [R] highly biased PCA data?
>> 
>> 
>> Hello, supposing that I have two or three clear categories 
>> for my data,
>> lets say pet preferece across fish, cat, dog. Lets say most 
>> people rate
>> their preference as being mostly one of the categories.
>> 
>> I want to do pca on the data to see three 'groups' of people, 
>> one group
>> for fish, one for cat and one for dog. I would like to see 
>> the odd person
>> who likes both or all three in the (appropriate) middle of 
>> the other main
>> groups.
>> 
>> Will my data be affected by the fact that I have interviewed 1000 dog
>> owners, 100 cat owners and 10 fish owners? (assuming that 
>> each scale of
>> preference has an equal range). 
>> 
>> Cheers,
>> dan.
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>> 
>



From skaruri at cw.bc.ca  Thu Nov  4 19:27:43 2004
From: skaruri at cw.bc.ca (Karuri, Stella)
Date: Thu, 4 Nov 2004 10:27:43 -0800
Subject: [R] Recovering R Workspace
Message-ID: <521B3A42ED3A0C4C850B78314304A53D01EC1845@srvex02.phsabc.ehcnet.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041104/7c5c260b/attachment.pl

From ggrothendieck at myway.com  Thu Nov  4 19:33:40 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 4 Nov 2004 18:33:40 +0000 (UTC)
Subject: [R] highly biased PCA data?
References: <Pine.LNX.4.21.0411041733420.29228-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <loom.20041104T192601-608@post.gmane.org>

Dan Bolser <dmb <at> mrc-dunn.cam.ac.uk> writes:

: 
: Hello, supposing that I have two or three clear categories for my data,
: lets say pet preferece across fish, cat, dog. Lets say most people rate
: their preference as being mostly one of the categories.
: 
: I want to do pca on the data to see three 'groups' of people, one group
: for fish, one for cat and one for dog. I would like to see the odd person
: who likes both or all three in the (appropriate) middle of the other main
: groups.
: 
: Will my data be affected by the fact that I have interviewed 1000 dog
: owners, 100 cat owners and 10 fish owners? (assuming that each scale of
: preference has an equal range). 

This is not PCA but randomForest has facilities for handling
classifications where the number of points vary widely.  See the 
help for randomForest and the sampsize= argument, in particular.  
Also see R News 2/3 and http://www.stat.berkeley.edu/users/chenchao/666.pdf



From ligges at statistik.uni-dortmund.de  Thu Nov  4 19:54:40 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 04 Nov 2004 19:54:40 +0100
Subject: [R] automation / interactivity of plots on S-PLUS
In-Reply-To: <web-9923509@univ-paris7.fr>
References: <web-9923509@univ-paris7.fr>
Message-ID: <418A7AF0.709@statistik.uni-dortmund.de>

Please do NOT crosspost!
Please read the posting guide!
This list is intended for *R* related questions.

Uwe Ligges




COMBES Florence wrote:

> Dear all,
> 
> I have 2 questions maybe more for developpers, and about an S-PLUS script.
> I think that people from R and maybe Bioconductor could also help me.
> 
> 1--
> When producing a graph with S-PLUS (under Windows XP), it is possible to 
> select (with the mouse) an area of the graph in order to highlight the 
> rows corresponding to the selected plots on the dataset sheet.
> 
> In my case I have to plot about 100 graphs, so I wrote a script with a 
> loop to do that automatically. Each time you pass through this "for" 
> loop the graph is done.
> My problem is that with this way to build graphs, I lost the possiblity 
> to highlight data in the data set sheet by defining a zone with the 
> mouse on the graph sheet.
> (actually it is impossible to define a zone on the graph with the mouse)
> 
> Does anyone already encountered this problem, and would handle a solution ?
> 
> I saw maybe the brush() function to be applied on the loop, but this 
> seems to return only the row indices and I would like a variable more 
> informative that is on a column in my data set sheet.
> 
> 2--
> In my script I want to use the menuSubset() command. It works in command 
> line, but I would like this command to appear as a graphical window.
> I can have the window with the following command line:
> 
> guiDisplayDialog("Function", "menuSubset")
> 
> but the problem is that the script continues to run, and does not wait 
> for the user to click on the "OK" button in order to wait for the result 
> of the command Subset, which is a matrix needed further on the script.
> Do you know a way to pass arguments from the data filled in the window 
> to the script, or to force S-PLUs to wait ?
> 
> 
> Any help gratefully welcome,
> 
> Florence Combes.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Thu Nov  4 20:04:51 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 04 Nov 2004 20:04:51 +0100
Subject: [R] Labelling contour lines
In-Reply-To: <Pine.LNX.4.44.0411041222360.28041-100000@red00.iridis.soton.ac.uk>
References: <Pine.LNX.4.44.0411041222360.28041-100000@red00.iridis.soton.ac.uk>
Message-ID: <418A7D53.3030906@statistik.uni-dortmund.de>

Nathaniel Street wrote:

> I am using contour to draw contour lines onto a photo (jpeg) of a leaf on 
> a white background.
> 
> I have two problems:
> 
> 1) The contour gets plotted at right angles to the jpeg image. 

I guess you either haven't realized the ordering of the y axis or you 
have created the matrix column-wise instead of row-wise (or vice versa).
I can only guess, because there is no reproducible example...


 > I guess
> this is a problem of referencing the start and end points of the image 
> matrix but I can't see how to over come this other than loading in a 
> second image that has been rotated 90 degrees and plotting the contours on 
> the non-rotated one (obviously a bad solution)

> 2) I want to be able to select a single contour after they have been 
> drawn. I know there is contourLines but I can't see how to relate this to 
> the contours plotted. In matlab there is a function to label contours so 
> that they can be called by their contour label in order to get the 
> coordinates of the contour. Can this be done in R.

Not really, what you can to is (extended from ?contour):


  x <- -6:16
  contour(outer(x, x), method = "edge", vfont = c("sans serif", "plain"))
  cL <- contourLines(outer(x, x), levels=75)
  lines(cL[[1]], col="red")

Now you have the contour lines coordinates for level 75 in cL, an 
plotted it afterwards.

Uwe Ligges


> Sorry if these are easy / stupid questions. I'm new to all this. I have 
> searched previos posts but can't find the answers I need and I would 
> rather be able to use R than matlab.
> 
> Thanks
>



From nair at sdsc.edu  Thu Nov  4 21:44:09 2004
From: nair at sdsc.edu (T. Murlidharan Nair)
Date: Thu, 04 Nov 2004 12:44:09 -0800
Subject: [R] biplot drawing conc ellipses
Message-ID: <418A9499.5010907@sdsc.edu>

Is there an option to draw concentration ellipses in biplots ? It seems
really nice to summarize large number of points of each group.

Cheers../ Murli



From tshort at epri-peac.com  Thu Nov  4 22:37:17 2004
From: tshort at epri-peac.com (Tom Short)
Date: Thu, 04 Nov 2004 16:37:17 -0500
Subject: [R] [ANN] a new R reference card available
Message-ID: <cme74t$5g9$1@sea.gmane.org>

The first release of a new quick reference card for R is available at:

   http://www.rpad.org/Rpad/R-refcard.pdf

This is a four-page reference listing common R functions with short
summaries. The reference card contains considerable material from Emmanuel
Paradis's excellent "R for Beginners"
(http://cran.r-project.org/doc/contrib/rdebuts_en.pdf). Thank you Emmanuel 
for your help on this.

Please send me comments and corrections.

Latex sources for this reference card are also available at: 
http://www.rpad.org/Rpad/R-refcard-sources.zip
Feel free to make customized versions of this with additions based on your 
favorite packages.

There's also a reference card for both R and Rpad. This was released as a 
part of the latest release of Rpad (2004-11-02), an interactive, web-based 
analysis interface to R. For more information, see http://www.rpad.org

- Tom

-- 
Tom Short
EPRI PEAC, www.epri-peac.com



From petzoldt at rcs.urz.tu-dresden.de  Thu Nov  4 23:03:30 2004
From: petzoldt at rcs.urz.tu-dresden.de (Thomas Petzoldt)
Date: Thu, 04 Nov 2004 23:03:30 +0100
Subject: [R] decision about random effects in lme
Message-ID: <418AA732.3080100@rcs.urz.tu-dresden.de>

Hello,

in an experimental field study my collegue made a design with samples on 
two manipulated sampling sites (site: control, treatment). Within each 
site she sampled 3 traps (trap)  at day and night (light: light, dark) 
at 3 consecutive days (day).

We applied lme models with abundance as response variable, site * light 
as fixed effects and day and trap as random effects.

I assumed, the following model may be adequate:

m1 <- lme(ab ~ site * light, data = dat,
           random = ~1|site/day/trap, method="ML")

or alternatively:

m2 <- update(m1, random = ~1|site/trap)

and I get a significant interaction effect, but (as expected) NaN for 
site as there are not enough df. With several alternative assumptions 
about random effects I get both, the significant interaction and an 
effect of site, but m1 is remains the "best" model measured by AIC and BIC.

If I however simplify down to a linear model without random effects

m3 <- lm(ab ~ site * light, data=dat)

the models m1 and m3 are "not very different" (AIC, BIC, p-value):

 > anova(m1, m2, m3)
    Model df       AIC      BIC    logLik   Test  L.Ratio p-value
m1     1  8  96.54522 111.5148 -40.27261
m2     2  7 100.42958 113.5280 -43.21479 1 vs 2 5.884358  0.0153
m3     3  5  98.05421 107.4102 -44.02711 2 vs 3 1.624633  0.4438

and with m3 I get a very strong effect of site and also the interaction 
effect. Both, site and interaction effects are plausible if plotted with 
bwplot, but I am still confused, whether one of these two is a good 
model, and how to decide this.

Please help me

Thomas P.



From pierre.bady at univ-lyon1.fr  Fri Nov  5 00:15:02 2004
From: pierre.bady at univ-lyon1.fr (Pierre BADY)
Date: Fri, 05 Nov 2004 00:15:02 +0100 (CET)
Subject: [R] biplot drawing conc ellipses
In-Reply-To: <418A9499.5010907@sdsc.edu>
References: <418A9499.5010907@sdsc.edu>
Message-ID: <1099610102.418ab7f6cadd0@webmail.univ-lyon1.fr>

Hi all,

you can see in the library(ade4).
there is  a nice graphical function called &#8216;s.class&#8217;.

Example for a principal components analysis:
?s.class

data(meaudret)
pca1 <- dudi.pca(meaudret$mil, scan = FALSE, nf = 4)
# plot row coordinates 
s.class(pca1$li, meaudret$plan$dat, sub = "Principal Component Analysis",add.
p=F,col=c(1,2,3,4),cstar=0)
# add principal axes
s.arrow(pca1$c1,clabel=0.7,add.p=T)


# &#8220;&#8230;  'scatter.dudi' is a factorial map of individuals and the
# projection of the vectors of the canonical basis multiplied by a
# constante of rescaling. &#8230;&#8221;

?scatter.dudi
scatter.dudi(pca1,clab.row=0)
s.class(pca1$li, meaudret$plan$dat, sub = "Principal Component Analysis",add.
p=T,col=c(1,2,3,4), cstar=0)

hope this helps,


P.BADY

Quoting "T. Murlidharan Nair" <nair at sdsc.edu>:

> Is there an option to draw concentration ellipses in biplots ? It
> seems
> really nice to summarize large number of points of each group.
> 
> Cheers../ Murli
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 



---------------------------------------------------------
Pierre BADY     <??)))><
Universit?? Claude Bernard Lyon 1
UMR CNRS 5023, LEHF
bat Alphonse Forel
43 boulevard du 11 novembre 1918 
F-69622 VILLEURBANNE CEDEX 
FRANCE
TEL : +33 (0)4 72 44 62 34 
FAX : +33 (0)4 72 43 28 92 
MEL : pierre.bady at univ-lyon1.fr
http://limnologie.univ-lyon1.fr



From gerifalte28 at hotmail.com  Fri Nov  5 01:01:41 2004
From: gerifalte28 at hotmail.com (F Z)
Date: Fri, 05 Nov 2004 00:01:41 +0000
Subject: [R] Legend placement in barplot?
Message-ID: <BAY2-F221s7nlT2NqFJ00020370@hotmail.com>

You can use locator()  nested within legend()
i.e.

plot(YourVariable)
legend(locator(1),legend="Your Legend")

Once you call this command it will display "Your Legend" in the place where 
you left clicked your mouse.  Beware that, as described in the documentation 
"...'locator' is only supported on screen devices such as 'X11','windows' 
and 'quartz'.  On other devices the call will do nothing"


Altrenativelly you can pass the exact coordinates of the position where you 
want the legend, instead of using locator
i.e.

plot(YourVariable)
x<-list(x=-91.76781, y=46.87375)
legend(x,legend="Your Legend")

I hope that this helps

Francisco

>From: Gabor Grothendieck <ggrothendieck at myway.com>
>To: r-help at stat.math.ethz.ch
>Subject: Re: [R] Legend placement in barplot?
>Date: Wed, 3 Nov 2004 18:48:48 +0000 (UTC)
>
>Dan Bolser <dmb <at> mrc-dunn.cam.ac.uk> writes:
>
>:
>: This has been asked before, but all the answers are hidiously complex.
>:
>: The
>:
>: legend.text=TRUE
>:
>: option of barplot is almost exactly what I need, except I need a
>:
>: legend.placement='tl'
>:
>: (top left) option. This option would be in contrast to the default
>: placement which we could call 'tr' (top right).
>:
>: Anyone know how to edit the barplot code to make this change? Could
>: someone like me work out how to do this?
>:
>: Cheers,
>: Dan.
>
>
>Check out gplots::smartlegend (in the R 2.0.0 gregmisc bundle).
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From dmb at mrc-dunn.cam.ac.uk  Fri Nov  5 01:27:19 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Fri, 5 Nov 2004 00:27:19 +0000 (GMT)
Subject: [R] Legend placement in barplot?
In-Reply-To: <BAY2-F221s7nlT2NqFJ00020370@hotmail.com>
Message-ID: <Pine.LNX.4.21.0411050018010.7322-100000@mail.mrc-dunn.cam.ac.uk>

On Fri, 5 Nov 2004, F Z wrote:

>You can use locator()  nested within legend()
>i.e.
>
>plot(YourVariable)
>legend(locator(1),legend="Your Legend")
>
>Once you call this command it will display "Your Legend" in the place where 
>you left clicked your mouse.  Beware that, as described in the documentation 
>"...'locator' is only supported on screen devices such as 'X11','windows' 
>and 'quartz'.  On other devices the call will do nothing"
>
>
>Altrenativelly you can pass the exact coordinates of the position where you 
>want the legend, instead of using locator
>i.e.
>
>plot(YourVariable)
>x<-list(x=-91.76781, y=46.87375)
>legend(x,legend="Your Legend")
>
>I hope that this helps
>
>Francisco

Thanks very much for the tips. Basically I want a very flexible solution
that lets me punch the numbers in - take a look at the result and then
immediatly dump a .ps / .eps / .png format of what I saw. The result
should be very quickly 'publication quality' (whatever that is). 

The problem with locator is that I don't know how to make it work with
postscript, and I don't want to find out. I don't want to have to probe my
figure for the coordinates every time I change the data in my figure. I am
happy saying something like 'oh, top left is bad, lets use top right' -
done.

smartlegend is almost there, I just think barplot should support exactly
the same functionality as smartlegend. This would save me the hassle of
creating a new legend every time my data changes, matching up colors and
names.

I am sure their is a way to code this, but I don't want to write code - at
least not code that I have to look at when what I want to see is my data. 


>
>>From: Gabor Grothendieck <ggrothendieck at myway.com>
>>To: r-help at stat.math.ethz.ch
>>Subject: Re: [R] Legend placement in barplot?
>>Date: Wed, 3 Nov 2004 18:48:48 +0000 (UTC)
>>
>>Dan Bolser <dmb <at> mrc-dunn.cam.ac.uk> writes:
>>
>>:
>>: This has been asked before, but all the answers are hidiously complex.
>>:
>>: The
>>:
>>: legend.text=TRUE
>>:
>>: option of barplot is almost exactly what I need, except I need a
>>:
>>: legend.placement='tl'
>>:
>>: (top left) option. This option would be in contrast to the default
>>: placement which we could call 'tr' (top right).
>>:
>>: Anyone know how to edit the barplot code to make this change? Could
>>: someone like me work out how to do this?
>>:
>>: Cheers,
>>: Dan.
>>
>>
>>Check out gplots::smartlegend (in the R 2.0.0 gregmisc bundle).
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Fri Nov  5 01:53:59 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 4 Nov 2004 19:53:59 -0500
Subject: [R] highly biased PCA data?
Message-ID: <3A822319EB35174CA3714066D590DCD50994E282@usrymx25.merck.com>

I am no expert on this sort of matters, but that has never stopped me from
tossing in my $0.02...

As Gabor and Bert hinted, this is what I would try:

Run randomForest on the data, using sampsize=c(10, 10, 10) and
importance=TRUE, for example.  Then take the few most important variables
with respect to each class and maybe do PCA on those to see if you can see
separation.

HTH,
Andy

> From: Dan Bolser
> 
> On Thu, 4 Nov 2004, Berton Gunter wrote:
> 
> >
> >Dan:
> >
> >
> >1) There is no guarantee that PCA will show separate groups, 
> of course, as
> >that is not its purpose, although it is frequently a side effect.
> >
> >2) If you were to use a classification method of some sort 
> (discriminant
> >analysis, neural nets, SVM's, model=based classification,  ...), my
> >understanding is that yes, indeed, severely unbalanced group 
> membership
> >would, indeed, affect results. A guess is that Bayesian or 
> other methods
> >that could explicitly model the prior membership 
> probabilities would do
> >better. To make it clear why, suppose that there was a 99.9% 
> preference of
> >"dog" and .05% each of the others. Than your datasets would 
> have almost no
> >information on how covariates could distinguish the classes 
> and the best
> >classifier would be to call everything a "dog" no matter 
> what values the
> >covariates had.
> >
> >I presume experts will have more and better to say about this.
> 
> Sounds interesting. Thanks very much for the input. Just out 
> of curiosity,
> given that I can make my data more uniform (less biased), how 
> could I best
> generate a 2d plot to encapsulate the clusters (and inter cluster
> relationships)?
> 
> Actually I am thinking of a 2d density.
> 
> 
> >
> >-- Bert Gunter
> >Genentech Non-Clinical Statistics
> >South San Francisco, CA
> > 
> >"The business of the statistician is to catalyze the 
> scientific learning
> >process."  - George E. P. Box
> > 
> > 
> >
> >> -----Original Message-----
> >> From: r-help-bounces at stat.math.ethz.ch 
> >> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dan Bolser
> >> Sent: Thursday, November 04, 2004 9:41 AM
> >> To: R mailing list
> >> Subject: [R] highly biased PCA data?
> >> 
> >> 
> >> Hello, supposing that I have two or three clear categories 
> >> for my data,
> >> lets say pet preferece across fish, cat, dog. Lets say most 
> >> people rate
> >> their preference as being mostly one of the categories.
> >> 
> >> I want to do pca on the data to see three 'groups' of people, 
> >> one group
> >> for fish, one for cat and one for dog. I would like to see 
> >> the odd person
> >> who likes both or all three in the (appropriate) middle of 
> >> the other main
> >> groups.
> >> 
> >> Will my data be affected by the fact that I have 
> interviewed 1000 dog
> >> owners, 100 cat owners and 10 fish owners? (assuming that 
> >> each scale of
> >> preference has an equal range). 
> >> 
> >> Cheers,
> >> dan.
> >> 
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide! 
> >> http://www.R-project.org/posting-guide.html
> >> 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From lederer at trium.de  Fri Nov  5 04:53:36 2004
From: lederer at trium.de (lederer@trium.de)
Date: Fri, 5 Nov 2004 04:53:36 +0100 (CET)
Subject: [R] Covariance bug in R-1.8.0
Message-ID: <46334.217.229.7.13.1099626816.squirrel@217.229.7.13>

R-1.8.0 seems to calculate wrong covariances, when the argument of cov()
is a matrix or a data frame.
The following should produce a matrix of zeroes and NaNs:

x <- matrix(c(NA ,NA ,0.9068995 ,NA ,-0.3116229,
              -0.06011117 ,0.7310134 ,NA ,1.738362 ,0.6276125,
              0.6615581 ,NA ,NA ,-2.646011 ,-2.126105,
              NA ,1.081825 ,NA ,1.253795 ,1.520708,
              0.2822814 ,NA ,NA ,NA ,NA,
              0.03291028 ,NA ,NA ,NA ,NA,
              NA ,NA ,NA ,-0.5462126 ,-0.1997394,
              NA ,-0.3419413 ,-0.2675226 ,-1.000133 ,-0.1346234,
              NA ,NA ,-0.411743 ,1.301612 ,NA,
              0.922197 ,NA ,0.9513522 ,0.2357021 ,NA),
            nrow=10, ncol=5)

c1 <- cov(x, use="pairwise.complete")

c2 <- matrix(nrow=5, ncol=5)
for (i in 1:5)
{
    for (j in 1:5)
    {
        c2[i,j] <- cov(x[,i], x[,j], use="pairwise.complete")
    }
}

c2-c1

Instead, R-1.8.0 produces this result:

            [,1]        [,2]       [,3]          [,4]        [,5]
[1,]  0.00000000 -0.03053828         NA -0.0144996353 -0.03485883
[2,] -0.03053828 -0.01649857         NA  0.0137259383 -0.02960707
[3,]          NA          NA -0.1296134            NA          NA
[4,] -0.01449964  0.01372594         NA -0.0003152629  0.08717648
[5,] -0.03485883 -0.02960707         NA  0.0871764791  0.04961190

This happens as well under Linux (Suse 9.1) as well as under Windows NT.

Under 1.9.1 (Linux) and 1.9.0 (Windows) i get the expected matrix of
zeroes and NaNs.

This example is not very special. Under R-1.8.0 cov produced wrong result
for any random matrix i tried.

Doesn't this mean, that *any* result obtained under R 1.8.0 is unreliable?

By the way, i just recompiled R-1.8.0 from source under Linux and tried
'make check'. All tests were ok.
Does there exist a more detailed set of tests, which could insure that
at least the most basic R functions work correctly?


Christian



From mpiktas at gmail.com  Fri Nov  5 08:19:51 2004
From: mpiktas at gmail.com (Vaidotas Zemlys)
Date: Fri, 5 Nov 2004 09:19:51 +0200
Subject: [R] Using East-european characters in R
In-Reply-To: <004201c4c0eb$fb796120$0d09f9c2@ales>
References: <004201c4c0eb$fb796120$0d09f9c2@ales>
Message-ID: <e4780832041104231913b97675@mail.gmail.com>

Hi,

On Tue, 2 Nov 2004 15:54:30 +0100, Ale? ?iberna
<ales.ziberna at guest.arnes.si> wrote:
> Does anybody know how to produce a East-European character "?" - "c with a
> v-shaped hat " in R (in text or plot)?
> 
> I do know how to get "?,?" - "s,z, with a v-shaped hat", but not this one.
> 
> Thanks in advance for any suggestions,
> 
> Ales Ziberna
> 
> P.S.: I am using Windows XP and R version 1.9.1 (details below)
> 

If you want to produce postscript you can set the encoding. This is an
ilustration how one can get all the lithuanian letters (??? are among
them). Source this code and run TestChars()

TestChars <- function(encoding="latin7x", family="URWHelvetica")     {
         postscript(encoding=encoding,family=family)
         par(pty="s")
         plot(c(-1,-1), c(-1,-1),
xlim=c(0,8),ylim=c(0,32),xlab="?????????", ylab="?????????")
         title(paste("Lietuvi?ka simboli? koduot?", encoding))
        # grid(255, 255, lty=1)
         x <- rep(1:8,each=32)
         y <- rep(32:1,8)
         for(i in c(1:256)) {
             points(x[i], y[i], pch=i-1)
         }
         dev.off()
}

The encoding file used is attached. You can get the one you need from
TeX instalations, look at dvips folder in main texmf tree. You must
copy it to afm folder in main R instalation tree.

Caveats:
1.  You must use the text editor which supports different encodings.
The title and axis labels of the example plot will be displayed
correctly if the function is sourced in iso8859-13 encoding.

2. The font used must have the characters (glyphs) described in
encoding. That's why I used URWHelvetica, it is provided with default 
instalation of R.

PS I do not know how gmail handles the encodings, so the text maybe
mangled, also R-help list may cut my attachment. Mail me privately
then.

From schouwla at yahoo.com  Fri Nov  5 09:31:59 2004
From: schouwla at yahoo.com (Lars Schouw)
Date: Fri, 5 Nov 2004 00:31:59 -0800 (PST)
Subject: [R] Building my own R package under Windows
Message-ID: <20041105083159.55224.qmail@web50310.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041105/cb6cb369/attachment.pl

From p.dalgaard at biostat.ku.dk  Fri Nov  5 09:32:45 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 05 Nov 2004 09:32:45 +0100
Subject: [R] Covariance bug in R-1.8.0
In-Reply-To: <46334.217.229.7.13.1099626816.squirrel@217.229.7.13>
References: <46334.217.229.7.13.1099626816.squirrel@217.229.7.13>
Message-ID: <x2ekj8itle.fsf@biostat.ku.dk>

lederer at trium.de writes:

> R-1.8.0 seems to calculate wrong covariances, when the argument of cov()
> is a matrix or a data frame.
> The following should produce a matrix of zeroes and NaNs:
...
> Under 1.9.1 (Linux) and 1.9.0 (Windows) i get the expected matrix of
> zeroes and NaNs.
> 
> This example is not very special. Under R-1.8.0 cov produced wrong result
> for any random matrix i tried.

Presumably, this is the same as PR#4646. 

> Doesn't this mean, that *any* result obtained under R 1.8.0 is unreliable?

It means that covariances and correlations are sometimes computed
incorrectly.

> By the way, i just recompiled R-1.8.0 from source under Linux and tried
> 'make check'. All tests were ok.

Yes. We don't release versions that don't pass their own tests.

> Does there exist a more detailed set of tests, which could insure that
> at least the most basic R functions work correctly?

We add regression tests as we discover and fix bugs. We can't fix old
versions retroactively though, we release patch versions (e.g. 1.8.1)
instead.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From bhx2 at mevik.net  Fri Nov  5 09:40:27 2004
From: bhx2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Fri, 05 Nov 2004 09:40:27 +0100
Subject: [R] Recovering R Workspace
In-Reply-To: <521B3A42ED3A0C4C850B78314304A53D01EC1845@srvex02.phsabc.ehcnet.ca>
	(Stella Karuri's message of "Thu, 4 Nov 2004 10:27:43 -0800")
References: <521B3A42ED3A0C4C850B78314304A53D01EC1845@srvex02.phsabc.ehcnet.ca>
Message-ID: <m0wtx0it8k.fsf@bar.nemo-project.org>

You probably don't need to re-install R; just remove or rename the
file .RData (it is probably located in your home directory (or "My
Documents" on MSWin).  Then R should start without problems.

As for recovering the workspace, I believ that is a lost cause (unless
you study the file format and use a binary editor to extract/repair
objects in the file -- and even then, if the file was compressed, you
may be out of luck).

-- 
Bj??rn-Helge Mevik



From ripley at stats.ox.ac.uk  Fri Nov  5 09:49:29 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 5 Nov 2004 08:49:29 +0000 (GMT)
Subject: [R] Building my own R package under Windows
In-Reply-To: <20041105083159.55224.qmail@web50310.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0411050845570.12470-100000@gannet.stats>

On Fri, 5 Nov 2004, Lars Schouw wrote:

> Hi

>  Is there any samples anywhere that shows how to build my own R package
> for Windows?

>  I read that package manual and well as the FAQ but would prefer to have
> an example as well.
>  
> I know I need perl and mingw. 

1) Read README.packages, carefully.

2) Apply what you read to the sample package `windlgs' that ships with the
source-package files component of the install.  E.g.

	Rcmd INSTALL /path/to/windlgs

Or by `build' did you mean `write'?  (If so there are 412 example packages
on CRAN.)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From silviakirkman at yahoo.com  Fri Nov  5 10:29:00 2004
From: silviakirkman at yahoo.com (Silvia Kirkman)
Date: Fri, 5 Nov 2004 01:29:00 -0800 (PST)
Subject: [R] bootstrap query
Message-ID: <20041105092900.20838.qmail@web50507.mail.yahoo.com>

Hi

I need to bootstrap a function in R and I am
struggling. Can anyone help? The following explains
what Im trying to do:

I have 2 different matrices, called "x" and "y". Each
has 34 columns, and the length of each column varies. 

I use this data to determine a certain measure (C),
which Ive calculated in R as follow:

> schoener<-function(x,y,z)
+ {
+ 
+ # x - seals
+ # y - fishery
+ # z - column of matrix
+ 
+  breaks<-c(0:66)/2
+  hseal<-hist(na.omit(x[,z]), breaks = breaks, freq =
FALSE, include.lowest = FALSE, right = FALSE, plot =
FALSE)
+  hfish<-hist(na.omit(y[,z]), breaks = breaks, freq =
FALSE, include.lowest = FALSE, right = FALSE, plot =
FALSE)
+  lseal<-length(na.omit(x[,z]))
+  lfish<-length(na.omit(y[,z]))
+  pseal<-(hseal$counts)/lseal
+  pfish<-(hfish$counts)/lfish
+  C<-(1-sum(abs(pseal-pfish))/2)*100
+  C
+ }

Ive also managed to resample (with replacement) the
data in each column of x and y as follows, to give me
new C values:

>resample<-function(x,y,z)
{
# x - seals
# y - fishery
# z - column of matrix

lseal<-length(na.omit(x[,z]))
lfish<-length(na.omit(y[,z]))
resampleseal<-sample(na.omit(x[,z]), lseal, replace =
TRUE)
resamplefish<-sample(na.omit(y[,z]), lfish, replace =
TRUE)

breaks<-c(0:66)/2
hseal<-hist(resampleseal, breaks = breaks, freq =
FALSE, include.lowest = FALSE, right = FALSE, plot =
FALSE)
hfish<-hist(resamplefish, breaks = breaks, freq =
FALSE, include.lowest = FALSE, right = FALSE, plot =
FALSE)
pseal<-(hseal$counts)/lseal
pfish<-(hfish$counts)/lfish
(1-sum(abs(pseal-pfish))/2)*100
}

What I want to be able to do is to obtain 10 000 C
values so that I can get the 95% confidence limits. In
other words, resample 10 000 times. I have tried to
use the "boot" function in R, but I just cant get it
right: 

boot(data, statistic, R, sim="ordinary", stype="i", 
     strata=rep(1,n), L=NULL, m=0, weights=NULL, 
     ran.gen=function(d, p) d, mle=NULL, ...)

According to above, "statistic=resample" (as Ive
defined above), "R=10000", and "data" would be x and
y. Im obviously not understanding something,
especially how to refer to x and y for "data". Im
sure it must be quite simple what I want to do - I
wonder if anyone out there can explain it to me.

Many thanks.

Silvia



From puetz at mpipsykl.mpg.de  Fri Nov  5 11:34:13 2004
From: puetz at mpipsykl.mpg.de (=?ISO-8859-1?Q?Benno_P=FCtz?=)
Date: Fri, 05 Nov 2004 11:34:13 +0100
Subject: [R] Sweave and transparent PDF?
Message-ID: <418B5725.3010604@mpipsykl.mpg.de>

Hello,

I have run across the following problem:

Creating PDF files manually by using

pdf(version='1.4')

I can make graphs using the new transparency feature of R2.0. If, 
however, I try to create the same graphs with Sweave, all transparent 
stuff is gone. This os likely due to the default version parameter in in 
pdf().

How can I tell Sweave to use the newer PDF version? The parameter seems 
to be hard-coded (not via some option) and I could not come up with some 
kind of hook ...

Thank you

   Benno

PS: Sorry for re-posting, yesterday I seem to have relied to another 
posting which may have buried mine in some readers



From tom_woody at web.de  Fri Nov  5 11:54:41 2004
From: tom_woody at web.de (=?ISO-8859-15?Q?Thomas_Sch=F6nhoff?=)
Date: Fri, 05 Nov 2004 11:54:41 +0100
Subject: [R] Error message from vignette strucchange-intro example
Message-ID: <418B5BF1.3080508@web.de>

Hello,

I am just studying the following example from vignette: 
strucchange-intro,

contineousely ending up in an error.


This is the given code:

1. library(strucchange)
2. data(USIncExp)
3. if (!"package:stats" %in% search()) library(ts)
4. USIncExp2 <- window(USIncExp, start = c(1985, 12))


A.Modelling:


coint.res <- residuals(lm(expenditure ~ income, data = USIncExp2))
coint.res <- lag(ts(coint.res, start = c(1985, 12), freq = 12),k = -1)
USIncExp2 <- cbind(USIncExp2, diff(USIncExp2), coint.res)
USIncExp2 <- window(USIncExp2, start = c(1986, 1), end = c(2001,2))

#here is what eval is looking for in vain: diff.expenditure
colnames(USIncExp2) <- 
c("income","expenditure","diff.income","diff.expenditure","coint.res")

B. Error correction formula

ecm.model <- diff.expenditure ~ coint.res + diff.income


C. Using EFP (OLS) test function

ocus <- efp(ecm.model, type = "OLS-CUSUM", data = USIncExp2)

or

me <- efp(ecm.model, type = "ME", data = USIncExp2, h = 0.2)


Both commads under C give me :

-------------------------------------------------------------------
Error in eval(expr, envir, enclos) : Object "diff.expenditure" not found
-------------------------------------------------------------------


Since quite some time I am sitting here and wonder whether there is an 
error in the given code or if it my simple inability to catch up with 
a self-breed error (???)


----------------------------------------------
platform i386-pc-linux-gnu
arch     i386
os       linux-gnu
system   i386, linux-gnu
status
major    2
minor    0.0
year     2004
month    10
day      04
language R
------------------------------------------------



Maybe someone to direct my delusion to something helpful, so far I've 
looked for typos and alike, as far I can say the code should work, 
isn't it?

Thomas



From ahenningsen at email.uni-kiel.de  Fri Nov  5 11:56:02 2004
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Fri, 5 Nov 2004 11:56:02 +0100
Subject: [R] Creating .Rout.save files for package subdirectory "tests"
Message-ID: <200411051156.02229.ahenningsen@email.uni-kiel.de>

Hi,

I added the "tests" subdirectory and a test file (say "myTest.R") to our 
"systemfit" package. Up to now I create the "myTest.Rout.save" file with 
> R CMD BATCH --vanilla myTest.R myTest.Rout.save

However, "R CMD check" reports two differences between myTest.Rout.save and 
the output of myTest.R:
a) myTest.Rout.save contains following extra line at the beginning of the 
file:
> invisible(options(echo = TRUE))
b) myTest.Rout.save contains the following 2 extra lines at the very end of 
the file:
> proc.time()
[1] 1.80 0.07 2.00 0.01 0.00

Everytime I change myTest.R and create a new myTest.R.save file I have to 
delete these 3 lines by hand. I could do this e.g. by a script using "sed", 
but I wonder if I can start "R CMD BATCH" with an option that suppresses any 
commands that are not in the input file, i.e. "invisible(options(echo = 
TRUE))" and "proc.time()".
Or what is the easiest way to create .Rout.save files?
(I am using R 2.0.0 on a i686 PC with SuSE Linux 9.0)

Thanks,
Arne

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From ramasamy at cancer.org.uk  Fri Nov  5 12:07:54 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Fri, 05 Nov 2004 11:07:54 +0000
Subject: [R] calling a var by name in another var
In-Reply-To: <loom.20041104T144526-955@post.gmane.org>
References: <loom.20041104T144526-955@post.gmane.org>
Message-ID: <1099652874.3133.2.camel@ndmpc126.ihs.ox.ac.uk>

get(myvar1) + get(myvar2)
[1] 295 411 230


On Thu, 2004-11-04 at 13:58, Oskar Villani wrote:
> Hello list,
> 
> I'd like to use a variable (or a column of a data frame) by using its name as a
> string. E.g.:
> 
> Data2003 <- c(150,200,120)
> Data2004 <- c(145,211,110)
> 
> myvar1 <- "Data2003"
> myvar2 <- "Data2004"
> 
> # now I'd like do do this
> 
> total <- Data2003 + Data2004
> 
> # in any way like
> # total <- ???(myvar1, myvar2)
> # or
> # total <- ???(myvar1) + ???(myvar2)
> # or something like that
> 
> Is there a possibility to do this in R - can't find a solution!
> 
> Thanks a lot
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ottorino-luca.pantani at unifi.it  Fri Nov  5 12:05:41 2004
From: ottorino-luca.pantani at unifi.it (8rino-Luca Pantani)
Date: Fri, 05 Nov 2004 12:05:41 +0100
Subject: [R] I: a coloured band within each panel of a lattice bwplot
Message-ID: <DNEELNJCLGBOLHCFLMHBMENMCHAA.OLPantani@unifi.it>

Hi all,
I would like to add to each panel of a bwplot a coloured central band,
centered on the mean of the values, being its width +- 2% of the mean
itself.

I know how to add lines, i.e. something like

bwplot(X ~ Y|FACTOR
       data=my.df,
       panel= function(x, y){
         panel.bwplot(x, y)
         panel.abline(v = mean(x, na.rm = T) - mean(x, na.rm = T) * 0.02
         panel.abline(v = mean(x, na.rm = T) + mean(x, na.rm = T) * 0.02
       })

but I cannot figure out how to shade the region included between the two
lines.

Is it possible, or I'm pretending too much?

Thanks to all.

Ottorino-Luca Pantani, Universit?? di Firenze
Dip. Scienza del Suolo e Nutrizione della Pianta



From maechler at stat.math.ethz.ch  Fri Nov  5 12:17:57 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 5 Nov 2004 12:17:57 +0100
Subject: [R] Creating .Rout.save files for package subdirectory "tests"
In-Reply-To: <200411051156.02229.ahenningsen@email.uni-kiel.de>
References: <200411051156.02229.ahenningsen@email.uni-kiel.de>
Message-ID: <16779.24933.911161.653300@gargle.gargle.HOWL>

>>>>> "Arne" == Arne Henningsen <ahenningsen at email.uni-kiel.de>
>>>>>     on Fri, 5 Nov 2004 11:56:02 +0100 writes:

    Arne> Hi, I added the "tests" subdirectory and a test file
    Arne> (say "myTest.R") to our "systemfit" package. Up to now
    Arne> I create the "myTest.Rout.save" file with
    >> R CMD BATCH --vanilla myTest.R myTest.Rout.save

that explains everything.  
Why do you do so?

Instead: 

1) Start with no '.Rout.save'

2) R CMD check  <pkg>
   will produce one in  <pkg>.Rcheck/tests/myTest.Rout
   Copy it to the package source, i.e. typically

   cp <pkg>.Rcheck/tests/myTest.Rout <pkg>/tests/myTest.Rout.save

3) R CMD check  <pkg>  
   now *will* do the comparison and give no difference hopefully

Martin Maechler, ETH Zurich


    Arne> However, "R CMD check" reports two differences between
    Arne> myTest.Rout.save and the output of myTest.R: a)
    Arne> myTest.Rout.save contains following extra line at the
    Arne> beginning of the file:
    >> invisible(options(echo = TRUE))
    Arne> b) myTest.Rout.save contains the following 2 extra
    Arne> lines at the very end of the file:
    >> proc.time()
    Arne> [1] 1.80 0.07 2.00 0.01 0.00

    Arne> Everytime I change myTest.R and create a new
    Arne> myTest.R.save file I have to delete these 3 lines by
    Arne> hand. I could do this e.g. by a script using "sed",
    Arne> but I wonder if I can start "R CMD BATCH" with an
    Arne> option that suppresses any commands that are not in
    Arne> the input file, i.e. "invisible(options(echo = TRUE))"
    Arne> and "proc.time()".  Or what is the easiest way to
    Arne> create .Rout.save files?  (I am using R 2.0.0 on a
    Arne> i686 PC with SuSE Linux 9.0)

    Arne> Thanks, Arne



From ripley at stats.ox.ac.uk  Fri Nov  5 12:22:50 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 5 Nov 2004 11:22:50 +0000 (GMT)
Subject: [R] Creating .Rout.save files for package subdirectory "tests"
In-Reply-To: <200411051156.02229.ahenningsen@email.uni-kiel.de>
Message-ID: <Pine.LNX.4.44.0411051118001.523-100000@gannet.stats>

On Fri, 5 Nov 2004, Arne Henningsen wrote:

> I added the "tests" subdirectory and a test file (say "myTest.R") to our 
> "systemfit" package. Up to now I create the "myTest.Rout.save" file with 
> > R CMD BATCH --vanilla myTest.R myTest.Rout.save
> 
> However, "R CMD check" reports two differences between myTest.Rout.save and 
> the output of myTest.R:
> a) myTest.Rout.save contains following extra line at the beginning of the 
> file:
> > invisible(options(echo = TRUE))
> b) myTest.Rout.save contains the following 2 extra lines at the very end of 
> the file:
> > proc.time()
> [1] 1.80 0.07 2.00 0.01 0.00
> 
> Everytime I change myTest.R and create a new myTest.R.save file I have to 
> delete these 3 lines by hand. I could do this e.g. by a script using "sed", 
> but I wonder if I can start "R CMD BATCH" with an option that suppresses any 
> commands that are not in the input file, i.e. "invisible(options(echo = 
> TRUE))" and "proc.time()".

This is not what R CMD BATCH is intended for, so on such option exists.

> Or what is the easiest way to create .Rout.save files?
> (I am using R 2.0.0 on a i686 PC with SuSE Linux 9.0)

1) Run R CMD check and copy myTest.Rout[.fail] to 
   pkg-name/tests/myTest.Rout.save.

2) R --vanilla < myTest.R > myTest.Rout.save 2>&1 if you use sh.
   R --vanilla < myTest.R >&! myTest.Rout.save    if you use csh

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Friedrich.Leisch at tuwien.ac.at  Fri Nov  5 12:24:39 2004
From: Friedrich.Leisch at tuwien.ac.at (Friedrich.Leisch@tuwien.ac.at)
Date: Fri, 5 Nov 2004 12:24:39 +0100
Subject: [R] Sweave and transparent PDF?
In-Reply-To: <418A2C95.10109@mpipsykl.mpg.de>
References: <D15343265276D31197BC00A024A6C110C79337@EXS_BDC>
	<200408091043.34409.deepayan@stat.wisc.edu>
	<418A2C95.10109@mpipsykl.mpg.de>
Message-ID: <16779.25335.142739.194218@galadriel.ci.tuwien.ac.at>

>>>>> On Thu, 04 Nov 2004 14:20:21 +0100,
>>>>> Benno P??tz (BP) wrote:

  > Hello,
  > I have run across the following problem:

  > Creating PDF files manually by using

  > pdf(version=1.4)

  > I can make graphs using the new transparency feature of R2.0. If, 
  > however, I try to create the same graphs with Sweave, all transparent 
  > stuff is gone. This os likely due to the default version parameter in in 
  > pdf().

  > How can I tell Sweave to use the newer PDF version? The parameter seems 
  > to be hard-coded (not via some option) and I could not come up with some 
  > kind of hook ...

Good point, currently I see no way short of directly creating the PDF
file and manually including it in latex, i.e., do

**********************************************************
<<fig=false>>=
pdf(file="foo.pdf", version="1.4")
myplot(...)
dev.off()
@

\incudegraphics{foo}
**********************************************************


But we should think about a more general mechansim to specify defaults
for the PDF device (not only for Sweave).

-- 
-------------------------------------------------------------------
                        Friedrich Leisch 
Institut f??r Statistik                     Tel: (+43 1) 58801 10715
Technische Universit??t Wien                Fax: (+43 1) 58801 10798
Wiedner Hauptstra??e 8-10/1071
A-1040 Wien, Austria             http://www.ci.tuwien.ac.at/~leisch



From ahenningsen at email.uni-kiel.de  Fri Nov  5 12:29:44 2004
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Fri, 5 Nov 2004 12:29:44 +0100
Subject: [R] Creating .Rout.save files for package subdirectory "tests"
In-Reply-To: <16779.24933.911161.653300@gargle.gargle.HOWL>
References: <200411051156.02229.ahenningsen@email.uni-kiel.de>
	<16779.24933.911161.653300@gargle.gargle.HOWL>
Message-ID: <200411051229.44278.ahenningsen@email.uni-kiel.de>

Thank you, Martin and Prof. Ripley, for your very helpful answers! 
It would be nice if this could be mentioned in section 1.1.4 of the "Writing R 
Extensions" manual. 
Arne

On Friday 05 November 2004 12:17, Martin Maechler wrote:
> >>>>> "Arne" == Arne Henningsen <ahenningsen at email.uni-kiel.de>
> >>>>>     on Fri, 5 Nov 2004 11:56:02 +0100 writes:
>
>     Arne> Hi, I added the "tests" subdirectory and a test file
>     Arne> (say "myTest.R") to our "systemfit" package. Up to now
>     Arne> I create the "myTest.Rout.save" file with
>
>     >> R CMD BATCH --vanilla myTest.R myTest.Rout.save
>
> that explains everything.
> Why do you do so?
>
> Instead:
>
> 1) Start with no '.Rout.save'
>
> 2) R CMD check  <pkg>
>    will produce one in  <pkg>.Rcheck/tests/myTest.Rout
>    Copy it to the package source, i.e. typically
>
>    cp <pkg>.Rcheck/tests/myTest.Rout <pkg>/tests/myTest.Rout.save
>
> 3) R CMD check  <pkg>
>    now *will* do the comparison and give no difference hopefully
>
> Martin Maechler, ETH Zurich
>
>
>     Arne> However, "R CMD check" reports two differences between
>     Arne> myTest.Rout.save and the output of myTest.R: a)
>     Arne> myTest.Rout.save contains following extra line at the
>
>     Arne> beginning of the file:
>     >> invisible(options(echo = TRUE))
>
>     Arne> b) myTest.Rout.save contains the following 2 extra
>
>     Arne> lines at the very end of the file:
>     >> proc.time()
>
>     Arne> [1] 1.80 0.07 2.00 0.01 0.00
>
>     Arne> Everytime I change myTest.R and create a new
>     Arne> myTest.R.save file I have to delete these 3 lines by
>     Arne> hand. I could do this e.g. by a script using "sed",
>     Arne> but I wonder if I can start "R CMD BATCH" with an
>     Arne> option that suppresses any commands that are not in
>     Arne> the input file, i.e. "invisible(options(echo = TRUE))"
>     Arne> and "proc.time()".  Or what is the easiest way to
>     Arne> create .Rout.save files?  (I am using R 2.0.0 on a
>     Arne> i686 PC with SuSE Linux 9.0)
>
>     Arne> Thanks, Arne

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From jarioksa at sun3.oulu.fi  Fri Nov  5 14:00:52 2004
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: 05 Nov 2004 15:00:52 +0200
Subject: [R] biplot drawing conc ellipses
In-Reply-To: <418A9499.5010907@sdsc.edu>
References: <418A9499.5010907@sdsc.edu>
Message-ID: <1099659652.4084.11.camel@biol102145.oulu.fi>

On Thu, 2004-11-04 at 22:44, T. Murlidharan Nair wrote:
> Is there an option to draw concentration ellipses in biplots ? It seems
> really nice to summarize large number of points of each group.

Murli,

If you mean biplot.prcomp function in stats package, and you want to
draw the "concentration ellipses" for row scores, the answer probably is
"not easily". Technically, the problem is that arrows for loadings are
drawn after labels for rowscores, and the scaling used for drawing row
scores is lost in the process. If you try to add points or segments to
the existing plots, you should use the scaling for arrows on sides 3 and
4 (top and right). If you want to add something for row scores, you just
don't have information on co-ordinates. I didn't check biplot.princomp,
but the situation may be similar there. 

Drawing of ellipsoids is possible in some alternative packages. You
already got a hint of ade4. In addition, vegan has pca as a special case
of its rda function, and there you have tools like ordiellipse (using
the ellipse package), ordispider and ordihull to display the variability
within factor levels. However, vegan doesn't have biplots like
biplot.prcomp, i.e. with arrows for loadings, Moreover, scaling of
results is different. 

It seems that the only thing you can do is to write your sweet on biplot
function. 

cheers, jari oksanen
-- 
Jari Oksanen -- Oulu, Finland.
"But, Mousie, thou art no thy lane, In proving foresight may be vain;
The best-laid schemes o' mice an 'men, Gang aft agley,
An'lea'e us nought but grief an' pain, For promis'd joy!" (Robert Burns)



From franz.faerber at sap.com  Fri Nov  5 14:47:08 2004
From: franz.faerber at sap.com (Faerber, Franz)
Date: Fri, 5 Nov 2004 14:47:08 +0100 
Subject: [R] Question about RSQLite
Message-ID: <B8FA8397DC482449B5A4BE230E98B118C710D3@dewdfe11.wdf.sap.corp>

I've installed RSQLite with version 1.9.1

When calling

m<-dbDriver("SQLite")

I get the error message
"Invalid names for slots of class SQLiteDriver: Id"

Whats my mistake?


Thx - franz



From Kay.Pilz at ruhr-uni-bochum.de  Fri Nov  5 14:48:22 2004
From: Kay.Pilz at ruhr-uni-bochum.de (Kay Pilz)
Date: Fri, 05 Nov 2004 14:48:22 +0100 (MET)
Subject: [R] Problems with the sort function!?
Message-ID: <permail-2004110513482214681-pilzkabz@ruhr-uni-bochum.de>

Hello All.

I am running R 2.0.0 with a Win XP operating system. The same problem occured
with R 1.9.1 but not with R.1.3.1 on a Win NT computer.

My request is about the following problem with the sort function:

The vector price is a vector of 789 asset prices of the following form,

> mode(price)
[1] "numeric"
> price[1:10]
 [1] 1.1315 1.1314 1.1313 1.1314 1.1315 1.1316 1.1315 1.1317
 [9] 1.1316 1.1316

Then the returns are defined to be the differences,

> ret <- diff(price)
> ret[1:10]
 [1] -1e-04 -1e-04  1e-04  1e-04  1e-04 -1e-04  2e-04 -1e-04
 [9]  0e+00  0e+00

Here R recognizes the first two entries and also the second ones to be equal
(I did not found any two equal entries, where R didnt recognized this
relation),

> ret[1]==ret[2]
[1] TRUE
> ret[3]==ret[4]
[1] TRUE

But after sorting the vector, this ist not longer true,

> s <- sort(ret)
> s[1:10]
 [1] -5e-04 -5e-04 -5e-04 -5e-04 -5e-04 -4e-04 -4e-04 -4e-04
 [9] -4e-04 -4e-04
> s[1]==s[2]
[1] FALSE

Taking the difference of the first two entries yields

> s[1]-s[2]
[1] -2.220446e-16

which seems to be a numerical artefact.

Is this a bug, or just a handling error by me?

Thanks a lot for your answers,
Kay Frederik Pilz
Ruhr-Universitaet Bochum
Germany
Kay.Pilz at rub.de



From ramasamy at cancer.org.uk  Fri Nov  5 15:18:42 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Fri, 05 Nov 2004 14:18:42 +0000
Subject: [R] Problems with the sort function!?
In-Reply-To: <permail-2004110513482214681-pilzkabz@ruhr-uni-bochum.de>
References: <permail-2004110513482214681-pilzkabz@ruhr-uni-bochum.de>
Message-ID: <1099664322.3604.6.camel@ndmpc126.ihs.ox.ac.uk>

> x <- 1.00000001
> all.equal(x, 1)
[1] TRUE
> x == 1
[1] FALSE

Reading help(all.equal) will tell you that the tolerance level by
default is around 1.490116e-08.


On Fri, 2004-11-05 at 13:48, Kay Pilz wrote:
> Hello All.
> 
> I am running R 2.0.0 with a Win XP operating system. The same problem occured
> with R 1.9.1 but not with R.1.3.1 on a Win NT computer.

So, which version are you having this problem with ?

> My request is about the following problem with the sort function:
> 
> The vector price is a vector of 789 asset prices of the following form,
> 
> > mode(price)
> [1] "numeric"
> > price[1:10]
>  [1] 1.1315 1.1314 1.1313 1.1314 1.1315 1.1316 1.1315 1.1317
>  [9] 1.1316 1.1316
> 
> Then the returns are defined to be the differences,
> 
> > ret <- diff(price)
> > ret[1:10]
>  [1] -1e-04 -1e-04  1e-04  1e-04  1e-04 -1e-04  2e-04 -1e-04
>  [9]  0e+00  0e+00
> 
> Here R recognizes the first two entries and also the second ones to be equal
> (I did not found any two equal entries, where R didnt recognized this
> relation),
> 
> > ret[1]==ret[2]
> [1] TRUE
> > ret[3]==ret[4]
> [1] TRUE
> 
> But after sorting the vector, this ist not longer true,
> 
> > s <- sort(ret)
> > s[1:10]
>  [1] -5e-04 -5e-04 -5e-04 -5e-04 -5e-04 -4e-04 -4e-04 -4e-04
>  [9] -4e-04 -4e-04
> > s[1]==s[2]
> [1] FALSE
> 
> Taking the difference of the first two entries yields
> 
> > s[1]-s[2]
> [1] -2.220446e-16
> 
> which seems to be a numerical artefact.
> 
> Is this a bug, or just a handling error by me?
> 
> Thanks a lot for your answers,
> Kay Frederik Pilz
> Ruhr-Universitaet Bochum
> Germany
> Kay.Pilz at rub.de
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From p.dalgaard at biostat.ku.dk  Fri Nov  5 15:20:32 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 05 Nov 2004 15:20:32 +0100
Subject: [R] Problems with the sort function!?
In-Reply-To: <permail-2004110513482214681-pilzkabz@ruhr-uni-bochum.de>
References: <permail-2004110513482214681-pilzkabz@ruhr-uni-bochum.de>
Message-ID: <x2y8hgnzrj.fsf@biostat.ku.dk>

Kay Pilz <Kay.Pilz at ruhr-uni-bochum.de> writes:

> Hello All.
> 
> I am running R 2.0.0 with a Win XP operating system. The same problem occured
> with R 1.9.1 but not with R.1.3.1 on a Win NT computer.
> 
> My request is about the following problem with the sort function:
...
> Taking the difference of the first two entries yields
> 
> > s[1]-s[2]
> [1] -2.220446e-16
> 
> which seems to be a numerical artefact.
> 
> Is this a bug, or just a handling error by me?

You, I think. Consider for instance


> x <- c(1.313,1.314,1.315,1.316)
> diff(x) - 1e-3
[1]  1.118897e-16 -1.101549e-16  1.118897e-16
> sort(diff(x) - 1e-3)
[1] -1.101549e-16  1.118897e-16  1.118897e-16
> diff(sort(diff(x) - 1e-3)[1:2])
[1] 2.220446e-16


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From wolski at molgen.mpg.de  Fri Nov  5 15:36:36 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Fri, 05 Nov 2004 15:36:36 +0100
Subject: [R] smooth.spline _df_ parameter?
Message-ID: <200411051536360592.0139FBE2@mail.math.fu-berlin.de>

Hi,

I am usign the smooth.spline function. 

I am not sure how the _df_ (degrees of freedom) parameter, 
if set, influences _lambda_ in eq:

L = (y - f)' W (y - f) + lambda c' Sigma c

Is _df_  substituting tr(Sigma), if defined, in the equation: r = tr(X' W X) / tr(Sigma),
which is used to compute: lambda = r * 256^(3*spar - 1)?


How _spar_ is set if not defined?


/E



Dipl. bio-chem. Eryk Witold Wolski             @    MPI-Moleculare Genetic   
Ihnestrasse 63-73 14195 Berlin                'v'                            
tel: 0049-30-83875219                        /   \                           
mail: witek96 at users.sourceforge.net        ---W-W----                        
http://r4proteomics.sourceforg.net



From xxq5 at case.edu  Fri Nov  5 16:07:33 2004
From: xxq5 at case.edu (Xin Qi)
Date: Fri, 05 Nov 2004 10:07:33 -0500
Subject: [R] (no subject)
Message-ID: <5.1.1.6.0.20041105100440.02a19e80@mail.cwru.edu>

Dear all R users and helpers:

I wonder whether there is a library call trees ( classification trees) in 
R? If  there is, where can download it?

Thank you very much.

Xin Qi



From tlumley at u.washington.edu  Fri Nov  5 16:14:19 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 5 Nov 2004 07:14:19 -0800 (PST)
Subject: [R] (no subject)
In-Reply-To: <5.1.1.6.0.20041105100440.02a19e80@mail.cwru.edu>
References: <5.1.1.6.0.20041105100440.02a19e80@mail.cwru.edu>
Message-ID: <Pine.A41.4.61b.0411050713310.58230@homer09.u.washington.edu>

On Fri, 5 Nov 2004, Xin Qi wrote:

> Dear all R users and helpers:
>
> I wonder whether there is a library call trees ( classification trees) in R? 
> If  there is, where can download it?
>

No, there isn't.

However, there is a package called "tree" that you can download from CRAN 
and another called "rpart" that came with your R distribution, both of 
which do classification trees.

 	-thomas



From Brian.Bielinski at trafalgarcapital.com  Fri Nov  5 16:19:11 2004
From: Brian.Bielinski at trafalgarcapital.com (Brian Bielinski)
Date: Fri, 5 Nov 2004 15:19:11 -0000
Subject: [R] pmin behavior change in 2.0.0
Message-ID: <2B2C136DEABA19409D5710EE2A394626314514@tc-exch01.trafalgarcapital.com>

Hi,

In 2.0.0 the behavior of pmin has changed.
It stops now with an error if all the elements
at a particular point are NA.

These examples were run on windows xp, but the behavior
for 2.0.0 is the same on linux.

R 1.9.1

>  pmin(c(1,NA,3),c(1,NA,2))
[1]  1 NA  2
>  pmin(c(1,NA,3),c(1,2,2))
[1]  1 NA  2
>  pmin(c(1,NA,3),c(1,NA,2),na.rm=TRUE)
[1]  1 NA  2
> 


R 2.0.0

> pmin(c(1,NA,3),c(1,NA,2))
Error: NAs are not allowed in subscripted assignments
> pmin(c(1,NA,3),c(1,2,2))
[1]  1 NA  2 
> pmin(c(1,NA,3),c(1,NA,2),na.rm=TRUE)
Error: NAs are not allowed in subscripted assignments
>

Is this intentional?

Regards,

Brian



From julien.trolet at curie.fr  Fri Nov  5 16:22:45 2004
From: julien.trolet at curie.fr (Julien Trolet)
Date: Fri, 05 Nov 2004 16:22:45 +0100
Subject: [R] Lda versus Rda
Message-ID: <418B9AC5.60502@curie.fr>

Hello,

I used the lda function from the MASS (VR) package and the rda function 
from the klaR package.
I wanted to compare the result of this two functions by using the same 
training set.
Thus, I used the rda function with lambda=1 an gamma=0, I should emulate 
the lda function and I should obtain the same result.

But this it not the case, the two result are very different.

My training set is 70 observations * 10 variables long, and I performed 
a leave one out for each  observations.

Do somebody have an idea for the cause(s) of this?

Thanks

Trolet Julien



From Michaell.Taylor at boxwoodmeans.com  Fri Nov  5 16:24:03 2004
From: Michaell.Taylor at boxwoodmeans.com (Michaell Taylor)
Date: Fri, 05 Nov 2004 09:24:03 -0600
Subject: [R] graphics site
Message-ID: <1099668243.9752.331.camel@localhost>


About six months ago there was a reference to a site (in french) that
did a spectacular job of demonstrating R's graphical capabilities.

My bookmarks were recently wiped and I cannot find this site despite my
best googling.

Anyone have the address which I have done a miserable job describing?

Thanks.

Michaell



From ramasamy at cancer.org.uk  Fri Nov  5 16:28:11 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Fri, 05 Nov 2004 15:28:11 +0000
Subject: [R] (no subject)
In-Reply-To: <5.1.1.6.0.20041105100440.02a19e80@mail.cwru.edu>
References: <5.1.1.6.0.20041105100440.02a19e80@mail.cwru.edu>
Message-ID: <1099668491.3604.22.camel@ndmpc126.ihs.ox.ac.uk>

Check http://cran.r-project.org/src/contrib/PACKAGES.html.

AFAIK, there is tree, rpart and knnTree.

The simplest wat to install, say the package tree, is to type in
install.packages("tree") in the R command line.


On Fri, 2004-11-05 at 15:07, Xin Qi wrote:
> Dear all R users and helpers:
> 
> I wonder whether there is a library call trees ( classification trees) in 
> R? If  there is, where can download it?
> 
> Thank you very much.
> 
> Xin Qi
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From 0034058 at fudan.edu.cn  Fri Nov  5 16:32:19 2004
From: 0034058 at fudan.edu.cn (rongguiwong)
Date: Fri, 05 Nov 2004 23:32:19 +0800
Subject: [R] any function for jackknife validation
Message-ID: <200411052332.19578.0034058@fudan.edu.cn>

i want to using jackknife validation for a small sample to the PCA.is there 
any function for jacknife?i know splus has ,but i can not find in R.



From ggrothendieck at myway.com  Fri Nov  5 16:48:43 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 5 Nov 2004 15:48:43 +0000 (UTC)
Subject: [R] graphics site
References: <1099668243.9752.331.camel@localhost>
Message-ID: <loom.20041105T164819-789@post.gmane.org>

Michaell Taylor <Michaell.Taylor <at> boxwoodmeans.com> writes:

: 
: About six months ago there was a reference to a site (in french) that
: did a spectacular job of demonstrating R's graphical capabilities.
: 
: My bookmarks were recently wiped and I cannot find this site despite my
: best googling.
: 
: Anyone have the address which I have done a miserable job describing?
: 
: Thanks.
: 
: Michaell


Try a google search for zoonekynd .



From Achim.Zeileis at wu-wien.ac.at  Fri Nov  5 17:50:57 2004
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Fri, 5 Nov 2004 17:50:57 +0100
Subject: [R] Error message from vignette strucchange-intro example
In-Reply-To: <418B5BF1.3080508@web.de>
References: <418B5BF1.3080508@web.de>
Message-ID: <20041105175057.7b2858b9.Achim.Zeileis@wu-wien.ac.at>

Thomas:

> I am just studying the following example from vignette: 
> strucchange-intro,

With problems like this, please contact the package maintainer, or at
least Cc.

> contineousely ending up in an error.
> 
> This is the given code:
> 
> 1. library(strucchange)
> 2. data(USIncExp)
> 3. if (!"package:stats" %in% search()) library(ts)
> 4. USIncExp2 <- window(USIncExp, start = c(1985, 12))
> 
> A.Modelling:
> 
> coint.res <- residuals(lm(expenditure ~ income, data = USIncExp2))
> coint.res <- lag(ts(coint.res, start = c(1985, 12), freq = 12),k = -1)
> USIncExp2 <- cbind(USIncExp2, diff(USIncExp2), coint.res)
> USIncExp2 <- window(USIncExp2, start = c(1986, 1), end = c(2001,2))
> 
> #here is what eval is looking for in vain: diff.expenditure
> colnames(USIncExp2) <- 
> c("income","expenditure","diff.income","diff.expenditure","coint.res")

But until here the code is working, right?
You've got a multivariate time series object now with the above column
names, haven't you? 

> B. Error correction formula
> 
> ecm.model <- diff.expenditure ~ coint.res + diff.income
> 
> C. Using EFP (OLS) test function
> 
> ocus <- efp(ecm.model, type = "OLS-CUSUM", data = USIncExp2)
> 
> 
> Both commads under C give me :
> 
> -------------------------------------------------------------------
> Error in eval(expr, envir, enclos) : Object "diff.expenditure" not
> found----------------------------------------------------------------
> ---
> 
> Since quite some time I am sitting here and wonder whether there is an
> error in the given code or if it my simple inability to catch up with 
> a self-breed error (???)

Looks like the latter. I tried this with the CRAN versions of
strucchange, sandwich and zoo with R --vanilla (in 2.0.0) and had no
problems. Also the daily CRAN checks don't indicate that something went
wrong:
  http://cran.r-project.org/src/contrib/checkSummary.html

I suggest that you have another thorough look. If you find something
str(ucch)ange, you can also contact me off-list.
Z

> 
> ----------------------------------------------
> platform i386-pc-linux-gnu
> arch     i386
> os       linux-gnu
> system   i386, linux-gnu
> status
> major    2
> minor    0.0
> year     2004
> month    10
> day      04
> language R
> ------------------------------------------------
> 
> 
> 
> Maybe someone to direct my delusion to something helpful, so far I've 
> looked for typos and alike, as far I can say the code should work, 
> isn't it?
> 
> Thomas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Fri Nov  5 16:59:03 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 5 Nov 2004 15:59:03 +0000 (GMT)
Subject: [R] pmin behavior change in 2.0.0
In-Reply-To: <2B2C136DEABA19409D5710EE2A394626314514@tc-exch01.trafalgarcapital.com>
Message-ID: <Pine.LNX.4.44.0411051551560.20582-100000@gannet.stats>

This is fixed in 2.0.1 beta that is currently available for testing.  It
was a bug in pmin that went undetected until things were tightened up.
[More precisely, pmin used an undefined construction, NA subscripts on the 
LHS of an assignment, that was not implemented consistently.]

Interestingly it came up on Monday, but in none of the testing of 2.0.0
nor for the first 4 weeks it was out.

On Fri, 5 Nov 2004, Brian Bielinski wrote:

> Hi,
> 
> In 2.0.0 the behavior of pmin has changed.
> It stops now with an error if all the elements
> at a particular point are NA.
> 
> These examples were run on windows xp, but the behavior
> for 2.0.0 is the same on linux.
> 
> R 1.9.1
> 
> >  pmin(c(1,NA,3),c(1,NA,2))
> [1]  1 NA  2
> >  pmin(c(1,NA,3),c(1,2,2))
> [1]  1 NA  2
> >  pmin(c(1,NA,3),c(1,NA,2),na.rm=TRUE)
> [1]  1 NA  2
> > 
> 
> 
> R 2.0.0
> 
> > pmin(c(1,NA,3),c(1,NA,2))
> Error: NAs are not allowed in subscripted assignments
> > pmin(c(1,NA,3),c(1,2,2))
> [1]  1 NA  2 
> > pmin(c(1,NA,3),c(1,NA,2),na.rm=TRUE)
> Error: NAs are not allowed in subscripted assignments
> >
> 
> Is this intentional?
> 
> Regards,
> 
> Brian
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tlumley at u.washington.edu  Fri Nov  5 17:02:05 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 5 Nov 2004 08:02:05 -0800 (PST)
Subject: [R] pmin behavior change in 2.0.0
In-Reply-To: <2B2C136DEABA19409D5710EE2A394626314514@tc-exch01.trafalgarcapital.com>
References: <2B2C136DEABA19409D5710EE2A394626314514@tc-exch01.trafalgarcapital.com>
Message-ID: <Pine.A41.4.61b.0411050801200.58230@homer09.u.washington.edu>

On Fri, 5 Nov 2004, Brian Bielinski wrote:

> Hi,
>
> In 2.0.0 the behavior of pmin has changed.
> It stops now with an error if all the elements
> at a particular point are NA.
>

Yes. This has already been fixed in r-patched.

 	-thomas



From deepayan at stat.wisc.edu  Fri Nov  5 18:07:49 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Fri, 5 Nov 2004 11:07:49 -0600
Subject: [R] I: a coloured band within each panel of a lattice bwplot
In-Reply-To: <DNEELNJCLGBOLHCFLMHBMENMCHAA.OLPantani@unifi.it>
References: <DNEELNJCLGBOLHCFLMHBMENMCHAA.OLPantani@unifi.it>
Message-ID: <200411051107.50062.deepayan@stat.wisc.edu>

On Friday 05 November 2004 05:05, 8rino-Luca Pantani wrote:
> Hi all,
> I would like to add to each panel of a bwplot a coloured central
> band, centered on the mean of the values, being its width +- 2% of
> the mean itself.
>
> I know how to add lines, i.e. something like
>
> bwplot(X ~ Y|FACTOR
>        data=my.df,
>        panel= function(x, y){
>          panel.bwplot(x, y)
>          panel.abline(v = mean(x, na.rm = T) - mean(x, na.rm = T) *
> 0.02 panel.abline(v = mean(x, na.rm = T) + mean(x, na.rm = T) * 0.02
> })
>
> but I cannot figure out how to shade the region included between the
> two lines.

Generally speaking, you want to draw a polygon . There is no lattice 
wrapper for the 'grid.polygon' function, so you will have to use it 
directly. Check out 

> library(grid)
> help(grid.polygon)

For this simpler case though, 'grid.rect' should be sufficient, e.g.

       panel= function(x, y){
           m <- mean(x, na.rm = T)
           grid.rect(x = unit(m, "native"),
                     w = unit(m * 0.04, "native"),
                     gp = gpar(fill = "lightgrey", col = "transparent"))
           panel.bwplot(x, y)
       }

Make sure to do a 'library(grid)' before this.

Deepayan



From jerosenb at fas.harvard.edu  Fri Nov  5 18:34:55 2004
From: jerosenb at fas.harvard.edu (Janet Elise Rosenbaum)
Date: Fri, 5 Nov 2004 12:34:55 -0500 (EST)
Subject: [R] Resources for optimizing code
In-Reply-To: <200411051119.iA5BJC2l030811@hypatia.math.ethz.ch> from
	"r-help-request@stat.math.ethz.ch" at Nov 05, 2004 12:19:12 PM
Message-ID: <20041105173455.89B7C1C003@ls01.fas.harvard.edu>


I want to eliminate certain observations in a large dataframe (21000x100).
I have written code which does this using a binary vector (0=delete obs,
1=keep), but it uses for loops, and so it's slow and in the extreme it 
causes R to hang for indefinite time periods.

I'm looking for one of two things:
1.  A document which discusses how to avoid for loops and situations in
which it's impossible to avoid for loops.

or

2.  A function which can do the above better than mine.  

My code is pasted below.

Thanks so much,

Janet 

# asst is a binary vector of length= nrow(DATAFRAME).  
# 1= observations you want to keep.  0= observation to get rid of.

remove.xtra.f <-function(asst, DATAFRAME) {
	n<-sum(asst, na.rm=T)
	newdata<-matrix(nrow=n, ncol=ncol(DATAFRAME))
	j<-1
	for(i in 1:length(data)) {
		if (asst[i]==1) {
			newdata[j,]<-DATAFRAME[i,]
			j<-j+1
		}
	}
	newdata.f<-as.data.frame(newdata)
	names(newdata.f)<-names(DATAFRAME)
	return(newdata.f)
}
--  
Janet Rosenbaum                                 jerosenb at fas.harvard.edu
PhD Candidate in Health Policy, Harvard GSAS
Harvard Injury Control Research Center, Harvard School of Public Health



From Roger.Bivand at nhh.no  Fri Nov  5 18:52:26 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 5 Nov 2004 18:52:26 +0100 (CET)
Subject: [R] Resources for optimizing code
In-Reply-To: <20041105173455.89B7C1C003@ls01.fas.harvard.edu>
Message-ID: <Pine.LNX.4.44.0411051850350.17589-100000@reclus.nhh.no>

On Fri, 5 Nov 2004, Janet Elise Rosenbaum wrote:

> 
> I want to eliminate certain observations in a large dataframe (21000x100).
> I have written code which does this using a binary vector (0=delete obs,
> 1=keep), but it uses for loops, and so it's slow and in the extreme it 
> causes R to hang for indefinite time periods.
> 
> I'm looking for one of two things:
> 1.  A document which discusses how to avoid for loops and situations in
> which it's impossible to avoid for loops.
> 
> or
> 
> 2.  A function which can do the above better than mine.  

?subset
newdata <- subset(DATAFRAME, asst==1)

which will work whether DATAFRAME is a matrix or data.frame (two different 
classes).

> 
> My code is pasted below.
> 
> Thanks so much,
> 
> Janet 
> 
> # asst is a binary vector of length= nrow(DATAFRAME).  
> # 1= observations you want to keep.  0= observation to get rid of.
> 
> remove.xtra.f <-function(asst, DATAFRAME) {
> 	n<-sum(asst, na.rm=T)
> 	newdata<-matrix(nrow=n, ncol=ncol(DATAFRAME))
> 	j<-1
> 	for(i in 1:length(data)) {
> 		if (asst[i]==1) {
> 			newdata[j,]<-DATAFRAME[i,]
> 			j<-j+1
> 		}
> 	}
> 	newdata.f<-as.data.frame(newdata)
> 	names(newdata.f)<-names(DATAFRAME)
> 	return(newdata.f)
> }
> --  
> Janet Rosenbaum                                 jerosenb at fas.harvard.edu
> PhD Candidate in Health Policy, Harvard GSAS
> Harvard Injury Control Research Center, Harvard School of Public Health
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From ripley at stats.ox.ac.uk  Fri Nov  5 19:04:55 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 5 Nov 2004 18:04:55 +0000 (GMT)
Subject: [R] Resources for optimizing code
In-Reply-To: <20041105173455.89B7C1C003@ls01.fas.harvard.edu>
Message-ID: <Pine.LNX.4.44.0411051754540.24480-100000@gannet.stats>

On Fri, 5 Nov 2004, Janet Elise Rosenbaum wrote:

> 
> I want to eliminate certain observations in a large dataframe (21000x100).
> I have written code which does this using a binary vector (0=delete obs,
> 1=keep), but it uses for loops, and so it's slow and in the extreme it 
> causes R to hang for indefinite time periods.
> 
> I'm looking for one of two things:
> 1.  A document which discusses how to avoid for loops and situations in
> which it's impossible to avoid for loops.

`S Programming': see the FAQ.
But at the level of the example below, chapter 2 of MASS4 (FAQ again for 
details).

> or
> 
> 2.  A function which can do the above better than mine.  
> 
> My code is pasted below.
> 
> Thanks so much,
> 
> Janet 
> 
> # asst is a binary vector of length= nrow(DATAFRAME).  
> # 1= observations you want to keep.  0= observation to get rid of.

How about DATAFRAME[asst == 1, ] ?

I am not sure if asst has NAs in, but if it has you will get an error from 
                if (asst[i]==1)
and if not, you don't need na.rm=T.

> DF <- as.data.frame(matrix(rnorm(21000*100),, 100))
> asst <- rbinom(21000, 1, 0.7)
> DF2 <- DF[asst==1,]

where the subsetting took less than a second for me.

Note that your code converts DATAFRAME to a matrix. If that is reasonable 
(e.g. it is all numeric), then matrix indexing will be faster.

> remove.xtra.f <-function(asst, DATAFRAME) {
> 	n<-sum(asst, na.rm=T)
> 	newdata<-matrix(nrow=n, ncol=ncol(DATAFRAME))
> 	j<-1
> 	for(i in 1:length(data)) {
> 		if (asst[i]==1) {
> 			newdata[j,]<-DATAFRAME[i,]
> 			j<-j+1
> 		}
> 	}
> 	newdata.f<-as.data.frame(newdata)
> 	names(newdata.f)<-names(DATAFRAME)
> 	return(newdata.f)
> }
> --  
> Janet Rosenbaum                                 jerosenb at fas.harvard.edu
> PhD Candidate in Health Policy, Harvard GSAS
> Harvard Injury Control Research Center, Harvard School of Public Health
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Nov  5 19:17:07 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 5 Nov 2004 18:17:07 +0000 (GMT)
Subject: [R] Resources for optimizing code
In-Reply-To: <Pine.LNX.4.44.0411051850350.17589-100000@reclus.nhh.no>
Message-ID: <Pine.LNX.4.44.0411051813200.24480-100000@gannet.stats>

On Fri, 5 Nov 2004, Roger Bivand wrote:

> On Fri, 5 Nov 2004, Janet Elise Rosenbaum wrote:
> 
> > 
> > I want to eliminate certain observations in a large dataframe (21000x100).
> > I have written code which does this using a binary vector (0=delete obs,
> > 1=keep), but it uses for loops, and so it's slow and in the extreme it 
> > causes R to hang for indefinite time periods.
> > 
> > I'm looking for one of two things:
> > 1.  A document which discusses how to avoid for loops and situations in
> > which it's impossible to avoid for loops.
> > 
> > or
> > 
> > 2.  A function which can do the above better than mine.  
> 
> ?subset
> newdata <- subset(DATAFRAME, asst==1)
> 
> which will work whether DATAFRAME is a matrix or data.frame (two different 
> classes).

Sorry, not for matrices:

> A <- matrix(1:20, 5)
> asst <- c(1,0,0,1,0)
> subset(A, asst)
[1]  1  4  6  9 11 14 16 19

Maybe it should, but in biggish problems like this it is almost certainly 
a bit more efficient to use the bare tools, that is indexing.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From pierre.bady at univ-lyon1.fr  Fri Nov  5 19:20:43 2004
From: pierre.bady at univ-lyon1.fr (Pierre BADY)
Date: Fri, 05 Nov 2004 19:20:43 +0100
Subject: [R] graphics site
In-Reply-To: <1099668243.9752.331.camel@localhost>
Message-ID: <5.1.0.14.2.20041105192034.01d46008@pop.univ-lyon1.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041105/20ed432f/attachment.pl

From tplate at acm.org  Fri Nov  5 19:46:36 2004
From: tplate at acm.org (Tony Plate)
Date: Fri, 05 Nov 2004 11:46:36 -0700
Subject: [R] Resources for optimizing code
In-Reply-To: <20041105173455.89B7C1C003@ls01.fas.harvard.edu>
References: <200411051119.iA5BJC2l030811@hypatia.math.ethz.ch>
	<20041105173455.89B7C1C003@ls01.fas.harvard.edu>
Message-ID: <6.1.0.6.2.20041105113843.0bc84438@mailhost.blackmesacapital.com>

Have you tried reading the manual "An Introduction to R", with special 
attention to "Array Indexing" (indexing for data frames is pretty similar 
to indexing for matrices).

Unless I'm misunderstanding, what you want to do is very simple.  It is 
possible to use numeric vectors with 0 and 1 to indicate whether you want 
to keep the row, but it's a little easier with logical vectors.  Here's an 
example:

 > x <- data.frame(a=1:5,b=letters[1:5])
 > keep.num <- ifelse(x$a %% 2 == 1, 1, 0)
 > keep.num
[1] 1 0 1 0 1
 > keep.logical <- (x$a %% 2) == 1
 > keep.logical
[1]  TRUE FALSE  TRUE FALSE  TRUE
 > x[keep.num==1,,drop=F]
   a b
1 1 a
3 3 c
5 5 e
 > x[keep.logical,,drop=F]
   a b
1 1 a
3 3 c
5 5 e
 >



At Friday 10:34 AM 11/5/2004, Janet Elise Rosenbaum wrote:

>I want to eliminate certain observations in a large dataframe (21000x100).
>I have written code which does this using a binary vector (0=delete obs,
>1=keep), but it uses for loops, and so it's slow and in the extreme it
>causes R to hang for indefinite time periods.
>
>I'm looking for one of two things:
>1.  A document which discusses how to avoid for loops and situations in
>which it's impossible to avoid for loops.
>
>or
>
>2.  A function which can do the above better than mine.
>
>My code is pasted below.
>
>Thanks so much,
>
>Janet
>
># asst is a binary vector of length= nrow(DATAFRAME).
># 1= observations you want to keep.  0= observation to get rid of.
>
>remove.xtra.f <-function(asst, DATAFRAME) {
>         n<-sum(asst, na.rm=T)
>         newdata<-matrix(nrow=n, ncol=ncol(DATAFRAME))
>         j<-1
>         for(i in 1:length(data)) {
>                 if (asst[i]==1) {
>                         newdata[j,]<-DATAFRAME[i,]
>                         j<-j+1
>                 }
>         }
>         newdata.f<-as.data.frame(newdata)
>         names(newdata.f)<-names(DATAFRAME)
>         return(newdata.f)
>}
>--
>Janet Rosenbaum                                 jerosenb at fas.harvard.edu
>PhD Candidate in Health Policy, Harvard GSAS
>Harvard Injury Control Research Center, Harvard School of Public Health
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From mike_saunders at umenfa.maine.edu  Fri Nov  5 19:46:41 2004
From: mike_saunders at umenfa.maine.edu (Mike Saunders)
Date: Fri, 5 Nov 2004 13:46:41 -0500
Subject: [R] Problems running a 4-parameter Weibull function with nls
Message-ID: <001b01c4c367$ca855500$9ba76f82@CFRU0104>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041105/04039f2d/attachment.pl

From leinwebe at eva.mpg.de  Fri Nov  5 19:57:23 2004
From: leinwebe at eva.mpg.de (Marcus Leinweber)
Date: Fri, 05 Nov 2004 19:57:23 +0100
Subject: [R] text colour in legend
Message-ID: <opsgz9lxi3f57zr1@bio04.eva.mpg.de>

hello,


the following example gives a plot with a legend:


plot(-10:10,-10:10,type="n")
x=1:10
y=1:10
tt=c("A","B","C","D","E","F")
text(x,y,tt,cex=.8)
legend(-10,10,paste(tt," ",x,y),text.col=c(2:4))  # each row in different  
colour

but how can I colour the legend text by "column" meaning tt in red, x in  
blue and y in green?

thanks for your help.

marcus



From tlumley at u.washington.edu  Fri Nov  5 19:57:15 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 5 Nov 2004 10:57:15 -0800 (PST)
Subject: [R] [R-pkgs] dichromat package
Message-ID: <Pine.A41.4.61b.0411051048540.140096@homer10.u.washington.edu>


This is an update (version 1.2), not a new package, but given recent 
postings on the R lists an announcement may be helpful.

The 'dichromat' package has color schemes designed for people with 
red-green deficient or anomalous vision, tools for simulating the effect 
of color blindness, and tools for creating color ramps and palettes (these 
last will be in R 2.1.0).

I would welcome feedback on the accuracy of the simulations of color 
blindness from people who are actually affected. They are based on 
published algorithms and methods, but color matching is always tricky.


 	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From ross at biostat.ucsf.edu  Fri Nov  5 20:16:22 2004
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Fri, 05 Nov 2004 11:16:22 -0800
Subject: [R] R check passes code and docs that don't match
Message-ID: <1099682182.12780.144.camel@iron.libaux.ucsf.edu>

I have code and documentation that don't match, but R CMD check didn't
flag it.

in mspath.R
mspath <- function(formula,   # formula with  observed Markov states  
~  observation times (required)
                qmatrix,    # matrix of 1s and 0s with indices of
allowed transitions (diagonal is ignored) (required)
                misc = FALSE,
                ematrix = NULL,    # matrix of 1s and 0s with indices of
allowed misclassfications (diagonal is ignored) (required)
                inits,      # initial values of optimisation or fixed
values (required)
                subject = NULL, # optional, defaults to all the same if
not given
                covariates = NULL, # formula specifying covariates on
transition rates.
                constraint = NULL, # which intensities have covariates
on them (as in Marshall et al.)
                misccovariates = NULL, # formula specifying covariates
on misclassification probs
                miscconstraint = NULL, # which misc probs have
covariates on them
                qconstraint = NULL, # constraints on equality of
baseline intensities
                econstraint = NULL, # constraints on equality of
baseline misc probs
                covmatch = "previous",   # take the covariate value from
the previous or next observation
                initprobs = NULL,  # initial state occupancy
probabilities
                data=list(),       # optional data frame in which to
interpret variable names
                fromto = FALSE,
                fromstate, #
                tostate,   #  data required if fromto is TRUE
                timelag,   #
                death = FALSE,  # 'death' states, ie, entry time known
exactly, but unknown transient state at previous instant
                tunit = 1.0, # no longer used
                exacttimes = FALSE,
                fixedpars = NULL, # specify which parameters to fix
                stepnumerator = 1,  # maximum step size in discrete
approximation
                stepdenominator = 1,  # rational number; use integers
                do.what = 1, # 1 for likelihood, 0 for counts
                ... # options to optim
                )

in mspath.Rd
\usage{
mspath ( formula, qmatrix, misc = FALSE, ematrix, inits, subject,
      covariates = NULL, constraint = NULL, misccovariates = NULL,
      miscconstraint = NULL, qconstraint=NULL, econstraint=NULL,
      covmatch = "previous", initprobs = NULL, 
      data = list(), fromto = FALSE, fromstate, tostate, timelag,
      death = FALSE, tunit = 1.0, exacttimes = FALSE,
      fixedpars = NULL, stepsize=1.0... )
}
with corresponding \item's in the \arguments list.

Among the differences (see the end of the argument lists)
In code by not Rd: stepnumerator, stepdenominator, do.what
In Rd but not code: stepsize

Yet R CMD check says
** help
 >>> Building/Updating help pages for package 'mspath'
     Formats: text html latex example 
  mspath                            text    html    latex
     missing link(s):  qmatrix.mspath ematrix.mspath simmulti.mspath
print.mspath plot.mspath summary.mspath qmatrix.mspath pmatrix.mspath
sojourn.mspath
* DONE (mspath)

* checking package directory ... OK
* checking for portable file names ... OK
* checking for sufficient/correct file permissions ... OK
* checking package dependencies ... OK
* checking index information ... OK
* checking package subdirectories ... OK
* checking R files for syntax errors ... OK
* checking R files for library.dynam ... OK
* checking S3 generic/method consistency ... OK
* checking replacement functions ... OK
* checking foreign function calls ... OK
* checking Rd files ... OK
* checking for missing documentation entries ... WARNING
Undocumented code objects:
  MatrixExp absorbing.mspath coef.mspath crudeinits.mspath deltamethod
  ematrix.mspath expit expitsum expsum hazard.mspath lik.mspath logit
  mspath.check.consistency mspath.check.ematrix mspath.check.model
  mspath.check.qmatrix mspath.check.state mspath.check.times
  mspath.form.output mspath.process.covs mspath.results
  observed.mspath odds.mspath plot.mspath pmatrix.mspath
  pmatrix.piecewise.mspath prevalence.mspath print.mspath
  print.summary.mspath qematrix.diagse.formstr qematrix.diagse.mspath
  qematrix.mspath qmatrix.mspath qratio.mspath qratio.se.mspath
  sojourn.mspath statetable.mspath summary.mspath totlos.mspath
  transient.mspath viterbi.mspath
Undocumented data sets:
  e2 gold q2
All user-level objects in a package should have documentation entries.
See chapter 'Writing R documentation files' in manual 'Writing R
Extensions'.
* checking for code/documentation mismatches ... OK
* checking Rd \usage sections ... OK

Have I misunderstood how this is suppsed to work?  Have the previous
errors confused or voided the code/documentation test?  Is it a bug?

Using R 2.0.0-3 on Debian.
-- 
Ross Boylan                                      wk:  (415) 502-4031
530 Parnassus Avenue (Library) rm 115-4          ross at biostat.ucsf.edu
Dept of Epidemiology and Biostatistics           fax: (415) 476-9856
University of California, San Francisco
San Francisco, CA 94143-0840                     hm:  (415) 550-1062



From spencer.graves at pdf.com  Fri Nov  5 21:28:52 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 05 Nov 2004 12:28:52 -0800
Subject: [R] Problems running a 4-parameter Weibull function with nls
In-Reply-To: <001b01c4c367$ca855500$9ba76f82@CFRU0104>
References: <001b01c4c367$ca855500$9ba76f82@CFRU0104>
Message-ID: <418BE284.3060008@pdf.com>

      Might "nls" be testing values for "lag" that exceed min(X)?  That 
might produce the error you observe. 

      I routine reparameterize problems like this to send boundaries to 
Inf, e.g.: 

      x0 <- min(df$X)     
      fit<-nls(Y ~ max.*(1 - 
exp(-(rate*(X-x0+exp(ln.lag)))^shape)),df,start=start.est, x0=x0)
      ##NOT TESTED

      (Also, I try to avoid conficts between objects / variables I use 
and, e.g., standard system functions like "max" and "df".  R can 
determine whether a function or a non-function is required in most but 
not all contexts.) 

      hope this helps. 
      spencer graves

Mike Saunders wrote:

>Hi,
>
>I am rather new to R, but both myself and another much more experience user cannot figure this out.  I have a collegue who has a 800+ nonlinear regressions to run for seed germination (different species, treatments, etc.) over time.  I created a looping structure to extract the parameters from each regression; I will then use the parameters themselves for further analysis.  I would like to fit a 4-parameter, sigmoidal shape Weibull because it it paramatized to have a maximum germination rate (max), a time lag for the start of germination (lag), a germination rate (rate) and a shape parameter (shape).
>
>Here is a copy of this bit of the code:
>
>start.est=list(max=max(df$Y),rate=1/(df$X[tx]-((df$X[tz-1]+df$X[tz])/2)),
>    lag=(df$X[tz-1]+df$X[tz])/2,shape=1.1)
>nls.control(maxiter=1000,minFactor=1/8192)
>fit<-nls(Y ~ max*(1 - exp(-(rate*(X-lag))^shape)),df,start=start.est)
>
>
>Here is one column of the data (X=Julian day, Y = % germination):
>
>       X        Y
>1   111 0.0000000
>2   125 0.0000000
>3   131 0.0000000
>4   138 0.3076923
>5   145 0.4260355
>6   152 0.4733728
>7   159 0.5443787
>8   166 0.5680473
>9   173 0.5680473
>10  180 0.5917160
>11  187 0.5917160
>12  194 0.5917160
>13  201 0.6153846
>14  208 0.6153846
>15  215 0.6153846
>16  223 0.6153846
>17  229 0.6153846
>18  236 0.6153846
>19  245 0.6153846
> 
>Here is the error I keep getting:
>
>Error in numericDeriv(form[[3]], names(ind), env) : 
>        Missing value or an Infinity produced when evaluating the model
>
>
>I have tried these things to get this to work:
>    1) Mess with the starting values quite a bit
>    2) Seed the regression with linear estimates between my known points (i.e., figure         out daily averages).  I then wanted to regress on these to get starting estimates         to use with the real data.
>    3) Drop out all the 0s in the seeded data
>    4) Add jitter to the seeded data
>I should note that this worked to get a 3-parameter Gompertz to work, but that functional form is more difficult to interpret biologically.
>
>I am out of ideas.  Thoughts anyone?  I would appreciate any help.
>
>Mike Saunders
>Research Assistant
>Department of Forest Ecosystem Sciences
>University of Maine
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From jon at mcauliffe.com  Fri Nov  5 21:35:03 2004
From: jon at mcauliffe.com (Jon McAuliffe)
Date: Fri, 5 Nov 2004 12:35:03 -0800
Subject: [R] fast partial spectral decompositions.
Message-ID: <2C0E69E5-2F6A-11D9-AC45-000A95DA144C@mcauliffe.com>

hello,

i want to compute the top k eigenvalues+eigenvectors of a (large)
real symmetric matrix. since it doesn't look like any top-level R
function does this, i'll call LAPACK from a C shlib and then
use .Call. the only LAPACK function i see to do this in
R_ext/Lapack.h is dsyevx. however, i know that in LAPACK dsyevr
can also return a partial eigendecomposition. why is dsyevr not
exported in R_ext/Lapack.h? my superficial understanding is that
dsyevr is "better" (faster? stabler?) for both complete and
partial eigenproblems than dsyevd/dsyevx, but only the complete
eigenproblem interface to dsyevr appears to be exported in
Lapack.h (as dsyev).

corrections to misunderstandings in the above are welcome. advice
on whether using dsyevr rather than dsyevx is (very) important
for partial decompositions is also gratefully accepted.

please include jon at mcauliffe.com in the reply.

jon.



From MSchwartz at MedAnalytics.com  Fri Nov  5 21:50:15 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 05 Nov 2004 14:50:15 -0600
Subject: [R] text colour in legend
In-Reply-To: <opsgz9lxi3f57zr1@bio04.eva.mpg.de>
References: <opsgz9lxi3f57zr1@bio04.eva.mpg.de>
Message-ID: <1099687814.8211.510.camel@localhost.localdomain>

On Fri, 2004-11-05 at 12:57, Marcus Leinweber wrote:
> hello,
> 
> 
> the following example gives a plot with a legend:
> 
> 
> plot(-10:10,-10:10,type="n")
> x=1:10
> y=1:10
> tt=c("A","B","C","D","E","F")
> text(x,y,tt,cex=.8)
> legend(-10,10,paste(tt," ",x,y),text.col=c(2:4))  # each row in different  
> colour
> 
> but how can I colour the legend text by "column" meaning tt in red, x in  
> blue and y in green?
> 
> thanks for your help.
> 
> marcus


This is possible, using a bit of a trick with legend(). Instead of
printing each row in sequence, we print each column in sequence:

x <- 1:10
y <- 1:10
tt <- LETTERS[1:10]

plot(x, y, pch = tt)

# Now use legend. Modify the vector of
# text items here so that each set of 10
# elements are in sequence. Modify the color
# argument to be a sequence of 10 of each color
# set the 'ncol' argument to 3

legend(1, 10, c(tt, x, y), 
       text.col = rep(2:4, each = 10), 
       ncol = 3) 


HTH,

Marc Schwartz



From grimes at altaplana.com  Fri Nov  5 21:59:42 2004
From: grimes at altaplana.com (Seth Grimes)
Date: Fri, 5 Nov 2004 12:59:42 -0800 (PST)
Subject: [R] R usage -- for article
Message-ID: <Pine.LNX.4.50.0411051252110.21880-100000@whirlwind.he.net>

Hello all,

	I write a decision-support column for a computing magazine,
Intelligent Enterprise.  In my next column, I'll be revisiting a topic I
wrote on 3 years ago, open-source analytical software.  R is perhaps the
most successful open-source analytical package.

	I'd like to hear from users, especially those who chose R for some
other reason than that it's free, about what you're doing with R and how
you think it compares with other software and especially if you're
using/developing one of the graphical/spatial/Web extensions.  If you are
able to help, please respond by November 7 because of my deadline.

	Thanks,

					Seth



--
Seth Grimes   Alta Plana Corp, analytical computing & data management
              Intelligent Enterprise magazine (CMP), Contributing Editor
grimes at altaplana.com       http://altaplana.com    301-270-0795



From partha_bagchi at hgsi.com  Fri Nov  5 22:35:49 2004
From: partha_bagchi at hgsi.com (partha_bagchi@hgsi.com)
Date: Fri, 5 Nov 2004 16:35:49 -0500
Subject: [R] R usage -- for article
Message-ID: <OFC756F8A2.2E9CAA4D-ON85256F43.007669B3-85256F43.0076A2D3@hgsi.com>

For me, it is extremely easy to integrate R into a webserver where 
Scientists perform preprogrammed analyses and get the report they want. 
This integration with a web server I found to be not so easy with other 
software. Also, the graphical capabilities of R are unsurpassed.






Seth Grimes <grimes at altaplana.com>
Sent by: r-help-bounces at stat.math.ethz.ch
11/05/2004 03:59 PM

 
        To:     r-help at stat.math.ethz.ch
        cc: 
        Subject:        [R] R usage -- for article


Hello all,

I write a decision-support column for a computing magazine,
Intelligent Enterprise.  In my next column, I'll be revisiting a topic I
wrote on 3 years ago, open-source analytical software.  R is perhaps the
most successful open-source analytical package.

I'd like to hear from users, especially those who chose R for some
other reason than that it's free, about what you're doing with R and how
you think it compares with other software and especially if you're
using/developing one of the graphical/spatial/Web extensions.  If you are
able to help, please respond by November 7 because of my deadline.

Thanks,

Seth



--
Seth Grimes   Alta Plana Corp, analytical computing & data management
Intelligent Enterprise magazine (CMP), Contributing Editor
grimes at altaplana.com       http://altaplana.com    301-270-0795

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From john.maindonald at anu.edu.au  Sat Nov  6 00:36:54 2004
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Sat, 6 Nov 2004 10:36:54 +1100
Subject: [R] highly biased PCA data?
In-Reply-To: <200411051118.iA5BHtmk030134@hypatia.math.ethz.ch>
References: <200411051118.iA5BHtmk030134@hypatia.math.ethz.ch>
Message-ID: <93AC67AA-2F83-11D9-95A3-000A95CDA0F2@anu.edu.au>

I'd suggest you start by using lda() or qda() from MASS,
benefits being that

(a) if the frequencies in the sample do not reflect the frequencies
in the target population, you can set 'prior' to mirror the target
frequencies.  The issue is, perhaps, is your odd person odd in
a 1000 dog : 100 cat owners : 10 fish population, or odd, e.g., in
a 1000:1000:50 population?  You can also vary the prior to see
what the effect is.  If however you set a large prior probability for
a group that is poorly represented, results will be 'noisy'.  Note
the use of 'classwt' for the prior probablities for randomForest().

(b) You can plot second versus first discriminant function scores,
to get a direct graphical representation of results.
Other discrimination techniques may have to use an ordination
technique or even lds() or qds() on a >2 dimensional representation
of results, in order to get a scatterplot.
[cf MDSplot() for randomForest()]

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Bioinformation Science, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.


On 5 Nov 2004, at 10:18 PM, r-help-request at stat.math.ethz.ch wrote:

> From: Berton Gunter <gunter.berton at gene.com>
> Date: 5 November 2004 5:08:38 AM
> To: "'Dan Bolser'" <dmb at mrc-dunn.cam.ac.uk>, "'R-help'" 
> <r-help at stat.math.ethz.ch>
> Cc: Subject: RE: [R] highly biased PCA data?
>
> Dan:
>
> 1) There is no guarantee that PCA will show separate groups, of 
> course, as
> that is not its purpose, although it is frequently a side effect.
>
> 2) If you were to use a classification method of some sort 
> (discriminant
> analysis, neural nets, SVM's, model=based classification,  ...), my
> understanding is that yes, indeed, severely unbalanced group membership
> would, indeed, affect results. A guess is that Bayesian or other 
> methods
> that could explicitly model the prior membership probabilities would do
> better. To make it clear why, suppose that there was a 99.9% 
> preference of
> "dog" and .05% each of the others. Than your datasets would have 
> almost no
> information on how covariates could distinguish the classes and the 
> best
> classifier would be to call everything a "dog" no matter what values 
> the
> covariates had.
>
> I presume experts will have more and better to say about this.
>
> -- Bert Gunter
>
>
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dan Bolser
>> Sent: Thursday, November 04, 2004 9:41 AM
>> To: R mailing list
>> Subject: [R] highly biased PCA data?
>>
>> Hello, supposing that I have two or three clear categories
>> for my data, lets say pet preferece across fish, cat, dog. Lets say 
>> most
>> people rate their preference as being mostly one of the categories.
>>
>> I want to do pca on the data to see three 'groups' of people,
>> one group for fish, one for cat and one for dog. I would like to see
>> the odd person who likes both or all three in the (appropriate) 
>> middle of
>> the other main groups.
>>
>> Will my data be affected by the fact that I have interviewed 1000 dog
>> owners, 100 cat owners and 10 fish owners? (assuming that
>> each scale of preference has an equal range).
>>
>> Cheers,
>> dan.



From 0034058 at fudan.edu.cn  Sat Nov  6 02:03:43 2004
From: 0034058 at fudan.edu.cn (rongguiwong)
Date: Sat, 06 Nov 2004 09:03:43 +0800
Subject: [R] how to read this matrix into R
Message-ID: <200411060903.43959.0034058@fudan.edu.cn>

the following the the lower.tri matrix in a file named luxry.car
and i want to  read it in R as a lower.tri matrix.how can i do?
i have try to use help.search("read"),but no result what i want.


  1.000                                                         
  0.591  1.000                                                 
  0.356  0.350  1.000                                          
 -0.098  0.072  0.380  1.000                                   
  0.573  0.408  0.382  0.062  1.000                            
  0.156  0.232  0.517  0.424  0.303  1.000                     
  0.400  0.414  0.611  0.320  0.401  0.479  1.000              
  0.282  0.375  0.512  0.346  0.308  0.463  0.605  1.000       
  0.519  0.484  0.467  0.167  0.455  0.311  0.574  0.557  1.000



From spencer.graves at pdf.com  Sat Nov  6 02:19:45 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 05 Nov 2004 17:19:45 -0800
Subject: [R] how to read this matrix into R
In-Reply-To: <200411060903.43959.0034058@fudan.edu.cn>
References: <200411060903.43959.0034058@fudan.edu.cn>
Message-ID: <418C26B1.9010502@pdf.com>

      How about the following: 

 > a <- scan("clipboard")
Read 45 items
 > n2 <- length(a)
 > n <- (-1+sqrt(1+8*n2))/2
 > A <- array(NA, dim=c(n,n))
 > A[lower.tri(A,diag=TRUE)] <- a
 > A
        [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8] [,9]
 [1,]  1.000    NA    NA    NA    NA    NA    NA    NA   NA
 [2,]  0.591 1.000    NA    NA    NA    NA    NA    NA   NA
 [3,]  1.000 0.573 0.517    NA    NA    NA    NA    NA   NA
 [4,]  0.356 0.408 0.424 0.320    NA    NA    NA    NA   NA
 [5,]  0.350 0.382 0.303 0.401 0.512    NA    NA    NA   NA
 [6,]  1.000 0.062 1.000 0.479 0.346 1.000    NA    NA   NA
 [7,] -0.098 1.000 0.400 1.000 0.308 0.519 0.167    NA   NA
 [8,]  0.072 0.156 0.414 0.282 0.463 0.484 0.455 0.574   NA
 [9,]  0.380 0.232 0.611 0.375 0.605 0.467 0.311 0.557    1

      hope this helps.  spencer graves

rongguiwong wrote:

>the following the the lower.tri matrix in a file named luxry.car
>and i want to  read it in R as a lower.tri matrix.how can i do?
>i have try to use help.search("read"),but no result what i want.
>
>
>  1.000                                                         
>  0.591  1.000                                                 
>  0.356  0.350  1.000                                          
> -0.098  0.072  0.380  1.000                                   
>  0.573  0.408  0.382  0.062  1.000                            
>  0.156  0.232  0.517  0.424  0.303  1.000                     
>  0.400  0.414  0.611  0.320  0.401  0.479  1.000              
>  0.282  0.375  0.512  0.346  0.308  0.463  0.605  1.000       
>  0.519  0.484  0.467  0.167  0.455  0.311  0.574  0.557  1.000
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From p.dalgaard at biostat.ku.dk  Sat Nov  6 02:26:38 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 06 Nov 2004 02:26:38 +0100
Subject: [R] how to read this matrix into R
In-Reply-To: <200411060903.43959.0034058@fudan.edu.cn>
References: <200411060903.43959.0034058@fudan.edu.cn>
Message-ID: <x2654ju5rl.fsf@biostat.ku.dk>

rongguiwong <0034058 at fudan.edu.cn> writes:

> the following the the lower.tri matrix in a file named luxry.car
> and i want to  read it in R as a lower.tri matrix.how can i do?
> i have try to use help.search("read"),but no result what i want.

Here's one way:

> x <- scan()
1:   1.000
2:   0.591  1.000
4:   0.356  0.350  1.000
7:  -0.098  0.072  0.380  1.000
11:   0.573  0.408  0.382  0.062  1.000
16:   0.156  0.232  0.517  0.424  0.303  1.000
22:   0.400  0.414  0.611  0.320  0.401  0.479  1.000
29:   0.282  0.375  0.512  0.346  0.308  0.463  0.605  1.000
37:   0.519  0.484  0.467  0.167  0.455  0.311  0.574  0.557  1.000
46:
Read 45 items
> m <- matrix(0,9,9)
> m[upper.tri(m,diag=TRUE)] <- x # Yes, upper!
> m <- t(m)
> m[upper.tri(m,diag=TRUE)] <- x 

(Assuming that you really want a symmetric matrix. If you want just
the lower triangle, simply omit the last line.)


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ggrothendieck at myway.com  Sat Nov  6 03:12:41 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 6 Nov 2004 02:12:41 +0000 (UTC)
Subject: [R] how to read this matrix into R
References: <200411060903.43959.0034058@fudan.edu.cn>
Message-ID: <loom.20041106T025310-531@post.gmane.org>

rongguiwong <0034058 <at> fudan.edu.cn> writes:

: the following the the lower.tri matrix in a file named luxry.car
: and i want to  read it in R as a lower.tri matrix.how can i do?
: i have try to use help.search("read"),but no result what i want.
: 
:   1.000                                                         
:   0.591  1.000                                                 
:   0.356  0.350  1.000                                          
:  -0.098  0.072  0.380  1.000                                   
:   0.573  0.408  0.382  0.062  1.000                            
:   0.156  0.232  0.517  0.424  0.303  1.000                     
:   0.400  0.414  0.611  0.320  0.401  0.479  1.000              
:   0.282  0.375  0.512  0.346  0.308  0.463  0.605  1.000       
:   0.519  0.484  0.467  0.167  0.455  0.311  0.574  0.557  1.000

Here is one way:

  data.matrix(read.table(file.dat, fill = TRUE, col.names = 1:9))


If you don't know that that its 9x9 in advance then try this:

  n <- max(count.fields(file.dat))
  data.matrix(read.table(file.dat, fill = TRUE, col.names = 1:n))



From ales.ziberna at guest.arnes.si  Sat Nov  6 10:01:25 2004
From: ales.ziberna at guest.arnes.si (=?iso-8859-2?Q?Ale=B9_=AEiberna?=)
Date: Sat, 6 Nov 2004 10:01:25 +0100
Subject: [R] Using East-european characters in R
References: <4188C2E1.11752.C61961@localhost>
Message-ID: <008c01c4c3e0$95bf7660$0a09f9c2@ales>

Thanks to everyone for usful tips. I managed to solve the problem by 
corecting fonts in Rconsole and Rdevga!

Ales Ziberna



From dmb at mrc-dunn.cam.ac.uk  Sat Nov  6 12:12:22 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Sat, 6 Nov 2004 11:12:22 +0000 (GMT)
Subject: [R] graphics site
In-Reply-To: <5.1.0.14.2.20041105192034.01d46008@pop.univ-lyon1.fr>
Message-ID: <Pine.LNX.4.21.0411061057500.22746-100000@mail.mrc-dunn.cam.ac.uk>

On Fri, 5 Nov 2004, Pierre BADY wrote:

>hi,
>
>you can see these  links:
>
>http://pbil.univ-lyon1.fr/R/enseignement.html
>http://zoonek2.free.fr/UNIX/48_R/all.html
>http://www.ceremade.dauphine.fr/~xian/Noise.html
>http://statwww.epfl.ch/davison/teaching/ProbStat/20032004/PDF
>http://www.ulb.ac.be/di/map/gbonte/mod_stoch

Wow! Any translations / equivelents of these resources?

I also find ...

http://www.stat.wisc.edu/~deepayan/SIBS/otopics/base-graphics.html

Which has an exelent series of images (in a PDF) linked to source code on
the site.

Any site designed like a decision tree (classification of R graphics) with
images to click until you find the graphics you want?



>
>perhaps the site you a looking for is here ... ;o))
>
>
>hope this helps
>
>P.BADY
>
>At 09:24 05/11/2004 -0600, Michaell Taylor wrote:
>
>>About six months ago there was a reference to a site (in french) that
>>did a spectacular job of demonstrating R's graphical capabilities.
>>
>>My bookmarks were recently wiped and I cannot find this site despite my
>>best googling.
>>
>>Anyone have the address which I have done a miserable job describing?
>>
>>Thanks.
>>
>>Michaell
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>Pierre BADY             <)))))><
>Universit Claude Bernard Lyon 1
>UMR CNRS 5023, LEHF
>bat Alphonse Forel
>43 boulevard du 11 novembre 1918
>F-69622 VILLEURBANNE CEDEX
>FRANCE
>TEL : +33 (0)4 72 44 62 34
>FAX : +33 (0)4 72 43 28 92
>MEL : pierre.bady at univ-lyon1.fr
>http://limnologie.univ-lyon1.fr
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From hellik at web.de  Sat Nov  6 14:09:49 2004
From: hellik at web.de (Helmut Kudrnovsky)
Date: Sat, 06 Nov 2004 14:09:49 +0100
Subject: [R] text size + text-dendrogram
Message-ID: <5.1.0.14.0.20041106133455.009f57c0@pop3.web.de>

dear R-friends,

i performed a cluster analysis (diana in package cluster) with about 300 
samples.

two questions:

- if i plot a dendrogram of the analysis, i can??t read the labels of the 
samples, because the size of the label-text is too big and the label-text 
of  3 or 4 samples are mixed. so i can??t read which label belongs to which 
sample. i tried to adjust the text-size with the option cex (a numerical 
vector giving the amount by which plotting text and symbols should be 
scaled relative to the default - from the help for plot.default(graphics)), 
but it didn??t help. the problem exist also, when i export the plot into a 
meta-file or a postscript-file for further work with the dendrogram in an 
other application.

- is it possible to export or save the dendrogram from the R-shell in a 
kind of a text-dendrogramm like:

                                        I
                               ---------------------
                               I                    I
                           ---------            ----------
                           I        I            I         I
s...sample          s1     s2         s3      s4

os: win xp
R: 2.0.0.

with greetings from the snowy tyrol
helli



From francoisromain at free.fr  Sat Nov  6 14:42:36 2004
From: francoisromain at free.fr (=?ISO-8859-1?Q?Romain_Fran=E7ois?=)
Date: Sat, 06 Nov 2004 14:42:36 +0100
Subject: [R] text size + text-dendrogram
In-Reply-To: <5.1.0.14.0.20041106133455.009f57c0@pop3.web.de>
References: <5.1.0.14.0.20041106133455.009f57c0@pop3.web.de>
Message-ID: <418CD4CC.100@free.fr>

Helmut Kudrnovsky a ??crit :

> dear R-friends,
>
> i performed a cluster analysis (diana in package cluster) with about 
> 300 samples.
>
> two questions:
>
> - if i plot a dendrogram of the analysis, i can??t read the labels of 
> the samples, because the size of the label-text is too big and the 
> label-text of  3 or 4 samples are mixed. so i can??t read which label 
> belongs to which sample. i tried to adjust the text-size with the 
> option cex (a numerical vector giving the amount by which plotting 
> text and symbols should be scaled relative to the default - from the 
> help for plot.default(graphics)), but it didn??t help. the problem 
> exist also, when i export the plot into a meta-file or a 
> postscript-file for further work with the dendrogram in an other 
> application.
>
> - is it possible to export or save the dendrogram from the R-shell in 
> a kind of a text-dendrogramm like:
>
>                                        I
>                               ---------------------
>                               I                    I
>                           ---------            ----------
>                           I        I            I         I
> s...sample          s1     s2         s3      s4
>
from : ?dendrogram

hc <- hclust(dist(USArrests), "ave")
(dend1 <- as.dendrogram(hc)) # "print()" method
str(dend1)          # "str()" method
str(dend1, max = 2) # only the first two sub-levels


The str functions will give a text representation of your tree.

> os: win xp
> R: 2.0.0.
>
> with greetings from the snowy tyrol
> helli
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>

-- 
Romain Fran??ois
25, avenue Guy Moquet
94 400 Vitry sur seine
FRANCE
_______________________
_______________________

francoisromain at free.fr
01 46 80 65 60
06 18 39 14 69



From patrick.giraudoux at univ-fcomte.fr  Sat Nov  6 14:55:14 2004
From: patrick.giraudoux at univ-fcomte.fr (Patrick Giraudoux)
Date: Sat, 6 Nov 2004 14:55:14 +0100
Subject: [R] reading .Rprofile at startup in R 2.0.0
Message-ID: <000501c4c408$3f7c4df0$96570b50@PC728329681112>

Dear listers,

Moving from R 1.9.1 to R 2.0.0 today, it happens that the "traditional" .RProfile (located in my home directory: C:\Documents and 
Settings\Giraudoux) is not read at startup with R 2.0.

Any suggestion?

Patrick Giraudoux



From ligges at statistik.uni-dortmund.de  Sat Nov  6 14:57:39 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 06 Nov 2004 14:57:39 +0100
Subject: [R] Lda versus Rda
In-Reply-To: <418B9AC5.60502@curie.fr>
References: <418B9AC5.60502@curie.fr>
Message-ID: <418CD853.2020707@statistik.uni-dortmund.de>

Julien Trolet wrote:

> Hello,
> 
> I used the lda function from the MASS (VR) package and the rda function 
> from the klaR package.
> I wanted to compare the result of this two functions by using the same 
> training set.
> Thus, I used the rda function with lambda=1 an gamma=0, I should emulate 
> the lda function and I should obtain the same result.
> 
> But this it not the case, the two result are very different.
> 
> My training set is 70 observations * 10 variables long, and I performed 
> a leave one out for each  observations.
> 
> Do somebody have an idea for the cause(s) of this?


With the iris data, the following works for me:

  x1 <- predict(lda(Species~., data=iris))
  x2 <- predict(rda(Species~., data=iris, lambda=1, gamma=0))

  all(x1$class == x2$class)
  all.equal(x1$posterior, x2$posterior)

So, can you specify an example (including data + code, in a private 
message) please?

If your analysis for your data is correct, the error is probably in 
rda(). I won't have time to look at it before monday (and the author of 
the rda() code is in Auckland these days).

It's always a good idea to ask the package maintainer first, BTW.

Uwe Ligges




> Thanks
> 
> Trolet Julien



From ripley at stats.ox.ac.uk  Sat Nov  6 15:19:42 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 6 Nov 2004 14:19:42 +0000 (GMT)
Subject: [R] reading .Rprofile at startup in R 2.0.0
In-Reply-To: <000501c4c408$3f7c4df0$96570b50@PC728329681112>
Message-ID: <Pine.LNX.4.44.0411061414440.26999-100000@gannet.stats>

On Sat, 6 Nov 2004, Patrick Giraudoux wrote:

> Moving from R 1.9.1 to R 2.0.0 today, it happens that the "traditional"
> .RProfile (located in my home directory: C:\Documents and
> Settings\Giraudoux) is not read at startup with R 2.0.
> 
> Any suggestion?

Do read the CHANGES file!  Probably that is a result of

   Changed search order for user home directory to use "My Documents"
   directory after HOME and before HOMEDRIVE/HOMEPATH.

To be sure, read the rw-FAQ (as the posting guide asks) Q2.12 and check 
the advice there.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Mike.Prager at noaa.gov  Sat Nov  6 15:39:22 2004
From: Mike.Prager at noaa.gov (Mike Prager)
Date: Sat, 06 Nov 2004 09:39:22 -0500
Subject: [R] reading .Rprofile at startup in R 2.0.0
In-Reply-To: <000501c4c408$3f7c4df0$96570b50@PC728329681112>
References: <000501c4c408$3f7c4df0$96570b50@PC728329681112>
Message-ID: <6.1.2.0.0.20041106093734.01949ac0@hermes.nos.noaa.gov>

Consider setting the environment variable R_USER="c:\Documents and 
Settings\Giraudoux"

MHP

At 08:55 AM 11/06/2004, you wrote:
>Dear listers,
>
>Moving from R 1.9.1 to R 2.0.0 today, it happens that the "traditional" 
>.RProfile (located in my home directory: C:\Documents and 
>Settings\Giraudoux) is not read at startup with R 2.0.
>
>Any suggestion?
>
>Patrick Giraudoux


-- 
Michael Prager, Ph.D.                <Mike.Prager at noaa.gov>
NOAA Beaufort Laboratory
Beaufort, North Carolina  28516
http://shrimp.ccfhrb.noaa.gov/~mprager/
***



From patrick.giraudoux at univ-fcomte.fr  Sat Nov  6 16:38:05 2004
From: patrick.giraudoux at univ-fcomte.fr (H Patrick Giraudoux)
Date: Sat, 06 Nov 2004 16:38:05 +0100
Subject: [R] reading .Rprofile at startup in R 2.0.0
In-Reply-To: <6.1.2.0.0.20041106093734.01949ac0@hermes.nos.noaa.gov>
References: <000501c4c408$3f7c4df0$96570b50@PC728329681112>
	<000501c4c408$3f7c4df0$96570b50@PC728329681112>
Message-ID: <5.0.2.1.2.20041106162627.00bd4ae8@utinam.univ-fcomte.fr>

Thanks for the hint. Having got through the "CHANGES" file as advised by Pr 
Ripley (I unfortunately checked first the "NEWS" file and forgot that 
one..) I found it may be more simple (to me) to put the .RProfile file into 
the "my documents" directory. It works fine now. Setting environment 
variables is probably easy when one knows exactly which file to manage with 
and where it is... which is not the case to me yet (even after having read 
carefully the rw-FAQ 2.13).

All the best,

Patrick


A 09:39 06/11/2004 -0500, vous avez ??crit :
>Consider setting the environment variable R_USER="c:\Documents and 
>Settings\Giraudoux"
>
>MHP
>
>At 08:55 AM 11/06/2004, you wrote:
>>Dear listers,
>>
>>Moving from R 1.9.1 to R 2.0.0 today, it happens that the "traditional" 
>>.RProfile (located in my home directory: C:\Documents and 
>>Settings\Giraudoux) is not read at startup with R 2.0.
>>
>>Any suggestion?
>>
>>Patrick Giraudoux
>
>
>--
>Michael Prager, Ph.D.                <Mike.Prager at noaa.gov>
>NOAA Beaufort Laboratory
>Beaufort, North Carolina  28516
>http://shrimp.ccfhrb.noaa.gov/~mprager/
>***



From Mike.Prager at noaa.gov  Sat Nov  6 17:15:24 2004
From: Mike.Prager at noaa.gov (Mike Prager)
Date: Sat, 06 Nov 2004 11:15:24 -0500
Subject: [R] reading .Rprofile at startup in R 2.0.0
In-Reply-To: <5.0.2.1.2.20041106162627.00bd4ae8@utinam.univ-fcomte.fr>
References: <000501c4c408$3f7c4df0$96570b50@PC728329681112>
	<000501c4c408$3f7c4df0$96570b50@PC728329681112>
	<5.0.2.1.2.20041106162627.00bd4ae8@utinam.univ-fcomte.fr>
Message-ID: <6.1.2.0.0.20041106110812.01911358@hermes.nos.noaa.gov>

You wouldn't find info on setting environment variables in the R FAQ 
because environment variables are an operating system feature.  Under 
recent versions of Windows, environment variables are stored in the 
Registry and can be set persistently through a dialog. In Windows XP, this 
is found under My Computer; Properties; Advanced; Environment Variables. I 
would set a User, rather than System, variable; it matters only if more 
than one user on your PC is making use of R and they want different profiles.

As you say, unless you are familiar with this, it could be easier just to 
move the file. The advantage of using an environment variable is that you 
can put the file where YOU want it. To most folks, that is a non-issue. 
Some of us however are determined to bend computers to our wills, rather 
than the other way around. We frequently fail but keep trying.

Regards,
MHP

At 10:38 AM 11/06/2004, H Patrick Giraudoux wrote:
>Thanks for the hint. Having got through the "CHANGES" file as advised by 
>Pr Ripley (I unfortunately checked first the "NEWS" file and forgot that 
>one..) I found it may be more simple (to me) to put the .RProfile file 
>into the "my documents" directory. It works fine now. Setting environment 
>variables is probably easy when one knows exactly which file to manage 
>with and where it is... which is not the case to me yet (even after having 
>read carefully the rw-FAQ 2.13).
>
>All the best,
>
>Patrick
>
>
>A 09:39 06/11/2004 -0500, vous avez ??crit :
>>Consider setting the environment variable R_USER="c:\Documents and 
>>Settings\Giraudoux"
>>
>>MHP
>>
>>At 08:55 AM 11/06/2004, you wrote:
>>>Dear listers,
>>>
>>>Moving from R 1.9.1 to R 2.0.0 today, it happens that the "traditional" 
>>>.RProfile (located in my home directory: C:\Documents and 
>>>Settings\Giraudoux) is not read at startup with R 2.0.
>>>
>>>Any suggestion?
>>>
>>>Patrick Giraudoux
>>
>>
>>--
>>Michael Prager, Ph.D.                <Mike.Prager at noaa.gov>
>>NOAA Beaufort Laboratory
>>Beaufort, North Carolina  28516
>>http://shrimp.ccfhrb.noaa.gov/~mprager/
>>***
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Michael Prager, Ph.D.                <Mike.Prager at noaa.gov>
NOAA Beaufort Laboratory
Beaufort, North Carolina  28516
http://shrimp.ccfhrb.noaa.gov/~mprager/
***



From ripley at stats.ox.ac.uk  Sat Nov  6 18:02:29 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 6 Nov 2004 17:02:29 +0000 (GMT)
Subject: [R] reading .Rprofile at startup in R 2.0.0
In-Reply-To: <6.1.2.0.0.20041106110812.01911358@hermes.nos.noaa.gov>
Message-ID: <Pine.LNX.4.44.0411061656090.12149-100000@gannet.stats>

On Sat, 6 Nov 2004, Mike Prager wrote:

> You wouldn't find info on setting environment variables in the R FAQ 
> because environment variables are an operating system feature.  Under 

You _will_ find it in the rw-FAQ, as mentioned in the posting guide and by 
Patrick!

It is actually very easy, just add

HOME="c:\Documents and Settings\Giraudoux"

to the end of target in the shortcut you use to start R.  If that is too 
difficult, Mike Prager's comments are way too advanced.

PLEASE don't post inaccurate versions of information that is in the FAQs:  
at least become familiar with FAQ yourself and ask your readers to
familiarize themselves if you want to amplify the entries.

> recent versions of Windows, environment variables are stored in the 
> Registry and can be set persistently through a dialog. In Windows XP, this 
> is found under My Computer; Properties; Advanced; Environment Variables. I 
> would set a User, rather than System, variable; it matters only if more 
> than one user on your PC is making use of R and they want different profiles.
> 
> As you say, unless you are familiar with this, it could be easier just to 
> move the file. The advantage of using an environment variable is that you 
> can put the file where YOU want it. To most folks, that is a non-issue. 
> Some of us however are determined to bend computers to our wills, rather 
> than the other way around. We frequently fail but keep trying.
> 
> Regards,
> MHP
> 
> At 10:38 AM 11/06/2004, H Patrick Giraudoux wrote:
> >Thanks for the hint. Having got through the "CHANGES" file as advised by 
> >Pr Ripley (I unfortunately checked first the "NEWS" file and forgot that 
> >one..) I found it may be more simple (to me) to put the .RProfile file 
> >into the "my documents" directory. It works fine now. Setting environment 
> >variables is probably easy when one knows exactly which file to manage 
> >with and where it is... which is not the case to me yet (even after having 
> >read carefully the rw-FAQ 2.13).
> >
> >All the best,
> >
> >Patrick
> >
> >
> >A 09:39 06/11/2004 -0500, vous avez ??crit :
> >>Consider setting the environment variable R_USER="c:\Documents and 
> >>Settings\Giraudoux"
> >>
> >>MHP
> >>
> >>At 08:55 AM 11/06/2004, you wrote:
> >>>Dear listers,
> >>>
> >>>Moving from R 1.9.1 to R 2.0.0 today, it happens that the "traditional" 
> >>>.RProfile (located in my home directory: C:\Documents and 
> >>>Settings\Giraudoux) is not read at startup with R 2.0.
> >>>
> >>>Any suggestion?
> >>>
> >>>Patrick Giraudoux
> >>
> >>
> >>--
> >>Michael Prager, Ph.D.                <Mike.Prager at noaa.gov>
> >>NOAA Beaufort Laboratory
> >>Beaufort, North Carolina  28516
> >>http://shrimp.ccfhrb.noaa.gov/~mprager/
> >>***
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From lefevre-gallois at voila.fr  Sat Nov  6 19:06:03 2004
From: lefevre-gallois at voila.fr (lefevre-gallois)
Date: Sat,  6 Nov 2004 19:06:03 +0100 (CET)
Subject: [R] Just a little problem
Message-ID: <31072983.1099764363287.JavaMail.www@wwinf4001>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041106/0288669b/attachment.pl

From giljustino at yahoo.com.br  Sat Nov  6 20:07:20 2004
From: giljustino at yahoo.com.br (Gilvan)
Date: Sat, 6 Nov 2004 17:07:20 -0200
Subject: [R] Some Tips about developing an interface.
Message-ID: <200411061907.iA6J7r3q013100@hypatia.math.ethz.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041106/615cd06e/attachment.pl

From jfox at mcmaster.ca  Sat Nov  6 20:21:47 2004
From: jfox at mcmaster.ca (John Fox)
Date: Sat, 6 Nov 2004 14:21:47 -0500
Subject: [R] Some Tips about developing an interface.
In-Reply-To: <200411061907.iA6J7r3q013100@hypatia.math.ethz.ch>
Message-ID: <20041106192145.QMMY1536.tomts25-srv.bellnexxia.net@JohnDesktop8300>

Dear Gilvan,

See the R-GUI web site <http://www.sciviews.org/_rgui/> for a variety of
information.

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gilvan
> Sent: Saturday, November 06, 2004 2:07 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Some Tips about developing an interface.
> 
> Hi.
>  
> I just started studing R language and I saw there is a 
> package to develop user interface called tcltk. I read about 
> it and I made some tests but I found it a little poor and 
> slow.. I was thinking if is possible to write a user 
> interface using java and ask to R to run all the maths I have 
> and R returns all the results, including graphs.
> If someone has any suggestions to me I will appreciate, 
> including other ways to write a user interface.
>  
> Thank you very much and sorry for my bad english.
> 
> Gilvan
>  
>  
> 
> ---
> 
> 
> 
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From rab at nauticom.net  Sat Nov  6 21:48:20 2004
From: rab at nauticom.net (rab)
Date: Sat, 06 Nov 2004 15:48:20 -0500
Subject: [R] Installing Packages on an Athlon 64 Linux System
Message-ID: <418D3894.4050100@nauticom.net>

I installed FC 2 X86-64 on an Athlon 64 system. I then installed R 
2.0.0. It runs fine except when I try to install or update packages 
using either "install.packages" or "update.packages". I get the 
following types of errors:

gcc -I/usr/lib/R/include  -I/usr/local/include -D__NO_MATH_INLINES 
-mieee-fp  -fPIC  -O2 -g -pipe -march=i386 -mcpu=i686 -c block.c -o block.o
cc1: error: CPU you selected does not support x86-64 instruction set
cc1: error: CPU you selected does not support x86-64 instruction set
make: *** [block.o] Error 1
ERROR: compilation failed for package 'gstat'
** Removing '/usr/lib/R/library/gstat'
** Restoring previous '/usr/lib/R/library/gstat'
 
How can I fix this?

Rick B.



From Ted.Harding at nessie.mcc.ac.uk  Sat Nov  6 20:44:32 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 06 Nov 2004 20:44:32 +0100 (GMT)
Subject: [R] Just a little problem
In-Reply-To: <31072983.1099764363287.JavaMail.www@wwinf4001>
Message-ID: <XFMail.041106185147.Ted.Harding@nessie.mcc.ac.uk>

On 06-Nov-04 lefevre-gallois wrote:
> Dear sir, Dear Madam
> I begin with R and I couldn't find in the manuels how I can solve this
> this problem 
> find n such as |1/ni*sum(Wi)|<0.2
> with Wi<-c(rnorm(200,0,1))
> Thanks for helping
> Best regards

Not sure that I understand the problem.

If ni is a single number (i.e. not involved in the summation)
then

  ni <- 5/abs(sum(Wi))

gives an lower bound for ni; if ni is to be an integer then

  ni <- floor(5/abs(sum(Wi)))+1

Since there is the (rare) possibility that 5/abs(sum(Wi)) may
be an exact integer, and you want "<" rather than "<=", in that
case you will need (ni + 1) rather than ni. So test the answer:

  ni <- 5/abs(sum(Wi))
  if(floor(ni) == ni) {ni <- floor(ni)+1} else {ni <- floor)ni)}

However, your use of "ni" and "Wi" suggests that you might be
thinking of "i" as a summation index, with each of {ni} and {Wi}
being a series of numbers, perhaps really meaning

  |1/sum(ni*Wi)| < 0.2

in which case the problem would not have a definite solution
unless you state restrictions on the series {ni}.

Hoping this helps,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 06-Nov-04                                       Time: 18:51:47
------------------------------ XFMail ------------------------------



From p.dalgaard at biostat.ku.dk  Sat Nov  6 22:32:32 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 06 Nov 2004 22:32:32 +0100
Subject: [R] Installing Packages on an Athlon 64 Linux System
In-Reply-To: <418D3894.4050100@nauticom.net>
References: <418D3894.4050100@nauticom.net>
Message-ID: <x2vfcid5ov.fsf@biostat.ku.dk>

rab <rab at nauticom.net> writes:

> I installed FC 2 X86-64 on an Athlon 64 system. I then installed R
> 2.0.0. It runs fine except when I try to install or update packages
> using either "install.packages" or "update.packages". I get the
> following types of errors:
> 
> gcc -I/usr/lib/R/include  -I/usr/local/include -D__NO_MATH_INLINES
> -mieee-fp  -fPIC  -O2 -g -pipe -march=i386 -mcpu=i686 -c block.c -o
> block.o

> cc1: error: CPU you selected does not support x86-64 instruction set
> cc1: error: CPU you selected does not support x86-64 instruction set

Well it doesn't. So what is -march=i386 -mcpu=i686 doing in the
CFLAGS?

Sounds like you are using a 386-compiled R in compatibility mode and
are getting bitten by the fact that your gcc is not a cross compiler.
Did you use Martyn's RPMs? If so, I'd try a local rebuild (look up the
docs for rpmbuild) of R from the SRPM.

Another option might be that you need to set up a 32 bit build
environment. I don't know FC3 but on SuSE you have the linux32 prefix
to configure &c. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From takahama at andrew.cmu.edu  Sat Nov  6 22:51:56 2004
From: takahama at andrew.cmu.edu (Satoshi Takahama)
Date: Sat, 6 Nov 2004 16:51:56 -0500 (EST)
Subject: [R] calendar-based time-series in R
Message-ID: <Pine.LNX.4.60-041.0411061634130.18248@unix40.andrew.cmu.edu>

Hello,

I am trying to switch to R from S-PLUS 6.1, and one problem I am having is 
using R for manipulation of calendar-based time-series. In S-PLUS, I 
commonly use the functions timeSequence(), timeDate(), and timeSeries() to 
align/average/aggregate data; and I also do a lot of plotting of 
time-series data (with calendar-based labels on the x-axis).

I was wondering if anyone is familiar with the S-PLUS functions and ways 
to perform the same tasks I mentioned above in R. From what I can gather 
in the documentation and previous help messages , the 'its' package can 
provide the same functionalities, but I am not clear on the functions and 
structure of the data that would correspond to those in S-PLUS. If I am 
incorrect in my assessment of the 'its' library, I would appreciate any 
advice on effective ways in which calendar-based time-series can be 
treated in R.

Thank you very much,
Satoshi
___
Satoshi Takahama
Carnegie Mellon University
Dept. of Chemical Engineering
Pittsburgh, PA 15213



From andy_liaw at merck.com  Sat Nov  6 23:44:41 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 6 Nov 2004 17:44:41 -0500
Subject: [R] Installing Packages on an Athlon 64 Linux System
Message-ID: <3A822319EB35174CA3714066D590DCD50994E292@usrymx25.merck.com>

On our SLES8 for amd64 installation, GCC 3.2.x (forgot what x is) is the
default, and does not support -mcpu=k8.  GCC 3.3.x is installed in
/opt/gcc33, and does support that flag.

Andy

> From: Peter Dalgaard
> 
> rab <rab at nauticom.net> writes:
> 
> > I installed FC 2 X86-64 on an Athlon 64 system. I then installed R
> > 2.0.0. It runs fine except when I try to install or update packages
> > using either "install.packages" or "update.packages". I get the
> > following types of errors:
> > 
> > gcc -I/usr/lib/R/include  -I/usr/local/include -D__NO_MATH_INLINES
> > -mieee-fp  -fPIC  -O2 -g -pipe -march=i386 -mcpu=i686 -c block.c -o
> > block.o
> 
> > cc1: error: CPU you selected does not support x86-64 instruction set
> > cc1: error: CPU you selected does not support x86-64 instruction set
> 
> Well it doesn't. So what is -march=i386 -mcpu=i686 doing in the
> CFLAGS?
> 
> Sounds like you are using a 386-compiled R in compatibility mode and
> are getting bitten by the fact that your gcc is not a cross compiler.
> Did you use Martyn's RPMs? If so, I'd try a local rebuild (look up the
> docs for rpmbuild) of R from the SRPM.
> 
> Another option might be that you need to set up a 32 bit build
> environment. I don't know FC3 but on SuSE you have the linux32 prefix
> to configure &c. 
> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: 
> (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: 
> (+45) 35327907
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From kjhealy at arizona.edu  Sat Nov  6 23:54:12 2004
From: kjhealy at arizona.edu (Kieran Healy)
Date: Sat, 6 Nov 2004 15:54:12 -0700
Subject: [R] basic bwplot query
Message-ID: <C6E6958F-3046-11D9-9E6C-000393A6C440@arizona.edu>

Hi -

I have some data consisting of a number of observations within each of 
15 countries. Each country falls into one of two groups. I'd like to 
use the lattice library's bwplot to present boxplots of the 
country-level data, with a separate panel for each group, but showing 
only the relevant countries in each panel.  Here's an analogous example 
using the "singer" data provided with the lattice. Say we assign the 
voice parts to groups, "M" and "F":

tmp <- rep("F",128)
tmp2 <- rep("M",length(129:nrow(singer)))
sex <- c(tmp,tmp2)

We'd like a two-panel figure showing the voice part boxplots for groups 
"M" and "F", but each panel should list only  those voice parts falling 
in its group. This way doesn't quite work:

bwplot(voice.part ~ height | sex, data=singer, xlab="Height 
(inches)",layout=c(1,2))

How can I have rows belonging to the "F" group just not appear at all 
in the "M" panel and vice versa? I guess this is quite straightforward, 
but I'm going about it in the wrong --- do I need to provide some extra 
information about subscripts via a panel function? I'd be grateful for 
any advice.

Thanks,

Kieran

--
Kieran Healy, http://www.u.arizona.edu/~kjhealy
Assistant Professor, Sociology Dept, University of Arizona.



From deepayan at stat.wisc.edu  Sun Nov  7 00:27:28 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Sat, 6 Nov 2004 17:27:28 -0600
Subject: [R] basic bwplot query
In-Reply-To: <C6E6958F-3046-11D9-9E6C-000393A6C440@arizona.edu>
References: <C6E6958F-3046-11D9-9E6C-000393A6C440@arizona.edu>
Message-ID: <200411061727.28905.deepayan@stat.wisc.edu>

On Saturday 06 November 2004 16:54, Kieran Healy wrote:
> Hi -
>
> I have some data consisting of a number of observations within each
> of 15 countries. Each country falls into one of two groups. I'd like
> to use the lattice library's bwplot to present boxplots of the
> country-level data, with a separate panel for each group, but showing
> only the relevant countries in each panel.  Here's an analogous
> example using the "singer" data provided with the lattice. Say we
> assign the voice parts to groups, "M" and "F":
>
> tmp <- rep("F",128)
> tmp2 <- rep("M",length(129:nrow(singer)))
> sex <- c(tmp,tmp2)
>
> We'd like a two-panel figure showing the voice part boxplots for
> groups "M" and "F", but each panel should list only  those voice
> parts falling in its group. This way doesn't quite work:
>
> bwplot(voice.part ~ height | sex, data=singer, xlab="Height
> (inches)",layout=c(1,2))
>
> How can I have rows belonging to the "F" group just not appear at all
> in the "M" panel and vice versa? I guess this is quite
> straightforward, but I'm going about it in the wrong --- do I need to
> provide some extra information about subscripts via a panel function?
> I'd be grateful for any advice.

Try

bwplot(voice.part ~ height | sex, data=singer,
       xlab="Height (inches)",layout=c(1,2),
       scales = list(y = list(relation = "free")))

Note that this will only work in R 2.0.0 or better.

Deepayan



From saiwing at saiwing.net  Sun Nov  7 01:54:44 2004
From: saiwing at saiwing.net (Saiwing Yeung)
Date: Sat, 06 Nov 2004 16:54:44 -0800
Subject: [R] rgl on Mac OS
Message-ID: <BDB2B254.2D6A%saiwing@saiwing.net>

Hi,

It seems like a number of people on this list can install rgl but have
problem loading it. I found myself in the same situation too.

I have tried the workaround of removing /usr/X11R6/lib from
DYLD_LIBRARY_PATH, but it doesn't seem to work for me, I am still getting
the same error (that everyone else seems to get). Can anyone give me some
ideas on what else to try?

I have Mac OS 10.3.5, running R2.0. Thanks in advance!

Saiwing



From ggrothendieck at myway.com  Sun Nov  7 02:20:17 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 7 Nov 2004 01:20:17 +0000 (UTC)
Subject: [R] calendar-based time-series in R
References: <Pine.LNX.4.60-041.0411061634130.18248@unix40.andrew.cmu.edu>
Message-ID: <loom.20041107T020759-326@post.gmane.org>

Satoshi Takahama <takahama <at> andrew.cmu.edu> writes:

: 
: Hello,
: 
: I am trying to switch to R from S-PLUS 6.1, and one problem I am having is 
: using R for manipulation of calendar-based time-series. In S-PLUS, I 
: commonly use the functions timeSequence(), timeDate(), and timeSeries() to 
: align/average/aggregate data; and I also do a lot of plotting of 
: time-series data (with calendar-based labels on the x-axis).
: 
: I was wondering if anyone is familiar with the S-PLUS functions and ways 
: to perform the same tasks I mentioned above in R. From what I can gather 
: in the documentation and previous help messages , the 'its' package can 
: provide the same functionalities, but I am not clear on the functions and 
: structure of the data that would correspond to those in S-PLUS. If I am 
: incorrect in my assessment of the 'its' library, I would appreciate any 
: advice on effective ways in which calendar-based time-series can be 
: treated in R.
: 

If you need regular time series such as monthly time series
you can use the ts class in the base of R.  If you need
irregular time series such as time series for business days
then there are four irregular time series classes and packages:

   irts is in package tseries
   its is in package its
   zoo is in package zoo
   timeSeries is in package fBasics (which is part 
                    of Rmetrics, www.rmetrics.org)

I don't have too much experience with irts.  Regarding the
others:

- its is probably the most mature.  It is based on S4 and likely has
  sufficient features for real applications.  A new maintainer
  recently took it over from the original developer.

- zoo is the most consistent with base R as most functions are
  just methods which extend base generics. It allows the user to 
  choose which date class to use whereas the others are hard coded 
  to use POSIXct.  It is based on S3.   zoo is being actively 
  developed.  Its command set includes plot and aggregate.  zoo is 
  also used in the strucchange package.  (I am involved in the 
  development of zoo so I know zoo the best.)

- fBasics/Rmetrics is closest to S and has a particularly
  wide range of functionality.  Be aware that you must set 
  the time zone on your computer to GMT to use it.  It is 
  being actively developed.  It is based on S4.  I believe that it
  has only been tested on Windows (is that right?)  You can 
  get a quick overview of what it has by looking at:
  http://www.itp.phys.ethz.ch/econophysics/R/pdf/DocRefcard.pdf

Hope that helps.



From ripley at stats.ox.ac.uk  Sun Nov  7 08:29:17 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 7 Nov 2004 07:29:17 +0000 (GMT)
Subject: [R] Installing Packages on an Athlon 64 Linux System
In-Reply-To: <418D3894.4050100@nauticom.net>
Message-ID: <Pine.LNX.4.44.0411070725580.16936-100000@gannet.stats>

On Sat, 6 Nov 2004, rab wrote:

> I installed FC 2 X86-64 on an Athlon 64 system. I then installed R 
> 2.0.0. It runs fine except when I try to install or update packages 
> using either "install.packages" or "update.packages". I get the 
> following types of errors:
> 
> gcc -I/usr/lib/R/include  -I/usr/local/include -D__NO_MATH_INLINES 
> -mieee-fp  -fPIC  -O2 -g -pipe -march=i386 -mcpu=i686 -c block.c -o block.o
> cc1: error: CPU you selected does not support x86-64 instruction set
> cc1: error: CPU you selected does not support x86-64 instruction set
> make: *** [block.o] Error 1
> ERROR: compilation failed for package 'gstat'
> ** Removing '/usr/lib/R/library/gstat'
> ** Restoring previous '/usr/lib/R/library/gstat'
>  
> How can I fix this?

How did you get  -pipe -march=i386 -mcpu=i686?  That was not put there by 
R, and is the problem.  If you did not build R from the sources on that 
machine, please do so -- if this was a binary install it is for the wrong 
architecture.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Sun Nov  7 08:36:09 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 7 Nov 2004 07:36:09 +0000 (GMT)
Subject: [R] Installing Packages on an Athlon 64 Linux System
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E292@usrymx25.merck.com>
Message-ID: <Pine.LNX.4.44.0411070730220.16936-100000@gannet.stats>

But this is FC2, not FC3 (not released until tomorrow) and not SuSE.  gcc
3.3.3 and 64-bit builds are the default on FC2, and R build out of the 
box (unsuprising as it is one of our standard test platforms).

I have offered Martyn an RPM for FC2 on x86_64, but not had a reply.

On Sat, 6 Nov 2004, Liaw, Andy wrote:

> On our SLES8 for amd64 installation, GCC 3.2.x (forgot what x is) is the
> default, and does not support -mcpu=k8.  GCC 3.3.x is installed in
> /opt/gcc33, and does support that flag.
> 
> Andy
> 
> > From: Peter Dalgaard
> > 
> > rab <rab at nauticom.net> writes:
> > 
> > > I installed FC 2 X86-64 on an Athlon 64 system. I then installed R
> > > 2.0.0. It runs fine except when I try to install or update packages
> > > using either "install.packages" or "update.packages". I get the
> > > following types of errors:
> > > 
> > > gcc -I/usr/lib/R/include  -I/usr/local/include -D__NO_MATH_INLINES
> > > -mieee-fp  -fPIC  -O2 -g -pipe -march=i386 -mcpu=i686 -c block.c -o
> > > block.o
> > 
> > > cc1: error: CPU you selected does not support x86-64 instruction set
> > > cc1: error: CPU you selected does not support x86-64 instruction set
> > 
> > Well it doesn't. So what is -march=i386 -mcpu=i686 doing in the
> > CFLAGS?
> > 
> > Sounds like you are using a 386-compiled R in compatibility mode and
> > are getting bitten by the fact that your gcc is not a cross compiler.
> > Did you use Martyn's RPMs? If so, I'd try a local rebuild (look up the
> > docs for rpmbuild) of R from the SRPM.
> > 
> > Another option might be that you need to set up a 32 bit build
> > environment. I don't know FC3 but on SuSE you have the linux32 prefix
> > to configure &c. 
> > 
> > -- 
> >    O__  ---- Peter Dalgaard             Blegdamsvej 3  
> >   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
> >  (*) \(*) -- University of Copenhagen   Denmark      Ph: 
> > (+45) 35327918
> > ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: 
> > (+45) 35327907
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From yiuyiu7 at yahoo.com  Sun Nov  7 10:57:14 2004
From: yiuyiu7 at yahoo.com (Brian Chu)
Date: Sun, 7 Nov 2004 17:57:14 +0800 (CST)
Subject: [R] Asking about a program
Message-ID: <20041107095714.32332.qmail@web51501.mail.yahoo.com>

Hi,
 
    I would like to ask that in my program, I would like to estimate the Ripley's K-function and the root-mean-squared error using different approaches (torus, border,isotropic,Ripley and translate).  When I run this program in R, R gives me this sentence:  Error in hist.default(x, breaks = breaks, plot = FALSE, probability = FALSE) : 
some `x' not counted; maybe `breaks' do not span range of `x'.
 
    I would like to know what is the problem and how this can be solved. The attached file is the program. Thank you for your kind attention.
 
Best Wishes,
Brian (a student from Hong Kong)


LB@B_^H...
jU

-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: 5K.txt
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041107/9051031c/5K.txt

From tom_woody at swissinfo.org  Sun Nov  7 11:42:33 2004
From: tom_woody at swissinfo.org (=?ISO-8859-15?Q?Thomas_Sch=F6nhoff?=)
Date: Sun, 07 Nov 2004 11:42:33 +0100
Subject: [R] Using GNU R on a two box "cluster"
Message-ID: <418DFC19.4000905@swissinfo.org>

Hello,

right now I'm thinking about running R 2.0.0 on box A (Debian SID) but 
at the same time having access to the ressources of box B (Ubuntu 
Linux) regarding disk capacity, RAM, idle CPU cycles . Is there anyone 
of you that has already installed and administred such a tiny (home 
based) cluster?

Well, the term cluster seems quite a bit of an exaggeration related to 
what I am looking for!

I am absolutely new to this subject and I would appreciate some hints 
on where to start doing such things with my GNU R on both boxes. Maybe 
a pointer too some introductory materials focussed to doing clustering 
stuff with GNU R would be sufficient.
Are there any recommended methods or tools to realize a two nodes cluster?


regards

Thomas



From dmb at mrc-dunn.cam.ac.uk  Sun Nov  7 12:45:17 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Sun, 7 Nov 2004 11:45:17 +0000 (GMT)
Subject: [R] Using GNU R on a two box "cluster"
In-Reply-To: <418DFC19.4000905@swissinfo.org>
Message-ID: <Pine.LNX.4.21.0411071138550.32406-100000@mail.mrc-dunn.cam.ac.uk>

On Sun, 7 Nov 2004, [ISO-8859-15] Thomas Schnhoff wrote:

>Hello,
>
>right now I'm thinking about running R 2.0.0 on box A (Debian SID) but 
>at the same time having access to the ressources of box B (Ubuntu 
>Linux) regarding disk capacity, RAM, idle CPU cycles . Is there anyone 
>of you that has already installed and administred such a tiny (home 
>based) cluster?
>
>Well, the term cluster seems quite a bit of an exaggeration related to 
>what I am looking for!
>
>I am absolutely new to this subject and I would appreciate some hints 
>on where to start doing such things with my GNU R on both boxes. Maybe 
>a pointer too some introductory materials focussed to doing clustering 
>stuff with GNU R would be sufficient.
>Are there any recommended methods or tools to realize a two nodes cluster?

The absolute *best* source of cluster information anywhere on the web
(subjectively speaking) is the bioclusters mailing list...

http://bioinformatics.org/lists/bioclusters

The list has a surfeit of cluster experts who make a living from
consultancy. 


>
>
>regards
>
>Thomas
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From patrick.giraudoux at univ-fcomte.fr  Sun Nov  7 15:34:21 2004
From: patrick.giraudoux at univ-fcomte.fr (H Patrick Giraudoux)
Date: Sun, 07 Nov 2004 15:34:21 +0100
Subject: [R] writing a simple package in R 2.0 under Windows XP
Message-ID: <5.0.2.1.2.20041107144917.00bbf320@utinam.univ-fcomte.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041107/f65fd926/attachment.pl

From tfliao at uiuc.edu  Sun Nov  7 15:50:25 2004
From: tfliao at uiuc.edu (Tim F Liao)
Date: Sun, 7 Nov 2004 08:50:25 -0600
Subject: [R] Calling Other (non-C or Fortran) Programs
 from R
Message-ID: <36ad1919.9b8d12c4.81a0700@expms6.cites.uiuc.edu>

Hi!

I wonder if anyone has experiences of calling other programs 
from R (i.e., not C or Fortran programs).

Specifically I want to call LEM from R and execute it in a 
loop to process its output in R.  Thanks,

Tim Liao



From ggrothendieck at myway.com  Sun Nov  7 15:52:04 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 7 Nov 2004 14:52:04 +0000 (UTC)
Subject: [R] writing a simple package in R 2.0 under Windows XP
References: <5.0.2.1.2.20041107144917.00bbf320@utinam.univ-fcomte.fr>
Message-ID: <loom.20041107T155030-593@post.gmane.org>

H Patrick Giraudoux <patrick.giraudoux <at> univ-fcomte.fr> writes:

: 
: Dear listers,
: 
: I have developped a set of functions that I would like to package on a 
: Windows XP plateform for some friends (this would be more simple than to 
: deliver them as a source text file without handy help). I am working under 
: Windows XP.
: 
: Of course I have gone through the manual "Writing R extension" and try to 
: sort out what a most simple "packaging" for beginner (without compiled 
: code, etc...)  could be. I also read 
: http://www.rap.ucar.edu/staff/ericg/RWinBuild.html It is very easy to build 
: R packages in Windows (for Windows)...
: 
: Perl has been installed and works fine C:\Perl (source 
: http://www.activestate.com/Products/ActivePerl/Download.html).
: R Tools have been installed: C:\Perl\Rtools (source: 
: http://www.murdoch-sutherland.com/Rtools/tools.zip)
: R tools functions delivered work fine (ex: ls, etc...)
: R has been installed "at the root" of C:\ to avoid any blank in directory 
: names: C:\R\rw2000
: The PATH has been defined as environment variable in Windows XP as: 
: C:\Perl\Rtools;C:\Perl\bin;...;C:\R\rw2000\bin
: 
: A trial package named "AnExample" has been prepared as indicated in the 
: example of package.skeleton() of the library "utils" and put in C:\
: 
: from there I have typewritten:
: 
: C:\>RCMD build AnExample
: 
: With this result:
: 
: Can't open perl script "C:\R\rw2000/bin/build": No such file or directory
: 
: I could catch that indeed 'build.exe' does not exist in this directory 
: (which is true...) and tried to find it somewhere on my c: disk. 
: Unfortunately, this 'build' or 'build.exe' file does not exist nor a 
: 'check' file, nor any complementary command of Rcmd
: 
: After some research hours (actually since yesterday...), I cannot find 
: where I have got wrong yet... and considering the result, I am sure I got 
: somewhere!
: 
: Can somebody help me on this?
: 

There have been relevant discussions on r-devel just this weekend.
Have a look at those archives 

   http://news.gmane.org/gmane.comp.lang.r.devel

and this post in particular:

   http://article.gmane.org/gmane.comp.lang.r.devel/1653



From ripley at stats.ox.ac.uk  Sun Nov  7 16:00:40 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 7 Nov 2004 15:00:40 +0000 (GMT)
Subject: [R] writing a simple package in R 2.0 under Windows XP
In-Reply-To: <5.0.2.1.2.20041107144917.00bbf320@utinam.univ-fcomte.fr>
Message-ID: <Pine.LNX.4.44.0411071450580.26896-100000@gannet.stats>

On Sun, 7 Nov 2004, H Patrick Giraudoux wrote:

> Dear listers,
> 
> I have developped a set of functions that I would like to package on a 
> Windows XP plateform for some friends (this would be more simple than to 
> deliver them as a source text file without handy help). I am working under 
> Windows XP.
> 
> Of course I have gone through the manual "Writing R extension" and try to 
> sort out what a most simple "packaging" for beginner (without compiled 
> code, etc...)  could be. I also read 
> http://www.rap.ucar.edu/staff/ericg/RWinBuild.html It is very easy to build 
> R packages in Windows (for Windows)...
> 
> Perl has been installed and works fine C:\Perl (source 
> http://www.activestate.com/Products/ActivePerl/Download.html).
> R Tools have been installed: C:\Perl\Rtools (source: 
> http://www.murdoch-sutherland.com/Rtools/tools.zip)
> R tools functions delivered work fine (ex: ls, etc...)
> R has been installed "at the root" of C:\ to avoid any blank in directory 
> names: C:\R\rw2000
> The PATH has been defined as environment variable in Windows XP as: 
> C:\Perl\Rtools;C:\Perl\bin;...;C:\R\rw2000\bin
> 
> A trial package named "AnExample" has been prepared as indicated in the 
> example of package.skeleton() of the library "utils" and put in C:\
> 
> from there I have typewritten:
> 
> C:\>RCMD build AnExample

Did you want to package up the sources of a package you haven't tested?
Surely you wanted to install and test it first?

> With this result:
> 
> Can't open perl script "C:\R\rw2000/bin/build": No such file or directory

A Perl script is not `build.exe': this really is just 'build'.

> I could catch that indeed 'build.exe' does not exist in this directory 
> (which is true...) and tried to find it somewhere on my c: disk. 
> Unfortunately, this 'build' or 'build.exe' file does not exist nor a 
> 'check' file, nor any complementary command of Rcmd
> 
> After some research hours (actually since yesterday...), I cannot find 
> where I have got wrong yet... and considering the result, I am sure I got 
> somewhere!

You apparently DEselected the installation of the source package tools 
when you installed R.  You need to go back and install them.

> Can somebody help me on this?
> 
> Thanks in advance,
> 
> Patrick Giraudoux
> 
> 
> 	[[alternative HTML version deleted]]

You apparently missed the comment about HTML mail in the posting guide.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jfox at mcmaster.ca  Sun Nov  7 16:24:20 2004
From: jfox at mcmaster.ca (John Fox)
Date: Sun, 7 Nov 2004 10:24:20 -0500
Subject: [R] writing a simple package in R 2.0 under Windows XP
In-Reply-To: <5.0.2.1.2.20041107144917.00bbf320@utinam.univ-fcomte.fr>
Message-ID: <20041107152419.MBTB29162.tomts5-srv.bellnexxia.net@JohnDesktop8300>

Dear Patrick,

I've prepared some basic instructions and a batch file for building simple
packages under Windows and have given them to several people who have
experienced problems. I've hesitated to send them to this list since they
really just duplicate information available elsewhere, and because the batch
file probably could be improved.

Since there has been a rash of Windows package-building problems recently
(probably because it's no longer possible to bypass the package-building
tools), I've appended the instructions and batch file to this message in the
hope that they prove useful. Of course, comments and suggestions for
improvement are appreciated.

Regards,
 John

-------------- snip ------------

Building Simple Packages Under R for Windows

1. Links to tools and additional information are at
<http://www.murdoch-sutherland.com/Rtools/>.

2. Make sure R is *not* installed under c:\Program Files (or in any location
with spaces in the path) and that it is installed with package-building
tools. I use c:\R for the installation; I'll assume this below -- make
changes as necessary. 

3. Download <http://www.murdoch-sutherland.com/Rtools/tools.zip> and unzip
e.g. to c:\Program Files\Utilities; put this directory at the beginning of
the path. (It may be necessary to copy sh.exe to c:\bin\sh.exe.)

4. Download Perl from
<http://www.activestate.com/Products/ActivePerl/Download.html> and install
it using defaults.

5. Download fptex from
<http://www.ctan.org/tex-archive/systems/win32/fptex/current/TeXSetup.exe>
and install it. Note: This requires a fast internet connection during the
installation. Don't install latex under c:\Program Files\!

6. Download HTML Help Workshop from
<http://msdn.microsoft.com/library/default.asp?url=/library/en-us/htmlhelp/h
tml/hwmicrosofthtmlhelpdownloads.asp> and install it. Add c:\Program
Files\HTML Help Workshop to the path.

7. If you want to be able to compile old-style Windows help files (probably
not necessary) download Microsoft Help Workshop
<ftp://ftp.microsoft.com/softlib/mslfiles/hcwsetup.exe> and install it. Add
c:\Program Files\Help Workshop to the path.

8. Put my file make-package.bat in c:\R\rwxxxx\src (where xxxx is the
version, e.g., 2000). If necessary, edit this file to reflect the location
of the R installation. 

9. Open a DOS (command) window. CD to R\rwxxxx\src. Make sure that the
package source files are in the directory package-name under
R\rwxxx\src\library. Enter make-package package-name. Carefully examine the
log file etc. 

10. After this process is completed, you'll have both a tar.gz file with the
source package and a .zip file with the Windows binary package. Install the
latter from the "Packages -> Install package(s) from local zip files" menu
in the normal manner. 

-------------- make-package.bat ------------

cd c:\R\rw2000\src\library
del %1\INDEX
del %1\data\00Index
del %1\chm\*.* /Q

..\..\bin\R CMD build --force --binary --auto-zip %1
..\..\bin\R CMD build --force %1
..\..\bin\R CMD check %1

cd %1.Rcheck
dvipdfm %1-manual

notepad 00check.log

---------------------------------------------

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of H 
> Patrick Giraudoux
> Sent: Sunday, November 07, 2004 9:34 AM
> To: r-help
> Subject: [R] writing a simple package in R 2.0 under Windows XP
> 
> Dear listers,
> 
> I have developped a set of functions that I would like to 
> package on a Windows XP plateform for some friends (this 
> would be more simple than to deliver them as a source text 
> file without handy help). I am working under Windows XP.
> 
> Of course I have gone through the manual "Writing R 
> extension" and try to sort out what a most simple "packaging" 
> for beginner (without compiled code, etc...)  could be. I 
> also read http://www.rap.ucar.edu/staff/ericg/RWinBuild.html 
> It is very easy to build R packages in Windows (for Windows)...
> 
> Perl has been installed and works fine C:\Perl (source 
> http://www.activestate.com/Products/ActivePerl/Download.html).
> R Tools have been installed: C:\Perl\Rtools (source: 
> http://www.murdoch-sutherland.com/Rtools/tools.zip)
> R tools functions delivered work fine (ex: ls, etc...) R has 
> been installed "at the root" of C:\ to avoid any blank in directory
> names: C:\R\rw2000
> The PATH has been defined as environment variable in Windows XP as: 
> C:\Perl\Rtools;C:\Perl\bin;...;C:\R\rw2000\bin
> 
> A trial package named "AnExample" has been prepared as 
> indicated in the example of package.skeleton() of the library 
> "utils" and put in C:\
> 
> from there I have typewritten:
> 
> C:\>RCMD build AnExample
> 
> With this result:
> 
> Can't open perl script "C:\R\rw2000/bin/build": No such file 
> or directory
> 
> I could catch that indeed 'build.exe' does not exist in this 
> directory (which is true...) and tried to find it somewhere 
> on my c: disk. 
> Unfortunately, this 'build' or 'build.exe' file does not 
> exist nor a 'check' file, nor any complementary command of Rcmd
> 
> After some research hours (actually since yesterday...), I 
> cannot find where I have got wrong yet... and considering the 
> result, I am sure I got somewhere!
> 
> Can somebody help me on this?
> 
> Thanks in advance,
> 
> Patrick Giraudoux
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From otnetobr at yahoo.com.br  Sun Nov  7 16:25:41 2004
From: otnetobr at yahoo.com.br (O. Neto)
Date: Sun, 7 Nov 2004 13:25:41 -0200
Subject: [R] Problem with dyn.load()
Message-ID: <001301c4c4de$1ba99960$b25962c8@otneto>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041107/c1c452a6/attachment.pl

From ripley at stats.ox.ac.uk  Sun Nov  7 16:54:16 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 7 Nov 2004 15:54:16 +0000 (GMT)
Subject: [R] Problem with dyn.load()
In-Reply-To: <001301c4c4de$1ba99960$b25962c8@otneto>
Message-ID: <Pine.LNX.4.44.0411071544090.27116-100000@gannet.stats>

Please do read the help file before posting. as the posting guide asks.
?dyn.load says

       x: a character string giving the pathname to a shared library or
          DLL.

Your DLL is "teste.dll", not "teste": you probably do have a file "teste" 
that is not a DLL or you would get a more informative error message.

That's not a mistake I have ever seen reported on R-help.

On Sun, 7 Nov 2004, O. Neto wrote:

> Hi!
> 
> I am studying C language to run with R 2.0.0. My system (Windows ME /BR) is configured to run RCMD ( I have installed ActivePerl, Rtools and MinGW as indicated in http://www.murdoch-sutherland.com/Rtools/ an with correct path).
> I would like run the coded write below named conv.c (Example from "Write R Extension") :
> 
>     #include <R.h>
>      #include <Rinternals.h>
>      SEXP convolve2(SEXP a, SEXP b)
>      {
>        R_len_t i, j, na, nb, nab;
>        double *xa, *xb, *xab;
>        SEXP ab;
>      
>        PROTECT(a = coerceVector(a, REALSXP));
>        PROTECT(b = coerceVector(b, REALSXP));
>        na = length(a); nb = length(b); nab = na + nb - 1;
>        PROTECT(ab = allocVector(REALSXP, nab));
>        xa = REAL(a); xb = REAL(b);
>        xab = REAL(ab);
>        for(i = 0; i < nab; i++) xab[i] = 0.0;
>        for(i = 0; i < na; i++)
>          for(j = 0; j < nb; j++) xab[i + j] += xa[i] * xb[j];
>        UNPROTECT(3);
>        return(ab);
>      }
> 
>     When I use C:\Arquiv~1\R\rw2000\bin>RCMD SHLIB -o
> C:/dev-cpp/teste.dll c:/dev-cpp/conv.c a teste.dll is created without
> error and located in that directory, but when I use it in RGui with
> "Change dir" set to C:/dev-cpp :
> 
> >dyn.load("teste")  results:
> Error in dyn.load(x, as.logical(local), as.logical(now)) : 
>         unable to load shared library "C:/Dev-Cpp/teste":
>   LoadLibrary failure:  Par??metro incorreto. (Incorrect Parameter)
> 
>     I tried find help in R-help, but none answers my questions.

>     My doubts are: Why does it occur? How do I fix it? How I can include
> directives in RCMD -SHLIB, for example, conv.def file to mangling names?
> It is editing Makevars.win?

> If anybody have time to answer my question, I apreciate.

> 
> Thanks,
> 
> O. Neto
> 
> PS: There is any discussion list of R in portuguese? I can't find one.
> 
> 
> 
>      
> 	[[alternative HTML version deleted]]

Please do note what the posting guide says about this!

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at myway.com  Sun Nov  7 16:56:58 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 7 Nov 2004 15:56:58 +0000 (UTC)
Subject: [R] writing a simple package in R 2.0 under Windows XP
References: <5.0.2.1.2.20041107144917.00bbf320@utinam.univ-fcomte.fr>
	<20041107152419.MBTB29162.tomts5-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <loom.20041107T163028-259@post.gmane.org>

John Fox <jfox <at> mcmaster.ca> writes:

> 
> Building Simple Packages Under R for Windows
> 
 
> 2. Make sure R is *not* installed under c:\Program Files (or in any location
> with spaces in the path) and that it is installed with package-building
> tools. I use c:\R for the installation; I'll assume this below -- make
> changes as necessary. 

I just have been through building packages with XP this weekend.
I used the default R installation which installs R 2.0.1 beta under 

   C:\Program Files\R\rw2001beta

and had no problems with using that pathname so I believe this warning 
is no longer needed, at least with XP and R 2.0.1.  That would 
slightly simplify installation since it allows one to use the 
default installation.

Uwe pointed out to me that one should use forward slashes rather
than backward slashes on path names when using R CMD ... .
I think you have avoided the need for pathnames in your script
altogether but thought I would mention it since it was a significant 
gotcha for me.

> 
> 5. Download fptex from
> <http://www.ctan.org/tex-archive/systems/win32/fptex/current/TeXSetup.exe>
> and install it. Note: This requires a fast internet connection during the
> installation. Don't install latex under c:\Program Files\!

I used MiKTeX which is a pain since there is some configuration
to be done to use it with R although it works well once MiKTeX 
is so configured.  The murdoch link you posted describes the 
configuration steps.  Someone else also 
mentioned to me privately I should have tried fptex and would
have avoided these configuration problems.  (It occurred to me
that if the Rd.sty file could be added to the MiKTeX repository
online where MiKTeX gets its other .sty files then it could be 
pulled out from there and the MiKTeX configuration would not have 
to be modified to access the R directory tree.)

> 
> ..\..\bin\R CMD build --force --binary --auto-zip %1
> ..\..\bin\R CMD build --force %1
> ..\..\bin\R CMD check %1
> 

It was also pointed out to me that --force
is an important switch; however, I did not use it and did
not notice any problems.  I also did not use --auto-zip.
Any comments on the importance of these?

Andy advised me to first use:

   R CMD install ...

to get my software going and that advice seemed particularly useful 
since it allowed me to install, run and test my software as a package 
in R so that I could get it in better shape before submitting it to 
R CMD check ... and R CMD build ... --binary .



From edd at debian.org  Sun Nov  7 19:04:36 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 7 Nov 2004 12:04:36 -0600
Subject: [R] Using GNU R on a two box "cluster"
In-Reply-To: <418DFC19.4000905@swissinfo.org>
References: <418DFC19.4000905@swissinfo.org>
Message-ID: <20041107180436.GA832@sonny.eddelbuettel.com>

On Sun, Nov 07, 2004 at 11:42:33AM +0100, Thomas Sch?nhoff wrote:
> right now I'm thinking about running R 2.0.0 on box A (Debian SID) but 
> at the same time having access to the ressources of box B (Ubuntu 
> Linux) regarding disk capacity, RAM, idle CPU cycles . Is there anyone 
> of you that has already installed and administred such a tiny (home 
> based) cluster?
[...]
> Are there any recommended methods or tools to realize a two nodes cluster?

The best place to start is probably the high-level snow package which does
all clustering in userspace -- i.e. it'll work on your machines just by
pulling in the packages it needs. Moreover, as both of your machines are
Debian-based, all you need should 

$ apt-get install r-cran-snow

modulo maybe deciding which communications protocol you want to use (pvm or
lam/mpi).

Another approach is to burn a dvd from the Quantian iso images (see
http://dirk.eddelbuettel.com/quantian/) and boot one machine with it (note
that there are also some tricks that help you do that without burning a dvd,
but that is still the best way to start).  You can then boot a second
machine rather easily by starting the openMosix Terminalserver that is part
of Quantian --- and you second machine can be booted off the first one
already running Quantian (provided is can boot via PXE, many recent BIOS
support this).  That gives you a two-node cluster with openMosix, and you
can start, say, two R sessions with lengthy simulations, and one should
migrate automagically to other box.   

Hth, Dirk

-- 
If your hair is standing up, then you are in extreme danger.
      -- http://www.usafa.af.mil/dfp/cockpit-phys/fp1ex3.htm



From murdoch at stats.uwo.ca  Sun Nov  7 19:40:43 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 07 Nov 2004 13:40:43 -0500
Subject: [R] Calling Other (non-C or Fortran) Programs from R
In-Reply-To: <36ad1919.9b8d12c4.81a0700@expms6.cites.uiuc.edu>
References: <36ad1919.9b8d12c4.81a0700@expms6.cites.uiuc.edu>
Message-ID: <bsqso011q7702i4vkvos7imbqi92cj66gl@4ax.com>

On Sun, 7 Nov 2004 08:50:25 -0600, Tim F Liao <tfliao at uiuc.edu> wrote:

>Hi!
>
>I wonder if anyone has experiences of calling other programs 
>from R (i.e., not C or Fortran programs).
>
>Specifically I want to call LEM from R and execute it in a 
>loop to process its output in R.  Thanks,

I don't know what LEM is.  If it's a language that can produce DLLs or
.so's, then it should be straightforward to call them.  If it's a
program using standard input and output, the system() function is what
you want.

Duncan Murdoch



From ripley at stats.ox.ac.uk  Sun Nov  7 19:57:52 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 7 Nov 2004 18:57:52 +0000 (GMT)
Subject: [R] Calling Other (non-C or Fortran) Programs from R
In-Reply-To: <bsqso011q7702i4vkvos7imbqi92cj66gl@4ax.com>
Message-ID: <Pine.LNX.4.44.0411071855350.27579-100000@gannet.stats>

On Sun, 7 Nov 2004, Duncan Murdoch wrote:

> On Sun, 7 Nov 2004 08:50:25 -0600, Tim F Liao <tfliao at uiuc.edu> wrote:
> 
> >Hi!
> >
> >I wonder if anyone has experiences of calling other programs 
> >from R (i.e., not C or Fortran programs).
> >
> >Specifically I want to call LEM from R and execute it in a 
> >loop to process its output in R.  Thanks,
> 
> I don't know what LEM is.  If it's a language that can produce DLLs or
> .so's, then it should be straightforward to call them.  If it's a
> program using standard input and output, the system() function is what
> you want.

Nor do, I, but my guess is that it is the program referred to at

http://ourworld.compuserve.com/homepages/jsuebersax/soft.htm#LEM

Perhaps Tim F Liao can enlighten us.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tom_woody at swissinfo.org  Sun Nov  7 20:10:04 2004
From: tom_woody at swissinfo.org (=?ISO-8859-1?Q?Thomas_Sch=F6nhoff?=)
Date: Sun, 07 Nov 2004 20:10:04 +0100
Subject: [R] Using GNU R on a two box "cluster"
In-Reply-To: <20041107180436.GA832@sonny.eddelbuettel.com>
References: <418DFC19.4000905@swissinfo.org>
	<20041107180436.GA832@sonny.eddelbuettel.com>
Message-ID: <418E730C.3080302@swissinfo.org>

Hello,

Dirk Eddelbuettel schrieb:
> On Sun, Nov 07, 2004 at 11:42:33AM +0100, Thomas Sch?nhoff wrote:
> 
>>right now I'm thinking about running R 2.0.0 on box A (Debian SID) but 
>>at the same time having access to the ressources of box B (Ubuntu 
>>Linux) regarding disk capacity, RAM, idle CPU cycles . Is there anyone 
>>of you that has already installed and administred such a tiny (home 
>>based) cluster?
> 
> [...]
> 
>>Are there any recommended methods or tools to realize a two nodes cluster?
> 
> 
> The best place to start is probably the high-level snow package which does
> all clustering in userspace -- i.e. it'll work on your machines just by
> pulling in the packages it needs. Moreover, as both of your machines are
> Debian-based, all you need should 
> 
> $ apt-get install r-cran-snow
> 
> modulo maybe deciding which communications protocol you want to use (pvm or
> lam/mpi).
> 
> Another approach is to burn a dvd from the Quantian iso images (see
> http://dirk.eddelbuettel.com/quantian/) and boot one machine with it (note
> that there are also some tricks that help you do that without burning a dvd,
> but that is still the best way to start).  You can then boot a second
> machine rather easily by starting the openMosix Terminalserver that is part
> of Quantian --- and you second machine can be booted off the first one
> already running Quantian (provided is can boot via PXE, many recent BIOS
> support this).  That gives you a two-node cluster with openMosix, and you
> can start, say, two R sessions with lengthy simulations, and one should
> migrate automagically to other box.   

Thanks to you and Dan for response, seems like a lot to be done ;-)


sincerely

Thomas



From patrick.giraudoux at univ-fcomte.fr  Sun Nov  7 20:58:46 2004
From: patrick.giraudoux at univ-fcomte.fr (H Patrick Giraudoux)
Date: Sun, 07 Nov 2004 20:58:46 +0100
Subject: [R] writing a simple package in R 2.0 under Windows XP
In-Reply-To: <20041107152419.MBTB29162.tomts5-srv.bellnexxia.net@JohnDes
	ktop8300>
References: <5.0.2.1.2.20041107144917.00bbf320@utinam.univ-fcomte.fr>
Message-ID: <5.0.2.1.2.20041107204216.00bc9250@utinam.univ-fcomte.fr>


Dear all,

Following John Fox and Brian Ripley instructions, things come better.  I 
still get errors but they just come at the first step from unappropriate 
DESCRIPTION file (which was expected: I just wanted to check if the 
software installation was OK for building packages with the "crude" 
AnExample from the package.skeleton() doc. I can "quitely" come back to the 
"Writing R extension guide" and work on it now).

One mishaps that can be worthy to bold for other listers is that I did not 
select the installation of the source package tools in an early 
installation of R (it is unselected by default with rw2000.exe). Advised to 
re-install R taking this into account for those who gradually try to move 
from function development to package writing...

Thanks for the hints kindly provided even a Sunday,

Patrick Giraudoux



A 10:24 07/11/2004 -0500, vous avez ??crit :
>Dear Patrick,
>
>I've prepared some basic instructions and a batch file for building simple
>packages under Windows and have given them to several people who have
>experienced problems. I've hesitated to send them to this list since they
>really just duplicate information available elsewhere, and because the batch
>file probably could be improved.
>
>Since there has been a rash of Windows package-building problems recently
>(probably because it's no longer possible to bypass the package-building
>tools), I've appended the instructions and batch file to this message in the
>hope that they prove useful. Of course, comments and suggestions for
>improvement are appreciated.
>
>Regards,
>  John
>
>-------------- snip ------------
>
>Building Simple Packages Under R for Windows
>
>1. Links to tools and additional information are at
><http://www.murdoch-sutherland.com/Rtools/>.
>
>2. Make sure R is *not* installed under c:\Program Files (or in any location
>with spaces in the path) and that it is installed with package-building
>tools. I use c:\R for the installation; I'll assume this below -- make
>changes as necessary.
>
>3. Download <http://www.murdoch-sutherland.com/Rtools/tools.zip> and unzip
>e.g. to c:\Program Files\Utilities; put this directory at the beginning of
>the path. (It may be necessary to copy sh.exe to c:\bin\sh.exe.)
>
>4. Download Perl from
><http://www.activestate.com/Products/ActivePerl/Download.html> and install
>it using defaults.
>
>5. Download fptex from
><http://www.ctan.org/tex-archive/systems/win32/fptex/current/TeXSetup.exe>
>and install it. Note: This requires a fast internet connection during the
>installation. Don't install latex under c:\Program Files\!
>
>6. Download HTML Help Workshop from
><http://msdn.microsoft.com/library/default.asp?url=/library/en-us/htmlhelp/h
>tml/hwmicrosofthtmlhelpdownloads.asp> and install it. Add c:\Program
>Files\HTML Help Workshop to the path.
>
>7. If you want to be able to compile old-style Windows help files (probably
>not necessary) download Microsoft Help Workshop
><ftp://ftp.microsoft.com/softlib/mslfiles/hcwsetup.exe> and install it. Add
>c:\Program Files\Help Workshop to the path.
>
>8. Put my file make-package.bat in c:\R\rwxxxx\src (where xxxx is the
>version, e.g., 2000). If necessary, edit this file to reflect the location
>of the R installation.
>
>9. Open a DOS (command) window. CD to R\rwxxxx\src. Make sure that the
>package source files are in the directory package-name under
>R\rwxxx\src\library. Enter make-package package-name. Carefully examine the
>log file etc.
>
>10. After this process is completed, you'll have both a tar.gz file with the
>source package and a .zip file with the Windows binary package. Install the
>latter from the "Packages -> Install package(s) from local zip files" menu
>in the normal manner.
>
>-------------- make-package.bat ------------
>
>cd c:\R\rw2000\src\library
>del %1\INDEX
>del %1\data\00Index
>del %1\chm\*.* /Q
>
>..\..\bin\R CMD build --force --binary --auto-zip %1
>..\..\bin\R CMD build --force %1
>..\..\bin\R CMD check %1
>
>cd %1.Rcheck
>dvipdfm %1-manual
>
>notepad 00check.log



From scott at nopdesign.com  Sun Nov  7 21:19:15 2004
From: scott at nopdesign.com (scott@nopdesign.com)
Date: Sun, 7 Nov 2004 13:19:15 -0700
Subject: [R] Re: diga
Message-ID: <200411072019.iA7KJFMR013614@hosting.cyberitas.com>



I'm sorry, but because I have been receiving more than 200 junk-mail
messages an hour, I no longer accept e-mail from unregistered senders.
To send me a message, please follow the path that best describes your needs:

Business Contacts
-----------------
  1. Register as a valid sender by visiting http://www.nopdesign.com/register/
  2. Re-send your e-mail


FreeCart Support
----------------
  1. Post a message to the support forum at http://www.nopdesign.com/forum/
  2. Send a private message through the support forums


Friends and Family
------------------
  1. Register as friends & family by visiting http://www.nopdesign.com/family/
  2. Re-send your e-mail


I'm sorry for the inconvenience, however, there was simply no way to keep up
with the volume of junk-mail I receive, even with the freeware or commercial
spam blocking tools.  Once you register, you will not need to register again,
unless you change your e-mail address.

If you didn't send me a message, there is a good chance someone you know
has an email virus or your email has been targetted by spammers.  After the 
3rd notice, this system will automatically and permanently ban your email, 
and never bother you again.  I received your email as:

   From: r-help at hypatia.math.ethz.ch
   Subject: diga

Thank you,

   Scott Moore
   NOP Design



From otnetobr at yahoo.com.br  Sun Nov  7 21:42:06 2004
From: otnetobr at yahoo.com.br (O. Neto)
Date: Sun, 7 Nov 2004 18:42:06 -0200
Subject: [R] Problem with dyn.load()
References: <Pine.LNX.4.44.0411071544090.27116-100000@gannet.stats>
Message-ID: <000d01c4c50a$411a3d40$7e5c62c8@otneto>

Prof. Ripley,

    I would, initially, thank you for help me.
    I did what you did say, I called that dll ( by "teste.dll" as wrote in
?dyn.load). It did not work.
    I make a new dll for testing and again, it didn??t work, the same message
appears. I don??t know how I fix it.  I have no idea.
I??ll read all "writing R extension" again. I believe that my mistake is in
config  MKRules.
    Thank you again for spend your time with my doubts. I??m grateful.

O. Neto



From ssim at lic.co.nz  Sun Nov  7 23:21:46 2004
From: ssim at lic.co.nz (ssim@lic.co.nz)
Date: Mon, 8 Nov 2004 11:21:46 +1300
Subject: [R] Re : QTL interval mapping for outbred population
Message-ID: <OF9C0BB301.AF10F87A-ONCC256F45.007AA687-CC256F45.007AD7AE@livestock.org.nz>

Dear lists,

Is there an add-on package in R for QTL interval mapping for outbred
population, eg. Haley-Knott regression method ?

Tanx
Stella

___________________________________________________________________________
This message, including attachments, is confidential. If you are not the
intended recipient, please contact us as soon as possible and then destroy
the message. Do not copy, disclose or use the contents in any way.

The recipient should check this email and any attachments for viruses and
other defects. Livestock Improvement Corporation Limited and any of its
subsidiaries and associates are not responsible for the consequences of any
virus, data corruption, interception or unauthorised amendments to this
email.

Because of the many uncertainties of email transmission we cannot guarantee
that a reply to this email will be received even if correctly sent. Unless
specifically stated to the contrary, this email does not designate an
information system for the purposes of section 11(a) of the New Zealand
Electronic Transactions Act 2002.



From erich.neuwirth at univie.ac.at  Mon Nov  8 00:45:47 2004
From: erich.neuwirth at univie.ac.at (Erich Neuwirth)
Date: Mon, 08 Nov 2004 00:45:47 +0100
Subject: [R] boxplot in extreme cases
Message-ID: <418EB3AB.6070204@univie.ac.at>

I noticed the following:
the 2 datasets
rep(c(0,1,2),c(10,20,40)) and
rep(c(0,1,2),c(10,40,20))
produce identical boxplots despite the fact that the medians are 
different. The reason is that the median in one case coincides with the
first quartile, and in the second case with the third quartile.
Is there a recommended way of displaying the median visibly in these 
cases? Setting notch=TRUE displays the median, but does look strange.



From andy_liaw at merck.com  Mon Nov  8 00:59:21 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sun, 7 Nov 2004 18:59:21 -0500
Subject: [R] boxplot in extreme cases
Message-ID: <3A822319EB35174CA3714066D590DCD50994E29C@usrymx25.merck.com>

Try:

x <- list(x1=rep(c(0,1,2),c(10,20,40)), x2=rep(c(0,1,2),c(10,40,20)))
boxplot(x, pars=list(medpch=20, medcex=3))

(Cf ?bxp, pointed to from ?boxplot.)

Andy

> From: Erich Neuwirth
> 
> I noticed the following:
> the 2 datasets
> rep(c(0,1,2),c(10,20,40)) and
> rep(c(0,1,2),c(10,40,20))
> produce identical boxplots despite the fact that the medians are 
> different. The reason is that the median in one case 
> coincides with the
> first quartile, and in the second case with the third quartile.
> Is there a recommended way of displaying the median visibly in these 
> cases? Setting notch=TRUE displays the median, but does look strange.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ihaka at stat.auckland.ac.nz  Mon Nov  8 02:01:26 2004
From: ihaka at stat.auckland.ac.nz (Ross Ihaka)
Date: Mon, 08 Nov 2004 14:01:26 +1300
Subject: [R] Suggested color schemes for points, not regions?
In-Reply-To: <3058.66.185.0.209.1099498019.squirrel@66.185.0.209>
References: <3058.66.185.0.209.1099498019.squirrel@66.185.0.209>
Message-ID: <418EC566.3010708@stat.auckland.ac.nz>

kwright at eskimo.com wrote:
> I have seen a couple of posts about color schemes like those at the
> ColorBrewer site.  Most recently:
> http://geography.uoregon.edu/datagraphics/color_scales.htm
> 
> These color schemes can work very well for regions (bars, polygons,
> images, etc.) but are not very suitable for points and/or lines.
> 
> Is anyone aware of research/suggestions for a color scheme to use for
> scatter plots?  I've looked at great length and have found little on this
> topic.
> 
> My current scheme of choice is a set of fairly saturated colors along the
> lines of:
> navy
> brown/orange
> black
> purple
> red
> medium green
> This is similar to the 'paired' color scheme, but using only the saturated
> colors and substituting black for yellow.  Depending on circumstances, I
> sometimes use a different glyph for each color.  The hard part about all
> this is to make sure that each color/glyph combination has the same
> 'attention-getting' power.
> 
> Any discussion or comments are welcome.
> 
> Kevin Wright

First a warning.  The use of ColorBrewer type color schemes is
inappropriate for many statistical displays -- eg. barplots
piecharts, mosaic plots.  The problem is that the colors in
the schemes vary a lot in luminance and there is a size illusion
associated with luminance variation.  (See Cleveland and McGill
(1983). "A Color-Caused Optical Illusion on a Statistical Graph,"
The American Statistician, 37:2 101-105.)  Varying luminance
in graphs which represent values as length or area can distort
the perception of the encoded values.

For line and glyph colors the size illusion is not as much
of a problem and it's probably best to concentrate on visibility.
The ISO 9241 standard recommends a luminance difference of at
least 3:1 and preferably 10:1 between text and its background.
Something similar probably applies here, and that severely limits
the color choices available (you need quite dark colors on
a white background).

One way around this is to draw a black border around
the line or glyph.  You can do this for glyphs by using a pch
value between 21 and 25 and using bg= in the base graphics or
fill= in grid.  Alternatively you can roll your own with
polygon (we need a better alternative to this).  For lines
you can draw them twice, superimposing a colored line on top
of a black one - e.g.

	lines(x,y,lwd=3)
	lines(x,y,lwd=1,col="yellow")

There are simultaneous contrast issues with this, but mucking
about with the lwd values will generally get you a reasonable
result.

I think this is a pretty interesting area and there is an
opportunity for someone to look at the perceptual questions
involved.

-- 
Ross Ihaka                         Email:  ihaka at stat.auckland.ac.nz
Department of Statistics           Phone:  (64-9) 373-7599 x 85054
University of Auckland             Fax:    (64-9) 373-7018
Private Bag 92019, Auckland
New Zealand



From gokim19 at hotmail.com  Mon Nov  8 04:19:53 2004
From: gokim19 at hotmail.com (Kim Fai Wong)
Date: Sun, 07 Nov 2004 22:19:53 -0500
Subject: [R] Matrix Indexing
Message-ID: <BAY14-F31KhBxT18j7n000268ba@hotmail.com>

Hi,

I have the following problem.
In a csv file I have under column A, the date, and column B, the prices.
Thus, for example, the file looks something like this ->

1/31/04      2.5
2/1/04        2.6
...
4/12/04       3.5

Basically,  I use the function inputframe = read.csv( )
which reads the csv file into the inputframe.

My question is, how can I make a function that has, as inputs, start_date 
and end_date,
so that the inputframe would not return all of these prices but only the 
ones during the
period I input?

For example, if I input 3/1/04 and 3/11/04, the inputframe only contains the 
prices for
this period.

Any help is appreciated!!

Kim



From ggrothendieck at myway.com  Mon Nov  8 05:38:50 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 8 Nov 2004 04:38:50 +0000 (UTC)
Subject: [R] Matrix Indexing
References: <BAY14-F31KhBxT18j7n000268ba@hotmail.com>
Message-ID: <loom.20041108T053534-412@post.gmane.org>

Kim Fai Wong <gokim19 <at> hotmail.com> writes:

: 
: Hi,
: 
: I have the following problem.
: In a csv file I have under column A, the date, and column B, the prices.
: Thus, for example, the file looks something like this ->
: 
: 1/31/04      2.5
: 2/1/04        2.6
: ...
: 4/12/04       3.5
: 
: Basically,  I use the function inputframe = read.csv( )
: which reads the csv file into the inputframe.
: 
: My question is, how can I make a function that has, as inputs, start_date 
: and end_date,
: so that the inputframe would not return all of these prices but only the 
: ones during the
: period I input?
: 
: For example, if I input 3/1/04 and 3/11/04, the inputframe only contains the 
: prices for
: this period.
: 
: Any help is appreciated!!
: 


Note that read.csv produces data frames, not matrices.  Lets
assume your data frame is called DF.

First make sure your A column is actually stored as some sort
of date object and not as a factor or character string.  You may
need something like this if its not.  (We are using chron here
since m/d/y is its default format so its particularly 
consistent with your data.):

	library(chron)
	DF$A <- chron(as.character(DF$A))

Now use subset:

	DF2 <- subset(DF, A >= chron("3/1/04") & A <= chron("3/11/04"))

You could alternately use Date class.  See R News 4/1 for an
article about all this.



From ripley at stats.ox.ac.uk  Mon Nov  8 08:21:02 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 8 Nov 2004 07:21:02 +0000 (GMT)
Subject: [R] selecting by date (was Matrix Indexing)
In-Reply-To: <BAY14-F31KhBxT18j7n000268ba@hotmail.com>
Message-ID: <Pine.LNX.4.44.0411080713000.28532-100000@gannet.stats>

This is about indexing a data frame: `matrix indexing' is something 
different.

On Sun, 7 Nov 2004, Kim Fai Wong wrote:

> Hi,
> 
> I have the following problem.
> In a csv file I have under column A, the date, and column B, the prices.
> Thus, for example, the file looks something like this ->
> 
> 1/31/04      2.5
> 2/1/04        2.6
> ...
> 4/12/04       3.5
> 
> Basically,  I use the function inputframe = read.csv( )
> which reads the csv file into the inputframe.

Well, it isn't going to work as the file is not Comma Separated Values.
Perhaps you need to look at ?read.table and tell us exactly what you 
propose to use.  I am guessing the first column will become row names.

When you have read this in, `inputframe' is a data frame, and not a 
matrix.

> My question is, how can I make a function that has, as inputs, start_date 
> and end_date,
> so that the inputframe would not return all of these prices but only the 
> ones during the
> period I input?
> 
> For example, if I input 3/1/04 and 3/11/04, the inputframe only contains the 
> prices for
> this period.

dates <- as.Date(row.names(inputframe), format="%m/%d/%y")
inputframe[dates >= start_date & dates < end_date, ]

where start_date and end_date are dates (not character strings in some 
strange non-ISO format).

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Your reading the posting guide and following its advice would be 
appreciated, including reading `An Introduction to R'.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rksh at soc.soton.ac.uk  Mon Nov  8 09:54:50 2004
From: rksh at soc.soton.ac.uk (Robin Hankin)
Date: Mon, 8 Nov 2004 08:54:50 +0000
Subject: [R] conv() example in R-exts
Message-ID: <a06002000bdb4e02b700b@[139.166.242.29]>


Hi

[I'm not sure if this is "intelligible to non-programmers" or not]

R-exts  (section 4.2) gives an example  of the .C() function whose 
third argument is

"as.integer(length(a))",

and urges the user to coerce all the arguments to the correct form 
(on pain of  "hard-to-catch errors"
which I now know to be very appropriate, if understated,  phrasing).

  The length()  function returns an integer, according to the helppage.

So, why does the argument above use as.integer()?



-- 
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
SO14 3ZH
tel +44(0)23-8059-7743
initialDOTsurname at soc.soton.ac.uk (edit in obvious way; spam precaution)



From ripley at stats.ox.ac.uk  Mon Nov  8 10:23:46 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 8 Nov 2004 09:23:46 +0000 (GMT)
Subject: [R] conv() example in R-exts
In-Reply-To: <a06002000bdb4e02b700b@[139.166.242.29]>
Message-ID: <Pine.LNX.4.44.0411080918400.3173-100000@gannet.stats>

On Mon, 8 Nov 2004, Robin Hankin wrote:

> [I'm not sure if this is "intelligible to non-programmers" or not]

Not, I suspect:  R-exts is not intended to be.

> R-exts  (section 4.2) gives an example  of the .C() function whose 
> third argument is
> 
> "as.integer(length(a))",
> 
> and urges the user to coerce all the arguments to the correct form 
> (on pain of  "hard-to-catch errors"
> which I now know to be very appropriate, if understated,  phrasing).
> 
>   The length()  function returns an integer, according to the helppage.
> 
> So, why does the argument above use as.integer()?

Because things written on help pages can change.

There *is* a potential issue here, as 64-bit platforms could support
longer vectors than an R integer can represent.  I suspect before too long
we will need a `size' type for lengths.  Already object.size() has been 
changed to return a double, not an integer, since people are created 
objects of more than 2Gb in size.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jarioksa at sun3.oulu.fi  Mon Nov  8 10:38:20 2004
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: 08 Nov 2004 11:38:20 +0200
Subject: [R] rgl on Mac OS
In-Reply-To: <BDB2B254.2D6A%saiwing@saiwing.net>
References: <BDB2B254.2D6A%saiwing@saiwing.net>
Message-ID: <1099906700.3765.12.camel@biol102145.oulu.fi>

On Sun, 2004-11-07 at 02:54, Saiwing Yeung wrote:

> It seems like a number of people on this list can install rgl but have
> problem loading it. I found myself in the same situation too.
> 
> I have tried the workaround of removing /usr/X11R6/lib from
> DYLD_LIBRARY_PATH, but it doesn't seem to work for me, I am still getting
> the same error (that everyone else seems to get). Can anyone give me some
> ideas on what else to try?
> 
> I have Mac OS 10.3.5, running R2.0. Thanks in advance!
> 
I had a quick look at this issue, and indeed, rgl failed to load in my
system (MacOS X 10.3.6, R 2.0.0) with various error messages. It seems
to me that the binary packages at CRAN were incompatible (g++ is
notorious for version changes incompatibilities). The solution was to 
use source packages and compile locally. For this you need to have a
compiler installed. The compiler comes with MacOS X 10.3.* installation
cd/dvd, but you have to install their "Developer Tools" separately.

One of the early error messages was that libpng was missing. When
installing from source, rgl was configured without png support, and this
message disappeared. However, CRAN binaries failed even after installing
png libraries, but now with other error messages. I got my libpng with
the help of http://www.rna.nl/ii.html (that you need anyway).

It may be that you have to start X11 separately before calling
library(rgl), but this was not necessary in my later attempts. 

Summary: install from source package. Optionally, you may install libpng
as well.

cheers, jari oksanen
-- 
Jari Oksanen -- Dept Biology, Univ Oulu, 90014 Oulu, Finland
email jari.oksanen at oulu.fi, homepage http://cc.oulu.fi/~jarioksa/



From wolski at molgen.mpg.de  Mon Nov  8 10:39:26 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Mon, 08 Nov 2004 10:39:26 +0100
Subject: [R] how lambda is computed in smoot.spline given _df_ 
Message-ID: <200411081039260477.0F9CFF04@mail.math.fu-berlin.de>

Hi,

I posted some days ago a question concerning the computation of lambda in the smooth.spline function (which I repreat at the bottom of the mail) given _df_ . 
Unfortunately the documentation is not clear to me. Maybee someone can help to answer in my view the basic question: 

If the penalized log likelihood is L = (y - f)' W (y - f) + lambda c' Sigma c

how the _lambda_ in the above equation is computed if _df_ is given and _spar_ not?

And, Is there a way to define lambda directly?

Yours.
/E


Hi,

I am usign the smooth.spline function. 

I am not sure how the _df_ (degrees of freedom) parameter, 
if set, influences _lambda_ in eq:

L = (y - f)' W (y - f) + lambda c' Sigma c

Is _df_  substituting tr(Sigma), if defined, in the equation: r = tr(X' W X) / tr(Sigma) 
which is used to compute: lambda = r * 256^(3*spar - 1)?


And how _spar_ is set if not defined?

/E



From cyracules at yahoo.co.uk  Mon Nov  8 12:58:04 2004
From: cyracules at yahoo.co.uk (John)
Date: Mon, 8 Nov 2004 11:58:04 +0000 (GMT)
Subject: [R] About 'choose' function
Message-ID: <20041108115804.49535.qmail@web26304.mail.ukl.yahoo.com>

Hello R-users,

When I didn't know about the internal 'choose'
function, I made such function, 'my.choose' below. But
when I used them instead of choose(6000,20), they
didn't give me any answer.

What is the difference between 'choose', 'my.choose1',
and 'my.choose2' below? That is, what is behind
'choose' function and what's the problem using 'prod'
or 'gamma' function?

Thanks a lot.

John

##########

> choose(6000,20)
[1] 1.455904e+57
>
> my.choose1 <- function(x,y) {
prod(1:x)/(prod(1:y)*prod(1:(x-y))) }
> my.choose1(6000,20)
[1] NaN
>
> my.choose2 <- function(x,y) {
gamma(x+1)/(gamma(y+1)*gamma(x-y+1)) }
> my.choose2(6000,20)
[1] NaN
>



From k.lindveld at imperial.ac.uk  Mon Nov  8 13:20:33 2004
From: k.lindveld at imperial.ac.uk (Lindveld, Charles)
Date: Mon, 8 Nov 2004 12:20:33 -0000
Subject: [R] Problems with DCOM client packages under R 2.0
Message-ID: <28AD4F9DFF3DD649B26B3027C78FB6732D74A4@icex1.ic.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041108/3f836311/attachment.pl

From mjw at celos.net  Mon Nov  8 13:26:15 2004
From: mjw at celos.net (Mark White)
Date: Mon, 8 Nov 2004 12:26:15 +0000
Subject: [R] Extension provokes crash in unzReadCurrentFile
Message-ID: <20041108122615.GB25111@celos.net>

I'm doing some work in C with the R_ExternalPointer
interface, and having some seg fault problems.  I expect the
crash is my fault, bad pointer in my code causing a fault
later etc, but I'm curious about the point of failure.

R almost always falls over in a call to unzReadCurrentFile
following a burst of disk activity.  I'm definitely not
doing anything that would call that explicitly, and I don't
notice the bursts of disk activity in similar work without
using my package.

Any idea what might be causing a compressed file read?
Something to do with lazy loading, maybe?  I've tried
disabling it in the package description without success.

This is R 2.0.0 on NetBSD/i386.

Mark <><



From ggrothendieck at myway.com  Mon Nov  8 14:16:25 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 8 Nov 2004 13:16:25 +0000 (UTC)
Subject: [R] Problems with DCOM client packages under R 2.0
References: <28AD4F9DFF3DD649B26B3027C78FB6732D74A4@icex1.ic.ac.uk>
Message-ID: <loom.20041108T141257-383@post.gmane.org>

Lindveld, Charles <k.lindveld <at> imperial.ac.uk> writes:

: 
: I am trying to use the Windows COM interface under R 2.0, and have
: encountered the following difficulties:
:  - the package RDCOMClient installs, loads and works under R 1.9.1,
: installs under R2.0, but does not load or work under R2.0
:  - the package SWinTypeLibs does not install or load under either R
: 1.9.1 or under R2.0
:  
: For the moment I am concentrating on the RDCOMClient package as it seems
: to be the most up-to-date one. 

rcom 1.0-1 has been tested on R 2.0.0. I have used it successfully
on R 2.0.1 beta with no problems. Its at:

   http://sunsite.univie.ac.at/rcom/download/rcom_1.0-1.zip

You also might want to review recent rcom archives at:

   http://mailman.csd.univie.ac.at/pipermail/rcom-l/



From ligges at statistik.uni-dortmund.de  Mon Nov  8 14:17:19 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 08 Nov 2004 14:17:19 +0100
Subject: [R] About 'choose' function
In-Reply-To: <20041108115804.49535.qmail@web26304.mail.ukl.yahoo.com>
References: <20041108115804.49535.qmail@web26304.mail.ukl.yahoo.com>
Message-ID: <418F71DF.4080304@statistik.uni-dortmund.de>

John wrote:

> Hello R-users,
> 
> When I didn't know about the internal 'choose'
> function, I made such function, 'my.choose' below. But
> when I used them instead of choose(6000,20), they
> didn't give me any answer.
> 
> What is the difference between 'choose', 'my.choose1',
> and 'my.choose2' below? That is, what is behind
> 'choose' function and what's the problem using 'prod'
> or 'gamma' function?

prod() calculates the whole product while choose() can do it the clever 
way avoiding overflows. On my machine, .Machine$double.xmax is 
1.797693e+308, hence prod(1:170) works while prod(1:171) is too much to 
be represented correctly ...

Uwe Ligges


> Thanks a lot.
> 
> John
> 
> ##########
> 
> 
>>choose(6000,20)
> 
> [1] 1.455904e+57
> 
>>my.choose1 <- function(x,y) {
> 
> prod(1:x)/(prod(1:y)*prod(1:(x-y))) }
> 
>>my.choose1(6000,20)
> 
> [1] NaN
> 
>>my.choose2 <- function(x,y) {
> 
> gamma(x+1)/(gamma(y+1)*gamma(x-y+1)) }
> 
>>my.choose2(6000,20)
> 
> [1] NaN
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Mon Nov  8 14:26:02 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 08 Nov 2004 14:26:02 +0100
Subject: [R] Problems with DCOM client packages under R 2.0
In-Reply-To: <28AD4F9DFF3DD649B26B3027C78FB6732D74A4@icex1.ic.ac.uk>
References: <28AD4F9DFF3DD649B26B3027C78FB6732D74A4@icex1.ic.ac.uk>
Message-ID: <418F73EA.6090203@statistik.uni-dortmund.de>

Lindveld, Charles wrote:

>  I am trying to use the Windows COM interface under R 2.0, and have
> encountered the following difficulties:
>  - the package RDCOMClient installs, loads and works under R 1.9.1,
> installs under R2.0, but does not load or work under R2.0
>  - the package SWinTypeLibs does not install or load under either R
> 1.9.1 or under R2.0
>  
> For the moment I am concentrating on the RDCOMClient package as it seems
> to be the most up-to-date one. 
> My questions are:
>  - has anybody else had this problem (and found a solution)
>  - is this something that I should be able to easily fix myself (on my
> local installation), or do I have to wait for the maintainer of the
> package to fix it?
>  - if it is something so complicated that only the maintainer can do it,
> is it usual to ask the maintainer if he can do that?
>  
> Same questions for the SWinTypeLibs package. The maintainers of
> SWinTypeLibs and RDCOMClient are the same person. 

You need the package's binary to be build under R-2.0.x.
You can try to compile from sources yourself.


Or you look at the more or less hidden archive of R-2.0.0 binary 
packages for Windows from Omegahat (Duncan TL told me in a private mail 
about it):

http://www.omegahat.org/download/R/packages/Windows/R-2.0.0/

Uwe Ligges



>  
> Charles Lindveld
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ramasamy at cancer.org.uk  Mon Nov  8 14:30:30 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 08 Nov 2004 13:30:30 +0000
Subject: [R] About 'choose' function
In-Reply-To: <20041108115804.49535.qmail@web26304.mail.ukl.yahoo.com>
References: <20041108115804.49535.qmail@web26304.mail.ukl.yahoo.com>
Message-ID: <1099920630.3414.22.camel@ndmpc126.ihs.ox.ac.uk>

Try with less ambitious numbers such as my.choose1(60,20). I think it
works fine.

I think the problem is that gamma(6001) and prod(1:6000) are so large
that it gives Inf as the answer. Hence the numerator and denominator
approaches Inf and division of two Inf gives NaN.

You could use the natural log version of gamma (or even lchoose) to
handle these large numbers as below 

my.choose3 <- function(x,y){
  y <- lgamma(x+1) - lgamma(y+1) - lgamma(x-y+1)
  return( exp(y) )
}

But have you tested the case when your inputs are not integers ?

Regards, Adai


On Mon, 2004-11-08 at 11:58, John wrote:
> Hello R-users,
> 
> When I didn't know about the internal 'choose'
> function, I made such function, 'my.choose' below. But
> when I used them instead of choose(6000,20), they
> didn't give me any answer.
> 
> What is the difference between 'choose', 'my.choose1',
> and 'my.choose2' below? That is, what is behind
> 'choose' function and what's the problem using 'prod'
> or 'gamma' function?
> 
> Thanks a lot.
> 
> John
> 
> ##########
> 
> > choose(6000,20)
> [1] 1.455904e+57
> >
> > my.choose1 <- function(x,y) {
> prod(1:x)/(prod(1:y)*prod(1:(x-y))) }
> > my.choose1(6000,20)
> [1] NaN
> >
> > my.choose2 <- function(x,y) {
> gamma(x+1)/(gamma(y+1)*gamma(x-y+1)) }
> > my.choose2(6000,20)
> [1] NaN
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Mon Nov  8 14:36:01 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 8 Nov 2004 13:36:01 +0000 (GMT)
Subject: [R] Problems with DCOM client packages under R 2.0
In-Reply-To: <28AD4F9DFF3DD649B26B3027C78FB6732D74A4@icex1.ic.ac.uk>
Message-ID: <Pine.LNX.4.44.0411081323420.3330-100000@gannet.stats>

On Mon, 8 Nov 2004, Lindveld, Charles wrote:

>  I am trying to use the Windows COM interface under R 2.0, and have
> encountered the following difficulties:
>  - the package RDCOMClient installs, loads and works under R 1.9.1,
> installs under R2.0, but does not load or work under R2.0
>  - the package SWinTypeLibs does not install or load under either R
> 1.9.1 or under R2.0
>  
> For the moment I am concentrating on the RDCOMClient package as it seems
> to be the most up-to-date one. 
> My questions are:
>  - has anybody else had this problem (and found a solution)
>  - is this something that I should be able to easily fix myself (on my
> local installation), or do I have to wait for the maintainer of the
> package to fix it?
>  - if it is something so complicated that only the maintainer can do it,
> is it usual to ask the maintainer if he can do that?
>  
> Same questions for the SWinTypeLibs package. The maintainers of
> SWinTypeLibs and RDCOMClient are the same person. 

Both are products of the Omegahat project, not of R.  As that project's 
mailing lists appear to be down, I suggest you contact DTL directly.
The versions on CRAN/src/contrib/Omegahat are way out of date.

There are Windows binaries for R 2.0.0 on www.omegahat.org.  I suspect you
have the wrong versions, but it is not clear which you are using,
including whether you are building from source or installing a binary.  
For SWinTypeLibs there are two binary versions there, 0.2-1 and 0.4-0, and
I was unable to find sources for the former.

I have succeeded in building RDCOMClient_0.8-1 from the sources on R
2.0.0, and it does appear to work.  (I did have to make some changes, at
least to the Makefiles.)  My build is available from 
http://www.stats.ox.ac.uk/pub/RWin/2.0.0.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Nov  8 14:45:22 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 8 Nov 2004 13:45:22 +0000 (GMT)
Subject: [R] Extension provokes crash in unzReadCurrentFile
In-Reply-To: <20041108122615.GB25111@celos.net>
Message-ID: <Pine.LNX.4.44.0411081341470.3330-100000@gannet.stats>

On Mon, 8 Nov 2004, Mark White wrote:

> I'm doing some work in C with the R_ExternalPointer
> interface, and having some seg fault problems.  I expect the
> crash is my fault, bad pointer in my code causing a fault
> later etc, but I'm curious about the point of failure.
> 
> R almost always falls over in a call to unzReadCurrentFile
> following a burst of disk activity.  I'm definitely not
> doing anything that would call that explicitly, and I don't
> notice the bursts of disk activity in similar work without
> using my package.
> 
> Any idea what might be causing a compressed file read?

It's not a compressed file read, but rather from the interface for unz()  
connections to zip files.  That is very unlikely to be used on a
non-Windows system, so if that really is being called this looks like
considerable internal corruption.

> Something to do with lazy loading, maybe?  I've tried
> disabling it in the package description without success.
> 
> This is R 2.0.0 on NetBSD/i386.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Mon Nov  8 14:51:39 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 8 Nov 2004 14:51:39 +0100
Subject: [R] how lambda is computed in smoot.spline given _df_ 
In-Reply-To: <200411081039260477.0F9CFF04@mail.math.fu-berlin.de>
References: <200411081039260477.0F9CFF04@mail.math.fu-berlin.de>
Message-ID: <16783.31211.289489.633268@gargle.gargle.HOWL>

>>>>> "Eryk" == Eryk Wolski <wolski at molgen.mpg.de>
>>>>>     on Mon, 08 Nov 2004 10:39:26 +0100 writes:

    Eryk> Hi,

    Eryk> I posted some days ago a question concerning the
    Eryk> computation of lambda in the smooth.spline function
    Eryk> (which I repreat at the bottom of the mail) given _df_.

I had thought one should be able to figure it out.
The help page cannot give all the theory behind smoothing splines!
The 'References' of  help(smooth.spline) do explain how "df" is
defined, and even the help page mentions that 
df = trace( <smoother matrix> ). 

    Eryk> Unfortunately the documentation is not clear to me.
    Eryk> Maybee someone can help to answer in my view the
    Eryk> basic question:

    Eryk> If the penalized log likelihood is

    Eryk>  L = (y - f)' W (y - f) + lambda c' Sigma c 
    Eryk>  how the _lambda_ in the above equation is computed if _df_ is
    Eryk>  given and _spar_ not?

Well, we have a computer for solving such problems:
spar (or lambda, equivalently) is determined such that the
resulting df matches the desired  df  by a traditional zero-finder
(Brent).  Basically the same algorithm as used by uniroot().

    Eryk> And, Is there a way to define lambda directly?

Almost {note: this is high school math}:
Since the help page gives the linear relation
between log(lambda) and spar, you call smooth.spline() once with
e.g. lambda = 0.5; and then know the formula how to compute spar
from lambda:

Using the 'cars' example from  help(smooth.spline):
assume you want to set  

my.lambda <- 1e-4

data(cars)# needed in R versions before 2.0.0
attach(cars)
## Call it with an arbitrary fixed 'spar':
(sspl <- smooth.spline(speed, dist, spar= .5))

## now solve the linear relationship for the only unknown  s0 :

s0 <- with(sspl, spar - 0.0601*log(lambda))

ssp. <- smooth.spline(speed, dist, spar= s0 + 0.0601*log(my.lambda))

str(ssp.)## confirms that lambda *is* 1e-4 = my.lambda

---------

Hoping this helped:
Martin Maechler Seminar fuer Statistik, ETH Zurich



From murdoch at stats.uwo.ca  Mon Nov  8 14:55:57 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 08 Nov 2004 08:55:57 -0500
Subject: [R] About 'choose' function
In-Reply-To: <20041108115804.49535.qmail@web26304.mail.ukl.yahoo.com>
References: <20041108115804.49535.qmail@web26304.mail.ukl.yahoo.com>
Message-ID: <v9uuo0dn06oka37i25ti4ndu0qdh0ckhsh@4ax.com>

On Mon, 8 Nov 2004 11:58:04 +0000 (GMT), John <cyracules at yahoo.co.uk>
wrote :

>What is the difference between 'choose', 'my.choose1',
>and 'my.choose2' below? That is, what is behind
>'choose' function and what's the problem using 'prod'
>or 'gamma' function?
>
>Thanks a lot.
>
>John
>
>##########
>
>> choose(6000,20)
>[1] 1.455904e+57
>>
>> my.choose1 <- function(x,y) {
>prod(1:x)/(prod(1:y)*prod(1:(x-y))) }
>> my.choose1(6000,20)
>[1] NaN

Generally 1:x will be taken to be of mode integer, so I would have
expected prod(1:x) to overflow around x==13, but whoever programmed it
was smart, and switched the values to floating point.  Floating point
evaluation of prod(1:x) overflows around x==171.  The ratio of two
overflows is undefined, so you get a NaN result.
>>
>> my.choose2 <- function(x,y) {
>gamma(x+1)/(gamma(y+1)*gamma(x-y+1)) }
>> my.choose2(6000,20)
>[1] NaN

Same problem.

Duncan Murdoch



From daodao99 at student.umu.se  Mon Nov  8 15:14:32 2004
From: daodao99 at student.umu.se (Danardono)
Date: Mon, 08 Nov 2004 15:14:32 +0100
Subject: [R] survSplit
In-Reply-To: <Pine.A41.4.61b.0411031035330.364448@homer10.u.washington.edu>
References: <20041103170843.28857.qmail@web40810.mail.yahoo.com>
	<Pine.A41.4.61b.0411031035330.364448@homer10.u.washington.edu>
Message-ID: <418F7F48.1020603@student.umu.se>

I am just realized that  survival has the facility to do survival time 
splitting survSplit
after read some postings about  time dependency  in the list.
Is it survSplit only for the survival data input (time,status)  and not 
for the 'counting process' input (start,stop,status)?

I take one example modified from the survSplit help:
 >data(aml)
 >aml3<-survSplit(aml,cut=c(5,10,50),end="time",start="start",event="status",episode="i",id="id")

 >coxph(Surv(time,status)~x,data=aml)

                coef exp(coef) se(coef)    z     p
xNonmaintained 0.916       2.5    0.512 1.79 0.074

Likelihood ratio test=3.38  on 1 df, p=0.0658  n= 23

#the same
 >coxph(Surv(time,status)~x,data=aml3)

                coef exp(coef) se(coef)    z     p
xNonmaintained 0.916       2.5    0.512 1.79 0.074

Likelihood ratio test=3.38  on 1 df, p=0.0658  n= 63

BUT If I split aml3 further:

 >aml4<-survSplit(aml3,cut=c(9,12,40),end="time",start="start",event="status",episode="i",id="id2")
#not the same!
 >coxph(Surv(start,time,status)~x,data=aml4)
               coef exp(coef) se(coef)    z     p
xNonmaintained 1.05      2.85    0.515 2.03 0.042

Likelihood ratio test=4.38  on 1 df, p=0.0363  n= 105

This one is corrrect
 >aml5<-survSplit(aml,cut=c(5,9,10,12,40,50),end="time",start="start",event="status")
 > coxph(Surv(start,time,status)~x,data=aml5)
                coef exp(coef) se(coef)    z     p
xNonmaintained 0.916       2.5    0.512 1.79 0.074
Likelihood ratio test=3.38  on 1 df, p=0.0658  n= 103

It would be useful to modify survSplit to accomodate counting process 
input as well.

Thanks
Danar
------------

Department of Statistics          
Ume?? University                  
SE-90187 Ume??, Sweden



From p.dalgaard at biostat.ku.dk  Mon Nov  8 15:33:53 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 08 Nov 2004 15:33:53 +0100
Subject: [R] Extension provokes crash in unzReadCurrentFile
In-Reply-To: <Pine.LNX.4.44.0411081341470.3330-100000@gannet.stats>
References: <Pine.LNX.4.44.0411081341470.3330-100000@gannet.stats>
Message-ID: <x2oei8fm0e.fsf@biostat.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> On Mon, 8 Nov 2004, Mark White wrote:
> 
> > I'm doing some work in C with the R_ExternalPointer
> > interface, and having some seg fault problems.  I expect the
> > crash is my fault, bad pointer in my code causing a fault
> > later etc, but I'm curious about the point of failure.
> > 
> > R almost always falls over in a call to unzReadCurrentFile
> > following a burst of disk activity.  I'm definitely not
> > doing anything that would call that explicitly, and I don't
> > notice the bursts of disk activity in similar work without
> > using my package.
> > 
> > Any idea what might be causing a compressed file read?
> 
> It's not a compressed file read, but rather from the interface for unz()  
> connections to zip files.  That is very unlikely to be used on a
> non-Windows system, so if that really is being called this looks like
> considerable internal corruption.
> 
> > Something to do with lazy loading, maybe?  I've tried
> > disabling it in the package description without success.
> > 
> > This is R 2.0.0 on NetBSD/i386.

In any case, since you're obviously running a more than half-decent
operating system, and you seem to know how to operate the debugger
(gdb?), could you please tell us what the backtrace ("bt", if gdb)
says at the point of crash.

This can be very enlightening. Sometimes it requires more work because
things have been corrupted beyond recognition, in which case you need
to use breakpoints and stuff.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Ted.Harding at nessie.mcc.ac.uk  Mon Nov  8 15:29:46 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 08 Nov 2004 14:29:46 -0000 (GMT)
Subject: [R] About 'choose' function
In-Reply-To: <20041108115804.49535.qmail@web26304.mail.ukl.yahoo.com>
Message-ID: <XFMail.041108142946.Ted.Harding@nessie.mcc.ac.uk>

On 08-Nov-04 John wrote:
> Hello R-users,
> 
> When I didn't know about the internal 'choose'
> function, I made such function, 'my.choose' below. But
> when I used them instead of choose(6000,20), they
> didn't give me any answer.
> 
> What is the difference between 'choose', 'my.choose1',
> and 'my.choose2' below? That is, what is behind
> 'choose' function and what's the problem using 'prod'
> or 'gamma' function?
> 
> Thanks a lot.
> 
> John
> 
>##########
> 
>> choose(6000,20)
> [1] 1.455904e+57
>>
>> my.choose1 <- function(x,y) {
> prod(1:x)/(prod(1:y)*prod(1:(x-y))) }
>> my.choose1(6000,20)
> [1] NaN
>>
>> my.choose2 <- function(x,y) {
> gamma(x+1)/(gamma(y+1)*gamma(x-y+1)) }
>> my.choose2(6000,20)
> [1] NaN

Your problem arises because the first thing that gets computed
in  your functions is the factorial of a large number, which
goes out of range; you then compound this by computing the
factorial of another large number, and dividing one by the
other:

  > prod(1:6000)
  [1] Inf
  > prod(1:5980)
  [1] Inf
  > prod(1:6000)/prod(1:5980)
  [1] NaN

It doesn't matter whether you do this using 'prod' or 'gamma'.

Those of us who grew up in the old days when you had to do things
the hard way learned tricks like:

  my.choose3 <- function(x,y){
    if((x==y)||(y==0)) return(1);
    m <- min(y,x-y)
    prod((x:(x-m+1))/(m:1))
  }

i.e. implementing, using term-by-term ratios (which keeps the
numbers within bounds all the way)  either

  (x/y)*((x-1)/(y-1))*...*((x-y+2)/2)*((x-y+1)/1)

  = x*(x-1)*...*(x-y+1)/{y*(y-1)*...*2*1}

or

  (x/(x-y))*((x-1)/(x-y-1))*...*((y+2)/2)*((y+1)/1)

  = x*(x-1)*...*(y+1)/{(x-y)*(x-y-1)*...*2*1}

whichever gives the shorter product. (Actually, in those days
we did it with loops too, or even by turning a handle -- all
the more reason to keep it short!).

Anyway:

  > choose(6000,20)
  [1] 1.455904e+57
  > my.choose3(6000,20)
  [1] 1.455904e+57

(And, if you try it, you'll see that 'my.choose3' is pretty fast).

However, as written above 'my.choose3' doesn't like really large
arguments:

  > choose(60000000000,31)
  [1] 1.612899e+300
  > my.choose3(60000000000,31)
  Error in x:(x - m + 1) : argument too large in magnitude

because the result of ":" is integer. However, it works OK
in the following form:

  my.choose3<-function(x,y){
    if((x==y)||(y==0)) return(1);
    m <- min(y,x-y)
   prod(seq(x,(x-m+1),by=-1)/(seq(m,1,by=-1)))
  }

when

  my.choose3(60000000000,31)
  [1] 1.613121e+300

which has a slight difference (0.014% greater) from the result of
'choose'.

Given the method of computation, I might feel inclined to trust
'my.choose3' rather than 'choose', but I'm not at sure of this
without studying the internal code of 'choose', and would welcome
comments!

Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 08-Nov-04                                       Time: 14:29:46
------------------------------ XFMail ------------------------------



From r.g.brown at cefas.co.uk  Mon Nov  8 16:02:56 2004
From: r.g.brown at cefas.co.uk (Robert Brown FM CEFAS)
Date: Mon, 8 Nov 2004 15:02:56 -0000
Subject: [R] Nonlinear weighted least squares estimation
Message-ID: <3589BC4D64C84341AE0C258244F977A2B60B61@expressa.corp.cefas.co.uk>

Hi there,

I'm trying to fit a growth curve to some data and need to use a weighted least squares estimator to account for heteroscedasticity in the data.  A weights argument is available in nls that would appear to be appropriate for this purpose, but it is listed as 'not yet implemented'. Is there another package which could implement this procedure?

Regards,

Robert Brown



From Ted.Harding at nessie.mcc.ac.uk  Mon Nov  8 16:11:37 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 08 Nov 2004 15:11:37 -0000 (GMT)
Subject: [R] About 'choose' function
In-Reply-To: <XFMail.041108142946.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <XFMail.041108151137.Ted.Harding@nessie.mcc.ac.uk>

On 08-Nov-04 Ted Harding wrote:
> [...]
> 
>   > choose(60000000000,31)
>   [1] 1.612899e+300
> 
> [...]
> 
>   my.choose3<-function(x,y){
>     if((x==y)||(y==0)) return(1);
>     m <- min(y,x-y)
>    prod(seq(x,(x-m+1),by=-1)/(seq(m,1,by=-1)))
>   }
> 
> when
> 
>   my.choose3(60000000000,31)
>   [1] 1.613121e+300
> 
> which has a slight difference (0.014% greater) from the result of
> 'choose'.
> 
> Given the method of computation, I might feel inclined to trust
> 'my.choose3' rather than 'choose', but I'm not at sure of this
> without studying the internal code of 'choose', and would welcome
> comments!

I just did this calculation using 'bc' (which works to arbitrary
decimal precision), getting (after rounding 59999999999.99...996,
1744 9's in all, to 60000000000):

16131211663389665874291103209859887097643782097733433535176164529013\
93347758720756812586746724689691665340326913939172715147819303293449\
54257506586926613288215108141984988883561331127046497391143349652601\
81932350377398578090978006727056282867666693363201127636940304448887\
82022049197129032260000000000

so the leading digits are 1613121..., agreeing with 'my.choose3'
as above, rather than with 'choose'!

Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 08-Nov-04                                       Time: 15:11:37
------------------------------ XFMail ------------------------------



From Jesus.Frias at dit.ie  Mon Nov  8 16:21:12 2004
From: Jesus.Frias at dit.ie (Jesus Frias)
Date: Mon, 08 Nov 2004 15:21:12 +0000
Subject: [R] Nonlinear weighted least squares estimation
In-Reply-To: <3589BC4D64C84341AE0C258244F977A2B60B61@expressa.corp.cefas.co.uk>
Message-ID: <LGECJJCANFBOOHCMGPJEAEGIDHAA.Jesus.Frias@dit.ie>

Hi Robert

	You can try gnls() in the nlme package or if that is not suitable, try to
follow the example in the nls() help page for weighted regression that has
an example of weighted regression from MASS.

from the nls help page:


     ## weighted nonlinear regression
     Treated <- Puromycin[Puromycin$state == "treated", ]
     weighted.MM <- function(resp, conc, Vm, K)
     {
         ## Purpose: exactly as white book p.451 -- RHS for nls()
         ##  Weighted version of Michaelis-Menten model
         ## ------------------------------------------------------------
         ## Arguments: 'y', 'x' and the two parameters (see book)
         ## ------------------------------------------------------------
         ## Author: Martin Maechler, Date: 23 Mar 2001, 18:48

         pred <- (Vm * conc)/(K + conc)
         (resp - pred) / sqrt(pred)
     }

     Pur.wt <- nls( ~ weighted.MM(rate, conc, Vm, K), data = Treated,
                   start = list(Vm = 200, K = 0.1),
                   trace = TRUE)



regards,

Jesus

--------------------------------------------------------------
Jes??s Mar??a Fr??as Celayeta
School of Food Sci. and Env. Health.
Faculty of Tourism and Food
Dublin Institute of Technology
Cathal Brugha St., Dublin 1. Ireland
t +353 1 4024459 f +353 1 4024495
w www.dit.ie/DIT/tourismfood/science/staff/frias.html
--------------------------------------------------------------

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Robert Brown FM
> CEFAS
> Sent: 08 November 2004 15:03
> To: r-help at stat.math.ethz.ch
> Subject: [R] Nonlinear weighted least squares estimation
>
>
> Hi there,
>
> I'm trying to fit a growth curve to some data and need to use a
> weighted least squares estimator to account for
> heteroscedasticity in the data.  A weights argument is available
> in nls that would appear to be appropriate for this purpose, but
> it is listed as 'not yet implemented'. Is there another package
> which could implement this procedure?
>
> Regards,
>
> Robert Brown
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
> --
> This message has been scanned for content and
> viruses by the DIT Information Services MailScanner
> Service, and is believed to be clean.
> http://www.dit.ie
>



-- 
This message has been scanned for content and 
viruses by the DIT Information Services MailScanner 
Service, and is believed to be clean.
http://www.dit.ie



From tlumley at u.washington.edu  Mon Nov  8 16:28:17 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 8 Nov 2004 07:28:17 -0800 (PST)
Subject: [R] survSplit
In-Reply-To: <418F7F48.1020603@student.umu.se>
References: <20041103170843.28857.qmail@web40810.mail.yahoo.com>
	<Pine.A41.4.61b.0411031035330.364448@homer10.u.washington.edu>
	<418F7F48.1020603@student.umu.se>
Message-ID: <Pine.A41.4.61b.0411080716530.139100@homer12.u.washington.edu>

On Mon, 8 Nov 2004, Danardono wrote:

> I am just realized that  survival has the facility to do survival time 
> splitting survSplit
> after read some postings about  time dependency  in the list.
> Is it survSplit only for the survival data input (time,status)  and not for 
> the 'counting process' input (start,stop,status)?
>
> I take one example modified from the survSplit help:
>> data(aml)
>> aml3<-survSplit(aml,cut=c(5,10,50),end="time",start="start",event="status",episode="i",id="id")
>
>> coxph(Surv(time,status)~x,data=aml)
>
>               coef exp(coef) se(coef)    z     p
> xNonmaintained 0.916       2.5    0.512 1.79 0.074
>
> Likelihood ratio test=3.38  on 1 df, p=0.0658  n= 23
>
> #the same
>> coxph(Surv(time,status)~x,data=aml3)
>
>               coef exp(coef) se(coef)    z     p
> xNonmaintained 0.916       2.5    0.512 1.79 0.074
>
> Likelihood ratio test=3.38  on 1 df, p=0.0658  n= 63

This should NOT be the same, and on my computer is not.  You should have 
to use the counting-process syntax.  I get

> coxph(Surv(time,status)~x,data=aml3)
Call:
coxph(formula = Surv(time, status) ~ x, data = aml3)


                coef exp(coef) se(coef)    z     p
xNonmaintained 1.07      2.92    0.514 2.08 0.037

Likelihood ratio test=4.61  on 1 df, p=0.0317  n= 63
> coxph(Surv(start,time,status)~x,data=aml3)
Call:
coxph(formula = Surv(start, time, status) ~ x, data = aml3)


                 coef exp(coef) se(coef)    z     p
xNonmaintained 0.916       2.5    0.512 1.79 0.074

Likelihood ratio test=3.38  on 1 df, p=0.0658  n= 63



> BUT If I split aml3 further:
>
>> aml4<-survSplit(aml3,cut=c(9,12,40),end="time",start="start",event="status",episode="i",id="id2")
> #not the same!
>> coxph(Surv(start,time,status)~x,data=aml4)
>              coef exp(coef) se(coef)    z     p
> xNonmaintained 1.05      2.85    0.515 2.03 0.042
>
> Likelihood ratio test=4.38  on 1 df, p=0.0363  n= 105
>

Hmm.  I suspect this is a <= vs < bug of some sort in handling start 
times.

 	-thomas



From andy_liaw at merck.com  Mon Nov  8 16:58:39 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 8 Nov 2004 10:58:39 -0500
Subject: [R] Nonlinear weighted least squares estimation
Message-ID: <3A822319EB35174CA3714066D590DCD50994E2A4@usrymx25.merck.com>

This question has been asked before on the list, but I'm not sure if the
answer were posted.  Basically, the trick is to write the formula a bit
differently in nls() so that it does weighted least squares.  nls() tries to
minimize the sum of squared differences between the two sides of ~.  If you
write the formula as 

~ sqrt(w) * (y - modelfun)

where modelfun is the nonlinear function being fitted, you get the weighted
nonlinear least squares solution.  (Cf. page 241 of MASS4 and Section 10.3.3
of the White Book.)  However, you need to watch out for predict(), etc., as
their output corresponds to what you specify in the formula.

HTH,
Andy

> From: Robert Brown FM CEFAS
> 
> Hi there,
> 
> I'm trying to fit a growth curve to some data and need to use 
> a weighted least squares estimator to account for 
> heteroscedasticity in the data.  A weights argument is 
> available in nls that would appear to be appropriate for this 
> purpose, but it is listed as 'not yet implemented'. Is there 
> another package which could implement this procedure?
> 
> Regards,
> 
> Robert Brown
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Ted.Harding at nessie.mcc.ac.uk  Mon Nov  8 16:56:49 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 08 Nov 2004 15:56:49 -0000 (GMT)
Subject: [R] About 'choose' function
In-Reply-To: <XFMail.041108151137.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <XFMail.041108155649.Ted.Harding@nessie.mcc.ac.uk>

On 08-Nov-04 Ted Harding wrote:
> On 08-Nov-04 Ted Harding wrote:
>> [...]
>> 
>>   > choose(60000000000,31)
>>   [1] 1.612899e+300
>> 
>> [...]
>> 
>>   my.choose3<-function(x,y){
>>     if((x==y)||(y==0)) return(1);
>>     m <- min(y,x-y)
>>    prod(seq(x,(x-m+1),by=-1)/(seq(m,1,by=-1)))
>>   }
>> 
>> when
>> 
>>   my.choose3(60000000000,31)
>>   [1] 1.613121e+300
>> 
>> which has a slight difference (0.014% greater) from the result of
>> 'choose'.
>> 
>> Given the method of computation, I might feel inclined to trust
>> 'my.choose3' rather than 'choose', but I'm not sure of this
>> without studying the internal code of 'choose', and would welcome
>> comments!
> 
> I just did this calculation using 'bc' (which works to arbitrary
> decimal precision), getting (after rounding 59999999999.99...996,
> 1744 9's in all, to 60000000000):
> 
> 16131211663389665874291103209859887097643782097733433535176164529013\
> 93347758720756812586746724689691665340326913939172715147819303293449\
> 54257506586926613288215108141984988883561331127046497391143349652601\
> 81932350377398578090978006727056282867666693363201127636940304448887\
> 82022049197129032260000000000
> 
> so the leading digits are 1613121..., agreeing with 'my.choose3'
> as above, rather than with 'choose'!

Further investigation indicates that there is an integer truncation
problem with 'choose':

  > print(choose(60000000000,1),digits=20)
  [1] 60001679906.00000
  > print(my.choose3(60000000000,1),digits=20)
  [1] 6e+10

Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 08-Nov-04                                       Time: 15:56:49
------------------------------ XFMail ------------------------------



From dm60062003 at yahoo.com  Mon Nov  8 17:39:06 2004
From: dm60062003 at yahoo.com (Derek Margetts)
Date: Mon, 8 Nov 2004 08:39:06 -0800 (PST)
Subject: [R] plotting lm coeficients with their means
Message-ID: <20041108163906.50940.qmail@web53208.mail.yahoo.com>

I am trying to write a function that will run a linear
model and plot the regression coeficients with their
corresponding means.  I am having two problems.  I can
get the plot with the function below, but I am having
trouble labeling the points.
 

function(y,x1,x2,x3,x4){
outlm<-lm(y~x1+x2+x3+x4)
imp<-as.data.frame(outlm$coef[-1])
meanvec<-c(mean(x1),mean(x2),mean(x3),mean(x4))
out<-list(imp,meanvec)
mf<-as.data.frame(out)
plot((mf),xlab="Impact",ylab="performance")
abline(h=mean(meanvec))
abline(v=mean(imp))
}

Problem #2:  If I only input x1,x2,and x3 how do I get
the function to ingnore the 4th instead of giving me
the unused argument error?

Thanks
Derek

 



From gunter.berton at gene.com  Mon Nov  8 17:45:27 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 8 Nov 2004 08:45:27 -0800
Subject: [R] Some Tips about developing an interface.
In-Reply-To: <20041106192145.QMMY1536.tomts25-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <200411081645.iA8GjRr9002490@hertz.gene.com>

In addition to John Fox's recommendations, some further hints and resources
are:

The tools of TCL/TK allow one to develop a comprehensive GUI. Note that the
Bioconductor project, http://www.bioconductor.org/ , has some packages
(tkWidgets, widgetTools are at least two that I know of) that claim to
simplify TK GUI development by allowing one to build GUI's without directly
having to learn TK. These are referenced at the GUI web site John pointed
you to.

In addition, I have found it useful to build very ** primitive, crude **
interfaces using a few native Windows capabilities built into the R for
Windows version (courtesy of Duncan Murdoch and Brian Ripley, I believe).
See, e.g. select.list, winDialog, winMenuAdd, and file.choose to see what's
available and get the links to remaining features. Finally, I have also used
plain old cat() and readline() for text interfacing. While these would be
considered laughable by professional standards, I have found that they allow
me to quickly build serviceable interfaces for relatively simple
applications (a very limited and structured set of tasks). When the
applications are to be used by only a very small number of users, I think
it's worth considering this approach, as building full-fledged GUI's in all
their glory is a difficult task, as I'm sure John would testify.

Cheers,

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of John Fox
> Sent: Saturday, November 06, 2004 11:22 AM
> To: 'Gilvan'
> Cc: r-help at stat.math.ethz.ch
> Subject: RE: [R] Some Tips about developing an interface.
> 
> Dear Gilvan,
> 
> See the R-GUI web site <http://www.sciviews.org/_rgui/> for a 
> variety of
> information.
> 
> I hope this helps,
>  John
> 
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox 
> -------------------------------- 
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Gilvan
> > Sent: Saturday, November 06, 2004 2:07 PM
> > To: r-help at stat.math.ethz.ch
> > Subject: [R] Some Tips about developing an interface.
> > 
> > Hi.
> >  
> > I just started studing R language and I saw there is a 
> > package to develop user interface called tcltk. I read about 
> > it and I made some tests but I found it a little poor and 
> > slow.. I was thinking if is possible to write a user 
> > interface using java and ask to R to run all the maths I have 
> > and R returns all the results, including graphs.
> > If someone has any suggestions to me I will appreciate, 
> > including other ways to write a user interface.
> >  
> > Thank you very much and sorry for my bad english.
> > 
> > Gilvan
> >  
> >  
> > 
> > ---
> > 
> > 
> > 
> >  
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From tfliao at uiuc.edu  Mon Nov  8 18:05:26 2004
From: tfliao at uiuc.edu (Tim F Liao)
Date: Mon, 8 Nov 2004 11:05:26 -0600
Subject: [R] Calling Other (non-C or Fortran)
 Programs from R
Message-ID: <37342050.9c1d4419.81afd00@expms6.cites.uiuc.edu>

Thanks to both Duncan Murdoch and Brian Ripley.  LEM indeed 
is the program that Brian referred to in his response, a 
program for latent class/trait loglinear and event history 
analysis compiled with Borland Pascal.  There's a dos 
version and a windows version.  Not sure the program will 
produce dll's, but I'll do a bit exploration and report back.

Tim Liao

---- Original message ----
>Date: Sun, 7 Nov 2004 18:57:52 +0000 (GMT)
>From: Prof Brian Ripley <ripley at stats.ox.ac.uk>  
>Subject: Re: [R] Calling Other (non-C or Fortran) Programs 
from R  
>To: Duncan Murdoch <murdoch at stats.uwo.ca>
>Cc: Tim F Liao <tfliao at uiuc.edu>, <r-help at stat.math.ethz.ch>
>
>On Sun, 7 Nov 2004, Duncan Murdoch wrote:
>
>> On Sun, 7 Nov 2004 08:50:25 -0600, Tim F Liao 
<tfliao at uiuc.edu> wrote:
>> 
>> >Hi!
>> >
>> >I wonder if anyone has experiences of calling other 
programs 
>> >from R (i.e., not C or Fortran programs).
>> >
>> >Specifically I want to call LEM from R and execute it in 
a 
>> >loop to process its output in R.  Thanks,
>> 
>> I don't know what LEM is.  If it's a language that can 
produce DLLs or
>> .so's, then it should be straightforward to call them.  
If it's a
>> program using standard input and output, the system() 
function is what
>> you want.
>
>Nor do, I, but my guess is that it is the program referred 
to at
>
>http://ourworld.compuserve.com/homepages/jsuebersax/soft.htm
#LEM
>
>Perhaps Tim F Liao can enlighten us.
>
>-- 
>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>Professor of Applied Statistics,  
http://www.stats.ox.ac.uk/~ripley/
>University of Oxford,             Tel:  +44 1865 272861 
(self)
>1 South Parks Road,                     +44 1865 272866 (PA)
>Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From br44114 at yahoo.com  Mon Nov  8 18:05:54 2004
From: br44114 at yahoo.com (bogdan romocea)
Date: Mon, 8 Nov 2004 09:05:54 -0800 (PST)
Subject: [R] misleading output after ordering data frame
Message-ID: <20041108170554.49854.qmail@web50303.mail.yahoo.com>

Dear R users,

I have a data frame which I create with read.csv and then order by
date:
d <- na.omit(read.csv(...))
d <- d[order(as.Date(as.character(d$Date), format="%d-%b-%y"), 
	decreasing=F, na.last=F),]

My problem is that even though the data frame is ordered as
requested, the old row numbers are preserved. For example:

* Before sorting:
> d[1:3,]
      Date   Amt
1 5-Nov-04 87.07
2 4-Nov-04 85.80
3 3-Nov-04 82.90

* After sorting:
> d[1:3,]
         Date   Amt  
500 12-Nov-02 84.23
499 13-Nov-02 85.05
498 14-Nov-02 84.95

Is there a way to update the row numbers as well? It's not that
important, but I find it a bit confusing.

Thank you,
b.



From perrelli at uiuc.edu  Mon Nov  8 18:20:21 2004
From: perrelli at uiuc.edu (Roberto Perrelli)
Date: Mon, 8 Nov 2004 11:20:21 -0600
Subject: [R] coxph models with frailty
Message-ID: <6b605cc1.9c1e9a04.bb38900@expms3.cites.uiuc.edu>

Dear R users:

I'm generating the following survival data:

set.seed(123)
n=200				#sample size
x=rbinom(n,size=1,prob=.5)	#binomial treatment
v=rgamma(n,shape=1,scale=1)	#gamma frailty
w=rweibull(n,shape=1,scale=1)	#Weibull deviates
b=-log(2)			#treatment's slope
t=exp( -x*b -log(v) + log(w) )	#failure times
c=rep(1,n) 			#uncensored indicator
id=seq(1:n)			#individual frailty indicator
group=rep(1:(n/10), 10)		#shared frailty indicator

Then I'm using the survival package in R 1.9.1 to estimate 
the following Cox models with frailty:  

fit1=coxph(Surv(t,c)~x+frailty
(id,dist='gamma',sparse=TRUE,method='em'))

fit2=coxph(Surv(t,c)~x+frailty
(group,dist='gamma',sparse=TRUE,method='em'))

fit3=coxph(Surv(t,c)~x+frailty
(id,dist='gamma',sparse=TRUE,method='aic',caic=TRUE))

fit4=coxph(Surv(t,c)~x+frailty
(group,dist='gamma',sparse=TRUE,method='aic',caic=TRUE))

In all cases, and after several replications, I am getting 
estimates of the variance of the random effect that are 
almost zero, whereas I thought that they should be around 1 
(the variance of the gamma frailty in my data generating 
process). Am I misunderstanding the procedures in some way, 
or is this a known feature? 

PS: Why the difference between the "penalty" (e.g. fit1
$history$frailty$theta) and the "variance of the random 
effect" reported in the gamma frailty models above?

Cordially,

Roberto Perrelli
Department of Economics
University of Illinois
484 Wohlers Hall
1206 South Sixth Street
Champaign, Illinois 61820 
USA



From tlumley at u.washington.edu  Mon Nov  8 18:27:24 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 8 Nov 2004 09:27:24 -0800 (PST)
Subject: [R] misleading output after ordering data frame
In-Reply-To: <20041108170554.49854.qmail@web50303.mail.yahoo.com>
References: <20041108170554.49854.qmail@web50303.mail.yahoo.com>
Message-ID: <Pine.A41.4.61b.0411080924270.249440@homer11.u.washington.edu>

On Mon, 8 Nov 2004, bogdan romocea wrote:

> My problem is that even though the data frame is ordered as
> requested, the old row numbers are preserved. For example:
>
> * Before sorting:
>> d[1:3,]
>      Date   Amt
> 1 5-Nov-04 87.07
> 2 4-Nov-04 85.80
> 3 3-Nov-04 82.90
>
> * After sorting:
>> d[1:3,]
>         Date   Amt
> 500 12-Nov-02 84.23
> 499 13-Nov-02 85.05
> 498 14-Nov-02 84.95
>
> Is there a way to update the row numbers as well? It's not that
> important, but I find it a bit confusing.
>

This is an important feature of R: the row names of a data frame stay 
fixed under subsetting or reordering.

You can change the rownames with eg

    rownames(d)<-1:nrow(d)

if you want to.


 	-thomas



From kkolanek at ippt.gov.pl  Mon Nov  8 18:28:40 2004
From: kkolanek at ippt.gov.pl (Krzysztof Kolanek)
Date: Mon, 8 Nov 2004 18:28:40 +0100
Subject: [R] Problems with installing Rmpi on x86_64
Message-ID: <006001c4c5b8$6643cbf0$58355194@kenny>

Dear All,
I am trying to install Rmpi on AMD 64 with Fedora Core 2 for x86_64, but I 
receive the following error:

/usr/bin/ld:
/usr/lib/gcc-lib/x86_64-redhat-linux/3.3.3/../../../../lib64/libmpi.a(abort.o):
relocation R_X86_64_32 can not be used when making a shared object;
recompile with -fPIC
/usr/lib/gcc-lib/x86_64-redhat-linux/3.3.3/../../../../lib64/libmpi.a:
could not read symbols: Bad value

Could anyone help me to solve this problem?
Thank you for your help,
Krzysiek



From ripley at stats.ox.ac.uk  Mon Nov  8 18:36:27 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 8 Nov 2004 17:36:27 +0000 (GMT)
Subject: [R] misleading output after ordering data frame
In-Reply-To: <20041108170554.49854.qmail@web50303.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0411081730060.1574-100000@gannet.stats>

It's your misinterpretion that is misleading, not the output.
Data frames have row *names* and not *numbers*.

On Mon, 8 Nov 2004, bogdan romocea wrote:

> Dear R users,
> 
> I have a data frame which I create with read.csv and then order by
> date:
> d <- na.omit(read.csv(...))
> d <- d[order(as.Date(as.character(d$Date), format="%d-%b-%y"), 
> 	decreasing=F, na.last=F),]
> 
> My problem is that even though the data frame is ordered as
> requested, the old row numbers are preserved. For example:
> 
> * Before sorting:
> > d[1:3,]
>       Date   Amt
> 1 5-Nov-04 87.07
> 2 4-Nov-04 85.80
> 3 3-Nov-04 82.90
> 
> * After sorting:
> > d[1:3,]
>          Date   Amt  
> 500 12-Nov-02 84.23
> 499 13-Nov-02 85.05
> 498 14-Nov-02 84.95
> 
> Is there a way to update the row numbers as well? It's not that
> important, but I find it a bit confusing.

I assure you that not to preserve the row *names* would be very confusing
indeed.  In your example it might well be more usual to have the Date the 
row names.

If you just want to change the contents of the data frame, and keep the 
row and column names, use

d[] <- your rhs

Or use

row.names(d) <- seq(len=nrow(d))

to reset the row *names*.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From plevy at ceh.ac.uk  Mon Nov  8 18:47:31 2004
From: plevy at ceh.ac.uk (Peter Levy)
Date: Mon, 08 Nov 2004 17:47:31 +0000
Subject: [R] Bug in power.anova.test?
Message-ID: <s18fb142.078@wpo.nerc.ac.uk>


I think there is a bug in power.anova.test.  Firstly, n, the number of
samples needed in each group, decreases as the number of groups
increases.  Should the reverse not be the case?
Secondly:

>      power.anova.test(groups = 2, between.var=424.36, within.var=256,
sig.level= 0.1, power=.90)

gives the answer n = 5.986304 where n is number of samples needed in
each group.

Four other sources give the answer n=12 (11.08 rounded up):

1. One-way ANOVA with 2 groups should be the same as a t-test. 
Power.t.test gives.
>  power.t.test(n=NULL, delta=20.6, sd=16.0, sig.level=0.1, power=0.9,
type=c("two.sample"), alternative=c("two.sided"), strict=FALSE)

2. p35, example 2 in Steidl, R.J. and Thomas, L.  (2001)  Power
analysis and experimental design.  In: Design and Analysis of Ecological
Experiments, (eds S. M. Scheiner and J. Gurevitch), pp. 415. Open
University Press, New York.
http://www.oup-usa.org/sc/0195131878/c2_ex2a.html

3. Minitab gives
Power and Sample Size 
One-way ANOVA
Alpha = 0.1  Assumed standard deviation = 16  Number of Levels = 2
          Sample  Target                   Maximum
SS Means    Size   Power  Actual Power  Difference
  212.18      12     0.9      0.920676        20.6
The sample size is for each level.

4. Minitab gives
Power and Sample Size 
2-Sample t Test
Testing mean 1 = mean 2 (versus not =)
Calculating power for mean 1 = mean 2 + difference
Alpha = 0.1  Assumed standard deviation = 16
            Sample  Target
Difference    Size   Power  Actual Power
      20.6      12     0.9      0.920676
The sample size is for each group.

 


 


Peter Levy
Centre for Ecology and Hydrology
Bush Estate, Penicuik
Midlothian, EH26 0QB, UK
Tel: 0131 445 8556 (direct)
       0131 445 4343 (switchboard)
Fax: 0131 445 3943
E-mail: plevy at ceh.ac.uk



From rolf at math.unb.ca  Mon Nov  8 18:52:54 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Mon, 8 Nov 2004 13:52:54 -0400 (AST)
Subject: [R] misleading output after ordering data frame
Message-ID: <200411081752.iA8Hqssr004293@erdos.math.unb.ca>


You wrote:

> Is there a way to update the row numbers as well? It's not that
> important, but I find it a bit confusing.

	They're not actually row numbers, they're row ***names***.
	These default to row numbers.
	
	If they were real-live names you'd want them to be carried
	along in the sort.

	To re-set them to be what you want:

		rownames(d) <- 1:nrow(d)


				cheers,

					Rolf Turner
					rolf at math.unb.ca



From ripley at stats.ox.ac.uk  Mon Nov  8 19:04:31 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 8 Nov 2004 18:04:31 +0000 (GMT)
Subject: [R] Problems with installing Rmpi on x86_64
In-Reply-To: <006001c4c5b8$6643cbf0$58355194@kenny>
Message-ID: <Pine.LNX.4.44.0411081800310.1628-100000@gannet.stats>

On Mon, 8 Nov 2004, Krzysztof Kolanek wrote:

> Dear All,
> I am trying to install Rmpi on AMD 64 with Fedora Core 2 for x86_64, but I 
> receive the following error:
> 
> /usr/bin/ld:
> /usr/lib/gcc-lib/x86_64-redhat-linux/3.3.3/../../../../lib64/libmpi.a(abort.o):
> relocation R_X86_64_32 can not be used when making a shared object;
> recompile with -fPIC
> /usr/lib/gcc-lib/x86_64-redhat-linux/3.3.3/../../../../lib64/libmpi.a:
> could not read symbols: Bad value
> 
> Could anyone help me to solve this problem?

It's not an R issue, but please do as it says and build a shareable 
version of libmpi.  There some comments about this in the R-admin manual.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Nov  8 19:22:50 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 8 Nov 2004 18:22:50 +0000 (GMT)
Subject: [R] Bug in power.anova.test?
In-Reply-To: <s18fb142.078@wpo.nerc.ac.uk>
Message-ID: <Pine.LNX.4.44.0411081814470.1628-100000@gannet.stats>

Two groups with a difference in mean of 20.6 have a between-group variance 
of 212.18 (as Minitab was given), not 424.36.

> var(c(0, 20.6))
[1] 212.18

(and the example on the help page shows this *is* what is meant by the 
variance).

On Mon, 8 Nov 2004, Peter Levy wrote:

> I think there is a bug in power.anova.test.  Firstly, n, the number of
> samples needed in each group, decreases as the number of groups
> increases.  Should the reverse not be the case?

No.  Think about having four groups, 2 with one mean and 2 with the 
another mean.  This is effectively a 2-group problem with twice the sample 
size.

> Secondly:
> 
> >      power.anova.test(groups = 2, between.var=424.36, within.var=256,
> sig.level= 0.1, power=.90)
> 
> gives the answer n = 5.986304 where n is number of samples needed in
> each group.
> 
> Four other sources give the answer n=12 (11.08 rounded up):
> 
> 1. One-way ANOVA with 2 groups should be the same as a t-test. 
> Power.t.test gives.
> >  power.t.test(n=NULL, delta=20.6, sd=16.0, sig.level=0.1, power=0.9,
> type=c("two.sample"), alternative=c("two.sided"), strict=FALSE)
> 
> 2. p35, example 2 in Steidl, R.J. and Thomas, L.  (2001)  Power
> analysis and experimental design.  In: Design and Analysis of Ecological
> Experiments, (eds S. M. Scheiner and J. Gurevitch), pp. 415. Open
> University Press, New York.
> http://www.oup-usa.org/sc/0195131878/c2_ex2a.html
> 
> 3. Minitab gives
> Power and Sample Size 
> One-way ANOVA
> Alpha = 0.1  Assumed standard deviation = 16  Number of Levels = 2
>           Sample  Target                   Maximum
> SS Means    Size   Power  Actual Power  Difference
>   212.18      12     0.9      0.920676        20.6
> The sample size is for each level.
> 
> 4. Minitab gives
> Power and Sample Size 
> 2-Sample t Test
> Testing mean 1 = mean 2 (versus not =)
> Calculating power for mean 1 = mean 2 + difference
> Alpha = 0.1  Assumed standard deviation = 16
>             Sample  Target
> Difference    Size   Power  Actual Power
>       20.6      12     0.9      0.920676
> The sample size is for each group.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From slacey at umich.edu  Mon Nov  8 19:23:04 2004
From: slacey at umich.edu (Steven Lacey)
Date: Mon, 8 Nov 2004 13:23:04 -0500
Subject: [R] location of key in panels of trellis plot
Message-ID: <000501c4c5c0$00618400$f182d38d@lsa.adsroot.itcs.umich.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041108/c4c90df5/attachment.pl

From slacey at umich.edu  Mon Nov  8 19:37:42 2004
From: slacey at umich.edu (Steven Lacey)
Date: Mon, 8 Nov 2004 13:37:42 -0500
Subject: [R] location of key in panels of trellis plot
Message-ID: <000b01c4c5c2$0e296e20$f182d38d@lsa.adsroot.itcs.umich.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041108/08d358fd/attachment.pl

From abunn at whrc.org  Mon Nov  8 19:39:41 2004
From: abunn at whrc.org (Andy Bunn)
Date: Mon, 8 Nov 2004 13:39:41 -0500
Subject: [R] plotting lm coeficients with their means
In-Reply-To: <20041108163906.50940.qmail@web53208.mail.yahoo.com>
Message-ID: <NEBBIPHDAMMOKDKPOFFICEMBCLAA.abunn@whrc.org>

How about something like this?

my.func <- function(y, x1, x2, x3, x4  = NULL){
    my.formula <- as.formula("y ~ x1 + x2 + x3 + x4")
    if(is.null(x4)) { my.formula <- as.formula("y ~ x1 + x2 + x3") }
    outlm <- lm(my.formula)
    meanvec<-c(mean(x1),mean(x2),mean(x3))
    if(is.null(x4) == F) { meanvec<-c(mean(x1),mean(x2),mean(x3),mean(x4)) }
    mf <- data.frame(Impact = outlm$coef[-1], Performance = meanvec)
    plot((mf),xlab="Impact",ylab="Performance", type = "n")
    points(mf, pch=21, bg="grey", cex=4)
    text(x = mf$Impact, y = mf$Performance, labels = rownames(mf))
    abline(h=mean(mf$Performance))
    abline(v=mean(mf$Impact))
}

response <- 1:10
pred.1 <- rnorm(10)
pred.2 <- runif(10)
pred.3 <- pred.1 + runif(10)
pred.4 <- runif(10)
my.func(y = response, x1 = pred.1, x2 = pred.2, x3 = pred.3)
my.func(y = response, x1 = pred.1, x2 = pred.2, x3 = pred.3, x4 = pred.4)


You'd have to do some work to generalize it further. I don't have much
experience writing functions with missing args and I expect this is a dumb
way of doing it.

HTH, Andy




> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Derek Margetts
> Sent: Monday, November 08, 2004 11:39 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] plotting lm coeficients with their means
>
>
> I am trying to write a function that will run a linear
> model and plot the regression coeficients with their
> corresponding means.  I am having two problems.  I can
> get the plot with the function below, but I am having
> trouble labeling the points.
>
>
> function(y,x1,x2,x3,x4){
> outlm<-lm(y~x1+x2+x3+x4)
> imp<-as.data.frame(outlm$coef[-1])
> meanvec<-c(mean(x1),mean(x2),mean(x3),mean(x4))
> out<-list(imp,meanvec)
> mf<-as.data.frame(out)
> plot((mf),xlab="Impact",ylab="performance")
> abline(h=mean(meanvec))
> abline(v=mean(imp))
> }
>
> Problem #2:  If I only input x1,x2,and x3 how do I get
> the function to ingnore the 4th instead of giving me
> the unused argument error?
>
> Thanks
> Derek
>
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From mattf at cnr.colostate.edu  Mon Nov  8 19:43:16 2004
From: mattf at cnr.colostate.edu (mattf)
Date: Mon, 8 Nov 2004 11:43:16 -0700
Subject: [R] small world models?
Message-ID: <41B39940@webmail.colostate.edu>

Hi,

I've searched the archives for a discussion on the use of R for developing 
"small world" or "scale-free" network models but have been unable to dig 
anything up.  Is anyone working on these types of models using R, and if so, 
have you found this language amenable to their development?

thanks,
matt farnsworth



From jmoreira at fe.up.pt  Mon Nov  8 19:49:39 2004
From: jmoreira at fe.up.pt (=?iso-8859-1?Q?Jo=E3o_Mendes_Moreira?=)
Date: Mon, 8 Nov 2004 18:49:39 -0000
Subject: [R] Converting strings to date
Message-ID: <009701c4c5c3$b39bc000$5e7aa8c0@FEUPsig.fe.up.pt>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041108/1ef24f22/attachment.pl

From murdoch at stats.uwo.ca  Mon Nov  8 19:51:06 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 08 Nov 2004 13:51:06 -0500
Subject: [R] About 'choose' function
In-Reply-To: <XFMail.041108155649.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.041108151137.Ted.Harding@nessie.mcc.ac.uk>
	<XFMail.041108155649.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <esfvo05em889dovu6f9akm4hse5on0rj7v@4ax.com>

On Mon, 08 Nov 2004 15:56:49 -0000 (GMT), (Ted Harding)
<Ted.Harding at nessie.mcc.ac.uk> wrote :

>Further investigation indicates that there is an integer truncation
>problem with 'choose':
>
>  > print(choose(60000000000,1),digits=20)
>  [1] 60001679906.00000
>  > print(my.choose3(60000000000,1),digits=20)
>  [1] 6e+10

Are you sure you're using the standard R choose()?  I get different
results from you:

> print(choose(60000000000,1),digits=20)
[1] 6e+10

What platform are you using?  I did this is Windows R 2.0.0, and get
the same result in the beta of 2.0.1.

Duncan Murdoch



From a.beckerman at sheffield.ac.uk  Mon Nov  8 19:52:17 2004
From: a.beckerman at sheffield.ac.uk (Andrew Beckerman)
Date: Mon, 8 Nov 2004 18:52:17 +0000
Subject: [R] bootstrap, lme, random effects
Message-ID: <5013AFD8-31B7-11D9-977F-000A95CD7F02@sheffield.ac.uk>

Hi there.  OSX/R2.0

We are trying to implement a bootstrap of the coeffecients of a mixed  
effect model. In particular, we are interested in the intercept and  
slope of the random effects.

Following from the basics for a linear model, we construct our lme  
models and a boot function:

library(nlme)
library(boot)
data<-read.csv("~/data.csv")

bootcoef<-function(data,index){
dat<-data[index,]
mod<-lme(Frames~Man2+Manip+Strings+Date.+Cut.,random=~Man2|ID.,data=dat)
fixef(mod)
}
boot.out<-boot(data,bootcoef,99)
boot.ci(boot.out) # produces information via boot.ci() that suggests  
this is not necessarily successful

ORDINARY NONPARAMETRIC BOOTSTRAP
Call:
boot(data = data, statistic = bootcoef, R = 99)

Bootstrap Statistics :
         original       bias    std. error
t1* 16.125904015  5.299827478  9.98818463
t2*  0.010901682 -0.004134585  0.01621935
t3* -0.038168126 -0.078833467  0.35778286
t4*  1.101486342 -0.021886290  0.45720400
t5*  0.005982241 -0.009140839  0.01563175
t6*  2.729537567  0.287663533  1.56150779
 > boot.ci(boot.out)
BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
Based on 99 bootstrap replicates
CALL :
boot.ci(boot.out = boot.out)
Intervals :
Level      Normal              Basic             Studentized
95%   ( -8.75,  30.40 )   ( -8.40,  36.01 )   (-40.93,  35.15 )
Level     Percentile            BCa
95%   (-3.75, 40.65 )   (-5.55, 28.65 )
Calculations and Intervals on Original Scale
Some basic intervals may be unstable
Some studentized intervals may be unstable
Some percentile intervals may be unstable
Warning : BCa Intervals used Extreme Quantiles
Some BCa intervals may be unstable
Warning messages:
1: NaNs produced in: sqrt(tv[, 2])
2: Extreme Order Statistics used as Endpoints in: norm.inter(t,  
adj.alpha)

As stated above, we are interested in the ranef(mod) components.....  
including this instead of fixef(mod) results in the error:
 > bootcoef<-function(data,index){
+ dat<-data[index,]
+  
mod<- 
lme(Frames~Man2+Manip+Strings+Date.+Cut.+ID.*Man2+ID.*Manip,random=~Man2 
|ID.,data=dat)
+ ranef(mod)
+ }
 >
 > boot.out<-boot(data,bootcoef,99)
Error: incorrect number of subscripts on matrix

Suggesting that the setup of the ranef(mod) list is different  
(clearly)....

Any suggestions on any of this?  I have a sneaking suspicion this is  
not a straightforward issue.

Cheers
andrew

------------------------------------------------------------------------ 
---------
Dr. Andrew Beckerman
Department of Animal and Plant Sciences, University of Sheffield,
Alfred Denny Building, Western Bank, Sheffield S10 2TN, UK
ph +44 (0)114 222 0026; fx +44 (0)114 222 0002
http://www.shef.ac.uk/beckslab
------------------------------------------------------------------------ 
----------



From Ted.Harding at nessie.mcc.ac.uk  Mon Nov  8 20:10:55 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 08 Nov 2004 19:10:55 -0000 (GMT)
Subject: [R] About 'choose' function
In-Reply-To: <esfvo05em889dovu6f9akm4hse5on0rj7v@4ax.com>
Message-ID: <XFMail.041108191055.Ted.Harding@nessie.mcc.ac.uk>

On 08-Nov-04 Duncan Murdoch wrote:
> On Mon, 08 Nov 2004 15:56:49 -0000 (GMT), (Ted Harding)
> <Ted.Harding at nessie.mcc.ac.uk> wrote :
> 
>>Further investigation indicates that there is an integer truncation
>>problem with 'choose':
>>
>>  > print(choose(60000000000,1),digits=20)
>>  [1] 60001679906.00000
>>  > print(my.choose3(60000000000,1),digits=20)
>>  [1] 6e+10
> 
> Are you sure you're using the standard R choose()?  I get different
> results from you:
> 
>> print(choose(60000000000,1),digits=20)
> [1] 6e+10
> 
> What platform are you using?  I did this is Windows R 2.0.0, and get
> the same result in the beta of 2.0.1.

Well, that's good to learn! As usual, I'm out of date, but this
time it seems to matter:

  R-1.8.0 on Linux

(Yes, it was the standard R choose() from that version).

Perhaps I'd better roll up my sleeves and finally upgrade (though
I'm really waiting for the next release since the move to 2.0 seems
to have needed a few things ironed out).

Thanks for the cross-check, Duncan.
Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 08-Nov-04                                       Time: 19:10:55
------------------------------ XFMail ------------------------------



From p.murrell at auckland.ac.nz  Mon Nov  8 20:36:50 2004
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Tue, 09 Nov 2004 08:36:50 +1300
Subject: [R] location of key in panels of trellis plot
References: <000501c4c5c0$00618400$f182d38d@lsa.adsroot.itcs.umich.edu>
Message-ID: <418FCAD2.6040404@stat.auckland.ac.nz>

Hi

Here's a modification of your panel function that I think does what you 
want (comments embedded):

tmp.xyplot <- function(x,y,subscripts=subscripts,cdata=cdata){
     # plot data points
     panel.xyplot(x,y)
     # extract parameter values
     right <- as.character(cdata[subscripts,][1,c(4,5,6)])
     #### Create the key for the panel
     key <- draw.key(list(text=list(expression(t[1],t[infinity],"p")),
                          text=list(c("=","=","=")),
                          text=list(right),
                          between=c(0.4),
                          rep=FALSE,
                          columns=1,
                          column.between=0),
                     draw=FALSE)
     #### Push a viewport in the top-right corner which is
     #### big enough to fit the key
     #### Uses grobWidth() and grobHeight() to get the size of the key
     #### (You'll need a library(grid) somewhere to direct access
     ####  to these grid functions)
     pushViewport(viewport(x=1, y=1,
                           width=grobWidth(key),
                           height=grobHeight(key),
                           just=c("right", "top")))
     #### Draw the key
     grid.draw(key)
     #### Pop the key viewport
     popViewport()
}

Hope that helps

Paul


Steven Lacey wrote:
> Hi,
> 
>  
> 
> I want to insert a key into each panel of a trellis plot, which I can do
> with a custom panel function that calles draw.key. The problem arises
> because I want the top right hand corner of the key to start in the top
> right hand corner of the panel. If you run my code below, you can see that
> the key appears in the center of each panel. This is because the default
> viewport in draw.key is the same size as the panel. I can readily change the
> height and width of the viewport such that only the key will fit (i.e.,
> viewport and key are the same size). While this works, it is not a robust
> solution. For example, if I change cex or resize, then I have to guess again
> what size viewport will be needed to fit only the key. How can I specify a
> viewport size that will always only fit the key itself? If I could do that,
> then I could have much more robust control over the key's position on the
> panel. That is, I could always specify the top right corner of the key to be
> at top right corner of the panel. 
> 
>  
> 
> In other words, if I had access to the width and height of the key in "npc"
> coordinates I could specify that in my viewport argument. But, I don't know
> how to access them without making a series of guesses.
> 
>  
> 
> Below is example code. 
> 
>  
> 
> Thanks, 
> Steve 
> 
>  
> 
> #### build a dataframe
> 
> dataframe<-data.frame(condition=factor(rep(c("A","B","C","D"),c(40,40,40,40)
> ),levels=c("A","B","C","D")),
> 
>                       var1=rnorm(160),
> 
>                       var2=rep(c(1:40),4),
> 
>                       parm1=rep(c(987.54,754,887.654,902),c(40,40,40,40)),
> 
>  
> parm2=rep(c(254.89,376.001,308,297.102),c(40,40,40,40)),
> 
>                       parm3=rep(c(0.2,38.5,1.5,0.654),c(40,40,40,40)))
> 
>  
> 
> ####Use xyplot to plot var1 against var2 for each condition. No problem. 
> 
> xyplot(var1~var2|condition,data=dataframe,ylim=c(4,-4))
> 
>  
> 
> ####Now I want to add a key to each panel indicating the values on the
> parameters 1-3.
> 
> ####To do so I write my own panel function that calls draw.key
> 
>  
> 
> tmp.xyplot <- function(x,y,subscripts=subscripts,cdata=cdata){
> 
>     # plot data points
> 
>     panel.xyplot(x,y)
> 
>     
> 
>     # extract parameter values
> 
>     right <- as.character(cdata[subscripts,][1,c(4,5,6)])
> 
>  
> 
>     #### draw the key on the panel
> 
>     draw.key(list(text=list(expression(t[1],t[infinity],"p")),
> 
>                   text=list(c("=","=","=")),
> 
>                   text=list(right),
> 
>                   between=c(0.4),
> 
>                   rep=FALSE,
> 
>                   columns=1,
> 
>                   column.between=0),
> 
>                   draw=TRUE)   
> 
>  
> 
> }
> 
>  
> 
> #### call xyplot with tmp.xyplot panel function
> 
> xyplot(var1~var2|condition,data=dataframe,ylim=c(4,-4),panel=tmp.xyplot,cdat
> a=dataframe)
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From SSchmaltz at jcaho.org  Mon Nov  8 20:52:26 2004
From: SSchmaltz at jcaho.org (Schmaltz, Stephen)
Date: Mon, 8 Nov 2004 13:52:26 -0600
Subject: [R] Availability of Data Sets in Graphical Methods for Data Analysis
Message-ID: <52B7E0AB329F7140BFB71EB3401DFE0F0204804E@jcexch1.jcaho.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041108/c81a30d4/attachment.pl

From p.murrell at auckland.ac.nz  Mon Nov  8 21:05:26 2004
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Tue, 09 Nov 2004 09:05:26 +1300
Subject: [R] location of key in panels of trellis plot
References: <000b01c4c5c2$0e296e20$f182d38d@lsa.adsroot.itcs.umich.edu>
Message-ID: <418FD186.4010105@stat.auckland.ac.nz>

Hi


Steven Lacey wrote:
> In my previous posting I forgot my system information. Sorry. It is listed
> below. 

<snip>
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    1              
> minor    8.1            
> year     2003           
> month    11             
> day      21             
> language R  


In that case, for the modifications I suggested, you may have to change 
pushViewport to push.viewport and popViewport to pop.viewport, and 
grobWidth(key) to unit(1, "grobwidth", key) and grobHeight(key) to 
unit(1, "grobheight", key), but hopefully then it will still work.

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From ripley at stats.ox.ac.uk  Mon Nov  8 21:18:16 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 8 Nov 2004 20:18:16 +0000 (GMT)
Subject: [R] Converting strings to date
In-Reply-To: <009701c4c5c3$b39bc000$5e7aa8c0@FEUPsig.fe.up.pt>
Message-ID: <Pine.LNX.4.44.0411082016310.2120-100000@gannet.stats>

You want as.Date, not strptime that gives you a *list*.

Please do read the help pages before posting, as the the posting guide
asks you to.

On Mon, 8 Nov 2004, Jo??o Mendes Moreira wrote:

> Hello,
> 
> I have the following problem:
> test is a data frame with 9 fields. The field test$Date is factorized with dates. The format is dd-mm-yyyy (using Oracle notation). I want to convert this to Date in '%Y-%m-%d format.
> What I am doing is:
> for (i in 1:nrow(test))
>   {
>    test[i,]$Data<-strptime(substring(test[i,]$Data,1,10),"%d-%m-%Y")
>   }
> 
> test is a data frame. 
> 
> The error is:
> Error in "$<-.data.frame"(`*tmp*`, "Data", value = list(sec = 0, min = 0,  : 
>         replacement has 9 rows, data has 1
> 
> But if I do:
> strptime(substring(test[1:nrow(teste),]$Data,1,10),"%d-%m-%Y")
> it works! Why the assignement does not work?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From roebuck at odin.mdacc.tmc.edu  Mon Nov  8 22:04:11 2004
From: roebuck at odin.mdacc.tmc.edu (Paul Roebuck)
Date: Mon, 8 Nov 2004 15:04:11 -0600 (CST)
Subject: [R] Rd \docType options
Message-ID: <Pine.OSF.4.58.0411081500070.488410@odin.mdacc.tmc.edu>

Where would I find a list of valid options for \docType{}?
I found the following types using grep:
	- class
	- data
	- genericFunction
	- methods
Are there others?

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From tlumley at u.washington.edu  Mon Nov  8 22:30:58 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 8 Nov 2004 13:30:58 -0800 (PST)
Subject: [R] coxph models with frailty
In-Reply-To: <6b605cc1.9c1e9a04.bb38900@expms3.cites.uiuc.edu>
References: <6b605cc1.9c1e9a04.bb38900@expms3.cites.uiuc.edu>
Message-ID: <Pine.A41.4.61b.0411081325580.249440@homer11.u.washington.edu>

On Mon, 8 Nov 2004, Roberto Perrelli wrote:

> Dear R users:
>
> I'm generating the following survival data:
>
> set.seed(123)
> n=200				#sample size
> x=rbinom(n,size=1,prob=.5)	#binomial treatment
> v=rgamma(n,shape=1,scale=1)	#gamma frailty
> w=rweibull(n,shape=1,scale=1)	#Weibull deviates
> b=-log(2)			#treatment's slope
> t=exp( -x*b -log(v) + log(w) )	#failure times
> c=rep(1,n) 			#uncensored indicator
> id=seq(1:n)			#individual frailty indicator
> group=rep(1:(n/10), 10)		#shared frailty indicator
>
> Then I'm using the survival package in R 1.9.1 to estimate
> the following Cox models with frailty:
>
> fit1=coxph(Surv(t,c)~x+frailty
> (id,dist='gamma',sparse=TRUE,method='em'))
>

These frailty models require multiple observations to share the same 
frailty value.  The model with independent observations and frailties is 
barely identifiable, and we aren't using maximum likelihood here, so it 
isn't surprising that it doesn't work.

Changing the simulation to
  v=rep(rgamma(n/2,shape=1,scale=1),2)     #gamma frailty
  id=rep(seq(1:(n/2)),2)

I get
> fit1
Call:
coxph(formula = Surv(t, c) ~ x + frailty(id, dist = "gamma",
     sparse = TRUE, method = "em"))

                           coef  se(coef) se2   Chisq DF p
x                         -1.21 0.206    0.184  34.8  1 3.7e-09
frailty(id, dist = "gamma                      265.4 62 0.0e+00

Iterations: 6 outer, 56 Newton-Raphson
      Variance of random effect= 0.837   I-likelihood = -838.7
Degrees of freedom for terms=  0.8 62.0
Likelihood ratio test=230  on 62.8 df, p=0  n= 200

so the estimation of the variance of the random effect is reasonable.

 	-thomas



From davidr at rhotrading.com  Mon Nov  8 22:47:35 2004
From: davidr at rhotrading.com (davidr@rhotrading.com)
Date: Mon, 8 Nov 2004 15:47:35 -0600
Subject: [R] can one evaluate an expression in a comment? (or insert results
	into history?)
Message-ID: <12AE52872B5C5348BE5CF47C707FF53A3277D8@rhosvr02.rhotrading.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041108/a8e4c966/attachment.pl

From maechler at stat.math.ethz.ch  Mon Nov  8 22:49:49 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 8 Nov 2004 22:49:49 +0100
Subject: [R] About 'choose' function
In-Reply-To: <XFMail.041108191055.Ted.Harding@nessie.mcc.ac.uk>
References: <esfvo05em889dovu6f9akm4hse5on0rj7v@4ax.com>
	<XFMail.041108191055.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <16783.59901.267456.455761@gargle.gargle.HOWL>

>>>>> "Ted" == Ted Harding <Ted.Harding at nessie.mcc.ac.uk>
>>>>>     on Mon, 08 Nov 2004 19:10:55 -0000 (GMT) writes:

    Ted> On 08-Nov-04 Duncan Murdoch wrote:
    >> On Mon, 08 Nov 2004 15:56:49 -0000 (GMT), (Ted Harding)
    >> <Ted.Harding at nessie.mcc.ac.uk> wrote :
    >> 
    >>> Further investigation indicates that there is an integer
    >>> truncation problem with 'choose':
    >>> 
    >>> > print(choose(60000000000,1),digits=20) [1]
    >>> 60001679906.00000 >
    >>> print(my.choose3(60000000000,1),digits=20) [1] 6e+10
    >>  Are you sure you're using the standard R choose()?  I
    >> get different results from you:
    >> 
    >>> print(choose(60000000000,1),digits=20)
    >> [1] 6e+10
    >> 
    >> What platform are you using?  I did this is Windows R
    >> 2.0.0, and get the same result in the beta of 2.0.1.

    Ted> Well, that's good to learn! As usual, I'm out of date,
    Ted> but this time it seems to matter:

    Ted>   R-1.8.0 on Linux

    Ted> (Yes, it was the standard R choose() from that
    Ted> version).

    Ted> Perhaps I'd better roll up my sleeves and finally
    Ted> upgrade (though I'm really waiting for the next release
    Ted> since the move to 2.0 seems to have needed a few things
    Ted> ironed out).

well, yes, but there are so many (mostly small) things that we
have ironed out in R since version 1.8.0....

The "NEWS" of 1.9.0 and 1.9.1 are already in file "ONEWS" (=
"Old News") and do contain

>>     BUG FIXES
>>
>>      .....
>> 
>>      o   [l]choose() use a more accurate formula which also slightly
>>          improves p- and qhyper(); choose(n, k) now returns 0 instead
>>          of NaN for k < 0 or > n.


    Ted> Thanks for the cross-check, Duncan.  
indeed!

Martin



From partha_bagchi at hgsi.com  Mon Nov  8 23:07:04 2004
From: partha_bagchi at hgsi.com (partha_bagchi@hgsi.com)
Date: Mon, 8 Nov 2004 17:07:04 -0500
Subject: [R] can one evaluate an expression in a comment? (or insert
	results into history?)
Message-ID: <OFDDEFB9EA.F6EFEB40-ON85256F46.00795AB1-85256F46.00797F9C@hgsi.com>

Have you perchance looked at
?paste

For example:
> paste("The current time is ", date())
[1] "The current time is  Mon Nov 08 17:06:25 2004"
>





<davidr at rhotrading.com>
Sent by: r-help-bounces at stat.math.ethz.ch
11/08/2004 04:47 PM

 
        To:     <r-help at stat.math.ethz.ch>
        cc: 
        Subject:        [R] can one evaluate an expression in a comment? (or insert results into 
history?)


I'd like to insert (for example) the current datetime into a comment so
it goes into the history.

I can of course cut and paste the results of date() into a comment line,


but it would be easier and more powerful to be able to type something
like

> hstamp()

and have it go into the history.

More generally, I would like to put any expression to be evaluated into
this function.

For example,

> hstamp(memory.size(TRUE))

would insert a comment like

# 12279808

after the hstamp command into the history.

Probably one should avoid any expressions with assignment effects or
large amounts of output.



Maybe I'm thinking about this all wrong, so please enlighten me if so.

I searched all the help and archives as well as I could.



--------------

> version

_

platform i386-pc-mingw32

arch     i386

os       mingw32

system   i386, mingw32

status   Patched

major    2

minor    0.0

year     2004

month    10

day      18

language R



Thanks!

David L. Reiner



Rho Trading

440 S. LaSalle St -- Suite 620

Chicago  IL  60605



312-362-4963 (voice)

312-362-4941 (fax)






[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From davidr at rhotrading.com  Mon Nov  8 23:27:58 2004
From: davidr at rhotrading.com (davidr@rhotrading.com)
Date: Mon, 8 Nov 2004 16:27:58 -0600
Subject: [R] can one evaluate an expression in a comment? (or insert
	resultsinto history?)
Message-ID: <12AE52872B5C5348BE5CF47C707FF53A3277E2@rhosvr02.rhotrading.com>

But that doesn't put the result into the history buffer, to be written
to a file only later when I savehistory(filename).

Bert Gunter also suggested ?capture.output and ?textConnection,
but I cannot see how to get text into the history buffer as comments,
but with evaluated expressions (values).

I know how to use paste, sink, write, etc. but nothing that I can see
inserts into the history buffer. I'm trying to avoid cutting and pasting
the results into a comment.

Thanks for any more help on this,
David

-----Original Message-----
From: partha_bagchi at hgsi.com [mailto:partha_bagchi at hgsi.com] 
Sent: Monday, November 08, 2004 4:07 PM
To: David Reiner <davidr at rhotrading.com>
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] can one evaluate an expression in a comment? (or insert
resultsinto history?)

Have you perchance looked at
?paste

For example:
> paste("The current time is ", date())
[1] "The current time is  Mon Nov 08 17:06:25 2004"
>





<davidr at rhotrading.com>
Sent by: r-help-bounces at stat.math.ethz.ch
11/08/2004 04:47 PM

 
        To:     <r-help at stat.math.ethz.ch>
        cc: 
        Subject:        [R] can one evaluate an expression in a comment?
(or insert results into 
history?)


I'd like to insert (for example) the current datetime into a comment so
it goes into the history.

I can of course cut and paste the results of date() into a comment line,


but it would be easier and more powerful to be able to type something
like

> hstamp()

and have it go into the history.

More generally, I would like to put any expression to be evaluated into
this function.

For example,

> hstamp(memory.size(TRUE))

would insert a comment like

# 12279808

after the hstamp command into the history.

Probably one should avoid any expressions with assignment effects or
large amounts of output.



Maybe I'm thinking about this all wrong, so please enlighten me if so.

I searched all the help and archives as well as I could.



--------------

> version

_

platform i386-pc-mingw32

arch     i386

os       mingw32

system   i386, mingw32

status   Patched

major    2

minor    0.0

year     2004

month    10

day      18

language R



Thanks!

David L. Reiner



Rho Trading

440 S. LaSalle St -- Suite 620

Chicago  IL  60605



312-362-4963 (voice)

312-362-4941 (fax)






[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From louize99 at yahoo.co.uk  Mon Nov  8 23:58:04 2004
From: louize99 at yahoo.co.uk (Louize Hill)
Date: Mon, 8 Nov 2004 22:58:04 -0000
Subject: [R] neater way to create data frame?
Message-ID: <013301c4c5e6$68031940$83f841c2@Louisept>

Hello,

I have a large data frame and am aiming to create a summary data frame in
order to plot quarterly means by age. However, it seems as though I am
taking a very long winded approach to this - can anybody point me in the
direction of something neater?

#original table is dat2,
dat5<- tapply (dat2$col1, INDEX=list(dat2$Age, dat2$Quarter), FUN=mean)

dat5 <- data.frame(dat5)
names(dat5) <- c("qt1", "qt2", "qt3", "qt4")
dat6 <- data.frame(age=row.names(dat5), dat5)
#transform rownames / 1st column from factor to variables
dat7 <- as.numeric (as.character(dat6$age))
#create new table with 5 columns
dat8 <- cbind (dat7, dat6$qt1, dat6$qt2, dat6$qt3, dat6$qt4)
#still need to add column names to this new table...

I am using R2.0.0 on W2K.
Thanks
Louize



From tlumley at u.washington.edu  Mon Nov  8 23:58:57 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 8 Nov 2004 14:58:57 -0800 (PST)
Subject: [R] can one evaluate an expression in a comment? (or insert
	resultsinto history?)
In-Reply-To: <12AE52872B5C5348BE5CF47C707FF53A3277E2@rhosvr02.rhotrading.com>
References: <12AE52872B5C5348BE5CF47C707FF53A3277E2@rhosvr02.rhotrading.com>
Message-ID: <Pine.A41.4.61b.0411081452410.249440@homer11.u.washington.edu>

On Mon, 8 Nov 2004 davidr at rhotrading.com wrote:

> But that doesn't put the result into the history buffer, to be written
> to a file only later when I savehistory(filename).
>
> Bert Gunter also suggested ?capture.output and ?textConnection,
> but I cannot see how to get text into the history buffer as comments,
> but with evaluated expressions (values).

You can cheat by writing out the history to a file, appending to the file, 
then reading the history back in.

> rewriteHistory <- function(comment){
  file1 <- tempfile("Rrawhist")
  on.exit(unlink(file1))
  savehistory(file1)
  conn<-file(file1,open="a")
  writeLines(comment, con=conn)
  close(conn)
  loadhistory(file1)
}

I now can get from history()

rewriteHistory("## 1984-4-12 13:00:00 GMT")
## 1984-4-12 13:00:00 GMT
rewriteHistory("TRUE == FALSE")
TRUE == FALSE

You still get the rewriteHistory() calls, though.


 	-thomas



From p.dalgaard at biostat.ku.dk  Tue Nov  9 00:16:04 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 09 Nov 2004 00:16:04 +0100
Subject: [R] About 'choose' function
In-Reply-To: <XFMail.041108191055.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.041108191055.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <x2sm7kue2z.fsf@biostat.ku.dk>

(Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:

> On 08-Nov-04 Duncan Murdoch wrote:
> > Are you sure you're using the standard R choose()?  I get different
> > results from you:
> > 
> >> print(choose(60000000000,1),digits=20)
> > [1] 6e+10
...
> Well, that's good to learn! As usual, I'm out of date, but this
> time it seems to matter:
> 
>   R-1.8.0 on Linux
> 
> (Yes, it was the standard R choose() from that version).
> 
> Perhaps I'd better roll up my sleeves and finally upgrade (though
> I'm really waiting for the next release since the move to 2.0 seems
> to have needed a few things ironed out).

And just to rub it in:

> choose(60000000000,31)
[1] 1.613121e+300

in 2.0.0, whereas the straightforward calculation

> exp(lgamma(60000000000+1)-lgamma(60000000000-31+1)-lgamma(31+1))
[1] 1.613161e+300

is off in the 6th significant digit, due to cancellation between

> lgamma(60000000000+1)
[1] 1.429057e+12

and

> lgamma(60000000000-31+1)
[1] 1.429057e+12

(the true difference between the two is supposed to be 
> lchoose(60000000000,31)+lgamma(31+1)
[1] 769.3459
)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From aragon at berkeley.edu  Tue Nov  9 00:57:22 2004
From: aragon at berkeley.edu (Tomas Aragon)
Date: Mon, 8 Nov 2004 15:57:22 -0800 (PST)
Subject: [R] small world models?
In-Reply-To: <41B39940@webmail.colostate.edu>
Message-ID: <20041108235722.99722.qmail@web80108.mail.yahoo.com>

--- mattf <mattf at cnr.colostate.edu> wrote:
> Hi,
> 
> I've searched the archives for a discussion on the use of R for
> developing 
> "small world" or "scale-free" network models but have been unable to
> dig 
> anything up.  Is anyone working on these types of models using R, and
> if so, 
> have you found this language amenable to their development?
> 
> thanks,
> matt farnsworth
> 

Contact Dr. Travis Porco at tporco at dhs.ca.gov . He uses R for
stochastic modeling of infectious diseases, including network models.
He maintains a website at http://www.mathepi.com 
Tomas



From davidD at qimr.edu.au  Tue Nov  9 03:28:44 2004
From: davidD at qimr.edu.au (David Duffy)
Date: Tue, 9 Nov 2004 12:28:44 +1000 (EST)
Subject: [R] Re: QTL interval mapping in outbred populations
In-Reply-To: <200411081122.iA8B9AjT021831@hypatia.math.ethz.ch>
References: <200411081122.iA8B9AjT021831@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.58.0411091155160.2321@orpheus.qimr.edu.au>

On Mon, 8 Nov 2004 ssim at lic.co.nz wrote:
>
> Is there an add-on package in R for QTL interval mapping for outbred
> population, eg. Haley-Knott regression method ?
>
> Stella

I believe Karl Broman's R/QTL (http://biosun01.biostat.jhsph.edu/~kbroman/software/)
and Brian Yandell's bim (http://www.stat.wisc.edu/~yandell/qtl/software/),
Richard Mott's happyR (http://www.well.ox.ac.uk/happy/happyR.shtml)
are mainly for experimental crosses, but do multipoint analyses.

You might also try QTL Express (http://qtl.cap.ed.ac.uk/), or if you are
analysing smaller families, programs like MERLIN (Goncalo Abecasis's).

For outbred families from natural populations, the estimation of
multipoint ibd sharing can be done by various programs like MERLIN,
LOKI, SIMWALK2 etc and the resulting kinship matrices can be imported
into R.  It's pretty easy to write a variance components or regression
based QTL routine to analyse them.  The programs by Jing hua Zhao, Beth
Atkinson and others could also then be used for linkage analysis of
survival data (http://www.ucl.ac.uk/~rmjdjhz/r-progs.htm) (I haven't any
experience with these programs yet).

David Duffy.

| David Duffy (MBBS PhD)                                         ,-_|\
| email: davidD at qimr.edu.au  ph: INT+61+7+3362-0217 fax: -0101  /     *
| Epidemiology Unit, Queensland Institute of Medical Research   \_,-._/
| 300 Herston Rd, Brisbane, Queensland 4029, Australia  GPG 4D0B994A v



From gerifalte28 at hotmail.com  Tue Nov  9 03:33:19 2004
From: gerifalte28 at hotmail.com (F Z)
Date: Tue, 09 Nov 2004 02:33:19 +0000
Subject: [R] Conditional selection of rows 
Message-ID: <BAY2-F42JrzdDW6lCPx00011e81@hotmail.com>

Hi,

I have a data.frame with several variables and 50,000 observations.
i.e.
data[1:2,1:7]
  Iteration Day Production.Type tsUSusc tsASusc tsULat tsALat
         1   0         Generic   17965 8833053      0      0
         1   1         Generic   17965 8833053      0      0
         .
         .
         .
         1 199         Generic   17237 8141028     26  23131
         2 127         Generic   15828 7307583     92  63463

I would like to extract only the observations (rows) for the last "day" for 
each "iteration" and store them in a data frame.

I tried lapply nested in a for loop without success.  Any help will be 
greatly appreciated!

Thanks

Francisco


Security. http://clinic.mcafee.com/clinic/ibuy/campaign.asp?cid=3963



From xmeng at capitalbio.com  Tue Nov  9 04:09:44 2004
From: xmeng at capitalbio.com (=?gb2312?B?w8/QwA==?=)
Date: Tue, 09 Nov 2004 11:09:44 +0800
Subject: [R] about p value
Message-ID: <299969784.00573@capitalbio.com>

Hello sir:
I am a data analysist of a bio-company.
Here's a question about microarray for identifying the differentially expressed genes.Thanks for your help.
First,I wanna make sure whether I've grasped the difference between the"unadjusted p-value" and "adjusted p-value".
 
1.Unadjusted p-value,which is calculated from t-statistic(just call it "t0").e.g if t=1.96,then p=0.05.
 
2.Adjusted p-value.p-value here is FDR(Falsely Discovery Rate) ,and it's different from the unadjusted p-value which is calculated according to the t0.In the permutation(say 100 permutations)of a gene,there will be 100 t values in total.If there are 20 t values whose absolute values are greater than t0,then the adjusted p-value=20/100=0.2.
 
Is that right?
 
And how can we define which genes are differentially expressed?
By t0(|t0|>1.96 is significant in 95% significant level) or by adjusted p-value,which needs us to set a cutoff of FDR?
 
 
Thanks a lot!
 
Best regards!



From ggrothendieck at myway.com  Tue Nov  9 03:58:35 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 9 Nov 2004 02:58:35 +0000 (UTC)
Subject: [R] Conditional selection of rows
References: <BAY2-F42JrzdDW6lCPx00011e81@hotmail.com>
Message-ID: <loom.20041109T035758-995@post.gmane.org>

F Z <gerifalte28 <at> hotmail.com> writes:

: 
: Hi,
: 
: I have a data.frame with several variables and 50,000 observations.
: i.e.
: data[1:2,1:7]
:   Iteration Day Production.Type tsUSusc tsASusc tsULat tsALat
:          1   0         Generic   17965 8833053      0      0
:          1   1         Generic   17965 8833053      0      0
:          .
:          .
:          .
:          1 199         Generic   17237 8141028     26  23131
:          2 127         Generic   15828 7307583     92  63463
: 
: I would like to extract only the observations (rows) for the last "day" for 
: each "iteration" and store them in a data frame.
: 


Try this:

   do.call("rbind", by(data, dat$Iteration, tail, 1))



From vokey at uleth.ca  Tue Nov  9 04:56:05 2004
From: vokey at uleth.ca (Dr. John R. Vokey)
Date: Mon, 8 Nov 2004 20:56:05 -0700
Subject: [R] no doubt a dumb question, but..
Message-ID: <4817319E-3203-11D9-A057-000A95CDE4AC@uleth.ca>

Yes, I am a newbie at R, but it is not the complex commands in R that 
have me baffled, but simple data commands.  For example, why does 
something like:

 > plot(Girth ~ Height)

*not* work after a command that allegedly loads the data:

 > data(trees)

with the error message:

Error in eval(expr, envir, enclos) : Object "Girth" not found

but does work after the command:

 > attach(trees)

?

- JRV
--
There are 10 kinds of people:  those who understand binary, and those 
who don't



From MSchwartz at MedAnalytics.com  Tue Nov  9 05:27:47 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon, 08 Nov 2004 22:27:47 -0600
Subject: [R] no doubt a dumb question, but..
In-Reply-To: <4817319E-3203-11D9-A057-000A95CDE4AC@uleth.ca>
References: <4817319E-3203-11D9-A057-000A95CDE4AC@uleth.ca>
Message-ID: <1099974466.5607.22.camel@localhost.localdomain>

On Mon, 2004-11-08 at 21:56, Dr. John R. Vokey wrote:
> Yes, I am a newbie at R, but it is not the complex commands in R that 
> have me baffled, but simple data commands.  For example, why does 
> something like:
> 
>  > plot(Girth ~ Height)
> 
> *not* work after a command that allegedly loads the data:
> 
>  > data(trees)
> 
> with the error message:
> 
> Error in eval(expr, envir, enclos) : Object "Girth" not found
> 
> but does work after the command:
> 
>  > attach(trees)
> 
> ?

As per ?attach:

"By attaching a data frame to the search path it is possible to refer to
the variables in the data frame by their names alone, rather than as
components of the data frame (eg in the example below, height rather
than women$height)."

data(trees) simply loads the dataset, but does not place it in the
search path, which is what attach(trees) does.

Prior to using attach(trees), you would need to use:

plot(trees$Girth ~ trees$Height)

to tell R that the variables Girth and Height are in the data frame
called 'trees'.

HTH,

Marc Schwartz



From Andy_Donaldson at bnz.co.nz  Tue Nov  9 05:52:16 2004
From: Andy_Donaldson at bnz.co.nz (Andy_Donaldson@bnz.co.nz)
Date: Tue, 9 Nov 2004 17:52:16 +1300
Subject: [R] Running SPLUS scripts in R
Message-ID: <OF126618C1.C3D1D70C-ONCC256F47.001AA341-CC256F47.001AD2F7@bnz.co.nz>





Hi, I have a number of SPLUS scripts that I wish to run in R. Can you point
me to the right FAQ or help area to do this?
Cheers
Andy

Andy Donaldson
Head Of Market Risk
Institutional Markets & Services
Bank of New Zealand

Phone:  +64 4 474 6937
Mob No: +64 29 222 0050



CAUTION - This message may contain privileged and confidential  information
intended only for the use of the addressee named above. If you are not the
intended recipient of this message you are hereby  notified that any use,
dissemination, distribution or reproduction  of this message is prohibited.
If you have received this message in  error please notify the Bank of the
New Zealand immediately.  Any views expressed in this message are those of
the individual  sender and may not necessarily reflect the views of the
Bank of  New Zealand.



From saiwing at berkeley.edu  Tue Nov  9 06:45:55 2004
From: saiwing at berkeley.edu (Saiwing Yeung)
Date: Mon, 08 Nov 2004 21:45:55 -0800
Subject: [R] rgl on Mac OS
In-Reply-To: <1099906700.3765.12.camel@biol102145.oulu.fi>
Message-ID: <BDB59993.2DCB%saiwing@berkeley.edu>


Jari, thanks so much for the advice. I followed your instruction and could
install rgl too. It's really fun playing with it.

I have written up a more detailed instruction, hopefully this will help less
experienced R users.

1) go to apple.com, download Developer Tools and X11. You need to sign up as
an ADC member if you haven't yet. Install both.
2) go to http://www.rna.nl/ii.html, get i-installer, install it. Run
i-installer, find the libpng package and install it.
3) Download the source of rgl
4) At the terminal, do "R CMD INSTALL rgl". I did this in super use mode, I
am not sure if I needed to though.

If this is successful, you will see
/Library/Frameworks/R.framework/Versions/2.0.0/Resources/library/rgl/libs/rg
l.so being created

Saiwing



On 11/8/04 1:38 AM, "Jari Oksanen" <jarioksa at sun3.oulu.fi> wrote:

> On Sun, 2004-11-07 at 02:54, Saiwing Yeung wrote:
> 
>> It seems like a number of people on this list can install rgl but have
>> problem loading it. I found myself in the same situation too.
>> 
>> I have tried the workaround of removing /usr/X11R6/lib from
>> DYLD_LIBRARY_PATH, but it doesn't seem to work for me, I am still getting
>> the same error (that everyone else seems to get). Can anyone give me some
>> ideas on what else to try?
>> 
>> I have Mac OS 10.3.5, running R2.0. Thanks in advance!
>> 
> I had a quick look at this issue, and indeed, rgl failed to load in my
> system (MacOS X 10.3.6, R 2.0.0) with various error messages. It seems
> to me that the binary packages at CRAN were incompatible (g++ is
> notorious for version changes incompatibilities). The solution was to
> use source packages and compile locally. For this you need to have a
> compiler installed. The compiler comes with MacOS X 10.3.* installation
> cd/dvd, but you have to install their "Developer Tools" separately.
> 
> One of the early error messages was that libpng was missing. When
> installing from source, rgl was configured without png support, and this
> message disappeared. However, CRAN binaries failed even after installing
> png libraries, but now with other error messages. I got my libpng with
> the help of http://www.rna.nl/ii.html (that you need anyway).
> 
> It may be that you have to start X11 separately before calling
> library(rgl), but this was not necessary in my later attempts.
> 
> Summary: install from source package. Optionally, you may install libpng
> as well.
> 
> cheers, jari oksanen



From Allan at SCIENCE.uct.ac.za  Tue Nov  9 08:06:19 2004
From: Allan at SCIENCE.uct.ac.za (UCT Staff Member - Allan)
Date: Tue, 09 Nov 2004 09:06:19 +0200
Subject: [R] r: neural networks
Message-ID: <41906C6B.8CAC7925@SCIENCE.uct.ac.za>

hi all

does anyone know of a package that does neural networks?



From ripley at stats.ox.ac.uk  Tue Nov  9 08:25:06 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 9 Nov 2004 07:25:06 +0000 (GMT)
Subject: [R] r: neural networks
In-Reply-To: <41906C6B.8CAC7925@SCIENCE.uct.ac.za>
Message-ID: <Pine.LNX.4.44.0411090721240.3212-100000@gannet.stats>

On Tue, 9 Nov 2004, UCT Staff Member - Allan wrote:

> does anyone know of a package that does neural networks?

Yes, very many people know.

Do please read the posting guide and do your homework.  Try
help.search("neural net") for example.  Or, there is a list of packages in
the R FAQ so why not search that?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Tue Nov  9 08:55:55 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 09 Nov 2004 08:55:55 +0100
Subject: [R] Running SPLUS scripts in R
In-Reply-To: <OF126618C1.C3D1D70C-ONCC256F47.001AA341-CC256F47.001AD2F7@bnz.co.nz>
References: <OF126618C1.C3D1D70C-ONCC256F47.001AA341-CC256F47.001AD2F7@bnz.co.nz>
Message-ID: <4190780B.60209@statistik.uni-dortmund.de>

Andy_Donaldson at bnz.co.nz wrote:

> 
> 
> 
> Hi, I have a number of SPLUS scripts that I wish to run in R. Can you point
> me to the right FAQ or help area to do this?

What about reading the FAQ and the manual "An Introduction to R" 
yourself? If you don't want to read this minimal amount of documentation 
before asking on R-help, I highly recommend to ask a consultant instead.

Uwe Ligges



> Cheers
> Andy
> 
> Andy Donaldson
> Head Of Market Risk
> Institutional Markets & Services
> Bank of New Zealand
> 
> Phone:  +64 4 474 6937
> Mob No: +64 29 222 0050
> 
> 
> 
> CAUTION - This message may contain privileged and confidential  information
> intended only for the use of the addressee named above. If you are not the
> intended recipient of this message you are hereby  notified that any use,
> dissemination, distribution or reproduction  of this message is prohibited.
> If you have received this message in  error please notify the Bank of the
> New Zealand immediately.  Any views expressed in this message are those of
> the individual  sender and may not necessarily reflect the views of the
> Bank of  New Zealand.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From maechler at stat.math.ethz.ch  Tue Nov  9 09:07:38 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 9 Nov 2004 09:07:38 +0100
Subject: [R] no doubt a dumb question, but..
In-Reply-To: <1099974466.5607.22.camel@localhost.localdomain>
References: <4817319E-3203-11D9-A057-000A95CDE4AC@uleth.ca>
	<1099974466.5607.22.camel@localhost.localdomain>
Message-ID: <16784.31434.711805.31472@gargle.gargle.HOWL>

>>>>> "Marc" == Marc Schwartz <MSchwartz at MedAnalytics.com>
>>>>>     on Mon, 08 Nov 2004 22:27:47 -0600 writes:

    Marc> On Mon, 2004-11-08 at 21:56, Dr. John R. Vokey wrote:
    >> Yes, I am a newbie at R, but it is not the complex
    >> commands in R that have me baffled, but simple data
    >> commands.  For example, why does something like:
    >> 
    >> > plot(Girth ~ Height)
    >> 
    >> *not* work after a command that allegedly loads the data:
    >> 
    >> > data(trees)
    >> 
    >> with the error message:
    >> 
    >> Error in eval(expr, envir, enclos) : Object "Girth" not
    >> found
    >> 
    >> but does work after the command:
    >> 
    >> > attach(trees)
    >> 
    >> ?

    Marc> As per ?attach:

    Marc> "By attaching a data frame to the search path it is
    Marc> possible to refer to the variables in the data frame
    Marc> by their names alone, rather than as components of the
    Marc> data frame (eg in the example below, height rather
    Marc> than women$height)."

    Marc> data(trees) simply loads the dataset, but does not
    Marc> place it in the search path, which is what
    Marc> attach(trees) does.

    Marc> Prior to using attach(trees), you would need to use:

    Marc> plot(trees$Girth ~ trees$Height)

    Marc> to tell R that the variables Girth and Height are in
    Marc> the data frame called 'trees'.

Yes, thank you, Marc!

For the specific case at hand, however, the recommdend way is to use

  plot(Girth ~ Height, data = trees)


Further note that many of us try to avoid attach()ing data frames
(most of the time; not always) and we have provided the nice alternative
    with( <data> ,  <expression_body> ) 

So, for the current example, you could also say

  with(trees, plot(Girth ~ Height))

The main advantage of with(): Only inside it, the components of
'trees' are visible - no need to remember to  detach() ;
see also  help(with) and its examples.

Martin Maechler



From lecoutre at stat.ucl.ac.be  Tue Nov  9 09:12:54 2004
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Tue, 09 Nov 2004 09:12:54 +0100
Subject: [R] do.call invoking a function with multiple arguments
Message-ID: <6.0.1.1.2.20041109090836.0384bb38@stat4ux.stat.ucl.ac.be>


Hi users,

I am not sure to understand the help page about do.call.

--
  do.call(what, args)
     args: a _list_ of arguments to the function call.  The 'names'
           attribute of 'args' gives the argument names.
--

If we take the following sample data:


 > (tab=as.data.frame(table(factor(1:10))))
    Var1 Freq
1     1    1
2     2    1
3     3    1
4     4    1
5     5    1
6     6    1
7     7    1
8     8    1
9     9    1
10   10    1

I can call "paste" on it:

 > do.call("paste",tab)
  [1] "1 1"  "2 1"  "3 1"  "4 1"  "5 1"  "6 1"  "7 1"  "8 1"  "9 1"  "10 1"

Now if I want to use ":" as the separator, I guess I should write

 > do.call("paste",list(tab,sep=":"))
[1] "c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)" "c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1)"

which does not return me what I want.
Of course, I know there are plenty other ways to achieve my goal but this 
is not the first case I encounter I cant' use "do.call" though I think it 
should be possible.

Could someone bring me an example of such a use?
basically, my needs are:

do.call("foo",args) args containing primary and extra-arguments to pass to 
"foo".

Best wishes,

Eric


Eric Lecoutre
UCL /  Institut de Statistique
Voie du Roman Pays, 20
1348 Louvain-la-Neuve
Belgium

tel: (+32)(0)10473050
lecoutre at stat.ucl.ac.be
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre

If the statistics are boring, then you've got the wrong numbers. -Edward 
Tufte



From ripley at stats.ox.ac.uk  Tue Nov  9 09:46:27 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 9 Nov 2004 08:46:27 +0000 (GMT)
Subject: [R] do.call invoking a function with multiple arguments
In-Reply-To: <6.0.1.1.2.20041109090836.0384bb38@stat4ux.stat.ucl.ac.be>
Message-ID: <Pine.LNX.4.44.0411090842530.7166-100000@gannet.stats>

On Tue, 9 Nov 2004, Eric Lecoutre wrote:

> I am not sure to understand the help page about do.call.

I think you do, just not how to construct a list.

> --
>   do.call(what, args)
>      args: a _list_ of arguments to the function call.  The 'names'
>            attribute of 'args' gives the argument names.
> --
> 
> If we take the following sample data:
> 
> 
>  > (tab=as.data.frame(table(factor(1:10))))
>     Var1 Freq
> 1     1    1
> 2     2    1
> 3     3    1
> 4     4    1
> 5     5    1
> 6     6    1
> 7     7    1
> 8     8    1
> 9     9    1
> 10   10    1
> 
> I can call "paste" on it:
> 
>  > do.call("paste",tab)
>   [1] "1 1"  "2 1"  "3 1"  "4 1"  "5 1"  "6 1"  "7 1"  "8 1"  "9 1"  "10 1"
> 
> Now if I want to use ":" as the separator, I guess I should write
> 
>  > do.call("paste",list(tab,sep=":"))
> [1] "c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)" "c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1)"

But tab is already a list, so you want to concatenate to it.

do.call("paste", c(tab,sep=":"))

Take a look at

str(list(tab,sep=":"))

which shows it is a list of 2 elements, the first of which is a data frame 
(which is a list).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dimitris.rizopoulos at med.kuleuven.ac.be  Tue Nov  9 09:59:58 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Tue, 9 Nov 2004 09:59:58 +0100
Subject: [R] do.call invoking a function with multiple arguments
References: <6.0.1.1.2.20041109090836.0384bb38@stat4ux.stat.ucl.ac.be>
Message-ID: <000b01c4c63a$7d478b20$0540210a@www.domain>

Hi Eric,

you could try it this way:

> tab <- as.data.frame(table(factor(1:10)))
> do.call("paste", c(tab, list(sep=":")))
 [1] "1:1"  "2:1"  "3:1"  "4:1"  "5:1"  "6:1"  "7:1"  "8:1"  "9:1" 
"10:1"


since `tab' is already a list

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Eric Lecoutre" <lecoutre at stat.ucl.ac.be>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, November 09, 2004 9:12 AM
Subject: [R] do.call invoking a function with multiple arguments


>
> Hi users,
>
> I am not sure to understand the help page about do.call.
>
> --
>  do.call(what, args)
>     args: a _list_ of arguments to the function call.  The 'names'
>           attribute of 'args' gives the argument names.
> --
>
> If we take the following sample data:
>
>
> > (tab=as.data.frame(table(factor(1:10))))
>    Var1 Freq
> 1     1    1
> 2     2    1
> 3     3    1
> 4     4    1
> 5     5    1
> 6     6    1
> 7     7    1
> 8     8    1
> 9     9    1
> 10   10    1
>
> I can call "paste" on it:
>
> > do.call("paste",tab)
>  [1] "1 1"  "2 1"  "3 1"  "4 1"  "5 1"  "6 1"  "7 1"  "8 1"  "9 1" 
> "10 1"
>
> Now if I want to use ":" as the separator, I guess I should write
>
> > do.call("paste",list(tab,sep=":"))
> [1] "c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)" "c(1, 1, 1, 1, 1, 1, 1, 1, 1, 
> 1)"
>
> which does not return me what I want.
> Of course, I know there are plenty other ways to achieve my goal but 
> this is not the first case I encounter I cant' use "do.call" though 
> I think it should be possible.
>
> Could someone bring me an example of such a use?
> basically, my needs are:
>
> do.call("foo",args) args containing primary and extra-arguments to 
> pass to "foo".
>
> Best wishes,
>
> Eric
>
>
> Eric Lecoutre
> UCL /  Institut de Statistique
> Voie du Roman Pays, 20
> 1348 Louvain-la-Neuve
> Belgium
>
> tel: (+32)(0)10473050
> lecoutre at stat.ucl.ac.be
> http://www.stat.ucl.ac.be/ISpersonnel/lecoutre
>
> If the statistics are boring, then you've got the wrong 
> numbers. -Edward Tufte
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From parrinel at med.unibs.it  Tue Nov  9 10:24:37 2004
From: parrinel at med.unibs.it (parrinel@med.unibs.it)
Date: Tue, 09 Nov 2004 10:24:37 +0100
Subject: [R] Strange results for Beta Distribution
Message-ID: <41909AE5.3191.46EF9B@localhost>

Dear All,
I got these results from the example in the function "dbeta":

>x <- seq(0, 1, length=21)
>      dbeta(x, 1, 1)
 [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
Any Idea?
TIA
Giovanni

dr. Giovanni Parrinello
Section of Medical Statistics
Department of Biosciences
University of Brescia
25127 Viale Europa, 11
Brescia Italy
Tel: +390303717528
Fax: +390303701157



From petr.pikal at precheza.cz  Tue Nov  9 10:25:42 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 09 Nov 2004 10:25:42 +0100
Subject: [R] neater way to create data frame?
In-Reply-To: <013301c4c5e6$68031940$83f841c2@Louisept>
Message-ID: <41909B26.130.55CDA3@localhost>

Hi Louize

On 8 Nov 2004 at 22:58, Louize Hill wrote:

> Hello,
> 
> I have a large data frame and am aiming to create a summary data frame
> in order to plot quarterly means by age. However, it seems as though I
> am taking a very long winded approach to this - can anybody point me
> in the direction of something neater?
> 
> #original table is dat2,
> dat5<- tapply (dat2$col1, INDEX=list(dat2$Age, dat2$Quarter),
> FUN=mean)

Maybe better approach would be to use

aggregate

dat5<- aggregate (dat2$col1, list(Age=dat2$Age, 
Quarter=dat2$Quarter)

With it you will get  data frame with 3 columns (first two will be 
factors).

and reshape the result, if necessary.


Cheers

Petr

> 
> dat5 <- data.frame(dat5)
> names(dat5) <- c("qt1", "qt2", "qt3", "qt4")
> dat6 <- data.frame(age=row.names(dat5), dat5)
> #transform rownames / 1st column from factor to variables
> dat7 <- as.numeric (as.character(dat6$age))
> #create new table with 5 columns
> dat8 <- cbind (dat7, dat6$qt1, dat6$qt2, dat6$qt3, dat6$qt4)
> #still need to add column names to this new table...
> 
> I am using R2.0.0 on W2K.
> Thanks
> Louize
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From asemeria at cramont.it  Tue Nov  9 10:39:04 2004
From: asemeria at cramont.it (asemeria@cramont.it)
Date: Tue, 9 Nov 2004 10:39:04 +0100
Subject: [R] small world models?
Message-ID: <OFEAC1550D.60660650-ONC1256F47.00350167@tomware.it>



From dimitris.rizopoulos at med.kuleuven.ac.be  Tue Nov  9 10:40:09 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Tue, 9 Nov 2004 10:40:09 +0100
Subject: [R] Strange results for Beta Distribution
References: <41909AE5.3191.46EF9B@localhost>
Message-ID: <005e01c4c640$1a5437b0$0540210a@www.domain>

Why strange? It is known that Beta(1, 1) = Uniform(0,1) whose density 
is 1/(1-0)=1

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: <parrinel at med.unibs.it>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, November 09, 2004 10:24 AM
Subject: [R] Strange results for Beta Distribution


> Dear All,
> I got these results from the example in the function "dbeta":
>
>>x <- seq(0, 1, length=21)
>>      dbeta(x, 1, 1)
> [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
> Any Idea?
> TIA
> Giovanni
>
> dr. Giovanni Parrinello
> Section of Medical Statistics
> Department of Biosciences
> University of Brescia
> 25127 Viale Europa, 11
> Brescia Italy
> Tel: +390303717528
> Fax: +390303701157
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Tue Nov  9 10:58:43 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 9 Nov 2004 09:58:43 +0000 (GMT)
Subject: [R] Strange results for Beta Distribution
In-Reply-To: <41909AE5.3191.46EF9B@localhost>
Message-ID: <Pine.LNX.4.44.0411090956040.12253-100000@gannet.stats>

On Tue, 9 Nov 2004 parrinel at med.unibs.it wrote:

> I got these results from the example in the function "dbeta":
> 
> >x <- seq(0, 1, length=21)
> >      dbeta(x, 1, 1)
>  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
> Any Idea?

What does help page say the density is?
A Beta(1,1) distribution is U(0, 1), and that has density 1.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Ted.Harding at nessie.mcc.ac.uk  Tue Nov  9 11:18:41 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 09 Nov 2004 10:18:41 -0000 (GMT)
Subject: [R] Strange results for Beta Distribution
In-Reply-To: <41909AE5.3191.46EF9B@localhost>
Message-ID: <XFMail.041109101841.Ted.Harding@nessie.mcc.ac.uk>

On 09-Nov-04 parrinel at med.unibs.it wrote:
> Dear All,
> I got these results from the example in the function "dbeta":
> 
>>x <- seq(0, 1, length=21)
>>      dbeta(x, 1, 1)
>  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
> Any Idea?

The density function for Beta(p,q) is (x^(p-1))*((1-x)^(q-1))/B(p,q).
In your example, p = q = 1, so the density function is 1 everywhere.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 09-Nov-04                                       Time: 10:18:41
------------------------------ XFMail ------------------------------



From christian.hoffmann at wsl.ch  Tue Nov  9 11:40:07 2004
From: christian.hoffmann at wsl.ch (Christian Hoffmann)
Date: Tue, 09 Nov 2004 11:40:07 +0100
Subject: [R] Package Documentation, cryptic
Message-ID: <41909E87.2050509@wsl.ch>

Hi, sorry for this lengthy post.
I am using R-2.0.0 on Unix, compiled.

Overview:
A) R CMD check: Unaccounted top-level text
B) In which sections of *.Rd is LaTeX notation allowed, where *not*?
C) Codoc mismatches and polyvalent parameters
D) Successful R CMD INSTALL and R CMD build
E)  cp ../Rd.sty .  is copy necessary?
F) latex ./CWHstat-manual.tex  runs into trouble

When running >R CMD check CWHstat    I get
                        =====
...
* checking foreign function calls ... OK
* checking Rd files ... WARNING
Rd files with likely Rd problems:
Unaccounted top-level text in file 
'/home/woodstock/hoffmacw/R/Sources/CWHstat/man/qnorm.appr.Rd':
Following section 'arguments':
"\n\\value {\n  "

See chapter 'Writing R documentation files' in manual 'Writing R
Extensions'.
* checking for missing documentation entries ... OK
* checking for code/documentation mismatches ... WARNING

C)  Codoc mismatches from documentation object 'my.table':
my.table.margin
   Code: function(v, w)
   Docs: function(m)

* checking Rd \usage sections ... OK
...

The relevant code snippets:
/CWHstat/man/qnorm.appr.Rd:
...
\arguments{
   \item{p}{vector of probabilities.}
   \item{mean}{vector of means.}
   \item{sd}{vector of standard deviations.}
   \item{log.p}{logical; if TRUE, probabilities p are given as log(p).}
   \item{lower.tail}{logical; if TRUE (default), probabilities are P[X 
<= x], otherwise, P[X > x].}
}
\value {
   \code{qnorm... gives the quantile function.}
}
...
A) ------- Question:
Unaccounted top-level text in file 
'/home/woodstock/hoffmacw/R/Sources/CWHstat/man/qnorm.appr.Rd':
Following section 'arguments':
"\n\\value {\n  "
--Question: Where in Writing .. is this topic treated? The string seems 
to come from a lower level of "check".
--Question: This error did *not* occur under 1.9.last. Does it have to 
do with [] ?

------- Question:
B) In which sections of *.Rd is LaTeX notation allowed, where *not*?

The relevant code snippets:
/CWHstat/man/my.table.margin.Rd:
\name{my.table}
\alias{my.table.margin}
\usage{
my.table.margin(v,w)
my.table.margin(m)
}
\arguments{
   \item{v, w}{Factors.}
   \item{m}{Matrix.}
}

/CWHstat/man/my.table.margin.r:
my.table.margin <- function(v, w) {
       if (missing(w)) tab <- v else tab <- table(v, w)

C) ------- Question:
How can I document this type of behaviour of polyvalent parameters 
without "check" baulking at me?

D) Next:
--------
hoffmacw at fluke:~/R/Sources >R CMD INSTALL --library=lib CWHstat
* Installing *source* package 'CWHstat' ...
** R
** preparing package for lazy loading

** help
  >>> Building/Updating help pages for package 'CWHstat'
      Formats: text html latex example
* DONE (CWHstat)
(*successful*)

Next:
-----
hoffmacw at fluke:~/R/Sources >R CMD build --force CWHstat
* checking for file 'CWHstat/DESCRIPTION' ... OK
* preparing 'CWHstat':
* removing junk files
* building 'CWHstat_4.0.4.tar.gz'
(*successful*)

Next:
-----
hoffmacw at fluke:~/R/Sources >cd CWHstat.Rcheck
hoffmacw at fluke:~/R/Sources/CWHstat.Rcheck >cp ../Rd.sty .

E) *comment* copy is necessary, because each run of check destroys old 
directory *.Rcheck, Is there a way around this? I cannot write to 
$R_HOME/share.

F)
hoffmacw at fluke:~/R/Sources/CWHstat.Rcheck >latex ./CWHstat-manual.tex
This is e-TeXk, Version 3.141592-2.1 (Web2C 7.5.2)
  %&-line parsing enabled.
  (/usr/local/TeX/texmf/web2c/cp8bit.tcx)
entering extended mode
(./CWHstat-manual.tex
LaTeX2e <2001/06/01>
Babel <v3.7j> and hyphenation patterns for english, dumylang, 
nohyphenation, ba
sque, czech, slovak, german, ngerman, spanish, catalan, french, 
ukenglish, ital
ian, dutch, polish, portuguese, russian, loaded.
(/usr/local/TeX/texmf/tex/latex/base/article.cls
Document Class: article 2001/04/21 v1.4e Standard LaTeX document class
(/usr/local/TeX/texmf/tex/latex/base/size10.clo)) (./Rd.sty
(/usr/local/TeX/texmf/tex/latex/base/ifthen.sty)
(/usr/local/TeX/texmf/tex/latex/tools/longtable.sty)
(/usr/local/TeX/texmf/tex/latex/tools/bm.sty)
(/usr/local/TeX/texmf/tex/latex/base/alltt.sty)
(/usr/local/TeX/texmf/tex/latex/tools/verbatim.sty)
(/usr/local/TeX/texmf-local/tex/latex/html/url.sty)
(/usr/local/TeX/texmf/tex/latex/upquote/upquote.sty)
(/usr/local/TeX/texmf/tex/latex/base/fontenc.sty
(/usr/local/TeX/texmf/tex/latex/base/t1enc.def))
(/usr/local/TeX/texmf/tex/latex/ae/ae.sty
(/usr/local/TeX/texmf/tex/latex/base/fontenc.sty
(/usr/local/TeX/texmf/tex/latex/base/t1enc.def)
(/usr/local/TeX/texmf/tex/latex/ae/t1aer.fd)))
(/usr/local/TeX/texmf/tex/latex/ae/t1aett.fd)
(/usr/local/TeX/texmf/tex/latex/graphics/color.sty
(/usr/local/TeX/texmf/tex/latex/texlive/color.cfg)
(/usr/local/TeX/texmf/tex/latex/graphics/dvips.def)
(/usr/local/TeX/texmf/tex/latex/graphics/dvipsnam.def))
(/usr/local/TeX/texmf/tex/latex/hyperref/hyperref.sty
(/usr/local/TeX/texmf/tex/latex/graphics/keyval.sty)
(/usr/local/TeX/texmf/tex/latex/hyperref/pd1enc.def)
(/usr/local/TeX/texmf/tex/latex/texlive/hyperref.cfg)
Implicit mode ON; LaTeX internals redefined
)
*hyperref using default driver hdvips*
(/usr/local/TeX/texmf/tex/latex/hyperref/hdvips.def
(/usr/local/TeX/texmf/tex/latex/hyperref/pdfmark.def))

Package hyperref Warning: Option `hyperindex' has already been used,
(hyperref)                setting the option has no effect on input line 
293.


Package hyperref Warning: Option `pagebackref' has already been used,
(hyperref)                setting the option has no effect on input line 
293.

) (./CWHstat-manual.aux) 
(/usr/local/TeX/texmf/tex/latex/hyperref/nameref.sty)
! Undefined control sequence.
l.4 \HeaderA
             {invgauss}{Inverse Gaussian Distribution}{invgauss}
? ^C! Interruption.
<to be read again>
                    {
l.4 \HeaderA{
              invgauss}{Inverse Gaussian Distribution}{invgauss}
? x

------- Question:
F) Are these follow-up errors?


Thanks for giving me any hints. I searched
http://stat.ethz.ch/CRAN/src/base/NEWS

but I cannot make sense of: `--no-codoc', because I want the check.

Thanks for any hints
Christian
-- 
Dr.sc.math.Christian W. Hoffmann, 
http://www.wsl.ch/staff/christian.hoffmann
Mathematics + Statistical Computing   e-mail: christian.hoffmann at wsl.ch
Swiss Federal Research Institute WSL  Tel: ++41-44-73922-   -77  (office)
CH-8903 Birmensdorf, Switzerland             -11(exchange), -15  (fax)



From alessio.boattini2 at unibo.it  Tue Nov  9 11:59:24 2004
From: alessio.boattini2 at unibo.it (Alessio Boattini)
Date: Tue, 9 Nov 2004 11:59:24 +0100
Subject: [R] gdist and gower distance
Message-ID: <3361928689C6934A8EEC66803C59EE9916CD20@EXBK02.personale.dir.unibo.it>

Dear All,
 
I would like to ask clarifications on the gower distnce matrix calculated by the function gdistin the library mvpart.
Here is a dummy example:
 
> library(mvpart)
Loading required package: survival 
Loading required package: splines 
 mvpart package loaded: extends rpart to include
 multivariate and distance-based partitioning
> x=matrix(1:6, byrow=T, ncol=2)
> x
     [,1] [,2]
[1,]    1    2
[2,]    3    4
[3,]    5    6
> gdist(x, method="euclid")
         1        2
2 2.828427         
3 5.656854 2.828427
 
##########################
doing the calculations by hand according to the formula in gdist help page I get the same results. The formula given is:
 'euclidean'   d[jk] = sqrt(sum (x[ij]-x[ik])^2)
#################################

> sqrt(8)
[1] 2.828427
> gdist(x, method="gower")
          1         2
2 0.7071068          
3 1.4142136 0.7071068
 
#######################################
doing the calculations by hand according to the formula in gdist help page cannot reproduce the same results. The formula given is:
'gower'       d[jk] = sum (abs(x[ij]-x[ik])/(max(i)-min(i))
##########################################
 
Could anybody please shed some light?
 
Regards,
 
Alessio Boattini



From jan.hattendorf at zos.unibe.ch  Tue Nov  9 12:19:01 2004
From: jan.hattendorf at zos.unibe.ch (Jan Hattendorf)
Date: Tue, 09 Nov 2004 12:19:01 +0100
Subject: [R] Some questions to GLMM
Message-ID: <1099999141.4190a7a5b702f@www.cx.unibe.ch>

Hello all R-user

I am relative new to the R-environment and also to GLMM, so please don't be 
irritated if some questions don't make sense.
I am using R 2.0.0 on Windows 2000.

I investigated the occurrence of insects (count) in different parts of 
different plants (plantid) and recorded as well some characteristics of the 
plant parts (e.g. thickness). It is an unbalanced design with 21 plants with 
approx 25 parts each.
Preference of the insects for a certain characteristic is usually unimodal.
As far as I understood, I have to use a model with random intercepts and 
slopes, because the observations within each plant are not independent.

So far so good

========(lme4)=========

glmm1<-GLMM(count~thick+I(thick^2),random=~thick+I(thick^2)
|plantid,poisson,data=Dataset,control=list(PQLmaxIt=10000))
> summary(glmm1)
Generalized Linear Mixed Model

Family: poisson family with log link
Fixed: lixt ~ thick + I(thick^2) 
Data: Dataset 
       AIC      BIC   logLik
 -125.2406 -83.7346 72.62031

Random effects:
 Groups  Name        Variance  Std.Dev. Corr          
 plantid (Intercept) 0.0173455 0.131702               
         thick       0.0389772 0.197426 -1.000        
         I(thick^2)  0.0013327 0.036507  1.000 -1.000 
# of obs: 469, groups: plantid, 21

Estimated scale (compare to 1)  1.402567 

Fixed effects:
             Estimate Std. Error z value  Pr(>|z|)    
(Intercept) -4.045569   0.346950 -11.660 < 2.2e-16 ***
thick        2.378207   0.195425  12.169 < 2.2e-16 ***
I(thick^2)  -0.280898   0.025458 -11.034 < 2.2e-16 ***
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 

Correlation of Fixed Effects:
           (Intr) thick 
thick      -0.968       
I(thick^2)  0.900 -0.977

============================

Question 1: Is the formula suitable for my design?

Question 2: What can be the reason for the positive logLik-value?

Question 3: It is correct, that I do not have to take care about over-
dispersion.

Question 4: When I use "poly(thick,2)" instead of "thick+I(thick^2)" I get 
completly different estimate-values (the latter one are the correct one). I 
thought it should be the same.

============================
> glmm2<-GLMM(count~poly(thick,2),random=~poly(thick,2)
|plantid,poisson,data=Dataset,control=list(PQLmaxIt=10000))
> summary(glmm2)
[...]

Fixed effects:
                 Estimate Std. Error  z value  Pr(>|z|)    
(Intercept)      -0.69293    0.10711  -6.4694 9.837e-11 ***
poly(thick, 2)1 -47.23211    5.97336  -7.9071 2.634e-15 ***
poly(thick, 2)2 -51.21421    4.64972 -11.0145 < 2.2e-16 ***

============================

Question 5: If I use the same formula in glmmPQL, I get more or less similar 
results, but different values for AIC BIC and logLik.
I read in this thread:
http://maths.newcastle.edu.au/~rking/R/help/03b/6849.html
That it should be the same value due to the same algorithm  

Maybe as additional comment, I specified a "NULL-model"
> dummy<-rep(1,469)
> glmm3<-GLMM(count~1,random=~1|dummy,poisson)
resp
> Dataset$dummy<-1
> glmm3<-glmmPQL(count~1,random=~1|dummy,poisson)

and GLMM gave the correct value (logLik = (Null deviance/2) from glm),whereas 
glmmPQL calculated a logLik almost twice as high.


=========(MASS)=====
> glmm1<-glmmPQL(lixt~thick+I(thick^2),random=~thick+I(thick^2)
|plantid,poisson,Dataset)
iteration 1 
[...] 
iteration 10 
> summary(glmm1)

Linear mixed-effects model fit by maximum likelihood
 Data: Dataset 
       AIC      BIC    logLik
  1988.500 2030.006 -984.2502

Random effects:
 Formula: ~thick + I(thick^2) | plantid
 Structure: General positive-definite, Log-Cholesky parametrization
            StdDev     Corr         
(Intercept) 0.27914612 (Intr) thick 
thick       0.03089333 -0.251       
I(thick^2)  0.01867846 -0.878 -0.067
Residual    1.37952490              

Variance function:
 Structure: fixed weights
 Formula: ~invwt 
Fixed effects: lixt ~ thick + I(thick^2) 
                Value Std.Error  DF   t-value p-value
(Intercept) -4.038800 0.4834792 446 -8.353617       0
thick        2.371842 0.2642474 446  8.975837       0
I(thick^2)  -0.279189 0.0337451 446 -8.273479       0
 Correlation: 
           (Intr) thick 
thick      -0.970       
I(thick^2)  0.896 -0.973

Standardized Within-Group Residuals:
       Min         Q1        Med         Q3        Max 
-1.2093444 -0.5972699 -0.3725736  0.2819543  6.1820947 

Number of Observations: 469
Number of Groups: 21 
=============================

Question 6: When I tried method="Laplace" an error occurred:
> g1<-GLMM(count~thick+I(thick^2),random=~thick+I(thick^2)
|plantid,poisson,data=Dataset,method="Laplace")
Using optimizer nlm 
Error: rank deficiency of ZtZ+W detected at column 11

Is that indicating multicollinearity?
Is there is a possibility to avoid this?

I got also from time to time the message

Omega[1] is not positive definite

Can I fix this somehow?

Question 7: Is there a possibility to calculate something like a pseudo-r-
square (e.g. 1-(logLik(Nullmodel)/loglike model)?)

Question 8: When I specify the whole model as fix as e.g:
glm1<-glm(count=(thick+I(thick^2))+(thick+I(thick^2))%in%
plantid,poisson,data=Dataset)
Is the model than wrong or just less powerful? I guess the latter is the case, 
but I would like to be sure. If in this version is just the power decreased, it 
would be very helpful for me to use for the more complex models this approach, 
because errors and warnings are becoming more frequent with more factors and 
covariates.  
 
I know that there are a lot of questions (I am sorry for that), and I don't 
expect that all will be commented. Most interesting for me are the questions 
1,6 and 8.
Thank you very much in advance.
Jan


========================================
Jan Hattendorf
University of Berne 
Zoological Institute
Baltzerstrasse 6 
CH-3012 Berne 
+41-31-631 4523
jan.hattendorf at zos.unibe.ch
http://www.cx.unibe.ch/zos/index.html



From Timur.Elzhov at jinr.ru  Tue Nov  9 12:52:09 2004
From: Timur.Elzhov at jinr.ru (Timur Elzhov)
Date: Tue, 9 Nov 2004 14:52:09 +0300
Subject: [R] R code debugging
Message-ID: <20041109115209.GA20792@nf034.jinr.ru>

Dear useRs,

it's quite difficult for me to find `Error:'s in my R code, because R does
say about error itself, but say nothing about its location, say, string
number and file with an error (which may be `source'd from another file).
Are there any option for turning of the similar feature, or R can not do
such a thing at all?

Thanks.


--
WBR,
Timur.



From ramzi.temanni at laposte.net  Tue Nov  9 12:55:50 2004
From: ramzi.temanni at laposte.net (Ramzi TEMANNI)
Date: Tue, 9 Nov 2004 12:55:50 +0100
Subject: [R] email result as attachement 
Message-ID: <200411091155.iA9Bt4c7001697@smbh.univ-paris13.fr>

Hi,
I??d like to know if it's possible to mail a file (image generated with R) in
attachments a similar to bug.report
Thanks in advance.


TEMANNI Ramzi
PhD Student
Lim&Bio
http://www.limbio-paris13.org
UFR de Sant??, M??decine et Biologie Humaine (SMBH) 
L??onard de Vinci 74, rue Marcel Cachin
93017 Bobigny Cedex
France.
T??l : 01.48.38.73.07
T??l : 06.21.43.27.59
temanni.ramzi at free.fr
http://temanni.ramzi.free.fr



From lars.strand at skogforsk.no  Tue Nov  9 12:59:00 2004
From: lars.strand at skogforsk.no (Lars Strand)
Date: Tue, 9 Nov 2004 12:59:00 +0100
Subject: [R] R under Pocket PC
Message-ID: <000001c4c653$7ff75f80$9cb93c0a@skogforsk.no>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041109/fee36bfe/attachment.pl

From sdavis2 at mail.nih.gov  Tue Nov  9 13:01:08 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Tue, 9 Nov 2004 07:01:08 -0500
Subject: [R] R code debugging
In-Reply-To: <20041109115209.GA20792@nf034.jinr.ru>
References: <20041109115209.GA20792@nf034.jinr.ru>
Message-ID: <0AA41958-3247-11D9-835B-000A95D7BA10@mail.nih.gov>

You are probably interested in traceback().  See ?traceback.  Also, the 
debugger is your friend.  See ?debug.

Sean

On Nov 9, 2004, at 6:52 AM, Timur Elzhov wrote:

> Dear useRs,
>
> it's quite difficult for me to find `Error:'s in my R code, because R 
> does
> say about error itself, but say nothing about its location, say, string
> number and file with an error (which may be `source'd from another 
> file).
> Are there any option for turning of the similar feature, or R can not 
> do
> such a thing at all?
>
> Thanks.
>
>
> --
> WBR,
> Timur.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From parrinel at med.unibs.it  Tue Nov  9 13:03:32 2004
From: parrinel at med.unibs.it (parrinel@med.unibs.it)
Date: Tue, 09 Nov 2004 13:03:32 +0100
Subject: [R] Beta distribution Example
Message-ID: <4190C024.6893.D86BFD@localhost>

Sorry , I had to think a little bit more about..
But I think that the "example" is strange. generally an example ir related to a quite 
general "condition", not to a special case..
Thanks

dr. Giovanni Parrinello
Section of Medical Statistics
Department of Biosciences
University of Brescia
25127 Viale Europa, 11
Brescia Italy
Tel: +390303717528
Fax: +390303701157



From ernesto at ipimar.pt  Tue Nov  9 12:45:36 2004
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Tue, 09 Nov 2004 11:45:36 +0000
Subject: [R] R code debugging
In-Reply-To: <20041109115209.GA20792@nf034.jinr.ru>
References: <20041109115209.GA20792@nf034.jinr.ru>
Message-ID: <1100000736.9684.6.camel@mordor.ipimar.pt>

Hi,

Check R's manual "R Language Definition".

Regards

EJ

On Tue, 2004-11-09 at 11:52, Timur Elzhov wrote:
> Dear useRs,
> 
> it's quite difficult for me to find `Error:'s in my R code, because R does
> say about error itself, but say nothing about its location, say, string
> number and file with an error (which may be `source'd from another file).
> Are there any option for turning of the similar feature, or R can not do
> such a thing at all?
> 
> Thanks.
> 
> 
> --
> WBR,
> Timur.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Tue Nov  9 13:16:14 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 9 Nov 2004 12:16:14 +0000 (GMT)
Subject: [R] R code debugging
In-Reply-To: <20041109115209.GA20792@nf034.jinr.ru>
Message-ID: <Pine.LNX.4.44.0411091211020.16089-100000@gannet.stats>

On Tue, 9 Nov 2004, Timur Elzhov wrote:

> it's quite difficult for me to find `Error:'s in my R code, because R does
> say about error itself, but say nothing about its location, say, string
> number and file with an error (which may be `source'd from another file).
> Are there any option for turning of the similar feature, or R can not do
> such a thing at all?

R code can be created dynamically by R code (called `computing on the
language'), and in most cases the source code is not retained (and it is
not used for execution).

traceback() will always tell you the function in which the error occurred.
If you write reasonably modular code that should suffice, but if not,
use debug() on that function and single-step through it to find where the 
error occurs.  Or set a suitable error handler: have you explored 
recover(), for example?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Nov  9 13:27:49 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 9 Nov 2004 12:27:49 +0000 (GMT)
Subject: [R] R under Pocket PC
In-Reply-To: <000001c4c653$7ff75f80$9cb93c0a@skogforsk.no>
Message-ID: <Pine.LNX.4.44.0411091223400.16452-100000@gannet.stats>

On Tue, 9 Nov 2004, Lars Strand wrote:

> Will R run under Windows Pocket PC?

We don't know!

There are no binary versions of R for that platform, but perhaps you could 
find a suitable compiler and manage to build the sources.

Outside pure mathematics it is usually very hard to establish that
something cannot be done (and it can be very hard in pure mathematics, 
too).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From toherd at tcd.ie  Tue Nov  9 13:30:50 2004
From: toherd at tcd.ie (Deirdre Toher)
Date: Tue,  9 Nov 2004 12:30:50 +0000
Subject: [R] PCA prcomp problem
Message-ID: <1100003450.4190b87a4436f@mymail.tcd.ie>

I've just starting using the prcomp function, and I want to be able to extract
individual principal components (e.g. PC1, PC2) in vector format.  I haven't
been able to find any documentation that explains how to do this (or even if it
is possible).  Any help on the subject would be greatly appreciated.

Many thanks
Deirdre Toher
Teagasc National Food Centre



From HDoran at air.org  Tue Nov  9 13:35:13 2004
From: HDoran at air.org (Doran, Harold)
Date: Tue, 9 Nov 2004 07:35:13 -0500
Subject: [R] no doubt a dumb question, but..
Message-ID: <88EAF3512A55DF46B06B1954AEF73F740655A1B4@dc1ex2.air.org>

It would work if you modified as follows:

plot(lm(Girth~Height))

Or alternatively,

fm1<-lm(Girth~Height);plot(fm1)

Harold

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Dr. John R. Vokey
Sent: Monday, November 08, 2004 10:56 PM
To: r-help at stat.math.ethz.ch
Subject: [R] no doubt a dumb question, but..

Yes, I am a newbie at R, but it is not the complex commands in R that
have me baffled, but simple data commands.  For example, why does
something like:

 > plot(Girth ~ Height)

*not* work after a command that allegedly loads the data:

 > data(trees)

with the error message:

Error in eval(expr, envir, enclos) : Object "Girth" not found

but does work after the command:

 > attach(trees)

?

- JRV
--
There are 10 kinds of people:  those who understand binary, and those
who don't

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From chrysopa at insecta.ufv.br  Tue Nov  9 14:01:34 2004
From: chrysopa at insecta.ufv.br (Ronaldo Reis Jr.)
Date: Tue, 9 Nov 2004 11:01:34 -0200
Subject: [R] glm.nb stop on Error. 
Message-ID: <200411091101.34912.chrysopa@insecta.ufv.br>

Hi,

I make an analysis sequence on R. In some cases the function glm.nb fail to 
ajust the model. Its Ok. The problem is that this error stop the program. I 
need treat this error and not stop the program. Something like this:

...
model <- glm.nb(y~x,maxit=1000)
if(glm.nb fail) {
teste[i] <- 0
}
else {       
teste[i] <- anova(modelo)$"P(>|Chi|)"[2]
}
...

I try this:
...
model <- NULL
model <- glm.nb(y~x,maxit=1000)
## When this fail model is not create, then model <- NULL
if(is.null(model)) {
teste[i] <- 0
}
else {       
teste[i] <- anova(modelo)$"P(>|Chi|)"[2]
}
...

But the problem is the glm.nb error stop the program.

Any idea?

Thanks
Ronaldo
-- 

Existe uma diferen??a entre o acidente e a cat??strofe. O acidente ?? quando cai 
o avi??o. A cat??strofe ?? quando morre todo mundo

--Pel??
--
|>   // | \\   [***********************************]
|   ( ??   ?? )  [Ronaldo Reis J??nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36571-000 Vi??osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-2532                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From ripley at stats.ox.ac.uk  Tue Nov  9 14:25:02 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 9 Nov 2004 13:25:02 +0000 (GMT)
Subject: [R] PCA prcomp problem
In-Reply-To: <1100003450.4190b87a4436f@mymail.tcd.ie>
Message-ID: <Pine.LNX.4.44.0411091319220.23385-100000@gannet.stats>

On Tue, 9 Nov 2004, Deirdre Toher wrote:

> I've just starting using the prcomp function, and I want to be able to extract
> individual principal components (e.g. PC1, PC2) in vector format.  I haven't
> been able to find any documentation that explains how to do this (or even if it
> is possible).  Any help on the subject would be greatly appreciated.

Depends of course what you mean: do you want the coefficients or the data 
predicted onto principal components?

The coefficients are component `rotation' of the result (see the help 
page).

Projections onto the PCs is component `x' of the result (see the help 
page).  E.g.

fit <- prcomp(USArrests, scale = TRUE, ret.x = TRUE)
PC1 <- fit$x[, 1]
PC2 <- fit$x[, 2]

OR predict(fit)  gives the projections of old (or new, with newdata) 
data onto the PCs.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rpeng at jhsph.edu  Tue Nov  9 14:28:59 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Tue, 09 Nov 2004 08:28:59 -0500
Subject: [R] Converting strings to date
In-Reply-To: <009701c4c5c3$b39bc000$5e7aa8c0@FEUPsig.fe.up.pt>
References: <009701c4c5c3$b39bc000$5e7aa8c0@FEUPsig.fe.up.pt>
Message-ID: <4190C61B.6060907@jhsph.edu>

You need to be more specific when you say "it works" =)  strptime() 
returns a POSIXlt object whose printed representation appears to be a 
string (but it is a list).  What you want, I think, is either 
as.POSIXct() or as.Date().  See ?DateTimeClasses.

-roger

Jo??o Mendes Moreira wrote:
> Hello,
> 
> I have the following problem:
> test is a data frame with 9 fields. The field test$Date is factorized with dates. The format is dd-mm-yyyy (using Oracle notation). I want to convert this to Date in '%Y-%m-%d format.
> What I am doing is:
> for (i in 1:nrow(test))
>   {
>    test[i,]$Data<-strptime(substring(test[i,]$Data,1,10),"%d-%m-%Y")
>   }
> 
> test is a data frame. 
> 
> The error is:
> Error in "$<-.data.frame"(`*tmp*`, "Data", value = list(sec = 0, min = 0,  : 
>         replacement has 9 rows, data has 1
> 
> But if I do:
> strptime(substring(test[1:nrow(teste),]$Data,1,10),"%d-%m-%Y")
> it works! Why the assignement does not work?
> 
> Thanks
> 
> Joao Moreira
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From jarioksa at sun3.oulu.fi  Tue Nov  9 14:33:33 2004
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: 09 Nov 2004 15:33:33 +0200
Subject: [R] gdist and gower distance
In-Reply-To: <3361928689C6934A8EEC66803C59EE9916CD20@EXBK02.personale.dir.unibo.it>
References: <3361928689C6934A8EEC66803C59EE9916CD20@EXBK02.personale.dir.unibo.it>
Message-ID: <1100007212.12705.5.camel@biol102145.oulu.fi>

On Tue, 2004-11-09 at 12:59, Alessio Boattini wrote:
> Dear All,
>  
> I would like to ask clarifications on the gower distnce matrix calculated by the function gdistin the library mvpart.
> Here is a dummy example:
>  
> > library(mvpart)
> Loading required package: survival 
> Loading required package: splines 
>  mvpart package loaded: extends rpart to include
>  multivariate and distance-based partitioning
> > x=matrix(1:6, byrow=T, ncol=2)
> > x
>      [,1] [,2]
> [1,]    1    2
> [2,]    3    4
> [3,]    5    6
> > gdist(x, method="euclid")
>          1        2
> 2 2.828427         
> 3 5.656854 2.828427
>  
> ##########################
> doing the calculations by hand according to the formula in gdist help page I get the same results. The formula given is:
>  'euclidean'   d[jk] = sqrt(sum (x[ij]-x[ik])^2)
> #################################
> 
> > sqrt(8)
> [1] 2.828427
> > gdist(x, method="gower")
>           1         2
> 2 0.7071068          
> 3 1.4142136 0.7071068
>  
> #######################################
> doing the calculations by hand according to the formula in gdist help page cannot reproduce the same results. The formula given is:
> 'gower'       d[jk] = sum (abs(x[ij]-x[ik])/(max(i)-min(i))
> ##########################################
>  
> Could anybody please shed some light?
>  

There seems to be a bug in documentation. The function uses different
calculation than the help page specifies. Look at the 'gdist' code. Just
to make things easier: In the function body, gower is method 6, and
Euclidean distances are method 2.

Gower's original paper is available through http://www.jstor.org/
(Biometrics Vol. 27, No. 4, p. 857-871; 1971).

cheers, jari oksanen
-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From p.drake at beatson.gla.ac.uk  Tue Nov  9 14:46:14 2004
From: p.drake at beatson.gla.ac.uk (Paul JH Drake)
Date: Tue, 09 Nov 2004 13:46:14 +0000
Subject: [R] Multiple stripcharts using "for loop"
Message-ID: <1100007974.9427.30.camel@G4.site>

Hi
I'm able to create multiple plots from a dataset using a "for loop" to
change variables, but when I try with stripchart it doesn't work.

Standard boxplot:
for(i in 2:14){boxplot(klk[,i]~Group,main=colnames(klk)[i])

Stripchart:
for(i in 2:14){stripchart(klk[,i]~Group,main=colnames(klk)[i])

gives:Error in plot.window(xlim, ylim, log, asp, ...) : 
        need finite xlim values
In addition: Warning messages: 
1: no finite arguments to min; returning Inf 
2: no finite arguments to max; returning -Inf 
3: no finite arguments to min; returning Inf 
4: no finite arguments to max; returning -Inf 


Does anyone know how to do this properly?

Thanks in advance

Paul



From mjw at celos.net  Tue Nov  9 14:46:23 2004
From: mjw at celos.net (Mark White)
Date: Tue, 9 Nov 2004 13:46:23 +0000
Subject: [R] Extension provokes crash in unzReadCurrentFile
In-Reply-To: <x2oei8fm0e.fsf@biostat.ku.dk>
References: <Pine.LNX.4.44.0411081341470.3330-100000@gannet.stats>
	<x2oei8fm0e.fsf@biostat.ku.dk>
Message-ID: <20041109134623.GA6490@celos.net>

> > On Mon, 8 Nov 2004, Mark White wrote:
> > > Any idea what might be causing a compressed file read?

Prof Brian Ripley writes:
> > It's not a compressed file read, but rather from the interface for unz()  
> > connections to zip files.  That is very unlikely to be used on a
> > non-Windows system, so if that really is being called this looks like
> > considerable internal corruption.
>
Peter Dalgaard writes:
> In any case, since you're obviously running a more than half-decent
> operating system, and you seem to know how to operate the debugger
> (gdb?), could you please tell us what the backtrace ("bt", if gdb)
> says at the point of crash.

Sure -- trace from a (fairly typical) core dump below.

However, I tentatively think I've found the problem now:
hearing that this func shouldn't be called at all gave me a
few other ideas.  Changing the length of a large test vector
completely changed the top few functions in the trace, and I
then found an R_MakeExternalPtr with the wrong protection
argument, causing the GC to sometimes clear away the vector
before it was next accessed.

Haven't seen a crash since then, some hopefully that was it.
Thanks for your help.

Mark <><

--------------------------------------------------

(gdb) where
#0  0x80697c8 in do_attributesgets (call=0x1737d000, op=0x81fc5f0, args=0x0, 
    env=0x48371736) at attrib.c:744
#1  0x48371758 in ?? ()
#2  0x48371dd1 in ?? ()
#3  0x809c471 in unzReadCurrentFile (file=0x8ce4e7c, buf=0x8225d9c, 
    len=141386072) at dounzip.c:1248
#4  0x80acf7d in bcEval (body=0x8ce4e7c, rho=0x86d6e7c) at eval.c:2811
#5  0x80af0b9 in bcEval (body=0x8ce4e0c, rho=0x86d6e7c) at eval.c:3061
#6  0x80ae553 in bcEval (body=0x8ce4df0, rho=0x81fc040) at eval.c:3055
#7  0x80acdd7 in bcEval (body=0x8ce4df0, rho=0x86d6e7c) at eval.c:2808
#8  0x80ae518 in bcEval (body=0x8ce4f78, rho=0x81fd9c8) at eval.c:3055
#9  0x80acdd7 in bcEval (body=0x8ce4f78, rho=0x86d6e7c) at eval.c:2808
#10 0x80ad39e in bcEval (body=0x86d6ae0, rho=0x8de5f24) at eval.c:2884
#11 0x80d5b66 in nmmin (n=141388512, Bvec=0x8de5f24, X=0x86d6904, 
    Fmin=0x8dab894, fminfn=0x86d6894, fail=0x81fc7b0, 
    abstol=6.9523080897444509e-270, intol=5.8871398777326369e-266, 
    ex=0x86d6894, alpha=4.4536092707482937e-268, bet=8.1937066744817333e-267, 
    gamm=1.5190943468362191e-309, trace=2, fncount=0xbfbfc03c, 
    maxit=-1077951856) at optim.c:795
#12 0x80d6434 in lbfgsb (n=136004430, m=149385256, x=0x8b0e8b0, l=0x86d6904, 
    u=0x8dab894, nbd=0x8dab894, Fmin=0x81fc7b0, fminfn=0xbfbfc348, 
    fmingr=0x8dab894, fail=0x81fd8b0, ex=0x8b0e8b0, 
    factr=-NaN(0xffffa1bfb9000), pgtol=6.9650001374865742e-174, 
    fncount=0x8128faf, grcount=0x2, maxit=0, msg=0x8e77028 "??", 
    trace=-1077950876, nREPORT=20) at optim.c:1025
#13 0x80afb55 in bcEval (body=0x8b0e8b0, rho=0x81fd8b0) at eval.c:3063
#14 0x8129c6e in Rf_initialize_R (ac=145811632, av=0x81fd8b0) at system.c:170
#15 0x80acdd7 in bcEval (body=0x8b0e8b0, rho=0x8dab894) at eval.c:2808
#16 0x80aecad in bcEval (body=0x8b0e904, rho=0x81fdac4) at eval.c:3060
#17 0x80acdd7 in bcEval (body=0x8b0e904, rho=0x8dab894) at eval.c:2808
#18 0x80ae518 in bcEval (body=0x8b0e93c, rho=0x81fd9c8) at eval.c:3055
#19 0x80acdd7 in bcEval (body=0x8b0e93c, rho=0x8dab894) at eval.c:2808
#20 0x80ae0e4 in bcEval (body=0x8b0ea00, rho=0x81fc190) at eval.c:3052
#21 0x80acdd7 in bcEval (body=0x8b0ea00, rho=0x8dab894) at eval.c:2808
#22 0x80ae518 in bcEval (body=0x8b0e564, rho=0x81fd9c8) at eval.c:3055
#23 0x80acdd7 in bcEval (body=0x8b0e564, rho=0x8dab894) at eval.c:2808
#24 0x80ad39e in bcEval (body=0x8dac75c, rho=0x8b0e59c) at eval.c:2884
#25 0x80acff6 in bcEval (body=0x8dac75c, rho=0x822cae0) at eval.c:2812
#26 0x80ae518 in bcEval (body=0x8dac778, rho=0x81fd9c8) at eval.c:3055
#27 0x80acdd7 in bcEval (body=0x8dac778, rho=0x822cae0) at eval.c:2808
#28 0x80ae271 in bcEval (body=0x8dab824, rho=0x81fc1e4) at eval.c:3053
#29 0x80acdd7 in bcEval (body=0x8dab824, rho=0x822cae0) at eval.c:2808
#30 0x80c9a7d in Rf_allocVector (type=136497888, length=0) at memory.c:1787
#31 0x80c9bc7 in Rf_allocVector (type=136497888, length=0) at memory.c:1820
#32 0x80ca402 in Rf_protect (s=0x3001f) at memory.c:2074
#33 0x80ca41c in Rf_unprotect (l=0) at memory.c:2082
#34 0x805c3ad in do_getRtoCConverterDescriptions (call=0x1, op=0xbfbfd220, 
    args=0xbfbfd228, env=0x246) at CConverters.c:244
#35 0x805c1a0 in R_addToCConverter (matcher=0x1, converter=0xbfbfd220, 
    reverse=0xbfbfd228, userData=0x481c0bc0, desc=0x481cd200 "z??P??\001")
    at CConverters.c:80
(gdb)



From MSchwartz at MedAnalytics.com  Tue Nov  9 14:46:59 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Tue, 09 Nov 2004 07:46:59 -0600
Subject: [R] no doubt a dumb question, but..
In-Reply-To: <16784.31434.711805.31472@gargle.gargle.HOWL>
References: <4817319E-3203-11D9-A057-000A95CDE4AC@uleth.ca>
	<1099974466.5607.22.camel@localhost.localdomain>
	<16784.31434.711805.31472@gargle.gargle.HOWL>
Message-ID: <1100008019.5607.54.camel@localhost.localdomain>

On Tue, 2004-11-09 at 02:07, Martin Maechler wrote:

<SNIP>

> 
> Further note that many of us try to avoid attach()ing data frames
> (most of the time; not always) and we have provided the nice alternative
>     with( <data> ,  <expression_body> ) 
> 
> So, for the current example, you could also say
> 
>   with(trees, plot(Girth ~ Height))
> 
> The main advantage of with(): Only inside it, the components of
> 'trees' are visible - no need to remember to  detach() ;
> see also  help(with) and its examples.

Right. Thanks Martin. After sending my reply, I kicked myself a few
times for not mentioning with()...especially because it also enables a
standardized mechanism for accessing data frame variables across
functions (not all of which have a 'data = ' argument).

Also, there is a substantial savings with respect to overhead by not
attaching and detaching (manipulating the search path), which I had
noted and benchmarked previously in a post, earlier this year I believe,
that I cannot locate at the moment. 

This can become very important if one is looping (in some fashion) over
data frames or subsets of data frames and therefore avoids the repeated
calls to attach() and detach().

Best regards,

Marc



From bxc at steno.dk  Tue Nov  9 15:01:36 2004
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Tue, 9 Nov 2004 15:01:36 +0100
Subject: [R] survSplit: further exploration and related topics
Message-ID: <0ABD88905D18E347874E0FB71C0B29E90237D0D3@exdkba022.novo.dk>

To Danardonos concern of splitting time for records with delayed entry:

This can fairly easily be accomodated, by simply splitting time in small
intervals of time since entry into the study, and then compute the value
of the other timescales for each of these e.g.:

current.age <- time.from.entry + age.at.entry

but the cut on the other timescales will not be exactly where you may
want 
them to be, but this should be of minor importance in real life.

This approach will also make it clearer what really is going on.

The effect of each of the timescales can then be modelled using the
usual
regression tools available in R.

David Clayton har written an R-function that does it correctly, you can
find it in:
http://biostat.ku.dk/~bxc/Lexis/ along with its .Rd file.

It is also included a Lexis-package which is a first shot at an
epidemiology package 
for R available at http://biostat.ku.dk/~bxc/SPE/library/, but built
under 1.9.
I recently tried to build it under 2.0, but it crashed for me and I was
advised to
wait till 2.1 before I had another go at it.

A little further exploration of what goes on in survSplit gives:
> data(aml)
> aml$id <- letters[1:dim(aml)[1]]
> aml3<-survSplit(aml,cut=c(5,10,50),end="time",start="start",
+                     event="status",episode="i")
> # Sort the rows sensibly
> aml3 <- aml3[order(aml3$id,aml3$start),]
> aml3$expand <- 1:(dim(aml3)[1])
> aml3[aml3$id=="k",c("id","expand","start","time","status","x")]
   id expand start time status          x
11  k     30     0    5      0 Maintained
34  k     31     5   10      0 Maintained
57  k     32    10   50      0 Maintained
80  k     33    50  161      0 Maintained
> 
> # All records are taken as if entry were at time 0:
> aml4<-survSplit(aml3,cut=c(9,12,40),end="time",start="entry",
+                      event="status",episode="i",id="id2")
> aml4 <- aml4[order(aml4$id,aml4$expand,aml4$start),]
>
aml4[aml4$id=="k",c("id","expand","start","entry","time","status","x")]
    id expand start entry time status          x
30   k     30     0     0    5      0 Maintained
31   k     31     5     0    9      0 Maintained
94   k     31     5     9   10      0 Maintained
32   k     32    10     0    9      0 Maintained
95   k     32    10     9   12      0 Maintained
158  k     32    10    12   40      0 Maintained
221  k     32    10    40   50      0 Maintained
33   k     33    50     0    9      0 Maintained
96   k     33    50     9   12      0 Maintained
159  k     33    50    12   40      0 Maintained
222  k     33    50    40  161      0 Maintained
> 
> # entry time is is "start", but it seems that it is more or less
> # assumed that entry is at the first split point given.
> aml5<-survSplit(aml3,cut=c(9,12,40),end="time",start="start",
+                      event="status",episode="i",id="id2")
> aml5 <- aml5[order(aml5$id,aml5$expand,aml5$start),]
> aml5[aml5$id=="k",c("id","expand","start","time","status","x")]
    id expand start time status          x
30   k     30     0    5      0 Maintained
31   k     31     5    9      0 Maintained
94   k     31     9   10      0 Maintained
95   k     32     9   12      0 Maintained
158  k     32    12   40      0 Maintained
221  k     32    40   50      0 Maintained
96   k     33     9   12      0 Maintained
159  k     33    12   40      0 Maintained
222  k     33    40  161      0 Maintained
> 

It appears that the intention has been to support counting process
input, 
but not quite succeeded.

Bendix Carstensen
----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc
----------------------



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Thomas Lumley
> Sent: Monday, November 08, 2004 4:28 PM
> To: Danardono
> Cc: R-help at stat.math.ethz.ch
> Subject: Re: [R] survSplit
> 
> 
> On Mon, 8 Nov 2004, Danardono wrote:
> 
> > I am just realized that  survival has the facility to do 
> survival time
> > splitting survSplit
> > after read some postings about  time dependency  in the list.
> > Is it survSplit only for the survival data input 
> (time,status)  and not for 
> > the 'counting process' input (start,stop,status)?
> >
> > I take one example modified from the survSplit help:
> >> data(aml)
> >> 
> aml3<-survSplit(aml,cut=c(5,10,50),end="time",start="start",event="st
> >> atus",episode="i",id="id")
> >
> >> coxph(Surv(time,status)~x,data=aml)
> >
> >               coef exp(coef) se(coef)    z     p
> > xNonmaintained 0.916       2.5    0.512 1.79 0.074
> >
> > Likelihood ratio test=3.38  on 1 df, p=0.0658  n= 23
> >
> > #the same
> >> coxph(Surv(time,status)~x,data=aml3)
> >
> >               coef exp(coef) se(coef)    z     p
> > xNonmaintained 0.916       2.5    0.512 1.79 0.074
> >
> > Likelihood ratio test=3.38  on 1 df, p=0.0658  n= 63
> 
> This should NOT be the same, and on my computer is not.  You 
> should have 
> to use the counting-process syntax.  I get
> 
> > coxph(Surv(time,status)~x,data=aml3)
> Call:
> coxph(formula = Surv(time, status) ~ x, data = aml3)
> 
> 
>                 coef exp(coef) se(coef)    z     p
> xNonmaintained 1.07      2.92    0.514 2.08 0.037
> 
> Likelihood ratio test=4.61  on 1 df, p=0.0317  n= 63
> > coxph(Surv(start,time,status)~x,data=aml3)
> Call:
> coxph(formula = Surv(start, time, status) ~ x, data = aml3)
> 
> 
>                  coef exp(coef) se(coef)    z     p
> xNonmaintained 0.916       2.5    0.512 1.79 0.074
> 
> Likelihood ratio test=3.38  on 1 df, p=0.0658  n= 63
> 
> 
> 
> > BUT If I split aml3 further:
> >
> >> 
> aml4<-survSplit(aml3,cut=c(9,12,40),end="time",start="start",event="s
> >> tatus",episode="i",id="id2")
> > #not the same!
> >> coxph(Surv(start,time,status)~x,data=aml4)
> >              coef exp(coef) se(coef)    z     p
> > xNonmaintained 1.05      2.85    0.515 2.03 0.042
> >
> > Likelihood ratio test=4.38  on 1 df, p=0.0363  n= 105
> >
> 
> Hmm.  I suspect this is a <= vs < bug of some sort in handling start 
> times.
> 
>  	-thomas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From asemeria at cramont.it  Tue Nov  9 15:17:07 2004
From: asemeria at cramont.it (asemeria@cramont.it)
Date: Tue, 9 Nov 2004 15:17:07 +0100
Subject: [R] small world models?
Message-ID: <OFEAC1550D.60660650-ONC1256F47.00350167@tomware.it>



From abunn at whrc.org  Tue Nov  9 15:23:25 2004
From: abunn at whrc.org (Andy Bunn)
Date: Tue, 9 Nov 2004 09:23:25 -0500
Subject: [R] Error in update() when repositories is specified.
Message-ID: <NEBBIPHDAMMOKDKPOFFIMENACLAA.abunn@whrc.org>

How can I specify the repositories for upgrade()?

> x <- packageStatus(repositories =
"http://cran.us.r-project.org//bin/windows/contrib/2.0")
> upgrade(x, ask = FALSE)
Error in update[, 3] : incorrect number of dimensions

x, the object of class "packageStatus", prints and summarizes fine. I also
ran install.packages() without incident.
> install.packages(x$avail$Package[x$avail$Status == "not installed"])

I get the same error if I specify the repositories with getOption:
> getOption("repositories")()[[1]]
[1] "http://cran.us.r-project.org//bin/windows/contrib/2.0"
> x <- packageStatus(repositories = getOption("repositories")()[[1]])
> upgrade(x, ask = FALSE)
Error in update[, 3] : incorrect number of dimensions

However, upgrade behaves if I don't specify "repositories" in
packageStatus() things work as expected.
> x <- packageStatus()
> upgrade(x)

I imagine that I am being a dimwit as per usual. Can anybody help?

-Andy



From tlumley at u.washington.edu  Tue Nov  9 16:03:07 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 9 Nov 2004 07:03:07 -0800 (PST)
Subject: [R] survSplit: further exploration and related topics
In-Reply-To: <0ABD88905D18E347874E0FB71C0B29E90237D0D3@exdkba022.novo.dk>
References: <0ABD88905D18E347874E0FB71C0B29E90237D0D3@exdkba022.novo.dk>
Message-ID: <Pine.A41.4.61b.0411090659250.226980@homer11.u.washington.edu>

On Tue, 9 Nov 2004, BXC (Bendix Carstensen) wrote:

> To Danardonos concern of splitting time for records with delayed entry:
>

There was actually no intention of supporting counting-process input, but 
it turns out to be a one-line change: from
        starttime <- c(starttime, rep(cut, each = n))
to
        starttime<-c(starttime, pmax(starttime, rep(cut,each=n)))

about 20 lines down from the top.  This will be in the next release of 
survival, but not until after R 2.0.1


 	-thomas



From bates at stat.wisc.edu  Tue Nov  9 16:04:28 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 09 Nov 2004 09:04:28 -0600
Subject: [R] Some questions to GLMM
In-Reply-To: <1099999141.4190a7a5b702f@www.cx.unibe.ch>
References: <1099999141.4190a7a5b702f@www.cx.unibe.ch>
Message-ID: <4190DC7C.1090409@stat.wisc.edu>

Jan Hattendorf wrote:
> Hello all R-user
> 
> I am relative new to the R-environment and also to GLMM, so please don't be 
> irritated if some questions don't make sense.
> I am using R 2.0.0 on Windows 2000.
> 
> I investigated the occurrence of insects (count) in different parts of 
> different plants (plantid) and recorded as well some characteristics of the 
> plant parts (e.g. thickness). It is an unbalanced design with 21 plants with 
> approx 25 parts each.
> Preference of the insects for a certain characteristic is usually unimodal.
> As far as I understood, I have to use a model with random intercepts and 
> slopes, because the observations within each plant are not independent.
> 
> So far so good
> 
> ========(lme4)=========
> 
> glmm1<-GLMM(count~thick+I(thick^2),random=~thick+I(thick^2)
> |plantid,poisson,data=Dataset,control=list(PQLmaxIt=10000))
> 
>>summary(glmm1)
> 
> Generalized Linear Mixed Model
> 
> Family: poisson family with log link
> Fixed: lixt ~ thick + I(thick^2) 
> Data: Dataset 
>        AIC      BIC   logLik
>  -125.2406 -83.7346 72.62031
> 
> Random effects:
>  Groups  Name        Variance  Std.Dev. Corr          
>  plantid (Intercept) 0.0173455 0.131702               
>          thick       0.0389772 0.197426 -1.000        
>          I(thick^2)  0.0013327 0.036507  1.000 -1.000 
> # of obs: 469, groups: plantid, 21
> 
> Estimated scale (compare to 1)  1.402567 
> 
> Fixed effects:
>              Estimate Std. Error z value  Pr(>|z|)    
> (Intercept) -4.045569   0.346950 -11.660 < 2.2e-16 ***
> thick        2.378207   0.195425  12.169 < 2.2e-16 ***
> I(thick^2)  -0.280898   0.025458 -11.034 < 2.2e-16 ***
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 
> 
> Correlation of Fixed Effects:
>            (Intr) thick 
> thick      -0.968       
> I(thick^2)  0.900 -0.977
> 
> ============================
> 
> Question 1: Is the formula suitable for my design?

My guess is that your model is overparameterized.  Notice that the 
estimated correlations of the random effects are all near the extremes 
of -1 or +1.  I would start with a model that had fewer random effects 
terms.

> Question 2: What can be the reason for the positive logLik-value?

It is a common misconception that a log-likelihood must be negative.  In 
fact, a log-likelihood can be positive when it is based on a probability 
density.  Probabilities cannot exceed one but probability densities can. 
  In this case the log-likelihood is a combination of the probability 
density for the random effects and the conditional probability of the 
observations.

> Question 3: It is correct, that I do not have to take care about over-
> dispersion.

There is an indication of overdispersion but I would not worry about 
that until I could get a handle on the random effects terms.

> Question 4: When I use "poly(thick,2)" instead of "thick+I(thick^2)" I get 
> completly different estimate-values (the latter one are the correct one). I 
> thought it should be the same.

They fit the same model but with a different set of coefficients hence 
the estimated values are different.

> ============================
> 
>>glmm2<-GLMM(count~poly(thick,2),random=~poly(thick,2)
> 
> |plantid,poisson,data=Dataset,control=list(PQLmaxIt=10000))
> 
>>summary(glmm2)
> 
> [...]
> 
> Fixed effects:
>                  Estimate Std. Error  z value  Pr(>|z|)    
> (Intercept)      -0.69293    0.10711  -6.4694 9.837e-11 ***
> poly(thick, 2)1 -47.23211    5.97336  -7.9071 2.634e-15 ***
> poly(thick, 2)2 -51.21421    4.64972 -11.0145 < 2.2e-16 ***
> 
> ============================
> 
> Question 5: If I use the same formula in glmmPQL, I get more or less similar 
> results, but different values for AIC BIC and logLik.
> I read in this thread:
> http://maths.newcastle.edu.au/~rking/R/help/03b/6849.html
> That it should be the same value due to the same algorithm  

The GLMM function evaluates the likelihood at the PQL estimates using 
the Laplacian approximation so the result will be different from that 
returned by glmmPQL.

> Maybe as additional comment, I specified a "NULL-model"
> 
>>dummy<-rep(1,469)
>>glmm3<-GLMM(count~1,random=~1|dummy,poisson)
> 
> resp
> 
>>Dataset$dummy<-1
>>glmm3<-glmmPQL(count~1,random=~1|dummy,poisson)
> 
> 
> and GLMM gave the correct value (logLik = (Null deviance/2) from glm),whereas 
> glmmPQL calculated a logLik almost twice as high.
> 
> 
> =========(MASS)=====
> 
>>glmm1<-glmmPQL(lixt~thick+I(thick^2),random=~thick+I(thick^2)
> 
> |plantid,poisson,Dataset)
> iteration 1 
> [...] 
> iteration 10 
> 
>>summary(glmm1)
> 
> 
> Linear mixed-effects model fit by maximum likelihood
>  Data: Dataset 
>        AIC      BIC    logLik
>   1988.500 2030.006 -984.2502
> 
> Random effects:
>  Formula: ~thick + I(thick^2) | plantid
>  Structure: General positive-definite, Log-Cholesky parametrization
>             StdDev     Corr         
> (Intercept) 0.27914612 (Intr) thick 
> thick       0.03089333 -0.251       
> I(thick^2)  0.01867846 -0.878 -0.067
> Residual    1.37952490              
> 
> Variance function:
>  Structure: fixed weights
>  Formula: ~invwt 
> Fixed effects: lixt ~ thick + I(thick^2) 
>                 Value Std.Error  DF   t-value p-value
> (Intercept) -4.038800 0.4834792 446 -8.353617       0
> thick        2.371842 0.2642474 446  8.975837       0
> I(thick^2)  -0.279189 0.0337451 446 -8.273479       0
>  Correlation: 
>            (Intr) thick 
> thick      -0.970       
> I(thick^2)  0.896 -0.973
> 
> Standardized Within-Group Residuals:
>        Min         Q1        Med         Q3        Max 
> -1.2093444 -0.5972699 -0.3725736  0.2819543  6.1820947 
> 
> Number of Observations: 469
> Number of Groups: 21 
> =============================
> 
> Question 6: When I tried method="Laplace" an error occurred:
> 
>>g1<-GLMM(count~thick+I(thick^2),random=~thick+I(thick^2)
> 
> |plantid,poisson,data=Dataset,method="Laplace")
> Using optimizer nlm 
> Error: rank deficiency of ZtZ+W detected at column 11

> Is that indicating multicollinearity?

No, it is an indication that the variance-covariance of the random 
effects is not positive definite.

> Is there is a possibility to avoid this?

You will need to reduce the number of random effects terms.

> I got also from time to time the message
> 
> Omega[1] is not positive definite
> 
> Can I fix this somehow?

Same as above.

> Question 7: Is there a possibility to calculate something like a pseudo-r-
> square (e.g. 1-(logLik(Nullmodel)/loglike model)?)
> 
> Question 8: When I specify the whole model as fix as e.g:
> glm1<-glm(count=(thick+I(thick^2))+(thick+I(thick^2))%in%
> plantid,poisson,data=Dataset)
> Is the model than wrong or just less powerful? I guess the latter is the case, 
> but I would like to be sure. If in this version is just the power decreased, it 
> would be very helpful for me to use for the more complex models this approach, 
> because errors and warnings are becoming more frequent with more factors and 
> covariates.  
>  
> I know that there are a lot of questions (I am sorry for that), and I don't 
> expect that all will be commented. Most interesting for me are the questions 
> 1,6 and 8.
> Thank you very much in advance.
> Jan
> 
> 
> ========================================
> Jan Hattendorf
> University of Berne 
> Zoological Institute
> Baltzerstrasse 6 
> CH-3012 Berne 
> +41-31-631 4523
> jan.hattendorf at zos.unibe.ch
> http://www.cx.unibe.ch/zos/index.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From abunn at whrc.org  Tue Nov  9 16:08:10 2004
From: abunn at whrc.org (Andy Bunn)
Date: Tue, 9 Nov 2004 10:08:10 -0500
Subject: [R] Error in update() when repositories is specified.
In-Reply-To: <NEBBIPHDAMMOKDKPOFFIMENACLAA.abunn@whrc.org>
Message-ID: <NEBBIPHDAMMOKDKPOFFICENECLAA.abunn@whrc.org>

Ooops. Make that subject line 'upgrade', not 'update!' Sorry. -AB

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Andy Bunn
> Sent: Tuesday, November 09, 2004 9:23 AM
> To: R-Help
> Subject: [R] Error in update() when repositories is specified.
>
>
> How can I specify the repositories for upgrade()?
>
> > x <- packageStatus(repositories =
> "http://cran.us.r-project.org//bin/windows/contrib/2.0")
> > upgrade(x, ask = FALSE)
> Error in update[, 3] : incorrect number of dimensions
>
> x, the object of class "packageStatus", prints and summarizes fine. I also
> ran install.packages() without incident.
> > install.packages(x$avail$Package[x$avail$Status == "not installed"])
>
> I get the same error if I specify the repositories with getOption:
> > getOption("repositories")()[[1]]
> [1] "http://cran.us.r-project.org//bin/windows/contrib/2.0"
> > x <- packageStatus(repositories = getOption("repositories")()[[1]])
> > upgrade(x, ask = FALSE)
> Error in update[, 3] : incorrect number of dimensions
>
> However, upgrade behaves if I don't specify "repositories" in
> packageStatus() things work as expected.
> > x <- packageStatus()
> > upgrade(x)
>
> I imagine that I am being a dimwit as per usual. Can anybody help?
>
> -Andy
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From tlumley at u.washington.edu  Tue Nov  9 16:24:19 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 9 Nov 2004 07:24:19 -0800 (PST)
Subject: [R] Conditional selection of rows 
In-Reply-To: <BAY2-F42JrzdDW6lCPx00011e81@hotmail.com>
References: <BAY2-F42JrzdDW6lCPx00011e81@hotmail.com>
Message-ID: <Pine.A41.4.61b.0411090720340.226980@homer11.u.washington.edu>

On Tue, 9 Nov 2004, F Z wrote:

> Hi,
>
> I have a data.frame with several variables and 50,000 observations.
> i.e.
> data[1:2,1:7]
> Iteration Day Production.Type tsUSusc tsASusc tsULat tsALat
>        1   0         Generic   17965 8833053      0      0
>        1   1         Generic   17965 8833053      0      0
>        .
>        .
>        .
>        1 199         Generic   17237 8141028     26  23131
>        2 127         Generic   15828 7307583     92  63463
>
> I would like to extract only the observations (rows) for the last "day" for 
> each "iteration" and store them in a data frame.
>
> I tried lapply nested in a for loop without success.  Any help will be 
> greatly appreciated!

If you reverse the ordering you are then looking for the first Day in each 
Iteration, which can be done efficiently with duplicated().

data <- data[order(data$Iteration, data$Day, decreasing=TRUE),]

subset <- data[!duplicated(data$Iteration),]

If you are sure that the data are in order to begin with you could just 
reverse the entire data set (  data <- data[nrow(data):1,] ), but I'm 
always reluctant to assume this.

 	-thomas



From p.dalgaard at biostat.ku.dk  Tue Nov  9 16:25:42 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 09 Nov 2004 16:25:42 +0100
Subject: [R] R under Pocket PC
In-Reply-To: <Pine.LNX.4.44.0411091223400.16452-100000@gannet.stats>
References: <Pine.LNX.4.44.0411091223400.16452-100000@gannet.stats>
Message-ID: <x2mzxrf3ih.fsf@biostat.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> On Tue, 9 Nov 2004, Lars Strand wrote:
> 
> > Will R run under Windows Pocket PC?
> 
> We don't know!
> 
> There are no binary versions of R for that platform, but perhaps you could 
> find a suitable compiler and manage to build the sources.

The PDA idea is quite interesting on all sorts of hardware.
Unfortunately, the tools for (cross-)building software tend to be
rather "hackish" and poorly documented. I believe people have had some
success with the Sharp Zaurus, but that of course runs a Linux
variant. Looks like at least some toolchains are around for WinCE, but
the bad news is that WinCE is a stripped-down Windows, so you are
likely to find that the Windows sources do not just work. A console
version could be fairly simple, but I wouldn't know for sure.

Googling for "pocketgcc"  chased up  some interesting stuff.
"wince+tcltk" suggested that there is a functional tcltk, which means
that you might use that for an initial GUI and - who knows - maybe
tkrplot for the graphics. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From vak at biovision-discovery.de  Tue Nov  9 16:56:44 2004
From: vak at biovision-discovery.de (Valery A.Khamenya)
Date: Tue, 09 Nov 2004 16:56:44 +0100
Subject: [R] list -> environment coercion
Message-ID: <4190E8BC.6080300@biovision-discovery.de>

Hi all,

there is "environment->list" coercion, i.e. as.list.environment.

Is there any "list->environment" coercion?

thank you.
--
Valery.



From tjrc at sanger.ac.uk  Tue Nov  9 17:06:19 2004
From: tjrc at sanger.ac.uk (Tim Cutts)
Date: Tue, 9 Nov 2004 16:06:19 +0000
Subject: [R] R under Pocket PC
In-Reply-To: <Pine.LNX.4.44.0411091223400.16452-100000@gannet.stats>
References: <Pine.LNX.4.44.0411091223400.16452-100000@gannet.stats>
Message-ID: <4B448BDE-3269-11D9-A207-000A95B2B140@sanger.ac.uk>


On 9 Nov 2004, at 12:27 pm, Prof Brian Ripley wrote:

> On Tue, 9 Nov 2004, Lars Strand wrote:
>
>> Will R run under Windows Pocket PC?
>
> We don't know!
>
> There are no binary versions of R for that platform, but perhaps you 
> could
> find a suitable compiler and manage to build the sources.
>
> Outside pure mathematics it is usually very hard to establish that
> something cannot be done (and it can be very hard in pure mathematics,
> too).

Do PocketPCs generally have enough memory to run something as big as R? 
  A standard R install requires a lot of storage space, by PDA 
standards...

Technically, I don't suppose there's much to stop R running on some 
PDAs, especially those based on Linux like the Sharp Zaurus, other than 
storage and memory requirements.  If there's a Qt graphical interface 
available for R, you could even get graphics working on the Zaurus, 
potentially.  The Windows API on Pocket PC is quite a reduced subset 
compared to full Windows, so you might have problems with that.  PalmOS 
would probably be right out.  :-)

Tim

-- 
Dr Tim Cutts
Informatics Systems Group, Wellcome Trust Sanger Institute
GPG: 1024D/E3134233 FE3D 6C73 BBD6 726A A3F5  860B 3CDD 3F56 E313 4233



From chrysopa at insecta.ufv.br  Tue Nov  9 17:12:24 2004
From: chrysopa at insecta.ufv.br (Ronaldo Reis Jr.)
Date: Tue, 9 Nov 2004 14:12:24 -0200
Subject: R: [R] glm.nb stop on Error.
In-Reply-To: <002d01c4c666$30095ba0$5c13070a@PROCGEN>
References: <200411091101.34912.chrysopa@insecta.ufv.br>
	<002d01c4c666$30095ba0$5c13070a@PROCGEN>
Message-ID: <200411091412.24682.chrysopa@insecta.ufv.br>

Em Tuesday 09 November 2004 12:12, Vito Muggeo escreveu:
> if(class(model)=="try-error")

Thanks

it works fine :)

Ronaldo
-- 
O importante n??o ?? ganhar. O que importa ?? competir
sem perder nem empatar.
--
|>   // | \\   [***********************************]
|   ( ??   ?? )  [Ronaldo Reis J??nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36571-000 Vi??osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-2532                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From hli at vbi.vt.edu  Tue Nov  9 17:24:46 2004
From: hli at vbi.vt.edu (Hua Li)
Date: Tue, 09 Nov 2004 11:24:46 -0500
Subject: [R] increase memory size on Unix
Message-ID: <5.1.0.14.0.20041109112235.00ad45d0@mail.vbi.vt.edu>

Is there any way I can increase memory size for R under Unix?  I tried 
Memory function, which was not working.

Thanks,

Hua Li



From ll9f at virginia.edu  Tue Nov  9 17:26:56 2004
From: ll9f at virginia.edu (Lei Liu)
Date: Tue, 09 Nov 2004 11:26:56 -0500
Subject: [R] frailty model for gap times
Message-ID: <6.1.2.0.1.20041109112147.01b36498@l.mail.virginia.edu>

Hi there,

I have a question on the frailty model for gap times with distinct baseline 
hazard for each gap. Can I use R coxph function to do it? For example, the 
following code assumes an identical baseline hazard for every gap time, but 
what if I want a distinct baseline hazard for each gap?

coxph(Surv(gap,status)~trt+frailty(id))

Thanks in advance!




Lei Liu
Assistant Professor
Division of Biostatistics and Epidemiology
Dept. of Health Evaluation Sciences
School of Medicine
University of Virginia

3181 Hospital West Complex
Charlottesville, VA 22908-0717

1-434-982-3364 (o)
1-734-730-1395 (c)

liulei at virginia.edu
ll9f at virginia.edu



From dimitris.rizopoulos at med.kuleuven.ac.be  Tue Nov  9 17:34:22 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Tue, 9 Nov 2004 17:34:22 +0100
Subject: [R] list -> environment coercion
References: <4190E8BC.6080300@biovision-discovery.de>
Message-ID: <009c01c4c679$f834f860$0540210a@www.domain>

Hi Valery,

a simple way to do it might be the following:

lis <- list(a=10, b=20, d="x")
e1 <- new.env()
for(i in 1:length(lis)) assign(names(lis)[i], lis[[i]], envir=e1)
####
d
get("d", env=e1)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Valery A.Khamenya" <vak at biovision-discovery.de>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, November 09, 2004 4:56 PM
Subject: [R] list -> environment coercion


> Hi all,
>
> there is "environment->list" coercion, i.e. as.list.environment.
>
> Is there any "list->environment" coercion?
>
> thank you.
> --
> Valery.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Tue Nov  9 17:36:44 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 9 Nov 2004 16:36:44 +0000 (GMT)
Subject: [R] R under Pocket PC
In-Reply-To: <4B448BDE-3269-11D9-A207-000A95B2B140@sanger.ac.uk>
Message-ID: <Pine.LNX.4.44.0411091631001.24326-100000@gannet.stats>

On Tue, 9 Nov 2004, Tim Cutts wrote:

> On 9 Nov 2004, at 12:27 pm, Prof Brian Ripley wrote:
> 
> > On Tue, 9 Nov 2004, Lars Strand wrote:
> >
> >> Will R run under Windows Pocket PC?
> >
> > We don't know!
> >
> > There are no binary versions of R for that platform, but perhaps you 
> > could
> > find a suitable compiler and manage to build the sources.
> >
> > Outside pure mathematics it is usually very hard to establish that
> > something cannot be done (and it can be very hard in pure mathematics,
> > too).
> 
> Do PocketPCs generally have enough memory to run something as big as R? 

Yes -- have you not seen the size of SD cards these days?  An R for 
Windows install is under 50Mb and needs about 12Mb RAM to run.

>   A standard R install requires a lot of storage space, by PDA 
> standards...
> 
> Technically, I don't suppose there's much to stop R running on some 
> PDAs, especially those based on Linux like the Sharp Zaurus, other than 
> storage and memory requirements.  If there's a Qt graphical interface 
> available for R, you could even get graphics working on the Zaurus, 
> potentially.  

That was not the question.

> The Windows API on Pocket PC is quite a reduced subset 
> compared to full Windows, so you might have problems with that.  

But that was.  However, as I said, you would have to find a suitable 
compiler or cross-compiler, that supports enough features (and I suspect 
IEEE arithmetic would be a problem, especially as it is for VC++ on full 
Windows).

Perhaps you would like to write such a compiler, as technically there's 
not much to stop you?

> PalmOS would probably be right out.  :-)

I think you are belittling the work done to get R running on as wide a 
range of platforms as it does.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From david.whiting at ncl.ac.uk  Tue Nov  9 17:34:40 2004
From: david.whiting at ncl.ac.uk (David Whiting)
Date: 09 Nov 2004 16:34:40 +0000
Subject: [R] R under Pocket PC
In-Reply-To: <x2mzxrf3ih.fsf@biostat.ku.dk>
References: <Pine.LNX.4.44.0411091223400.16452-100000@gannet.stats>
	<x2mzxrf3ih.fsf@biostat.ku.dk>
Message-ID: <m28y9bq8v3.fsf@ganymede.ammp.or.tz>

Peter Dalgaard <p.dalgaard at biostat.ku.dk> writes:

> Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:
> 
> > On Tue, 9 Nov 2004, Lars Strand wrote:
> > 
> > > Will R run under Windows Pocket PC?
> > 
> > We don't know!
> > 
> > There are no binary versions of R for that platform, but perhaps you could 
> > find a suitable compiler and manage to build the sources.
> 
> The PDA idea is quite interesting on all sorts of hardware.
> Unfortunately, the tools for (cross-)building software tend to be
> rather "hackish" and poorly documented. I believe people have had some
> success with the Sharp Zaurus, but that of course runs a Linux
> variant. 

[...]

If the OP has a Compaq Ipaq and does not mind using Linux it seems to
be possible to runs linux on the Ipaq and therefore it might
conceivably be able to run R.

http://www.ipaqlinux.com/

It is interesting this has come up at this time. Two days ago I
installed Linux on a Psion 5MX (16Mb RAM) and am tickled pink by
it. Installation is easy. At the moment I only have a small
compactflash disk so I have not been able to install X windows or R
yet. It runs debian woody. Linux is installed on the CF so it is
actually a dual-boot machine---I have both the wonderful Psion
software [why did they stop making them?] and can boot into Linux when
I need it. When in Linux switching it on and off is instantaneous,
just like any ordinary PDA.

My ultimate aim is to have R, emacs, ESS and latex on it.  Today I
ordered a 1Gb CF disk and expect to get it in a week or two and I will
let you know how I get on.

Here's the howto: 
http://linux-7110.sourceforge.net/howtos/series5mx/5MXHOWTO.htm

Some screenshots: (there's one with emacs about half way down the page
and octave and gnuplot at the bottom). 
http://www.openpsion.org/howtos/series5mx/5MXHOWTO/5MX_howto_11.htm

-- 
David Whiting
University of Newcastle upon Tyne, UK



From tlumley at u.washington.edu  Tue Nov  9 17:45:21 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 9 Nov 2004 08:45:21 -0800 (PST)
Subject: [R] increase memory size on Unix
In-Reply-To: <5.1.0.14.0.20041109112235.00ad45d0@mail.vbi.vt.edu>
References: <5.1.0.14.0.20041109112235.00ad45d0@mail.vbi.vt.edu>
Message-ID: <Pine.A41.4.61b.0411090837450.129478@homer06.u.washington.edu>

On Tue, 9 Nov 2004, Hua Li wrote:

> Is there any way I can increase memory size for R under Unix?  I tried Memory 
> function, which was not working.
>

Possibly because there is no Memory function.

The standard configuration of R under Unix uses all the memory the 
operating system will give it.  If your computer is set up this way then 
you probably cannot increase the memory size as an R user.  It is possible 
that your computer administrator may be able to increase the memory size 
that the operating system provides by changing quotas or swap space or 
something.

On the other hand, running out of memory now often means hitting the 
address space limits of 32-bit systems, which can't be solved in software.

 	-thomas



From ripley at stats.ox.ac.uk  Tue Nov  9 17:47:31 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 9 Nov 2004 16:47:31 +0000 (GMT)
Subject: [R] Error in update() when repositories is specified.
In-Reply-To: <NEBBIPHDAMMOKDKPOFFICENECLAA.abunn@whrc.org>
Message-ID: <Pine.LNX.4.44.0411091637260.24326-100000@gannet.stats>

I don't think that has ever been tested under Windows: until recently
packageStatus did not work under Windows at all.

The problem seems to be in ask=FALSE, which looks completely broken.
ask=TRUE seems to work.  This was true of Unix, 1.9.1 as well so I guess 
no one ever uses it ....

On Tue, 9 Nov 2004, Andy Bunn wrote:

> Ooops. Make that subject line 'upgrade', not 'update!' Sorry. -AB
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Andy Bunn
> > Sent: Tuesday, November 09, 2004 9:23 AM
> > To: R-Help
> > Subject: [R] Error in update() when repositories is specified.
> >
> >
> > How can I specify the repositories for upgrade()?

You will find it easier to use contrib.url() here.

> > > x <- packageStatus(repositories =
> > "http://cran.us.r-project.org//bin/windows/contrib/2.0")
> > > upgrade(x, ask = FALSE)
> > Error in update[, 3] : incorrect number of dimensions
> >
> > x, the object of class "packageStatus", prints and summarizes fine. I also
> > ran install.packages() without incident.
> > > install.packages(x$avail$Package[x$avail$Status == "not installed"])
> >
> > I get the same error if I specify the repositories with getOption:
> > > getOption("repositories")()[[1]]
> > [1] "http://cran.us.r-project.org//bin/windows/contrib/2.0"
> > > x <- packageStatus(repositories = getOption("repositories")()[[1]])
> > > upgrade(x, ask = FALSE)
> > Error in update[, 3] : incorrect number of dimensions
> >
> > However, upgrade behaves if I don't specify "repositories" in
> > packageStatus() things work as expected.
> > > x <- packageStatus()
> > > upgrade(x)
> >
> > I imagine that I am being a dimwit as per usual. Can anybody help?
> >
> > -Andy
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jwdougherty at mcihispeed.net  Tue Nov  9 17:52:43 2004
From: jwdougherty at mcihispeed.net (John)
Date: Tue, 9 Nov 2004 08:52:43 -0800
Subject: [R] no doubt a dumb question, but..
In-Reply-To: <4817319E-3203-11D9-A057-000A95CDE4AC@uleth.ca>
References: <4817319E-3203-11D9-A057-000A95CDE4AC@uleth.ca>
Message-ID: <200411090852.44048.jwdougherty@mcihispeed.net>

On Monday 08 November 2004 19:56, Dr. John R. Vokey wrote:
> Yes, I am a newbie at R, but it is not the complex commands in R that
> have me baffled, but simple data commands.  For example, why does
>
> something like:
>  > plot(Girth ~ Height)
>
> *not* work after a command that allegedly loads the data:
>  > data(trees)
>
> with the error message:
>
> Error in eval(expr, envir, enclos) : Object "Girth" not found
>
> but does work after the command:
>  > attach(trees)
>
I have wondered this myself, but one reason may be that more than one data 
table can be loaded at a time.  Quite often data tables will use the same 
names for fields.  It would inconvenient for instance, if you had two tables 
loaded, say one for trees and one for melons with a field named "Girth", and 
tried a plot(Girth ~ Height) command.  So data() more or less accesses that 
table, and attach explicit identifies the table of interest.

JWDougherty



From p.dalgaard at biostat.ku.dk  Tue Nov  9 17:56:17 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 09 Nov 2004 17:56:17 +0100
Subject: [R] R under Pocket PC
In-Reply-To: <4B448BDE-3269-11D9-A207-000A95B2B140@sanger.ac.uk>
References: <Pine.LNX.4.44.0411091223400.16452-100000@gannet.stats>
	<4B448BDE-3269-11D9-A207-000A95B2B140@sanger.ac.uk>
Message-ID: <x2hdnzezbi.fsf@biostat.ku.dk>

Tim Cutts <tjrc at sanger.ac.uk> writes:

> Technically, I don't suppose there's much to stop R running on some
> PDAs, especially those based on Linux like the Sharp Zaurus, other
> than storage and memory requirements.  If there's a Qt graphical
> interface available for R, you could even get graphics working on the
> Zaurus, potentially.  

You can actually run X on the Zaurus, in which case you have a nearly
complete no-brainer. You do need an expansion card though.

Qt for R is not in the cards currently, as far as I know. The
cross-platform/licensing issues makes it a bit unappealing. R isn't
all that big, unless you use it for big things: about 60MB on the
disk, and 20MB initial RAM usage, on a PC.

>The Windows API on Pocket PC is quite a reduced
> subset compared to full Windows, so you might have problems with that.
> PalmOS would probably be right out.  :-)

Wouldn't rule those out even, we do have at least some bitmap drivers.

In all cases, I suspect that the small screens would require some
usability rethinking. You need to have the item in hand to really know
what to do, I suppose.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From KWollenberg at tufts-nemc.org  Tue Nov  9 18:04:42 2004
From: KWollenberg at tufts-nemc.org (Wollenberg, Kurt R)
Date: Tue, 9 Nov 2004 12:04:42 -0500 
Subject: [R] Vector multiplication wrap-around
Message-ID: <A16C769D4A7E564597075EA02A91C003047E8785@neexchange01.nemc.org>

Greetings:

In a function I'm trying to write I am multiplying each row in a matrix by a
vector. When the vector extends beyond the end of the matrix I need to have
the multiplication wrap around (continue with column 1 of the matrix).
Initially I got this to work with a loop (old programming habits die hard).
Now, after going back through MASS I've figured out how to do the
multiplication efficiently (without loops) but I can't seem to solve the
wrapping problem. Is there an efficient method for making vector
multiplication wrap around the end of the matrix? I'm using R-2.0.0 running
on Windows XP Pro 2002.

Cheers,
Kurt Wollenberg, PhD
Tufts Center for Vision Research 
New England Medical Center
750 Washington St, Box 450 
Boston, MA, USA
kwollenberg at tufts-nemc.org 
617-636-8945 (Fax)
617-636-9028 (Lab)

The most exciting phrase to hear in science, the one that heralds new
discoveries, is not "Eureka!" (I found it!) but  "That's funny ..." 
--Isaac Asimov


********************** 
Confidentiality Notice\ **********************\      The inf...{{dropped}}



From p.dalgaard at biostat.ku.dk  Tue Nov  9 18:14:48 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 09 Nov 2004 18:14:48 +0100
Subject: [R] R under Pocket PC
In-Reply-To: <Pine.LNX.4.44.0411091631001.24326-100000@gannet.stats>
References: <Pine.LNX.4.44.0411091631001.24326-100000@gannet.stats>
Message-ID: <x2d5yneygn.fsf@biostat.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> 
> > The Windows API on Pocket PC is quite a reduced subset 
> > compared to full Windows, so you might have problems with that.  
> 
> But that was.  However, as I said, you would have to find a suitable 
> compiler or cross-compiler, that supports enough features (and I suspect 
> IEEE arithmetic would be a problem, especially as it is for VC++ on full 
> Windows).
> 
> Perhaps you would like to write such a compiler, as technically there's 
> not much to stop you?

The compiler would seem to be there, basically, in the form of gcc
ports. Dealing with the platform libraries is the hard thing in my
view.
 
> > PalmOS would probably be right out.  :-)
> 
> I think you are belittling the work done to get R running on as wide a 
> range of platforms as it does.

? 

I only saw a bit of excessive pessimism in that remark.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ggrothendieck at myway.com  Tue Nov  9 18:19:00 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 9 Nov 2004 17:19:00 +0000 (UTC)
Subject: [R] R under Pocket PC
References: <Pine.LNX.4.44.0411091223400.16452-100000@gannet.stats>
	<x2mzxrf3ih.fsf@biostat.ku.dk> <m28y9bq8v3.fsf@ganymede.ammp.or.tz>
Message-ID: <loom.20041109T181422-634@post.gmane.org>

David Whiting <david.whiting <at> ncl.ac.uk> writes:


: It is interesting this has come up at this time. Two days ago I
: installed Linux on a Psion 5MX (16Mb RAM) and am tickled pink by
: it. Installation is easy. At the moment I only have a small
: compactflash disk so I have not been able to install X windows or R
: yet. 

You could see if MacAnova will install -- its an R/S-like package.  
Its pretty small (even runs on 640K DOS) and is quite portable.  Its
not as powerful as R but its still amazingly powerful and you might
be able to fit it on.



From baron at psych.upenn.edu  Tue Nov  9 18:44:42 2004
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Tue, 9 Nov 2004 12:44:42 -0500
Subject: [R] R works on Fedora Core 3
Message-ID: <20041109174442.GA27104@psych>

The RPM for Fedora Core 2 seems to work just fine on Core 3.

(The graphics window got smaller, but I'm sure there is a setting
for that.)

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron
R search page: http://finzi.psych.upenn.edu/



From jari.oksanen at oulu.fi  Tue Nov  9 19:37:23 2004
From: jari.oksanen at oulu.fi (Jari Oksanen)
Date: Tue, 09 Nov 2004 20:37:23 +0200
Subject: [R] R works on Fedora Core 3
In-Reply-To: <20041109174442.GA27104@psych>
References: <20041109174442.GA27104@psych>
Message-ID: <65C11A98-327E-11D9-AE8C-000A95C76CA8@oulu.fi>


On 9 Nov 2004, at 19:44, Jonathan Baron wrote:

> The RPM for Fedora Core 2 seems to work just fine on Core 3.
>
> (The graphics window got smaller, but I'm sure there is a setting
> for that.)
>
That would be good news. I really don't know how the graphics window 
became so big at some stage. (MacOS X is just cute here: tiny, sharp, 
fast graphics window.)

Has the options()printcmd reappeared, so that dev.print() works without 
changing default options?

cheers, jazza
--
Jari Oksanen, Oulu, Finland



From baron at psych.upenn.edu  Tue Nov  9 19:47:17 2004
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Tue, 9 Nov 2004 13:47:17 -0500
Subject: [R] R works on Fedora Core 3
In-Reply-To: <65C11A98-327E-11D9-AE8C-000A95C76CA8@oulu.fi>
References: <20041109174442.GA27104@psych>
	<65C11A98-327E-11D9-AE8C-000A95C76CA8@oulu.fi>
Message-ID: <20041109184717.GA15407@psych>

On 11/09/04 20:37, Jari Oksanen wrote:
>
>On 9 Nov 2004, at 19:44, Jonathan Baron wrote:
>
>> The RPM for Fedora Core 2 seems to work just fine on Core 3.
>>
>> (The graphics window got smaller, but I'm sure there is a setting
>> for that.)
>>
>That would be good news. I really don't know how the graphics window
>became so big at some stage. (MacOS X is just cute here: tiny, sharp,
>fast graphics window.)

I have the opposite problem, a 1680x1050 display.

>Has the options()printcmd reappeared, so that dev.print() works without
>changing default options?

I can't imagine how this would change.  This is the same "old"
RPM, not a new one.  The option is there, and I don't think it
ever disappeared.  I can't test it.  This is my laptop, which is
not set up to print anything.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron



From gerifalte28 at hotmail.com  Tue Nov  9 19:48:39 2004
From: gerifalte28 at hotmail.com (F Z)
Date: Tue, 09 Nov 2004 18:48:39 +0000
Subject: [R] Conditional selection of rows
Message-ID: <BAY2-F38NFr9oD6XMLD00010cc9@hotmail.com>

Many thanks to Gabor Grothendieck, Thomas Lumley and James Holtman for their 
useful answers on this thread.  The three solutions worked for the problem.  
Here is a sumary of their responses (modified for consistency on notations):


>F Z <gerifalte28 <at> hotmail.com> writes:
>: Hi,
>:
>: I have a data.frame with several variables and 50,000 observations.
>: i.e.
>: data[1:2,1:7]
>:   Iteration Day Production.Type tsUSusc tsASusc tsULat tsALat
>:          1   0         Generic   17965 8833053      0      0
>:          1   1         Generic   17965 8833053      0      0
>:          .
>:          .
>:          .
>:          1 199         Generic   17237 8141028     26  23131
>:          2 127         Generic   15828 7307583     92  63463
>:
>: I would like to extract only the observations (rows) for the last "day" 
>for
>: each "iteration" and store them in a data frame.

Gabor Grothendieck's solution:


subset<-do.call("rbind", by(data, data$Iteration, tail, 1))


James Holtman's solution:

subset<- by(data, data$Iteration, function(x)x[nrow(x),])
subset<-do.call('rbind',subset)

Thomas Lumley's solution:

data <- data[order(data$Iteration, data$Day, decreasing=TRUE),]

subset <- data[!duplicated(data$Iteration),]

If you are sure that the data are in order to begin with you could just 
reverse the entire data set (  data <- data[nrow(data):1,] ), but I'm always 
reluctant to assume this.



>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html



From marius7 at 012.net.il  Tue Nov  9 19:48:24 2004
From: marius7 at 012.net.il (Marius A)
Date: Tue, 9 Nov 2004 20:48:24 +0200
Subject: [R] Problems on Bsprob procedure  
Message-ID: <001001c4c68c$b1ade060$f39a19ac@c6v7j4>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041109/f35179fa/attachment.pl

From david.whiting at ncl.ac.uk  Tue Nov  9 20:06:39 2004
From: david.whiting at ncl.ac.uk (David Whiting)
Date: 09 Nov 2004 19:06:39 +0000
Subject: [R] R under Pocket PC
In-Reply-To: <loom.20041109T181422-634@post.gmane.org>
References: <Pine.LNX.4.44.0411091223400.16452-100000@gannet.stats>
	<x2mzxrf3ih.fsf@biostat.ku.dk> <m28y9bq8v3.fsf@ganymede.ammp.or.tz>
	<loom.20041109T181422-634@post.gmane.org>
Message-ID: <m2y8haq1ts.fsf@ganymede.ammp.or.tz>

Gabor Grothendieck <ggrothendieck at myway.com> writes:

> David Whiting <david.whiting <at> ncl.ac.uk> writes:
> 
> 
> : It is interesting this has come up at this time. Two days ago I
> : installed Linux on a Psion 5MX (16Mb RAM) and am tickled pink by
> : it. Installation is easy. At the moment I only have a small
> : compactflash disk so I have not been able to install X windows or R
> : yet. 
> 
> You could see if MacAnova will install -- its an R/S-like package.  
> Its pretty small (even runs on 640K DOS) and is quite portable.  Its
> not as powerful as R but its still amazingly powerful and you might
> be able to fit it on.

Thanks. I took a quick look and I think I would have to cross-compile
it (I didn't find an ARM binary).  When I get my larger compactflash
card I am hoping that I will be able to just get the ARM debian
package (and all the assorted dependencies) without having to setup a
cross-compiling tool chain---I will have to take a look and see what
is involved in doing this. I don't have a compiler installed on the
Psion and probably would not have enough room for all the libraries
(just guessing here).

Dave

-- 
David Whiting
University of Newcastle upon Tyne, UK



From wolski at molgen.mpg.de  Tue Nov  9 20:30:35 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Tue, 09 Nov 2004 20:30:35 +0100
Subject: [R] gplot.hexbin - how to set figure margin.
Message-ID: <200411092030350675.16E0FBD1@mail.math.fu-berlin.de>

Hi,

Would like to use to the hexbin package to plot a 2D hist - because it looks really _cool_.
My problem  is related to  drawing a pdf hexbin graphic in series of other graphics.
All other figures have a par(mar=c(3.2,3.2,1,1)). So the gplot.hexbin figure in this series looks a little alienated?

Was trying a to specify the _mar_ using par, viewport, hexViewport etc. a little.

My question. How to set the margins in the sample code below. 


x <- rnorm(10000)
y <- rnorm(10000)
bin <- hexbin(x,y)

## Plot : Note that 'gplot.hexbin' is the S4 plot method for hexbin !
## ----              ------------         --------------
plot(bin)


/E


Dipl. bio-chem. Eryk Witold Wolski             @    MPI-Moleculare Genetic   
Ihnestrasse 63-73 14195 Berlin                'v'                            
tel: 0049-30-83875219                        /   \                           
mail: witek96 at users.sourceforge.net        ---W-W----                        
http://r4proteomics.sourceforg.net



From vograno at evafunds.com  Tue Nov  9 20:34:52 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Tue, 9 Nov 2004 11:34:52 -0800
Subject: [R] R code debugging
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A5750FC5@phost015.EVAFUNDS.intermedia.net>

You might want to check out the "debug" package on CRAN:

debug: MVB's debugger for R

Debugger for R functions, with code display, graceful error recovery,
line-numbered conditional breakpoints, access to exit code, flow
control, and full keyboard input.
Version:	1.0.1
Depends:	R (>= 1.8), mvbutils, tcltk
Date:	18/2/2004
Author:	Mark V. Bravington
Maintainer:	Mark V. Bravington
License:	GPL version 2 or later

 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Prof 
> Brian Ripley
> Sent: Tuesday, November 09, 2004 4:16 AM
> To: Timur Elzhov
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] R code debugging
> 
> On Tue, 9 Nov 2004, Timur Elzhov wrote:
> 
> > it's quite difficult for me to find `Error:'s in my R code, 
> because R 
> > does say about error itself, but say nothing about its 
> location, say, 
> > string number and file with an error (which may be 
> `source'd from another file).
> > Are there any option for turning of the similar feature, or 
> R can not 
> > do such a thing at all?
> 
> R code can be created dynamically by R code (called 
> `computing on the language'), and in most cases the source 
> code is not retained (and it is not used for execution).
> 
> traceback() will always tell you the function in which the 
> error occurred.
> If you write reasonably modular code that should suffice, but 
> if not, use debug() on that function and single-step through 
> it to find where the error occurs.  Or set a suitable error 
> handler: have you explored recover(), for example?
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From scott.rifkin at yale.edu  Tue Nov  9 20:43:13 2004
From: scott.rifkin at yale.edu (Scott Rifkin)
Date: Tue, 9 Nov 2004 14:43:13 -0500 (EST)
Subject: [R] Boxplot plot range
In-Reply-To: <200411091116.iA9BGcAR031241@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.44.0411091441380.22550-100000@ares.its.yale.edu>

How can I set the plot range for a boxplot using the boxplot() function?

For example, the values in the boxplot go from -3 to 3 (outliers plotted), 
but I'd like the y axis to go from -5 to 5.

Thanks much
Scott dot Rifkin at yale dot edu



From andy_liaw at merck.com  Tue Nov  9 20:50:40 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 9 Nov 2004 14:50:40 -0500
Subject: [R] Boxplot plot range
Message-ID: <3A822319EB35174CA3714066D590DCD50994E2B7@usrymx25.merck.com>

> From: Scott Rifkin
> 
> How can I set the plot range for a boxplot using the 
> boxplot() function?
> 
> For example, the values in the boxplot go from -3 to 3 
> (outliers plotted), 
> but I'd like the y axis to go from -5 to 5.

as in:

 boxplot(rnorm(30), ylim=c(-5, 5))

?

Andy
 
> Thanks much
> Scott dot Rifkin at yale dot edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From abunn at whrc.org  Tue Nov  9 20:52:59 2004
From: abunn at whrc.org (Andy Bunn)
Date: Tue, 9 Nov 2004 14:52:59 -0500
Subject: [R] Boxplot plot range
In-Reply-To: <Pine.LNX.4.44.0411091441380.22550-100000@ares.its.yale.edu>
Message-ID: <NEBBIPHDAMMOKDKPOFFIGENMCLAA.abunn@whrc.org>

How about using ylim?

foo <- rnorm(100, 0, 1)
boxplot(foo, ylim = c(-5, 5))

HTH, Andy

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Scott Rifkin
> Sent: Tuesday, November 09, 2004 2:43 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Boxplot plot range
> 
> 
> How can I set the plot range for a boxplot using the boxplot() function?
> 
> For example, the values in the boxplot go from -3 to 3 (outliers 
> plotted), 
> but I'd like the y axis to go from -5 to 5.
> 
> Thanks much
> Scott dot Rifkin at yale dot edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From roebuck at odin.mdacc.tmc.edu  Tue Nov  9 21:28:42 2004
From: roebuck at odin.mdacc.tmc.edu (Paul Roebuck)
Date: Tue, 9 Nov 2004 14:28:42 -0600 (CST)
Subject: [R] Need car() and cdr() for '...'
Message-ID: <Pine.OSF.4.58.0411091409120.37458@odin.mdacc.tmc.edu>

Needed to redefine function "sum" for my MATLAB package.
There's something similar in Chambers's Green Book (pg 351)
so I modified it as such:

library(methods)
setGeneric("sum", function(x, ..., na.rm = FALSE) {
    if (nDotArgs(...) > 0)
        sum(c(sum(x, na.rm = na.rm),
              sum(..., na.rm = na.rm)))
    else
        standardGeneric("sum")
})
setMethod("sum", "vector", function(x, na.rm) {
    return(base::sum(x, na.rm));
})
setMethod("sum", "matrix", function(x, na.rm) {
    return(apply(x, 2, sum, na.rm));
})
setMethod("sum", "array", function(x, na.rm) {
    stop('Argument "x" must either be a vector or matrix')
})
setMethod("sum", "missing", function() {
    stop('Argument "x" missing')
})

Problem is that R's definition for summary functions
doesn't specify the "x" argument so the generic
won't work. So does someone have LISP-derived car/adr
functions I can use to split the '...' list such that
the generic function could use this instead:

        sum(c(sum(car(...), na.rm = na.rm),
              sum(cdr(...), na.rm = na.rm)))

Of course, better ideas are welcome too.

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From ross at biostat.ucsf.edu  Tue Nov  9 22:42:31 2004
From: ross at biostat.ucsf.edu (Ross Boylan)
Date: Tue, 09 Nov 2004 13:42:31 -0800
Subject: [R] Is nesting {} inside \eqn OK?
Message-ID: <1100036551.12779.262.camel@iron.libaux.ucsf.edu>

I'm seeing various things fail when I try to next braces inside \eqn.
This source
  \eqn{{\bf\beta}_j}{b(j)} is the vector
produces this error
----------------------------------------------
[4]
! Missing $ inserted.
<inserted text> 
                $
l.258 \eqn{{\bf\beta}_j}{\bf\beta}_
                                   j{{b(j)} is the vector of
coefficients fo...
I've inserted a begin-math/end-math symbol since I think
you left one out. Proceed, with fingers crossed.

! Missing } inserted.
<inserted text> 
                }
l.258 ...tor of coefficients for outcome \eqn{j}{}
                                                   and
I've inserted something that you may have forgotten.
-------------------------------------------------------

Notice that the argument seems to have been doubled.
I tried using \begingroup and \endgroup, and ran into problem with a
later \eqn with nested braces.

For reference, particularly in case I've missed something earlier, here
is a fuller excerpt from the input file:
--------------------------------------------
  With \eqn{J} possible outcomes and \eqn{p_j}{p(j)} the probability of
  the \eqn{j}'th outcome,
  the formula is \deqn{\newcommand{\B}{{\bf \beta}}\newcommand{\X}{{\bf
X}}
    p_j = \frac{e^{\X\B_j}}{\displaystyle\sum_{k=0}^J e^{\X\B_k}}.}{
    p(j) = exp[X*b(j)]/sum{exp[X*b(k)], k=0 to J}.}
  \eqn{{\bf\beta}_j}{b(j)} is the vector of coefficients for outcome
\eqn{j} and
  \eqn{{\bf X}}{X} are the covariates.

--------------------------------------------

By the way, the \newcommand is not global, so I can't use \B (for
example) in later \eqn's.

Using R 2.0.0.
-- 
Ross Boylan                                      wk:  (415) 502-4031
530 Parnassus Avenue (Library) rm 115-4          ross at biostat.ucsf.edu
Dept of Epidemiology and Biostatistics           fax: (415) 476-9856
University of California, San Francisco
San Francisco, CA 94143-0840                     hm:  (415) 550-1062



From wib2004 at med.cornell.edu  Tue Nov  9 23:08:10 2004
From: wib2004 at med.cornell.edu (William Briggs)
Date: Tue, 09 Nov 2004 17:08:10 -0500
Subject: [R] remove missing values from matrix or data frame
Message-ID: <41913FCA.2060505@med.cornell.edu>


Is there any way besides looping to remove complete rows from a matrix 
or data frame where there is at least one NA in any of the columns?

For example
 > a
        [,1]     [,2]
   [1,] 0        2.6875
   [2,] 8.366667 6.625
   [3,] 15.6     4.375
   [4,] 23.4     6.25
   [5,] 29       5.09375
   [6,] 18       NA
   [7,] 0        4.15625
   [8,] 9.366667 6.25
   [9,] 14.73333 5.875
  [10,] 31.26667 6.15625
  [11,] NA       2.357
  [12,] NA       5.4234
  [13,] 0        3.34375
  [14,] 7.666667 2.78125
  [15,] NA       NA

In a, rows 6, 11, 12, and 15 should be removed.

na.omit(a) does nothing, nor does na.omit(as.data.frame(a)).  I can get 
a matrix of which are NA and not by "i<-!is.na(a)", but this doesn't 
seem to help ("a[i]" isn't the thing I'm after).

I know I am missing something simple and standard, but I haven't been 
able to see it yet (nor on Google).

Thanks.



From spencer.graves at pdf.com  Tue Nov  9 23:19:19 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 09 Nov 2004 14:19:19 -0800
Subject: [R] remove missing values from matrix or data frame
In-Reply-To: <41913FCA.2060505@med.cornell.edu>
References: <41913FCA.2060505@med.cornell.edu>
Message-ID: <41914267.5060600@pdf.com>

      How about the following: 

 > (A <- array(c(1, NA, 3, NA, 4, 5), dim=c(3,2)))
     [,1] [,2]
[1,]    1   NA
[2,]   NA    4
[3,]    3    5
 > A[apply(A, 1, function(x)!any(is.na(x))), , drop=F]
     [,1] [,2]
[1,]    3    5

      hope this helps.  spencer graves

William Briggs wrote:

>
> Is there any way besides looping to remove complete rows from a matrix 
> or data frame where there is at least one NA in any of the columns?
>
> For example
> > a
>        [,1]     [,2]
>   [1,] 0        2.6875
>   [2,] 8.366667 6.625
>   [3,] 15.6     4.375
>   [4,] 23.4     6.25
>   [5,] 29       5.09375
>   [6,] 18       NA
>   [7,] 0        4.15625
>   [8,] 9.366667 6.25
>   [9,] 14.73333 5.875
>  [10,] 31.26667 6.15625
>  [11,] NA       2.357
>  [12,] NA       5.4234
>  [13,] 0        3.34375
>  [14,] 7.666667 2.78125
>  [15,] NA       NA
>
> In a, rows 6, 11, 12, and 15 should be removed.
>
> na.omit(a) does nothing, nor does na.omit(as.data.frame(a)).  I can 
> get a matrix of which are NA and not by "i<-!is.na(a)", but this 
> doesn't seem to help ("a[i]" isn't the thing I'm after).
>
> I know I am missing something simple and standard, but I haven't been 
> able to see it yet (nor on Google).
>
> Thanks.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html


-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From ripley at stats.ox.ac.uk  Tue Nov  9 23:29:18 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 9 Nov 2004 22:29:18 +0000 (GMT)
Subject: [R] remove missing values from matrix or data frame
In-Reply-To: <41913FCA.2060505@med.cornell.edu>
Message-ID: <Pine.LNX.4.44.0411092226160.13465-100000@gannet.stats>

Something is not as it seems:

> a <- matrix(scan(),,2,byrow=T)
1:  0        2.6875
3:  8.366667 6.625
5:  15.6     4.375
7:  23.4     6.25
9:  29       5.09375
11:  18       NA
13:  0        4.15625
15:  9.366667 6.25
17:  14.73333 5.875
19:  31.26667 6.15625
21:  NA       2.357
23:  NA       5.4234
25:  0        3.34375
27:  7.666667 2.78125
29:  NA       NA
31:
Read 30 items

and a looks like yours and

> na.omit(a)
           [,1]    [,2]
 [1,]  0.000000 2.68750
 [2,]  8.366667 6.62500
 [3,] 15.600000 4.37500
 [4,] 23.400000 6.25000
 [5,] 29.000000 5.09375
 [6,]  0.000000 4.15625
 [7,]  9.366667 6.25000
 [8,] 14.733330 5.87500
 [9,] 31.266670 6.15625
[10,]  0.000000 3.34375
[11,]  7.666667 2.78125
attr(,"na.action")
[1] 11 12 15  6
attr(,"class")
[1] "omit"

does something, in fact what you asked for.

So what is a?  What does str(a) say about it?


On Tue, 9 Nov 2004, William Briggs wrote:

> 
> Is there any way besides looping to remove complete rows from a matrix 
> or data frame where there is at least one NA in any of the columns?
> 
> For example
>  > a
>         [,1]     [,2]
>    [1,] 0        2.6875
>    [2,] 8.366667 6.625
>    [3,] 15.6     4.375
>    [4,] 23.4     6.25
>    [5,] 29       5.09375
>    [6,] 18       NA
>    [7,] 0        4.15625
>    [8,] 9.366667 6.25
>    [9,] 14.73333 5.875
>   [10,] 31.26667 6.15625
>   [11,] NA       2.357
>   [12,] NA       5.4234
>   [13,] 0        3.34375
>   [14,] 7.666667 2.78125
>   [15,] NA       NA
> 
> In a, rows 6, 11, 12, and 15 should be removed.
> 
> na.omit(a) does nothing, nor does na.omit(as.data.frame(a)).  I can get 
> a matrix of which are NA and not by "i<-!is.na(a)", but this doesn't 
> seem to help ("a[i]" isn't the thing I'm after).
> 
> I know I am missing something simple and standard, but I haven't been 
> able to see it yet (nor on Google).
> 
> Thanks.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From gunter.berton at gene.com  Tue Nov  9 23:31:33 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 9 Nov 2004 14:31:33 -0800
Subject: [R] remove missing values from matrix or data frame
In-Reply-To: <41914267.5060600@pdf.com>
Message-ID: <200411092231.iA9MVXjk005675@hertz.gene.com>



?na.omit  

as in A<-na.omit(A)

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Spencer Graves
> Sent: Tuesday, November 09, 2004 2:19 PM
> To: William Briggs
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] remove missing values from matrix or data frame
> 
>       How about the following: 
> 
>  > (A <- array(c(1, NA, 3, NA, 4, 5), dim=c(3,2)))
>      [,1] [,2]
> [1,]    1   NA
> [2,]   NA    4
> [3,]    3    5
>  > A[apply(A, 1, function(x)!any(is.na(x))), , drop=F]
>      [,1] [,2]
> [1,]    3    5
> 
>       hope this helps.  spencer graves
> 
> William Briggs wrote:
> 
> >
> > Is there any way besides looping to remove complete rows 
> from a matrix 
> > or data frame where there is at least one NA in any of the columns?
> >
> > For example
> > > a
> >        [,1]     [,2]
> >   [1,] 0        2.6875
> >   [2,] 8.366667 6.625
> >   [3,] 15.6     4.375
> >   [4,] 23.4     6.25
> >   [5,] 29       5.09375
> >   [6,] 18       NA
> >   [7,] 0        4.15625
> >   [8,] 9.366667 6.25
> >   [9,] 14.73333 5.875
> >  [10,] 31.26667 6.15625
> >  [11,] NA       2.357
> >  [12,] NA       5.4234
> >  [13,] 0        3.34375
> >  [14,] 7.666667 2.78125
> >  [15,] NA       NA
> >
> > In a, rows 6, 11, 12, and 15 should be removed.
> >
> > na.omit(a) does nothing, nor does na.omit(as.data.frame(a)).  I can 
> > get a matrix of which are NA and not by "i<-!is.na(a)", but this 
> > doesn't seem to help ("a[i]" isn't the thing I'm after).
> >
> > I know I am missing something simple and standard, but I 
> haven't been 
> > able to see it yet (nor on Google).
> >
> > Thanks.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> 
> 
> -- 
> Spencer Graves, PhD, Senior Development Engineer
> O:  (408)938-4420;  mobile:  (408)655-4567
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 

From rpeng at jhsph.edu  Tue Nov  9 23:32:13 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Tue, 09 Nov 2004 17:32:13 -0500
Subject: [R] remove missing values from matrix or data frame
In-Reply-To: <41913FCA.2060505@med.cornell.edu>
References: <41913FCA.2060505@med.cornell.edu>
Message-ID: <4191456D.1080807@jhsph.edu>

You might be interested in complete.cases(), as in:

use <- complete.cases(a)
a[use, ]

-roger

William Briggs wrote:
> 
> Is there any way besides looping to remove complete rows from a matrix 
> or data frame where there is at least one NA in any of the columns?
> 
> For example
>  > a
>        [,1]     [,2]
>   [1,] 0        2.6875
>   [2,] 8.366667 6.625
>   [3,] 15.6     4.375
>   [4,] 23.4     6.25
>   [5,] 29       5.09375
>   [6,] 18       NA
>   [7,] 0        4.15625
>   [8,] 9.366667 6.25
>   [9,] 14.73333 5.875
>  [10,] 31.26667 6.15625
>  [11,] NA       2.357
>  [12,] NA       5.4234
>  [13,] 0        3.34375
>  [14,] 7.666667 2.78125
>  [15,] NA       NA
> 
> In a, rows 6, 11, 12, and 15 should be removed.
> 
> na.omit(a) does nothing, nor does na.omit(as.data.frame(a)).  I can get 
> a matrix of which are NA and not by "i<-!is.na(a)", but this doesn't 
> seem to help ("a[i]" isn't the thing I'm after).
> 
> I know I am missing something simple and standard, but I haven't been 
> able to see it yet (nor on Google).
> 
> Thanks.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From kef at plantpath.wisc.edu  Tue Nov  9 23:53:40 2004
From: kef at plantpath.wisc.edu (Kenneth Frost)
Date: Tue, 9 Nov 2004 16:53:40 -0600
Subject: [R] Data Censoring and Normality Tests
Message-ID: <32CE22DA-32A2-11D9-9087-00039354CE80@plantpath.wisc.edu>

Hello,

I would like to know if there is a function in R that will test for 
normality and handle censored data sets.  Currently, I evaluate each 
censored data set by the extent to which a normal scores plot 
approximate a straight line.  For complete data sets I use 
shapiro.test().

Below is an example of a censored data set.

data1<-c(0.00, 0.00, 0.00, 5.86, 5.17,  8.17,  5.12, 4.92, 7.08, 5.73, 
5.44, 6.61, 6.34, 6.23, 5.97, 5.86, 5.15, 7.98, 6.72, 5.15, 3.58, 6.86, 
6.12, 4.58, 6.07, 5.38, 5.21, 3.78)

The zero values occur because I cannot detect a pathogen with a value 
below 2.4.  Using shapiro.test(), it seems that the data are different 
from a normal distribution.  However, the normal scores plot suggest 
otherwise.

Using R version 1.9.1

Thanks in advance.

Ken
________________________________________________________
Kenneth E. Frost
Research Assistant
University of Wisconsin - Madison
Dept. of Plant Pathology
1630 Linden Dr.
Madison, WI 53706
kef at plantpath.wisc.edu



From andy_liaw at merck.com  Wed Nov 10 00:11:58 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 9 Nov 2004 18:11:58 -0500
Subject: [R] Data Censoring and Normality Tests
Message-ID: <3A822319EB35174CA3714066D590DCD50994E2BC@usrymx25.merck.com>

I don't know much about censored data, but what you described doesn't sound
like censored data, but rather truncated data.  

I think normality tests are over-(ab)used, and would suggest inspection of
normal QQ plots instead (as you already do).  For formal tests, you might be
able to cook up something that test for truncated normality.

Cheers,
Andy

> From: Kenneth Frost
> 
> Hello,
> 
> I would like to know if there is a function in R that will test for 
> normality and handle censored data sets.  Currently, I evaluate each 
> censored data set by the extent to which a normal scores plot 
> approximate a straight line.  For complete data sets I use 
> shapiro.test().
> 
> Below is an example of a censored data set.
> 
> data1<-c(0.00, 0.00, 0.00, 5.86, 5.17,  8.17,  5.12, 4.92, 
> 7.08, 5.73, 
> 5.44, 6.61, 6.34, 6.23, 5.97, 5.86, 5.15, 7.98, 6.72, 5.15, 
> 3.58, 6.86, 
> 6.12, 4.58, 6.07, 5.38, 5.21, 3.78)
> 
> The zero values occur because I cannot detect a pathogen with a value 
> below 2.4.  Using shapiro.test(), it seems that the data are 
> different 
> from a normal distribution.  However, the normal scores plot suggest 
> otherwise.
> 
> Using R version 1.9.1
> 
> Thanks in advance.
> 
> Ken
> ________________________________________________________
> Kenneth E. Frost
> Research Assistant
> University of Wisconsin - Madison
> Dept. of Plant Pathology
> 1630 Linden Dr.
> Madison, WI 53706
> kef at plantpath.wisc.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From andy_liaw at merck.com  Wed Nov 10 00:47:00 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 9 Nov 2004 18:47:00 -0500
Subject: [R] Data Censoring and Normality Tests
Message-ID: <3A822319EB35174CA3714066D590DCD50994E2BD@usrymx25.merck.com>

I stand corrected!  Hope Bert doesn't mind me CC'ing the list.

Andy

> From: Berton Gunter
> 
> No Andy, they're (left) censored alright: their values are 
> known to be below
> 2.4, but not how far below. Truncated data "arise when 
> observations are
> actually observed only when they take on values in a 
> particular range (above
> 2.4, say). For observations outside that range, their EXISTENCE is not
> known." -- Meeker and Escobar, "Statistical Methods for 
> Reliability Data."
> 
> Cheers,
> Bert
>  
>  
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Liaw, Andy
> > Sent: Tuesday, November 09, 2004 3:12 PM
> > To: 'Kenneth Frost'; r-help at stat.math.ethz.ch
> > Subject: RE: [R] Data Censoring and Normality Tests
> > 
> > I don't know much about censored data, but what you described 
> > doesn't sound
> > like censored data, but rather truncated data.  
> > 
> > I think normality tests are over-(ab)used, and would suggest 
> > inspection of
> > normal QQ plots instead (as you already do).  For formal 
> > tests, you might be
> > able to cook up something that test for truncated normality.
> > 
> > Cheers,
> > Andy
> > 
> > > From: Kenneth Frost
> > > 
> > > Hello,
> > > 
> > > I would like to know if there is a function in R that 
> will test for 
> > > normality and handle censored data sets.  Currently, I 
> > evaluate each 
> > > censored data set by the extent to which a normal scores plot 
> > > approximate a straight line.  For complete data sets I use 
> > > shapiro.test().
> > > 
> > > Below is an example of a censored data set.
> > > 
> > > data1<-c(0.00, 0.00, 0.00, 5.86, 5.17,  8.17,  5.12, 4.92, 
> > > 7.08, 5.73, 
> > > 5.44, 6.61, 6.34, 6.23, 5.97, 5.86, 5.15, 7.98, 6.72, 5.15, 
> > > 3.58, 6.86, 
> > > 6.12, 4.58, 6.07, 5.38, 5.21, 3.78)
> > > 
> > > The zero values occur because I cannot detect a pathogen 
> > with a value 
> > > below 2.4.  Using shapiro.test(), it seems that the data are 
> > > different 
> > > from a normal distribution.  However, the normal scores 
> > plot suggest 
> > > otherwise.
> > > 
> > > Using R version 1.9.1
> > > 
> > > Thanks in advance.
> > > 
> > > Ken
> > > ________________________________________________________
> > > Kenneth E. Frost
> > > Research Assistant
> > > University of Wisconsin - Madison
> > > Dept. of Plant Pathology
> > > 1630 Linden Dr.
> > > Madison, WI 53706
> > > kef at plantpath.wisc.edu
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! 
> > > http://www.R-project.org/posting-guide.html
> > > 
> > >
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
>



From tlumley at u.washington.edu  Wed Nov 10 01:22:11 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 9 Nov 2004 16:22:11 -0800 (PST)
Subject: [R] frailty model for gap times
In-Reply-To: <6.1.2.0.1.20041109112147.01b36498@l.mail.virginia.edu>
References: <6.1.2.0.1.20041109112147.01b36498@l.mail.virginia.edu>
Message-ID: <Pine.A41.4.61b.0411091621170.129478@homer06.u.washington.edu>

On Tue, 9 Nov 2004, Lei Liu wrote:

> Hi there,
>
> I have a question on the frailty model for gap times with distinct baseline 
> hazard for each gap. Can I use R coxph function to do it? For example, the 
> following code assumes an identical baseline hazard for every gap time, but 
> what if I want a distinct baseline hazard for each gap?
>
> coxph(Surv(gap,status)~trt+frailty(id))
>

You need a stratified model, eg if gapnumber is 1,2,3,4,.. within each 
person
coxph(Surv(gap,status)~trt+frailty(id)+strata(gapnumber))

 	-thomas



From arrayprofile at yahoo.com  Wed Nov 10 01:50:09 2004
From: arrayprofile at yahoo.com (array chip)
Date: Tue, 9 Nov 2004 16:50:09 -0800 (PST)
Subject: [R] worked in R, but not in S-Plus
Message-ID: <20041110005009.79940.qmail@web40824.mail.yahoo.com>

Hi, 

I wrote a function that worked well in R, but not in
S-Plus, can anyone suggest a solution?

> f.coxph.zph<-function(x)
{
	cox.fit <- coxph(Surv(time.cox, status.cox) ~ x,
na.action = na.exclude, method = "breslow")
	fit.zph<-cox.zph(cox.fit,transform='log')
	fit.zph$table[,3]
}

yyy is my data frame that contains survial time,
censor status and predictor variables.

> time.cox<-yyy$time
> status.cox<-yyy$status
> apply(yyy[,-(1:2)],2,f.coxph.zph)

When run in S-Plus, it gave the following error
message:

Problem in model.frame.coxph(fit): Object "x" not
found

If I remove the 2nd and 3rd lines inside the function,
it worked, so definitely something is wrong in passing
the coxph object into the cox.zph().

Thanks



From xmeng at capitalbio.com  Wed Nov 10 02:24:29 2004
From: xmeng at capitalbio.com (=?gb2312?B?w8/QwA==?=)
Date: Wed, 10 Nov 2004 09:24:29 +0800
Subject: [R] about vsn
Message-ID: <300049869.18936@capitalbio.com>

Hi all: Are thare anyones that use vsn?Some questions need your help.Thanks a lot 1.If e1(raw data) contain 2419 genes, after e2<-vsn(e1),e2 contains 2419*2 genes. Why does it happen? 
2.How the calibrated and glog-transformation is perform. In other words, whats the calculation method behind it? Thanks !



From maustin at amgen.com  Wed Nov 10 04:06:02 2004
From: maustin at amgen.com (Austin, Matt)
Date: Tue, 9 Nov 2004 19:06:02 -0800 
Subject: [R] RE: [S] worked in R, but not in S-Plus
Message-ID: <E7D5AB4811D20B489622AABA9C53859104E0DAFA@teal-exch.amgen.com>

The following works, you need to include x=TRUE in the call to coxph.
Passing the time and status variables as additional arguments is a matter of
personal preference.

f.coxph.zph<-function(x, timeVar, statusVar)
{
	cox.fit <- coxph(Surv(timeVar, statusVar) ~ x, na.action =
na.exclude, method = "breslow", x=TRUE)
	fit.zph<-cox.zph(cox.fit)
	fit.zph$table[,3]
}

time.cox   <- ovarian$futime
status.cox <- ovarian$fustat

apply(ovarian[,-(1:2)],2, f.coxph.zph, timeVar = time.cox, statusVar =
status.cox)


--Matt


-----Original Message-----
From: s-news-owner at lists.biostat.wustl.edu
[mailto:s-news-owner at lists.biostat.wustl.edu]On Behalf Of array chip
Sent: Tuesday, November 09, 2004 16:50 PM
To: s-news at lists.biostat.wustl.edu; r-help at stat.math.ethz.ch
Subject: [S] worked in R, but not in S-Plus


Hi, 

I wrote a function that worked well in R, but not in
S-Plus, can anyone suggest a solution?

> f.coxph.zph<-function(x)
{
	cox.fit <- coxph(Surv(time.cox, status.cox) ~ x,
na.action = na.exclude, method = "breslow")
	fit.zph<-cox.zph(cox.fit,transform='log')
	fit.zph$table[,3]
}

yyy is my data frame that contains survial time,
censor status and predictor variables.

> time.cox<-yyy$time
> status.cox<-yyy$status
> apply(yyy[,-(1:2)],2,f.coxph.zph)

When run in S-Plus, it gave the following error
message:

Problem in model.frame.coxph(fit): Object "x" not
found

If I remove the 2nd and 3rd lines inside the function,
it worked, so definitely something is wrong in passing
the coxph object into the cox.zph().

Thanks

__________________________________ 
 

--------------------------------------------------------------------
This message was distributed by s-news at lists.biostat.wustl.edu.  To
...(s-news.. clipped)...



From andrewr at uidaho.edu  Wed Nov 10 04:43:52 2004
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Wed, 10 Nov 2004 14:43:52 +1100
Subject: [R] List seems to drop empty levels of factors when containing them
Message-ID: <20041110034352.GK10340@uidaho.edu>

Greetings R community,

I am curious about the following behaviour: if I define a factor, and
then store a subset of it in a list, the stored version seems to drop
levels that were not included in the subset.  E.g. ..

> mask <- c(T, F)
> grp.1 <- factor(c(1,2))
> list(grp.1)
[[1]]
[1] 1 2
Levels: 1 2

> list(grp.1[mask])
[[1]]
[1] 1
Levels: 1

It is as though the list were redefining the factor and dropping
empty levels.  I would like to keep them (I am using the list for a
two-dimensional tapply). Is there any way to avoid this?

I'm using R 2.0.0 on FreeBSD 5.2.1.

Thanks,

Andrew
-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From maustin at amgen.com  Wed Nov 10 05:12:57 2004
From: maustin at amgen.com (Austin, Matt)
Date: Tue, 9 Nov 2004 20:12:57 -0800 
Subject: [R] List seems to drop empty levels of factors when containin
	g them
Message-ID: <E7D5AB4811D20B489622AABA9C53859104E0DAFB@teal-exch.amgen.com>

I don't get the same result, do you have a package loaded that would change
the default behavior (such as Hmisc)?

> list(grp.1)
[[1]]
[1] 1 2
Levels: 1 2

> list(grp.1[mask])
[[1]]
[1] 1
Levels: 1 2

> library(Hmisc)
<<snip>>

> list(grp.1[mask])
[[1]]
[1] 1
Levels: 1

--Matt

> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    0.0            
year     2004           
month    10             
day      04             
language R             


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Andrew Robinson
Sent: Tuesday, November 09, 2004 19:44 PM
To: R-Help Discussion
Cc: Andrew Robinson
Subject: [R] List seems to drop empty levels of factors when containing
them


Greetings R community,

I am curious about the following behaviour: if I define a factor, and
then store a subset of it in a list, the stored version seems to drop
levels that were not included in the subset.  E.g. ..

> mask <- c(T, F)
> grp.1 <- factor(c(1,2))
> list(grp.1)
[[1]]
[1] 1 2
Levels: 1 2

> list(grp.1[mask])
[[1]]
[1] 1
Levels: 1

It is as though the list were redefining the factor and dropping
empty levels.  I would like to keep them (I am using the list for a
two-dimensional tapply). Is there any way to avoid this?

I'm using R 2.0.0 on FreeBSD 5.2.1.

Thanks,

Andrew
-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From sundar.dorai-raj at pdf.com  Wed Nov 10 05:18:43 2004
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 09 Nov 2004 20:18:43 -0800
Subject: [R] List seems to drop empty levels of factors when containing
	them
In-Reply-To: <20041110034352.GK10340@uidaho.edu>
References: <20041110034352.GK10340@uidaho.edu>
Message-ID: <419196A3.4030508@pdf.com>



Andrew Robinson wrote:
> Greetings R community,
> 
> I am curious about the following behaviour: if I define a factor, and
> then store a subset of it in a list, the stored version seems to drop
> levels that were not included in the subset.  E.g. ..
> 
> 
>>mask <- c(T, F)
>>grp.1 <- factor(c(1,2))
>>list(grp.1)
> 
> [[1]]
> [1] 1 2
> Levels: 1 2
> 
> 
>>list(grp.1[mask])
> 
> [[1]]
> [1] 1
> Levels: 1
> 
> It is as though the list were redefining the factor and dropping
> empty levels.  I would like to keep them (I am using the list for a
> two-dimensional tapply). Is there any way to avoid this?
> 
> I'm using R 2.0.0 on FreeBSD 5.2.1.
> 
> Thanks,
> 
> Andrew

Andrew,

I cannot replicate this with R-2.0.0 patched on Win2000Pro:


 > mask <- c(T, F)
 > grp.1 <- factor(c(1,2))
 > list(grp.1)
[[1]]
[1] 1 2
Levels: 1 2

 > grp.1[mask]
[1] 1
Levels: 1 2
 > list(grp.1[mask])
[[1]]
[1] 1
Levels: 1 2

--sundar



From andrewr at uidaho.edu  Wed Nov 10 05:19:51 2004
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Wed, 10 Nov 2004 15:19:51 +1100
Subject: [R] List seems to drop empty levels of factors when containin	g
	them
In-Reply-To: <E7D5AB4811D20B489622AABA9C53859104E0DAFB@teal-exch.amgen.com>
References: <E7D5AB4811D20B489622AABA9C53859104E0DAFB@teal-exch.amgen.com>
Message-ID: <20041110041951.GN10340@uidaho.edu>

Matt,

very astute - thanks.  I did indeed have Hmisc loaded.

Andrew

On Tue, Nov 09, 2004 at 08:12:57PM -0800, Austin, Matt wrote:
> I don't get the same result, do you have a package loaded that would change
> the default behavior (such as Hmisc)?
> 
> > list(grp.1)
> [[1]]
> [1] 1 2
> Levels: 1 2
> 
> > list(grp.1[mask])
> [[1]]
> [1] 1
> Levels: 1 2
> 
> > library(Hmisc)
> <<snip>>
> 
> > list(grp.1[mask])
> [[1]]
> [1] 1
> Levels: 1
> 
> --Matt
> 
> > version
>          _              
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    0.0            
> year     2004           
> month    10             
> day      04             
> language R             
> 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Andrew Robinson
> Sent: Tuesday, November 09, 2004 19:44 PM
> To: R-Help Discussion
> Cc: Andrew Robinson
> Subject: [R] List seems to drop empty levels of factors when containing
> them
> 
> 
> Greetings R community,
> 
> I am curious about the following behaviour: if I define a factor, and
> then store a subset of it in a list, the stored version seems to drop
> levels that were not included in the subset.  E.g. ..
> 
> > mask <- c(T, F)
> > grp.1 <- factor(c(1,2))
> > list(grp.1)
> [[1]]
> [1] 1 2
> Levels: 1 2
> 
> > list(grp.1[mask])
> [[1]]
> [1] 1
> Levels: 1
> 
> It is as though the list were redefining the factor and dropping
> empty levels.  I would like to keep them (I am using the list for a
> two-dimensional tapply). Is there any way to avoid this?
> 
> I'm using R 2.0.0 on FreeBSD 5.2.1.
> 
> Thanks,
> 
> Andrew
> -- 
> Andrew Robinson                      Ph: 208 885 7115
> Department of Forest Resources       Fa: 208 885 6226
> University of Idaho                  E : andrewr at uidaho.edu
> PO Box 441133                        W : http://www.uidaho.edu/~andrewr
> Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
> No statement above necessarily represents my employer's opinion.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From petr.pikal at precheza.cz  Wed Nov 10 07:22:57 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 10 Nov 2004 07:22:57 +0100
Subject: [R] Multiple stripcharts using "for loop"
In-Reply-To: <1100007974.9427.30.camel@G4.site>
Message-ID: <4191C1D1.22531.35EFAD@localhost>

Hallo

I can not reproduce your result with artificial data (R2.0.0). Error 
says that some of your data have infinite values somewhere. Try to 
figure out in which cycle it gives this error and inspect your data 
from this cycle. I suppose you have all of them missing, but I 
wonder why boxplot workes and stripchart does not. Usually both 
gives me the same error message in that case.

I did not help much but the problem is probably in your data.

BTW, which version? what platform? 
> PLEASE do read the posting guide!

Cheers
Petr


On 9 Nov 2004 at 13:46, Paul JH Drake wrote:

> Hi
> I'm able to create multiple plots from a dataset using a "for loop" to
> change variables, but when I try with stripchart it doesn't work.
> 
> Standard boxplot:
> for(i in 2:14){boxplot(klk[,i]~Group,main=colnames(klk)[i])
> 
> Stripchart:
> for(i in 2:14){stripchart(klk[,i]~Group,main=colnames(klk)[i])
> 
> gives:Error in plot.window(xlim, ylim, log, asp, ...) : 
>         need finite xlim values
> In addition: Warning messages: 
> 1: no finite arguments to min; returning Inf 
> 2: no finite arguments to max; returning -Inf 
> 3: no finite arguments to min; returning Inf 
> 4: no finite arguments to max; returning -Inf 
> 
> 
> Does anyone know how to do this properly?
> 
> Thanks in advance
> 
> Paul
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From blindglobe at gmail.com  Wed Nov 10 08:49:17 2004
From: blindglobe at gmail.com (A.J. Rossini)
Date: Wed, 10 Nov 2004 08:49:17 +0100
Subject: [R] R under Pocket PC
In-Reply-To: <m2y8haq1ts.fsf@ganymede.ammp.or.tz>
References: <Pine.LNX.4.44.0411091223400.16452-100000@gannet.stats>
	<x2mzxrf3ih.fsf@biostat.ku.dk> <m28y9bq8v3.fsf@ganymede.ammp.or.tz>
	<loom.20041109T181422-634@post.gmane.org>
	<m2y8haq1ts.fsf@ganymede.ammp.or.tz>
Message-ID: <1abe3fa9041109234924b3cc90@mail.gmail.com>

One concern I recall from looking into this a while back (hopefully
not founded) was the issue with floating point handling on the ARMs
and similar PDA CPUs.

This was relevant to the familiar/intimate linux distro's around 18-24
months ago, when I was considering an IPAQ or Zaurus to replace my old
but not dead yet palm.


On 09 Nov 2004 19:06:39 +0000, David Whiting <david.whiting at ncl.ac.uk> wrote:
> Gabor Grothendieck <ggrothendieck at myway.com> writes:
> 
> 
> 
> > David Whiting <david.whiting <at> ncl.ac.uk> writes:
> >
> >
> > : It is interesting this has come up at this time. Two days ago I
> > : installed Linux on a Psion 5MX (16Mb RAM) and am tickled pink by
> > : it. Installation is easy. At the moment I only have a small
> > : compactflash disk so I have not been able to install X windows or R
> > : yet.
> >
> > You could see if MacAnova will install -- its an R/S-like package.
> > Its pretty small (even runs on 640K DOS) and is quite portable.  Its
> > not as powerful as R but its still amazingly powerful and you might
> > be able to fit it on.
> 
> Thanks. I took a quick look and I think I would have to cross-compile
> it (I didn't find an ARM binary).  When I get my larger compactflash
> card I am hoping that I will be able to just get the ARM debian
> package (and all the assorted dependencies) without having to setup a
> cross-compiling tool chain---I will have to take a look and see what
> is involved in doing this. I don't have a compiler installed on the
> Psion and probably would not have enough room for all the libraries
> (just guessing here).
> 
> Dave
> 
> 
> 
> --
> David Whiting
> University of Newcastle upon Tyne, UK
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 

best,
-tony

---
A.J. Rossini
blindglobe at gmail.com



From vito_ricci at yahoo.com  Wed Nov 10 09:14:43 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Wed, 10 Nov 2004 09:14:43 +0100 (CET)
Subject: [R] Loading some function at R startup
Message-ID: <20041110081443.15728.qmail@web41208.mail.yahoo.com>

Dear R-users,

I've built these functions usefell for me to
import/export data from/to Excel:

importa.da.excel<-function(){read.delim2("clipboard",
dec=",")
## questa funzione consente di importare dati da Excel
in R
## selezionare in Excel le celle che contengono i
dati, 
## compresi in nomi delle colonne
## Autore: Vito Ricci email:vito_ricci at yahoo.com
## Data di creazione: 09/11/04
}

esporta.in.excel<-function(dati){write.table(dati,"clipboard",
sep="\t", dec=",", col.names=NA)
## questa funzione consente di esportare dati in Excel
da R
## passare come argomento il dataframe, la matrice,
vettore da esportare 
## Autore: Vito Ricci email:vito_ricci at yahoo.com
## Data di creazione: 09/11/04
}

I wish those functions will be loaded each time starts
an R session. How can I get this? 

platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    0.0            
year     2004           
month    10             
day      04             
language R   

Thanks in advance.
Vito

=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese http://www.modugno.it/archivio/cat_palese.shtml



From plummer at iarc.fr  Wed Nov 10 09:19:06 2004
From: plummer at iarc.fr (Martyn Plummer)
Date: Wed, 10 Nov 2004 09:19:06 +0100
Subject: [R] R works on Fedora Core 3
In-Reply-To: <20041109184717.GA15407@psych>
References: <20041109174442.GA27104@psych>
	<65C11A98-327E-11D9-AE8C-000A95C76CA8@oulu.fi>
	<20041109184717.GA15407@psych>
Message-ID: <1100074746.6198.9.camel@nemo>

On Tue, 2004-11-09 at 19:47, Jonathan Baron wrote:
> On 11/09/04 20:37, Jari Oksanen wrote:
> >
> >On 9 Nov 2004, at 19:44, Jonathan Baron wrote:
> >
> >> The RPM for Fedora Core 2 seems to work just fine on Core 3.
> >>
> >> (The graphics window got smaller, but I'm sure there is a setting
> >> for that.)
> >>
> >That would be good news. I really don't know how the graphics window
> >became so big at some stage. (MacOS X is just cute here: tiny, sharp,
> >fast graphics window.)
> 
> I have the opposite problem, a 1680x1050 display.
> 
> >Has the options()printcmd reappeared, so that dev.print() works without
> >changing default options?
> 
> I can't imagine how this would change.  This is the same "old"
> RPM, not a new one.  The option is there, and I don't think it
> ever disappeared.  I can't test it.  This is my laptop, which is
> not set up to print anything.

My mistake.  The default print command is determined at configure time.
But the RedHat RPMS are built in a sandbox that has only the minimal
configuration needed to build R. This doesn't include the lpr package so
the default print command is null. I will fix this in the next RPM
release, but right now I am upgrading to FC3. 

Martyn



From ripley at stats.ox.ac.uk  Wed Nov 10 09:35:54 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 10 Nov 2004 08:35:54 +0000 (GMT)
Subject: [R] Loading some function at R startup
In-Reply-To: <20041110081443.15728.qmail@web41208.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0411100832360.14720-100000@gannet.stats>

?Startup will tell you how: most likely you want to use ~/.Rprofile.

BTW, read.delim2 already sets dec="," by default!

On Wed, 10 Nov 2004, Vito Ricci wrote:

> Dear R-users,
> 
> I've built these functions usefell for me to
> import/export data from/to Excel:
> 
> importa.da.excel<-function(){read.delim2("clipboard",
> dec=",")
> ## questa funzione consente di importare dati da Excel
> in R
> ## selezionare in Excel le celle che contengono i
> dati, 
> ## compresi in nomi delle colonne
> ## Autore: Vito Ricci email:vito_ricci at yahoo.com
> ## Data di creazione: 09/11/04
> }
> 
> esporta.in.excel<-function(dati){write.table(dati,"clipboard",
> sep="\t", dec=",", col.names=NA)
> ## questa funzione consente di esportare dati in Excel
> da R
> ## passare come argomento il dataframe, la matrice,
> vettore da esportare 
> ## Autore: Vito Ricci email:vito_ricci at yahoo.com
> ## Data di creazione: 09/11/04
> }
> 
> I wish those functions will be loaded each time starts
> an R session. How can I get this? 
> 
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    2              
> minor    0.0            
> year     2004           
> month    10             
> day      04             
> language R   
> 
> Thanks in advance.
> Vito
> 
> =====
> Diventare costruttori di soluzioni
> Became solutions' constructors
> 
> "The business of the statistician is to catalyze 
> the scientific learning process."  
> George E. P. Box
> 
> 
> Visitate il portale http://www.modugno.it/
> e in particolare la sezione su Palese http://www.modugno.it/archivio/cat_palese.shtml
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From lars.strand at skogforsk.no  Wed Nov 10 09:29:50 2004
From: lars.strand at skogforsk.no (Lars Strand)
Date: Wed, 10 Nov 2004 09:29:50 +0100
Subject: [R] R under Pocket PC
Message-ID: <000001c4c6ff$722e65e0$9cb93c0a@skogforsk.no>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041110/96690097/attachment.pl

From stuart.leask at nottingham.ac.uk  Wed Nov 10 10:01:11 2004
From: stuart.leask at nottingham.ac.uk (Stuart Leask)
Date: Wed, 10 Nov 2004 09:01:11 -0000
Subject: [R] R under Pocket PC
References: <Pine.LNX.4.44.0411091223400.16452-100000@gannet.stats><x2mzxrf3ih.fsf@biostat.ku.dk>
	<m28y9bq8v3.fsf@ganymede.ammp.or.tz><loom.20041109T181422-634@post.gmane.org><m2y8haq1ts.fsf@ganymede.ammp.or.tz>
	<1abe3fa9041109234924b3cc90@mail.gmail.com>
Message-ID: <006101c4c703$d394a520$f2e1f380@OPENZAURUS>

> One concern I recall from looking into this a while back (hopefully
> not founded) was the issue with floating point handling on the ARMs
> and similar PDA CPUs.
>
> This was relevant to the familiar/intimate linux distro's around 18-24
> months ago, when I was considering an IPAQ or Zaurus to replace my old
> but not dead yet palm.

Thanks to the excellent work of the R developers in keeping all the code
quite standard, and some specific work by Simon Pickering (Bath University),
I have had R working (with graphics) on my Sharp Zaurus for some time.
Sadly, checking today his site is down, but binaries for the Zaurus are
available.

There was a problem with NA handling that needed specifically addressing,
and some missing fonts, but the fpu emulation worked fine. This was using
X11, keypebble and VNC. It could _just about_ run in the standard RAM -
although was a lot happier with extra memory. Most impressive, and well done
Simon (and the R team).

However, a note of caution - presumably due to the lack of a hardware fpu,
and perhaps also the relatively slow access speed of SD ram, benchmarks on
the zaurus ran anything up to (wait for it!) 100 times slower than a pentium
of the same clock speed. This has sadly left R on my Zaurus largely idle, as
in practice I found it just too slow to be usable.

I fear this problem could afflict any handheld implementation, until they
start putting fpu's on these chips.

Stuart

(PS. I have the same problem with a mini-ITX 'silent PC' - many of these
low-power cpus (eg. Eden) lack hardware FPU. They run many office apps fine,
but when I benchmarked with a stats program, I found it 50 times slower than
a pentium of equivalent clock speed! I am advised that even those that do
have hardware FPU eg. the Nehemiah chip are only 50-60% as fast as a
pentium-class cpu of similar clock speed)

Dr Stuart Leask DM MRCPsych, Senior Lecturer
University Dept of Psychiatry, Duncan Macmillan House
Porchester Road, Nottingham. NG3 6AA. UK
tel. 0115 924 9924 xtn 40784
http://www.nottingham.ac.uk/psychiatry/staff/s_leask.html


This message has been scanned but we cannot guarantee that it and any
attachments are free from viruses or other damaging content: you are
advised to perform your own checks.  Email communications with the
University of Nottingham may be monitored as permitted by UK legislation.



From maechler at stat.math.ethz.ch  Wed Nov 10 10:07:58 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 10 Nov 2004 10:07:58 +0100
Subject: [R] gplot.hexbin - how to set figure margin.
In-Reply-To: <200411092030350675.16E0FBD1@mail.math.fu-berlin.de>
References: <200411092030350675.16E0FBD1@mail.math.fu-berlin.de>
Message-ID: <16785.55918.887432.647156@gargle.gargle.HOWL>

This is really about the development version of a bioconductor
package, so why didn't you contact the package maintainer or authors, as
the posting guide asks you?

This (unreleased) version of hexbin is working with 
"grid" based graphics
       ===> no par() mess.

One reason that it is "development"  *is* the fact that 
currently, you really have to know grid graphics pretty well,
before you can customize hexbin plots.

To learn about grid,
start reading 
Paul (= grid's prinicipal author)'s Keynote speech at the
useR!2004 conference:
   http://www.ci.tuwien.ac.at/Conferences/useR-2004/Keynotes/
and then
   browseURL(system.file("doc/index.html", package="grid"))

Martin Maechler (2nd author of "hexbin").

>>>>> "Eryk" == Eryk Wolski <wolski at molgen.mpg.de>
>>>>>     on Tue, 09 Nov 2004 20:30:35 +0100 writes:

    Eryk> Hi,
    Eryk> Would like to use to the hexbin package to plot a 2D hist - because it looks really _cool_.
    Eryk> My problem  is related to  drawing a pdf hexbin graphic in series of other graphics.
    Eryk> All other figures have a par(mar=c(3.2,3.2,1,1)). So the gplot.hexbin figure in this series looks a little alienated?

    Eryk> Was trying a to specify the _mar_ using par, viewport, hexViewport etc. a little.

    Eryk> My question. How to set the margins in the sample code below. 


    Eryk> x <- rnorm(10000)
    Eryk> y <- rnorm(10000)
    Eryk> bin <- hexbin(x,y)

    Eryk> ## Plot : Note that 'gplot.hexbin' is the S4 plot method for hexbin !
    Eryk> ## ----              ------------         --------------
    Eryk> plot(bin)

This

    Eryk> /E


    Eryk> Dipl. bio-chem. Eryk Witold Wolski             @    MPI-Moleculare Genetic   
    Eryk> Ihnestrasse 63-73 14195 Berlin                'v'                            
    Eryk> tel: 0049-30-83875219                        /   \                           
    Eryk> mail: witek96 at users.sourceforge.net        ---W-W----                        
    Eryk> http://r4proteomics.sourceforg.net

    Eryk> ______________________________________________
    Eryk> R-help at stat.math.ethz.ch mailing list
    Eryk> https://stat.ethz.ch/mailman/listinfo/r-help
    Eryk> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From maechler at stat.math.ethz.ch  Wed Nov 10 10:14:16 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 10 Nov 2004 10:14:16 +0100
Subject: [R] Need car() and cdr() for '...'
In-Reply-To: <Pine.OSF.4.58.0411091409120.37458@odin.mdacc.tmc.edu>
References: <Pine.OSF.4.58.0411091409120.37458@odin.mdacc.tmc.edu>
Message-ID: <16785.56296.306204.127923@gargle.gargle.HOWL>

>>>>> "Paul" == Paul Roebuck <roebuck at odin.mdacc.tmc.edu>
>>>>>     on Tue, 9 Nov 2004 14:28:42 -0600 (CST) writes:

    Paul> Needed to redefine function "sum" for my MATLAB package.

("MATLAB" package, hmm, interesting, let us know more..)

    Paul> There's something similar in Chambers's Green Book (pg 351)
    Paul> so I modified it as such:

    .........

    Paul> So does someone have LISP-derived car/adr
    Paul> functions I can use to split the '...' list such that
    Paul> the generic function could use this instead:

    Paul> sum(c(sum(car(...), na.rm = na.rm),
    Paul>       sum(cdr(...), na.rm = na.rm)))

###---------------

car <- function(...) list(...)[[1]]
cdr <- function(...) list(...) [-1]

## testing:
tst <- function(...) list(car = car(...), cdr = cdr(...))

str(tst(a=1, b=2:4, c=5))

###---------------

gives 

 List of 2
  $ car: num 1
  $ cdr:List of 2
   ..$ b: int [1:3] 2 3 4
   ..$ c: num 5

which I think is what you want.



From david.whiting at ncl.ac.uk  Wed Nov 10 10:09:09 2004
From: david.whiting at ncl.ac.uk (David Whiting)
Date: 10 Nov 2004 09:09:09 +0000
Subject: [R] R under Pocket PC
In-Reply-To: <1abe3fa9041109234924b3cc90@mail.gmail.com>
References: <Pine.LNX.4.44.0411091223400.16452-100000@gannet.stats>
	<x2mzxrf3ih.fsf@biostat.ku.dk> <m28y9bq8v3.fsf@ganymede.ammp.or.tz>
	<loom.20041109T181422-634@post.gmane.org>
	<m2y8haq1ts.fsf@ganymede.ammp.or.tz>
	<1abe3fa9041109234924b3cc90@mail.gmail.com>
Message-ID: <m2u0ryoytm.fsf@ganymede.ammp.or.tz>

"A.J. Rossini" <blindglobe at gmail.com> writes:

> One concern I recall from looking into this a while back (hopefully
> not founded) was the issue with floating point handling on the ARMs
> and similar PDA CPUs.

> This was relevant to the familiar/intimate linux distro's around 18-24
> months ago, when I was considering an IPAQ or Zaurus to replace my old
> but not dead yet palm.

Ah, yes, I remember this now. I've just searched the R archives and
must admit the prospects do not look good.  I don't know much about
floating point handling, the FPU, etc., but a quick search of the
Linux/Psion 5MX archive led me to this:

"So, if you decide to switch to using soft-float for some application,
you MUST recompile ALL the libraries that application is going to use,
including system libraries like the C library.  If those libraries are
dynamically linked libraries, you then must also recompile all the
applications that share those same libraries. And then your
application will run correctly only on systems with soft-float
environments, unless you link it statically in which case it will run
anywhere (even on a kernel with NWFPE configured in -- it will simply
not be invoked). That"s the main reason why mainstream ARM
distributions are still reluctant to switch to soft-float because of
the associated compatibility pain."

If I understand this correctly it looks like it might be possible if I
recompile everything, something that I was hoping I would be able to
avoid. On the other hand, it could be a great learning experience.

Even if R does not work 100% I'm still going to be happy having my
other applications and files with me.  I'll report back on how I get
on.

Dave.

-- 
David Whiting
University of Newcastle upon Tyne, UK



From asemeria at cramont.it  Wed Nov 10 10:27:46 2004
From: asemeria at cramont.it (asemeria@cramont.it)
Date: Wed, 10 Nov 2004 10:27:46 +0100
Subject: [R] small world models?
Message-ID: <OF3FEA7020.A8C1AEB3-ONC1256F48.0033BACB-C1256F48.0032EDFF@tomware.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041110/eee60b94/attachment.pl

From rksh at soc.soton.ac.uk  Wed Nov 10 10:47:25 2004
From: rksh at soc.soton.ac.uk (Robin Hankin)
Date: Wed, 10 Nov 2004 09:47:25 +0000
Subject: [R] Need car() and cdr() for '...'
In-Reply-To: <16785.56296.306204.127923@gargle.gargle.HOWL>
References: <Pine.OSF.4.58.0411091409120.37458@odin.mdacc.tmc.edu>
	<16785.56296.306204.127923@gargle.gargle.HOWL>
Message-ID: <a06002001bdb792ac996d@[139.166.242.29]>

>  >>>>> "Paul" == Paul Roebuck <roebuck at odin.mdacc.tmc.edu>
>>>>>>      on Tue, 9 Nov 2004 14:28:42 -0600 (CST) writes:
>
>     Paul> Needed to redefine function "sum" for my MATLAB package.
>
>("MATLAB" package, hmm, interesting, let us know more..)

well, a few years ago I put together a selection of matlab commands 
that don't have a natural R
analogue.  These include

quiver()
feather()
compass()
flipud()
fliplr()

but I never got round to  packaging it up (mostly because of a 
problem with quiver()
that I never resolved).

If there is enough (any?) interest, I'll dig up the code and make a package.



-- 
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
SO14 3ZH
tel +44(0)23-8059-7743
initialDOTsurname at soc.soton.ac.uk (edit in obvious way; spam precaution)



From vantini at mate.polimi.it  Wed Nov 10 11:27:00 2004
From: vantini at mate.polimi.it (Simone Vantini)
Date: Wed, 10 Nov 2004 11:27:00 +0100 (CET)
Subject: [R] Does something like partition.rpart() exist?
Message-ID: <13007.81.208.60.192.1100082420.squirrel@webmail.mate.polimi.it>

I'd like to create a bidimensional presentation of a classification tree
built using the rpart() function. I've seen that a partition.tree()
function exists for the tree() function. Does a similar function  exist
for the rpart() function?
Thanks a lot Simone Vantini



From chrysopa at insecta.ufv.br  Wed Nov 10 11:59:49 2004
From: chrysopa at insecta.ufv.br (Ronaldo Reis Jr.)
Date: Wed, 10 Nov 2004 08:59:49 -0200
Subject: [R] glm.nb
Message-ID: <200411100859.49819.chrysopa@insecta.ufv.br>

Hi,

I make some simulations with rnbinom and try to test with glm.nb.

But in some data set the glm.nb fail.

Look:

pop <- rnbinom(n=1000,size=1,mu=0.05)
> glm.nb(pop~1,maxit=1000)
Error in while ((it <- it + 1) < limit && abs(del) > eps) { : 
 missing value where TRUE/FALSE needed

look some pop charactetistics:
> summary(pop)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  0.000   0.000   0.000   0.052   0.000   2.000

> hist(pop,plot=F)
$breaks
 [1] 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0

$counts
 [1] 949   0   0   0  50   0   0   0   0   1

$intensities
 [1] 4.744999 0.000000 0.000000 0.000000 0.250000 0.000000 0.000000 0.000000
 [9] 0.000000 0.005000

$density
 [1] 4.744999 0.000000 0.000000 0.000000 0.250000 0.000000 0.000000 0.000000
 [9] 0.000000 0.005000

$mids
 [1] 0.1 0.3 0.5 0.7 0.9 1.1 1.3 1.5 1.7 1.9

My question is:

Why this error occour and how to solve this problem without increase the mu 
parameter in rnbinom?


Thanks fo all

Ronaldo

---
Your ignorance cramps my conversation.
--
|>   // | \\   [***********************************]
|   ( ??   ?? )  [Ronaldo Reis J??nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36571-000 Vi??osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-2532                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From p.dalgaard at biostat.ku.dk  Wed Nov 10 12:03:36 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 10 Nov 2004 12:03:36 +0100
Subject: [R] R under Pocket PC
In-Reply-To: <006101c4c703$d394a520$f2e1f380@OPENZAURUS>
References: <Pine.LNX.4.44.0411091223400.16452-100000@gannet.stats>
	<x2mzxrf3ih.fsf@biostat.ku.dk> <m28y9bq8v3.fsf@ganymede.ammp.or.tz>
	<loom.20041109T181422-634@post.gmane.org>
	<m2y8haq1ts.fsf@ganymede.ammp.or.tz>
	<1abe3fa9041109234924b3cc90@mail.gmail.com>
	<006101c4c703$d394a520$f2e1f380@OPENZAURUS>
Message-ID: <x2d5ymj793.fsf@biostat.ku.dk>

"Stuart Leask" <stuart.leask at nottingham.ac.uk> writes:

> > One concern I recall from looking into this a while back (hopefully
> > not founded) was the issue with floating point handling on the ARMs
> > and similar PDA CPUs.
> >
> > This was relevant to the familiar/intimate linux distro's around 18-24
> > months ago, when I was considering an IPAQ or Zaurus to replace my old
> > but not dead yet palm.
> 
> Thanks to the excellent work of the R developers in keeping all the code
> quite standard, and some specific work by Simon Pickering (Bath University),
> I have had R working (with graphics) on my Sharp Zaurus for some time.
> Sadly, checking today his site is down, but binaries for the Zaurus are
> available.
> 
> There was a problem with NA handling that needed specifically addressing,
> and some missing fonts, but the fpu emulation worked fine. This was using
> X11, keypebble and VNC. It could _just about_ run in the standard RAM -
> although was a lot happier with extra memory. Most impressive, and well done
> Simon (and the R team).

Right. As I recall it, the FPU/NA issue was a case of configure
getting the byte-order wrong and not (as was believed for a while) a
problem with non-IEEE arithmetic.
 
> However, a note of caution - presumably due to the lack of a hardware fpu,
> and perhaps also the relatively slow access speed of SD ram, benchmarks on
> the zaurus ran anything up to (wait for it!) 100 times slower than a pentium
> of the same clock speed. This has sadly left R on my Zaurus largely idle, as
> in practice I found it just too slow to be usable.

Hmm. So you're probably limited to problems that require on the order
of tens of milliseconds FPU time on a normal machine. I suppose that
there are usages where this is feasible.

> I fear this problem could afflict any handheld implementation, until they
> start putting fpu's on these chips.

Yes. Now where are the video gamers when you need them?

> (PS. I have the same problem with a mini-ITX 'silent PC' - many of these
> low-power cpus (eg. Eden) lack hardware FPU. They run many office apps fine,
> but when I benchmarked with a stats program, I found it 50 times slower than
> a pentium of equivalent clock speed! I am advised that even those that do
> have hardware FPU eg. the Nehemiah chip are only 50-60% as fast as a
> pentium-class cpu of similar clock speed)

Hmm, the newer ones (Eden-N, Eden-ESP) claim "full-speed FPU". I've
been toying around with the idea of building a system around the
MythTV stuff to get a harddisk video recording, etc. Could be kind of
fun to run R on it with display to the TV (but could you control R
with the TV remote control?)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From wolfram at fischer-zim.ch  Wed Nov 10 12:28:13 2004
From: wolfram at fischer-zim.ch (Wolfram Fischer)
Date: Wed, 10 Nov 2004 12:28:13 +0100
Subject: [R] lattice: ordering the entries in a dotplot of a vector
Message-ID: <20041110112813.GA4165@s1x.local>

I tried:
    n <- 9
    x <- sample(n)
    names(x) <- LETTERS[1:n]
    dotplot( sort(x) )

The lines of the dotplot are not ordered according to the values of x
(but according to the names of x).

So I did:

    dfx <- data.frame( x=x, label=factor( names(x), names(x)[order(x)] ) )
    dotplot( label ~ x, data=dfx )

So I got what I wanted.

My question: Is there an easier solution for doing that?

Wolfram



From p.drake at beatson.gla.ac.uk  Wed Nov 10 12:49:27 2004
From: p.drake at beatson.gla.ac.uk (Paul JH Drake)
Date: Wed, 10 Nov 2004 11:49:27 +0000
Subject: [R] Multiple stripcharts using "for loop"
Message-ID: <1100087367.12807.8.camel@G4.site>

Sorry, I didn't post my system details:
SuSE Linux 9.1
R version 2.0.0
P4 Laptop, 512MB RAM

THanks for the reply. I'll see what I can do with the data.

Paul



From r.g.brown at cefas.co.uk  Wed Nov 10 13:03:38 2004
From: r.g.brown at cefas.co.uk (Robert Brown FM CEFAS)
Date: Wed, 10 Nov 2004 12:03:38 -0000
Subject: [R] Basic Q on coercing factors in data frames to numeric
Message-ID: <3589BC4D64C84341AE0C258244F977A2B60B63@expressa.corp.cefas.co.uk>

Hi there,

I'm running R 2.0.0 on Windows 95.  I'm trying to coerce a column of factors within a data frame to numeric.  This is not a problem with a vector, but I can't find a way to index a column within a data frame to achieve this. All the examples from 'An introduction to R', 'S-plus 6 programmers guide', etc, use simple vectors.  I'm sure I'm missing something obvious but I can't find a way to change a single column in data frame. Ive tried several approaches: here is one

> summary(test)
      age        yrclass       weight              year     
 1      :10   1992   :10   Min.   :0.005333   Min.   :1993  
 2      :10   1989   : 9   1st Qu.:0.221790   1st Qu.:1995  
 3      :10   1990   : 9   Median :0.413411   Median :1997  
 4      :10   1991   : 9   Mean   :0.420119   Mean   :1997  
 5      :10   1988   : 8   3rd Qu.:0.559819   3rd Qu.:2000  
 6      :10   1993   : 8   Max.   :1.189000   Max.   :2002  
 (Other):82   (Other):89                                    
> contents(test)

Data frame:test 142 observations and 4 variables    Maximum # NAs:0

        Levels Storage
age         23 integer
yrclass     28 integer
weight          double
year            double

+--------+---------------------------------------------------------------------+
|Variable|Levels                                                               |
+--------+---------------------------------------------------------------------+
| age    |0,1,10,11,12,13,14,15,16,18,19,2,20,21,24,25,3,4,5,6,7,8,9           |
+--------+---------------------------------------------------------------------+
| yrclass|1969,1974,1975,1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988|
|        |1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000,2001,2002|
+--------+---------------------------------------------------------------------+
> is(test$yrclass,"factor")
[1] TRUE
> is(test$yrclass,"numeric")
[1] FALSE
> as(test[,2],"numeric")
  [1]  1  1  2  3  3  4  5  6  6  6  7  7  8  8  8  8  9  9  9  9 10 10 10 10 10
 [26] 10 10 11 11 11 11 11 12 12 12 12 12 12 12 13 13 13 13 13 13 13 14 14 14 14
 [51] 14 14 14 14 15 15 15 15 15 15 15 15 15 16 16 16 16 16 16 16 16 16 17 17 17
 [76] 17 17 17 17 17 17 18 18 18 18 18 18 18 18 18 18 19 19 19 19 19 19 19 19 20
[101] 20 20 20 20 20 20 20 21 21 21 21 21 21 21 21 22 22 22 22 22 22 22 23 23 23
[126] 23 23 23 24 24 24 24 24 25 25 25 25 26 26 27 27 28
> is(test$yrclass,"numeric")
[1] FALSE


Regards,

Robert Brown



From B.Rowlingson at lancaster.ac.uk  Wed Nov 10 13:11:51 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Wed, 10 Nov 2004 12:11:51 +0000
Subject: [R] R under Pocket PC
In-Reply-To: <x2d5ymj793.fsf@biostat.ku.dk>
References: <Pine.LNX.4.44.0411091223400.16452-100000@gannet.stats>	<x2mzxrf3ih.fsf@biostat.ku.dk>
	<m28y9bq8v3.fsf@ganymede.ammp.or.tz>	<loom.20041109T181422-634@post.gmane.org>	<m2y8haq1ts.fsf@ganymede.ammp.or.tz>	<1abe3fa9041109234924b3cc90@mail.gmail.com>	<006101c4c703$d394a520$f2e1f380@OPENZAURUS>
	<x2d5ymj793.fsf@biostat.ku.dk>
Message-ID: <41920587.6000103@lancaster.ac.uk>


> Could be kind of
> fun to run R on it with display to the TV (but could you control R
> with the TV remote control?)

  Peter,

    I'm sure everyone on this list feels you work hard enough on R as it 
is, dont let it invade your living room as well!

Baz



From Arne.Muller at aventis.com  Wed Nov 10 13:21:48 2004
From: Arne.Muller at aventis.com (Arne.Muller@aventis.com)
Date: Wed, 10 Nov 2004 13:21:48 +0100
Subject: [R] printing to stderr
Message-ID: <C80ECAFA2ACC1B45BE45D133ED660ADE010BF296@crbsmxsusr04.pharma.aventis.com>

Hello,

is it possible to configure the print function to print to stderr?

	kind regards,

	Arne



From tjrc at sanger.ac.uk  Wed Nov 10 13:27:05 2004
From: tjrc at sanger.ac.uk (Tim Cutts)
Date: Wed, 10 Nov 2004 12:27:05 +0000
Subject: [R] R under Pocket PC
In-Reply-To: <x2d5yneygn.fsf@biostat.ku.dk>
References: <Pine.LNX.4.44.0411091631001.24326-100000@gannet.stats>
	<x2d5yneygn.fsf@biostat.ku.dk>
Message-ID: <D4D5A05A-3313-11D9-AF52-000A95B2B140@sanger.ac.uk>


On 9 Nov 2004, at 5:14 pm, Peter Dalgaard wrote:
>>> PalmOS would probably be right out.  :-)
>>
>> I think you are belittling the work done to get R running on as wide a
>> range of platforms as it does.
>
> ?
>
> I only saw a bit of excessive pessimism in that remark.

Yes, no disrespect was intended at all.  Porting to PalmOS is quite a 
different kettle of fish compared to porting between (say) Windows and 
UNIX, because the OS doesn't have a libc; if you use almost any 
standard C library function your binary becomes a huge (because it 
needs to be statically linked with the C library that comes with your 
cross compiler).  A true PalmOS port of any piece of software often 
requires a lot of work to replace standard C library calls with the 
equivalents built into the machine.  Someone may have created some sort 
of wrapper C library, but when I last wrote any software for Palm 
devices, such a thing did not exist.

Admittedly, that was about two years ago, when I wrote some stuff for 
PalmOS 4.

Tim

-- 
Dr Tim Cutts
Informatics Systems Group, Wellcome Trust Sanger Institute
GPG: 1024D/E3134233 FE3D 6C73 BBD6 726A A3F5  860B 3CDD 3F56 E313 4233



From ozric at web.de  Wed Nov 10 13:52:29 2004
From: ozric at web.de (Christian Schulz)
Date: Wed, 10 Nov 2004 13:52:29 +0100
Subject: [R] "conditional duplicates"?
Message-ID: <41920F0D.9030900@web.de>

Hi,

i would like check a repeated measurement dataset, whether
cases (which (id) could be more than one time included and not all same 
times included.)
have more than one times the same SMONTH!?

#This pseudo code didn't work with a for loop, because the [i+1] isn't 
known.
How i could refer to [i+1] ?

if(ID[i] == ID[i+1] & SMONTH[i] !=[i+1])  {res[i] <- 0 }
if(ID[i] == ID[i+1] & SMONTH[i] == [i+1]) {res[i] <- 1 }
 
many thanks,
christian



From ripley at stats.ox.ac.uk  Wed Nov 10 14:06:51 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 10 Nov 2004 13:06:51 +0000 (GMT)
Subject: [R] printing to stderr
In-Reply-To: <C80ECAFA2ACC1B45BE45D133ED660ADE010BF296@crbsmxsusr04.pharma.aventis.com>
Message-ID: <Pine.LNX.4.44.0411101304390.18087-100000@gannet.stats>

On Wed, 10 Nov 2004 Arne.Muller at aventis.com wrote:

> is it possible to configure the print function to print to stderr?

No, but where standard output goes is controlled by sink(), so you can 
achieve the same effect.  R internally has no idea what output comes from 
print() (which is not just one function but hundreds of methods).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dimitris.rizopoulos at med.kuleuven.ac.be  Wed Nov 10 14:09:01 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Wed, 10 Nov 2004 14:09:01 +0100
Subject: [R] "conditional duplicates"?
References: <41920F0D.9030900@web.de>
Message-ID: <003f01c4c726$7230ec30$0540210a@www.domain>

Hi Christian,

may be this is helpful:

dat <- data.frame(id=rep(1:4, each=5), smonth=sample(1:5, 20, TRUE))
dat$duplicated <- unlist(tapply(dat$smonth, dat$id, duplicated))
dat

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Christian Schulz" <ozric at web.de>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, November 10, 2004 1:52 PM
Subject: [R] "conditional duplicates"?


> Hi,
>
> i would like check a repeated measurement dataset, whether
> cases (which (id) could be more than one time included and not all 
> same times included.)
> have more than one times the same SMONTH!?
>
> #This pseudo code didn't work with a for loop, because the [i+1] 
> isn't known.
> How i could refer to [i+1] ?
>
> if(ID[i] == ID[i+1] & SMONTH[i] !=[i+1])  {res[i] <- 0 }
> if(ID[i] == ID[i+1] & SMONTH[i] == [i+1]) {res[i] <- 1 }
>
> many thanks,
> christian
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From rpeng at jhsph.edu  Wed Nov 10 14:20:27 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Wed, 10 Nov 2004 08:20:27 -0500
Subject: [R] Basic Q on coercing factors in data frames to numeric
In-Reply-To: <3589BC4D64C84341AE0C258244F977A2B60B63@expressa.corp.cefas.co.uk>
References: <3589BC4D64C84341AE0C258244F977A2B60B63@expressa.corp.cefas.co.uk>
Message-ID: <4192159B.4050602@jhsph.edu>

I believe this is a FAQ.  See 
http://cran.r-project.org/doc/FAQ/R-FAQ.html#How-do-I-convert-factors-to-numeric_003f

-roger

Robert Brown FM CEFAS wrote:
> Hi there,
> 
> I'm running R 2.0.0 on Windows 95.  I'm trying to coerce a column of factors within a data frame to numeric.  This is not a problem with a vector, but I can't find a way to index a column within a data frame to achieve this. All the examples from 'An introduction to R', 'S-plus 6 programmers guide', etc, use simple vectors.  I'm sure I'm missing something obvious but I can't find a way to change a single column in data frame. Ive tried several approaches: here is one
> 
> 
>>summary(test)
> 
>       age        yrclass       weight              year     
>  1      :10   1992   :10   Min.   :0.005333   Min.   :1993  
>  2      :10   1989   : 9   1st Qu.:0.221790   1st Qu.:1995  
>  3      :10   1990   : 9   Median :0.413411   Median :1997  
>  4      :10   1991   : 9   Mean   :0.420119   Mean   :1997  
>  5      :10   1988   : 8   3rd Qu.:0.559819   3rd Qu.:2000  
>  6      :10   1993   : 8   Max.   :1.189000   Max.   :2002  
>  (Other):82   (Other):89                                    
> 
>>contents(test)
> 
> 
> Data frame:test 142 observations and 4 variables    Maximum # NAs:0
> 
>         Levels Storage
> age         23 integer
> yrclass     28 integer
> weight          double
> year            double
> 
> +--------+---------------------------------------------------------------------+
> |Variable|Levels                                                               |
> +--------+---------------------------------------------------------------------+
> | age    |0,1,10,11,12,13,14,15,16,18,19,2,20,21,24,25,3,4,5,6,7,8,9           |
> +--------+---------------------------------------------------------------------+
> | yrclass|1969,1974,1975,1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988|
> |        |1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000,2001,2002|
> +--------+---------------------------------------------------------------------+
> 
>>is(test$yrclass,"factor")
> 
> [1] TRUE
> 
>>is(test$yrclass,"numeric")
> 
> [1] FALSE
> 
>>as(test[,2],"numeric")
> 
>   [1]  1  1  2  3  3  4  5  6  6  6  7  7  8  8  8  8  9  9  9  9 10 10 10 10 10
>  [26] 10 10 11 11 11 11 11 12 12 12 12 12 12 12 13 13 13 13 13 13 13 14 14 14 14
>  [51] 14 14 14 14 15 15 15 15 15 15 15 15 15 16 16 16 16 16 16 16 16 16 17 17 17
>  [76] 17 17 17 17 17 17 18 18 18 18 18 18 18 18 18 18 19 19 19 19 19 19 19 19 20
> [101] 20 20 20 20 20 20 20 21 21 21 21 21 21 21 21 22 22 22 22 22 22 22 23 23 23
> [126] 23 23 23 24 24 24 24 24 25 25 25 25 26 26 27 27 28
> 
>>is(test$yrclass,"numeric")
> 
> [1] FALSE
> 
> 
> Regards,
> 
> Robert Brown
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From ozric at web.de  Wed Nov 10 14:27:07 2004
From: ozric at web.de (Christian Schulz)
Date: Wed, 10 Nov 2004 14:27:07 +0100
Subject: [R] "conditional duplicates"?
In-Reply-To: <003f01c4c726$7230ec30$0540210a@www.domain>
References: <41920F0D.9030900@web.de>
	<003f01c4c726$7230ec30$0540210a@www.domain>
Message-ID: <4192172B.4010800@web.de>

Many Thanks!

Dimitris Rizopoulos wrote:

> Hi Christian,
>
> may be this is helpful:
>
> dat <- data.frame(id=rep(1:4, each=5), smonth=sample(1:5, 20, TRUE))
> dat$duplicated <- unlist(tapply(dat$smonth, dat$id, duplicated))
> dat
>
> Best,
> Dimitris
>
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
>
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/16/396887
> Fax: +32/16/337015
> Web: http://www.med.kuleuven.ac.be/biostat/
>     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
>
>
> ----- Original Message ----- From: "Christian Schulz" <ozric at web.de>
> To: <r-help at stat.math.ethz.ch>
> Sent: Wednesday, November 10, 2004 1:52 PM
> Subject: [R] "conditional duplicates"?
>
>
>> Hi,
>>
>> i would like check a repeated measurement dataset, whether
>> cases (which (id) could be more than one time included and not all 
>> same times included.)
>> have more than one times the same SMONTH!?
>>
>> #This pseudo code didn't work with a for loop, because the [i+1] 
>> isn't known.
>> How i could refer to [i+1] ?
>>
>> if(ID[i] == ID[i+1] & SMONTH[i] !=[i+1])  {res[i] <- 0 }
>> if(ID[i] == ID[i+1] & SMONTH[i] == [i+1]) {res[i] <- 1 }
>>
>> many thanks,
>> christian
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From abunn at whrc.org  Wed Nov 10 15:39:09 2004
From: abunn at whrc.org (Andy Bunn)
Date: Wed, 10 Nov 2004 09:39:09 -0500
Subject: [R] Does something like partition.rpart() exist?
In-Reply-To: <13007.81.208.60.192.1100082420.squirrel@webmail.mate.polimi.it>
Message-ID: <NEBBIPHDAMMOKDKPOFFIKEOGCLAA.abunn@whrc.org>

I might well be wrong, but I don't think there is. I went about rewriting
partition.tree for rpart once but stopped after I realized that it was much
easier to add lines and segments to plots by hand using the coordinates from
the rpart object (I then added a third predictor to my dataset making the
whole process pointless anyways.) Looking at the code for partition.tree and
its embedded function ptXlines gives you hints how to extract the relevant
bits of the tree to add to a plot.

I assume that you've seen this thread?

http://www.r-project.org/nocvs/mail/r-help/2002/0142.html

Sorry not to have been more help.

-Andy


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Simone Vantini
> Sent: Wednesday, November 10, 2004 5:27 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] Does something like partition.rpart() exist?
>
>
> I'd like to create a bidimensional presentation of a classification tree
> built using the rpart() function. I've seen that a partition.tree()
> function exists for the tree() function. Does a similar function  exist
> for the rpart() function?
> Thanks a lot Simone Vantini
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From p.dalgaard at biostat.ku.dk  Wed Nov 10 16:36:04 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 10 Nov 2004 16:36:04 +0100
Subject: [R] R under Pocket PC
In-Reply-To: <41920587.6000103@lancaster.ac.uk>
References: <Pine.LNX.4.44.0411091223400.16452-100000@gannet.stats>
	<x2mzxrf3ih.fsf@biostat.ku.dk> <m28y9bq8v3.fsf@ganymede.ammp.or.tz>
	<loom.20041109T181422-634@post.gmane.org>
	<m2y8haq1ts.fsf@ganymede.ammp.or.tz>
	<1abe3fa9041109234924b3cc90@mail.gmail.com>
	<006101c4c703$d394a520$f2e1f380@OPENZAURUS>
	<x2d5ymj793.fsf@biostat.ku.dk> <41920587.6000103@lancaster.ac.uk>
Message-ID: <x2zn1piumz.fsf@biostat.ku.dk>

Barry Rowlingson <B.Rowlingson at lancaster.ac.uk> writes:

>     I'm sure everyone on this list feels you work hard enough on R as
> it is, dont let it invade your living room as well!

Too late. Laptop + WiFi + ADSL did that a while back.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From CYRIL.CAILLAULT at FORTISINVESTMENTS.COM  Wed Nov 10 16:38:05 2004
From: CYRIL.CAILLAULT at FORTISINVESTMENTS.COM (CYRIL.CAILLAULT@FORTISINVESTMENTS.COM)
Date: Wed, 10 Nov 2004 16:38:05 +0100
Subject: [R] fSeries
Message-ID: <E66B5035E7D16C4B89028BAE9AC268A50403644B@I04MB144.fr.fimgroup>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041110/ed61e456/attachment.pl

From tlumley at u.washington.edu  Wed Nov 10 16:52:05 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 10 Nov 2004 07:52:05 -0800 (PST)
Subject: [R] worked in R, but not in S-Plus
In-Reply-To: <20041110005009.79940.qmail@web40824.mail.yahoo.com>
References: <20041110005009.79940.qmail@web40824.mail.yahoo.com>
Message-ID: <Pine.A41.4.61b.0411100748310.162604@homer04.u.washington.edu>

On Tue, 9 Nov 2004, array chip wrote:

> Hi,
>
> I wrote a function that worked well in R, but not in
> S-Plus, can anyone suggest a solution?
>

If you change the argument of the function from x to ... it will give
  Error in eval(expr, envir, enclos) : Object "x" not found
which is almost compatible with what it does in S-PLUS.

If you want advice on how to change the behaviour in S-PLUS, this is the 
wrong list.

>> f.coxph.zph<-function(x)
> {
> 	cox.fit <- coxph(Surv(time.cox, status.cox) ~ x,
> na.action = na.exclude, method = "breslow")
> 	fit.zph<-cox.zph(cox.fit,transform='log')
> 	fit.zph$table[,3]
> }


 	-thomas



From vito_ricci at yahoo.com  Wed Nov 10 17:18:17 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Wed, 10 Nov 2004 17:18:17 +0100 (CET)
Subject: [R] fSeries
Message-ID: <20041110161817.82969.qmail@web41215.mail.yahoo.com>

Hi,

see ? garch in tseries package.

library(tseries)
> garch(x)

 ***** ESTIMATION WITH ANALYTICAL GRADIENT ***** 


Warning: singular information

Call:
garch(x = x)

Coefficient(s):
       a0         a1         b1  
8.564e-07  5.000e-02  5.000e-02  

Best
Vito


You wrote:

Good morning everyone,

I use for the first time the package fSeries and i try
to run the example
given by Diethelm W??rtz. But when i run its example
which is the following 
#
# Example: 
#	Model a GARCH time series process 
#
# Description:
#	PART I: Estimate GARCH models of the following type
ARCH(2) 
#     and GARCH(1,1) with normal conditional
distribution functions.
#   PART II: Simulate GARCH models of the following
type, ARCH(2) 
#     and GARCH(1,1),
#	with normal conditional distribution functions.
#
# Author:
#	(C) 2002, Diethelm Wuertz, GPL
#


############################################################################
####
# PART I: Estimation:

	# Settings:
	set.seed(547)
    # Bollerslev's GARCH(1,1) with normal innovations:
	model = list(omega = 1e-6, alpha = 0.1, beta = 0.8,
mu = 0)
	x = garchSim(model, n = 1000)
	fit = garchFit(as.numeric(x), order = c(1, 1))
	print(fit)
	# Summary and Diagnostic Analysis:
	summary(fit)
	# Plot Results:
	par(mfrow = c(2, 2))
	plot(fit)
	###

Results of the estimations are false.

Call:
garchFit(x = as.numeric(x), order = c(1, 1))

Coefficient(s):
    omega         a1         b1  
8.564e-07  5.000e-02  5.000e-02  

To compare with : omega = 1e-6, alpha = 0.1, beta =
0.8.

Do you have some information about this?
Can I give some initials values to start the
estimations?
Can I use different innovation process like student-t
and GED

Thanks for your answers

Cyril 







=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese http://www.modugno.it/archivio/cat_palese.shtml



From deepayan at stat.wisc.edu  Wed Nov 10 17:47:28 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Wed, 10 Nov 2004 10:47:28 -0600
Subject: [R] lattice: ordering the entries in a dotplot of a vector
In-Reply-To: <20041110112813.GA4165@s1x.local>
References: <20041110112813.GA4165@s1x.local>
Message-ID: <200411101047.28801.deepayan@stat.wisc.edu>

On Wednesday 10 November 2004 05:28, Wolfram Fischer wrote:
> I tried:
>     n <- 9
>     x <- sample(n)
>     names(x) <- LETTERS[1:n]
>     dotplot( sort(x) )
>
> The lines of the dotplot are not ordered according to the values of x
> (but according to the names of x).
>
> So I did:
>
>     dfx <- data.frame( x=x, label=factor( names(x),
> names(x)[order(x)] ) ) dotplot( label ~ x, data=dfx )
>
> So I got what I wanted.
>
> My question: Is there an easier solution for doing that?

It should have been the default behaviour (according to S-PLUS). The 
next release should fix this.

Deepayan



From aragon at berkeley.edu  Wed Nov 10 18:40:21 2004
From: aragon at berkeley.edu (Tomas Aragon)
Date: Wed, 10 Nov 2004 09:40:21 -0800 (PST)
Subject: [R] Building MacOSX binary in Windows XP
Message-ID: <20041110174021.26501.qmail@web80104.mail.yahoo.com>

Under Windows XP, I am able to build R package archived as source
(.tar.gz) and Windows binary (.zip). Can I create MaxOSX archive file
(.tgz) in Windows XP?

Thanks,
Tomas

====
Tomas Aragon
http://www.epitools.net



From ripley at stats.ox.ac.uk  Wed Nov 10 19:09:36 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 10 Nov 2004 18:09:36 +0000 (GMT)
Subject: [R] Building MacOSX binary in Windows XP
In-Reply-To: <20041110174021.26501.qmail@web80104.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0411101807020.30809-100000@gannet.stats>

On Wed, 10 Nov 2004, Tomas Aragon wrote:

> Under Windows XP, I am able to build R package archived as source
> (.tar.gz) and Windows binary (.zip). Can I create MaxOSX archive file
> (.tgz) in Windows XP?

If you mean a .tgz containing a binary MacOSX version of the package, not 
without a cross-compiler (which I very much doubt you have).

You can certainly create .tgz files, but the it's the contents that 
matter.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From suzette at sdac.harvard.edu  Wed Nov 10 22:08:05 2004
From: suzette at sdac.harvard.edu (Suzette Blanchard)
Date: Wed, 10 Nov 2004 16:08:05 -0500 (EST)
Subject: [R] cubic spline/smoother with nlme
Message-ID: <Pine.GSO.4.40.0411101558200.17683-100000@sdac.harvard.edu>


Greetings,  I would like to use a cubic spline
or smoother to model the fixed effects within
nlme.  So far the only smoother I have been able
to get to run successfully in nlme is smooth().

I tried smooth.spline:
   fixed=list(lKa~1,lCL~smooth.spline(BSA, df=3))
   the error I got was the following.
   Error in model.frame(formula, rownames, variables, varnames, extras,
   extranames,  :  invalid variable type

Can anyone suggest a cubic spline that would work within
this context?

Thank you for any help you can send,
Suzette


=================================
Suzette Blanchard, Ph.D.
UCSD-PPRU



From p.dalgaard at biostat.ku.dk  Wed Nov 10 22:53:58 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 10 Nov 2004 22:53:58 +0100
Subject: [R] cubic spline/smoother with nlme
In-Reply-To: <Pine.GSO.4.40.0411101558200.17683-100000@sdac.harvard.edu>
References: <Pine.GSO.4.40.0411101558200.17683-100000@sdac.harvard.edu>
Message-ID: <x2fz3h9xqh.fsf@biostat.ku.dk>

Suzette Blanchard <suzette at sdac.harvard.edu> writes:

> Greetings,  I would like to use a cubic spline
> or smoother to model the fixed effects within
> nlme.  So far the only smoother I have been able
> to get to run successfully in nlme is smooth().
> 
> I tried smooth.spline:
>    fixed=list(lKa~1,lCL~smooth.spline(BSA, df=3))
>    the error I got was the following.
>    Error in model.frame(formula, rownames, variables, varnames, extras,
>    extranames,  :  invalid variable type
> 
> Can anyone suggest a cubic spline that would work within
> this context?

The fixed-knots ones (ns(), bs()) should work (and did so in at least
one case a couple of years ago...). These are linear, so lme() is used
rather than nlme(), unless of course you have other parts that need to
be modeled nonlinearly.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From v_bill_pikounis at merck.com  Wed Nov 10 23:29:28 2004
From: v_bill_pikounis at merck.com (Pikounis, Bill)
Date: Wed, 10 Nov 2004 17:29:28 -0500
Subject: [R] cubic spline/smoother with nlme
Message-ID: <CFBD404F5E0C9547B4E10B7BDC3DFA2F0B16072D@usrymx18.merck.com>

Suzette,
In addition to Professor Daalgard's suggestions of ns() and bs(), you could
also try out rcs() from Frank Harrell Design package (you may need his Hmisc
package as well). This function helps to fit natural (restricted cubic
splines), and have been very useful for me in practice to use in tandem with
lme() in modeling longitudinal data.

Hope that helps,
Bill

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Peter Dalgaard
> Sent: Wednesday, November 10, 2004 4:54 PM
> To: Suzette Blanchard
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] cubic spline/smoother with nlme
> 
> 
> Suzette Blanchard <suzette at sdac.harvard.edu> writes:
> 
> > Greetings,  I would like to use a cubic spline
> > or smoother to model the fixed effects within
> > nlme.  So far the only smoother I have been able
> > to get to run successfully in nlme is smooth().
> > 
> > I tried smooth.spline:
> >    fixed=list(lKa~1,lCL~smooth.spline(BSA, df=3))
> >    the error I got was the following.
> >    Error in model.frame(formula, rownames, variables, 
> varnames, extras,
> >    extranames,  :  invalid variable type
> > 
> > Can anyone suggest a cubic spline that would work within
> > this context?
> 
> The fixed-knots ones (ns(), bs()) should work (and did so in at least
> one case a couple of years ago...). These are linear, so lme() is used
> rather than nlme(), unless of course you have other parts that need to
> be modeled nonlinearly.
> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: 
> (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: 
> (+45) 35327907
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From tmulholland at bigpond.com  Thu Nov 11 00:55:17 2004
From: tmulholland at bigpond.com (Tom Mulholland)
Date: Thu, 11 Nov 2004 07:55:17 +0800
Subject: [R] R under Pocket PC
In-Reply-To: <41920587.6000103@lancaster.ac.uk>
References: <Pine.LNX.4.44.0411091223400.16452-100000@gannet.stats>
	<x2mzxrf3ih.fsf@biostat.ku.dk> <m28y9bq8v3.fsf@ganymede.ammp.or.tz>
	<loom.20041109T181422-634@post.gmane.org>
	<m2y8haq1ts.fsf@ganymede.ammp.or.tz>
	<1abe3fa9041109234924b3cc90@mail.gmail.com>
	<006101c4c703$d394a520$f2e1f380@OPENZAURUS>
	<x2d5ymj793.fsf@biostat.ku.dk> <41920587.6000103@lancaster.ac.uk>
Message-ID: <4192AA65.7090000@bigpond.com>

What is really scary, is the thought that in some housholds this would 
be an improvement. Instead of the family being closeted away in their 
various rooms playing with their computer, they could all be sitting 
together, while at the same time ignoring each other, but occassionally 
engaging in royal (as in "The Royals") interaction and the odd cuppa.

For those not familiar with the TV series "The Royals" its a chunk of 
domesticity that has to be seen to be appreciated ( or disgusted 
depending upon your view of life.)

Tom Mulholland


Barry Rowlingson wrote:
> 
>> Could be kind of
>> fun to run R on it with display to the TV (but could you control R
>> with the TV remote control?)
> 
> 
>  Peter,
> 
>    I'm sure everyone on this list feels you work hard enough on R as it 
> is, dont let it invade your living room as well!
> 
> Baz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From dunn at usq.edu.au  Thu Nov 11 01:25:27 2004
From: dunn at usq.edu.au (Peter Dunn)
Date: Thu, 11 Nov 2004 10:25:27 +1000
Subject: [R] substitute/paste question for using Greek in plot titles
Message-ID: <4192B177.5020109@usq.edu.au>

Hi all

I am having troubles making sense of why code (1)
below fails but code (2) below works.

Code (1):

 > phi.1 <- 1
 > plot(0 ~ 0,
+ main=substitute(paste("A vaue for ",phi," = ",phival), 
list(phival=phi.1)) )

Error in paste("The two deviances for ", phi, " = ", 2) :
         Object "phi" not found

But this works:

Code (2):
 > plot(0,0,
+ main=substitute(paste("A value for ",phi," = ",phival), 
list(phival=phi.1)) )
 >

It appears that if the plot command takes the formula style entry,
the substitue/paste fails.

Is this documented as a feature (I couldn't find it if that is the
case), or is it a bug?  If it is a feature, it is a subtle difference
between (1) and (2) that has potential to be quite frustrating!

Perhaps I should just upgrade to version 2.0.0, though I can't see
anything in the Release Notes that might cause this.

Thanks.

P.

 > version
          _
platform i686-pc-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status
major    1
minor    9.1
year     2004
month    06
day      21
language R




-- 
Dr Peter Dunn          (USQ CRICOS No. 00244B)
   Web:    http://www.sci.usq.edu.au/staff/dunn
   Email:  dunn @ usq.edu.au
Opinions expressed are mine, not those of USQ.  Obviously...



From jeaneid at chass.utoronto.ca  Thu Nov 11 01:46:21 2004
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Wed, 10 Nov 2004 19:46:21 -0500
Subject: [R] polr probit versus stata oprobit  
Message-ID: <Pine.SGI.4.40.0411101930330.52337307-100000@origin.chass.utoronto.ca>

Dear All,
I have been struggling to understand why for the housing data in MASS
library R and stata give coef. estimates that are really different. I also
tried to come up with many many examples myself (see below, of course I
did not have the set.seed command included) and all of my
`random' examples seem to give verry similar output. For the housing data,
I have changed the data into numeric vectors instead of factors/ordered
factors. I did so to try and get the same results as in STATA and to have
the housing example as close as possible to the one I constructed.

I run a debian sid, kernel 2.4, R 2.0.0, and STATA version 8.2, MASS
version  7.2-8.


here's the example ( I assume that you have STATA installed and can run in
batch mode, if not the output is also given below)

library(MASS)
library(foreign)
set.seed(100)
X <- rnorm(1000)
X1 <- rnorm(1000)
X2 <- rnorm(1000)
X <- X +X1+X2
XX <- X<=quantile(X, .25)
XX[X>quantile(X, .25) & X<=quantile(X, .50)] <- 2
XX[X>quantile(X, .5) & X<=quantile(X, .75)] <- 3
XX[X>quantile(X, .75)] <- 4
temp <- data.frame(XX=XX, X1=X1, X2=X2, X=X)
summary(polr(factor(XX)~X1 +X2, data=temp, method="probit"))
write.dta(temp, "temp.dta")
####################################
#Stata stuff
####################################
cat("use temp.dta\n oprobit XX X1 X2\n", file="temp.ado")
system("stata -b do temp.ado&")
system("cat temp.log")


#
##### here's R's output
#############################
Re-fitting to get Hessian

Call:
polr(formula = factor(XX) ~ X1 + X2, data = temp, method = "probit")

Coefficients:
       Value Std. Error  t value
X1 0.9891735 0.04749225 20.82811
X2 0.9400804 0.04527653 20.76309

Intercepts:
    Value    Std. Error t value
1|2  -1.1411   0.0572   -19.9613
2|3  -0.0372   0.0486    -0.7656
3|4   1.1101   0.0579    19.1865

Residual Deviance: 1969.734
AIC: 1979.734


##############################################
#and here  Stata's output
##############################################

Ordered probit estimates                          Number of obs   =
1000
                                                  LR chi2(2)      =
802.86
                                                  Prob > chi2     =
0.0000
Log likelihood = -984.86675                       Pseudo R2       =
0.2896

------------------------------------------------------------------------------
          XX |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
          X1 |   .9891651   .0474922    20.83   0.000     .8960822    1.082248
          X2 |     .94007   .0452765    20.76   0.000     .8513298     1.02881
-------------+----------------------------------------------------------------
       _cut1 |  -1.141119   .0571667          (Ancillary parameters)
       _cut2 |  -.0371779   .0485592
       _cut3 |   1.110117   .0578593



From fbizuet at banxico.org.mx  Thu Nov 11 01:40:43 2004
From: fbizuet at banxico.org.mx (Bizuet Cabrera Fernando)
Date: Wed, 10 Nov 2004 18:40:43 -0600
Subject: [R] R with Sun Studio Fortran 95 compiler
Message-ID: <F74A1EABDCCFFB4893FD93C96408F58A021A53B8@BMCORREO.banxico.org.mx>

Hi,

I am trying to compile R 1.9.1 and 2.0.0 on Solaris 2.8 with Sun Studio
Fortran 95 compiler but I have obtained some errors which I enclose. I
have compiled R with g77 GNU compiler and it was OK.

Does anyone know if R 2.0.0 has been successfully built on Solaris 2.8
with Sun Studio Fortran 95 compiler? or at least, How could I achieve
this? The R installation manual does not have any reference.

I want Sun Fortran in order to use the Sun performance library
libsunperf since I noticed that R on Windows XP is twice faster than R
on Sun Solaris 2.8 without library libsunperf.

Thanks in advance.

$ ./configure --enable-R-shlib
checking for a BSD-compatible install... tools/install-sh -c
checking whether build environment is sane... yes
checking whether make sets $(MAKE)... yes

... Skip

checking build system type... sparc-sun-solaris2.8
checking host system type... sparc-sun-solaris2.8
loading site script './config.site'

...skip

checking for gcc... gcc
checking for C compiler default output file name... a.out
checking whether the C compiler works... yes
checking whether we are cross compiling... no
checking for suffix of executables... 
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ANSI C... none needed
checking how to run the C preprocessor... gcc -E
checking whether gcc needs -traditional... no
checking how to run the C preprocessor... gcc -E
defining F77 to be f95
checking whether we are using the GNU Fortran 77 compiler... no
checking whether f95 accepts -g... yes
checking for g++... g++
checking whether we are using the GNU C++ compiler... yes
checking whether g++ accepts -g... yes

... skip

checking for f95 option to produce PIC... -fPIC
checking if f95 PIC flag -fPIC works... no
checking if f95 supports -c -o file.o... yes
checking whether the f95 linker (/usr/ccs/bin/ld) supports shared
libraries... yes
checking dynamic linker characteristics... f95: Warning: Option
-print-search-dirs passed to ld, if ld is invoked, ignored otherwise
Usage: f95 [ options ] files.  Use 'f95 -flags' for details
solaris2.8 ld.so
checking how to hardcode library paths into programs... immediate

...skip

checking how to get verbose linking output from f95... -v
checking for Fortran libraries of f95...  -L/usr/local/lib
-R/opt/SUNWspro/lib/v8plus:/opt/SUNWspro/lib -L/opt/SUNWspro/lib/v8plus
-L/opt/SUNWspro/prod/lib/v8plus -L/opt/SUNWspro/lib
-L/opt/SUNWspro/prod/lib -L/usr/ccs/lib -L/lib -L/usr/lib -lompstubs
-lfui -lfai -lfai2 -lfsumai -lfprodai -lfminlai -lfmaxlai -lfminvai
-lfmaxvai -lfsu -lsunmath -lm
checking for dummy main to link with Fortran libraries... unknown
configure: error: linking to Fortran libraries from C fails
See `config.log' for more details.



From sundar.dorai-raj at pdf.com  Thu Nov 11 01:43:41 2004
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Wed, 10 Nov 2004 16:43:41 -0800
Subject: [R] substitute/paste question for using Greek in plot titles
In-Reply-To: <4192B177.5020109@usq.edu.au>
References: <4192B177.5020109@usq.edu.au>
Message-ID: <4192B5BD.4070103@pdf.com>



Peter Dunn wrote:

> Hi all
> 
> I am having troubles making sense of why code (1)
> below fails but code (2) below works.
> 
> Code (1):
> 
>  > phi.1 <- 1
>  > plot(0 ~ 0,
> + main=substitute(paste("A vaue for ",phi," = ",phival), 
> list(phival=phi.1)) )
> 
> Error in paste("The two deviances for ", phi, " = ", 2) :
>         Object "phi" not found
> 
> But this works:
> 
> Code (2):
>  > plot(0,0,
> + main=substitute(paste("A value for ",phi," = ",phival), 
> list(phival=phi.1)) )
>  >
> 
> It appears that if the plot command takes the formula style entry,
> the substitue/paste fails.
> 
> Is this documented as a feature (I couldn't find it if that is the
> case), or is it a bug?  If it is a feature, it is a subtle difference
> between (1) and (2) that has potential to be quite frustrating!
> 
> Perhaps I should just upgrade to version 2.0.0, though I can't see
> anything in the Release Notes that might cause this.
> 
> Thanks.
> 
> P.
> 
>  > version
>          _
> platform i686-pc-linux-gnu
> arch     i686
> os       linux-gnu
> system   i686, linux-gnu
> status
> major    1
> minor    9.1
> year     2004
> month    06
> day      21
> language R
> 
> 

Peter,

Because in the first couple of lines of plot.formula we see this:

dots <- m$...
dots <- lapply(dots, eval, data, parent.frame())

which for your case is equivalent to:

expr <- substitute(paste("A vaue for ",phi," = ",phival), 
list(phival=phi.1))
eval(expr)

which returns an error saying "phi" cannot be found which is the correct 
behaviour of eval. I'll let others comment on whether or not this is a 
bug in plot.formula but you can always get around it by calling title:

plot(0 ~ 0)
title(main = expr)

which is exactly what your second example is doing in plot.default.

HTH,

--sundar



From tlumley at u.washington.edu  Thu Nov 11 02:15:26 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 10 Nov 2004 17:15:26 -0800 (PST)
Subject: [R] polr probit versus stata oprobit  
In-Reply-To: <Pine.SGI.4.40.0411101930330.52337307-100000@origin.chass.utoronto.ca>
References: <Pine.SGI.4.40.0411101930330.52337307-100000@origin.chass.utoronto.ca>
Message-ID: <Pine.A41.4.61b.0411101707400.230306@homer11.u.washington.edu>

On Wed, 10 Nov 2004, Jean Eid wrote:

> Dear All,
> I have been struggling to understand why for the housing data in MASS
> library R and stata give coef. estimates that are really different. I also
> tried to come up with many many examples myself (see below, of course I
> did not have the set.seed command included) and all of my
> `random' examples seem to give verry similar output. For the housing data,
> I have changed the data into numeric vectors instead of factors/ordered
> factors. I did so to try and get the same results as in STATA and to have
> the housing example as close as possible to the one I constructed.
>
> I run a debian sid, kernel 2.4, R 2.0.0, and STATA version 8.2, MASS
> version  7.2-8.
>
>
> here's the example ( I assume that you have STATA installed and can run in
> batch mode, if not the output is also given below)
>

That example shows the same results with Stata and polr() from MASS.

For the housing data, I also get the same coefficients in Stata as with 
polr():

In R:
library(MASS)
library(foreign)
write.dta(housing, file="housing.dta")
house.probit<-polr(Sat ~ Infl + Type + Cont, data = housing, weights = 
Freq, method = "probit")
summary(house.probit)
-------------------------
Re-fitting to get Hessian

Call:
polr(formula = Sat ~ Infl + Type + Cont, data = housing, weights = Freq,
     method = "probit")

Coefficients:
                    Value Std. Error   t value
InflMedium     0.3464233 0.06413706  5.401297
InflHigh       0.7829149 0.07642620 10.244063
TypeApartment -0.3475372 0.07229093 -4.807480
TypeAtrium    -0.2178874 0.09476607 -2.299213
TypeTerrace   -0.6641737 0.09180004 -7.235005
ContHigh       0.2223862 0.05812267  3.826153

Intercepts:
             Value   Std. Error t value
Low|Medium  -0.2998  0.0762    -3.9371
Medium|High  0.4267  0.0764     5.5850

Residual Deviance: 3479.689
AIC: 3495.689
------------------------


In Stata
-----------------
. use housing.dta
. xi: oprobit Sat i.Infl i.Type i.Cont [fw=Freq]
i.Infl            _IInfl_1-3          (naturally coded; _IInfl_1 omitted)
i.Type            _IType_1-4          (naturally coded; _IType_1 omitted)
i.Cont            _ICont_1-2          (naturally coded; _ICont_1 omitted)

Iteration 0:   log likelihood = -1824.4388
Iteration 1:   log likelihood = -1739.9254
Iteration 2:   log likelihood = -1739.8444

Ordered probit estimates                          Number of obs   =    1681
                                                   LR chi2(6)      =   169.19
                                                   Prob > chi2     =   0.0000
Log likelihood = -1739.8444                       Pseudo R2       =   0.0464

------------------------------------------------------------------------------
          Sat |      Coef.   Std. Err.      z    P>|z|     [95% Conf. 
Interval]
-------------+----------------------------------------------------------------
     _IInfl_2 |   .3464228    .064137     5.40   0.000     .2207165     .472129
     _IInfl_3 |   .7829146    .076426    10.24   0.000     .6331224    .9327069
     _IType_2 |  -.3475367   .0722908    -4.81   0.000    -.4892241   -.2058493
     _IType_3 |  -.2178875    .094766    -2.30   0.021    -.4036254   -.0321497
     _IType_4 |  -.6641735   .0917999    -7.24   0.000     -.844098    -.484249
     _ICont_2 |   .2223858   .0581226     3.83   0.000     .1084676     .336304
-------------+----------------------------------------------------------------
        _cut1 |  -.2998279   .0761537          (Ancillary parameters)
        _cut2 |   .4267208   .0764043
------------------------------------------------------------------------------



 	-thomas



From jeaneid at chass.utoronto.ca  Thu Nov 11 03:09:21 2004
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Wed, 10 Nov 2004 21:09:21 -0500
Subject: [R] polr probit versus stata oprobit  
In-Reply-To: <Pine.A41.4.61b.0411101707400.230306@homer11.u.washington.edu>
Message-ID: <Pine.SGI.4.40.0411102108010.52139612-100000@origin.chass.utoronto.ca>

Dear Thomas,

Where you also able to replicate the second example?  (the exaample
that I turned the housing data into numerical variables) That is the one
that my estimates differ.

Jean,

On Wed, 10 Nov 2004, Thomas Lumley wrote:

> On Wed, 10 Nov 2004, Jean Eid wrote:
>
> > Dear All,
> > I have been struggling to understand why for the housing data in MASS
> > library R and stata give coef. estimates that are really different. I also
> > tried to come up with many many examples myself (see below, of course I
> > did not have the set.seed command included) and all of my
> > `random' examples seem to give verry similar output. For the housing data,
> > I have changed the data into numeric vectors instead of factors/ordered
> > factors. I did so to try and get the same results as in STATA and to have
> > the housing example as close as possible to the one I constructed.
> >
> > I run a debian sid, kernel 2.4, R 2.0.0, and STATA version 8.2, MASS
> > version  7.2-8.
> >
> >
> > here's the example ( I assume that you have STATA installed and can run in
> > batch mode, if not the output is also given below)
> >
>
> That example shows the same results with Stata and polr() from MASS.
>
> For the housing data, I also get the same coefficients in Stata as with
> polr():
>
> In R:
> library(MASS)
> library(foreign)
> write.dta(housing, file="housing.dta")
> house.probit<-polr(Sat ~ Infl + Type + Cont, data = housing, weights =
> Freq, method = "probit")
> summary(house.probit)
> -------------------------
> Re-fitting to get Hessian
>
> Call:
> polr(formula = Sat ~ Infl + Type + Cont, data = housing, weights = Freq,
>      method = "probit")
>
> Coefficients:
>                     Value Std. Error   t value
> InflMedium     0.3464233 0.06413706  5.401297
> InflHigh       0.7829149 0.07642620 10.244063
> TypeApartment -0.3475372 0.07229093 -4.807480
> TypeAtrium    -0.2178874 0.09476607 -2.299213
> TypeTerrace   -0.6641737 0.09180004 -7.235005
> ContHigh       0.2223862 0.05812267  3.826153
>
> Intercepts:
>              Value   Std. Error t value
> Low|Medium  -0.2998  0.0762    -3.9371
> Medium|High  0.4267  0.0764     5.5850
>
> Residual Deviance: 3479.689
> AIC: 3495.689
> ------------------------
>
>
> In Stata
> -----------------
> . use housing.dta
> . xi: oprobit Sat i.Infl i.Type i.Cont [fw=Freq]
> i.Infl            _IInfl_1-3          (naturally coded; _IInfl_1 omitted)
> i.Type            _IType_1-4          (naturally coded; _IType_1 omitted)
> i.Cont            _ICont_1-2          (naturally coded; _ICont_1 omitted)
>
> Iteration 0:   log likelihood = -1824.4388
> Iteration 1:   log likelihood = -1739.9254
> Iteration 2:   log likelihood = -1739.8444
>
> Ordered probit estimates                          Number of obs   =    1681
>                                                    LR chi2(6)      =   169.19
>                                                    Prob > chi2     =   0.0000
> Log likelihood = -1739.8444                       Pseudo R2       =   0.0464
>
> ------------------------------------------------------------------------------
>           Sat |      Coef.   Std. Err.      z    P>|z|     [95% Conf.
> Interval]
> -------------+----------------------------------------------------------------
>      _IInfl_2 |   .3464228    .064137     5.40   0.000     .2207165     .472129
>      _IInfl_3 |   .7829146    .076426    10.24   0.000     .6331224    .9327069
>      _IType_2 |  -.3475367   .0722908    -4.81   0.000    -.4892241   -.2058493
>      _IType_3 |  -.2178875    .094766    -2.30   0.021    -.4036254   -.0321497
>      _IType_4 |  -.6641735   .0917999    -7.24   0.000     -.844098    -.484249
>      _ICont_2 |   .2223858   .0581226     3.83   0.000     .1084676     .336304
> -------------+----------------------------------------------------------------
>         _cut1 |  -.2998279   .0761537          (Ancillary parameters)
>         _cut2 |   .4267208   .0764043
> ------------------------------------------------------------------------------
>
>
>
>  	-thomas
>



From rpeng at jhsph.edu  Thu Nov 11 05:08:23 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Wed, 10 Nov 2004 23:08:23 -0500
Subject: [R] substitute/paste question for using Greek in plot titles
In-Reply-To: <4192B5BD.4070103@pdf.com>
References: <4192B177.5020109@usq.edu.au> <4192B5BD.4070103@pdf.com>
Message-ID: <4192E5B7.3050203@jhsph.edu>

This is not a bug.  The object passed to `main' should be either character or an 
expression (documented in ?title).  substitute() returns neither a character 
object nor an expression -- it return a call object.  This is documented in 
?substitute:

      Substituting and quoting often causes confusion when the argument
      is 'expression(...)'. The result is a call to the 'expression'
      constructor function and needs to be evaluated with 'eval' to give
      the actual expression object.

What you want, I think, is

label <- substitute(expression(paste("A vaue for ",phi," = ",phival)), 
list(phival=phi.1))
plot(0 ~ 0, main = eval(label))

-roger

Sundar Dorai-Raj wrote:
> 
> 
> Peter Dunn wrote:
> 
>> Hi all
>>
>> I am having troubles making sense of why code (1)
>> below fails but code (2) below works.
>>
>> Code (1):
>>
>>  > phi.1 <- 1
>>  > plot(0 ~ 0,
>> + main=substitute(paste("A vaue for ",phi," = ",phival), 
>> list(phival=phi.1)) )
>>
>> Error in paste("The two deviances for ", phi, " = ", 2) :
>>         Object "phi" not found
>>
>> But this works:
>>
>> Code (2):
>>  > plot(0,0,
>> + main=substitute(paste("A value for ",phi," = ",phival), 
>> list(phival=phi.1)) )
>>  >
>>
>> It appears that if the plot command takes the formula style entry,
>> the substitue/paste fails.
>>
>> Is this documented as a feature (I couldn't find it if that is the
>> case), or is it a bug?  If it is a feature, it is a subtle difference
>> between (1) and (2) that has potential to be quite frustrating!
>>
>> Perhaps I should just upgrade to version 2.0.0, though I can't see
>> anything in the Release Notes that might cause this.
>>
>> Thanks.
>>
>> P.
>>
>>  > version
>>          _
>> platform i686-pc-linux-gnu
>> arch     i686
>> os       linux-gnu
>> system   i686, linux-gnu
>> status
>> major    1
>> minor    9.1
>> year     2004
>> month    06
>> day      21
>> language R
>>
>>
> 
> Peter,
> 
> Because in the first couple of lines of plot.formula we see this:
> 
> dots <- m$...
> dots <- lapply(dots, eval, data, parent.frame())
> 
> which for your case is equivalent to:
> 
> expr <- substitute(paste("A vaue for ",phi," = ",phival), 
> list(phival=phi.1))
> eval(expr)
> 
> which returns an error saying "phi" cannot be found which is the correct 
> behaviour of eval. I'll let others comment on whether or not this is a 
> bug in plot.formula but you can always get around it by calling title:
> 
> plot(0 ~ 0)
> title(main = expr)
> 
> which is exactly what your second example is doing in plot.default.
> 
> HTH,
> 
> --sundar
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From liang at ics.uci.edu  Thu Nov 11 05:20:01 2004
From: liang at ics.uci.edu (Gang Liang)
Date: Wed, 10 Nov 2004 20:20:01 -0800
Subject: [R] "<<-" assignment no long work in class methods
Message-ID: <20041111042001.GL11620@opensrc>

Hi-

I used to use "<<-" to do assignment inside a class method, and
just found that now it is broken in R 2.0. For example, the
following code

-----------------------------------------------------------------------
setClass( "myclass", representation(x="numeric") )
setGeneric("incrXByOne", function(obj) standardGeneric("incrXByOne"))
setMethod( "incrXByOne", "myclass", function(obj) obj at x <<- obj at x + 1 )

incrXByOne( new("myclass") )
-----------------------------------------------------------------------

will give an error message like:

   Error in incrXByOne(new("myclass")) : Object "obj" not found
   ## R failed to trace the object back to the GlobalEnv...

It used to work under R1.7 - 1.9. I don't know whether this is a
bug or a new feature...

Anyone can recommend a workaround?

Thanks, Gang

-----------------------------
debian unstable, kernel 2.6.8

> version

platform i386-pc-linux-gnu
arch     i386
os       linux-gnu
system   i386, linux-gnu
status
major    2
minor    0.0
year     2004
month    10
day      04
language R



From wolski at molgen.mpg.de  Thu Nov 11 08:30:25 2004
From: wolski at molgen.mpg.de (Witold Eryk Wolski)
Date: Thu, 11 Nov 2004 08:30:25 +0100
Subject: [R] "<<-" assignment no long work in class methods
In-Reply-To: <20041111042001.GL11620@opensrc>
References: <20041111042001.GL11620@opensrc>
Message-ID: <41931511.7080809@molgen.mpg.de>

Gang Liang wrote:

>Hi-
>
>I used to use "<<-" to do assignment inside a class method, and
>just found that now it is broken in R 2.0. For example, the
>following code
>
>-----------------------------------------------------------------------
>setClass( "myclass", representation(x="numeric") )
>setGeneric("incrXByOne", function(obj) standardGeneric("incrXByOne"))
>setMethod( "incrXByOne", "myclass", function(obj) obj at x <<- obj at x + 1 )
>
>  
>
Hi,

What (my guess) you want to define is:

setMethod( "incrXByOne", "myclass", function(obj) {obj at x <- obj at x + 1;obj})

You do not need "<<-" to assign to a function argument? Its in my view 
even erroneous.

This S code gives an error too.

test <- function(x)
{
x$bla<<-x$bla + 1
}
test(1)
Error in test(1) : Object "x" not found

/E

>incrXByOne( new("myclass") )
>-----------------------------------------------------------------------
>
>will give an error message like:
>
>   Error in incrXByOne(new("myclass")) : Object "obj" not found
>   ## R failed to trace the object back to the GlobalEnv...
>
>It used to work under R1.7 - 1.9. I don't know whether this is a
>bug or a new feature...
>
>Anyone can recommend a workaround?
>
>Thanks, Gang
>
>-----------------------------
>debian unstable, kernel 2.6.8
>
>  
>
>>version
>>    
>>
>
>platform i386-pc-linux-gnu
>arch     i386
>os       linux-gnu
>system   i386, linux-gnu
>status
>major    2
>minor    0.0
>year     2004
>month    10
>day      04
>language R
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>


-- 
Dipl. bio-chem. Witold Eryk Wolski
MPI-Moleculare Genetic
Ihnestrasse 63-73 14195 Berlin
tel: 0049-30-83875219                 __("<    _
http://www.molgen.mpg.de/~wolski      \__/    'v'
http://r4proteomics.sourceforge.net    ||    /   \
mail: witek96 at users.sourceforge.net    ^^     m m
      wolski at molgen.mpg.de



From ripley at stats.ox.ac.uk  Thu Nov 11 08:32:30 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 11 Nov 2004 07:32:30 +0000 (GMT)
Subject: [R] R with Sun Studio Fortran 95 compiler
In-Reply-To: <F74A1EABDCCFFB4893FD93C96408F58A021A53B8@BMCORREO.banxico.org.mx>
Message-ID: <Pine.LNX.4.44.0411110723220.9124-100000@gannet.stats>

On Wed, 10 Nov 2004, Bizuet Cabrera Fernando wrote:

> I am trying to compile R 1.9.1 and 2.0.0 on Solaris 2.8 with Sun Studio
> Fortran 95 compiler but I have obtained some errors which I enclose. I
> have compiled R with g77 GNU compiler and it was OK.

Please do read the final line of your excerpt and do as it says ....

> Does anyone know if R 2.0.0 has been successfully built on Solaris 2.8
> with Sun Studio Fortran 95 compiler? or at least, How could I achieve
> this? The R installation manual does not have any reference.

What is `Sun Studio'?  The R-admin manual does have references to what I
think is the same compiler, `Sun ONE Studio 7 Compiler Suite' (aka Forte
7) (possibly not the same version).  Forte 9 has also been used 
successfully, if -xopenmp=stubs was added to LDFLAGS (and that is in the 
R-admin manual for the imminent R 2.0.1).

> I want Sun Fortran in order to use the Sun performance library
> libsunperf since I noticed that R on Windows XP is twice faster than R
> on Sun Solaris 2.8 without library libsunperf.

Yes, well, not at the same clock speed.  Please do read the experience in 
the R-admin manual that you deny exists.

Either you are talking about a different product with a very similar name 
or you have missed a lot of useful information.

> Thanks in advance.
> 
> $ ./configure --enable-R-shlib
> checking for a BSD-compatible install... tools/install-sh -c
> checking whether build environment is sane... yes
> checking whether make sets $(MAKE)... yes
> 
> ... Skip
> 
> checking build system type... sparc-sun-solaris2.8
> checking host system type... sparc-sun-solaris2.8
> loading site script './config.site'
> 
> ...skip
> 
> checking for gcc... gcc
> checking for C compiler default output file name... a.out
> checking whether the C compiler works... yes
> checking whether we are cross compiling... no
> checking for suffix of executables... 
> checking for suffix of object files... o
> checking whether we are using the GNU C compiler... yes
> checking whether gcc accepts -g... yes
> checking for gcc option to accept ANSI C... none needed
> checking how to run the C preprocessor... gcc -E
> checking whether gcc needs -traditional... no
> checking how to run the C preprocessor... gcc -E
> defining F77 to be f95
> checking whether we are using the GNU Fortran 77 compiler... no
> checking whether f95 accepts -g... yes
> checking for g++... g++
> checking whether we are using the GNU C++ compiler... yes
> checking whether g++ accepts -g... yes
> 
> ... skip
> 
> checking for f95 option to produce PIC... -fPIC
> checking if f95 PIC flag -fPIC works... no
> checking if f95 supports -c -o file.o... yes
> checking whether the f95 linker (/usr/ccs/bin/ld) supports shared
> libraries... yes
> checking dynamic linker characteristics... f95: Warning: Option
> -print-search-dirs passed to ld, if ld is invoked, ignored otherwise
> Usage: f95 [ options ] files.  Use 'f95 -flags' for details
> solaris2.8 ld.so
> checking how to hardcode library paths into programs... immediate
> 
> ...skip
> 
> checking how to get verbose linking output from f95... -v
> checking for Fortran libraries of f95...  -L/usr/local/lib
> -R/opt/SUNWspro/lib/v8plus:/opt/SUNWspro/lib -L/opt/SUNWspro/lib/v8plus
> -L/opt/SUNWspro/prod/lib/v8plus -L/opt/SUNWspro/lib
> -L/opt/SUNWspro/prod/lib -L/usr/ccs/lib -L/lib -L/usr/lib -lompstubs
> -lfui -lfai -lfai2 -lfsumai -lfprodai -lfminlai -lfmaxlai -lfminvai
> -lfmaxvai -lfsu -lsunmath -lm
> checking for dummy main to link with Fortran libraries... unknown
> configure: error: linking to Fortran libraries from C fails
> See `config.log' for more details.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From wuertz at itp.phys.ethz.ch  Thu Nov 11 09:27:51 2004
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Thu, 11 Nov 2004 08:27:51 +0000
Subject: [R] fSeries
In-Reply-To: <E66B5035E7D16C4B89028BAE9AC268A50403644B@I04MB144.fr.fimgroup>
References: <E66B5035E7D16C4B89028BAE9AC268A50403644B@I04MB144.fr.fimgroup>
Message-ID: <41932287.8020000@itp.phys.ethz.ch>


1. Currently I'm writing a complete new GARCH package, and as long as it 
is not
yet ready I have added the garch function from Adrian Trapletti's 
tseries package
to the fSeries package.

2. But the error in the output you  observed  comes not from the "garch" 
function,
it comes from my "garchSim" function. Please modify the function 
garchSim() in the
following way ----


garchSim =
function(model = list(omega = 1.0e-06, alpha = 0.1, beta = 0.8, mu = 0),
n = 100, innov = NULL, n.start = 100, start.innov = NULL, rand.gen = 
rnorm, ...)
{    
    # Doesn't work, replace the three following lines ... 
    # if (!exists("model$alpha")) model$alpha = 0
    # if (!exists("model$beta")) model$beta = 0
    # if (!exists("model$mu")) model$mu = 0
   
    # with ...
    if (is.null(model$alpha)) model$alpha = 0
    if (is.null(model$beta)) model$beta = 0
    if (is.null(model$mu)) model$mu = 0

    max.order = max(length(model$alpha), length(model$beta))
    if (n.start < max.order)
        stop("n.start must be greater or equal max(alpha, beta)")
    if (is.null(start.innov))
        start.innov = rand.gen(n.start, ...)
    if (is.null(innov))
        innov = rand.gen(n, ...)
    h = x = z = c(start.innov, innov)
    for (i in 1:max.order) {
        h[i] = model$omega/(1 - sum(model$alpha) - sum(model$beta))
        x[i] = sqrt(h[i]) * z[i] + model$mu
    }
    n.alpha = length(model$alpha)
    n.beta = length(model$beta)
    for (i in (max.order + 1):(n.start + n)) {
        h[i] = model$omega + sum(model$alpha * x[i - (1:n.alpha)]^2) +
            sum(model$beta * h[i - (1:n.beta)])
        x[i] = sqrt(h[i]) * z[i] + model$mu
    }
    as.ts(x[-(1:n.start)])
}


and try the folloowing examples:


require(fSeries)

garchFit(garchSim(n = 1000))

garchFit(garchSim(model = list(omega = 1.0e-06, alpha = 0.1, beta = 0.8,
    mu = 0), n =1000))

garchFit(garchSim(model = list(omega = 1.0e-06, alpha = 0.6), n = 1000))
garchFit(garchSim(model = list(omega = 1.0e-06, alpha = 0.6), n = 1000),
    order=c(0, 1))
   

The code will be updated in the next fSeries package.
I apologize for any inconvenience caused by this bug.

Diethelm Wuertz



CYRIL.CAILLAULT at FORTISINVESTMENTS.COM wrote:

>Good morning everyone,
>
>I use for the first time the package fSeries and i try to run the example
>given by Diethelm W??rtz. But when i run its example which is the following 
>#
># Example: 
>#	Model a GARCH time series process 
>#
># Description:
>#	PART I: Estimate GARCH models of the following type ARCH(2) 
>#     and GARCH(1,1) with normal conditional distribution functions.
>#   PART II: Simulate GARCH models of the following type, ARCH(2) 
>#     and GARCH(1,1),
>#	with normal conditional distribution functions.
>#
># Author:
>#	(C) 2002, Diethelm Wuertz, GPL
>#
>
>
>############################################################################
>####
># PART I: Estimation:
>
>	# Settings:
>	set.seed(547)
>    # Bollerslev's GARCH(1,1) with normal innovations:
>	model = list(omega = 1e-6, alpha = 0.1, beta = 0.8, mu = 0)
>	x = garchSim(model, n = 1000)
>	fit = garchFit(as.numeric(x), order = c(1, 1))
>	print(fit)
>	# Summary and Diagnostic Analysis:
>	summary(fit)
>	# Plot Results:
>	par(mfrow = c(2, 2))
>	plot(fit)
>	###
>
>Results of the estimations are false.
>
>Call:
>garchFit(x = as.numeric(x), order = c(1, 1))
>
>Coefficient(s):
>    omega         a1         b1  
>8.564e-07  5.000e-02  5.000e-02  
>
>To compare with : omega = 1e-6, alpha = 0.1, beta = 0.8.
>
>Do you have some information about this?
>Can I give some initials values to start the estimations?
>Can I use different innovation process like student-t and GED
>
>Thanks for your answers
>
>Cyril 
>
>
>
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From wuertz at itp.phys.ethz.ch  Thu Nov 11 09:42:38 2004
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Thu, 11 Nov 2004 08:42:38 +0000
Subject: [R] fSeries
In-Reply-To: <41932287.8020000@itp.phys.ethz.ch>
References: <E66B5035E7D16C4B89028BAE9AC268A50403644B@I04MB144.fr.fimgroup>
	<41932287.8020000@itp.phys.ethz.ch>
Message-ID: <419325FE.5010906@itp.phys.ethz.ch>

Diethelm Wuertz wrote:

Tabs removed from the code, makes a nicer printout ....

garchSim =
function(model = list(omega = 1.0e-06, alpha = 0.1, beta = 0.8, mu = 0),
n = 100, innov = NULL, n.start = 100, start.innov = NULL, rand.gen = 
rnorm, ...)
{    
    # Doesn't work, replace the three following three lines ... 
    # if (!exists("model$alpha")) model$alpha = 0
    # if (!exists("model$beta")) model$beta = 0
    # if (!exists("model$mu")) model$mu = 0
   
    # with ...
    if (is.null(model$alpha)) model$alpha = 0
    if (is.null(model$beta)) model$beta = 0
    if (is.null(model$mu)) model$mu = 0

    max.order = max(length(model$alpha), length(model$beta))
    if (n.start < max.order)
        stop("n.start must be greater or equal max(alpha,beta)")
    if (is.null(start.innov))
        start.innov = rand.gen(n.start, ...)
    if (is.null(innov))
        innov = rand.gen(n, ...)
    h = x = z = c(start.innov, innov)
    for (i in 1:max.order) {
        h[i] = model$omega/(1 - sum(model$alpha) - sum(model$beta))
        x[i] = sqrt(h[i]) * z[i] + model$mu
    }
    n.alpha = length(model$alpha)
    n.beta = length(model$beta)
    for (i in (max.order + 1):(n.start + n)) {
        h[i] = model$omega + sum(model$alpha * x[i - (1:n.alpha)]^2) +
            sum(model$beta * h[i - (1:n.beta)])
        x[i] = sqrt(h[i]) * z[i] + model$mu
    }
    as.ts(x[-(1:n.start)])
}



From Alicia.Amadoz at uv.es  Thu Nov 11 10:29:58 2004
From: Alicia.Amadoz at uv.es (Alicia Amadoz)
Date: Thu, 11 Nov 2004 10:29:58 +0100 (CET)
Subject: [R] RSPerl problem with testing
Message-ID: <6217119899amadoz@uv.es>

Hi,

I'm trying to install de RSPerl module and i have some problems trying
to test it. I've tried to contact with the author but the e-mail address
seems not to exist. Hope that anyone in this list could explain me a
little about this problem. I have a bash shell and what i do is the
following:

#PERLLIB=/usr/lib/R/library/RSPerl/share/blib/arch:/usr/lib/R/library/RSPerl/share/blib/lib
# export PERLLIB
# LD_LIBRARY_PATH=/usr/lib/R/bin:/usr/lib/R/library/RSPerl/libs
# export LD_LIBRARY_PATH
# perl test.pl
1..1
Can't load '/usr/lib/R/library/RSPerl/share/blib/arch/auto/R/R.so' for
module R: /usr/lib/R/library/RSPerl/libs/libPerlConverter.so: undefined
symbol: R_GlobalEnv at
/usr/lib/perl5/5.8.3/i386-linux-thread-multi/DynaLoader.pm line 229.
 at test.pl line 11
Compilation failed in require at test.pl line 11.
BEGIN failed--compilation aborted at test.pl line 11.
not ok 1

I move to the /usr/local/src to test again and  i get the same error.

Any help would be much appreciated. Thanks.

Regards,
Alicia


***********************************************
Alicia Amadoz
Evolutionary Genetics Unit
Cavanilles Institute for Biodiversity and Evolutionary
Biology
University of Valencia
Apartado Oficial 22085
E-46071 Valencia SPAIN
Phone: (+34) 96 354 3687
FAX: (+34) 96 354 3670
e-mail: alicia.amadoz at uv.es
http://www.uv.es/~amadoz
***********************************************
NOTE! For shipments by EXPRESS COURIER use "Instituto
Cavanilles de Biodiversidad y Biolog??a Evolutiva,
Pol??gono de la Coma s/n, 46980 Paterna (Valencia),
Spain" instead of P.O. Box no. and Post Code/City above.



From ripley at stats.ox.ac.uk  Thu Nov 11 10:44:27 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 11 Nov 2004 09:44:27 +0000 (GMT)
Subject: [R] RSPerl problem with testing
In-Reply-To: <6217119899amadoz@uv.es>
Message-ID: <Pine.LNX.4.44.0411110941310.16804-100000@gannet.stats>

The author is at  <duncan at wald.ucdavis.edu>: this is an Omegahat product 
and I think the Omegahat web pages do lead you there.

Looks to me as if you haven't got the R shared library in your load 
library path: if this is R 2.0.x it is in /usr/lib/R/lib, not bin.

On Thu, 11 Nov 2004, Alicia Amadoz wrote:

> Hi,
> 
> I'm trying to install de RSPerl module and i have some problems trying
> to test it. I've tried to contact with the author but the e-mail address
> seems not to exist. Hope that anyone in this list could explain me a
> little about this problem. I have a bash shell and what i do is the
> following:
> 
> #PERLLIB=/usr/lib/R/library/RSPerl/share/blib/arch:/usr/lib/R/library/RSPerl/share/blib/lib
> # export PERLLIB
> # LD_LIBRARY_PATH=/usr/lib/R/bin:/usr/lib/R/library/RSPerl/libs
> # export LD_LIBRARY_PATH
> # perl test.pl
> 1..1
> Can't load '/usr/lib/R/library/RSPerl/share/blib/arch/auto/R/R.so' for
> module R: /usr/lib/R/library/RSPerl/libs/libPerlConverter.so: undefined
> symbol: R_GlobalEnv at
> /usr/lib/perl5/5.8.3/i386-linux-thread-multi/DynaLoader.pm line 229.
>  at test.pl line 11
> Compilation failed in require at test.pl line 11.
> BEGIN failed--compilation aborted at test.pl line 11.
> not ok 1

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dave_wilton at hotmail.com  Thu Nov 11 11:08:47 2004
From: dave_wilton at hotmail.com (dave wilton)
Date: Thu, 11 Nov 2004 10:08:47 +0000
Subject: [R] smoothing techniques
Message-ID: <BAY16-F11KO8FNpyLAP0001a602@hotmail.com>

Hi

I was wondering if you could help. I am trying to programme both the maximum 
entropy method algorithm and the continuous wavelet transform into R but am 
having quite a lot of diffulculties. Could you suggest anything.

Many Thanks

David Wilton



From plummer at iarc.fr  Thu Nov 11 11:31:00 2004
From: plummer at iarc.fr (Martyn Plummer)
Date: Thu, 11 Nov 2004 11:31:00 +0100
Subject: [R] R works on Fedora Core 3
In-Reply-To: <1100074746.6198.9.camel@nemo>
References: <20041109174442.GA27104@psych>
	<65C11A98-327E-11D9-AE8C-000A95C76CA8@oulu.fi>
	<20041109184717.GA15407@psych>  <1100074746.6198.9.camel@nemo>
Message-ID: <1100169060.24764.51.camel@nemo>

On Wed, 2004-11-10 at 09:19 +0100, Martyn Plummer wrote:
> On Tue, 2004-11-09 at 19:47, Jonathan Baron wrote:
> > On 11/09/04 20:37, Jari Oksanen wrote:
> > >
> > >On 9 Nov 2004, at 19:44, Jonathan Baron wrote:
> > >
> > >> The RPM for Fedora Core 2 seems to work just fine on Core 3.
> > >>
> > >> (The graphics window got smaller, but I'm sure there is a setting
> > >> for that.)
> > >>
> > >That would be good news. I really don't know how the graphics window
> > >became so big at some stage. (MacOS X is just cute here: tiny, sharp,
> > >fast graphics window.)
> > 
> > I have the opposite problem, a 1680x1050 display.
> > 
> > >Has the options()printcmd reappeared, so that dev.print() works without
> > >changing default options?
> > 
> > I can't imagine how this would change.  This is the same "old"
> > RPM, not a new one.  The option is there, and I don't think it
> > ever disappeared.  I can't test it.  This is my laptop, which is
> > not set up to print anything.
> 
> My mistake.  The default print command is determined at configure time.
> But the RedHat RPMS are built in a sandbox that has only the minimal
> configuration needed to build R. This doesn't include the lpr package so
> the default print command is null. I will fix this in the next RPM
> release, but right now I am upgrading to FC3. 

An RPM for Fedora Core 3 should be on a CRAN mirror near you by the
weekend. This fixes the printcmd bug.

The X11() window is the right size for me, but it doesn't have a title
bar, which is a nuisance.

Martyn



From gavin.simpson at ucl.ac.uk  Thu Nov 11 11:46:04 2004
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Thu, 11 Nov 2004 10:46:04 +0000
Subject: [R] R works on Fedora Core 3
In-Reply-To: <1100169060.24764.51.camel@nemo>
References: <20041109174442.GA27104@psych>	<65C11A98-327E-11D9-AE8C-000A95C76CA8@oulu.fi>	<20041109184717.GA15407@psych>
	<1100074746.6198.9.camel@nemo> <1100169060.24764.51.camel@nemo>
Message-ID: <419342EC.5090008@ucl.ac.uk>

Martyn Plummer wrote:
> On Wed, 2004-11-10 at 09:19 +0100, Martyn Plummer wrote:
> 

<snip>

> An RPM for Fedora Core 3 should be on a CRAN mirror near you by the
> weekend. This fixes the printcmd bug.
> 
> The X11() window is the right size for me, but it doesn't have a title
> bar, which is a nuisance.
> 
> Martyn
> 

I have this "problem" with both your rpm on FC2 and when compiled from 
sources (1.9.1-patched 2.0.0-patched [now 2.0.1 beta]). The title bar 
comes and goes, seemingly at random (I've not systematically test this 
so I haven't found an underlying cause) when a new X11() device is 
started. Mainly a more complex plot (over several lines and commands) 
will fail to produce the title bar, if I then close that device and open 
another using something simple, say plot(1:10) then I more often than 
not get the title bar. I say more often than not, because sometimes I 
have to run that command a few times, closing the opened faulty device 
window in between to get the title bar to appear. Same result running R 
from within xemacs/ess.

Gav

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpson at ucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From bm8 at st-andrews.ac.uk  Thu Nov 11 11:58:46 2004
From: bm8 at st-andrews.ac.uk (Bernie McConnell)
Date: Thu, 11 Nov 2004 10:58:46 +0000
Subject: [R] RODBC & POSIX & Daylight Saving blues
Message-ID: <6.1.2.0.0.20041110173817.025a3e40@bute.st-and.ac.uk>

Dear All,

The recent improvement in RODBC to recognize datetimes in tables has 
exposed my ongoing confusion.

All my data are obtained from a satellite system (Argos) which tags events 
in the GMT time zone.  Daylight saving is ignored.  To my way of thinking 
this means that

   1.  twelve-o-clock means halfway through the day regardless of season, and
   2.  the difftime of any two dates where the time is set to 
twelve-o-clock should be an integer, regardless of which season each of the 
dates are in.

I illustrate my confusion with a two-line table in an Access 2000 database 
table where the single field called 'theDate' contains the two values:

30/07/04 12:00:00
30/11/04 12:00:00

then I bring the datetimes into R with the following code:

 > library (RODBC)
 > theChannel <- odbcConnect("phonetagcopy", "", "")
 > pp <- sqlFetch(theChannel, "DateTest")
 > odbcClose(theChannel)
 > pp$theDate
[1] "2004-07-30 12:00:00 GMT Daylight Time"
[2] "2004-11-30 12:00:00 GMT Standard Time"
 >
 > unclass(pp$theDate) / 86400
[1] 12629.46 12752.50
attr(,"tzone")
[1] ""
 >
 > difftime (pp$theDate[1], pp$theDate[2])
Time difference of -123.0417 days

It appears that sqlFetch has (in this case wrongly) assumed that my 
datetimes are corrected for Daylight Saving.  How can I persuade it to 
accept that all my datetimes are in straight GMT?

OS:Win2000
R 2.0.0
RODBC version 1.1-2
Sys.getlocale()
[1] "LC_COLLATE=English_United Kingdom.1252;LC_CTYPE=English_United 
Kingdom.1252;LC_MONETARY=C;LC_NUMERIC=C;LC_TIME=English_United Kingdom.1252"
 >


Many thanks

Bernie McConnell

Sea Mammal Research Unit
University of St Andrews



From daodao99 at student.umu.se  Thu Nov 11 12:04:29 2004
From: daodao99 at student.umu.se (Danardono)
Date: Thu, 11 Nov 2004 12:04:29 +0100
Subject: [R] survSplit: further exploration and related topics
In-Reply-To: <Pine.A41.4.61b.0411090659250.226980@homer11.u.washington.edu>
References: <0ABD88905D18E347874E0FB71C0B29E90237D0D3@exdkba022.novo.dk>
	<Pine.A41.4.61b.0411090659250.226980@homer11.u.washington.edu>
Message-ID: <4193473C.7020200@student.umu.se>

While waiting for R.2.0.1 or 2.1, for you  who need function for this 
survival-splitting business, as I do,   this 'survcut' function below 
might be helpful.
It is not an elegant nor efficient function but it works, at least for 
some examples below.

data(aml)
m1<-coxph(Surv(time,status)~x,data=aml)

#unfortunately, start time must be created first
#but not a big deal I believe
aml$t0<-0

#then try :

d1<-survcut(cut=c(5,10,50),c("t0","time","status"),data=aml)
coxph(Surv(t0,time,status)~x,data=d1)
d2<-survcut(cut=c(9,12,40),c("t0","time","status"),data=d1)
coxph(Surv(t0,time,status)~x,data=d2)
d3<-survcut(cut=c(9,12,40),c("t0","time","status"),data=d2)
coxph(Surv(t0,time,status)~x,data=d3)

# splitting at the risk times
# useful for coxph with time-dependent covariate
d4<-survcut(cut=unique(aml$time[aml$status==1]),c("t0","time","status"),data=d2)
coxph(Surv(t0,time,status)~x,data=d4)

# "random" splitting
dr<-survcut(cut=runif(rpois(1,4),0,100),c("t0","time","status"),data=d1)
dim(dr)
coxph(Surv(t0,time,status)~x,data=dr)


# "per unit time" splitting
d5<-survcut(cut=0:161,c("t0","time","status"),data=d4)
coxph(Surv(t0,time,status)~x,data=d5)


#### the code --------------------------------------------------
survcut<-function (cuts, surv = c("t0", "t1", "event"), data,
   id = NULL, addv = TRUE, sq = FALSE)
# cuts: vector of timepoints to cut at
# surv: a Surv like input
# id :  (optional) variable name, if the data has an id variable
#advv :  (optional) include other variables in the output?
#sq: (optional) sequence of splitting , may be similar with episode in 
survSplit
{
   #cutting one counting process survival line
    cutting <- function(x, a, s) {
        x <- as.numeric(x)
        tmp <- sort(c(x[2:3], a[(x[2] < a) & (x[3] > a)]))
        n <- length(tmp)
        idx <- rep(x[1], n - 1)
        t0 <- tmp[1:(n - 1)]
        t1 <- tmp[2:n]
        event <- rep(0, n - 1)
        event[n - 1] <- x[4]
        if (s)
            data.frame(idx, t0, t1, event, s = 1:(n - 1))
        else data.frame(idx, t0, t1, event)
    }

  #Note that this database-joint-like function is similar to 'merge'
  #I made this before I knew 'merge' is available in R
  #merge can substitute this function, I believe
  #x: the main data (all rows will be selected)
  #variable: variable name (character vector) from refdat
  #key: key variable has to be exist in x and refdat
  #refdat: reference data, must have unique 'key' id
  addvars<-function (x, variable, key = "id", refdat)
  {
    nama <- names(refdat)
    if (is.numeric(variable)) {
        if (!prod(variable %in% 1:length(nama)))
            stop("variable does not exist")
        newname <- names(refdat)[variable]
    }
    else {
        if (!prod(variable %in% nama))
            stop("variable does not exist")
        newname <- variable
    }
    if (!(length(unique(refdat[, key])) == length(refdat[, key])))
        stop("key must be unique")
    newvar <- refdat[, variable, drop = FALSE]
    newvar <- newvar[match(x[, key], refdat[, key]), , drop = FALSE]
    x <- cbind(x, newvar)
    return(x)
}

    vars <- names(data)
    if (sum(surv %in% vars) != 3)
        stop("one or more surv variables do not exist")
    idx <- 1:NROW(data)
    data<- cbind(idx, data)
    t0 <- data[, surv[1]]
    t1 <- data[, surv[2]]
    event <- data[, surv[3]]
    x <- data.frame(idx, t0, t1, event)
    out <- by(x, list(1:NROW(x)), cutting, a = cuts, s = sq)
    out <- do.call("rbind", out)
    rownames(out) <- 1:NROW(out)
    names(out)[2:4] <- surv
    if (addv)
        out <- addvars(out, setdiff(names(data), c(surv, "idx")),
            key = "idx", refdat = data)
    out <- out[, -1]
    if (!is.null(id)) {
        slct <- c(id, setdiff(names(out), id))
        out <- subset(out, select = slct)
    }
    out
}

-----------------------------
/Danar



From ripley at stats.ox.ac.uk  Thu Nov 11 12:16:41 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 11 Nov 2004 11:16:41 +0000 (GMT)
Subject: [R] RODBC & POSIX & Daylight Saving blues
In-Reply-To: <6.1.2.0.0.20041110173817.025a3e40@bute.st-and.ac.uk>
Message-ID: <Pine.GSO.4.31.0411111108180.27538-100000@toucan.stats>

On Thu, 11 Nov 2004, Bernie McConnell wrote:

> Dear All,
>
> The recent improvement in RODBC to recognize datetimes in tables has
> exposed my ongoing confusion.
>
> All my data are obtained from a satellite system (Argos) which tags events
> in the GMT time zone.  Daylight saving is ignored.  To my way of thinking
> this means that
>
>    1.  twelve-o-clock means halfway through the day regardless of season, and
>    2.  the difftime of any two dates where the time is set to
> twelve-o-clock should be an integer, regardless of which season each of the
> dates are in.
>
> I illustrate my confusion with a two-line table in an Access 2000 database
> table where the single field called 'theDate' contains the two values:
>
> 30/07/04 12:00:00
> 30/11/04 12:00:00
>
> then I bring the datetimes into R with the following code:
>
>  > library (RODBC)
>  > theChannel <- odbcConnect("phonetagcopy", "", "")
>  > pp <- sqlFetch(theChannel, "DateTest")
>  > odbcClose(theChannel)
>  > pp$theDate
> [1] "2004-07-30 12:00:00 GMT Daylight Time"
> [2] "2004-11-30 12:00:00 GMT Standard Time"
>  >
>  > unclass(pp$theDate) / 86400
> [1] 12629.46 12752.50
> attr(,"tzone")
> [1] ""
>  >
>  > difftime (pp$theDate[1], pp$theDate[2])
> Time difference of -123.0417 days
>
> It appears that sqlFetch has (in this case wrongly) assumed that my
> datetimes are corrected for Daylight Saving.  How can I persuade it to
> accept that all my datetimes are in straight GMT?

All date-time conversions in R by default occurs in the current timezone.
If you set it to GMT for the duration of the sqlFetch call, it should do
as you intended (but had not told R, which is not clairvoyant).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rpeng at jhsph.edu  Thu Nov 11 14:10:18 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 11 Nov 2004 08:10:18 -0500
Subject: [R] R works on Fedora Core 3
In-Reply-To: <419342EC.5090008@ucl.ac.uk>
References: <20041109174442.GA27104@psych>	<65C11A98-327E-11D9-AE8C-000A95C76CA8@oulu.fi>	<20041109184717.GA15407@psych>	<1100074746.6198.9.camel@nemo>
	<1100169060.24764.51.camel@nemo> <419342EC.5090008@ucl.ac.uk>
Message-ID: <419364BA.7080603@jhsph.edu>

I've run into this title bar problem on FC1 and FC2 (not tried FC3 
yet) when I compile from sources.  I figured it was just a weird 
window manager bug.  I find that if I maximize the window (Alt-F10, on 
my system) and then restore the window to its original size (Alt-F5), 
I get the title bar back.

-roger

Gavin Simpson wrote:
> Martyn Plummer wrote:
> 
>> On Wed, 2004-11-10 at 09:19 +0100, Martyn Plummer wrote:
>>
> 
> <snip>
> 
>> An RPM for Fedora Core 3 should be on a CRAN mirror near you by the
>> weekend. This fixes the printcmd bug.
>>
>> The X11() window is the right size for me, but it doesn't have a title
>> bar, which is a nuisance.
>>
>> Martyn
>>
> 
> I have this "problem" with both your rpm on FC2 and when compiled from 
> sources (1.9.1-patched 2.0.0-patched [now 2.0.1 beta]). The title bar 
> comes and goes, seemingly at random (I've not systematically test this 
> so I haven't found an underlying cause) when a new X11() device is 
> started. Mainly a more complex plot (over several lines and commands) 
> will fail to produce the title bar, if I then close that device and open 
> another using something simple, say plot(1:10) then I more often than 
> not get the title bar. I say more often than not, because sometimes I 
> have to run that command a few times, closing the opened faulty device 
> window in between to get the title bar to appear. Same result running R 
> from within xemacs/ess.
> 
> Gav
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From alexandersokol at ofir.dk  Thu Nov 11 14:33:46 2004
From: alexandersokol at ofir.dk (Alexander Sokol)
Date: Thu, 11 Nov 2004 14:33:46 +0100
Subject: [R] Logical "and"
Message-ID: <41B2867D@webmail2.ofir.dk>

Hello,

I have the following very simple problem:

Say I have two vectors,

a<-c(1,7,4,5,9,11)
b<-c(7,4,9)

I would like to create a vector containing the elements in a which are not in 
b.

Obviously, this is possible by writing

a[a!=b[1] & a!=b[2] & a!=b[3]]

But I would like a solution which is applicable to the situation where the 
number of elements in b is unknown.

I have looked in the R manuals, the FAQ and the mailing lists, but have been 
unable to find a solution.

Thank you for your replies,
 Alexander



From matthew_wiener at merck.com  Thu Nov 11 14:39:24 2004
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Thu, 11 Nov 2004 08:39:24 -0500
Subject: [R] Logical "and"
Message-ID: <45AAE6FD142DCB43A38C00A11FF5DF3E04993AD0@uswsmx03.merck.com>

Alexander --

a[!(a %in% b)] should do the trick.

setdiff could also probably be used.

Hope this helps,

Matt Wiener

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Alexander Sokol
Sent: Thursday, November 11, 2004 8:34 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Logical "and"


Hello,

I have the following very simple problem:

Say I have two vectors,

a<-c(1,7,4,5,9,11)
b<-c(7,4,9)

I would like to create a vector containing the elements in a which are not
in 
b.

Obviously, this is possible by writing

a[a!=b[1] & a!=b[2] & a!=b[3]]

But I would like a solution which is applicable to the situation where the 
number of elements in b is unknown.

I have looked in the R manuals, the FAQ and the mailing lists, but have been

unable to find a solution.

Thank you for your replies,
 Alexander

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From sdavis2 at mail.nih.gov  Thu Nov 11 14:42:30 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 11 Nov 2004 08:42:30 -0500
Subject: [R] Logical "and"
In-Reply-To: <41B2867D@webmail2.ofir.dk>
References: <41B2867D@webmail2.ofir.dk>
Message-ID: <884AFDE0-33E7-11D9-AC13-000A95D7BA10@mail.nih.gov>


On Nov 11, 2004, at 8:33 AM, Alexander Sokol wrote:

> Say I have two vectors,
>
> a<-c(1,7,4,5,9,11)
> b<-c(7,4,9)
>
> I would like to create a vector containing the elements in a which are 
> not in
> b.

 > a[!(a %in% b)]
[1]  1  5 11

Sean



From Jussi.Makinen at valtiokonttori.fi  Thu Nov 11 14:42:31 2004
From: Jussi.Makinen at valtiokonttori.fi (=?iso-8859-1?Q?M=E4kinen_Jussi?=)
Date: Thu, 11 Nov 2004 15:42:31 +0200
Subject: [R] Logical "and"
Message-ID: <0BDE2460F08BF0429F933A40431A61E801E820D1@vk2kmail01.valtiokonttori.local>

a[!is.element(a, b)]

Jussi

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Alexander Sokol
Sent: 11. marraskuuta 2004 15:34
To: r-help at stat.math.ethz.ch
Subject: [R] Logical "and"


Hello,

I have the following very simple problem:

Say I have two vectors,

a<-c(1,7,4,5,9,11)
b<-c(7,4,9)

I would like to create a vector containing the elements in a which are not in 
b.

Obviously, this is possible by writing

a[a!=b[1] & a!=b[2] & a!=b[3]]

But I would like a solution which is applicable to the situation where the 
number of elements in b is unknown.

I have looked in the R manuals, the FAQ and the mailing lists, but have been 
unable to find a solution.

Thank you for your replies,
 Alexander

______________________________________________
R-help at stat.math.ethz.ch mailing list https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From abunn at whrc.org  Thu Nov 11 14:42:32 2004
From: abunn at whrc.org (Andy Bunn)
Date: Thu, 11 Nov 2004 08:42:32 -0500
Subject: [R] Logical "and"
In-Reply-To: <41B2867D@webmail2.ofir.dk>
Message-ID: <NEBBIPHDAMMOKDKPOFFIEEPHCLAA.abunn@whrc.org>

How about this?

a<-c(1,7,4,5,9,11)
b<-c(7,4,9)
a[!a %in% b]

b<-c(7,4,9, 100, 20, 34, 54)
a[!a %in% b]

see ?match, too

HTH, Andy


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Alexander Sokol
> Sent: Thursday, November 11, 2004 8:34 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Logical "and"
> 
> 
> Hello,
> 
> I have the following very simple problem:
> 
> Say I have two vectors,
> 
> a<-c(1,7,4,5,9,11)
> b<-c(7,4,9)
> 
> I would like to create a vector containing the elements in a 
> which are not in 
> b.
> 
> Obviously, this is possible by writing
> 
> a[a!=b[1] & a!=b[2] & a!=b[3]]
> 
> But I would like a solution which is applicable to the situation 
> where the 
> number of elements in b is unknown.
> 
> I have looked in the R manuals, the FAQ and the mailing lists, 
> but have been 
> unable to find a solution.
> 
> Thank you for your replies,
>  Alexander
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From p.dalgaard at biostat.ku.dk  Thu Nov 11 14:44:47 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 11 Nov 2004 14:44:47 +0100
Subject: [R] Logical "and"
In-Reply-To: <41B2867D@webmail2.ofir.dk>
References: <41B2867D@webmail2.ofir.dk>
Message-ID: <x2mzxoijow.fsf@biostat.ku.dk>

Alexander Sokol <alexandersokol at ofir.dk> writes:

> Hello,
> 
> I have the following very simple problem:
> 
> Say I have two vectors,
> 
> a<-c(1,7,4,5,9,11)
> b<-c(7,4,9)
> 
> I would like to create a vector containing the elements in a which are not in 
> b.

As in

> setdiff(a,b)
[1]  1  5 11

or 

> a[!(a %in% b)]
[1]  1  5 11

?

(Note that they differ if a has nonunique values).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Roger.Bivand at nhh.no  Thu Nov 11 14:46:08 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 11 Nov 2004 14:46:08 +0100 (CET)
Subject: [R] Logical "and"
In-Reply-To: <41B2867D@webmail2.ofir.dk>
Message-ID: <Pine.LNX.4.44.0411111445150.538-100000@reclus.nhh.no>

On Thu, 11 Nov 2004, Alexander Sokol wrote:

?setdiff

> setdiff(a,b)
[1]  1  5 11


> Hello,
> 
> I have the following very simple problem:
> 
> Say I have two vectors,
> 
> a<-c(1,7,4,5,9,11)
> b<-c(7,4,9)
> 
> I would like to create a vector containing the elements in a which are not in 
> b.
> 
> Obviously, this is possible by writing
> 
> a[a!=b[1] & a!=b[2] & a!=b[3]]
> 
> But I would like a solution which is applicable to the situation where the 
> number of elements in b is unknown.
> 
> I have looked in the R manuals, the FAQ and the mailing lists, but have been 
> unable to find a solution.
> 
> Thank you for your replies,
>  Alexander
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From petr.pikal at precheza.cz  Thu Nov 11 14:47:53 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 11 Nov 2004 14:47:53 +0100
Subject: [R] Logical "and"
In-Reply-To: <41B2867D@webmail2.ofir.dk>
Message-ID: <41937B99.25134.1D309A5@localhost>

Hi

On 11 Nov 2004 at 14:33, Alexander Sokol wrote:

> Hello,
> 
> I have the following very simple problem:
> 
> Say I have two vectors,
> 
> a<-c(1,7,4,5,9,11)
> b<-c(7,4,9)
> 
> I would like to create a vector containing the elements in a which are
> not in b.

> a%in%b
[1] FALSE  TRUE  TRUE FALSE  TRUE FALSE
> !a%in%b
[1]  TRUE FALSE FALSE  TRUE FALSE  TRUE
> a[!a%in%b]
[1]  1  5 11
>

Is it OK?

Cheers
Petr

> 
> Obviously, this is possible by writing
> 
> a[a!=b[1] & a!=b[2] & a!=b[3]]
> 
> But I would like a solution which is applicable to the situation where
> the number of elements in b is unknown.
> 
> I have looked in the R manuals, the FAQ and the mailing lists, but
> have been unable to find a solution.
> 
> Thank you for your replies,
>  Alexander
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From ripley at stats.ox.ac.uk  Thu Nov 11 14:47:51 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 11 Nov 2004 13:47:51 +0000 (GMT)
Subject: [R] set differences (was Logical "and")
In-Reply-To: <41B2867D@webmail2.ofir.dk>
Message-ID: <Pine.GSO.4.31.0411111344480.27879-100000@toucan.stats>

The answer is the function setdiff(), but I suppose you have to know what
the operation is called to find it.

> setdiff(a, b)
[1]  1  5 11

You may find it illuminating to see how it is implemented.

On Thu, 11 Nov 2004, Alexander Sokol wrote:

> Hello,
>
> I have the following very simple problem:
>
> Say I have two vectors,
>
> a<-c(1,7,4,5,9,11)
> b<-c(7,4,9)
>
> I would like to create a vector containing the elements in a which are not in
> b.
>
> Obviously, this is possible by writing
>
> a[a!=b[1] & a!=b[2] & a!=b[3]]
>
> But I would like a solution which is applicable to the situation where the
> number of elements in b is unknown.
>
> I have looked in the R manuals, the FAQ and the mailing lists, but have been
> unable to find a solution.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rpeng at jhsph.edu  Thu Nov 11 14:49:13 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 11 Nov 2004 08:49:13 -0500
Subject: [R] Logical "and"
In-Reply-To: <41B2867D@webmail2.ofir.dk>
References: <41B2867D@webmail2.ofir.dk>
Message-ID: <41936DD9.6030800@jhsph.edu>

Try

setdiff(a, b)

-roger

Alexander Sokol wrote:
> Hello,
> 
> I have the following very simple problem:
> 
> Say I have two vectors,
> 
> a<-c(1,7,4,5,9,11)
> b<-c(7,4,9)
> 
> I would like to create a vector containing the elements in a which are not in 
> b.
> 
> Obviously, this is possible by writing
> 
> a[a!=b[1] & a!=b[2] & a!=b[3]]
> 
> But I would like a solution which is applicable to the situation where the 
> number of elements in b is unknown.
> 
> I have looked in the R manuals, the FAQ and the mailing lists, but have been 
> unable to find a solution.
> 
> Thank you for your replies,
>  Alexander
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From ligges at statistik.uni-dortmund.de  Thu Nov 11 14:51:40 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 11 Nov 2004 14:51:40 +0100
Subject: [R] Logical "and"
In-Reply-To: <41B2867D@webmail2.ofir.dk>
References: <41B2867D@webmail2.ofir.dk>
Message-ID: <41936E6C.1030902@statistik.uni-dortmund.de>

Alexander Sokol wrote:

> Hello,
> 
> I have the following very simple problem:
> 
> Say I have two vectors,
> 
> a<-c(1,7,4,5,9,11)
> b<-c(7,4,9)
> 
> I would like to create a vector containing the elements in a which are not in 
> b.
> 
> Obviously, this is possible by writing
> 
> a[a!=b[1] & a!=b[2] & a!=b[3]]


a[!(a %in% b)]

or see ?setdiff

Uwe Ligges


> But I would like a solution which is applicable to the situation where the 
> number of elements in b is unknown.
> 
> I have looked in the R manuals, the FAQ and the mailing lists, but have been 
> unable to find a solution.
> 
> Thank you for your replies,
>  Alexander
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From luke at stat.uiowa.edu  Thu Nov 11 15:16:00 2004
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Thu, 11 Nov 2004 08:16:00 -0600 (CST)
Subject: [R] R works on Fedora Core 3
In-Reply-To: <1100169060.24764.51.camel@nemo>
References: <20041109174442.GA27104@psych>
	<65C11A98-327E-11D9-AE8C-000A95C76CA8@oulu.fi>
	<20041109184717.GA15407@psych>  <1100074746.6198.9.camel@nemo>
	<1100169060.24764.51.camel@nemo>
Message-ID: <Pine.LNX.4.58.0411110802120.10820@nokomis.stat.uiowa.edu>

On Thu, 11 Nov 2004, Martyn Plummer wrote:

> On Wed, 2004-11-10 at 09:19 +0100, Martyn Plummer wrote:
> > On Tue, 2004-11-09 at 19:47, Jonathan Baron wrote:
> > > On 11/09/04 20:37, Jari Oksanen wrote:
> > > >
> > > >On 9 Nov 2004, at 19:44, Jonathan Baron wrote:
> > > >
> > > >> The RPM for Fedora Core 2 seems to work just fine on Core 3.
> > > >>
> > > >> (The graphics window got smaller, but I'm sure there is a setting
> > > >> for that.)
> > > >>
> > > >That would be good news. I really don't know how the graphics window
> > > >became so big at some stage. (MacOS X is just cute here: tiny, sharp,
> > > >fast graphics window.)
> > > 
> > > I have the opposite problem, a 1680x1050 display.
> > > 
> > > >Has the options()printcmd reappeared, so that dev.print() works without
> > > >changing default options?
> > > 
> > > I can't imagine how this would change.  This is the same "old"
> > > RPM, not a new one.  The option is there, and I don't think it
> > > ever disappeared.  I can't test it.  This is my laptop, which is
> > > not set up to print anything.
> > 
> > My mistake.  The default print command is determined at configure time.
> > But the RedHat RPMS are built in a sandbox that has only the minimal
> > configuration needed to build R. This doesn't include the lpr package so
> > the default print command is null. I will fix this in the next RPM
> > release, but right now I am upgrading to FC3. 
> 
> An RPM for Fedora Core 3 should be on a CRAN mirror near you by the
> weekend. This fixes the printcmd bug.
> 
> The X11() window is the right size for me, but it doesn't have a title
> bar, which is a nuisance.
> 
> Martyn
> 

I've seen this with X11 a little on FC2 but much more with rgl (maybe
50% of the time with rgl vs 5-10% for X11()).  Ross Ihaka tried
replacing the default metacity window manager with sawfish and found
that the problem seemed to go away. Ross also found a bug report for
metacity,

   https://bugzilla.redhat.com/bugzilla/show_bug.cgi?id=126571

that might be related.  It might also be something we are not doing
quite right in opening a window that happens to bite metacity more
than other wms.  If anyone has the time and energy to pursue this
please do.

Best,

luke

-- 
Luke Tierney
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
   Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu



From jeaneid at chass.utoronto.ca  Thu Nov 11 15:31:02 2004
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Thu, 11 Nov 2004 09:31:02 -0500
Subject: [R] Logical "and"
In-Reply-To: <41B2867D@webmail2.ofir.dk>
Message-ID: <Pine.SGI.4.40.0411110929180.51600905-100000@origin.chass.utoronto.ca>

You can use setdiff if you only need the unique values of a that are not
in b. If you want all values you can use

a[a%in%setdiff(a,b)]

There are also intersection, union etc...
see
?setdiff


On Thu, 11 Nov 2004, Alexander Sokol wrote:

> Hello,
>
> I have the following very simple problem:
>
> Say I have two vectors,
>
> a<-c(1,7,4,5,9,11)
> b<-c(7,4,9)
>
> I would like to create a vector containing the elements in a which are not in
> b.
>
> Obviously, this is possible by writing
>
> a[a!=b[1] & a!=b[2] & a!=b[3]]
>
> But I would like a solution which is applicable to the situation where the
> number of elements in b is unknown.
>
> I have looked in the R manuals, the FAQ and the mailing lists, but have been
> unable to find a solution.
>
> Thank you for your replies,
>  Alexander
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From chris.brown at amd.com  Thu Nov 11 16:10:43 2004
From: chris.brown at amd.com (Brown, Chris)
Date: Thu, 11 Nov 2004 09:10:43 -0600
Subject: [R] ROracle SQL length limitation
Message-ID: <84EA05E2CA77634C82730353CBE3A843B0487B@SAUSEXMB1.amd.com>

Hi All,

This question was brought up some time ago but I never saw a reply so I'd like to bring it up again. When using ROracle package (version 0.5-5), I am unable to run any queries that are greater than 4000 characters in length. If I do, I get the following message:

Error in oraPrepareStatement(con, statement, bind=NULL) :
	RS-DBI driver: (too long a statement -- it must has less than 4000 chars)

I realize this is not a "bug", but simply a predefined limit that has been set. Can this string limit be increased? Based on the only email I saw on this issue I edited RS-Oracle.h to be:

#define RS_ORA_MAX_STRING 8000
#define RS_ORA_STATEMENT_LEN 8000

Both of these values were previously set to 4000. However, this does not solve the problem and I am still seeing the same 4000 char limit message after header edit and recompile. Any ideas? Is this limit being read from DBI library directly?

Version info:
platform i686-pc-linux-gnu
arch     i686             
os       linux-gnu        
system   i686, linux-gnu  
status                    
major    1                
minor    9.1              
year     2004             
month    06               
day      21               
language R     

Thanks,
Chris



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of r-help-request at stat.math.ethz.ch
Sent: Thursday, November 11, 2004 5:22 AM
To: r-help at stat.math.ethz.ch
Subject: R-help Digest, Vol 21, Issue 11

Send R-help mailing list submissions to
	r-help at stat.math.ethz.ch

To subscribe or unsubscribe via the World Wide Web, visit
	https://stat.ethz.ch/mailman/listinfo/r-help
or, via email, send a message with subject or body 'help' to
	r-help-request at stat.math.ethz.ch

You can reach the person managing the list at
	r-help-owner at stat.math.ethz.ch

When replying, please edit your Subject line so it is more specific
than "Re: Contents of R-help digest..."


Today's Topics:

   1. glm.nb (Ronaldo Reis Jr.)
   2. Re: R under Pocket PC (Peter Dalgaard)
   3. lattice: ordering the entries in a dotplot of a vector
      (Wolfram Fischer)
   4. Multiple stripcharts using "for loop" (Paul JH Drake)
   5. Basic Q on coercing factors in data frames to numeric
      (Robert Brown FM CEFAS)
   6. Re: R under Pocket PC (Barry Rowlingson)
   7. printing to stderr (Arne.Muller at aventis.com)
   8. Re: R under Pocket PC (Tim Cutts)
   9. "conditional duplicates"? (Christian Schulz)
  10. Re: printing to stderr (Prof Brian Ripley)
  11. Re: "conditional duplicates"? (Dimitris Rizopoulos)
  12. Re: Basic Q on coercing factors in data frames to numeric
      (Roger D. Peng)
  13. Re: "conditional duplicates"? (Christian Schulz)
  14. RE: Does something like partition.rpart() exist? (Andy Bunn)
  15. Re: R under Pocket PC (Peter Dalgaard)
  16. fSeries (CYRIL.CAILLAULT at FORTISINVESTMENTS.COM)
  17. Re: worked in R, but not in S-Plus (Thomas Lumley)
  18. fSeries (Vito Ricci)
  19. Re: lattice: ordering the entries in a dotplot of a vector
      (Deepayan Sarkar)
  20. Building MacOSX binary in Windows XP (Tomas Aragon)
  21. Re: Building MacOSX binary in Windows XP (Prof Brian Ripley)
  22. cubic spline/smoother with nlme (Suzette Blanchard)
  23. Re: cubic spline/smoother with nlme (Peter Dalgaard)
  24. RE: cubic spline/smoother with nlme (Pikounis, Bill)
  25. Re: R under Pocket PC (Tom Mulholland)
  26. substitute/paste question for using Greek in plot titles
      (Peter Dunn)
  27. polr probit versus stata oprobit   (Jean Eid)
  28. R with Sun Studio Fortran 95 compiler (Bizuet Cabrera Fernando)
  29. Re: substitute/paste question for using Greek in plot titles
      (Sundar Dorai-Raj)
  30. Re: polr probit versus stata oprobit   (Thomas Lumley)
  31. Re: polr probit versus stata oprobit   (Jean Eid)
  32. Re: substitute/paste question for using Greek in plot titles
      (Roger D. Peng)
  33. "<<-" assignment no long work in class methods (Gang Liang)
  34. Re: "<<-" assignment no long work in class methods
      (Witold Eryk Wolski)
  35. Re: R with Sun Studio Fortran 95 compiler (Prof Brian Ripley)
  36. Re: fSeries (Diethelm Wuertz)
  37. Re: fSeries (Diethelm Wuertz)
  38. RSPerl problem with testing (Alicia Amadoz)
  39. Re: RSPerl problem with testing (Prof Brian Ripley)
  40. smoothing techniques (dave wilton)
  41. Re: R works on Fedora Core 3 (Martyn Plummer)
  42. Re: R works on Fedora Core 3 (Gavin Simpson)
  43. RODBC & POSIX & Daylight Saving blues (Bernie McConnell)


----------------------------------------------------------------------

Message: 1
Date: Wed, 10 Nov 2004 08:59:49 -0200
From: "Ronaldo Reis Jr." <chrysopa at insecta.ufv.br>
Subject: [R] glm.nb
To: "R-Help" <r-help at stat.math.ethz.ch>
Message-ID: <200411100859.49819.chrysopa at insecta.ufv.br>
Content-Type: text/plain;  charset="iso-8859-1"

Hi,

I make some simulations with rnbinom and try to test with glm.nb.

But in some data set the glm.nb fail.

Look:

pop <- rnbinom(n=1000,size=1,mu=0.05)
> glm.nb(pop~1,maxit=1000)
Error in while ((it <- it + 1) < limit && abs(del) > eps) { : 
 missing value where TRUE/FALSE needed

look some pop charactetistics:
> summary(pop)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  0.000   0.000   0.000   0.052   0.000   2.000

> hist(pop,plot=F)
$breaks
 [1] 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0

$counts
 [1] 949   0   0   0  50   0   0   0   0   1

$intensities
 [1] 4.744999 0.000000 0.000000 0.000000 0.250000 0.000000 0.000000 0.000000
 [9] 0.000000 0.005000

$density
 [1] 4.744999 0.000000 0.000000 0.000000 0.250000 0.000000 0.000000 0.000000
 [9] 0.000000 0.005000

$mids
 [1] 0.1 0.3 0.5 0.7 0.9 1.1 1.3 1.5 1.7 1.9

My question is:

Why this error occour and how to solve this problem without increase the mu 
parameter in rnbinom?


Thanks fo all

Ronaldo

---
Your ignorance cramps my conversation.
--
|>   // | \\   [***********************************]
|   ( ??   ?? )  [Ronaldo Reis J??nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36571-000 Vi??osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-2532                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



------------------------------

Message: 2
Date: 10 Nov 2004 12:03:36 +0100
From: Peter Dalgaard <p.dalgaard at biostat.ku.dk>
Subject: Re: [R] R under Pocket PC
To: "Stuart Leask" <stuart.leask at nottingham.ac.uk>
Cc: r-help at stat.math.ethz.ch
Message-ID: <x2d5ymj793.fsf at biostat.ku.dk>
Content-Type: text/plain; charset=us-ascii

"Stuart Leask" <stuart.leask at nottingham.ac.uk> writes:

> > One concern I recall from looking into this a while back (hopefully
> > not founded) was the issue with floating point handling on the ARMs
> > and similar PDA CPUs.
> >
> > This was relevant to the familiar/intimate linux distro's around 18-24
> > months ago, when I was considering an IPAQ or Zaurus to replace my old
> > but not dead yet palm.
> 
> Thanks to the excellent work of the R developers in keeping all the code
> quite standard, and some specific work by Simon Pickering (Bath University),
> I have had R working (with graphics) on my Sharp Zaurus for some time.
> Sadly, checking today his site is down, but binaries for the Zaurus are
> available.
> 
> There was a problem with NA handling that needed specifically addressing,
> and some missing fonts, but the fpu emulation worked fine. This was using
> X11, keypebble and VNC. It could _just about_ run in the standard RAM -
> although was a lot happier with extra memory. Most impressive, and well done
> Simon (and the R team).

Right. As I recall it, the FPU/NA issue was a case of configure
getting the byte-order wrong and not (as was believed for a while) a
problem with non-IEEE arithmetic.
 
> However, a note of caution - presumably due to the lack of a hardware fpu,
> and perhaps also the relatively slow access speed of SD ram, benchmarks on
> the zaurus ran anything up to (wait for it!) 100 times slower than a pentium
> of the same clock speed. This has sadly left R on my Zaurus largely idle, as
> in practice I found it just too slow to be usable.

Hmm. So you're probably limited to problems that require on the order
of tens of milliseconds FPU time on a normal machine. I suppose that
there are usages where this is feasible.

> I fear this problem could afflict any handheld implementation, until they
> start putting fpu's on these chips.

Yes. Now where are the video gamers when you need them?

> (PS. I have the same problem with a mini-ITX 'silent PC' - many of these
> low-power cpus (eg. Eden) lack hardware FPU. They run many office apps fine,
> but when I benchmarked with a stats program, I found it 50 times slower than
> a pentium of equivalent clock speed! I am advised that even those that do
> have hardware FPU eg. the Nehemiah chip are only 50-60% as fast as a
> pentium-class cpu of similar clock speed)

Hmm, the newer ones (Eden-N, Eden-ESP) claim "full-speed FPU". I've
been toying around with the idea of building a system around the
MythTV stuff to get a harddisk video recording, etc. Could be kind of
fun to run R on it with display to the TV (but could you control R
with the TV remote control?)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



------------------------------

Message: 3
Date: Wed, 10 Nov 2004 12:28:13 +0100
From: Wolfram Fischer <wolfram at fischer-zim.ch>
Subject: [R] lattice: ordering the entries in a dotplot of a vector
To: r-help at stat.math.ethz.ch
Message-ID: <20041110112813.GA4165 at s1x.local>
Content-Type: text/plain; charset=us-ascii

I tried:
    n <- 9
    x <- sample(n)
    names(x) <- LETTERS[1:n]
    dotplot( sort(x) )

The lines of the dotplot are not ordered according to the values of x
(but according to the names of x).

So I did:

    dfx <- data.frame( x=x, label=factor( names(x), names(x)[order(x)] ) )
    dotplot( label ~ x, data=dfx )

So I got what I wanted.

My question: Is there an easier solution for doing that?

Wolfram



------------------------------

Message: 4
Date: Wed, 10 Nov 2004 11:49:27 +0000
From: Paul JH Drake <p.drake at beatson.gla.ac.uk>
Subject: [R] Multiple stripcharts using "for loop"
To: R-help at stat.math.ethz.ch
Message-ID: <1100087367.12807.8.camel at G4.site>
Content-Type: text/plain

Sorry, I didn't post my system details:
SuSE Linux 9.1
R version 2.0.0
P4 Laptop, 512MB RAM

THanks for the reply. I'll see what I can do with the data.

Paul



------------------------------

Message: 5
Date: Wed, 10 Nov 2004 12:03:38 -0000
From: "Robert Brown FM CEFAS" <r.g.brown at cefas.co.uk>
Subject: [R] Basic Q on coercing factors in data frames to numeric
To: <r-help at stat.math.ethz.ch>
Message-ID:
	<3589BC4D64C84341AE0C258244F977A2B60B63 at expressa.corp.cefas.co.uk>
Content-Type: text/plain;	charset="iso-8859-1"

Hi there,

I'm running R 2.0.0 on Windows 95.  I'm trying to coerce a column of factors within a data frame to numeric.  This is not a problem with a vector, but I can't find a way to index a column within a data frame to achieve this. All the examples from 'An introduction to R', 'S-plus 6 programmers guide', etc, use simple vectors.  I'm sure I'm missing something obvious but I can't find a way to change a single column in data frame. Ive tried several approaches: here is one

> summary(test)
      age        yrclass       weight              year     
 1      :10   1992   :10   Min.   :0.005333   Min.   :1993  
 2      :10   1989   : 9   1st Qu.:0.221790   1st Qu.:1995  
 3      :10   1990   : 9   Median :0.413411   Median :1997  
 4      :10   1991   : 9   Mean   :0.420119   Mean   :1997  
 5      :10   1988   : 8   3rd Qu.:0.559819   3rd Qu.:2000  
 6      :10   1993   : 8   Max.   :1.189000   Max.   :2002  
 (Other):82   (Other):89                                    
> contents(test)

Data frame:test 142 observations and 4 variables    Maximum # NAs:0

        Levels Storage
age         23 integer
yrclass     28 integer
weight          double
year            double

+--------+---------------------------------------------------------------------+
|Variable|Levels                                                               |
+--------+---------------------------------------------------------------------+
| age    |0,1,10,11,12,13,14,15,16,18,19,2,20,21,24,25,3,4,5,6,7,8,9           |
+--------+---------------------------------------------------------------------+
| yrclass|1969,1974,1975,1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988|
|        |1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000,2001,2002|
+--------+---------------------------------------------------------------------+
> is(test$yrclass,"factor")
[1] TRUE
> is(test$yrclass,"numeric")
[1] FALSE
> as(test[,2],"numeric")
  [1]  1  1  2  3  3  4  5  6  6  6  7  7  8  8  8  8  9  9  9  9 10 10 10 10 10
 [26] 10 10 11 11 11 11 11 12 12 12 12 12 12 12 13 13 13 13 13 13 13 14 14 14 14
 [51] 14 14 14 14 15 15 15 15 15 15 15 15 15 16 16 16 16 16 16 16 16 16 17 17 17
 [76] 17 17 17 17 17 17 18 18 18 18 18 18 18 18 18 18 19 19 19 19 19 19 19 19 20
[101] 20 20 20 20 20 20 20 21 21 21 21 21 21 21 21 22 22 22 22 22 22 22 23 23 23
[126] 23 23 23 24 24 24 24 24 25 25 25 25 26 26 27 27 28
> is(test$yrclass,"numeric")
[1] FALSE


Regards,

Robert Brown



------------------------------

Message: 6
Date: Wed, 10 Nov 2004 12:11:51 +0000
From: Barry Rowlingson <B.Rowlingson at lancaster.ac.uk>
Subject: Re: [R] R under Pocket PC
To: r-help at stat.math.ethz.ch
Message-ID: <41920587.6000103 at lancaster.ac.uk>
Content-Type: text/plain; charset=us-ascii; format=flowed


> Could be kind of
> fun to run R on it with display to the TV (but could you control R
> with the TV remote control?)

  Peter,

    I'm sure everyone on this list feels you work hard enough on R as it 
is, dont let it invade your living room as well!

Baz



------------------------------

Message: 7
Date: Wed, 10 Nov 2004 13:21:48 +0100
From: <Arne.Muller at aventis.com>
Subject: [R] printing to stderr
To: <r-help at stat.math.ethz.ch>
Message-ID:
	<C80ECAFA2ACC1B45BE45D133ED660ADE010BF296 at crbsmxsusr04.pharma.aventis.com>
	
Content-Type: text/plain;	charset="iso-8859-1"

Hello,

is it possible to configure the print function to print to stderr?

	kind regards,

	Arne



------------------------------

Message: 8
Date: Wed, 10 Nov 2004 12:27:05 +0000
From: Tim Cutts <tjrc at sanger.ac.uk>
Subject: Re: [R] R under Pocket PC
To: Peter Dalgaard <p.dalgaard at biostat.ku.dk>
Cc: Lars Strand <lars.strand at skogforsk.no>,	Prof Brian Ripley
	<ripley at stats.ox.ac.uk>, R-help at stat.math.ethz.ch
Message-ID: <D4D5A05A-3313-11D9-AF52-000A95B2B140 at sanger.ac.uk>
Content-Type: text/plain; charset=US-ASCII; format=flowed


On 9 Nov 2004, at 5:14 pm, Peter Dalgaard wrote:
>>> PalmOS would probably be right out.  :-)
>>
>> I think you are belittling the work done to get R running on as wide a
>> range of platforms as it does.
>
> ?
>
> I only saw a bit of excessive pessimism in that remark.

Yes, no disrespect was intended at all.  Porting to PalmOS is quite a 
different kettle of fish compared to porting between (say) Windows and 
UNIX, because the OS doesn't have a libc; if you use almost any 
standard C library function your binary becomes a huge (because it 
needs to be statically linked with the C library that comes with your 
cross compiler).  A true PalmOS port of any piece of software often 
requires a lot of work to replace standard C library calls with the 
equivalents built into the machine.  Someone may have created some sort 
of wrapper C library, but when I last wrote any software for Palm 
devices, such a thing did not exist.

Admittedly, that was about two years ago, when I wrote some stuff for 
PalmOS 4.

Tim

-- 
Dr Tim Cutts
Informatics Systems Group, Wellcome Trust Sanger Institute
GPG: 1024D/E3134233 FE3D 6C73 BBD6 726A A3F5  860B 3CDD 3F56 E313 4233



------------------------------

Message: 9
Date: Wed, 10 Nov 2004 13:52:29 +0100
From: Christian Schulz <ozric at web.de>
Subject: [R] "conditional duplicates"?
To: r-help at stat.math.ethz.ch
Message-ID: <41920F0D.9030900 at web.de>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed

Hi,

i would like check a repeated measurement dataset, whether
cases (which (id) could be more than one time included and not all same 
times included.)
have more than one times the same SMONTH!?

#This pseudo code didn't work with a for loop, because the [i+1] isn't 
known.
How i could refer to [i+1] ?

if(ID[i] == ID[i+1] & SMONTH[i] !=[i+1])  {res[i] <- 0 }
if(ID[i] == ID[i+1] & SMONTH[i] == [i+1]) {res[i] <- 1 }
 
many thanks,
christian



------------------------------

Message: 10
Date: Wed, 10 Nov 2004 13:06:51 +0000 (GMT)
From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
Subject: Re: [R] printing to stderr
To: Arne.Muller at aventis.com
Cc: r-help at stat.math.ethz.ch
Message-ID: <Pine.LNX.4.44.0411101304390.18087-100000 at gannet.stats>
Content-Type: TEXT/PLAIN; charset=US-ASCII

On Wed, 10 Nov 2004 Arne.Muller at aventis.com wrote:

> is it possible to configure the print function to print to stderr?

No, but where standard output goes is controlled by sink(), so you can 
achieve the same effect.  R internally has no idea what output comes from 
print() (which is not just one function but hundreds of methods).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



------------------------------

Message: 11
Date: Wed, 10 Nov 2004 14:09:01 +0100
From: "Dimitris Rizopoulos" <dimitris.rizopoulos at med.kuleuven.ac.be>
Subject: Re: [R] "conditional duplicates"?
To: "Christian Schulz" <ozric at web.de>
Cc: r-help at stat.math.ethz.ch
Message-ID: <003f01c4c726$7230ec30$0540210a at www.domain>
Content-Type: text/plain; format=flowed; charset="iso-8859-1";
	reply-type=response

Hi Christian,

may be this is helpful:

dat <- data.frame(id=rep(1:4, each=5), smonth=sample(1:5, 20, TRUE))
dat$duplicated <- unlist(tapply(dat$smonth, dat$id, duplicated))
dat

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Christian Schulz" <ozric at web.de>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, November 10, 2004 1:52 PM
Subject: [R] "conditional duplicates"?


> Hi,
>
> i would like check a repeated measurement dataset, whether
> cases (which (id) could be more than one time included and not all 
> same times included.)
> have more than one times the same SMONTH!?
>
> #This pseudo code didn't work with a for loop, because the [i+1] 
> isn't known.
> How i could refer to [i+1] ?
>
> if(ID[i] == ID[i+1] & SMONTH[i] !=[i+1])  {res[i] <- 0 }
> if(ID[i] == ID[i+1] & SMONTH[i] == [i+1]) {res[i] <- 1 }
>
> many thanks,
> christian
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



------------------------------

Message: 12
Date: Wed, 10 Nov 2004 08:20:27 -0500
From: "Roger D. Peng" <rpeng at jhsph.edu>
Subject: Re: [R] Basic Q on coercing factors in data frames to numeric
To: Robert Brown FM CEFAS <r.g.brown at cefas.co.uk>
Cc: r-help at stat.math.ethz.ch
Message-ID: <4192159B.4050602 at jhsph.edu>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed

I believe this is a FAQ.  See 
http://cran.r-project.org/doc/FAQ/R-FAQ.html#How-do-I-convert-factors-to-numeric_003f

-roger

Robert Brown FM CEFAS wrote:
> Hi there,
> 
> I'm running R 2.0.0 on Windows 95.  I'm trying to coerce a column of factors within a data frame to numeric.  This is not a problem with a vector, but I can't find a way to index a column within a data frame to achieve this. All the examples from 'An introduction to R', 'S-plus 6 programmers guide', etc, use simple vectors.  I'm sure I'm missing something obvious but I can't find a way to change a single column in data frame. Ive tried several approaches: here is one
> 
> 
>>summary(test)
> 
>       age        yrclass       weight              year     
>  1      :10   1992   :10   Min.   :0.005333   Min.   :1993  
>  2      :10   1989   : 9   1st Qu.:0.221790   1st Qu.:1995  
>  3      :10   1990   : 9   Median :0.413411   Median :1997  
>  4      :10   1991   : 9   Mean   :0.420119   Mean   :1997  
>  5      :10   1988   : 8   3rd Qu.:0.559819   3rd Qu.:2000  
>  6      :10   1993   : 8   Max.   :1.189000   Max.   :2002  
>  (Other):82   (Other):89                                    
> 
>>contents(test)
> 
> 
> Data frame:test 142 observations and 4 variables    Maximum # NAs:0
> 
>         Levels Storage
> age         23 integer
> yrclass     28 integer
> weight          double
> year            double
> 
> +--------+---------------------------------------------------------------------+
> |Variable|Levels                                                               |
> +--------+---------------------------------------------------------------------+
> | age    |0,1,10,11,12,13,14,15,16,18,19,2,20,21,24,25,3,4,5,6,7,8,9           |
> +--------+---------------------------------------------------------------------+
> | yrclass|1969,1974,1975,1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988|
> |        |1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000,2001,2002|
> +--------+---------------------------------------------------------------------+
> 
>>is(test$yrclass,"factor")
> 
> [1] TRUE
> 
>>is(test$yrclass,"numeric")
> 
> [1] FALSE
> 
>>as(test[,2],"numeric")
> 
>   [1]  1  1  2  3  3  4  5  6  6  6  7  7  8  8  8  8  9  9  9  9 10 10 10 10 10
>  [26] 10 10 11 11 11 11 11 12 12 12 12 12 12 12 13 13 13 13 13 13 13 14 14 14 14
>  [51] 14 14 14 14 15 15 15 15 15 15 15 15 15 16 16 16 16 16 16 16 16 16 17 17 17
>  [76] 17 17 17 17 17 17 18 18 18 18 18 18 18 18 18 18 19 19 19 19 19 19 19 19 20
> [101] 20 20 20 20 20 20 20 21 21 21 21 21 21 21 21 22 22 22 22 22 22 22 23 23 23
> [126] 23 23 23 24 24 24 24 24 25 25 25 25 26 26 27 27 28
> 
>>is(test$yrclass,"numeric")
> 
> [1] FALSE
> 
> 
> Regards,
> 
> Robert Brown
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



------------------------------

Message: 13
Date: Wed, 10 Nov 2004 14:27:07 +0100
From: Christian Schulz <ozric at web.de>
Subject: Re: [R] "conditional duplicates"?
To: Dimitris Rizopoulos <dimitris.rizopoulos at med.kuleuven.ac.be>
Cc: r-help at stat.math.ethz.ch
Message-ID: <4192172B.4010800 at web.de>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed

Many Thanks!

Dimitris Rizopoulos wrote:

> Hi Christian,
>
> may be this is helpful:
>
> dat <- data.frame(id=rep(1:4, each=5), smonth=sample(1:5, 20, TRUE))
> dat$duplicated <- unlist(tapply(dat$smonth, dat$id, duplicated))
> dat
>
> Best,
> Dimitris
>
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
>
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/16/396887
> Fax: +32/16/337015
> Web: http://www.med.kuleuven.ac.be/biostat/
>     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
>
>
> ----- Original Message ----- From: "Christian Schulz" <ozric at web.de>
> To: <r-help at stat.math.ethz.ch>
> Sent: Wednesday, November 10, 2004 1:52 PM
> Subject: [R] "conditional duplicates"?
>
>
>> Hi,
>>
>> i would like check a repeated measurement dataset, whether
>> cases (which (id) could be more than one time included and not all 
>> same times included.)
>> have more than one times the same SMONTH!?
>>
>> #This pseudo code didn't work with a for loop, because the [i+1] 
>> isn't known.
>> How i could refer to [i+1] ?
>>
>> if(ID[i] == ID[i+1] & SMONTH[i] !=[i+1])  {res[i] <- 0 }
>> if(ID[i] == ID[i+1] & SMONTH[i] == [i+1]) {res[i] <- 1 }
>>
>> many thanks,
>> christian
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



------------------------------

Message: 14
Date: Wed, 10 Nov 2004 09:39:09 -0500
From: "Andy Bunn" <abunn at whrc.org>
Subject: RE: [R] Does something like partition.rpart() exist?
To: "Simone Vantini" <vantini at mate.polimi.it>,
	<R-help at stat.math.ethz.ch>
Message-ID: <NEBBIPHDAMMOKDKPOFFIKEOGCLAA.abunn at whrc.org>
Content-Type: text/plain;	charset="iso-8859-1"

I might well be wrong, but I don't think there is. I went about rewriting
partition.tree for rpart once but stopped after I realized that it was much
easier to add lines and segments to plots by hand using the coordinates from
the rpart object (I then added a third predictor to my dataset making the
whole process pointless anyways.) Looking at the code for partition.tree and
its embedded function ptXlines gives you hints how to extract the relevant
bits of the tree to add to a plot.

I assume that you've seen this thread?

http://www.r-project.org/nocvs/mail/r-help/2002/0142.html

Sorry not to have been more help.

-Andy


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Simone Vantini
> Sent: Wednesday, November 10, 2004 5:27 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] Does something like partition.rpart() exist?
>
>
> I'd like to create a bidimensional presentation of a classification tree
> built using the rpart() function. I've seen that a partition.tree()
> function exists for the tree() function. Does a similar function  exist
> for the rpart() function?
> Thanks a lot Simone Vantini
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



------------------------------

Message: 15
Date: 10 Nov 2004 16:36:04 +0100
From: Peter Dalgaard <p.dalgaard at biostat.ku.dk>
Subject: Re: [R] R under Pocket PC
To: Barry Rowlingson <B.Rowlingson at lancaster.ac.uk>
Cc: r-help at stat.math.ethz.ch
Message-ID: <x2zn1piumz.fsf at biostat.ku.dk>
Content-Type: text/plain; charset=us-ascii

Barry Rowlingson <B.Rowlingson at lancaster.ac.uk> writes:

>     I'm sure everyone on this list feels you work hard enough on R as
> it is, dont let it invade your living room as well!

Too late. Laptop + WiFi + ADSL did that a while back.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



------------------------------

Message: 16
Date: Wed, 10 Nov 2004 16:38:05 +0100
From: <CYRIL.CAILLAULT at FORTISINVESTMENTS.COM>
Subject: [R] fSeries
To: "' (r-help at stat.math.ethz.ch)'" <r-help at stat.math.ethz.ch>
Message-ID:
	<E66B5035E7D16C4B89028BAE9AC268A50403644B at I04MB144.fr.fimgroup>
Content-Type: text/plain

Good morning everyone,

I use for the first time the package fSeries and i try to run the example
given by Diethelm W??rtz. But when i run its example which is the following 
#
# Example: 
#	Model a GARCH time series process 
#
# Description:
#	PART I: Estimate GARCH models of the following type ARCH(2) 
#     and GARCH(1,1) with normal conditional distribution functions.
#   PART II: Simulate GARCH models of the following type, ARCH(2) 
#     and GARCH(1,1),
#	with normal conditional distribution functions.
#
# Author:
#	(C) 2002, Diethelm Wuertz, GPL
#


############################################################################
####
# PART I: Estimation:

	# Settings:
	set.seed(547)
    # Bollerslev's GARCH(1,1) with normal innovations:
	model = list(omega = 1e-6, alpha = 0.1, beta = 0.8, mu = 0)
	x = garchSim(model, n = 1000)
	fit = garchFit(as.numeric(x), order = c(1, 1))
	print(fit)
	# Summary and Diagnostic Analysis:
	summary(fit)
	# Plot Results:
	par(mfrow = c(2, 2))
	plot(fit)
	###

Results of the estimations are false.

Call:
garchFit(x = as.numeric(x), order = c(1, 1))

Coefficient(s):
    omega         a1         b1  
8.564e-07  5.000e-02  5.000e-02  

To compare with : omega = 1e-6, alpha = 0.1, beta = 0.8.

Do you have some information about this?
Can I give some initials values to start the estimations?
Can I use different innovation process like student-t and GED

Thanks for your answers

Cyril 





	[[alternative HTML version deleted]]



------------------------------

Message: 17
Date: Wed, 10 Nov 2004 07:52:05 -0800 (PST)
From: Thomas Lumley <tlumley at u.washington.edu>
Subject: Re: [R] worked in R, but not in S-Plus
To: array chip <arrayprofile at yahoo.com>
Cc: r-help at stat.math.ethz.ch
Message-ID:
	<Pine.A41.4.61b.0411100748310.162604 at homer04.u.washington.edu>
Content-Type: TEXT/PLAIN; charset=US-ASCII; format=flowed

On Tue, 9 Nov 2004, array chip wrote:

> Hi,
>
> I wrote a function that worked well in R, but not in
> S-Plus, can anyone suggest a solution?
>

If you change the argument of the function from x to ... it will give
  Error in eval(expr, envir, enclos) : Object "x" not found
which is almost compatible with what it does in S-PLUS.

If you want advice on how to change the behaviour in S-PLUS, this is the 
wrong list.

>> f.coxph.zph<-function(x)
> {
> 	cox.fit <- coxph(Surv(time.cox, status.cox) ~ x,
> na.action = na.exclude, method = "breslow")
> 	fit.zph<-cox.zph(cox.fit,transform='log')
> 	fit.zph$table[,3]
> }


 	-thomas



------------------------------

Message: 18
Date: Wed, 10 Nov 2004 17:18:17 +0100 (CET)
From: Vito Ricci <vito_ricci at yahoo.com>
Subject: [R] fSeries
To: r-help at stat.math.ethz.ch
Cc: CYRIL.CAILLAULT at FORTISINVESTMENTS.COM
Message-ID: <20041110161817.82969.qmail at web41215.mail.yahoo.com>
Content-Type: text/plain; charset=iso-8859-1

Hi,

see ? garch in tseries package.

library(tseries)
> garch(x)

 ***** ESTIMATION WITH ANALYTICAL GRADIENT ***** 


Warning: singular information

Call:
garch(x = x)

Coefficient(s):
       a0         a1         b1  
8.564e-07  5.000e-02  5.000e-02  

Best
Vito


You wrote:

Good morning everyone,

I use for the first time the package fSeries and i try
to run the example
given by Diethelm W??rtz. But when i run its example
which is the following 
#
# Example: 
#	Model a GARCH time series process 
#
# Description:
#	PART I: Estimate GARCH models of the following type
ARCH(2) 
#     and GARCH(1,1) with normal conditional
distribution functions.
#   PART II: Simulate GARCH models of the following
type, ARCH(2) 
#     and GARCH(1,1),
#	with normal conditional distribution functions.
#
# Author:
#	(C) 2002, Diethelm Wuertz, GPL
#


############################################################################
####
# PART I: Estimation:

	# Settings:
	set.seed(547)
    # Bollerslev's GARCH(1,1) with normal innovations:
	model = list(omega = 1e-6, alpha = 0.1, beta = 0.8,
mu = 0)
	x = garchSim(model, n = 1000)
	fit = garchFit(as.numeric(x), order = c(1, 1))
	print(fit)
	# Summary and Diagnostic Analysis:
	summary(fit)
	# Plot Results:
	par(mfrow = c(2, 2))
	plot(fit)
	###

Results of the estimations are false.

Call:
garchFit(x = as.numeric(x), order = c(1, 1))

Coefficient(s):
    omega         a1         b1  
8.564e-07  5.000e-02  5.000e-02  

To compare with : omega = 1e-6, alpha = 0.1, beta =
0.8.

Do you have some information about this?
Can I give some initials values to start the
estimations?
Can I use different innovation process like student-t
and GED

Thanks for your answers

Cyril 







=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese http://www.modugno.it/archivio/cat_palese.shtml



------------------------------

Message: 19
Date: Wed, 10 Nov 2004 10:47:28 -0600
From: Deepayan Sarkar <deepayan at stat.wisc.edu>
Subject: Re: [R] lattice: ordering the entries in a dotplot of a
	vector
To: r-help at stat.math.ethz.ch
Cc: Wolfram Fischer <wolfram at fischer-zim.ch>
Message-ID: <200411101047.28801.deepayan at stat.wisc.edu>
Content-Type: text/plain;  charset="iso-8859-1"

On Wednesday 10 November 2004 05:28, Wolfram Fischer wrote:
> I tried:
>     n <- 9
>     x <- sample(n)
>     names(x) <- LETTERS[1:n]
>     dotplot( sort(x) )
>
> The lines of the dotplot are not ordered according to the values of x
> (but according to the names of x).
>
> So I did:
>
>     dfx <- data.frame( x=x, label=factor( names(x),
> names(x)[order(x)] ) ) dotplot( label ~ x, data=dfx )
>
> So I got what I wanted.
>
> My question: Is there an easier solution for doing that?

It should have been the default behaviour (according to S-PLUS). The 
next release should fix this.

Deepayan



------------------------------

Message: 20
Date: Wed, 10 Nov 2004 09:40:21 -0800 (PST)
From: Tomas Aragon <aragon at berkeley.edu>
Subject: [R] Building MacOSX binary in Windows XP
To: r-help at stat.math.ethz.ch
Message-ID: <20041110174021.26501.qmail at web80104.mail.yahoo.com>
Content-Type: text/plain; charset=us-ascii

Under Windows XP, I am able to build R package archived as source
(.tar.gz) and Windows binary (.zip). Can I create MaxOSX archive file
(.tgz) in Windows XP?

Thanks,
Tomas

====
Tomas Aragon
http://www.epitools.net



------------------------------

Message: 21
Date: Wed, 10 Nov 2004 18:09:36 +0000 (GMT)
From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
Subject: Re: [R] Building MacOSX binary in Windows XP
To: Tomas Aragon <aragon at berkeley.edu>
Cc: r-help at stat.math.ethz.ch
Message-ID: <Pine.LNX.4.44.0411101807020.30809-100000 at gannet.stats>
Content-Type: TEXT/PLAIN; charset=US-ASCII

On Wed, 10 Nov 2004, Tomas Aragon wrote:

> Under Windows XP, I am able to build R package archived as source
> (.tar.gz) and Windows binary (.zip). Can I create MaxOSX archive file
> (.tgz) in Windows XP?

If you mean a .tgz containing a binary MacOSX version of the package, not 
without a cross-compiler (which I very much doubt you have).

You can certainly create .tgz files, but the it's the contents that 
matter.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



------------------------------

Message: 22
Date: Wed, 10 Nov 2004 16:08:05 -0500 (EST)
From: Suzette Blanchard <suzette at sdac.harvard.edu>
Subject: [R] cubic spline/smoother with nlme
To: r-help at stat.math.ethz.ch
Message-ID:
	<Pine.GSO.4.40.0411101558200.17683-100000 at sdac.harvard.edu>
Content-Type: TEXT/PLAIN; charset=US-ASCII


Greetings,  I would like to use a cubic spline
or smoother to model the fixed effects within
nlme.  So far the only smoother I have been able
to get to run successfully in nlme is smooth().

I tried smooth.spline:
   fixed=list(lKa~1,lCL~smooth.spline(BSA, df=3))
   the error I got was the following.
   Error in model.frame(formula, rownames, variables, varnames, extras,
   extranames,  :  invalid variable type

Can anyone suggest a cubic spline that would work within
this context?

Thank you for any help you can send,
Suzette


=================================
Suzette Blanchard, Ph.D.
UCSD-PPRU



------------------------------

Message: 23
Date: 10 Nov 2004 22:53:58 +0100
From: Peter Dalgaard <p.dalgaard at biostat.ku.dk>
Subject: Re: [R] cubic spline/smoother with nlme
To: Suzette Blanchard <suzette at sdac.harvard.edu>
Cc: r-help at stat.math.ethz.ch
Message-ID: <x2fz3h9xqh.fsf at biostat.ku.dk>
Content-Type: text/plain; charset=us-ascii

Suzette Blanchard <suzette at sdac.harvard.edu> writes:

> Greetings,  I would like to use a cubic spline
> or smoother to model the fixed effects within
> nlme.  So far the only smoother I have been able
> to get to run successfully in nlme is smooth().
> 
> I tried smooth.spline:
>    fixed=list(lKa~1,lCL~smooth.spline(BSA, df=3))
>    the error I got was the following.
>    Error in model.frame(formula, rownames, variables, varnames, extras,
>    extranames,  :  invalid variable type
> 
> Can anyone suggest a cubic spline that would work within
> this context?

The fixed-knots ones (ns(), bs()) should work (and did so in at least
one case a couple of years ago...). These are linear, so lme() is used
rather than nlme(), unless of course you have other parts that need to
be modeled nonlinearly.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



------------------------------

Message: 24
Date: Wed, 10 Nov 2004 17:29:28 -0500
From: "Pikounis, Bill" <v_bill_pikounis at merck.com>
Subject: RE: [R] cubic spline/smoother with nlme
To: "Suzette Blanchard" <suzette at sdac.harvard.edu>
Cc: r-help at stat.math.ethz.ch
Message-ID:
	<CFBD404F5E0C9547B4E10B7BDC3DFA2F0B16072D at usrymx18.merck.com>
Content-Type: text/plain

Suzette,
In addition to Professor Daalgard's suggestions of ns() and bs(), you could
also try out rcs() from Frank Harrell Design package (you may need his Hmisc
package as well). This function helps to fit natural (restricted cubic
splines), and have been very useful for me in practice to use in tandem with
lme() in modeling longitudinal data.

Hope that helps,
Bill

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Peter Dalgaard
> Sent: Wednesday, November 10, 2004 4:54 PM
> To: Suzette Blanchard
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] cubic spline/smoother with nlme
> 
> 
> Suzette Blanchard <suzette at sdac.harvard.edu> writes:
> 
> > Greetings,  I would like to use a cubic spline
> > or smoother to model the fixed effects within
> > nlme.  So far the only smoother I have been able
> > to get to run successfully in nlme is smooth().
> > 
> > I tried smooth.spline:
> >    fixed=list(lKa~1,lCL~smooth.spline(BSA, df=3))
> >    the error I got was the following.
> >    Error in model.frame(formula, rownames, variables, 
> varnames, extras,
> >    extranames,  :  invalid variable type
> > 
> > Can anyone suggest a cubic spline that would work within
> > this context?
> 
> The fixed-knots ones (ns(), bs()) should work (and did so in at least
> one case a couple of years ago...). These are linear, so lme() is used
> rather than nlme(), unless of course you have other parts that need to
> be modeled nonlinearly.
> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: 
> (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: 
> (+45) 35327907
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



------------------------------

Message: 25
Date: Thu, 11 Nov 2004 07:55:17 +0800
From: Tom Mulholland <tmulholland at bigpond.com>
Subject: Re: [R] R under Pocket PC
To: r-help at stat.math.ethz.ch
Message-ID: <4192AA65.7090000 at bigpond.com>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed

What is really scary, is the thought that in some housholds this would 
be an improvement. Instead of the family being closeted away in their 
various rooms playing with their computer, they could all be sitting 
together, while at the same time ignoring each other, but occassionally 
engaging in royal (as in "The Royals") interaction and the odd cuppa.

For those not familiar with the TV series "The Royals" its a chunk of 
domesticity that has to be seen to be appreciated ( or disgusted 
depending upon your view of life.)

Tom Mulholland


Barry Rowlingson wrote:
> 
>> Could be kind of
>> fun to run R on it with display to the TV (but could you control R
>> with the TV remote control?)
> 
> 
>  Peter,
> 
>    I'm sure everyone on this list feels you work hard enough on R as it 
> is, dont let it invade your living room as well!
> 
> Baz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



------------------------------

Message: 26
Date: Thu, 11 Nov 2004 10:25:27 +1000
From: Peter Dunn <dunn at usq.edu.au>
Subject: [R] substitute/paste question for using Greek in plot titles
To: R-help mailing list <R-help at stat.math.ethz.ch>
Message-ID: <4192B177.5020109 at usq.edu.au>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed

Hi all

I am having troubles making sense of why code (1)
below fails but code (2) below works.

Code (1):

 > phi.1 <- 1
 > plot(0 ~ 0,
+ main=substitute(paste("A vaue for ",phi," = ",phival), 
list(phival=phi.1)) )

Error in paste("The two deviances for ", phi, " = ", 2) :
         Object "phi" not found

But this works:

Code (2):
 > plot(0,0,
+ main=substitute(paste("A value for ",phi," = ",phival), 
list(phival=phi.1)) )
 >

It appears that if the plot command takes the formula style entry,
the substitue/paste fails.

Is this documented as a feature (I couldn't find it if that is the
case), or is it a bug?  If it is a feature, it is a subtle difference
between (1) and (2) that has potential to be quite frustrating!

Perhaps I should just upgrade to version 2.0.0, though I can't see
anything in the Release Notes that might cause this.

Thanks.

P.

 > version
          _
platform i686-pc-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status
major    1
minor    9.1
year     2004
month    06
day      21
language R




-- 
Dr Peter Dunn          (USQ CRICOS No. 00244B)
   Web:    http://www.sci.usq.edu.au/staff/dunn
   Email:  dunn @ usq.edu.au
Opinions expressed are mine, not those of USQ.  Obviously...



------------------------------

Message: 27
Date: Wed, 10 Nov 2004 19:46:21 -0500
From: Jean Eid <jeaneid at chass.utoronto.ca>
Subject: [R] polr probit versus stata oprobit  
To: r-help at stat.math.ethz.ch
Message-ID:
	<Pine.SGI.4.40.0411101930330.52337307-100000 at origin.chass.utoronto.ca>
Content-Type: TEXT/PLAIN; charset=US-ASCII

Dear All,
I have been struggling to understand why for the housing data in MASS
library R and stata give coef. estimates that are really different. I also
tried to come up with many many examples myself (see below, of course I
did not have the set.seed command included) and all of my
`random' examples seem to give verry similar output. For the housing data,
I have changed the data into numeric vectors instead of factors/ordered
factors. I did so to try and get the same results as in STATA and to have
the housing example as close as possible to the one I constructed.

I run a debian sid, kernel 2.4, R 2.0.0, and STATA version 8.2, MASS
version  7.2-8.


here's the example ( I assume that you have STATA installed and can run in
batch mode, if not the output is also given below)

library(MASS)
library(foreign)
set.seed(100)
X <- rnorm(1000)
X1 <- rnorm(1000)
X2 <- rnorm(1000)
X <- X +X1+X2
XX <- X<=quantile(X, .25)
XX[X>quantile(X, .25) & X<=quantile(X, .50)] <- 2
XX[X>quantile(X, .5) & X<=quantile(X, .75)] <- 3
XX[X>quantile(X, .75)] <- 4
temp <- data.frame(XX=XX, X1=X1, X2=X2, X=X)
summary(polr(factor(XX)~X1 +X2, data=temp, method="probit"))
write.dta(temp, "temp.dta")
####################################
#Stata stuff
####################################
cat("use temp.dta\n oprobit XX X1 X2\n", file="temp.ado")
system("stata -b do temp.ado&")
system("cat temp.log")


#
##### here's R's output
#############################
Re-fitting to get Hessian

Call:
polr(formula = factor(XX) ~ X1 + X2, data = temp, method = "probit")

Coefficients:
       Value Std. Error  t value
X1 0.9891735 0.04749225 20.82811
X2 0.9400804 0.04527653 20.76309

Intercepts:
    Value    Std. Error t value
1|2  -1.1411   0.0572   -19.9613
2|3  -0.0372   0.0486    -0.7656
3|4   1.1101   0.0579    19.1865

Residual Deviance: 1969.734
AIC: 1979.734


##############################################
#and here  Stata's output
##############################################

Ordered probit estimates                          Number of obs   =
1000
                                                  LR chi2(2)      =
802.86
                                                  Prob > chi2     =
0.0000
Log likelihood = -984.86675                       Pseudo R2       =
0.2896

------------------------------------------------------------------------------
          XX |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
          X1 |   .9891651   .0474922    20.83   0.000     .8960822    1.082248
          X2 |     .94007   .0452765    20.76   0.000     .8513298     1.02881
-------------+----------------------------------------------------------------
       _cut1 |  -1.141119   .0571667          (Ancillary parameters)
       _cut2 |  -.0371779   .0485592
       _cut3 |   1.110117   .0578593



------------------------------

Message: 28
Date: Wed, 10 Nov 2004 18:40:43 -0600
From: "Bizuet Cabrera Fernando" <fbizuet at banxico.org.mx>
Subject: [R] R with Sun Studio Fortran 95 compiler
To: <r-help at stat.math.ethz.ch>
Message-ID:
	<F74A1EABDCCFFB4893FD93C96408F58A021A53B8 at BMCORREO.banxico.org.mx>
Content-Type: text/plain;	charset="us-ascii"

Hi,

I am trying to compile R 1.9.1 and 2.0.0 on Solaris 2.8 with Sun Studio
Fortran 95 compiler but I have obtained some errors which I enclose. I
have compiled R with g77 GNU compiler and it was OK.

Does anyone know if R 2.0.0 has been successfully built on Solaris 2.8
with Sun Studio Fortran 95 compiler? or at least, How could I achieve
this? The R installation manual does not have any reference.

I want Sun Fortran in order to use the Sun performance library
libsunperf since I noticed that R on Windows XP is twice faster than R
on Sun Solaris 2.8 without library libsunperf.

Thanks in advance.

$ ./configure --enable-R-shlib
checking for a BSD-compatible install... tools/install-sh -c
checking whether build environment is sane... yes
checking whether make sets $(MAKE)... yes

... Skip

checking build system type... sparc-sun-solaris2.8
checking host system type... sparc-sun-solaris2.8
loading site script './config.site'

...skip

checking for gcc... gcc
checking for C compiler default output file name... a.out
checking whether the C compiler works... yes
checking whether we are cross compiling... no
checking for suffix of executables... 
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ANSI C... none needed
checking how to run the C preprocessor... gcc -E
checking whether gcc needs -traditional... no
checking how to run the C preprocessor... gcc -E
defining F77 to be f95
checking whether we are using the GNU Fortran 77 compiler... no
checking whether f95 accepts -g... yes
checking for g++... g++
checking whether we are using the GNU C++ compiler... yes
checking whether g++ accepts -g... yes

... skip

checking for f95 option to produce PIC... -fPIC
checking if f95 PIC flag -fPIC works... no
checking if f95 supports -c -o file.o... yes
checking whether the f95 linker (/usr/ccs/bin/ld) supports shared
libraries... yes
checking dynamic linker characteristics... f95: Warning: Option
-print-search-dirs passed to ld, if ld is invoked, ignored otherwise
Usage: f95 [ options ] files.  Use 'f95 -flags' for details
solaris2.8 ld.so
checking how to hardcode library paths into programs... immediate

...skip

checking how to get verbose linking output from f95... -v
checking for Fortran libraries of f95...  -L/usr/local/lib
-R/opt/SUNWspro/lib/v8plus:/opt/SUNWspro/lib -L/opt/SUNWspro/lib/v8plus
-L/opt/SUNWspro/prod/lib/v8plus -L/opt/SUNWspro/lib
-L/opt/SUNWspro/prod/lib -L/usr/ccs/lib -L/lib -L/usr/lib -lompstubs
-lfui -lfai -lfai2 -lfsumai -lfprodai -lfminlai -lfmaxlai -lfminvai
-lfmaxvai -lfsu -lsunmath -lm
checking for dummy main to link with Fortran libraries... unknown
configure: error: linking to Fortran libraries from C fails
See `config.log' for more details.



------------------------------

Message: 29
Date: Wed, 10 Nov 2004 16:43:41 -0800
From: Sundar Dorai-Raj <sundar.dorai-raj at pdf.com>
Subject: Re: [R] substitute/paste question for using Greek in plot
	titles
To: Peter Dunn <dunn at usq.edu.au>
Cc: R-help mailing list <R-help at stat.math.ethz.ch>
Message-ID: <4192B5BD.4070103 at pdf.com>
Content-Type: text/plain; charset=us-ascii; format=flowed



Peter Dunn wrote:

> Hi all
> 
> I am having troubles making sense of why code (1)
> below fails but code (2) below works.
> 
> Code (1):
> 
>  > phi.1 <- 1
>  > plot(0 ~ 0,
> + main=substitute(paste("A vaue for ",phi," = ",phival), 
> list(phival=phi.1)) )
> 
> Error in paste("The two deviances for ", phi, " = ", 2) :
>         Object "phi" not found
> 
> But this works:
> 
> Code (2):
>  > plot(0,0,
> + main=substitute(paste("A value for ",phi," = ",phival), 
> list(phival=phi.1)) )
>  >
> 
> It appears that if the plot command takes the formula style entry,
> the substitue/paste fails.
> 
> Is this documented as a feature (I couldn't find it if that is the
> case), or is it a bug?  If it is a feature, it is a subtle difference
> between (1) and (2) that has potential to be quite frustrating!
> 
> Perhaps I should just upgrade to version 2.0.0, though I can't see
> anything in the Release Notes that might cause this.
> 
> Thanks.
> 
> P.
> 
>  > version
>          _
> platform i686-pc-linux-gnu
> arch     i686
> os       linux-gnu
> system   i686, linux-gnu
> status
> major    1
> minor    9.1
> year     2004
> month    06
> day      21
> language R
> 
> 

Peter,

Because in the first couple of lines of plot.formula we see this:

dots <- m$...
dots <- lapply(dots, eval, data, parent.frame())

which for your case is equivalent to:

expr <- substitute(paste("A vaue for ",phi," = ",phival), 
list(phival=phi.1))
eval(expr)

which returns an error saying "phi" cannot be found which is the correct 
behaviour of eval. I'll let others comment on whether or not this is a 
bug in plot.formula but you can always get around it by calling title:

plot(0 ~ 0)
title(main = expr)

which is exactly what your second example is doing in plot.default.

HTH,

--sundar



------------------------------

Message: 30
Date: Wed, 10 Nov 2004 17:15:26 -0800 (PST)
From: Thomas Lumley <tlumley at u.washington.edu>
Subject: Re: [R] polr probit versus stata oprobit  
To: Jean Eid <jeaneid at chass.utoronto.ca>
Cc: r-help at stat.math.ethz.ch
Message-ID:
	<Pine.A41.4.61b.0411101707400.230306 at homer11.u.washington.edu>
Content-Type: TEXT/PLAIN; charset=US-ASCII; format=flowed

On Wed, 10 Nov 2004, Jean Eid wrote:

> Dear All,
> I have been struggling to understand why for the housing data in MASS
> library R and stata give coef. estimates that are really different. I also
> tried to come up with many many examples myself (see below, of course I
> did not have the set.seed command included) and all of my
> `random' examples seem to give verry similar output. For the housing data,
> I have changed the data into numeric vectors instead of factors/ordered
> factors. I did so to try and get the same results as in STATA and to have
> the housing example as close as possible to the one I constructed.
>
> I run a debian sid, kernel 2.4, R 2.0.0, and STATA version 8.2, MASS
> version  7.2-8.
>
>
> here's the example ( I assume that you have STATA installed and can run in
> batch mode, if not the output is also given below)
>

That example shows the same results with Stata and polr() from MASS.

For the housing data, I also get the same coefficients in Stata as with 
polr():

In R:
library(MASS)
library(foreign)
write.dta(housing, file="housing.dta")
house.probit<-polr(Sat ~ Infl + Type + Cont, data = housing, weights = 
Freq, method = "probit")
summary(house.probit)
-------------------------
Re-fitting to get Hessian

Call:
polr(formula = Sat ~ Infl + Type + Cont, data = housing, weights = Freq,
     method = "probit")

Coefficients:
                    Value Std. Error   t value
InflMedium     0.3464233 0.06413706  5.401297
InflHigh       0.7829149 0.07642620 10.244063
TypeApartment -0.3475372 0.07229093 -4.807480
TypeAtrium    -0.2178874 0.09476607 -2.299213
TypeTerrace   -0.6641737 0.09180004 -7.235005
ContHigh       0.2223862 0.05812267  3.826153

Intercepts:
             Value   Std. Error t value
Low|Medium  -0.2998  0.0762    -3.9371
Medium|High  0.4267  0.0764     5.5850

Residual Deviance: 3479.689
AIC: 3495.689
------------------------


In Stata
-----------------
. use housing.dta
. xi: oprobit Sat i.Infl i.Type i.Cont [fw=Freq]
i.Infl            _IInfl_1-3          (naturally coded; _IInfl_1 omitted)
i.Type            _IType_1-4          (naturally coded; _IType_1 omitted)
i.Cont            _ICont_1-2          (naturally coded; _ICont_1 omitted)

Iteration 0:   log likelihood = -1824.4388
Iteration 1:   log likelihood = -1739.9254
Iteration 2:   log likelihood = -1739.8444

Ordered probit estimates                          Number of obs   =    1681
                                                   LR chi2(6)      =   169.19
                                                   Prob > chi2     =   0.0000
Log likelihood = -1739.8444                       Pseudo R2       =   0.0464

------------------------------------------------------------------------------
          Sat |      Coef.   Std. Err.      z    P>|z|     [95% Conf. 
Interval]
-------------+----------------------------------------------------------------
     _IInfl_2 |   .3464228    .064137     5.40   0.000     .2207165     .472129
     _IInfl_3 |   .7829146    .076426    10.24   0.000     .6331224    .9327069
     _IType_2 |  -.3475367   .0722908    -4.81   0.000    -.4892241   -.2058493
     _IType_3 |  -.2178875    .094766    -2.30   0.021    -.4036254   -.0321497
     _IType_4 |  -.6641735   .0917999    -7.24   0.000     -.844098    -.484249
     _ICont_2 |   .2223858   .0581226     3.83   0.000     .1084676     .336304
-------------+----------------------------------------------------------------
        _cut1 |  -.2998279   .0761537          (Ancillary parameters)
        _cut2 |   .4267208   .0764043
------------------------------------------------------------------------------



 	-thomas



------------------------------

Message: 31
Date: Wed, 10 Nov 2004 21:09:21 -0500
From: Jean Eid <jeaneid at chass.utoronto.ca>
Subject: Re: [R] polr probit versus stata oprobit  
To: Thomas Lumley <tlumley at u.washington.edu>
Cc: r-help at stat.math.ethz.ch
Message-ID:
	<Pine.SGI.4.40.0411102108010.52139612-100000 at origin.chass.utoronto.ca>
Content-Type: TEXT/PLAIN; charset=US-ASCII

Dear Thomas,

Where you also able to replicate the second example?  (the exaample
that I turned the housing data into numerical variables) That is the one
that my estimates differ.

Jean,

On Wed, 10 Nov 2004, Thomas Lumley wrote:

> On Wed, 10 Nov 2004, Jean Eid wrote:
>
> > Dear All,
> > I have been struggling to understand why for the housing data in MASS
> > library R and stata give coef. estimates that are really different. I also
> > tried to come up with many many examples myself (see below, of course I
> > did not have the set.seed command included) and all of my
> > `random' examples seem to give verry similar output. For the housing data,
> > I have changed the data into numeric vectors instead of factors/ordered
> > factors. I did so to try and get the same results as in STATA and to have
> > the housing example as close as possible to the one I constructed.
> >
> > I run a debian sid, kernel 2.4, R 2.0.0, and STATA version 8.2, MASS
> > version  7.2-8.
> >
> >
> > here's the example ( I assume that you have STATA installed and can run in
> > batch mode, if not the output is also given below)
> >
>
> That example shows the same results with Stata and polr() from MASS.
>
> For the housing data, I also get the same coefficients in Stata as with
> polr():
>
> In R:
> library(MASS)
> library(foreign)
> write.dta(housing, file="housing.dta")
> house.probit<-polr(Sat ~ Infl + Type + Cont, data = housing, weights =
> Freq, method = "probit")
> summary(house.probit)
> -------------------------
> Re-fitting to get Hessian
>
> Call:
> polr(formula = Sat ~ Infl + Type + Cont, data = housing, weights = Freq,
>      method = "probit")
>
> Coefficients:
>                     Value Std. Error   t value
> InflMedium     0.3464233 0.06413706  5.401297
> InflHigh       0.7829149 0.07642620 10.244063
> TypeApartment -0.3475372 0.07229093 -4.807480
> TypeAtrium    -0.2178874 0.09476607 -2.299213
> TypeTerrace   -0.6641737 0.09180004 -7.235005
> ContHigh       0.2223862 0.05812267  3.826153
>
> Intercepts:
>              Value   Std. Error t value
> Low|Medium  -0.2998  0.0762    -3.9371
> Medium|High  0.4267  0.0764     5.5850
>
> Residual Deviance: 3479.689
> AIC: 3495.689
> ------------------------
>
>
> In Stata
> -----------------
> . use housing.dta
> . xi: oprobit Sat i.Infl i.Type i.Cont [fw=Freq]
> i.Infl            _IInfl_1-3          (naturally coded; _IInfl_1 omitted)
> i.Type            _IType_1-4          (naturally coded; _IType_1 omitted)
> i.Cont            _ICont_1-2          (naturally coded; _ICont_1 omitted)
>
> Iteration 0:   log likelihood = -1824.4388
> Iteration 1:   log likelihood = -1739.9254
> Iteration 2:   log likelihood = -1739.8444
>
> Ordered probit estimates                          Number of obs   =    1681
>                                                    LR chi2(6)      =   169.19
>                                                    Prob > chi2     =   0.0000
> Log likelihood = -1739.8444                       Pseudo R2       =   0.0464
>
> ------------------------------------------------------------------------------
>           Sat |      Coef.   Std. Err.      z    P>|z|     [95% Conf.
> Interval]
> -------------+----------------------------------------------------------------
>      _IInfl_2 |   .3464228    .064137     5.40   0.000     .2207165     .472129
>      _IInfl_3 |   .7829146    .076426    10.24   0.000     .6331224    .9327069
>      _IType_2 |  -.3475367   .0722908    -4.81   0.000    -.4892241   -.2058493
>      _IType_3 |  -.2178875    .094766    -2.30   0.021    -.4036254   -.0321497
>      _IType_4 |  -.6641735   .0917999    -7.24   0.000     -.844098    -.484249
>      _ICont_2 |   .2223858   .0581226     3.83   0.000     .1084676     .336304
> -------------+----------------------------------------------------------------
>         _cut1 |  -.2998279   .0761537          (Ancillary parameters)
>         _cut2 |   .4267208   .0764043
> ------------------------------------------------------------------------------
>
>
>
>  	-thomas
>



------------------------------

Message: 32
Date: Wed, 10 Nov 2004 23:08:23 -0500
From: "Roger D. Peng" <rpeng at jhsph.edu>
Subject: Re: [R] substitute/paste question for using Greek in plot
	titles
To: Sundar Dorai-Raj <sundar.dorai-raj at pdf.com>
Cc: Peter Dunn <dunn at usq.edu.au>,	R-help mailing list
	<R-help at stat.math.ethz.ch>
Message-ID: <4192E5B7.3050203 at jhsph.edu>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed

This is not a bug.  The object passed to `main' should be either character or an 
expression (documented in ?title).  substitute() returns neither a character 
object nor an expression -- it return a call object.  This is documented in 
?substitute:

      Substituting and quoting often causes confusion when the argument
      is 'expression(...)'. The result is a call to the 'expression'
      constructor function and needs to be evaluated with 'eval' to give
      the actual expression object.

What you want, I think, is

label <- substitute(expression(paste("A vaue for ",phi," = ",phival)), 
list(phival=phi.1))
plot(0 ~ 0, main = eval(label))

-roger

Sundar Dorai-Raj wrote:
> 
> 
> Peter Dunn wrote:
> 
>> Hi all
>>
>> I am having troubles making sense of why code (1)
>> below fails but code (2) below works.
>>
>> Code (1):
>>
>>  > phi.1 <- 1
>>  > plot(0 ~ 0,
>> + main=substitute(paste("A vaue for ",phi," = ",phival), 
>> list(phival=phi.1)) )
>>
>> Error in paste("The two deviances for ", phi, " = ", 2) :
>>         Object "phi" not found
>>
>> But this works:
>>
>> Code (2):
>>  > plot(0,0,
>> + main=substitute(paste("A value for ",phi," = ",phival), 
>> list(phival=phi.1)) )
>>  >
>>
>> It appears that if the plot command takes the formula style entry,
>> the substitue/paste fails.
>>
>> Is this documented as a feature (I couldn't find it if that is the
>> case), or is it a bug?  If it is a feature, it is a subtle difference
>> between (1) and (2) that has potential to be quite frustrating!
>>
>> Perhaps I should just upgrade to version 2.0.0, though I can't see
>> anything in the Release Notes that might cause this.
>>
>> Thanks.
>>
>> P.
>>
>>  > version
>>          _
>> platform i686-pc-linux-gnu
>> arch     i686
>> os       linux-gnu
>> system   i686, linux-gnu
>> status
>> major    1
>> minor    9.1
>> year     2004
>> month    06
>> day      21
>> language R
>>
>>
> 
> Peter,
> 
> Because in the first couple of lines of plot.formula we see this:
> 
> dots <- m$...
> dots <- lapply(dots, eval, data, parent.frame())
> 
> which for your case is equivalent to:
> 
> expr <- substitute(paste("A vaue for ",phi," = ",phival), 
> list(phival=phi.1))
> eval(expr)
> 
> which returns an error saying "phi" cannot be found which is the correct 
> behaviour of eval. I'll let others comment on whether or not this is a 
> bug in plot.formula but you can always get around it by calling title:
> 
> plot(0 ~ 0)
> title(main = expr)
> 
> which is exactly what your second example is doing in plot.default.
> 
> HTH,
> 
> --sundar
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



------------------------------

Message: 33
Date: Wed, 10 Nov 2004 20:20:01 -0800
From: Gang Liang <liang at ics.uci.edu>
Subject: [R] "<<-" assignment no long work in class methods
To: r-help at stat.math.ethz.ch
Message-ID: <20041111042001.GL11620 at opensrc>
Content-Type: text/plain; charset=us-ascii

Hi-

I used to use "<<-" to do assignment inside a class method, and
just found that now it is broken in R 2.0. For example, the
following code

-----------------------------------------------------------------------
setClass( "myclass", representation(x="numeric") )
setGeneric("incrXByOne", function(obj) standardGeneric("incrXByOne"))
setMethod( "incrXByOne", "myclass", function(obj) obj at x <<- obj at x + 1 )

incrXByOne( new("myclass") )
-----------------------------------------------------------------------

will give an error message like:

   Error in incrXByOne(new("myclass")) : Object "obj" not found
   ## R failed to trace the object back to the GlobalEnv...

It used to work under R1.7 - 1.9. I don't know whether this is a
bug or a new feature...

Anyone can recommend a workaround?

Thanks, Gang

-----------------------------
debian unstable, kernel 2.6.8

> version

platform i386-pc-linux-gnu
arch     i386
os       linux-gnu
system   i386, linux-gnu
status
major    2
minor    0.0
year     2004
month    10
day      04
language R



------------------------------

Message: 34
Date: Thu, 11 Nov 2004 08:30:25 +0100
From: Witold Eryk Wolski <wolski at molgen.mpg.de>
Subject: Re: [R] "<<-" assignment no long work in class methods
To: Gang Liang <liang at ics.uci.edu>
Cc: r-help at stat.math.ethz.ch
Message-ID: <41931511.7080809 at molgen.mpg.de>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed

Gang Liang wrote:

>Hi-
>
>I used to use "<<-" to do assignment inside a class method, and
>just found that now it is broken in R 2.0. For example, the
>following code
>
>-----------------------------------------------------------------------
>setClass( "myclass", representation(x="numeric") )
>setGeneric("incrXByOne", function(obj) standardGeneric("incrXByOne"))
>setMethod( "incrXByOne", "myclass", function(obj) obj at x <<- obj at x + 1 )
>
>  
>
Hi,

What (my guess) you want to define is:

setMethod( "incrXByOne", "myclass", function(obj) {obj at x <- obj at x + 1;obj})

You do not need "<<-" to assign to a function argument? Its in my view 
even erroneous.

This S code gives an error too.

test <- function(x)
{
x$bla<<-x$bla + 1
}
test(1)
Error in test(1) : Object "x" not found

/E

>incrXByOne( new("myclass") )
>-----------------------------------------------------------------------
>
>will give an error message like:
>
>   Error in incrXByOne(new("myclass")) : Object "obj" not found
>   ## R failed to trace the object back to the GlobalEnv...
>
>It used to work under R1.7 - 1.9. I don't know whether this is a
>bug or a new feature...
>
>Anyone can recommend a workaround?
>
>Thanks, Gang
>
>-----------------------------
>debian unstable, kernel 2.6.8
>
>  
>
>>version
>>    
>>
>
>platform i386-pc-linux-gnu
>arch     i386
>os       linux-gnu
>system   i386, linux-gnu
>status
>major    2
>minor    0.0
>year     2004
>month    10
>day      04
>language R
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>


-- 
Dipl. bio-chem. Witold Eryk Wolski
MPI-Moleculare Genetic
Ihnestrasse 63-73 14195 Berlin
tel: 0049-30-83875219                 __("<    _
http://www.molgen.mpg.de/~wolski      \__/    'v'
http://r4proteomics.sourceforge.net    ||    /   \
mail: witek96 at users.sourceforge.net    ^^     m m
      wolski at molgen.mpg.de



------------------------------

Message: 35
Date: Thu, 11 Nov 2004 07:32:30 +0000 (GMT)
From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
Subject: Re: [R] R with Sun Studio Fortran 95 compiler
To: Bizuet Cabrera Fernando <fbizuet at banxico.org.mx>
Cc: r-help at stat.math.ethz.ch
Message-ID: <Pine.LNX.4.44.0411110723220.9124-100000 at gannet.stats>
Content-Type: TEXT/PLAIN; charset=US-ASCII

On Wed, 10 Nov 2004, Bizuet Cabrera Fernando wrote:

> I am trying to compile R 1.9.1 and 2.0.0 on Solaris 2.8 with Sun Studio
> Fortran 95 compiler but I have obtained some errors which I enclose. I
> have compiled R with g77 GNU compiler and it was OK.

Please do read the final line of your excerpt and do as it says ....

> Does anyone know if R 2.0.0 has been successfully built on Solaris 2.8
> with Sun Studio Fortran 95 compiler? or at least, How could I achieve
> this? The R installation manual does not have any reference.

What is `Sun Studio'?  The R-admin manual does have references to what I
think is the same compiler, `Sun ONE Studio 7 Compiler Suite' (aka Forte
7) (possibly not the same version).  Forte 9 has also been used 
successfully, if -xopenmp=stubs was added to LDFLAGS (and that is in the 
R-admin manual for the imminent R 2.0.1).

> I want Sun Fortran in order to use the Sun performance library
> libsunperf since I noticed that R on Windows XP is twice faster than R
> on Sun Solaris 2.8 without library libsunperf.

Yes, well, not at the same clock speed.  Please do read the experience in 
the R-admin manual that you deny exists.

Either you are talking about a different product with a very similar name 
or you have missed a lot of useful information.

> Thanks in advance.
> 
> $ ./configure --enable-R-shlib
> checking for a BSD-compatible install... tools/install-sh -c
> checking whether build environment is sane... yes
> checking whether make sets $(MAKE)... yes
> 
> ... Skip
> 
> checking build system type... sparc-sun-solaris2.8
> checking host system type... sparc-sun-solaris2.8
> loading site script './config.site'
> 
> ...skip
> 
> checking for gcc... gcc
> checking for C compiler default output file name... a.out
> checking whether the C compiler works... yes
> checking whether we are cross compiling... no
> checking for suffix of executables... 
> checking for suffix of object files... o
> checking whether we are using the GNU C compiler... yes
> checking whether gcc accepts -g... yes
> checking for gcc option to accept ANSI C... none needed
> checking how to run the C preprocessor... gcc -E
> checking whether gcc needs -traditional... no
> checking how to run the C preprocessor... gcc -E
> defining F77 to be f95
> checking whether we are using the GNU Fortran 77 compiler... no
> checking whether f95 accepts -g... yes
> checking for g++... g++
> checking whether we are using the GNU C++ compiler... yes
> checking whether g++ accepts -g... yes
> 
> ... skip
> 
> checking for f95 option to produce PIC... -fPIC
> checking if f95 PIC flag -fPIC works... no
> checking if f95 supports -c -o file.o... yes
> checking whether the f95 linker (/usr/ccs/bin/ld) supports shared
> libraries... yes
> checking dynamic linker characteristics... f95: Warning: Option
> -print-search-dirs passed to ld, if ld is invoked, ignored otherwise
> Usage: f95 [ options ] files.  Use 'f95 -flags' for details
> solaris2.8 ld.so
> checking how to hardcode library paths into programs... immediate
> 
> ...skip
> 
> checking how to get verbose linking output from f95... -v
> checking for Fortran libraries of f95...  -L/usr/local/lib
> -R/opt/SUNWspro/lib/v8plus:/opt/SUNWspro/lib -L/opt/SUNWspro/lib/v8plus
> -L/opt/SUNWspro/prod/lib/v8plus -L/opt/SUNWspro/lib
> -L/opt/SUNWspro/prod/lib -L/usr/ccs/lib -L/lib -L/usr/lib -lompstubs
> -lfui -lfai -lfai2 -lfsumai -lfprodai -lfminlai -lfmaxlai -lfminvai
> -lfmaxvai -lfsu -lsunmath -lm
> checking for dummy main to link with Fortran libraries... unknown
> configure: error: linking to Fortran libraries from C fails
> See `config.log' for more details.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



------------------------------

Message: 36
Date: Thu, 11 Nov 2004 08:27:51 +0000
From: Diethelm Wuertz <wuertz at itp.phys.ethz.ch>
Subject: Re: [R] fSeries
To: CYRIL.CAILLAULT at FORTISINVESTMENTS.COM, r-help at stat.math.ethz.ch,
	vito_ricci at yahoo.com
Message-ID: <41932287.8020000 at itp.phys.ethz.ch>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed


1. Currently I'm writing a complete new GARCH package, and as long as it 
is not
yet ready I have added the garch function from Adrian Trapletti's 
tseries package
to the fSeries package.

2. But the error in the output you  observed  comes not from the "garch" 
function,
it comes from my "garchSim" function. Please modify the function 
garchSim() in the
following way ----


garchSim =
function(model = list(omega = 1.0e-06, alpha = 0.1, beta = 0.8, mu = 0),
n = 100, innov = NULL, n.start = 100, start.innov = NULL, rand.gen = 
rnorm, ...)
{    
    # Doesn't work, replace the three following lines ... 
    # if (!exists("model$alpha")) model$alpha = 0
    # if (!exists("model$beta")) model$beta = 0
    # if (!exists("model$mu")) model$mu = 0
   
    # with ...
    if (is.null(model$alpha)) model$alpha = 0
    if (is.null(model$beta)) model$beta = 0
    if (is.null(model$mu)) model$mu = 0

    max.order = max(length(model$alpha), length(model$beta))
    if (n.start < max.order)
        stop("n.start must be greater or equal max(alpha, beta)")
    if (is.null(start.innov))
        start.innov = rand.gen(n.start, ...)
    if (is.null(innov))
        innov = rand.gen(n, ...)
    h = x = z = c(start.innov, innov)
    for (i in 1:max.order) {
        h[i] = model$omega/(1 - sum(model$alpha) - sum(model$beta))
        x[i] = sqrt(h[i]) * z[i] + model$mu
    }
    n.alpha = length(model$alpha)
    n.beta = length(model$beta)
    for (i in (max.order + 1):(n.start + n)) {
        h[i] = model$omega + sum(model$alpha * x[i - (1:n.alpha)]^2) +
            sum(model$beta * h[i - (1:n.beta)])
        x[i] = sqrt(h[i]) * z[i] + model$mu
    }
    as.ts(x[-(1:n.start)])
}


and try the folloowing examples:


require(fSeries)

garchFit(garchSim(n = 1000))

garchFit(garchSim(model = list(omega = 1.0e-06, alpha = 0.1, beta = 0.8,
    mu = 0), n =1000))

garchFit(garchSim(model = list(omega = 1.0e-06, alpha = 0.6), n = 1000))
garchFit(garchSim(model = list(omega = 1.0e-06, alpha = 0.6), n = 1000),
    order=c(0, 1))
   

The code will be updated in the next fSeries package.
I apologize for any inconvenience caused by this bug.

Diethelm Wuertz



CYRIL.CAILLAULT at FORTISINVESTMENTS.COM wrote:

>Good morning everyone,
>
>I use for the first time the package fSeries and i try to run the example
>given by Diethelm W??rtz. But when i run its example which is the following 
>#
># Example: 
>#	Model a GARCH time series process 
>#
># Description:
>#	PART I: Estimate GARCH models of the following type ARCH(2) 
>#     and GARCH(1,1) with normal conditional distribution functions.
>#   PART II: Simulate GARCH models of the following type, ARCH(2) 
>#     and GARCH(1,1),
>#	with normal conditional distribution functions.
>#
># Author:
>#	(C) 2002, Diethelm Wuertz, GPL
>#
>
>
>############################################################################
>####
># PART I: Estimation:
>
>	# Settings:
>	set.seed(547)
>    # Bollerslev's GARCH(1,1) with normal innovations:
>	model = list(omega = 1e-6, alpha = 0.1, beta = 0.8, mu = 0)
>	x = garchSim(model, n = 1000)
>	fit = garchFit(as.numeric(x), order = c(1, 1))
>	print(fit)
>	# Summary and Diagnostic Analysis:
>	summary(fit)
>	# Plot Results:
>	par(mfrow = c(2, 2))
>	plot(fit)
>	###
>
>Results of the estimations are false.
>
>Call:
>garchFit(x = as.numeric(x), order = c(1, 1))
>
>Coefficient(s):
>    omega         a1         b1  
>8.564e-07  5.000e-02  5.000e-02  
>
>To compare with : omega = 1e-6, alpha = 0.1, beta = 0.8.
>
>Do you have some information about this?
>Can I give some initials values to start the estimations?
>Can I use different innovation process like student-t and GED
>
>Thanks for your answers
>
>Cyril 
>
>
>
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



------------------------------

Message: 37
Date: Thu, 11 Nov 2004 08:42:38 +0000
From: Diethelm Wuertz <wuertz at itp.phys.ethz.ch>
Subject: Re: [R] fSeries
To: Diethelm Wuertz <wuertz at itp.phys.ethz.ch>,
	r-help at stat.math.ethz.ch
Message-ID: <419325FE.5010906 at itp.phys.ethz.ch>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed

Diethelm Wuertz wrote:

Tabs removed from the code, makes a nicer printout ....

garchSim =
function(model = list(omega = 1.0e-06, alpha = 0.1, beta = 0.8, mu = 0),
n = 100, innov = NULL, n.start = 100, start.innov = NULL, rand.gen = 
rnorm, ...)
{    
    # Doesn't work, replace the three following three lines ... 
    # if (!exists("model$alpha")) model$alpha = 0
    # if (!exists("model$beta")) model$beta = 0
    # if (!exists("model$mu")) model$mu = 0
   
    # with ...
    if (is.null(model$alpha)) model$alpha = 0
    if (is.null(model$beta)) model$beta = 0
    if (is.null(model$mu)) model$mu = 0

    max.order = max(length(model$alpha), length(model$beta))
    if (n.start < max.order)
        stop("n.start must be greater or equal max(alpha,beta)")
    if (is.null(start.innov))
        start.innov = rand.gen(n.start, ...)
    if (is.null(innov))
        innov = rand.gen(n, ...)
    h = x = z = c(start.innov, innov)
    for (i in 1:max.order) {
        h[i] = model$omega/(1 - sum(model$alpha) - sum(model$beta))
        x[i] = sqrt(h[i]) * z[i] + model$mu
    }
    n.alpha = length(model$alpha)
    n.beta = length(model$beta)
    for (i in (max.order + 1):(n.start + n)) {
        h[i] = model$omega + sum(model$alpha * x[i - (1:n.alpha)]^2) +
            sum(model$beta * h[i - (1:n.beta)])
        x[i] = sqrt(h[i]) * z[i] + model$mu
    }
    as.ts(x[-(1:n.start)])
}



------------------------------

Message: 38
Date: Thu, 11 Nov 2004 10:29:58 +0100 (CET)
From: "Alicia Amadoz"<Alicia.Amadoz at uv.es>
Subject: [R] RSPerl problem with testing
To: r-help at stat.math.ethz.ch
Message-ID: <6217119899amadoz at uv.es>
Content-Type: text/plain;	charset="ISO-8859-1"

Hi,

I'm trying to install de RSPerl module and i have some problems trying
to test it. I've tried to contact with the author but the e-mail address
seems not to exist. Hope that anyone in this list could explain me a
little about this problem. I have a bash shell and what i do is the
following:

#PERLLIB=/usr/lib/R/library/RSPerl/share/blib/arch:/usr/lib/R/library/RSPerl/share/blib/lib
# export PERLLIB
# LD_LIBRARY_PATH=/usr/lib/R/bin:/usr/lib/R/library/RSPerl/libs
# export LD_LIBRARY_PATH
# perl test.pl
1..1
Can't load '/usr/lib/R/library/RSPerl/share/blib/arch/auto/R/R.so' for
module R: /usr/lib/R/library/RSPerl/libs/libPerlConverter.so: undefined
symbol: R_GlobalEnv at
/usr/lib/perl5/5.8.3/i386-linux-thread-multi/DynaLoader.pm line 229.
 at test.pl line 11
Compilation failed in require at test.pl line 11.
BEGIN failed--compilation aborted at test.pl line 11.
not ok 1

I move to the /usr/local/src to test again and  i get the same error.

Any help would be much appreciated. Thanks.

Regards,
Alicia


***********************************************
Alicia Amadoz
Evolutionary Genetics Unit
Cavanilles Institute for Biodiversity and Evolutionary
Biology
University of Valencia
Apartado Oficial 22085
E-46071 Valencia SPAIN
Phone: (+34) 96 354 3687
FAX: (+34) 96 354 3670
e-mail: alicia.amadoz at uv.es
http://www.uv.es/~amadoz
***********************************************
NOTE! For shipments by EXPRESS COURIER use "Instituto
Cavanilles de Biodiversidad y Biolog??a Evolutiva,
Pol??gono de la Coma s/n, 46980 Paterna (Valencia),
Spain" instead of P.O. Box no. and Post Code/City above.



------------------------------

Message: 39
Date: Thu, 11 Nov 2004 09:44:27 +0000 (GMT)
From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
Subject: Re: [R] RSPerl problem with testing
To: Alicia Amadoz <Alicia.Amadoz at uv.es>
Cc: r-help at stat.math.ethz.ch
Message-ID: <Pine.LNX.4.44.0411110941310.16804-100000 at gannet.stats>
Content-Type: TEXT/PLAIN; charset=US-ASCII

The author is at  <duncan at wald.ucdavis.edu>: this is an Omegahat product 
and I think the Omegahat web pages do lead you there.

Looks to me as if you haven't got the R shared library in your load 
library path: if this is R 2.0.x it is in /usr/lib/R/lib, not bin.

On Thu, 11 Nov 2004, Alicia Amadoz wrote:

> Hi,
> 
> I'm trying to install de RSPerl module and i have some problems trying
> to test it. I've tried to contact with the author but the e-mail address
> seems not to exist. Hope that anyone in this list could explain me a
> little about this problem. I have a bash shell and what i do is the
> following:
> 
> #PERLLIB=/usr/lib/R/library/RSPerl/share/blib/arch:/usr/lib/R/library/RSPerl/share/blib/lib
> # export PERLLIB
> # LD_LIBRARY_PATH=/usr/lib/R/bin:/usr/lib/R/library/RSPerl/libs
> # export LD_LIBRARY_PATH
> # perl test.pl
> 1..1
> Can't load '/usr/lib/R/library/RSPerl/share/blib/arch/auto/R/R.so' for
> module R: /usr/lib/R/library/RSPerl/libs/libPerlConverter.so: undefined
> symbol: R_GlobalEnv at
> /usr/lib/perl5/5.8.3/i386-linux-thread-multi/DynaLoader.pm line 229.
>  at test.pl line 11
> Compilation failed in require at test.pl line 11.
> BEGIN failed--compilation aborted at test.pl line 11.
> not ok 1

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



------------------------------

Message: 40
Date: Thu, 11 Nov 2004 10:08:47 +0000
From: "dave wilton" <dave_wilton at hotmail.com>
Subject: [R] smoothing techniques
To: R-help at stat.math.ethz.ch
Message-ID: <BAY16-F11KO8FNpyLAP0001a602 at hotmail.com>
Content-Type: text/plain; format=flowed

Hi

I was wondering if you could help. I am trying to programme both the maximum 
entropy method algorithm and the continuous wavelet transform into R but am 
having quite a lot of diffulculties. Could you suggest anything.

Many Thanks

David Wilton



------------------------------

Message: 41
Date: Thu, 11 Nov 2004 11:31:00 +0100
From: Martyn Plummer <plummer at iarc.fr>
Subject: Re: [R] R works on Fedora Core 3
To: Jonathan Baron <baron at psych.upenn.edu>
Cc: Jari Oksanen <jari.oksanen at oulu.fi>, r-help at stat.math.ethz.ch
Message-ID: <1100169060.24764.51.camel at nemo>
Content-Type: text/plain

On Wed, 2004-11-10 at 09:19 +0100, Martyn Plummer wrote:
> On Tue, 2004-11-09 at 19:47, Jonathan Baron wrote:
> > On 11/09/04 20:37, Jari Oksanen wrote:
> > >
> > >On 9 Nov 2004, at 19:44, Jonathan Baron wrote:
> > >
> > >> The RPM for Fedora Core 2 seems to work just fine on Core 3.
> > >>
> > >> (The graphics window got smaller, but I'm sure there is a setting
> > >> for that.)
> > >>
> > >That would be good news. I really don't know how the graphics window
> > >became so big at some stage. (MacOS X is just cute here: tiny, sharp,
> > >fast graphics window.)
> > 
> > I have the opposite problem, a 1680x1050 display.
> > 
> > >Has the options()printcmd reappeared, so that dev.print() works without
> > >changing default options?
> > 
> > I can't imagine how this would change.  This is the same "old"
> > RPM, not a new one.  The option is there, and I don't think it
> > ever disappeared.  I can't test it.  This is my laptop, which is
> > not set up to print anything.
> 
> My mistake.  The default print command is determined at configure time.
> But the RedHat RPMS are built in a sandbox that has only the minimal
> configuration needed to build R. This doesn't include the lpr package so
> the default print command is null. I will fix this in the next RPM
> release, but right now I am upgrading to FC3. 

An RPM for Fedora Core 3 should be on a CRAN mirror near you by the
weekend. This fixes the printcmd bug.

The X11() window is the right size for me, but it doesn't have a title
bar, which is a nuisance.

Martyn



------------------------------

Message: 42
Date: Thu, 11 Nov 2004 10:46:04 +0000
From: Gavin Simpson <gavin.simpson at ucl.ac.uk>
Subject: Re: [R] R works on Fedora Core 3
To: Martyn Plummer <plummer at iarc.fr>
Cc: Jonathan Baron <baron at psych.upenn.edu>, Jari Oksanen
	<jari.oksanen at oulu.fi>,	r-help at stat.math.ethz.ch
Message-ID: <419342EC.5090008 at ucl.ac.uk>
Content-Type: text/plain; charset=us-ascii; format=flowed

Martyn Plummer wrote:
> On Wed, 2004-11-10 at 09:19 +0100, Martyn Plummer wrote:
> 

<snip>

> An RPM for Fedora Core 3 should be on a CRAN mirror near you by the
> weekend. This fixes the printcmd bug.
> 
> The X11() window is the right size for me, but it doesn't have a title
> bar, which is a nuisance.
> 
> Martyn
> 

I have this "problem" with both your rpm on FC2 and when compiled from 
sources (1.9.1-patched 2.0.0-patched [now 2.0.1 beta]). The title bar 
comes and goes, seemingly at random (I've not systematically test this 
so I haven't found an underlying cause) when a new X11() device is 
started. Mainly a more complex plot (over several lines and commands) 
will fail to produce the title bar, if I then close that device and open 
another using something simple, say plot(1:10) then I more often than 
not get the title bar. I say more often than not, because sometimes I 
have to run that command a few times, closing the opened faulty device 
window in between to get the title bar to appear. Same result running R 
from within xemacs/ess.

Gav

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpson at ucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



------------------------------

Message: 43
Date: Thu, 11 Nov 2004 10:58:46 +0000
From: Bernie McConnell <bm8 at st-andrews.ac.uk>
Subject: [R] RODBC & POSIX & Daylight Saving blues
To: r-help at stat.math.ethz.ch
Message-ID: <6.1.2.0.0.20041110173817.025a3e40 at bute.st-and.ac.uk>
Content-Type: text/plain; charset="us-ascii"; format=flowed

Dear All,

The recent improvement in RODBC to recognize datetimes in tables has 
exposed my ongoing confusion.

All my data are obtained from a satellite system (Argos) which tags events 
in the GMT time zone.  Daylight saving is ignored.  To my way of thinking 
this means that

   1.  twelve-o-clock means halfway through the day regardless of season, and
   2.  the difftime of any two dates where the time is set to 
twelve-o-clock should be an integer, regardless of which season each of the 
dates are in.

I illustrate my confusion with a two-line table in an Access 2000 database 
table where the single field called 'theDate' contains the two values:

30/07/04 12:00:00
30/11/04 12:00:00

then I bring the datetimes into R with the following code:

 > library (RODBC)
 > theChannel <- odbcConnect("phonetagcopy", "", "")
 > pp <- sqlFetch(theChannel, "DateTest")
 > odbcClose(theChannel)
 > pp$theDate
[1] "2004-07-30 12:00:00 GMT Daylight Time"
[2] "2004-11-30 12:00:00 GMT Standard Time"
 >
 > unclass(pp$theDate) / 86400
[1] 12629.46 12752.50
attr(,"tzone")
[1] ""
 >
 > difftime (pp$theDate[1], pp$theDate[2])
Time difference of -123.0417 days

It appears that sqlFetch has (in this case wrongly) assumed that my 
datetimes are corrected for Daylight Saving.  How can I persuade it to 
accept that all my datetimes are in straight GMT?

OS:Win2000
R 2.0.0
RODBC version 1.1-2
Sys.getlocale()
[1] "LC_COLLATE=English_United Kingdom.1252;LC_CTYPE=English_United 
Kingdom.1252;LC_MONETARY=C;LC_NUMERIC=C;LC_TIME=English_United Kingdom.1252"
 >


Many thanks

Bernie McConnell

Sea Mammal Research Unit
University of St Andrews



------------------------------

_______________________________________________
R-help at stat.math.ethz.ch mailing list  
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE read the posting guide! http://www.R-project.org/posting-guide.html


End of R-help Digest, Vol 21, Issue 11



From ggrothendieck at myway.com  Thu Nov 11 16:36:40 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 11 Nov 2004 15:36:40 +0000 (UTC)
Subject: [R] RODBC & POSIX & Daylight Saving blues
References: <6.1.2.0.0.20041110173817.025a3e40@bute.st-and.ac.uk>
	<Pine.GSO.4.31.0411111108180.27538-100000@toucan.stats>
Message-ID: <loom.20041111T162512-695@post.gmane.org>

Prof Brian Ripley <ripley <at> stats.ox.ac.uk> writes:

: 
: On Thu, 11 Nov 2004, Bernie McConnell wrote:
: 
: > Dear All,
: >
: > The recent improvement in RODBC to recognize datetimes in tables has
: > exposed my ongoing confusion.
: >
: > All my data are obtained from a satellite system (Argos) which tags events
: > in the GMT time zone.  Daylight saving is ignored.  To my way of thinking
: > this means that
: >
: >    1.  twelve-o-clock means halfway through the day regardless of season, 
and
: >    2.  the difftime of any two dates where the time is set to
: > twelve-o-clock should be an integer, regardless of which season each of the
: > dates are in.
: >
: > I illustrate my confusion with a two-line table in an Access 2000 database
: > table where the single field called 'theDate' contains the two values:
: >
: > 30/07/04 12:00:00
: > 30/11/04 12:00:00
: >
: > then I bring the datetimes into R with the following code:
: >
: >  > library (RODBC)
: >  > theChannel <- odbcConnect("phonetagcopy", "", "")
: >  > pp <- sqlFetch(theChannel, "DateTest")
: >  > odbcClose(theChannel)
: >  > pp$theDate
: > [1] "2004-07-30 12:00:00 GMT Daylight Time"
: > [2] "2004-11-30 12:00:00 GMT Standard Time"
: >  >
: >  > unclass(pp$theDate) / 86400
: > [1] 12629.46 12752.50
: > attr(,"tzone")
: > [1] ""
: >  >
: >  > difftime (pp$theDate[1], pp$theDate[2])
: > Time difference of -123.0417 days
: >
: > It appears that sqlFetch has (in this case wrongly) assumed that my
: > datetimes are corrected for Daylight Saving.  How can I persuade it to
: > accept that all my datetimes are in straight GMT?
: 
: All date-time conversions in R by default occurs in the current timezone.
: If you set it to GMT for the duration of the sqlFetch call, it should do
: as you intended (but had not told R, which is not clairvoyant).
: 

On Windows you have to set the whole computer to GMT which has
the potential to interfere with other programs so I don't think 
that that can be the final solution.  I believe that on UNIX it
would be better since its per process there but even then
it has the potential to disrupt the current R session.

If the data is stored in a table using a type that does not have
time zones then it should be mapped into a datetime class in R that 
also has no time zones.   It would be safer to convert it to chron
or to create some new datetime class that matches the data's type
or not convert it and leave it to the user.



From rksh at soc.soton.ac.uk  Thu Nov 11 16:50:43 2004
From: rksh at soc.soton.ac.uk (Robin Hankin)
Date: Thu, 11 Nov 2004 15:50:43 +0000
Subject: [R] axis lines crossing at origin
Message-ID: <a06002004bdb938209c49@[139.166.242.29]>

Hi

how do I make my axes cross at the origin?

x <- seq(from=-pi,to=pi,len=30)
plot(x,sin(x))

makes the axes cross at about (-pi,-1).

How do I get my x and y axes to cross in the centre of the graph, 
with the sine curve passing through
the intersection?

I couldn't find anything in ?par or ?axis; searching R-FAQ for "axis" 
didn't help.




-- 
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
SO14 3ZH
tel +44(0)23-8059-7743
initialDOTsurname at soc.soton.ac.uk (edit in obvious way; spam precaution)



From abunn at whrc.org  Thu Nov 11 16:57:12 2004
From: abunn at whrc.org (Andy Bunn)
Date: Thu, 11 Nov 2004 10:57:12 -0500
Subject: [R] scan or source a text file into a list
Message-ID: <NEBBIPHDAMMOKDKPOFFIIEPKCLAA.abunn@whrc.org>

I've ported somebody else's rather cumbersome Matlab model to R for
colleagues that want a free build of the model with the same type of I/O.

The Matlab model reads a text file with the initial parameters specified as:

C:\Data\Carluc\Rport>more Params.R
# Number of years to simulate
nYears = 50;
# Initial year for graphing purposes
year0 = 1970;
# NPP/GPP ratio (cpp0 unitless)
fnr = 0.30;
# Quantum efficency
qe  = 0.040;

That is, there are four input variables (for this run - there can be many
more) written in a way that R can understand them. In R, I can have the
model source the parameter text file easily enough and have the objects in
the workspace. The model function in R takes a list at runtime. How can I
have R read that file and put the contents into the list I need?

E.g.,
> rm(list = ls())
> source("Params.R")
> ls()
[1] "fnr"    "nYears" "qe"     "year0"
> fnr
[1] 0.3
> nYears
[1] 50
> foo.list <- list(fnr = fnr, nYears = nYears)
>
> foo.list
$fnr
[1] 0.3

$nYears
[1] 50


The model is then run with
> CarlucR(inputParamList = foo.list, ...)

I can't build inputParamList "by hand" as above because the number of
initial parameters changes with the model run and this runs in a wrapper.

Any thoughts? Some combination of paste with scan or parse?
-Andy


> version
         _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    2
minor    0.0
year     2004
month    10
day      04
language R
>



From Anne.Olga.Piotet at omsv.vd.ch  Thu Nov 11 16:58:54 2004
From: Anne.Olga.Piotet at omsv.vd.ch (Anne Piotet)
Date: Thu, 11 Nov 2004 16:58:54 +0100
Subject: [R] Setting plots margin
Message-ID: <001301c4c807$58b74ab0$83dad10a@prod.omsv.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041111/c339c814/attachment.pl

From andy_liaw at merck.com  Thu Nov 11 17:27:10 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 11 Nov 2004 11:27:10 -0500
Subject: [R] axis lines crossing at origin
Message-ID: <3A822319EB35174CA3714066D590DCD50994E2D4@usrymx25.merck.com>

You probably want something like:

plot(x,sin(x), xaxt="n", yaxt="n")
axis(1, pos=0)
axis(2, pos=0)

... but note that axis() only draws the axes from the minimum tick to the
maximum tick.  You may still want to add:

abline(v=0, h=0)

HTH,
Andy


> From: Robin Hankin
> 
> Hi
> 
> how do I make my axes cross at the origin?
> 
> x <- seq(from=-pi,to=pi,len=30)
> plot(x,sin(x))
> 
> makes the axes cross at about (-pi,-1).
> 
> How do I get my x and y axes to cross in the centre of the graph, 
> with the sine curve passing through
> the intersection?
> 
> I couldn't find anything in ?par or ?axis; searching R-FAQ for "axis" 
> didn't help.
> 
> 
> 
> 
> -- 
> Robin Hankin
> Uncertainty Analyst
> Southampton Oceanography Centre
> SO14 3ZH
> tel +44(0)23-8059-7743
> initialDOTsurname at soc.soton.ac.uk (edit in obvious way; spam 
> precaution)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From withhzy at hotmail.com  Thu Nov 11 17:28:08 2004
From: withhzy at hotmail.com (Wei Yang)
Date: Thu, 11 Nov 2004 11:28:08 -0500
Subject: [R] (no subject)
Message-ID: <BAY24-DAV1359mTetXo00017874@hotmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041111/b6da153f/attachment.pl

From james.holtman at convergys.com  Thu Nov 11 17:30:13 2004
From: james.holtman at convergys.com (james.holtman@convergys.com)
Date: Thu, 11 Nov 2004 11:30:13 -0500
Subject: [R] scan or source a text file into a list
Message-ID: <OF4F8F1E3F.4E5276BF-ON85256F49.005A85A0@nd.convergys.com>





use an 'environment' to read in the values; e.g.,

> with(e1 <- new.env(),source('/tempxx.txt', local=T)) # read in the file
to a new environment
> myList <- list()  # define empty list
> for (i in ls(e1)){  # process each element
+ myList[i] <- get(i, e1)
+ }
>
> ls(e1)  # show objects in the list
[1] "fnr"    "nYears" "qe"     "year0"
> myList  # output my list
$fnr
[1] 0.3

$nYears
[1] 50

$qe
[1] 0.04

$year0
[1] 1970

>
__________________________________________________________
James Holtman        "What is the problem you are trying to solve?"
Executive Technical Consultant  --  Office of Technology, Convergys
james.holtman at convergys.com
+1 (513) 723-2929


                                                                                                                                           
                      "Andy Bunn"                                                                                                          
                      <abunn at whrc.org>             To:       "R-Help" <r-help at stat.math.ethz.ch>                                           
                      Sent by:                     cc:                                                                                     
                      r-help-bounces at stat.m        Subject:  [R] scan or source a text file into a list                                    
                      ath.ethz.ch                                                                                                          
                                                                                                                                           
                                                                                                                                           
                      11/11/2004 10:57                                                                                                     
                                                                                                                                           
                                                                                                                                           




I've ported somebody else's rather cumbersome Matlab model to R for
colleagues that want a free build of the model with the same type of I/O.

The Matlab model reads a text file with the initial parameters specified
as:

C:\Data\Carluc\Rport>more Params.R
# Number of years to simulate
nYears = 50;
# Initial year for graphing purposes
year0 = 1970;
# NPP/GPP ratio (cpp0 unitless)
fnr = 0.30;
# Quantum efficency
qe  = 0.040;

That is, there are four input variables (for this run - there can be many
more) written in a way that R can understand them. In R, I can have the
model source the parameter text file easily enough and have the objects in
the workspace. The model function in R takes a list at runtime. How can I
have R read that file and put the contents into the list I need?

E.g.,
> rm(list = ls())
> source("Params.R")
> ls()
[1] "fnr"    "nYears" "qe"     "year0"
> fnr
[1] 0.3
> nYears
[1] 50
> foo.list <- list(fnr = fnr, nYears = nYears)
>
> foo.list
$fnr
[1] 0.3

$nYears
[1] 50


The model is then run with
> CarlucR(inputParamList = foo.list, ...)

I can't build inputParamList "by hand" as above because the number of
initial parameters changes with the model run and this runs in a wrapper.

Any thoughts? Some combination of paste with scan or parse?
-Andy


> version
         _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    2
minor    0.0
year     2004
month    10
day      04
language R
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From emily.baldock at ctsu.ox.ac.uk  Thu Nov 11 17:31:10 2004
From: emily.baldock at ctsu.ox.ac.uk (Emily Baldock)
Date: Thu, 11 Nov 2004 16:31:10 -0000
Subject: [R] expressions and paste
Message-ID: <419393CE.31353.1A1A01C@localhost>

I have written a function to plot data which will be used for various different chemistries.
A simplified version is:
plot_data <- function(risk,levels,chem,sd2,measure){
  plot(risk, levels,main=paste ("per", sd2, measure, "\n in usual", chem))
}
The problem is with the title.
This works fine if the variable "chem" is just text, but if it is an expression then obviously it won't work.
I have experimented with various things and I am at a complete loss for how to insert an expression into the middle of 
a title.
If the expression was going in directly I would use main=expression(paste("text ", expression, " text")) but again this 
doesn't work.
Can anyone help?
thanks
Emily.



From ggrothendieck at myway.com  Thu Nov 11 17:41:13 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 11 Nov 2004 16:41:13 +0000 (UTC)
Subject: [R] RODBC & POSIX & Daylight Saving blues
References: <6.1.2.0.0.20041110173817.025a3e40@bute.st-and.ac.uk>
	<Pine.GSO.4.31.0411111108180.27538-100000@toucan.stats>
	<loom.20041111T162512-695@post.gmane.org>
Message-ID: <loom.20041111T173329-664@post.gmane.org>

Gabor Grothendieck <ggrothendieck <at> myway.com> writes:

: 
: Prof Brian Ripley <ripley <at> stats.ox.ac.uk> writes:
: 

: : If you set it to GMT for the duration of the sqlFetch call, it should do
: : as you intended (but had not told R, which is not clairvoyant).
: : 
: 
: On Windows you have to set the whole computer to GMT which has

Paul Roebuck pointed out to me offlist that this can be done
per process in Windows too so I was wrong on this point.

   ... start up Windows console ...
   cd \Program Files\rw2001beta
   set TZ=GMT
   bin\Rgui

In R,

   Sys.time() # displays date and time in GMT time zone


I also tried doing this from within R but was unsuccessful:

   R> Sys.time()
   [1] "2004-11-11 11:37:53 Eastern Standard Time"
   R> Sys.putenv(TZ = "GMT")
   R> Sys.time()  # wanted GMT result but did not get it
   [1] "2004-11-11 11:38:08 Eastern Standard Time"
   R> R.version.string  # Windows XP
   [1] "R version 2.0.1, 2004-11-04"



From vito_ricci at yahoo.com  Thu Nov 11 17:41:33 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Thu, 11 Nov 2004 17:41:33 +0100 (CET)
Subject: [R] Re: (no subject)
Message-ID: <20041111164133.5777.qmail@web41203.mail.yahoo.com>

Please could you specify better your problem, an
example would be appreciatted. If you explain in a
clear way we can help you. 
Please read posting-guide:

http://www.r-project.org/posting-guide.html

Cordially
Vito


You wrote:

Hi, 

I have a list of numbers. For each of the numbers, I
take sum of squares of
the numbers centered on the number chosen. If it is
less than a certain
constant, I will take the average of the numbers
chosen.  

 

Anyone can give me a sample code. You help will be
greatly appreciated. 

 

Peter

=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese http://www.modugno.it/archivio/cat_palese.shtml


		
___________________________________

e posta indesiderata



From andy_liaw at merck.com  Thu Nov 11 17:45:36 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 11 Nov 2004 11:45:36 -0500
Subject: [R] scan or source a text file into a list
Message-ID: <3A822319EB35174CA3714066D590DCD50994E2D5@usrymx25.merck.com>

Here's one way:

> myList <- mget(ls(), .GlobalEnv)
> myList
$fnr
[1] 0.3

$nYears
[1] 50

$qe
[1] 0.04

$year0
[1] 1970

HTH,
Andy

> From: Andy Bunn
> 
> I've ported somebody else's rather cumbersome Matlab model to R for
> colleagues that want a free build of the model with the same 
> type of I/O.
> 
> The Matlab model reads a text file with the initial 
> parameters specified as:
> 
> C:\Data\Carluc\Rport>more Params.R
> # Number of years to simulate
> nYears = 50;
> # Initial year for graphing purposes
> year0 = 1970;
> # NPP/GPP ratio (cpp0 unitless)
> fnr = 0.30;
> # Quantum efficency
> qe  = 0.040;
> 
> That is, there are four input variables (for this run - there 
> can be many
> more) written in a way that R can understand them. In R, I 
> can have the
> model source the parameter text file easily enough and have 
> the objects in
> the workspace. The model function in R takes a list at 
> runtime. How can I
> have R read that file and put the contents into the list I need?
> 
> E.g.,
> > rm(list = ls())
> > source("Params.R")
> > ls()
> [1] "fnr"    "nYears" "qe"     "year0"
> > fnr
> [1] 0.3
> > nYears
> [1] 50
> > foo.list <- list(fnr = fnr, nYears = nYears)
> >
> > foo.list
> $fnr
> [1] 0.3
> 
> $nYears
> [1] 50
> 
> 
> The model is then run with
> > CarlucR(inputParamList = foo.list, ...)
> 
> I can't build inputParamList "by hand" as above because the number of
> initial parameters changes with the model run and this runs 
> in a wrapper.
> 
> Any thoughts? Some combination of paste with scan or parse?
> -Andy
> 
> 
> > version
>          _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    2
> minor    0.0
> year     2004
> month    10
> day      04
> language R
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ramasamy at cancer.org.uk  Thu Nov 11 17:55:37 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Thu, 11 Nov 2004 16:55:37 +0000
Subject: [R] scan or source a text file into a list
In-Reply-To: <NEBBIPHDAMMOKDKPOFFIIEPKCLAA.abunn@whrc.org>
References: <NEBBIPHDAMMOKDKPOFFIIEPKCLAA.abunn@whrc.org>
Message-ID: <1100192137.3055.60.camel@ramasamy.stats>

Not sure if I understand your question.

If you know the complete list of all possible variable names, then you
can grep for each occurrence and split on the "=". Otherwise, I hope you
have some sort of markers (like main()) to seperate the input and actual
commands. I think perl and bash are probably more suitable for this kind
of job.

Then you can try commandArgs() to feed into R.



On Thu, 2004-11-11 at 15:57, Andy Bunn wrote:
> I've ported somebody else's rather cumbersome Matlab model to R for
> colleagues that want a free build of the model with the same type of I/O.
> 
> The Matlab model reads a text file with the initial parameters specified as:
> 
> C:\Data\Carluc\Rport>more Params.R
> # Number of years to simulate
> nYears = 50;
> # Initial year for graphing purposes
> year0 = 1970;
> # NPP/GPP ratio (cpp0 unitless)
> fnr = 0.30;
> # Quantum efficency
> qe  = 0.040;
> 
> That is, there are four input variables (for this run - there can be many
> more) written in a way that R can understand them. In R, I can have the
> model source the parameter text file easily enough and have the objects in
> the workspace. The model function in R takes a list at runtime. How can I
> have R read that file and put the contents into the list I need?
> 
> E.g.,
> > rm(list = ls())
> > source("Params.R")
> > ls()
> [1] "fnr"    "nYears" "qe"     "year0"
> > fnr
> [1] 0.3
> > nYears
> [1] 50
> > foo.list <- list(fnr = fnr, nYears = nYears)
> >
> > foo.list
> $fnr
> [1] 0.3
> 
> $nYears
> [1] 50
> 
> 
> The model is then run with
> > CarlucR(inputParamList = foo.list, ...)
> 
> I can't build inputParamList "by hand" as above because the number of
> initial parameters changes with the model run and this runs in a wrapper.
> 
> Any thoughts? Some combination of paste with scan or parse?
> -Andy
> 
> 
> > version
>          _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    2
> minor    0.0
> year     2004
> month    10
> day      04
> language R
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jon.davies at imperial.ac.uk  Thu Nov 11 18:15:04 2004
From: jon.davies at imperial.ac.uk (Davies, Jonathan)
Date: Thu, 11 Nov 2004 17:15:04 -0000
Subject: [R] glm.fit warning message
Message-ID: <45535E88F09A2F47A747D2DBF3E5BA420F384B@icex1.ic.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041111/3a680aa6/attachment.pl

From tlumley at u.washington.edu  Thu Nov 11 18:16:50 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 11 Nov 2004 09:16:50 -0800 (PST)
Subject: [R] polr probit versus stata oprobit  
In-Reply-To: <Pine.SGI.4.40.0411102108010.52139612-100000@origin.chass.utoronto.ca>
References: <Pine.SGI.4.40.0411102108010.52139612-100000@origin.chass.utoronto.ca>
Message-ID: <Pine.A41.4.61b.0411110915210.195158@homer04.u.washington.edu>

On Wed, 10 Nov 2004, Jean Eid wrote:

> Dear Thomas,
>
> Where you also able to replicate the second example?  (the exaample
> that I turned the housing data into numerical variables) That is the one
> that my estimates differ.
>

I don't have your second example, but I get the same results from
    polr(formula = Sat ~ as.numeric(Infl) + as.numeric(Type) +
      as.numeric(Cont), data = housing, weights = Freq, method = "probit")
and
    oprobit Sat Infl Type Cont [fw=Freq]
for example.

 	-thomas


>
> On Wed, 10 Nov 2004, Thomas Lumley wrote:
>
>> On Wed, 10 Nov 2004, Jean Eid wrote:
>>
>>> Dear All,
>>> I have been struggling to understand why for the housing data in MASS
>>> library R and stata give coef. estimates that are really different. I also
>>> tried to come up with many many examples myself (see below, of course I
>>> did not have the set.seed command included) and all of my
>>> `random' examples seem to give verry similar output. For the housing data,
>>> I have changed the data into numeric vectors instead of factors/ordered
>>> factors. I did so to try and get the same results as in STATA and to have
>>> the housing example as close as possible to the one I constructed.
>>>
>>> I run a debian sid, kernel 2.4, R 2.0.0, and STATA version 8.2, MASS
>>> version  7.2-8.
>>>
>>>
>>> here's the example ( I assume that you have STATA installed and can run in
>>> batch mode, if not the output is also given below)
>>>
>>
>> That example shows the same results with Stata and polr() from MASS.
>>
>> For the housing data, I also get the same coefficients in Stata as with
>> polr():
>>
>> In R:
>> library(MASS)
>> library(foreign)
>> write.dta(housing, file="housing.dta")
>> house.probit<-polr(Sat ~ Infl + Type + Cont, data = housing, weights =
>> Freq, method = "probit")
>> summary(house.probit)
>> -------------------------
>> Re-fitting to get Hessian
>>
>> Call:
>> polr(formula = Sat ~ Infl + Type + Cont, data = housing, weights = Freq,
>>      method = "probit")
>>
>> Coefficients:
>>                     Value Std. Error   t value
>> InflMedium     0.3464233 0.06413706  5.401297
>> InflHigh       0.7829149 0.07642620 10.244063
>> TypeApartment -0.3475372 0.07229093 -4.807480
>> TypeAtrium    -0.2178874 0.09476607 -2.299213
>> TypeTerrace   -0.6641737 0.09180004 -7.235005
>> ContHigh       0.2223862 0.05812267  3.826153
>>
>> Intercepts:
>>              Value   Std. Error t value
>> Low|Medium  -0.2998  0.0762    -3.9371
>> Medium|High  0.4267  0.0764     5.5850
>>
>> Residual Deviance: 3479.689
>> AIC: 3495.689
>> ------------------------
>>
>>
>> In Stata
>> -----------------
>> . use housing.dta
>> . xi: oprobit Sat i.Infl i.Type i.Cont [fw=Freq]
>> i.Infl            _IInfl_1-3          (naturally coded; _IInfl_1 omitted)
>> i.Type            _IType_1-4          (naturally coded; _IType_1 omitted)
>> i.Cont            _ICont_1-2          (naturally coded; _ICont_1 omitted)
>>
>> Iteration 0:   log likelihood = -1824.4388
>> Iteration 1:   log likelihood = -1739.9254
>> Iteration 2:   log likelihood = -1739.8444
>>
>> Ordered probit estimates                          Number of obs   =    1681
>>                                                    LR chi2(6)      =   169.19
>>                                                    Prob > chi2     =   0.0000
>> Log likelihood = -1739.8444                       Pseudo R2       =   0.0464
>>
>> ------------------------------------------------------------------------------
>>           Sat |      Coef.   Std. Err.      z    P>|z|     [95% Conf.
>> Interval]
>> -------------+----------------------------------------------------------------
>>      _IInfl_2 |   .3464228    .064137     5.40   0.000     .2207165     .472129
>>      _IInfl_3 |   .7829146    .076426    10.24   0.000     .6331224    .9327069
>>      _IType_2 |  -.3475367   .0722908    -4.81   0.000    -.4892241   -.2058493
>>      _IType_3 |  -.2178875    .094766    -2.30   0.021    -.4036254   -.0321497
>>      _IType_4 |  -.6641735   .0917999    -7.24   0.000     -.844098    -.484249
>>      _ICont_2 |   .2223858   .0581226     3.83   0.000     .1084676     .336304
>> -------------+----------------------------------------------------------------
>>         _cut1 |  -.2998279   .0761537          (Ancillary parameters)
>>         _cut2 |   .4267208   .0764043
>> ------------------------------------------------------------------------------
>>
>>
>>
>>  	-thomas
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From deepayan at stat.wisc.edu  Thu Nov 11 18:24:24 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 11 Nov 2004 11:24:24 -0600
Subject: [R] scan or source a text file into a list
In-Reply-To: <NEBBIPHDAMMOKDKPOFFIIEPKCLAA.abunn@whrc.org>
References: <NEBBIPHDAMMOKDKPOFFIIEPKCLAA.abunn@whrc.org>
Message-ID: <200411111124.24793.deepayan@stat.wisc.edu>

On Thursday 11 November 2004 09:57, Andy Bunn wrote:
> I've ported somebody else's rather cumbersome Matlab model to R for
> colleagues that want a free build of the model with the same type of
> I/O.
>
> The Matlab model reads a text file with the initial parameters
> specified as:
>
> C:\Data\Carluc\Rport>more Params.R
> # Number of years to simulate
> nYears = 50;
> # Initial year for graphing purposes
> year0 = 1970;
> # NPP/GPP ratio (cpp0 unitless)
> fnr = 0.30;
> # Quantum efficency
> qe  = 0.040;
>
> That is, there are four input variables (for this run - there can be
> many more) written in a way that R can understand them. In R, I can
> have the model source the parameter text file easily enough and have
> the objects in the workspace. The model function in R takes a list at
> runtime. How can I have R read that file and put the contents into
> the list I need?
>
> E.g.,
>
> > rm(list = ls())
> > source("Params.R")
> > ls()
>
> [1] "fnr"    "nYears" "qe"     "year0"
>
> > fnr
>
> [1] 0.3
>
> > nYears
>
> [1] 50
>
> > foo.list <- list(fnr = fnr, nYears = nYears)
> >
> > foo.list
>
> $fnr
> [1] 0.3
>
> $nYears
> [1] 50
>
>
> The model is then run with
>
> > CarlucR(inputParamList = foo.list, ...)
>
> I can't build inputParamList "by hand" as above because the number of
> initial parameters changes with the model run and this runs in a
> wrapper.
>
> Any thoughts? Some combination of paste with scan or parse?

Approaching this slightly differently, how about 

read.pars <- function(file)
{
    foo <- read.table(file, as.is = TRUE)
    ans <- lapply(strsplit(foo[[3]], ";"), as.numeric)
    names(ans) <- foo[[1]]
    ans
}

Deepayan



From dhoysak at ccs.carleton.ca  Thu Nov 11 18:24:10 2004
From: dhoysak at ccs.carleton.ca (Drew Hoysak)
Date: Thu, 11 Nov 2004 12:24:10 -0500
Subject: [R] wrong answer for simple expressions
Message-ID: <1100193850.2945.0.camel@zippy.local.ca>

I am experiencing strange (to me) output when trying to do simple
calculations.  Expressions that should equal zero yield non-zero
values.  
Examples:

> a <- 4.1-3.1
> b <- 5.1-4.1
> a-b
[1] -4.440892e-16


> (4.1-3.1)-(5.1-4.1)
[1] -4.440892e-16


When this last expression is expanded, I get the right answer:

> 4.1-3.1-5.1+4.1
[1] 0


I am using the binary packaged version R-2.0.0-0.fdr.1.fc2.i386.rpm for
Linux Fedora Core 2.  I had the same problem with version 1.9.0-0

Can anyone tell me what is going on?

Thanks,


Drew Hoysak



From tlumley at u.washington.edu  Thu Nov 11 18:26:06 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 11 Nov 2004 09:26:06 -0800 (PST)
Subject: [R] expressions and paste
In-Reply-To: <419393CE.31353.1A1A01C@localhost>
References: <419393CE.31353.1A1A01C@localhost>
Message-ID: <Pine.A41.4.61b.0411110917570.195158@homer04.u.washington.edu>

On Thu, 11 Nov 2004, Emily Baldock wrote:

> I have written a function to plot data which will be used for various different chemistries.
> A simplified version is:
> plot_data <- function(risk,levels,chem,sd2,measure){
>  plot(risk, levels,main=paste ("per", sd2, measure, "\n in usual", chem))
> }
> The problem is with the title.
> This works fine if the variable "chem" is just text, but if it is an expression then obviously it won't work.
> I have experimented with various things and I am at a complete loss for how to insert an expression into the middle of
> a title.

With
  sd2<-10
  measure<-quote(mu*g*m^{-3})
  chem<-quote(H[2]*SO[4])
You can use
  plot(1,1,main=bquote("per "*.(sd2)*.(measure)*" in usual "*.(chem)))
or
  plot(1,1,main=substitute("per "*sd2*measure*" in usual "*chem,
    list(sd2=sd2,measure=measure,chem=chem)) )

You don't get the newlines, someone else will have to work this out.

 	-thomas



From Roger.Bivand at nhh.no  Thu Nov 11 18:31:19 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 11 Nov 2004 18:31:19 +0100 (CET)
Subject: [R] scan or source a text file into a list
In-Reply-To: <NEBBIPHDAMMOKDKPOFFIIEPKCLAA.abunn@whrc.org>
Message-ID: <Pine.LNX.4.44.0411111808090.1045-100000@reclus.nhh.no>

On Thu, 11 Nov 2004, Andy Bunn wrote:

> I've ported somebody else's rather cumbersome Matlab model to R for
> colleagues that want a free build of the model with the same type of I/O.
> 
> The Matlab model reads a text file with the initial parameters specified as:

I think I would be tempted to try out read.dcf() for this, which returns a 
character matrix of field values in named columns, from which values cast 
to numeric:

> system("cat tmp/Params.dcf")
nYears: 50
year0: 1970
fnr: 0.30
qe: 0.040
> x <- read.dcf("tmp/Params.dcf")
> x
     nYears year0  fnr    qe     
[1,] "50"   "1970" "0.30" "0.040"
> str(x)
 chr [1, 1:4] "50" "1970" "0.30" "0.040"
 - attr(*, "dimnames")=List of 2
  ..$ : NULL
  ..$ : chr [1:4] "nYears" "year0" "fnr" "qe"


Since you will know the mode you need, you could read the same-mode fields 
together and cast them:

> x <- read.dcf("tmp/Params.dcf", fields=c("nYears", "year0"))
> mode(x) <- "integer"
> x
     nYears year0
[1,]     50  1970

and make x a data frame if say x$nYears is needed, which gets to list:

> as.list(data.frame(x))
$nYears
[1] 50

$year0
[1] 1970

and c() of the different mode lists to make inputParamList. I think 
treating everything as numeric may be enough, and the DCF format is 
flexible.

Roger

> 
> C:\Data\Carluc\Rport>more Params.R
> # Number of years to simulate
> nYears = 50;
> # Initial year for graphing purposes
> year0 = 1970;
> # NPP/GPP ratio (cpp0 unitless)
> fnr = 0.30;
> # Quantum efficency
> qe  = 0.040;
> 
> That is, there are four input variables (for this run - there can be many
> more) written in a way that R can understand them. In R, I can have the
> model source the parameter text file easily enough and have the objects in
> the workspace. The model function in R takes a list at runtime. How can I
> have R read that file and put the contents into the list I need?
> 
> E.g.,
> > rm(list = ls())
> > source("Params.R")
> > ls()
> [1] "fnr"    "nYears" "qe"     "year0"
> > fnr
> [1] 0.3
> > nYears
> [1] 50
> > foo.list <- list(fnr = fnr, nYears = nYears)
> >
> > foo.list
> $fnr
> [1] 0.3
> 
> $nYears
> [1] 50
> 
> 
> The model is then run with
> > CarlucR(inputParamList = foo.list, ...)
> 
> I can't build inputParamList "by hand" as above because the number of
> initial parameters changes with the model run and this runs in a wrapper.
> 
> Any thoughts? Some combination of paste with scan or parse?
> -Andy
> 
> 
> > version
>          _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    2
> minor    0.0
> year     2004
> month    10
> day      04
> language R
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From drakegis at dacafe.com  Thu Nov 11 18:29:38 2004
From: drakegis at dacafe.com (DrakeGis)
Date: Thu, 11 Nov 2004 12:29:38 -0500 (EST)
Subject: [R] OLS error
Message-ID: <33976.141.211.77.129.1100194178.squirrel@cafemail.giscafe.com>

Hi,
  I have 142 observations off different variables and I'm trying to do a
OLS. And I get this error. Any Ideas ?

> f <- ols(lnmigr3~popden78+income+modern+spareha+rain)
Error in La.chol2inv(x, size) : size cannot exceed nrow(x) = 1


  D.


-----------------------------------------
Stay ahead of the information curve.
Receive GIS news and jobs on your desktop daily.
Subscribe today to the GIS CafeNews newsletter.
[ http://www10.giscafe.com/nl/newsletter_subscribe.php ]
It's informative and essential.



From rpeng at jhsph.edu  Thu Nov 11 18:58:26 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 11 Nov 2004 12:58:26 -0500
Subject: [R] expressions and paste
In-Reply-To: <419393CE.31353.1A1A01C@localhost>
References: <419393CE.31353.1A1A01C@localhost>
Message-ID: <4193A842.4010406@jhsph.edu>

Can you be a bit more specific?  Exactly what kind of symbol are you 
trying to put in the title?

-roger

Emily Baldock wrote:
> I have written a function to plot data which will be used for various different chemistries.
> A simplified version is:
> plot_data <- function(risk,levels,chem,sd2,measure){
>   plot(risk, levels,main=paste ("per", sd2, measure, "\n in usual", chem))
> }
> The problem is with the title.
> This works fine if the variable "chem" is just text, but if it is an expression then obviously it won't work.
> I have experimented with various things and I am at a complete loss for how to insert an expression into the middle of 
> a title.
> If the expression was going in directly I would use main=expression(paste("text ", expression, " text")) but again this 
> doesn't work.
> Can anyone help?
> thanks
> Emily.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From jeaneid at chass.utoronto.ca  Thu Nov 11 19:10:34 2004
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Thu, 11 Nov 2004 13:10:34 -0500
Subject: [R] polr probit versus stata oprobit  
In-Reply-To: <Pine.A41.4.61b.0411110915210.195158@homer04.u.washington.edu>
Message-ID: <Pine.SGI.4.40.0411111308270.52667286-100000@origin.chass.utoronto.ca>

Thank you Thomas for your answer. It was the weights that are giving me
problems and I still have no idea why. i.e. when I try your example,
everything work fine. However when I do not include the weights=Freq and
[fw=Freq] in both softwares, I do get verry different results.


Jean,

On Thu, 11 Nov 2004, Thomas Lumley wrote:

> On Wed, 10 Nov 2004, Jean Eid wrote:
>
> > Dear Thomas,
> >
> > Where you also able to replicate the second example?  (the exaample
> > that I turned the housing data into numerical variables) That is the one
> > that my estimates differ.
> >
>
> I don't have your second example, but I get the same results from
>     polr(formula = Sat ~ as.numeric(Infl) + as.numeric(Type) +
>       as.numeric(Cont), data = housing, weights = Freq, method = "probit")
> and
>     oprobit Sat Infl Type Cont [fw=Freq]
> for example.
>
>  	-thomas
>
>
> >
> > On Wed, 10 Nov 2004, Thomas Lumley wrote:
> >
> >> On Wed, 10 Nov 2004, Jean Eid wrote:
> >>
> >>> Dear All,
> >>> I have been struggling to understand why for the housing data in MASS
> >>> library R and stata give coef. estimates that are really different. I also
> >>> tried to come up with many many examples myself (see below, of course I
> >>> did not have the set.seed command included) and all of my
> >>> `random' examples seem to give verry similar output. For the housing data,
> >>> I have changed the data into numeric vectors instead of factors/ordered
> >>> factors. I did so to try and get the same results as in STATA and to have
> >>> the housing example as close as possible to the one I constructed.
> >>>
> >>> I run a debian sid, kernel 2.4, R 2.0.0, and STATA version 8.2, MASS
> >>> version  7.2-8.
> >>>
> >>>
> >>> here's the example ( I assume that you have STATA installed and can run in
> >>> batch mode, if not the output is also given below)
> >>>
> >>
> >> That example shows the same results with Stata and polr() from MASS.
> >>
> >> For the housing data, I also get the same coefficients in Stata as with
> >> polr():
> >>
> >> In R:
> >> library(MASS)
> >> library(foreign)
> >> write.dta(housing, file="housing.dta")
> >> house.probit<-polr(Sat ~ Infl + Type + Cont, data = housing, weights =
> >> Freq, method = "probit")
> >> summary(house.probit)
> >> -------------------------
> >> Re-fitting to get Hessian
> >>
> >> Call:
> >> polr(formula = Sat ~ Infl + Type + Cont, data = housing, weights = Freq,
> >>      method = "probit")
> >>
> >> Coefficients:
> >>                     Value Std. Error   t value
> >> InflMedium     0.3464233 0.06413706  5.401297
> >> InflHigh       0.7829149 0.07642620 10.244063
> >> TypeApartment -0.3475372 0.07229093 -4.807480
> >> TypeAtrium    -0.2178874 0.09476607 -2.299213
> >> TypeTerrace   -0.6641737 0.09180004 -7.235005
> >> ContHigh       0.2223862 0.05812267  3.826153
> >>
> >> Intercepts:
> >>              Value   Std. Error t value
> >> Low|Medium  -0.2998  0.0762    -3.9371
> >> Medium|High  0.4267  0.0764     5.5850
> >>
> >> Residual Deviance: 3479.689
> >> AIC: 3495.689
> >> ------------------------
> >>
> >>
> >> In Stata
> >> -----------------
> >> . use housing.dta
> >> . xi: oprobit Sat i.Infl i.Type i.Cont [fw=Freq]
> >> i.Infl            _IInfl_1-3          (naturally coded; _IInfl_1 omitted)
> >> i.Type            _IType_1-4          (naturally coded; _IType_1 omitted)
> >> i.Cont            _ICont_1-2          (naturally coded; _ICont_1 omitted)
> >>
> >> Iteration 0:   log likelihood = -1824.4388
> >> Iteration 1:   log likelihood = -1739.9254
> >> Iteration 2:   log likelihood = -1739.8444
> >>
> >> Ordered probit estimates                          Number of obs   =    1681
> >>                                                    LR chi2(6)      =   169.19
> >>                                                    Prob > chi2     =   0.0000
> >> Log likelihood = -1739.8444                       Pseudo R2       =   0.0464
> >>
> >> ------------------------------------------------------------------------------
> >>           Sat |      Coef.   Std. Err.      z    P>|z|     [95% Conf.
> >> Interval]
> >> -------------+----------------------------------------------------------------
> >>      _IInfl_2 |   .3464228    .064137     5.40   0.000     .2207165     .472129
> >>      _IInfl_3 |   .7829146    .076426    10.24   0.000     .6331224    .9327069
> >>      _IType_2 |  -.3475367   .0722908    -4.81   0.000    -.4892241   -.2058493
> >>      _IType_3 |  -.2178875    .094766    -2.30   0.021    -.4036254   -.0321497
> >>      _IType_4 |  -.6641735   .0917999    -7.24   0.000     -.844098    -.484249
> >>      _ICont_2 |   .2223858   .0581226     3.83   0.000     .1084676     .336304
> >> -------------+----------------------------------------------------------------
> >>         _cut1 |  -.2998279   .0761537          (Ancillary parameters)
> >>         _cut2 |   .4267208   .0764043
> >> ------------------------------------------------------------------------------
> >>
> >>
> >>
> >>  	-thomas
> >>
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>
> Thomas Lumley			Assoc. Professor, Biostatistics
> tlumley at u.washington.edu	University of Washington, Seattle
>



From ggrothendieck at myway.com  Thu Nov 11 19:08:40 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 11 Nov 2004 18:08:40 +0000 (UTC)
Subject: [R] scan or source a text file into a list
References: <NEBBIPHDAMMOKDKPOFFIIEPKCLAA.abunn@whrc.org>
Message-ID: <loom.20041111T190143-317@post.gmane.org>

Andy Bunn <abunn <at> whrc.org> writes:

: 
: I've ported somebody else's rather cumbersome Matlab model to R for
: colleagues that want a free build of the model with the same type of I/O.
: 
: The Matlab model reads a text file with the initial parameters specified as:
: 
: C:\Data\Carluc\Rport>more Params.R
: # Number of years to simulate
: nYears = 50;
: # Initial year for graphing purposes
: year0 = 1970;
: # NPP/GPP ratio (cpp0 unitless)
: fnr = 0.30;
: # Quantum efficency
: qe  = 0.040;
: 
: That is, there are four input variables (for this run - there can be many
: more) written in a way that R can understand them. In R, I can have the
: model source the parameter text file easily enough and have the objects in
: the workspace. The model function in R takes a list at runtime. How can I
: have R read that file and put the contents into the list I need?
: 
: E.g.,
: > rm(list = ls())
: > source("Params.R")
: > ls()
: [1] "fnr"    "nYears" "qe"     "year0"
: > fnr
: [1] 0.3
: > nYears
: [1] 50
: > foo.list <- list(fnr = fnr, nYears = nYears)
: >
: > foo.list
: $fnr
: [1] 0.3
: 
: $nYears
: [1] 50
: 
: 
: The model is then run with
: > CarlucR(inputParamList = foo.list, ...)
: 
: I can't build inputParamList "by hand" as above because the number of
: initial parameters changes with the model run and this runs in a wrapper.
: 


The following sources the file in the environment that
exists within function f returning a list of all variables in
that environment except those variables that begin with a dot. 
You could alternately replace the source(...) statement with 
eval(parse(.file)) .

R> f <- function(.file) { source(.file, local = TRUE); as.list(environment()) }
R> f("clipboard")
$qe
[1] 0.04

$fnr
[1] 0.3

$year0
[1] 1970

$nYears
[1] 50

:



From p.dalgaard at biostat.ku.dk  Thu Nov 11 19:17:44 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 11 Nov 2004 19:17:44 +0100
Subject: [R] axis lines crossing at origin
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E2D4@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E2D4@usrymx25.merck.com>
Message-ID: <x21xf0i71z.fsf@biostat.ku.dk>

"Liaw, Andy" <andy_liaw at merck.com> writes:

> You probably want something like:
> 
> plot(x,sin(x), xaxt="n", yaxt="n")
> axis(1, pos=0)
> axis(2, pos=0)
> 
> ... but note that axis() only draws the axes from the minimum tick to the
> maximum tick.  You may still want to add:
> 
> abline(v=0, h=0)

Also, you generally don't want to have a tick mark at the origin since
the other axis would go straight through the label, so you should
learn about the at= argument to axis() too.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Thu Nov 11 19:22:17 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 11 Nov 2004 18:22:17 +0000 (GMT)
Subject: [R] glm.fit warning message
In-Reply-To: <45535E88F09A2F47A747D2DBF3E5BA420F384B@icex1.ic.ac.uk>
Message-ID: <Pine.GSO.4.31.0411111817000.28453-100000@toucan.stats>

It is a warning: it may or may not be serious.

Such a fitted value could well occur if you have a cell in the
cross-classification which has only zero observations.  Then the fitted
value may be zero, and the distribution theory for a Poisson glm is
invalid in that case (as the MLE is on the boundary of the parameter
space, for example).

The problem may be overfitting the data, or it may be a structural zero
(that means that zero did not occur by chance, but would always have
happened).

I suggest you need to read more of the literature on Poisson aka
log-linear regressions to better understand what might be happening.

On Thu, 11 Nov 2004, Davies, Jonathan wrote:

> I am feeling my way in the use of GLM's and have come across a warning whilst manually simplifying a model with interaction terms, removing terms one at a time from the maximum model (R1.9.0).
>
>
>
> > model<-glm(midpnts~(AET+tempave+tempvar+MDE+sqrtarea)^2+Lat,family=poisson,weights=weightS)
>
> > model2<-update(model,~.-tempave:tempvar)
>
> Warning message:
>
> fitted rates numerically 0 occurred in: glm.fit(x = X, y = Y, weights = weights, start = start, etastart = etastart,
>
>
>
> I have had limited success in interpreting this message, further when I use the STEP function [step(model)] the two way interaction term is removed from the model (along with several other interaction terms) and no warning message appears. I saw that there had previously been issue with the glm.fit function (in a much earlier version R 1.4.1), though I am more than ready to believe the problem lies with my data.
>
>
>
> I would appreciate comments on whether the problem is with the data, and if so what is the interpretation of the warning message; finally, as the step function removes the interaction term in any case, can I continue the model simplification procedure from where the step function leaves me?
>
>
>
> Thanks,
>
> Jonathan
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tlumley at u.washington.edu  Thu Nov 11 19:25:34 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 11 Nov 2004 10:25:34 -0800 (PST)
Subject: [R] polr probit versus stata oprobit  
In-Reply-To: <Pine.SGI.4.40.0411111308270.52667286-100000@origin.chass.utoronto.ca>
References: <Pine.SGI.4.40.0411111308270.52667286-100000@origin.chass.utoronto.ca>
Message-ID: <Pine.A41.4.61b.0411111020470.76970@homer07.u.washington.edu>

On Thu, 11 Nov 2004, Jean Eid wrote:

> Thank you Thomas for your answer. It was the weights that are giving me
> problems and I still have no idea why. i.e. when I try your example,
> everything work fine. However when I do not include the weights=Freq and
> [fw=Freq] in both softwares, I do get verry different results.
>

I still don't understand what example you are using to find the 
difference.  I tried two ways of not using weights

1)  Expand the data to have a record for each observation (so 1681 rows 
instead of 72).
     Fitting these expanded data  without weights gives the same answers as 
fitting the compressed data with weights, in both MASS::polr and Stata's 
oprobit.


2) Pretend that the housing data have only 72 observations and ignore the 
weights (though why you would do this...)
    The true coefficients are all zero in this situation. R gives numbers 
zero to about 6 digits and Stata gives zero to about 30 digits.  The 
intercepts are the same in both packages.


 	-thomas



From p.dalgaard at biostat.ku.dk  Thu Nov 11 19:27:40 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 11 Nov 2004 19:27:40 +0100
Subject: [R] wrong answer for simple expressions
In-Reply-To: <1100193850.2945.0.camel@zippy.local.ca>
References: <1100193850.2945.0.camel@zippy.local.ca>
Message-ID: <x2wtwsgs0z.fsf@biostat.ku.dk>

Drew Hoysak <dhoysak at ccs.carleton.ca> writes:

> I am experiencing strange (to me) output when trying to do simple
> calculations.  Expressions that should equal zero yield non-zero
> values.  
> Examples:
> 
> > a <- 4.1-3.1
> > b <- 5.1-4.1
> > a-b
> [1] -4.440892e-16
> 
> 
> > (4.1-3.1)-(5.1-4.1)
> [1] -4.440892e-16
> 
> 
> When this last expression is expanded, I get the right answer:
> 
> > 4.1-3.1-5.1+4.1
> [1] 0

Welcome to the world of floating point arithmetic! Since one tenth
cannot be represented exactly in binary, you are going to see these
small deviations once in a while. It is is really no stranger than 

   3/3 - (1/3 + 1/3 + 1/3) = 1.000 -  (0.333 + 0.333 + 0.333) = 0.001

in decimal notaion.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From tlumley at u.washington.edu  Thu Nov 11 19:33:07 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 11 Nov 2004 10:33:07 -0800 (PST)
Subject: [R] wrong answer for simple expressions
In-Reply-To: <1100193850.2945.0.camel@zippy.local.ca>
References: <1100193850.2945.0.camel@zippy.local.ca>
Message-ID: <Pine.A41.4.61b.0411111026290.76970@homer07.u.washington.edu>

On Thu, 11 Nov 2004, Drew Hoysak wrote:

> I am experiencing strange (to me) output when trying to do simple
> calculations.  Expressions that should equal zero yield non-zero
> values.

No. There is no reason why these expressions should yield zero values. 
Remember that computers work in base 2, and that 0.1 has an infinitely 
recurring binary expansion in base 2.  You should expect that 0.1 computed 
two different ways should differ in the last few bits. You have managed to 
get zero to 52 bits accuracy, which is not bad when you consider that the 
machine only works to 54 bits.

 	-thomas

> Examples:
>
>> a <- 4.1-3.1
>> b <- 5.1-4.1
>> a-b
> [1] -4.440892e-16
>
>
>> (4.1-3.1)-(5.1-4.1)
> [1] -4.440892e-16
>
>
> When this last expression is expanded, I get the right answer:
>
>> 4.1-3.1-5.1+4.1
> [1] 0
>
>
> I am using the binary packaged version R-2.0.0-0.fdr.1.fc2.i386.rpm for
> Linux Fedora Core 2.  I had the same problem with version 1.9.0-0
>
> Can anyone tell me what is going on?
>
> Thanks,
>
>
> Drew Hoysak
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From spencer.graves at pdf.com  Thu Nov 11 19:36:48 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 11 Nov 2004 10:36:48 -0800
Subject: [R] wrong answer for simple expressions
In-Reply-To: <1100193850.2945.0.camel@zippy.local.ca>
References: <1100193850.2945.0.camel@zippy.local.ca>
Message-ID: <4193B140.4020101@pdf.com>

      R does double precision arithmetic and accumulates roundoff error 
like any other double precision computations.  I would therefore expect 
it to accumulate roundoff error as you have reported.  In most cases 
like you mentioned, a difference of 4e-16 is "not material", to use 
Accounting jargon.  If it is an issue, you either need to do error 
analysis or use something like Mathematica that does infinite precision 
arithmetic. 

      hope this helps.  spencer graves

Drew Hoysak wrote:

>I am experiencing strange (to me) output when trying to do simple
>calculations.  Expressions that should equal zero yield non-zero
>values.  
>Examples:
>
>  
>
>>a <- 4.1-3.1
>>b <- 5.1-4.1
>>a-b
>>    
>>
>[1] -4.440892e-16
>
>
>  
>
>>(4.1-3.1)-(5.1-4.1)
>>    
>>
>[1] -4.440892e-16
>
>
>When this last expression is expanded, I get the right answer:
>
>  
>
>>4.1-3.1-5.1+4.1
>>    
>>
>[1] 0
>
>
>I am using the binary packaged version R-2.0.0-0.fdr.1.fc2.i386.rpm for
>Linux Fedora Core 2.  I had the same problem with version 1.9.0-0
>
>Can anyone tell me what is going on?
>
>Thanks,
>
>
>Drew Hoysak
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From jeaneid at chass.utoronto.ca  Thu Nov 11 19:52:48 2004
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Thu, 11 Nov 2004 13:52:48 -0500
Subject: [R] polr probit versus stata oprobit  
In-Reply-To: <Pine.A41.4.61b.0411111020470.76970@homer07.u.washington.edu>
Message-ID: <Pine.SGI.4.40.0411111351590.52804222-100000@origin.chass.utoronto.ca>

Now I understand,
> R gives numbers zero to about 6 digits and Stata gives zero to about 30
> digits.  The intercepts are the same in both packages.


Thank you,


Jean,

On Thu, 11 Nov 2004, Thomas Lumley wrote:

> On Thu, 11 Nov 2004, Jean Eid wrote:
>
> > Thank you Thomas for your answer. It was the weights that are giving me
> > problems and I still have no idea why. i.e. when I try your example,
> > everything work fine. However when I do not include the weights=Freq and
> > [fw=Freq] in both softwares, I do get verry different results.
> >
>
> I still don't understand what example you are using to find the
> difference.  I tried two ways of not using weights
>
> 1)  Expand the data to have a record for each observation (so 1681 rows
> instead of 72).
>      Fitting these expanded data  without weights gives the same answers as
> fitting the compressed data with weights, in both MASS::polr and Stata's
> oprobit.
>
>
> 2) Pretend that the housing data have only 72 observations and ignore the
> weights (though why you would do this...)
>     The true coefficients are all zero in this situation. R gives numbers
> zero to about 6 digits and Stata gives zero to about 30 digits.  The
> intercepts are the same in both packages.
>
>
>  	-thomas
>



From romain at berkeley.edu  Thu Nov 11 19:48:15 2004
From: romain at berkeley.edu (Romain Neugebauer)
Date: Thu, 11 Nov 2004 10:48:15 -0800
Subject: [R] problem building an R package under Windows XP with calls to
Message-ID: <001701c4c81f$00f787a0$14fea8c0@RNlaptop>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041111/db357e62/attachment.pl

From MSchwartz at MedAnalytics.com  Thu Nov 11 20:00:30 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 11 Nov 2004 13:00:30 -0600
Subject: [R] wrong answer for simple expressions
In-Reply-To: <1100193850.2945.0.camel@zippy.local.ca>
References: <1100193850.2945.0.camel@zippy.local.ca>
Message-ID: <1100199630.7525.32.camel@horizons.localdomain>

On Thu, 2004-11-11 at 12:24 -0500, Drew Hoysak wrote: 
> I am experiencing strange (to me) output when trying to do simple
> calculations.  Expressions that should equal zero yield non-zero
> values.  
> Examples:
> 
> > a <- 4.1-3.1
> > b <- 5.1-4.1
> > a-b
> [1] -4.440892e-16
> 
> 
> > (4.1-3.1)-(5.1-4.1)
> [1] -4.440892e-16
> 
> 
> When this last expression is expanded, I get the right answer:
> 
> > 4.1-3.1-5.1+4.1
> [1] 0
> 
> 
> I am using the binary packaged version R-2.0.0-0.fdr.1.fc2.i386.rpm for
> Linux Fedora Core 2.  I had the same problem with version 1.9.0-0

> Can anyone tell me what is going on?

A lack of understanding as to how floating point numbers are represented
by computers under the IEEE 754 floating point standard. 

Hint: Take note of the following:

> print(0.1, digits = 20)
[1] 0.10000000000000000555

> print(4.1, digits = 20)
[1] 4.0999999999999996447

> print(4.1 - 3.1, digits = 20)
[1] 0.99999999999999955591

> print(4.1 - 3.1 - 5.1, digits = 20)
[1] -4.0999999999999996447


Read the last FAQ "Why is 0.1 not 0.1?" here:

http://grouper.ieee.org/groups/754/faq.html#binary-decimal

and read David Goldberg's article, "What Every Computer Scientist Should
Know about Floating-Point Arithmetic", which is available here:

http://grouper.ieee.org/groups/754/

in a Postscript file or here in an edited form in HTML:

http://docs.sun.com/source/806-3568/ncg_goldberg.html


HTH,

Marc Schwartz



From ramasamy at cancer.org.uk  Thu Nov 11 20:07:33 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Thu, 11 Nov 2004 19:07:33 +0000
Subject: [R] (no subject)
In-Reply-To: <BAY24-DAV1359mTetXo00017874@hotmail.com>
References: <BAY24-DAV1359mTetXo00017874@hotmail.com>
Message-ID: <1100200053.3055.179.camel@ramasamy.stats>

Please read the posting guide that tells you to a) use a meaningful
subject line b) give a reproducible example if you can c) read the
manuals and help files first

If you have a vector (not a list) of numbers, say x, and you want to
center it on the median you can do

x <- c(1,2,3,4,5)

Then x - median(x) will give you the deviation of x from its median.
Note that operations in R is vectorised, so you do not need to to do
something like "for(i in 1:5) y[i] <- x[i] - median(x)".

Next you want to square this deviation and sum over all elements.

d <- x - median(x)
sum( d^2 )

Or you can do it in one line as  "sum( (x - median(x))^2 )"


On Thu, 2004-11-11 at 16:28, Wei Yang wrote:
> Hi, 
> 
>  
> 
> I have a list of numbers. For each of the numbers, I take sum of squares of
> the numbers centered on the number chosen. If it is less than a certain
> constant, I will take the average of the numbers chosen.  
> 
>  
> 
> Anyone can give me a sample code. You help will be greatly appreciated. 
> 
>  
> 
> Peter
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
-- 
Adaikalavan Ramasamy                    ramasamy at cancer.org.uk
Centre for Statistics in Medicine       http://www.ihs.ox.ac.uk/csm/
Cancer Research UK                      Tel : 01865 226 677
Old Road Campus, Headington, Oxford     Fax : 01865 226 962



From p.murrell at auckland.ac.nz  Thu Nov 11 20:31:10 2004
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Fri, 12 Nov 2004 08:31:10 +1300
Subject: [R] expressions and paste
References: <419393CE.31353.1A1A01C@localhost>
	<Pine.A41.4.61b.0411110917570.195158@homer04.u.washington.edu>
Message-ID: <4193BDFE.5080807@stat.auckland.ac.nz>

Hi


Thomas Lumley wrote:
> On Thu, 11 Nov 2004, Emily Baldock wrote:
> 
>> I have written a function to plot data which will be used for various 
>> different chemistries.
>> A simplified version is:
>> plot_data <- function(risk,levels,chem,sd2,measure){
>>  plot(risk, levels,main=paste ("per", sd2, measure, "\n in usual", chem))
>> }
>> The problem is with the title.
>> This works fine if the variable "chem" is just text, but if it is an 
>> expression then obviously it won't work.
>> I have experimented with various things and I am at a complete loss 
>> for how to insert an expression into the middle of
>> a title.
> 
> 
> With
>  sd2<-10
>  measure<-quote(mu*g*m^{-3})
>  chem<-quote(H[2]*SO[4])
> You can use
>  plot(1,1,main=bquote("per "*.(sd2)*.(measure)*" in usual "*.(chem)))
> or
>  plot(1,1,main=substitute("per "*sd2*measure*" in usual "*chem,
>    list(sd2=sd2,measure=measure,chem=chem)) )
> 
> You don't get the newlines, someone else will have to work this out.


Expressions (for plotting text) don't handle newlines.  A workaround 
would be to do the title "by hand" by calling mtext to place two 
separate expressions on separate lines.  Something like ...

sd2<-10
measure<-quote(mu*g*m^{-3})
chem<-quote(H[2]*SO[4])
plot_data <- function(risk,levels,chem,sd2,measure){
   plot(1,1)
   mtext(substitute("per "*sd2*measure,
                    list(sd2=sd2,measure=measure)),
         side=3, line=2)
   mtext(substitute("in usual "*chem,
                    list(chem=chem)),
         side=3, line=1)
}
plot_data(1, 2, chem, sd2, measure)

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From rsparapa at post.its.mcw.edu  Thu Nov 11 20:57:48 2004
From: rsparapa at post.its.mcw.edu (Rodney Sparapani)
Date: Thu, 11 Nov 2004 13:57:48 -0600 (CST)
Subject: [R] R "sumo" package suggestion
Message-ID: <200411111957.iABJvmO29139@post.its.mcw.edu>

r-help:

I have an R package suggestion.  After spending
several hours the other day installing about a dozen
packages, I had an idea.  In xemacs, there is a
"sumo" package which allows me to install a large
bundle of xemacs packages at one time (about a 120
modes including ESS).  I think R should have a 
similar bundle.  It would be so much easier than
hunting/downloading/installing.  Martin encouraged 
me to send this suggestion to r-help.  In addition, 
he put together a few comments relating to the previous 
times that this, or a similar suggestion, has been 
brought up here.

Martin wrote:

If you search for "install all CRAN packages"
on
	http://maths.newcastle.edu.au/~rking/R/

(the URL which is quickly found from the [Search] sidebar of
http://www.R-project.org/) 

You find things like Greg Warnes 'Makefile'    
http://tolstoy.newcastle.edu.au/R/help/04/04/0723.html
and
http://tolstoy.newcastle.edu.au/R/help/04/04/0616.html
which is from Tony and has the following small function:

  installNewCRANPackages <- function() {
    ## (C) A.J. Rossini, 2002--2004
    test2 <- packageStatus()$avail["Status"]
    install.packages(row.names(test2)[which(test2$Status=="not installed")])
  }

----------

Rodney Sparapani              Medical College of Wisconsin
Sr. Biostatistician           Patient Care & Outcomes Research
rsparapa at mcw.edu              http://www.mcw.edu/pcor
Was 'Name That Tune' rigged?  WWLD -- What Would Lombardi Do



From murdoch at stats.uwo.ca  Thu Nov 11 20:58:53 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 11 Nov 2004 14:58:53 -0500
Subject: FW: [R] problem building an R package under Windows XP with calls
	to NAG C routines
In-Reply-To: <001c01c4c823$5b81f2b0$14fea8c0@RNlaptop>
References: <001c01c4c823$5b81f2b0$14fea8c0@RNlaptop>
Message-ID: <bng7p0he81tcse8srp3h0gsm58lg32ethk@4ax.com>

On Thu, 11 Nov 2004 11:19:25 -0800, "Romain Neugebauer"
<romain at berkeley.edu> wrote :

>Dear Duncan Murdoch,
> 
>I just posted the following message to the R-help mailing list in
>response to your email of October 29th (see below).  Unfortunately it
>looks like my new message was not correctly posted, i.e. it does not
>appear as a reply to yours but as a new thread. 
> 
>I would like to thank you again for your important help. Also, I would
>be pleased to help out by sending "patches" as you suggested but I am
>not sure what you mean by those "patches". If you can get back to me to
>clarify how I could be useful, I would be happy to consider helping if I
>have the appropriate skills.

What I meant by "patches" was corrections to the readme.packages file.
The instructions there are not actively maintained, so they may apply
to an obsolete version of MSVC, or may be incomplete, or may just
contain errors.  Now that things are (almost) working for you, go back
and read the instructions and see if you can improve on them; when
you've done that, send me a copy of the changes (using the "patch"
program if you know how, or just sending me a full copy of the updated
file).

More comments intermixed with your letter below...

> 
>Thank you again,
> 
>Romain
> 
>-----Original Message-----
>From: Romain Neugebauer [mailto:romain at berkeley.edu] 
>Sent: Thursday, November 11, 2004 10:48 AM
>To: 'r-help at stat.math.ethz.ch'
>Subject: [R] problem building an R package under Windows XP with calls
>to
> 
>Thank you very much for your help.
> 
>I was able to figure out a way to make this simple package work and was
>able to build a more useful and complex package as well. I am however
>left with one problem when building that more complex package. Before I
>go on with this issue, here is how I solved my initial problem:
> 
>After creating an R package directory structure, I included in the \src
>directory my test.c file containing the following C code:
> 
>#include <stdio.h>
>#include <nag.h>
>#include <nagg05.h>
> 
>void test()
>{
>  long i;
> 
>  g05ccc();
>  for(i=0;i<10000;i++)printf("Hello the world - NAG random number:
>%lf\n",g05cac());
> 
>}

Read the Writing R Extensions manual to find out how to do output
using Rprintf.

>and a test.R file in the \R directory with the following R code:
> 
>.First.lib<-function(lib,pkg){
>  library.dynam("test",pkg,lib)
>}
> 
>.Last.lib <- function(libpath)
>{
>  library.dynam.unload("test",libpath)
>}
> 
>gotest <- function()
>  {
>    .C("test")
>    return(0)
>  }
> 
>Based on Duncan Murdoch's recommendation, I then included a Makefile.win
>file containing the following code:
> 
>test.dll : test.obj test.def
>      link /dll /def:test.def /out:test.dll test.obj nagcsmt-mkl.lib
>mkl_s.lib mkl_def.lib mkl_lapack.lib advapi32.lib netapi32.lib Rdll.lib
>test.obj : test.c
>      cl /MT /Ox /D "WIN32" /c -I"C:\Program Files\Numerical Algorithms
>Group\CLW3207DA\include" test.c
> 
>I inspired myself from the readme.package file in the \R folder to write
>these lines. Note that this makefile relies on a .def file which
>indicates (at least this is my understanding of what it does) to R how
>to find the test() C function in the test.dll. 

Actually, the .def file is instructions to the linker, telling it
which functions in the DLL that you're producing should be made
visible to callers.  R will only see functions if they are made
visible, and the MS linker chooses what to make visible based on the
.def file.

Duncan Murdoch


>Before realizing the need
>for this .def file, I omitted this file in the Makefile.win. As a
>consequence I was able to build and install the package successfully,
>i.e. without error message. However, when I tried to use it, R would
>give me an error message suggesting that it could not find the test() C
>routine.
> 
>Here is the code included in the test.def that I placed in the \src
>directory:
> 
>LIBRARY test
>EXPORTS
> test
> 
>After doing so I was able to successfully build, install and use the R
>package that I called "test":
> 
>C:\Rdevelop>Rcmd INSTALL test
> 
> 
>---------- Making package test ------------
>  adding build stamp to DESCRIPTION
>  running src/Makefile.win ...
>cl /MT /Ox /D "WIN32" /c -I"C:\Program Files\Numerical Algorithms
>Group\CLW3207DA\include" test.c
>Microsoft (R) 32-bit C/C++ Optimizing Compiler Version 12.00.8804 for
>80x86
>Copyright (C) Microsoft Corp 1984-1998. All rights reserved.
> 
>test.c
>link /dll /def:test.def /out:test.dll test.obj nagcsmt-mkl.lib mkl_s.lib
>mkl_def.lib mkl_lapack.lib advapi32.lib netapi32.lib Rdll.lib
>Microsoft (R) Incremental Linker Version 6.00.8447
>Copyright (C) Microsoft Corp 1992-1998. All rights reserved.
> 
>   Creating library test.lib and object test.exp
>  ... done
>  installing DLL
>  installing R files
>  installing data files
>  installing man source files
>  installing indices
>  not zipping data
>  installing help
> >>> Building/Updating help pages for package 'test'
>     Formats: text html latex example chm
>  f                                 text    html    latex   example
>  adding MD5 sums
> 
>* DONE (test)
> 
> 
>Now, using the same methodology I was able to build a more complex R
>package successfully. The package works very well and returns the
>answers I expect except that it does not display in the R interface the
>things that the C program requests to be printed.
> 
>I this more complex package, I use several calls to "printf" in the C
>program of that package. I also use "cat" calls in the R function of the
>package. It turns out that all calls to "cat" work without problem
>whereas calls to "printf" always fail (i.e. nothing is displayed in R).
>What is also interesting is that the calls to "printf" are however all
>displayed after I quit R. So it looks like it is R that does not allow
>my C program to display text on the standard output. This is very
>surprising because the simple package "test" I described above does work
>just fine even though it also uses "printf" calls.
> 
>Again, any suggestions on what can cause this problem or any
>recommendation on how to solve it will be greatly appreciated.
> 
>Thank you.
> 
>Romain
> 
> 
> 
> 
>You wrote:
> 
>The problem is that MSVC libraries are not compatible with gcc.  The
>gcc linker doesn't know what to do with them, they're in a different
>format than it expects.
> 
>To do static linking, you'll need to use MSVC to compile and link your
>DLL in the package.  This means setting up a Makefile.win so that MSVC
>gets called instead of gcc.  There's some info on this in the
>README.packages file, but it's incomplete, because none of the core
>developers use MSVC.   If you want to add to it, send me the patches.
> 
>Alternatively, just install the package using your "Hello, world" DLL,
>then compile the real DLL using MSVC and put it into the libs
>subdirectory where your package was installed.
> 
>Note that if you want to put your package on CRAN, you'll have to be
>able to build it without proprietary tools.
> 
>Duncan Murdoch
> 
>On Thu, 28 Oct 2004 18:15:46 -0700, "Romain Neugebauer"
>< <https://stat.ethz.ch/mailman/listinfo/r-help> romainn at hotmail.com>
>wrote:
> 
>>Hello all,
>>
>>I was able to create R packages under windows XP in the past using the 
>>dynamic NAG C library for windows XP (Mark 6).
>>
>>Recently, I changed computers and I am now using the static NAG C
>library 
>>for windows XP (Mark 7) to create a simple R package (called "test")
>which 
>>simply returns random numbers simulated using repetitive calls to a NAG
>C 
>>routine (uniform random generator). I use R Version 2.0.0. and
>installed all 
>>the tools needed to build R packages as described in 
>> <http://www.murdochsutherland.com/Rtools/>
>http://www.murdochsutherland.com/Rtools/
>and readme.packages.
> <http://www.murdochsutherland.com/Rtools/> >
> <http://www.murdochsutherland.com/Rtools/> >I was able to successfully
>execute the C program that is part of this 
> <http://www.murdochsutherland.com/Rtools/> >package when compiling it
>using Microsoft visual studio. However, when I now 
> <http://www.murdochsutherland.com/Rtools/> >try to create the R package
>with the command "Rcmd INSTALL test" I obtain 
> <http://www.murdochsutherland.com/Rtools/> >the following:
> <http://www.murdochsutherland.com/Rtools/> >
> <http://www.murdochsutherland.com/Rtools/> >C:\Rdevelop>Rcmd INSTALL
>test
> <http://www.murdochsutherland.com/Rtools/> >
> <http://www.murdochsutherland.com/Rtools/> >
> <http://www.murdochsutherland.com/Rtools/> >---------- Making package
>test ------------
> <http://www.murdochsutherland.com/Rtools/> >  adding build stamp to
>DESCRIPTION
> <http://www.murdochsutherland.com/Rtools/> >  making DLL ...
> <http://www.murdochsutherland.com/Rtools/> >making test.d from test.c
> <http://www.murdochsutherland.com/Rtools/> >gcc   -Ic:/R/rw2000/include
>-Wall -O2 -I"C:\Program Files\Numerical 
> <http://www.murdochsutherland.com/Rtools/> >Algorithms
>Group\CLW3207DA\include"  -c test.c -o test.o
> <http://www.murdochsutherland.com/Rtools/> >ar cr test.a test.o
> <http://www.murdochsutherland.com/Rtools/> >ranlib test.a
> <http://www.murdochsutherland.com/Rtools/> >windres --include-dir
>c:/R/rw2000/include  -i test_res.rc -o test_res.o
> <http://www.murdochsutherland.com/Rtools/> >gcc  --shared -s  -o
>test.dll test.def test.a test_res.o  
> <http://www.murdochsutherland.com/Rtools/> >-Lc:/R/rw2000/src/gnuwin32
>-L"C:\Program Files\Microsoft Visual 
> <http://www.murdochsutherland.com/Rtools/> >Studio\VC98\Lib"
>-L"C:\Program Files\
> <http://www.murdochsutherland.com/Rtools/> >Numerical Algorithms
>Group\CLW3207DA" -L"C:\Program Files\Numerical 
> <http://www.murdochsutherland.com/Rtools/> >Algorithms
>Group\CLW3207DA\mkl\lib" -lLIBCMT -lnagcsmt-mkl -lmkl_s -lmkl_def 
> <http://www.murdochsutherland.com/Rtools/> >-lmkl_lapack
> <http://www.murdochsutherland.com/Rtools/> >-lADVAPI32 -lNETAPI32
>-lg2c -lR
> <http://www.murdochsutherland.com/Rtools/> >Warning: .drectve `%.*s'
>unrecognized
> <http://www.murdochsutherland.com/Rtools/> >Warning: .drectve `%.*s'
>unrecognized
> <http://www.murdochsutherland.com/Rtools/> >Warning: .drectve `%.*s'
>unrecognized
> <http://www.murdochsutherland.com/Rtools/> >Warning: .drectve `%.*s'
>unrecognized
> <http://www.murdochsutherland.com/Rtools/> >Warning: .drectve `%.*s'
>unrecognized
> <http://www.murdochsutherland.com/Rtools/> >Warning: .drectve `%.*s'
>unrecognized
> <http://www.murdochsutherland.com/Rtools/> >Warning: .drectve `%.*s'
>unrecognized
> <http://www.murdochsutherland.com/Rtools/> >Warning: .drectve `%.*s'
>unrecognized
> <http://www.murdochsutherland.com/Rtools/> >Warning: .drectve `%.*s'
>unrecognized
> <http://www.murdochsutherland.com/Rtools/> >Warning: .drectve `%.*s'
>unrecognized
> <http://www.murdochsutherland.com/Rtools/> >Warning: .drectve `%.*s'
>unrecognized
> <http://www.murdochsutherland.com/Rtools/> >Warning: .drectve `%.*s'
>unrecognized
> <http://www.murdochsutherland.com/Rtools/> >Warning: .drectve `%.*s'
>unrecognized
> <http://www.murdochsutherland.com/Rtools/> >Warning: .drectve `%.*s'
>unrecognized
> <http://www.murdochsutherland.com/Rtools/> >Warning: .drectve `%.*s'
>unrecognized
> <http://www.murdochsutherland.com/Rtools/> >Warning: .drectve `%.*s'
>unrecognized
> <http://www.murdochsutherland.com/Rtools/> >Warning: .drectve `%.*s'
>unrecognized
> <http://www.murdochsutherland.com/Rtools/> >Warning: .drectve `%.*s'
>unrecognized
> <http://www.murdochsutherland.com/Rtools/> >Warning: .drectve `%.*s'
>unrecognized
> <http://www.murdochsutherland.com/Rtools/> >Warning: .drectve `%.*s'
>unrecognized
> <http://www.murdochsutherland.com/Rtools/> >Warning: .drectve `%.*s'
>unrecognized
> <http://www.murdochsutherland.com/Rtools/> >Warning: .drectve `%.*s'
>unrecognized
> <http://www.murdochsutherland.com/Rtools/> >Warning: .drectve `%.*s'
>unrecognized
> <http://www.murdochsutherland.com/Rtools/> >Warning: .drectve `%.*s'
>unrecognized
> <http://www.murdochsutherland.com/Rtools/> >Warning: .drectve `%.*s'
>unrecognized
> <http://www.murdochsutherland.com/Rtools/> >Warning: .drectve `%.*s'
>unrecognized
> <http://www.murdochsutherland.com/Rtools/>
>>/mingw/lib/libmingw32.a(main.o)(.text+0x106):main.c: undefined
>reference to 
> <http://www.murdochsutherland.com/Rtools/> >`WinMain at 16'
> <https://stat.ethz.ch/mailman/listinfo/r-help> >make[3]: *** [test.dll]
>Error 1
> <https://stat.ethz.ch/mailman/listinfo/r-help> >make[2]: ***
>[srcDynlib] Error 2
> <https://stat.ethz.ch/mailman/listinfo/r-help> >make[1]: *** [all]
>Error 2
> <https://stat.ethz.ch/mailman/listinfo/r-help> >make: *** [pkg-test]
>Error 2
> <https://stat.ethz.ch/mailman/listinfo/r-help> >*** Installation of
>test failed ***
> <https://stat.ethz.ch/mailman/listinfo/r-help> >
> <https://stat.ethz.ch/mailman/listinfo/r-help> >Removing
>'C:/R/rw2000/library/test'
> <https://stat.ethz.ch/mailman/listinfo/r-help> >Restoring previous
>'C:/R/rw2000/library/test'
> <https://stat.ethz.ch/mailman/listinfo/r-help> >
> <https://stat.ethz.ch/mailman/listinfo/r-help> >
> <https://stat.ethz.ch/mailman/listinfo/r-help> >Interestingly enough,
>if I remove the calls to the NAG C routine in the C 
> <https://stat.ethz.ch/mailman/listinfo/r-help> >program AND if I remove
>the gcc flag "-lLIBCMT", the R package can be 
> <https://stat.ethz.ch/mailman/listinfo/r-help> >created successfully
>and I can use the package in R (it will simply print a 
> <https://stat.ethz.ch/mailman/listinfo/r-help> >"hello the world"
>line). I obtain:
> <https://stat.ethz.ch/mailman/listinfo/r-help> >
> <https://stat.ethz.ch/mailman/listinfo/r-help> >C:\Rdevelop>Rcmd
>INSTALL test
> <https://stat.ethz.ch/mailman/listinfo/r-help> >
> <https://stat.ethz.ch/mailman/listinfo/r-help> >
> <https://stat.ethz.ch/mailman/listinfo/r-help> >---------- Making
>package test ------------
> <https://stat.ethz.ch/mailman/listinfo/r-help> >  adding build stamp to
>DESCRIPTION
> <https://stat.ethz.ch/mailman/listinfo/r-help> >  making DLL ...
> <https://stat.ethz.ch/mailman/listinfo/r-help> >gcc  --shared -s  -o
>test.dll test.def test.a test_res.o  
> <https://stat.ethz.ch/mailman/listinfo/r-help>
>>-Lc:/R/rw2000/src/gnuwin32 -L"C:\Program Files\Microsoft Visual 
> <https://stat.ethz.ch/mailman/listinfo/r-help> >Studio\VC98\Lib"
>-L"C:\Program Files\
> <https://stat.ethz.ch/mailman/listinfo/r-help> >Numerical Algorithms
>Group\CLW3207DA" -L"C:\Program Files\Numerical 
> <https://stat.ethz.ch/mailman/listinfo/r-help> >Algorithms
>Group\CLW3207DA\mkl\lib"  -lnagcsmt-mkl -lmkl_s -lmkl_def 
> <https://stat.ethz.ch/mailman/listinfo/r-help> >-lmkl_lapack  -lADVAPI
> <https://stat.ethz.ch/mailman/listinfo/r-help> >32 -lNETAPI32  -lg2c
>-lR
> <https://stat.ethz.ch/mailman/listinfo/r-help> >  ... DLL made
> <https://stat.ethz.ch/mailman/listinfo/r-help> >  installing DLL
> <https://stat.ethz.ch/mailman/listinfo/r-help> >  installing R files
> <https://stat.ethz.ch/mailman/listinfo/r-help> >  installing data files
> <https://stat.ethz.ch/mailman/listinfo/r-help> >  installing man source
>files
> <https://stat.ethz.ch/mailman/listinfo/r-help> >  installing indices
> <https://stat.ethz.ch/mailman/listinfo/r-help> >  not zipping data
> <https://stat.ethz.ch/mailman/listinfo/r-help> >  installing help
> <https://stat.ethz.ch/mailman/listinfo/r-help> > >>> Building/Updating
>help pages for package 'test'
> <https://stat.ethz.ch/mailman/listinfo/r-help> >     Formats: text html
>latex example chm
> <https://stat.ethz.ch/mailman/listinfo/r-help> >  f
>text    html    latex   example
> <https://stat.ethz.ch/mailman/listinfo/r-help> >  adding MD5 sums
> <https://stat.ethz.ch/mailman/listinfo/r-help> >
> <https://stat.ethz.ch/mailman/listinfo/r-help> >* DONE (test)
> <https://stat.ethz.ch/mailman/listinfo/r-help> >
> <https://stat.ethz.ch/mailman/listinfo/r-help> >If I add the flag
>"-lLIBCMT" (and the C program still does not call any NAG 
> <https://stat.ethz.ch/mailman/listinfo/r-help> >routine) I get the
>following:
> <https://stat.ethz.ch/mailman/listinfo/r-help> >
> <https://stat.ethz.ch/mailman/listinfo/r-help> >C:\Rdevelop>Rcmd
>INSTALL test
> <https://stat.ethz.ch/mailman/listinfo/r-help> >
> <https://stat.ethz.ch/mailman/listinfo/r-help> >
> <https://stat.ethz.ch/mailman/listinfo/r-help> >---------- Making
>package test ------------
> <https://stat.ethz.ch/mailman/listinfo/r-help> >  adding build stamp to
>DESCRIPTION
> <https://stat.ethz.ch/mailman/listinfo/r-help> >  making DLL ...
> <https://stat.ethz.ch/mailman/listinfo/r-help> >making test.d from
>test.c
> <https://stat.ethz.ch/mailman/listinfo/r-help> >gcc
>-Ic:/R/rw2000/include -Wall -O2 -I"C:\Program Files\Numerical 
> <https://stat.ethz.ch/mailman/listinfo/r-help> >Algorithms
>Group\CLW3207DA\include"  -c test.c -o test.o
> <https://stat.ethz.ch/mailman/listinfo/r-help> >test.c: In function
>`test':
> <https://stat.ethz.ch/mailman/listinfo/r-help> >test.c:7: warning:
>unused variable `i'
> <https://stat.ethz.ch/mailman/listinfo/r-help> >ar cr test.a test.o
> <https://stat.ethz.ch/mailman/listinfo/r-help> >ranlib test.a
> <https://stat.ethz.ch/mailman/listinfo/r-help> >gcc  --shared -s  -o
>test.dll test.def test.a test_res.o  
> <https://stat.ethz.ch/mailman/listinfo/r-help>
>>-Lc:/R/rw2000/src/gnuwin32 -L"C:\Program Files\Microsoft Visual 
> <https://stat.ethz.ch/mailman/listinfo/r-help> >Studio\VC98\Lib"
>-L"C:\Program Files\
> <https://stat.ethz.ch/mailman/listinfo/r-help> >Numerical Algorithms
>Group\CLW3207DA" -L"C:\Program Files\Numerical 
> <https://stat.ethz.ch/mailman/listinfo/r-help> >Algorithms
>Group\CLW3207DA\mkl\lib" -lLIBCMT -lnagcsmt-mkl -lmkl_s -lmkl_def 
> <https://stat.ethz.ch/mailman/listinfo/r-help> >-lmkl_lapack
> <https://stat.ethz.ch/mailman/listinfo/r-help> >-lADVAPI32 -lNETAPI32
>-lg2c -lR
> <https://stat.ethz.ch/mailman/listinfo/r-help> >Warning: .drectve
>`%.*s' unrecognized
> <https://stat.ethz.ch/mailman/listinfo/r-help> >Warning: .drectve
>`%.*s' unrecognized
> <https://stat.ethz.ch/mailman/listinfo/r-help>
>>/mingw/lib/libmingw32.a(main.o)(.text+0x106):main.c: undefined
>reference to 
> <https://stat.ethz.ch/mailman/listinfo/r-help> >`WinMain at 16'
> <https://stat.ethz.ch/mailman/listinfo/r-help> >make[3]: *** [test.dll]
>Error 1
> <https://stat.ethz.ch/mailman/listinfo/r-help> >make[2]: ***
>[srcDynlib] Error 2
> <https://stat.ethz.ch/mailman/listinfo/r-help> >make[1]: *** [all]
>Error 2
> <https://stat.ethz.ch/mailman/listinfo/r-help> >make: *** [pkg-test]
>Error 2
> <https://stat.ethz.ch/mailman/listinfo/r-help> >*** Installation of
>test failed ***
> <https://stat.ethz.ch/mailman/listinfo/r-help> >
> <https://stat.ethz.ch/mailman/listinfo/r-help> >Removing
>'C:/R/rw2000/library/test'
> <https://stat.ethz.ch/mailman/listinfo/r-help> >Restoring previous
>'C:/R/rw2000/library/test'
> <https://stat.ethz.ch/mailman/listinfo/r-help> >
> <https://stat.ethz.ch/mailman/listinfo/r-help> >So it appears that the
>problem is not related to the calls to the NAG C 
> <https://stat.ethz.ch/mailman/listinfo/r-help> >routines but to the
>library "LIBCMT" which is required when using the NAG 
> <https://stat.ethz.ch/mailman/listinfo/r-help> >routines.
> <https://stat.ethz.ch/mailman/listinfo/r-help> >I contacted the NAG
>people who recommended to check the version of the 
> <https://stat.ethz.ch/mailman/listinfo/r-help> >binutils and gcc from
>Mingw. I use gcc version 3.3.1 and I update the 
> <https://stat.ethz.ch/mailman/listinfo/r-help> >binutils from
> <https://stat.ethz.ch/mailman/listinfo/r-help>
>>http://www.mingw.org/download.shtml using 
> <http://www.mingw.org/download.shtml>
>>binutils-2.13.90-20021006-2.tar.gz.
> <http://www.mingw.org/download.shtml> >
> <http://www.mingw.org/download.shtml> >Any help would be greatly
>appreciated.
> <http://www.mingw.org/download.shtml> >Note that I am able to build the
>same R package with calls to NAG routines 
> <http://www.mingw.org/download.shtml> >under linux with the static NAG
>C library for linux.
> <http://www.mingw.org/download.shtml> >
> <http://www.mingw.org/download.shtml> >Thank you,
> <http://www.mingw.org/download.shtml> >
> <http://www.mingw.org/download.shtml> >Romain
> <http://www.mingw.org/download.shtml> >
> <http://www.mingw.org/download.shtml>
>>______________________________________________
> <http://www.mingw.org/download.shtml> >R-help at stat.math.ethz.ch
>mailing list
> <https://stat.ethz.ch/mailman/listinfo/r-help>
>>https://stat.ethz.ch/mailman/listinfo/r-help
> <https://stat.ethz.ch/mailman/listinfo/r-help> >PLEASE do read the
>posting guide! http://www.R-project.org/posting-guide.html
> <http://www.R-project.org/posting-guide.html>  
> <http://www.R-project.org/posting-guide.html>  
> <http://www.R-project.org/posting-guide.html>  
> <http://www.R-project.org/posting-guide.html>



From Roger.Bivand at nhh.no  Thu Nov 11 21:03:48 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 11 Nov 2004 21:03:48 +0100 (CET)
Subject: [R] OLS error
In-Reply-To: <33976.141.211.77.129.1100194178.squirrel@cafemail.giscafe.com>
Message-ID: <Pine.LNX.4.44.0411112045500.1104-100000@reclus.nhh.no>

On Thu, 11 Nov 2004, DrakeGis wrote:

> Hi,
>   I have 142 observations off different variables and I'm trying to do a
> OLS. And I get this error. Any Ideas ?

Well, the posting guide does suggest that precision in the question should
help. The function you are using called ols() seems to be from a local or
contributed package (perhaps Design), since the standard function is lm().  
So in addition to telling us which version of R you are using, you could
say which package you are using, and provide a reproducible example.

If you look at help(La.chol2inv), you will see that the function takes
arguments: La.chol2inv(x, size = ncol(x)), where the x provided within the
ols() function with your data has only one row. Could you perhaps make a
data frame out of your variables, and use the data= argument? Are there
many missing values in your variables? Have you tried complete.cases() on 
your input data?

> 
> > f <- ols(lnmigr3~popden78+income+modern+spareha+rain)
> Error in La.chol2inv(x, size) : size cannot exceed nrow(x) = 1
> 
> 
>   D.
> 
> 
> -----------------------------------------
> Stay ahead of the information curve.
> Receive GIS news and jobs on your desktop daily.
> Subscribe today to the GIS CafeNews newsletter.
> [ http://www10.giscafe.com/nl/newsletter_subscribe.php ]
> It's informative and essential.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From andy_liaw at merck.com  Thu Nov 11 21:59:09 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 11 Nov 2004 15:59:09 -0500
Subject: [R] R "sumo" package suggestion
Message-ID: <3A822319EB35174CA3714066D590DCD50994E2D9@usrymx25.merck.com>

Good idea, IMHO, but there are some practical difficulties:

I guess the XEmacs packages are (most, if not all) pure elisp code, and do
not need other stuff to work.  However, quite a few CRAN packages depend on
external libraries or programs, and do not necessarily work on all platforms
that R runs on.  How would such dependencies be resolved in such a kitchen
sink bundle?

I have a somewhat related idea:  Start labelling packages with a set of
pre-defined categories, and a package can be labelled with more than one
categories (especially those *misc type packages).  It is then possible to
have facility to let people install all packages that fall in a particular
category (e.g., `spatial statistics').  I believe several systems have such
facilities, Debian being one of them, TeXLive being another.

Just my $0.02...

Andy

> From: Rodney Sparapani
> 
> r-help:
> 
> I have an R package suggestion.  After spending
> several hours the other day installing about a dozen
> packages, I had an idea.  In xemacs, there is a
> "sumo" package which allows me to install a large
> bundle of xemacs packages at one time (about a 120
> modes including ESS).  I think R should have a 
> similar bundle.  It would be so much easier than
> hunting/downloading/installing.  Martin encouraged 
> me to send this suggestion to r-help.  In addition, 
> he put together a few comments relating to the previous 
> times that this, or a similar suggestion, has been 
> brought up here.
> 
> Martin wrote:
> 
> If you search for "install all CRAN packages"
> on
> 	http://maths.newcastle.edu.au/~rking/R/
> 
> (the URL which is quickly found from the [Search] sidebar of
> http://www.R-project.org/) 
> 
> You find things like Greg Warnes 'Makefile'    
> http://tolstoy.newcastle.edu.au/R/help/04/04/0723.html
> and
> http://tolstoy.newcastle.edu.au/R/help/04/04/0616.html
> which is from Tony and has the following small function:
> 
>   installNewCRANPackages <- function() {
>     ## (C) A.J. Rossini, 2002--2004
>     test2 <- packageStatus()$avail["Status"]
>     
> install.packages(row.names(test2)[which(test2$Status=="not 
> installed")])
>   }
> 
> ----------
> 
> Rodney Sparapani              Medical College of Wisconsin
> Sr. Biostatistician           Patient Care & Outcomes Research
> rsparapa at mcw.edu              http://www.mcw.edu/pcor
> Was 'Name That Tune' rigged?  WWLD -- What Would Lombardi Do
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Ted.Harding at nessie.mcc.ac.uk  Thu Nov 11 21:56:08 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 11 Nov 2004 20:56:08 -0000 (GMT)
Subject: [R] (no subject)
In-Reply-To: <BAY24-DAV1359mTetXo00017874@hotmail.com>
Message-ID: <XFMail.041111205608.Ted.Harding@nessie.mcc.ac.uk>

On 11-Nov-04 Wei Yang wrote:
> Hi, 
> 
> I have a list of numbers. For each of the numbers, I take
> sum of squares of the numbers centered on the number chosen.
> If it is less than a certain constant, I will take the
> average of the numbers chosen.  
> 
> Anyone can give me a sample code. You help will be greatly appreciated.
> 
> Peter

Let X = (x1, x2, ... , xn} be your list of numbers.

It seems that what you are looking for is the mean of the set Y:

  Y = {y1, y2, ... , ym} such that, for each yj,

    yj is in X and sum[over i]( (xi - yj)^2 ) < const

Is this right?

If so, you can do it straightforwardly with a loop, like:

  x<-rnorm(100)
  const <-150
  for( i in (1:100) ) {if(sum(x-x[i])^2<const) Y[i]<-x[i]}
  Y[!is.na(Y)]
  [1]  0.17096364 -0.32720155  0.19542299  0.13363724
  [5] -0.19961480 -0.24486536 -0.31485802 -0.33369635
  [9]  0.09981291  0.04263151  0.11127977  0.12144595
 [13] -0.27767009 -0.01242218  0.06244776  0.11646301

  mean(Y[!is.na(Y)])
  [1] -0.04101397

but I'm sure somebody out there will come up with a much
more elegant solution!

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 11-Nov-04                                       Time: 20:56:08
------------------------------ XFMail ------------------------------



From ripley at stats.ox.ac.uk  Thu Nov 11 22:04:23 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 11 Nov 2004 21:04:23 +0000 (GMT)
Subject: [R] problem building an R package under Windows XP with calls to
In-Reply-To: <001701c4c81f$00f787a0$14fea8c0@RNlaptop>
Message-ID: <Pine.GSO.4.31.0411112058190.28731-100000@toucan.stats>

Please do read the posting guide: that clearly indicates you sent this to
the wrong list.

On Thu, 11 Nov 2004, Romain Neugebauer wrote:

[Comments removed about how you didn't know how to use VC++, a compiler we
don't recommend but for which e.g. `S Programming' has extensive
examples.]

> I this more complex package, I use several calls to "printf" in the C
> program of that package. I also use "cat" calls in the R function of the
> package. It turns out that all calls to "cat" work without problem
> whereas calls to "printf" always fail (i.e. nothing is displayed in R).
> What is also interesting is that the calls to "printf" are however all
> displayed after I quit R. So it looks like it is R that does not allow
> my C program to display text on the standard output. This is very
> surprising because the simple package "test" I described above does work
> just fine even though it also uses "printf" calls.

The answer to your question is `use Rprintf'.  Please read `Writing R
Extensions', carefully, and you will be less surprised.
Reading the rw-FAQ would help, too.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From srivas at ksu.edu  Thu Nov 11 22:09:32 2004
From: srivas at ksu.edu (Sivakumar Mohandass)
Date: Thu, 11 Nov 2004 15:09:32 -0600
Subject: [R] Background Colour in Lattice Plots
Message-ID: <200411112107.iABL7uuZ012514@mail-h12-02.cc.ksu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041111/ae32a779/attachment.pl

From ripley at stats.ox.ac.uk  Thu Nov 11 22:08:49 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 11 Nov 2004 21:08:49 +0000 (GMT)
Subject: [R] R "sumo" package suggestion
In-Reply-To: <200411111957.iABJvmO29139@post.its.mcw.edu>
Message-ID: <Pine.GSO.4.31.0411112105280.28731-100000@toucan.stats>

What exactly is the suggestion?

R does have package bundles.  It ships with a basic collection of
packages.

install.packages will install many packages at once.

R-devel has a function new.packages to find the currently uninstalled
packages, so install.packages(new.packages()) installs them all, should
they actually all work on your system (and that's impossible as several
require a Unix-alike and one requires Windows)


On Thu, 11 Nov 2004, Rodney Sparapani wrote:

> r-help:
>
> I have an R package suggestion.  After spending
> several hours the other day installing about a dozen
> packages, I had an idea.  In xemacs, there is a
> "sumo" package which allows me to install a large
> bundle of xemacs packages at one time (about a 120
> modes including ESS).  I think R should have a
> similar bundle.  It would be so much easier than
> hunting/downloading/installing.  Martin encouraged
> me to send this suggestion to r-help.  In addition,
> he put together a few comments relating to the previous
> times that this, or a similar suggestion, has been
> brought up here.
>
> Martin wrote:
>
> If you search for "install all CRAN packages"
> on
> 	http://maths.newcastle.edu.au/~rking/R/
>
> (the URL which is quickly found from the [Search] sidebar of
> http://www.R-project.org/)
>
> You find things like Greg Warnes 'Makefile'
> http://tolstoy.newcastle.edu.au/R/help/04/04/0723.html
> and
> http://tolstoy.newcastle.edu.au/R/help/04/04/0616.html
> which is from Tony and has the following small function:
>
>   installNewCRANPackages <- function() {
>     ## (C) A.J. Rossini, 2002--2004
>     test2 <- packageStatus()$avail["Status"]
>     install.packages(row.names(test2)[which(test2$Status=="not installed")])
>   }
>
> ----------
>
> Rodney Sparapani              Medical College of Wisconsin
> Sr. Biostatistician           Patient Care & Outcomes Research
> rsparapa at mcw.edu              http://www.mcw.edu/pcor
> Was 'Name That Tune' rigged?  WWLD -- What Would Lombardi Do
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Achim.Zeileis at wu-wien.ac.at  Thu Nov 11 22:21:14 2004
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 11 Nov 2004 22:21:14 +0100
Subject: [R] R "sumo" package suggestion
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E2D9@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E2D9@usrymx25.merck.com>
Message-ID: <20041111222114.5a4f605e.Achim.Zeileis@wu-wien.ac.at>

On Thu, 11 Nov 2004 15:59:09 -0500 Liaw, Andy wrote:

> Good idea, IMHO, but there are some practical difficulties:
> 
> I guess the XEmacs packages are (most, if not all) pure elisp code,
> and do not need other stuff to work.  However, quite a few CRAN
> packages depend on external libraries or programs, and do not
> necessarily work on all platforms that R runs on.  How would such
> dependencies be resolved in such a kitchen sink bundle?
> 
> I have a somewhat related idea:  Start labelling packages with a set
> of pre-defined categories, and a package can be labelled with more
> than one categories (especially those *misc type packages).  It is
> then possible to have facility to let people install all packages that
> fall in a particular category (e.g., `spatial statistics').  I believe
> several systems have such facilities, Debian being one of them,
> TeXLive being another.

This is similar to idea that has been discussed from time to time for
several years now: it would be nice to have maintained "CRAN task views"
(or something like that), i.e., we could have a maintainer for, say
"spatial stats", another one for "machine learning", "biostats" which
can of course be overlapping. Then the maintainers would have to produce
some sort of list of packages (in a standardized format) with a little
bit of markup such that a web page can be generated from it and that the
information could be used by install.packages().
I think most users would profit from that, but nobody has done the work
to provide the infrastructure so far. I've just discussed this with Kurt
again, a week ago or so...I wanted to play around with some ideas, but
didn't get round to really do something yet. But hopefully, I'll get
round to work on this in the next weeks.
Z

> Just my $0.02...
> 
> Andy
> 
> > From: Rodney Sparapani
> > 
> > r-help:
> > 
> > I have an R package suggestion.  After spending
> > several hours the other day installing about a dozen
> > packages, I had an idea.  In xemacs, there is a
> > "sumo" package which allows me to install a large
> > bundle of xemacs packages at one time (about a 120
> > modes including ESS).  I think R should have a 
> > similar bundle.  It would be so much easier than
> > hunting/downloading/installing.  Martin encouraged 
> > me to send this suggestion to r-help.  In addition, 
> > he put together a few comments relating to the previous 
> > times that this, or a similar suggestion, has been 
> > brought up here.
> > 
> > Martin wrote:
> > 
> > If you search for "install all CRAN packages"
> > on
> > 	http://maths.newcastle.edu.au/~rking/R/
> > 
> > (the URL which is quickly found from the [Search] sidebar of
> > http://www.R-project.org/) 
> > 
> > You find things like Greg Warnes 'Makefile'    
> > http://tolstoy.newcastle.edu.au/R/help/04/04/0723.html
> > and
> > http://tolstoy.newcastle.edu.au/R/help/04/04/0616.html
> > which is from Tony and has the following small function:
> > 
> >   installNewCRANPackages <- function() {
> >     ## (C) A.J. Rossini, 2002--2004
> >     test2 <- packageStatus()$avail["Status"]
> >     
> > install.packages(row.names(test2)[which(test2$Status=="not 
> > installed")])
> >   }
> > 
> > ----------
> > 
> > Rodney Sparapani              Medical College of Wisconsin
> > Sr. Biostatistician           Patient Care & Outcomes Research
> > rsparapa at mcw.edu              http://www.mcw.edu/pcor
> > Was 'Name That Tune' rigged?  WWLD -- What Would Lombardi Do
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From sundar.dorai-raj at pdf.com  Thu Nov 11 22:26:38 2004
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 11 Nov 2004 13:26:38 -0800
Subject: [R] Background Colour in Lattice Plots
In-Reply-To: <200411112107.iABL7uuZ012514@mail-h12-02.cc.ksu.edu>
References: <200411112107.iABL7uuZ012514@mail-h12-02.cc.ksu.edu>
Message-ID: <4193D90E.1010101@pdf.com>



Sivakumar Mohandass wrote:

> Dear all,
> 
> How can I change the default background to white in lattice plots?
> 
> Thanks,
> Shiva.
> 

Hi Shiva,

Depending on what version of R/Lattice you have, you can do the following:

# R-2.0.0/Lattice 0.10-13
trellis.par.set(theme = col.whitebg())

# pre R-2.0.0
lset(col.whitebg())

--sundar



From deepayan at stat.wisc.edu  Thu Nov 11 22:36:23 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 11 Nov 2004 15:36:23 -0600
Subject: [R] Background Colour in Lattice Plots
In-Reply-To: <200411112107.iABL7uuZ012514@mail-h12-02.cc.ksu.edu>
References: <200411112107.iABL7uuZ012514@mail-h12-02.cc.ksu.edu>
Message-ID: <200411111536.23345.deepayan@stat.wisc.edu>

On Thursday 11 November 2004 15:09, Sivakumar Mohandass wrote:
> Dear all,
>
> How can I change the default background to white in lattice plots?

One possibility: start with 

> library(lattice)
> lattice.options(default.theme = col.whitebg()) 

For more, read ?trellis.device.

Deepayan



From Ted.Harding at nessie.mcc.ac.uk  Thu Nov 11 22:23:33 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 11 Nov 2004 21:23:33 -0000 (GMT)
Subject: [R] (no subject)
In-Reply-To: <XFMail.041111205608.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <XFMail.041111212333.Ted.Harding@nessie.mcc.ac.uk>

Sorry! I had omitted to copy in an essential line in the code
below:

On 11-Nov-04 Ted Harding wrote:
> On 11-Nov-04 Wei Yang wrote:
>> Hi, 
>> 
>> I have a list of numbers. For each of the numbers, I take
>> sum of squares of the numbers centered on the number chosen.
>> If it is less than a certain constant, I will take the
>> average of the numbers chosen.  
>> 
>> Anyone can give me a sample code. You help will be greatly
>> appreciated.

   Y<-rep(NA,100) ##### This line is needed!
   x<-rnorm(100)
   const <-150
   for( i in (1:100) ) {if(sum(x-x[i])^2<const) Y[i]<-x[i]}
   Y[!is.na(Y)]
   [1]  0.17096364 -0.32720155  0.19542299  0.13363724
   [5] -0.19961480 -0.24486536 -0.31485802 -0.33369635
   [9]  0.09981291  0.04263151  0.11127977  0.12144595
  [13] -0.27767009 -0.01242218  0.06244776  0.11646301
 
   mean(Y[!is.na(Y)])
   [1] -0.04101397
 
> but I'm sure somebody out there will come up with a much
> more elegant solution!
> 
> Best wishes,
> Ted.
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
> Date: 11-Nov-04                                       Time: 20:56:08
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 11-Nov-04                                       Time: 21:23:33
------------------------------ XFMail ------------------------------



From murdoch at stats.uwo.ca  Thu Nov 11 22:54:46 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 11 Nov 2004 16:54:46 -0500
Subject: FW: [R] problem building an R package under Windows XP with calls
	to NAG C routines
In-Reply-To: <bng7p0he81tcse8srp3h0gsm58lg32ethk@4ax.com>
References: <001c01c4c823$5b81f2b0$14fea8c0@RNlaptop>
	<bng7p0he81tcse8srp3h0gsm58lg32ethk@4ax.com>
Message-ID: <ipn7p01ckvpmhsr6lltu6mi0r7eovcpm5t@4ax.com>

On Thu, 11 Nov 2004 14:58:53 -0500, Duncan Murdoch
<murdoch at stats.uwo.ca> wrote :

>What I meant by "patches" was corrections to the readme.packages file.
>The instructions there are not actively maintained, so they may apply
>to an obsolete version of MSVC, or may be incomplete, or may just
>contain errors.

Sorry, this was wrong.  I'm not using or updating the MSVC
instructions in the README.packages file, but Brian Ripley is still
doing so.  They should work, patches are unnecessary.

Duncan Murdoch



From drakegis at dacafe.com  Thu Nov 11 22:59:40 2004
From: drakegis at dacafe.com (DrakeGis)
Date: Thu, 11 Nov 2004 16:59:40 -0500 (EST)
Subject: [R] OLS error
In-Reply-To: <Pine.LNX.4.44.0411112045500.1104-100000@reclus.nhh.no>
References: <33976.141.211.77.129.1100194178.squirrel@cafemail.giscafe.com>
	<Pine.LNX.4.44.0411112045500.1104-100000@reclus.nhh.no>
Message-ID: <34874.141.211.77.129.1100210380.squirrel@cafemail.giscafe.com>

Thanks for the help.
-I'm really sorry, but I'm affraid I can't publish any data in order to
allow a reproduction of the results (enterprise policies :( ).
-I'm using ols() from Design (Is there another ?)
-Does version of R influence on the behavior of the packages ? I didn't
know, I'm using Version 2.0.0  (2004-10-04), ISBN 3-900051-07-0


> So in addition to telling us which version of R you are using, you could
> say which package you are using, and provide a reproducible example.

> If you look at help(La.chol2inv), you will see that the function takes
> arguments: La.chol2inv(x, size = ncol(x)), where the x provided within the
> ols() function with your data has only one row. Could you perhaps make a
> data frame out of your variables, and use the data= argument? Are there
> many missing values in your variables? Have you tried complete.cases() on
> your input data?

-I have no missing values. I previously try to use a data.frame but the
was other error message, and because of that I split the data.frame,
finally after you suggestions, I create a new data.frame and still didn't
work until I attached a the new data.frame and it works.

Thanks

   D.



-----------------------------------------
Stay ahead of the information curve.
Receive GIS news and jobs on your desktop daily.
Subscribe today to the GIS CafeNews newsletter.
[ http://www10.giscafe.com/nl/newsletter_subscribe.php ]
It's informative and essential.



From fhduan at gmail.com  Fri Nov 12 00:00:31 2004
From: fhduan at gmail.com (Frank Duan)
Date: Thu, 11 Nov 2004 18:00:31 -0500
Subject: [R] Could anyone tell me how to plot a hierarchical clustering in a
	black-white color?
Message-ID: <3b9172310411111500579ad96e@mail.gmail.com>

Dear R people,

I am trying to do a hierarchical clustering to a matrix using
heatmap() function. But I like to have the plot to display in a
black-white manner? Could anyone tell me how to do that?

Thank you.

Frank



From andy_liaw at merck.com  Fri Nov 12 00:14:15 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 11 Nov 2004 18:14:15 -0500
Subject: [R] Could anyone tell me how to plot a hierarchical
	clusterin g in a black-white color?
Message-ID: <3A822319EB35174CA3714066D590DCD50994E2DA@usrymx25.merck.com>

Try using the argument col=gray(seq(0, 1, length=64)), or something like
that.

Andy

> From: Frank Duan
> 
> Dear R people,
> 
> I am trying to do a hierarchical clustering to a matrix using
> heatmap() function. But I like to have the plot to display in a
> black-white manner? Could anyone tell me how to do that?
> 
> Thank you.
> 
> Frank
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From sundar.dorai-raj at pdf.com  Fri Nov 12 00:19:19 2004
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Thu, 11 Nov 2004 15:19:19 -0800
Subject: [R] Could anyone tell me how to plot a hierarchical clustering
	in a	black-white color?
In-Reply-To: <3b9172310411111500579ad96e@mail.gmail.com>
References: <3b9172310411111500579ad96e@mail.gmail.com>
Message-ID: <4193F377.4090807@pdf.com>



Frank Duan wrote:

> Dear R people,
> 
> I am trying to do a hierarchical clustering to a matrix using
> heatmap() function. But I like to have the plot to display in a
> black-white manner? Could anyone tell me how to do that?
> 
> Thank you.
> 
> Frank
> 

Will ?gray do what you need?

# from ?heatmap
require(graphics)
x  <- as.matrix(mtcars)
rc <- gray(seq(0, 1, len = nrow(x)))
cc <- gray(seq(0, 1, len = ncol(x)))
col <- gray(seq(0, 1, len = 256))
hv <- heatmap(x, col = col, scale="column",
               RowSideColors = rc, ColSideColors = cc, margin=c(5,10),
               xlab = "specification variables", ylab= "Car Models",
               main = "heatmap(<Mtcars data>, ..., scale = \"column\")")

This is for R-2.0.0 on Win2000.

--sundar



From fhduan at gmail.com  Fri Nov 12 01:08:44 2004
From: fhduan at gmail.com (Frank Duan)
Date: Thu, 11 Nov 2004 19:08:44 -0500
Subject: [R] Could anyone tell me how to plot a hierarchical clustering in
	a black-white color?
In-Reply-To: <4193F377.4090807@pdf.com>
References: <3b9172310411111500579ad96e@mail.gmail.com>
	<4193F377.4090807@pdf.com>
Message-ID: <3b91723104111116087e9b5e31@mail.gmail.com>

Thank you very much, Sundar. That's exactly what I want.

Frank


On Thu, 11 Nov 2004 15:19:19 -0800, Sundar Dorai-Raj
<sundar.dorai-raj at pdf.com> wrote:
> 
> 
> 
> 
> Frank Duan wrote:
> 
> > Dear R people,
> >
> > I am trying to do a hierarchical clustering to a matrix using
> > heatmap() function. But I like to have the plot to display in a
> > black-white manner? Could anyone tell me how to do that?
> >
> > Thank you.
> >
> > Frank
> >
> 
> Will ?gray do what you need?
> 
> # from ?heatmap
> require(graphics)
> x  <- as.matrix(mtcars)
> rc <- gray(seq(0, 1, len = nrow(x)))
> cc <- gray(seq(0, 1, len = ncol(x)))
> col <- gray(seq(0, 1, len = 256))
> hv <- heatmap(x, col = col, scale="column",
>               RowSideColors = rc, ColSideColors = cc, margin=c(5,10),
>               xlab = "specification variables", ylab= "Car Models",
>               main = "heatmap(<Mtcars data>, ..., scale = \"column\")")
> 
> This is for R-2.0.0 on Win2000.
> 
> --sundar
> 
>



From f.harrell at vanderbilt.edu  Fri Nov 12 02:39:54 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 11 Nov 2004 19:39:54 -0600
Subject: [R] OLS error
In-Reply-To: <34874.141.211.77.129.1100210380.squirrel@cafemail.giscafe.com>
References: <33976.141.211.77.129.1100194178.squirrel@cafemail.giscafe.com>	<Pine.LNX.4.44.0411112045500.1104-100000@reclus.nhh.no>
	<34874.141.211.77.129.1100210380.squirrel@cafemail.giscafe.com>
Message-ID: <4194146A.5020103@vanderbilt.edu>

DrakeGis wrote:
> Thanks for the help.
> -I'm really sorry, but I'm affraid I can't publish any data in order to
> allow a reproduction of the results (enterprise policies :( ).

That is silly.  You can surely simulate data that provides an example of 
the problem you are having.

Frank Harrell



From ok at cs.otago.ac.nz  Fri Nov 12 03:04:02 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Fri, 12 Nov 2004 15:04:02 +1300 (NZDT)
Subject: [R] (no subject)
Message-ID: <200411120204.iAC242db127449@atlas.otago.ac.nz>

	On 11-Nov-04 Wei Yang wrote:
	> Hi, 
	> 
	> I have a list of numbers. For each of the numbers, I take
	> sum of squares of the numbers centered on the number chosen.
	> If it is less than a certain constant, I will take the
	> average of the numbers chosen.  

Assuming I've understood this correctly, one approach is

    mean(v[k > sapply(v, function (x) sum((v-x)^2))])

where v is the vector of numbers
  and k is the "certain constant".
However, this formulation requires O(length(v)^2) time,
which means that it is not a particularly efficient way to do it.

What to me is far more interesting is WHY is this calculation to be done?
If you think about it, if v is sorted, the "k > sapply(...)" part will be
FALSE... TRUE... FALSE...
so this is an arithmetic mean of a "central" subset of values.  Why not just
use an ordinary trimmed mean (see ?mean to find out about the trim= argument)?
Or an M-estimator if some other robust estimate of location is wanted?
I ask this in all seriousness, because in the few quick experiments I tried,
this estimator was _further_ from the population mean than the classical mean.
Is that the point of it?



From wang at galton.uchicago.edu  Fri Nov 12 03:24:57 2004
From: wang at galton.uchicago.edu (Yong Wang)
Date: Thu, 11 Nov 2004 20:24:57 -0600 (CST)
Subject: [R] How to updating R to the newest version conveniently
Message-ID: <Pine.LNX.4.61.0411112020240.5379@aitken.uchicago.edu>

Dear R users
I have been using R for a while. However, I don't know what is the 
convenient way to update R to the newest version while keep all packages 
I previously downloaded and installed from CRAN, if updating all those 
packages the same will be even better.
for the time being, I reinstall all those package evrytime after updating 
the version.

Thank you.

best regards
yong



From 0034058 at fudan.edu.cn  Fri Nov 12 03:35:32 2004
From: 0034058 at fudan.edu.cn (ronggui)
Date: Fri, 12 Nov 2004 10:35:32 +0800
Subject: [R] can i copy the graphics to openoffice?
Message-ID: <200411121035.33007.0034058@fudan.edu.cn>

under windows,i can plot a graphics in windows() device and then copy it to 
ms-office.but it under linux,can i do similar things?
i want to use plot() to draw a graphic in X11() so i can have a look to make 
sure if that's what i want,if so,i then want to copy it to openoffice. can i?
i know ,i can use plot graphic into jpeg() ,but if i do so,i can not see the 
graphic directly.that's not what i want.
any suggestion?
thank you.



From liuwensui at gmail.com  Fri Nov 12 04:20:16 2004
From: liuwensui at gmail.com (Wensui Liu)
Date: Thu, 11 Nov 2004 22:20:16 -0500
Subject: [R] an off-topic question -> model validation
Message-ID: <1115a2b004111119203a556524@mail.gmail.com>

Currently, I am working on a data mining project and plan to divide
the data table into 2 parts, one for modeling and the other for
validation to compare several models.

But I am not sure about the percentage of data I should use to build
the model and the one I should keep to validate the model.

Is there any literature reference about this topic? 

Thank you so much!



From Tom.Mulholland at dpi.wa.gov.au  Fri Nov 12 04:50:40 2004
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Fri, 12 Nov 2004 11:50:40 +0800
Subject: [R] How to updating R to the newest version conveniently
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA2C@afhex01.dpi.wa.gov.au>

I've seen various answers to this question and there does not seem to be a single best way.

I use a separate library for downloaded packages. In windows I set the R_LIBS environment variable. See the usual suspects such as the appropriate FAQ and the r-admin pdf file. On the exisitng installation I run code like this

myPackages <- .packages(all.available = TRUE,lib.loc = "c:/progs/mylib")
save(myPackages,file = "f:\backup\settings\myPackages.rdata",compress = T)

This stores a list of all the packages in "mylib" so that when a new install comes I can just retrieve my backup and do a new install. When everything is working well a new version can be downloaded in the old directory (having cleaned it out first) and the update from CRAN option in windows can be used. However with R2.0 there was a need to recompile packages so those that did not have a new version did not update, but didn't work with the new version.

load("f:\backup\settings\myPackages.rdata")
install.packages(myPackages,lib = "c:/progs/mylib",
   "CRAN = http://cran.au.r-project.org/")

I can't guarantee the code as I have just put it together from what I recall (this is how I did it at home) I don't have that sort of access to the work PC so I have to get a tech support person to do it all for me and they have to do it manually because they don't understand the process.

Ciao, Tom
   

-----Original Message-----
From: Yong Wang [mailto:wang at galton.uchicago.edu]
Sent: Friday, 12 November 2004 10:25 AM
To: r-help at stat.math.ethz.ch
Subject: [R] How to updating R to the newest version conveniently


Dear R users
I have been using R for a while. However, I don't know what is the 
convenient way to update R to the newest version while keep all packages 
I previously downloaded and installed from CRAN, if updating all those 
packages the same will be even better.
for the time being, I reinstall all those package evrytime after updating 
the version.

Thank you.

best regards
yong

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From f.harrell at vanderbilt.edu  Fri Nov 12 05:51:12 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 11 Nov 2004 22:51:12 -0600
Subject: [R] an off-topic question -> model validation
In-Reply-To: <1115a2b004111119203a556524@mail.gmail.com>
References: <1115a2b004111119203a556524@mail.gmail.com>
Message-ID: <41944140.7070507@vanderbilt.edu>

Wensui Liu wrote:
> Currently, I am working on a data mining project and plan to divide
> the data table into 2 parts, one for modeling and the other for
> validation to compare several models.
> 
> But I am not sure about the percentage of data I should use to build
> the model and the one I should keep to validate the model.
> 
> Is there any literature reference about this topic? 
> 
> Thank you so much!

Data splitting is very inefficient for model validation unless the 
sample size is extremely large.  Consider using Efron's "optimism" 
bootstrap as is used in the validate function in the Design package. 
validate will also do data splitting and cross-validation though.

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From roebuck at odin.mdacc.tmc.edu  Fri Nov 12 06:07:55 2004
From: roebuck at odin.mdacc.tmc.edu (Paul Roebuck)
Date: Thu, 11 Nov 2004 23:07:55 -0600 (CST)
Subject: [R] can i copy the graphics to openoffice?
In-Reply-To: <200411121035.33007.0034058@fudan.edu.cn>
References: <200411121035.33007.0034058@fudan.edu.cn>
Message-ID: <Pine.OSF.4.58.0411112300260.182316@odin.mdacc.tmc.edu>

On Fri, 12 Nov 2004, ronggui wrote:

> under windows,i can plot a graphics in windows() device and then copy it to
> ms-office.but it under linux,can i do similar things?
> i want to use plot() to draw a graphic in X11() so i can have a look to make
> sure if that's what i want,if so,i then want to copy it to openoffice. can i?
> i know ,i can use plot graphic into jpeg() ,but if i do so,i can not see the
> graphic directly.that's not what i want.
> any suggestion?

Why not just plot it twice?
Once to x11() and another to png()/jpeg()/postscript()
or whatever format you desire. If you don't like the
results, don't use the resulting files...

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From ripley at stats.ox.ac.uk  Fri Nov 12 07:47:06 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 12 Nov 2004 06:47:06 +0000 (GMT)
Subject: [R] How to updating R to the newest version conveniently
In-Reply-To: <Pine.LNX.4.61.0411112020240.5379@aitken.uchicago.edu>
Message-ID: <Pine.GSO.4.31.0411120645070.29039-100000@toucan.stats>

On what OS?  If Windows, this is covered in the rw-FAQ.  If other, it
depends on how you install R (from sources, RPMs, etc).

On Thu, 11 Nov 2004, Yong Wang wrote:

> I have been using R for a while. However, I don't know what is the
> convenient way to update R to the newest version while keep all packages
> I previously downloaded and installed from CRAN, if updating all those
> packages the same will be even better.
> for the time being, I reinstall all those package evrytime after updating
> the version.

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

PLEASE do, and give basic information as requested.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From blindglobe at gmail.com  Fri Nov 12 09:09:17 2004
From: blindglobe at gmail.com (A.J. Rossini)
Date: Fri, 12 Nov 2004 09:09:17 +0100
Subject: [R] R "sumo" package suggestion
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E2D9@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E2D9@usrymx25.merck.com>
Message-ID: <1abe3fa904111200095b00349c@mail.gmail.com>

Let them fail.   Not all of the elisp code installed is self contained
in the XEmacs bundles, but you won't use or know about them if you
don't care.

That's exactly what that snippet that I wrote before did -- ignored errors. 

best,
-tony


On Thu, 11 Nov 2004 15:59:09 -0500, Liaw, Andy <andy_liaw at merck.com> wrote:
> Good idea, IMHO, but there are some practical difficulties:
> 
> I guess the XEmacs packages are (most, if not all) pure elisp code, and do
> not need other stuff to work.  However, quite a few CRAN packages depend on
> external libraries or programs, and do not necessarily work on all platforms
> that R runs on.  How would such dependencies be resolved in such a kitchen
> sink bundle?
> 
> I have a somewhat related idea:  Start labelling packages with a set of
> pre-defined categories, and a package can be labelled with more than one
> categories (especially those *misc type packages).  It is then possible to
> have facility to let people install all packages that fall in a particular
> category (e.g., `spatial statistics').  I believe several systems have such
> facilities, Debian being one of them, TeXLive being another.
> 
> Just my $0.02...
> 
> Andy
> 
> > From: Rodney Sparapani
> 
> 
> >
> > r-help:
> >
> > I have an R package suggestion.  After spending
> > several hours the other day installing about a dozen
> > packages, I had an idea.  In xemacs, there is a
> > "sumo" package which allows me to install a large
> > bundle of xemacs packages at one time (about a 120
> > modes including ESS).  I think R should have a
> > similar bundle.  It would be so much easier than
> > hunting/downloading/installing.  Martin encouraged
> > me to send this suggestion to r-help.  In addition,
> > he put together a few comments relating to the previous
> > times that this, or a similar suggestion, has been
> > brought up here.
> >
> > Martin wrote:
> >
> > If you search for "install all CRAN packages"
> > on
> >       http://maths.newcastle.edu.au/~rking/R/
> >
> > (the URL which is quickly found from the [Search] sidebar of
> > http://www.R-project.org/)
> >
> > You find things like Greg Warnes 'Makefile'
> > http://tolstoy.newcastle.edu.au/R/help/04/04/0723.html
> > and
> > http://tolstoy.newcastle.edu.au/R/help/04/04/0616.html
> > which is from Tony and has the following small function:
> >
> >   installNewCRANPackages <- function() {
> >     ## (C) A.J. Rossini, 2002--2004
> >     test2 <- packageStatus()$avail["Status"]
> >
> > install.packages(row.names(test2)[which(test2$Status=="not
> > installed")])
> >   }
> >
> > ----------
> >
> > Rodney Sparapani              Medical College of Wisconsin
> > Sr. Biostatistician           Patient Care & Outcomes Research
> > rsparapa at mcw.edu              http://www.mcw.edu/pcor
> > Was 'Name That Tune' rigged?  WWLD -- What Would Lombardi Do
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 

best,
-tony

---
A.J. Rossini
blindglobe at gmail.com



From Dag.Steinskog at student.uib.no  Fri Nov 12 09:41:33 2004
From: Dag.Steinskog at student.uib.no (Dag.Steinskog@student.uib.no)
Date: Fri, 12 Nov 2004 09:41:33 +0100
Subject: [R] Problems installing ncdf package into R
Message-ID: <1100248893.4194773d99591@webmail.uib.no>

Hello

Can someone please look into the problem with installing ncdf package into R. 
I get this message when I try install it:

> install.packages("ncdf")
trying URL `http://cran.r-project.org/bin/windows/contrib/2.0/PACKAGES'
Content type `text/plain; charset=iso-8859-1' length 22827 bytes
opened URL
downloaded 22Kb

Warning message: 
No package "ncdf" on CRAN. in: download.packages(pkgs, destdir = tmpd, 
available = available,  
>

I checked with the homepage cran.r-project.org, and the package is there. I 
have installed packages earlier into R with no problems. 

In advance, thanks for all help!

Best Wishes
Dag Johan Steinskog
Bjerknes Center of Climate Research - University of Bergen, Norway



From Roger.Bivand at nhh.no  Fri Nov 12 09:46:15 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 12 Nov 2004 09:46:15 +0100 (CET)
Subject: [R] OLS error
In-Reply-To: <34874.141.211.77.129.1100210380.squirrel@cafemail.giscafe.com>
Message-ID: <Pine.LNX.4.44.0411120931280.1715-100000@reclus.nhh.no>

On Thu, 11 Nov 2004, DrakeGis wrote:

> Thanks for the help.
> -I'm really sorry, but I'm affraid I can't publish any data in order to
> allow a reproduction of the results (enterprise policies :( ).

The author of the Design package has already replied to this point. If the 
function only fails with your data, the data are the problem, not the 
function.

> -I'm using ols() from Design (Is there another ?)

The posting guide does say read the appropriate documentation. R is 
distributed with lots of documentation, including "An Introduction to R", 
see -> Statistical models in R -> Linear models. Reading the help page for 
the ols() function from the Design package will show that it adds 
functionality to the base lm() function, which your example is not using, 
so you can certainly use lm() until you need more.

> -Does version of R influence on the behavior of the packages ? I didn't
> know, I'm using Version 2.0.0  (2004-10-04), ISBN 3-900051-07-0
> 

Yes, but this isn't the case here. If you had read the posting guide, you 
would understand that some questions are easier to answer when the version 
is known, and the version is found by typing version at the R prompt, 
surprisingly, and shows basic information about the OS too.

> 
> > So in addition to telling us which version of R you are using, you could
> > say which package you are using, and provide a reproducible example.
> 
> > If you look at help(La.chol2inv), you will see that the function takes
> > arguments: La.chol2inv(x, size = ncol(x)), where the x provided within the
> > ols() function with your data has only one row. Could you perhaps make a
> > data frame out of your variables, and use the data= argument? Are there
> > many missing values in your variables? Have you tried complete.cases() on
> > your input data?
> 
> -I have no missing values. I previously try to use a data.frame but the
> was other error message, and because of that I split the data.frame,
> finally after you suggestions, I create a new data.frame and still didn't
> work until I attached a the new data.frame and it works.

Please copy the contents of the console to your messages. Any of this 
could be caused by simple mistakes, attach() is not needed often (I never 
use it), and especially not for lm() and friends, which have the data= 
argument to be sure that the object in the formula is the one in the data= 
object, not any other with the same name that might be lying around. Try 
running in a fresh session too without loading your old workspace - one or 
other of your RHS variables is not what you think it is.

There is very little doubt that you have problems with your data.  
However, you have not bothered to show the error messages to the list. Try
to go through the same steps using lm() and making the data from runif()  
and rnorm() - simulated data using set.seed() - and then run the examples
for lm(), etc, and if you can demonstrate that the functions still return
errors, the cause should be easier to establish.

> 
> Thanks
> 
>    D.
> 
> 
> 
> -----------------------------------------
> Stay ahead of the information curve.
> Receive GIS news and jobs on your desktop daily.
> Subscribe today to the GIS CafeNews newsletter.
> [ http://www10.giscafe.com/nl/newsletter_subscribe.php ]
> It's informative and essential.
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From petr.pikal at precheza.cz  Fri Nov 12 09:50:57 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 12 Nov 2004 09:50:57 +0100
Subject: [R] wrong answer for simple expressions
In-Reply-To: <1100193850.2945.0.camel@zippy.local.ca>
Message-ID: <41948781.26812.C1B8F4@localhost>

Hi

On 11 Nov 2004 at 12:24, Drew Hoysak wrote:

> I am experiencing strange (to me) output when trying to do simple
> calculations.  Expressions that should equal zero yield non-zero
> values.  Examples:
> 
> > a <- 4.1-3.1
> > b <- 5.1-4.1
> > a-b
> [1] -4.440892e-16
> 
> 
> > (4.1-3.1)-(5.1-4.1)
> [1] -4.440892e-16
> 
> 
> When this last expression is expanded, I get the right answer:
> 
> > 4.1-3.1-5.1+4.1
> [1] 0
> 
> 
> I am using the binary packaged version R-2.0.0-0.fdr.1.fc2.i386.rpm
> for Linux Fedora Core 2.  I had the same problem with version 1.9.0-0
> 
> Can anyone tell me what is going on?

Floating point arithmetic is imprecise.

Cheers
Petr

> 
> Thanks,
> 
> 
> Drew Hoysak
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From phgrosjean at sciviews.org  Fri Nov 12 10:48:04 2004
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Fri, 12 Nov 2004 10:48:04 +0100
Subject: [R] R "sumo" package suggestion
In-Reply-To: <20041111222114.5a4f605e.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <200411120948.iAC9m4Mj012655@outmx021.isp.belgacom.be>

Hello,

I think Achim suggestion is more realistic: it does not imply automatic
installation of all packages, but just a restricted list of packages
available on CRAN about a specific topic.

An easy way to get this result is to propose separate lists of packages. It
means separate lists than
http://cran.r-project.org/src/contrib/PACKAGES.html (and 'PACKAGES' in the
Windows packages binaries). I don't see the problem to propose other lists
that could be called 'SpatialStats.html' (in packages
sources)/'SpatialStats' (in Windows binaries)... and the same for
'MachineLearning', 'Biostats', etc...

Then, of course the various functions that install packages should be
adapted to use these lists. It does not look like an unsurmontable task.

Of course, if this is not done yet by the R Core Team, I presume that there
must be difficulties that I don't see. It is obvious that, either we need a
list maintainer for each topic, or we have to propose keywords for packages
(similar to the keywords for functions) that will be used to automatically
generate those separate lists. 

An alternative that can currently be used for groups of users in an
institution is to maintain a local copy of R packages repository, which
contains only the packages of interest for this group. I do so for my
students. Under Windows, in the new R 2.0.1 beta, there is a new menu entry
in packages -> Set CRAN mirror... (in my version it does not work yet,
looking for a missing .\doc\CRAN_mirrors.csv file), but I can easily figure
out how it works and how I could append my own repository to the list to
ease installation of a restricted list of R packages by my students. This is
only for Windows, but a similar approach can also be used on other platforms
with a little bit of coding.

Best,

Philippe

..............................................<??}))><........
 ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
 ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
 ) ) ) ) )   Mons-Hainaut University, Pentagone
( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
 ) ) ) ) )   6, av du Champ de Mars, 7000 Mons, Belgium  
( ( ( ( (       
 ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.33.12
( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
 ) ) ) ) )      
( ( ( ( (    web:   http://www.umh.ac.be/~econum
 ) ) ) ) )
..............................................................


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Achim Zeileis
> Sent: Thursday, November 11, 2004 10:21 PM
> To: Liaw, Andy
> Cc: r-help at stat.math.ethz.ch; rsparapa at post.its.mcw.edu
> Subject: Re: [R] R "sumo" package suggestion
> 
> On Thu, 11 Nov 2004 15:59:09 -0500 Liaw, Andy wrote:
> 
> > Good idea, IMHO, but there are some practical difficulties:
> > 
> > I guess the XEmacs packages are (most, if not all) pure elisp code, 
> > and do not need other stuff to work.  However, quite a few CRAN 
> > packages depend on external libraries or programs, and do not 
> > necessarily work on all platforms that R runs on.  How would such 
> > dependencies be resolved in such a kitchen sink bundle?
> > 
> > I have a somewhat related idea:  Start labelling packages 
> with a set 
> > of pre-defined categories, and a package can be labelled with more 
> > than one categories (especially those *misc type packages).  It is 
> > then possible to have facility to let people install all 
> packages that 
> > fall in a particular category (e.g., `spatial statistics'). 
>  I believe 
> > several systems have such facilities, Debian being one of them, 
> > TeXLive being another.
> 
> This is similar to idea that has been discussed from time to 
> time for several years now: it would be nice to have 
> maintained "CRAN task views"
> (or something like that), i.e., we could have a maintainer 
> for, say "spatial stats", another one for "machine learning", 
> "biostats" which can of course be overlapping. Then the 
> maintainers would have to produce some sort of list of 
> packages (in a standardized format) with a little bit of 
> markup such that a web page can be generated from it and that 
> the information could be used by install.packages().
> I think most users would profit from that, but nobody has 
> done the work to provide the infrastructure so far. I've just 
> discussed this with Kurt again, a week ago or so...I wanted 
> to play around with some ideas, but didn't get round to 
> really do something yet. But hopefully, I'll get round to 
> work on this in the next weeks.
> Z
> 
> > Just my $0.02...
> > 
> > Andy
> > 
> > > From: Rodney Sparapani
> > > 
> > > r-help:
> > > 
> > > I have an R package suggestion.  After spending several hours the 
> > > other day installing about a dozen packages, I had an idea.  In 
> > > xemacs, there is a "sumo" package which allows me to 
> install a large 
> > > bundle of xemacs packages at one time (about a 120 modes 
> including 
> > > ESS).  I think R should have a similar bundle.  It would 
> be so much 
> > > easier than hunting/downloading/installing.  Martin 
> encouraged me to 
> > > send this suggestion to r-help.  In addition, he put 
> together a few 
> > > comments relating to the previous times that this, or a similar 
> > > suggestion, has been brought up here.
> > > 
> > > Martin wrote:
> > > 
> > > If you search for "install all CRAN packages"
> > > on
> > > 	http://maths.newcastle.edu.au/~rking/R/
> > > 
> > > (the URL which is quickly found from the [Search] sidebar of
> > > http://www.R-project.org/)
> > > 
> > > You find things like Greg Warnes 'Makefile'    
> > > http://tolstoy.newcastle.edu.au/R/help/04/04/0723.html
> > > and
> > > http://tolstoy.newcastle.edu.au/R/help/04/04/0616.html
> > > which is from Tony and has the following small function:
> > > 
> > >   installNewCRANPackages <- function() {
> > >     ## (C) A.J. Rossini, 2002--2004
> > >     test2 <- packageStatus()$avail["Status"]
> > >     
> > > install.packages(row.names(test2)[which(test2$Status=="not
> > > installed")])
> > >   }
> > > 
> > > ----------
> > > 
> > > Rodney Sparapani              Medical College of Wisconsin
> > > Sr. Biostatistician           Patient Care & Outcomes Research
> > > rsparapa at mcw.edu              http://www.mcw.edu/pcor
> > > Was 'Name That Tune' rigged?  WWLD -- What Would Lombardi Do
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list 
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! 
> > > http://www.R-project.org/posting-guide.html
> > > 
> > >
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From a.beckerman at sheffield.ac.uk  Fri Nov 12 11:28:08 2004
From: a.beckerman at sheffield.ac.uk (Andrew Beckerman)
Date: Fri, 12 Nov 2004 10:28:08 +0000
Subject: [R] R "sumo" package suggestion
In-Reply-To: <200411120948.iAC9m4Mj012655@outmx021.isp.belgacom.be>
References: <200411120948.iAC9m4Mj012655@outmx021.isp.belgacom.be>
Message-ID: <8C05066E-3495-11D9-B630-000A95CD7F02@sheffield.ac.uk>

One place to look, linked to the R development community, is the  
Bioconductor project where their GetBioC() function has package  
groupings (see http://www.bioconductor.org/getBioC.R) as an option.   
Perhaps, because of the specific focus of the Bioconductor suite, this  
is easier, but at least a template with R functionality already exists.

andrew

On 12 Nov 2004, at 09:48, Philippe Grosjean wrote:

> Hello,
>
> I think Achim suggestion is more realistic: it does not imply automatic
> installation of all packages, but just a restricted list of packages
> available on CRAN about a specific topic.
>
> An easy way to get this result is to propose separate lists of  
> packages. It
> means separate lists than
> http://cran.r-project.org/src/contrib/PACKAGES.html (and 'PACKAGES' in  
> the
> Windows packages binaries). I don't see the problem to propose other  
> lists
> that could be called 'SpatialStats.html' (in packages
> sources)/'SpatialStats' (in Windows binaries)... and the same for
> 'MachineLearning', 'Biostats', etc...
>
> Then, of course the various functions that install packages should be
> adapted to use these lists. It does not look like an unsurmontable  
> task.
>
> Of course, if this is not done yet by the R Core Team, I presume that  
> there
> must be difficulties that I don't see. It is obvious that, either we  
> need a
> list maintainer for each topic, or we have to propose keywords for  
> packages
> (similar to the keywords for functions) that will be used to  
> automatically
> generate those separate lists.
>
> An alternative that can currently be used for groups of users in an
> institution is to maintain a local copy of R packages repository, which
> contains only the packages of interest for this group. I do so for my
> students. Under Windows, in the new R 2.0.1 beta, there is a new menu  
> entry
> in packages -> Set CRAN mirror... (in my version it does not work yet,
> looking for a missing .\doc\CRAN_mirrors.csv file), but I can easily  
> figure
> out how it works and how I could append my own repository to the list  
> to
> ease installation of a restricted list of R packages by my students.  
> This is
> only for Windows, but a similar approach can also be used on other  
> platforms
> with a little bit of coding.
>
> Best,
>
> Philippe
>
> ..............................................<??}))><........
>  ) ) ) ) )
> ( ( ( ( (    Prof. Philippe Grosjean
>  ) ) ) ) )
> ( ( ( ( (    Numerical Ecology of Aquatic Systems
>  ) ) ) ) )   Mons-Hainaut University, Pentagone
> ( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
>  ) ) ) ) )   6, av du Champ de Mars, 7000 Mons, Belgium
> ( ( ( ( (
>  ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.33.12
> ( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
>  ) ) ) ) )
> ( ( ( ( (    web:   http://www.umh.ac.be/~econum
>  ) ) ) ) )
> ..............................................................
>
>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Achim Zeileis
>> Sent: Thursday, November 11, 2004 10:21 PM
>> To: Liaw, Andy
>> Cc: r-help at stat.math.ethz.ch; rsparapa at post.its.mcw.edu
>> Subject: Re: [R] R "sumo" package suggestion
>>
>> On Thu, 11 Nov 2004 15:59:09 -0500 Liaw, Andy wrote:
>>
>>> Good idea, IMHO, but there are some practical difficulties:
>>>
>>> I guess the XEmacs packages are (most, if not all) pure elisp code,
>>> and do not need other stuff to work.  However, quite a few CRAN
>>> packages depend on external libraries or programs, and do not
>>> necessarily work on all platforms that R runs on.  How would such
>>> dependencies be resolved in such a kitchen sink bundle?
>>>
>>> I have a somewhat related idea:  Start labelling packages
>> with a set
>>> of pre-defined categories, and a package can be labelled with more
>>> than one categories (especially those *misc type packages).  It is
>>> then possible to have facility to let people install all
>> packages that
>>> fall in a particular category (e.g., `spatial statistics').
>>  I believe
>>> several systems have such facilities, Debian being one of them,
>>> TeXLive being another.
>>
>> This is similar to idea that has been discussed from time to
>> time for several years now: it would be nice to have
>> maintained "CRAN task views"
>> (or something like that), i.e., we could have a maintainer
>> for, say "spatial stats", another one for "machine learning",
>> "biostats" which can of course be overlapping. Then the
>> maintainers would have to produce some sort of list of
>> packages (in a standardized format) with a little bit of
>> markup such that a web page can be generated from it and that
>> the information could be used by install.packages().
>> I think most users would profit from that, but nobody has
>> done the work to provide the infrastructure so far. I've just
>> discussed this with Kurt again, a week ago or so...I wanted
>> to play around with some ideas, but didn't get round to
>> really do something yet. But hopefully, I'll get round to
>> work on this in the next weeks.
>> Z
>>
>>> Just my $0.02...
>>>
>>> Andy
>>>
>>>> From: Rodney Sparapani
>>>>
>>>> r-help:
>>>>
>>>> I have an R package suggestion.  After spending several hours the
>>>> other day installing about a dozen packages, I had an idea.  In
>>>> xemacs, there is a "sumo" package which allows me to
>> install a large
>>>> bundle of xemacs packages at one time (about a 120 modes
>> including
>>>> ESS).  I think R should have a similar bundle.  It would
>> be so much
>>>> easier than hunting/downloading/installing.  Martin
>> encouraged me to
>>>> send this suggestion to r-help.  In addition, he put
>> together a few
>>>> comments relating to the previous times that this, or a similar
>>>> suggestion, has been brought up here.
>>>>
>>>> Martin wrote:
>>>>
>>>> If you search for "install all CRAN packages"
>>>> on
>>>> 	http://maths.newcastle.edu.au/~rking/R/
>>>>
>>>> (the URL which is quickly found from the [Search] sidebar of
>>>> http://www.R-project.org/)
>>>>
>>>> You find things like Greg Warnes 'Makefile'
>>>> http://tolstoy.newcastle.edu.au/R/help/04/04/0723.html
>>>> and
>>>> http://tolstoy.newcastle.edu.au/R/help/04/04/0616.html
>>>> which is from Tony and has the following small function:
>>>>
>>>>   installNewCRANPackages <- function() {
>>>>     ## (C) A.J. Rossini, 2002--2004
>>>>     test2 <- packageStatus()$avail["Status"]
>>>>
>>>> install.packages(row.names(test2)[which(test2$Status=="not
>>>> installed")])
>>>>   }
>>>>
>>>> ----------
>>>>
>>>> Rodney Sparapani              Medical College of Wisconsin
>>>> Sr. Biostatistician           Patient Care & Outcomes Research
>>>> rsparapa at mcw.edu              http://www.mcw.edu/pcor
>>>> Was 'Name That Tune' rigged?  WWLD -- What Would Lombardi Do
>>>>
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide!
>>>> http://www.R-project.org/posting-guide.html
>>>>
>>>>
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide!
>>> http://www.R-project.org/posting-guide.html
>>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!  
> http://www.R-project.org/posting-guide.html
>
>
------------------------------------------------------------------------ 
---------
Dr. Andrew Beckerman
Department of Animal and Plant Sciences, University of Sheffield,
Alfred Denny Building, Western Bank, Sheffield S10 2TN, UK
ph +44 (0)114 222 0026; fx +44 (0)114 222 0002
http://www.shef.ac.uk/beckslab
------------------------------------------------------------------------ 
----------



From emily.baldock at ctsu.ox.ac.uk  Fri Nov 12 12:26:19 2004
From: emily.baldock at ctsu.ox.ac.uk (Emily Baldock)
Date: Fri, 12 Nov 2004 11:26:19 -0000
Subject: [R] expressions and paste
In-Reply-To: <4193A842.4010406@jhsph.edu>
References: <419393CE.31353.1A1A01C@localhost>
Message-ID: <41949DDB.21124.941BB5@localhost>

I am trying to get a subscript into the title.
My expression is quote(bold(apoA[bold("1")]))
I have managed to get something near to what I want with

sd2 <-1.882
measure <- "g/L"
direction <- "increase"
chem <- quote(bold(apoA[bold("1")]))
plot(0,0)
  titletxt <- substitute(paste("per ", sd2, " ",measure," ", direction, "\n in usual ",chem),
   list(chem = chem,sd2=sd2,measure=measure,direction=direction))
  title(main=titletxt,cex.main=1.4,font.main=2)

However, I can't get bold text this way.

Emily.

On 11 Nov 2004 at 12:58, Roger D. Peng wrote:

> Can you be a bit more specific?  Exactly what kind of symbol are you 
> trying to put in the title?
> 
> -roger
> 
> Emily Baldock wrote:
> > I have written a function to plot data which will be used for various different chemistries.
> > A simplified version is:
> > plot_data <- function(risk,levels,chem,sd2,measure){
> >   plot(risk, levels,main=paste ("per", sd2, measure, "\n in usual", chem))
> > }
> > The problem is with the title.
> > This works fine if the variable "chem" is just text, but if it is an expression then obviously it won't work.
> > I have experimented with various things and I am at a complete loss for how to insert an expression into the middle of
> > a title. If the expression was going in directly I would use main=expression(paste("text ", expression, " text")) but
> > again this doesn't work. Can anyone help? thanks Emily.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> > 
> 
> -- 
> Roger D. Peng
> http://www.biostat.jhsph.edu/~rpeng/
>



From gregor.gorjanc at bfro.uni-lj.si  Fri Nov 12 13:48:12 2004
From: gregor.gorjanc at bfro.uni-lj.si (Gregor GORJANC)
Date: Fri, 12 Nov 2004 12:48:12 +0000
Subject: [R] How to get mode (the most frequent value in distribution)?
Message-ID: <4194B10C.9080802@bfro.uni-lj.si>

Hello!

I have a continous distribution and would like to get mode  (the most 
frequent value in distribution). I easily found mean, median and other 
basic thing but not mode function. Can anyone help?

I know there my might be problems with multiple modes, but still I think 
that there should be a mode function in R.

Please send mail to R-help list and me, so I can get response faster.

Thank you!

-- 
Lep pozdrav / With regards,
     Gregor GORJANC

---------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty       URI: http://www.bfro.uni-lj.si
Zootechnical Department    mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                  tel: +386 (0)1 72 17 861
SI-1230 Domzale            fax: +386 (0)1 72 41 005
Slovenia



From asemeria at cramont.it  Fri Nov 12 13:12:07 2004
From: asemeria at cramont.it (asemeria@cramont.it)
Date: Fri, 12 Nov 2004 13:12:07 +0100
Subject: [R] expressions and paste
Message-ID: <OFBCF73259.63E7DE27-ONC1256F4A.00425E03-C1256F4A.0041F687@tomware.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041112/bb5bf069/attachment.pl

From uctpgde at ucl.ac.uk  Fri Nov 12 13:04:14 2004
From: uctpgde at ucl.ac.uk (Giacomo De Giorgi )
Date: Fri, 12 Nov 2004 12:04:14 -0000
Subject: [R] Simple operation on a subset of data
Message-ID: <001501c4c8af$ba809f00$db5a2880@economics.ucl.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041112/89082bfc/attachment.pl

From otnetobr at yahoo.com.br  Fri Nov 12 13:31:05 2004
From: otnetobr at yahoo.com.br (O. Neto)
Date: Fri, 12 Nov 2004 10:31:05 -0200
Subject: [R] dyn.load problem
Message-ID: <001401c4c8b3$81f477c0$855a62c8@otneto>

Hi R-Users

    I wrote 1 week ago asking about a message that appears when I try run
dyn.load.
   I'm trying to do an example in C code from "Writing R Extension"  to
learn how to do it.
    I have  R 2.0.0, Rtools, Perl and MinGW as describe  in
http://www.murdoch-sutherland.com/Rtools/ with path sets.
 When I use C:\R\rw2000\bin>RCMD SHLIB -o C:/dev-cpp/teste.dll
C:/dev-cpp/conv.c
a teste.dll is created without error and located in that directory, but when
I use it in RGui with "Change dir" set to C:/dev-cpp :

>dyn.load("teste.dll")  results:
Error in dyn.load(x, as.logical(local), as.logical(now)) :
        unable to load shared library "C:/Dev-Cpp/teste":
  LoadLibrary failure:  Par??metro incorreto. (Incorrect Parameter)

My C code is (extracted form Writing R Extension):

#include <R.h>
     #include <Rinternals.h>
     SEXP convolve2(SEXP a, SEXP b)
     {
       R_len_t i, j, na, nb, nab;
       double *xa, *xb, *xab;
       SEXP ab;

       PROTECT(a = coerceVector(a, REALSXP));
       PROTECT(b = coerceVector(b, REALSXP));
       na = length(a); nb = length(b); nab = na + nb - 1;
       PROTECT(ab = allocVector(REALSXP, nab));
       xa = REAL(a); xb = REAL(b);
       xab = REAL(ab);
       for(i = 0; i < nab; i++) xab[i] = 0.0;
       for(i = 0; i < na; i++)
         for(j = 0; j < nb; j++) xab[i + j] += xa[i] * xb[j];
       UNPROTECT(3);
       return(ab);
     }

    I??m using RCMD under Windows ME.

   How can I fix this? Someone can help me ? Prof. Ripley told me to catch
more information debbuging this DLL. Is it hard to do this one under
Windows? It??s necessary modify  files SHLIB or MkDLL?


Thanks

O. Neto



From dimitris.rizopoulos at med.kuleuven.ac.be  Fri Nov 12 13:30:46 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Fri, 12 Nov 2004 13:30:46 +0100
Subject: [R] Simple operation on a subset of data
References: <001501c4c8af$ba809f00$db5a2880@economics.ucl.ac.uk>
Message-ID: <001201c4c8b3$6f423e50$0540210a@www.domain>

Hi Giacomo,

"An Introduction to R" is very useful document for all these things! 
Look at ?subset and try:

dat <- data.frame(x=sample(1:2, 10, TRUE), y=sample(c(4,5), 10, TRUE), 
z=rnorm(10))
######
summary(dat$z)
summary(subset(dat, x==1 & y==4, select=z))


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Giacomo De Giorgi " <uctpgde at ucl.ac.uk>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, November 12, 2004 1:04 PM
Subject: [R] Simple operation on a subset of data


> Sorry for the silly question.
>
> I am trying to perform a simple operation on a subsample of my data
> loaded as data and attached:
>
> data<-read.dta(name file)
> attach(data)
>
> Say x=1,2 and y=4,5 I want to summarize z only if x=1 & y=4. I 
> thought
> that the way to do that would be to
> write if((x=1) & (y=4)) summary(z)
> butwhen I do this the result I get is for the whole data 
> (irrespective
> of the conditions imposed). Can anyone help?
>
>
> Thanks
> Giacomo
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From rpeng at jhsph.edu  Fri Nov 12 13:43:27 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Fri, 12 Nov 2004 07:43:27 -0500
Subject: [R] Problems installing ncdf package into R
In-Reply-To: <1100248893.4194773d99591@webmail.uib.no>
References: <1100248893.4194773d99591@webmail.uib.no>
Message-ID: <4194AFEF.7050908@jhsph.edu>

Did you look at

http://cran.us.r-project.org/bin/windows/contrib/2.0/ReadMe

?

-roger

Dag.Steinskog at student.uib.no wrote:
> Hello
> 
> Can someone please look into the problem with installing ncdf package into R. 
> I get this message when I try install it:
> 
> 
>>install.packages("ncdf")
> 
> trying URL `http://cran.r-project.org/bin/windows/contrib/2.0/PACKAGES'
> Content type `text/plain; charset=iso-8859-1' length 22827 bytes
> opened URL
> downloaded 22Kb
> 
> Warning message: 
> No package "ncdf" on CRAN. in: download.packages(pkgs, destdir = tmpd, 
> available = available,  
> 
> 
> I checked with the homepage cran.r-project.org, and the package is there. I 
> have installed packages earlier into R with no problems. 
> 
> In advance, thanks for all help!
> 
> Best Wishes
> Dag Johan Steinskog
> Bjerknes Center of Climate Research - University of Bergen, Norway
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From petr.pikal at precheza.cz  Fri Nov 12 13:44:33 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Fri, 12 Nov 2004 13:44:33 +0100
Subject: [R] How to get mode (the most frequent value in distribution)?
In-Reply-To: <4194B10C.9080802@bfro.uni-lj.si>
Message-ID: <4194BE41.13261.197985C@localhost>



From vito_ricci at yahoo.com  Fri Nov 12 13:51:02 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Fri, 12 Nov 2004 13:51:02 +0100 (CET)
Subject: [R] Re: How to get mode (the most frequent value in distribution)?
Message-ID: <20041112125102.16447.qmail@web41203.mail.yahoo.com>

Hi Gregor,

if you have a continous distribution is rather
difficult that the same value is repeated many times.
In this case I believe you could not find  the mode
(the most frequent value in distribution) as your
distribution is continous, but only the modal
class/interval (the interval of the distribution
having the most frequency).

See this example:

> set.seed(13)
> x<-rnorm(100)
> xcut<-cut(x, breaks=c(-4,-3,-2,-1,0,1,2,3,4))
> table(xcut)
xcut
(-4,-3] (-3,-2] (-2,-1]  (-1,0]   (0,1]   (1,2]  
(2,3]   (3,4] 
      0       1      19      33      30      17      
0       0 
> max(table(xcut))
[1] 33

the modal class/interval is (-1,0].

I hope I give a little help.
Best
Vito

You wrote:

Hello!

I have a continous distribution and would like to get
mode  (the most 
frequent value in distribution). I easily found mean,
median and other 
basic thing but not mode function. Can anyone help?

I know there my might be problems with multiple modes,
but still I think 
that there should be a mode function in R.

Please send mail to R-help list and me, so I can get
response faster.

Thank you!

-- 
Lep pozdrav / With regards,
     Gregor GORJANC

=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese http://www.modugno.it/archivio/cat_palese.shtml



From andy_liaw at merck.com  Fri Nov 12 14:10:53 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 12 Nov 2004 08:10:53 -0500
Subject: [R] How to get mode (the most frequent value in
 distribution) ?
Message-ID: <3A822319EB35174CA3714066D590DCD50994E2DC@usrymx25.merck.com>

A sample from a continuous distribution most likely has no meaningful sample
mode.  Shown below is a way to estimate the location of mode from a kernel
density estimate (using the locfit package):

> x <- rnorm(50)
> library(locfit)
Locfit for R.
August 3, 2000.  (Updated for R 1.7.0, March 21, 2003)

Attaching package 'locfit':


        The following object(s) are masked from package:stats :

         knots 

> x.den <- locfit(~x)
> x[which.max(predict(x.den, newdata=x))]
[1] -0.615337

HTH,
Andy

> From: Gregor GORJANC
> 
> Hello!
> 
> I have a continous distribution and would like to get mode  (the most 
> frequent value in distribution). I easily found mean, median 
> and other 
> basic thing but not mode function. Can anyone help?
> 
> I know there my might be problems with multiple modes, but 
> still I think 
> that there should be a mode function in R.
> 
> Please send mail to R-help list and me, so I can get response faster.
> 
> Thank you!
> 
> -- 
> Lep pozdrav / With regards,
>      Gregor GORJANC
> 
> ---------------------------------------------------------------
> University of Ljubljana
> Biotechnical Faculty       URI: http://www.bfro.uni-lj.si
> Zootechnical Department    mail: gregor.gorjanc <at> bfro.uni-lj.si
> Groblje 3                  tel: +386 (0)1 72 17 861
> SI-1230 Domzale            fax: +386 (0)1 72 41 005
> Slovenia
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From vito_ricci at yahoo.com  Fri Nov 12 14:20:37 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Fri, 12 Nov 2004 14:20:37 +0100 (CET)
Subject: [R] How to get mode in case of discrete or categorial data
Message-ID: <20041112132037.74274.qmail@web41209.mail.yahoo.com>

Dear all,

in a previuos message was asked  how get the mode of
continous distribution. Now I'm asking if there an R
function to obtain the mode in case of a discrete
distribution or categorial data. The only way is to
use table():

> x<-rep(1:5,100)
> s<-sample(x,40)
> t<-table(s)
> t
s
 1  2  3  4  5 
13 10  5  4  8 

the mode is value=1

Thanks
Cordially
Vito

=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese http://www.modugno.it/archivio/cat_palese.shtml



From jfox at mcmaster.ca  Fri Nov 12 14:52:39 2004
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 12 Nov 2004 08:52:39 -0500
Subject: [R] How to get mode in case of discrete or categorial data
In-Reply-To: <20041112132037.74274.qmail@web41209.mail.yahoo.com>
Message-ID: <20041112135238.LLBR1836.tomts16-srv.bellnexxia.net@JohnDesktop8300>

Dear Vito,

How about names(t)[t == max(t)], which will pick up multiple modes if
there's a tie. By the way, I recall a discussion of this question on r-help
not too long ago.

I hope this helps,
 John


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Vito Ricci
> Sent: Friday, November 12, 2004 8:21 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] How to get mode in case of discrete or categorial data
> 
> Dear all,
> 
> in a previuos message was asked  how get the mode of 
> continous distribution. Now I'm asking if there an R function 
> to obtain the mode in case of a discrete distribution or 
> categorial data. The only way is to use table():
> 
> > x<-rep(1:5,100)
> > s<-sample(x,40)
> > t<-table(s)
> > t
> s
>  1  2  3  4  5
> 13 10  5  4  8 
> 
> the mode is value=1
> 
> Thanks
> Cordially
> Vito
>



From br44114 at yahoo.com  Fri Nov 12 15:10:57 2004
From: br44114 at yahoo.com (bogdan romocea)
Date: Fri, 12 Nov 2004 06:10:57 -0800 (PST)
Subject: [R] an off-topic question -> model validation
Message-ID: <20041112141057.19606.qmail@web50308.mail.yahoo.com>

Assuming you have enough data, usually 1/4 to 1/2 is used for
validation. 

One reference would be
Picard, R.R. and Berk, K.N. (1990)
"Data Splitting," The American Statistician, 44;140-147.

hth,
b.

-----Original Message-----
From: Wensui Liu [mailto:liuwensui at gmail.com]
Sent: Thursday, November 11, 2004 10:20 PM
To: r-help at stat.math.ethz.ch
Subject: [R] an off-topic question -> model validation


Currently, I am working on a data mining project and plan to divide
the data table into 2 parts, one for modeling and the other for
validation to compare several models.

But I am not sure about the percentage of data I should use to build
the model and the one I should keep to validate the model.

Is there any literature reference about this topic? 

Thank you so much!

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From Bernhard.Pfaff at drkw.com  Fri Nov 12 15:22:47 2004
From: Bernhard.Pfaff at drkw.com (Pfaff, Bernhard)
Date: Fri, 12 Nov 2004 15:22:47 +0100
Subject: [R] R "sumo" package suggestion
Message-ID: <29E0BC0C716A584582941615CF9FFB0902585D15@ibfftce107.de.ad.drkw.net>

Dear list member,

sorry, for jumping in this thread and/or the gun. 
What springs immediately to my mind is: if such a functionality should be
realised, it might be beneficial too to have something similar to Thomas
Ruedas `texdocTK' at hand; i.e. the retrieval of the pdf-package
documentations sorted by category and of course the manuals that are shipped
with a base installation. Well, but one after the other...

Best Regards,
Bernhard

> Hello,
> >
> > I think Achim suggestion is more realistic: it does not 
> imply automatic
> > installation of all packages, but just a restricted list of packages
> > available on CRAN about a specific topic.
> >
> > An easy way to get this result is to propose separate lists of  
> > packages. It
> > means separate lists than
> > http://cran.r-project.org/src/contrib/PACKAGES.html (and 
> 'PACKAGES' in  
> > the
> > Windows packages binaries). I don't see the problem to 
> propose other  
> > lists
> > that could be called 'SpatialStats.html' (in packages
> > sources)/'SpatialStats' (in Windows binaries)... and the same for
> > 'MachineLearning', 'Biostats', etc...
> >
> > Then, of course the various functions that install packages 
> should be
> > adapted to use these lists. It does not look like an unsurmontable  
> > task.
> >
> > Of course, if this is not done yet by the R Core Team, I 
> presume that  
> > there
> > must be difficulties that I don't see. It is obvious that, 
> either we  
> > need a
> > list maintainer for each topic, or we have to propose keywords for  
> > packages
> > (similar to the keywords for functions) that will be used to  
> > automatically
> > generate those separate lists.
> >
> > An alternative that can currently be used for groups of users in an
> > institution is to maintain a local copy of R packages 
> repository, which
> > contains only the packages of interest for this group. I do 
> so for my
> > students. Under Windows, in the new R 2.0.1 beta, there is 
> a new menu  
> > entry
> > in packages -> Set CRAN mirror... (in my version it does 
> not work yet,
> > looking for a missing .\doc\CRAN_mirrors.csv file), but I 
> can easily  
> > figure
> > out how it works and how I could append my own repository 
> to the list  
> > to
> > ease installation of a restricted list of R packages by my 
> students.  
> > This is
> > only for Windows, but a similar approach can also be used on other  
> > platforms
> > with a little bit of coding.
> >
> > Best,
> >
> > Philippe
> >
> > ..............................................<??}))><........
> >  ) ) ) ) )
> > ( ( ( ( (    Prof. Philippe Grosjean
> >  ) ) ) ) )
> > ( ( ( ( (    Numerical Ecology of Aquatic Systems
> >  ) ) ) ) )   Mons-Hainaut University, Pentagone
> > ( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
> >  ) ) ) ) )   6, av du Champ de Mars, 7000 Mons, Belgium
> > ( ( ( ( (
> >  ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.33.12
> > ( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
> >  ) ) ) ) )
> > ( ( ( ( (    web:   http://www.umh.ac.be/~econum
> >  ) ) ) ) )
> > ..............................................................
> >
> >
> >> -----Original Message-----
> >> From: r-help-bounces at stat.math.ethz.ch
> >> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Achim Zeileis
> >> Sent: Thursday, November 11, 2004 10:21 PM
> >> To: Liaw, Andy
> >> Cc: r-help at stat.math.ethz.ch; rsparapa at post.its.mcw.edu
> >> Subject: Re: [R] R "sumo" package suggestion
> >>
> >> On Thu, 11 Nov 2004 15:59:09 -0500 Liaw, Andy wrote:
> >>
> >>> Good idea, IMHO, but there are some practical difficulties:
> >>>
> >>> I guess the XEmacs packages are (most, if not all) pure 
> elisp code,
> >>> and do not need other stuff to work.  However, quite a few CRAN
> >>> packages depend on external libraries or programs, and do not
> >>> necessarily work on all platforms that R runs on.  How would such
> >>> dependencies be resolved in such a kitchen sink bundle?
> >>>
> >>> I have a somewhat related idea:  Start labelling packages
> >> with a set
> >>> of pre-defined categories, and a package can be labelled with more
> >>> than one categories (especially those *misc type packages).  It is
> >>> then possible to have facility to let people install all
> >> packages that
> >>> fall in a particular category (e.g., `spatial statistics').
> >>  I believe
> >>> several systems have such facilities, Debian being one of them,
> >>> TeXLive being another.
> >>
> >> This is similar to idea that has been discussed from time to
> >> time for several years now: it would be nice to have
> >> maintained "CRAN task views"
> >> (or something like that), i.e., we could have a maintainer
> >> for, say "spatial stats", another one for "machine learning",
> >> "biostats" which can of course be overlapping. Then the
> >> maintainers would have to produce some sort of list of
> >> packages (in a standardized format) with a little bit of
> >> markup such that a web page can be generated from it and that
> >> the information could be used by install.packages().
> >> I think most users would profit from that, but nobody has
> >> done the work to provide the infrastructure so far. I've just
> >> discussed this with Kurt again, a week ago or so...I wanted
> >> to play around with some ideas, but didn't get round to
> >> really do something yet. But hopefully, I'll get round to
> >> work on this in the next weeks.
> >> Z
> >>
> >>> Just my $0.02...
> >>>
> >>> Andy
> >>>
> >>>> From: Rodney Sparapani
> >>>>
> >>>> r-help:
> >>>>
> >>>> I have an R package suggestion.  After spending several hours the
> >>>> other day installing about a dozen packages, I had an idea.  In
> >>>> xemacs, there is a "sumo" package which allows me to
> >> install a large
> >>>> bundle of xemacs packages at one time (about a 120 modes
> >> including
> >>>> ESS).  I think R should have a similar bundle.  It would
> >> be so much
> >>>> easier than hunting/downloading/installing.  Martin
> >> encouraged me to
> >>>> send this suggestion to r-help.  In addition, he put
> >> together a few
> >>>> comments relating to the previous times that this, or a similar
> >>>> suggestion, has been brought up here.
> >>>>
> >>>> Martin wrote:
> >>>>
> >>>> If you search for "install all CRAN packages"
> >>>> on
> >>>> 	http://maths.newcastle.edu.au/~rking/R/
> >>>>
> >>>> (the URL which is quickly found from the [Search] sidebar of
> >>>> http://www.R-project.org/)
> >>>>
> >>>> You find things like Greg Warnes 'Makefile'
> >>>> http://tolstoy.newcastle.edu.au/R/help/04/04/0723.html
> >>>> and
> >>>> http://tolstoy.newcastle.edu.au/R/help/04/04/0616.html
> >>>> which is from Tony and has the following small function:
> >>>>
> >>>>   installNewCRANPackages <- function() {
> >>>>     ## (C) A.J. Rossini, 2002--2004
> >>>>     test2 <- packageStatus()$avail["Status"]
> >>>>
> >>>> install.packages(row.names(test2)[which(test2$Status=="not
> >>>> installed")])
> >>>>   }
> >>>>
> >>>> ----------
> >>>>
> >>>> Rodney Sparapani              Medical College of Wisconsin
> >>>> Sr. Biostatistician           Patient Care & Outcomes Research
> >>>> rsparapa at mcw.edu              http://www.mcw.edu/pcor
> >>>> Was 'Name That Tune' rigged?  WWLD -- What Would Lombardi Do
> >>>>
> >>>> ______________________________________________
> >>>> R-help at stat.math.ethz.ch mailing list
> >>>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>>> PLEASE do read the posting guide!
> >>>> http://www.R-project.org/posting-guide.html
> >>>>
> >>>>
> >>>
> >>> ______________________________________________
> >>> R-help at stat.math.ethz.ch mailing list
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide!
> >>> http://www.R-project.org/posting-guide.html
> >>>
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide!
> >> http://www.R-project.org/posting-guide.html
> >>
> >>
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!  
> > http://www.R-project.org/posting-guide.html
> >
> >
> --------------------------------------------------------------
> ---------- 
> ---------
> Dr. Andrew Beckerman
> Department of Animal and Plant Sciences, University of Sheffield,
> Alfred Denny Building, Western Bank, Sheffield S10 2TN, UK
> ph +44 (0)114 222 0026; fx +44 (0)114 222 0002
> http://www.shef.ac.uk/beckslab
> --------------------------------------------------------------
> ---------- 
> ----------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html


--------------------------------------------------------------------------------
The information contained herein is confidential and is inte...{{dropped}}



From ktiwari at bgc-jena.mpg.de  Fri Nov 12 15:39:14 2004
From: ktiwari at bgc-jena.mpg.de (Yogesh K. Tiwari)
Date: Fri, 12 Nov 2004 15:39:14 +0100
Subject: [R] whether R can import netcdf file format data ?
Message-ID: <4194CB12.20903@bgc-jena.mpg.de>

Hello R Users,

Whether 'R' can import Netcdf format data files ??

Any package in 'R' that can help me on this front ??

Many thanks in advance,

Regards,
Yogesh
-- 

===========================================
Yogesh K. Tiwari,
Max-Planck Institute for Biogeochemistry,
Postfach 10 01 64, D-07701 Jena,
Germany

Office   : +49 3641 576376
Fax      : +49 3641 577300
Home     : +49 3641 672232
Mobile   : +49 1736988789
e-mail   : yogesh.tiwari at bgc-jena.mpg.de



From vito_ricci at yahoo.com  Fri Nov 12 15:46:18 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Fri, 12 Nov 2004 15:46:18 +0100 (CET)
Subject: [R] Mode in case of discrete or categorial data
Message-ID: <20041112144618.57054.qmail@web41214.mail.yahoo.com>

Thanking John for his suggestion I build this function
which get the mode of both categorial and discrete
data.


Mode<-function(x){t<-table(x)
if (is.numeric(x)) as.numeric(names(t)[t == max(t)])
else (names(t)[t == max(t)])
}

Any other improvement and suggestion will welcome.

Best

Vito

> s
 [1] 1 1 6 1 1 7 6 5 6 2 1 4 5 6 6 7 3 5 4 1 7 3 7 3 3
7 7 2 1 4 4 2 7 7 6 6 1 2
[39] 5 1 7 7 5 5 7 3 5 6 5 6 3 6 6 4 2 1 5 3 3 3 6 5 2
4 3 2 2 1 5 3 4 3 1 3 3
> Mode(s)
[1] 3
> ss
 [1] "C" "A" "C" "D" "B" "A" "B" "B" "B" "A" "D" "D"
"A" "D" "D" "A" "D" "C" "B"
[20] "D" "C" "B" "D" "C" "B" "C" "D" "A" "C" "A" "A"
"A" "C" "A" "D" "A" "B" "B"
[39] "A" "B"
> Mode(ss)
[1] "A"


=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese http://www.modugno.it/archivio/cat_palese.shtml



From ripley at stats.ox.ac.uk  Fri Nov 12 16:05:10 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 12 Nov 2004 15:05:10 +0000 (GMT)
Subject: [R] whether R can import netcdf file format data ?
In-Reply-To: <4194CB12.20903@bgc-jena.mpg.de>
References: <4194CB12.20903@bgc-jena.mpg.de>
Message-ID: <Pine.LNX.4.61.0411121500140.7938@gannet.stats>

On Fri, 12 Nov 2004, Yogesh K. Tiwari wrote:

> Whether 'R' can import Netcdf format data files ??

It can read from and write to them, at least.

> Any package in 'R' that can help me on this front ??

Do look at the `R Data Import/Export' manual that ships with R (a recent 
version is needed), or the R FAQ or the list of packages on CRAN.
You will find RNetCDF, ncdf and ncvar.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From macq at llnl.gov  Fri Nov 12 16:15:50 2004
From: macq at llnl.gov (Don MacQueen)
Date: Fri, 12 Nov 2004 07:15:50 -0800
Subject: [R] whether R can import netcdf file format data ?
In-Reply-To: <4194CB12.20903@bgc-jena.mpg.de>
References: <4194CB12.20903@bgc-jena.mpg.de>
Message-ID: <p06110400bdba83ecbe4f@[128.115.153.6]>

Please go to http://lib.stat.cmu.edu/R/CRAN/ , click on the 
"Packages" link, and search for one.

-Don

At 3:39 PM +0100 11/12/04, Yogesh K. Tiwari wrote:
>Hello R Users,
>
>Whether 'R' can import Netcdf format data files ??
>
>Any package in 'R' that can help me on this front ??
>
>Many thanks in advance,
>
>Regards,
>Yogesh
>--
>
>===========================================
>Yogesh K. Tiwari,
>Max-Planck Institute for Biogeochemistry,
>Postfach 10 01 64, D-07701 Jena,
>Germany
>
>Office   : +49 3641 576376
>Fax      : +49 3641 577300
>Home     : +49 3641 672232
>Mobile   : +49 1736988789
>e-mail   : yogesh.tiwari at bgc-jena.mpg.de
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From asemeria at cramont.it  Fri Nov 12 16:33:02 2004
From: asemeria at cramont.it (asemeria@cramont.it)
Date: Fri, 12 Nov 2004 16:33:02 +0100
Subject: [R] expressions and paste
Message-ID: <OF150AB7FC.B9740CF0-ONC1256F4A.00553D1E-C1256F4A.00545BC6@tomware.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041112/340a190c/attachment.pl

From andy_liaw at merck.com  Fri Nov 12 16:20:54 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 12 Nov 2004 10:20:54 -0500
Subject: [R] Mode in case of discrete or categorial data
Message-ID: <3A822319EB35174CA3714066D590DCD50994E2DD@usrymx25.merck.com>

You might want to do a bit to handle NAs, as table() excludes them by
default.  Also, you could write it a bit cleaner as:

Mode <- function(x) {
    tab <- table(x)
    m <- names(tab)[tab == max(tab)]
    if (is.numeric(x)) m <- as.numeric(m)
    m
}

(Generally I try avoiding constructs like:
   if (cond) var <- alt1 else var <- alt2
especially when alt1 and alt2 are very similar.  If you need to make changes
later, it's easy to change one and forget the other, etc.  I believe Martin
also made this point in his talk at useR! 2004.)

HTH,
Andy

> From: Vito Ricci
> 
> Thanking John for his suggestion I build this function
> which get the mode of both categorial and discrete
> data.
> 
> 
> Mode<-function(x){t<-table(x)
> if (is.numeric(x)) as.numeric(names(t)[t == max(t)])
> else (names(t)[t == max(t)])
> }
> 
> Any other improvement and suggestion will welcome.
> 
> Best
> 
> Vito
> 
> > s
>  [1] 1 1 6 1 1 7 6 5 6 2 1 4 5 6 6 7 3 5 4 1 7 3 7 3 3
> 7 7 2 1 4 4 2 7 7 6 6 1 2
> [39] 5 1 7 7 5 5 7 3 5 6 5 6 3 6 6 4 2 1 5 3 3 3 6 5 2
> 4 3 2 2 1 5 3 4 3 1 3 3
> > Mode(s)
> [1] 3
> > ss
>  [1] "C" "A" "C" "D" "B" "A" "B" "B" "B" "A" "D" "D"
> "A" "D" "D" "A" "D" "C" "B"
> [20] "D" "C" "B" "D" "C" "B" "C" "D" "A" "C" "A" "A"
> "A" "C" "A" "D" "A" "B" "B"
> [39] "A" "B"
> > Mode(ss)
> [1] "A"
> 
> 
> =====
> Diventare costruttori di soluzioni
> Became solutions' constructors
> 
> "The business of the statistician is to catalyze 
> the scientific learning process."  
> George E. P. Box
> 
> 
> Visitate il portale http://www.modugno.it/
> e in particolare la sezione su Palese 
> http://www.modugno.it/archivio/cat_palese.shtm> l
> 
> 
> ______________________________________________
> 
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From tlumley at u.washington.edu  Fri Nov 12 16:40:36 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 12 Nov 2004 07:40:36 -0800 (PST)
Subject: [R] Simple operation on a subset of data
In-Reply-To: <001501c4c8af$ba809f00$db5a2880@economics.ucl.ac.uk>
References: <001501c4c8af$ba809f00$db5a2880@economics.ucl.ac.uk>
Message-ID: <Pine.A41.4.61b.0411120734230.157858@homer07.u.washington.edu>

On Fri, 12 Nov 2004, Giacomo De Giorgi  wrote:
>
> Say x=1,2 and y=4,5 I want to summarize z only if x=1 & y=4. I thought
> that the way to do that would be to
> write if((x=1) & (y=4)) summary(z)
> butwhen I do this the result I get is for the whole data (irrespective
> of the conditions imposed). Can anyone help?

There are actually worse problems with
   if((x=1) & (y=4)) summary(z) 
than that: you have just set x to 1 and y to 4.

You can do
    summary(z[(x==1) & (y==4)])
to get the answer you want (or various other things)

You really need to read the Introduction to R, which will tell you, among 
other things, what the = operator does.

 	-thomas



From ripley at stats.ox.ac.uk  Fri Nov 12 16:44:03 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 12 Nov 2004 15:44:03 +0000 (GMT)
Subject: [R] Mode in case of discrete or categorial data
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E2DD@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E2DD@usrymx25.merck.com>
Message-ID: <Pine.LNX.4.61.0411121542110.23198@gannet.stats>

On Fri, 12 Nov 2004, Liaw, Andy wrote:

> You might want to do a bit to handle NAs, as table() excludes them by
> default.  Also, you could write it a bit cleaner as:
>
> Mode <- function(x) {
>    tab <- table(x)
>    m <- names(tab)[tab == max(tab)]
>    if (is.numeric(x)) m <- as.numeric(m)
>    m
> }
>
> (Generally I try avoiding constructs like:
>   if (cond) var <- alt1 else var <- alt2
> especially when alt1 and alt2 are very similar.  If you need to make changes
> later, it's easy to change one and forget the other, etc.  I believe Martin
> also made this point in his talk at useR! 2004.)

Also,

    var <- if(cond) alt1 else alt2

is more idiomatic S (and easier to read).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tlumley at u.washington.edu  Fri Nov 12 16:44:24 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 12 Nov 2004 07:44:24 -0800 (PST)
Subject: [R] Mode in case of discrete or categorial data
In-Reply-To: <20041112144618.57054.qmail@web41214.mail.yahoo.com>
References: <20041112144618.57054.qmail@web41214.mail.yahoo.com>
Message-ID: <Pine.A41.4.61b.0411120742440.157858@homer07.u.washington.edu>

On Fri, 12 Nov 2004, Vito Ricci wrote:
> Mode<-function(x){t<-table(x)
> if (is.numeric(x)) as.numeric(names(t)[t == max(t)])
> else (names(t)[t == max(t)])
> }
>
> Any other improvement and suggestion will welcome.
>

which.max is design for finding the maximum, so
    names(t)[which.max(t)]


 	-thomas



From blh at mssl.ucl.ac.uk  Fri Nov 12 16:44:20 2004
From: blh at mssl.ucl.ac.uk (Benjamin Lloyd-Hughes)
Date: Fri, 12 Nov 2004 15:44:20 -0000
Subject: [R] whether R can import netcdf file format data ?
In-Reply-To: <Pine.LNX.4.61.0411121500140.7938@gannet.stats>
Message-ID: <AJEJIOOKADBOGMFEONHJIEOKCCAA.blh@mssl.ucl.ac.uk>

I've been using the ncdf package for reading climate data (mostly, NCEP/NCAR
and ECMWF reanalysis stuff). The package does what it says on the tin, but
can become quite slow when working with large data sets (e.g. a stack of
daily grids say 144x73 elements 1970-to date). Does anyone have any tips for
speedy retrieval? I'm thinking about typical drilling/slicing type
operations.

Cheers, Ben

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Prof Brian Ripley
Sent: 12 November 2004 15:05
To: Yogesh K. Tiwari
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] whether R can import netcdf file format data ?


On Fri, 12 Nov 2004, Yogesh K. Tiwari wrote:

> Whether 'R' can import Netcdf format data files ??

It can read from and write to them, at least.

> Any package in 'R' that can help me on this front ??

Do look at the `R Data Import/Export' manual that ships with R (a recent
version is needed), or the R FAQ or the list of packages on CRAN.
You will find RNetCDF, ncdf and ncvar.

--
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From maechler at stat.math.ethz.ch  Fri Nov 12 17:01:24 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 12 Nov 2004 17:01:24 +0100
Subject: [R] email result as attachement 
In-Reply-To: <200411091155.iA9Bt4c7001697@smbh.univ-paris13.fr>
References: <200411091155.iA9Bt4c7001697@smbh.univ-paris13.fr>
Message-ID: <16788.56916.798919.851938@gargle.gargle.HOWL>

>>>>> "Ramzi" == Ramzi TEMANNI <ramzi.temanni at laposte.net>
>>>>>     on Tue, 9 Nov 2004 12:55:50 +0100 writes:

    Ramzi> Hi, I??d like to know if it's possible to mail a file
    Ramzi> (image generated with R) in attachments a similar to
    Ramzi> bug.report Thanks in advance.

I don't understand what 
  "attachments a similar to bug.report"
should mean.

However the posting guide tells you to look here for more details

   http://www.r-project.org/mail#instructions

and there it says, that image/png is an allowed attachment type
(and application/pdf is too).

Regards,
Martin Maechler ETH Zurich; mailing lists' maintainer



From bm8 at st-andrews.ac.uk  Fri Nov 12 17:27:41 2004
From: bm8 at st-andrews.ac.uk (Bernie McConnell)
Date: Fri, 12 Nov 2004 16:27:41 +0000
Subject: [R] RODBC & POSIX & Daylight Saving blues
Message-ID: <6.1.2.0.0.20041112152038.025dcd30@bute.st-and.ac.uk>

Many thanks for the responses about how to read datetimes into POSIXct 
which refer to straight GMT times without regard to Daylight Saving.  To 
reiterate, I''m using R 2.0.0 on win2000.

I have tried Gabor's suggestion,see below, which did not work for me.  The 
fundamental setting (for me) appears to be in the registry:

HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control\TimeZoneInformation

The values here are most readily altered using the 'Date/ TIme properties' 
window (just double click the wee digital clock on the right of the window 
task bar).  When I disable Daylight Saving a registry variable 
called  DisableAutoDaylightTimeSet is created and is set to 1.  This can be 
observed with regedit.  Now all works perfectly with my date handling.  So 
it seems that setting the timezone to GMT is necessary but not sufficient 
for my needs

I confess that I do not understand the relationship between setting 
registry variables and environment (eg TZ="GMT").

In practice I would like to keep my win200 machine with Daylight Saving 
enabled - for the sake of other applications.  One strategy is to make the 
required registry alterations immediately before and after any R code which 
handles dates.  But then I'm rather worried whether continuously enabling / 
disabling Daylight Saving would have dire consequences for other 
applications and the well-being of my already shaky OS.  Is this the right 
route - and if so could someone please guide me how to achieve this in R 
for win2000?

Perhaps a better strategy would be to introduce some set-able option within 
R that forced date handling to ignore Daylight Saving.  But here I have 
insufficient expertise and can only appeal to the continuing generosity of 
the developer community.

I am sure that there are many R users in the same position as me - but 
perhaps they don't all realize it!

Many thanks

Bernie McConnell
bm8 at st-andrews.ac.uk
Sea Mammal Reserach Unit
University of St Andrews

------------------------------

Gabor Grothendieck <ggrothendieck <at> myway.com> writes:

:
: Prof Brian Ripley <ripley <at> stats.ox.ac.uk> writes:
:

: : If you set it to GMT for the duration of the sqlFetch call, it should do
: : as you intended (but had not told R, which is not clairvoyant).
: :
:
: On Windows you have to set the whole computer to GMT which has

Paul Roebuck pointed out to me offlist that this can be done
per process in Windows too so I was wrong on this point.

    ... start up Windows console ...
    cd \Program Files\rw2001beta
    set TZ=GMT
    bin\Rgui

In R,

    Sys.time() # displays date and time in GMT time zone


I also tried doing this from within R but was unsuccessful:

    R> Sys.time()
    [1] "2004-11-11 11:37:53 Eastern Standard Time"
    R> Sys.putenv(TZ = "GMT")
    R> Sys.time()  # wanted GMT result but did not get it
    [1] "2004-11-11 11:38:08 Eastern Standard Time"
    R> R.version.string  # Windows XP
    [1] "R version 2.0.1, 2004-11-04"



From jfox at mcmaster.ca  Fri Nov 12 17:32:54 2004
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 12 Nov 2004 11:32:54 -0500
Subject: [R] Mode in case of discrete or categorial data
In-Reply-To: <Pine.A41.4.61b.0411120742440.157858@homer07.u.washington.edu>
Message-ID: <20041112163253.LHBM1584.tomts36-srv.bellnexxia.net@JohnDesktop8300>

Dear Thomas,

I believe that which.max() will report only the first maximum in case of
ties [which is why I suggested the more awkward t == max(t)].

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Thomas Lumley
> Sent: Friday, November 12, 2004 10:44 AM
> To: Vito Ricci
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Mode in case of discrete or categorial data
> 
> On Fri, 12 Nov 2004, Vito Ricci wrote:
> > Mode<-function(x){t<-table(x)
> > if (is.numeric(x)) as.numeric(names(t)[t == max(t)]) else 
> (names(t)[t 
> > == max(t)]) }
> >
> > Any other improvement and suggestion will welcome.
> >
> 
> which.max is design for finding the maximum, so
>     names(t)[which.max(t)]
> 
> 
>  	-thomas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From gregor.gorjanc at bfro.uni-lj.si  Fri Nov 12 18:38:33 2004
From: gregor.gorjanc at bfro.uni-lj.si (Gregor GORJANC)
Date: Fri, 12 Nov 2004 17:38:33 +0000
Subject: [R] How to get mode (the most frequent value in distribution)?
In-Reply-To: <20041112124449.UCVY21843.smta03.mail.ozemail.net@there>
References: <4194B10C.9080802@bfro.uni-lj.si>
	<20041112124449.UCVY21843.smta03.mail.ozemail.net@there>
Message-ID: <4194F519.1050908@bfro.uni-lj.si>

Thanks to all!

It appears that mode calculation is not easy or simple at all. I 
understand that it is problematic when you have continous distribution, 
but there should still be some general function, as for median, mean, ...

-- 
Lep pozdrav / With regards,
     Gregor GORJANC

---------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty       URI: http://www.bfro.uni-lj.si
Zootechnical Department    mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                  tel: +386 (0)1 72 17 861
SI-1230 Domzale            fax: +386 (0)1 72 41 005
Slovenia



From andy_liaw at merck.com  Fri Nov 12 17:51:27 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 12 Nov 2004 11:51:27 -0500
Subject: [R] Mode in case of discrete or categorial data
Message-ID: <3A822319EB35174CA3714066D590DCD50994E2DE@usrymx25.merck.com>

> From: Thomas Lumley
> 
> On Fri, 12 Nov 2004, Vito Ricci wrote:
> > Mode<-function(x){t<-table(x)
> > if (is.numeric(x)) as.numeric(names(t)[t == max(t)])
> > else (names(t)[t == max(t)])
> > }
> >
> > Any other improvement and suggestion will welcome.
> >
> 
> which.max is design for finding the maximum, so
>     names(t)[which.max(t)]

If you only care about one, which.max() is great.  However, if you want to
know about all possible ones, which.max() is not the tool:

> x <- rep(1:5, c(5, 2, 1, 5, 1))
> table(x)
x
1 2 3 4 5 
5 2 1 5 1 
> tab <- table(x)
> tab[tab == max(tab)]
x
1 4 
5 5 
> tab[which.max(tab)]
1 
5 

Andy 
 
>  	-thomas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From murdoch at stats.uwo.ca  Fri Nov 12 17:55:21 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 12 Nov 2004 11:55:21 -0500
Subject: [R] RODBC & POSIX & Daylight Saving blues
In-Reply-To: <6.1.2.0.0.20041112152038.025dcd30@bute.st-and.ac.uk>
References: <6.1.2.0.0.20041112152038.025dcd30@bute.st-and.ac.uk>
Message-ID: <fbq9p0dg30jq2gnnkst06ifbpv2t9ifqjf@4ax.com>

On Fri, 12 Nov 2004 16:27:41 +0000, Bernie McConnell
<bm8 at st-andrews.ac.uk> wrote :

>Many thanks for the responses about how to read datetimes into POSIXct 
>which refer to straight GMT times without regard to Daylight Saving.  To 
>reiterate, I''m using R 2.0.0 on win2000.
>
>I have tried Gabor's suggestion,see below, which did not work for me.  The 
>fundamental setting (for me) appears to be in the registry:
>
>HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control\TimeZoneInformation
>
>The values here are most readily altered using the 'Date/ TIme properties' 
>window (just double click the wee digital clock on the right of the window 
>task bar).  When I disable Daylight Saving a registry variable 
>called  DisableAutoDaylightTimeSet is created and is set to 1.  This can be 
>observed with regedit.  Now all works perfectly with my date handling.  So 
>it seems that setting the timezone to GMT is necessary but not sufficient 
>for my needs
>
>I confess that I do not understand the relationship between setting 
>registry variables and environment (eg TZ="GMT").
>
>In practice I would like to keep my win200 machine with Daylight Saving 
>enabled - for the sake of other applications.  One strategy is to make the 
>required registry alterations immediately before and after any R code which 
>handles dates.  But then I'm rather worried whether continuously enabling / 
>disabling Daylight Saving would have dire consequences for other 
>applications and the well-being of my already shaky OS.  Is this the right 
>route - and if so could someone please guide me how to achieve this in R 
>for win2000?
>
>Perhaps a better strategy would be to introduce some set-able option within 
>R that forced date handling to ignore Daylight Saving.  But here I have 
>insufficient expertise and can only appeal to the continuing generosity of 
>the developer community.
>
>I am sure that there are many R users in the same position as me - but 
>perhaps they don't all realize it!

I think you probably want to use as.is=TRUE when you retrieve the
results, then use the date conversion functions to specify the time
zone explicitly, e.g. asPOSIXct(x, tz='GMT'). 

Duncan Murdoch



From TERUHITO at jp.ibm.com  Fri Nov 12 19:21:36 2004
From: TERUHITO at jp.ibm.com (Teruhito Hosono)
Date: Sat, 13 Nov 2004 03:21:36 +0900
Subject: [R] Linux on PowerPC 970 (IBM BladeCenter JS20)
Message-ID: <OFB7387962.06C3C37E-ON49256F4A.006454B5-49256F4A.0064FE40@jp.ibm.com>





Hi,

I'm just wondering if I can run "R" software on IBM BladeCenter JS20
PowerPC 970 based now.
If it's not available, do you have any plan to port it for JS20?
Both Red Hat Enterprise Linux AS3 and SUSE Linux Enterprise Server 8/9 are
available on JS20.

Thanks.



From kwright at eskimo.com  Fri Nov 12 19:34:45 2004
From: kwright at eskimo.com (kwright@eskimo.com)
Date: Fri, 12 Nov 2004 10:34:45 -0800 (PST)
Subject: [R] Firefox keyword searches for R & S-Plus
Message-ID: <37367.170.54.59.167.1100284485.squirrel@170.54.59.167>


If you've 'rediscovered the web' with Firefox (http://getfirefox.com), you
might find this tip handy.  Suppose you want to search the (local) R
documentation for information about the lme function.  Wouldn't it be nice
to just type into the browser address bar:
  r lme
and have the search performed?  Here's how (on Windows):

Click on this link: 
file:///C:/Progra~1/rw2000/doc/html/search/SearchObject.html?%s

Bookmark the page and name the bookmark something like: R 2.0.0 doc search

Edit the properties of the bookmark (by right-clicking on the bookmark)
and in the keywords field enter: r

Click OK.


That's all there is to it except for a couple of notes:

1. The first time the local search is performed (during a browser
session), the java search applet has to be loaded.  This takes several
seconds.

2. If you use a different operating system, a different version of R, or
install R in a different location, change the bookmark path to the
"SearchObject" file as appropriate.

3. By default this only searches Help page titles.  You may prefer a more
comprehensive search that includes Keywords and Object names.  Edit the
SearchObject.html file and change 'false' to 'true' in the following line:
  line = line + document.SearchEngine.search (searchstring,true,true,true);



Here are my R and S-Plus bookmarks.  I'll just give the link, my bookmark
name, and my bookmark keyword.

http://www.google.com/u/newcastlemaths?q=%s
R mail archive search
rh

http://www.google.com/search?q=site:www.biostat.wustl.edu+[S]+%s
S-news archive search
sn

file:///C:/Progra~1/rw2000/doc/html/search/SearchObject.html?%s
R 2.0.0 doc search
r


Happy searching!

Kevin Wright



From p.dalgaard at biostat.ku.dk  Fri Nov 12 19:35:14 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Nov 2004 19:35:14 +0100
Subject: [R] Linux on PowerPC 970 (IBM BladeCenter JS20)
In-Reply-To: <OFB7387962.06C3C37E-ON49256F4A.006454B5-49256F4A.0064FE40@jp.ibm.com>
References: <OFB7387962.06C3C37E-ON49256F4A.006454B5-49256F4A.0064FE40@jp.ibm.com>
Message-ID: <x2y8h69aql.fsf@biostat.ku.dk>

Teruhito Hosono <TERUHITO at jp.ibm.com> writes:

> Hi,
> 
> I'm just wondering if I can run "R" software on IBM BladeCenter JS20
> PowerPC 970 based now.
> If it's not available, do you have any plan to port it for JS20?
> Both Red Hat Enterprise Linux AS3 and SUSE Linux Enterprise Server 8/9 are
> available on JS20.

Shouldn't be a problem, but you'll most likely need to compile from
sources. You could do us and yourself a favour and try to build the
current beta version of 2.0.1 (release Monday) and report back so that
we can try to fix problems before release (no promises though).

The build process is very simple: 

./configure ; make ; make check ; make install

but you may find that you need to install a bunch of development
libraries and such.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From vograno at evafunds.com  Fri Nov 12 22:06:54 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Fri, 12 Nov 2004 13:06:54 -0800
Subject: [R] R on 64-bit Linux machine
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A5751163@phost015.EVAFUNDS.intermedia.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041112/b82c6ba0/attachment.pl

From xhuil102 at uottawa.ca  Fri Nov 12 22:12:50 2004
From: xhuil102 at uottawa.ca (xhuil102@uottawa.ca)
Date: Fri, 12 Nov 2004 16:12:50 -0500 (EST)
Subject: [R] Design Matrix
Message-ID: <59505.137.122.14.20.1100293970.squirrel@137.122.14.20>

Dear all,

I need some help on matrix design and B statistics by using limma package.
I want to compare gene expression in 2 groups of cDNA samples.
The experiment compares 4 treated mice(#1,#2,#3,#4) and 4 control mice
(#5,#6,#7,#8).

The target file is
FileName     Cy3           Cy5
mice1.spot   Control_#5   Treat_#1
mice2.spot   Treat_#1     Control_#5
mice3.spot   Control_#6   Treat_#2
mice4.spot   Treat_#3     Control_#7
mice5.spot   Control_#8   Treat_#4

The first slide (mice1.spot) and the second slide(mice2.spot) are
dye-swap. There is no common reference. There are 3 replicated spots of
each gene on each array (384 genes in total).

MA is an object of class marrayNorm, below is what I did.
>design <- c(1,-1,1,-1,1)
>cor <- duplicateCorrelation(MA,design,ndups=3)
>cor$consensus.correlation
     [1] 0.506
>fit <- lmFit(MA,design,ndups=3,correlation=cor$consensus.correlation)
>fit <- eBayes(fit)
>topTable(fit,n=20,adjust="fdr")
The result is,

ID	M	A	t	P.Value	B
348	-1.3	10.8	-3.98	0.577	-4.47
371	-1.91	11.5	-3.36	0.577	-4.47
172	-2.56	13.4	-3.36	0.577	-4.47
273	-0.98	10.3	-3.22	0.577	-4.48
...

It seems this is no evidence of differential expression. But if I use the
first three slides to do analysis, design <- c(1,-1,1),the result is good,
B>5, P.Value is very small. I am wondering if my design matrix is right?

Many thanks in advance and best regards.
Michelle



From rpeng at jhsph.edu  Fri Nov 12 22:24:49 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Fri, 12 Nov 2004 16:24:49 -0500
Subject: [R] R on 64-bit Linux machine
In-Reply-To: <C698D707214E6F4AB39AB7096C3DE5A5751163@phost015.EVAFUNDS.intermedia.net>
References: <C698D707214E6F4AB39AB7096C3DE5A5751163@phost015.EVAFUNDS.intermedia.net>
Message-ID: <41952A21.4060502@jhsph.edu>

I've built (and routinely use) 64 bit R on the following platforms:

Red Hat Enterprise Linux AS release 3 (AMD Opteron 848)
Fedora Core 2 x86_64 (AMD Athlon 64 3800+)
SuSE SLES 8 (AMD Opteron 248)

One problem that has come up is that if you want to link R with ATLAS, 
you need to build shared ATLAS libraries (rather than static).  This 
requires some modifications to the configuation files for ATLAS.  But 
my experience shows that R itself builds out of the box on these systems.

-roger

Vadim Ogranovich wrote:
> Hi,
>  
> We are planning to buy a 64-bit Linux machine which will mainly run R.
> There was an interesting thread on 64-bits on r-help back in April that
> basically confirmed that the 64-bit R is fine as long as the length of
> an atomic object is less than 2^31 - 1.
>  
> My specific question is on which 64-bit Linux distros (SUSE or RedHat)
> and processors R is *known* to build out-of-box and run well. Ease of
> maintenance is essential here. We have RedHat 7.3 on other (32-bit)
> machines and would try not to proliferate the OS-s.
>  
>  
> Your information will be highly appreciated,
>  
> Thanks,
> Vadim
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From ripley at stats.ox.ac.uk  Fri Nov 12 22:33:48 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 12 Nov 2004 21:33:48 +0000 (GMT)
Subject: [R] R on 64-bit Linux machine
In-Reply-To: <C698D707214E6F4AB39AB7096C3DE5A5751163@phost015.EVAFUNDS.intermedia.net>
References: <C698D707214E6F4AB39AB7096C3DE5A5751163@phost015.EVAFUNDS.intermedia.net>
Message-ID: <Pine.LNX.4.61.0411122128340.30572@gannet.stats>

On Fri, 12 Nov 2004, Vadim Ogranovich wrote:

> Hi,
>
> We are planning to buy a 64-bit Linux machine which will mainly run R.
> There was an interesting thread on 64-bits on r-help back in April that
> basically confirmed that the 64-bit R is fine as long as the length of
> an atomic object is less than 2^31 - 1.

That's an R limitation, not a 64-bit version one.

> My specific question is on which 64-bit Linux distros (SUSE or RedHat)
> and processors R is *known* to build out-of-box and run well. Ease of
> maintenance is essential here. We have RedHat 7.3 on other (32-bit)
> machines and would try not to proliferate the OS-s.

Not RHEL 3: we sent that back for a refund and its compilers are too old 
to work well on AMD64. Fedora Core 3 is fine on AMD64, and is what I would 
recommend.  We also run SuSe 9.1, but for people used to RH, Fedora is 
more familiar.

CRAN will have 64-bit RPMs for x86_64 FC3 come R 2.0.1 (released on 
Monday)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andy_liaw at merck.com  Fri Nov 12 22:34:04 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 12 Nov 2004 16:34:04 -0500
Subject: [R] R on 64-bit Linux machine
Message-ID: <3A822319EB35174CA3714066D590DCD50994E2EC@usrymx25.merck.com>

We've had good experience so far with the threaded Goto BLAS (on Opteron
244/248/250, SLES8).  

Has anyone tried building R with supposedly more optimized compilers (PGI,
EKO, etc.)?  If so, how do they stack up against GCC?

Best,
Andy

> From: Roger D. Peng
> 
> I've built (and routinely use) 64 bit R on the following platforms:
> 
> Red Hat Enterprise Linux AS release 3 (AMD Opteron 848)
> Fedora Core 2 x86_64 (AMD Athlon 64 3800+)
> SuSE SLES 8 (AMD Opteron 248)
> 
> One problem that has come up is that if you want to link R 
> with ATLAS, 
> you need to build shared ATLAS libraries (rather than static).  This 
> requires some modifications to the configuation files for ATLAS.  But 
> my experience shows that R itself builds out of the box on 
> these systems.
> 
> -roger
> 
> Vadim Ogranovich wrote:
> > Hi,
> >  
> > We are planning to buy a 64-bit Linux machine which will 
> mainly run R.
> > There was an interesting thread on 64-bits on r-help back 
> in April that
> > basically confirmed that the 64-bit R is fine as long as 
> the length of
> > an atomic object is less than 2^31 - 1.
> >  
> > My specific question is on which 64-bit Linux distros (SUSE 
> or RedHat)
> > and processors R is *known* to build out-of-box and run 
> well. Ease of
> > maintenance is essential here. We have RedHat 7.3 on other (32-bit)
> > machines and would try not to proliferate the OS-s.
> >  
> >  
> > Your information will be highly appreciated,
> >  
> > Thanks,
> > Vadim
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> > 
> 
> -- 
> Roger D. 
> Peng
> http://www.biostat.jhsph.edu/~rpeng/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ripley at stats.ox.ac.uk  Fri Nov 12 22:37:38 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 12 Nov 2004 21:37:38 +0000 (GMT)
Subject: [R] R on 64-bit Linux machine
In-Reply-To: <41952A21.4060502@jhsph.edu>
References: <C698D707214E6F4AB39AB7096C3DE5A5751163@phost015.EVAFUNDS.intermedia.net>
	<41952A21.4060502@jhsph.edu>
Message-ID: <Pine.LNX.4.61.0411122134120.30572@gannet.stats>

On Fri, 12 Nov 2004, Roger D. Peng wrote:

> I've built (and routinely use) 64 bit R on the following platforms:
>
> Red Hat Enterprise Linux AS release 3 (AMD Opteron 848)
> Fedora Core 2 x86_64 (AMD Athlon 64 3800+)
> SuSE SLES 8 (AMD Opteron 248)
>
> One problem that has come up is that if you want to link R with ATLAS, you 
> need to build shared ATLAS libraries (rather than static).  This requires 
> some modifications to the configuation files for ATLAS.  But my experience 
> shows that R itself builds out of the box on these systems.

However, you will almost certainly get better performance out of the Goto 
BLAS implementations, and they are shared (and easy to use, much more so 
than ATLAS).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From francoisromain at free.fr  Fri Nov 12 22:57:07 2004
From: francoisromain at free.fr (=?ISO-8859-1?Q?Romain_Fran=E7ois?=)
Date: Fri, 12 Nov 2004 22:57:07 +0100
Subject: [R] Firefox keyword searches for R & S-Plus
In-Reply-To: <37367.170.54.59.167.1100284485.squirrel@170.54.59.167>
References: <37367.170.54.59.167.1100284485.squirrel@170.54.59.167>
Message-ID: <419531B3.3000706@free.fr>

Hello Kevin,

That's a good idea to use the functionnality of firefox to search for 
help in R.
I wonder if that is possible to perform the search in the searchplugins 
(like google), I tried to but there is always the characters %20= that 
messed up with the one I was actually looking at.
Maybe something I don't see well, I created the following file in my 
C:/Program Files/Mozilla Firefox/searchplugins directory (just copying 
the google one)

_____________________________________________________________________________________________
# Mozilla/R plug-in by francoisromain at free.fr

<search 
   name="R"
   description="R search engine"
   method="GET"
   action="file:///C:/Program%20Files/R/rw2000/doc/html/search/SearchObject.html"
>

<input name=" " user>

<interpret 
    browserResultType="result" 
    charset = "UTF-8"
    resultListStart="<!--a-->" 
    resultListEnd="<!--z-->" 
    resultItemStart="<!--m-->" 
    resultItemEnd="<!--n-->"
>
</search>
_______________________________________________________________________________________________


Any idea ?
Sorry for my english.

Romain.

kwright at eskimo.com a ??crit :

>If you've 'rediscovered the web' with Firefox (http://getfirefox.com), you
>might find this tip handy.  Suppose you want to search the (local) R
>documentation for information about the lme function.  Wouldn't it be nice
>to just type into the browser address bar:
>  r lme
>and have the search performed?  Here's how (on Windows):
>
>Click on this link: 
>file:///C:/Progra~1/rw2000/doc/html/search/SearchObject.html?%s
>
>Bookmark the page and name the bookmark something like: R 2.0.0 doc search
>
>Edit the properties of the bookmark (by right-clicking on the bookmark)
>and in the keywords field enter: r
>
>Click OK.
>
>
>That's all there is to it except for a couple of notes:
>
>1. The first time the local search is performed (during a browser
>session), the java search applet has to be loaded.  This takes several
>seconds.
>
>2. If you use a different operating system, a different version of R, or
>install R in a different location, change the bookmark path to the
>"SearchObject" file as appropriate.
>
>3. By default this only searches Help page titles.  You may prefer a more
>comprehensive search that includes Keywords and Object names.  Edit the
>SearchObject.html file and change 'false' to 'true' in the following line:
>  line = line + document.SearchEngine.search (searchstring,true,true,true);
>
>
>
>Here are my R and S-Plus bookmarks.  I'll just give the link, my bookmark
>name, and my bookmark keyword.
>
>http://www.google.com/u/newcastlemaths?q=%s
>R mail archive search
>rh
>
>http://www.google.com/search?q=site:www.biostat.wustl.edu+[S]+%s
>S-news archive search
>sn
>
>file:///C:/Progra~1/rw2000/doc/html/search/SearchObject.html?%s
>R 2.0.0 doc search
>r
>
>
>Happy searching!
>
>Kevin Wright
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>

-- 
Romain Fran??ois
25, avenue Guy Moquet
94 400 Vitry sur seine
FRANCE
_______________________
_______________________

francoisromain at free.fr
01 46 80 65 60
06 18 39 14 69



From p.dalgaard at biostat.ku.dk  Fri Nov 12 23:08:53 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Nov 2004 23:08:53 +0100
Subject: [R] R on 64-bit Linux machine
In-Reply-To: <41952A21.4060502@jhsph.edu>
References: <C698D707214E6F4AB39AB7096C3DE5A5751163@phost015.EVAFUNDS.intermedia.net>
	<41952A21.4060502@jhsph.edu>
Message-ID: <x2u0ru90ui.fsf@biostat.ku.dk>

"Roger D. Peng" <rpeng at jhsph.edu> writes:

> I've built (and routinely use) 64 bit R on the following platforms:
> 
> Red Hat Enterprise Linux AS release 3 (AMD Opteron 848)
> Fedora Core 2 x86_64 (AMD Athlon 64 3800+)
> SuSE SLES 8 (AMD Opteron 248)

Nice to know about the Enterprise variants. FC2/3 and SUSE 9.1 are
known good too (did anyone check 9.2 yet?).
 
> One problem that has come up is that if you want to link R with ATLAS,
> you need to build shared ATLAS libraries (rather than static).  This
> requires some modifications to the configuation files for ATLAS.  But
> my experience shows that R itself builds out of the box on these
> systems.

Actually, you can just go through configuration and add -fPIC to the
compiler flags. Then at the end, run

 ld --shared --whole-archive -o libatlas.so libatlas.a

etc. [If you don't accept architectural defaults (as you probably
shouldn't), the compile/optimize is going to take a while. I wouldn't
know how big the difference is, but -fPIC _will_ force code
differences.]

> -roger
> 
> Vadim Ogranovich wrote:
> > Hi,
> >  We are planning to buy a 64-bit Linux machine which will mainly run
> > R.
> > There was an interesting thread on 64-bits on r-help back in April that
> > basically confirmed that the 64-bit R is fine as long as the length of
> > an atomic object is less than 2^31 - 1.
> >  My specific question is on which 64-bit Linux distros (SUSE or
> > RedHat)
> > and processors R is *known* to build out-of-box and run well. Ease of
> > maintenance is essential here. We have RedHat 7.3 on other (32-bit)
> > machines and would try not to proliferate the OS-s.
> >   Your information will be highly appreciated,
> >  Thanks,
> > Vadim
> > 	[[alternative HTML version deleted]]
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> 
> -- 
> Roger D. Peng
> http://www.biostat.jhsph.edu/~rpeng/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Fri Nov 12 23:28:55 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Nov 2004 23:28:55 +0100
Subject: [R] R on 64-bit Linux machine
In-Reply-To: <Pine.LNX.4.61.0411122134120.30572@gannet.stats>
References: <C698D707214E6F4AB39AB7096C3DE5A5751163@phost015.EVAFUNDS.intermedia.net>
	<41952A21.4060502@jhsph.edu>
	<Pine.LNX.4.61.0411122134120.30572@gannet.stats>
Message-ID: <x2pt2i8zx4.fsf@biostat.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> > ATLAS, you need to build shared ATLAS libraries (rather than
> > static).  This requires some modifications to the configuation files
> > for ATLAS.  But my experience shows that R itself builds out of the
> > box on these systems.
> 
> However, you will almost certainly get better performance out of the
> Goto BLAS implementations, and they are shared (and easy to use, much
> more so than ATLAS).

I actually have different experience in the multithreaded case, at
least with my favourite "benchmark suite": inversion of a large 
matrix. I'd do some timings, but I have this ATLAS compile running
just now...

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jose.pinheiro at pharma.novartis.com  Fri Nov 12 23:38:57 2004
From: jose.pinheiro at pharma.novartis.com (jose.pinheiro@pharma.novartis.com)
Date: Fri, 12 Nov 2004 17:38:57 -0500
Subject: [R] Stat. Computing 2005 Chambers Award competition
Message-ID: <OF157D1F0C.8669631C-ON85256F4A.007C5805-85256F4A.007C6AB8@EU.novartis.net>

The Statistical Computing Section of the American Statistical
Association announces the competition for the John M. Chambers
Statistical Software Award. In 1998 the Association for Computing
Machinery presented its Software System Award to John Chambers for the
design and development of S.  Dr. Chambers generously donated his
award to the Statistical Computing Section to endow an annual prize
for statistical software written by an undergraduate or graduate
student.  The prize carries with it a cash award of $1000, plus a
substantial allowance for travel to the annual Joint Statistical
Meetings where the award will be presented. Enclosed below is the full
text of the award announcement. More details can be found at the Stat.
Computing Section website at http://www.statcomputing.org. 



Best Regards,

--Jos? Pinheiro

Awards Chair
ASA Statistical Computing Section
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: ChambersAward2005.txt
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041112/15a7b886/ChambersAward2005.txt

From suzette at sdac.harvard.edu  Sat Nov 13 00:39:19 2004
From: suzette at sdac.harvard.edu (Suzette Blanchard)
Date: Fri, 12 Nov 2004 18:39:19 -0500 (EST)
Subject: [R] geese
Message-ID: <Pine.GSO.4.40.0411121834440.5714-100000@sdac.harvard.edu>


I am working with geese from geepack, does
anyone know how to get the predicted values
from this program. I tried the predict function
and got the error

Error in predict(results.e, newdat = prog) :
        no applicable method for "predict"

Thank you for any help you can send,
Suzette

=================================
Suzette Blanchard, Ph.D.
UCSD-PPRU



From kwright at eskimo.com  Sat Nov 13 03:13:02 2004
From: kwright at eskimo.com (kwright@eskimo.com)
Date: Fri, 12 Nov 2004 18:13:02 -0800 (PST)
Subject: [R] Firefox keyword searches for R & S-Plus
In-Reply-To: <419531B3.3000706@free.fr>
References: <37367.170.54.59.167.1100284485.squirrel@170.54.59.167>
	<419531B3.3000706@free.fr>
Message-ID: <3371.66.185.0.207.1100311982.squirrel@66.185.0.207>


Nice idea.  A couple of comments.

1. I personally always install R in c:/R/ instead of c:/Program Files/
There are frequently comments on R-help that it is better to avoid paths
with spaces.  Still, you should be able to use /Progra~1/ instead of
/Program Files/

2. The search engine plugin that you have created generates a URL like this:
.../SearchObject.html?q=nlme

What is needed is a URL like
.../SearchObject.html?%s
where the %s is the string from the text box.

I speculate it might be possible to change the SearchObject.html file to
grab the "q=nlme" and strip off the "q=" but I have no idea if this is
possible.

I'm not comfortable with HTML/Java/Javascript to get this working properly.

3. My comment about editing the SearchObject.html file to change the
"false" to "true" doesn't seem to cause the search to match the results
obtained using the traditional way of opening the search page and typing
in the text.
I'm not sure why.  Would love it if someone figured this out.

Seems like all of this would be fairly easy for a good web programmer.

Kevin


> Hello Kevin,
>
> That's a good idea to use the functionnality of firefox to search for
> help in R.
> I wonder if that is possible to perform the search in the searchplugins
> (like google), I tried to but there is always the characters %20= that
> messed up with the one I was actually looking at.
> Maybe something I don't see well, I created the following file in my
> C:/Program Files/Mozilla Firefox/searchplugins directory (just copying
> the google one)
>
> _____________________________________________________________________________________________
> # Mozilla/R plug-in by francoisromain at free.fr
>
> <search
>    name="R"
>    description="R search engine"
>    method="GET"
>    action="file:///C:/Program%20Files/R/rw2000/doc/html/search/SearchObject.html"
>>
>
> <input name=" " user>
>
> <interpret
>     browserResultType="result"
>     charset = "UTF-8"
>     resultListStart="<!--a-->"
>     resultListEnd="<!--z-->"
>     resultItemStart="<!--m-->"
>     resultItemEnd="<!--n-->"
>>
> </search>
> _______________________________________________________________________________________________
>
>
> Any idea ?
> Sorry for my english.
>
> Romain.
>
> kwright at eskimo.com a ??crit :
>
>>If you've 'rediscovered the web' with Firefox (http://getfirefox.com),
>> you
>>might find this tip handy.  Suppose you want to search the (local) R
>>documentation for information about the lme function.  Wouldn't it be
>> nice
>>to just type into the browser address bar:
>>  r lme
>>and have the search performed?  Here's how (on Windows):
>>
>>Click on this link:
>>file:///C:/Progra~1/rw2000/doc/html/search/SearchObject.html?%s
>>
>>Bookmark the page and name the bookmark something like: R 2.0.0 doc
>> search
>>
>>Edit the properties of the bookmark (by right-clicking on the bookmark)
>>and in the keywords field enter: r
>>
>>Click OK.
>>
>>
>>That's all there is to it except for a couple of notes:
>>
>>1. The first time the local search is performed (during a browser
>>session), the java search applet has to be loaded.  This takes several
>>seconds.
>>
>>2. If you use a different operating system, a different version of R, or
>>install R in a different location, change the bookmark path to the
>>"SearchObject" file as appropriate.
>>
>>3. By default this only searches Help page titles.  You may prefer a more
>>comprehensive search that includes Keywords and Object names.  Edit the
>>SearchObject.html file and change 'false' to 'true' in the following
>> line:
>>  line = line + document.SearchEngine.search
>> (searchstring,true,true,true);
>>
>>
>>
>>Here are my R and S-Plus bookmarks.  I'll just give the link, my bookmark
>>name, and my bookmark keyword.
>>
>>http://www.google.com/u/newcastlemaths?q=%s
>>R mail archive search
>>rh
>>
>>http://www.google.com/search?q=site:www.biostat.wustl.edu+[S]+%s
>>S-news archive search
>>sn
>>
>>file:///C:/Progra~1/rw2000/doc/html/search/SearchObject.html?%s
>>R 2.0.0 doc search
>>r
>>
>>
>>Happy searching!
>>
>>Kevin Wright
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>>
>>
>>
>
> --
> Romain Fran??ois
> 25, avenue Guy Moquet
> 94 400 Vitry sur seine
> FRANCE
> _______________________
> _______________________
>
> francoisromain at free.fr
> 01 46 80 65 60
> 06 18 39 14 69
>
>
>



From alexandrepatrot at yahoo.com.br  Sat Nov 13 03:20:22 2004
From: alexandrepatrot at yahoo.com.br (=?iso-8859-1?q?Alexandre=20Galv=E3o=20Patriota?=)
Date: Fri, 12 Nov 2004 23:20:22 -0300 (ART)
Subject: [R] Variance and Covariance Matrix D and R in nlme or lme4.
Message-ID: <20041113022022.29334.qmail@web52703.mail.yahoo.com>

Hi, 
      How extract the Variance and Covariance Matrices
D of random effects and R of error in the lme object?

Thanks in advance.

Alexandre Galv??o



From alexandrepatrot at yahoo.com.br  Sat Nov 13 03:46:16 2004
From: alexandrepatrot at yahoo.com.br (=?iso-8859-1?q?Alexandre=20Galv=E3o=20Patriota?=)
Date: Fri, 12 Nov 2004 23:46:16 -0300 (ART)
Subject: [R] Variance and Covariance Matrix D and R in nlme or lme4 part II
Message-ID: <20041113024616.86470.qmail@web52707.mail.yahoo.com>

The model is Y = XB + Zg + e

where

g~N(0, D)

e~N(0, R)

How to extract the VAR(g)= D, VAR(e)=R and V=ZDZ'+R?

thanks



From br44114 at yahoo.com  Sat Nov 13 04:53:10 2004
From: br44114 at yahoo.com (bogdan romocea)
Date: Fri, 12 Nov 2004 19:53:10 -0800 (PST)
Subject: [R] density estimation: compute sum(value * probability) for given
	distribution
Message-ID: <20041113035310.89960.qmail@web50301.mail.yahoo.com>

Dear R users,

This is a KDE beginner's question. 
I have this distribution:
> length(cap)
[1] 200
> summary(cap)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  459.9   802.3   991.6  1066.0  1242.0  2382.0 
I need to compute the sum of the values times their probability of
occurence.

The graph is fine,
den <- density(cap, from=min(cap), 
       to=max(cap), give.Rkern=F)
plot(den)

However, how do I compute sum(values*probabilities)? The
probabilities produced by the density function sum to only 26%: 
> sum(den$y)
[1] 0.2611142

Would it perhaps be ok to simply do
> sum(den$x*den$y) * (1/sum(den$y))
[1] 1073.22
?

Thank you,
b.



From james.muller at anu.edu.au  Sat Nov 13 07:51:22 2004
From: james.muller at anu.edu.au (James Muller)
Date: Sat, 13 Nov 2004 17:51:22 +1100
Subject: [R] unavoidable loop? a better way??
Message-ID: <opshd5bwr1z5jgoz@muller>

Hi all, I have the following problem, best expressed by my present  
solution:

# p is a vector
myfunc <- function (p) {
   x[1] <- p[1]
   for (i in c(2:length(p))) {
     x[i] <- 0.8*p[i] + 0.2*p[i-1]
   }
   return (x)
}

That is, I'm calculating a time-weighted average. Unfortunately the scale  
of the problem is big. length(p) in this case is such that each call takes  
about 6 seconds, and I have to call it about 2000 times (~3 hours). And,  
I'd like to do this each day. Thus, a more efficient method is desirable.

Of course, this could be done faster by writing it in c, but I want to  
avoid doing that if there already exists something internal to do the  
operation quickly (because I've never programmed c for use in R).

Can anybody offer a solution?

I apologise if this is a naive question.

James



From mn216 at columbia.edu  Sat Nov 13 07:48:14 2004
From: mn216 at columbia.edu (Murad Nayal)
Date: Sat, 13 Nov 2004 01:48:14 -0500
Subject: [R] unavoidable loop? a better way??
References: <opshd5bwr1z5jgoz@muller>
Message-ID: <4195AE2E.D73BFBC0@columbia.edu>



From james.muller at anu.edu.au  Sat Nov 13 08:08:50 2004
From: james.muller at anu.edu.au (James Muller)
Date: Sat, 13 Nov 2004 18:08:50 +1100
Subject: [R] unavoidable loop? a better way??
In-Reply-To: <4195AE2E.D73BFBC0@columbia.edu>
References: <opshd5bwr1z5jgoz@muller> <4195AE2E.D73BFBC0@columbia.edu>
Message-ID: <opshd540gwz5jgoz@muller>

I am very sorry. I've made a typo. The function should be:

# p is a vector
myfunc <- function (p) {
   x[1] <- p[1]
   for (i in c(2:length(p))) {
     x[i] <- 0.8*p[i] + 0.2*x[i-1]
   }
   return (x)
}


James



From mn216 at columbia.edu  Sat Nov 13 07:53:14 2004
From: mn216 at columbia.edu (Murad Nayal)
Date: Sat, 13 Nov 2004 01:53:14 -0500
Subject: [Fwd: Re: [R] unavoidable loop? a better way??]
Message-ID: <4195AF5A.6D9B6E5@columbia.edu>



From deepayan at stat.wisc.edu  Sat Nov 13 08:12:50 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Sat, 13 Nov 2004 01:12:50 -0600
Subject: [R] unavoidable loop? a better way??
In-Reply-To: <opshd5bwr1z5jgoz@muller>
References: <opshd5bwr1z5jgoz@muller>
Message-ID: <200411130112.50127.deepayan@stat.wisc.edu>

On Saturday 13 November 2004 00:51, James Muller wrote:
> Hi all, I have the following problem, best expressed by my present
> solution:
>
> # p is a vector
> myfunc <- function (p) {
>    x[1] <- p[1]
>    for (i in c(2:length(p))) {
>      x[i] <- 0.8*p[i] + 0.2*p[i-1]
>    }
>    return (x)
> }

Does this work at all? I get

> myfunc <- function (p) {
+    x[1] <- p[1]
+    for (i in c(2:length(p))) {
+      x[i] <- 0.8*p[i] + 0.2*p[i-1]
+    }
+    return (x)
+ }
>
> myfunc(1:10)
Error in myfunc(1:10) : Object "x" not found


Anyway, simple loops are almost always avoidable. e.g.,

myfunc <- function (p) {
   x <- p
   x[-1] <- 0.8 * p[-1] + 0.2 * p[-length(p)]
   x
}

Deepayan

>
> That is, I'm calculating a time-weighted average. Unfortunately the
> scale of the problem is big. length(p) in this case is such that each
> call takes about 6 seconds, and I have to call it about 2000 times
> (~3 hours). And, I'd like to do this each day. Thus, a more efficient
> method is desirable.
>
> Of course, this could be done faster by writing it in c, but I want
> to avoid doing that if there already exists something internal to do
> the operation quickly (because I've never programmed c for use in R).
>
> Can anybody offer a solution?
>
> I apologise if this is a naive question.
>
> James



From james.muller at anu.edu.au  Sat Nov 13 08:17:29 2004
From: james.muller at anu.edu.au (James Muller)
Date: Sat, 13 Nov 2004 18:17:29 +1100
Subject: [R] unavoidable loop? a better way??
In-Reply-To: <200411130112.50127.deepayan@stat.wisc.edu>
References: <opshd5bwr1z5jgoz@muller>
	<200411130112.50127.deepayan@stat.wisc.edu>
Message-ID: <opshd6jfvsz5jgoz@muller>

Take 3:

# p is a vector
myfunc <- function (p) {
    x <- rep(0,length(p))
    x[1] <- p[1]
    for (i in c(2:length(p))) {
      x[i] <- 0.8*p[i] + 0.2*x[i-1]   # note the x in the last term
    }
    return (x)
}

James





On Sat, 13 Nov 2004 01:12:50 -0600, Deepayan Sarkar  
<deepayan at stat.wisc.edu> wrote:

> On Saturday 13 November 2004 00:51, James Muller wrote:
>> Hi all, I have the following problem, best expressed by my present
>> solution:
>>
>> # p is a vector
>> myfunc <- function (p) {
>>    x[1] <- p[1]
>>    for (i in c(2:length(p))) {
>>      x[i] <- 0.8*p[i] + 0.2*p[i-1]
>>    }
>>    return (x)
>> }
>
> Does this work at all? I get
>
>> myfunc <- function (p) {
> +    x[1] <- p[1]
> +    for (i in c(2:length(p))) {
> +      x[i] <- 0.8*p[i] + 0.2*p[i-1]
> +    }
> +    return (x)
> + }
>>
>> myfunc(1:10)
> Error in myfunc(1:10) : Object "x" not found
>
>
> Anyway, simple loops are almost always avoidable. e.g.,
>
> myfunc <- function (p) {
>    x <- p
>    x[-1] <- 0.8 * p[-1] + 0.2 * p[-length(p)]
>    x
> }
>
> Deepayan
>
>>
>> That is, I'm calculating a time-weighted average. Unfortunately the
>> scale of the problem is big. length(p) in this case is such that each
>> call takes about 6 seconds, and I have to call it about 2000 times
>> (~3 hours). And, I'd like to do this each day. Thus, a more efficient
>> method is desirable.
>>
>> Of course, this could be done faster by writing it in c, but I want
>> to avoid doing that if there already exists something internal to do
>> the operation quickly (because I've never programmed c for use in R).
>>
>> Can anybody offer a solution?
>>
>> I apologise if this is a naive question.
>>
>> James



From ripley at stats.ox.ac.uk  Sat Nov 13 09:00:49 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 13 Nov 2004 08:00:49 +0000 (GMT)
Subject: [R] unavoidable loop? a better way??
In-Reply-To: <opshd6jfvsz5jgoz@muller>
References: <opshd5bwr1z5jgoz@muller>
	<200411130112.50127.deepayan@stat.wisc.edu>
	<opshd6jfvsz5jgoz@muller>
Message-ID: <Pine.LNX.4.61.0411130759280.7173@gannet.stats>

Ah, that is now a recursive linear filter.  In fact filter() would do both 
your examples.

On Sat, 13 Nov 2004, James Muller wrote:

> Take 3:
>
> # p is a vector
> myfunc <- function (p) {
>   x <- rep(0,length(p))
>   x[1] <- p[1]
>   for (i in c(2:length(p))) {
>     x[i] <- 0.8*p[i] + 0.2*x[i-1]   # note the x in the last term
>   }
>   return (x)
> }
>
> James
>
>
>
>
>
> On Sat, 13 Nov 2004 01:12:50 -0600, Deepayan Sarkar <deepayan at stat.wisc.edu> 
> wrote:
>
>> On Saturday 13 November 2004 00:51, James Muller wrote:
>>> Hi all, I have the following problem, best expressed by my present
>>> solution:
>>> 
>>> # p is a vector
>>> myfunc <- function (p) {
>>>    x[1] <- p[1]
>>>    for (i in c(2:length(p))) {
>>>      x[i] <- 0.8*p[i] + 0.2*p[i-1]
>>>    }
>>>    return (x)
>>> }
>> 
>> Does this work at all? I get
>> 
>>> myfunc <- function (p) {
>> +    x[1] <- p[1]
>> +    for (i in c(2:length(p))) {
>> +      x[i] <- 0.8*p[i] + 0.2*p[i-1]
>> +    }
>> +    return (x)
>> + }
>>> 
>>> myfunc(1:10)
>> Error in myfunc(1:10) : Object "x" not found
>> 
>> 
>> Anyway, simple loops are almost always avoidable. e.g.,
>> 
>> myfunc <- function (p) {
>>    x <- p
>>    x[-1] <- 0.8 * p[-1] + 0.2 * p[-length(p)]
>>    x
>> }
>> 
>> Deepayan
>> 
>>> 
>>> That is, I'm calculating a time-weighted average. Unfortunately the
>>> scale of the problem is big. length(p) in this case is such that each
>>> call takes about 6 seconds, and I have to call it about 2000 times
>>> (~3 hours). And, I'd like to do this each day. Thus, a more efficient
>>> method is desirable.
>>> 
>>> Of course, this could be done faster by writing it in c, but I want
>>> to avoid doing that if there already exists something internal to do
>>> the operation quickly (because I've never programmed c for use in R).
>>> 
>>> Can anybody offer a solution?
>>> 
>>> I apologise if this is a naive question.
>>> 
>>> James
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ales.ziberna at guest.arnes.si  Sat Nov 13 12:15:54 2004
From: ales.ziberna at guest.arnes.si (=?iso-8859-1?Q?Ales_Ziberna?=)
Date: Sat, 13 Nov 2004 12:15:54 +0100
Subject: [R] How to get mode (the most frequent value in distribution)?
References: <4194B10C.9080802@bfro.uni-lj.si><20041112124449.UCVY21843.smta03.mail.ozemail.net@there>
	<4194F519.1050908@bfro.uni-lj.si>
Message-ID: <01a701c4c972$2b233980$1e09f9c2@ales>

Maybe a litle simpler solution:

x<-rnorm(50)
den<-density(x) #see ?density for more details
den$x[which(den$y==max(den$y))]

Hope it helps,
Ales Ziberna

----- Original Message ----- 
From: "Gregor GORJANC" <gregor.gorjanc at bfro.uni-lj.si>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, November 12, 2004 6:38 PM
Subject: Re: [R] How to get mode (the most frequent value in distribution)?


Thanks to all!

It appears that mode calculation is not easy or simple at all. I
understand that it is problematic when you have continous distribution,
but there should still be some general function, as for median, mean, ...

-- 
Lep pozdrav / With regards,
     Gregor GORJANC

---------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty       URI: http://www.bfro.uni-lj.si
Zootechnical Department    mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                  tel: +386 (0)1 72 17 861
SI-1230 Domzale            fax: +386 (0)1 72 41 005
Slovenia

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html



From francoisromain at free.fr  Sat Nov 13 13:35:40 2004
From: francoisromain at free.fr (=?ISO-8859-1?Q?Romain_Fran=E7ois?=)
Date: Sat, 13 Nov 2004 13:35:40 +0100
Subject: [R] Firefox keyword searches for R & S-Plus
In-Reply-To: <3371.66.185.0.207.1100311982.squirrel@66.185.0.207>
References: <37367.170.54.59.167.1100284485.squirrel@170.54.59.167>
	<419531B3.3000706@free.fr>
	<3371.66.185.0.207.1100311982.squirrel@66.185.0.207>
Message-ID: <4195FF9C.7000503@free.fr>

Dear wizaRds, Kevin,

I came up with a solution to perform a r doc search in the Firefox 
search bar, you'll have to create a file called R.src in the search 
plugins directory  : (for me it is : C:/Program Files/Mozilla 
Firefox/searchplugins) and  put a png image of the R logo in the same 
directory (the logo has to be named R.png) : same prefix as the .src file.

My R.src file looks like that:

# Mozilla/R plug-in by francoisromain at free.fr

<search 
   name="R"
   description="R search engine"
   method="GET"
   action="file:///C:/Program%20Files/R/rw2000/doc/html/search/SearchObject.html"
>

<input name="SEARCHTERM" user>

<interpret 
    browserResultType="result" 
    charset = "UTF-8"
    resultListStart="<!--a-->" 
    resultListEnd="<!--z-->" 
    resultItemStart="<!--m-->" 
    resultItemEnd="<!--n-->"
>
</search>
#end of file


With only that the problem is that Firefox is calling the page

file:///C:/Program%20Files/R/rw2000/doc/html/search/SearchObject.html?_SEARCHTERM=_glm

if I want entries about glm. So the response is that

"No matches for " SEARCHTERM=glm " have been found 



I looked at several searchplugins but I think we can't get rid of the 
"SEARCHTERM=".

Then, I changed the searchObject.html file to remove that part of the 
searchstring, just after the commentary //call the applet code!, I just 
add that :

if(searchstring.length > 10 & searchstring.substring(0,10) == 'SEARCHTERM') { 
       searchstring = searchstring.substring(11,searchstring.length) ;
}



That works fine for me except that I don't get as much entries as the 
searchPluging.html gives : any ideas ?

Hope that will be helpfull for somebody else.

Romain.

> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    0.0            
year     2004           
month    10             
day      04             
language R 




kwright at eskimo.com a ??crit :

>Nice idea.  A couple of comments.
>
>1. I personally always install R in c:/R/ instead of c:/Program Files/
>There are frequently comments on R-help that it is better to avoid paths
>with spaces.  Still, you should be able to use /Progra~1/ instead of
>/Program Files/
>
>2. The search engine plugin that you have created generates a URL like this:
>.../SearchObject.html?q=nlme
>
>What is needed is a URL like
>.../SearchObject.html?%s
>where the %s is the string from the text box.
>
>I speculate it might be possible to change the SearchObject.html file to
>grab the "q=nlme" and strip off the "q=" but I have no idea if this is
>possible.
>
>I'm not comfortable with HTML/Java/Javascript to get this working properly.
>
>3. My comment about editing the SearchObject.html file to change the
>"false" to "true" doesn't seem to cause the search to match the results
>obtained using the traditional way of opening the search page and typing
>in the text.
>I'm not sure why.  Would love it if someone figured this out.
>
>Seems like all of this would be fairly easy for a good web programmer.
>
>Kevin
>
>
>  
>
>>Hello Kevin,
>>
>>That's a good idea to use the functionnality of firefox to search for
>>help in R.
>>I wonder if that is possible to perform the search in the searchplugins
>>(like google), I tried to but there is always the characters %20= that
>>messed up with the one I was actually looking at.
>>Maybe something I don't see well, I created the following file in my
>>C:/Program Files/Mozilla Firefox/searchplugins directory (just copying
>>the google one)
>>
>>_____________________________________________________________________________________________
>># Mozilla/R plug-in by francoisromain at free.fr
>>
>><search
>>   name="R"
>>   description="R search engine"
>>   method="GET"
>>   action="file:///C:/Program%20Files/R/rw2000/doc/html/search/SearchObject.html"
>>    
>>
>><input name=" " user>
>>
>><interpret
>>    browserResultType="result"
>>    charset = "UTF-8"
>>    resultListStart="<!--a-->"
>>    resultListEnd="<!--z-->"
>>    resultItemStart="<!--m-->"
>>    resultItemEnd="<!--n-->"
>>    
>>
>></search>
>>_______________________________________________________________________________________________
>>
>>
>>Any idea ?
>>Sorry for my english.
>>
>>Romain.
>>
>>kwright at eskimo.com a ??crit :
>>
>>    
>>
>>>If you've 'rediscovered the web' with Firefox (http://getfirefox.com),
>>>you
>>>might find this tip handy.  Suppose you want to search the (local) R
>>>documentation for information about the lme function.  Wouldn't it be
>>>nice
>>>to just type into the browser address bar:
>>> r lme
>>>and have the search performed?  Here's how (on Windows):
>>>
>>>Click on this link:
>>>file:///C:/Progra~1/rw2000/doc/html/search/SearchObject.html?%s
>>>
>>>Bookmark the page and name the bookmark something like: R 2.0.0 doc
>>>search
>>>
>>>Edit the properties of the bookmark (by right-clicking on the bookmark)
>>>and in the keywords field enter: r
>>>
>>>Click OK.
>>>
>>>
>>>That's all there is to it except for a couple of notes:
>>>
>>>1. The first time the local search is performed (during a browser
>>>session), the java search applet has to be loaded.  This takes several
>>>seconds.
>>>
>>>2. If you use a different operating system, a different version of R, or
>>>install R in a different location, change the bookmark path to the
>>>"SearchObject" file as appropriate.
>>>
>>>3. By default this only searches Help page titles.  You may prefer a more
>>>comprehensive search that includes Keywords and Object names.  Edit the
>>>SearchObject.html file and change 'false' to 'true' in the following
>>>line:
>>> line = line + document.SearchEngine.search
>>>(searchstring,true,true,true);
>>>
>>>
>>>
>>>Here are my R and S-Plus bookmarks.  I'll just give the link, my bookmark
>>>name, and my bookmark keyword.
>>>
>>>http://www.google.com/u/newcastlemaths?q=%s
>>>R mail archive search
>>>rh
>>>
>>>http://www.google.com/search?q=site:www.biostat.wustl.edu+[S]+%s
>>>S-news archive search
>>>sn
>>>
>>>file:///C:/Progra~1/rw2000/doc/html/search/SearchObject.html?%s
>>>R 2.0.0 doc search
>>>r
>>>
>>>
>>>Happy searching!
>>>
>>>Kevin Wright
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide!
>>>http://www.R-project.org/posting-guide.html
>>>
>>>
>>>
>>>
>>>      
>>>
>>--
>>Romain Fran??ois
>>25, avenue Guy Moquet
>>94 400 Vitry sur seine
>>FRANCE
>>_______________________
>>_______________________
>>
>>francoisromain at free.fr
>>01 46 80 65 60
>>06 18 39 14 69
>>
>>
>>
>>    
>>
>
>
>
>  
>

-- 
Romain Fran??ois
25, avenue Guy Moquet
94 400 Vitry sur seine
FRANCE
_______________________
_______________________

francoisromain at free.fr
01 46 80 65 60
06 18 39 14 69



From andy_liaw at merck.com  Sat Nov 13 14:11:45 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 13 Nov 2004 08:11:45 -0500
Subject: [R] density estimation: compute sum(value * probability)
	for given distribution
Message-ID: <3A822319EB35174CA3714066D590DCD50994E2F3@usrymx25.merck.com>

First thing you probably should realize is that density is _not_
probability.  A probability density function _integrates_ to one, not _sum_
to one.  If X is an absolutely continuous RV with density f, then Pr(X=x)=0
for all x, and Pr(a < X < b) = \int_a^b f(x) dx.

sum x*Pr(X=x) (over all possible values of x) for a discrete distribution is
just the expectation, or mean, of the distribution.  The expectation for a
continuous distribution is \int x f(x) dx, where the integral is over the
support of f.  This is all elementary math stat that you can find in any
textbook.

Could you tell us exactly what you are trying to compute, or why you're
computing it?

HTH,
Andy

> From: bogdan romocea
> 
> Dear R users,
> 
> This is a KDE beginner's question. 
> I have this distribution:
> > length(cap)
> [1] 200
> > summary(cap)
>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
>   459.9   802.3   991.6  1066.0  1242.0  2382.0 
> I need to compute the sum of the values times their probability of
> occurence.
> 
> The graph is fine,
> den <- density(cap, from=min(cap), 
>        to=max(cap), give.Rkern=F)
> plot(den)
> 
> However, how do I compute sum(values*probabilities)? The
> probabilities produced by the density function sum to only 26%: 
> > sum(den$y)
> [1] 0.2611142
> 
> Would it perhaps be ok to simply do
> > sum(den$x*den$y) * (1/sum(den$y))
> [1] 1073.22
> ?
> 
> Thank you,
> b.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From annhuxtable at hotmail.com  Sat Nov 13 15:27:55 2004
From: annhuxtable at hotmail.com (Ann Huxtable)
Date: Sat, 13 Nov 2004 14:27:55 +0000
Subject: [R] determing the distribution of a sample data set etc..
Message-ID: <BAY2-F28zDpubxcbz370005054a@hotmail.com>

Hello,

I have only recently started using R. I have two data samples that I want to 
carry out some initial explorative data analysis to:

i). Determine the distribution of the data
ii). Determine whether both datasets are from the same distribution.

I have managed to create unit probability histograms and created qqplots for 
the data. I have attached one of the qqplots. It is clear that the data is 
not from a normal distribution (it forms a convex curve underneath the 
straight line).  the nature of the curve suggest the data is from either 
Chi-square or F distribution (if you think otherwise, I would appreciate 
your help in correcting my analysis).

The point of this mail however, is how do I use R to:

1). Test if the data is from another distribution (F, Ch-Square etc.. )
2). How can I check if the samples are drawn from the same distribution?

many thanks in advance for your help.

Ann



From p.dalgaard at biostat.ku.dk  Sat Nov 13 15:28:26 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 13 Nov 2004 15:28:26 +0100
Subject: [R] R on 64-bit Linux machine
In-Reply-To: <x2pt2i8zx4.fsf@biostat.ku.dk>
References: <C698D707214E6F4AB39AB7096C3DE5A5751163@phost015.EVAFUNDS.intermedia.net>
	<41952A21.4060502@jhsph.edu>
	<Pine.LNX.4.61.0411122134120.30572@gannet.stats>
	<x2pt2i8zx4.fsf@biostat.ku.dk>
Message-ID: <x24qjt962d.fsf@biostat.ku.dk>

Peter Dalgaard <p.dalgaard at biostat.ku.dk> writes:

> Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:
> 
> > > ATLAS, you need to build shared ATLAS libraries (rather than
> > > static).  This requires some modifications to the configuation files
> > > for ATLAS.  But my experience shows that R itself builds out of the
> > > box on these systems.
> > 
> > However, you will almost certainly get better performance out of the
> > Goto BLAS implementations, and they are shared (and easy to use, much
> > more so than ATLAS).
> 
> I actually have different experience in the multithreaded case, at
> least with my favourite "benchmark suite": inversion of a large 
> matrix. I'd do some timings, but I have this ATLAS compile running
> just now...

Specifically, here's what I got:

pd at linux:~/r-devel> echo 'set.seed(1);M<-matrix(rnorm(9e6),3e3);system.time(solve(M))' | BUILD/bin/R -q --vanilla
> set.seed(1);M<-matrix(rnorm(9e6),3e3);system.time(solve(M))
[1] 218.00   1.27 219.62   0.00   0.00
>
pd at linux:~/r-devel> echo 'set.seed(1);M<-matrix(rnorm(9e6),3e3);system.time(solve(M))' | BUILD-GOTO/bin/R -q --vanilla
> set.seed(1);M<-matrix(rnorm(9e6),3e3);system.time(solve(M))
[1] 29.12  1.39 32.21  0.00  0.00
>
pd at linux:~/r-devel> echo 'set.seed(1);M<-matrix(rnorm(9e6),3e3);system.time(solve(M))' | BUILD-ATLAS/bin/R -q --vanilla
> set.seed(1);M<-matrix(rnorm(9e6),3e3);system.time(solve(M))
[1]  3.24  1.31 21.45 31.75  0.24
>

So ATLAS is faster than GOTO by about 10 seconds. It is a bit odd that
the GOTO timings don't seem to include any subprocess time but it
should be the threaded library libgoto_opt64p-r0.93.so (I know;
there's a 0.96 now, will upgrade).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From br44114 at yahoo.com  Sat Nov 13 15:45:00 2004
From: br44114 at yahoo.com (bogdan romocea)
Date: Sat, 13 Nov 2004 06:45:00 -0800 (PST)
Subject: [R] density estimation: compute sum(value * probability) for
	given distribution
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E2F3@usrymx25.merck.com>
Message-ID: <20041113144500.8138.qmail@web50303.mail.yahoo.com>

Andy,

Thanks a lot for the clarifications. I was running a simulation a
number of times and trying to come up with a number to summarize the
results. And, I failed to realize from the beginning that what I was
trying to compute was just the mean.....

Regards,
b.


--- "Liaw, Andy" <andy_liaw at merck.com> wrote:

> First thing you probably should realize is that density is _not_
> probability.  A probability density function _integrates_ to one,
> not _sum_
> to one.  If X is an absolutely continuous RV with density f, then
> Pr(X=x)=0
> for all x, and Pr(a < X < b) = \int_a^b f(x) dx.
> 
> sum x*Pr(X=x) (over all possible values of x) for a discrete
> distribution is
> just the expectation, or mean, of the distribution.  The
> expectation for a
> continuous distribution is \int x f(x) dx, where the integral is
> over the
> support of f.  This is all elementary math stat that you can find
> in any
> textbook.
> 
> Could you tell us exactly what you are trying to compute, or why
> you're
> computing it?
> 
> HTH,
> Andy
> 
> > From: bogdan romocea
> > 
> > Dear R users,
> > 
> > This is a KDE beginner's question. 
> > I have this distribution:
> > > length(cap)
> > [1] 200
> > > summary(cap)
> >    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
> >   459.9   802.3   991.6  1066.0  1242.0  2382.0 
> > I need to compute the sum of the values times their probability
> of
> > occurence.
> > 
> > The graph is fine,
> > den <- density(cap, from=min(cap), 
> >        to=max(cap), give.Rkern=F)
> > plot(den)
> > 
> > However, how do I compute sum(values*probabilities)? The
> > probabilities produced by the density function sum to only 26%: 
> > > sum(den$y)
> > [1] 0.2611142
> > 
> > Would it perhaps be ok to simply do
> > > sum(den$x*den$y) * (1/sum(den$y))
> > [1] 1073.22
> > ?
> > 
> > Thank you,
> > b.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> > 
> 
> 
>
------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any attachments,
> contains information of Merck & Co., Inc. (One Merck Drive,
> Whitehouse Station, New Jersey, USA 08889), and/or its affiliates
> (which may be known outside the United States as Merck Frosst,
> Merck Sharp & Dohme or MSD and in Japan, as Banyu) that may be
> confidential, proprietary copyrighted and/or legally privileged. It
> is intended solely for the use of the individual or entity named on
> this message.  If you are not the intended recipient, and have
> received this message in error, please notify us immediately by
> reply e-mail and then delete it from your system.
>
------------------------------------------------------------------------------
>



From ripley at stats.ox.ac.uk  Sat Nov 13 15:57:45 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 13 Nov 2004 14:57:45 +0000 (GMT)
Subject: [R] R on 64-bit Linux machine
In-Reply-To: <x24qjt962d.fsf@biostat.ku.dk>
References: <C698D707214E6F4AB39AB7096C3DE5A5751163@phost015.EVAFUNDS.intermedia.net>
	<41952A21.4060502@jhsph.edu>
	<Pine.LNX.4.61.0411122134120.30572@gannet.stats>
	<x2pt2i8zx4.fsf@biostat.ku.dk> <x24qjt962d.fsf@biostat.ku.dk>
Message-ID: <Pine.LNX.4.61.0411131442320.11699@gannet.stats>

On Sat, 13 Nov 2004, Peter Dalgaard wrote:

> Peter Dalgaard <p.dalgaard at biostat.ku.dk> writes:
>
>> Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:
>>
>>>> ATLAS, you need to build shared ATLAS libraries (rather than
>>>> static).  This requires some modifications to the configuation files
>>>> for ATLAS.  But my experience shows that R itself builds out of the
>>>> box on these systems.
>>>
>>> However, you will almost certainly get better performance out of the
>>> Goto BLAS implementations, and they are shared (and easy to use, much
>>> more so than ATLAS).
>>
>> I actually have different experience in the multithreaded case, at
>> least with my favourite "benchmark suite": inversion of a large
>> matrix. I'd do some timings, but I have this ATLAS compile running
>> just now...
>
> Specifically, here's what I got:
>
> pd at linux:~/r-devel> echo 'set.seed(1);M<-matrix(rnorm(9e6),3e3);system.time(solve(M))' | BUILD/bin/R -q --vanilla
>> set.seed(1);M<-matrix(rnorm(9e6),3e3);system.time(solve(M))
> [1] 218.00   1.27 219.62   0.00   0.00
>>
> pd at linux:~/r-devel> echo 'set.seed(1);M<-matrix(rnorm(9e6),3e3);system.time(solve(M))' | BUILD-GOTO/bin/R -q --vanilla
>> set.seed(1);M<-matrix(rnorm(9e6),3e3);system.time(solve(M))
> [1] 29.12  1.39 32.21  0.00  0.00
>>
> pd at linux:~/r-devel> echo 'set.seed(1);M<-matrix(rnorm(9e6),3e3);system.time(solve(M))' | BUILD-ATLAS/bin/R -q --vanilla
>> set.seed(1);M<-matrix(rnorm(9e6),3e3);system.time(solve(M))
> [1]  3.24  1.31 21.45 31.75  0.24
>>
>
> So ATLAS is faster than GOTO by about 10 seconds. It is a bit odd that

Not on total CPU time (it's slower by about the margin I would expect), 
only on elapsed time.

> the GOTO timings don't seem to include any subprocess time but it
> should be the threaded library libgoto_opt64p-r0.93.so (I know;
> there's a 0.96 now, will upgrade).

I get (on a dual Opteron 248 with 0.96-2)

[1] 20.59  1.01 19.10  0.00  0.00

which note is using more than 100% CPU time.  Are you sure you are using
multiple threads with Goto?

I have never built a threaded ATLAS for that machine, as in our 
environment people are normally running multiple jobs and it is total CPU 
time that counts.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bates at stat.wisc.edu  Sat Nov 13 15:59:19 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 13 Nov 2004 08:59:19 -0600
Subject: [R] Variance and Covariance Matrix D and R in nlme or lme4 part II
In-Reply-To: <20041113024616.86470.qmail@web52707.mail.yahoo.com>
References: <20041113024616.86470.qmail@web52707.mail.yahoo.com>
Message-ID: <41962147.5060009@stat.wisc.edu>

Alexandre Galv??o Patriota wrote:
> The model is Y = XB + Zg + e
> 
> where
> 
> g~N(0, D)
> 
> e~N(0, R)
> 
> How to extract the VAR(g)= D, VAR(e)=R and V=ZDZ'+R?
> 
> thanks

The VarCorr function can provide D.



From ripley at stats.ox.ac.uk  Sat Nov 13 16:10:18 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 13 Nov 2004 15:10:18 +0000 (GMT)
Subject: [R] determing the distribution of a sample data set etc..
In-Reply-To: <BAY2-F28zDpubxcbz370005054a@hotmail.com>
References: <BAY2-F28zDpubxcbz370005054a@hotmail.com>
Message-ID: <Pine.LNX.4.61.0411131459321.11699@gannet.stats>

On Sat, 13 Nov 2004, Ann Huxtable wrote:

> Hello,
>
> I have only recently started using R. I have two data samples that I want to 
> carry out some initial explorative data analysis to:
>
> i). Determine the distribution of the data
> ii). Determine whether both datasets are from the same distribution.
>
> I have managed to create unit probability histograms and created qqplots for 
> the data. I have attached one of the qqplots. It is clear that the data is

No plot made it to the list: see the posting guide for what attachments 
are allowed.

> not from a normal distribution (it forms a convex curve underneath the 
> straight line).  the nature of the curve suggest the data is from either 
> Chi-square or F distribution (if you think otherwise, I would appreciate your 
> help in correcting my analysis).
>
> The point of this mail however, is how do I use R to:
>
> 1). Test if the data is from another distribution (F, Ch-Square etc.. )
> 2). How can I check if the samples are drawn from the same distribution?

I would use qqplots for both purposes.  qqplot will plot one dataset 
against another: see its examples.  It will also plot against another 
distribution: continuing that example

qqplot(y, qt(ppoints(200), df=5))

You could also compare two samples via the ecdfs and the 
Kolmogorov-Smirnov test (examples in the MASS ch05.R script).  But formal 
testing is not much help unless you know what sort of differences are 
interesting _a priori_ -- you would need enormous samples to distinguish 
a t_5 from a t_4, for example.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Sat Nov 13 16:29:52 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 13 Nov 2004 16:29:52 +0100
Subject: [R] R on 64-bit Linux machine
In-Reply-To: <Pine.LNX.4.61.0411131442320.11699@gannet.stats>
References: <C698D707214E6F4AB39AB7096C3DE5A5751163@phost015.EVAFUNDS.intermedia.net>
	<41952A21.4060502@jhsph.edu>
	<Pine.LNX.4.61.0411122134120.30572@gannet.stats>
	<x2pt2i8zx4.fsf@biostat.ku.dk> <x24qjt962d.fsf@biostat.ku.dk>
	<Pine.LNX.4.61.0411131442320.11699@gannet.stats>
Message-ID: <x2zn1l7onj.fsf@biostat.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> > pd at linux:~/r-devel> echo 'set.seed(1);M<-matrix(rnorm(9e6),3e3);system.time(solve(M))' | BUILD-GOTO/bin/R -q --vanilla
> >> set.seed(1);M<-matrix(rnorm(9e6),3e3);system.time(solve(M))
> > [1] 29.12  1.39 32.21  0.00  0.00
> >>
> > pd at linux:~/r-devel> echo 'set.seed(1);M<-matrix(rnorm(9e6),3e3);system.time(solve(M))' | BUILD-ATLAS/bin/R -q --vanilla
> >> set.seed(1);M<-matrix(rnorm(9e6),3e3);system.time(solve(M))
> > [1]  3.24  1.31 21.45 31.75  0.24
> >>
> >
> > So ATLAS is faster than GOTO by about 10 seconds. It is a bit odd that
> 
> Not on total CPU time (it's slower by about the margin I would
> expect), only on elapsed time.

True, but there's always a penalty on multithreading nontrivial code,
so to minimize total time, use only one CPU...
 
> > the GOTO timings don't seem to include any subprocess time but it
> > should be the threaded library libgoto_opt64p-r0.93.so (I know;
> > there's a 0.96 now, will upgrade).
> 
> I get (on a dual Opteron 248 with 0.96-2)
> 
> [1] 20.59  1.01 19.10  0.00  0.00
> 
> which note is using more than 100% CPU time.  Are you sure you are using
> multiple threads with Goto?

Fairly sure... I got (dual Opteron 240, now also 0.96-2)

[1] 29.21  1.50 30.97  0.00  0.00

so less than 100% but the timing ratio seems fairly consistent with
the clock speeds (1.4 GHz vs. 2.2 GHz).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From alexandrepatrot at yahoo.com.br  Sat Nov 13 18:15:50 2004
From: alexandrepatrot at yahoo.com.br (=?iso-8859-1?q?Alexandre=20Galv=E3o=20Patriota?=)
Date: Sat, 13 Nov 2004 14:15:50 -0300 (ART)
Subject: [R] Variance and Covariance Matrix D and R in nlme or lme4 part II
	[Please]
In-Reply-To: <41962147.5060009@stat.wisc.edu>
Message-ID: <20041113171550.93876.qmail@web52705.mail.yahoo.com>

Hi Douglas, I need to find the matrix V=ZDZ'+R, for
example:

require(nlme)
data(Orthodont)
attach(Orthodont)
fm1 <- lme(distance ~ age + Sex, data =
Orthodont,random=~age)
X<-model.matrix(distance ~ age + Sex)
Z<-model.matrix(distance ~ age + Subject -1)
D<-diag(ncol(Z))
cova<-VarCorr(fm1,rdig=7)
D[,1]<-D[1,]<-as.numeric(cova[2,3])*as.numeric(cova[1,2])*as.numeric(cova[2,2])
diag(D)<-c(as.numeric(cova[1,1]),rep(as.numeric(cova[2,1]),
27))
se2<-summary(fm1)$sigma^2
V<-Z%*%D%*%t(Z) + se2*diag(108)

Is It right?? I compared a function vcov(fm1) with
solve(t(X)%*%solve(V)%*%X). Could you give me a hint
on why the two functions are giving incompatible
results?

thank you in advance for your help


 --- Douglas Bates <bates at stat.wisc.edu> escreveu: 
> Alexandre Galv??o Patriota wrote:
> > The model is Y = XB + Zg + e
> > 
> > where
> > 
> > g~N(0, D)
> > 
> > e~N(0, R)
> > 
> > How to extract the VAR(g)= D, VAR(e)=R and
> V=ZDZ'+R?
> > 
> > thanks
> 
> The VarCorr function can provide D.
> 
>



From bates at stat.wisc.edu  Sat Nov 13 21:33:39 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 13 Nov 2004 14:33:39 -0600
Subject: [R] Variance and Covariance Matrix D and R in nlme or lme4 part
	II [Please]
In-Reply-To: <20041113171550.93876.qmail@web52705.mail.yahoo.com>
References: <20041113171550.93876.qmail@web52705.mail.yahoo.com>
Message-ID: <41966FA3.7020704@stat.wisc.edu>

Alexandre Galv??o Patriota wrote:
> Hi Douglas, I need to find the matrix V=ZDZ'+R, for
> example:
> 
> require(nlme)
> data(Orthodont)
> attach(Orthodont)
> fm1 <- lme(distance ~ age + Sex, data =
> Orthodont,random=~age)
> X<-model.matrix(distance ~ age + Sex)
> Z<-model.matrix(distance ~ age + Subject -1)
> D<-diag(ncol(Z))
> cova<-VarCorr(fm1,rdig=7)
> D[,1]<-D[1,]<-as.numeric(cova[2,3])*as.numeric(cova[1,2])*as.numeric(cova[2,2])
> diag(D)<-c(as.numeric(cova[1,1]),rep(as.numeric(cova[2,1]),
> 27))
> se2<-summary(fm1)$sigma^2
> V<-Z%*%D%*%t(Z) + se2*diag(108)
> 
> Is It right?? I compared a function vcov(fm1) with
> solve(t(X)%*%solve(V)%*%X). Could you give me a hint
> on why the two functions are giving incompatible
> results?

I don't know.

I wouldn't calculate the variance-covariance matrix for the parameter 
estimates that way.



From ligges at statistik.uni-dortmund.de  Sat Nov 13 21:53:27 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 13 Nov 2004 21:53:27 +0100
Subject: [R] density estimation: compute sum(value * probability) for
	given	distribution
In-Reply-To: <20041113035310.89960.qmail@web50301.mail.yahoo.com>
References: <20041113035310.89960.qmail@web50301.mail.yahoo.com>
Message-ID: <41967447.30303@statistik.uni-dortmund.de>

bogdan romocea wrote:
> Dear R users,
> 
> This is a KDE beginner's question. 
> I have this distribution:
> 
>>length(cap)
> 
> [1] 200
> 
>>summary(cap)
> 
>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
>   459.9   802.3   991.6  1066.0  1242.0  2382.0 
> I need to compute the sum of the values times their probability of
> occurence.
> 
> The graph is fine,
> den <- density(cap, from=min(cap), 
>        to=max(cap), give.Rkern=F)
> plot(den)
> 
> However, how do I compute sum(values*probabilities)? 

I don't get the point. You are estimating using a gaussian kernel.
Hint: What's the probability to get x=0 for a N(0,1) distribution?
So sum(values*probabilities) is zero!

 > The
> probabilities produced by the density function sum to only 26%: 

and could also sum to, e.g., 783453.9, depending on the number of 
observations and the estimated parameters of the desnity ...

>>sum(den$y)
> 
> [1] 0.2611142
> 
> Would it perhaps be ok to simply do
> 
>>sum(den$x*den$y) * (1/sum(den$y))
> 
> [1] 1073.22
> ?

No. den$x is a point where the density function is equal to den$y, but 
den$y is not the probability to get den$x (you know, the stuff with 
intervals)! I fear you are mixing theory from discrete with continuous 
distributions.

Uwe Ligges



> Thank you,
> b.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From rob at fatkat.com  Sat Nov 13 22:09:26 2004
From: rob at fatkat.com (Rob Steele)
Date: Sat, 13 Nov 2004 16:09:26 -0500
Subject: [R] Thrashing?
Message-ID: <41967806.8030407@fatkat.com>

Does R do its own swapping out to disk?  I disabled Linux swapping and 
the system still gets stuck in Purgatory where there's little CPU 
activity but the disk goes like crazy.  That's with R having almost the 
whole machine to itself and running a memory hungry compute only function.

I've seen this behavior with other version numbers but I'm running R 
2.0.0 under Fedora Core 3.  The system is a laptop with 1 Gig. RAM.

Thanks!



From ligges at statistik.uni-dortmund.de  Sat Nov 13 22:10:19 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 13 Nov 2004 22:10:19 +0100
Subject: [R] dyn.load problem
In-Reply-To: <001401c4c8b3$81f477c0$855a62c8@otneto>
References: <001401c4c8b3$81f477c0$855a62c8@otneto>
Message-ID: <4196783B.4030005@statistik.uni-dortmund.de>

O. Neto wrote:

> Hi R-Users
> 
>     I wrote 1 week ago asking about a message that appears when I try run
> dyn.load.
>    I'm trying to do an example in C code from "Writing R Extension"  to
> learn how to do it.
>     I have  R 2.0.0, Rtools, Perl and MinGW as describe  in
> http://www.murdoch-sutherland.com/Rtools/ with path sets.

Really, *really* sure you have followed all of readme.packages?
Works for me.

>  When I use C:\R\rw2000\bin>RCMD SHLIB -o C:/dev-cpp/teste.dll
> C:/dev-cpp/conv.c

What is the output?


> a teste.dll is created without error and located in that directory, but when
> I use it in RGui with "Change dir" set to C:/dev-cpp :
  >

> 
>>dyn.load("teste.dll")  results:
> 
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>         unable to load shared library "C:/Dev-Cpp/teste":
>   LoadLibrary failure:  Par??metro incorreto. (Incorrect Parameter)
> 
> My C code is (extracted form Writing R Extension):
> 
> #include <R.h>
>      #include <Rinternals.h>
>      SEXP convolve2(SEXP a, SEXP b)
>      {
>        R_len_t i, j, na, nb, nab;
>        double *xa, *xb, *xab;
>        SEXP ab;
> 
>        PROTECT(a = coerceVector(a, REALSXP));
>        PROTECT(b = coerceVector(b, REALSXP));
>        na = length(a); nb = length(b); nab = na + nb - 1;
>        PROTECT(ab = allocVector(REALSXP, nab));
>        xa = REAL(a); xb = REAL(b);
>        xab = REAL(ab);
>        for(i = 0; i < nab; i++) xab[i] = 0.0;
>        for(i = 0; i < na; i++)
>          for(j = 0; j < nb; j++) xab[i + j] += xa[i] * xb[j];
>        UNPROTECT(3);
>        return(ab);
>      }
> 
>     I??m using RCMD under Windows ME.
> 
>    How can I fix this? Someone can help me ? Prof. Ripley told me to catch
> more information debbuging this DLL. Is it hard to do this one under
> Windows? It??s necessary modify  files SHLIB or MkDLL?

Not that easy as under Linux, at least.


Uwe Ligges

> 
> Thanks
> 
> O. Neto
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Sat Nov 13 22:24:19 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 13 Nov 2004 22:24:19 +0100
Subject: [R] expressions and paste
In-Reply-To: <41949DDB.21124.941BB5@localhost>
References: <419393CE.31353.1A1A01C@localhost>
	<41949DDB.21124.941BB5@localhost>
Message-ID: <41967B83.6060704@statistik.uni-dortmund.de>

Emily Baldock wrote:

> I am trying to get a subscript into the title.
> My expression is quote(bold(apoA[bold("1")]))
> I have managed to get something near to what I want with
> 
> sd2 <-1.882
> measure <- "g/L"
> direction <- "increase"
> chem <- quote(bold(apoA[bold("1")]))
> plot(0,0)
>   titletxt <- substitute(paste("per ", sd2, " ",measure," ", direction, "\n in usual ",chem),
>    list(chem = chem,sd2=sd2,measure=measure,direction=direction))
>   title(main=titletxt,cex.main=1.4,font.main=2)


You don't need paste(), and so we get:

sd2 <-1.882
measure <- "g/L"
direction <- "increase"
plot(0,0)
  titletxt <- substitute(bold("per " * sd2 * measure * " " *
    direction * " in usual " * apoA["1"]),
    list(sd2 = as.character(sd2), measure = measure,
         direction = direction))
   title(main=titletxt,cex.main=1.4,font.main=2)

Uwe Ligges




> However, I can't get bold text this way.
> 
> Emily.
> 
> On 11 Nov 2004 at 12:58, Roger D. Peng wrote:
> 
> 
>>Can you be a bit more specific?  Exactly what kind of symbol are you 
>>trying to put in the title?
>>
>>-roger
>>
>>Emily Baldock wrote:
>>
>>>I have written a function to plot data which will be used for various different chemistries.
>>>A simplified version is:
>>>plot_data <- function(risk,levels,chem,sd2,measure){
>>>  plot(risk, levels,main=paste ("per", sd2, measure, "\n in usual", chem))
>>>}
>>>The problem is with the title.
>>>This works fine if the variable "chem" is just text, but if it is an expression then obviously it won't work.
>>>I have experimented with various things and I am at a complete loss for how to insert an expression into the middle of
>>>a title. If the expression was going in directly I would use main=expression(paste("text ", expression, " text")) but
>>>again this doesn't work. Can anyone help? thanks Emily.
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>>
>>
>>-- 
>>Roger D. Peng
>>http://www.biostat.jhsph.edu/~rpeng/
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Sat Nov 13 22:27:57 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 13 Nov 2004 22:27:57 +0100
Subject: [R] Setting plots margin
In-Reply-To: <001301c4c807$58b74ab0$83dad10a@prod.omsv.ch>
References: <001301c4c807$58b74ab0$83dad10a@prod.omsv.ch>
Message-ID: <41967C5D.5010602@statistik.uni-dortmund.de>

Anne Piotet wrote:

> I have a problem with plot for summary
> 
> s<-summary(response~ x1+x2+x3+x4+x5+x6+x7+
> + x8+ x9+x10)
> 
>>plot(s)
> 
> 
> Error in plot.new() : Figure margins too large
>   I tried to set the margins to null with par(mai=c(0,0,0,0))
> 
> but keep getting the same error message....
> 
> What is wrong?


Anne, at first, we can only see that your specification of the problem 
is obviously wrong:

response <- 1:10
x1 <- 1:10
x2 <- 1:10
summary(response ~ x1 + x2)
# Length   Class    Mode
#      3 formula    call
plot(summary(response ~ x1 + x2))
# Error in plot.table(summary(response ~ x1 + x2)) :
#         invalid table `x'


What do you expect?

Uwe Ligges



> 
> Thanks 
> Anne
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Sat Nov 13 22:35:35 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 13 Nov 2004 21:35:35 +0000 (GMT)
Subject: [R] Thrashing?
In-Reply-To: <41967806.8030407@fatkat.com>
References: <41967806.8030407@fatkat.com>
Message-ID: <Pine.LNX.4.61.0411132128440.18781@gannet.stats>

On Sat, 13 Nov 2004, Rob Steele wrote:

> Does R do its own swapping out to disk?  I disabled Linux swapping and the

No.

> system still gets stuck in Purgatory where there's little CPU activity but 
> the disk goes like crazy.  That's with R having almost the whole machine to 
> itself and running a memory hungry compute only function.
>
> I've seen this behavior with other version numbers but I'm running R 2.0.0 
> under Fedora Core 3.  The system is a laptop with 1 Gig. RAM.

I don't see it on machines with 1 or 2Gb of RAM and 3-10Gb of swap.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From otnetobr at yahoo.com.br  Sat Nov 13 23:28:08 2004
From: otnetobr at yahoo.com.br (O. Neto)
Date: Sat, 13 Nov 2004 20:28:08 -0200
Subject: [R] dyn.load problem
References: <001401c4c8b3$81f477c0$855a62c8@otneto>
	<4196783B.4030005@statistik.uni-dortmund.de>
Message-ID: <004101c4c9d0$0f9acbc0$666662c8@otneto>

Hi, Uwe.

    Thank you for help.

    Yesterday another R-user Mr. Ramasamy told me to do a simple program
such as (changing too the directories, now C:/minhadll/):

/*file conv.c*/

#include <R.h>
void printhello (){
  Rprintf("%s", "hello world\n");
}

    When I use C:\R\rw2000\bin>RCMD SHLIB -o c:/minhadll/meuteste.dll
c:/minhadll/conv.c
 to compile this one. Below is the output.

C:\R\rw2000\bin> RCMD SHLIB -o c:/minhadll/meuteste.dll c:/minhadll/conv.c
making c:/minhadll/conv.d from c:/minhadll/conv.c
gcc   -IC:/R/RW2000/include -Wall -O2   -c c:/minhadll/conv.c -o
c:/minhadll/conv.o
ar cr c:/minhadll/meuteste.a c:/minhadll/conv.o
ranlib c:/minhadll/meuteste.a
gcc  --shared -s  -o c:/minhadll/meuteste.dll c:/minhadll/meuteste.def
c:/minhadll/meuteste.a  -LC:/R/RW2000/src/gnuwin32  -lg2c -lR

    The result is " meuteste.dll"  located in C:/minhadll/meuteste.dll. Now,
talking about paths:
    I believe that I have make a correct way... but I'll list here to you:

SET PATH=C:\Rtools\;C:\Perl\bin\;C:\MinGW\bin;C:\R\rw2000\include\
        Is it correct?
Thank you again for help me. I'm trying to do a dll for 3 weeks or more,
without a good result.

PS: The same error dyn.load appears


O. Neto



From tfliao at uiuc.edu  Sun Nov 14 02:14:21 2004
From: tfliao at uiuc.edu (Tim F Liao)
Date: Sat, 13 Nov 2004 19:14:21 -0600
Subject: [R] Calling Other (non-C or Fortran)
 Programs from R
Message-ID: <a4d1eb9b.9edd4eb1.81c9400@expms6.cites.uiuc.edu>

Here's my report on the issue:

The command 'system' combined with either 'scan' 
or 'read.table' (depending on what I want to do with the 
output in each step) worked beautifully.

Cheers,

Tim Liao



From h_m_ at po.harenet.ne.jp  Sun Nov 14 06:18:03 2004
From: h_m_ at po.harenet.ne.jp (Hiroto Miyoshi)
Date: Sun, 14 Nov 2004 14:18:03 +0900
Subject: [R] Odd behaviour of R 2.00
Message-ID: <005801c4ca09$51a99080$0b01a8c0@hirotohome>

Dear R users

I have a data frame containing character and numeric variables, whose
name is seishin.  When I tried to assign NA to "" in the data frame, R, 2.00
showed an error message, such as

> seishin[seishin==""]<-NA
Error: NAs are not allowed in subscripted assignments

This did not happen under R 1.9.0.

More oddly,
The following commands work just fine under R 2.0.0
> a<-1:10
> b<-letters[1:10]
> b[3]<-""
> c<-data.frame(cbind(a,b))
> c[c==""]<-NA

Why is this so?
And how can I assign NA to "" data.framewise in seishin data.frame?
Please help.

Sincerely
--------------------------------
Hiroto Miyoshi
h_m_ at po.harenet.ne.jp



From ozric at web.de  Sun Nov 14 09:29:31 2004
From: ozric at web.de (Christian Schulz)
Date: Sun, 14 Nov 2004 09:29:31 +0100
Subject: [R] Thrashing?
In-Reply-To: <41967806.8030407@fatkat.com>
References: <41967806.8030407@fatkat.com>
Message-ID: <4197176B.2050907@web.de>

IMHO it's dependend on your data size and the functions you use.
I'm getting same behaviour (suse9) when i use mysqlWriteTable and 
reshape with large datasets and a
upgrade from 512MB to 1GB works much more better -  but now i dream from 
2GB-4GB.

christian

Rob Steele wrote:

> Does R do its own swapping out to disk?  I disabled Linux swapping and 
> the system still gets stuck in Purgatory where there's little CPU 
> activity but the disk goes like crazy.  That's with R having almost 
> the whole machine to itself and running a memory hungry compute only 
> function.
>
> I've seen this behavior with other version numbers but I'm running R 
> 2.0.0 under Fedora Core 3.  The system is a laptop with 1 Gig. RAM.
>
> Thanks!
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Ted.Harding at nessie.mcc.ac.uk  Sun Nov 14 09:50:53 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 14 Nov 2004 08:50:53 -0000 (GMT)
Subject: [R] density estimation: compute sum(value * probability) for
In-Reply-To: <20041113035310.89960.qmail@web50301.mail.yahoo.com>
Message-ID: <XFMail.041114085053.Ted.Harding@nessie.mcc.ac.uk>

On 13-Nov-04 bogdan romocea wrote:
> Dear R users,
> 
> However, how do I compute sum(values*probabilities)? The
> probabilities produced by the density function sum to only 26%: 
>> sum(den$y)
> [1] 0.2611142
> 
> Would it perhaps be ok to simply do
>> sum(den$x*den$y) * (1/sum(den$y))
> [1] 1073.22
> ?

What you're missing is the "dx"! A density estimation estimates
the probability density function g(x) such that int[g(x)*dx] = 1,
and R's 'density' function returns estimated values of "g" at a
discrete set of points.

An integral can be approximated by a discrete summation of the
form

    sum(g(x.i)*delta.x

You can recover the set of x-values at which the density is estimated,
and hence the implicit value of delta.x, from the returned density.

Example:

  X<-rnorm(1000)
  f<-density(X)
  x<-f$x
  delta.x<-x[2]-x[1]
  g<-f$y
  sum(g*delta.x)

  [1] 1.000976

Hoping this helps,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 14-Nov-04                                       Time: 08:50:53
------------------------------ XFMail ------------------------------



From p.dalgaard at biostat.ku.dk  Sun Nov 14 11:26:48 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 Nov 2004 11:26:48 +0100
Subject: [R] Odd behaviour of R 2.00
In-Reply-To: <005801c4ca09$51a99080$0b01a8c0@hirotohome>
References: <005801c4ca09$51a99080$0b01a8c0@hirotohome>
Message-ID: <x2r7mwg1zr.fsf@biostat.ku.dk>

"Hiroto Miyoshi" <h_m_ at po.harenet.ne.jp> writes:

> Dear R users
> 
> I have a data frame containing character and numeric variables, whose
> name is seishin.  When I tried to assign NA to "" in the data frame, R, 2.00
> showed an error message, such as
> 
> > seishin[seishin==""]<-NA
> Error: NAs are not allowed in subscripted assignments
> 
> This did not happen under R 1.9.0.
> 
> More oddly,
> The following commands work just fine under R 2.0.0
> > a<-1:10
> > b<-letters[1:10]
> > b[3]<-""
> > c<-data.frame(cbind(a,b))
> > c[c==""]<-NA
> 
> Why is this so?

It is the NA pattern on the left hand side that matters. Does it help
to use 

seishin[!is.na(seishin) & seishin==""]<-NA

?

(For atomic vectors you could also use %in% instead of ==, but this
doesn't work with data frames.)

> And how can I assign NA to "" data.framewise in seishin data.frame?

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ligges at statistik.uni-dortmund.de  Sun Nov 14 12:14:44 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 14 Nov 2004 12:14:44 +0100
Subject: [R] dyn.load problem
In-Reply-To: <004101c4c9d0$0f9acbc0$666662c8@otneto>
References: <001401c4c8b3$81f477c0$855a62c8@otneto>
	<4196783B.4030005@statistik.uni-dortmund.de>
	<004101c4c9d0$0f9acbc0$666662c8@otneto>
Message-ID: <41973E24.6030104@statistik.uni-dortmund.de>

O. Neto wrote:

> Hi, Uwe.
> 
>     Thank you for help.
> 
>     Yesterday another R-user Mr. Ramasamy told me to do a simple program
> such as (changing too the directories, now C:/minhadll/):
> 
> /*file conv.c*/
> 
> #include <R.h>
> void printhello (){
>   Rprintf("%s", "hello world\n");
> }
> 
>     When I use C:\R\rw2000\bin>RCMD SHLIB -o c:/minhadll/meuteste.dll
> c:/minhadll/conv.c
>  to compile this one. Below is the output.
> 
> C:\R\rw2000\bin> RCMD SHLIB -o c:/minhadll/meuteste.dll c:/minhadll/conv.c
> making c:/minhadll/conv.d from c:/minhadll/conv.c
> gcc   -IC:/R/RW2000/include -Wall -O2   -c c:/minhadll/conv.c -o
> c:/minhadll/conv.o
> ar cr c:/minhadll/meuteste.a c:/minhadll/conv.o
> ranlib c:/minhadll/meuteste.a
> gcc  --shared -s  -o c:/minhadll/meuteste.dll c:/minhadll/meuteste.def
> c:/minhadll/meuteste.a  -LC:/R/RW2000/src/gnuwin32  -lg2c -lR
> 
>     The result is " meuteste.dll"  located in C:/minhadll/meuteste.dll. Now,
> talking about paths:
>     I believe that I have make a correct way... but I'll list here to you:
> 
> SET PATH=C:\Rtools\;C:\Perl\bin\;C:\MinGW\bin;C:\R\rw2000\include\

I'd rather vote for
set PATH=.;C:\Rtools;C:\Perl\bin;C:\MinGW\bin;%PATH%

Anyway, this doesn't seem to be the culprit.
Do you have the recommended tools in their most recent versions?

The last point might be your OS. I think nobody of the developers is 
working on non-NT based Windows versions (such as 95, 98, ME) these days.


Uwe Ligges


>         Is it correct?
> Thank you again for help me. I'm trying to do a dll for 3 weeks or more,
> without a good result.
> 
> PS: The same error dyn.load appears
> 
> 
> O. Neto



From h_m_ at po.harenet.ne.jp  Sun Nov 14 12:39:31 2004
From: h_m_ at po.harenet.ne.jp (Hiroto Miyoshi)
Date: Sun, 14 Nov 2004 20:39:31 +0900
Subject: [R] Odd behaviour of R 2.00
References: <005801c4ca09$51a99080$0b01a8c0@hirotohome>
	<x2r7mwg1zr.fsf@biostat.ku.dk>
Message-ID: <001401c4ca3e$9b8cde20$0b01a8c0@hirotohome>

Dear  Professor Dalgaard

> It is the NA pattern on the left hand side that matters. Does it help
> to use
>
> seishin[!is.na(seishin) & seishin==""]<-NA
>
> ?

Yes!  the above line worked perfectly.
But why? To me, !is.na(seishin) & seishin=="" seems redundant.
If you could explain this, it would be greatly appreciated.
Thank you.
--------------------------------
Hiroto Miyoshi
h_m_ at po.harenet.ne.jp
----- Original Message ----- 
From: "Peter Dalgaard" <p.dalgaard at biostat.ku.dk>
To: "Hiroto Miyoshi" <h_m_ at po.harenet.ne.jp>
Cc: <r-help at stat.math.ethz.ch>
Sent: Sunday, November 14, 2004 7:26 PM
Subject: Re: [R] Odd behaviour of R 2.00


> "Hiroto Miyoshi" <h_m_ at po.harenet.ne.jp> writes:
>
> > Dear R users
> >
> > I have a data frame containing character and numeric variables, whose
> > name is seishin.  When I tried to assign NA to "" in the data frame, R,
2.00
> > showed an error message, such as
> >
> > > seishin[seishin==""]<-NA
> > Error: NAs are not allowed in subscripted assignments
> >
> > This did not happen under R 1.9.0.
> >
> > More oddly,
> > The following commands work just fine under R 2.0.0
> > > a<-1:10
> > > b<-letters[1:10]
> > > b[3]<-""
> > > c<-data.frame(cbind(a,b))
> > > c[c==""]<-NA
> >
> > Why is this so?
>
>
> (For atomic vectors you could also use %in% instead of ==, but this
> doesn't work with data frames.)
>
> > And how can I assign NA to "" data.framewise in seishin data.frame?
>
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>
>



From ripley at stats.ox.ac.uk  Sun Nov 14 13:27:48 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 14 Nov 2004 12:27:48 +0000 (GMT)
Subject: [R] Odd behaviour of R 2.00
In-Reply-To: <001401c4ca3e$9b8cde20$0b01a8c0@hirotohome>
References: <005801c4ca09$51a99080$0b01a8c0@hirotohome>
	<x2r7mwg1zr.fsf@biostat.ku.dk>
	<001401c4ca3e$9b8cde20$0b01a8c0@hirotohome>
Message-ID: <Pine.LNX.4.61.0411141213270.884@gannet.stats>

On Sun, 14 Nov 2004, Hiroto Miyoshi wrote:

> Dear  Professor Dalgaard
>
>> It is the NA pattern on the left hand side that matters. Does it help
>> to use
>>
>> seishin[!is.na(seishin) & seishin==""]<-NA
>>
>> ?
>
> Yes!  the above line worked perfectly.
> But why? To me, !is.na(seishin) & seishin=="" seems redundant.
> If you could explain this, it would be greatly appreciated.

Without it, your logical matrix index contains NA.  What do you intend 
that to do?  Do you replace the corresponding element or not?  You don't 
know, so perhaps you set it to NA, whatever the rhs?  And do you use up a 
value on the rhs or not (not relevant if as here you are recycling a 
single value, except that you need to know how many times to recycle it)?

Prior to 2.0.0, R behaved inconsistently (both within itself and with S) 
with NA indices in assignments, so now we force the user to say what he 
intended.  This has picked up quite a number of errors.

I do think this would be cleaner and faster using a loop over columns, 
especially as you probably have factors in the data frame.  Read the code 
of "[<-.data.frame" if you don't see that.


> Thank you.
> --------------------------------
> Hiroto Miyoshi
> h_m_ at po.harenet.ne.jp
> ----- Original Message -----
> From: "Peter Dalgaard" <p.dalgaard at biostat.ku.dk>
> To: "Hiroto Miyoshi" <h_m_ at po.harenet.ne.jp>
> Cc: <r-help at stat.math.ethz.ch>
> Sent: Sunday, November 14, 2004 7:26 PM
> Subject: Re: [R] Odd behaviour of R 2.00
>
>
>> "Hiroto Miyoshi" <h_m_ at po.harenet.ne.jp> writes:
>>
>>> Dear R users
>>>
>>> I have a data frame containing character and numeric variables, whose
>>> name is seishin.  When I tried to assign NA to "" in the data frame, R,
> 2.00
>>> showed an error message, such as
>>>
>>>> seishin[seishin==""]<-NA
>>> Error: NAs are not allowed in subscripted assignments
>>>
>>> This did not happen under R 1.9.0.
>>>
>>> More oddly,
>>> The following commands work just fine under R 2.0.0
>>>> a<-1:10
>>>> b<-letters[1:10]
>>>> b[3]<-""
>>>> c<-data.frame(cbind(a,b))
>>>> c[c==""]<-NA
>>>
>>> Why is this so?
>>
>>
>> (For atomic vectors you could also use %in% instead of ==, but this
>> doesn't work with data frames.)
>>
>>> And how can I assign NA to "" data.framewise in seishin data.frame?
>>
>> --
>>    O__  ---- Peter Dalgaard             Blegdamsvej 3
>>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N
>>  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
>> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Sun Nov 14 13:27:57 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 Nov 2004 13:27:57 +0100
Subject: [R] Odd behaviour of R 2.00
In-Reply-To: <001401c4ca3e$9b8cde20$0b01a8c0@hirotohome>
References: <005801c4ca09$51a99080$0b01a8c0@hirotohome>
	<x2r7mwg1zr.fsf@biostat.ku.dk>
	<001401c4ca3e$9b8cde20$0b01a8c0@hirotohome>
Message-ID: <x2lld4fwdu.fsf@biostat.ku.dk>

"Hiroto Miyoshi" <h_m_ at po.harenet.ne.jp> writes:

> Dear  Professor Dalgaard
> 
> > It is the NA pattern on the left hand side that matters. Does it help
> > to use
> >
> > seishin[!is.na(seishin) & seishin==""]<-NA
> >
> > ?
> 
> Yes!  the above line worked perfectly.
> But why? To me, !is.na(seishin) & seishin=="" seems redundant.
> If you could explain this, it would be greatly appreciated.
> Thank you.

It's due to this change (do check the NEWS file when things change
unexpectedly...):
 
    o   Subassignments involving NAs and with a replacement value of
        length > 1 are now disallowed.  (They were handled
        inconsistently in R < 2.0.0, see PR#7210.)  For data frames
        they are disallowed altogether, even for logical matrix indices
        (the only case which used to work).

Now, I'm getting slightly confused here, since there appears to be
exceptions:

> str(c)
`data.frame':   10 obs. of  2 variables:
 $ a: Factor w/ 10 levels "1","10","2","3",..: 1 3 4 5 6 NA 8 9 10 2
 $ b: Factor w/ 9 levels "","a","b","e",..: 2 3 1 NA 4 5 6 7 8 9
> c[c==""]<-NA
> str(c)
`data.frame':   10 obs. of  2 variables:
 $ a: Factor w/ 10 levels "1","10","2","3",..: 1 3 4 5 6 NA 8 9 10 2
 $ b: Factor w/ 9 levels "","a","b","e",..: 2 3 NA NA 4 5 6 7 8 9

> version
         _
platform i686-pc-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status
major    2
minor    0.0
year     2004
month    10
day      04
language R

Also:

> aq <- airquality
> aq[aq==5] <- 98765432
> aq[aq==97] <- 98765432
Error: NAs are not allowed in subscripted assignments

(difference being that 97 occurs in columns with NA's and 5 does not)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From h_m_ at po.harenet.ne.jp  Sun Nov 14 13:48:35 2004
From: h_m_ at po.harenet.ne.jp (Hiroto Miyoshi)
Date: Sun, 14 Nov 2004 21:48:35 +0900
Subject: [R] Odd behaviour of R 2.00
References: <005801c4ca09$51a99080$0b01a8c0@hirotohome>
	<x2r7mwg1zr.fsf@biostat.ku.dk>
	<001401c4ca3e$9b8cde20$0b01a8c0@hirotohome>
	<Pine.LNX.4.61.0411141213270.884@gannet.stats>
Message-ID: <003201c4ca48$41dee800$0b01a8c0@hirotohome>

Dear Professor Ripley and Professor Dalgaard

Thank you for your quick reply.
Now, I understand.
Thank you.

--------------------------------
Hiroto Miyoshi
h_m_ at po.harenet.ne.jp
----- Original Message ----- 
From: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
To: "Hiroto Miyoshi" <h_m_ at po.harenet.ne.jp>
Cc: "Peter Dalgaard" <p.dalgaard at biostat.ku.dk>; <r-help at stat.math.ethz.ch>
Sent: Sunday, November 14, 2004 9:27 PM
Subject: Re: [R] Odd behaviour of R 2.00


> On Sun, 14 Nov 2004, Hiroto Miyoshi wrote:
>
> > Dear  Professor Dalgaard
> >
> >> It is the NA pattern on the left hand side that matters. Does it help
> >> to use
> >>
> >> seishin[!is.na(seishin) & seishin==""]<-NA
> >>
> >> ?
> >
> > Yes!  the above line worked perfectly.
> > But why? To me, !is.na(seishin) & seishin=="" seems redundant.
> > If you could explain this, it would be greatly appreciated.
>
> Without it, your logical matrix index contains NA.  What do you intend
> that to do?  Do you replace the corresponding element or not?  You don't
> know, so perhaps you set it to NA, whatever the rhs?  And do you use up a
> value on the rhs or not (not relevant if as here you are recycling a
> single value, except that you need to know how many times to recycle it)?
>
> Prior to 2.0.0, R behaved inconsistently (both within itself and with S)
> with NA indices in assignments, so now we force the user to say what he
> intended.  This has picked up quite a number of errors.
>
> I do think this would be cleaner and faster using a loop over columns,
> especially as you probably have factors in the data frame.  Read the code
> of "[<-.data.frame" if you don't see that.
>
>
> > Thank you.
> > --------------------------------
> > Hiroto Miyoshi
> > h_m_ at po.harenet.ne.jp


--------------------------------
Hiroto Miyoshi
h_m_ at po.harenet.ne.jp
----- Original Message ----- 
From: "Peter Dalgaard" <p.dalgaard at biostat.ku.dk>
To: "Hiroto Miyoshi" <h_m_ at po.harenet.ne.jp>
Cc: "Peter Dalgaard" <p.dalgaard at biostat.ku.dk>;
<r-help at hypatia.math.ethz.ch>
Sent: Sunday, November 14, 2004 9:27 PM
Subject: Re: [R] Odd behaviour of R 2.00


> "Hiroto Miyoshi" <h_m_ at po.harenet.ne.jp> writes:
>
> > Dear  Professor Dalgaard
> >
> > > It is the NA pattern on the left hand side that matters. Does it help
> > > to use
> > >
> > > seishin[!is.na(seishin) & seishin==""]<-NA
> > >
> > > ?
> >
> > Yes!  the above line worked perfectly.
> > But why? To me, !is.na(seishin) & seishin=="" seems redundant.
> > If you could explain this, it would be greatly appreciated.
> > Thank you.
>
> It's due to this change (do check the NEWS file when things change
> unexpectedly...):
>
>     o   Subassignments involving NAs and with a replacement value of
>         length > 1 are now disallowed.  (They were handled
>         inconsistently in R < 2.0.0, see PR#7210.)  For data frames
>         they are disallowed altogether, even for logical matrix indices
>         (the only case which used to work).
>
> Now, I'm getting slightly confused here, since there appears to be
> exceptions:
>
> > str(c)
> `data.frame':   10 obs. of  2 variables:
>  $ a: Factor w/ 10 levels "1","10","2","3",..: 1 3 4 5 6 NA 8 9 10 2
>  $ b: Factor w/ 9 levels "","a","b","e",..: 2 3 1 NA 4 5 6 7 8 9
> > c[c==""]<-NA
> > str(c)
> `data.frame':   10 obs. of  2 variables:
>  $ a: Factor w/ 10 levels "1","10","2","3",..: 1 3 4 5 6 NA 8 9 10 2
>  $ b: Factor w/ 9 levels "","a","b","e",..: 2 3 NA NA 4 5 6 7 8 9
>
> > version
>          _
> platform i686-pc-linux-gnu
> arch     i686
> os       linux-gnu
> system   i686, linux-gnu
> status
> major    2
> minor    0.0
> year     2004
> month    10
> day      04
> language R
>
> Also:
>
> > aq <- airquality
> > aq[aq==5] <- 98765432
> > aq[aq==97] <- 98765432
> Error: NAs are not allowed in subscripted assignments
>
> (difference being that 97 occurs in columns with NA's and 5 does not)
>
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907
>
>


> > ----- Original Message -----
> > From: "Peter Dalgaard" <p.dalgaard at biostat.ku.dk>
> > To: "Hiroto Miyoshi" <h_m_ at po.harenet.ne.jp>
> > Cc: <r-help at stat.math.ethz.ch>
> > Sent: Sunday, November 14, 2004 7:26 PM
> > Subject: Re: [R] Odd behaviour of R 2.00
> >
> >
> >> "Hiroto Miyoshi" <h_m_ at po.harenet.ne.jp> writes:
> >>
> >>> Dear R users
> >>>
> >>> I have a data frame containing character and numeric variables, whose
> >>> name is seishin.  When I tried to assign NA to "" in the data frame,
R,
> > 2.00
> >>> showed an error message, such as
> >>>
> >>>> seishin[seishin==""]<-NA
> >>> Error: NAs are not allowed in subscripted assignments
> >>>
> >>> This did not happen under R 1.9.0.
> >>>
> >>> More oddly,
> >>> The following commands work just fine under R 2.0.0
> >>>> a<-1:10
> >>>> b<-letters[1:10]
> >>>> b[3]<-""
> >>>> c<-data.frame(cbind(a,b))
> >>>> c[c==""]<-NA
> >>>
> >>> Why is this so?
> >>
> >>
> >> (For atomic vectors you could also use %in% instead of ==, but this
> >> doesn't work with data frames.)
> >>
> >>> And how can I assign NA to "" data.framewise in seishin data.frame?
> >>
> >> --
> >>    O__  ---- Peter Dalgaard             Blegdamsvej 3
> >>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N
> >>  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
> >> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://stat.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >>
> >>
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
> >
> >
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>



From dataanalytics at rediffmail.com  Sun Nov 14 13:48:38 2004
From: dataanalytics at rediffmail.com (Arin Basu)
Date: 14 Nov 2004 12:48:38 -0000
Subject: [R] Problem updating the package "foreign"
Message-ID: <20041114124838.23251.qmail@webmail17.rediffmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041114/a0bc7ac5/attachment.pl

From SMH318 at cnrinternational.com  Sun Nov 14 14:07:05 2004
From: SMH318 at cnrinternational.com (SMH318@cnrinternational.com)
Date: Sun, 14 Nov 2004 13:07:05 +0000 (GMT)
Subject: [R] Inbound Virus Alert
Message-ID: <200411141307.iAED7Bau004189@hypatia.math.ethz.ch>

A virus has been detected in a message addressed to helpdesk.aberdeen at cnrinternational.com
 from r-help at lists.r-project.org.  

The subject of the message is Re: Here.

The virus could not be cleaned and the message is held in Dirty Messages

Message from: r-help at lists.r-project.org.
Message subject: Re: Here
Sent to: helpdesk.aberdeen at cnrinternational.com

Dated: Sun, 14 Nov 2004 13:16:48 +0000

Detection Summary:

Scenarios/Incoming/Incoming Virus Scanning: Information 0x42060008, W32/Netsky-D
Scenarios/Incoming/Block Executables: 'ItemLength.GE.0'.



From ripley at stats.ox.ac.uk  Sun Nov 14 14:13:01 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 14 Nov 2004 13:13:01 +0000 (GMT)
Subject: [R] Problem updating the package "foreign"
In-Reply-To: <20041114124838.23251.qmail@webmail17.rediffmail.com>
References: <20041114124838.23251.qmail@webmail17.rediffmail.com>
Message-ID: <Pine.LNX.4.61.0411141307210.1276@gannet.stats>

On Sun, 14 Nov 2004, Arin Basu wrote:

> Hi Group:
>
>> From an R session, I wanted to update packages, and issued the command:
> update.packages()

> R started to update the package "foreign" and proceeded. After 
> downloading, while it attempted to install the packages, it came up with 
> the following error message. I could not understand the error message. 
> Does it indicate that the problem lies with the gcc? How do I install 
> the current version of "foreign"?
>
> My operating system is MepisLinux, based on Debian, and the computer is 
> a compaq presario laptop with dual booting with Win XP(the linux 
> partition has 10 GB space, with 128 MB RAM). I have appended the R 
> message in this mail. I could not locate the config.log file.

You haven't told us the version of R, though, or how you installed it (do 
read the posting guide, as we ask).

To find the config.log you need to download and unpack the foreign 
tarball, then use R CMD INSTALL on the unpacked directory, which is where 
config.log will be put.

I suggest you wait until tomorrow and install R-2.0.1 (released then) from 
the sources.

> Would greatly appreciate  your advice in solving the problem.
>
> /Arin Basu
>
> <---Beginning of R command and error message--->
>> update.packages()
> trying URL `http://cran.r-project.org/src/contrib/PACKAGES'
> Content type `text/plain; charset=iso-8859-1' length 212630 bytes
> opened URL
> ==================================================
> downloaded 207Kb
>
> Update (y/N)?  y
> trying URL `http://cran.r-project.org/src/contrib/foreign_0.8-0.tar.gz'
> Content type `application/x-tar' length 259351 bytes
> opened URL
> ==================================================
> downloaded 253Kb
>
> * Installing *source* package 'foreign' ...
> checking for gcc... gcc
> checking for C compiler default output file name... configure: error: C compiler cannot create executables
> See `config.log' for more details.
> ERROR: configuration failed for package 'foreign'
> <---end of R command and error message--->

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From laura at env.leeds.ac.uk  Sun Nov 14 14:44:53 2004
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Sun, 14 Nov 2004 13:44:53 +0000 (GMT)
Subject: [R] Exporting to file: passing source name to file name in loop
Message-ID: <Pine.LNX.4.44.0411141322540.926-100000@gw.env.leeds.ac.uk>

Hi,

I'm having a mental block as to how I can automatically assign filenames
to the output of the following code. I am wishing to create a separate
.png file for every image created, each of them having a sequential
filename ie "sourcefile_index.png" so that I can create a movie from
them.

Please could someone tell me where I am going wrong?

the following code works fine and outputs to screen:

fun_pca_vector_movie_plot<-function(x,y,z,t){
zz=seq(1,(nrow(x)-t),by=t);jj=seq(t+1,(nrow(x)),by=t)
for(i in seq(along=zz)){
         pca<-prcomp(x[zz[i]:jj[i],], retx=T, center=T,scale=T)
        {
     par(mfrow=c(1,z))
      for(i in 1:z){
       image(east,north,t(map.matrix),col=my.colors,axes=T,
           xlab="",ylab="")
       text(y[,3],y[,2],labels=as.character(y[,1]))
       title(paste("Component",i,"Step:"))
        arrows(y[,3],y[,2],(y[,3]+50*(pca$rotation[i,1:(ncol(x)/2)])),
        (y[,2]+50*(pca$rotation[i,((ncol(x)/2)+1):(ncol(x))])),
        angle=30,length=0.05,code=2)
      }
     box()
    }
   }}

but when I try to save to file as follows it doesn't work:

fun_pca_vector_movie_plot<-function(x,y,z,t){
zz=seq(1,(nrow(x)-t),by=t);jj=seq(t+1,(nrow(x)),by=t)
for(i in seq(along=zz)){
      pca<-prcomp(x[zz[i]:jj[i],], retx=T, center=T,scale=T)
    {
     par(mfrow=c(1,z))
       for(i in 1:z){
       plot.new()
       png(file=(paste(x".",i,"_",zz,".png",sep="")),width=240,height=240)
       image(east,north,t(map.matrix),col=my.colors,axes=T,
           xlab="",ylab="")
       text(y[,3],y[,2],labels=as.character(y[,1]))
       title(paste("Component",i,"Step:"))
        arrows(y[,3],y[,2],(y[,3]+50*(pca$rotation[i,1:(ncol(x)/2)])),
        (y[,2]+50*(pca$rotation[i,((ncol(x)/2)+1):(ncol(x))])),
        angle=30,length=0.05,code=2)
       dev.off()
      }
     box()
    }
   }}

many thanks in advance!
Laura Quinn
Institute of Atmospheric Science
School of Earth and Environment
University of Leeds
Leeds
LS2 9JT

tel: +44 113 343 1596
fax: +44 113 343 6716
mail: laura at env.leeds.ac.uk



From ripley at stats.ox.ac.uk  Sun Nov 14 15:00:54 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 14 Nov 2004 14:00:54 +0000 (GMT)
Subject: [R] Exporting to file: passing source name to file name in loop
In-Reply-To: <Pine.LNX.4.44.0411141322540.926-100000@gw.env.leeds.ac.uk>
References: <Pine.LNX.4.44.0411141322540.926-100000@gw.env.leeds.ac.uk>
Message-ID: <Pine.LNX.4.61.0411141357550.13617@gannet.stats>

What does `it doesn't work' actually mean?  Please read the posting guide
and follow its advice.

Also, please show us readable and properly indented code, using spaces 
consistently.  (There is a chapter in `Writing R Extensions' showing you 
how to do this.)  I just cannot parse your code, and suspect you cannot 
either.

On Sun, 14 Nov 2004, Laura Quinn wrote:

> Hi,
>
> I'm having a mental block as to how I can automatically assign filenames
> to the output of the following code. I am wishing to create a separate
> .png file for every image created, each of them having a sequential
> filename ie "sourcefile_index.png" so that I can create a movie from
> them.
>
> Please could someone tell me where I am going wrong?
>
> the following code works fine and outputs to screen:
>
> fun_pca_vector_movie_plot<-function(x,y,z,t){
> zz=seq(1,(nrow(x)-t),by=t);jj=seq(t+1,(nrow(x)),by=t)
> for(i in seq(along=zz)){
>         pca<-prcomp(x[zz[i]:jj[i],], retx=T, center=T,scale=T)
>        {
>     par(mfrow=c(1,z))
>      for(i in 1:z){
>       image(east,north,t(map.matrix),col=my.colors,axes=T,
>           xlab="",ylab="")
>       text(y[,3],y[,2],labels=as.character(y[,1]))
>       title(paste("Component",i,"Step:"))
>        arrows(y[,3],y[,2],(y[,3]+50*(pca$rotation[i,1:(ncol(x)/2)])),
>        (y[,2]+50*(pca$rotation[i,((ncol(x)/2)+1):(ncol(x))])),
>        angle=30,length=0.05,code=2)
>      }
>     box()
>    }
>   }}
>
> but when I try to save to file as follows it doesn't work:
>
> fun_pca_vector_movie_plot<-function(x,y,z,t){
> zz=seq(1,(nrow(x)-t),by=t);jj=seq(t+1,(nrow(x)),by=t)
> for(i in seq(along=zz)){
>      pca<-prcomp(x[zz[i]:jj[i],], retx=T, center=T,scale=T)
>    {
>     par(mfrow=c(1,z))
>       for(i in 1:z){
>       plot.new()
>       png(file=(paste(x".",i,"_",zz,".png",sep="")),width=240,height=240)
>       image(east,north,t(map.matrix),col=my.colors,axes=T,
>           xlab="",ylab="")
>       text(y[,3],y[,2],labels=as.character(y[,1]))
>       title(paste("Component",i,"Step:"))
>        arrows(y[,3],y[,2],(y[,3]+50*(pca$rotation[i,1:(ncol(x)/2)])),
>        (y[,2]+50*(pca$rotation[i,((ncol(x)/2)+1):(ncol(x))])),
>        angle=30,length=0.05,code=2)
>       dev.off()
>      }
>     box()
>    }
>   }}
>
> many thanks in advance!
> Laura Quinn
> Institute of Atmospheric Science
> School of Earth and Environment
> University of Leeds
> Leeds
> LS2 9JT
>
> tel: +44 113 343 1596
> fax: +44 113 343 6716
> mail: laura at env.leeds.ac.uk
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From raheem at gmail.com  Sun Nov 14 15:45:51 2004
From: raheem at gmail.com (Enayetur RAHEEM)
Date: Sun, 14 Nov 2004 09:45:51 -0500
Subject: [R] solving system of nonlinear equations
Message-ID: <8290920504111406452ab79603@mail.gmail.com>

Hello there

Can anybody please tell me if there is any package in R to solve the
following 4 nonlinear equations with 4 unknowns:

alpha*exp(20/sigma)+ beta*exp(21/tau) = 2
alpha*exp(22/sigma)+ beta*exp(9/tau) = 4
alpha*exp(10/sigma)+ beta*exp(30/tau) = 6
alpha*exp(40/sigma)+ beta*exp(39/tau) = 5

where 

alpha = exp(lambda/sigma)
beta= exp(delta/tau)

I need to estimate lambda, sigma, delta, tau

Thanks.
E Raheem



From laura at env.leeds.ac.uk  Sun Nov 14 16:16:33 2004
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Sun, 14 Nov 2004 15:16:33 +0000 (GMT)
Subject: [R] Exporting to file: passing source name to file name in loop
In-Reply-To: <Pine.LNX.4.61.0411141357550.13617@gannet.stats>
Message-ID: <Pine.LNX.4.44.0411141504190.926-100000@gw.env.leeds.ac.uk>

Apologies - Sunday afternoon coding, not my forte.

I am trying to pass the name of my input variable "x" (my.data) into the
name of my output file, and am wanting to combine this with a "count"
value.

I hope that this code is a little more readable (though I seem to be
having problems pasting the formatting into my email composition):

fun_pca_vector_movie_plot <- function (x,y,z,t) {
  zz=seq(1,(nrow(x)-t),by=t);jj=seq(t+1,(nrow(x)),by=t)
    for(i in seq(along=zz)){
       plot.new()
	png(file=(paste(as.character(x),".",i,".png",sep="")),width=240,height=240)
        pca<-prcomp(x[zz[i]:jj[i],], retx=T, center=T,scale=T)
          {
           par(mfrow=c(1,z))
             for(i in 1:z){
                image(east,north,t(map.matrix),col=my.colors,axes=T,
                      xlab="",ylab="")
                text(y[,3],y[,2],labels=as.character(y[,1]))
                title(paste("Component",i,"Step:"))
              	arrows(y[,3],y[,2],(y[,3]+50*(pca$rotation[i,1:(ncol(x)/2)])),
                      (y[,2]+50*(pca$rotation[i,((ncol(x)/2)+1):(ncol(x))])),
                       angle=30,length=0.05,code=2)
            }
         box()
         }
       dev.off()
     }
   }

I get a "syntax error" relating to the as.character(x) part of the file
name - if I remove this, the code works fine for the rest of the file
extension. I have tried deparse(x) but this returns a file extension of
the following nature:

structure(list(f1 = c(5.56358661715647, 6.10364037003176,
6.24040147126807, .10.png

Thanks,
Laura

Laura Quinn
Institute of Atmospheric Science
School of Earth and Environment
University of Leeds
Leeds
LS2 9JT

tel: +44 113 343 1596
fax: +44 113 343 6716
mail: laura at env.leeds.ac.uk



From mentus at gmx.de  Sun Nov 14 17:20:35 2004
From: mentus at gmx.de (Fernando Henrique Ferraz P. da Rosa)
Date: Sun, 14 Nov 2004 14:20:35 -0200
Subject: [R] Exporting to file: passing source name to file name in loop
In-Reply-To: <Pine.LNX.4.44.0411141504190.926-100000@gw.env.leeds.ac.uk>
References: <Pine.LNX.4.61.0411141357550.13617@gannet.stats>
	<Pine.LNX.4.44.0411141504190.926-100000@gw.env.leeds.ac.uk>
Message-ID: <20041114162035.GA5722@ime.usp.br>

Laura Quinn writes:
> Apologies - Sunday afternoon coding, not my forte.
> 
> I am trying to pass the name of my input variable "x" (my.data) into the
> name of my output file, and am wanting to combine this with a "count"
> value.
> 
> I hope that this code is a little more readable (though I seem to be
> having problems pasting the formatting into my email composition):
> 
> fun_pca_vector_movie_plot <- function (x,y,z,t) {
>   zz=seq(1,(nrow(x)-t),by=t);jj=seq(t+1,(nrow(x)),by=t)
>     for(i in seq(along=zz)){
>        plot.new()
> 	png(file=(paste(as.character(x),".",i,".png",sep="")),width=240,height=240)
>         pca<-prcomp(x[zz[i]:jj[i],], retx=T, center=T,scale=T)
>           {
>            par(mfrow=c(1,z))
>              for(i in 1:z){
>                 image(east,north,t(map.matrix),col=my.colors,axes=T,
>                       xlab="",ylab="")
>                 text(y[,3],y[,2],labels=as.character(y[,1]))
>                 title(paste("Component",i,"Step:"))
>               	arrows(y[,3],y[,2],(y[,3]+50*(pca$rotation[i,1:(ncol(x)/2)])),
>                       (y[,2]+50*(pca$rotation[i,((ncol(x)/2)+1):(ncol(x))])),
>                        angle=30,length=0.05,code=2)
>             }
>          box()
>          }
>        dev.off()
>      }
>    }
> 
> I get a "syntax error" relating to the as.character(x) part of the file
> name - if I remove this, the code works fine for the rest of the file
> extension. I have tried deparse(x) but this returns a file extension of
> the following nature:
> 
> structure(list(f1 = c(5.56358661715647, 6.10364037003176,
> 6.24040147126807, .10.png

        You should try something like:

        png(file=paste(substitute(x),".",i,".png",sep=""),width=240,height=240)

        Also, drop the 'plot.new()', it's useless. png() already sets up
a new ploting device.


--
Fernando Henrique Ferraz P. da Rosa
http://www.ime.usp.br/~feferraz



From andreasbetz at earthlink.net  Sun Nov 14 21:35:13 2004
From: andreasbetz at earthlink.net (Andreas Betz)
Date: Sun, 14 Nov 2004 12:35:13 -0800
Subject: [R] excel/r interface
Message-ID: <410-2200411014203513470@earthlink.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041114/d5cf6a70/attachment.pl

From jahernan at umn.edu  Sun Nov 14 21:01:03 2004
From: jahernan at umn.edu (Jose A. Hernandez)
Date: Sun, 14 Nov 2004 14:01:03 -0600
Subject: [R] Combining expressions and objects within labels
Message-ID: <4197B97F.1010904@umn.edu>

Hello all,

I am an R novice and I have a simple question and hope somebody can help 
me out.

I need to place several labels in a plot, this labels are some kind of 
text and also some objects (which come from some more complicated R 
calculations).

In one of this labels I'd like to place a superscript, however I cannot 
find a way to place both the expression and the object in the same label.

Thanks in advance and best regards.

Please look at the example below:

# my objects
eonr <- 75
yldeonr <- 150
r_2 <- 0.95

# the plot and simple labels
plot(0:100, 0:100)
text(60,40, paste("EONR=",eonr))
text(60,36, paste("Yield at EONR=",yldeonr))

# I'd like to place the superscript on the r

text(60,32, paste("Pseudo r^2=",r_2))

# This does not seem to work ... I can get the subscript
# but now I cannot paste the r_2 object. Any ideas ?

text(60,28, expression(paste("Pseudo r" ^2, "=" r_2)))

-- 
Jose A. Hernandez
Department of Soil, Water, and Climate
University of Minnesota
1991 Upper Buford Circle
St. Paul, MN 55108

Ph. (612) 625-0445, Fax. (612) 625-2208



From ripley at stats.ox.ac.uk  Sun Nov 14 22:09:44 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 14 Nov 2004 21:09:44 +0000 (GMT)
Subject: [R] excel/r interface
In-Reply-To: <410-2200411014203513470@earthlink.net>
References: <410-2200411014203513470@earthlink.net>
Message-ID: <Pine.LNX.4.61.0411142107500.26232@gannet.stats>

On Sun, 14 Nov 2004, Andreas Betz wrote:

> I am quite new to R and for preofessional reasons I was interested in 
> the R/excel interface by Baier and Neuwirth. After setup I see the 
> Rexcel and the Rhelp on the Menu bar of Microsoft Excel XP. However, 
> after putting the formula =RApply("pchisqr", 30, 1) Excel returns the 
> message "could not start Rserver". Any suggestions how to fix this. How 
> do run this application in R.

Please ask such questions on their mailing list, not the R one.  See

 	http://cran.r-project.org/contrib/extra/dcom/RSrv135.html

for where.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.murrell at auckland.ac.nz  Sun Nov 14 22:31:16 2004
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Mon, 15 Nov 2004 10:31:16 +1300
Subject: [R] Combining expressions and objects within labels
References: <4197B97F.1010904@umn.edu>
Message-ID: <4197CEA4.70205@stat.auckland.ac.nz>

Hi


Jose A. Hernandez wrote:
> Hello all,
> 
> I am an R novice and I have a simple question and hope somebody can help 
> me out.
> 
> I need to place several labels in a plot, this labels are some kind of 
> text and also some objects (which come from some more complicated R 
> calculations).
> 
> In one of this labels I'd like to place a superscript, however I cannot 
> find a way to place both the expression and the object in the same label.
> 
> Thanks in advance and best regards.
> 
> Please look at the example below:
> 
> # my objects
> eonr <- 75
> yldeonr <- 150
> r_2 <- 0.95
> 
> # the plot and simple labels
> plot(0:100, 0:100)
> text(60,40, paste("EONR=",eonr))
> text(60,36, paste("Yield at EONR=",yldeonr))
> 
> # I'd like to place the superscript on the r
> 
> text(60,32, paste("Pseudo r^2=",r_2))
> 
> # This does not seem to work ... I can get the subscript
> # but now I cannot paste the r_2 object. Any ideas ?
> 
> text(60,28, expression(paste("Pseudo r" ^2, "=" r_2)))


text(60, 28, substitute(paste("Pseudo ", r^2 == r_2), list(r_2=r_2)))

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From xuhy at ucla.edu  Sun Nov 14 23:44:35 2004
From: xuhy at ucla.edu (Haiyong Xu)
Date: Sun, 14 Nov 2004 14:44:35 -0800
Subject: [R] question about the tick label.
Message-ID: <4197DFD3.2090503@ucla.edu>

Hello,

I am a beginner of R. I want to plot the vectors Y vs. X where X has 
continuous interger values. How can I use the name of each element as 
the tick label?

Thanks a lot!

Haiyong



From christoph.bier at web.de  Sun Nov 14 22:53:42 2004
From: christoph.bier at web.de (Christoph Bier)
Date: Sun, 14 Nov 2004 22:53:42 +0100
Subject: [R] Where has the Debian respository gone?
Message-ID: <cn8k7h$lin$1@sea.gmane.org>

Hi all!

Did I miss something or is it just a temporary problem? Where has
the Debian respository

http://cran.r-project.org woody/main Packages
resp.
http://cran.r-project.org/bin/linux/debian/

gone? I tried it for about the last 7 hours.

$ apt-get update
[...]
Err http://cran.r-project.org woody/main Packages

  404 Not Found
[...]
Failed to fetch
http://cran.r-project.org/bin/linux/debian/dists/woody/main/binary-i386/Packages
 404 Not Found

Greetings,
    Christoph



From andy_liaw at merck.com  Mon Nov 15 02:01:11 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sun, 14 Nov 2004 20:01:11 -0500
Subject: [R] question about the tick label.
Message-ID: <3A822319EB35174CA3714066D590DCD50994E2F8@usrymx25.merck.com>

Not sure if this is what you want, but give it a shot anyway:

plot(x, y, xaxt="n")
axis(1, at=x, label=names(x))

Andy

> From: Haiyong Xu
> 
> Hello,
> 
> I am a beginner of R. I want to plot the vectors Y vs. X where X has 
> continuous interger values. How can I use the name of each element as 
> the tick label?
> 
> Thanks a lot!
> 
> Haiyong
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From richars3 at lincoln.ac.nz  Mon Nov 15 02:08:10 2004
From: richars3 at lincoln.ac.nz (Sean David Richards)
Date: Mon, 15 Nov 2004 14:08:10 +1300
Subject: [R] Legend help needed
Message-ID: <4198B84A.27421.244A7C2A@tui.lincoln.ac.nz>

R : Version 1.9.1

Hi,

Am having trouble adding a legend to scatterplot. R code is shown below. 
I have tried various incantations to add a legend (using the legend() 
function) to the resulting plot but without any success. Looks like it 
should be simple but I must be missing something. Any pointers would be 
welcome.
Have looked at help(legend) etc.

--8<----------------------------------------------------------------------
---

sfiles <- c("72_12_12_V.csv ",
            "150_25_15_V.csv",    
            "150_25_20_V.csv",    
            "150_25_25_V.csv",    
            "150_25_40_V.csv",    
            "150_25_60_V.csv",    
            "150_25_90_V.csv",    
            "240_40_40_V.csv")                  
            
## process each file in list
for (i in 1:length(sfiles)) {
data <- read.csv(paste("../data/",sfiles[i],sep=""))

## assign columns to some nice names
K <- data[,8]
AN <- data[,3] * (data[,2] - data[,4])

## plot K against AN

if ( i == 1) {
    plot(AN, K, ylim=c(1000,9000), xlim=c(0,1500), 
          xlab="Area above Notch (mm)",
          main="Size Effect Specimens")
    par(new=TRUE)                        
    }
else{
    plot(AN,K, pch=(i),ylim=c(1000,9000), xlim=c(0,1500), 
          axes=FALSE,xlab="")
    par(new=TRUE)
    }
}

--8<----------------------------------------------------------------------
---

-- 
Sean Richards

C-fACS
P.O. Box 84, Lincoln University,
Canterbury, New Zealand
Phone:    (64)(3) 325-2811 ext 8636
Email:      richars3 at lincoln.ac.nz



From spencer.graves at pdf.com  Mon Nov 15 02:09:06 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 14 Nov 2004 17:09:06 -0800
Subject: [R] solving system of nonlinear equations
In-Reply-To: <8290920504111406452ab79603@mail.gmail.com>
References: <8290920504111406452ab79603@mail.gmail.com>
Message-ID: <419801B2.30409@pdf.com>

      Have you considered "nls"?  If you read the help file and work 
through the examples, there is a good chance you can make it work, I 
think.  I think I would start trying "plinear" in "nls", parameterizing 
the problem in terms of alpha, beta, ln.sigma, and ln.tau, unless you 
think a solution might require sigma < 0 or tau < 0.  Using logarithms 
will get rid of the constraint and may make the problem numerically 
easier.  Using alpha and beta rather than lambda and delta transforms 
the problem into an ordinary least squares problem for alpha and beta 
given any two numbers for sigma and tau (or ln.sigma and ln.tau).  

      If I had trouble with this, I might try two other things: 
     
      (a) The "solver" in Excel. 

      (b) I might generate a grid in ln.sigma and ln.tau using 
expand.grid.  For each combination of levels, I'd set up the linear 
regression problem and use "lm" to estimate alpha and beta and compute 
and store the sum of squares of residuals.  Then I'd use "contour" to 
visualize the sum of squares surface. 

      I've done all these things with crudely similar problems in the 
past and been happy with the results.  If I only had this one problem, 
I'd be surprised if it would require more than a few hours.  If I wanted 
a general algorithm for other purposes, I might do it two or three 
different ways both to help select a good algorithm and to build 
confidence in the results. 

      hope this helps. 
      spencer graves
p.s.  Some of these techniques are discussed in Venables and Ripley 
(2002) Modern Applied Statistics with S, 4th ed. (Springer).  If you 
don't have this, I'd encourage you to consider spending some time with it. 

Enayetur RAHEEM wrote:

>Hello there
>
>Can anybody please tell me if there is any package in R to solve the
>following 4 nonlinear equations with 4 unknowns:
>
>alpha*exp(20/sigma)+ beta*exp(21/tau) = 2
>alpha*exp(22/sigma)+ beta*exp(9/tau) = 4
>alpha*exp(10/sigma)+ beta*exp(30/tau) = 6
>alpha*exp(40/sigma)+ beta*exp(39/tau) = 5
>
>where 
>
>alpha = exp(lambda/sigma)
>beta= exp(delta/tau)
>
>I need to estimate lambda, sigma, delta, tau
>
>Thanks.
>E Raheem
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From HarrisDD at Cardiff.ac.uk  Mon Nov 15 01:19:08 2004
From: HarrisDD at Cardiff.ac.uk (Duncan Harris)
Date: Mon, 15 Nov 2004 01:19:08 GMT0BST
Subject: [R] Power curves
Message-ID: <4198040C.31196.D21C35C@localhost>

How do I draw/calculate power curves in R?

Cheers,

Duncan.



From edd at debian.org  Mon Nov 15 02:35:27 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 14 Nov 2004 19:35:27 -0600
Subject: [R] Where has the Debian respository gone?
In-Reply-To: <cn8k7h$lin$1@sea.gmane.org>
References: <cn8k7h$lin$1@sea.gmane.org>
Message-ID: <20041115013527.GA17007@sonny.eddelbuettel.com>

On Sun, Nov 14, 2004 at 10:53:42PM +0100, Christoph Bier wrote:
> Hi all!
> 
> Did I miss something or is it just a temporary problem? Where has
> the Debian respository
> 
> http://cran.r-project.org woody/main Packages
> resp.
> http://cran.r-project.org/bin/linux/debian/
> 
> gone? I tried it for about the last 7 hours.
[...]

It has been turned off by the CRAN masters as the content had slipped
further and further behind the Debian content.  

Current R and CRAN packages are on the Debian archives; you can install
these on testing too.  To the best of my knowledge, there are no backports
of current R and Debian CRAN packages to Debian stable. 

Hope this helps, Dirk

-- 
If your hair is standing up, then you are in extreme danger.
      -- http://www.usafa.af.mil/dfp/cockpit-phys/fp1ex3.htm



From edd at debian.org  Mon Nov 15 04:12:34 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 14 Nov 2004 21:12:34 -0600
Subject: [R] Where has the Debian respository gone?
In-Reply-To: <20041115013527.GA17007@sonny.eddelbuettel.com>
References: <cn8k7h$lin$1@sea.gmane.org>
	<20041115013527.GA17007@sonny.eddelbuettel.com>
Message-ID: <20041115031233.GA17861@sonny.eddelbuettel.com>

On Sun, Nov 14, 2004 at 07:35:27PM -0600, Dirk Eddelbuettel wrote:
> On Sun, Nov 14, 2004 at 10:53:42PM +0100, Christoph Bier wrote:
> > Hi all!
> > 
> > Did I miss something or is it just a temporary problem? Where has
> > the Debian respository
> > 
> > http://cran.r-project.org woody/main Packages
> > resp.
> > http://cran.r-project.org/bin/linux/debian/
> > 
> > gone? I tried it for about the last 7 hours.
> [...]
> 
> It has been turned off by the CRAN masters as the content had slipped
> further and further behind the Debian content.  
> 
> Current R and CRAN packages are on the Debian archives; you can install
> these on testing too.  To the best of my knowledge, there are no backports
> of current R and Debian CRAN packages to Debian stable. 

Upon re-reading this, I should clarify that in this context "CRAN packages"
refers to the several dozen CRAN packages that are in Debian; the list is
growing but still far from exhaustive.

Dirk

-- 
If your hair is standing up, then you are in extreme danger.
      -- http://www.usafa.af.mil/dfp/cockpit-phys/fp1ex3.htm



From jenny_edmondson at hotmail.com  Mon Nov 15 09:08:31 2004
From: jenny_edmondson at hotmail.com (Jenny Edmondson)
Date: Mon, 15 Nov 2004 08:08:31 +0000
Subject: [R] installing knncat package
Message-ID: <BAY1-F383riCTVuy16x0000f17e@hotmail.com>

Hi all,

I am wondering if anyone has problem installing 'knncat' package. I tried to 
install using  'Rcmd INSTALL knncat_1.1.1.tar.gz', error message appeared. I 
attached the message in the end. I allso tried to install from RGui, but it 
was not listed.

Thanks in advance.

Regards, jenny



---------- Making package knncat ------------
  adding build stamp to DESCRIPTION
  making DLL ...
making com.d from com.c
making dodisc.d from dodisc.c
making donn.d from donn.c
making donnwrap.d from donnwrap.c
making dsort.d from dsort.c
making linpack.d from linpack.c
making matrix.d from matrix.c
making objective.d from objective.c
making ords.d from ords.c
making ranlib.d from ranlib.c
making utilsR.d from utilsR.c
gcc   -Ic:/R/rw1081/src/include -Wall -O2 -I . -pedantic -Wall -DUSE_R_ALLOC 
-DC
ALL_FROM_R  -c com.c -o com.o
gcc   -Ic:/R/rw1081/src/include -Wall -O2 -I . -pedantic -Wall -DUSE_R_ALLOC 
-DC
ALL_FROM_R  -c dodisc.c -o dodisc.o
gcc   -Ic:/R/rw1081/src/include -Wall -O2 -I . -pedantic -Wall -DUSE_R_ALLOC 
-DC
ALL_FROM_R  -c donn.c -o donn.o
gcc   -Ic:/R/rw1081/src/include -Wall -O2 -I . -pedantic -Wall -DUSE_R_ALLOC 
-DC
ALL_FROM_R  -c donnwrap.c -o donnwrap.o
donnwrap.c: In function `donnwrap':
donnwrap.c:84: warning: passing arg 5 of `do_nn' from incompatible pointer 
type
donnwrap.c:84: warning: passing arg 9 of `do_nn' from incompatible pointer 
type
donnwrap.c:84: warning: passing arg 10 of `do_nn' from incompatible pointer 
type

donnwrap.c:84: warning: passing arg 16 of `do_nn' from incompatible pointer 
type

donnwrap.c:84: warning: passing arg 21 of `do_nn' from incompatible pointer 
type

donnwrap.c:84: warning: passing arg 22 of `do_nn' from incompatible pointer 
type

donnwrap.c:84: warning: passing arg 23 of `do_nn' from incompatible pointer 
type

donnwrap.c:94: warning: passing arg 5 of `do_nn' from incompatible pointer 
type
donnwrap.c:94: warning: passing arg 9 of `do_nn' from incompatible pointer 
type
donnwrap.c:94: warning: passing arg 10 of `do_nn' from incompatible pointer 
type

donnwrap.c:94: warning: passing arg 16 of `do_nn' from incompatible pointer 
type

donnwrap.c:94: warning: passing arg 21 of `do_nn' from incompatible pointer 
type

donnwrap.c:94: warning: passing arg 22 of `do_nn' from incompatible pointer 
type

donnwrap.c:94: warning: passing arg 23 of `do_nn' from incompatible pointer 
type

gcc   -Ic:/R/rw1081/src/include -Wall -O2 -I . -pedantic -Wall -DUSE_R_ALLOC 
-DC
ALL_FROM_R  -c dsort.c -o dsort.o
gcc   -Ic:/R/rw1081/src/include -Wall -O2 -I . -pedantic -Wall -DUSE_R_ALLOC 
-DC
ALL_FROM_R  -c linpack.c -o linpack.o
gcc   -Ic:/R/rw1081/src/include -Wall -O2 -I . -pedantic -Wall -DUSE_R_ALLOC 
-DC
ALL_FROM_R  -c matrix.c -o matrix.o
gcc   -Ic:/R/rw1081/src/include -Wall -O2 -I . -pedantic -Wall -DUSE_R_ALLOC 
-DC
ALL_FROM_R  -c objective.c -o objective.o
gcc   -Ic:/R/rw1081/src/include -Wall -O2 -I . -pedantic -Wall -DUSE_R_ALLOC 
-DC
ALL_FROM_R  -c ords.c -o ords.o
gcc   -Ic:/R/rw1081/src/include -Wall -O2 -I . -pedantic -Wall -DUSE_R_ALLOC 
-DC
ALL_FROM_R  -c ranlib.c -o ranlib.o
gcc   -Ic:/R/rw1081/src/include -Wall -O2 -I . -pedantic -Wall -DUSE_R_ALLOC 
-DC
ALL_FROM_R  -c utilsR.c -o utilsR.o
ar cr knncat.a *.o
ranlib knncat.a
windres --include-dir c:/R/rw1081/src/include  -i knncat_res.rc -o 
knncat_res.o
gcc  --shared -s  -o knncat.dll knncat.def knncat.a knncat_res.o  
-Lc:/R/rw1081/
src/gnuwin32 c:/R/rw1081/src/modules/lapack/libRlapack.a -lRblas -lg2c -lg2c 
-lR

gcc.exe: c:/R/rw1081/src/modules/lapack/libRlapack.a: No such file or 
directory
make[2]: *** [knncat.dll] Error 1
make[1]: *** [srcDynlib] Error 2
make: *** [pkg-knncat] Error 2
*** Installation of knncat failed ***



From ripley at stats.ox.ac.uk  Mon Nov 15 09:32:54 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 15 Nov 2004 08:32:54 +0000 (GMT)
Subject: [R] installing knncat package
In-Reply-To: <BAY1-F383riCTVuy16x0000f17e@hotmail.com>
References: <BAY1-F383riCTVuy16x0000f17e@hotmail.com>
Message-ID: <Pine.LNX.4.61.0411150823450.15674@gannet.stats>

This is an error in the package.  In file src/Makevars it has

LAPACK_LIBS=$(R_HOME)/src/modules/lapack/libRlapack.a

which is a reference to the R sources.  That should not be there: please 
remove it and this may well work.

However, your version of R is rather old, which is why you are not seeing 
a pre-compiled version.   R 2.0.1 will be out this week, and please 
upgrade to it when all the current packages which compile under Windows 
will be available to you, pre-compiled.


On Mon, 15 Nov 2004, Jenny Edmondson wrote:

> Hi all,
>
> I am wondering if anyone has problem installing 'knncat' package. I tried to 
> install using  'Rcmd INSTALL knncat_1.1.1.tar.gz', error message appeared. I 
> attached the message in the end. I allso tried to install from RGui, but it 
> was not listed.

[...]

> gcc  --shared -s  -o knncat.dll knncat.def knncat.a knncat_res.o 
> -Lc:/R/rw1081/src/gnuwin32 c:/R/rw1081/src/modules/lapack/libRlapack.a 
-lRblas -lg2c -lg2c 
> -lR
>
> gcc.exe: c:/R/rw1081/src/modules/lapack/libRlapack.a: No such file or 
> directory

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rksh at soc.soton.ac.uk  Mon Nov 15 09:44:12 2004
From: rksh at soc.soton.ac.uk (Robin Hankin)
Date: Mon, 15 Nov 2004 08:44:12 +0000
Subject: [R] argument rationalization
Message-ID: <a06002003bdbe1c7d2523@[139.166.242.29]>


Hi

I am writing a bunch of functions that take two, three or four
arguments.  These functions operate on vectors of the same length; 
but  I want the function
to behave sensibly if one or more arguments are scalars.  "+" does 
this for two arguments:


"+"(1:10,3)    # interpreted as "+"(1:10,rep(3,10))

But my functions can take more arguments.  Say f() takes three:

f(1:10,1:10,1:10)  # default
f(3,1:10,1:10)     # interpret as f(rep(3,10),1:10,1:10)
f(1:10,3,1:10)     # interpret as f(1:10,rep(3,10),1:10)
f(1:10,3,5)        # interpret as f(1:10,rep(3,10),rep(5,10))

and h() takes four:

h(2,4,5,1:10)    # interpret as h(rep(2,10),rep(4,10),rep(5,10),1:10)
h(2,3,1:10,1)    # interpret as h(rep(2,10),rep(3,10),1:10,rep(1:10)
h(1:20,3,1:20,1) # interpret as h(1:20,rep(3,20),1:20,rep(1,20))

I haven't got any that need five yet, but this may change in the future.
How do I implement this desired behaviour nicely?

(I pass the arguments to .C(), which is why I need this).

-- 
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
SO14 3ZH
tel +44(0)23-8059-7743
initialDOTsurname at soc.soton.ac.uk (edit in obvious way; spam precaution)



From ripley at stats.ox.ac.uk  Mon Nov 15 10:07:36 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 15 Nov 2004 09:07:36 +0000 (GMT)
Subject: [R] argument rationalization
In-Reply-To: <a06002003bdbe1c7d2523@[139.166.242.29]>
References: <a06002003bdbe1c7d2523@[139.166.242.29]>
Message-ID: <Pine.LNX.4.61.0411150904280.18692@gannet.stats>

I think you should implement recycling, ideally at C level.
But you could have

f <- function(x, y, z)
{
    n <- max(length(x), length(y), length(z))
    .C("something", as.double(rep(x, len=n)), as.double(rep(y, len=n)),
       as.double(rep(z, len=n)), as.integer(n), ans)$ans
}

On Mon, 15 Nov 2004, Robin Hankin wrote:

>
> Hi
>
> I am writing a bunch of functions that take two, three or four
> arguments.  These functions operate on vectors of the same length; but  I 
> want the function
> to behave sensibly if one or more arguments are scalars.  "+" does this for 
> two arguments:
>
>
> "+"(1:10,3)    # interpreted as "+"(1:10,rep(3,10))
>
> But my functions can take more arguments.  Say f() takes three:
>
> f(1:10,1:10,1:10)  # default
> f(3,1:10,1:10)     # interpret as f(rep(3,10),1:10,1:10)
> f(1:10,3,1:10)     # interpret as f(1:10,rep(3,10),1:10)
> f(1:10,3,5)        # interpret as f(1:10,rep(3,10),rep(5,10))
>
> and h() takes four:
>
> h(2,4,5,1:10)    # interpret as h(rep(2,10),rep(4,10),rep(5,10),1:10)
> h(2,3,1:10,1)    # interpret as h(rep(2,10),rep(3,10),1:10,rep(1:10)
> h(1:20,3,1:20,1) # interpret as h(1:20,rep(3,20),1:20,rep(1,20))
>
> I haven't got any that need five yet, but this may change in the future.
> How do I implement this desired behaviour nicely?
>
> (I pass the arguments to .C(), which is why I need this).
>
> -- 
> Robin Hankin
> Uncertainty Analyst
> Southampton Oceanography Centre
> SO14 3ZH
> tel +44(0)23-8059-7743
> initialDOTsurname at soc.soton.ac.uk (edit in obvious way; spam precaution)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From machud at intellektik.informatik.tu-darmstadt.de  Mon Nov 15 10:24:06 2004
From: machud at intellektik.informatik.tu-darmstadt.de (Marco Chiarandini)
Date: Mon, 15 Nov 2004 10:24:06 +0100
Subject: [R] Problems installing packages on MacOS with R 2.00 
Message-ID: <190B1716-36E8-11D9-A0BD-000A95ECC72C@intellektik.informatik.tu-darmstadt.de>

Dear all,

I have a problem installing a package required by Hmisc on MacOS 10.3.5  
with R 2.00.

g77   -fno-common  -g -O2 -c avas.f -o avas.o
g77   -fno-common  -g -O2 -c rlsmo.f -o rlsmo.o
gcc -bundle -flat_namespace -undefined suppress -L/usr/local/lib -o  
acepack.so ace.o avas.o rlsmo.o  -L/usr/local/lib  
-L/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2  
-L/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2/../../.. -lfrtbegin  
-lg2c -lSystem -lcc_dynamic -framework R
ld: warning -L: directory name  
(/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2) does not exist
ld: warning -L: directory name  
(/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2/../../..) does not  
exist
ld: can't locate file for: -lfrtbegin
make: *** [acepack.so] Error 1
ERROR: compilation failed for package 'acepack'

I found on the Internet a fix for R 1.8 which suggests to delete the  
-lfrtbegin library from /Applications/StartR.app/RAqua.app/Contents/etc  
but this path does not exists anymore on R 2.00.

How could I solve the problem.

Thank you in advance for the help.

Marco


------------------------------------------------------------------------ 
-----------------
Marco Chiarandini, Fachgebiet Intellektik, Fachbereich Informatik,
Technische Universit??t Darmstadt, Hochschulstra??e 10,
D-64289 Darmstadt - Germany, Office: S2/02 Raum E317
Tel: +49.(0)6151.166802 Fax: +49.(0)6151.165326
email: machud at intellektik.informatik.tu-darmstadt.de
web page: http://www.intellektik.informatik.tu-darmstadt.de/~machud



From rksh at soc.soton.ac.uk  Mon Nov 15 10:25:03 2004
From: rksh at soc.soton.ac.uk (Robin Hankin)
Date: Mon, 15 Nov 2004 09:25:03 +0000
Subject: [R] argument rationalization
In-Reply-To: <Pine.LNX.4.61.0411150904280.18692@gannet.stats>
References: <a06002003bdbe1c7d2523@[139.166.242.29]>
	<Pine.LNX.4.61.0411150904280.18692@gannet.stats>
Message-ID: <a06002005bdbe25a249eb@[139.166.242.29]>



  > I think you should implement recycling, ideally at C level.
  > But you could have
  >
  > f <- function(x, y, z)
  > {
  >    n <- max(length(x), length(y), length(z))
  >    .C("something", as.double(rep(x, len=n)), as.double(rep(y, len=n)),
  >       as.double(rep(z, len=n)), as.integer(n), ans)$ans
  > }


yes!  this works exactly as desired.  Thank you.  Another thing
that is incidentally satisfied by this scheme is to preserve
attributes such as dimensions and dimnames.  It seems to me to make
sense to use the attributes of the longest argument, and then set them
after the call


to wit

f <- function(x, y, z)
{
    lens <- c(length(x), length(y), length(z))
    all.attributes <- list(attributes(x),attributes(y),attributes(z))
    n <- max(lens)
    attributes.desired <- all.attributes[[which.max(lens)]]

    .C("something", as.double(rep(x, len=n)), as.double(rep(y, len=n)),
       as.double(rep(z, len=n)), as.integer(n), ans)$ans

    attributes(ans) <- attributes.desired
    return(ans)
}



Is this good practice?

best wishes

rksh




>  >
>  > On Mon, 15 Nov 2004, Robin Hankin wrote:
>  >
>
>  Hi
>
>  I am writing a bunch of functions that take two, three or four
>  arguments.  These functions operate on vectors of the same length; but
>  I want the function
>  to behave sensibly if one or more arguments are scalars


-- 
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
SO14 3ZH
tel +44(0)23-8059-7743
initialDOTsurname at soc.soton.ac.uk (edit in obvious way; spam precaution)



From ripley at stats.ox.ac.uk  Mon Nov 15 10:54:15 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 15 Nov 2004 09:54:15 +0000 (GMT)
Subject: [R] Problems installing packages on MacOS with R 2.00 
In-Reply-To: <190B1716-36E8-11D9-A0BD-000A95ECC72C@intellektik.informatik.tu-darmstadt.de>
References: <190B1716-36E8-11D9-A0BD-000A95ECC72C@intellektik.informatik.tu-darmstadt.de>
Message-ID: <Pine.LNX.4.61.0411150940410.19342@gannet.stats>

It is R 2.0.0!  Your problem is that you do not have g77 installed, or at 
least, not the same version as was used to compile your version of R.
(Please do read the posting guide and tell us where you got R from -- I 
suspect you did not compile it yourself.)

I think this should be in the MacOS X FAQ, but unfortunately the version 
on CRAN linked from the sidebar and the main FAQ at

    http://cran.r-project.org/bin/macosx/RMacOSX-FAQ.html

is for R 1.9.1, not 2.0.0.  Did your installation come with a current 
version?

However, a further problem is that many packages which use Fortran code 
cannot be compiled for MacOS X as it does not have a shared Fortran 
run-time library.  So I suspect that if you do install g77-3.4.2 you will 
find that you cannot compile package acepack, and that is why no 
pre-compiled version of the package is available.


On Mon, 15 Nov 2004, Marco Chiarandini wrote:

> Dear all,
>
> I have a problem installing a package required by Hmisc on MacOS 10.3.5 with 
> R 2.00.
>
> g77   -fno-common  -g -O2 -c avas.f -o avas.o
> g77   -fno-common  -g -O2 -c rlsmo.f -o rlsmo.o
> gcc -bundle -flat_namespace -undefined suppress -L/usr/local/lib -o 
> acepack.so ace.o avas.o rlsmo.o  -L/usr/local/lib 
> -L/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2 
> -L/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2/../../.. -lfrtbegin -lg2c 
> -lSystem -lcc_dynamic -framework R
> ld: warning -L: directory name 
> (/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2) does not exist
> ld: warning -L: directory name 
> (/usr/local/lib/gcc/powerpc-apple-darwin6.8/3.4.2/../../..) does not exist
> ld: can't locate file for: -lfrtbegin
> make: *** [acepack.so] Error 1
> ERROR: compilation failed for package 'acepack'
>
> I found on the Internet a fix for R 1.8 which suggests to delete the 
> -lfrtbegin library from /Applications/StartR.app/RAqua.app/Contents/etc but 
> this path does not exists anymore on R 2.00.
>
> How could I solve the problem.

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

PLEASE do!


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From christoph.bier at web.de  Mon Nov 15 11:09:28 2004
From: christoph.bier at web.de (Christoph Bier)
Date: Mon, 15 Nov 2004 11:09:28 +0100
Subject: [R] Where has the Debian respository gone?
In-Reply-To: <20041115013527.GA17007@sonny.eddelbuettel.com>
References: <cn8k7h$lin$1@sea.gmane.org>
	<20041115013527.GA17007@sonny.eddelbuettel.com>
Message-ID: <cn9v8q$ubj$1@sea.gmane.org>

Dirk Eddelbuettel schrieb am 15.11.2004 02:35

[CRAN Debian respository]

> It has been turned off by the CRAN masters as the content had slipped
> further and further behind the Debian content.  
> 
> Current R and CRAN packages are on the Debian archives; you can install
> these on testing too.  To the best of my knowledge, there are no backports
> of current R and Debian CRAN packages to Debian stable. 

Ok, thanks! Is there a list this fact was mentioned on?

Greetings,
     Christoph



From christoph.bier at web.de  Mon Nov 15 11:39:45 2004
From: christoph.bier at web.de (Christoph Bier)
Date: Mon, 15 Nov 2004 11:39:45 +0100
Subject: [R] Where has the Debian respository gone?
In-Reply-To: <20041115031233.GA17861@sonny.eddelbuettel.com>
References: <cn8k7h$lin$1@sea.gmane.org>	<20041115013527.GA17007@sonny.eddelbuettel.com>
	<20041115031233.GA17861@sonny.eddelbuettel.com>
Message-ID: <cna11p$3d1$1@sea.gmane.org>

Dirk Eddelbuettel schrieb am 15.11.2004 04:12

> On Sun, Nov 14, 2004 at 07:35:27PM -0600, Dirk Eddelbuettel wrote:

[...]

>>Current R and CRAN packages are on the Debian archives; you can install
>>these on testing too.  To the best of my knowledge, there are no backports
>>of current R and Debian CRAN packages to Debian stable. 
> 
> Upon re-reading this, I should clarify that in this context "CRAN packages"
> refers to the several dozen CRAN packages that are in Debian; 

Thanks for the clarification. That's how I already understood it.

> the list is
> growing but still far from exhaustive.

Yes, I saw it on the Ubuntu machine of my girl friend (I changed her
Debian Woody/Sarge some weeks ago to Warty), when I used synaptic
for the first time; didn't know, that there was so many CRAN
packages for Debian! Up to now I always installed packages from
within R by install.packages("foo"). Has one of these methods
advantages compared with the other?

Greetings,
     Christoph



From machud at intellektik.informatik.tu-darmstadt.de  Mon Nov 15 11:50:51 2004
From: machud at intellektik.informatik.tu-darmstadt.de (Marco Chiarandini)
Date: Mon, 15 Nov 2004 11:50:51 +0100 (CET)
Subject: [R] Problems installing packages on MacOS with R 2.00
In-Reply-To: <Pine.LNX.4.61.0411150940410.19342@gannet.stats>
References: <190B1716-36E8-11D9-A0BD-000A95ECC72C@intellektik.informatik.tu-darmstadt.de>
	<Pine.LNX.4.61.0411150940410.19342@gannet.stats>
Message-ID: <Pine.LNX.4.58.0411151140240.21699@kika.intellektik.informatik.tu-darmstadt.de>

Dear Prof. Ripley,

> It is R 2.0.0!  Your problem is that you do not have g77 installed, or at
> least, not the same version as was used to compile your version of R.
> (Please do read the posting guide and tell us where you got R from -- I
> suspect you did not compile it yourself.)


I took R from http://cran.at.r-project.org/. True, I did not compiled
it: it was the R 2.0.0 (lastest version) bin package. However, I have
g77 version 3.4 (October 2003) installed on MacOS.


I deleted both -lfrtbegin and -lg2c from FLIBS in
/Library/Frameworks/R.framework/Resources/etc/Makeconf
and now Hmisc compiles fine.
I could not find a directory
/usr/local/lib/gcc/powerpc-apple-darwin6.8/
in my installation.


> I think this should be in the MacOS X FAQ, but unfortunately the version
> on CRAN linked from the sidebar and the main FAQ at
>
>     http://cran.r-project.org/bin/macosx/RMacOSX-FAQ.html
>
> is for R 1.9.1, not 2.0.0.  Did your installation come with a current
> version?
>
> However, a further problem is that many packages which use Fortran code
> cannot be compiled for MacOS X as it does not have a shared Fortran
> run-time library.  So I suspect that if you do install g77-3.4.2 you will
> find that you cannot compile package acepack, and that is why no
> pre-compiled version of the package is available.
>


-------------------------------------------------------------------
Marco Chiarandini, Fachgebiet Intellektik, Fachbereich Informatik,
Technische Universit??t Darmstadt, Hochschulstra??e 10,
D-64289 Darmstadt - Germany, Office: S2/02 Raum E317
Tel: +49.(0)6151.166802 Fax: +49.(0)6151.165326
email: machud at intellektik.informatik.tu-darmstadt.de
web page: http://www.intellektik.informatik.tu-darmstadt.de/~machud



From ramasamy at cancer.org.uk  Mon Nov 15 12:39:30 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 15 Nov 2004 11:39:30 +0000
Subject: [R] Legend help needed
In-Reply-To: <4198B84A.27421.244A7C2A@tui.lincoln.ac.nz>
References: <4198B84A.27421.244A7C2A@tui.lincoln.ac.nz>
Message-ID: <1100518770.3195.45.camel@ndmpc126.ihs.ox.ac.uk>

You have not called legend() in your codes below, so we do not know what
your problem is. See other comments below.

On Mon, 2004-11-15 at 01:08, Sean David Richards wrote:
> R : Version 1.9.1
> 
> Hi,
> 
> Am having trouble adding a legend to scatterplot. R code is shown below. 
> I have tried various incantations to add a legend (using the legend() 
> function) to the resulting plot but without any success. Looks like it 
> should be simple but I must be missing something. Any pointers would be 
> welcome.
> Have looked at help(legend) etc.

help(legend) provides many nice examples. Here is a simplified one :

x <- seq(-pi, pi, len = 65)
plot(x, sin(x), type="l", lty=1, col=1)
lines(x, cos(x), type="l", lty=2, col=2)
legend(-pi, 1, legend=c("sin", "cosine"), lty=1:2, col=1:2)

Or you can replace the last line with 
 legend(locator(1), legend=c("sin", "cosine"), lty=1:2, col=1:2)
where the legend will be placed on mouse left click.

> --8<----------------------------------------------------------------------
> ---
> 
> sfiles <- c("72_12_12_V.csv ",
>             "150_25_15_V.csv",    
>             "150_25_20_V.csv",    
>             "150_25_25_V.csv",    
>             "150_25_40_V.csv",    
>             "150_25_60_V.csv",    
>             "150_25_90_V.csv",    
>             "240_40_40_V.csv")                  
>             
> ## process each file in list
> for (i in 1:length(sfiles)) {
> data <- read.csv(paste("../data/",sfiles[i],sep=""))
> 
> ## assign columns to some nice names
> K <- data[,8]
> AN <- data[,3] * (data[,2] - data[,4])
> 
> ## plot K against AN

Please give a simplified example. You do not need to show us all the
preprocessing steps. It can be distracting.

> if ( i == 1) {
>     plot(AN, K, ylim=c(1000,9000), xlim=c(0,1500), 
>           xlab="Area above Notch (mm)",
>           main="Size Effect Specimens")
>     par(new=TRUE)                        
>     }
> else{
>     plot(AN,K, pch=(i),ylim=c(1000,9000), xlim=c(0,1500), 
>           axes=FALSE,xlab="")
>     par(new=TRUE)
>     }
> }

Have you considered points() or lines() here ? You could simplify to

plot(0,1000, type="n", xlim=c(0,1500), ylim=c(1000,9000),
     xlab="Area above Notch (mm)", main="Size Effect Speciments")

n <- length(sfiles)

for (i in 1:n) {
  data <- read.csv(paste("../data/",sfiles[i],sep="")) 
  K    <- data[,8]
  AN   <- data[,3] * (data[,2] - data[,4])

  points( AN, K, pch=i, col=i )
}

legend( 1500, 9000, legend=paste("Data from", sfiles), pch=1:n, col=i )

> --8<----------------------------------------------------------------------
> ---



From ripley at stats.ox.ac.uk  Mon Nov 15 12:38:22 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 15 Nov 2004 11:38:22 +0000 (GMT)
Subject: [R] Problems installing packages on MacOS with R 2.00
In-Reply-To: <Pine.LNX.4.58.0411151140240.21699@kika.intellektik.informatik.tu-darmstadt.de>
References: <190B1716-36E8-11D9-A0BD-000A95ECC72C@intellektik.informatik.tu-darmstadt.de>
	<Pine.LNX.4.61.0411150940410.19342@gannet.stats>
	<Pine.LNX.4.58.0411151140240.21699@kika.intellektik.informatik.tu-darmstadt.de>
Message-ID: <Pine.LNX.4.61.0411151133350.23099@gannet.stats>

On Mon, 15 Nov 2004, Marco Chiarandini wrote:

> Dear Prof. Ripley,
>
>> It is R 2.0.0!  Your problem is that you do not have g77 installed, or at
>> least, not the same version as was used to compile your version of R.
>> (Please do read the posting guide and tell us where you got R from -- I
>> suspect you did not compile it yourself.)
>
>
> I took R from http://cran.at.r-project.org/. True, I did not compiled
> it: it was the R 2.0.0 (lastest version) bin package. However, I have
> g77 version 3.4 (October 2003) installed on MacOS.

Which is rather old, and this was looking for 3.4.2 (and 3.4.3 is 
current).

Have you read the posting guide yet?

> I deleted both -lfrtbegin and -lg2c from FLIBS in

You can safely delete -lfrtbegin.  I don't believe you can safely delete 
-lg2c, as some packages do need code from it.  On my systems that includes 
acepack, but as you have a system using libR.dylib, it may be that 
libR.dylib contains the routines that acepack needs from -lg2c.

> /Library/Frameworks/R.framework/Resources/etc/Makeconf
> and now Hmisc compiles fine.
> I could not find a directory
> /usr/local/lib/gcc/powerpc-apple-darwin6.8/
> in my installation.
>
>
>> I think this should be in the MacOS X FAQ, but unfortunately the version
>> on CRAN linked from the sidebar and the main FAQ at
>>
>>     http://cran.r-project.org/bin/macosx/RMacOSX-FAQ.html
>>
>> is for R 1.9.1, not 2.0.0.  Did your installation come with a current
>> version?
>>
>> However, a further problem is that many packages which use Fortran code
>> cannot be compiled for MacOS X as it does not have a shared Fortran
>> run-time library.  So I suspect that if you do install g77-3.4.2 you will
>> find that you cannot compile package acepack, and that is why no
>> pre-compiled version of the package is available.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ramasamy at cancer.org.uk  Mon Nov 15 12:52:01 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 15 Nov 2004 11:52:01 +0000
Subject: [R] Power curves
In-Reply-To: <4198040C.31196.D21C35C@localhost>
References: <4198040C.31196.D21C35C@localhost>
Message-ID: <1100519521.3195.59.camel@ndmpc126.ihs.ox.ac.uk>

What do you mean by "power curves" ? Is it the power of a study as the
effect size varies or power output of a machine with some other
parameter ?

I usually generate a sequence of numbers (for the x-axis) that spans the
range of interest and calculate its output. 

# Example 1
f <- function(x) sin(x)
x <- seq(0, 10, by=0.01)
y <- f(x)
plot(x, y, type="l")

Or you can call
plot(f, xlim=c(0, 10) )

# Example 2
plot(2:100, power.t.test(2:100, delta=1, sd=1, sig.level=0.05)$power)

Regards, Adai

On Mon, 2004-11-15 at 01:19, Duncan Harris wrote:
> How do I draw/calculate power curves in R?
> 
> Cheers,
> 
> Duncan.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ramasamy at cancer.org.uk  Mon Nov 15 13:11:20 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Mon, 15 Nov 2004 12:11:20 +0000
Subject: [R] Legend help needed
In-Reply-To: <1100518770.3195.45.camel@ndmpc126.ihs.ox.ac.uk>
References: <4198B84A.27421.244A7C2A@tui.lincoln.ac.nz>
	<1100518770.3195.45.camel@ndmpc126.ihs.ox.ac.uk>
Message-ID: <1100520680.3195.77.camel@ndmpc126.ihs.ox.ac.uk>

Sorry typo. The last line should read

legend(1500, 9000, legend=paste("Data from", sfiles), pch=1:n, col=1:n )
                                                                   ^^^

On Mon, 2004-11-15 at 11:39, Adaikalavan Ramasamy wrote:
> You have not called legend() in your codes below, so we do not know what
> your problem is. See other comments below.
> 
> On Mon, 2004-11-15 at 01:08, Sean David Richards wrote:
> > R : Version 1.9.1
> > 
> > Hi,
> > 
> > Am having trouble adding a legend to scatterplot. R code is shown below. 
> > I have tried various incantations to add a legend (using the legend() 
> > function) to the resulting plot but without any success. Looks like it 
> > should be simple but I must be missing something. Any pointers would be 
> > welcome.
> > Have looked at help(legend) etc.
> 
> help(legend) provides many nice examples. Here is a simplified one :
> 
> x <- seq(-pi, pi, len = 65)
> plot(x, sin(x), type="l", lty=1, col=1)
> lines(x, cos(x), type="l", lty=2, col=2)
> legend(-pi, 1, legend=c("sin", "cosine"), lty=1:2, col=1:2)
> 
> Or you can replace the last line with 
>  legend(locator(1), legend=c("sin", "cosine"), lty=1:2, col=1:2)
> where the legend will be placed on mouse left click.
> 
> > --8<----------------------------------------------------------------------
> > ---
> > 
> > sfiles <- c("72_12_12_V.csv ",
> >             "150_25_15_V.csv",    
> >             "150_25_20_V.csv",    
> >             "150_25_25_V.csv",    
> >             "150_25_40_V.csv",    
> >             "150_25_60_V.csv",    
> >             "150_25_90_V.csv",    
> >             "240_40_40_V.csv")                  
> >             
> > ## process each file in list
> > for (i in 1:length(sfiles)) {
> > data <- read.csv(paste("../data/",sfiles[i],sep=""))
> > 
> > ## assign columns to some nice names
> > K <- data[,8]
> > AN <- data[,3] * (data[,2] - data[,4])
> > 
> > ## plot K against AN
> 
> Please give a simplified example. You do not need to show us all the
> preprocessing steps. It can be distracting.
> 
> > if ( i == 1) {
> >     plot(AN, K, ylim=c(1000,9000), xlim=c(0,1500), 
> >           xlab="Area above Notch (mm)",
> >           main="Size Effect Specimens")
> >     par(new=TRUE)                        
> >     }
> > else{
> >     plot(AN,K, pch=(i),ylim=c(1000,9000), xlim=c(0,1500), 
> >           axes=FALSE,xlab="")
> >     par(new=TRUE)
> >     }
> > }
> 
> Have you considered points() or lines() here ? You could simplify to
> 
> plot(0,1000, type="n", xlim=c(0,1500), ylim=c(1000,9000),
>      xlab="Area above Notch (mm)", main="Size Effect Speciments")
> 
> n <- length(sfiles)
> 
> for (i in 1:n) {
>   data <- read.csv(paste("../data/",sfiles[i],sep="")) 
>   K    <- data[,8]
>   AN   <- data[,3] * (data[,2] - data[,4])
> 
>   points( AN, K, pch=i, col=i )
> }
> 
> legend( 1500, 9000, legend=paste("Data from", sfiles), pch=1:n, col=i )
> 
> > --8<----------------------------------------------------------------------
> > ---
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
-- 
Adaikalavan Ramasamy                    ramasamy at cancer.org.uk
Centre for Statistics in Medicine       http://www.ihs.ox.ac.uk/csm/
Cancer Research UK                      Tel : 01865 226 677
Old Road Campus, Headington, Oxford     Fax : 01865 226 962



From rachelpearce at msn.com  Mon Nov 15 13:25:08 2004
From: rachelpearce at msn.com (Rachel Pearce)
Date: Mon, 15 Nov 2004 12:25:08 -0000
Subject: [R] Somewhat off-topic : Hand and Taylor reference
In-Reply-To: <200411151116.iAFB71nk008133@hypatia.math.ethz.ch>
Message-ID: <000e01c4cb0e$25d62b70$0ca74c51@wibble>

I am sorry to trouble the list with this, but I'm hoping that someone
will be able to help me track down this reference. 
I am looking for a good reference on repeated measures, and many of the
relevant functions in R refer in their "help" to:

     Hand, D. J. and Taylor, C. C.  (1987) _Multivariate Analysis of
     Variance and Repeated Measures._ Chapman and Hall.

This sounds like a useful text, but I can't find it on Amazon. I can
find:

Multivariate Analysis of Variance for Behavioural Scientists (Chapman &
Hall Statistics Text Series)  
D.J. Hand, C.C. Taylor 

http://www.amazon.co.uk/exec/obidos/ASIN/0412258005/qid=1100520579/sr=1-
4/ref=sr_1_8_4/026-0799039-6262851#product-details

Is this the same book? It has the same publication date. Alternatively,
is the reference mentioned in R help now out of print? If so, are there
any other books which you would recommend for this subject? 

Thanks in advance for any help with this and apologies if this query is
inappropriate for the list. 

Rachel Pearce

British Society of Blood and Marrow Transplantation



From ylong at smu.edu.sg  Mon Nov 15 13:26:13 2004
From: ylong at smu.edu.sg (LONG Yu)
Date: Mon, 15 Nov 2004 20:26:13 +0800
Subject: [R] how can draw probability density plot?
Message-ID: <CF99BBA00A3F5D4183C2FA33089FBC340221E06D@EX01.staff.smu.edu.sg>

Dear support,

I want to draw a probability density plot in R. For example, I provide the mean and variance of a normal distribution, then R can provide me the probability density plot. Now I always generate random numbers of normal distribution and calculate their dnorm(mu, var), finally plot them. I am eager to know some directly operation.

Thank you for your attention.

I am looking forward to hearing from you soon. 

 

Best regards,

Long Yu


From vito_ricci at yahoo.com  Mon Nov 15 13:50:40 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Mon, 15 Nov 2004 13:50:40 +0100 (CET)
Subject: [R] R: how can draw probability density plot?
Message-ID: <20041115125040.90105.qmail@web41202.mail.yahoo.com>

I hope this example could help you
best
vito

> x<-seq(-3.5,3.5,0.1)
> x
 [1] -3.5 -3.4 -3.3 -3.2 -3.1 -3.0 -2.9 -2.8 -2.7 -2.6
-2.5 -2.4 -2.3 -2.2 -2.1
[16] -2.0 -1.9 -1.8 -1.7 -1.6 -1.5 -1.4 -1.3 -1.2 -1.1
-1.0 -0.9 -0.8 -0.7 -0.6
[31] -0.5 -0.4 -0.3 -0.2 -0.1  0.0  0.1  0.2  0.3  0.4
 0.5  0.6  0.7  0.8  0.9
[46]  1.0  1.1  1.2  1.3  1.4  1.5  1.6  1.7  1.8  1.9
 2.0  2.1  2.2  2.3  2.4
[61]  2.5  2.6  2.7  2.8  2.9  3.0  3.1  3.2  3.3  3.4
 3.5
>d<- dnorm(x,0,1)
 
> plot(d),type="l")

you wrote:

Dear support,

I want to draw a probability density plot in R. For
example, I provide the mean and variance of a normal
distribution, then R can provide me the probability
density plot. Now I always generate random numbers of
normal distribution and calculate their dnorm(mu,
var), finally plot them. I am eager to know some
directly operation.

Thank you for your attention.

I am looking forward to hearing from you soon. 

 

Best regards,

Long Yu

=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese http://www.modugno.it/archivio/cat_palese.shtml



From p.dalgaard at biostat.ku.dk  Mon Nov 15 13:54:19 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Nov 2004 13:54:19 +0100
Subject: [R] Somewhat off-topic : Hand and Taylor reference
In-Reply-To: <000e01c4cb0e$25d62b70$0ca74c51@wibble>
References: <000e01c4cb0e$25d62b70$0ca74c51@wibble>
Message-ID: <x2lld3z30k.fsf@biostat.ku.dk>

"Rachel Pearce" <rachelpearce at msn.com> writes:

>      Hand, D. J. and Taylor, C. C.  (1987) _Multivariate Analysis of
>      Variance and Repeated Measures._ Chapman and Hall.
> 
> This sounds like a useful text, but I can't find it on Amazon. I can
> find:
> 
> Multivariate Analysis of Variance for Behavioural Scientists (Chapman &
> Hall Statistics Text Series)  
> D.J. Hand, C.C. Taylor 
> 
> http://www.amazon.co.uk/exec/obidos/ASIN/0412258005/qid=1100520579/sr=1-
> 4/ref=sr_1_8_4/026-0799039-6262851#product-details
> 
> Is this the same book? It has the same publication date. Alternatively,
> is the reference mentioned in R help now out of print? If so, are there
> any other books which you would recommend for this subject? 

It also has the same ISBN and is subtitled "A practical approach for
behavioural scientists"...

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From vito_ricci at yahoo.com  Mon Nov 15 13:59:57 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Mon, 15 Nov 2004 13:59:57 +0100 (CET)
Subject: [R] R: how can draw probability density plot?
Message-ID: <20041115125957.20592.qmail@web41209.mail.yahoo.com>

I believe this is better and that you want!
x<-rnorm(1000,10,1)
plot( function(y) dnorm(y, mean(x), sd(x)),
from=min(x), to=max(x))


best
vito


You wrote:

Dear support,

I want to draw a probability density plot in R. For
example, I provide the mean and variance of a normal
distribution, then R can provide me the probability
density plot. Now I always generate random numbers of
normal distribution and calculate their dnorm(mu,
var), finally plot them. I am eager to know some
directly operation.

Thank you for your attention.

I am looking forward to hearing from you soon. 

 

Best regards,

Long Yu

=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese http://www.modugno.it/archivio/cat_palese.shtml



From ripley at stats.ox.ac.uk  Mon Nov 15 14:03:05 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 15 Nov 2004 13:03:05 +0000 (GMT)
Subject: [R] Somewhat off-topic : Hand and Taylor reference
In-Reply-To: <000e01c4cb0e$25d62b70$0ca74c51@wibble>
References: <000e01c4cb0e$25d62b70$0ca74c51@wibble>
Message-ID: <Pine.LNX.4.61.0411151249150.16675@gannet.stats>

The full library info is

Multivariate analysis of variance and repeated measures : a practical 
approach for behavioural scientists

Publisher 	London : Chapman and Hall, 1987.
Description 	xiii, 262 p. : ill. ; 24 cm.
Notes 	Bibliography: p. 208-209. - Includes index.
ISBN 	0412258005 (pbk)  0412258102


although the part of the title after the colon is in very small print. 
It looks the same as that now published by CRC Press, who are showing that 
title at

http://www.crcpress.com/shopping_cart/products/product_detail.asp?sku=C5800&parent_id=501&pc=

So my guess is that Amazon have it garbled.

On Mon, 15 Nov 2004, Rachel Pearce wrote:

> I am sorry to trouble the list with this, but I'm hoping that someone
> will be able to help me track down this reference.
> I am looking for a good reference on repeated measures, and many of the
> relevant functions in R refer in their "help" to:
>
>     Hand, D. J. and Taylor, C. C.  (1987) _Multivariate Analysis of
>     Variance and Repeated Measures._ Chapman and Hall.
>
> This sounds like a useful text, but I can't find it on Amazon. I can
> find:
>
> Multivariate Analysis of Variance for Behavioural Scientists (Chapman &
> Hall Statistics Text Series)
> D.J. Hand, C.C. Taylor
>
> http://www.amazon.co.uk/exec/obidos/ASIN/0412258005/qid=1100520579/sr=1-
> 4/ref=sr_1_8_4/026-0799039-6262851#product-details
>
> Is this the same book? It has the same publication date. Alternatively,
> is the reference mentioned in R help now out of print? If so, are there
> any other books which you would recommend for this subject?
>
> Thanks in advance for any help with this and apologies if this query is
> inappropriate for the list.
>
> Rachel Pearce
>
> British Society of Blood and Marrow Transplantation
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From OG1092 at hotmail.com  Mon Nov 15 14:33:28 2004
From: OG1092 at hotmail.com (Jonathan Croft)
Date: Mon, 15 Nov 2004 13:33:28 -0000
Subject: [R] Error when scan()ning dead URL's
Message-ID: <BAY101-DAV9jnZS08i700018bf4@hotmail.com>

Hi,

I often scan web pages directly into R using

scan('http://etc...','')

however this gives an error if the page/url doesn't exist, or the connection
to it is not available.
Is it possible to still use scan but have R return something other than an
error (which crashes code)
when the page or the connection isn't available.

Cheers

John Haynes



From p.dalgaard at biostat.ku.dk  Mon Nov 15 14:40:10 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Nov 2004 14:40:10 +0100
Subject: [R] Error when scan()ning dead URL's
In-Reply-To: <BAY101-DAV9jnZS08i700018bf4@hotmail.com>
References: <BAY101-DAV9jnZS08i700018bf4@hotmail.com>
Message-ID: <x28y93z0w5.fsf@biostat.ku.dk>

"Jonathan Croft" <OG1092 at hotmail.com> writes:

> I often scan web pages directly into R using
> 
> scan('http://etc...','')
> 
> however this gives an error if the page/url doesn't exist, or the connection
> to it is not available.
> Is it possible to still use scan but have R return something other than an
> error (which crashes code)
> when the page or the connection isn't available.

Can't you just wrap it in a try() construct?

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From CBARNERIAS at caen.ifn.fr  Mon Nov 15 14:42:10 2004
From: CBARNERIAS at caen.ifn.fr (BARNERIAS Cyrille)
Date: Mon, 15 Nov 2004 14:42:10 +0100
Subject: [R] help for nls 
Message-ID: <78D81634DAB9D6119D200010DC1D60981BC91E@malus.caen.ifn.fr>

Hello,

I am beginning with R and I would like to test a non linear model. But I do
not find exactly wath I am looking for in nls packages (or I do not know
where to search).
I would like to try a model like this : y=b * x exp(n)/(a exp(n) + x exp
(n))
Where 
a = a0 + a1z
b= b0 + b1z
x and z are variables
y the variable that I am trying to modelise
a0, a1, b0 and b1 are parameters to determine.
I am wondering if I can use nls and if so how do I have to write my command
?
Thanks in advance for your help.

Cyrille Barn??rias
Adjoint au chef d'??chelon interr??gional de Caen
Inventaire forestier national
73 rue Marie Curie
14 200 H??rouville Saint-Clair
France
Tel : 02.31.47.71.53



From Graham.Law at egu.york.ac.uk  Mon Nov 15 15:11:42 2004
From: Graham.Law at egu.york.ac.uk (Graham Law)
Date: Mon, 15 Nov 2004 14:11:42 -0000
Subject: [R] Using R in parallel on a 2 processor machine
Message-ID: <855D4EEB81E2044990C7908A29B8A5FE038CCF@basil.lrf.leeds.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041115/aaad2092/attachment.pl

From abunn at whrc.org  Mon Nov 15 15:34:21 2004
From: abunn at whrc.org (Andy Bunn)
Date: Mon, 15 Nov 2004 09:34:21 -0500
Subject: [R] Using R in parallel on a 2 processor machine
In-Reply-To: <855D4EEB81E2044990C7908A29B8A5FE038CCF@basil.lrf.leeds.ac.uk>
Message-ID: <NEBBIPHDAMMOKDKPOFFIKEBLCMAA.abunn@whrc.org>

How about installing an operating system that knows its way around that much
RAM?

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Graham Law
> Sent: Monday, November 15, 2004 9:12 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Using R in parallel on a 2 processor machine
>
>
> Hi,
>
>
>
> I have installed R (2.0.0) onto a two processor machine running Windows
> XP (these two processors have been split into 4 logical processors),
> with 4Gb of RAM.  Rather than multi-threading, I wanted to run 2
> instances of R on the machine in parallel, which uses two of the logical
> processors.  Unfortunately, R seems to be accessing a total of 1Gb of
> RAM, not taking advantage of 2 lots of 1Gb.  This has led to memory
> problems, with one R process using the RAM, and the other one running
> into memory problems.
>
>
>
> Looking at previous posts on this issue, I have tried using gc(), to
> take back unused memory.  This has allowed the two parallel processes to
> work, but has not given me access to the other 3Gb.
>
>
>
> Any help would be much appreciated into how R may run in parallel using
> more than 1Gb.
>
>
>
> Graham
>
>
>
> Graham Law
>
> Senior Scientist
>
> Epidemiology and Genetics Unit
>
> Department of Health Sciences
>
> University of York
>
> York YO10 5DD UK
>
> (t) +44 (0) 1904 32 1883
>
> (m) +44 (0) 790 500 8828
>
>
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Mon Nov 15 15:49:26 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 15 Nov 2004 14:49:26 +0000 (GMT)
Subject: [R] Using R in parallel on a 2 processor machine
In-Reply-To: <855D4EEB81E2044990C7908A29B8A5FE038CCF@basil.lrf.leeds.ac.uk>
References: <855D4EEB81E2044990C7908A29B8A5FE038CCF@basil.lrf.leeds.ac.uk>
Message-ID: <Pine.LNX.4.61.0411151444590.4248@gannet.stats>

Try reading the rw-FAQ!

There is no way user processes in Windows XP can access all 4Gb of the 
address space, but if you tune both R and XP correctly you should be able 
to get above 2Gb in total.  See the rw-FAQ for how to tune R.

Nevertheless, as you have already been told, it is a lot easier to do this
under a

On Mon, 15 Nov 2004, Graham Law wrote:

> Hi,
>
> I have installed R (2.0.0) onto a two processor machine running Windows
> XP (these two processors have been split into 4 logical processors),
> with 4Gb of RAM.  Rather than multi-threading, I wanted to run 2
> instances of R on the machine in parallel, which uses two of the logical
> processors.  Unfortunately, R seems to be accessing a total of 1Gb of
> RAM, not taking advantage of 2 lots of 1Gb.  This has led to memory
> problems, with one R process using the RAM, and the other one running
> into memory problems.

> Looking at previous posts on this issue, I have tried using gc(), to
> take back unused memory.  This has allowed the two parallel processes to
> work, but has not given me access to the other 3Gb.
>
> Any help would be much appreciated into how R may run in parallel using
> more than 1Gb.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ahenningsen at email.uni-kiel.de  Mon Nov 15 16:07:00 2004
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Mon, 15 Nov 2004 16:07:00 +0100
Subject: [R] help for nls
In-Reply-To: <78D81634DAB9D6119D200010DC1D60981BC91E@malus.caen.ifn.fr>
References: <78D81634DAB9D6119D200010DC1D60981BC91E@malus.caen.ifn.fr>
Message-ID: <200411151607.00497.ahenningsen@email.uni-kiel.de>

Dear Cyrille,

type "?nls" in the R command line. 
This will show you how to use the nls function.

And please read the "posting guide" (see bootom of each message).
This will help you to get more helpful answers.

Best wishes,
Arne

On Monday 15 November 2004 14:42, BARNERIAS Cyrille wrote:
> Hello,
>
> I am beginning with R and I would like to test a non linear model. But I do
> not find exactly wath I am looking for in nls packages (or I do not know
> where to search).
> I would like to try a model like this : y=b * x exp(n)/(a exp(n) + x exp
> (n))
> Where
> a = a0 + a1z
> b= b0 + b1z
> x and z are variables
> y the variable that I am trying to modelise
> a0, a1, b0 and b1 are parameters to determine.
> I am wondering if I can use nls and if so how do I have to write my command
> ?
> Thanks in advance for your help.
>
> Cyrille Barn??rias
> Adjoint au chef d'??chelon interr??gional de Caen
> Inventaire forestier national
> 73 rue Marie Curie
> 14 200 H??rouville Saint-Clair
> France
> Tel : 02.31.47.71.53
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From p.dalgaard at biostat.ku.dk  Mon Nov 15 16:07:05 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Nov 2004 16:07:05 +0100
Subject: [R] R-2.0.1 is released
Message-ID: <x2r7mvxiau.fsf@biostat.ku.dk>

I've rolled up R-2.0.1.tgz a short while ago. This is a maintenance
version mainly to fix a number of minor bugs and issues. (Some rather
nasty ones were Windows-specific - please check the CHANGES file for
those.)

You can get it from

http://cran.r-project.org/src/base/R-2.0.1.tar.gz

or wait for it to be mirrored at a CRAN site nearer to you. Binaries
for various platforms will appear in due course.
 
There is also a version split for floppies.

These are the md5sums for the freshly created files, in case you wish
to check that they are uncorrupted:

6f75951d61cc321f597ea28ad43a9ada  INSTALL
b5519f795224506e7702f74dbbb152b1  NEWS
1a2248b10e3dbf078559705a25b52ca4  ONEWS
fb47b1fdef4323031e24d541a2f36b2b  R-2.0.1.tar.gz
4e334bc539e5a2d8cc3e6b8cca4171be  R-2.0.1.tar.gz-split.aa
bc7cb22034948998a4eeb18006a53094  R-2.0.1.tar.gz-split.ab
b73f587c39599ce47bba00544f2cd100  R-2.0.1.tar.gz-split.ac
bbec006c26c6a236c6b5e98989a2f0a2  R-2.0.1.tar.gz-split.ad
8d04842319525ba6cead4bff1b259263  R-2.0.1.tar.gz-split.ae
e3d1af261c7ea83820597f6fef9e9449  R-2.0.1.tar.gz-split.af
3595c1939484762a6440a957908023ce  R-2.0.1.tar.gz-split.ag
4492e468cc5003e06a2a85c496ff2672  R-2.0.1.tar.gz-split.ah
fb47b1fdef4323031e24d541a2f36b2b  R-latest.tgz
b5509d1e6232e66dee66d9644bf65490  README
7a5a7cfe9419affd9574aba3cf525741  RESOURCES


Here is the relevant bit of the NEWS file:


		CHANGES IN R VERSION 2.0.1


NEW FEATURES

    o	Platform equivalence in library() is tested by a new function
	testPlatformEquivalence() which ignores the 'vendor' field and
	can be customized by cognescenti.

    o   The assignment form of split() allows recycling of vectors 
	within the value list. In particular, things like 
	    split(x, g) <- lapply(split(x, g), mean)
	now work


DOCUMENTATION

    o	Manual `Writing R Extensions' has new sections on writing
	portable packages and on writing new front-ends for R -- the
	latter will be more comprehensive in R 2.1.0 which has new
	public header files.


DEPRECATED & DEFUNCT

    o	The aqua module in MacOS X is deprecated.

    o	Capabilities "bzip2", "GNOME, "libz" and "PCRE" are deprecated.

    o	The GNOME GUI on Unix-alikes is deprecated as part of R;
	it will be available in another form as from R 2.1.0.

    o	The undocumented use of UseMethod() with no argument is now
	formally deprecated.


INSTALLATION CHANGES

    o	Building on Alpha OSF/1 no longer forces the C flag -std1,
	which appears to be no longer needed.  (PR#7257)

    o	The compiler flag -mieee-fp is no longer used on i386 Linux
	(these days it is only passed to the linker and was only
	invoked for compilation steps).

    o	-D__NO_MATH_INLINES is only used on older ix86 glibc-based
	systems which need it (tested at configure time).  This leads to
	small improvements in speed and accuracy on modern systems.

    o	If makeinfo >= 4.5 is not available, warnings are given that
	some of the HTML manuals will be missing, and the index page
	given by help.start() will link to CRAN versions of those manuals.

    o	Files aclocal.m4 and acinclude.m4 used in maintainer builds
	are not longer included in the distribution.


C-LEVEL FACILITIES

    o	It was not clear in 'Writing R Extensions' that some of the
	entry points in the 'Utilities' section were not declared in
	<R.h> (they were in <R_ext/Applic.h>).  Now all the entry
	points in that section are declared in <R_ext/Utils.h>,
	included by <R.h>.


BUG FIXES

    o   The grid.grab() function in package grid would throw an error
	if there were no viewports pushed (now returns NULL).

    o	model.frame.default() takes row names from the response
	variable if that has suitable names and there is no 'data'
	argument.  (This follows S but was not previously implemented
	in R.)

    o	write.table() was not respecting the 'dec' argument for complex
	numbers.

    o	write.table() printed a mixture of numeric and complex numbers
	as all complex.	 (PR#7260)

    o	R CMD INSTALL failed with versioned installs on packages which
	save images (only).

    o	dlogis() gave NaN not 0 for large negative arguments.

    o	Importing from another namespace was broken for versioned
	installs, incorrectly reporting something like
	"package 'imported_from' does not have a name space".

    o	The GNOME interface under Linux/Unix was broken. (PR#7276)

    o	For the jpeg/png devices under Linux/Unix, under certain rare
	circumstances clipping needed to be cleared before starting a
	new page.  (PR#7270, which has been the case since the devices
	were introduced in 1.1.0.)

    o	First lattice plot (first grid.newpage() call) did not start
	a new page IF there had been a previous traditional graphics
	plot (on the same device).

    o	Using install.packages() to install the same package to more
	than one library gave an incorrect warning message.  (If there
	were two or more such packages it might give an error.)

    o	.packages(all.available=TRUE) returned packages with an invalid
	version field in their DESCRIPTION whereas .find.packages() and
	packageDescription() did not.  Now all do not.

    o	packageDescription() now correctly reports that a package does
	not exist, rather than that its DESCRIPTION file is 'missing
	or broken'.

    o	'make dist' from builddir != sourcedir was copying not linking
	recommended packages to *.tgz.

    o	Slots in prototype objects can inherit from locally defined
	classes (which were not being found correctly before).

    o	Several fixes to the behavior of as() when there are either
	coerce= or replace= methods supplied in a call to
	setIs(). Related fixes to setIs() to handle correctly previous
	methods, if there were any.

    o	splinefun(1[0], 1[0])(1) doesn't segfault anymore (PR#7290).
	spline() and splinefun() now also work with missing values by
	omiting them.

    o	ecdf() was failing on inputs containing NAs. (Part of PR#7292)

    o	tools:::.install_package_description was splitting the Built:
	field across lines on platforms with very long names.

    o	capabilities() was wrong for the Aqua GUI on MacOS X.

    o   Using Rprof() with a non-writable 'file' argument is now a 
	non-fatal error and does not abort R.

    o   binom.test() did not deparse its arguments early enough such
        that the reported data were ugly if x was a table.

    o	Systems based on glibc, including those using R's substitute for
	strptime, were handling strptime("2001", "%Y") incorrectly, in
	some cases crashing.  R's substitute code has been corrected
	(but problems may remain if glibc is used).  See the ?strptime
	for what should happen (which is system-specific).

    o	untrace() after trace() failed if package 'methods' was attached.
	(PR#7301)

    o	summary.stepfun() was reporting for n > 6 summaries of the
	knots and levels as the actual values.  Both print() and
	summary() methods called the constant values "step heights",
	although they were not the heights of the steps.

    o	is.na/is.nan() were giving spurious warnings if applied to a
	raw vector.

    o	is.atomic() gave incorrect result (false) for a raw vector.

    o	rank() and order() accepted raw and list inputs, but did not
	give a sensible answer (always 1:n).  Similarly, partial sorts
	of a raw vector were accepted but did nothing.

    o	require() without a version argument tried for an unversioned
	load of a package even though a versioned install was already
	loaded.  This often led to a message that a required package
	was being loaded when it was not actually being loaded.

    o	str(<S4.object>) made use of attributes() instead of slot(),
	and hence didn't properly print NULL slots.

    o	contrib.url() now handles URLs ending in '/' correctly.

    o	str() removed any class from externalptr objects.

    o	logLik() and hence AIC() failed or gave incorrect answers 
	on "lm" fits with na.action = na.exclude (and perhaps other 
	na.actions's except na.omit and na.fail).

    o	pmax() and pmin() sometimes used NAs in internal subassignments,
	and sometimes these failed.

    o	Subassigning an expression, e.g. expr[2] <- 1, could leave an
	invalid object and so cause a segfault. (PR#7326)

    o	download/install.packages() would misbehave if there was more
	than one version of a package in a repository.

    o	sort(partial=) silently ignored some other arguments: using
	'decreasing' or 'index.return' or supplying a factor are now
	errors.
	
    o   The ave() function had trouble if the grouping contained 
        unused levels. 
	
    o   read.fwf() got confused by skip > 0 and could infinite loop
	under some circumstances.  (PR#7350)

    o	upgrade(x, ask = FALSE) was broken for a "packageStatus" object.

    o   Class "raw" had been omitted from the list of basic classes in
	the "methods" package and so could not be used in S4 classes.

    o   Function getGroupMembers(), part of the definition of S4
	classes, had been promised for release 2.0, but slipped through.


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907

_______________________________________________
R-announce at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-announce



From dan.bailey at bristol.ac.uk  Mon Nov 15 16:22:04 2004
From: dan.bailey at bristol.ac.uk (Dan Bailey)
Date: Mon, 15 Nov 2004 15:22:04 -0000
Subject: [R] Error whilst building packages
Message-ID: <19453171.1100532124@math-pc85.maths.bris.ac.uk>

Dear All,

I have been working on building a new version of the Wavethresh package for 
some time now. Having build a working version on Linux, I am getting the 
following error when checking on Windows:


C:\Rpackages\R\rw2000\bin>Rcmd check wavethresh
* checking for working latex ... OK
* using log directory 'C:/Rpackages/R/rw2000/bin/wavethresh.Rcheck'
* checking for file 'wavethresh/DESCRIPTION' ... OK
* checking if this is a source package ... OK

installing R.css in C:/Rpackages/R/rw2000/bin/wavethresh.Rcheck

make: *** [pkg-wavethresh] Error 255
*** Installation of wavethresh failed ***

Removing 'C:/Rpackages/R/rw2000/bin/wavethresh.Rcheck/wavethresh'
 ERROR
Installation failed.


I've worked through many other problems but have been stuck on this one for 
a while - can anyone enlighten me?

Thanks,
Dan


----------------------
Dan Bailey
School of Mathematics
Room 1.2



From stefan.albrecht at allianz.com  Mon Nov 15 16:49:55 2004
From: stefan.albrecht at allianz.com (stefan.albrecht@allianz.com)
Date: Mon, 15 Nov 2004 16:49:55 +0100
Subject: [R] Multivariate Sampling
Message-ID: <OF9B7689D3.C8F57514-ONC1256F4D.0056752B-C1256F4D.0056F7EA@inside.allianz.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041115/0bacc898/attachment.pl

From akniss at uwyo.edu  Mon Nov 15 16:53:12 2004
From: akniss at uwyo.edu (Andrew Kniss)
Date: Mon, 15 Nov 2004 08:53:12 -0700
Subject: [R] tsdiag() titles
Message-ID: <!~!AAAAAMry0xp5bcdHuCFwuJIk0LgkTiAA@uwyo.edu>

I am using the ts package to fit ARIMA models, and the tsdiag() function to
plot diagnostics.  In doing so I'm generating an awful lot of diagnostic
plots of different models and different data sets all within the same R
session.  So my question is, is there an option in tsdiag() similar to
<main="Title"> that I can use?  This would be quite helpful when I print out
the plots, so I can tell which plot goes with a particular data set and
model.  I can't seem to find any examples where this has been done, and no
options (other than gof.lag) are listed in the R manual.

library(ts)
data(tbills)       #Treasury Bills
attach(tbills)
ts.tbills<-ts(tbills)
diff.tbills<-diff(ts.tbills)  #Differenced Series
arima.diff.tbills.100<-arima(ts.tbills, order=c(1,0,0))
win.metafile("HW_ARIMA/tbill1.wmf")
  tsdiag(arima.diff.tbills.100, main="Treasury Bills")  #main= does not
work, is there a way to name the plot?
dev.off()

?
Thanks for any help or ideas.
?
Andrew Kniss
Assistant Research Scientist
University of Wyoming
Dept. 3354? 
1000 E. University Ave.
Laramie, WY  82071
(307) 766-3949
akniss at uwyo.edu


From p.dalgaard at biostat.ku.dk  Mon Nov 15 16:41:04 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Nov 2004 16:41:04 +0100
Subject: [R] R-2.0.1 is released
Message-ID: <x2oehzxgq7.fsf@biostat.ku.dk>

Whoops!

You can get it from

http://cran.r-project.org/src/base/R-2/R-2.0.1.tar.gz

(Notice the R-2 subdir)

I also forgot to sign it on behalf of The R Core Team.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907

_______________________________________________
R-announce at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-announce



From abunn at whrc.org  Mon Nov 15 17:27:42 2004
From: abunn at whrc.org (Andy Bunn)
Date: Mon, 15 Nov 2004 11:27:42 -0500
Subject: [R] tsdiag() titles
In-Reply-To: <!~!AAAAAMry0xp5bcdHuCFwuJIk0LgkTiAA@uwyo.edu>
Message-ID: <NEBBIPHDAMMOKDKPOFFIKEBPCMAA.abunn@whrc.org>

You can use title, but the result is unsatisfying:

> fit <- arima(lh, c(1,0,0))
> tsdiag(fit)
> title("junk")

Perhaps mtext with an appropriate par configuration?

HTH, Andy



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Andrew Kniss
> Sent: Monday, November 15, 2004 10:53 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] tsdiag() titles
>
>
> I am using the ts package to fit ARIMA models, and the tsdiag()
> function to
> plot diagnostics.  In doing so I'm generating an awful lot of diagnostic
> plots of different models and different data sets all within the same R
> session.  So my question is, is there an option in tsdiag() similar to
> <main="Title"> that I can use?  This would be quite helpful when
> I print out
> the plots, so I can tell which plot goes with a particular data set and
> model.  I can't seem to find any examples where this has been done, and no
> options (other than gof.lag) are listed in the R manual.
>
> library(ts)
> data(tbills)       #Treasury Bills
> attach(tbills)
> ts.tbills<-ts(tbills)
> diff.tbills<-diff(ts.tbills)  #Differenced Series
> arima.diff.tbills.100<-arima(ts.tbills, order=c(1,0,0))
> win.metafile("HW_ARIMA/tbill1.wmf")
>   tsdiag(arima.diff.tbills.100, main="Treasury Bills")  #main= does not
> work, is there a way to name the plot?
> dev.off()
>
>  
> Thanks for any help or ideas.
>  
> Andrew Kniss
> Assistant Research Scientist
> University of Wyoming
> Dept. 3354 
> 1000 E. University Ave.
> Laramie, WY  82071
> (307) 766-3949
> akniss at uwyo.edu
>
>



From ripley at stats.ox.ac.uk  Mon Nov 15 17:30:45 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 15 Nov 2004 16:30:45 +0000 (GMT)
Subject: [R] Error whilst building packages
In-Reply-To: <19453171.1100532124@math-pc85.maths.bris.ac.uk>
References: <19453171.1100532124@math-pc85.maths.bris.ac.uk>
Message-ID: <Pine.LNX.4.61.0411151629110.19084@gannet.stats>

On Mon, 15 Nov 2004, Dan Bailey wrote:

> Dear All,
>
> I have been working on building a new version of the Wavethresh package for 
> some time now. Having build a working version on Linux, I am getting the 
> following error when checking on Windows:
>
>
> C:\Rpackages\R\rw2000\bin>Rcmd check wavethresh
> * checking for working latex ... OK
> * using log directory 'C:/Rpackages/R/rw2000/bin/wavethresh.Rcheck'
> * checking for file 'wavethresh/DESCRIPTION' ... OK
> * checking if this is a source package ... OK
>
> installing R.css in C:/Rpackages/R/rw2000/bin/wavethresh.Rcheck
>
> make: *** [pkg-wavethresh] Error 255
> *** Installation of wavethresh failed ***
>
> Removing 'C:/Rpackages/R/rw2000/bin/wavethresh.Rcheck/wavethresh'
> ERROR
> Installation failed.
>
>
> I've worked through many other problems but have been stuck on this one for a 
> while - can anyone enlighten me?

Please try installing first -- you may get more informative error 
messages.  But almost certainly only of your tools is missing, and you 
need to cross-check the list in README.packages.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Nov 15 17:36:11 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 15 Nov 2004 16:36:11 +0000 (GMT)
Subject: [R] tsdiag() titles
In-Reply-To: <!~!AAAAAMry0xp5bcdHuCFwuJIk0LgkTiAA@uwyo.edu>
References: <!~!AAAAAMry0xp5bcdHuCFwuJIk0LgkTiAA@uwyo.edu>
Message-ID: <Pine.LNX.4.61.0411151631500.19084@gannet.stats>

Did you notice that tsdiag() plots about three plots and gives each a 
title?

You can add a title to the array of plots using title(outer=TRUE), 
surprisingly enough, but you will need to adjust the outer margins to make 
room for it.  Something like

par(oma=c(0,0,2,0))
fit <- arima(lh, c(1,0,0))
tsdiag(fit)
title("Some title or another", outer = TRUE)


On Mon, 15 Nov 2004, Andrew Kniss wrote:

> I am using the ts package to fit ARIMA models, and the tsdiag() function to
> plot diagnostics.  In doing so I'm generating an awful lot of diagnostic
> plots of different models and different data sets all within the same R
> session.  So my question is, is there an option in tsdiag() similar to
> <main="Title"> that I can use?  This would be quite helpful when I print out
> the plots, so I can tell which plot goes with a particular data set and
> model.  I can't seem to find any examples where this has been done, and no
> options (other than gof.lag) are listed in the R manual.
>
> library(ts)
> data(tbills)       #Treasury Bills
> attach(tbills)
> ts.tbills<-ts(tbills)
> diff.tbills<-diff(ts.tbills)  #Differenced Series
> arima.diff.tbills.100<-arima(ts.tbills, order=c(1,0,0))
> win.metafile("HW_ARIMA/tbill1.wmf")
>  tsdiag(arima.diff.tbills.100, main="Treasury Bills")  #main= does not
> work, is there a way to name the plot?
> dev.off()
>
> 
> Thanks for any help or ideas.
> 
> Andrew Kniss
> Assistant Research Scientist
> University of Wyoming
> Dept. 3354
> 1000 E. University Ave.
> Laramie, WY  82071
> (307) 766-3949
> akniss at uwyo.edu
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From huh at rti.org  Mon Nov 15 17:51:11 2004
From: huh at rti.org (Huh, Seungho)
Date: Mon, 15 Nov 2004 11:51:11 -0500
Subject: [R] how to obtain predicted labels for test data using "kernelpls"
Message-ID: <54535CBB16ABDA469D72C7614960429B0973DF@rtpwexc04.RCC_NT.RTI.ORG>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041115/28659de3/attachment.pl

From david.whiting at ncl.ac.uk  Mon Nov 15 18:17:39 2004
From: david.whiting at ncl.ac.uk (David Whiting)
Date: 15 Nov 2004 17:17:39 +0000
Subject: [R] R and fluxbox: 100% CPU usage
Message-ID: <m2sm7bqbf0.fsf@ganymede.ammp.or.tz>


Hi,

In July I reported a problem I was having with R and the fluxbox
windows manager (with Linux).  The interaction of R and fluxbox causes
CPU to go to 100% when trying to create even a simple plot. It is the
R process that is at 100%. R with any other window manager was fine,
and all other applications (that I use) seem to run okay in fluxbox.

I have updated both R (2.0.0) and fluxbox (0.9.10) and still have the
same problem. I am not very familiar with gdb but gave it a go. I ran
R and created a plot so that the R process was at 100% CPU usage. I
then started gdb and attached the R process and CPU usage returned to
normal (but I didn't get my plot). I detached the process and CPU
usage went back up to 100%, attached it again and it went back to
normal . . ..

I would love to get this sorted out because I would like to use
fluxbox, but can't because I am so dependent on R. 

Any ideas how I should proceed (incl. possibly spending more time
reading the gdb man page).

Thanks.

Dave

-- 
David Whiting
University of Newcastle upon Tyne, UK



From tfliao at uiuc.edu  Mon Nov 15 18:51:10 2004
From: tfliao at uiuc.edu (Tim F Liao)
Date: Mon, 15 Nov 2004 11:51:10 -0600
Subject: [R] glim in R?
Message-ID: <af4e9054.9fbc65f4.826da00@expms6.cites.uiuc.edu>

After some futile searches, I decided to ask the list to see 
if any of the sages out there would have an answer:

I have a function I wrote a few years ago in S, which calls 
glim numerous times.  I'd like to port it to R, but glm 
works differently from glim, which takes as part of its 
input an X design matrix.  I probably could write a function 
to convert glim to glm, but hope this wouldn't be 
necessary...

Tim Liao



From MSchwartz at MedAnalytics.com  Mon Nov 15 19:00:39 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon, 15 Nov 2004 12:00:39 -0600
Subject: [R] R and fluxbox: 100% CPU usage
In-Reply-To: <m2sm7bqbf0.fsf@ganymede.ammp.or.tz>
References: <m2sm7bqbf0.fsf@ganymede.ammp.or.tz>
Message-ID: <1100541639.29341.71.camel@horizons.localdomain>

On Mon, 2004-11-15 at 17:17 +0000, David Whiting wrote:
> Hi,
> 
> In July I reported a problem I was having with R and the fluxbox
> windows manager (with Linux).  The interaction of R and fluxbox causes
> CPU to go to 100% when trying to create even a simple plot. It is the
> R process that is at 100%. R with any other window manager was fine,
> and all other applications (that I use) seem to run okay in fluxbox.
> 
> I have updated both R (2.0.0) and fluxbox (0.9.10) and still have the
> same problem. I am not very familiar with gdb but gave it a go. I ran
> R and created a plot so that the R process was at 100% CPU usage. I
> then started gdb and attached the R process and CPU usage returned to
> normal (but I didn't get my plot). I detached the process and CPU
> usage went back up to 100%, attached it again and it went back to
> normal . . ..
> 
> I would love to get this sorted out because I would like to use
> fluxbox, but can't because I am so dependent on R. 
> 
> Any ideas how I should proceed (incl. possibly spending more time
> reading the gdb man page).

Dave,

Have you reported anything to the fluxbox folks for their consideration?

Not exactly the same thing, but there was a problem a while back (under
FC 1 if memory is correct) with Metacity which is GNOME's default window
manager and Xemacs. 

When using Xemacs (with ESS for example) and you maximized the Xemacs
window, CPU use went to 100% and Xemacs effectively locked. If you
simply increased the size of the Xemacs window via dragging with the
mouse, there was no problem.

Under other window managers (ie. Xfwm4 which I use with Xfce), there was
no problem. It was specific to Metacity under GNOME.

To my knowledge the bug is unresolved and there was some dispute as to
the source of the problem.

I am not familiar with fluxbox, but their developers might have some
insight into any particular interactions that might be the root cause of
your problem.

HTH,

Marc Schwartz



From sundar.dorai-raj at pdf.com  Mon Nov 15 19:04:05 2004
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Mon, 15 Nov 2004 10:04:05 -0800
Subject: [R] glim in R?
In-Reply-To: <af4e9054.9fbc65f4.826da00@expms6.cites.uiuc.edu>
References: <af4e9054.9fbc65f4.826da00@expms6.cites.uiuc.edu>
Message-ID: <4198EF95.3060109@pdf.com>



Tim F Liao wrote:

> After some futile searches, I decided to ask the list to see 
> if any of the sages out there would have an answer:
> 
> I have a function I wrote a few years ago in S, which calls 
> glim numerous times.  I'd like to port it to R, but glm 
> works differently from glim, which takes as part of its 
> input an X design matrix.  I probably could write a function 
> to convert glim to glm, but hope this wouldn't be 
> necessary...
> 
> Tim Liao
> 

Would glm.fit do what you need? It has much less overhead than glm and 
takes a matrix as it's first argument.

--sundar



From rolf at math.unb.ca  Mon Nov 15 19:28:50 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Mon, 15 Nov 2004 14:28:50 -0400 (AST)
Subject: [R] glim in R?
Message-ID: <200411151828.iAFISo30000566@erdos.math.unb.ca>

Tim Liao wrote:

> After some futile searches, I decided to ask the list to see if any
> of the sages out there would have an answer:
> 
> I have a function I wrote a few years ago in S, which calls glim
> numerous times.  I'd like to port it to R, but glm works differently
> from glim, which takes as part of its input an X design matrix.  I
> probably could write a function to convert glim to glm, but hope this
> wouldn't be necessary...

I doubt that you will get any joy in locating a glim() function for
R.  No-one would write one; that would be wheel-re-invention given the
existence of glm().

The glim() function is antiquated and is or should be deprecated.
The technology has moved beyond that.  What you really should do is
re-write your code to call glm().

If it is ***really*** necessary to pass the design matrix, you should
be able to

	o convert that matrix to a data frame, say ``ddd''
	o call glm(formula,data=ddd)
	o the formula would presumably be simply something
	  like ``y ~ .'' since the predictors would simply
	  be all of the individual columns of your data frame.

I can't see this as being particularly difficult recoding.  Or if you
insist, you could do just create your glim() function as:

	glim <- function(y,X,...) {
		X <- as.data.frame(X)
		glm(y~.,data=X,...)
	}

(I can't really remember the glim syntax, but ``glim(y,X,...)'' is
a reasonable facsimile.)

If your design matrix has a constant column you would want to strip
it out before passing the matrix to you glim() function.

				cheers,

					Rolf Turner
					rolf at math.unb.ca



From vograno at evafunds.com  Mon Nov 15 20:56:01 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Mon, 15 Nov 2004 11:56:01 -0800
Subject: [R] R on 64-bit Linux machine
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A57511D8@phost015.EVAFUNDS.intermedia.net>

Thanks to everyone for the info. It is very valuable. I am a little bit
uneasy about conflicting reports regarding RHEL 3, but I guess at this
point I just need to try and see. It's also very soothing to know that
there is an "official" 64-bit build on CRAN.

Thanks again for taking time to answer,
Vadim



From richars3 at lincoln.ac.nz  Tue Nov 16 00:31:26 2004
From: richars3 at lincoln.ac.nz (Sean David Richards)
Date: Tue, 16 Nov 2004 12:31:26 +1300
Subject: [R] Legend help needed
In-Reply-To: <1100520680.3195.77.camel@ndmpc126.ihs.ox.ac.uk>
References: <1100518770.3195.45.camel@ndmpc126.ihs.ox.ac.uk>
Message-ID: <4199F31D.24437.4AEFB71@tui.lincoln.ac.nz>

On 15 Nov 2004 at 12:11, Adaikalavan Ramasamy wrote:

> > Have you considered points() or lines() here ? You could simplify 
to
> > 
> > plot(0,1000, type="n", xlim=c(0,1500), ylim=c(1000,9000),
> >      xlab="Area above Notch (mm)", main="Size Effect Speciments")
> > 
> > n <- length(sfiles)
> > 
> > for (i in 1:n) {
> >   data <- read.csv(paste("../data/",sfiles[i],sep="")) 
> >   K    <- data[,8]
> >   AN   <- data[,3] * (data[,2] - data[,4])
> > 
> >   points( AN, K, pch=i, col=i )
> > }
> > 
> > legend( 1500, 9000, legend=paste("Data from", sfiles), pch=1:n, 
col=i:n)

Thanks this got me going on the right track. The code is a lot more 
concise as well :) 
Using locator() instead of x,y coord was suggested by Tom and that 
showed me where my problem was. The legend was being created just not 
where it would be visible.
I found this bit of code in the R-help archives and it makes thing a 
lot more straightforward when positioning a legend

## set the range of the usr coordinates to x = (0,1), y = (0,1) 
opar <- par(no.readonly=TRUE) 
par(usr=c(0,1,0,1)) 

## add the legend
legend(0.75,0.9,sub(".csv","",nfiles), pch=1:length(nfiles), cex=0.7)

Cheers

-- 
Sean Richards

C-fACS
P.O. Box 84, Lincoln University,
Canterbury, New Zealand
Phone:    (64)(3) 325-2811 ext 8636
Email:      richars3 at lincoln.ac.nz



From faberfedor at gmail.com  Tue Nov 16 00:34:07 2004
From: faberfedor at gmail.com (Faber Fedor)
Date: Mon, 15 Nov 2004 18:34:07 -0500
Subject: [R] logging messages
Message-ID: <300ccfa50411151534384d58e3@mail.gmail.com>

Hi all,

I'm trying to learn R from an non-statistician's POV. I've got a
statistician who uses R, but I'm the schmuck who has to integrate his
R functions into an automated process.

One of the things I would really like is the ability to log messages
to file, specifically using syslog on a Linux box.  (I also want to
write messages to STDOUT based on a command-line flag, but I figure
that can't be too hard to figure out. If it is too hard to figure out,
you'll be seeing another message from me shortly. :-)

Googling for "syslog site:www.R-project.org" however doesn't give me
much hope. From what I gather, you guys just pipe everything to
STDOUT.

Is there a syslog module, er, extension for R?  If so, how do I find it?

TIA

--

Faber
http://www.linuxnj.com



From yzhang4 at turing.une.edu.au  Tue Nov 16 00:47:39 2004
From: yzhang4 at turing.une.edu.au (Yuandan Zhang)
Date: Tue, 16 Nov 2004 10:47:39 +1100
Subject: [R] anova, multiple comparison
In-Reply-To: <4199F31D.24437.4AEFB71@tui.lincoln.ac.nz>
References: <1100518770.3195.45.camel@ndmpc126.ihs.ox.ac.uk>
	<4199F31D.24437.4AEFB71@tui.lincoln.ac.nz>
Message-ID: <20041116104739.20bd052a.yzhang4@turing.une.edu.au>

hi,

I try to used R to do one-way anova. 

here is the simple code

f1<- lm (y ~ block, data=yd)

there are 8 levels of factor block, 

I also want produce multiple pairwise  comparisons for the 8 levels of block, inlcuding mean and std err for each of the 8 lelevls. It is tidious to do pair test. I looked the manuals, find no clues. any suggest?

Regards,

Yuandan



From hjb at pdq.com  Tue Nov 16 01:03:24 2004
From: hjb at pdq.com (Heather J. Branton)
Date: Mon, 15 Nov 2004 19:03:24 -0500
Subject: [R] Barplot difficulties
Message-ID: <419943CC.1060708@pdq.com>

Hello. I am an R newbie struggling to learn and use R . I have read many 
portions of the R Reference Manual, as well as the FAQs. Given that I 
learn something new each time, I know I might be missing something 
obvious. But I appeal to your good nature to help me through this 
initial problem.

I have attached a pdf file to demonstrate what I desire and have listed 
what my data looks like in Excel (below). Following is the data and 
script I developed - which does not provide what I want.

My immediate goal is to create a barplot in R similar to the attached 
pdf chart. But I am stuck on several problems. First, I would like to 
have 2 labels below the barplot - one label for each bar and one for 
each group of bars. Second, I would like to vary color by group (instead 
of by bar). I assume that I need to do use some sort of syntax within 
the color option but have not yet figured it out. I have made two 
different plot attempts -- one resulting in the bars being grouped 
appropriately but missing the labels below the x-axis; the other giving 
me the individual labels but not grouped as I need.

This is my version information:
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    1
minor    9.1
year     2004
month    06
day      21
language R

Thank you in advance for any help you can give me.

...heather


----------------------------------

Spreadsheet data to obtain attached pdf:

Year  Group    Rate
2002  Alpha    17.6
      Beta     13.0
      Gamma     8.9
      Delta     7.1
      Epsilon   6.0
      Zeta      5.4
      Eta       3.7
      Theta     2.5

2003  Beta     11.6
      Epsilon   8.7
      Zeta      6.4
      Theta     3.3
      Xi       10.2
      Omicron   7.9

2004  Alpha     8.9
      Gamma     8.0
      Delta     7.7
      Episilon  6.9
      Eta       6.1
      Xi        3.8
      Omicron   1.2

----------------------------------

R data set ("sample.dat"):

Alpha,Beta,Gamma,Delta,Epsilon,Zeta,Eta,Theta,Xi,Omicron
2002,17.6,13.0,8.9,7.1,6.0,5.4,3.7,2.5,0,0
2003,0,11.6,0,0,8.7,6.4,0,3.3,10.2,7.9
2004,8.9,0,8.0,7.7,6.9,0,6.1,0,3.8,1.2

----------------------------------

My R code attempt:

# Read in data
sample <- t(read.table("sample.dat", sep=",", header=T))

# Set color palette
shade <- palette(c("cyan2","yellow","magenta1"))

# Set plot limits:
ymax <- as.integer(max(sample)+1)

# Bar graph (get grouped plot but not grouped color nor individual 
labels below)
par(mar=c(6,4,6,4))
barplot(sample, beside=T, xlab="Test and Year", ylab="Rate", font.lab=3, 
axis.lty=1, col=shade)
legend(1,1, rownames(sample), xjust=-5.4, yjust=-2, col=shade, lty=1, lwd=2)
title(main="Rate by Test and Year", outer=F, font.main=2, line=3)

# Bar graph (get individual labels below but not grouped by year)
barplot(t(sample), beside=T, xlab="Test and Year", ylab="Rate", 
font.lab=3, axis.lty=1, las=2, col=shade)
legend(1,1, colnames(sample), xjust=-6, yjust=-7, col=shade, lty=1, lwd=2)
title(main="Rate by Test and Year", outer=F, font.main=2, line=3)

----------------------------------


-------------- next part --------------
A non-text attachment was scrubbed...
Name: sample.pdf
Type: application/pdf
Size: 36182 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20041115/ba818ad9/sample.pdf

From hodgess at gator.uhd.edu  Tue Nov 16 01:06:17 2004
From: hodgess at gator.uhd.edu (Erin Hodgess)
Date: Mon, 15 Nov 2004 18:06:17 -0600
Subject: [R] Multiple comparisons	
Message-ID: <200411160006.iAG06Hu29248@gator.dt.uh.edu>

Someone just asked about multiple comparisons in R
You could look at pairwise.t.test in the stats library
or p.adjust (also in stats)

Hope this helps!

Sincerely,
Erin Hodgess
mailto: hodgess at gator.uhd.edu



From mp2370 at columbia.edu  Tue Nov 16 01:37:17 2004
From: mp2370 at columbia.edu (Martina Pavlicova, PhD)
Date: Mon, 15 Nov 2004 19:37:17 -0500
Subject: [R] lme, two random effects, poisson distribution
Message-ID: <1100565437.41994bbd811fa@cubmail.cc.columbia.edu>


Hello,

I have a dataset concerning slugs. For each slug, the number of
pumps per one time slot was counted. The number of pumps follows
Bi(30, p) where p is very small, thus could be approximated by
Poisson dist. (# of pumps is very often = 0)

The slugs were observed during 12 time slots which are correlated in
time as AR(1). The time slots are divided into two categories:
      Resting time slots (the first 10)
      Excited time slots (the last 2)

I used model:

************pumps_ti = state_t + slugs_i + error_ti

slugs and error are normaly distributed

      pumps_ti - # of pumps for i-th animal and t-th time slot
      x_t - order of the time slot (x_1 = 1, ..., x_12 = 12)
      state_t - state_t = 0 for resting time slots (t=1,...,10)
              state_t = 1 for excited time slots (t=11,12)
      slugs_i - ith animal, where i = 1,...,25

I would like to find out if the # of pumps depends on the variable
state, assuming the correlation AR(1) between x_t and slugs being a
random-effect on intercept.

slugs.lmedata <- groupedData(pumps ~ state | slugs,
data=as.data.frame(data.slugs))
cs <- corAR1(form= ~ x|slugs)
res1 <- lme(pumps ~ state, random = ~1 | slugs, data=slugs.lmedata,
cor=cs)

---------------------------------

Now, I would like to add a complication to the model: The slugs were
observed in batches:
Batch_1 = {slugs_1, slugs_2, slugs_3}
Batch_2 = {slugs_5, slugs_6}
Batch_3 = {slugs_7, slugs_8, slugs_9, slugs_10}
Batch_4 = {slugs_11}
.
.
.
Batch_12 = {slugs_24, slugs_25}
Notice that there are 12 batches, and the number of slugs in each
batch differ, from 1 slug to 4 slugs.

I consider batch to be another random-effect on intercept. Thus I
fit model:

************pumps_tij = state_t + slugs_i + batch_ij + error_tij

Slugs, batch and error are normally distributed, but slugs and batch
are not nested factors.

I had fit following (however I'm not sure if that is right):

slugs.lmedataB <- groupedData(pumps ~ state | slugs/batch,
data=as.data.frame(data.slugs))
csB <- corAR1(form= ~ x|slugs/batch)
res1B <- lme(pumps ~ state, random = ~1 | slugs/batch,
data=slugs.lmedataB, cor=csB)

----------------------------------------

QUESTIONS:
1) Are my models right? Do I model the res1B model properly?

2) Until now, I have assumed that the number of pumps follow the
normal distribution. However I know that the variable pumps is
distributed along Poisson distribution. How can I model that? I
would like to use LOG or SQRT transformation, but I don't know how.

*****************************************

Thank you very much for all your help.

Martina Pavlicova

-- 
Department of Biostatistics
Columbia University
722 W. 168th Street, 6th floor
New York, NY 10032

Phone: (212) 305-9405
Fax: (212) 305-9408
Email: mp2370 at columbia.edu



From tfliao at uiuc.edu  Tue Nov 16 02:44:56 2004
From: tfliao at uiuc.edu (Tim F Liao)
Date: Mon, 15 Nov 2004 19:44:56 -0600
Subject: [R] glim in R?
Message-ID: <4d9009a5.9fe7c56d.8229500@expms6.cites.uiuc.edu>

Great suggestions, and it looks like either suggestion 
should work, although the output from glm may not conform to 
those from glim, thus some more code there perhaps.

Many thanks,

Tim

---- Original message ----
>Date: Mon, 15 Nov 2004 14:28:50 -0400 (AST)
>From: Rolf Turner <rolf at math.unb.ca>  
>Subject: Re: [R] glim in R?  
>To: tfliao at uiuc.edu
>Cc: r-help at stat.math.ethz.ch
>
>Tim Liao wrote:
>
>> After some futile searches, I decided to ask the list to 
see if any
>> of the sages out there would have an answer:
>> 
>> I have a function I wrote a few years ago in S, which 
calls glim
>> numerous times.  I'd like to port it to R, but glm works 
differently
>> from glim, which takes as part of its input an X design 
matrix.  I
>> probably could write a function to convert glim to glm, 
but hope this
>> wouldn't be necessary...
>
>I doubt that you will get any joy in locating a glim() 
function for
>R.  No-one would write one; that would be wheel-re-
invention given the
>existence of glm().
>
>The glim() function is antiquated and is or should be 
deprecated.
>The technology has moved beyond that.  What you really 
should do is
>re-write your code to call glm().
>
>If it is ***really*** necessary to pass the design matrix, 
you should
>be able to
>
>	o convert that matrix to a data frame, say ``ddd''
>	o call glm(formula,data=ddd)
>	o the formula would presumably be simply something
>	  like ``y ~ .'' since the predictors would simply
>	  be all of the individual columns of your data 
frame.
>
>I can't see this as being particularly difficult recoding.  
Or if you
>insist, you could do just create your glim() function as:
>
>	glim <- function(y,X,...) {
>		X <- as.data.frame(X)
>		glm(y~.,data=X,...)
>	}
>
>(I can't really remember the glim syntax, but ``glim
(y,X,...)'' is
>a reasonable facsimile.)
>
>If your design matrix has a constant column you would want 
to strip
>it out before passing the matrix to you glim() function.
>
>				cheers,
>
>					Rolf Turner
>					rolf at math.unb.ca



From MSchwartz at MedAnalytics.com  Tue Nov 16 03:01:56 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon, 15 Nov 2004 20:01:56 -0600
Subject: [R] Barplot difficulties
In-Reply-To: <419943CC.1060708@pdq.com>
References: <419943CC.1060708@pdq.com>
Message-ID: <1100570516.19620.8.camel@horizons.localdomain>

On Mon, 2004-11-15 at 19:03 -0500, Heather J. Branton wrote:
> Hello. I am an R newbie struggling to learn and use R . I have read many 
> portions of the R Reference Manual, as well as the FAQs. Given that I 
> learn something new each time, I know I might be missing something 
> obvious. But I appeal to your good nature to help me through this 
> initial problem.
> 
> I have attached a pdf file to demonstrate what I desire and have listed 
> what my data looks like in Excel (below). Following is the data and 
> script I developed - which does not provide what I want.
> 
> My immediate goal is to create a barplot in R similar to the attached 
> pdf chart. But I am stuck on several problems. First, I would like to 
> have 2 labels below the barplot - one label for each bar and one for 
> each group of bars. Second, I would like to vary color by group (instead 
> of by bar). I assume that I need to do use some sort of syntax within 
> the color option but have not yet figured it out. I have made two 
> different plot attempts -- one resulting in the bars being grouped 
> appropriately but missing the labels below the x-axis; the other giving 
> me the individual labels but not grouped as I need.

<snip>

How about something like this:

# Don't use 'sample' for the name here, as sample() is a function
MyData <- t(read.table("sample.dat", sep= "," , header = TRUE))

# These may be closer to the PDF chart colors
# You need to repeat them to color each group the same, rather
# than alternating bar colors
MyCols <- rep(c("lightcyan","cornsilk","lavender"), each = 10)

# adjust the margins
par(mar = c(7, 5, 6, 4))

# Now do the barplot:
# Note barplot() returns the bar midpoints in 'mp'
# use 'names.arg' for the individual bar names from MyData
# set 'las = 2' for vertical labels
# set 'ylim' to c(0, 20) for the y axis range
# set 'yaxt = "n"' to not draw the y axis tick marks
mp <- barplot(MyData, beside = TRUE, col = MyCols, 
              main = "Rate by Group and Year", 
              ylab = "Rate", 
              names.arg = rep(rownames(MyData), 3), las = 2,
              cex.names = 0.75, ylim = c(0,20), yaxt = "n")

# Now set up the y axis tick marks and labels
ticks <- seq(0, 20, 2)
axis(2, at = ticks, las = 1, 
     labels = formatC(ticks, format = "f", digits = 1))

# Draw a box around the whole thing
box()

# Now draw the years. Note from ?barplot that colMeans(mp) are
# the group midpoints
mtext(side = 1, at = colMeans(mp), line = 3.5, text = colnames(MyData))

# Now draw the x axis label
mtext(side = 1, line = 5.5, text = "Test and Year")


Hope that gets you what you need. You can adjust the font sizes, etc. as
you require.

Note that unlike Excel, the 0 (zero) columns are not dropped. :-)

HTH,

Marc Schwartz



From tvictor at dolphin.upenn.edu  Tue Nov 16 04:26:30 2004
From: tvictor at dolphin.upenn.edu (Timothy W. Victor)
Date: Mon, 15 Nov 2004 22:26:30 -0500
Subject: [R] Pairwise Distances -- How to vectorize the loop
Message-ID: <41997366.3000107@dolphin.upenn.edu>

R-List,

I'm trying to compute pairwise distances among pairs of observations, 
which each pair containing data from 2 groups. There are more than 
100000 unique pairs. I have programmed a distance function that has 
three parameters, a vector of covariates from the ith observation in 
Group 1, a vector of covarites from the jth observation in Group 2, and 
a weighting matrix.

I have used expand.grid to create a matrix containing all possible pairs 
of Keys (unique identifiers) from Group 1 and Group 2 and a placeholder 
for each pairwise distance in an effort to pre-allocate memory.

The data containing the covariates are currently in a matrix that looks 
like:

x1 x2 x3 ... xn Key Group

where x1,...,xn are the covariates of interest. The Key values 
correspond to the row number.

I'm trying to figure out a way to calculate all of the distances by 
vectorizing a loop. I'm trying to avoid the nested-loop paradigm. I'm 
sure I'm missing something quite obvious.

Any insights would be appreciated.

Best,

Tim



From andy_liaw at merck.com  Mon Nov 15 19:24:14 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 15 Nov 2004 13:24:14 -0500
Subject: [R] how to obtain predicted labels for test data using
	"kerne lpls"
Message-ID: <3A822319EB35174CA3714066D590DCD50994E2FF@usrymx25.merck.com>

You need to do some extra work if you want to do classification with a
regression method.  One simple way to do classification with PLS is to code
the classes as 0s and 1s (assuming there are only two classes) or -1s and
1s, fit the model, then threshold the prediction; e.g., those with predicted
values < 0.5 (in the 0/1 coding) get labeled as 0s.  There's a predict()
method for mvr objects, and that's what you need to use to get prediction on
test set.

There's one more complication:  The CV done internal to mvr is optimizing
the MSE (because it rightly thinks it has a regression problem), but that
will almost certainly not be the thing to do for classification.  You have
two choices:  Do your own CV, or modify code in pls.pcr to do the "right" CV
when given classification data.

You might want to look at the `gpls' package, which started out as part of
BioConductor, then was made available on CRAN, but now seem to have move
back to BioConductor.  It treats classification problems in a more `natural'
way.

HTH,
Andy

> From: Huh, Seungho
> 
> Dear members,
> 
> My name is Seungho Huh. I am a statistician who tries to use 
> the Kernel
> PLS method in a classification problem. I am sending this email to ask
> you something about the "kernelpls" function in R (pls.pcr package).
> 
> I would like to obtain the predicted Y values for test data, using the
> Kernel PLS method. Let's take the example in the R help:
> 
> > data(NIR)
> > attach(NIR)
> > NIR.kernelpls <- mvr(Xtrain, Ytrain, 1:6, validation = "CV",
> method="kernelPLS")
> 
>  
> 
> How can we get the predicted Y values ("Ypred") for Xtest in 
> this case?
> As far as I checked, there is no parameter to specify the test data in
> "mvr" or "pls". I, therefore, thought about the "kernelpls" 
> function as
> follows:
> 
>  
> 
> > Kernelpls(Xtrain, Ytrain, ncomp = 21, Xtest)
> 
>  
> 
> Is this the correct way of getting Ypred for Xtest? I am 
> afraid that it
> says in the help of "kernelpls", "this function should not be called
> directly, but through the generic pls function with the argument
> method="kernel" (default)." I would really appreciate it if 
> you can give
> some advice on this.
> 
>  
> 
> Thanks a lot for your time.
> 
>  
> 
> Seungho Huh, Ph.D.
> 
> Research Statistician
> 
> RTI International
> 
> North Carolina, USA
> 
>  
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From mp2370 at columbia.edu  Tue Nov 16 06:22:03 2004
From: mp2370 at columbia.edu (Martina Pavlicova, PhD)
Date: Tue, 16 Nov 2004 00:22:03 -0500
Subject: [R] Re: lme, two random effects, poisson distribution
In-Reply-To: <1100565437.41994bbd811fa@cubmail.cc.columbia.edu>
References: <1100565437.41994bbd811fa@cubmail.cc.columbia.edu>
Message-ID: <1100582523.41998e7bbdb94@cubmail.cc.columbia.edu>


Hello all,

I think that with the help of Mark Irwin, we solved the problem:

We fit the model using glmmPQL and instead of using variable,
'state', we model the independent fixed-effect 'state' as
I(x>10); i.e. it's 0 for resting time slots and 1 for
excited times slots.

Here is the code:
=================
slugs.lmedata <- groupedData(y ~ I(x>10) | slugs/batch,
data=as.data.frame(data.slugs))

csnew <- corAR1(form = ~ x | slugs/batch)

res.glmm <- glmmPQL(pumps ~ I(x>10), random = ~1|slugs/batch, family
= poisson, data=slugs.lmedata, correlation=csnew)

The summary of the model looks like this:
=========================================
> summary(res.glmm)
Linear mixed-effects model fit by maximum likelihood
 Data: slugs.lmedata
       AIC      BIC    logLik
  1265.934 1288.157 -626.9672

Random effects:
 Formula: ~1 | slugs
        (Intercept)
StdDev:   0.6959097

 Formula: ~1 | batch %in% slugs
        (Intercept) Residual
StdDev:    1.138034 1.304958

Correlation Structure: AR(1)
 Formula: ~x | slugs/batch
 Parameter estimate(s):
       Phi
-0.2820534
Variance function:
 Structure: fixed weights
 Formula: ~invwt
Fixed effects: y ~ I(x > 10)
                   Value Std.Error  DF   t-value p-value
(Intercept)   -0.5004731 0.2006282 142 -2.494531  0.0138
I(x > 10)TRUE  1.2553644 0.2494294 142  5.032944  0.0000
 Correlation:
              (Intr)
I(x > 10)TRUE -0.32

Standardized Within-Group Residuals:
       Min         Q1        Med         Q3        Max
-2.0856605 -0.5439464 -0.3606277  0.1855235  5.7606962

Number of Observations: 300
Number of Groups:
           slugs batch %in% slugs
              25              157

*******************************************************

I think this is the right model. But I am interested very much in
your opinions. I do not use mixed-effect modelling very often.

Thank your for all your help.

Martina

Quoting "Martina Pavlicova, PhD" <mp2370 at columbia.edu>:

>
> Hello,
>
> I have a dataset concerning slugs. For each slug, the number of
> pumps per one time slot was counted. The number of pumps follows
> Bi(30, p) where p is very small, thus could be approximated by
> Poisson dist. (# of pumps is very often = 0)
>
> The slugs were observed during 12 time slots which are correlated
> in
> time as AR(1). The time slots are divided into two categories:
>       Resting time slots (the first 10)
>       Excited time slots (the last 2)
>
> I used model:
>
> ************pumps_ti = state_t + slugs_i + error_ti
>
> slugs and error are normaly distributed
>
>       pumps_ti - # of pumps for i-th animal and t-th time slot
>       x_t - order of the time slot (x_1 = 1, ..., x_12 = 12)
>       state_t - state_t = 0 for resting time slots (t=1,...,10)
>               state_t = 1 for excited time slots (t=11,12)
>       slugs_i - ith animal, where i = 1,...,25
>
> I would like to find out if the # of pumps depends on the
> variable
> state, assuming the correlation AR(1) between x_t and slugs being
> a
> random-effect on intercept.
>
> slugs.lmedata <- groupedData(pumps ~ state | slugs,
> data=as.data.frame(data.slugs))
> cs <- corAR1(form= ~ x|slugs)
> res1 <- lme(pumps ~ state, random = ~1 | slugs,
> data=slugs.lmedata,
> cor=cs)
>
> ---------------------------------
>
> Now, I would like to add a complication to the model: The slugs
> were
> observed in batches:
> Batch_1 = {slugs_1, slugs_2, slugs_3}
> Batch_2 = {slugs_5, slugs_6}
> Batch_3 = {slugs_7, slugs_8, slugs_9, slugs_10}
> Batch_4 = {slugs_11}
> .
> .
> .
> Batch_12 = {slugs_24, slugs_25}
> Notice that there are 12 batches, and the number of slugs in each
> batch differ, from 1 slug to 4 slugs.
>
> I consider batch to be another random-effect on intercept. Thus I
> fit model:
>
> ************pumps_tij = state_t + slugs_i + batch_ij + error_tij
>
> Slugs, batch and error are normally distributed, but slugs and
> batch
> are not nested factors.
>
> I had fit following (however I'm not sure if that is right):
>
> slugs.lmedataB <- groupedData(pumps ~ state | slugs/batch,
> data=as.data.frame(data.slugs))
> csB <- corAR1(form= ~ x|slugs/batch)
> res1B <- lme(pumps ~ state, random = ~1 | slugs/batch,
> data=slugs.lmedataB, cor=csB)
>
> ----------------------------------------
>
> QUESTIONS:
> 1) Are my models right? Do I model the res1B model properly?
>
> 2) Until now, I have assumed that the number of pumps follow the
> normal distribution. However I know that the variable pumps is
> distributed along Poisson distribution. How can I model that? I
> would like to use LOG or SQRT transformation, but I don't know
> how.
>
> *****************************************
>
> Thank you very much for all your help.
>
> Martina Pavlicova
>
> --
> Department of Biostatistics
> Columbia University
> 722 W. 168th Street, 6th floor
> New York, NY 10032
>
> Phone: (212) 305-9405
> Fax: (212) 305-9408
> Email: mp2370 at columbia.edu
>



From ripley at stats.ox.ac.uk  Tue Nov 16 08:43:44 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 16 Nov 2004 07:43:44 +0000 (GMT)
Subject: [R] anova, multiple comparison
In-Reply-To: <20041116104739.20bd052a.yzhang4@turing.une.edu.au>
References: <1100518770.3195.45.camel@ndmpc126.ihs.ox.ac.uk>
	<4199F31D.24437.4AEFB71@tui.lincoln.ac.nz>
	<20041116104739.20bd052a.yzhang4@turing.une.edu.au>
Message-ID: <Pine.LNX.4.61.0411160739110.16006@gannet.stats>

On Tue, 16 Nov 2004, Yuandan Zhang wrote:

> I try to used R to do one-way anova.
>
> here is the simple code
>
> f1<- lm (y ~ block, data=yd)
>
> there are 8 levels of factor block,
>
> I also want produce multiple pairwise comparisons for the 8 levels of 
> block, inlcuding mean and std err for each of the 8 lelevls. It is 
> tidious to do pair test. I looked the manuals, find no clues. any 
> suggest?


?TukeyHSD
library(help=multcomp)

Chapter 6 of MASS4, a book recommended in the FAQ, has worked examples, 
and its scripts are included in the VR package for R.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Nov 16 09:05:38 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 16 Nov 2004 08:05:38 +0000 (GMT)
Subject: [R] logging messages
In-Reply-To: <300ccfa50411151534384d58e3@mail.gmail.com>
References: <300ccfa50411151534384d58e3@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0411160749150.16006@gannet.stats>

On Mon, 15 Nov 2004, Faber Fedor wrote:

> Hi all,
>
> I'm trying to learn R from an non-statistician's POV. I've got a
> statistician who uses R, but I'm the schmuck who has to integrate his
> R functions into an automated process.

Defining a POV (undefined, as is `schmuck') as `not something' is really 
non-communication.  Please do read the R posting guide and its references, 
and try to formulate questions in standard (and polite) English.  Also, 
check the correct list to use as described in that posting guide -- R-help 
is not intended for questions involving programming concepts (like 
`syslog', which needs explanation even there).

> One of the things I would really like is the ability to log messages
> to file, specifically using syslog on a Linux box.  (I also want to
> write messages to STDOUT based on a command-line flag, but I figure
> that can't be too hard to figure out. If it is too hard to figure out,
> you'll be seeing another message from me shortly. :-)

It's not clear what you mean by this.  If you mean send warnings and 
messages to stdout (what is `STDOUT'?) then `why?'  You deal with this the 
same way as any other Linux application that uses stderr for warning/error 
messages.

To `tee' output, see ?sink, or indeed `man tee'.

R is principally an interactive program used for exploration.  You haven't 
told us what you are trying to do, and you may be better off embedding R 
in some other front end to give you more control.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From petr.pikal at precheza.cz  Tue Nov 16 09:11:33 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 16 Nov 2004 09:11:33 +0100
Subject: [R] Pairwise Distances -- How to vectorize the loop
In-Reply-To: <41997366.3000107@dolphin.upenn.edu>
Message-ID: <4199C445.464.4EA201@localhost>

Hi

>From what you write it seems to me a work for outer(). Did you 
try it?

Cheers
Petr


On 15 Nov 2004 at 22:26, Timothy W. Victor wrote:

> R-List,
> 
> I'm trying to compute pairwise distances among pairs of observations,
> which each pair containing data from 2 groups. There are more than
> 100000 unique pairs. I have programmed a distance function that has
> three parameters, a vector of covariates from the ith observation in
> Group 1, a vector of covarites from the jth observation in Group 2,
> and a weighting matrix.
> 
> I have used expand.grid to create a matrix containing all possible
> pairs of Keys (unique identifiers) from Group 1 and Group 2 and a
> placeholder for each pairwise distance in an effort to pre-allocate
> memory.
> 
> The data containing the covariates are currently in a matrix that
> looks like:
> 
> x1 x2 x3 ... xn Key Group
> 
> where x1,...,xn are the covariates of interest. The Key values 
> correspond to the row number.
> 
> I'm trying to figure out a way to calculate all of the distances by
> vectorizing a loop. I'm trying to avoid the nested-loop paradigm. I'm
> sure I'm missing something quite obvious.
> 
> Any insights would be appreciated.
> 
> Best,
> 
> Tim
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From abitbol at sent.com  Tue Nov 16 09:55:28 2004
From: abitbol at sent.com (Jean-Louis Abitbol)
Date: Tue, 16 Nov 2004 09:55:28 +0100
Subject: [R] Adding mean and SEM in Hmisc ecdf
Message-ID: <1100595328.25310.208760971@webmail.messagingengine.com>

Dear R Gurus,

Sorry if this has been asked before but I did not find it in the
archives.

I would like to add a horizontal display of mean and SEM on Hmisc ecdf
plots done by group (ie variate~treatment).

Has anyone written some code to do that ?

Thanks and kind regards,

Jean-Louis



From ales.ziberna at guest.arnes.si  Tue Nov 16 10:02:33 2004
From: ales.ziberna at guest.arnes.si (=?iso-8859-1?Q?Ales_Ziberna?=)
Date: Tue, 16 Nov 2004 10:02:33 +0100
Subject: [R] how can draw probability density plot?
References: <CF99BBA00A3F5D4183C2FA33089FBC340221E06D@EX01.staff.smu.edu.sg>
Message-ID: <006301c4cbbb$0e73f5e0$1409f9c2@ales>

I hope this is the direct operation you are looking for:

 curve(dnorm(x),from=-3,to=3)

Best regards,

Ales Ziberna

----- Original Message ----- 
From: "LONG Yu" <ylong at smu.edu.sg>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, November 15, 2004 1:26 PM
Subject: [R] how can draw probability density plot?


Dear support,

I want to draw a probability density plot in R. For example, I provide the 
mean and variance of a normal distribution, then R can provide me the 
probability density plot. Now I always generate random numbers of normal 
distribution and calculate their dnorm(mu, var), finally plot them. I am 
eager to know some directly operation.

Thank you for your attention.

I am looking forward to hearing from you soon.



Best regards,

Long Yu




--------------------------------------------------------------------------------


______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html



From dlavecchia at tiscali.it  Tue Nov 16 10:12:42 2004
From: dlavecchia at tiscali.it (dlavecchia@tiscali.it)
Date: Tue, 16 Nov 2004 10:12:42 +0100
Subject: [R] question about AIC
Message-ID: <418222A60003E994@mail-6.tiscali.it>

Hi everybody,
we are a pool of phd students and we have a question about AIC. We are interested
in calculating the AIC for a mixture model on galaxies data. So far we have
found AIC works only for regression models, whereas we need AIC for a mixture
of Normal with mean, sd and weights given by our  EM algorithm.
May you help us?
thanks in advance
                        

__________________________________________________________________
Tiscali Adsl 2 Mega Free: naviga gratis tutto l'anno!
Supera tutti i limiti di velocita' con Tiscali Adsl 2 Mega Free.
Sei libero da costi fissi e, se ti abboni entro il 25 novembre,
navighi gratis fino al 31 dicembre 2004 e non paghi il costo di adesione.
http://abbonati.tiscali.it/adsl/



From braud at lyon.cemagref.fr  Tue Nov 16 10:27:58 2004
From: braud at lyon.cemagref.fr (Isabelle Braud)
Date: Tue, 16 Nov 2004 10:27:58 +0100
Subject: [R] From S-Plus to R?
Message-ID: <4.2.0.58.20041116102230.011ca010@popexterne.lyon.cemagref.fr>

Dear collegues,

I am using S-Plus for many years and is thinking of moving to R.
In the FAQ I did not manage to find an answer to the following questions:
Is it easy to move and transform functions, written under S-plus to 
functions under R?

Furthermore, S-Plus has interesting functions to deal with regular and 
irregular times series (rts, its, ts.plot, ts.lines, ts.points, 
aggregate.rts, etc..) and the possibility to plot, on the same graphic, 
regular time series with different frequencies and even irregular time 
series. Is it still possible in R?

Thanks in advance for your answers.
Best wishes
Isabelle Braud
***************************************************************
Isabelle BRAUD
Charg??e de recherche au LTHE-Grenoble/CEMAGREF-Lyon

CEMAGREF
Unit?? de Recherche Hydrologie-Hydraulique
3 bis, Quai Chauveau, CP 220
69336 Lyon Cedex 9

Tel: 04 72 20 87 78 (from abroad 33 4 72 20 87 78)
Fax: 04 78 47 78 75 (from abroad 33 4 78 47 78 75)
E-mail: braud at lyon.cemagref.fr
            ou Isabelle.Braud at hmg.inpg.fr



From stefan.albrecht at allianz.com  Tue Nov 16 10:33:22 2004
From: stefan.albrecht at allianz.com (stefan.albrecht@allianz.com)
Date: Tue, 16 Nov 2004 10:33:22 +0100
Subject: [R] Multivariate Sampling
Message-ID: <OF2EEACE10.D310F8A2-ONC1256F4E.003400A8-C1256F4E.00347DDD@inside.allianz.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041116/cf0c2b2a/attachment.pl

From ripley at stats.ox.ac.uk  Tue Nov 16 10:33:54 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 16 Nov 2004 09:33:54 +0000 (GMT)
Subject: [R] question about AIC
In-Reply-To: <418222A60003E994@mail-6.tiscali.it>
References: <418222A60003E994@mail-6.tiscali.it>
Message-ID: <Pine.LNX.4.61.0411160927290.17536@gannet.stats>

On Tue, 16 Nov 2004 dlavecchia at tiscali.it wrote:

> we are a pool of phd students and we have a question about AIC. We are interested
> in calculating the AIC for a mixture model on galaxies data. So far we have
> found AIC works only for regression models, whereas we need AIC for a mixture
> of Normal with mean, sd and weights given by our  EM algorithm.

What is the problem?  The concept AIC or the function AIC()?  If the 
concept, it was originally developed in the area of AR(p) time series 
models, using asymptotic theory for MLEs, and you need to check that the 
concepts apply to your situation (the original papers do not).

Note there are some fundamental problems here: the likelihood for a 
mixture of normals is unbounded, the MLE corresponding to degenerate 
(zero-variance) components.  And AIC is about maximized likelihoods, with 
maxima in the interior of the space.  So you need to take some 
professional statistical advice about the concepts.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dimitris.rizopoulos at med.kuleuven.ac.be  Tue Nov 16 10:35:42 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Tue, 16 Nov 2004 10:35:42 +0100
Subject: [R] question about AIC
References: <418222A60003E994@mail-6.tiscali.it>
Message-ID: <00c101c4cbbf$a4006d60$0540210a@www.domain>

did you try google? e.g., with key word: `AIC mixture models'. 
Moreover, look at package `flexmix' (i.e., help(package="flexmix")) 
whose usage is illustrated in a recent article in JSS:

http://www.jstatsoft.org/v11/i08/v11i08.pdf

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: <dlavecchia at tiscali.it>
To: <R-help at stat.math.ethz.ch>
Sent: Tuesday, November 16, 2004 10:12 AM
Subject: [R] question about AIC


> Hi everybody,
> we are a pool of phd students and we have a question about AIC. We 
> are interested
> in calculating the AIC for a mixture model on galaxies data. So far 
> we have
> found AIC works only for regression models, whereas we need AIC for 
> a mixture
> of Normal with mean, sd and weights given by our  EM algorithm.
> May you help us?
> thanks in advance
>
>
> __________________________________________________________________
> Tiscali Adsl 2 Mega Free: naviga gratis tutto l'anno!
> Supera tutti i limiti di velocita' con Tiscali Adsl 2 Mega Free.
> Sei libero da costi fissi e, se ti abboni entro il 25 novembre,
> navighi gratis fino al 31 dicembre 2004 e non paghi il costo di 
> adesione.
> http://abbonati.tiscali.it/adsl/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From stefan.albrecht at allianz.com  Tue Nov 16 10:52:19 2004
From: stefan.albrecht at allianz.com (stefan.albrecht@allianz.com)
Date: Tue, 16 Nov 2004 10:52:19 +0100
Subject: [R] From S-Plus to R
Message-ID: <OF66DE22A6.74754D54-ONC1256F4E.0035062F-C1256F4E.00363A35@inside.allianz.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041116/0811e855/attachment.pl

From n.wayne at laposte.net  Tue Nov 16 11:03:28 2004
From: n.wayne at laposte.net (wayne)
Date: Tue, 16 Nov 2004 11:03:28 +0100
Subject: [R] UML programming
Message-ID: <I79NXS$5839BF48DF40084B84FAC5CEF2D68146@laposte.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041116/edfb419a/attachment.pl

From max.marinucci at ya.com  Tue Nov 16 12:15:22 2004
From: max.marinucci at ya.com (max.marinucci)
Date: Tue, 16 Nov 2004 12:15:22 +0100
Subject: [R] question about AIC
References: <418222A60003E994@mail-6.tiscali.it>
	<00c101c4cbbf$a4006d60$0540210a@www.domain>
Message-ID: <000b01c4cbcd$913e7420$e6d71a50@maxmad9rubu4nk>

I would also mention the "mixreg" package on CRAN.
Regards
m.

**********************************************
Massimiliano Marinucci
http://personales.ya.com/max_mar/
Ph.D Candidate in Economics
Fundamentos del Analisis Econ??mico II
(Econom??a Cuantitativa)
Facultad de CC.EE.
Universidad Complutense Madrid
Campus de Somosaguas
Madrid - Spain
**********************************************

----- Original Message -----
From: "Dimitris Rizopoulos" <dimitris.rizopoulos at med.kuleuven.ac.be>
To: <dlavecchia at tiscali.it>
Cc: <r-help at stat.math.ethz.ch>
Sent: Tuesday, November 16, 2004 10:35 AM
Subject: Re: [R] question about AIC


> did you try google? e.g., with key word: `AIC mixture models'.
> Moreover, look at package `flexmix' (i.e., help(package="flexmix"))
> whose usage is illustrated in a recent article in JSS:
>
> http://www.jstatsoft.org/v11/i08/v11i08.pdf
>
> I hope it helps.
>
> Best,
> Dimitris
>
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
>
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/16/396887
> Fax: +32/16/337015
> Web: http://www.med.kuleuven.ac.be/biostat
>      http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
>
>
> ----- Original Message -----
> From: <dlavecchia at tiscali.it>
> To: <R-help at stat.math.ethz.ch>
> Sent: Tuesday, November 16, 2004 10:12 AM
> Subject: [R] question about AIC
>
>
> > Hi everybody,
> > we are a pool of phd students and we have a question about AIC. We
> > are interested
> > in calculating the AIC for a mixture model on galaxies data. So far
> > we have
> > found AIC works only for regression models, whereas we need AIC for
> > a mixture
> > of Normal with mean, sd and weights given by our  EM algorithm.
> > May you help us?
> > thanks in advance
> >
> >
> > __________________________________________________________________
> > Tiscali Adsl 2 Mega Free: naviga gratis tutto l'anno!
> > Supera tutti i limiti di velocita' con Tiscali Adsl 2 Mega Free.
> > Sei libero da costi fissi e, se ti abboni entro il 25 novembre,
> > navighi gratis fino al 31 dicembre 2004 e non paghi il costo di
> > adesione.
> > http://abbonati.tiscali.it/adsl/
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>



From blindglobe at gmail.com  Tue Nov 16 12:33:53 2004
From: blindglobe at gmail.com (A.J. Rossini)
Date: Tue, 16 Nov 2004 12:33:53 +0100
Subject: [R] UML programming
In-Reply-To: <I79NXS$5839BF48DF40084B84FAC5CEF2D68146@laposte.net>
References: <I79NXS$5839BF48DF40084B84FAC5CEF2D68146@laposte.net>
Message-ID: <1abe3fa90411160333784b2019@mail.gmail.com>

Sure, you could do it in a UML way.  But I know of no tools yet that
will, and more importantly, the object-orientation style (generic
functions, i.e. similar to CLOS) isn't the easiest to use in that way.


On Tue, 16 Nov 2004 11:03:28 +0100, wayne <n.wayne at laposte.net> wrote:
> hello,
> I'd like to know if it is possible to use R in a UML way by creating classes, methods,etc...
> If yes, can you please give me links for documentation and example ?
> 
> thanks
> 
> 
>        [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 

best,
-tony

---
A.J. Rossini
blindglobe at gmail.com



From Jin.Li at csiro.au  Tue Nov 16 13:06:07 2004
From: Jin.Li at csiro.au (Jin.Li@csiro.au)
Date: Tue, 16 Nov 2004 22:06:07 +1000
Subject: [R] How to remove x, y labels from a plot
Message-ID: <2BEE99D7F6F1484EBDD1D22167385E75375ACE@exqld1-ath.nexus.csiro.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041116/db834b34/attachment.pl

From francoisromain at free.fr  Tue Nov 16 13:18:02 2004
From: francoisromain at free.fr (francoisromain@free.fr)
Date: Tue, 16 Nov 2004 13:18:02 +0100
Subject: [R] How to remove x, y labels from a plot
In-Reply-To: <2BEE99D7F6F1484EBDD1D22167385E75375ACE@exqld1-ath.nexus.csiro.au>
References: <2BEE99D7F6F1484EBDD1D22167385E75375ACE@exqld1-ath.nexus.csiro.au>
Message-ID: <1100607482.4199effa5bdb8@imp1-q.free.fr>

try :

plot(rnorm(20),rnorm(20),axes=F,xlab="",ylab="")


Selon Jin.Li at csiro.au:

> Hi there,
>
>
>
> I need to plot an illustrative figure without ticks, x, y labels in R. I
> managed to get the ticks removed, but had no luck with x, y labels.
>
>
>
> Any suggestions would be much appreciated.
>
>
>
> Jin Li
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ahenningsen at email.uni-kiel.de  Tue Nov 16 13:32:32 2004
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Tue, 16 Nov 2004 13:32:32 +0100
Subject: [R] How to remove x, y labels from a plot
In-Reply-To: <2BEE99D7F6F1484EBDD1D22167385E75375ACE@exqld1-ath.nexus.csiro.au>
References: <2BEE99D7F6F1484EBDD1D22167385E75375ACE@exqld1-ath.nexus.csiro.au>
Message-ID: <200411161332.32691.ahenningsen@email.uni-kiel.de>

Hi Jin Li,

does
   plot( 1:100, rnorm(100), ann=FALSE, xaxt="n", yaxt="n" )
produce what you want?

Arne


On Tuesday 16 November 2004 13:06, Jin.Li at csiro.au wrote:
> Hi there,
>
>
>
> I need to plot an illustrative figure without ticks, x, y labels in R. I
> managed to get the ticks removed, but had no luck with x, y labels.
>
>
>
> Any suggestions would be much appreciated.
>
>
>
> Jin Li
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From f.harrell at vanderbilt.edu  Tue Nov 16 01:51:00 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Mon, 15 Nov 2004 18:51:00 -0600
Subject: [R] Barplot difficulties
In-Reply-To: <419943CC.1060708@pdq.com>
References: <419943CC.1060708@pdq.com>
Message-ID: <41994EF4.8090402@vanderbilt.edu>

Heather J. Branton wrote:
> Hello. I am an R newbie struggling to learn and use R . I have read many 
> portions of the R Reference Manual, as well as the FAQs. Given that I 
> learn something new each time, I know I might be missing something 
> obvious. But I appeal to your good nature to help me through this 
> initial problem.
> 
> I have attached a pdf file to demonstrate what I desire and have listed 
> what my data looks like in Excel (below). Following is the data and 
> script I developed - which does not provide what I want.

....

Dear Heather - Please read Bill Cleveland's book The Elements of 
Graphing Data.  A MUCH better plot can be produced.  And let time be one 
of the first variables to vary.



-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From f.harrell at vanderbilt.edu  Tue Nov 16 14:11:25 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 16 Nov 2004 07:11:25 -0600
Subject: [R] Adding mean and SEM in Hmisc ecdf
In-Reply-To: <1100595328.25310.208760971@webmail.messagingengine.com>
References: <1100595328.25310.208760971@webmail.messagingengine.com>
Message-ID: <4199FC7D.3090401@vanderbilt.edu>

Jean-Louis Abitbol wrote:
> Dear R Gurus,
> 
> Sorry if this has been asked before but I did not find it in the
> archives.
> 
> I would like to add a horizontal display of mean and SEM on Hmisc ecdf
> plots done by group (ie variate~treatment).
> 
> Has anyone written some code to do that ?
> 
> Thanks and kind regards,
> 
> Jean-Louis

It is customary to show quantiles on ecdfs, and the functions make that 
easy.  You can add other points or reference lines to an existing plot 
easily if using the non-lattice ecdf.  To do it with the lattice version 
ecdf.formula you will have to change its default panel function.

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From zseby at fokus.fraunhofer.de  Tue Nov 16 14:31:49 2004
From: zseby at fokus.fraunhofer.de (Tanja Zseby)
Date: Tue, 16 Nov 2004 14:31:49 +0100
Subject: [R] violinplot options
Message-ID: <419A0145.5080306@fokus.fraunhofer.de>

Hi,

I am using the function  vioplot() to generate violin plots. Now I would 
like to add a label to the y axix and a title to the diagram.
Just setting ylab didnt work. Is it possible to set such options for the 
function ?
I tried also with the function simple.violinplot, but also with this I 
couldnt set the options.

Kind Regards
Tanja



From michael.watson at bbsrc.ac.uk  Tue Nov 16 15:45:24 2004
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Tue, 16 Nov 2004 14:45:24 -0000
Subject: [R] Simple plot() question
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950121B889@iahce2knas1.iah.bbsrc.reserved>

Hi

First a simple question to do with plot().  How do I change the x-axis
labels on a plot?

For example, I am plotting each row of a matrix, and I want each row to
be a line on my graph.  Simple really.  Eg:

plot(sg[1,], type="l")

When I do this, the x-axis is labelled 1:38, as I have 38 columns in my
matrix.  When I do:

plot(sg[1,order(sg[1,])], type="l")

Ideally I would like the x-axis labels to reflect the new order of the
columns, but they're still numbered 1:38... I've read the plot() docs
and the par() docs and can't figure it out - I know I'm missing
something obvious, but what?

Cheers

Mick

Michael Watson
Head of Informatics
Institute for Animal Health,
Compton Laboratory,
Compton,
Newbury,
Berkshire RG20 7NN
UK

Phone : +44 (0)1635 578411 ext. 2535
Mobile: +44 (0)7990 827831
E-mail: michael.watson at bbsrc.ac.uk



From michael.watson at bbsrc.ac.uk  Tue Nov 16 15:49:49 2004
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Tue, 16 Nov 2004 14:49:49 -0000
Subject: [R] Difference between two correlation matrices
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950121B88A@iahce2knas1.iah.bbsrc.reserved>

Hi 

Now a more theoretical question.  I have two correlation matrices - one
of a set of variables under a particular condition, the other of the
same set of variables under a different condition.  Is there a
statistical test I can use to see if these correlation matrices are
"different"?

Thanks
Mick



From jmacdon at med.umich.edu  Tue Nov 16 16:01:59 2004
From: jmacdon at med.umich.edu (James W. MacDonald)
Date: Tue, 16 Nov 2004 10:01:59 -0500
Subject: [R] Simple plot() question
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950121B889@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E950121B889@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <419A1667.4010508@med.umich.edu>

michael watson (IAH-C) wrote:
> Hi
> 
> First a simple question to do with plot().  How do I change the x-axis
> labels on a plot?
> 
> For example, I am plotting each row of a matrix, and I want each row to
> be a line on my graph.  Simple really.  Eg:
> 
> plot(sg[1,], type="l")
> 
> When I do this, the x-axis is labelled 1:38, as I have 38 columns in my
> matrix.  When I do:
> 
> plot(sg[1,order(sg[1,])], type="l")
> 
> Ideally I would like the x-axis labels to reflect the new order of the
> columns, but they're still numbered 1:38... I've read the plot() docs
> and the par() docs and can't figure it out - I know I'm missing
> something obvious, but what?

You want to add xaxt="n" so the x-axis labels are not printed. Then add 
your new labels using axis(1, at=1:38, labels=<some vector of label names>)

HTH,

Jim


> 
> Cheers
> 
> Mick
> 
> Michael Watson
> Head of Informatics
> Institute for Animal Health,
> Compton Laboratory,
> Compton,
> Newbury,
> Berkshire RG20 7NN
> UK
> 
> Phone : +44 (0)1635 578411 ext. 2535
> Mobile: +44 (0)7990 827831
> E-mail: michael.watson at bbsrc.ac.uk
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
James W. MacDonald
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109



From dimitris.rizopoulos at med.kuleuven.ac.be  Tue Nov 16 16:00:48 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Tue, 16 Nov 2004 16:00:48 +0100
Subject: [R] Simple plot() question
References: <8975119BCD0AC5419D61A9CF1A923E950121B889@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <001c01c4cbed$0e6feb80$0540210a@www.domain>

try something like

plot(..., axes=FALSE)
axis(2); axis(1, labels=c("your labels"))

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "michael watson (IAH-C)" <michael.watson at bbsrc.ac.uk>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, November 16, 2004 3:45 PM
Subject: [R] Simple plot() question


> Hi
>
> First a simple question to do with plot().  How do I change the 
> x-axis
> labels on a plot?
>
> For example, I am plotting each row of a matrix, and I want each row 
> to
> be a line on my graph.  Simple really.  Eg:
>
> plot(sg[1,], type="l")
>
> When I do this, the x-axis is labelled 1:38, as I have 38 columns in 
> my
> matrix.  When I do:
>
> plot(sg[1,order(sg[1,])], type="l")
>
> Ideally I would like the x-axis labels to reflect the new order of 
> the
> columns, but they're still numbered 1:38... I've read the plot() 
> docs
> and the par() docs and can't figure it out - I know I'm missing
> something obvious, but what?
>
> Cheers
>
> Mick
>
> Michael Watson
> Head of Informatics
> Institute for Animal Health,
> Compton Laboratory,
> Compton,
> Newbury,
> Berkshire RG20 7NN
> UK
>
> Phone : +44 (0)1635 578411 ext. 2535
> Mobile: +44 (0)7990 827831
> E-mail: michael.watson at bbsrc.ac.uk
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From abunn at whrc.org  Tue Nov 16 16:13:32 2004
From: abunn at whrc.org (Andy Bunn)
Date: Tue, 16 Nov 2004 10:13:32 -0500
Subject: [R] Simple plot() question
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950121B889@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <NEBBIPHDAMMOKDKPOFFIMEDACMAA.abunn@whrc.org>

Hi Mick:

I'm a little unsure if this is what you are after but does this do it?

foo.mat <- matrix(rnorm(100), nrow = 10, ncol = 10)
plot(foo.mat[1,], type="l", xlab = "Crud", ylab = "More Crud")
plot(foo.mat[1,order(foo.mat[1,])], type="l", xaxt = "n", xlab = "Crud",
ylab = "More Crud")
axis(1, at=1:length(foo.mat[1,]), labels= order(foo.mat[1,]))


Seems like a barplot might be a nice way to go too:
barplot(foo.mat[1,order(foo.mat[1,])], names.arg = order(foo.mat[1,]))


A reproducible example would help the list see what you are after as I might
be totally off on this.
HTH, Andy


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of michael watson
> (IAH-C)
> Sent: Tuesday, November 16, 2004 9:45 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Simple plot() question
>
>
> Hi
>
> First a simple question to do with plot().  How do I change the x-axis
> labels on a plot?
>
> For example, I am plotting each row of a matrix, and I want each row to
> be a line on my graph.  Simple really.  Eg:
>
> plot(sg[1,], type="l")
>
> When I do this, the x-axis is labelled 1:38, as I have 38 columns in my
> matrix.  When I do:
>
> plot(sg[1,order(sg[1,])], type="l")
>
> Ideally I would like the x-axis labels to reflect the new order of the
> columns, but they're still numbered 1:38... I've read the plot() docs
> and the par() docs and can't figure it out - I know I'm missing
> something obvious, but what?
>
> Cheers
>
> Mick
>
> Michael Watson
> Head of Informatics
> Institute for Animal Health,
> Compton Laboratory,
> Compton,
> Newbury,
> Berkshire RG20 7NN
> UK
>
> Phone : +44 (0)1635 578411 ext. 2535
> Mobile: +44 (0)7990 827831
> E-mail: michael.watson at bbsrc.ac.uk
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From mrufino at ipimar.ualg.pt  Tue Nov 16 16:31:26 2004
From: mrufino at ipimar.ualg.pt (Marta Rufino)
Date: Tue, 16 Nov 2004 15:31:26 -0000
Subject: [R] Difference between two correlation matrices
References: <8975119BCD0AC5419D61A9CF1A923E950121B88A@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <00d801c4cbf1$56346e10$d31a0e0a@rufino>

Hello,

I don't know if it is applicable in your case, but have you tried Mantel
test?
You can use it to determine significant correlation between two matrices,
using Pearsion, Spearman or Kendal correlation indices.

Hope it helps,
All the best,
Marta


----- Original Message ----- 
From: "michael watson (IAH-C)" <michael.watson at bbsrc.ac.uk>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, November 16, 2004 2:49 PM
Subject: [R] Difference between two correlation matrices


> Hi
>
> Now a more theoretical question.  I have two correlation matrices - one
> of a set of variables under a particular condition, the other of the
> same set of variables under a different condition.  Is there a
> statistical test I can use to see if these correlation matrices are
> "different"?
>
> Thanks
> Mick
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From R.C.Gill at maths.soton.ac.uk  Tue Nov 16 16:43:12 2004
From: R.C.Gill at maths.soton.ac.uk (Roger Gill)
Date: Tue, 16 Nov 2004 15:43:12 -0000
Subject: [R] deleting a file
Message-ID: <004401c4cbf2$fad82550$a3284e98@maths.soton.ac.uk>

Dear all,

I have a very simple question.  Simple, that is, if you know the answer.

I wish to delete a file in a given directory after having first checked its
existence.  I issue the following commands

path<-'c:example/R/'

Thus creating the directory.

indicator<-length(grep(filename,path))

If indicator is greater than zero then the file exists.  Now I wish to
remove this file.  However, I would like to make the code compatible for
Linux/Windows and Macs.

In Linux I would issue the command

system('rm paste(path,'filename',sep='')')

and this works just fine.  It does not however work for windows, and I have
no idea whether it would work on a mac.

Is there a single command thay escapes me?

Best Wishes

Roger Gill



From jgentry at jimmy.harvard.edu  Tue Nov 16 16:48:53 2004
From: jgentry at jimmy.harvard.edu (Jeff Gentry)
Date: Tue, 16 Nov 2004 10:48:53 -0500 (EST)
Subject: [R] deleting a file
In-Reply-To: <004401c4cbf2$fad82550$a3284e98@maths.soton.ac.uk>
Message-ID: <Pine.SOL.4.20.0411161048340.9053-100000@santiam.dfci.harvard.edu>

> In Linux I would issue the command
> system('rm paste(path,'filename',sep='')')
> and this works just fine.  It does not however work for windows, and I have
> no idea whether it would work on a mac.
> Is there a single command thay escapes me?

Yes.

help("file.remove")



From massimiliano.copetti at uni-bocconi.it  Tue Nov 16 16:48:54 2004
From: massimiliano.copetti at uni-bocconi.it (Massimiliano Copetti)
Date: Tue, 16 Nov 2004 16:48:54 +0100
Subject: [R] memory allocation
Message-ID: <1100620134.419a21661d05a@webmail.uni-bocconi.it>


Dear sirs,

I'm using the Splancs package to compute standard errors of the estimates of a 
spatio-temporal k function.
When I use as spatial and temporal distances too long vectors (respectively 60 
and 80 entries) for a dataset of 1000 observations, R gives me the message

Error: cannot allocate vector of size 180000 Kb

Reached total allocation of 512 Mb.

I ran the function memory.size() and obtained 392000000 more or less.

Can anyone help me? 

Thanks in advance.

Massimiliano Copetti



-- 
Massimiliano Copetti, PhD Student
Institute of Quantitative Methods
L.Bocconi University
Viale Isonzo 25
20135 Milano (Italy)
http://www.unibocconi.it
http://spazioinwind.libero.it/maxcop78



From sabolk at zoominternet.net  Tue Nov 16 16:52:44 2004
From: sabolk at zoominternet.net (Keith J Sabol)
Date: Tue, 16 Nov 2004 10:52:44 -0500
Subject: [R] User package - Simple Ports
Message-ID: <000b01c4cbf4$4fd53c90$151f10ac@Patton>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041116/42577089/attachment.pl

From rpeng at jhsph.edu  Tue Nov 16 16:53:24 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Tue, 16 Nov 2004 10:53:24 -0500
Subject: [R] deleting a file
In-Reply-To: <004401c4cbf2$fad82550$a3284e98@maths.soton.ac.uk>
References: <004401c4cbf2$fad82550$a3284e98@maths.soton.ac.uk>
Message-ID: <419A2274.2050006@jhsph.edu>

You may be interested in one of dir.create(), file.exists(), or unlink().

-roger

Roger Gill wrote:
> Dear all,
> 
> I have a very simple question.  Simple, that is, if you know the answer.
> 
> I wish to delete a file in a given directory after having first checked its
> existence.  I issue the following commands
> 
> path<-'c:example/R/'
> 
> Thus creating the directory.
> 
> indicator<-length(grep(filename,path))
> 
> If indicator is greater than zero then the file exists.  Now I wish to
> remove this file.  However, I would like to make the code compatible for
> Linux/Windows and Macs.
> 
> In Linux I would issue the command
> 
> system('rm paste(path,'filename',sep='')')
> 
> and this works just fine.  It does not however work for windows, and I have
> no idea whether it would work on a mac.
> 
> Is there a single command thay escapes me?
> 
> Best Wishes
> 
> Roger Gill
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From dray at biomserv.univ-lyon1.fr  Tue Nov 16 16:57:36 2004
From: dray at biomserv.univ-lyon1.fr (Stephane DRAY)
Date: Tue, 16 Nov 2004 10:57:36 -0500
Subject: [R] Difference between two correlation matrices
In-Reply-To: <00d801c4cbf1$56346e10$d31a0e0a@rufino>
References: <8975119BCD0AC5419D61A9CF1A923E950121B88A@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <5.2.1.1.0.20041116104956.03d6a308@biomserv.univ-lyon1.fr>

One solution is to use procruste analysis.
Perform a principal coordinate analysis on your data (cmdscale) to obtain 
two individuals by variables tables.
Then, you can perform procruste rotation and test to know if the two 
matrice tell the same thing. It is more powerful than mantel test.
As you have correlations, I suppose than you can consider your original 
variables rather than correlation and cmdscale



Functions are available in the ade4 package:

procuste(ade4)          Simple Procruste Rotation between two sets of
                         points
procuste.randtest(ade4)
                         Monte-Carlo Test on the sum of the singular
                         values of a procustean rotation (in C).
procuste.rtest(ade4)    Monte-Carlo Test on the sum of the singular
                         values of a procustean rotation (in R).

A reference on the analysis:

<http://www.steph280.freesurf.fr/files/articles/Ecosci2003.pdf>S. Dray, D. 
Chessel, J. Thioulouse : Procrustean co-inertia analysis for the linking of 
ecological tables. 
Ecoscience<http://www.steph280.freesurf.fr/files/articles/Ecosci2003.pdf>, 
2003, vol. 10, 110-119.

available at :

http://www.steph280.freesurf.fr/files/articles/Ecosci2003.pdf

Hope this helps !



At 10:31 16/11/2004, Marta Rufino wrote:
>Hello,
>
>I don't know if it is applicable in your case, but have you tried Mantel
>test?
>You can use it to determine significant correlation between two matrices,
>using Pearsion, Spearman or Kendal correlation indices.
>
>Hope it helps,
>All the best,
>Marta
>
>
>----- Original Message -----
>From: "michael watson (IAH-C)" <michael.watson at bbsrc.ac.uk>
>To: <r-help at stat.math.ethz.ch>
>Sent: Tuesday, November 16, 2004 2:49 PM
>Subject: [R] Difference between two correlation matrices
>
>
> > Hi
> >
> > Now a more theoretical question.  I have two correlation matrices - one
> > of a set of variables under a particular condition, the other of the
> > same set of variables under a different condition.  Is there a
> > statistical test I can use to see if these correlation matrices are
> > "different"?
> >
> > Thanks
> > Mick
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
>http://www.R-project.org/posting-guide.html
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

St??phane DRAY
-------------------------------------------------------------------------------------------------- 

D??partement des Sciences Biologiques
Universit?? de Montr??al, C.P. 6128, succursale centre-ville
Montr??al, Qu??bec H3C 3J7, Canada

Tel : (514) 343-6111 poste 1233         Fax : (514) 343-2293
E-mail : stephane.dray at umontreal.ca
-------------------------------------------------------------------------------------------------- 

Web                                          http://www.steph280.freesurf.fr/



From buddanaak at ornl.gov  Tue Nov 16 17:04:02 2004
From: buddanaak at ornl.gov (Aruna Buddana)
Date: Tue, 16 Nov 2004 11:04:02 -0500
Subject: [R] Rggobi-color maps and colors
Message-ID: <419A24F2.5060906@ornl.gov>

Hello,
I was wondering if the function
get(set)ColorMap.ggobi() is working for anyone..
The function is not defined when I tried to work on it.

Also, I would really like to know, if I can increase the number of 
colors in any of the color scheme, if possible.
We only got 12 colors (maximum?) to look at the clusters, but what if I 
got as many as 25 clusters..
How do people look at many clusters?

Any help is appreciated.

Thank you

Sincerely
Aruna



From mrufino at ipimar.ualg.pt  Tue Nov 16 17:17:44 2004
From: mrufino at ipimar.ualg.pt (Marta Rufino)
Date: Tue, 16 Nov 2004 16:17:44 -0000
Subject: [R] Fourier analysis
References: <1100620134.419a21661d05a@webmail.uni-bocconi.it>
Message-ID: <010701c4cbf7$cde1e270$d31a0e0a@rufino>

Dear colegues,

Is there any function in R for calculating the four coefficients of each
harmonic in fourier analysis, from a set of x,y coordinates is given? Is it
possible to do Fourier analysis?
Has anyone done contour analysis using R?

Any help wil be apretiated,

Thank you very much,
All the best,
Marta



From ripley at stats.ox.ac.uk  Tue Nov 16 17:18:58 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 16 Nov 2004 16:18:58 +0000 (GMT)
Subject: [R] memory allocation
In-Reply-To: <1100620134.419a21661d05a@webmail.uni-bocconi.it>
References: <1100620134.419a21661d05a@webmail.uni-bocconi.it>
Message-ID: <Pine.LNX.4.61.0411161616110.3117@gannet.stats>

There is more to that message you have not reproduced.

I am pretty sure you are on Windows, and you need to read the rw-FAQ (as 
the posting guide asks you too).  See Q2.7.

On Tue, 16 Nov 2004, Massimiliano Copetti wrote:

> I'm using the Splancs package to compute standard errors of the 
> estimates of a spatio-temporal k function. When I use as spatial and 
> temporal distances too long vectors (respectively 60 and 80 entries) for 
> a dataset of 1000 observations, R gives me the message
>
> Error: cannot allocate vector of size 180000 Kb
>
> Reached total allocation of 512 Mb.
>
> I ran the function memory.size() and obtained 392000000 more or less.

Yes, but see its help page for the relevant usage.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sundar.dorai-raj at pdf.com  Tue Nov 16 17:30:10 2004
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 16 Nov 2004 10:30:10 -0600
Subject: [R] User package - Simple Ports
In-Reply-To: <000b01c4cbf4$4fd53c90$151f10ac@Patton>
References: <000b01c4cbf4$4fd53c90$151f10ac@Patton>
Message-ID: <419A2B12.4040207@pdf.com>



Keith J Sabol wrote:

> R experts,
> 
> I created a package X for R 1.8 that I want to re-install for 2.0.1.  In
> addition to downloading/installing Perl and Rtools as directed in the
> README.packages file and ensuring that they appear in my path, I have
> modified X's DESCRIPTION file to include all required fields.
> Additionally I have deleted  the "built" line since the  make process
> will add that.  The source contains no Fortran/C/C++ and resides in   .
> R/rw2000/src/library
> 
>  
> 
>>From .R/rw2000/src/gnuwin32>  make X -X seems to run the make utility,
> but the package is never installed.
> 
>  
> 
> In R 1.8  manually added the build line and copied the package directory
> into the library tree and library(X) worked without a formal install (As
> you would guess this was a work-around for my earlier and evidently
> persistent lack of success in my package installation endeavors.).  The
> same work-around approach seems not to work with 2.0.
> 
>  

Keith,

Have you read "Writing R Extensions" that comes with each R 
distribution? In it you will see that to CHECK/BUILD/INSTALL packages 
you need to use R CMD. As in

 > R CMD CHECK X
<snip check messages>
 > R CMD INSTALL X
<snip install messages>
 > R CMD BUILD X # to build the package into X_<version>.tar.gz
<snip build messages>
 > R CMD BUILD --binary X # to compile the package into X_<version>.zip
<snip build messages>

This is no different from the 1.8.x way of building packages on Windows 
except I think back then Rcmd was the equivalent of R CMD.

My version of R is currently R-2.0.0patched on Windows 2000.

--sundar



From Achim.Zeileis at wu-wien.ac.at  Tue Nov 16 17:30:49 2004
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 16 Nov 2004 17:30:49 +0100
Subject: [R] deleting a file
In-Reply-To: <004401c4cbf2$fad82550$a3284e98@maths.soton.ac.uk>
References: <004401c4cbf2$fad82550$a3284e98@maths.soton.ac.uk>
Message-ID: <20041116173049.6af1c77b.Achim.Zeileis@wu-wien.ac.at>

On Tue, 16 Nov 2004 15:43:12 -0000 Roger Gill wrote:

> Dear all,
> 
> I have a very simple question.  Simple, that is, if you know the
> answer.

Yes, look at
  ?file.remove
Z

> I wish to delete a file in a given directory after having first
> checked its existence.  I issue the following commands
> 
> path<-'c:example/R/'
> 
> Thus creating the directory.
> 
> indicator<-length(grep(filename,path))
> 
> If indicator is greater than zero then the file exists.  Now I wish to
> remove this file.  However, I would like to make the code compatible
> for Linux/Windows and Macs.
> 
> In Linux I would issue the command
> 
> system('rm paste(path,'filename',sep='')')
> 
> and this works just fine.  It does not however work for windows, and I
> have no idea whether it would work on a mac.
> 
> Is there a single command thay escapes me?
> 
> Best Wishes
> 
> Roger Gill
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From lauraholt_983 at hotmail.com  Tue Nov 16 17:31:12 2004
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Tue, 16 Nov 2004 10:31:12 -0600
Subject: [R] Changing zeros to NAs in a data frame
Message-ID: <BAY12-F17tvElzXqHof00011084@hotmail.com>

Dear R People:

I have a data frame with some columns that are numeric and some which are 
factors.

There are zeros in the numeric columns and I would like to change them to 
NAs.  However, there are zeros in some of the factor columns, and I would 
like them to be left alone.

Is there a "global" way to do this, please?  I was thinking about use the 
results from "str" but am not sure.

R Version 2.0.0 Windows.

Thanks in advance.
Sincerely,
Laura Holt
mailto: lauraholt_983 at hotmail.com


Security. http://clinic.mcafee.com/clinic/ibuy/campaign.asp?cid=3963



From dmrocke at ucdavis.edu  Tue Nov 16 17:48:26 2004
From: dmrocke at ucdavis.edu (David Rocke)
Date: Tue, 16 Nov 2004 08:48:26 -0800 (PST)
Subject: [R] R 2.0.0 Installation Problem
Message-ID: <200411161648.iAGGmQwB009099@mercury.cipic.ucdavis.edu>

I and my students have been having an odd problem with this release,
which is that packages are disappearing. After installation the
package is found with the library command, but later in the same
session or in a later session, the library command returns a not found
error. Then later it is back. Happening on both Windows and OS X,
mostly but not entirely with Bioconductor packages.

David


-----------------------------------------------------------------------
| David M. Rocke, Professor               Phone:  (530) 752-0510      |
| Division of Biostatistics (Medicine) and        (530) 752-7368      |
| Department of Applied Science (Engineering)                         |
| Co-Director of IDAV                     FAX:    (530) 752-8894      |
| University of California, Davis         E-mail: dmrocke at ucdavis.edu |
| Davis, CA 95616-8553                 www.cipic.ucdavis.edu/~dmrocke |



From lauraholt_983 at hotmail.com  Tue Nov 16 17:59:00 2004
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Tue, 16 Nov 2004 10:59:00 -0600
Subject: [R] changing character to a vector name
Message-ID: <BAY12-F174FTA7OIkUq0001157c@hotmail.com>

Dear R People:

I would like to generate a vector/variable name from within a loop to be 
passed to a table
function.

This is what I have so far:
>assign("p1",paste("raw3.df$",rw2$V1[3],sep=""))
>p1
[1] "raw3.df$CITIZEN"
>
Essentially, I want to use the raw3.df$CITIZEN along with another value to 
generate a table.

However, I'm stuck here.  I know this is incredibly stupid.

Thanks in advance.
Sincerely
Laura Holt
mailto: lauraholt_983 at hotmail.com



From vito_ricci at yahoo.com  Tue Nov 16 18:20:37 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Tue, 16 Nov 2004 18:20:37 +0100 (CET)
Subject: [R] R: Changing zeros to NAs in a data frame
Message-ID: <20041116172037.56484.qmail@web41201.mail.yahoo.com>

Hi,

see this example. I hope I helped you a little bit.
Bye
Vito


dati
   X  Y    Z
1 10  0  "0"
2  0 20 "60"
3 30 40 "50"
4 11 12   3"

> dati[dati==0]<-NA
> dati

   X  Y    Z
1 10 NA  "0"
2 NA 20 "60"
3 30 40 "50"
4 11 12   3"




=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese http://www.modugno.it/archivio/cat_palese.shtml



From p935460 at phd.unibocconi.it  Tue Nov 16 18:22:07 2004
From: p935460 at phd.unibocconi.it (p935460@phd.unibocconi.it)
Date: Tue, 16 Nov 2004 18:22:07 +0100
Subject: [R] help on EM Algorithm for bivariate normal
Message-ID: <1100625727.419a373f7c853@webmail.phd.unibocconi.it>


Hi,
I woul like to know if it is possible to have a "R code" to generate EM
Algorithm for a normal bivariate mixture.
Best regard,
S.F.



From BosR at ny.rothinc.com  Tue Nov 16 18:26:33 2004
From: BosR at ny.rothinc.com (Bos, Roger)
Date: Tue, 16 Nov 2004 12:26:33 -0500
Subject: [R] Please help using SQL with R & SQL Server
Message-ID: <E07964B84690CC47B01421B2A71D4F0F095F4A8E@rinnycs0000>

I cannot get a database connection in R to my MS SQL Server database.  In
S-Plus 6.2 I have used the following successfully:

	rawdata <- importData(type="DIRECT-SQL", user="xfl2",
password="xfl2", server="rinnycs0059", database="xf", table="",
sqlQuery="select * from testTbl")

In R I have tried both DBI and RSQLite, but have not had any luck.  It
always says cannot find driver.  All the examples use My SQL or some other
program.  I only have SQL Server by Microsoft.  I know R might not have a
native driver, but I have tried "ODBC", too, without luck.  If anyone can
give me an example and hints on finding the driver that would be great!
Thanks.  Roger

********************************************************************** * This message is for the named person's use only. It may 
contain confidential, proprietary or legally privileged 
information. No right to confidential or privileged treatment 
of this message is waived or lost by any error in 
transmission. If you have received this message in error, 
please immediately notify the sender by e-mail, 
delete the message and all copies from your system and destroy 
any hard copies. You must not, directly or indirectly, use, 
disclose, distribute, print or copy any part of this message 
if you are not the intended recipient.



From sundar.dorai-raj at pdf.com  Tue Nov 16 18:28:17 2004
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Tue, 16 Nov 2004 11:28:17 -0600
Subject: [R] Changing zeros to NAs in a data frame
In-Reply-To: <BAY12-F17tvElzXqHof00011084@hotmail.com>
References: <BAY12-F17tvElzXqHof00011084@hotmail.com>
Message-ID: <419A38B1.2070805@pdf.com>



Laura Holt wrote:

> Dear R People:
> 
> I have a data frame with some columns that are numeric and some which 
> are factors.
> 
> There are zeros in the numeric columns and I would like to change them 
> to NAs.  However, there are zeros in some of the factor columns, and I 
> would like them to be left alone.
> 
> Is there a "global" way to do this, please?  I was thinking about use 
> the results from "str" but am not sure.
> 
> R Version 2.0.0 Windows.
> 

Let x be your data.frame. Then use:

is.num <- sapply(x, is.numeric)
x[is.num] <- lapply(x[is.num], function(y) ifelse(y == 0, NA, y))

HTH,

--sundar



From Roger.Bivand at nhh.no  Tue Nov 16 18:29:25 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 16 Nov 2004 18:29:25 +0100 (CET)
Subject: [R] memory allocation
In-Reply-To: <1100620134.419a21661d05a@webmail.uni-bocconi.it>
Message-ID: <Pine.LNX.4.44.0411161821040.15210-100000@reclus.nhh.no>

On Tue, 16 Nov 2004, Massimiliano Copetti wrote:

> 
> Dear sirs,
> 
> I'm using the Splancs package to compute standard errors of the
> estimates of a spatio-temporal k function. When I use as spatial and
> temporal distances too long vectors (respectively 60 and 80 entries) for
> a dataset of 1000 observations, R gives me the message

The memory demands arise in stvmat() called inside stsecal(), in which an 
(s*tm)*(s*tm) numeric matrix is allocated, described in the code as a 
"full spacetime variance/covariance matrix", and which is necessarily 
large. In your case 23040000 elements are used of 8 bytes each, so to do 
this in the way you describe you need more RAM - the matrix may be copied 
as well. The authors of the package and the underlying method did not 
anticipate needing this high degree of spatial and temporal resolution.

Roger Bivand

> 
> Error: cannot allocate vector of size 180000 Kb
> 
> Reached total allocation of 512 Mb.
> 
> I ran the function memory.size() and obtained 392000000 more or less.
> 
> Can anyone help me? 
> 
> Thanks in advance.
> 
> Massimiliano Copetti
> 
> 
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From abunn at whrc.org  Tue Nov 16 18:30:38 2004
From: abunn at whrc.org (Andy Bunn)
Date: Tue, 16 Nov 2004 12:30:38 -0500
Subject: [R] Changing zeros to NAs in a data frame
In-Reply-To: <BAY12-F17tvElzXqHof00011084@hotmail.com>
Message-ID: <NEBBIPHDAMMOKDKPOFFIEEDFCMAA.abunn@whrc.org>

This isn't pretty but it's a way to do it:

foo <- data.frame(x = c(1,0,1,1,0,2,4), y = as.factor(c(0,2,1,1,0,3,1)))
Zero2NA <- function(x){
    if(is.numeric(x)) { x[x == 0] <- NA; }
    return(x)
}
foo2 <- as.data.frame(lapply(foo, Zero2NA))
foo
foo2

HTH, Andy


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Laura Holt
> Sent: Tuesday, November 16, 2004 11:31 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Changing zeros to NAs in a data frame
>
>
> Dear R People:
>
> I have a data frame with some columns that are numeric and some which are
> factors.
>
> There are zeros in the numeric columns and I would like to change them to
> NAs.  However, there are zeros in some of the factor columns, and I would
> like them to be left alone.
>
> Is there a "global" way to do this, please?  I was thinking about use the
> results from "str" but am not sure.
>
> R Version 2.0.0 Windows.
>
> Thanks in advance.
> Sincerely,
> Laura Holt
> mailto: lauraholt_983 at hotmail.com
>
>
> Security. http://clinic.mcafee.com/clinic/ibuy/campaign.asp?cid=3963
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From gdsr15 at yahoo.com  Tue Nov 16 18:49:45 2004
From: gdsr15 at yahoo.com (gauri)
Date: Tue, 16 Nov 2004 09:49:45 -0800 (PST)
Subject: [R] R help 
Message-ID: <20041116174945.42579.qmail@web52908.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041116/d4f6fba3/attachment.pl

From ripley at stats.ox.ac.uk  Tue Nov 16 18:53:28 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 16 Nov 2004 17:53:28 +0000 (GMT)
Subject: [R] Changing zeros to NAs in a data frame
In-Reply-To: <BAY12-F17tvElzXqHof00011084@hotmail.com>
References: <BAY12-F17tvElzXqHof00011084@hotmail.com>
Message-ID: <Pine.LNX.4.61.0411161750410.24032@gannet.stats>

On Tue, 16 Nov 2004, Laura Holt wrote:

> Dear R People:
>
> I have a data frame with some columns that are numeric and some which are 
> factors.
>
> There are zeros in the numeric columns and I would like to change them to 
> NAs.  However, there are zeros in some of the factor columns, and I would 
> like them to be left alone.
>
> Is there a "global" way to do this, please?  I was thinking about use the 
> results from "str" but am not sure.

myDF <- data.frame(a=0:4, b=letters[1:5], c=-2:2)
myDF[] <- lapply(myDF, function(x) if(is.numeric(x)) {x[x==0] <- NA; x} else x)
> myDF
    a b  c
1 NA a -2
2  1 b -1
3  2 c NA
4  3 d  1
5  4 e  2


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From max.marinucci at ya.com  Tue Nov 16 19:50:34 2004
From: max.marinucci at ya.com (max.marinucci)
Date: Tue, 16 Nov 2004 19:50:34 +0100
Subject: [R] help on EM Algorithm for bivariate normal
References: <1100625727.419a373f7c853@webmail.phd.unibocconi.it>
Message-ID: <005801c4cc0d$28a09430$e6d71a50@maxmad9rubu4nk>

take a look at:

flexmix
mixreg
mclust

packages on CRAN

You may also define your own M-step in some of these packages...
EM is used to find the parameters of the mixture.
Ciao
M.


**********************************************
Massimiliano Marinucci
http://personales.ya.com/max_mar/
Ph.D Candidate in Economics
Fundamentos del Analisis Econ??mico II
(Econom??a Cuantitativa)
Facultad de CC.EE.
Universidad Complutense Madrid
Campus de Somosaguas
Madrid - Spain
**********************************************
----- Original Message -----
From: <p935460 at phd.unibocconi.it>
To: <R-help at stat.math.ethz.ch>
Sent: Tuesday, November 16, 2004 6:22 PM
Subject: [R] help on EM Algorithm for bivariate normal


>
> Hi,
> I woul like to know if it is possible to have a "R code" to generate EM
> Algorithm for a normal bivariate mixture.
> Best regard,
> S.F.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Tue Nov 16 19:52:44 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 16 Nov 2004 13:52:44 -0500
Subject: [R] User package - Simple Ports
Message-ID: <3A822319EB35174CA3714066D590DCD50994E316@usrymx25.merck.com>

If I'm not mistaken, I believe you need to do

    make pkg-X

or

    R CMD INSTALL X

Andy

> From: Keith J Sabol
> 
> R experts,
> 
> I created a package X for R 1.8 that I want to re-install for 
> 2.0.1.  In
> addition to downloading/installing Perl and Rtools as directed in the
> README.packages file and ensuring that they appear in my path, I have
> modified X's DESCRIPTION file to include all required fields.
> Additionally I have deleted  the "built" line since the  make process
> will add that.  The source contains no Fortran/C/C++ and 
> resides in   .
> R/rw2000/src/library
> 
>  
> 
> >From .R/rw2000/src/gnuwin32>  make X -X seems to run the 
> make utility,
> but the package is never installed.
> 
>  
> 
> In R 1.8  manually added the build line and copied the 
> package directory
> into the library tree and library(X) worked without a formal 
> install (As
> you would guess this was a work-around for my earlier and evidently
> persistent lack of success in my package installation 
> endeavors.).  The
> same work-around approach seems not to work with 2.0.
> 
>  
> 
> Your suggestions are appreciated.
> 
>  
> 
> Thank you!
> 
>  
> 
> Keith Sabol
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From spencer.graves at pdf.com  Tue Nov 16 19:53:11 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 16 Nov 2004 10:53:11 -0800
Subject: [R] R help
In-Reply-To: <20041116174945.42579.qmail@web52908.mail.yahoo.com>
References: <20041116174945.42579.qmail@web52908.mail.yahoo.com>
Message-ID: <419A4C97.9010506@pdf.com>

  I don't understand your question. "PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html". In particular, have you 
read the "R Data Import / Export" documentation that some with R 
[available, from "www.r-project.org" -> Manuals or R -> help.start()]? 
Have you tried "read.spss" in library(foreign)? [help(package="foreign") 
provides additional documentation.] If you've tried all these without 
satisfaction, please explain what you've tried, the error message, the 
version of R, operating system, etc., as requested in the posting guide.

hope this helps.
spencer graves

gauri wrote:

>Hi,
>I was wondering as to how I could convert SPSS data imported to R into tabular form. In the sense, direct usage of read.table( ) doesnt help.
> 
>Thanks
>
>		
>---------------------------------
>

>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From r.ghezzo at staff.mcgill.ca  Tue Nov 16 20:42:18 2004
From: r.ghezzo at staff.mcgill.ca (r.ghezzo@staff.mcgill.ca)
Date: Tue, 16 Nov 2004 14:42:18 -0500
Subject: [R] problems with compiling a package
Message-ID: <1100634138.419a581aa81b4@webmail.mcgill.ca>


Hello, I am trying to compile packages for R2.0.0 patch in a win XP machine.
Most of the packages compile without problems, with C or FTG or only R.
Now some packeges give the following error which I do not understand how to
correct
...
preparing package xxx for lazy loading
Error in "names <- .default"('*tmp*',value=c("R","Platform","Date",   :
  names attribute[4] must be the same length as the vector [3]
Execution halted
make: *** [lazyload] Error 1
....
Can somebody tell me how I can correct this error?

One other question, this npreparing package for lazy loading does not occur for
all packages, although their DESCRIPTION and folders are similar, When does a
package goes to lazy loading and when it does not?
Thanks
Heberto Ghezzo
McGill U
Montreal - Canada



From tfliao at uiuc.edu  Tue Nov 16 20:59:24 2004
From: tfliao at uiuc.edu (Tim F Liao)
Date: Tue, 16 Nov 2004 13:59:24 -0600
Subject: [R] help on EM Algorithm for bivariate
 normal
Message-ID: <9791c89b.a04bf843.81a0800@expms6.cites.uiuc.edu>

The three packages that someone else mentioned in response 
to your post

flexmix
mixreg
mclust

may be usefule in real applications.  On the other hand, if 
your interest is in teaching EM or developing your own EM by 
using a simple example, the most recent issue of The 
American Statistician has an article with a sample R code 
for the purpose.  Right now I'm away from my journal.  But 
if you would want further info, let me know and I can find 
it for you.

Tim Liao

---- Original message ----
>Date: Tue, 16 Nov 2004 18:22:07 +0100
>From: p935460 at phd.unibocconi.it  
>Subject: [R] help on EM Algorithm for bivariate normal  
>To: R-help at stat.math.ethz.ch
>
>
>Hi,
>I woul like to know if it is possible to have a "R code" to 
generate EM
>Algorithm for a normal bivariate mixture.
>Best regard,
>S.F.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-
project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Tue Nov 16 21:31:08 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 16 Nov 2004 20:31:08 +0000 (GMT)
Subject: [R] problems with compiling a package
In-Reply-To: <1100634138.419a581aa81b4@webmail.mcgill.ca>
References: <1100634138.419a581aa81b4@webmail.mcgill.ca>
Message-ID: <Pine.LNX.4.61.0411162009280.25809@gannet.stats>

On Tue, 16 Nov 2004 r.ghezzo at staff.mcgill.ca wrote:

> Hello, I am trying to compile packages for R2.0.0 patch in a win XP machine.

What exactly is `R2.0.0 patch'?  If you mean 2.0.0 patched, it has a date, 
and the date is crucial.

> Most of the packages compile without problems, with C or FTG or only R.
> Now some packeges give the following error which I do not understand how to
> correct
> ...
> preparing package xxx for lazy loading
> Error in "names <- .default"('*tmp*',value=c("R","Platform","Date",   :
>  names attribute[4] must be the same length as the vector [3]
> Execution halted
> make: *** [lazyload] Error 1
> ....
> Can somebody tell me how I can correct this error?

My guess is that this comes from tools:::.split_description which has

     if(!is.na(Built <- db["Built"])) {
         Built <- as.list(strsplit(Built, "; ")[[1]])
         names(Built) <- c("R", "Platform", "Date", "OStype")
         Built[["R"]] <- package_version(sub("^R ([0-9.]+)", "\\1",
                                             Built[["R"]]))
     } else Built <- NULL

so probably your DESCRIPTION file is faulty.  Did you have a Built: line
in it that you put there yourself?  I can reproduce this by doing so.
If so, `Writing R Extensions' does say

   There should be no @samp{Built} or @samp{Packaged} fields, as these are
   added by the package management tools.

and R CMD check on such package throws an immediate error.


> One other question, this npreparing package for lazy loading does not occur for
> all packages, although their DESCRIPTION and folders are similar, When does a
> package goes to lazy loading and when it does not?

Please do read the NEWS (or ONEWS) file!  It says in the first 20 lines

         Packages are by default installed using lazy loading if they
         have more than 25Kb of R code and did not use a saved image.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at myway.com  Tue Nov 16 21:35:44 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 16 Nov 2004 20:35:44 +0000 (UTC)
Subject: [R] changing character to a vector name
References: <BAY12-F174FTA7OIkUq0001157c@hotmail.com>
Message-ID: <loom.20041116T213517-502@post.gmane.org>

Laura Holt <lauraholt_983 <at> hotmail.com> writes:

.> I would like to generate a vector/variable name from within a loop to be 
.> passed to a table
.> function.
.> 
.> This is what I have so far:
.> >assign("p1",paste("raw3.df$",rw2$V1[3],sep=""))
.> >p1
.> [1] "raw3.df$CITIZEN"
.> >
.> Essentially, I want to use the raw3.df$CITIZEN along with another value to 
.> generate a table.

You can use eval and parse to parse and execute 
arbitrary R commands that you construct from strings.  The exec
function below is a one statement convenience function
that pastes its arguments together and executes them this way in
its parent's environment.  (Sometimes when one gets complex
requirements like this if one backs up a bit one discovers
that the requirements can be simplified and you might
consider whether that is the case here in which case you
might be able to disregard all this.)

R> ### create some test data ###
R> data(iris)
R> irish <- head(iris)
R> irish
  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1          5.1         3.5          1.4         0.2  setosa
2          4.9         3.0          1.4         0.2  setosa
3          4.7         3.2          1.3         0.2  setosa
4          4.6         3.1          1.5         0.2  setosa
5          5.0         3.6          1.4         0.2  setosa
6          5.4         3.9          1.7         0.4  setosa

R> ### this is the actual processing ###
R> exec <- function(...) eval.parent(parse(text = paste(..., collapse = "")))
R> exec("irish$Sepal.Length", "<-", "irish$Sepal.Length", "+1")
R> irish
  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1          6.1         3.5          1.4         0.2  setosa
2          5.9         3.0          1.4         0.2  setosa
3          5.7         3.2          1.3         0.2  setosa
4          5.6         3.1          1.5         0.2  setosa
5          6.0         3.6          1.4         0.2  setosa
6          6.4         3.9          1.7         0.4  setosa



From ggrothendieck at myway.com  Tue Nov 16 21:39:14 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 16 Nov 2004 20:39:14 +0000 (UTC)
Subject: [R] problems with compiling a package
References: <1100634138.419a581aa81b4@webmail.mcgill.ca>
Message-ID: <loom.20041116T213636-563@post.gmane.org>

 <r.ghezzo <at> staff.mcgill.ca> writes:
: One other question, this npreparing package for lazy loading does not occur 
for
: all packages, although their DESCRIPTION and folders are similar, When does a
: package goes to lazy loading and when it does not?

You can use 

LazyLoad: no

in the DESCRIPTION file to prevent lazy loading.  
Alternately you can use the --no-lazy switch on 
R CMD install



From pburns at pburns.seanet.com  Tue Nov 16 22:28:18 2004
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Tue, 16 Nov 2004 21:28:18 +0000
Subject: [R] Difference between two correlation matrices
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950121B88A@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E950121B88A@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <419A70F2.6010509@pburns.seanet.com>

If you have the data that created the correlation matrices, then
you can do a permutation test.

The first question to ask is, "What does 'different' mean?"
Some choices include:

max(abs(cor1 - cor2))

max(abs(eigen(cor1 - cor2)$values))

Once you have decided what metric makes most sense, you can
perform the test. First pool all of the data (if you think that there
are different means in the two conditions, then remove the means
within each group before pooling).  Then perform a number of
random allocations of the pooled data  into two groups with the
number in each group equal to the original numbers.  The test
compares your statistic using the original correlation matrices to
the distribution of statistics of the correlations from the random
allocations.

Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

michael watson (IAH-C) wrote:

>Hi 
>
>Now a more theoretical question.  I have two correlation matrices - one
>of a set of variables under a particular condition, the other of the
>same set of variables under a different condition.  Is there a
>statistical test I can use to see if these correlation matrices are
>"different"?
>
>Thanks
>Mick
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>



From aubert at inapg.fr  Tue Nov 16 17:55:25 2004
From: aubert at inapg.fr (aubert@inapg.fr)
Date: Tue, 16 Nov 2004 17:55:25 +0100
Subject: [R] gcrma package
Message-ID: <5.1.0.14.0.20041116175055.00bc6838@pop1p.inapg.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041116/d10d935d/attachment.pl

From Ted.Harding at nessie.mcc.ac.uk  Tue Nov 16 22:39:14 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 16 Nov 2004 21:39:14 -0000 (GMT)
Subject: [R] CDs for R?
Message-ID: <XFMail.041116213914.Ted.Harding@nessie.mcc.ac.uk>

Hi Folks,

I'm sure I'm speaking for more than a few (though
possibly a minority) here.

There's something of a hidden assumption that R users
can readily download whatever they need from CRAN.

Some of us are on narrow bandwidth dialup connections,
so downloading large quantities of stuff is out of the
question (e.g. at approx. 5min/MB, it would take over
2 days to download a single CD). The meat of CRAN
(including contributed packages and documentation)
is enough to fill 5 CDs, though one individual probably
wouldn't be interested in all of that.

(And, before anyone asks, no I won't be seeing broadband
in the foreseeable future!)

I've worked round this in various ways in the past.
Some Linux distributions (e.g. RedHat, SuSE) come with the
basics of R on their CDs, so when I've upgraded Linux
I've got a new R (though not the latest). Linux Emporium
once obligingly did me a custom CD very cheaply with the
contributed packages. A friend with high-bandwidth access
did me the 5 CDs in return for a pub lunch. And so on.
One can get round it by bothering someone.

What I'd like to suggest, for consideration, is that along
with the stirling work done at many centres to set up and
maintain mirrors of CRAN, some might consider offering also
the service of burning CDs on request, for a reasonable
charge. The difficulty, of course, is that it's going to
take someone's time om something which may well not be
their proper business.

(I'm prompted to think about this again by the emergence
of R-2 -> 2.0.1 -> one day soon? -> 2.1. This is clearly
a major advance in R and changes several things, so I'm
looking at a major download operation, if not by me then
by somebody that I can get CDs from, if I'm to be sure
of being up to date on everything I'd like to have. And
I'm sure I'm not the only one.)

I'd be interested in people's comments on this proposal.

Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 16-Nov-04                                       Time: 21:39:14
------------------------------ XFMail ------------------------------



From ligges at statistik.uni-dortmund.de  Wed Nov 17 00:02:02 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 17 Nov 2004 00:02:02 +0100
Subject: [R] deleting a file
In-Reply-To: <20041116173049.6af1c77b.Achim.Zeileis@wu-wien.ac.at>
References: <004401c4cbf2$fad82550$a3284e98@maths.soton.ac.uk>
	<20041116173049.6af1c77b.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <419A86EA.1090409@statistik.uni-dortmund.de>

Achim Zeileis wrote:
> On Tue, 16 Nov 2004 15:43:12 -0000 Roger Gill wrote:
> 
> 
>>Dear all,
>>
>>I have a very simple question.  Simple, that is, if you know the
>>answer.
> 
> 
> Yes, look at
>   ?file.remove
> Z

... and file.path() in order to specify the location in an OS 
independent way.

Uwe Ligges


> 
>>I wish to delete a file in a given directory after having first
>>checked its existence.  I issue the following commands
>>
>>path<-'c:example/R/'
>>
>>Thus creating the directory.
>>
>>indicator<-length(grep(filename,path))
>>
>>If indicator is greater than zero then the file exists.  Now I wish to
>>remove this file.  However, I would like to make the code compatible
>>for Linux/Windows and Macs.
>>
>>In Linux I would issue the command
>>
>>system('rm paste(path,'filename',sep='')')
>>
>>and this works just fine.  It does not however work for windows, and I
>>have no idea whether it would work on a mac.
>>
>>Is there a single command thay escapes me?
>>
>>Best Wishes
>>
>>Roger Gill
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>>http://www.R-project.org/posting-guide.html
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ramasamy at cancer.org.uk  Wed Nov 17 00:03:35 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Tue, 16 Nov 2004 23:03:35 +0000
Subject: [R] gcrma package
In-Reply-To: <5.1.0.14.0.20041116175055.00bc6838@pop1p.inapg.fr>
References: <5.1.0.14.0.20041116175055.00bc6838@pop1p.inapg.fr>
Message-ID: <1100646163.3085.2.camel@localhost.localdomain>

This question is more for the BioConductor mailing list. 

If you type in 

	library(gcrma)
	help(affinity.spline.coefs)

at the command prompt, it suggests you to see compute.affinities. Doing
a help(compute.affinities), you will see under "References" section :

     Hekstra, D., Taussig, A. R., Magnasco, M., and Naef, F. (2003)
     Absolute mRNA concentrations from sequence-specific calibration of
     oligonucleotide array. Nucleic Acids Research, 31. 1962-1968.



On Tue, 2004-11-16 at 16:55, aubert at inapg.fr wrote:
> Hi !
> 
> I would like to understand where do affinity.spline.coefs used in function 
> compute.affinities come from ?
> 
> library(gcrma)
> data(affinity.spline.coefs)
>   affinity.spline.coefs
>           X1          X2          X3          X4          X5          X1
> -0.55004171 -0.58579091 -0.08870557 -0.47774242  0.23205570  0.58002746
>           X2          X3          X4          X5          X1          X2
>   0.61274425  0.10294309  0.57388808 -0.28334367  0.01350914 -0.02030805
>           X3          X4          X5
> -0.14747469  0.05436463 -0.10124882
> 
> Thank you in advance!
> 
> Julie AUBERT
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
-- 
Adaikalavan Ramasamy                    ramasamy at cancer.org.uk
Centre for Statistics in Medicine       http://www.ihs.ox.ac.uk/csm/
Cancer Research UK                      Tel : 01865 226 677
Old Road Campus, Headington, Oxford     Fax : 01865 226 962



From Jin.Li at csiro.au  Wed Nov 17 00:04:52 2004
From: Jin.Li at csiro.au (Jin.Li@csiro.au)
Date: Wed, 17 Nov 2004 09:04:52 +1000
Subject: [R] How to remove x, y labels from a plot
Message-ID: <2BEE99D7F6F1484EBDD1D22167385E75375AD2@exqld1-ath.nexus.csiro.au>

Hi Arne,
It works. It produced what I wanted. 
y<-rnorm(1000, 2, 0)
plot(density(y), ylab="Abundance of species", xlab="Environmental
gradient", main=" ", lty=2, col=4, xaxt="n", yaxt="n")
Thanks, Arne. And also thanks to other responses.
Regards,
Jin

-----Original Message-----
From: Arne Henningsen [mailto:ahenningsen at email.uni-kiel.de] 
Sent: Tuesday, 16 November 2004 10:33 P
To: r-help at stat.math.ethz.ch
Cc: Li, Jin (CSE, Atherton)
Subject: Re: [R] How to remove x, y labels from a plot

Hi Jin Li,

does
   plot( 1:100, rnorm(100), ann=FALSE, xaxt="n", yaxt="n" )
produce what you want?

Arne


On Tuesday 16 November 2004 13:06, Jin.Li at csiro.au wrote:
> Hi there,
>
>
>
> I need to plot an illustrative figure without ticks, x, y labels in R.
I
> managed to get the ticks removed, but had no luck with x, y labels.
>
>
>
> Any suggestions would be much appreciated.
>
>
>
> Jin Li
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From david.whiting at ncl.ac.uk  Wed Nov 17 00:18:46 2004
From: david.whiting at ncl.ac.uk (David Whiting)
Date: 16 Nov 2004 23:18:46 +0000
Subject: [R] CDs for R?
In-Reply-To: <XFMail.041116213914.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.041116213914.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <m2acthqt61.fsf@ganymede.ammp.or.tz>

(Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:

[...]

> There's something of a hidden assumption that R users
> can readily download whatever they need from CRAN.

[...]

> What I'd like to suggest, for consideration, is that along
> with the stirling work done at many centres to set up and
> maintain mirrors of CRAN, some might consider offering also
> the service of burning CDs on request, for a reasonable
> charge. The difficulty, of course, is that it's going to
> take someone's time om something which may well not be
> their proper business.

I have been in a similar situation a fair bit in the past and
understand your position.  Now I'm back in the UK and have a
reasonably fast broadband connection at home I'd be willing to help
out now and then. I guess that to make this work more generally we
would need to work out how to make sure that only the CDs get burned
and not the prospective "customer" or "supplier". 

As for charges, I think I'd only be interested in covering costs of
the CDs and postage. I don't have industrial strength hardware so I
could not get into mass production.

Perhaps we could establish informal groups of "R buddies" where, for
example, I help you and a small number of other people out each time
there is an update and we establish some kind of trust between
ourselves, rather than new people coming to me each time. People could
sign up to be "suppliers" and be allocated or choose a group of people
they provide the service to.

I would feel comfortable with something like this working for people
who have been on the R-help list for a while and have some recgonised
identity, and something to lose in terms of reputation if they take
advantage. But, it is possible that new users might need it the most
and, by definition, we might not feel comfortable dealing with new
people.

Dave

-- 
David Whiting
University of Newcastle upon Tyne, UK



From ramasamy at cancer.org.uk  Wed Nov 17 00:32:05 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Tue, 16 Nov 2004 23:32:05 +0000
Subject: [R] CDs for R?
In-Reply-To: <XFMail.041116213914.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.041116213914.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <1100647925.3085.20.camel@localhost.localdomain>

There was a similar thread earlier this year 
http://tolstoy.newcastle.edu.au/R/help/04/03/1785.html

I think I previously made the suggestion that you could download this
from an internet cafe. I am not sure if these cafes allow you to plug in
your own laptop or burn it for you.

If you have a DVD readable CDROM and a DVD writer, you could opt for
that.

Failing that, you could see if someone in your local R-group or
university department has the latest R on disc.

But the real question is that if there are enough people on slow
connection that are interested in obtaining R.

Regards, Adai


On Tue, 2004-11-16 at 21:39, Ted.Harding at nessie.mcc.ac.uk wrote:
> Hi Folks,
> 
> I'm sure I'm speaking for more than a few (though
> possibly a minority) here.
> 
> There's something of a hidden assumption that R users
> can readily download whatever they need from CRAN.
> 
> Some of us are on narrow bandwidth dialup connections,
> so downloading large quantities of stuff is out of the
> question (e.g. at approx. 5min/MB, it would take over
> 2 days to download a single CD). The meat of CRAN
> (including contributed packages and documentation)
> is enough to fill 5 CDs, though one individual probably
> wouldn't be interested in all of that.
> 
> (And, before anyone asks, no I won't be seeing broadband
> in the foreseeable future!)
> 
> I've worked round this in various ways in the past.
> Some Linux distributions (e.g. RedHat, SuSE) come with the
> basics of R on their CDs, so when I've upgraded Linux
> I've got a new R (though not the latest). Linux Emporium
> once obligingly did me a custom CD very cheaply with the
> contributed packages. A friend with high-bandwidth access
> did me the 5 CDs in return for a pub lunch. And so on.
> One can get round it by bothering someone.
> 
> What I'd like to suggest, for consideration, is that along
> with the stirling work done at many centres to set up and
> maintain mirrors of CRAN, some might consider offering also
> the service of burning CDs on request, for a reasonable
> charge. The difficulty, of course, is that it's going to
> take someone's time om something which may well not be
> their proper business.
> 
> (I'm prompted to think about this again by the emergence
> of R-2 -> 2.0.1 -> one day soon? -> 2.1. This is clearly
> a major advance in R and changes several things, so I'm
> looking at a major download operation, if not by me then
> by somebody that I can get CDs from, if I'm to be sure
> of being up to date on everything I'd like to have. And
> I'm sure I'm not the only one.)
> 
> I'd be interested in people's comments on this proposal.
> 
> Best wishes to all,
> Ted.
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
> Date: 16-Nov-04                                       Time: 21:39:14
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
-- 
Adaikalavan Ramasamy                    ramasamy at cancer.org.uk
Centre for Statistics in Medicine       http://www.ihs.ox.ac.uk/csm/
Cancer Research UK                      Tel : 01865 226 677
Old Road Campus, Headington, Oxford     Fax : 01865 226 962



From murdoch at stats.uwo.ca  Wed Nov 17 01:05:15 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 16 Nov 2004 19:05:15 -0500
Subject: [R] CDs for R?
In-Reply-To: <XFMail.041116213914.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.041116213914.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <5s4lp095ntuuj8n96v3rfcu5o6r18tnkm3@4ax.com>

On Tue, 16 Nov 2004 21:39:14 -0000 (GMT), (Ted Harding)
<Ted.Harding at nessie.mcc.ac.uk> wrote:

>Hi Folks,
>
>I'm sure I'm speaking for more than a few (though
>possibly a minority) here.
>
>There's something of a hidden assumption that R users
>can readily download whatever they need from CRAN.
>
>Some of us are on narrow bandwidth dialup connections,
>so downloading large quantities of stuff is out of the
>question (e.g. at approx. 5min/MB, it would take over
>2 days to download a single CD). The meat of CRAN
>(including contributed packages and documentation)
>is enough to fill 5 CDs, though one individual probably
>wouldn't be interested in all of that.
>
>(And, before anyone asks, no I won't be seeing broadband
>in the foreseeable future!)

The tarballs for the base source installation are only around 10 MB.
That's only 50 minutes for you to do the main download. Then you can
pick and choose what other packages to install.  (It's 23 MB to
download the Windows binary; I don't know the size of the Linux
binaries.)

Is it really worth setting up a duplication service?  How much would
you think is reasonable to pay for a CD to be mailed to you?

Duncan Murdoch



From muster at gmail.com  Wed Nov 17 00:07:23 2004
From: muster at gmail.com (Terry Mu)
Date: Tue, 16 Nov 2004 18:07:23 -0500
Subject: [R] beginner's problem in displaying large data
Message-ID: <b68812e7041116150774a548b9@mail.gmail.com>

I got a sample data (let's call it sample.data), which is about 2200 by 15.

I tried to take a look of all data

>sample.data

It shows only a part of data that I thought was a corner. It does not
really affect my job, but I thought it is nice to have a look of all
data. I can see individual records and they are fine.

Is this normal because of buffer size or some reasons? Can I use other
commands or change some settings to display all data?

Thanks,
Terry



From Zack.Apoian at sac.com  Wed Nov 17 01:31:46 2004
From: Zack.Apoian at sac.com (Apoian, Zack)
Date: Tue, 16 Nov 2004 19:31:46 -0500
Subject: [R] plot question
Message-ID: <D35A7FAE1220DB49A018F51A225565E8059F9F@mailisny3.saccap.int>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041116/dfda5167/attachment.pl

From spencer.graves at pdf.com  Wed Nov 17 01:37:29 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 16 Nov 2004 16:37:29 -0800
Subject: [R] beginner's problem in displaying large data
In-Reply-To: <b68812e7041116150774a548b9@mail.gmail.com>
References: <b68812e7041116150774a548b9@mail.gmail.com>
Message-ID: <419A9D49.1030501@pdf.com>

      1.  Did you try "dim(sample.data)"?  Is it actually 2200 by 15?  
Or are you reading in just some subset of the data?  If it is 2200 by 
15, could you also please do "class(sample.data)"? 

      2.  I just got a full listing from the following: 

      (tst <- data.frame(array(rnorm(2200), dim=c(2200, 15))))

      You might try this.  With R 2.0.0patched under Windows 2000, I got 
rows 1:2200 flying by 3 times, each with 5 columns. 

      3.  Have you considered doing plots (including qqnorm) of numeric 
variables and tables of character variables?  These can often reveal 
problems I might never see in a simple scan of numbers. 

      4.  "PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html".  At minimum, please tell 
us which version of R under which operating system, and specifically 
what you did to get it into R and how you know it's 2200 by 15. 

      hope this helps. 
      spencer graves

Terry Mu wrote:

>I got a sample data (let's call it sample.data), which is about 2200 by 15.
>
>I tried to take a look of all data
>
>  
>
>>sample.data
>>    
>>
>
>It shows only a part of data that I thought was a corner. It does not
>really affect my job, but I thought it is nice to have a look of all
>data. I can see individual records and they are fine.
>
>Is this normal because of buffer size or some reasons? Can I use other
>commands or change some settings to display all data?
>
>Thanks,
>Terry
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From agobena at ualberta.ca  Wed Nov 17 01:44:23 2004
From: agobena at ualberta.ca (Adam Gobena)
Date: Tue, 16 Nov 2004 17:44:23 -0700
Subject: [R] Cross-correlated variables in kernel density estimation
Message-ID: <000001c4cc3e$95997f80$a74e8081@RSGIS>

Hi,
I am wondering if the kde2d 2-D kernel density estimation function in the
MASS package can take into account the effect of correlations between the
variables. I couldn't find any achieved information on this issue.
Unfortunately, I don't have the 2002 edition of Modern Applied Statistics
with S by Venables and Ripley in case it was described there. 

Thanks in advance.
Adam
----------------------------------------------------------------------------
Adam Kenea Gobena
Research Assistant, Water Resources Engineering
Department of Civil & Environmental Engineering
220 Civil/Electrical Eng Bldg
University of Alberta
Edmonton, AB
CANADA T6G 2G7



From andy_liaw at merck.com  Wed Nov 17 02:05:41 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 16 Nov 2004 20:05:41 -0500
Subject: [R] Cross-correlated variables in kernel density estimation
Message-ID: <3A822319EB35174CA3714066D590DCD50994E319@usrymx25.merck.com>

> From: Adam Gobena
> 
> Hi,
> I am wondering if the kde2d 2-D kernel density estimation 
> function in the
> MASS package can take into account the effect of correlations 
> between the
> variables. I couldn't find any achieved information on this issue.
> Unfortunately, I don't have the 2002 edition of Modern 
> Applied Statistics
> with S by Venables and Ripley in case it was described there. 

The subject of your message doesn't seem to have much to do with your
question...  Also, it's not clear to me what you mean by taking into account
the effect of correlations between variables.  Do you mean a kernel function
that is something like a bivariate Gaussian density with non-diagonal
covariance matrix?  If so, ?kde2d in MASS says:

     Two-dimensional kernel density estimation with an axis-aligned
     bivariate normal kernel, evaluated on a square grid.

so the answer is no.  No other R packages that does 2D kernel density
estimation (that I know of, anyway) can do it, either, and probably for a
good reason.  Why would you need it?  If there are correlation structures in
the (X, Y) data, small enough bandwidths in both direction should give
satisfactory estimate of the density.

Andy
 
> Thanks in advance.
> Adam
> --------------------------------------------------------------
> --------------
> Adam Kenea Gobena
> Research Assistant, Water Resources Engineering
> Department of Civil & Environmental Engineering
> 220 Civil/Electrical Eng Bldg
> University of Alberta
> Edmonton, AB
> CANADA T6G 2G7
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From xmeng at capitalbio.com  Wed Nov 17 02:58:48 2004
From: xmeng at capitalbio.com (=?gb2312?B?w8/QwA==?=)
Date: Wed, 17 Nov 2004 09:58:48 +0800
Subject: [R] about 2 color variance stabilization
Message-ID: <300656728.07671@capitalbio.com>

Hello sir:
A paper of you named"Exact and Approximate Variance-Stabilizing Transformations for Two-Color Microarrays" is very helpful for my work.
A question which needs your help.Thanks a lot.
For instance,cy3~control  cy5~treatment
And after calculating the "h"transformation for each channel,we can get h(cy3) and h(cy5).But obviously,each h value isn't calculated individually ,but through the information of the all 2 channels.I wanna know how the "all 2 channels"'s information is included in h transformation.
By using the "vsn"package under R environment,if I enter only 1 channel(i.e. just 1 column data for gene expression),the system gives warning which notice me that I must enter at least 2 channels(i.e. 2 column data for gene expression).If I enter to channel,I can get the h value for each of the 2 channels.So I wanna know how to use the "all 2 channels"'s imformation for the h transformation.
Thanks again.
 
My best regards!



From ggrothendieck at myway.com  Wed Nov 17 03:31:55 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 17 Nov 2004 02:31:55 +0000 (UTC)
Subject: [R] plot question
References: <D35A7FAE1220DB49A018F51A225565E8059F9F@mailisny3.saccap.int>
Message-ID: <loom.20041117T033016-536@post.gmane.org>

Apoian, Zack <Zack.Apoian <at> sac.com> writes:

> Say y and z are two time series (functions of "date").  What is the R
> command to plot y and z together on a graph with "date" on the x-axis?

There are several time series classes (ts, zoo in zoo, irts
in tseries, its in its, timeDate in fBasics) and its also possible 
to plot a numeric vector against a vector of dates.

Here is an example using zoo which also illustrates the case
where the times are not the same:


library(zoo)

# -- test data --
# y is over today and next 4 days. 
# z over today and next 9 days.
y <- zoo(11:15, Sys.Date() + 0:4)
z <- zoo(21:30, Sys.Date() + 0:9)

plot(merge(y,z), plot.type = "single", ylim = range(c(y,z)),
	col = c("green", "red"))



From diana.abdueva at gmail.com  Wed Nov 17 03:33:14 2004
From: diana.abdueva at gmail.com (Diana Abdueva)
Date: Tue, 16 Nov 2004 18:33:14 -0800
Subject: [R] Non-Linear Regression on a Matrix
Message-ID: <bd371f90041116183339557e3d@mail.gmail.com>

Hi, I'm terribly sorry for submitting my primitive question, I'm a
beginner in R and was hoping to get some help re: non-linear fit.

I have a 2D data with the following structure:

A     B        C
1      1      111
1      2      121
1      3      131
2      1      141
2      2      151
2      3      161
3      1      171
3      2      181
3      3      191

I'm trying to fit C = non-linear function (A,B). I was wondering if
there's a package that would save my time of doing direct least square
estimation.

Thank you,
Diana



From agobena at ualberta.ca  Wed Nov 17 03:44:18 2004
From: agobena at ualberta.ca (Adam Gobena)
Date: Tue, 16 Nov 2004 19:44:18 -0700
Subject: [R] Cross-correlated variables in kernel density estimation
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E319@usrymx25.merck.com>
Message-ID: <000101c4cc4f$55723fd0$a74e8081@RSGIS>

Hi Andy,
Sorry about the vague subject line. I think I overlooked a lot of things
including the description of the function itself. Anyway, you have got the
essence of my question. Thanks for the reply. 

I am using kernel density estimation to estimate the pdf some data. The pdf
estimation is an intermediate step in a modeling work. So, I can work with
one variable at a time and combine the final results from my model in some
way but I thought it would be good to look at the joint PDF. The problem is,
my (X,Y) data have a correlation structure.  

Thanks,
Adam

----------------------------------------------------------------------------
Adam K. Gobena
Research Assistant, Water Resources Engineering
Department of Civil & Environmental Engineering
220 Civil/Electrical Eng Bldg
University of Alberta
Edmonton, AB
CANADA T6G 2G7
 

-----Original Message-----
From: Liaw, Andy [mailto:andy_liaw at merck.com] 
Sent: Tuesday, November 16, 2004 6:06 PM
To: 'Adam Gobena'; r-help at stat.math.ethz.ch
Subject: RE: [R] Cross-correlated variables in kernel density estimation

> From: Adam Gobena
> 
> Hi,
> I am wondering if the kde2d 2-D kernel density estimation 
> function in the
> MASS package can take into account the effect of correlations 
> between the
> variables. I couldn't find any achieved information on this issue.
> Unfortunately, I don't have the 2002 edition of Modern 
> Applied Statistics
> with S by Venables and Ripley in case it was described there. 

The subject of your message doesn't seem to have much to do with your
question...  Also, it's not clear to me what you mean by taking into account
the effect of correlations between variables.  Do you mean a kernel function
that is something like a bivariate Gaussian density with non-diagonal
covariance matrix?  If so, ?kde2d in MASS says:

     Two-dimensional kernel density estimation with an axis-aligned
     bivariate normal kernel, evaluated on a square grid.

so the answer is no.  No other R packages that does 2D kernel density
estimation (that I know of, anyway) can do it, either, and probably for a
good reason.  Why would you need it?  If there are correlation structures in
the (X, Y) data, small enough bandwidths in both direction should give
satisfactory estimate of the density.

Andy
 
> Thanks in advance.
> Adam
> --------------------------------------------------------------
> --------------
> Adam Kenea Gobena
> Research Assistant, Water Resources Engineering
> Department of Civil & Environmental Engineering
> 220 Civil/Electrical Eng Bldg
> University of Alberta
> Edmonton, AB
> CANADA T6G 2G7
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 


----------------------------------------------------------------------------
--
Notice:  This e-mail message, together with any attachments,...{{dropped}}



From apjaworski at mmm.com  Wed Nov 17 03:56:13 2004
From: apjaworski at mmm.com (apjaworski@mmm.com)
Date: Tue, 16 Nov 2004 20:56:13 -0600
Subject: [R] Non-Linear Regression on a Matrix
In-Reply-To: <bd371f90041116183339557e3d@mail.gmail.com>
Message-ID: <OF18EA0EA7.8603EDA2-ON86256F4F.000FE4DC-86256F4F.00102277@mmm.com>






If your "non-linear function (A, B)" is parametric nls should do it for
you.  If you have R version 2 (perhaps even 1.9) do ?nls to see the help
page.  Older versions of R require library(nls) first.

Hope this helps,

Andy

__________________________________
Andy Jaworski
518-1-01
Process Laboratory
3M Corporate Research Laboratory
-----
E-mail: apjaworski at mmm.com
Tel:  (651) 733-6092
Fax:  (651) 736-3122


                                                                           
             Diana Abdueva                                                 
             <diana.abdueva at gm                                             
             ail.com>                                                   To 
             Sent by:                  r-help at stat.math.ethz.ch            
             r-help-bounces at st                                          cc 
             at.math.ethz.ch                                               
                                                                   Subject 
                                       [R] Non-Linear Regression on a      
             11/16/2004 08:33          Matrix                              
             PM                                                            
                                                                           
                                                                           
             Please respond to                                             
               Diana Abdueva                                               
             <diana.abdueva at gm                                             
                 ail.com>                                                  
                                                                           
                                                                           




Hi, I'm terribly sorry for submitting my primitive question, I'm a
beginner in R and was hoping to get some help re: non-linear fit.

I have a 2D data with the following structure:

A     B        C
1      1      111
1      2      121
1      3      131
2      1      141
2      2      151
2      3      161
3      1      171
3      2      181
3      3      191

I'm trying to fit C = non-linear function (A,B). I was wondering if
there's a package that would save my time of doing direct least square
estimation.

Thank you,
Diana

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From yuleih at umich.edu  Wed Nov 17 04:15:18 2004
From: yuleih at umich.edu (Yulei He)
Date: Tue, 16 Nov 2004 22:15:18 -0500 (EST)
Subject: [R] how to estimate conditional density
Message-ID: <Pine.SOL.4.58.0411162214120.23688@rygar.gpcc.itd.umich.edu>

Hi, there.

Suppose I have a bivariate data set y1 and y2. Can anybody tell me how to
estimate the conditional density of f(y1|y2) and vice versa? Thanks.

Yulei


$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
Yulei He
1586 Murfin Ave. Apt 37
Ann Arbor, MI 48105-3135
yuleih at umich.edu
734-647-0305(H)
734-763-0421(O)
734-763-0427(O)
734-764-8263(fax)
$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$



From edd at debian.org  Wed Nov 17 04:45:47 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 16 Nov 2004 21:45:47 -0600
Subject: [R] CDs for R?
In-Reply-To: <XFMail.041116213914.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.041116213914.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <20041117034547.GA4341@sonny.eddelbuettel.com>

Ted,

On Tue, Nov 16, 2004 at 09:39:14PM -0000, Ted Harding wrote:
> Hi Folks,
> 
> I'm sure I'm speaking for more than a few (though
> possibly a minority) here.
> 
> There's something of a hidden assumption that R users
> can readily download whatever they need from CRAN.
> 
> Some of us are on narrow bandwidth dialup connections,
> so downloading large quantities of stuff is out of the
> question (e.g. at approx. 5min/MB, it would take over
> 2 days to download a single CD). The meat of CRAN
> (including contributed packages and documentation)
> is enough to fill 5 CDs, though one individual probably
> wouldn't be interested in all of that.

I'll cc this reply to Mark Walker. His shop, budgetlinuxcds.com / blcds.com,
is one of the resellers of my Quantian 'scientific / cluster-computing
workstation on a bootable dvd' Linux distribution / environment
(see http://dirk.eddelbuettel.com/quantian for more on this).

Mark has been consistently responsive while offering a low-cost cd/dvd
service (of which I receive no cut, in case you're wondering about
disclaimers).  I think he'd be happy to add regular snapshots of certain
portions of http://cran.r-project.org/src/, maybe for the sources and/or
windows binaries, for his failry reasonable fees. 

> (And, before anyone asks, no I won't be seeing broadband
> in the foreseeable future!)
> 
> I've worked round this in various ways in the past.
> Some Linux distributions (e.g. RedHat, SuSE) come with the
> basics of R on their CDs, so when I've upgraded Linux

I cannot resist mentioning that Debian will almost surely have the best R
coverage with R, several dozen CRAN packages, ESS and many other goodies.
Debian may well not be for everyone, but maybe Ubuntu will make the initial
experience more pleasant.

Hope this helps, Dirk



> I've got a new R (though not the latest). Linux Emporium
> once obligingly did me a custom CD very cheaply with the
> contributed packages. A friend with high-bandwidth access
> did me the 5 CDs in return for a pub lunch. And so on.
> One can get round it by bothering someone.
> 
> What I'd like to suggest, for consideration, is that along
> with the stirling work done at many centres to set up and
> maintain mirrors of CRAN, some might consider offering also
> the service of burning CDs on request, for a reasonable
> charge. The difficulty, of course, is that it's going to
> take someone's time om something which may well not be
> their proper business.
> 
> (I'm prompted to think about this again by the emergence
> of R-2 -> 2.0.1 -> one day soon? -> 2.1. This is clearly
> a major advance in R and changes several things, so I'm
> looking at a major download operation, if not by me then
> by somebody that I can get CDs from, if I'm to be sure
> of being up to date on everything I'd like to have. And
> I'm sure I'm not the only one.)
> 
> I'd be interested in people's comments on this proposal.
> 
> Best wishes to all,
> Ted.
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
> Date: 16-Nov-04                                       Time: 21:39:14
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
If your hair is standing up, then you are in extreme danger.
      -- http://www.usafa.af.mil/dfp/cockpit-phys/fp1ex3.htm



From yzhang4 at pobox.une.edu.au  Wed Nov 17 05:47:59 2004
From: yzhang4 at pobox.une.edu.au (Yuandan Zhang)
Date: Wed, 17 Nov 2004 15:47:59 +1100
Subject: [R] Search engine for LINUX MOZILLA
Message-ID: <20041117154759.0b9cf790.yzhang4@pobox.une.edu.au>

Hi,

I am looking for help on searc engine at R htmlhelp page. Here is the system settings:

R 2.0.0 (2004-10-04).

Installed at Linux RH 9.0 (kernal 2.4.20-8) and FC2 (kernal 2.5.8). 

The web broswer is 
Mozilla 1.7
Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.7) Gecko/20040616

>From help.start(), web broswer started ok. However, the search engine does not respond. The status bar only shows done. In Windows settings, it shows "Applet started " or something similar.

I tried a few plugins, did not work.

Any suggest?

Yuandan



From joel3000 at gmail.com  Wed Nov 17 07:15:44 2004
From: joel3000 at gmail.com (Joel Bremson)
Date: Tue, 16 Nov 2004 22:15:44 -0800
Subject: [R] persp grid
Message-ID: <1253d67a04111622154ad48780@mail.gmail.com>

I've got a 4x4 matrix of points from a 2-way ANOVA I'd like to plot.

The x,y correspond to the treatment groups and look like this
((1,1),(1,2),(1,3),(1,4),(2,1),...).

The z is the 4x4 matrix.

How can I get persp to grid the x,y axis with only the numbers 1-4 on both?

Regards,

Joel Bremson
UC Davis Statistics Dept.



From jari.oksanen at oulu.fi  Wed Nov 17 07:25:54 2004
From: jari.oksanen at oulu.fi (Jari Oksanen)
Date: Wed, 17 Nov 2004 08:25:54 +0200
Subject: [R] CDs for R?
In-Reply-To: <XFMail.041116213914.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.041116213914.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <88ADDA4C-3861-11D9-86F5-000A95C76CA8@oulu.fi>


On 16 Nov 2004, at 23:39, (Ted Harding) wrote:
>
> Some of us are on narrow bandwidth dialup connections,
> so downloading large quantities of stuff is out of the
> question (e.g. at approx. 5min/MB, it would take over
> 2 days to download a single CD). The meat of CRAN
> (including contributed packages and documentation)
> is enough to fill 5 CDs, though one individual probably
> wouldn't be interested in all of that.

5 CDs sounds 4 too many. I once burnt CDs for my students, and they 
fitted nicely in one CD (Windows binaries, all packages as Windows 
binaries and sources, contributed documents).  I guess you can fit 
Windows, Mac and some Linux binaries all in one CD.

Now comes my suggestion to CRAN maintainer: this all would be easier, 
if you would produce a CD image file ('iso') that would contain a 
snapshot of the latest version: main binaries, all contributed 
packages, and docs. Getting somebody to help downloading this iso would 
be much easier than trying to collect all first and then make up your 
own cd image.

Actually, only Windows and Mac users need binary versions of packages. 
The former because they don't have tools to install from source, the 
latter because they don't know that they have the tools (being command 
line challenged).

To Dirk Eddelbuettel: Yes indeed, Ubuntu gives human face to Debian and 
is a much more pleasant experience. However, changing OS for R may be 
asking too much. Further, Ubuntu/Debian comes with a tiny and biased 
selection of packages, and if that's not your kind of bias, you have 
got to go to the Internet again. Further, Ubuntu (and other Linuxes) 
lag behind R. The current Ubuntu release comes with R 1.9.1, and it 
won't be upgraded but in the next release scheduled for April 2005 (and 
just in the same time as the next R, so that Ubuntu will be one R 
version off again). I guess the lag is even worse in packages.

cheers, jari oksanen
--
Jari Oksanen, Oulu, Finland



From Jin.Li at csiro.au  Wed Nov 17 07:39:24 2004
From: Jin.Li at csiro.au (Jin.Li@csiro.au)
Date: Wed, 17 Nov 2004 16:39:24 +1000
Subject: [R] How to plot this
Message-ID: <2BEE99D7F6F1484EBDD1D22167385E75375AEA@exqld1-ath.nexus.csiro.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041117/fa16805b/attachment.pl

From davidD at qimr.edu.au  Wed Nov 17 07:37:13 2004
From: davidD at qimr.edu.au (David Duffy)
Date: Wed, 17 Nov 2004 16:37:13 +1000 (EST)
Subject: [R] Re: variations on the theme of survSplit
In-Reply-To: <200411121118.iACB8RT8012951@hypatia.math.ethz.ch>
References: <200411121118.iACB8RT8012951@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.58.0411171625590.16345@orpheus.qimr.edu.au>

Danardono <daodao99 at student.umu.se> wrote:
>
> While waiting for 2.1, for those who need function[s] for this
> survival-splitting business, as I do,   this 'survcut' function below
> might be helpful.
> It is not an elegant nor efficient function but it works, at least for
> some examples below.
>
Ditto the following, for the case where there are multiple time-varying
(irreversible) binary covariates, here slicing as coarsely as possible.

#
# Create dataset for survival analysis with time-dependent covariate
# Gill-Anderson model
#
x <- data.frame(onset=c(46, 32, 53, 76, 64, 43),
                case=c(1,1,1,0,0,0),
                ooph=c(NA, 30, 38, 50, NA, NA),
                ocp=c(1,1,0,0,1,0),
                parity=c(2,0,1,3,3,2),
                age.preg=c(28,NA,27,20,22,23))

make.dep <- function(onset, case, time.dep, covs=NULL) {
  if (is.null(n.time.dep <- ncol(time.dep))) {
    if (!is.null(time.dep)) {
      n.time.dep <- 1
      time.dep <- as.matrix(time.dep)
    }else{
      n.time.dep <- 0
      warning("No time dependent covariates")
    }
  }
  if (is.null(n.covs <- ncol(covs))) {
    if (!is.null(covs)) {
      n.covs <- 1
      covs <- as.matrix(covs)
    }else{
      n.covs <- 0
    }
  }
  ordered.t <- t(apply(cbind(onset,time.dep),1,sort,na.last=TRUE))
  tot.time.dep <- apply(ordered.t,1,function(x) sum(!is.na(x)))
  ordered.t <- cbind(rep(0, nrow(ordered.t)), ordered.t)
  npars <- 4+n.time.dep+n.covs
  nrecs <- sum(tot.time.dep)
  new.x <- as.data.frame(matrix(nr=nrecs, nc=npars))
  names(new.x) <- c("start", "stop", "event", names(time.dep),names(covs),"episode")
  this.rec<-0
  for(i in 1:length(onset)) {
    for(j in 1:tot.time.dep[i]) {
      this.rec <- this.rec+1
      new.x[this.rec,1] <- ordered.t[i, j]
      new.x[this.rec,2] <- ordered.t[i, j+1]
      new.x[this.rec,3] <- 0
      new.x[this.rec,4:(3+n.time.dep)] <- (ordered.t[i,j]>=time.dep[i,])
      missing <- is.na(new.x[this.rec,])
      new.x[this.rec,missing] <- 0
      if (n.covs>0) {
        new.x[this.rec, (4+n.time.dep):(4+n.time.dep+n.covs)] <- covs[i,]
      }
      new.x[this.rec, npars]<-paste(i,j,sep=".")
    }
    new.x[this.rec,3]<-case[i]
  }
  new.x
}

David Duffy.



From ripley at stats.ox.ac.uk  Wed Nov 17 08:24:04 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 17 Nov 2004 07:24:04 +0000 (GMT)
Subject: [R] Search engine for LINUX MOZILLA
In-Reply-To: <20041117154759.0b9cf790.yzhang4@pobox.une.edu.au>
References: <20041117154759.0b9cf790.yzhang4@pobox.une.edu.au>
Message-ID: <Pine.LNX.4.61.0411170722060.2671@gannet.stats>

This is covered in the R-admin manual, linked to from that HTML page.
Please follow carefully the advice there.  In particular ensure that you 
have a version of Java that is said to work (preferably the latest, 
1.5.0).

On Wed, 17 Nov 2004, Yuandan Zhang wrote:

> Hi,
>
> I am looking for help on searc engine at R htmlhelp page. Here is the system settings:
>
> R 2.0.0 (2004-10-04).
>
> Installed at Linux RH 9.0 (kernal 2.4.20-8) and FC2 (kernal 2.5.8).
>
> The web broswer is
> Mozilla 1.7
> Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.7) Gecko/20040616
>
>> From help.start(), web broswer started ok. However, the search engine does not respond. The status bar only shows done. In Windows settings, it shows "Applet started " or something similar.
>
> I tried a few plugins, did not work.
>
> Any suggest?
>
> Yuandan
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Nov 17 08:27:13 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 17 Nov 2004 07:27:13 +0000 (GMT)
Subject: [R] Cross-correlated variables in kernel density estimation
In-Reply-To: <000001c4cc3e$95997f80$a74e8081@RSGIS>
References: <000001c4cc3e$95997f80$a74e8081@RSGIS>
Message-ID: <Pine.LNX.4.61.0411170726310.2671@gannet.stats>

Yes it can and it is in the reference.

On Tue, 16 Nov 2004, Adam Gobena wrote:

> Hi,
> I am wondering if the kde2d 2-D kernel density estimation function in the
> MASS package can take into account the effect of correlations between the
> variables. I couldn't find any achieved information on this issue.
> Unfortunately, I don't have the 2002 edition of Modern Applied Statistics
> with S by Venables and Ripley in case it was described there.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Tom.Mulholland at dpi.wa.gov.au  Wed Nov 17 08:34:31 2004
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Wed, 17 Nov 2004 15:34:31 +0800
Subject: [R] How to plot this
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA2D@afhex01.dpi.wa.gov.au>

If this is a quick and dirty process you want rather than learning all the capabilities that are in R then I would copy the density curve (or the bits you like) into your favourite image editor, and use it's capabilities to pretty it up.

However there are a number of options. Firstly you have chosen to plot density(y). When I looked at the help for density it gives the values returned by density. If you want a custom plot maybe you should try

dcurve <- density(y)

you could then directly access the $x and $y components as you would in any plot

For instance plot(density(y)) gives you the grey line. However plot(dcurve$x,dcurve$y,type = "l") gives you a different type of plot.

As for arrowheads one could create an appropriate "polygon" to stick at each end. Which for a one off might be a bit of overkill. Sometime in all of this you'll also probably encounter clipping, in which case par(xpd = TRUE) will often help. Just remember to turn if off or you may find unwanted graphics appearing later on.

For putting the labels where you want you could use "mtext." This gives you control over where you want to place the text.

A word of caution. If you are going to start prettying up you plots to very specific standards make sure that you are working on the final device from which you wish to take the final copy. Each of the devices have their own capabilities which are often not related to R but rather to their own environment. That is you can't get a plot looking perfect in a window and assume that the same code sent to a postscript device will produce identical results. 

R can give you very good graphics, often straight out of the box, but like any publishing process it can be a bit fiddly.

Tom Mulholland
Senior Demographer
Department for Planning and Infrastructure
Perth, WA, Australia.




-----Original Message-----
From: Jin.Li at csiro.au [mailto:Jin.Li at csiro.au]
Sent: Wednesday, 17 November 2004 2:39 PM
To: r-help at stat.math.ethz.ch
Subject: [R] How to plot this


Hi there,

 

I produced a plot using the following codes:

 

y<-rnorm(1000, 2, 0)

x0<-c(0, 0)

y0<-c(0, 0)

y1<-c(0, 1)

x1<-c(0, 4)

plot(density(y), ylab="Abundance of species", xlab="Environmental
gradient", main=" ", 

   xlim=c(0, 4), ylim=c(0, 1), lty=2, col=4, xaxt="n", yaxt="n",
frame.plot=F)

lines(x0, y1) # add an axis

lines(x1, y0) # add an axis

arrows(3.95, 0, 4, 0,  angle = 15, length = 0.1)

arrows(0, 0.98, 0, 1,  angle = 15, length = 0.1)

 

Please help me to remove the grey horizontal line and put the axis
labels closer to the axes. And also appreciate any suggestions on how to
make those arrows look nicer, e.g. a filled small arrow for each axis,
like what from points(0, 1,   pch=17), but a slightly narrowed one.
Thanks.

 

Regards,

 

Jin Li

========================

Jin Li, PhD

Climate Impacts Modeller

CSIRO Sustainable Ecosystems

Atherton, QLD 4883, Australia

========================

 


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Stefano.Guazzetti at ausl.re.it  Wed Nov 17 09:28:28 2004
From: Stefano.Guazzetti at ausl.re.it (Guazzetti Stefano)
Date: Wed, 17 Nov 2004 09:28:28 +0100
Subject: R: [R] How to plot this
Message-ID: <B8A1EED732379B44A7E59D22E82E444236537A@IMHOTEP.ausl.org>

Hi,
looking at "?plot.density" you will find a "zero.line" argument: 
set it to FALSE and no gray lines will appear in the plot.

plot(density(y), zero.line = F,  main=" ", ann = F,
  xlim = c(0, 4), ylim = c(0, 1), lty = 2, col = 4, axes = F)
#and the add
 mtext(side = 1, line = 0, text = "Environmental gradient" )
 mtext(side = 2, line = 0, text = "Abundance of species"  )
 arrows(0, 0, 4, 0,  angle = 15, length = 0.1, lwd=2)
 arrows(0, 0, 0, 1,  angle = 15, length = 0.1, lwd=2)


Hope this helps
Stefano

> -----Messaggio originale-----
> Da: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]Per conto di Jin.Li at csiro.au
> Inviato: mercoled?? 17 novembre 2004 7.39
> A: r-help at stat.math.ethz.ch
> Oggetto: [R] How to plot this
> 
> 
> Hi there,
> 
>  
> 
> I produced a plot using the following codes:
> 
>  
> 
> y<-rnorm(1000, 2, 0)
> 
> x0<-c(0, 0)
> 
> y0<-c(0, 0)
> 
> y1<-c(0, 1)
> 
> x1<-c(0, 4)
> 
> plot(density(y), ylab="Abundance of species", xlab="Environmental
> gradient", main=" ", 
> 
>    xlim=c(0, 4), ylim=c(0, 1), lty=2, col=4, xaxt="n", yaxt="n",
> frame.plot=F)
> 
> lines(x0, y1) # add an axis
> 
> lines(x1, y0) # add an axis
> 
> arrows(3.95, 0, 4, 0,  angle = 15, length = 0.1)
> 
> arrows(0, 0.98, 0, 1,  angle = 15, length = 0.1)
> 
>  
> 
> Please help me to remove the grey horizontal line and put the axis
> labels closer to the axes. And also appreciate any 
> suggestions on how to
> make those arrows look nicer, e.g. a filled small arrow for each axis,
> like what from points(0, 1,   pch=17), but a slightly narrowed one.
> Thanks.
> 
>  
> 
> Regards,
> 
>  
> 
> Jin Li
> 
> ========================
> 
> Jin Li, PhD
> 
> Climate Impacts Modeller
> 
> CSIRO Sustainable Ecosystems
> 
> Atherton, QLD 4883, Australia
> 
> ========================
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Wed Nov 17 10:00:00 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 17 Nov 2004 10:00:00 +0100
Subject: [R] R 2.0.0 Installation Problem
In-Reply-To: <200411161648.iAGGmQwB009099@mercury.cipic.ucdavis.edu>
References: <200411161648.iAGGmQwB009099@mercury.cipic.ucdavis.edu>
Message-ID: <419B1310.8060603@statistik.uni-dortmund.de>

David Rocke wrote:

> I and my students have been having an odd problem with this release,
> which is that packages are disappearing. After installation the
> package is found with the library command, but later in the same
> session or in a later session, the library command returns a not found
> error. Then later it is back. Happening on both Windows and OS X,
> mostly but not entirely with Bioconductor packages.
>

Please tell us more details. It never happened to anybody else that 
packages dissapear. Are the files still at the correct location? Have 
you instaled into another library tree?

Uwe Ligges


> David
> 
> 
> -----------------------------------------------------------------------
> | David M. Rocke, Professor               Phone:  (530) 752-0510      |
> | Division of Biostatistics (Medicine) and        (530) 752-7368      |
> | Department of Applied Science (Engineering)                         |
> | Co-Director of IDAV                     FAX:    (530) 752-8894      |
> | University of California, Davis         E-mail: dmrocke at ucdavis.edu |
> | Davis, CA 95616-8553                 www.cipic.ucdavis.edu/~dmrocke |
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From stuart.leask at nottingham.ac.uk  Wed Nov 17 10:05:48 2004
From: stuart.leask at nottingham.ac.uk (Stuart Leask)
Date: Wed, 17 Nov 2004 09:05:48 -0000
Subject: [R] CDs for R?
References: <XFMail.041116213914.Ted.Harding@nessie.mcc.ac.uk>
	<88ADDA4C-3861-11D9-86F5-000A95C76CA8@oulu.fi>
Message-ID: <005f01c4cc84$a1398fb0$f2e1f380@OPENZAURUS>

I note similar discussions re. 'linux live' distributions, and another key
point made there is that, with a moving target (ie. several significant
upgrades a year), one shouldn't contribute to the vast mountain of landfill
CDRs already represent.

Which makes me wonder about changing the model a bit ie. folks who want it
on CD send a CDRW or USB key with a stamped, self-addressed enveloped to
somewhere (central, or the 'buddy list' already suggested) where the
requested files will be burnt on. The 'cost' of burning these could be seen
as one consequence of the GPL!

That way, most of the 'manufacture & distribution' costs stay where they
should ie. with the person who wants the CD, and we aren't generating more
rapidly-useless CDs...

Stuart


----- Original Message ----- 
From: "Jari Oksanen" <jari.oksanen at oulu.fi>
To: <ted.harding at nessie.mcc.ac.uk>
Cc: <r-help at stat.math.ethz.ch>
Sent: Wednesday, November 17, 2004 6:25 AM
Subject: Re: [R] CDs for R?


>
> On 16 Nov 2004, at 23:39, (Ted Harding) wrote:
> >
> > Some of us are on narrow bandwidth dialup connections,
> > so downloading large quantities of stuff is out of the
> > question (e.g. at approx. 5min/MB, it would take over
> > 2 days to download a single CD). The meat of CRAN
> > (including contributed packages and documentation)
> > is enough to fill 5 CDs, though one individual probably
> > wouldn't be interested in all of that.
>
> 5 CDs sounds 4 too many. I once burnt CDs for my students, and they
> fitted nicely in one CD (Windows binaries, all packages as Windows
> binaries and sources, contributed documents).  I guess you can fit
> Windows, Mac and some Linux binaries all in one CD.
>
> Now comes my suggestion to CRAN maintainer: this all would be easier,
> if you would produce a CD image file ('iso') that would contain a
> snapshot of the latest version: main binaries, all contributed
> packages, and docs. Getting somebody to help downloading this iso would
> be much easier than trying to collect all first and then make up your
> own cd image.
>
> Actually, only Windows and Mac users need binary versions of packages.
> The former because they don't have tools to install from source, the
> latter because they don't know that they have the tools (being command
> line challenged).
>
> To Dirk Eddelbuettel: Yes indeed, Ubuntu gives human face to Debian and
> is a much more pleasant experience. However, changing OS for R may be
> asking too much. Further, Ubuntu/Debian comes with a tiny and biased
> selection of packages, and if that's not your kind of bias, you have
> got to go to the Internet again. Further, Ubuntu (and other Linuxes)
> lag behind R. The current Ubuntu release comes with R 1.9.1, and it
> won't be upgraded but in the next release scheduled for April 2005 (and
> just in the same time as the next R, so that Ubuntu will be one R
> version off again). I guess the lag is even worse in packages.
>
> cheers, jari oksanen
> --
> Jari Oksanen, Oulu, Finland
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html


This message has been scanned but we cannot guarantee that it and any
attachments are free from viruses or other damaging content: you are
advised to perform your own checks.  Email communications with the
University of Nottingham may be monitored as permitted by UK legislation.



From Ted.Harding at nessie.mcc.ac.uk  Wed Nov 17 10:37:20 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 17 Nov 2004 09:37:20 -0000 (GMT)
Subject: [R] how to estimate conditional density
In-Reply-To: <Pine.SOL.4.58.0411162214120.23688@rygar.gpcc.itd.umich.edu>
Message-ID: <XFMail.041117093720.Ted.Harding@nessie.mcc.ac.uk>

On 17-Nov-04 Yulei He wrote:
> Hi, there.
> 
> Suppose I have a bivariate data set y1 and y2. Can anybody
> tell me how to estimate the conditional density of f(y1|y2)
> and vice versa? Thanks.
> 
> Yulei

In the absence of a parametric model for the distribution,
a simple-minded approach could be the following:

1. Use 'f<-kde2d(...)' from the MASS library to generate a
   kernel density estimate of the bivariate distribution,
   ensuring that your (y1,y2) grid includes the value of y2
   at which you want to get f(y1|y2). Suppose that different
   values of y2 correspond to different rows of the matrix
   f$z in the returned result (see "?kde2d").

2. For the row [i] corresponding to the conditioning value
   of y2, normalise the values so that sum(f$z[i,]*dy1)=1,
   where dy1 is the step between different values of y1 in
   the grid used in (1).

The resulting normalised row of values is then an estimate
of f(y1|y2), for each such value of y2.

Similarly, applying (2) to the columns of f$z, you can get
an estimate of f(y2|y1).

[Note: for each single value of y2, you don't need to estimate
 the density of y2, i.e. for this purpose you can forget about
 the definition f(y1,y2)/f(y2) of f(y1|y2).]

Hoping this helps,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 17-Nov-04                                       Time: 09:37:20
------------------------------ XFMail ------------------------------



From phgrosjean at sciviews.org  Wed Nov 17 10:53:28 2004
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Wed, 17 Nov 2004 10:53:28 +0100
Subject: [R] The hidden costs of GPL software?
Message-ID: <200411170955.iAH9toV21278008@hedwig1.umh.ac.be>

Hello,

In the latest 'Scientific Computing World' magazine (issue 78, p. 22), there
is a review on free statistical software by Felix Grant ("doesn't have to
pay good money to obtain good statistics software"). As far as I know, this
is the first time that R is even mentioned in this magazine, given that it
usually discuss commercial products.

In this article, the analysis of R is interesting. It is admitted that R is
a great software with lots of potentials, but: "All in all, R was a good
lesson in the price that may have to be paid for free software: I spent many
hours relearning some quite basic things taken for granted in the commercial
package." Those basic things are releated with data import, obtention of
basic plots, etc... with a claim for a missing more intuitive GUI in order
to smooth a little bit the learning curve.

There are several R GUI projects ongoing, but these are progressing very
slowly. The main reason is, I believe, that a relatively low number of
programmers working on R are interested by this field. Most people wanting
such a GUI are basic user that do not (cannot) contribute... And if they
eventually become more knowledgeable, they tend to have other interests.

So, is this analysis correct: are there hidden costs for free software like
R in the time required to learn it? At least currently, for the people I
know (biologists, ecologists, oceanographers, ...), this is perfectly true.
This is even an insurmountable barrier for many of them I know, and they
have given up (they come back to Statistica, Systat, or S-PLUS using
exclusively functions they can reach through menus/dialog boxes).

Of course, the solution is to have a decent GUI for R, but this is a lot of
work, and I wonder if the intrinsic mechanism of GPL is not working against
such a development (leading to a very low pool of programmers actively
involved in the elaboration of such a GUI, in comparison to the very large
pool of competent developers working on R itself).

Do not misunderstand me: I don't give up with my GUI project, I am just
wondering if there is a general, ineluctable mechanism that leads to the
current R / R GUI situation as it stands,... and consequently to a "general
rule" that there are indeed most of the time "hidden costs" in free
software, due to the larger time required to learn it. I am sure there are
counter-examples, however, my feeling is that, for Linux, Apache, etc... the
GUI (if there is one) is often a way back in comparison to the potentials in
the software, leading to a steep learning curve in order to use all these
features.

I would be interested by your impressions and ideas on this topic.

Best regards,

Philippe Grosjean  

..............................................<??}))><........
 ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
 ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
 ) ) ) ) )   Mons-Hainaut University, Pentagone
( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
 ) ) ) ) )   6, av du Champ de Mars, 7000 Mons, Belgium  
( ( ( ( (       
 ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.33.12
( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
 ) ) ) ) )      
( ( ( ( (    web:   http://www.umh.ac.be/~econum
 ) ) ) ) )
..............................................................



From ligges at statistik.uni-dortmund.de  Wed Nov 17 10:55:33 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 17 Nov 2004 10:55:33 +0100
Subject: [R] violinplot options
In-Reply-To: <419A0145.5080306@fokus.fraunhofer.de>
References: <419A0145.5080306@fokus.fraunhofer.de>
Message-ID: <419B2015.6020201@statistik.uni-dortmund.de>

Tanja Zseby wrote:

> Hi,
> 
> I am using the function  vioplot() to generate violin plots. Now I would 
> like to add a label to the y axix and a title to the diagram.
> Just setting ylab didnt work. Is it possible to set such options for the 
> function ?
> I tried also with the function simple.violinplot, but also with this I 
> couldnt set the options.
> 
> Kind Regards
> Tanja

Looks like nobody else has responded so far.
If you are talking about the function in the package also called 
"vioplot": The function is not very well designed. But since there is 
not much code in it, it is quite easy to add additional functionality 
yourself by adapting the whole function.

Uwe Ligges




> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From wolski at molgen.mpg.de  Wed Nov 17 10:09:04 2004
From: wolski at molgen.mpg.de (Witold Eryk Wolski)
Date: Wed, 17 Nov 2004 10:09:04 +0100
Subject: [R] R/S-related projects on  Sourceforge?   Trove Categorization 
Message-ID: <419B1530.1090208@molgen.mpg.de>

Hi R-Users and Developers,

Several months ago I made a request on Sourceforge to add the R/S - 
programming language to the _Trove_ categorization. ("The Trove is a 
means to convey basic metainformation about your project.")

Today I got the following response of one of the sourceforge admins.

<SNIP>

SourceForge.net will consider the inclusion of a programming
language within the Trove system when we host at least 5
projects based on that language.  Please advise: Do you know
of 5 projects hosted on SourceForge.net based on this language?
<SNIP>


If anyone of you knew about R-packages, or projects using the R/S programming language, which are hosted on sourceforge, please reply to this thread. I hope that your answers will enable me to give more then 5 examples of R projects hosted on Sourceforge.

Yours Eryk


Ps.

The ID of my original "feature request"  on Sourceforge is 967697.
https://sourceforge.net/tracker/?func=detail&atid=350001&aid=967697&group_id=1

-- 
Dipl. bio-chem. Witold Eryk Wolski
MPI-Moleculare Genetic
Ihnestrasse 63-73 14195 Berlin
tel: 0049-30-83875219                 __("<    _
http://www.molgen.mpg.de/~wolski      \__/    'v'
http://r4proteomics.sourceforge.net    ||    /   \
mail: witek96 at users.sourceforge.net    ^^     m m
      wolski at molgen.mpg.de



From ligges at statistik.uni-dortmund.de  Wed Nov 17 11:15:49 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 17 Nov 2004 11:15:49 +0100
Subject: [R] persp grid
In-Reply-To: <1253d67a04111622154ad48780@mail.gmail.com>
References: <1253d67a04111622154ad48780@mail.gmail.com>
Message-ID: <419B24D5.2040304@statistik.uni-dortmund.de>

Joel Bremson wrote:

> I've got a 4x4 matrix of points from a 2-way ANOVA I'd like to plot.
> 
> The x,y correspond to the treatment groups and look like this
> ((1,1),(1,2),(1,3),(1,4),(2,1),...).
> 
> The z is the 4x4 matrix.
> 
> How can I get persp to grid the x,y axis with only the numbers 1-4 on both?

The first question is whether it will be nice to have a surface by just 
4x4 points (of probably non-continous variables), the second point is 
that it is very hard to have a fixed number of tick marks in persp().
I think it is almost impossible without changing internal code.
As an ugly workaround, you can add text add the corresponding positions 
along the axes using the transformation matrix given in the examples of 
?persp.

Uwe Ligges



> Regards,
> 
> Joel Bremson
> UC Davis Statistics Dept.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Wed Nov 17 11:22:33 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 17 Nov 2004 11:22:33 +0100
Subject: [R] changing character to a vector name
In-Reply-To: <BAY12-F174FTA7OIkUq0001157c@hotmail.com>
References: <BAY12-F174FTA7OIkUq0001157c@hotmail.com>
Message-ID: <419B2669.20407@statistik.uni-dortmund.de>

Laura Holt wrote:

> Dear R People:
> 
> I would like to generate a vector/variable name from within a loop to be 
> passed to a table
> function.
> 
> This is what I have so far:
> 
>> assign("p1",paste("raw3.df$",rw2$V1[3],sep=""))
>> p1
 >
> [1] "raw3.df$CITIZEN"

Life is much easier: Consider to use raw3.df[[rw2$V1[3]]] instead.

Uwe Ligges


>>
> Essentially, I want to use the raw3.df$CITIZEN along with another value 
> to generate a table.
> 
> However, I'm stuck here.  I know this is incredibly stupid.
> 
> Thanks in advance.
> Sincerely
> Laura Holt
> mailto: lauraholt_983 at hotmail.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ernesto at ipimar.pt  Wed Nov 17 11:09:23 2004
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Wed, 17 Nov 2004 10:09:23 +0000
Subject: [R] R/S-related projects on  Sourceforge?   Trove Categorization
In-Reply-To: <419B1530.1090208@molgen.mpg.de>
References: <419B1530.1090208@molgen.mpg.de>
Message-ID: <1100686162.8471.4.camel@mordor.ipimar.pt>

Hi,

I have 2 _small_ projects hosted in sf.net that use R 

FLR :: R for fisheries science (http://flr.sf.net)
fsap: fish stock assessment for R (http://sf.net/projects/fsap)

The first one is getting some hip and the second is dying ...

Hope it helps.

Regards

EJ

On Wed, 2004-11-17 at 09:09, Witold Eryk Wolski wrote:
> Hi R-Users and Developers,
> 
> Several months ago I made a request on Sourceforge to add the R/S - 
> programming language to the _Trove_ categorization. ("The Trove is a 
> means to convey basic metainformation about your project.")
> 
> Today I got the following response of one of the sourceforge admins.
> 
> <SNIP>
> 
> SourceForge.net will consider the inclusion of a programming
> language within the Trove system when we host at least 5
> projects based on that language.  Please advise: Do you know
> of 5 projects hosted on SourceForge.net based on this language?
> <SNIP>
> 
> 
> If anyone of you knew about R-packages, or projects using the R/S programming language, which are hosted on sourceforge, please reply to this thread. I hope that your answers will enable me to give more then 5 examples of R projects hosted on Sourceforge.
> 
> Yours Eryk
> 
> 
> Ps.
> 
> The ID of my original "feature request"  on Sourceforge is 967697.
> https://sourceforge.net/tracker/?func=detail&atid=350001&aid=967697&group_id=1



From Lorenz.Gygax at fat.admin.ch  Wed Nov 17 11:29:42 2004
From: Lorenz.Gygax at fat.admin.ch (Lorenz.Gygax@fat.admin.ch)
Date: Wed, 17 Nov 2004 11:29:42 +0100
Subject: [R] The hidden costs of GPL software?
Message-ID: <BF74FADD4B44554CA7E53D0B5242CD6A01FC62ED@evd-s7014.bk.evdad.admin.ch>

> So, is this analysis correct: are there hidden costs for free 
> software like R in the time required to learn it? At least
> currently, for the people I know (biologists, ecologists,
> oceanographers, ...), this is perfectly true. This is even an
> insurmountable barrier for many of them I know, and they
> have given up (they come back to Statistica, Systat, or S-PLUS
> using exclusively functions they can reach through menus/dialog
> boxes).

I guess you are right, in that the steep initial learning curve could be
smoothed for beginners. On the other hand I do not see how a GUI for R could
cover more than the bare essentials because the available functionality is
so vast. We also have S-Plus at our research institution and even there, I
see, that people who do not know about the underlying code have difficulties
in using the GUI.

I personally believe that it is more a question how one is used to do
statistics. Click and drag is the norm. (And I guess it is usually also the
norm of how people/scientists use other Software.) In my eyes, using code
instead, means that one is able to repeat the steps of an evaluation easily
and to document at the same time what has been done. Very soon evaluations
(and data handling) can be done far more efficiently than with click and
drag. All these advantages outweigh the initial costs by several orders of
magnitude. Thus, in my opinion it is more a question of education such that
people might realize how they can work efficiently and cleanly. Perhaps one
could even say that such an approach is more scientific because, in
principal, it can be easily communicated and reproduced.

It is, of course, easy for me to make these statements, as in the meantime I
have been using S (S-Plus and R) for - gosh - over 10 years. But I see in
some projects that I supervise that people get started easily with a snippet
of code that I provide and the insight of the usefulness of such a work
approach is usually easily within reach.

Lorenz
- 
Lorenz Gygax, Dr. sc. nat.
Tel: +41 (0)52 368 33 84 / lorenz.gygax at fat.admin.ch      
Centre for proper housing of ruminants and pigs
Swiss Federal Veterinary Office



From francoisromain at free.fr  Wed Nov 17 11:39:34 2004
From: francoisromain at free.fr (=?ISO-8859-1?Q?Romain_Fran=E7ois?=)
Date: Wed, 17 Nov 2004 11:39:34 +0100
Subject: [R] beginner's problem in displaying large data
In-Reply-To: <419A9D49.1030501@pdf.com>
References: <b68812e7041116150774a548b9@mail.gmail.com>
	<419A9D49.1030501@pdf.com>
Message-ID: <419B2A66.2060807@free.fr>

You can also try :

ab <- matrix(rnorm(10000),nc=5)
edit(ab)


Hope this helps.

Spencer Graves a ??crit :

>      1.  Did you try "dim(sample.data)"?  Is it actually 2200 by 15?  
> Or are you reading in just some subset of the data?  If it is 2200 by 
> 15, could you also please do "class(sample.data)"?
>      2.  I just got a full listing from the following:
>      (tst <- data.frame(array(rnorm(2200), dim=c(2200, 15))))
>
>      You might try this.  With R 2.0.0patched under Windows 2000, I 
> got rows 1:2200 flying by 3 times, each with 5 columns.
>      3.  Have you considered doing plots (including qqnorm) of numeric 
> variables and tables of character variables?  These can often reveal 
> problems I might never see in a simple scan of numbers.
>      4.  "PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html".  At minimum, please tell 
> us which version of R under which operating system, and specifically 
> what you did to get it into R and how you know it's 2200 by 15.
>      hope this helps.      spencer graves
>
> Terry Mu wrote:
>
>> I got a sample data (let's call it sample.data), which is about 2200 
>> by 15.
>>
>> I tried to take a look of all data
>>
>>  
>>
>>> sample.data
>>>   
>>
>>
>> It shows only a part of data that I thought was a corner. It does not
>> really affect my job, but I thought it is nice to have a look of all
>> data. I can see individual records and they are fine.
>>
>> Is this normal because of buffer size or some reasons? Can I use other
>> commands or change some settings to display all data?
>>
>> Thanks,
>> Terry
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>  
>>
>

-- 
Romain Fran??ois
25, avenue Guy Moquet
94 400 Vitry sur seine
FRANCE
_______________________
_______________________

francoisromain at free.fr
01 46 80 65 60
06 18 39 14 69



From jps at srres.com  Wed Nov 17 11:49:24 2004
From: jps at srres.com (Jan P. Smit)
Date: Wed, 17 Nov 2004 17:49:24 +0700
Subject: [R] The hidden costs of GPL software?
In-Reply-To: <200411170955.iAH9toV21278008@hedwig1.umh.ac.be>
References: <200411170955.iAH9toV21278008@hedwig1.umh.ac.be>
Message-ID: <419B2CB4.7010708@srres.com>

Dear Phillippe,

Very interesting. The URL of the article is 
http://www.scientific-computing.com/scwsepoct04free_statistics.html.

Best regards,

Jan Smit


Philippe Grosjean wrote:
> Hello,
> 
> In the latest 'Scientific Computing World' magazine (issue 78, p. 22), there
> is a review on free statistical software by Felix Grant ("doesn't have to
> pay good money to obtain good statistics software"). As far as I know, this
> is the first time that R is even mentioned in this magazine, given that it
> usually discuss commercial products.
> 
> In this article, the analysis of R is interesting. It is admitted that R is
> a great software with lots of potentials, but: "All in all, R was a good
> lesson in the price that may have to be paid for free software: I spent many
> hours relearning some quite basic things taken for granted in the commercial
> package." Those basic things are releated with data import, obtention of
> basic plots, etc... with a claim for a missing more intuitive GUI in order
> to smooth a little bit the learning curve.
> 
> There are several R GUI projects ongoing, but these are progressing very
> slowly. The main reason is, I believe, that a relatively low number of
> programmers working on R are interested by this field. Most people wanting
> such a GUI are basic user that do not (cannot) contribute... And if they
> eventually become more knowledgeable, they tend to have other interests.
> 
> So, is this analysis correct: are there hidden costs for free software like
> R in the time required to learn it? At least currently, for the people I
> know (biologists, ecologists, oceanographers, ...), this is perfectly true.
> This is even an insurmountable barrier for many of them I know, and they
> have given up (they come back to Statistica, Systat, or S-PLUS using
> exclusively functions they can reach through menus/dialog boxes).
> 
> Of course, the solution is to have a decent GUI for R, but this is a lot of
> work, and I wonder if the intrinsic mechanism of GPL is not working against
> such a development (leading to a very low pool of programmers actively
> involved in the elaboration of such a GUI, in comparison to the very large
> pool of competent developers working on R itself).
> 
> Do not misunderstand me: I don't give up with my GUI project, I am just
> wondering if there is a general, ineluctable mechanism that leads to the
> current R / R GUI situation as it stands,... and consequently to a "general
> rule" that there are indeed most of the time "hidden costs" in free
> software, due to the larger time required to learn it. I am sure there are
> counter-examples, however, my feeling is that, for Linux, Apache, etc... the
> GUI (if there is one) is often a way back in comparison to the potentials in
> the software, leading to a steep learning curve in order to use all these
> features.
> 
> I would be interested by your impressions and ideas on this topic.
> 
> Best regards,
> 
> Philippe Grosjean  
> 
> ..............................................<??}))><........
>  ) ) ) ) )
> ( ( ( ( (    Prof. Philippe Grosjean
>  ) ) ) ) )
> ( ( ( ( (    Numerical Ecology of Aquatic Systems
>  ) ) ) ) )   Mons-Hainaut University, Pentagone
> ( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
>  ) ) ) ) )   6, av du Champ de Mars, 7000 Mons, Belgium  
> ( ( ( ( (       
>  ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.33.12
> ( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
>  ) ) ) ) )      
> ( ( ( ( (    web:   http://www.umh.ac.be/~econum
>  ) ) ) ) )
> ..............................................................
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From plummer at iarc.fr  Wed Nov 17 11:59:34 2004
From: plummer at iarc.fr (Martyn Plummer)
Date: Wed, 17 Nov 2004 11:59:34 +0100
Subject: [R] RPMS for Fedora/RedHat
Message-ID: <1100689174.4619.12.camel@nemo>

An RPM for R 2.0.1 on Fedora Core 3/i386 should now be available on a
CRAN mirror near you.  

Unfortunately, I am temporarily unable to build RPMS for previous
versions of Fedora and Red Hat Linux due to problems with the mach
chroot system on FC3.  I expect this situation will be resolved soon and
I will let you know. Thank you for your patience. 

(NB This is not a call for volunteers to contribute RPMS).

Prof. Brian Ripley has contributed RPMS for Fedora Core 3/x86_64 and
these will soon be available on CRAN.

Martyn



From henric.nilsson at statisticon.se  Wed Nov 17 12:37:47 2004
From: henric.nilsson at statisticon.se (Henric Nilsson)
Date: Wed, 17 Nov 2004 12:37:47 +0100
Subject: [R] violinplot options
In-Reply-To: <419B2015.6020201@statistik.uni-dortmund.de>
References: <419A0145.5080306@fokus.fraunhofer.de>
	<419B2015.6020201@statistik.uni-dortmund.de>
Message-ID: <6.1.2.0.0.20041117121916.0acd14a0@10.0.10.66>

At 10:55 2004-11-17 +0100, Uwe Ligges wrote:

>Looks like nobody else has responded so far.

I actually wrote something, but forgot to send it...

>If you are talking about the function in the package also called 
>"vioplot": The function is not very well designed. But since there is not 
>much code in it, it is quite easy to add additional functionality yourself 
>by adapting the whole function.

That's the ambitious approach!

Tanja: If you just want to add the title and y axis label, the title 
function suffices. E.g.

 > library(vioplot)
 > vioplot(runif(10))
 > title(main = "A title", ylab = "The y label")

HTH,
Henric



From siegfried.gonzi at stud.uni-graz.at  Wed Nov 17 13:43:10 2004
From: siegfried.gonzi at stud.uni-graz.at (Siegfried Gonzi)
Date: Wed, 17 Nov 2004 13:43:10 +0100
Subject: [R] log-normal distribution and shapiro test
Message-ID: <419B475E.4070903@stud.uni-graz.at>

Hello:

Yes I know that sort of questions comes up quite often. But with all due 
respect I din't find how to perform what I want. I am searching archives 
and bowsing manuals but it isn't there, though, it is a ridiculous 
simple task for the experienced R user.

I have data and can do the following with them:

==
hist(y, prob=TRUE)
lines(density(y,bw=0.03)
==

The result actually is a nice histogram superimposed by a line plot.

The histogram is a bit skewed to the left. My assumption actually is 
that a log-normal transformation would cure the problem. But how the 
hell can one plot such a density function or Gaussian function which has 
logarithmic scales on x axis.

For example I tried:

==
plot(hist(y),log="x")

or

plot(hist(log10(y)),log="x")
==

But with no avail. I want  my axis like: 1,10,100



What would be other methods to test whether the data are logaritmically 
distributed.

A last question to the Shapiro-Wilk test. Were can I get critical 
parameters? I mean I get for my distribution: W=0.9686, p-value=6.887e-07.
What does that mean? Yes I have got some books about statics, but none 
of them says what one should do with the values then. The logaritmic 
transformation "shapiro.test(log10(y))" says: W=0.9773, p-value= 2.512e-05.

Sorry for disturbing you. Although, it is really no homework. I need it 
for my Phd in physics; after a lengthy computation on the computer I 
would like to go to see whether the outputs are log-normal or normal 
distributed.

Regards,
Siegfried Gonzi
==
University of Graz
Institute for Physics
Tel.: ++43-316-380-8620
==



From Ted.Harding at nessie.mcc.ac.uk  Wed Nov 17 13:34:31 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 17 Nov 2004 12:34:31 -0000 (GMT)
Subject: [R] The hidden costs of GPL software?
In-Reply-To: <200411170955.iAH9toV21278008@hedwig1.umh.ac.be>
Message-ID: <XFMail.041117123431.Ted.Harding@nessie.mcc.ac.uk>

On 17-Nov-04 Philippe Grosjean wrote:
> Hello,
> 
> In the latest 'Scientific Computing World' magazine
> (issue 78, p. 22), there is a review on free statistical
> software by Felix Grant ("doesn't have to pay good money
> to obtain good statistics software"). As far as I know,
> this is the first time that R is even mentioned in this
> magazine, given that it usually discuss commercial products.

Hi Philippe,
Thanks for a most interesting post on this question. Further
comments below. Felix Grant's article is excellent, and well
balanced.

> In this article, the analysis of R is interesting. It is
> admitted that R is a great software with lots of potentials,
> but: "All in all, R was a good lesson in the price that may
> have to be paid for free software: I spent many hours
> relearning some quite basic things taken for granted in the
> commercial package." Those basic things are releated with
> data import, obtention of basic plots, etc... with a claim
> for a missing more intuitive GUI in order to smooth a little
> bit the learning curve.

It would better represent the balanced view of the article
to further quote:

  "In fact, the whole file menu in R looks either elegantly
   uncluttered of frightenly obscure, depending on your point
   of view."

  "It [the effort of learning] is the price paid, just as the
   dollars or euros for a commercial package would be. For
   that price, I've learned a great deal -- and nor only
   about R. And I shall remember it when I next have to find
   a heavyweight solution for a big problem presented by a
   small charitable client with an invisible budget. It's a
   huge, awe-inspiring package -- easier to perceive as such
   because the power is not hidden beneath a cosmetic veneer."

This last remark is, in my view, particularly significant.
See below.

> There are several R GUI projects ongoing, but these are
> progressing very slowly. The main reason is, I believe,
> that a relatively low number of programmers working on R
> are interested by this field. Most people wanting such a
> GUI are basic user that do not (cannot) contribute... 
> And if they eventually become more knowledgeable, they
> tend to have other interests.
> 
> So, is this analysis correct: are there hidden costs for
> free software like R in the time required to learn it?
> At least currently, for the people I know (biologists,
> ecologists, oceanographers, ...), this is perfectly true.
> This is even an insurmountable barrier for many of them
> I know, and they have given up (they come back to Statistica,
> Systat, or S-PLUS using exclusively functions they can
> reach through menus/dialog boxes).

Non-GUI vs GUI is not intrinsically linked to Free Software
as such. There are well-known FS programs which are essentially
GUI-based -- as an easy example, consider all the FS Web
Browsers such as Netscape, Mozilla, ... . If you want the
graphics experiences offered by the Web, you're in a graphics
screen anyway, and so it may as well be programmed around
a GUI. Others, such as OpenOffice, have deliberately built
on a GUI approach in order to emulate The Other Thing.

There are a lot of FS programs which offer a GUI, usually
somewhat on the basic side, which nonetheless encapsulates
the entire functionality of the program and saves the user
the task of composing a possibly complex command-line or
even a script.

The comment "hidden beneath a cosmetic veneer" is, in my
view, somewhat directly linked to commercial software.
If you sell software, you want a big market. So you want
to include the people who will never learn how to work
software from a command line; and the sweeter the taste of
the eye candy, the more such people will feel enjoyment
in using the software. The fact that their usage is limited
to what has been pre-programmed into the menus is not going
to affect many such people, since typically their useage
is limited to a very small subset of what is in fact possible.
This in turn leads, of course, to the phenomenon of
"software-driven analysis", where people only do what the
GUI allows (or, more precisely, easily allows); and this
leads on in turn to a culture in which people tend to believe
that Statistics is what they can do with a particular
software package.

S-Plus does its best to compromise: as well as GUI access
to a pretty wide range of functions, there is the Command
Line Window where the user can explicitly type in commands.
(I dare say many R users, in S-Plus, may tend to work in
the latter since they are already used to it.) But, as always
in a GUI, one can tend to get lost in the ramifications.
Also, things like the big arrays of tiny icons you get when
you click on the "2D Plots" or "3D Plots" buttons in the
S-Plus toolbar can be trying on the eyes and time-consuming
to pick through.

> Of course, the solution is to have a decent GUI for R,
> but this is a lot of work, and I wonder if the intrinsic
> mechanism of GPL is not working against such a development
> (leading to a very low pool of programmers actively involved
> in the elaboration of such a GUI, in comparison to the very
> large pool of competent developers working on R itself).
> 
> Do not misunderstand me: I don't give up with my GUI project,
> I am just wondering if there is a general, ineluctable
> mechanism that leads to the current R / R GUI situation as
> it stands,... and consequently to a "general rule" that
> there are indeed most of the time "hidden costs" in free
> software, due to the larger time required to learn it.
> I am sure there are counter-examples, however, my feeling
> is that, for Linux, Apache, etc... the GUI (if there is one)
> is often a way back in comparison to the potentials in
> the software, leading to a steep learning curve in order to
> use all these features.

Often, I think, in the Free Software world, people get involved
because they want to produce something which achieves a task.
Once they have a program which does that, then their aim is
satisfied. The GUI, in many cases, would be additional work
which would add nothing to what the software can do in terms
of tasks to be achieved. So in such cases, yes, I would tend
to agree that there is an intrinsic mechanism that discourages
work on a GUI for its own sake. You can add to that the fact
that once a developer has got to the point of creating such
software, successful in the tasks, they may have got beyond
the point at which they can readily sympathise with users who
have not acquired such skills: they no longer perceive, from
their own experience, that there is a problem.

However, this leaves people like you, having colleagues who
"come back to Statistica, Systat, or S-PLUS using exclusively
functions they can reach through menus/dialog boxes." By this
experience, you are aware of the problem, and rightly feel
that they would be helped by having access to the sort of
GUI/Menu interface that they are used to using.

One genuine benefit that the GUI offers, especially to
beginners with a particular software package, is that the
resources of the software can perhaps more easily and rapidly
be explored through the GUI, rather than searching laboriously
through the documentation of functions, extra packages, and
so on. This means that they more readily come to perceive
what is available though of course this is limited to what
the GUI will show them. But a good "Help" window can break
that barrier.

Perhaps R itself is less helpful than it might be in this
respect. The R-help list bristles with queries of the form
"How can I do X?", which I think is evidence of a problem.
While some of these queries clearly originate from people
who have taken no trouble to explore readily accessible
information, many others can not be so easily dismissed.

If you know something about what you're after, once you
realise that a judiciously formulated "help.search" can
throw up a lot of possibilities you are well on your way.
So, for instance (as in a recent query about 2-D Fourier
transform for spatial data) 'help.search("fourier")' gives
relevant information.

This, though, still fails for information in packages which
you have not installed. Perhaps I'm about to reveal my own
culpable ignorance here, but I'm not aware of a "full R info"
package which would be installed as part of R-base, being
a database of info about R-base itself and also every current
additional package, such that a "help.search" would show
all resources -- including those not installed -- which
match a query (and flag the non-installed ones as such so
that the user knows what to install for a particular purpose).

Whether this needs to be supplemented by a GUI is a point
that could be discussed from several points of view.
Philippe's biological/oceanographic users no doubt would
be considerably helped, provided they can in due course
come to the point where they can start to work "beyond
the GUI" (if indeed they need to).

Personally, however, I find that GUI work is slower and
more error-prone than command-line work. Swanning the
mouse around the screen, visually idebtifying icons and
buttons, clicking on this and that in order to see whether
it's what you want, and so on, is much more time-consuming
than typiing in a command.
And God help you if you accidentally click on something
destructive!

I'll close with an immortal quotation (from Charles Curran,
of the UK Unix Users Group):

  "I can touch-type, but I can't touch-mouse"

Best wishes to all,
Ted.

> I would be interested by your impressions and ideas on this topic.
> 
> Best regards,
> 
> Philippe Grosjean  
> 
> ..............................................<??}))><........

                                                   /\
                                                 /   |
  .............................<??}))><........  :)    >=---
                                                 \   |
                                                   \/

Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 17-Nov-04                                       Time: 12:34:31
------------------------------ XFMail ------------------------------



From f.calboli at ucl.ac.uk  Wed Nov 17 13:49:45 2004
From: f.calboli at ucl.ac.uk (Federico Calboli)
Date: Wed, 17 Nov 2004 13:49:45 +0100
Subject: [R] The hidden costs of GPL software?
Message-ID: <1100695785.6737.44.camel@badger.home>

Philippe Grosjean wrote:
> I would be interested by your impressions and ideas on this topic.

I have found that "user friendly" packages make a lot of assumptions and
take a lot of decisions for the user. This makes things easy, but you do
not really know what is going on, and I'd say this is a hidden cost of
"commercial" software. 

I wrote to the list in February asking how to reproduce some results
previously obtained with Statistica. It turned out that Statistica does
some data manipulation without telling the user, with poor documentation
and no options or choice. Do you trust results obtained this way? I
don't. 

So I'd argue that the lack of a GUI is a good thing, because it forces
the users to think a bit more about what they want to do, and gives more
control on what is going on.

Best,

Federico Calboli
-- 
Federico C. F. Calboli

Dipartimento di Biologia Evoluzionistica Sperimentale
Universit?? di Bologna
Via Selmi, 3
40126 Bologna - ITALY

Tel - +39 051 2094187
Fax - +39 051 2094286
f.calboli at ucl.ac.uk
fcalboli at alma.unibo.it



From tjrc at sanger.ac.uk  Wed Nov 17 13:58:56 2004
From: tjrc at sanger.ac.uk (Tim Cutts)
Date: Wed, 17 Nov 2004 12:58:56 +0000
Subject: [R] R 2.0.0 Installation Problem
In-Reply-To: <419B1310.8060603@statistik.uni-dortmund.de>
References: <200411161648.iAGGmQwB009099@mercury.cipic.ucdavis.edu>
	<419B1310.8060603@statistik.uni-dortmund.de>
Message-ID: <7127D3BE-3898-11D9-A2B1-000A95B2B140@sanger.ac.uk>


On 17 Nov 2004, at 9:00 am, Uwe Ligges wrote:

> David Rocke wrote:
>
>> I and my students have been having an odd problem with this release,
>> which is that packages are disappearing. After installation the
>> package is found with the library command, but later in the same
>> session or in a later session, the library command returns a not found
>> error. Then later it is back. Happening on both Windows and OS X,
>> mostly but not entirely with Bioconductor packages.
>>
>
> Please tell us more details. It never happened to anybody else that 
> packages dissapear. Are the files still at the correct location? Have 
> you instaled into another library tree?

Have you installed them on a network volume?  I've seen this sort of 
behaviour with overloaded NFS servers in the past.

Tim

-- 
Dr Tim Cutts
Informatics Systems Group, Wellcome Trust Sanger Institute
GPG: 1024D/E3134233 FE3D 6C73 BBD6 726A A3F5  860B 3CDD 3F56 E313 4233



From emanuela.rossi at unimib.it  Wed Nov 17 13:59:40 2004
From: emanuela.rossi at unimib.it (Emanuela Rossi)
Date: Wed, 17 Nov 2004 13:59:40 +0100
Subject: [R] frailty and time-dependent covariate
Message-ID: <008401c4cca5$4d532020$36ec8495@Emanuela>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041117/4baaf211/attachment.pl

From vito_ricci at yahoo.com  Wed Nov 17 14:10:11 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Wed, 17 Nov 2004 14:10:11 +0100 (CET)
Subject: [R] R: log-normal distribution and shapiro test
Message-ID: <20041117131011.32761.qmail@web41205.mail.yahoo.com>

Hi,

from what you're writing:
"The logaritmic transformation
"shapiro.test(log10(y))" says: W=0.9773, p-value=
2.512e-05." it seems the log-values are not
distributed normally and so original data are not
distributed like a log-normal: the p-value is
extremally small!

Other tests for normality are available in package:
nortest

compare the log-transformation of your ecdf with
normal cdf: see ? ecdf

use qqnorm and qqplot

did you calculate skewness and kurtosis? see in
package fBasics.

I remember to you that the log-normal distribution as
three parameters: shape parameter, location parameter
and scale parameter. Transfroming by the simple log,
you are missing the location parameter, or implicitely
 you assuming is =0.

See:
http://www.itl.nist.gov/div898/handbook/eda/section3/eda3669.htm
for more news about log-normal distribution.

I hope I give you a little help.
Best
Vito


 

you wrote:

Hello:

Yes I know that sort of questions comes up quite
often. But with all due 
respect I din't find how to perform what I want. I am
searching archives 
and bowsing manuals but it isn't there, though, it is
a ridiculous 
simple task for the experienced R user.

I have data and can do the following with them:

==
hist(y, prob=TRUE)
lines(density(y,bw=0.03)
==

The result actually is a nice histogram superimposed
by a line plot.

The histogram is a bit skewed to the left. My
assumption actually is 
that a log-normal transformation would cure the
problem. But how the 
hell can one plot such a density function or Gaussian
function which has 
logarithmic scales on x axis.

For example I tried:

==
plot(hist(y),log="x")

or

plot(hist(log10(y)),log="x")
==

But with no avail. I want  my axis like: 1,10,100



What would be other methods to test whether the data
are logaritmically 
distributed.

A last question to the Shapiro-Wilk test. Were can I
get critical 
parameters? I mean I get for my distribution:
W=0.9686, p-value=6.887e-07.
What does that mean? Yes I have got some books about
statics, but none 
of them says what one should do with the values then.
The logaritmic 
transformation "shapiro.test(log10(y))" says:
W=0.9773, p-value= 2.512e-05.

Sorry for disturbing you. Although, it is really no
homework. I need it 
for my Phd in physics; after a lengthy computation on
the computer I 
would like to go to see whether the outputs are
log-normal or normal 
distributed.

Regards,
Siegfried Gonzi
==
University of Graz
Institute for Physics
Tel.: ++43-316-380-8620

=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese http://www.modugno.it/archivio/cat_palese.shtml



From tdjtdj at umich.edu  Wed Nov 17 14:37:42 2004
From: tdjtdj at umich.edu (Timothy D. Johnson)
Date: Wed, 17 Nov 2004 08:37:42 -0500
Subject: [R] R package installation
Message-ID: <DB78B092-389D-11D9-BA19-000A95A1DC28@umich.edu>

Dear Marco,

I was given an excerpt with your problem about installing package on
a MAC, such as Hmisc.

I had the same problems and found a work around.

I have not had any trouble loading in source packages since, include
Hmisc and Design, acepack and vgam.

First, I downloaded and installed the g77 compiler.

I use a progam named FINK to find, download and intall g77 (so first I 
installed FINK
then from within FINK I downloaded/installed the g77 compiler.) Do a 
"Google" search
for FINK, it is easy to find and install.

After g77 was installed I had to make a symbolic link so R could find 
it:

ln -s \sw\bin\g77 \usr\bin\g77  (I think I had to make a link to my gcc 
compiler also)
\n -s \sw\bin\gcc \usr\bin\g77

It looks like you already have the g77 compiler from the message.  the 
next mesage you can also remedy by symbolic links.  Try

ln -s /sw/lib/gcc   /usr/local/lib/gcc
ln -s /sw/lib/gcc/powerpc-apple-darwin7.5.0   
\usr\local\lib\gcc\powerpc-apple-darwin6.8
ln -s /sw/lib/gcc/powerpc-apple-darwin7.5.0/3.4.1   
\usr\local\lib\gcc\powerpc-apple-darwin6.8\3.4.2

The first directory path in each of the above may be specific to your 
configuration for gcc.

But this did work for me, and if you find the correct location for gcc/ 
powerpc-apple-darwinX.Y.Z/U.V.W, you should have no trouble either.

Good luck.

Sincerely

Tim Johnson
Adjunct Asst. Professor
University of Michigan



From bates at stat.wisc.edu  Wed Nov 17 14:40:51 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 17 Nov 2004 07:40:51 -0600
Subject: [R] Non-Linear Regression on a Matrix
In-Reply-To: <OF18EA0EA7.8603EDA2-ON86256F4F.000FE4DC-86256F4F.00102277@mmm.com>
References: <OF18EA0EA7.8603EDA2-ON86256F4F.000FE4DC-86256F4F.00102277@mmm.com>
Message-ID: <419B54E3.5080703@stat.wisc.edu>

apjaworski at mmm.com wrote:
> 
> 
> 
> 
> If your "non-linear function (A, B)" is parametric nls should do it for
> you.  If you have R version 2 (perhaps even 1.9) do ?nls to see the help
> page.  Older versions of R require library(nls) first.
> 
> Hope this helps,
> 
> Andy
> 
> __________________________________
> Andy Jaworski
> 518-1-01
> Process Laboratory
> 3M Corporate Research Laboratory
> -----
> E-mail: apjaworski at mmm.com
> Tel:  (651) 733-6092
> Fax:  (651) 736-3122
> 
> 
>                                                                            
>              Diana Abdueva                                                 
>              <diana.abdueva at gm                                             
>              ail.com>                                                   To 
>              Sent by:                  r-help at stat.math.ethz.ch            
>              r-help-bounces at st                                          cc 
>              at.math.ethz.ch                                               
>                                                                    Subject 
>                                        [R] Non-Linear Regression on a      
>              11/16/2004 08:33          Matrix                              
>              PM                                                            
>                                                                            
>                                                                            
>              Please respond to                                             
>                Diana Abdueva                                               
>              <diana.abdueva at gm                                             
>                  ail.com>                                                  
>                                                                            
>                                                                            
> 
> 
> 
> 
> Hi, I'm terribly sorry for submitting my primitive question, I'm a
> beginner in R and was hoping to get some help re: non-linear fit.
> 
> I have a 2D data with the following structure:
> 
> A     B        C
> 1      1      111
> 1      2      121
> 1      3      131
> 2      1      141
> 2      2      151
> 2      3      161
> 3      1      171
> 3      2      181
> 3      3      191
> 
> I'm trying to fit C = non-linear function (A,B). I was wondering if
> there's a package that would save my time of doing direct least square
> estimation.
> 
> Thank you,
> Diana

By "non-linear" do you mean something like a response surface model that 
has quadratic terms in A and B and an interaction term?

If so, you can fit the model using the lm function, as in

 > rs <- read.table("/tmp/rs.dat", header = TRUE)
 > rs
   A B   C
1 1 1 111
2 1 2 121
3 1 3 131
4 2 1 141
5 2 2 151
6 2 3 161
7 3 1 171
8 3 2 181
9 3 3 191
 > fm <- lm(C ~ A * B + I(A^2) + I(B^2), rs)
 > fm

Call:
lm(formula = C ~ A * B + I(A^2) + I(B^2), data = rs)

Coefficients:
(Intercept)            A            B       I(A^2)       I(B^2) 
  A:B
   7.100e+01    3.000e+01    1.000e+01   -1.174e-15    7.217e-16 
-4.008e-15



From HDoran at air.org  Wed Nov 17 14:42:00 2004
From: HDoran at air.org (Doran, Harold)
Date: Wed, 17 Nov 2004 08:42:00 -0500
Subject: [R] "Impossible to run" error message when using Sweave
Message-ID: <88EAF3512A55DF46B06B1954AEF73F74068134A1@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041117/7535f0e4/attachment.pl

From baron at psych.upenn.edu  Wed Nov 17 14:47:09 2004
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Wed, 17 Nov 2004 08:47:09 -0500
Subject: [R] The hidden costs of GPL software?
In-Reply-To: <XFMail.041117123431.Ted.Harding@nessie.mcc.ac.uk>
References: <200411170955.iAH9toV21278008@hedwig1.umh.ac.be>
	<XFMail.041117123431.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <20041117134709.GA3117@psych>

On 11/17/04 12:34, Ted Harding wrote:
>This, though, still fails for information in packages which
>you have not installed. Perhaps I'm about to reveal my own
>culpable ignorance here, but I'm not aware of a "full R info"
>package which would be installed as part of R-base, being
>a database of info about R-base itself and also every current
>additional package, such that a "help.search" would show
>all resources -- including those not installed -- which
>match a query (and flag the non-installed ones as such so
>that the user knows what to install for a particular purpose).

This is one of the purpose of my R search page.  I have all
packages installed.  You can also search the help list, etc., in
the same search.  Some people have bookmarks for it.  Of course
you need to be connected to the internet.

I think that any attempt to replicate this for a single user, or
even the packages, would be difficult.

BUT, it might help to install just the help pages for all
packages, without the packages themselves.  Then help.search()
would find things.  (I have no interest in figuring out how to do
this, but maybe someone else does.)

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron
R search page: http://finzi.psych.upenn.edu/



From charles.edwin.white at us.army.mil  Wed Nov 17 15:00:22 2004
From: charles.edwin.white at us.army.mil (White, Charles E WRAIR-Wash DC)
Date: Wed, 17 Nov 2004 09:00:22 -0500
Subject: [R] R 2.0.0 Installation Problem
Message-ID: <8BAEC5E546879B4FAA536200A292C614052743@AMEDMLNARMC135.amed.ds.army.mil>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041117/07a77f3e/attachment.pl

From siegfried.gonzi at stud.uni-graz.at  Wed Nov 17 15:01:37 2004
From: siegfried.gonzi at stud.uni-graz.at (Siegfried Gonzi)
Date: Wed, 17 Nov 2004 15:01:37 +0100
Subject: [R] Re: R: log-normal distribution and shapiro test
References: <20041117131011.32761.qmail@web41205.mail.yahoo.com>
Message-ID: <419B59C1.40606@stud.uni-graz.at>

Hi:

Thanks for your answer.

Do you know how to test whether the data would fit to a gamma-distribution?

How can I call fBasics?

Note: I installed R-language on my Macintosh today; I have used the 
binary -- pre compiled -- package.

Some of the R-help facilties do not function on my Mac.

Again to my data: How can I compute the skew? I think I lack some basic 
packages - right?

The curious things actually is that the median and the mean are quite 
similar, e.g. 0.19 and 0.2 respectively; the skew is about 1.0 (I 
calculated the skew by my own computer code in Bigloo).

The problem actually is: my boss expects from me that I make some tests; 
personally I am a bit generous and everything is a Gaussian or 
log-Gaussian distribution, because how can I be sure that the underlying 
data to not have any serious flaws? Statistics is black art - right?

Regards,
S. Gonzi



Vito Ricci wrote:

>Hi,
>
>from what you're writing:
>"The logaritmic transformation
>"shapiro.test(log10(y))" says: W=0.9773, p-value=
>2.512e-05." it seems the log-values are not
>distributed normally and so original data are not
>distributed like a log-normal: the p-value is
>extremally small!
>
>Other tests for normality are available in package:
>nortest
>
>compare the log-transformation of your ecdf with
>normal cdf: see ? ecdf
>
>use qqnorm and qqplot
>
>did you calculate skewness and kurtosis? see in
>package fBasics.
>
>I remember to you that the log-normal distribution as
>three parameters: shape parameter, location parameter
>and scale parameter. Transfroming by the simple log,
>you are missing the location parameter, or implicitely
> you assuming is =0.
>
>See:
>http://www.itl.nist.gov/div898/handbook/eda/section3/eda3669.htm
>for more news about log-normal distribution.
>
>I hope I give you a little help.
>Best
>Vito
>
>
> 
>
>you wrote:
>
>Hello:
>
>Yes I know that sort of questions comes up quite
>often. But with all due 
>respect I din't find how to perform what I want. I am
>searching archives 
>and bowsing manuals but it isn't there, though, it is
>a ridiculous 
>simple task for the experienced R user.
>
>I have data and can do the following with them:
>
>==
>hist(y, prob=TRUE)
>lines(density(y,bw=0.03)
>==
>
>The result actually is a nice histogram superimposed
>by a line plot.
>
>The histogram is a bit skewed to the left. My
>assumption actually is 
>that a log-normal transformation would cure the
>problem. But how the 
>hell can one plot such a density function or Gaussian
>function which has 
>logarithmic scales on x axis.
>
>For example I tried:
>
>==
>plot(hist(y),log="x")
>
>or
>
>plot(hist(log10(y)),log="x")
>==
>
>But with no avail. I want  my axis like: 1,10,100
>
>
>
>What would be other methods to test whether the data
>are logaritmically 
>distributed.
>
>A last question to the Shapiro-Wilk test. Were can I
>get critical 
>parameters? I mean I get for my distribution:
>W=0.9686, p-value=6.887e-07.
>What does that mean? Yes I have got some books about
>statics, but none 
>of them says what one should do with the values then.
>The logaritmic 
>transformation "shapiro.test(log10(y))" says:
>W=0.9773, p-value= 2.512e-05.
>
>Sorry for disturbing you. Although, it is really no
>homework. I need it 
>for my Phd in physics; after a lengthy computation on
>the computer I 
>would like to go to see whether the outputs are
>log-normal or normal 
>distributed.
>
>Regards,
>Siegfried Gonzi
>==
>University of Graz
>Institute for Physics
>Tel.: ++43-316-380-8620
>
>=====
>Diventare costruttori di soluzioni
>Became solutions' constructors
>
>"The business of the statistician is to catalyze 
>the scientific learning process."  
>George E. P. Box
>
>
>Visitate il portale http://www.modugno.it/
>e in particolare la sezione su Palese http://www.modugno.it/archivio/cat_palese.shtml
>
>
>		
>___________________________________ 




From vito_ricci at yahoo.com  Wed Nov 17 15:16:16 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Wed, 17 Nov 2004 15:16:16 +0100 (CET)
Subject: [R] Re: R: log-normal distribution and shapiro test
In-Reply-To: <419B59C1.40606@stud.uni-graz.at>
Message-ID: <20041117141616.41138.qmail@web41215.mail.yahoo.com>

Dear Siegfried,

you could find fBasics at this web address:

http://cran.at.r-project.org/src/contrib/Descriptions/fBasics.html

it includes skewness() and kurtosis() function.

I usually run R on WIN 2000 and I don't know MAC!

I can suggest to use Kolgomorov-Smirnov test to test
whether the data would fit to a gamma-distribution
 
? ks.test 

and this web page for theory about ks test:

http://www.itl.nist.gov/div898/handbook/eda/section3/eda35g.htm

Having mean and median quite similar don't mean it's a
normal distribution, but, maybe, could be only a
simmetric distribution!

Best
Vito

 --- Siegfried Gonzi
<siegfried.gonzi at stud.uni-graz.at> ha scritto: 
> Hi:
> 
> Thanks for your answer.
> 
> Do you know how to test whether the data would fit
> to a gamma-distribution?
> 
> How can I call fBasics?
> 
> Note: I installed R-language on my Macintosh today;
> I have used the 
> binary -- pre compiled -- package.
> 
> Some of the R-help facilties do not function on my
> Mac.
> 
> Again to my data: How can I compute the skew? I
> think I lack some basic 
> packages - right?
> 
> The curious things actually is that the median and
> the mean are quite 
> similar, e.g. 0.19 and 0.2 respectively; the skew is
> about 1.0 (I 
> calculated the skew by my own computer code in
> Bigloo).
> 
> The problem actually is: my boss expects from me
> that I make some tests; 
> personally I am a bit generous and everything is a
> Gaussian or 
> log-Gaussian distribution, because how can I be sure
> that the underlying 
> data to not have any serious flaws? Statistics is
> black art - right?
> 
> Regards,
> S. Gonzi
> 
> 
> 
> Vito Ricci wrote:
> 
> >Hi,
> >
> >from what you're writing:
> >"The logaritmic transformation
> >"shapiro.test(log10(y))" says: W=0.9773, p-value=
> >2.512e-05." it seems the log-values are not
> >distributed normally and so original data are not
> >distributed like a log-normal: the p-value is
> >extremally small!
> >
> >Other tests for normality are available in package:
> >nortest
> >
> >compare the log-transformation of your ecdf with
> >normal cdf: see ? ecdf
> >
> >use qqnorm and qqplot
> >
> >did you calculate skewness and kurtosis? see in
> >package fBasics.
> >
> >I remember to you that the log-normal distribution
> as
> >three parameters: shape parameter, location
> parameter
> >and scale parameter. Transfroming by the simple
> log,
> >you are missing the location parameter, or
> implicitely
> > you assuming is =0.
> >
> >See:
>
>http://www.itl.nist.gov/div898/handbook/eda/section3/eda3669.htm
> >for more news about log-normal distribution.
> >
> >I hope I give you a little help.
> >Best
> >Vito
> >
> >
> > 
> >
> >you wrote:
> >
> >Hello:
> >
> >Yes I know that sort of questions comes up quite
> >often. But with all due 
> >respect I din't find how to perform what I want. I
> am
> >searching archives 
> >and bowsing manuals but it isn't there, though, it
> is
> >a ridiculous 
> >simple task for the experienced R user.
> >
> >I have data and can do the following with them:
> >
> >==
> >hist(y, prob=TRUE)
> >lines(density(y,bw=0.03)
> >==
> >
> >The result actually is a nice histogram
> superimposed
> >by a line plot.
> >
> >The histogram is a bit skewed to the left. My
> >assumption actually is 
> >that a log-normal transformation would cure the
> >problem. But how the 
> >hell can one plot such a density function or
> Gaussian
> >function which has 
> >logarithmic scales on x axis.
> >
> >For example I tried:
> >
> >==
> >plot(hist(y),log="x")
> >
> >or
> >
> >plot(hist(log10(y)),log="x")
> >==
> >
> >But with no avail. I want  my axis like: 1,10,100
> >
> >
> >
> >What would be other methods to test whether the
> data
> >are logaritmically 
> >distributed.
> >
> >A last question to the Shapiro-Wilk test. Were can
> I
> >get critical 
> >parameters? I mean I get for my distribution:
> >W=0.9686, p-value=6.887e-07.
> >What does that mean? Yes I have got some books
> about
> >statics, but none 
> >of them says what one should do with the values
> then.
> >The logaritmic 
> >transformation "shapiro.test(log10(y))" says:
> >W=0.9773, p-value= 2.512e-05.
> >
> >Sorry for disturbing you. Although, it is really no
> >homework. I need it 
> >for my Phd in physics; after a lengthy computation
> on
> >the computer I 
> >would like to go to see whether the outputs are
> >log-normal or normal 
> >distributed.
> >
> >Regards,
> >Siegfried Gonzi
> >==
> >University of Graz
> >Institute for Physics
> >Tel.: ++43-316-380-8620
> >
> >=====
> >Diventare costruttori di soluzioni
> >Became solutions' constructors
> >
> >"The business of the statistician is to catalyze 
> >the scientific learning process."  
> >George E. P. Box
> >
> >
> >Visitate il portale http://www.modugno.it/
> >e in particolare la sezione su Palese
> http://www.modugno.it/archivio/cat_palese.shtml
> >
> >
> >		
> >___________________________________ 

> >
> >
> 
> 
> 
>  

=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese http://www.modugno.it/archivio/cat_palese.shtml



From roebuck at odin.mdacc.tmc.edu  Wed Nov 17 15:17:23 2004
From: roebuck at odin.mdacc.tmc.edu (Paul Roebuck)
Date: Wed, 17 Nov 2004 08:17:23 -0600 (CST)
Subject: [R] R package installation
In-Reply-To: <DB78B092-389D-11D9-BA19-000A95A1DC28@umich.edu>
References: <DB78B092-389D-11D9-BA19-000A95A1DC28@umich.edu>
Message-ID: <Pine.OSF.4.58.0411170803550.167957@odin.mdacc.tmc.edu>

On Wed, 17 Nov 2004, Timothy D. Johnson wrote:

> I was given an excerpt with your problem about installing package on
> a MAC, such as Hmisc.
>
> I had the same problems and found a work around.
>
> I have not had any trouble loading in source packages since, include
> Hmisc and Design, acepack and vgam.
>
> First, I downloaded and installed the g77 compiler.
>
> I use a progam named FINK to find, download and intall g77 (so first I
> installed FINK then from within FINK I downloaded/installed the
> g77 compiler.)
> [SNIP details]

Still seems like simply installing this from this link would be simpler;
FINK is unnecessary. Of course, this is already covered in the OS X FAQ
<http://cran.r-project.org/bin/macosx/RAqua-FAQ.html> although some
explanation could be added that a Fortran77 compiler might also be needed
for linking other packages.

<http://hpc.sf.net/g77v3.4-bin.tar.gz>

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From pburns at pburns.seanet.com  Wed Nov 17 15:27:49 2004
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Wed, 17 Nov 2004 14:27:49 +0000
Subject: [R] The hidden costs of GPL software?
In-Reply-To: <419B2CB4.7010708@srres.com>
References: <200411170955.iAH9toV21278008@hedwig1.umh.ac.be>
	<419B2CB4.7010708@srres.com>
Message-ID: <419B5FE5.8040009@pburns.seanet.com>

I'm a big advocate -- perhaps even fanatic -- of  making R easier for
novices in order to spread its use, but I'm not convinced that  a GUI
(at least in the traditional form) is the most valuable approach.

Perhaps an overly harsh summary of some of Ted Harding's statements
is: You can make a truck easier to get into by taking off the wheels, but
that doesn't make it more useful.

In terms of GUIs, I think what R should focus on is the ability for  user's
to make their own specialized GUI.  So that a knowledgeable programmer
at an installation can create a system that is easy for unsophisticated
users for the limited number of tasks that are to be done.  The ultimate
users may not even need to know that R exists.

I think Ted Harding was on  the mark when he said that it is the help
system that needs enhancement.  I can imagine a system that gets the
user to the right function and then helps fill in the arguments; all of the
time pointing them towards the command line rather than away from
it.

The author of the referenced article highlighted some hidden costs of R,
but did not highlight the hidden benefits (because they were hidden from
him).  A big benefit of R is all of the bugs that aren't in it (which may or
may not be due to its free status).

Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Jan P. Smit wrote:

> Dear Phillippe,
>
> Very interesting. The URL of the article is 
> http://www.scientific-computing.com/scwsepoct04free_statistics.html.
>
> Best regards,
>
> Jan Smit
>
>
> Philippe Grosjean wrote:
>
>> Hello,
>>
>> In the latest 'Scientific Computing World' magazine (issue 78, p. 
>> 22), there
>> is a review on free statistical software by Felix Grant ("doesn't 
>> have to
>> pay good money to obtain good statistics software"). As far as I 
>> know, this
>> is the first time that R is even mentioned in this magazine, given 
>> that it
>> usually discuss commercial products.
>>
[ ...]

>>
>
>



From danbebber at forestecology.co.uk  Wed Nov 17 15:35:25 2004
From: danbebber at forestecology.co.uk (Dan Bebber)
Date: Wed, 17 Nov 2004 14:35:25 -0000
Subject: [R] summary.lme() vs. anova.lme()
In-Reply-To: <200411161118.iAGB81MF014111@hypatia.math.ethz.ch>
Message-ID: <001001c4ccb2$ad71ae10$7d2501a3@plants.ox.ac.uk>

Dear R list:

I modelled changes in a variable (mconc) over time (d) for individuals
(replicate) given one of three treatments (treatment) using:
mconc.lme <- lme(mconc~treatment*poly(d,2), random=~poly(d,2)|replicate,
data=my.data)

summary(mconc.lme) shows that the linear coefficient of one of the
treatments is significantly different to zero, viz.
                            Value Std.Error  DF   t-value p-value
					...	     ... ...      ...
...
treatmentf:poly(d, 2)1  1.3058562 0.5072409 315  2.574430  0.0105

But anova(mconc.lme) gives a non-significant result for the treatment*time
interaction, viz.
                     numDF denDF   F-value p-value
(Intercept)              1   315 159.17267  <.0001
treatment                2    39   0.51364  0.6023
poly(d, 2)               2   315  17.43810  <.0001
treatment:poly(d, 2)     4   315   2.01592  0.0920

Pinheiro & Bates (2000) only discusses anova() for single arguments briefly
on p.90.
I would like to know whether these results indicate that the significant
effect found in summary(mconc.lme) is spurious (perhaps due to
multiplicity).

Many thanks,
Dan Bebber

Department of Plant Sciences
University of Oxford
South Parks Road
Oxford OX1 3RB
UK
Tel. 01865 275000



From edd at debian.org  Wed Nov 17 15:54:41 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 17 Nov 2004 08:54:41 -0600
Subject: [R] CDs for R?
In-Reply-To: <88ADDA4C-3861-11D9-86F5-000A95C76CA8@oulu.fi>
References: <XFMail.041116213914.Ted.Harding@nessie.mcc.ac.uk>
	<88ADDA4C-3861-11D9-86F5-000A95C76CA8@oulu.fi>
Message-ID: <20041117145441.GA9902@sonny.eddelbuettel.com>

On Wed, Nov 17, 2004 at 08:25:54AM +0200, Jari Oksanen wrote:
> 
> On 16 Nov 2004, at 23:39, (Ted Harding) wrote:
> Now comes my suggestion to CRAN maintainer: this all would be easier, 
> if you would produce a CD image file ('iso') that would contain a 
> snapshot of the latest version: main binaries, all contributed 
> packages, and docs. Getting somebody to help downloading this iso would 
> be much easier than trying to collect all first and then make up your 
> own cd image.

It's volunteer effort, so someone actually has to do this. Can you help?

> Actually, only Windows and Mac users need binary versions of packages. 
> The former because they don't have tools to install from source, the 
> latter because they don't know that they have the tools (being command 
> line challenged).
> 
> To Dirk Eddelbuettel: Yes indeed, Ubuntu gives human face to Debian and 
> is a much more pleasant experience. However, changing OS for R may be 
> asking too much. Further, Ubuntu/Debian comes with a tiny and biased 
> selection of packages, and if that's not your kind of bias, you have 
> got to go to the Internet again. Further, Ubuntu (and other Linuxes) 

Again, it reflects the interests of the volunteers involved. If you want to
see other things done, come join in and do them.

> lag behind R. The current Ubuntu release comes with R 1.9.1, and it 
> won't be upgraded but in the next release scheduled for April 2005 (and 
> just in the same time as the next R, so that Ubuntu will be one R 
> version off again). I guess the lag is even worse in packages.

This actually requires a response. Here is a quick log (from my mail folder)
about what new packages (of mine, can't speak for others) got uploaded
recently -- in most cases, this is on the day of the source release, so the
lag would be close to zero.

 575     Nov 08 Debian Installe (  20) rpy_0.4.0-1_i386.changes ACCEPTED
 576     Nov 09 Debian Installe (  14) strucchange_1.2.7-1_i386.changes ACCEPTED
 577     Nov 11 Debian Installe (  12) cluster_1.9.6-3_i386.changes ACCEPTED
 578     Nov 11 Debian Installe (  12) survival_2.15-2_i386.changes ACCEPTED
 579     Nov 12 Debian Installe (  26) octave2.1_2.1.62-1_i386.changes ACCEPTED
 580     Nov 12 Debian Installe (  12) cluster_1.9.6-4_i386.changes ACCEPTED
 581     Nov 12 Debian Installe (  14) mgcv_1.1.8-1_i386.changes ACCEPTED
 582     Nov 12 Debian Installe (  14) tseries_0.9.24-1_i386.changes ACCEPTED
 583     Nov 12 Debian Installe (  14) lattice_0.10.14-1_i386.changes ACCEPTED
 584     Nov 12 Debian Installe (  12) mgcv_1.1.8-2_i386.changes ACCEPTED
 585     Nov 13 Debian Installe (  14) dbd-odbc_1.13-1_i386.changes ACCEPTED
 586     Nov 13 Debian Installe (  14) ole-storage-lite_0.14-1_i386.changes ACCEPTED
 587     Nov 13 Debian Installe (  12) semidef-oct_2.2-21_i386.changes ACCEPTED
 588     Nov 14 Debian Installe (  15) wajig_2.0.13-1_i386.changes ACCEPTED
 589     Nov 14 Debian Installe (  14) sm_2.0.13-1_i386.changes ACCEPTED
 590     Nov 14 Debian Installe (  12) vr_7.2.10-2_i386.changes ACCEPTED
 591     Nov 15 Debian Installe (  34) r-base_2.0.1-1_i386.changes ACCEPTED
 592     Nov 15 Debian Installe (  24) gretl_1.3.0-1_i386.changes ACCEPTED
 593     Nov 16 Debian Installe (  14) survival_2.16-1_i386.changes ACCEPTED
 594     Nov 17 Debian Installe (  14) wajig_2.0.14-1_i386.changes ACCEPTED
 
I could go back further if you want. 
 
Now, if and when these get pressed into a release by Debian or Ubuntu I do
not control. Which is, I guess, why we're discussing archive snapshots in
this thread. 

Hth, Dirk
























-- 
If your hair is standing up, then you are in extreme danger.
      -- http://www.usafa.af.mil/dfp/cockpit-phys/fp1ex3.htm



From murdoch at stats.uwo.ca  Wed Nov 17 15:57:05 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 17 Nov 2004 09:57:05 -0500
Subject: [R-gui] Re: [R] The hidden costs of GPL software?
In-Reply-To: <419B5FE5.8040009@pburns.seanet.com>
References: <200411170955.iAH9toV21278008@hedwig1.umh.ac.be>
	<419B2CB4.7010708@srres.com> <419B5FE5.8040009@pburns.seanet.com>
Message-ID: <2qomp0lbje3evtcp245vbe0lqgk2nfvore@4ax.com>

On Wed, 17 Nov 2004 14:27:49 +0000, Patrick Burns
<pburns at pburns.seanet.com> wrote :

>In terms of GUIs, I think what R should focus on is the ability for  user's
>to make their own specialized GUI.  So that a knowledgeable programmer
>at an installation can create a system that is easy for unsophisticated
>users for the limited number of tasks that are to be done.  The ultimate
>users may not even need to know that R exists.

I think there is (slow) movement towards that.  Certainly it's
possible now (you can add menus to Rgui in Windows, you can do nice
things like Rcmdr using TCL/TK on any platform).   However, designing
a nice GUI is very hard work.

>I think Ted Harding was on  the mark when he said that it is the help
>system that needs enhancement.  I can imagine a system that gets the
>user to the right function and then helps fill in the arguments; all of the
>time pointing them towards the command line rather than away from
>it.

That would be helpful, and the only really difficult part would be the
first part:  getting the user to the right function.  help.search()
sometimes works, but often people ask for the wrong thing.

After that, R knows a lot about the structure of its help files, so it
could display all of the arguments with their defaults and the help
text that corresponds to each argument, as well as the help text for
the rest of the help file.

Probably the main obstacle to getting this is finding someone with the
time and interest to do it.

Duncan Murdoch



From dimitris.rizopoulos at med.kuleuven.ac.be  Wed Nov 17 16:03:26 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Wed, 17 Nov 2004 16:03:26 +0100
Subject: [R] summary.lme() vs. anova.lme()
References: <001001c4ccb2$ad71ae10$7d2501a3@plants.ox.ac.uk>
Message-ID: <00a901c4ccb6$972cb420$0540210a@www.domain>

Hi Dan,

check the `type' argument of `anova.lme()' which defaults to 
"sequential". This is also discussed in Pinheiro and Bates but I don't 
have the book with me now to trace the page.

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Dan Bebber" <danbebber at forestecology.co.uk>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, November 17, 2004 3:35 PM
Subject: [R] summary.lme() vs. anova.lme()


> Dear R list:
>
> I modelled changes in a variable (mconc) over time (d) for 
> individuals
> (replicate) given one of three treatments (treatment) using:
> mconc.lme <- lme(mconc~treatment*poly(d,2), 
> random=~poly(d,2)|replicate,
> data=my.data)
>
> summary(mconc.lme) shows that the linear coefficient of one of the
> treatments is significantly different to zero, viz.
>                            Value Std.Error  DF   t-value p-value
> ...      ... ...      ...
> ...
> treatmentf:poly(d, 2)1  1.3058562 0.5072409 315  2.574430  0.0105
>
> But anova(mconc.lme) gives a non-significant result for the 
> treatment*time
> interaction, viz.
>                     numDF denDF   F-value p-value
> (Intercept)              1   315 159.17267  <.0001
> treatment                2    39   0.51364  0.6023
> poly(d, 2)               2   315  17.43810  <.0001
> treatment:poly(d, 2)     4   315   2.01592  0.0920
>
> Pinheiro & Bates (2000) only discusses anova() for single arguments 
> briefly
> on p.90.
> I would like to know whether these results indicate that the 
> significant
> effect found in summary(mconc.lme) is spurious (perhaps due to
> multiplicity).
>
> Many thanks,
> Dan Bebber
>
> Department of Plant Sciences
> University of Oxford
> South Parks Road
> Oxford OX1 3RB
> UK
> Tel. 01865 275000
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From assuncao.senra at portugalmail.com  Wed Nov 17 16:07:34 2004
From: assuncao.senra at portugalmail.com (assuncao.senra@portugalmail.com)
Date: Wed, 17 Nov 2004 15:07:34 +0000
Subject: [R] Multi-way tables
Message-ID: <1100704054.419b6936e9e1b@webmail4.portugalmail.pt>


Hello,
can someone tell me how to extract a partial table from a multiway table?
Something else: I tried to do basic operations on cross-tables, like 
subtracting two cross tables with same dimensions and i was unable to do so. 
Is there another way?

thanks!

__________________________________________________________



From bates at stat.wisc.edu  Wed Nov 17 16:12:59 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 17 Nov 2004 09:12:59 -0600
Subject: [R] "Impossible to run" error message when using Sweave
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F74068134A1@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F74068134A1@dc1ex2.air.org>
Message-ID: <419B6A7B.2060901@stat.wisc.edu>

Doran, Harold wrote:
> Dear List:
> 
> I have a large dataset of multiple schools. My goal is to produce a
> separate tex file for each school that plots some of the student
> achievement scores. Essentially, the aim is to develop a custom report
> for each school. To accomplish this, I have code for a loop that gets
> sourced into R and then Sweaves the multiple files to create the
> individual school reports.
> 
> Here is the code for the loop
> 
> schnum.list <- as.vector(unique(cmu$schid))
> 
> for(current.school in schnum.list) {
>     school.df <- subset(cmu, cmu$schid==current.school)
>     school.grades <- as.vector(unique(school.df$grade))
> 
>         sname <- paste(paste("read", current.school,
> sep=""),"Rnw",sep=".")
>         system(paste("schoolread.Rnw", paste("Reading/", sname, sep=""),
> sep=" "))
>         Sweave(file= paste("Reading/", sname, sep=""))
> }
> 
> 
> However, this begins to work and then produces the following errors
> 
> Writing to file read151-496-2982.tex
> Processing code chunks ...
> Error in file(con, "r") : unable to open connection
> In addition: Warning messages: 
> 1: Impossible to run 'G:\SWEAVE~1\SCHOOL~2.RNW
> Reading/read151-496-2982.Rnw 
> 2: cannot open file `Reading/read151-496-2982.Rnw' 
> 
> I'm not sure exactly what the impossible to run error is. I think this
> is on the verge of running correctly, but could use a little help.
> 
> Thanks,
> 
> Harold
> 
> Ver 1.9.0, Windows XP
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

It appears that you don't have the full path name for the input file 
correctly stated.  Rather than using paste to construct names, I would 
suggest using file.path.  You may also find it helpful to use getwd() to 
get the current working directory.



From abunn at whrc.org  Wed Nov 17 16:26:06 2004
From: abunn at whrc.org (Andy Bunn)
Date: Wed, 17 Nov 2004 10:26:06 -0500
Subject: [R] Time difference in months? (difftime, units)
In-Reply-To: <001001c4ccb2$ad71ae10$7d2501a3@plants.ox.ac.uk>
Message-ID: <NEBBIPHDAMMOKDKPOFFIKEEGCMAA.abunn@whrc.org>

Is there a way to calculate the number of months between dates?

StartDate <- strptime("01 March 1950", "%d %B %Y")
EventDates <- strptime(c("01 April 1955", "01 July 1980"), "%d %B %Y")
difftime(EventDates, StartDate)

So, there are 61 months between 01 March 1950 and 01 April 1955. There are
364 months between 01 March 1950 and 01 July 1980. What I want is for there
to be a "months" argument to units in difftime. Anybody have a bright idea?
Is there a better way to approach this than difftime?

Thanks in advance, Andy


> version
         _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    2
minor    0.0
year     2004
month    10
day      04
language R



From jarioksa at sun3.oulu.fi  Wed Nov 17 16:27:22 2004
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: 17 Nov 2004 17:27:22 +0200
Subject: [R] CDs for R?
In-Reply-To: <20041117145441.GA9902@sonny.eddelbuettel.com>
References: <XFMail.041116213914.Ted.Harding@nessie.mcc.ac.uk>
	<88ADDA4C-3861-11D9-86F5-000A95C76CA8@oulu.fi>
	<20041117145441.GA9902@sonny.eddelbuettel.com>
Message-ID: <1100705242.16981.32.camel@biol102145.oulu.fi>

On Wed, 2004-11-17 at 16:54, Dirk Eddelbuettel wrote:
> On Wed, Nov 17, 2004 at 08:25:54AM +0200, Jari Oksanen wrote:
> > 
> > On 16 Nov 2004, at 23:39, (Ted Harding) wrote:
> > Now comes my suggestion to CRAN maintainer: this all would be easier, 
> > if you would produce a CD image file ('iso') that would contain a 
> > snapshot of the latest version: main binaries, all contributed 
> > packages, and docs. Getting somebody to help downloading this iso would 
> > be much easier than trying to collect all first and then make up your 
> > own cd image.
> 
> It's volunteer effort, so someone actually has to do this. Can you help?
> 
Probably not. Not because I wouldn't be willing, but I may not be
able... 

I have done this a couple of time using wget to build a local "subtree"
of selected parts of CRAN. Then running mkisofs was pretty simple. I
guess this could be automated pretty easily if you have the repository
already at hand: all you need is mkisofs + info of its targets. However,
I am not that kind of guru.

All this would require that people think this is worthwhile. I think
that the general feeling has been that there is no need for a
"R-current.iso" snapshot (or the same as a valid Windows name). So this
is an academic issue (suits me).

> > 
> > To Dirk Eddelbuettel: Yes indeed, Ubuntu gives human face to Debian and 
> > is a much more pleasant experience. However, changing OS for R may be 
> > asking too much. Further, Ubuntu/Debian comes with a tiny and biased 
> > selection of packages, and if that's not your kind of bias, you have 
> > got to go to the Internet again. Further, Ubuntu (and other Linuxes) 
> 
> Again, it reflects the interests of the volunteers involved. If you want to
> see other things done, come join in and do them.
> 
I know this is volunteer work, and I do appreciate this volunteer work.
It is all biased -- hence the formulation of "your kind of bias". At the
moment I have no idea how to build a deb package of R packages, so I
don't know what to say. 

> > lag behind R. The current Ubuntu release comes with R 1.9.1, and it 
> > won't be upgraded but in the next release scheduled for April 2005 (and 
> > just in the same time as the next R, so that Ubuntu will be one R 
> > version off again). I guess the lag is even worse in packages.
> 
> This actually requires a response. Here is a quick log (from my mail folder)
> about what new packages (of mine, can't speak for others) got uploaded
> recently -- in most cases, this is on the day of the source release, so the
> lag would be close to zero.
>  
> Now, if and when these get pressed into a release by Debian or Ubuntu I do
> not control. Which is, I guess, why we're discussing archive snapshots in
> this thread. 
> 
They go, I guess, through a testing period in Debian, and if they don't
wait for anybody else, they may appear in some version of Debian after
that. In Debian repository you typically see much older versions. As to
Ubuntu (that I know a bit better), they will go into next release which
is nearly six months ahead (they are not upgraded in between). 

Actually, Ubuntu is a bad choice if you just want to have R, since R is
not among the core packages, but it is unsupported. Moreover, Ubuntu is
a bad choice for the original problem of slow wires: Even for an
ordinary install you need internet connection, if you want to get beyond
a very rudimentary system. I just forgot this in my previous message:
when you're wired, you think it's natural to be wired. So forget Ubuntu
if you want to have R without fast internet connection. 

I have Ubuntu since it was about the only easily managed powerpc system
I found. At the moment, I have R 2.0.0 built from source distribution
there. Packages are from source files, too. 

Thanks for the good work with Debian!

cheers, jari oksanen
-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From ripley at stats.ox.ac.uk  Wed Nov 17 16:32:20 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 17 Nov 2004 15:32:20 +0000 (GMT)
Subject: [R] summary.lme() vs. anova.lme()
In-Reply-To: <001001c4ccb2$ad71ae10$7d2501a3@plants.ox.ac.uk>
References: <001001c4ccb2$ad71ae10$7d2501a3@plants.ox.ac.uk>
Message-ID: <Pine.LNX.4.61.0411171524560.9736@gannet.stats>

On Wed, 17 Nov 2004, Dan Bebber wrote:

> I modelled changes in a variable (mconc) over time (d) for individuals
> (replicate) given one of three treatments (treatment) using:
> mconc.lme <- lme(mconc~treatment*poly(d,2), random=~poly(d,2)|replicate,
> data=my.data)
>
> summary(mconc.lme) shows that the linear coefficient of one of the
> treatments is significantly different to zero, viz.
>                            Value Std.Error  DF   t-value p-value
> 					...	     ... ...      ...
> ...
> treatmentf:poly(d, 2)1  1.3058562 0.5072409 315  2.574430  0.0105
>
> But anova(mconc.lme) gives a non-significant result for the treatment*time
> interaction, viz.
>                     numDF denDF   F-value p-value
> (Intercept)              1   315 159.17267  <.0001
> treatment                2    39   0.51364  0.6023
> poly(d, 2)               2   315  17.43810  <.0001
> treatment:poly(d, 2)     4   315   2.01592  0.0920
>
> Pinheiro & Bates (2000) only discusses anova() for single arguments briefly
> on p.90.
> I would like to know whether these results indicate that the significant
> effect found in summary(mconc.lme) is spurious (perhaps due to
> multiplicity).

Probably yes (but p values of 9% and 1% are not that different, and in 
both cases you are looking at a few p values).

But since both summary.lme and anova.lme use Wald tests, I would use a 
LRT, using anova on two fits (and I would use ML fits to get a genuine 
LRT but that is perhaps being cautious).

To  Dimitris Rizopoulos: as this is the last term in the sequential anova, 
it is the correct Wald test.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From James.Callahan at CityofOrlando.net  Wed Nov 17 16:35:47 2004
From: James.Callahan at CityofOrlando.net (James.Callahan@CityofOrlando.net)
Date: Wed, 17 Nov 2004 10:35:47 -0500
Subject: [R] R/S-related projects on  Sourceforge?   Trove Categorization
Message-ID: <OF898A5EDA.9DC14E71-ON85256F4F.004E6C8F-85256F4F.00557F01@ci.orlando.fl.us>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041117/46ed2096/attachment.pl

From ripley at stats.ox.ac.uk  Wed Nov 17 16:44:59 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 17 Nov 2004 15:44:59 +0000 (GMT)
Subject: [R] Time difference in months? (difftime, units)
In-Reply-To: <NEBBIPHDAMMOKDKPOFFIKEEGCMAA.abunn@whrc.org>
References: <NEBBIPHDAMMOKDKPOFFIKEEGCMAA.abunn@whrc.org>
Message-ID: <Pine.LNX.4.61.0411171542060.10158@gannet.stats>

On Wed, 17 Nov 2004, Andy Bunn wrote:

> Is there a way to calculate the number of months between dates?
>
> StartDate <- strptime("01 March 1950", "%d %B %Y")
> EventDates <- strptime(c("01 April 1955", "01 July 1980"), "%d %B %Y")
> difftime(EventDates, StartDate)
>
> So, there are 61 months between 01 March 1950 and 01 April 1955. There are
> 364 months between 01 March 1950 and 01 July 1980. What I want is for there
> to be a "months" argument to units in difftime. Anybody have a bright idea?

Ah, but months are of variable length.

> Is there a better way to approach this than difftime?

Pretty easy: convert to POSIXlt, then use something like

12*x$year + x$month

and subtract those.  You could set up a class for "months" and have 
as.months and a `-' method ....

> Thanks in advance, Andy
>
>
>> version
>         _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    2
> minor    0.0
> year     2004
> month    10
> day      04
> language R
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tlumley at u.washington.edu  Wed Nov 17 16:48:19 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 17 Nov 2004 07:48:19 -0800 (PST)
Subject: [R] Re: variations on the theme of survSplit
In-Reply-To: <Pine.LNX.4.58.0411171625590.16345@orpheus.qimr.edu.au>
References: <200411121118.iACB8RT8012951@hypatia.math.ethz.ch>
	<Pine.LNX.4.58.0411171625590.16345@orpheus.qimr.edu.au>
Message-ID: <Pine.A41.4.61b.0411170747550.76010@homer05.u.washington.edu>

On Wed, 17 Nov 2004, David Duffy wrote:

> Danardono <daodao99 at student.umu.se> wrote:
>>
>> While waiting for 2.1, for those who need function[s] for this
>> survival-splitting business, as I do,   this 'survcut' function below
>> might be helpful.

You don't need to wait for 2.1.  survival_2.16 is available on CRAN.

 	-thomas



>> It is not an elegant nor efficient function but it works, at least for
>> some examples below.
>>
> Ditto the following, for the case where there are multiple time-varying
> (irreversible) binary covariates, here slicing as coarsely as possible.
>
> #
> # Create dataset for survival analysis with time-dependent covariate
> # Gill-Anderson model
> #
> x <- data.frame(onset=c(46, 32, 53, 76, 64, 43),
>                case=c(1,1,1,0,0,0),
>                ooph=c(NA, 30, 38, 50, NA, NA),
>                ocp=c(1,1,0,0,1,0),
>                parity=c(2,0,1,3,3,2),
>                age.preg=c(28,NA,27,20,22,23))
>
> make.dep <- function(onset, case, time.dep, covs=NULL) {
>  if (is.null(n.time.dep <- ncol(time.dep))) {
>    if (!is.null(time.dep)) {
>      n.time.dep <- 1
>      time.dep <- as.matrix(time.dep)
>    }else{
>      n.time.dep <- 0
>      warning("No time dependent covariates")
>    }
>  }
>  if (is.null(n.covs <- ncol(covs))) {
>    if (!is.null(covs)) {
>      n.covs <- 1
>      covs <- as.matrix(covs)
>    }else{
>      n.covs <- 0
>    }
>  }
>  ordered.t <- t(apply(cbind(onset,time.dep),1,sort,na.last=TRUE))
>  tot.time.dep <- apply(ordered.t,1,function(x) sum(!is.na(x)))
>  ordered.t <- cbind(rep(0, nrow(ordered.t)), ordered.t)
>  npars <- 4+n.time.dep+n.covs
>  nrecs <- sum(tot.time.dep)
>  new.x <- as.data.frame(matrix(nr=nrecs, nc=npars))
>  names(new.x) <- c("start", "stop", "event", names(time.dep),names(covs),"episode")
>  this.rec<-0
>  for(i in 1:length(onset)) {
>    for(j in 1:tot.time.dep[i]) {
>      this.rec <- this.rec+1
>      new.x[this.rec,1] <- ordered.t[i, j]
>      new.x[this.rec,2] <- ordered.t[i, j+1]
>      new.x[this.rec,3] <- 0
>      new.x[this.rec,4:(3+n.time.dep)] <- (ordered.t[i,j]>=time.dep[i,])
>      missing <- is.na(new.x[this.rec,])
>      new.x[this.rec,missing] <- 0
>      if (n.covs>0) {
>        new.x[this.rec, (4+n.time.dep):(4+n.time.dep+n.covs)] <- covs[i,]
>      }
>      new.x[this.rec, npars]<-paste(i,j,sep=".")
>    }
>    new.x[this.rec,3]<-case[i]
>  }
>  new.x
> }
>
> David Duffy.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From Zack.Apoian at sac.com  Wed Nov 17 16:48:46 2004
From: Zack.Apoian at sac.com (Apoian, Zack)
Date: Wed, 17 Nov 2004 10:48:46 -0500
Subject: [R] referencing values of strings
Message-ID: <D35A7FAE1220DB49A018F51A225565E8059FA3@mailisny3.saccap.int>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041117/1b4dc83f/attachment.pl

From tfliao at uiuc.edu  Wed Nov 17 16:56:40 2004
From: tfliao at uiuc.edu (Tim F Liao)
Date: Wed, 17 Nov 2004 09:56:40 -0600
Subject: [R] violinplot options
Message-ID: <51ba2718.a0b997cf.81b4900@expms6.cites.uiuc.edu>

Uwe is right.  You can add titles quite easily.  
Specifically, you can use title(main="...",ylab="...") and 
that will do the trick after you have used vioplot to do the 
plot.

Tim Liao

---- Original message ----
>Date: Wed, 17 Nov 2004 10:55:33 +0100
>From: Uwe Ligges <ligges at statistik.uni-dortmund.de>  
>Subject: Re: [R] violinplot options  
>To: Tanja Zseby <zseby at fokus.fraunhofer.de>
>Cc: r-help at stat.math.ethz.ch
>
>Tanja Zseby wrote:
>
>> Hi,
>> 
>> I am using the function  vioplot() to generate violin 
plots. Now I would 
>> like to add a label to the y axix and a title to the 
diagram.
>> Just setting ylab didnt work. Is it possible to set such 
options for the 
>> function ?
>> I tried also with the function simple.violinplot, but 
also with this I 
>> couldnt set the options.
>> 
>> Kind Regards
>> Tanja
>
>Looks like nobody else has responded so far.
>If you are talking about the function in the package also 
called 
>"vioplot": The function is not very well designed. But 
since there is 
>not much code in it, it is quite easy to add additional 
functionality 
>yourself by adapting the whole function.
>
>Uwe Ligges
>
>
>
>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-
project.org/posting-guide.html



From ggrothendieck at myway.com  Wed Nov 17 17:16:53 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 17 Nov 2004 16:16:53 +0000 (UTC)
Subject: [R] Time difference in months? (difftime, units)
References: <001001c4ccb2$ad71ae10$7d2501a3@plants.ox.ac.uk>
	<NEBBIPHDAMMOKDKPOFFIKEEGCMAA.abunn@whrc.org>
Message-ID: <loom.20041117T164933-4@post.gmane.org>

Andy Bunn <abunn <at> whrc.org> writes:

: 
: Is there a way to calculate the number of months between dates?
: 
: StartDate <- strptime("01 March 1950", "%d %B %Y")
: EventDates <- strptime(c("01 April 1955", "01 July 1980"), "%d %B %Y")
: difftime(EventDates, StartDate)
: 
: So, there are 61 months between 01 March 1950 and 01 April 1955. There are
: 364 months between 01 March 1950 and 01 July 1980. What I want is for there
: to be a "months" argument to units in difftime. Anybody have a bright idea?
: Is there a better way to approach this than difftime?

1. There are an average of 365.25/12 days per month so the following
expression gives the number of months between d1 and d2:

   # test data
   d1 <- as.Date("01 March 1950", "%d %B %Y")
   d2 <- as.Date(c("01 April 1955", "01 July 1980"), "%d %B %Y")

   # calculation
   round((d2 - d1)/(365.25/12))

2. Another possibility is to get the length of seq.Dates like this:

   as.Date.numeric <- function(x) structure(floor(x+.001), class = "Date")
   sapply(d2, function(d2) length(seq(d1, as.Date(d2), by = "month")))-1



From gunter.berton at gene.com  Wed Nov 17 17:15:08 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 17 Nov 2004 08:15:08 -0800
Subject: [R] The hidden costs of GPL software?
In-Reply-To: <419B5FE5.8040009@pburns.seanet.com>
Message-ID: <200411171615.iAHGF8ag026603@hertz.gene.com>

All:

I have much enjoyed the discussion. Thanks to all who have contibuted.

Two quick comments:

1. The problem of designing a GUI to make R's functionality more accessible
is, I believe just one component of the larger issue of making
statistical/data analysis functionality available to those who need to use
it but do not have sufficient understanding and background to do so
properly. I certainly include myself in this category in many circumstances.
A willingness and commitment to learning ( = hard work!) is the only
rational solution here, and saying that one doesn't have the time really
doesn't cut it for me. Ditto for R language functionality?

2. However, R has many attractive features for data manipulation and
graphics that make it attractive for common tasks that are now done most
frequently with (ugh!) Excel (NOT Statistica, Systat, et. al.). For this
subset of R's functionality a GUI would be attractive. However, writing a
good GUI for graphing that even begins to take advantage of R's flexibility
and power in this arena is an enormous -- perhaps an impossible -- task.
Witness the S-Plus graphics GUI, which I think is truly awful (and appears
to thwart more than it helps, at least from many of the queries one sees on
that news list). So I'm not sanguine.

Again, thanks to all for a thoughful and enjoyable discussion.

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Patrick Burns
> Sent: Wednesday, November 17, 2004 6:28 AM
> To: Jan P. Smit
> Cc: r-help at stat.math.ethz.ch; Philippe Grosjean; 
> r-sig-gui at stat.math.ethz.ch
> Subject: Re: [R] The hidden costs of GPL software?
> 
> I'm a big advocate -- perhaps even fanatic -- of  making R easier for
> novices in order to spread its use, but I'm not convinced that  a GUI
> (at least in the traditional form) is the most valuable approach.
> 
> Perhaps an overly harsh summary of some of Ted Harding's statements
> is: You can make a truck easier to get into by taking off the 
> wheels, but
> that doesn't make it more useful.
> 
> In terms of GUIs, I think what R should focus on is the 
> ability for  user's
> to make their own specialized GUI.  So that a knowledgeable programmer
> at an installation can create a system that is easy for 
> unsophisticated
> users for the limited number of tasks that are to be done.  
> The ultimate
> users may not even need to know that R exists.
> 
> I think Ted Harding was on  the mark when he said that it is the help
> system that needs enhancement.  I can imagine a system that gets the
> user to the right function and then helps fill in the 
> arguments; all of the
> time pointing them towards the command line rather than away from
> it.
> 
> The author of the referenced article highlighted some hidden 
> costs of R,
> but did not highlight the hidden benefits (because they were 
> hidden from
> him).  A big benefit of R is all of the bugs that aren't in 
> it (which may or
> may not be due to its free status).
> 
> Patrick Burns
> 
> Burns Statistics
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
> 
> Jan P. Smit wrote:
> 
> > Dear Phillippe,
> >
> > Very interesting. The URL of the article is 
> > http://www.scientific-computing.com/scwsepoct04free_statistics.html.
> >
> > Best regards,
> >
> > Jan Smit
> >
> >
> > Philippe Grosjean wrote:
> >
> >> Hello,
> >>
> >> In the latest 'Scientific Computing World' magazine (issue 78, p. 
> >> 22), there
> >> is a review on free statistical software by Felix Grant ("doesn't 
> >> have to
> >> pay good money to obtain good statistics software"). As far as I 
> >> know, this
> >> is the first time that R is even mentioned in this magazine, given 
> >> that it
> >> usually discuss commercial products.
> >>
> [ ...]
> 
> >>
> >
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From tlumley at u.washington.edu  Wed Nov 17 17:21:38 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 17 Nov 2004 08:21:38 -0800 (PST)
Subject: [R] frailty and time-dependent covariate
In-Reply-To: <008401c4cca5$4d532020$36ec8495@Emanuela>
References: <008401c4cca5$4d532020$36ec8495@Emanuela>
Message-ID: <Pine.A41.4.61b.0411170757300.76010@homer05.u.washington.edu>

On Wed, 17 Nov 2004, Emanuela Rossi wrote:

> Hello,
>
> I'm trying to estimate a cox model with a frailty variable and 
> time-dependent covariate (below there is the statement I use and the 
> error message). It's seems to be impossible, because every time I add 
> the time-dependent covariate the model doesn't converge. Instead, if I 
> estimate the same model without the time-dependent covariate it's 
> converge. I'd like knowing if it's a statistical problem due to the 
> model formula or if it could be a problem related to my data.

The message you quote probably does not indicate a convergence problem, 
since it is only in "iterations 1 3".

Howver, there does seem to be a bug with interactions of any sort in 
frailty models, something connected with reordering terms.  You could try 
defining a variable by hand for your interaction term and fitting the 
model that way.

Incidentally, I am not nearly as enthusiastic about the frailty models as 
many users seem to be.  For most multivariate survival problems I 
wouldn't want to fit a frailty model, and I'm not sure that I would trust 
the penalised likelihood approximation if I did.   The exception would be 
situations where I was actually interested in the variance components, as 
in Dr Therneau's new "kinship" package for linkage analysis with survival 
data.


 	-thomas


>
> Thanks a lot
>
> Emanuela Rossi
>
>
>> fit_19_1<-coxph(Surv(DATA_INI1,DATA_FIN1,EVENT1)~ 
>> V1+V2+alt1+alt2+strata(autocorr1)+cap1+SP+SP2+SP3+SP3:log(DATA_FIN1)+D1500+D3000+D4500+frailty.gaussian(ID),data=SURV1)
>
> Warning messages:
> 1: Inner loop failed to coverge for iterations 1 3 in: coxpenal.fit(X, Y, strats, offset, init = init, control, weights = weights,
> 2: longer object length
>        is not a multiple of shorter object length in: offset + coxfit$fcoef[x[, fcol]]
> 3: X matrix deemed to be singular; variable 8 in: coxph(Surv(DATA_INI1, DATA_FIN1, EVENT1) ~ V1 + V2 + alt1 + alt2 +
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From vito_ricci at yahoo.com  Wed Nov 17 17:27:27 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Wed, 17 Nov 2004 17:27:27 +0100 (CET)
Subject: [R] Re: referencing values of strings
Message-ID: <20041117162727.73136.qmail@web41214.mail.yahoo.com>

Hi,

see ? as.numeric

as.numeric(c("-.1"," 2.7 ","-1.5"))
[1] -0.1  2.7 -1.5

you wrote:

Say you have a vector named x and a function which
returns the character
string "x" .  How would I take "x" as an input and
return the vector x?

=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese http://www.modugno.it/archivio/cat_palese.shtml



From dataanalytics at rediffmail.com  Wed Nov 17 17:35:24 2004
From: dataanalytics at rediffmail.com (Arin Basu)
Date: 17 Nov 2004 16:35:24 -0000
Subject: [R] Importing data from SPSS
Message-ID: <20041117163524.30646.qmail@webmail17.rediffmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041117/932c75c1/attachment.pl

From dimitris.rizopoulos at med.kuleuven.ac.be  Wed Nov 17 17:35:42 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Wed, 17 Nov 2004 17:35:42 +0100
Subject: [R] referencing values of strings
References: <D35A7FAE1220DB49A018F51A225565E8059FA3@mailisny3.saccap.int>
Message-ID: <00fe01c4ccc3$7afb11e0$0540210a@www.domain>

get("x") see also r-FAQ 7.21

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Apoian, Zack" <Zack.Apoian at sac.com>
To: "'R-help at lists.R-project.org'" <R-help at stat.math.ethz.ch>
Sent: Wednesday, November 17, 2004 4:48 PM
Subject: [R] referencing values of strings


> Say you have a vector named x and a function which returns the 
> character
> string "x" .  How would I take "x" as an input and return the vector 
> x?
>
>
> DISCLAIMER: This e-mail message and any attachments are 
> inte...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Wed Nov 17 17:41:59 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 17 Nov 2004 16:41:59 +0000 (GMT)
Subject: [R] referencing values of strings
In-Reply-To: <D35A7FAE1220DB49A018F51A225565E8059FA3@mailisny3.saccap.int>
References: <D35A7FAE1220DB49A018F51A225565E8059FA3@mailisny3.saccap.int>
Message-ID: <Pine.LNX.4.61.0411171641370.11480@gannet.stats>

On Wed, 17 Nov 2004, Apoian, Zack wrote:

> Say you have a vector named x and a function which returns the character
> string "x" .  How would I take "x" as an input and return the vector x?

?get


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ahenningsen at email.uni-kiel.de  Wed Nov 17 17:52:22 2004
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Wed, 17 Nov 2004 17:52:22 +0100
Subject: [R] referencing values of strings
In-Reply-To: <D35A7FAE1220DB49A018F51A225565E8059FA3@mailisny3.saccap.int>
References: <D35A7FAE1220DB49A018F51A225565E8059FA3@mailisny3.saccap.int>
Message-ID: <200411171752.22106.ahenningsen@email.uni-kiel.de>

"get" is your friend:

R> x <- c( 1, 2, 3 )
R> get( "x" )
[1] 1 2 3

All the best,
Arne

On Wednesday 17 November 2004 16:48, Apoian, Zack wrote:
> Say you have a vector named x and a function which returns the character
> string "x" .  How would I take "x" as an input and return the vector x?
>
>
> DISCLAIMER: This e-mail message and any attachments are inte...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From James.Callahan at CityofOrlando.net  Wed Nov 17 17:52:57 2004
From: James.Callahan at CityofOrlando.net (James.Callahan@CityofOrlando.net)
Date: Wed, 17 Nov 2004 11:52:57 -0500
Subject: [R] R/S-related projects on Sourceforge? Trove Categorization -
	GDAL
In-Reply-To: <OF898A5EDA.9DC14E71-ON85256F4F.004E6C8F-85256F4F.00557F01@LocalDomain>
Message-ID: <OFA43FBCFA.8058A07A-ON85256F4F.005C03CD-85256F4F.005C8FF8@ci.orlando.fl.us>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041117/5507a5e5/attachment.pl

From sfalcon at fhcrc.org  Wed Nov 17 18:01:22 2004
From: sfalcon at fhcrc.org (Seth Falcon)
Date: Wed, 17 Nov 2004 09:01:22 -0800
Subject: [R] referencing values of strings
In-Reply-To: <D35A7FAE1220DB49A018F51A225565E8059FA3@mailisny3.saccap.int>
References: <D35A7FAE1220DB49A018F51A225565E8059FA3@mailisny3.saccap.int>
Message-ID: <20041117170122.GA15863@queenbee.fhcrc.org>

On Wed, Nov 17, 2004 at 10:48:46AM -0500, Apoian, Zack wrote:
> Say you have a vector named x and a function which returns the character
> string "x" .  How would I take "x" as an input and return the vector x?

get("x")

See ?get.  You may also be interested in ?assign.

+ seth



From ernesto at ipimar.pt  Wed Nov 17 17:49:18 2004
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Wed, 17 Nov 2004 16:49:18 +0000
Subject: [R] R/S-related projects on  Sourceforge?   Trove
	Categorization - GDAL
In-Reply-To: <OFA43FBCFA.8058A07A-ON85256F4F.005C03CD-85256F4F.005C8FF8@ci.orlando.fl.us>
References: <OFA43FBCFA.8058A07A-ON85256F4F.005C03CD-85256F4F.005C8FF8@ci.orlando.fl.us>
Message-ID: <1100710158.8521.82.camel@mordor.ipimar.pt>

Just a small correction. FLR and FSAp are not _my_ packages :-) 

For FLR there's a team working on its development.

Regards

EJ

ps: my fault anyway, the first message was not clear.

On Wed, 2004-11-17 at 16:52, James.Callahan at CityofOrlando.net wrote:
> GDAL Package for R
> http://sourceforge.net/projects/rgdal
> The R GDAL package is an interface for accessing Frank Warmerdam's
> Geographic Data Abstraction Library
>  from within R. 
> GDAL is capable of reading and writing a wide range of geographic data
> formats including ESRI grid format and geotiff.
> 
> On Wed, 2004-11-17 at 09:09, Witold Eryk Wolski wrote:
> > <SNIP>
> > SourceForge.net will consider the inclusion of a programming
> > language within the Trove system when we host at least 5
> > projects based on that language.  Please advise: Do you know
> > of 5 projects hosted on SourceForge.net based on this language?
> > <SNIP>
> 
> Gretl, RPad and RMetrics, plus Ernesto's FLR and  fsap make five.
> "GDAL Package for R", makes six.
> 
> 
> Jim Callahan
> Management, Budget & Accounting
> City of Orlando
> (407) 246-3039 office
> (407) 234-3744 cell phone



From kjetil at acelerate.com  Wed Nov 17 17:12:36 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Wed, 17 Nov 2004 12:12:36 -0400
Subject: [R] R/S-related projects on  Sourceforge?   Trove Categorization
In-Reply-To: <OF898A5EDA.9DC14E71-ON85256F4F.004E6C8F-85256F4F.00557F01@ci.orlando.fl.us>
References: <OF898A5EDA.9DC14E71-ON85256F4F.004E6C8F-85256F4F.00557F01@ci.orlando.fl.us>
Message-ID: <419B7874.805@acelerate.com>

James.Callahan at CityofOrlando.net wrote:

>><SNIP>
>>SourceForge.net will consider the inclusion of a programming
>>language within the Trove system when we host at least 5
>>projects based on that language.  Please advise: Do you know
>>of 5 projects hosted on SourceForge.net based on this language?
>><SNIP>
>>    
>>
>
>Gretl, RPad and RMetrics, plus Ernesto's FLR and  fsap make five. 
>  
>
Then you can also add nlmeODE:

http://nlmeode.sourceforge.net/

>Gretl           Allin Cottrell, USA
>Gnu Regression, Econometrics and Time-series Library
>Gretl is hosted on SourceForge.
>http://gretl.sourceforge.net/
>
>Gretl is not written in R, but interfaces to  R.
>http://gretl.sourceforge.net/gretl_and_R.html
>"gretl ... is designed as a very user-friendly econometrics package. While 
>it is also reasonably sophisticated, it lacks some of the specialized 
>statistical methods that a working econometrician might desire.As a way 
>around this limitation, gretl offers an interface to the comprehensive 
>free-software statistical package, GNU R."
>
>Both RPAd and RMetrics are open-source projects using (and acknowledging 
>using) R.
>As far as I know, neither is listed on SourceForge.
>
>RPad            EPRI (Electric Power Research Institute), USA
>http://www.Rpad.org/Rpad/ 
>"Rpad is an interactive, web-based analysis program. Rpad pages are 
>interactive workbook-type sheets based on R, an open-source implementation 
>of the S language. Rpad is an analysis package, a web-page designer, and a 
>gui designer all wrapped in one. Rpad makes it easy to develop powerful 
>data analysis applications that can be shared with others (most likely on 
>an intranet). The user doesn't have to install anything--everything's done 
>through a browser."
>
>RMetrics        Diethelm Wuertz, ETH Zurich, Switzerland
>http://www.itp.phys.ethz.ch/econophysics/R/
>"Rmetrics is the premier open source solution for financial market 
>analysis and valuation of financial instruments. With hundreds of 
>functions build on modern and powerful methods Rmetrics combines 
>explorative data analysis and statistical modeling with object oriented 
>rapid prototyping. Rmetrics is embedded in R,"
>My impression is that many projects "on SourceForge" have links to home 
>pages hosted on other sites -- so I suppose
>if the project authors are willing -- they could be cross listed on 
>SourceForge.
>
>Gretl, RPad and RMetrics, plus Ernesto's FLR and  fsap make five.
>
>On the other hand, SourceForge's criteria for inclusion appears to be very 
>arbitrary.
>
>Objective criteria that would be more relevant to R include:
>- R has several active mailing lists (archived for verification)
>- The R Project is archived on CRAN (Comprehensive R Archive Network) 
>which has more than 20 mirrors on 6 continents:
>Africa
>Asia
>Australia
>Europe
>North America
>South America
>- More than a dozen books have been published with either "R" or "S" 
>mentioned in the title ("R" is an open-source  implementation of "S"). For 
>example, "Introductory Statistics with R" is available from major 
>booksellers including Amazon.com, BarnesandNoble.com and even 
>Wal-mart.com!
>
>Subjective criteria for inclusion on SourceForge
>- "R" is an implementation of "S" a language developed at Bell Labs -- the 
>organization that developed the C programming language and the Unix 
>operating system. Open source implementations of C include the GNU 
>Complier Collection (GCC) and open source implementations of Unix include 
>GNU Linux. So why not include, the open source implementation of S, GNU R?
>
>- R is respected in the statistical community. There are awards and 
>articles that could be cited. A personal story -- in the early 1990s I 
>attended an American Statistical Association meeting in San Francisco. I 
>saw then that the topnotch statisticians, people like Frank Harrell -- 
>whose short course on Regression I attended, were maxing out SAS and 
>switching to S. I heard about StatLib at Carnegie Mellon and contributed 
>code. At the time, the institution I was working for was committed to SAS 
>and SPSS and would not have been open to spending more on S. But, years 
>latter I was delighted to learn that there was an open source 
>implementation S, R and that it was available at StatLib, which now is a 
>mirror for the worldwide CRAN mirror sites.
>
>On Wed, 2004-11-17 at 09:09, Witold Eryk Wolski wrote:
>  
>
>>Hi R-Users and Developers,
>>
>>Several months ago I made a request on Sourceforge to add the R/S - 
>>programming language to the _Trove_ categorization. ("The Trove is a 
>>means to convey basic metainformation about your project.")
>>
>>Today I got the following response of one of the sourceforge admins.
>>
>><SNIP>
>>
>>SourceForge.net will consider the inclusion of a programming
>>language within the Trove system when we host at least 5
>>projects based on that language.  Please advise: Do you know
>>of 5 projects hosted on SourceForge.net based on this language?
>><SNIP>
>>
>>
>>If anyone of you knew about R-packages, or projects using the R/S 
>>    
>>
>programming language, which are hosted on sourceforge, please reply to 
>this thread. I hope that your answers will enable me to give more then 5 
>examples of R projects hosted on Sourceforge.
>  
>
>>Yours Eryk
>>
>>    
>>
>Jim Callahan
>Management, Budget & Accounting
>City of Orlando
>(407) 246-3039 office
>(407) 234-3744 cell phone
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From andy_liaw at merck.com  Wed Nov 17 18:23:49 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 17 Nov 2004 12:23:49 -0500
Subject: [R] referencing values of strings
Message-ID: <3A822319EB35174CA3714066D590DCD50994E31F@usrymx25.merck.com>

Can you give some hypothetical code on what you want to do?  Does get() do
what you want:

> dog <- "Spot"
> f <- function(x) get(x)
> f("dog")
[1] "Spot"

?

Andy

> From: Apoian, Zack
> 
> Say you have a vector named x and a function which returns 
> the character
> string "x" .  How would I take "x" as an input and return the 
> vector x?
> 
> 
> DISCLAIMER: This e-mail message and any attachments are 
> inte...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From MSchwartz at MedAnalytics.com  Wed Nov 17 18:30:42 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Wed, 17 Nov 2004 11:30:42 -0600
Subject: [R] referencing values of strings
In-Reply-To: <D35A7FAE1220DB49A018F51A225565E8059FA3@mailisny3.saccap.int>
References: <D35A7FAE1220DB49A018F51A225565E8059FA3@mailisny3.saccap.int>
Message-ID: <1100712642.2495.13.camel@horizons.localdomain>

On Wed, 2004-11-17 at 10:48 -0500, Apoian, Zack wrote:
> Say you have a vector named x and a function which returns the character
> string "x" .  How would I take "x" as an input and return the vector x?


If I am understanding you correctly:

x <- 1:10

> x
 [1]  1  2  3  4  5  6  7  8  9 10

x.char <- "x"

> get(x.char)
 [1]  1  2  3  4  5  6  7  8  9 10


See ?get for more information.

HTH,

Marc Schwartz



From bates at stat.wisc.edu  Wed Nov 17 18:32:21 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 17 Nov 2004 11:32:21 -0600
Subject: [R] Re: referencing values of strings
In-Reply-To: <20041117162727.73136.qmail@web41214.mail.yahoo.com>
References: <20041117162727.73136.qmail@web41214.mail.yahoo.com>
Message-ID: <419B8B25.6070002@stat.wisc.edu>

Vito Ricci wrote:
> Hi,
> 
> see ? as.numeric
> 
> as.numeric(c("-.1"," 2.7 ","-1.5"))
> [1] -0.1  2.7 -1.5
> 
> you wrote:
> 
> Say you have a vector named x and a function which
> returns the character
> string "x" .  How would I take "x" as an input and
> return the vector x?

I think the question might have been about obtaining the value of the 
symbol "x", in which case the answer is

get("x")



From dsonneborn at ucdavis.edu  Wed Nov 17 19:09:03 2004
From: dsonneborn at ucdavis.edu (Dean Sonneborn)
Date: Wed, 17 Nov 2004 10:09:03 -0800
Subject: [R] dotchart background color
Message-ID: <6.1.0.6.2.20041117100516.01b98060@yellow.ucdavis.edu>

I'm creating a dotchart but the background color is gray. In fact, when 
ever I use the Lattice package the background is gray, which prints as 
black on my non-color printer. How do I change the background color to 
white? I'm also plotting two groups and would like to use circles and 
triangles as the plot characters.

Thanks
Dean



From angelare at to.infn.it  Wed Nov 17 19:20:31 2004
From: angelare at to.infn.it (Angela Re)
Date: Wed, 17 Nov 2004 19:20:31 +0100
Subject: [R] (no subject)
Message-ID: <419B966F.3060903@to.infn.it>

Good evening,
I'm going to use R to calculate the P-value for Pearson coefficient. 
Does it exist an already defined function?How can I do?Thanks for 
helping me.
Angela



From edd at debian.org  Wed Nov 17 19:25:16 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 17 Nov 2004 12:25:16 -0600
Subject: [R] R/S-related projects on Sourceforge? Trove Categorization -
	GDAL
In-Reply-To: <OFA43FBCFA.8058A07A-ON85256F4F.005C03CD-85256F4F.005C8FF8@ci.orlando.fl.us>
References: <OF898A5EDA.9DC14E71-ON85256F4F.004E6C8F-85256F4F.00557F01@LocalDomain>
	<OFA43FBCFA.8058A07A-ON85256F4F.005C03CD-85256F4F.005C8FF8@ci.orlando.fl.us>
Message-ID: <20041117182516.GA12201@sonny.eddelbuettel.com>

On Wed, Nov 17, 2004 at 11:52:57AM -0500, James.Callahan at CityofOrlando.net wrote:
> Gretl, RPad and RMetrics, plus Ernesto's FLR and  fsap make five. 

Isn't RMetrics at rmetrics.org at the ETH in Zuerich, CH?

> "GDAL Package for R", makes six.

Add RPy (rpy.sf.net) to make seven (or six, if remove RMetrics). Gretl is a
tad marginal, though.

Dirk

-- 
If your hair is standing up, then you are in extreme danger.
      -- http://www.usafa.af.mil/dfp/cockpit-phys/fp1ex3.htm



From spencer.graves at pdf.com  Wed Nov 17 19:28:09 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 17 Nov 2004 10:28:09 -0800
Subject: [R] Multi-way tables
In-Reply-To: <1100704054.419b6936e9e1b@webmail4.portugalmail.pt>
References: <1100704054.419b6936e9e1b@webmail4.portugalmail.pt>
Message-ID: <419B9839.3000206@pdf.com>

      ?apply

      hope this helps.  spencer graves

assuncao.senra at portugalmail.com wrote:

>Hello,
>can someone tell me how to extract a partial table from a multiway table?
>Something else: I tried to do basic operations on cross-tables, like 
>subtracting two cross tables with same dimensions and i was unable to do so. 
>Is there another way?
>
>thanks!
>
>__________________________________________________________
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From spencer.graves at pdf.com  Wed Nov 17 19:22:01 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 17 Nov 2004 10:22:01 -0800
Subject: [R] The hidden costs of GPL software?
In-Reply-To: <200411171615.iAHGF8ag026603@hertz.gene.com>
References: <200411171615.iAHGF8ag026603@hertz.gene.com>
Message-ID: <419B96C9.3030108@pdf.com>

      I agree with Bert.  Thanks to all who contributed.  I'd like to 
add one comment I didn't see in the thread so far: 

      The corporate legal where I work is deathly afraid of the GNU 
General Public License (GPL), because if we touch GPL software 
inappropriately with our commercial software, our copyrights are 
replaced by the GPL.  This in turn means we can't charge royalties, 
which means we can't repay the investors who covered our initial 
development costs, and we file for bankruptcy.  The rabid capitalists 
meet the rabid socialists and walk away, shaking their heads.  (Sec. 2.b 
of the GPL:  "You must cause any work that you distribute or publish, 
that in whole or in part contains or is derived from the Program or any 
part thereof, to be licensed as a whole at no charge to all third 
parties under the terms of this License."  We can get around this by 
packaging accesses to GPL software as separately installed add-on(s), 
because then only the add-on(s) would be covered by the GPL.)  Our 
corporate legal is more concerned about a possible law suit from a 
possible competitor than from the R Foundation, but the threat is still 
real and still being adjudicated in other cases. 

      If the GPL were not so tight on this point, someone could 
commercialize a GUI for R without having to offer their source code 
under the GPL. 

      However, even without this change, R seems to be the platform of 
choice for new statistical algorithm development by a growing portion of 
the international scientific community.  Moreover, from my experience 
with this listserve, the technical support here is far superior to 
anything I've experienced with any other software in the 40+ years since 
I wrote my first Fortran code. 

      Best Wishes,
      spencer graves

Berton Gunter wrote:

>All:
>
>I have much enjoyed the discussion. Thanks to all who have contibuted.
>
>Two quick comments:
>
>1. The problem of designing a GUI to make R's functionality more accessible
>is, I believe just one component of the larger issue of making
>statistical/data analysis functionality available to those who need to use
>it but do not have sufficient understanding and background to do so
>properly. I certainly include myself in this category in many circumstances.
>A willingness and commitment to learning ( = hard work!) is the only
>rational solution here, and saying that one doesn't have the time really
>doesn't cut it for me. Ditto for R language functionality?
>
>2. However, R has many attractive features for data manipulation and
>graphics that make it attractive for common tasks that are now done most
>frequently with (ugh!) Excel (NOT Statistica, Systat, et. al.). For this
>subset of R's functionality a GUI would be attractive. However, writing a
>good GUI for graphing that even begins to take advantage of R's flexibility
>and power in this arena is an enormous -- perhaps an impossible -- task.
>Witness the S-Plus graphics GUI, which I think is truly awful (and appears
>to thwart more than it helps, at least from many of the queries one sees on
>that news list). So I'm not sanguine.
>
>Again, thanks to all for a thoughful and enjoyable discussion.
>
>-- Bert Gunter
>Genentech Non-Clinical Statistics
>South San Francisco, CA
> 
>"The business of the statistician is to catalyze the scientific learning
>process."  - George E. P. Box
> 
> 
>
>  
>
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Patrick Burns
>>Sent: Wednesday, November 17, 2004 6:28 AM
>>To: Jan P. Smit
>>Cc: r-help at stat.math.ethz.ch; Philippe Grosjean; 
>>r-sig-gui at stat.math.ethz.ch
>>Subject: Re: [R] The hidden costs of GPL software?
>>
>>I'm a big advocate -- perhaps even fanatic -- of  making R easier for
>>novices in order to spread its use, but I'm not convinced that  a GUI
>>(at least in the traditional form) is the most valuable approach.
>>
>>Perhaps an overly harsh summary of some of Ted Harding's statements
>>is: You can make a truck easier to get into by taking off the 
>>wheels, but
>>that doesn't make it more useful.
>>
>>In terms of GUIs, I think what R should focus on is the 
>>ability for  user's
>>to make their own specialized GUI.  So that a knowledgeable programmer
>>at an installation can create a system that is easy for 
>>unsophisticated
>>users for the limited number of tasks that are to be done.  
>>The ultimate
>>users may not even need to know that R exists.
>>
>>I think Ted Harding was on  the mark when he said that it is the help
>>system that needs enhancement.  I can imagine a system that gets the
>>user to the right function and then helps fill in the 
>>arguments; all of the
>>time pointing them towards the command line rather than away from
>>it.
>>
>>The author of the referenced article highlighted some hidden 
>>costs of R,
>>but did not highlight the hidden benefits (because they were 
>>hidden from
>>him).  A big benefit of R is all of the bugs that aren't in 
>>it (which may or
>>may not be due to its free status).
>>
>>Patrick Burns
>>
>>Burns Statistics
>>patrick at burns-stat.com
>>+44 (0)20 8525 0696
>>http://www.burns-stat.com
>>(home of S Poetry and "A Guide for the Unwilling S User")
>>
>>Jan P. Smit wrote:
>>
>>    
>>
>>>Dear Phillippe,
>>>
>>>Very interesting. The URL of the article is 
>>>http://www.scientific-computing.com/scwsepoct04free_statistics.html.
>>>
>>>Best regards,
>>>
>>>Jan Smit
>>>
>>>
>>>Philippe Grosjean wrote:
>>>
>>>      
>>>
>>>>Hello,
>>>>
>>>>In the latest 'Scientific Computing World' magazine (issue 78, p. 
>>>>22), there
>>>>is a review on free statistical software by Felix Grant ("doesn't 
>>>>have to
>>>>pay good money to obtain good statistics software"). As far as I 
>>>>know, this
>>>>is the first time that R is even mentioned in this magazine, given 
>>>>that it
>>>>usually discuss commercial products.
>>>>
>>>>        
>>>>
>>[ ...]
>>
>>    
>>
>>>      
>>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From spencer.graves at pdf.com  Wed Nov 17 19:39:38 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 17 Nov 2004 10:39:38 -0800
Subject: [R] beginner's problem in displaying large data
In-Reply-To: <b68812e7041116150774a548b9@mail.gmail.com>
References: <b68812e7041116150774a548b9@mail.gmail.com>
Message-ID: <419B9AEA.4000704@pdf.com>

	  Are you sure you are only getting the last 5 columns, rows 1723:2200? 
  There isn't a scroll bar some place?

	  What do you get from the following?

      (tst <- data.frame(array(rnorm(500), dim=c(500, 6))))

	  This should come in 2 sets of 500 rows, the first with 5 columns, the 
second with only 1.  If what you said earlier is accurate, I would guess 
that when this is done, the screen would display rows 23:500 of column 
6.  Is this what you get?

	  If you still have troubles, check "?sink", pipe the output to a text 
file file, and look at the file with some other application, e.g.:

	  sink("huh.txt")
	  (tst <- data.frame(array(rnorm(500), dim=c(500, 6))))
	  sink()

	  hope this helps.
	  spencer graves

Terry Mu wrote:

 >Dear Spencer,
 >
 >Thank you for your comment.
 >
 >>     1.  Did you try "dim(sample.data)"?  Is it actually 2200 by 15?
 >>Or are you reading in just some subset of the data?  If it is 2200 by
 >>15, could you also please do "class(sample.data)"?
 >
 >
 >Yes, dim() gives the number.
 >class(sample.data) gives "data.frame"
 >
 >
 >>     2.  I just got a full listing from the following:
 >>
 >>     (tst <- data.frame(array(rnorm(2200), dim=c(2200, 15))))
 >>
 >>     You might try this.  With R 2.0.0patched under Windows 2000, I got
 >>rows 1:2200 flying by 3 times, each with 5 columns.
 >
 >
 >I tried this, did not get full listing. What I got was last 5 columns
 >from 1723. I am using R 2.0.0 under Windows 2000.
 >
 >
 >>     3.  Have you considered doing plots (including qqnorm) of numeric
 >>variables and tables of character variables?  These can often reveal
 >>problems I might never see in a simple scan of numbers.
 >
 >
 >They are all numbers.
 >
 >>     4.  "PLEASE do read the posting guide!
 >>http://www.R-project.org/posting-guide.html".  At minimum, please tell
 >>us which version of R under which operating system, and specifically
 >>what you did to get it into R and how you know it's 2200 by 15.
 >
 >
 >Sorry about that.
 >
 >Thanks,
 >Terry Mu
 >
 >
 >>Terry Mu wrote:
 >>
 >>>I got a sample data (let's call it sample.data), which is about 2200 
by 15.
 >>>
 >>>I tried to take a look of all data
 >>>
 >>>
 >>>
 >>>>sample.data
 >>>>
 >>>>
 >>>It shows only a part of data that I thought was a corner. It does not
 >>>really affect my job, but I thought it is nice to have a look of all
 >>>data. I can see individual records and they are fine.
 >>>
 >>>Is this normal because of buffer size or some reasons? Can I use other
 >>>commands or change some settings to display all data?
 >>>
 >>>Thanks,
 >>>Terry
 >>>
 >>>______________________________________________
 >>>R-help at stat.math.ethz.ch mailing list
 >>>https://stat.ethz.ch/mailman/listinfo/r-help
 >>>PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html
 >>>
 >>>
 >>--
 >>Spencer Graves, PhD, Senior Development Engineer
 >>O:  (408)938-4420;  mobile:  (408)655-4567
 >>
 >>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From MSchwartz at MedAnalytics.com  Wed Nov 17 20:12:33 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Wed, 17 Nov 2004 13:12:33 -0600
Subject: [R] (no subject)
In-Reply-To: <419B966F.3060903@to.infn.it>
References: <419B966F.3060903@to.infn.it>
Message-ID: <1100718754.5809.17.camel@horizons.localdomain>

On Wed, 2004-11-17 at 19:20 +0100, Angela Re wrote:
> Good evening,
> I'm going to use R to calculate the P-value for Pearson coefficient. 
> Does it exist an already defined function?How can I do?Thanks for 
> helping me.
> Angela


help.search("Pearson") shows you:

...
cor.test(stats)      Test for Association/Correlation Between
                     Paired Samples
...


See ?cor.test

HTH,

Marc Schwartz



From deepayan at stat.wisc.edu  Wed Nov 17 20:15:22 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Wed, 17 Nov 2004 13:15:22 -0600
Subject: [R] dotchart background color
In-Reply-To: <6.1.0.6.2.20041117100516.01b98060@yellow.ucdavis.edu>
References: <6.1.0.6.2.20041117100516.01b98060@yellow.ucdavis.edu>
Message-ID: <200411171315.22248.deepayan@stat.wisc.edu>

On Wednesday 17 November 2004 12:09, Dean Sonneborn wrote:
> I'm creating a dotchart but the background color is gray. In fact,
> when ever I use the Lattice package the background is gray, which
> prints as black on my non-color printer. How do I change the
> background color to white? 

See ?trellis.device. Note that if you want to print stuff, you should 
use the postscript or pdf device, in which case you should 
automatically get what you want.

> I'm also plotting two groups and would 
> like to use circles and triangles as the plot characters.

If you are unhappy with what you get after reading ?trellis.device, you 
could supply the pch, e.g.

dotplot(..., pch = 1:2)

Deepayan



From MSchwartz at MedAnalytics.com  Wed Nov 17 20:16:23 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Wed, 17 Nov 2004 13:16:23 -0600
Subject: [R] Pearson coefficient p value (was: no subject)
In-Reply-To: <1100718754.5809.17.camel@horizons.localdomain>
References: <419B966F.3060903@to.infn.it>
	<1100718754.5809.17.camel@horizons.localdomain>
Message-ID: <1100718983.5809.21.camel@horizons.localdomain>

On Wed, 2004-11-17 at 13:12 -0600, Marc Schwartz wrote:
> On Wed, 2004-11-17 at 19:20 +0100, Angela Re wrote:
> > Good evening,
> > I'm going to use R to calculate the P-value for Pearson coefficient. 
> > Does it exist an already defined function?How can I do?Thanks for 
> > helping me.
> > Angela
> 
> 
> help.search("Pearson") shows you:
> 
> ...
> cor.test(stats)      Test for Association/Correlation Between
>                      Paired Samples
> ...
> 
> 
> See ?cor.test
> 
> HTH,
> 
> Marc Schwartz


Ack.  Too quick on the send key.


Please also use a sensible Subject for your posts. It helps others when
searching the list archives, among other things.

Marc



From elvis at xlsolutions-corp.com  Wed Nov 17 20:25:29 2004
From: elvis at xlsolutions-corp.com (elvis@xlsolutions-corp.com)
Date: Wed, 17 Nov 2004 12:25:29 -0700
Subject: [R] Course R/S+  December 2004 in San Francisco
Message-ID: <20041117192529.548.qmail@webmail04.mesa1.secureserver.net>

XLSolutions Corporation (www.xlsolutions-corp.com) is proud 
to announce our December-2004 2-day R/S-plus courses in San Francisco &
Washington, DC


San Francisco -------------------------------------->  December
16th-17th
Washington, DC ----------------------------------->  December 16th-17th

Reserve your seat now at the early bird rates! Payment due AFTER the 
class. 


R/S-plus Fundamentals and Programming Techniques 
=======================================

Course Description: 
This two-day beginner to intermediate R/S-plus course focuses 
on a broad spectrum of topics, \
from reading raw data to a comparison of R and S. We will learn 
the essentials of data manipulation, graphical visualization 
and R/S-plus programming. We will explore statistical data analysis 
tools,including graphics with data sets. How to enhance your plots. 
We will perform basic statistics and fit linear regression models. 
Participants are encouraged to bring data for interactive sessions 


With the following outline: 
- An Overview of R and S 
- Data Manipulation and Graphics 
- Using Lattice Graphics 
- A Comparison of R and S-Plus 
- How can R Complement SAS? 
- Writing Functions 
- Avoiding Loops 
- Vectorization 
- Statistical Modeling 
- Project Management 
- Techniques for Effective use of R and S 
- Enhancing Plots 
- Using High-level Plotting Functions 
- Building and Distributing Packages (libraries) 


Email us for group discounts. 
Email Sue Turner: sue at xlsolutions-corp.com 
Phone: 206-686-1578 
Visit us: www.xlsolutions-corp.com/training.htm 
Please let us know if you and your colleagues are interested in this 
classto take advantage of group discount. Register now to secure your 
seat! 
Interested in R/Splus Advanced course? email us. 


Cheers, 
Elvis Miller, PhD 
Manager Training. 
XLSolutions Corporation 
206 686 1578 
www.xlsolutions-corp.com 
elvis at xlsolutions-corp.com



From kamala.thomas at pmigroup.com  Wed Nov 17 20:39:43 2004
From: kamala.thomas at pmigroup.com (Kamala Thomas)
Date: Wed, 17 Nov 2004 11:39:43 -0800
Subject: [R] need help on R
Message-ID: <88F97D511E6DC64CAE5D482370F1742333F4C9@PMI-HOME01.us.corp.pmigroup.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041117/ed0e9f17/attachment.pl

From joel3000 at gmail.com  Wed Nov 17 21:03:54 2004
From: joel3000 at gmail.com (Joel Bremson)
Date: Wed, 17 Nov 2004 12:03:54 -0800
Subject: [R] 3d scatter plot with drop line
Message-ID: <1253d67a04111712032a31b921@mail.gmail.com>

This is a follow up to my question from yesterday. I want to do in R
what is called a "3d scatter plot with drop lines" in S-PLUS.

Basically, it's a 3dscatterplot with lines connecting the x-y grid to
the z points.
The lines give a better perspective on the shape of the data surface.

How to?

Joel Bremson
UC Davis Statistics



From Roger.Bivand at nhh.no  Wed Nov 17 21:14:32 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Wed, 17 Nov 2004 21:14:32 +0100 (CET)
Subject: [R] R/S-related projects on Sourceforge? Trove Categorization  
	-GDAL
In-Reply-To: <20041117182516.GA12201@sonny.eddelbuettel.com>
Message-ID: <Pine.LNX.4.44.0411172109520.16382-100000@reclus.nhh.no>

On Wed, 17 Nov 2004, Dirk Eddelbuettel wrote:

> On Wed, Nov 17, 2004 at 11:52:57AM -0500, James.Callahan at CityofOrlando.net wrote:
> > Gretl, RPad and RMetrics, plus Ernesto's FLR and  fsap make five. 
> 
> Isn't RMetrics at rmetrics.org at the ETH in Zuerich, CH?
> 
> > "GDAL Package for R", makes six.
> 
> Add RPy (rpy.sf.net) to make seven (or six, if remove RMetrics). Gretl is a
> tad marginal, though.
> 

I'm losing count, rgdal was already mentioned, but I haven't seen:

r-spatial: An R-package for dealing with spatial data in S (R or S-PLUS); 
this package should provide classes and methods for spatial data (points, 
lines, polygons, grids) that can be relied upon by other packages that use 
spatial data.

rarcinfo: RArcInfo is a package for R (http://www.r-project.org) to import 
data from binary Arc/Info V7.X coverages and E00 files . This will allow R 
users to used it as a primary GIS tool.

Roger


> Dirk
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From ripley at stats.ox.ac.uk  Wed Nov 17 21:14:52 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 17 Nov 2004 20:14:52 +0000 (GMT)
Subject: [R] need help on R
In-Reply-To: <88F97D511E6DC64CAE5D482370F1742333F4C9@PMI-HOME01.us.corp.pmigroup.com>
References: <88F97D511E6DC64CAE5D482370F1742333F4C9@PMI-HOME01.us.corp.pmigroup.com>
Message-ID: <Pine.LNX.4.61.0411172010180.23495@gannet.stats>

On Wed, 17 Nov 2004, Kamala Thomas wrote:

> I am working on a modeling project for PMI-Australia and interested in
> using R, especially POLYMARS or PLYCLASS. I use SAS /PC on WINDOWS for
> the statistical analyses including Modeling. I got R downloaded to the
> system but can't figure out how to interface with SAS so that I could
> use SAS datasets already in place.

Please read the `R Data Import/Export' manual.

> Also I could not find POLYMARS on the library packages. I need to know
> which library has POLYMARS in it.

Unlike SAS, R does not SHOUT all the time.

Function polymars is in package 'polspline', as is polyclass, both thanks 
to Dr Kooperberg.

> Could you please help me with these issues?.
>
> I got your e-mail address from 'Other Resources' section and if I am not
> addressing the correct person could you please forward this to the right
> address and let me know?.

This is a list not a person, and it has a posting guide:

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andreasbetz at earthlink.net  Wed Nov 17 22:18:09 2004
From: andreasbetz at earthlink.net (Andreas Betz)
Date: Wed, 17 Nov 2004 13:18:09 -0800
Subject: [R] bioassay, excel
Message-ID: <410-220041131721189750@earthlink.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041117/ca64c2b7/attachment.pl

From Ted.Harding at nessie.mcc.ac.uk  Wed Nov 17 21:18:06 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 17 Nov 2004 20:18:06 -0000 (GMT)
Subject: [R] CDs for R?
In-Reply-To: <88ADDA4C-3861-11D9-86F5-000A95C76CA8@oulu.fi>
Message-ID: <XFMail.041117201651.Ted.Harding@nessie.mcc.ac.uk>

Thanks to everyone who joined in the discussion about
this and made comments or suggestions.

Special thanks too to those who mailed me off-list
with further suggestions, and offers to help me privately.
I'm appreciative of the latter, and may take up some
offers, but I hope it was clear originally that I was
raising the issue as one which might affect several
people and perhaps justify making some special provision
which would ease the situation.


The most positive general suggestion, I thought, came from
Jari Oksanen:

On 17-Nov-04 Jari Oksanen wrote:
> [...]
> 5 CDs sounds 4 too many. I once burnt CDs for my students,
> and they fitted nicely in one CD (Windows binaries, all
> packages as Windows binaries and sources, contributed
> documents).  I guess you can fit Windows, Mac and some
> Linux binaries all in one CD.
> 
> Now comes my suggestion to CRAN maintainer: this all would
> be easier, if you would produce a CD image file ('iso') that
> would contain a snapshot of the latest version: main binaries,
> all contributed packages, and docs. Getting somebody to help
> downloading this iso would be much easier than trying to
> collect all first and then make up your own cd image.

This would provide a ready target for people who need to ask
someone else to do the job for them. It's much easier to
specify "download the ISO image from the following URL and
burn me a CD" then to hope that a possibly ill-specified
'wget' would produce the desired result (as happened when
Linux Emporium did me a CD: it was mostly there, but there
were gaps and some things I hadn't wanted).

So I'd like to back Jari's proposal for an ISO image to be
planted on CRAN as a separate file with its own unique URL.
Exactly what its content should be may still be discussable,
but I would be satisfied with full sources and documentation
for R base and all contributed packages (maybe the Newsletter
would also be handy).


I was also interested in Dirk Edelbuettel's suggestion related
to Quantian:

On 17-Nov-04 Dirk Eddelbuettel wrote:
> I'll cc this reply to Mark Walker. His shop, budgetlinuxcds.com /
> blcds.com, is one of the resellers of my Quantian 'scientific /
> cluster-computing workstation on a bootable dvd' Linux
> distribution / environment
> (see http://dirk.eddelbuettel.com/quantian for more on this).
> 
> Mark has been consistently responsive while offering a low-cost
> cd/dvd service (of which I receive no cut, in case you're
> wondering about disclaimers).  I think he'd be happy to add
> regular snapshots of certain portions of
> http://cran.r-project.org/src/ , maybe for the sources and/or
> windows binaries, for his failry reasonable fees. 

Since Mark would need a specification of what to download
and burn, this could pehaps be a convenient primary source of
burned CDs derived from the proposed CRAN ISO.


I also liked David Whiting's suggestion of "R buddies" who
would be willing to provide CDs for cost + postage to people.
Though I am (for obvious reasons) not in a position to
download and burn the CD in the first place, I'd be happy
to join in, and help coordinate and distribute (once someone
has sent me a CD, it's then straightforward to produce more
copies and mail these on, though like David I don't have
"industrial strength hardware" and would only be able to
do it on a small scale -- but that reinforces the case for
a group of "buddies" who could share the load!):

On 16-Nov-04 David Whiting wrote:
> [...]
> I have been in a similar situation a fair bit in the past
> and understand your position. Now I'm back in the UK and
> have a reasonably fast broadband connection at home I'd be
> willing to help out now and then. I guess that to make this
> work more generally we would need to work out how to make
> sure that only the CDs get burned and not the prospective
> "customer" or "supplier". 
> 
> As for charges, I think I'd only be interested in covering
> costs of the CDs and postage. I don't have industrial
> strength hardware so I could not get into mass production.
> 
> Perhaps we could establish informal groups of "R buddies"
> where, for example, I help you and a small number of other
> people out each time there is an update and we establish
> some kind of trust between ourselves, rather than new people
> coming to me each time. People could sign up to be "suppliers"
> and be allocated or choose a group of people they provide the
> service to.
> 
> I would feel comfortable with something like this working for
> people who have been on the R-help list for a while and have
> some recgonised identity, and something to lose in terms of
> reputation if they take advantage. But, it is possible that
> new users might need it the most and, by definition, we might
> not feel comfortable dealing with new people.

This suggestion could conveniently be linked with the suggestion
for "ISO on CRAN".


On a final point:

On 16-Nov-04 Adaikalavan Ramasamy wrote:
> [...]
> But the real question is that if there are enough people on
> slow connection that are interested in obtaining R.

Well, I have to agree with that! If I were the only person
in this position then it wouldn't be fair to put pressure on
others to provide the "general support". As I explained, I do
feel that there are probably a number of people who might
benefit (though, apart from David whose woes are in the past,
there have so far been no "me too!" posts).


Hoping for further comment,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 17-Nov-04                                       Time: 20:16:51
------------------------------ XFMail ------------------------------



From Ted.Harding at nessie.mcc.ac.uk  Wed Nov 17 21:18:10 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 17 Nov 2004 20:18:10 -0000 (GMT)
Subject: [R] (no subject)
In-Reply-To: <419B966F.3060903@to.infn.it>
Message-ID: <XFMail.041117192843.Ted.Harding@nessie.mcc.ac.uk>

On 17-Nov-04 Angela Re wrote:
> Good evening,
> I'm going to use R to calculate the P-value for Pearson coefficient. 
> Does it exist an already defined function?How can I do?Thanks for 
> helping me.
> Angela

cor.test is what you need (according to your statement).
The Pearson correlation is the default coefficient.

Example:

  u<-rnorm(10);v<-rnorm(10);X<-u+v;Y<-v;
  cor.test(X,Y)

          Pearson's product-moment correlation

  data:  X and Y 
  t = 2.9483, df = 8, p-value = 0.01847
  alternative hypothesis: true correlation is not equal to 0 
  95 percent confidence interval:
   0.1686027 0.9291072 
  sample estimates:
        cor 
  0.7216238 

enter

  ?cor.test

for details of the various different ways of using this function.

Hoping this helps,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 17-Nov-04                                       Time: 19:28:43
------------------------------ XFMail ------------------------------



From brian_pfeng at yahoo.com  Wed Nov 17 21:44:08 2004
From: brian_pfeng at yahoo.com (Brian pfeng)
Date: Wed, 17 Nov 2004 14:44:08 -0600 (CST)
Subject: [R] AYUDA
Message-ID: <20041117204408.29285.qmail@web54702.mail.yahoo.com>

Hola, necesito informacion sobre como aplicar un
modelo espacio estado y filtro de kalman en R. Soy
nuevo en R.

Gracias

_________________________________________________________




From Mike.Prager at noaa.gov  Wed Nov 17 21:55:11 2004
From: Mike.Prager at noaa.gov (Mike Prager)
Date: Wed, 17 Nov 2004 15:55:11 -0500
Subject: [R] The hidden costs of GPL software?
In-Reply-To: <200411170955.iAH9toV21278008@hedwig1.umh.ac.be>
References: <200411170955.iAH9toV21278008@hedwig1.umh.ac.be>
Message-ID: <6.1.2.0.2.20041117151226.01eb1170@hermes.nos.noaa.gov>

This has been an interesting discussion. I make the following comment with 
hesitation, since I have neither the time nor the ability to implement it 
myself.

Using CLI software, an infrequent user has trouble remembering the known 
functions needed and trouble finding new ones (especially as that user gets 
older).  What might help is an added help facility more oriented towards 
tasks, rather than structured around functions or packages.

Such a help facility might have a tree structure.

Want help?  Are you looking for information on (1) data manipulation or (2) 
analysis?  If (1), do you want to to (3) import or export data, (4) 
transform data, (5) reshape data, or (6) select data?  If (2), do you want 
to (7) fit a model or (8) make a graph?  And so on....

Once appropriate function(s) are located, the user would be directed (by 
hyperlinks) to the existing help framework.

That could help the problem of knowing what you want to do, but not what it 
is called.  I think that "Introductory Statistics with R" is a step in that 
direction for the basics, as MASS is for more complex matters.  The 
question is whether such material can be incorporated into a help system 
that will allow users to find, more easily, what they need.  That largely 
depends, it seems to me, on a great deal of work by volunteers.

I agree also with the suggestion that a dedicated editor (or add-in) that 
could supply arguments for functions might be considerable help.

MHP


-- 
Michael Prager, Ph.D.
Population Dynamics Team, NMFS SE Fisheries Science Center
NOAA Center for Coastal Fisheries and Habitat Research
Beaufort, North Carolina  28516
http://shrimp.ccfhrb.noaa.gov/~mprager/



From andy_liaw at merck.com  Wed Nov 17 21:55:16 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 17 Nov 2004 15:55:16 -0500
Subject: [R] bioassay, excel
Message-ID: <3A822319EB35174CA3714066D590DCD50994E32A@usrymx25.merck.com>

It's not in R, but the system described in the pair of papers our group
published in the Journal of Biomolecular Screening late last year:

http://jbx.sagepub.com/cgi/content/refs/8/6/624
http://jbx.sagepub.com/cgi/content/refs/8/6/634

is based on S-PLUS and StatServer.  It was also presented at the Insightful
Tech. Conf. back in 2000.

For Excel/R connection, look for the R-(D)COM server on CRAN, which has an
RExcel plug-in for Excel.

Andy

> From: Andreas Betz
> 
> 
> Dear all,
> 
> iis there literature about application of R in bioassay and 
> HTS around or does anybody experience using R in these areas. 
> Is there any documentation on Excel XP/R interface around, 
> where the use of the com server is described. My Rserver can 
> not be started.
> 
> Andreas
> 
> Andreas Betz
> andreasbetz at earthlink.net
> Why Wait? Move to EarthLink.
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From tom_woody at web.de  Wed Nov 17 22:05:18 2004
From: tom_woody at web.de (=?ISO-8859-15?Q?Thomas_Sch=F6nhoff?=)
Date: Wed, 17 Nov 2004 22:05:18 +0100
Subject: [R] Power sampling
Message-ID: <419BBD0E.4020502@web.de>

Hello,

after a unsuccessful search in lists maliarchive I wonder how I could 
estimate the power of a sample size related to an unknown population.

Given the following (fake))situation:

I do have a database containing about 5 millions observations over 70 
variables.
I would like to compute (as epidemiologists are used to) the required 
  size of a sample to do some test on a test sample (test data), later 
doing some subsequent analysis of a new sample to build a prediction 
model.

Help facilities of R show some entries regarding power, but none of 
them seemed to be appropriate for my purpose (maybe I am wrong, but 
sometimes I have some difficulties to decipher the message of those 
tiny hints for available packages)

I would appreciate somebody effort to point me to the right 
package/function to archieve this task!


regards

Thomas



From tom_woody at web.de  Wed Nov 17 22:07:54 2004
From: tom_woody at web.de (=?ISO-8859-1?Q?Thomas_Sch=F6nhoff?=)
Date: Wed, 17 Nov 2004 22:07:54 +0100
Subject: [R] Power sampling(II)
Message-ID: <419BBDAA.90205@web.de>

Hmm, sorry for hitting the send button to quick, here is


the version output:

platform i386-pc-linux-gnu
arch     i386
os       linux-gnu
system   i386, linux-gnu
status
major    2
minor    0.0
year     2004
month    10
day      04
language R

regards

Thomas



From murdoch at stats.uwo.ca  Wed Nov 17 22:11:13 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 17 Nov 2004 16:11:13 -0500
Subject: [R] 3d scatter plot with drop line
In-Reply-To: <1253d67a04111712032a31b921@mail.gmail.com>
References: <1253d67a04111712032a31b921@mail.gmail.com>
Message-ID: <rffnp0t6cbmo2pjnjil3gee145bd33b02j@4ax.com>

On Wed, 17 Nov 2004 12:03:54 -0800, Joel Bremson <joel3000 at gmail.com>
wrote :

>This is a follow up to my question from yesterday. I want to do in R
>what is called a "3d scatter plot with drop lines" in S-PLUS.
>
>Basically, it's a 3dscatterplot with lines connecting the x-y grid to
>the z points.
>The lines give a better perspective on the shape of the data surface.
>
>How to?

The scatterplot3d package does this in one of its examples, using
type='h'.

Duncan Murdoch



From ripley at stats.ox.ac.uk  Wed Nov 17 22:12:28 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 17 Nov 2004 21:12:28 +0000 (GMT)
Subject: [R] CDs for R?
In-Reply-To: <XFMail.041117201651.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.041117201651.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.LNX.4.61.0411172056280.14129@gannet.stats>

On Wed, 17 Nov 2004 Ted.Harding at nessie.mcc.ac.uk wrote:

> So I'd like to back Jari's proposal for an ISO image to be
> planted on CRAN as a separate file with its own unique URL.
> Exactly what its content should be may still be discussable,
> but I would be satisfied with full sources and documentation
> for R base and all contributed packages (maybe the Newsletter
> would also be handy).

You do realize that this would change at least daily?  So it really isn't 
something that one would want to be mirrored around the CRAN network.

Even for the sources it is tricky, as those of us who rsync part of CRAN 
know -- for example src/contrib does not contain all the current packages, 
the orphaned ones being links.  One would probably want recent R-patched 
and R-devel tarballs, and they are not actually on CRAN.

For binaries (and I suspect that most `customers' would want binaries)
it is trickier still, as to meet GPL some of the sources in the Archive 
area would need to be included (and we have not bothered to work out 
what).

I suggest a suitable first step is for some interested party to write a 
script to prepare such an ISO image and to put it (the image) up for 
comment (modern OSes can mount such an image, allowing browsing). I 
suspect it would be worth producing only say monthly.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From deepayan at stat.wisc.edu  Wed Nov 17 22:15:57 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Wed, 17 Nov 2004 15:15:57 -0600
Subject: [R] 3d scatter plot with drop line
In-Reply-To: <1253d67a04111712032a31b921@mail.gmail.com>
References: <1253d67a04111712032a31b921@mail.gmail.com>
Message-ID: <200411171515.57405.deepayan@stat.wisc.edu>

On Wednesday 17 November 2004 14:03, Joel Bremson wrote:
> This is a follow up to my question from yesterday. I want to do in R
> what is called a "3d scatter plot with drop lines" in S-PLUS.
>
> Basically, it's a 3dscatterplot with lines connecting the x-y grid to
> the z points.
> The lines give a better perspective on the shape of the data surface.
>
> How to?

One possibility would be 

library(lattice)
cloud(y ~ x * z, type = 'h')

This actually tries to drop the lines to the X-Y plane (z = 0) (which is 
truncated on the 'x-y grid' if the z-limits don't include 0).

Deepayan



From spencer.graves at pdf.com  Wed Nov 17 22:33:03 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 17 Nov 2004 13:33:03 -0800
Subject: [R] State Space Modeling & Kalman Filtering (was "AYUDA")
In-Reply-To: <20041117204408.29285.qmail@web54702.mail.yahoo.com>
References: <20041117204408.29285.qmail@web54702.mail.yahoo.com>
Message-ID: <419BC38F.1090401@pdf.com>

Estimado Brian: 

      1.  El ingles es la lengua oficial de esta lista.  Muchas personas 
quienes pudieron haber contestado a su pregunta y para quienes el ingles 
no es su lengua materna no entienen el castellano. 

      2.  Have you tried "www.r-project.org" -> Search -> "R site 
search" -> Kalman?  I just got 97 hits there, several of which may 
interest you. 

      hope this helps. 
      spencer graves

Brian pfeng wrote:

>Hola, necesito informacion sobre como aplicar un
>modelo espacio estado y filtro de kalman en R. Soy
>nuevo en R.
>
>Gracias
>
>_________________________________________________________
>

>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From phgrosjean at sciviews.org  Wed Nov 17 22:37:32 2004
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Wed, 17 Nov 2004 22:37:32 +0100
Subject: [R] The hidden costs of GPL software?
In-Reply-To: <419B96C9.3030108@pdf.com>
Message-ID: <200411172137.iAHLbZad028789@outmx004.isp.belgacom.be>

Thank you all (+ a couple of offline comments) on this topic.
To summarize your comments:

- "Hidden" costs, may be better called "indirect" costs are not so easy to
calculate. In the cited paper
http://www.scientific-computing.com/scwsepoct04free_statistics.html, there
is an interesting advice from a people used to test and wrote about
commercial software. Indeed, the whole context around the use of a
(statistical) software should be taken into account, which would reveil also
indirect costs for commercial packages. Indeed, it is the Total Cost of
Ownership (TCO) that should be better considered in this context.

- This discussion is connected with the many discussions pro/cons for a R
GUI, or any other tool that will facilitate use of R, but loosing one big
advantage: currently, you have to know what you are doing to get a result
with R... What kind of nonsenses would we get from naive people if they can
obtain results with no, or little knowledge?

- R is viewed by some as a statistical development platform, mainly for the
scientific community. It excels there, but, is it even desirable to get it
also used "by the mass"?

- ***Many of you claim for a better help system to find a function more
easily, than for a GUI***. I think this point is very important and should
be placed somewhere high in the "to do" list in order to make R more
accessible to beginners/occasional users!

- There is no possibility to make a commercial GUI for R (thanks to the
GPL), and volunteer R developers tend to work on a problem until they get
the solution they need... And this rarely lead to the development of a GUI
on top of it, conserning statistical analyses. In this way, yes, there is an
intrinsic mechanism that makes R a program by experts, for experts.

- A GUI could cover only the bare essentials, is rather unflexible, etc...
For all these reasons, how would it help to learn such a feature-rich
environment as R? This is not the solution to the problem.

- It is more a question of education: it takes so much time to find a
function in a menu/dialog box, than to consult help pages to find the right
function. However, some categories of people are more accustomished to click
and drag that to read help pages!

- GUIs, by providing access to a limited amount of analyses in an inflexible
way, lead to the phenomenon of "software-driven analysis" where the way data
are analyzed is dependent on the software used.

- Only commercial software care about eye candy stuffs to get clients more
happy to use their software (and thus to sell more); "hidden beneath a
cosmetic veneer" in the original paper. R does not care, because there is
nothing to sell. So, as a consequence, you face the bare power, but sorry,
no eye candy!

- GUI work is slower and more error-prone... So, this should be considered
in the hidden costs AFTER the learning stage... in favor of R!

- "User-friendly" software tend to make a lot of assumptions (to present the
analysis in an easier way), and does not tell about it. These could lead to
nonsenses in some case, and the user even don't know, precisely because
these assumptions are not explained!

- The author of the paper talks about hidden costs, but he does not talk
about hidden benefits, because he does even not notice them: ***all the bugs
that aren't in it*** (I add: transparence in code + possibility for everyone
to propose a patch = a big part of the success of Open Source software,
especially for data analysis software)!

That's all, I think, for the summary!

Otherwise:
Patrick Burns <pburns at pburns.seanet.com> wrote :
>[...]
>I think Ted Harding was on  the mark when he said that it is the help 
>system that needs enhancement.  I can imagine a system that gets the 
>user to the right function and then helps fill in the arguments; all of 
>the time pointing them towards the command line rather than away from 
>it.

Duncan Murdoch [murdoch at stats.uwo.ca] answered:
>That would be helpful, and the only really difficult part would be the
>first part:  getting the user to the right function.  help.search()
>sometimes works, but often people ask for the wrong thing.

>After that, R knows a lot about the structure of its help files, so it
>could display all of the arguments with their defaults and the help text
>that corresponds to each argument, as well as the help text for the rest
>of the help file.

>Probably the main obstacle to getting this is finding someone with the
>time and interest to do it.

Humm, excuse me, but I think that SciViews and JGR *already* do that,... So
it appears that at least two people already spend their time and got their
interest focused on this topic. Also, functions for such purposes will be
added to the R GUI API... Meaning they will be available for a wider use.
And I am close to a solution under Windows where hitting a combination of
keys in ANY program will display a function tip with arguments, or a
contextual completion list for R code.

Finally:

It seems that a GUI for R is not just lacking, it is purposedly lacking...
And there are many argument in favor of this lack. OK for most R users. But
could you, please, consider these examples:

1) I teach basic biostat with R/SciViews-R/R Commander. It is a frank
success and almost all my students install it on their computer and start
using it...
So, the next year, I teach them an advanced biostat course with R. I decide
to give up with the GUI and to present analyses like PCA, MDS, LDA,
clustering, etc... directly in R. For each analysis, I make a small script
(10 lines or so), I explain it and show them how it works and how they can
edit it to analyze other data. It is a fiasco! It seems that a psychological
barrier induced by this unfamiliar object (the script) tends to obscure
everything in the mind of my students. I got returns in this way: most of
the students that started to use R seem disgusted after this second course,
and they switch back to another software with a GUI! When I ask them, they
say: SciViews-R/R commander is nice but limited to simple analyses. For
other analyses, the R scripts are just too complex for me, so I prefer to
use a different software.

2) Second case: I write an original analysis and I want to make it widely
available for oceanographers. Most of them do not want, and will never learn
the S language. They obviously need a simple and easy GUI on top of my R
function, because they want to run the analysis without knowing all the
details...

Obviously, these are concrete examples where a GUI should be a benefit...
unless one consider that R should be restricted to experts only!

Best regards,

Philippe Grosjean

..............................................<??}))><........
 ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
 ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
 ) ) ) ) )   Mons-Hainaut University, Pentagone
( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
 ) ) ) ) )   6, av du Champ de Mars, 7000 Mons, Belgium  
( ( ( ( (       
 ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.33.12
( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
 ) ) ) ) )      
( ( ( ( (    web:   http://www.umh.ac.be/~econum
 ) ) ) ) )
..............................................................



From yzhang4 at pobox.une.edu.au  Wed Nov 17 22:53:34 2004
From: yzhang4 at pobox.une.edu.au (Yuandan Zhang)
Date: Thu, 18 Nov 2004 08:53:34 +1100
Subject: [R] Search engine for LINUX MOZILLA
In-Reply-To: <Pine.LNX.4.61.0411170722060.2671@gannet.stats>
References: <20041117154759.0b9cf790.yzhang4@pobox.une.edu.au>
	<Pine.LNX.4.61.0411170722060.2671@gannet.stats>
Message-ID: <20041118085334.4c1edc88.yzhang4@pobox.une.edu.au>

Thanks for instructions, it works. here are a few simple steps which may be useful for others too.


1 GET SOURCE: Following the link http://www.MedAnalytics.com/INSTALL, go  and download 

http://www.MedAnalytics.com/j2re-1_4_2_01-linux-i586-rpm.bin

2 INSTALL: 
chown 007 j2re-1_4_2_01-linux-i586-rpm.bin
./j2re-1_4_2_01-linux-i586-rpm.bin

you will get j2re-1_4_2_01-linux-i586.rpm
have root  access 

rpm -Uh j2re-1_4_2_01-linux-i586.rpm

3 create a symbolic link

eg

   cd /usr/local/mozilla/plugins
ln -s /usr/java/j2re1.4.2_01/plugin/i386/ns610-gcc32/libjavaplugin_oji.so .

4 configure broswer to java enabled
  Edit => Preferences => Advanced, tick 'Enable Java'



From monch1962 at gmail.com  Wed Nov 17 23:59:31 2004
From: monch1962 at gmail.com (David Mitchell)
Date: Thu, 18 Nov 2004 09:59:31 +1100
Subject: [R] The hidden costs of GPL software?
In-Reply-To: <XFMail.041117123431.Ted.Harding@nessie.mcc.ac.uk>
References: <200411170955.iAH9toV21278008@hedwig1.umh.ac.be>
	<XFMail.041117123431.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <f6508a8604111714594eaed0f8@mail.gmail.com>

Hopefully my experience with R may add something to this discussion.

I majored in computer science in 1983, with minors in mathematics and
statistics.  As this was in the days when computers were largely big
centralised boxes with remote terminals, I didn't get to use computers
for stats while I was at uni.

Fast forward to a couple of years ago, and I've got to start "doing
statistics" on the computer for the type of work I now do.  A friend
pointed me to R, so off I went.  Between 1983 and then, I did a lot of
development, testing, documentation, management, troubleshooting, etc
work, so I think it's fair to say that, while my statistics knowledge
needed a top up, my computing background was very strong.

As of today, after approx 2 years of using R for relatively ad-hoc
tasks every few weeks, here's my thoughts about it:
- it's extremely powerful and well-maintained; kudos to everyone involved
- it's extremely concise; you can do a huge amount of work in very few
lines of code
- provided a particular task is close to one I've already done before,
using R I can extract info from a set of data at an amazing rate. 
Tasks that would take me an hour or so with another programming
language or toolset, may take me under a minute using R (obviously
depending on the size of the dataset)

Problems arise whenever I need to step outside my existing R knowledge
base, and use a feature or function that I haven't used before:
- the help documentation in general desperately needs work,
particularly the examples.  My thinking is that examples should pretty
much lead you through a trivial exercise using the tool being
discussed.  This is very rarely the case with R, and the examples seem
to assume you fully understand how e.g. a library works and just need
a simple reminder of the syntax.  For the purposes of comparison,
compare the documentation that comes with the Perl language; even if
you don't know what a function or keyword does, you can pretty much
read through the given examples and work it out without difficulty
- the GUI is pretty much just a working area on the screen; it's just
not "helpful".  It would probably be reasonably simple to add menu or
toolbar options to help a user identify how they can actually achieve
a particular task in R (e.g. select a function from a drop-down list,
and get one-liner documentation about what it does), but that hasn't
been done.  Many of the questions asked on this list (which are often
answered with "RTFM") are of the nature "I've got this conceptually
simple task to do, but I can't find out how to do it using R.  Please
help"; this is gratifying to me personally, since I frequently
encounter the same problem.  These issues are extremely frustrating,
as you often know the answer will be a one-liner but you may struggle
for hours or days trying to find it

As I said above, once you understand how to do a particular task in R,
you can leverage that knowledge to do similar tasks amazingly quickly;
the productivity that comes with using R in this context is
incredible.  However, that productivity tends to disappear when you
need to take even a small step outside your existing R knowledge base.

Now maybe I'm the only occasional R user out here, and everyone else
is using it 8 hours a day and acquired my 2 years' worth of knowledge
in their first week of use.  I doubt that is actually the case, and
the rest of us could really do with some help from the GUI.

Finally, please don't think I don't appreciate the mass of effort
required to get R to its current state.  I do, and it's made my life a
lot easier than it would otherwise have been.

Regards

Dave Mitchell



From f.harrell at vanderbilt.edu  Wed Nov 17 23:55:54 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 17 Nov 2004 16:55:54 -0600
Subject: [R-gui] Re: [R] The hidden costs of GPL software?
In-Reply-To: <419B5FE5.8040009@pburns.seanet.com>
References: <200411170955.iAH9toV21278008@hedwig1.umh.ac.be>	<419B2CB4.7010708@srres.com>
	<419B5FE5.8040009@pburns.seanet.com>
Message-ID: <419BD6FA.3050205@vanderbilt.edu>

Patrick Burns wrote:
> I'm a big advocate -- perhaps even fanatic -- of  making R easier for
> novices in order to spread its use, but I'm not convinced that  a GUI
> (at least in the traditional form) is the most valuable approach.
> 
> Perhaps an overly harsh summary of some of Ted Harding's statements
> is: You can make a truck easier to get into by taking off the wheels, but
> that doesn't make it more useful.
> 
> In terms of GUIs, I think what R should focus on is the ability for  user's
> to make their own specialized GUI.  So that a knowledgeable programmer
> at an installation can create a system that is easy for unsophisticated
> users for the limited number of tasks that are to be done.  The ultimate
> users may not even need to know that R exists.
> 
> I think Ted Harding was on  the mark when he said that it is the help
> system that needs enhancement.  I can imagine a system that gets the
> user to the right function and then helps fill in the arguments; all of the
> time pointing them towards the command line rather than away from
> it.
> 
> The author of the referenced article highlighted some hidden costs of R,
> but did not highlight the hidden benefits (because they were hidden from
> him).  A big benefit of R is all of the bugs that aren't in it (which 
> may or
> may not be due to its free status).
> 
> Patrick Burns
> 
> Burns Statistics
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
> 
> Jan P. Smit wrote:
> 
>> Dear Phillippe,
>>
>> Very interesting. The URL of the article is 
>> http://www.scientific-computing.com/scwsepoct04free_statistics.html.
>>
>> Best regards,
>>
>> Jan Smit
>>
>>
>> Philippe Grosjean wrote:
>>
>>> Hello,
>>>
>>> In the latest 'Scientific Computing World' magazine (issue 78, p. 
>>> 22), there
>>> is a review on free statistical software by Felix Grant ("doesn't 
>>> have to
>>> pay good money to obtain good statistics software"). As far as I 
>>> know, this
>>> is the first time that R is even mentioned in this magazine, given 
>>> that it
>>> usually discuss commercial products.
>>>
> [ ...]
> 
>>>

I really agree with you Patrick.  To me the keys are having better help 
search capabilities, linking help files to case studies or at least 
detailed examples, having a navigator by keywords (a rudimentary one is 
at http://biostat.mc.vanderbilt.edu/s/finder/finder.html), having a 
great library of examples keyed by statistical goals (a la BUGS examples 
guides), and having a menu-driven skeleton code generator that gives 
beginners a starting script to edit to use their variable names, etc. 
Also I think we need a discussion board that has a better "memory" for 
new users, like some of the user forums currently on the web, or using a 
wiki.

Frank

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From jemwa at sun.ac.za  Thu Nov 18 00:39:33 2004
From: jemwa at sun.ac.za (Gorden Jemwa)
Date: Thu, 18 Nov 2004 01:39:33 +0200
Subject: [R] OOP pkg compilation failure 
Message-ID: <419BE135.60004@sun.ac.za>

Hi all,

I'm trying to install the OOP package (http://www.omegahat.org/OOP) but 
having difficulty in resolving the errors generated during compilation. 
Googling doesn't seem to be giving much help.

Can anyone please help. Below is the transcript of what I get from my 
command prompt. (I'm running on rw_2.0.0 WIN XP SP2 platform).

Thanks in advance

---transcript----


Microsoft Windows XP [Version 5.1.2600]
(C) Copyright 1985-2001 Microsoft Corp.

C:\...\>R CMD INSTALL OOP.tar.gz


---------- Making package OOP ------------
   adding build stamp to DESCRIPTION
   making DLL ...
making treeApply.d from treeApply.c
gcc   -Ic:/R/rw2000/include -Wall -O2   -c treeApply.c -o treeApply.o
ar cr OOP.a treeApply.o
ranlib OOP.a
windres --include-dir c:/R/rw2000/include  -i OOP_res.rc -o OOP_res.o
gcc  --shared -s  -o OOP.dll OOP.def OOP.a OOP_res.o 
-Lc:/R/rw2000/src/gnuwin32  -lg2c -lR
   ... DLL made
   installing DLL
   installing R files
   save image
[1] TRUE
Initializing OOP objects in database 1
execution of package source for 'OOP' failed
make[2]: *** [c:/R/rw2000/library/OOP/R/OOP] Error 1
make[1]: *** [all] Error 2
make: *** [pkg-OOP] Error 2*** Installation of OOP failed ***

Removing 'c:/R/rw2000/library/OOP'

---end transcript----



From rjvbertin at hotmail.com  Thu Nov 18 01:04:45 2004
From: rjvbertin at hotmail.com (RenE J.V. Bertin)
Date: Thu, 18 Nov 2004 01:04:45 +0100
Subject: [R] changing (core) function argument defaults?
In-Reply-To: <20041117224310.GA2823@hortresearch.co.nz>
Message-ID: <BAY22-F8BBB2BB7E3C5D182A17A0A7C20@phx.gbl>

&gt;From: Patrick Connolly &lt;p.connolly at hortresearch.co.nz&gt;
&gt;To: &quot;RenE J.V. Bertin&quot; &lt;rjvbertin at hotmail.com&gt;
&gt;Subject: Re: [R] changing (core) function argument defaults?
&gt;Date: Thu, 18 Nov 2004 11:43:10 +1300

&gt;
&gt;On Wed, 20-Oct-2004 at 07:48PM +0200, RenE J.V. Bertin wrote:
&gt;
&gt;|&gt; Hello,
&gt;|&gt;
&gt;
&gt;|&gt; Is it possible to change the defaults for the arguments to a
&gt;|&gt; function, without changing the function code itself?  I'm asking
&gt;|&gt; because I'd like to override the default dimensions and font 
family
&gt;|&gt; for a graphics device. Before 2.0.0, I'd just do that with a small
&gt;|&gt; edit in the appropriate .R file containing the device function
&gt;|&gt; definition. I appears to be possible no longer. So rather than
&gt;|&gt; copying the definition into my own .Rprofile, it would be nice if
&gt;|&gt; just the defaults could be modified...
&gt;
&gt;I didn't notice a response to this question.  I'd like to do something
&gt;similar and haven't been able to work out how to do it.
&gt;
&gt;
&gt;best
&gt;
&gt;--
&gt;Patrick Connolly
&gt;HortResearch
&gt;Mt Albert
&gt;Auckland
&gt;New Zealand
&gt;Ph: +64-9 815 4200 x 7188

No, I haven't noticed a reply to this question neither.

Best,
Ren Bertin



From spencer.graves at pdf.com  Thu Nov 18 01:47:13 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 17 Nov 2004 16:47:13 -0800
Subject: [R] Changing graphics defaults [was:  changing (core) function
	argument defaults?]
In-Reply-To: <BAY22-F8BBB2BB7E3C5D182A17A0A7C20@phx.gbl>
References: <BAY22-F8BBB2BB7E3C5D182A17A0A7C20@phx.gbl>
Message-ID: <419BF111.10507@pdf.com>

      Under the S3 standard, you could make a local copy of any function 
and change the defaults in that local copy.  That may not always work 
under the S4 standard methods dispatch going to code hidden in 
namespaces.  In any event, it should be easy (and safer) to write a 
function with a slightly different name, e.g., adding a dot "." to the 
end of the name, that would have different defaults and would do nothing 
but call the function of interest.  This might be safer

      I don't have any suggestions about changing graphics defaults 
other than to ask for that specifically -- e.g., by changing the subject 
line to this email. 

      hope this helps.  spencer graves

RenE J.V. Bertin wrote:

> &gt;From: Patrick Connolly &lt;p.connolly at hortresearch.co.nz&gt;
> &gt;To: &quot;RenE J.V. Bertin&quot; &lt;rjvbertin at hotmail.com&gt;
> &gt;Subject: Re: [R] changing (core) function argument defaults?
> &gt;Date: Thu, 18 Nov 2004 11:43:10 +1300
>
> &gt;
> &gt;On Wed, 20-Oct-2004 at 07:48PM +0200, RenE J.V. Bertin wrote:
> &gt;
> &gt;|&gt; Hello,
> &gt;|&gt;
> &gt;
> &gt;|&gt; Is it possible to change the defaults for the arguments to a
> &gt;|&gt; function, without changing the function code itself?  I'm 
> asking
> &gt;|&gt; because I'd like to override the default dimensions and font 
> family
> &gt;|&gt; for a graphics device. Before 2.0.0, I'd just do that with a 
> small
> &gt;|&gt; edit in the appropriate .R file containing the device function
> &gt;|&gt; definition. I appears to be possible no longer. So rather than
> &gt;|&gt; copying the definition into my own .Rprofile, it would be 
> nice if
> &gt;|&gt; just the defaults could be modified...
> &gt;
> &gt;I didn't notice a response to this question.  I'd like to do 
> something
> &gt;similar and haven't been able to work out how to do it.
> &gt;
> &gt;
> &gt;best
> &gt;
> &gt;--
> &gt;Patrick Connolly
> &gt;HortResearch
> &gt;Mt Albert
> &gt;Auckland
> &gt;New Zealand
> &gt;Ph: +64-9 815 4200 x 7188
>
> No, I haven't noticed a reply to this question neither.
>
> Best,
> Ren?? Bertin
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html


-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From gunter.berton at gene.com  Thu Nov 18 01:55:25 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Wed, 17 Nov 2004 16:55:25 -0800
Subject: [R] changing (core) function argument defaults?
In-Reply-To: <BAY22-F8BBB2BB7E3C5D182A17A0A7C20@phx.gbl>
Message-ID: <200411180055.iAI0tPuA007130@faraday.gene.com>


Yes, I think for all practical purposes it (usually?) is. Here's an example.
Suppose I wish to change the default "constant" argument of mad from 1.48 to
2. Then

> z<-formals(mad)
> z$constant<-2
> mad<-as.function(c(z,body(mad)))
> mad
function (x, center = median(x), constant = 2, na.rm = FALSE, 
    low = FALSE, high = FALSE) 
{
    if (na.rm) 
        x <- x[!is.na(x)]
    n <- length(x)
    constant * if ((low || high) && n%%2 == 0) {
        if (low && high) 
            stop("`low' and `high' can't be both TRUE")
        n2 <- n%/%2 + as.integer(high)
        sort(abs(x - center), partial = n2)[n2]
    }
    else median(abs(x - center))
}

If you now attach the workspace/environment containing this newly defined
mad function to the search list before the stats package (which contains the
original mad()) you have effectively changed the default argument without
changing the function.

I hope experts will let us know when this can't be done (perhaps with
.internal functions or non-exported functions in namespaces, though it isn't
clear to me that one couldn't manually export them and do this here, too).

Of course, all the usual warnings about masking existing functions apply.

Cheers,

-- Bert Gunter
Genentech Non-Clinical Statistics
South San Francisco, CA
 
"The business of the statistician is to catalyze the scientific learning
process."  - George E. P. Box
 
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of RenE 
> J.V. Bertin
> Sent: Wednesday, November 17, 2004 4:05 PM
> To: p.connolly at hortresearch.co.nz
> Cc: r-help at r-project.org; r-devel at r-project.org
> Subject: Re: [R] changing (core) function argument defaults?
> 
> &gt;From: Patrick Connolly &lt;p.connolly at hortresearch.co.nz&gt;
> &gt;To: &quot;RenE J.V. Bertin&quot; &lt;rjvbertin at hotmail.com&gt;
> &gt;Subject: Re: [R] changing (core) function argument defaults?
> &gt;Date: Thu, 18 Nov 2004 11:43:10 +1300
> 
> &gt;
> &gt;On Wed, 20-Oct-2004 at 07:48PM +0200, RenE J.V. Bertin wrote:
> &gt;
> &gt;|&gt; Hello,
> &gt;|&gt;
> &gt;
> &gt;|&gt; Is it possible to change the defaults for the arguments to a
> &gt;|&gt; function, without changing the function code 
> itself?  I'm asking
> &gt;|&gt; because I'd like to override the default dimensions 
> and font 
> family
> &gt;|&gt; for a graphics device. Before 2.0.0, I'd just do 
> that with a small
> &gt;|&gt; edit in the appropriate .R file containing the 
> device function
> &gt;|&gt; definition. I appears to be possible no longer. So 
> rather than
> &gt;|&gt; copying the definition into my own .Rprofile, it 
> would be nice if
> &gt;|&gt; just the defaults could be modified...
> &gt;
> &gt;I didn't notice a response to this question.  I'd like to 
> do something
> &gt;similar and haven't been able to work out how to do it.
> &gt;
> &gt;
> &gt;best
> &gt;
> &gt;--
> &gt;Patrick Connolly
> &gt;HortResearch
> &gt;Mt Albert
> &gt;Auckland
> &gt;New Zealand
> &gt;Ph: +64-9 815 4200 x 7188
> 
> No, I haven't noticed a reply to this question neither.
> 
> Best,
> Ren?? Bertin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From r-stats at arcriswell.com  Thu Nov 18 02:59:17 2004
From: r-stats at arcriswell.com (Andrew Criswell)
Date: Thu, 18 Nov 2004 08:59:17 +0700
Subject: [R] The hidden costs of GPL software?
In-Reply-To: <6.1.2.0.2.20041117151226.01eb1170@hermes.nos.noaa.gov>
References: <200411170955.iAH9toV21278008@hedwig1.umh.ac.be>
	<6.1.2.0.2.20041117151226.01eb1170@hermes.nos.noaa.gov>
Message-ID: <419C01F5.9000107@arcriswell.com>

Hi All,

GRETL, a Gnu Regression, Econometrics and Time-series Library is 
open-source, cross-platform, multi-language and fully GUI based. The 
website is http://gretl.sourceforge.net/ This is NOT a personal plug, 
simply posted to show what can be done.

Andrew



From phwang11776 at gmail.com  Thu Nov 18 03:03:56 2004
From: phwang11776 at gmail.com (Piin-cherng Hwang)
Date: Wed, 17 Nov 2004 21:03:56 -0500
Subject: [R] R statistical Library download problem
Message-ID: <920a844c041117180379902354@mail.gmail.com>

Hello,

      I just download R 2.0.0 and R 2.0.1. After I tried to un-zip
them , I got error message "Error reading header after processing 0
entries" for both of them.

    Could you help? Thanks!
 
    Sincerely, 

     Piin-cherng Hwang



From vrobles at fi.upm.es  Thu Nov 18 03:55:44 2004
From: vrobles at fi.upm.es (Victor Robles)
Date: Wed, 17 Nov 2004 21:55:44 -0500
Subject: [R] Redirect standard input and output of R
Message-ID: <000001c4cd1a$1a69ed90$1035ae86@kakuru>

Dear R-people!

I??m trying to write a C program that write to the standard input of R
and read the standard output.
I can perfectly read the R output, but I??m not able of writing anything
to R.

This program really works with the 'cat?? UNIX command, but it does not
work with R. What I??m doing wrong??? It is possible to do it???

I want to start R once and use it thousands of times... that??s why I??m
trying to use this system.


      pipe(fdW);
      pipe(fdR);

	pid = fork();
	if (pid==-1)
		perror("Error fork");
	
	if (pid==0)
	{
		close(0);
		dup(fdW[0]);
		close(fdW[0]);
		close(fdW[1]);
		
		close(1);
		dup(fdR[1]);
		close(fdR[1]);
		close(fdR[0]);
		
		execlp("R","R","--slave",NULL);
		perror("Error exec");
	}
	else
	{
		close(fdW[0]);
        	close(fdR[1]); 

		write(fdW[1], "3+7", 3);

		total = 0;
		while (total<3)
		{
			leidos = read(fdR[0], &k, sizeof(k));
			printf("leidos son %d \n",leidos);
			k[leidos]='\0';
			if (leidos > 0){
				total = total + leidos;
				printf("%s\n",k);
			}
		}
	}
	exit(0);


Best regards,
Victor



From paulaugust2003 at yahoo.com  Thu Nov 18 04:55:59 2004
From: paulaugust2003 at yahoo.com (Paul August)
Date: Wed, 17 Nov 2004 19:55:59 -0800 (PST)
Subject: [R] Calling Rdqags doesn't produce correct result.
Message-ID: <20041118035559.86756.qmail@web50007.mail.yahoo.com>

Does anyone has a clue what went wrong in the
following attempt?

I am trying to call the R built-in function Rdqags()
from my C
program for numerical integration. Following are the C
program
and the corresponding R program:

C program
---------
    void test(double *a,
              double *b,
              double *epsabs,
              double *epsrel,
              double *result,
              double *abserr,
              int *neval,
              int *ier,
              int *limit,
              int *lenw,
              int *last,
              int *iwork,
              double *work,
              double *exx)
    {
        void *ex;
    
        ex = exx;
        Rdqags(tmpfun, ex, a, b, epsabs, epsrel,
result, abserr, neval, ier,
                limit, lenw, last, iwork, work);
    }
    
    // User supplied function
    void tmpfun(double *x, int n, void *ex)
    {
        int i;
        double *tmp;
    
        tmp = (double *)ex;
    
    //    for(i=1;i<=n;i++) {x[i] = pow(x[i], *tmp);}
        for(i=1;i<=n;i++) {x[i] = 2.0*x[i];}
        return;
    }



R program
---------
    test <- function(a, b, epsabs =
.Machine$double.eps^0.25, epsrel = epsabs,
                limit = 100, lenw = 400, ex = -0.5)
    {
        val <- .C("test",
            as.double(a),
            as.double(b),
            as.double(epsabs),
            as.double(epsrel),
            result = double(1),
            abserr = as.double(-1),
            neval = as.integer(-1),
            ier = as.integer(-1),
            as.integer(limit),
            as.integer(lenw),
            last = as.integer(-1),
            integer(limit+1),
            double(lenw+1),
            as.double(ex),
            NAOK=T,
            specialsok=T
        )
        val
    }


I followed the instructions in Writing R Extensions ->
The R API
-> Integration, and the program was complied and
loaded into R
successfully. However, calling R function test(0,
20)$result
gives me 385.0554, not 400! The returned ier value of
test(0, 20)
is 0! But if I call integrate(function(x) 2 * x, lower
= 0, upper
= 20), I got the correct result of 400. Apparently,
there must be
something wrong in my programs. But I cannot see where
is the
mistake. Do I miss anything obvious here?

The programs above were tested in R-1.9.0 for
MS-Windows and the
compiler is MSC++ V6. I have used similar calls to the
built-in
random generators and never had any problems.

Paul.



From ok at cs.otago.ac.nz  Thu Nov 18 05:38:36 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Thu, 18 Nov 2004 17:38:36 +1300 (NZDT)
Subject: [R] The hidden costs of GPL software?
Message-ID: <200411180438.iAI4ca93221910@atlas.otago.ac.nz>

Background:  I'm a Computer Science lecturer, and I read the blue book cover
to cover before ever setting finger to keyboard with R.

Observation:  I really only use R for very simple things, but there's
practically *nothing* I've done with R since installing it could have
been done via menus.  I seem to need lots of little R functions, lots
of little try this transformation plot that, fiddle with the other...
I've had a student use it, I've introduced it to colleauges, and they
would not have benefited one iota from a GUI interface.

I'm glad that the effort put into R has gone into the things it has.



From yzhang4 at pobox.une.edu.au  Thu Nov 18 06:57:58 2004
From: yzhang4 at pobox.une.edu.au (Yuandan Zhang)
Date: Thu, 18 Nov 2004 16:57:58 +1100
Subject: [R] gdata package for genetics
In-Reply-To: <200411180438.iAI4ca93221910@atlas.otago.ac.nz>
References: <200411180438.iAI4ca93221910@atlas.otago.ac.nz>
Message-ID: <20041118165758.20de1bd5.yzhang4@pobox.une.edu.au>

Hi,

I try to install genetics_1.1.1.tar.gz and get following errors. it looks for a package call 'gdata'. I looked and search r-project web, did not find any thing 

> R CMD INSTALL src/contrib/genetics_1.1.1.tar.gz
* Installing *source* package 'genetics' ...
** R
** data
** inst
** preparing package for lazy loading
Error in loadNamespace(i[[1]], c(lib.loc, .libPaths()), keep.source) :
        There is no package called 'gdata'
Execution halted
ERROR: lazy loading failed for package 'genetics'
** Removing '/usr/local/lib/R/library/genetics'
** Restoring previous '/usr/local/lib/R/library/genetics'

Can you help me out this?

Yuandan



From ripley at stats.ox.ac.uk  Thu Nov 18 08:26:39 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 18 Nov 2004 07:26:39 +0000 (GMT)
Subject: [R] R statistical Library download problem
In-Reply-To: <920a844c041117180379902354@mail.gmail.com>
References: <920a844c041117180379902354@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0411180723320.17654@gannet.stats>

On Wed, 17 Nov 2004, Piin-cherng Hwang wrote:

>      I just download R 2.0.0 and R 2.0.1. After I tried to un-zip
> them , I got error message "Error reading header after processing 0
> entries" for both of them.

No distribution of R is a zip file.  The sources are .tar.gz files, 
gzipped tar archives.

>    Could you help? Thanks!

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

The information you need is in the FAQs, which that points you too.
It also explains why we have no idea how to help you as we have almost no 
idea what you did, for you have not told us.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From felber at slf.ch  Thu Nov 18 08:35:32 2004
From: felber at slf.ch (Andi Felber)
Date: Thu, 18 Nov 2004 08:35:32 +0100
Subject: [R] ROracle connection problem
Message-ID: <5.1.0.14.1.20041118082940.02eb2e40@mail.slf.ch>

Hi,

I found the same question in the mailing list already a few months ago - 
but there was no answer to it - so I'll try it again

Could somebody help me to solve this following problem? I just begin to 
learn how to connect my Oracle database with R.

 > library(DBI)

 > library(ROracle)

Warning message:
DLL attempted to change FPU control word from 8001f to 9001f

 > ora=dbDriver("Oracle")
Error in initialize(value, ...) : Invalid names for slots of class 
OraDriver: Id
 >

My system is:

Windows 2000,
Oracle 9.2
R1.9.0

Thank you very much


andi
---------------------------------------------------------------------------------------------------------------- 

Andreas Felber
Swiss Federal Institute for Snow and Avalanche Research
Fl??elastrasse 11
CH-7260 Davos Dorf
phone ++41 81 417 02 52
email: felber at slf.ch
personal homepage: http://www.slf.ch/staff/pers-home/felber/felber-en.html
web www.slf.ch



From ripley at stats.ox.ac.uk  Thu Nov 18 08:53:30 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 18 Nov 2004 07:53:30 +0000 (GMT)
Subject: [R] gdata package for genetics
In-Reply-To: <20041118165758.20de1bd5.yzhang4@pobox.une.edu.au>
References: <200411180438.iAI4ca93221910@atlas.otago.ac.nz>
	<20041118165758.20de1bd5.yzhang4@pobox.une.edu.au>
Message-ID: <Pine.LNX.4.61.0411180749080.17654@gannet.stats>

>From the description file

Depends:  combinat, gdata, MASS

Now, gdata is part of the gregmisc bundle.

>  install.packages("genetics", depend=TRUE)

would have figured this out for you.


On Thu, 18 Nov 2004, Yuandan Zhang wrote:

> Hi,
>
> I try to install genetics_1.1.1.tar.gz and get following errors. it 
> looks for a package call 'gdata'. I looked and search r-project web, did 
> not find any thing

Really?: please search the list of packages at

http://cran.r-project.org/src/contrib/PACKAGES.html

again, for my browser finds `gdata' there.

>
>> R CMD INSTALL src/contrib/genetics_1.1.1.tar.gz
> * Installing *source* package 'genetics' ...
> ** R
> ** data
> ** inst
> ** preparing package for lazy loading
> Error in loadNamespace(i[[1]], c(lib.loc, .libPaths()), keep.source) :
>        There is no package called 'gdata'
> Execution halted
> ERROR: lazy loading failed for package 'genetics'
> ** Removing '/usr/local/lib/R/library/genetics'
> ** Restoring previous '/usr/local/lib/R/library/genetics'
>
> Can you help me out this?
>
> Yuandan
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Nov 18 08:59:14 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 18 Nov 2004 07:59:14 +0000 (GMT)
Subject: [R] ROracle connection problem
In-Reply-To: <5.1.0.14.1.20041118082940.02eb2e40@mail.slf.ch>
References: <5.1.0.14.1.20041118082940.02eb2e40@mail.slf.ch>
Message-ID: <Pine.LNX.4.61.0411180754440.17654@gannet.stats>

I guess the issue is the use of R-1.9.0 (2.0.1 is current).  Please ask 
the person who provided you with a Windows binary for ROracle which 
version it worked with.

On Thu, 18 Nov 2004, Andi Felber wrote:

> Hi,
>
> I found the same question in the mailing list already a few months ago - but 
> there was no answer to it - so I'll try it again
>
> Could somebody help me to solve this following problem? I just begin to learn 
> how to connect my Oracle database with R.
>
>> library(DBI)
>
>> library(ROracle)
>
> Warning message:
> DLL attempted to change FPU control word from 8001f to 9001f
>
>> ora=dbDriver("Oracle")
> Error in initialize(value, ...) : Invalid names for slots of class OraDriver: 
> Id
>>
>
> My system is:
>
> Windows 2000,
> Oracle 9.2
> R1.9.0
>
> Thank you very much
>
>
> andi
> ---------------------------------------------------------------------------------------------------------------- 
> Andreas Felber
> Swiss Federal Institute for Snow and Avalanche Research
> Fl?elastrasse 11
> CH-7260 Davos Dorf
> phone ++41 81 417 02 52
> email: felber at slf.ch
> personal homepage: http://www.slf.ch/staff/pers-home/felber/felber-en.html
> web www.slf.ch
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ligges at statistik.uni-dortmund.de  Thu Nov 18 09:34:53 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 18 Nov 2004 09:34:53 +0100
Subject: [R] Redirect standard input and output of R
In-Reply-To: <000001c4cd1a$1a69ed90$1035ae86@kakuru>
References: <000001c4cd1a$1a69ed90$1035ae86@kakuru>
Message-ID: <419C5EAD.3050305@statistik.uni-dortmund.de>

Victor Robles wrote:
> Dear R-people!
> 
> I??m trying to write a C program that write to the standard input of R
> and read the standard output.
> I can perfectly read the R output, but I??m not able of writing anything
> to R.
> 
> This program really works with the 'cat?? UNIX command, but it does not
> work with R. What I??m doing wrong??? It is possible to do it???


Please read the manual "Writing R Extensions", in particular the lines 
about "Rprintf".

Uwe Ligges


> I want to start R once and use it thousands of times... that??s why I??m
> trying to use this system.
> 
> 
>       pipe(fdW);
>       pipe(fdR);
> 
> 	pid = fork();
> 	if (pid==-1)
> 		perror("Error fork");
> 	
> 	if (pid==0)
> 	{
> 		close(0);
> 		dup(fdW[0]);
> 		close(fdW[0]);
> 		close(fdW[1]);
> 		
> 		close(1);
> 		dup(fdR[1]);
> 		close(fdR[1]);
> 		close(fdR[0]);
> 		
> 		execlp("R","R","--slave",NULL);
> 		perror("Error exec");
> 	}
> 	else
> 	{
> 		close(fdW[0]);
>         	close(fdR[1]); 
> 
> 		write(fdW[1], "3+7", 3);
> 
> 		total = 0;
> 		while (total<3)
> 		{
> 			leidos = read(fdR[0], &k, sizeof(k));
> 			printf("leidos son %d \n",leidos);
> 			k[leidos]='\0';
> 			if (leidos > 0){
> 				total = total + leidos;
> 				printf("%s\n",k);
> 			}
> 		}
> 	}
> 	exit(0);
> 
> 
> Best regards,
> Victor
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Thu Nov 18 09:41:52 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 18 Nov 2004 09:41:52 +0100
Subject: [R] Power sampling
In-Reply-To: <419BBD0E.4020502@web.de>
References: <419BBD0E.4020502@web.de>
Message-ID: <419C6050.9050109@statistik.uni-dortmund.de>

Thomas Sch??nhoff wrote:

> Hello,
> 
> after a unsuccessful search in lists maliarchive I wonder how I could 
> estimate the power of a sample size related to an unknown population.
> 
> Given the following (fake))situation:
> 
> I do have a database containing about 5 millions observations over 70 
> variables.
> I would like to compute (as epidemiologists are used to) the required 
>  size of a sample to do some test on a test sample (test data), later 
> doing some subsequent analysis of a new sample to build a prediction model.
> 
> Help facilities of R show some entries regarding power, but none of them 
> seemed to be appropriate for my purpose (maybe I am wrong, but sometimes 
> I have some difficulties to decipher the message of those tiny hints for 
> available packages)

You have to tell us for which test you are going to calculate the power 
... (and there might be nothing, since calculating the power precisely 
is not always that easy).

Uwe Ligges



> I would appreciate somebody effort to point me to the right 
> package/function to archieve this task!
> 
> 
> regards
> 
> Thomas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Thu Nov 18 09:47:20 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 18 Nov 2004 09:47:20 +0100
Subject: [R] OOP pkg compilation failure
In-Reply-To: <419BE135.60004@sun.ac.za>
References: <419BE135.60004@sun.ac.za>
Message-ID: <419C6198.3040805@statistik.uni-dortmund.de>

Gorden Jemwa wrote:

> Hi all,
> 
> I'm trying to install the OOP package (http://www.omegahat.org/OOP) but 
> having difficulty in resolving the errors generated during compilation. 
> Googling doesn't seem to be giving much help.
> 
> Can anyone please help. Below is the transcript of what I get from my 
> command prompt. (I'm running on rw_2.0.0 WIN XP SP2 platform).
> 
> Thanks in advance
> 
> ---transcript----
> 
> 
> Microsoft Windows XP [Version 5.1.2600]
> (C) Copyright 1985-2001 Microsoft Corp.
> 
> C:\...\>R CMD INSTALL OOP.tar.gz
> 
> 
> ---------- Making package OOP ------------
>   adding build stamp to DESCRIPTION
>   making DLL ...
> making treeApply.d from treeApply.c
> gcc   -Ic:/R/rw2000/include -Wall -O2   -c treeApply.c -o treeApply.o
> ar cr OOP.a treeApply.o
> ranlib OOP.a
> windres --include-dir c:/R/rw2000/include  -i OOP_res.rc -o OOP_res.o
> gcc  --shared -s  -o OOP.dll OOP.def OOP.a OOP_res.o 
> -Lc:/R/rw2000/src/gnuwin32  -lg2c -lR
>   ... DLL made
>   installing DLL
>   installing R files
>   save image
> [1] TRUE
> Initializing OOP objects in database 1
> execution of package source for 'OOP' failed
> make[2]: *** [c:/R/rw2000/library/OOP/R/OOP] Error 1
> make[1]: *** [all] Error 2
> make: *** [pkg-OOP] Error 2*** Installation of OOP failed ***
> 
> Removing 'c:/R/rw2000/library/OOP'

a) The package does not pass the checks under R-2.0.1 on Windows, so 
please contact the maintainer!
b) It can be installed, I guess something is wrong with your settings or 
collection of tools.
c) I have *temporarily* put a binary up to 
http://www.statistik.uni-dortmund.de/~ligges/OOP_0.5-1.zip

Uwe Ligges




> ---end transcript----
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From stats at psyctc.org  Thu Nov 18 09:52:38 2004
From: stats at psyctc.org (stats)
Date: Thu, 18 Nov 2004 08:52:38 +0000
Subject: [R] Where has the Debian respository gone?
In-Reply-To: <20041115013527.GA17007@sonny.eddelbuettel.com>
References: <cn8k7h$lin$1@sea.gmane.org>
	<20041115013527.GA17007@sonny.eddelbuettel.com>
Message-ID: <793505964.20041118085238@psyctc.org>

Hello Dirk,

Monday, November 15, 2004, 1:35:27 AM, you wrote:

DE> On Sun, Nov 14, 2004 at 10:53:42PM +0100, Christoph Bier wrote:
>> Hi all!
>> 
>> Did I miss something or is it just a temporary problem? Where has
>> the Debian respository
>> 
>> http://cran.r-project.org woody/main Packages
>> resp.
>> http://cran.r-project.org/bin/linux/debian/
>> 
>> gone? I tried it for about the last 7 hours.

DE> Current R and CRAN packages are on the Debian archives; you can install
DE> these on testing too.  To the best of my knowledge, there are no backports
DE> of current R and Debian CRAN packages to Debian stable. 

I'm a bit puzzled.  I had
        deb http://cran.r-project.org/bin/linux/debian woody main
in /etc/apt/sources.list and had hoped, perhaps rather unwisely, that
this would look after the transition from 1.8.0 on my internet server (Debian
stable) where it serves up some cgi-bin work.  (Most of my R work is
on a Win2k machine, much though I'd like to go Debian all the way,
that isn't possible for my main job in near future.)

Is there an easy way of upgrading R on a Debian stable machine?  I
don't want to move off stable as the security side of that server is
too important.  I also don't really want to compile it myself if I can
avoid that, the server is pretty old iron and that might back up all
the Email stuff it does.

Advice anyone?

TIA,

Chris



From john.zhao at synovate.com  Thu Nov 18 09:45:57 2004
From: john.zhao at synovate.com (john zhao)
Date: Thu, 18 Nov 2004 16:45:57 +0800
Subject: [R] Chinese character
Message-ID: <0E6C568558A9C64280EC171DAB66E3E3018595D9@mail.ami-group.com.cn>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041118/ddfd6d3a/attachment.pl

From wolski at molgen.mpg.de  Thu Nov 18 10:04:20 2004
From: wolski at molgen.mpg.de (Witold Eryk Wolski)
Date: Thu, 18 Nov 2004 10:04:20 +0100
Subject: [R] R/S-related projects on  Sourceforge?   Trove Categorization
In-Reply-To: <419B1530.1090208@molgen.mpg.de>
References: <419B1530.1090208@molgen.mpg.de>
Message-ID: <419C6594.2020808@molgen.mpg.de>

Hi,

This are the project which I have extracted from all your mails.
  
1. http://sourceforge.net/projects/rpgsql/      R PostgreSQL Interface
2. http://sourceforge.net/projects/r-spatial/   R package for spatial 
data classes
3. http://sourceforge.net/projects/rpy/         RPy (R from Python)
4. http://sourceforge.net/projects/ep-sf/       Expression Profiler
5. http://sourceforge.net/projects/fsap         fish stock assessment for R
6. http://sourceforge.net/projects/flr/         R packages for use in 
fisheries modelling
7. https://sourceforge.net/projects/runit/      R Unit Test Framework
8. http://sourceforge.net/projects/ep-sf/       Expression Profiler
9. http://sourceforge.net/projects/r-asp/       Analysis of Spatial Data 
in R
10. http://sourceforge.net/projects/r4proteomics/  R Packages for proteomics
11. http://sourceforge.net/projects/rdbi/          R Database Interface
12. http://sourceforge.net/projects/nlmeode/       R package combining 
nlme and odesolve
13. http://sourceforge.net/projects/rarcinfo/      RArcInfo
14. http://sourceforge.net/projects/rgdal          GDAL Package for R
15. http://sourceforge.net/projects/gretl/         GNU Regression, 
Econometrics and Time-series Library
16. ...
 
Sorry if I have missed any. But it is surely more than five.

Thanks all of you for your numerous response and also for some more 
general thoughts concerning
*collaboratory* development using source code repositories:
https://stat.ethz.ch/pipermail/r-devel/2004-November/031398.html.
If I see how R is evolving I believe that this ideas will be realized soon.
Especially that there are already repositories covering some of the 
functionality and dedicated to R --  Bioconductor  with a svn archive. 
Unfortunately, as I was told by Geff Gentry the costs of administering 
the svn, cvs servers are high, hence the number of developers must be 
limited. Also CRAN has some of the functionality but the svn or cvs 
support is missing.
Luckily hosting the sources on e.g. sourceforge and the builds on CRAN 
or Bioconductor gives already some of the essential functionality.

Therefore at this place big thanks to the Bioconductor, CRAN and 
sourceforge people.

/E



From rksh at soc.soton.ac.uk  Thu Nov 18 10:39:24 2004
From: rksh at soc.soton.ac.uk (Robin Hankin)
Date: Thu, 18 Nov 2004 09:39:24 +0000
Subject: Fwd: Re: [R] 3d scatter plot with drop line
Message-ID: <a06002001bdc21de4f046@[139.166.242.29]>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041118/16a6c22c/attachment.pl

From danbebber at forestecology.co.uk  Thu Nov 18 10:52:57 2004
From: danbebber at forestecology.co.uk (Dan Bebber)
Date: Thu, 18 Nov 2004 09:52:57 -0000
Subject: [R] summary.lme() vs. anova.lme()
In-Reply-To: <Pine.LNX.4.61.0411171524560.9736@gannet.stats>
Message-ID: <001101c4cd54$61fa0a20$7d2501a3@plants.ox.ac.uk>

For anyone following this thread in the future:
Following Prof. Ripley's advice, I compared models fitted with ML, with and
without treatment as a predictor:

> anova(mconc.lme1,mconc.lme2)
           Model df       AIC       BIC   logLik   Test L.Ratio p-value
mconc.lme1     1 10 -1366.184 -1327.240 693.0919                       
mconc.lme2     2 16 -1363.095 -1300.785 697.5475 1 vs 2 8.91124  0.1786

I can't reject the null hypothesis of no effect of treatment.

Many thanks.
Dan Bebber

Department of Plant Sciences
University of Oxford
South Parks Road
Oxford OX1 3RB
UK
Tel. 01865 275000



> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
> Sent: 17 November 2004 15:32
> To: Dan Bebber
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] summary.lme() vs. anova.lme()
> 
> 
> On Wed, 17 Nov 2004, Dan Bebber wrote:
> 
> > I modelled changes in a variable (mconc) over time (d) for 
> individuals
> > (replicate) given one of three treatments (treatment) 
> using: mconc.lme 
> > <- lme(mconc~treatment*poly(d,2), random=~poly(d,2)|replicate,
> > data=my.data)
> >
> > summary(mconc.lme) shows that the linear coefficient of one of the 
> > treatments is significantly different to zero, viz.
> >                            Value Std.Error  DF   t-value p-value
> > 					...	     ... ...      ...
> > ...
> > treatmentf:poly(d, 2)1  1.3058562 0.5072409 315  2.574430  0.0105
> >
> > But anova(mconc.lme) gives a non-significant result for the 
> > treatment*time interaction, viz.
> >                     numDF denDF   F-value p-value
> > (Intercept)              1   315 159.17267  <.0001
> > treatment                2    39   0.51364  0.6023
> > poly(d, 2)               2   315  17.43810  <.0001
> > treatment:poly(d, 2)     4   315   2.01592  0.0920
> >
> > Pinheiro & Bates (2000) only discusses anova() for single arguments 
> > briefly on p.90. I would like to know whether these results 
> indicate 
> > that the significant effect found in summary(mconc.lme) is spurious 
> > (perhaps due to multiplicity).
> 
> Probably yes (but p values of 9% and 1% are not that 
> different, and in 
> both cases you are looking at a few p values).
> 
> But since both summary.lme and anova.lme use Wald tests, I 
> would use a 
> LRT, using anova on two fits (and I would use ML fits to get 
> a genuine 
> LRT but that is perhaps being cautious).
> 
> To  Dimitris Rizopoulos: as this is the last term in the 
> sequential anova, 
> it is the correct Wald test.
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From roebuck at odin.mdacc.tmc.edu  Thu Nov 18 11:05:59 2004
From: roebuck at odin.mdacc.tmc.edu (Paul Roebuck)
Date: Thu, 18 Nov 2004 04:05:59 -0600 (CST)
Subject: [R] Redirect standard input and output of R
Message-ID: <Pine.OSF.4.58.0411180405100.237686@odin.mdacc.tmc.edu>

On Wed, 17 Nov 2004, Victor Robles wrote:

> I'm trying to write a C program that write to the standard input of R
> and read the standard output. I can perfectly read the R output, but
> I'm not able of writing anything to R.
> [SNIP C code]

Several years ago, I wrote some software that used S-plus as its
back-end and Java for the front-end. I found what worked best was
to add a pseudoterminal inbetween; you can use the code from
"UNIX Programming Environment" to create one. The solution was
later reused when a similar project used R for its back-end.

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From phgrosjean at sciviews.org  Thu Nov 18 11:17:53 2004
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Thu, 18 Nov 2004 11:17:53 +0100
Subject: [R-gui] RE: [R] The hidden costs of GPL software?
In-Reply-To: <16796.3808.311268.952393@devzero.bogus.domain>
Message-ID: <200411181017.iAIAHrI5024559@outmx014.isp.belgacom.be>

John W. Eaton wrote:
> On 17-Nov-2004, Philippe Grosjean <phgrosjean at sciviews.org> wrote:
> 
> | - There is no possibility to make a commercial GUI for R (thanks to 
> | the GPL),
> 
> This is false.  Please don't confuse "commercial" (Red Hat 
> and SuSE GNU/Linux distributions are commercial software) 
> with "proprietary".
> 
> jwe

Ooops! Sorry, and thank you for correcting me. I mean "proprietary", of
course.
Best,

Philippe Grosjean



From tom_woody at swissinfo.org  Thu Nov 18 11:22:29 2004
From: tom_woody at swissinfo.org (=?ISO-8859-15?Q?Thomas_Sch=F6nhoff?=)
Date: Thu, 18 Nov 2004 11:22:29 +0100
Subject: [R] Power sampling
In-Reply-To: <419C6050.9050109@statistik.uni-dortmund.de>
References: <419BBD0E.4020502@web.de>
	<419C6050.9050109@statistik.uni-dortmund.de>
Message-ID: <419C77E5.80001@swissinfo.org>

Hello Uwe,

Uwe Ligges schrieb:
> Thomas Sch??nhoff wrote:

> You have to tell us for which test you are going to calculate the power 
> ... (and there might be nothing, since calculating the power precisely 
> is not always that easy).

Given my example from the first message I asked for a function which 
enables me to calculate a reasonable sample size.

I don't know the true mean or standard deviation of the population, I 
only know:

n= 5.000.000 observations over 70 variables

determined alpha = 0.01

I want to know how to generate a sufficiently sized sample based upon 
above mentioned facts to make some valid predictions regarding my 
population.

All I can hink of for now is that a two-tailed power test is required 
to find out if H0= random effect  or H1= no random effect hypothesis 
is accepted/rejected.

In epidemiological studies this situation is described like this:

How many cases do I have to include in my sample (s) to gain some 
representative results from a unknown population (=true mean, 
std-deviation etc.)?

How can I approach a situation like this in R ?



Regards

Thomas



platform i386-pc-linux-gnu
arch     i386
os       linux-gnu
system   i386, linux-gnu
status
major    2
minor    0.1
year     2004
month    11
day      15
language R



From tjrc at sanger.ac.uk  Thu Nov 18 11:27:02 2004
From: tjrc at sanger.ac.uk (Tim Cutts)
Date: Thu, 18 Nov 2004 10:27:02 +0000
Subject: [R] The hidden costs of GPL software?
In-Reply-To: <419B5FE5.8040009@pburns.seanet.com>
References: <200411170955.iAH9toV21278008@hedwig1.umh.ac.be>
	<419B2CB4.7010708@srres.com> <419B5FE5.8040009@pburns.seanet.com>
Message-ID: <630CA0C2-394C-11D9-A2B1-000A95B2B140@sanger.ac.uk>


On 17 Nov 2004, at 2:27 pm, Patrick Burns wrote:

> I think Ted Harding was on  the mark when he said that it is the help
> system that needs enhancement.  I can imagine a system that gets the
> user to the right function and then helps fill in the arguments; all 
> of the
> time pointing them towards the command line rather than away from
> it.

I think this is spot on.  My situation is that I am a scientist turned 
system administrator, and R is a package which I am increasingly being 
asked to install for the use of scientists at this Institute.  I am by 
no means a statistician;  the statistics I learned in A-level maths 
almost 20 years ago were as far as I got, and most of that I have 
forgotten.  But I like to have some understanding of the software 
packages I am asked to support, so I've been looking at R with a view 
to learning some of its more basic functions.  It looks potentially 
very useful to me anyway for summarising activity on the supercomputing 
cluster that I run.

So I'm a newbie to R, armed with only a very basic knowledge of 
statistics (I know the difference between a Normal and a Poisson 
distribution at least, and with a bit of prodding could probably 
remember a binomial distribution too).  I'm an experienced programmer 
in several languages, and a PhD-level scientist.

And yet I have still found R really quite hard to learn, and this is 
principally because the on-line help is a reference manual.  I'm sure 
it's a fabulous resource if you're a statistician who uses R every day, 
but for me it's not very helpful.

The R Intro PDF is good, but it would be nice if it were integrated 
better, with hyperlinks to the reference documentation, or to other 
parts of the introduction, for those platforms that support such things 
(it looks like this was intended for MacOS X, which is the version I am 
playing with for my own use, although the version I maintain for users 
is on Linux [ and would be on Alpha/Tru64 too if I could get it to pass 
its tests ]) but the on-line help link to the Intro on the Aqua R 
version brings up a blank page, so I'm using the generic PDF document 
instead.

I think the GUI question has nothing to do with the hidden costs of the 
GPL, or otherwise.  This is the age-old ease-of-use versus power and 
capability argument.

I don't think a fancy GUI is necessary - the GUI aspects that have been 
added to R on Mac OS X are sufficient.  I get the impression that the 
real power of R is the fact that really it's a programming language, 
and should probably be treated and learned as such.  Quite apart from 
the fact that a GUI will necessarily be a somewhat restricted subset of 
the total functionality, and a lot slower to use once you've taken the 
effort to learn the software, I think there is another danger, which I 
have already seen in other pieces of software in the bioinformatics 
community.  Users frequently run completely pointless analyses through 
the GUI wrappers we provide.  The users using the command line 
interfaces typically do much more sensible things.

If you make a piece of software trivial for a user to use without 
thinking about what they're doing, then the users won't think.  I may 
not know much about statistics, but what little I do know is that 
understanding exactly what form of analysis or significance test is 
required to be meaningful is a real skill that takes a lot of 
experience to master.   Having to perform that analysis with written 
commands means that your method is recorded, and could be published, 
and more importantly be checked and reproduced by other researchers.  
It also gives you ample time to think about what you're doing, rather 
than just bashing out a pretty graph which actually has no real meaning 
whatsoever.

Any GUI to R could (and should) be able to store the command line 
equivalent to what it has just done, to satisfy the reproducible 
criterion above, but I suspect it could still lead to some pretty 
shoddy work being done by careless and lazy scientists, and we get 
enough of that already.

Tim

-- 
Dr Tim Cutts
Informatics Systems Group, Wellcome Trust Sanger Institute
GPG: 1024D/E3134233 FE3D 6C73 BBD6 726A A3F5  860B 3CDD 3F56 E313 4233



From p.dalgaard at biostat.ku.dk  Thu Nov 18 11:28:40 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Nov 2004 11:28:40 +0100
Subject: [R] changing (core) function argument defaults?
In-Reply-To: <200411180055.iAI0tPuA007130@faraday.gene.com>
References: <200411180055.iAI0tPuA007130@faraday.gene.com>
Message-ID: <x2llcztprb.fsf@biostat.ku.dk>

Berton Gunter <gunter.berton at gene.com> writes:

> Yes, I think for all practical purposes it (usually?) is. Here's an example.
> Suppose I wish to change the default "constant" argument of mad from 1.48 to
> 2. Then
> 
> > z<-formals(mad)
> > z$constant<-2
> > mad<-as.function(c(z,body(mad)))

Actually, formals(mad)$constant<-2 will do the same. In either case,
you need to be careful to note that the environment of the function is
changed to the current frame. So if you redefine median(), this will
be picked up by your mad(), but not by stats::mad(). 

> If you now attach the workspace/environment containing this newly defined
> mad function to the search list before the stats package (which contains the
> original mad()) you have effectively changed the default argument without
> changing the function.
> 
> I hope experts will let us know when this can't be done (perhaps with
> .internal functions or non-exported functions in namespaces, though it isn't
> clear to me that one couldn't manually export them and do this here, too).

I'd expect that it works whenever the function has default arguments
to modify (.Primitive functions do not). The namespace mechanism only
ensures that you don't overwrite the original, and that packages
expecting to use the original can continue to do so.
 
> Of course, all the usual warnings about masking existing functions apply.

Yes, R is not preventing users from shooting themselves in the foot,
nor preventing package writers from shooting users in the foot (as a
recent query involving [.factor showed). 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From lecoutre at stat.ucl.ac.be  Thu Nov 18 12:05:34 2004
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Thu, 18 Nov 2004 12:05:34 +0100
Subject: [R] Lexical Scoping: eval(expr,envir=)
Message-ID: <6.0.1.1.2.20041118115648.02077ec0@stat4ux.stat.ucl.ac.be>


Hi R-listers,

I am trying to better undertand what we would call "functional paradigm" 
use of S/R to better map my programming activities in other languages.

This little function is aimed to create an object (at the end end, it would 
have it's own class):

--
   myObject =function(){
     list(
       a=1,
       foo=function(b)
       {
       cat("b:",b)
       cat("\na:", a)
       }
     )
   }
--
To my minds, "a" would be a property of the object and "foo" one of it's 
method.

Let instantiate one version of this object:

--
 > tmp = myObject()
 > tmp
$a
[1] 1

$foo
function(b)
       {
       cat("b:",b)
       cat("\na:", a)
       }
<environment: 012DDFC8>
--

Now I try to "invoke it's foo method" (definitively not a S terminology!)
For sure, tmp$foo() wont work, as it can't know anything about "a".

Reading eval() help page, It is said:

envir: the 'environment' in which 'expr' is to be evaluated.  May
           also be a list, a data frame, or an integer as in 'sys.call' was

so that I was thinking that

 > eval(tmp$foo(),envir=tmp)
Error in cat("b:", b) : Argument "b" is missing, with no default

would solve my problem, which is not the case.
tmp is a list, in which "a" is defined hand has a value.

Where is my fault?



Eric

R version 2.0.1, Windows




Eric Lecoutre
UCL /  Institut de Statistique
Voie du Roman Pays, 20
1348 Louvain-la-Neuve
Belgium

tel: (+32)(0)10473050
lecoutre at stat.ucl.ac.be
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre

If the statistics are boring, then you've got the wrong numbers. -Edward 
Tufte



From trujillo at soton.ac.uk  Thu Nov 18 12:16:28 2004
From: trujillo at soton.ac.uk (Trujillo L.)
Date: Thu, 18 Nov 2004 11:16:28 -0000
Subject: [R] AYUDA
Message-ID: <41310D8A2BD91B4CB42AB540EF248F59B75CAB@exchange4.soton.ac.uk>

Hi Brian

You could check the R-Documentation: functions KalmanLike, KalmanRun, KalmanSmooth, KalmanForecast and makeARIMA.

Leonardo Trujillo
------------------------------------------------------------------------------ 
Leonardo Trujillo
Southampton Statistical Sciences Research - S3RI
University of Southampton
------------------------------------------------------------------------------
-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Brian pfeng
Sent: 17 November 2004 20:44
To: r-help at stat.math.ethz.ch
Subject: [R] AYUDA

Hola, necesito informacion sobre como aplicar un modelo espacio estado y filtro de kalman en R. Soy nuevo en R.

Gracias

_________________________________________________________

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From tjrc at sanger.ac.uk  Thu Nov 18 12:40:31 2004
From: tjrc at sanger.ac.uk (Tim Cutts)
Date: Thu, 18 Nov 2004 11:40:31 +0000
Subject: [R] The hidden costs of GPL software?
In-Reply-To: <630CA0C2-394C-11D9-A2B1-000A95B2B140@sanger.ac.uk>
References: <200411170955.iAH9toV21278008@hedwig1.umh.ac.be>
	<419B2CB4.7010708@srres.com> <419B5FE5.8040009@pburns.seanet.com>
	<630CA0C2-394C-11D9-A2B1-000A95B2B140@sanger.ac.uk>
Message-ID: <A7197ABA-3956-11D9-A2B1-000A95B2B140@sanger.ac.uk>


On 18 Nov 2004, at 10:27 am, Tim Cutts wrote:

> The R Intro PDF is good, but it would be nice if it were integrated 
> better, with hyperlinks to the reference documentation, or to other 
> parts of the introduction, for those platforms that support such 
> things

I should correct myself here, and note that there are some 
cross-references within the PDF document, it's not completely devoid of 
them.

Tim

-- 
Dr Tim Cutts
Informatics Systems Group, Wellcome Trust Sanger Institute
GPG: 1024D/E3134233 FE3D 6C73 BBD6 726A A3F5  860B 3CDD 3F56 E313 4233



From mwgrant2001 at yahoo.com  Thu Nov 18 12:24:01 2004
From: mwgrant2001 at yahoo.com (Michael Grant)
Date: Thu, 18 Nov 2004 03:24:01 -0800 (PST)
Subject: [R] The hidden costs of GPL software?
In-Reply-To: <200411170955.iAH9toV21278008@hedwig1.umh.ac.be>
Message-ID: <20041118112401.43406.qmail@web54410.mail.yahoo.com>

Hmmmm, interesting thread and minds will not be
changed but regarding GUIs...I thought S (aka R) was a
PROGRAMMING LANGUAGE with a statistical and numerical
slant, and not a statistics application. ;O)  

Certainly there is an important place for GUIs but I
believe that it is very much overemphasized in modern
computer culture. My experience and bias--and I
started in the 1960's-- is that except for 'trivial'
uses, GUIs are a detriment to any reasonably complex
CREATIVE computational task. They are adequate for the
simple, common task. But even then, typing a command
or two is not overly taxing--- particularly when
compared to navigating layer upon layer of submenus as
is some times needed. If I need to, I will add a
little syntactical sugaring when coding and move on. 

GUIs encourage a passive approach to using computers
when solving problems. In addition, it is regretable
that a lot of people in the 'workplace' will carry out
incomplete and/or incorrect quantitative work because
of the real or perceived limitations of the particular
(GUI) apps they are using. There is no inclination to
go beyond the menu and even then many menu items
gather 'electronic dust'.

Finally, there are times for many of us when work
'goes home' at the end of the day. That just comes
with the territory. I (and most others) can not afford
the luxury of S-plus, Statistica, SPSS, etc. at home.
So in a sense there is a very real 'loss of
productivity' cost associated with using commercial
software. Now that does bring us around to supporting
R doesn't it? (Mea culpa. And I resolve to do better!)
What value does one put on the vitality of the R
community?

Best regards,
Michael Grant, Ph.D. 

* The requirements for creating packages are on
target,  and have the desired impact on both the
quality and breadth of R. 

--- Philippe Grosjean <phgrosjean at sciviews.org> wrote:

> Hello,
> 
> In the latest 'Scientific Computing World' magazine
> (issue 78, p. 22), there
> is a review on free statistical software by Felix
> Grant ...2.)



From tom_woody at swissinfo.org  Thu Nov 18 13:01:17 2004
From: tom_woody at swissinfo.org (=?ISO-8859-15?Q?Thomas_Sch=F6nhoff?=)
Date: Thu, 18 Nov 2004 13:01:17 +0100
Subject: [R] The hidden costs of GPL software?
In-Reply-To: <630CA0C2-394C-11D9-A2B1-000A95B2B140@sanger.ac.uk>
References: <200411170955.iAH9toV21278008@hedwig1.umh.ac.be>	<419B2CB4.7010708@srres.com>
	<419B5FE5.8040009@pburns.seanet.com>
	<630CA0C2-394C-11D9-A2B1-000A95B2B140@sanger.ac.uk>
Message-ID: <419C8F0D.1040601@swissinfo.org>

Tim Cutts schrieb:

> Any GUI to R could (and should) be able to store the command line 
> equivalent to what it has just done, to satisfy the reproducible 
> criterion above, but I suspect it could still lead to some pretty shoddy 
> work being done by careless and lazy scientists, and we get enough of 
> that already.

In that respect you should have a look at Emacs/XEmacs/ESS package. 
This package combines the power of command line and reproducibility of 
what has been done to generate graphs or whatever you like. Its also 
equipped with a nice ref-card-pdf which is very helpful to learn 
common shortcuts to increase your productivity levels. I wouldn't call 
ESS necessarily a GUI in a traditional sense, though.


When I started using R I was inclined to use the RCommander-GUI. After 
fiddling with this for a while I came to the conclusion that its 
possibilities are, at least for the moment, really limited. 
Furthermore some things increased my irritation levels, i.e. 
orientation to push the correct buttons to achieve a specific task. If 
I hit a false button I hardly wasn't able to find out what actually 
went wrong.

Nevertheless, for me as a beginner in GNU R, who never used S before, 
but primarily SPSS and BMDP in early times, it is a long way to gain 
some control of advanced aspects of using R. This is also true despite 
the fact that I took statistics courses for several years and do have 
experiences in research projects (social sciences and epidemiology), 
so I'll would agree that using GNU R has some hidden costs for me!

To sum up, what I am in need to is an extensive example based 
help-system, focused on how to do things in R. In parts this is 
already there, i.e. SimpleR from Verzani (contributed docs area) etc.

Hopefully I can  contribute to this in future, since it is seems to me 
invaluable to learn R by going through example-based lessons (some are 
found in vignette() ).
These are much more comprehensible to me than those short reference 
like entries in the current help-system, mostly due to their very 
technical approach (same is to be said about the official GNU R 
manuals, especially "The R Language", which wasn't a great help for me 
when I took my first look at GNU R). In this context something like 
the GuideMaps of Vista come to my mind!

But to be as clear as possible, I think GNU R is great and I 
appreciate all the efforts done by the R core team and associates!

Nevertheless it seems to be valuable to re-think the help-system in R 
with respect to those who may have a good understanding in statistics, 
but lacking some basic experiences in how to introduce themselves to 
sophisticated world of R/S languages.



Regards

Thomas



From francoisromain at free.fr  Thu Nov 18 13:35:15 2004
From: francoisromain at free.fr (francoisromain@free.fr)
Date: Thu, 18 Nov 2004 13:35:15 +0100
Subject: [R] Lexical Scoping: eval(expr,envir=)
In-Reply-To: <6.0.1.1.2.20041118115648.02077ec0@stat4ux.stat.ucl.ac.be>
References: <6.0.1.1.2.20041118115648.02077ec0@stat4ux.stat.ucl.ac.be>
Message-ID: <1100781315.419c9703de9eb@imp2-q.free.fr>

Hello,

I'm quite new to the objet-oriented vision of life in R, but I think you are
looking for

?setClass

or other functions in the methods package.



Selon Eric Lecoutre <lecoutre at stat.ucl.ac.be>:

>
> Hi R-listers,
>
> I am trying to better undertand what we would call "functional paradigm"
> use of S/R to better map my programming activities in other languages.
>
> This little function is aimed to create an object (at the end end, it would
> have it's own class):
>
> --
>    myObject =function(){
>      list(
>        a=1,
>        foo=function(b)
>        {
>        cat("b:",b)
>        cat("\na:", a)
>        }
>      )
>    }
> --
> To my minds, "a" would be a property of the object and "foo" one of it's
> method.
>
> Let instantiate one version of this object:
>
> --
>  > tmp = myObject()
>  > tmp
> $a
> [1] 1
>
> $foo
> function(b)
>        {
>        cat("b:",b)
>        cat("\na:", a)
>        }
> <environment: 012DDFC8>
> --
>
> Now I try to "invoke it's foo method" (definitively not a S terminology!)
> For sure, tmp$foo() wont work, as it can't know anything about "a".
>
> Reading eval() help page, It is said:
>
> envir: the 'environment' in which 'expr' is to be evaluated.  May
>            also be a list, a data frame, or an integer as in 'sys.call' was
>
> so that I was thinking that
>
>  > eval(tmp$foo(),envir=tmp)
> Error in cat("b:", b) : Argument "b" is missing, with no default
>
> would solve my problem, which is not the case.
> tmp is a list, in which "a" is defined hand has a value.
>
> Where is my fault?
>
>
>
> Eric
>
> R version 2.0.1, Windows
>
>
>
>
> Eric Lecoutre
> UCL /  Institut de Statistique
> Voie du Roman Pays, 20
> 1348 Louvain-la-Neuve
> Belgium
>
> tel: (+32)(0)10473050
> lecoutre at stat.ucl.ac.be
> http://www.stat.ucl.ac.be/ISpersonnel/lecoutre
>
> If the statistics are boring, then you've got the wrong numbers. -Edward
> Tufte
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From dimitris.rizopoulos at med.kuleuven.ac.be  Thu Nov 18 13:51:59 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Thu, 18 Nov 2004 13:51:59 +0100
Subject: [R] Lexical Scoping: eval(expr,envir=)
References: <6.0.1.1.2.20041118115648.02077ec0@stat4ux.stat.ucl.ac.be>
Message-ID: <005e01c4cd6d$6491c2a0$0540210a@www.domain>

Hi Eric,

this my novice point of view (since I'm still learning R) about what 
is happening:

First, `tmp' is not an evironment. Check:

is.environment(tmp)
[1] FALSE

If you'd like to create an environment based on tmp then a simple way 
could be:

e1 <- new.env()
for(i in seq(along=tmp)) assign(names(tmp)[[i]], tmp[[i]], envir=e1)

Then, `tmp$foo()' is defined in the enviroment of `myObject()', thus 
you could set its evironment to be "e1" using:

environment(tmp$foo) <- e1

now

eval(tmp$foo(2), envir=e1)

maybe is what you want.

Of course maybe someone more experienced than me has a better 
solution-explanation but I hope this helps.

Best,
Dimitrs

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Eric Lecoutre" <lecoutre at stat.ucl.ac.be>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, November 18, 2004 12:05 PM
Subject: [R] Lexical Scoping: eval(expr,envir=)


>
> Hi R-listers,
>
> I am trying to better undertand what we would call "functional 
> paradigm" use of S/R to better map my programming activities in 
> other languages.
>
> This little function is aimed to create an object (at the end end, 
> it would have it's own class):
>
> --
>   myObject =function(){
>     list(
>       a=1,
>       foo=function(b)
>       {
>       cat("b:",b)
>       cat("\na:", a)
>       }
>     )
>   }
> --
> To my minds, "a" would be a property of the object and "foo" one of 
> it's method.
>
> Let instantiate one version of this object:
>
> --
> > tmp = myObject()
> > tmp
> $a
> [1] 1
>
> $foo
> function(b)
>       {
>       cat("b:",b)
>       cat("\na:", a)
>       }
> <environment: 012DDFC8>
> --
>
> Now I try to "invoke it's foo method" (definitively not a S 
> terminology!)
> For sure, tmp$foo() wont work, as it can't know anything about "a".
>
> Reading eval() help page, It is said:
>
> envir: the 'environment' in which 'expr' is to be evaluated.  May
>           also be a list, a data frame, or an integer as in 
> 'sys.call' was
>
> so that I was thinking that
>
> > eval(tmp$foo(),envir=tmp)
> Error in cat("b:", b) : Argument "b" is missing, with no default
>
> would solve my problem, which is not the case.
> tmp is a list, in which "a" is defined hand has a value.
>
> Where is my fault?
>
>
>
> Eric
>
> R version 2.0.1, Windows
>
>
>
>
> Eric Lecoutre
> UCL /  Institut de Statistique
> Voie du Roman Pays, 20
> 1348 Louvain-la-Neuve
> Belgium
>
> tel: (+32)(0)10473050
> lecoutre at stat.ucl.ac.be
> http://www.stat.ucl.ac.be/ISpersonnel/lecoutre
>
> If the statistics are boring, then you've got the wrong 
> numbers. -Edward Tufte
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From michael.watson at bbsrc.ac.uk  Thu Nov 18 14:52:33 2004
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Thu, 18 Nov 2004 13:52:33 -0000
Subject: [R] Errors checking a library
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950121B896@iahce2knas1.iah.bbsrc.reserved>

Hi

I am writing an R library.  The documentation for one of my functions
includes an example that I *know* works - simply cut and paste into R on
either Windows and Linux and it works perfectly, no errors or warnings,
nothing, nyet.

However, when I run "R CMD check" on the library, I get an error.  I am
running R CMD check on linux, and the offending piece of code appears to
be:

cox[cox$group %in% onc,]

"cox" is a data frame, one of the columns of which is group, which
contains numbers.  "onc" is a vector of numbers.

The output on Linux from R CMD check is this:

> # lots of code
> # lots of code
>cox[cox$group
+
+
+ # the rest of my code
Error: syntax error
Execution halted

As can be seen, my code "cox[cox$group %in% onc,]" seems to have been
executed incorrectly.  Does R CMD check have a problem with the "%in%"
operator?  It would seem that R has somehow got mixed up and has lost
the rest of my command.

I'm using R 1.9.1 on Suse Linux 8.2.  The example code works fine on
both Windows and Linux when cut-and-pasted into an R window.

Mick



From lecoutre at stat.ucl.ac.be  Thu Nov 18 14:49:36 2004
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Thu, 18 Nov 2004 14:49:36 +0100
Subject: [R] Lexical Scoping: eval(expr,envir=)
In-Reply-To: <6.0.1.1.2.20041118115648.02077ec0@stat4ux.stat.ucl.ac.be>
References: <6.0.1.1.2.20041118115648.02077ec0@stat4ux.stat.ucl.ac.be>
Message-ID: <6.0.1.1.2.20041118144354.038c8ec0@stat4ux.stat.ucl.ac.be>



Hi again,

In a sense, I have answered myself my question.
The functional paradigm is very well described in the article
"lexical scope and Statistical computing" by Ross Ihaka and Robert Gentleman.
And I did have read it several times...


Solution is function closure. And following code will work as I want.
Except i dont understand the help page on "eval".
What about the ability to pass a list for the value of environment()? Can I 
have an example of such a use?


My (now working) code:

---

 > ### First version: function closure
 >
 >   myObject1 =function(){
+     # Function Closure
+     function(){
+       a=1
+       list(foo=function(b=3)
+       {
+       cat("b:",b)
+       cat("\na:", a)
+       }
+        )
+       }
+   }
 >
 >   (tmp=myObject())
function(){
       a=1
       list(
       foo=function(b=3)    {
         cat("b:",b)
         cat("\na:", a)
         },
        set.a=function(newval) a <<-newval
        )
       }
<environment: 012B2CAC>
 >   tmp()$foo()
b: 3
a: 1>   tmp()$foo(b=32)
b: 32
a: 1>
 > ### Second version: add a function that allows to change the property a
 >     myObject2 =function(){
+     function(){
+       a=1
+       return(list(
+       foo=function(b=3)    {
+         cat("b:",b)
+         cat("\na:", a)
+         },
+        set.a=function(newval) a <<-newval
+        ))
+       }
+   }
 >
 >    (tmp=myObject2()())
$foo
function(b=3)    {
         cat("b:",b)
         cat("\na:", a)
         }
<environment: 012BBF08>

$set.a
function(newval) a <<-newval
<environment: 012BBF08>

 >   tmp$foo(b=32)
b: 32
a: 1>   tmp$set.a(10)
 >   tmp$foo(b=32)
b: 32
a: 10>

---

This achieves exactly the object-oriented aspect I wanted to have. And in 
fact myObject()() acts as a new instantiation of my object.

Best wishes,

Eric




At 12:05 18/11/2004, Eric Lecoutre wrote:

>Hi R-listers,
>
>I am trying to better undertand what we would call "functional paradigm" 
>use of S/R to better map my programming activities in other languages.
>
>This little function is aimed to create an object (at the end end, it 
>would have it's own class):
>
>--
>   myObject =function(){
>     list(
>       a=1,
>       foo=function(b)
>       {
>       cat("b:",b)
>       cat("\na:", a)
>       }
>     )
>   }
>--
>To my minds, "a" would be a property of the object and "foo" one of it's 
>method.
>
>Let instantiate one version of this object:
>
>--
> > tmp = myObject()
> > tmp
>$a
>[1] 1
>
>$foo
>function(b)
>       {
>       cat("b:",b)
>       cat("\na:", a)
>       }
><environment: 012DDFC8>
>--
>
>Now I try to "invoke it's foo method" (definitively not a S terminology!)
>For sure, tmp$foo() wont work, as it can't know anything about "a".
>
>Reading eval() help page, It is said:
>
>envir: the 'environment' in which 'expr' is to be evaluated.  May
>           also be a list, a data frame, or an integer as in 'sys.call' was
>
>so that I was thinking that
>
> > eval(tmp$foo(),envir=tmp)
>Error in cat("b:", b) : Argument "b" is missing, with no default
>
>would solve my problem, which is not the case.
>tmp is a list, in which "a" is defined hand has a value.
>
>Where is my fault?
>
>
>
>Eric
>
>R version 2.0.1, Windows
>
>
>
>
>Eric Lecoutre
>UCL /  Institut de Statistique
>Voie du Roman Pays, 20
>1348 Louvain-la-Neuve
>Belgium
>
>tel: (+32)(0)10473050
>lecoutre at stat.ucl.ac.be
>http://www.stat.ucl.ac.be/ISpersonnel/lecoutre
>
>If the statistics are boring, then you've got the wrong numbers. -Edward Tufte
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Thu Nov 18 15:12:34 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 18 Nov 2004 15:12:34 +0100
Subject: [R] Errors checking a library
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950121B896@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E950121B896@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <419CADD2.107@statistik.uni-dortmund.de>

michael watson (IAH-C) wrote:

> Hi
> 
> I am writing an R library.  The documentation for one of my functions
> includes an example that I *know* works - simply cut and paste into R on
> either Windows and Linux and it works perfectly, no errors or warnings,
> nothing, nyet.
> 
> However, when I run "R CMD check" on the library, I get an error.  I am
> running R CMD check on linux, and the offending piece of code appears to
> be:
> 
> cox[cox$group %in% onc,]

You need to quote "%": "\%".

Uwe Ligges


> "cox" is a data frame, one of the columns of which is group, which
> contains numbers.  "onc" is a vector of numbers.
> 
> The output on Linux from R CMD check is this:
> 
> 
>># lots of code
>># lots of code
>>cox[cox$group
> 
> +
> +
> + # the rest of my code
> Error: syntax error
> Execution halted
> 
> As can be seen, my code "cox[cox$group %in% onc,]" seems to have been
> executed incorrectly.  Does R CMD check have a problem with the "%in%"
> operator?  It would seem that R has somehow got mixed up and has lost
> the rest of my command.
> 
> I'm using R 1.9.1 on Suse Linux 8.2.  The example code works fine on
> both Windows and Linux when cut-and-pasted into an R window.
> 
> Mick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From murdoch at stats.uwo.ca  Thu Nov 18 15:12:19 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 18 Nov 2004 09:12:19 -0500
Subject: [R] Lexical Scoping: eval(expr,envir=)
In-Reply-To: <6.0.1.1.2.20041118115648.02077ec0@stat4ux.stat.ucl.ac.be>
References: <6.0.1.1.2.20041118115648.02077ec0@stat4ux.stat.ucl.ac.be>
Message-ID: <0capp01pe7cou17co9anlid98hmtmgc49h@4ax.com>

On Thu, 18 Nov 2004 12:05:34 +0100, Eric Lecoutre
<lecoutre at stat.ucl.ac.be> wrote :

>
>Hi R-listers,
>
>I am trying to better undertand what we would call "functional paradigm" 
>use of S/R to better map my programming activities in other languages.
>
>This little function is aimed to create an object (at the end end, it would 
>have it's own class):
>
>--
>   myObject =function(){
>     list(
>       a=1,
>       foo=function(b)
>       {
>       cat("b:",b)
>       cat("\na:", a)
>       }
>     )
>   }
>--
>To my minds, "a" would be a property of the object and "foo" one of it's 
>method.

To work with lexical scoping, you could do it as

 myObject  <- function(){
    a <- 1 
     list(
       a=function() a,
       "a<-" = function(x, value) a <<- value,
       foo=function(b)
       {
       cat("b:",b,"\n")
       cat("a:", a,"\n")
       }
     )
   }

Then you can access the a property pretty easily, but changing it is
really ugly:

> m <- myObject()
> m$a()
[1] 1
> m$foo()
Error in cat("b:", b, "\n") : Argument "b" is missing, with no default
> m$foo(1)
b: 1 
a: 1 
> (m$"a<-")(m, 4)
> m$foo(1)
b: 1 
a: 4 

It would be slightly nicer (but still ugly) if this worked:

> (m$a)(m) <- 4
Error: invalid function in complex assignment

but it doesn't.

Duncan Murdoch



From charles.edwin.white at us.army.mil  Thu Nov 18 15:23:48 2004
From: charles.edwin.white at us.army.mil (White, Charles E WRAIR-Wash DC)
Date: Thu, 18 Nov 2004 09:23:48 -0500
Subject: [R] JGR & Tablet PC
Message-ID: <8BAEC5E546879B4FAA536200A292C6140A6A6D@AMEDMLNARMC135.amed.ds.army.mil>

(1) A minor point; the JGR console recognizes when the input panel (think tool bar) is docked at the top of the screen and acts appropriately. However, the editor opens under the input panel. I can close the input panel, move the editor, and open the input panel to continue work. On the other hand, it would be real nice if the editor just opened in a more appropriate place.

(2) I am very impressed with JGR as it is and I have completely switched from XEmacs to JGR.

Chuck

Charles E. White, Senior Biostatistician, MS
Walter Reed Army Institute of Research
503 Robert Grant Ave., Room 1w102
Silver Spring, MD 20910-1557
301 319-9781
Personal/Professional Site: http://users.starpower.net/cwhite571/professional/



From Roger.Bivand at nhh.no  Thu Nov 18 15:26:34 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Thu, 18 Nov 2004 15:26:34 +0100 (CET)
Subject: [R] Errors checking a library
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950121B896@iahce2knas1.iah.bbsrc
	.reserved>
Message-ID: <Pine.LNX.4.44.0411181513520.16983-100000@reclus.nhh.no>

On Thu, 18 Nov 2004, michael watson (IAH-C) wrote:

> Hi
> 
> I am writing an R library.  The documentation for one of my functions
> includes an example that I *know* works - simply cut and paste into R on
> either Windows and Linux and it works perfectly, no errors or warnings,
> nothing, nyet.
> 
> However, when I run "R CMD check" on the library, I get an error.  I am
> running R CMD check on linux, and the offending piece of code appears to
> be:
> 
> cox[cox$group %in% onc,]
> 
> "cox" is a data frame, one of the columns of which is group, which
> contains numbers.  "onc" is a vector of numbers.
> 
> The output on Linux from R CMD check is this:
> 
> > # lots of code
> > # lots of code
> >cox[cox$group
> +
> +
> + # the rest of my code
> Error: syntax error
> Execution halted
> 
> As can be seen, my code "cox[cox$group %in% onc,]" seems to have been
> executed incorrectly.  Does R CMD check have a problem with the "%in%"
> operator?  It would seem that R has somehow got mixed up and has lost
> the rest of my command.

I'm guessing at what you may mean, so excuse me getting it wrong. If the 
code is in an example in a help file (*.Rd, \examples{} block), then the % 
may be being interpreted as a comment character, and the remainder of the 
line not processed - see Writing R Extensions -> Writing R documentation 
files -> Insertions.

If that is the problem, then escaping the % by \% should fix it, that it 
what is done in src/library/base/man/match.Rd anyway.

> 
> I'm using R 1.9.1 on Suse Linux 8.2.  The example code works fine on
> both Windows and Linux when cut-and-pasted into an R window.
> 
> Mick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From abu3ammar at gmail.com  Thu Nov 18 15:31:15 2004
From: abu3ammar at gmail.com (Yasser El-Zein)
Date: Thu, 18 Nov 2004 09:31:15 -0500
Subject: [R] Informix database
Message-ID: <b1d31504041118063118ae4735@mail.gmail.com>

We use Informix database. I was able to connect to the database with
S-PLUS by using its CONNECT/Java through the JDBC driver.
How can I connect to Informix with R, wither using JDBC or any other
method? we run Linux so I prefer a method other than ODBC.

Thankx for the help



From p.dalgaard at biostat.ku.dk  Thu Nov 18 15:32:59 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Nov 2004 15:32:59 +0100
Subject: [R] Errors checking a library
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950121B896@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E950121B896@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <x2pt2brzvo.fsf@biostat.ku.dk>

"michael watson (IAH-C)" <michael.watson at bbsrc.ac.uk> writes:

> I am writing an R library.  

A whole library? Shouldn't you try a package first?

> The documentation for one of my functions
> includes an example that I *know* works - simply cut and paste into R on
> either Windows and Linux and it works perfectly, no errors or warnings,
> nothing, nyet.
> 
> However, when I run "R CMD check" on the library, I get an error.  I am
> running R CMD check on linux, and the offending piece of code appears to
> be:
> 
> cox[cox$group %in% onc,]
> 
> "cox" is a data frame, one of the columns of which is group, which
> contains numbers.  "onc" is a vector of numbers.
> 
> The output on Linux from R CMD check is this:
> 
> > # lots of code
> > # lots of code
> >cox[cox$group
> +
> +
> + # the rest of my code
> Error: syntax error
> Execution halted
> 
> As can be seen, my code "cox[cox$group %in% onc,]" seems to have been
> executed incorrectly.  Does R CMD check have a problem with the "%in%"
> operator?  It would seem that R has somehow got mixed up and has lost
> the rest of my command.

Think: What is the comment character in .Rd files?

Checking out src/library/base/man/match.Rd should be enlightening, but
since you might not have R sources installed, here are the relevant
bits:

\name{match}
\alias{match}
\alias{\%in\%}
\title{Value Matching}
\description{
  \code{match} returns a vector of the positions of (first) matches of
  its first argument in its second.

  \code{\%in\%} is a more intuitive interface as a binary operator,
  which returns a logical vector indicating if there is a match or not
  for its left operand.
}
\usage{
match(x, table, nomatch = NA, incomparables = FALSE)

x \%in\% table
}
....
\examples{
## The intersection of two sets :
intersect <- function(x, y) y[match(x, y, nomatch = 0)]
intersect(1:10,7:20)

1:10 \%in\% c(1,3,5,9)
sstr <- c("c","ab","B","bba","c","@","bla","a","Ba","\%")
sstr[sstr \%in\% c(letters,LETTERS)]

"\%w/o\%" <- function(x,y) x[!x \%in\% y] #--  x without y
(1:10) \%w/o\% c(3,7,12)
}
\keyword{manip}
 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From stijn.lievens at ugent.be  Thu Nov 18 15:44:21 2004
From: stijn.lievens at ugent.be (Stijn Lievens)
Date: Thu, 18 Nov 2004 15:44:21 +0100
Subject: [R] how to rewrite this without a loop ?
Message-ID: <419CB545.1000807@ugent.be>

Dear Rexperts,

First of all let me say that R is a wonderful and useful piece of 
software.

The only thing is that sometimes it takes me a long time to find out how 
something can be done, especially when aiming to write compact (and 
efficient) code.

For instance, I have the following function (very rudimentary) which 
takes a (very specific) data frame as input and for certain subsets
calculates the rank correlation between two corresponding columns.
The aim is to add all the rank correlations.

<code>
add.fun <- function(perf.data) {
    ss <- 0
    for (i in 0:29) {
    	ss <- ss + cor(subset(perf.data, dataset == i)[3], 
subset(perf.data, dataset == i)[7], method = "kendall")
    }
    ss	
}
</code>

As one can see this function uses a for-loop.  Now chapter 9 of 'An 
introduction to R' tells us that we should avoid for-loops as much as 
possible.

Is there an obvious way to avoid this for-loop is this case ?

I would like to see something in the lines of

(maple style)

<code>
add( seq(FUN(i), i = 0..29) )
</code>

Greetings

Stijn.


-- 
==========================================================================
Dept. of Applied Mathematics and Computer Science, University of Ghent
Krijgslaan 281 - S9, B - 9000 Ghent, Belgium
Phone: +32-9-264.48.91, Fax: +32-9-264.49.95
E-mail: Stijn.Lievens at ugent.be, URL: http://allserv.ugent.be/~slievens/



From vito_ricci at yahoo.com  Thu Nov 18 15:45:25 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Thu, 18 Nov 2004 15:45:25 +0100 (CET)
Subject: [R] Re:  Informix database
Message-ID: <20041118144525.34688.qmail@web41205.mail.yahoo.com>

Hi,

see DBI: R/S-Plus Database Interface, maybe it could
help you: 

http://stat.bell-labs.com/RS-DBI
http://www.ci.tuwien.ac.at/Conferences/DSC-2001/Proceedings/HothornJamesRipley.pdf

I found:

Other Database Connections from S-Plus

Depending on the platform it's running, S-Plus
provides access to some DBMS through the
importData/exportData functions. Under Microsoft
Windows you may import/export data through ODBC; under
Solaris (and only Solaris) you may import data from
Informix, Oracle, and Sybase. See also the function
executeSQL.  

Best regards
Vito

You wrote:
We use Informix database. I was able to connect to the
database with
S-PLUS by using its CONNECT/Java through the JDBC
driver.
How can I connect to Informix with R, wither using
JDBC or any other
method? we run Linux so I prefer a method other than
ODBC.

Thankx for the help

=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/



From ggrothendieck at myway.com  Thu Nov 18 15:52:23 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 18 Nov 2004 14:52:23 +0000 (UTC)
Subject: [R] Lexical Scoping: eval(expr,envir=)
References: <6.0.1.1.2.20041118115648.02077ec0@stat4ux.stat.ucl.ac.be>
Message-ID: <loom.20041118T145857-401@post.gmane.org>

Eric Lecoutre <lecoutre <at> stat.ucl.ac.be> writes:

: 
: Hi R-listers,
: 
: I am trying to better undertand what we would call "functional paradigm" 
: use of S/R to better map my programming activities in other languages.
: 
: This little function is aimed to create an object (at the end end, it would 
: have it's own class):
: 
: --
:    myObject =function(){
:      list(
:        a=1,
:        foo=function(b)
:        {
:        cat("b:",b)
:        cat("\na:", a)
:        }
:      )
:    }
: --
: To my minds, "a" would be a property of the object and "foo" one of it's 
: method.
: 
: Let instantiate one version of this object:
: 
: --
:  > tmp = myObject()
:  > tmp
: $a
: [1] 1
: 
: $foo
: function(b)
:        {
:        cat("b:",b)
:        cat("\na:", a)
:        }
: <environment: 012DDFC8>
: --
: 
: Now I try to "invoke it's foo method" (definitively not a S terminology!)
: For sure, tmp$foo() wont work, as it can't know anything about "a".
: 
: Reading eval() help page, It is said:
: 
: envir: the 'environment' in which 'expr' is to be evaluated.  May
:            also be a list, a data frame, or an integer as in 'sys.call' was
: 
: so that I was thinking that
: 
:  > eval(tmp$foo(),envir=tmp)
: Error in cat("b:", b) : Argument "b" is missing, with no default
: 
: would solve my problem, which is not the case.
: tmp is a list, in which "a" is defined hand has a value.
: 
: Where is my fault?
: 
: Eric
: 
: R version 2.0.1, Windows

An example of lexical scoping lose to yours
is found by issuing the R command:

   demo(scoping) 

However, you probably want to use the builtin
facilities, namely the S3 or S4 object oriented
system or possibly the more conventional oo system
defined in the R.oo package of the R.classes bundle
found at:
   http://www.maths.lth.se/help/R/R.classes/

In S3 your problem is done like this:

   # use S3 to define 2 objects of class "myObject" and a method

   tmp <- list(a=1); class(tmp) <- "myObject"
   tmp2 <- list(a=2); class(tmp2) <- "myObject"

   foo <- function(obj, b) UseMethod("foo")
   foo.myObject <- function(obj, b) cat("b", b, "\a", obj$a, "\n")

   # try out the method
   foo(tmp, 3)
   foo(tmp2, 5)



From abu3ammar at gmail.com  Thu Nov 18 15:52:41 2004
From: abu3ammar at gmail.com (Yasser El-Zein)
Date: Thu, 18 Nov 2004 09:52:41 -0500
Subject: [R] Re: Informix database
In-Reply-To: <20041118144525.34688.qmail@web41205.mail.yahoo.com>
References: <20041118144525.34688.qmail@web41205.mail.yahoo.com>
Message-ID: <b1d315040411180652b5581ee@mail.gmail.com>

There is no DBI implementation for Informix. I would appreciate some
pointers to R's Java extension, if exists.
I already have a solution for S-PLUS. I am interested in finding one
for R to compare the two, otherwise my company will go with S-PLUS.


On Thu, 18 Nov 2004 15:45:25 +0100 (CET), Vito Ricci
<vito_ricci at yahoo.com> wrote:
> Hi,
> 
> see DBI: R/S-Plus Database Interface, maybe it could
> help you:
> 
> http://stat.bell-labs.com/RS-DBI
> http://www.ci.tuwien.ac.at/Conferences/DSC-2001/Proceedings/HothornJamesRipley.pdf
> 
> I found:
> 
> Other Database Connections from S-Plus
> 
> Depending on the platform it's running, S-Plus
> provides access to some DBMS through the
> importData/exportData functions. Under Microsoft
> Windows you may import/export data through ODBC; under
> Solaris (and only Solaris) you may import data from
> Informix, Oracle, and Sybase. See also the function
> executeSQL.
> 
> Best regards
> Vito
> 
> 
> 
> You wrote:
> We use Informix database. I was able to connect to the
> database with
> S-PLUS by using its CONNECT/Java through the JDBC
> driver.
> How can I connect to Informix with R, wither using
> JDBC or any other
> method? we run Linux so I prefer a method other than
> ODBC.
> 
> Thankx for the help
> 
> =====
> Diventare costruttori di soluzioni
> Became solutions' constructors
> 
> "The business of the statistician is to catalyze
> the scientific learning process."
> George E. P. Box
> 
> Visitate il portale http://www.modugno.it/
> e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/
> 
> 
> ___________________________________




From Mike.Prager at noaa.gov  Thu Nov 18 15:59:36 2004
From: Mike.Prager at noaa.gov (Mike Prager)
Date: Thu, 18 Nov 2004 09:59:36 -0500
Subject: [R] The hidden costs of GPL software?
In-Reply-To: <419C8F0D.1040601@swissinfo.org>
References: <200411170955.iAH9toV21278008@hedwig1.umh.ac.be>
	<419B2CB4.7010708@srres.com> <419B5FE5.8040009@pburns.seanet.com>
	<630CA0C2-394C-11D9-A2B1-000A95B2B140@sanger.ac.uk>
	<419C8F0D.1040601@swissinfo.org>
Message-ID: <6.1.2.0.2.20041118094347.01ef21d0@hermes.nos.noaa.gov>

At 11/18/2004 07:01 AM Thursday, Thomas Sch??nhoff wrote:

>To sum up, what I am in need to is an extensive example based help-system, 
>focused on how to do things in R. In parts this is already there, i.e. 
>SimpleR from Verzani (contributed docs area) etc.
>
>Hopefully I can  contribute to this in future, since it is seems to me 
>invaluable to learn R by going through example-based lessons (some are 
>found in vignette() ).
>These are much more comprehensible to me than those short reference like 
>entries in the current help-system, mostly due to their very technical 
>approach (same is to be said about the official GNU R manuals, especially 
>"The R Language", which wasn't a great help for me when I took my first 
>look at GNU R). In this context something like the GuideMaps of Vista come 
>to my mind!
>
>But to be as clear as possible, I think GNU R is great and I appreciate 
>all the efforts done by the R core team and associates!
>
>Nevertheless it seems to be valuable to re-think the help-system in R with 
>respect to those who may have a good understanding in statistics, but 
>lacking some basic experiences in how to introduce themselves to 
>sophisticated world of R/S languages.

(I posted similar material before, but it was moved to R-devel, and I 
wanted to express a bit of it here.)

I have frequently felt, like Thomas, that what could make R easier to use 
is not a GUI, but a help system more focused on tasks and examples, rather 
than on functions and packages.  This has obvious and large costs of 
development, and I am unlikely to contribute much myself, for reasons of 
time and ability.  Yet, I mention it for the sake of this discussion.

Such a help system could be a tree (or key) structure in which through 
making choices, the user's description of the desired task is gradually 
narrowed.  At the end of each twig of the tree would be a list of suggested 
functions for solving the problem, hyperlinked into the existing help 
system (which in many ways is outstanding and has evolved just as fast as R 
itself).  This could be coupled with the continued expansion of the number 
of examples in the help system.

Now I must express appreciation for what exists already that helps in this 
regard:  MASS (in its many editions), Introductory Statistics with R, 
Simple R, and the other free documentation that so many authors have 
generously provided.  Not to mention the superlative contribution of R 
itself, and the work of the R development team.  It is beyond my 
understanding how something so valuable and well thought out has been 
created by people with so many other responsibilities.

Mike


-- 
Michael Prager, Ph.D.
Population Dynamics Team, NMFS SE Fisheries Science Center
NOAA Center for Coastal Fisheries and Habitat Research
Beaufort, North Carolina  28516
http://shrimp.ccfhrb.noaa.gov/~mprager/



From ripley at stats.ox.ac.uk  Thu Nov 18 16:15:05 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 18 Nov 2004 15:15:05 +0000 (GMT)
Subject: [R] Chinese character
In-Reply-To: <0E6C568558A9C64280EC171DAB66E3E3018595D9@mail.ami-group.com.cn>
References: <0E6C568558A9C64280EC171DAB66E3E3018595D9@mail.ami-group.com.cn>
Message-ID: <Pine.LNX.4.61.0411181512360.3711@gannet.stats>

On Thu, 18 Nov 2004, john zhao wrote:

> Hello R-help team,
>
> I am a R user in China.  I just downloaded the latest R which is R2001 for
> Windows. This new version can not store Chinese character which the previous
> version R 1.9.1 does.  Specifically when I enter

The previous version was 2.0.0.

>> x<-""
> then type "x", I got
>> x
> [1] "\262\342\312\324" .

That is because your locale is saying those are non-printable characters. 
It's an OS issue, not an R issue (and has been discussed before for R 
2.0.0).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From edd at debian.org  Thu Nov 18 16:18:40 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 18 Nov 2004 09:18:40 -0600
Subject: [R] Where has the Debian respository gone?
In-Reply-To: <793505964.20041118085238@psyctc.org>
References: <cn8k7h$lin$1@sea.gmane.org>
	<20041115013527.GA17007@sonny.eddelbuettel.com>
	<793505964.20041118085238@psyctc.org>
Message-ID: <20041118151840.GA22159@sonny.eddelbuettel.com>

On Thu, Nov 18, 2004 at 08:52:38AM +0000, stats wrote:
> Hello Dirk,
> 
> Monday, November 15, 2004, 1:35:27 AM, you wrote:
> 
> DE> On Sun, Nov 14, 2004 at 10:53:42PM +0100, Christoph Bier wrote:
> >> Hi all!
> >> 
> >> Did I miss something or is it just a temporary problem? Where has
> >> the Debian respository
> >> 
> >> http://cran.r-project.org woody/main Packages
> >> resp.
> >> http://cran.r-project.org/bin/linux/debian/
> >> 
> >> gone? I tried it for about the last 7 hours.
> 
> DE> Current R and CRAN packages are on the Debian archives; you can install
> DE> these on testing too.  To the best of my knowledge, there are no backports
> DE> of current R and Debian CRAN packages to Debian stable. 
> 
> I'm a bit puzzled.  I had
>         deb http://cran.r-project.org/bin/linux/debian woody main
> in /etc/apt/sources.list and had hoped, perhaps rather unwisely, that
> this would look after the transition from 1.8.0 on my internet server (Debian
> stable) where it serves up some cgi-bin work.  (Most of my R work is
> on a Win2k machine, much though I'd like to go Debian all the way,
> that isn't possible for my main job in near future.)
> 
> Is there an easy way of upgrading R on a Debian stable machine?  I
> don't want to move off stable as the security side of that server is
> too important.  I also don't really want to compile it myself if I can
> avoid that, the server is pretty old iron and that might back up all
> the Email stuff it does.
> 
> Advice anyone?

More than advice, we need a volunteer to "backport" the current R package(s)
for Debian to the Debian stable distribution. As I said, testing and
unstable are taken care of (and yes, testing is still lagging because of the
now much more formal interdependence of packages; R 2.0.* will appears once
all dependent packages are available on all architectures)

Dirk

-- 
If your hair is standing up, then you are in extreme danger.
      -- http://www.usafa.af.mil/dfp/cockpit-phys/fp1ex3.htm



From ripley at stats.ox.ac.uk  Thu Nov 18 16:22:56 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 18 Nov 2004 15:22:56 +0000 (GMT)
Subject: [R] Informix database
In-Reply-To: <b1d31504041118063118ae4735@mail.gmail.com>
References: <b1d31504041118063118ae4735@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0411181518340.3711@gannet.stats>

On Thu, 18 Nov 2004, Yasser El-Zein wrote:

> We use Informix database. I was able to connect to the database with
> S-PLUS by using its CONNECT/Java through the JDBC driver.
> How can I connect to Informix with R, wither using JDBC or any other
> method? we run Linux so I prefer a method other than ODBC.

ODBC runs perfectly well under Linux.  There is an Informix driver, see

http://www.unixodbc.org/doc/informix.html

R does not have a JDBC driver, and trying to use one under RSJava is 
likely to be *far* more work that using ODBC.

Vito Ricci mentioned DBI, but I am unaware of any Informix backend for 
DBI, which is not of itself a DBMS interface, more a meta-driver (as ODBC 
is).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From phgrosjean at sciviews.org  Thu Nov 18 16:24:31 2004
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Thu, 18 Nov 2004 16:24:31 +0100
Subject: [R-gui] Re: [R] The hidden costs of GPL software?
In-Reply-To: <419BD6FA.3050205@vanderbilt.edu>
Message-ID: <200411181524.iAIFOWUw004395@outmx011.isp.belgacom.be>

Hello,

I appreciate many comments and the various points of view, especially
because there are a couple of clear explanations why several people do not
need (or even do not want) a GUI for R!

Another part of the discussion seems to switch to the never-ending question
of "what kind of GUI"... which will never be answered, because there is not
one best GUI, and it also depends on the use (both the application and the
user). It's a long time I hesitate to propose in R-SIG-GUI + the R GUI
projects web site to place a description for one or several "prototype"
GUI(s) we would like for R, with the intention to include all the good ideas
everybody has in this list.

I never did that, because I am pretty sure it is useless! Now, I feel that
one guy, with a clear view of what he wants, a lot of free time, a lot of
energy, and some decent skills in programming, is actually required to make
real what he has in his head! Indeed, it is such a huge work that several
people are required! Here are the topics currently developed (sorry if I
don't cite Bioconductor stuff: I don't know it):

- Most of the "low-level" work is done, I think, like interface with
graphical toolkits: tcltk by Peter Dalgaard, of course, but many others
(Gtk, wxPython, ...), a better control of Rgui under Windows (ongoing,
Duncan Murdoch), ESS, ... All this is already available, even if one could
always argue that it is not optimal in some respects.

- A better console (multiple-lines editing, syntax coloring, code tip
presenting the syntax of a function when you type it, contextual completion
list, ...). This is ongoing project in both JGR and SciViews-R.

- A better table editor: RKward team.

- A classical menus/dialog box approach: John Fox's R commander,

- An object explorer: JGR, RKward, SciViews-R, experimental functions in R,

- A "plug-in" approach, that is, a piece of code that brings a GUI for a
targeted analysis and builds R code for you: RKward team, but also some
functions in svDialogs (part of the SciViews bundle, R GUI API),

- Interactive documents mixing formatted text, graphs, etc... with R
input/output: Rpad, Sweave (not interactive), and some other,

- Rich-formatted output of R objects (in/out, views, reporting,...): Eric
Lecoutre's R2HTML + SciViews-R,

- Code editor with interaction with R: Tinn-R, WinEdt, Emacs, and many
others, 

- IDE (humm, some code editors are not so far away from an IDE, but there is
still some lack here),

- A R GUI API: SciViews.

I hope all these projects will continue, will mature, and their developers
will ultimately realize that they provide complementary pieces of a giant
puzzle and start to work together. This is when it will become most
exciting! I hope also that it will result in an original GUI that keeps most
of the spirit of R, that is, not a simplified point&click UI, leading to
meaningless analyses by lazy people, but a real tool whose goal is to make R
easier and faster to learn for beginner, and pretty usable for occasional
users.

May be, I am just a dreamer, but all I read in this discussion reinforce my
conviction that an **innovative** GUI would be a good addition to R: most
criticisms clearly relate to the kind of inflexible GUI, with a forest of
menus and submenus, and other bad things one could find. I never, and will
never advocate for such a GUI!

For sure, the alternate GUI will only support you in writing R code, and
will deliver plenty of help to achieve this goal. I think it is possible...
with enough people collaborating in a common project! I think the later
point is really the problem: not enough people, too many projects! Is it a
consequence of the way R is developed (GPL)? Well, I think so, but only
partly. It is also the consequence of ego (everybody wants to be the leader
of his own project), and a lack of communication (R-SIG-GUI is not what one
would call an active list!) Or, may be, a "good GUI" for R is a fuzzy target
and it is not possible to cristallize enough power around a common goal: to
reach it!

Anyway, despite R GUI projects are progressing very slowly, I think only
when we would have a "good GUI" available for R, we would be able to
evaluate if there are really "hidden costs" in R, as Felix Grant suggests in
his paper.

Best regards and thank you all for your comments and suggestions.

Philippe Grosjean



From abu3ammar at gmail.com  Thu Nov 18 16:30:42 2004
From: abu3ammar at gmail.com (Yasser El-Zein)
Date: Thu, 18 Nov 2004 10:30:42 -0500
Subject: [R] Informix database
In-Reply-To: <Pine.LNX.4.61.0411181518340.3711@gannet.stats>
References: <b1d31504041118063118ae4735@mail.gmail.com>
	<Pine.LNX.4.61.0411181518340.3711@gannet.stats>
Message-ID: <b1d315040411180730d19e151@mail.gmail.com>

The Informix linux ODBC driver is not free, isn't it?
Shoudl I be worried of ODBC performance on linux? 


On Thu, 18 Nov 2004 15:22:56 +0000 (GMT), Prof Brian Ripley
<ripley at stats.ox.ac.uk> wrote:
> On Thu, 18 Nov 2004, Yasser El-Zein wrote:
> 
> 
> 
> > We use Informix database. I was able to connect to the database with
> > S-PLUS by using its CONNECT/Java through the JDBC driver.
> > How can I connect to Informix with R, wither using JDBC or any other
> > method? we run Linux so I prefer a method other than ODBC.
> 
> ODBC runs perfectly well under Linux.  There is an Informix driver, see
> 
> http://www.unixodbc.org/doc/informix.html
> 
> R does not have a JDBC driver, and trying to use one under RSJava is
> likely to be *far* more work that using ODBC.
> 
> Vito Ricci mentioned DBI, but I am unaware of any Informix backend for
> DBI, which is not of itself a DBMS interface, more a meta-driver (as ODBC
> is).
> 
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From stijn.lievens at ugent.be  Thu Nov 18 16:47:04 2004
From: stijn.lievens at ugent.be (Stijn Lievens)
Date: Thu, 18 Nov 2004 16:47:04 +0100
Subject: [R] how to rewrite this without a loop ?
In-Reply-To: <419CB545.1000807@ugent.be>
References: <419CB545.1000807@ugent.be>
Message-ID: <419CC3F8.2020800@ugent.be>

Stijn Lievens wrote:
> Dear Rexperts,
> 
> First of all let me say that R is a wonderful and useful piece of software.
> 
> The only thing is that sometimes it takes me a long time to find out how 
> something can be done, especially when aiming to write compact (and 
> efficient) code.
> 
> For instance, I have the following function (very rudimentary) which 
> takes a (very specific) data frame as input and for certain subsets
> calculates the rank correlation between two corresponding columns.
> The aim is to add all the rank correlations.
> 
> <code>
> add.fun <- function(perf.data) {
>    ss <- 0
>    for (i in 0:29) {
>        ss <- ss + cor(subset(perf.data, dataset == i)[3], 
> subset(perf.data, dataset == i)[7], method = "kendall")
>    }
>    ss   
> }
> </code>
> 
> As one can see this function uses a for-loop.  Now chapter 9 of 'An 
> introduction to R' tells us that we should avoid for-loops as much as 
> possible.
> 
> Is there an obvious way to avoid this for-loop is this case ?
> 

Using the lapply function in the e-mail of James, I came up with the 
following.

<code>
  sum (as.numeric( lapply( split(perf.data, perf.data$dataset), 
function(x) cor(x[3],x[7],method="kendall") ) ))
</code>

So, first I split the dataframe into a list of dataframes using split,
and using lapply I get a list of correlations, which I convert to
numeric and finally sum up.

I definitely avoided the for-loop in this way, although I am not sure 
whether this is more efficient or not.

Cheers,

Stijn.



> I would like to see something in the lines of
> 
> (maple style)
> 
> <code>
> add( seq(FUN(i), i = 0..29) )
> </code>
> 
> Greetings
> 
> Stijn.
> 
> 


-- 
==========================================================================
Dept. of Applied Mathematics and Computer Science, University of Ghent
Krijgslaan 281 - S9, B - 9000 Ghent, Belgium
Phone: +32-9-264.48.91, Fax: +32-9-264.49.95
E-mail: Stijn.Lievens at ugent.be, URL: http://allserv.ugent.be/~slievens/



From tlumley at u.washington.edu  Thu Nov 18 16:59:47 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 18 Nov 2004 07:59:47 -0800 (PST)
Subject: [R-gui] RE: [R] The hidden costs of GPL software?
In-Reply-To: <200411181017.iAIAHrI5024559@outmx014.isp.belgacom.be>
References: <200411181017.iAIAHrI5024559@outmx014.isp.belgacom.be>
Message-ID: <Pine.A41.4.61b.0411180755580.116790@homer11.u.washington.edu>

On Thu, 18 Nov 2004, Philippe Grosjean wrote:

> John W. Eaton wrote:
>> On 17-Nov-2004, Philippe Grosjean <phgrosjean at sciviews.org> wrote:
>>
>> | - There is no possibility to make a commercial GUI for R (thanks to
>> | the GPL),
>>
>> This is false.  Please don't confuse "commercial" (Red Hat
>> and SuSE GNU/Linux distributions are commercial software)
>> with "proprietary".
>>
>> jwe
>
> Ooops! Sorry, and thank you for correcting me. I mean "proprietary", of
> course.
> Best,
>

And it isn't obvious that it is true even if you mean proprietary. A GUI 
that ran R just by sending commands to stdin and getting results from 
stdout could clearly be proprietary without violating the GPL.  The 
question of exactly what level of closer integration is allowed would get 
complicated and I won't speculate.

 	-thomas



From MSchwartz at MedAnalytics.com  Thu Nov 18 17:03:27 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 18 Nov 2004 10:03:27 -0600
Subject: [R] The hidden costs of GPL software?
In-Reply-To: <20041118112401.43406.qmail@web54410.mail.yahoo.com>
References: <20041118112401.43406.qmail@web54410.mail.yahoo.com>
Message-ID: <1100793807.28822.152.camel@horizons.localdomain>

On Thu, 2004-11-18 at 03:24 -0800, Michael Grant wrote:
> Hmmmm, interesting thread and minds will not be
> changed but regarding GUIs...I thought S (aka R) was a
> PROGRAMMING LANGUAGE with a statistical and numerical
> slant, and not a statistics application. ;O)  

>From the R web site:

"R is a language and environment for statistical computing and
graphics."


I think that this is a critical point and that there is, to my mind, a
false predicate at play here.

That predicate is that somehow one should be able to rapidly learn R (or
any programming language for that matter) solely via the available
online reference help or via the freely provided documentation (whether
via R Core or via Contributors).

How many people here have learned to use C, FORTRAN, SAS, VBA, Perl or
any other language strictly by using built-in reference help systems. If
any, it will be a very small proportion.

Sure, SAS comes with documentation that can be measured in hernia
inducing tonnage, but at a substantial annual cost, which I have
referenced here and elsewhere previously. R is free.

Is there anyone who has learned to code in C that does not have a copy
of K&R someplace on their shelf, probably along with copies of other
both general and application specific C references published by
Prentice-Hall, Addison-Wesley, McGraw-Hill or Hayden?

It has been years since I actively coded in C, but I have almost 3
shelves filled with C reference books. I have books dating back to the
early 80's for 80x86 Assembly, MS-DOS/BIOS interrupts and Windows API
technical references and other such books that I used to use on a daily
basis in a former life.

For Linux, I have two shelves filled with various O'Reilly and other
references running the gambit from general Linux stuff to Perl,
Procmail, Postfix, Bash, Regex, Emacs, Admin, Firewalls and others.

For R, I have most of a shelf filled with multiple references, including
three of the four editions of MASS (somehow I missed the 2nd edition). I
have a copy of Peter's ISwR (because on occasion I have an acute attack
of cerebral flatulence and have to go back to basics) along with copies
of Pinheiro & Bates, Fox, Maindonald & Braun, Krause & Olson, Everitt &
Rabe-Hesketh and V&R's S Programming. I have copies of the "White Book"
and the "Green Book" and I have copies of Harrell and Therneau &
Grambsch for specific applications of R.

There are a fair number of already published books on R/S with more
coming by Faraway, Heiberger & Holland, Verzani and others including a
new series from Springer.

My point being that the old philosophy of "No Pain, No Gain" is a
component of the learning curve with R. R is not going to be for
everybody. That's why there are other "point and click" statistical
_applications_ like JMP (albeit not cheap). They are relatively easy,
but at the same time, they are self-limiting. No single math/statistical
"product" is going to meet the needs of the entire spectrum of the
potential user space.

As I have mentioned previously, I am a firm believer in Pareto's 80/20
Rule. In this case, you develop a "product" to meet the needs of 80% of
your target user space, because you will go "bankrupt" meeting the needs
of the other 20%. Said differently, meeting the needs of the other 20%
will consume 80% of your development resources, restricting your ability
to meet the needs of the larger audience.

Having spent 12 years previously with a commercial medical software
company, I will also suggest that typically 20% of your user base will
consume 80% of your support resources.

I will also note that having been on both sides of that equation, the
support provided here within this community is superb and has no peer in
the commercial arena.

In R's case, the 80% of the user space has perhaps been extended by the
kind offerings of those who have made specialty packages available via
CRAN, BioC and others.

It takes a certain level of commitment and time with R to become
effective with it.

That commitment includes, in my mind, supplementing the available _free_
documentation that has kindly been provided by R Core and others, with
other available resources. That does not mean that everyone needs to get
on Amazon.com and spend hundreds of $YOUR_MONETARY_UNIT on books. Many
are available via libraries and/or other resources, especially for those
here in academic environments.

This is a community effort folks and not everything is going to be
provided to you free of charge, with that notion being either in actual
financial cost or time.

It appears that, since this is not the first time this subject has come
up, there is strong interest in building a c("new", "different",
"better", ...) documentation/help system for R. That's fine. For those
that have interest in pursuing this, perhaps the time has come for a
group to form a new r-sig-doc list and move forward with the development
of a framework for a new system that can be developed and implemented by
that same group and then provided back to the community. 

Writing technical and user documentation is a specialty skill set unto
itself and perhaps those with the requisite skill sets will contribute
them for the benefit of all.

For those that do not have the skills and/or the time to contribute, I
would urge you to financially contribute to the R Foundation in whatever
way you can afford. Through that mechanism you will support the
community at large and the future development and enhancement of R.

There is no "hidden cost" here and certainly not one that is unique to
GPL software. The cost is self-evident and it is measured in time and 
$YOUR_MONETARY_UNITs. "Time is money" as they say and that is the same
whether you are using GPL software or a commercial proprietary product. 

A key difference here if any, is that none of us have paid anything for
R, where a portion of that "revenue" would go to support a dedicated
documentation team. In this case, it is "If you want it, you will need
to design and build it."

Best regards,

Marc Schwartz



From david.whiting at ncl.ac.uk  Thu Nov 18 17:14:03 2004
From: david.whiting at ncl.ac.uk (David Whiting)
Date: 18 Nov 2004 16:14:03 +0000
Subject: [R] Re: Informix database
In-Reply-To: <b1d315040411180652b5581ee@mail.gmail.com>
References: <20041118144525.34688.qmail@web41205.mail.yahoo.com>
	<b1d315040411180652b5581ee@mail.gmail.com>
Message-ID: <m21xerrv78.fsf@ganymede.ammp.or.tz>

Yasser El-Zein <abu3ammar at gmail.com> writes:

> There is no DBI implementation for Informix. I would appreciate some
> pointers to R's Java extension, if exists.
> I already have a solution for S-PLUS. I am interested in finding one
> for R to compare the two, otherwise my company will go with S-PLUS.

unixODBC works fine for me using MySQL with R and there appears to be
an Informix driver that works:

"The driver from informix works fine, but you need to have a look at
the doc in the manuals section of the this site"

http://www.unixodbc.org/drivers.html

I haven't tried it, but it looks in principle like you should be able
to access the Informix database from R via unixODBC.

Dave

-- 
David Whiting
University of Newcastle upon Tyne, UK



From tlumley at u.washington.edu  Thu Nov 18 17:13:48 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 18 Nov 2004 08:13:48 -0800 (PST)
Subject: [R] how to rewrite this without a loop ?
In-Reply-To: <419CB545.1000807@ugent.be>
References: <419CB545.1000807@ugent.be>
Message-ID: <Pine.A41.4.61b.0411180804320.116790@homer11.u.washington.edu>

On Thu, 18 Nov 2004, Stijn Lievens wrote:
>
> <code>
> add.fun <- function(perf.data) {
>   ss <- 0
>   for (i in 0:29) {
>   	ss <- ss + cor(subset(perf.data, dataset == i)[3], subset(perf.data, 
> dataset == i)[7], method = "kendall")
>   }
>   ss	}
> </code>
>
> As one can see this function uses a for-loop.  Now chapter 9 of 'An 
> introduction to R' tells us that we should avoid for-loops as much as 
> possible.


You don't say whether `dataset' is the name of a column in `perf.data'. 
Assuming it is, and assuming that 0:29 are all the values of `dataset'

sum(by(perf.data, list(perf.data$dataset),
           function(d)  cor(d[,3],d[,7], method="kendall")))

would work.  If this is faster it will be because you don't call subset() 
twice per iteration, rather than because you are avoiding a loop.  However 
it has other benefits: it doesn't have the variable `i', it doesn't have 
to change the value of `ss', and it doesn't have the range of `dataset' 
hard-coded into it.  These are all clarity optimisations.

 	-thomas



From ulask at bu.edu  Thu Nov 18 17:29:32 2004
From: ulask at bu.edu (ulas karaoz)
Date: Thu, 18 Nov 2004 11:29:32 -0500
Subject: [R] hashing using named lists
Message-ID: <06E6EB1E-397F-11D9-AD4A-000A95764B04@bu.edu>

hi all,
I am trying to use named list to hash a bunch of vector by name, for 
instance:
test = list()
test$name = c(1,2,3)

the problem is that when i try to get the values back by using the 
name, the matching isn't done in an exact way, so
test$na is not NULL.

is there a way around this?
Why by default all.equal.list doesnt require an exact match?
How can I do hashing in R?

thanks.
ulas.



From tiago17 at socrates.Berkeley.EDU  Thu Nov 18 17:39:47 2004
From: tiago17 at socrates.Berkeley.EDU (Tiago R Magalhaes)
Date: Thu, 18 Nov 2004 16:39:47 +0000
Subject: [R] Re: The hidden costs of GPL software?
In-Reply-To: <200411181106.iAIB6XI2020702@hypatia.math.ethz.ch>
References: <200411181106.iAIB6XI2020702@hypatia.math.ethz.ch>
Message-ID: <p06100500bdc27e59020e@[83.132.28.34]>

My background:

I am a biologist coming to R via Bioconductor. I have no computer 
background in computer sciences and only basic undergraduate training 
level in statistics.

I have used R with great pleasure and great pains. The most difficult 
thing is to know what functions to use - sometimes I know that one 
function is most likely available, but there's really no easy way to 
get it (yes, even going to the archives and reading the help files). 
I feel that more examples in the help files would definitely be a 
good way to fully understand the potencial of the functions. I know 
how difficult this is to do and how much of a time sink it must be.

One thing I defeinitely think would be a great improvement is to have 
a beefed up Object Explorer as Splus does. I think it's the great 
advantage of Splus when compared to R is to have much easier access 
to what type of object, col names, classes and so on there are. And 
how much easier it is to change all these attributes in Splus. I 
think R 2.0 in Mac did a lot to improve this, but I think that for 
someone that very frequently needs to know whether the object created 
turned out to be a vector or a list, the easy access to objects is 
very, very, very important and would be a great improvement in the 
ease of use of R.



From drf5n at maplepark.com  Thu Nov 18 18:14:55 2004
From: drf5n at maplepark.com (David Forrest)
Date: Thu, 18 Nov 2004 11:14:55 -0600 (CST)
Subject: [R] The hidden costs of GPL software?
In-Reply-To: <6.1.2.0.2.20041117151226.01eb1170@hermes.nos.noaa.gov>
References: <200411170955.iAH9toV21278008@hedwig1.umh.ac.be>
	<6.1.2.0.2.20041117151226.01eb1170@hermes.nos.noaa.gov>
Message-ID: <Pine.LNX.4.58.0411181102340.14150@maplepark.com>

On Wed, 17 Nov 2004, Mike Prager wrote:

...
> Using CLI software, an infrequent user has trouble remembering the known
> functions needed and trouble finding new ones (especially as that user gets
> older).  What might help is an added help facility more oriented towards
> tasks, rather than structured around functions or packages.
...

Another good (non-GUI) tool for the CLI is keyword completion.  R in ESS
does this, giving you lists of possible functions, variables and objects,
or feedback if there isn't any.  R's CLI completes, but only with
filenames in the current directory.

Dave
-- 
 Dave Forrest
 drf at vims.edu                                    (804)684-7900w
 drf5n at maplepark.com                             (804)642-0662h
                                   http://maplepark.com/~drf5n/



From Ted.Harding at nessie.mcc.ac.uk  Thu Nov 18 17:57:20 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 18 Nov 2004 16:57:20 -0000 (GMT)
Subject: [R] The hidden costs of GPL software?
In-Reply-To: <419B5FE5.8040009@pburns.seanet.com>
Message-ID: <XFMail.041118165720.Ted.Harding@nessie.mcc.ac.uk>

On 17-Nov-04 Patrick Burns wrote:
> [...]
> Perhaps an overly harsh summary of some of Ted Harding's
> statements is: You can make a truck easier to get into
> by taking off the wheels, but that doesn't make it more useful.

Yes, perhaps overly harsh ... but if you had said instead
"by deflating the tyres" then I think I'd agree that you were spot on!

Otherwise I agree with your other comments.

All best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 18-Nov-04                                       Time: 16:57:20
------------------------------ XFMail ------------------------------



From rpeng at jhsph.edu  Thu Nov 18 18:47:42 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 18 Nov 2004 12:47:42 -0500
Subject: [R] hashing using named lists
In-Reply-To: <06E6EB1E-397F-11D9-AD4A-000A95764B04@bu.edu>
References: <06E6EB1E-397F-11D9-AD4A-000A95764B04@bu.edu>
Message-ID: <419CE03E.7070307@jhsph.edu>

You could try using environments:

 > e <- new.env(hash = TRUE)
 > e$new <- 1:4
 > ls(e)
[1] "new"
 > e$new
[1] 1 2 3 4
 > e$ne
NULL

-roger

ulas karaoz wrote:
> hi all,
> I am trying to use named list to hash a bunch of vector by name, for 
> instance:
> test = list()
> test$name = c(1,2,3)
> 
> the problem is that when i try to get the values back by using the name, 
> the matching isn't done in an exact way, so
> test$na is not NULL.
> 
> is there a way around this?
> Why by default all.equal.list doesnt require an exact match?
> How can I do hashing in R?
> 
> thanks.
> ulas.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From ripley at stats.ox.ac.uk  Thu Nov 18 18:50:40 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 18 Nov 2004 17:50:40 +0000 (GMT)
Subject: [R] hashing using named lists
In-Reply-To: <06E6EB1E-397F-11D9-AD4A-000A95764B04@bu.edu>
References: <06E6EB1E-397F-11D9-AD4A-000A95764B04@bu.edu>
Message-ID: <Pine.LNX.4.61.0411181743190.26215@gannet.stats>

On Thu, 18 Nov 2004, ulas karaoz wrote:

> hi all,
> I am trying to use named list to hash a bunch of vector by name, for 
> instance:
> test = list()
> test$name = c(1,2,3)
>
> the problem is that when i try to get the values back by using the name, the 
> matching isn't done in an exact way, so
> test$na is not NULL.
>
> is there a way around this?

?match on the names

> Why by default all.equal.list doesnt require an exact match?

What do you mean by that?  It (by default or not) tests all the 
attributes, including the names:

test <- list(name=1:3)
test2 <- list(na=1:3)
all.equal(test, test2)
[1] "Names: 1 string mismatches"

Now, all.equal does not require an exact match, deliberately: that is what 
identical() is for:

identical(test, test2)
[1] FALSE

> How can I do hashing in R?

?match

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Arne.Muller at aventis.com  Thu Nov 18 18:50:38 2004
From: Arne.Muller at aventis.com (Arne.Muller@aventis.com)
Date: Thu, 18 Nov 2004 18:50:38 +0100
Subject: [R] Re: The hidden costs of GPL software?
Message-ID: <C80ECAFA2ACC1B45BE45D133ED660ADE010BF297@crbsmxsusr04.pharma.aventis.com>

[...]
> I am a biologist coming to R via Bioconductor. I have no computer 
> background in computer sciences and only basic undergraduate training 
> level in statistics.
> 
> I have used R with great pleasure and great pains. The most difficult 
> thing is to know what functions to use - sometimes I know that one 
> function is most likely available, but there's really no easy way to 
> get it (yes, even going to the archives and reading the help files). 
> I feel that more examples in the help files would definitely be a 
> good way to fully understand the potencial of the functions. I know 
> how difficult this is to do and how much of a time sink it must be.

Yes, I' often have the same problem when it comes to programming in R (data manipulation, formatting etc ...). When thinking about a solution, I often come up with something slow and complicated. A positng to this list usually reveals a very simple solution thanks to a function that I didn't find when exploring help, help.search and the archives (and thanks to those who give me the hint ;-). However, I don't know how to improve this, i.e. how to implement a more sophisticated help.search. Maybe the keywords in the help files or some kind of free text mining would help - well, maybe this is a bit over the top.

On the other hand, when it comes to the statistics (I'm a not a statistician) and it's minimal formatting of data etc , I think that developing an understanding of the stats itself is the main probelm and a GUI doesn't help very much in for this. Once the basic understanding is there (which one needs anyway, even with a GUI), the rest is not too difficult. In addition I usually need to script the calculations for many different datasets, and again most GUIs are bad in repeating tasks systematically.

I've spent quite some time with learing R (and I haven't stoped yet ;-), but it's devinitely worth it. As a scientists I appreciate it, and since it is a tool that use often, I would not exchange the command-line for any GUI.

This list and the many books and manuals (mentioned in the other postings here) do a pretty good job in teaching R!

	kind regards,

	Arne

[...]



From B.Rowlingson at lancaster.ac.uk  Thu Nov 18 18:58:21 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Thu, 18 Nov 2004 17:58:21 +0000
Subject: [R] hashing using named lists
In-Reply-To: <06E6EB1E-397F-11D9-AD4A-000A95764B04@bu.edu>
References: <06E6EB1E-397F-11D9-AD4A-000A95764B04@bu.edu>
Message-ID: <419CE2BD.5000504@lancaster.ac.uk>

ulas karaoz wrote:

> is there a way around this?

  yes...

> Why by default all.equal.list doesnt require an exact match?

  because we're lazy? :)

> How can I do hashing in R?

  you can explicitly test the names for equality, eg with this 2-element 
list:

  > x
  $name
  [1] 1 2 3

  $n
  [1] 3 2 1

  You can do:

  > x[names(x)=='name']
  $name
  [1] 1 2 3

  > x[names(x)=='na']
  list()

  > x[names(x)=='n']
  $n
  [1] 3 2 1

  Of course, the right way would be to create a new class, 'hash' 
perhaps, that did all this in its '$' or '[' methods.

Baz



From maechler at stat.math.ethz.ch  Thu Nov 18 19:00:44 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 18 Nov 2004 19:00:44 +0100
Subject: [R] R/S-related projects on Sourceforge? Trove Categorization -
	GDAL
In-Reply-To: <20041117182516.GA12201@sonny.eddelbuettel.com>
References: <OF898A5EDA.9DC14E71-ON85256F4F.004E6C8F-85256F4F.00557F01@LocalDomain>
	<OFA43FBCFA.8058A07A-ON85256F4F.005C03CD-85256F4F.005C8FF8@ci.orlando.fl.us>
	<20041117182516.GA12201@sonny.eddelbuettel.com>
Message-ID: <16796.58188.910803.778772@gargle.gargle.HOWL>

>>>>> "Dirk" == Dirk Eddelbuettel <edd at debian.org>
>>>>>     on Wed, 17 Nov 2004 12:25:16 -0600 writes:

    Dirk> On Wed, Nov 17, 2004 at 11:52:57AM -0500,
    Dirk> James.Callahan at CityofOrlando.net wrote:
    >> Gretl, RPad and RMetrics, plus Ernesto's FLR and fsap
    >> make five.

    Dirk> Isn't RMetrics at rmetrics.org at the ETH in Zuerich, CH?

that's definitely correct.
There might have been "oral" plans to change the development
process, but I'm not even sure about that.

Martin Maechler (at ETH, but not "connected" to Rmetrics in any ways).



From tplate at blackmesacapital.com  Thu Nov 18 19:15:16 2004
From: tplate at blackmesacapital.com (Tony Plate)
Date: Thu, 18 Nov 2004 11:15:16 -0700
Subject: [R] hashing using named lists
In-Reply-To: <06E6EB1E-397F-11D9-AD4A-000A95764B04@bu.edu>
References: <06E6EB1E-397F-11D9-AD4A-000A95764B04@bu.edu>
Message-ID: <6.1.0.6.2.20041118110117.05f0d2c8@mailhost.blackmesacapital.com>

Use match() for exact matching,

i.e.,

 > test[[match("name", names(test))]]

Yes, it is more cumbersome.  This partial matching is considered by some to 
be a design fault, but changing it would break too many programs that 
depend upon it.

I don't understand your question about all.equal.list() -- it does seem to 
require exact matches on names, e.g.:

 > all.equal(list(a=1:3), list(aa=1:3))
[1] "Names: 1 string mismatches"
 > all.equal(list(aa=1:3), list(a=1:3))
[1] "Names: 1 string mismatches"
 >

(the above run in R 2.0.0)

-- Tony Plate

(BTW, in R this operation is generally called "indexing" or "subscripting" 
or "extraction", but not "hashing".  "Hashing" is a specific technique for 
managing and looking up indices, which is why some other programming 
languages refer to list-like objects that are indexed by character strings 
as "hashes".  I don't think hashing is used for list names in R, but 
someone please correct me if I'm wrong! )

At Thursday 09:29 AM 11/18/2004, ulas karaoz wrote:
>hi all,
>I am trying to use named list to hash a bunch of vector by name, for instance:
>test = list()
>test$name = c(1,2,3)
>
>the problem is that when i try to get the values back by using the name, 
>the matching isn't done in an exact way, so
>test$na is not NULL.
>
>is there a way around this?
>Why by default all.equal.list doesnt require an exact match?
>How can I do hashing in R?
>
>thanks.
>ulas.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From reid_huntsinger at merck.com  Thu Nov 18 19:14:54 2004
From: reid_huntsinger at merck.com (Huntsinger, Reid)
Date: Thu, 18 Nov 2004 13:14:54 -0500
Subject: [R] hashing using named lists
Message-ID: <D9A95B4B7B20354992E165EEADA31999056A91D8@uswpmx00.merck.com>

It seems that that behavior is hard-coded in the subscript code, 
but I bet you could fix it easily by changing the call to get1index

 offset = get1index(CAR(subs), getAttrib(x, R_NamesSymbol),
                           length(x), /*partial ok*/TRUE, i);

in src/main/subset.c (line 762 I think, R-2.0.0) to supply FALSE in place of
TRUE and recompiling... I haven't tried yet though so maybe I'm quite badly
wrong.

Reid Huntsinger 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of ulas karaoz
Sent: Thursday, November 18, 2004 11:30 AM
To: r-help at stat.math.ethz.ch
Subject: [R] hashing using named lists


hi all,
I am trying to use named list to hash a bunch of vector by name, for 
instance:
test = list()
test$name = c(1,2,3)

the problem is that when i try to get the values back by using the 
name, the matching isn't done in an exact way, so
test$na is not NULL.

is there a way around this?
Why by default all.equal.list doesnt require an exact match?
How can I do hashing in R?

thanks.
ulas.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From f.harrell at vanderbilt.edu  Thu Nov 18 19:18:22 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 18 Nov 2004 13:18:22 -0500
Subject: [R] The hidden costs of GPL software?
In-Reply-To: <6.1.2.0.2.20041118094347.01ef21d0@hermes.nos.noaa.gov>
References: <200411170955.iAH9toV21278008@hedwig1.umh.ac.be>	<419B2CB4.7010708@srres.com>
	<419B5FE5.8040009@pburns.seanet.com>	<630CA0C2-394C-11D9-A2B1-000A95B2B140@sanger.ac.uk>	<419C8F0D.1040601@swissinfo.org>
	<6.1.2.0.2.20041118094347.01ef21d0@hermes.nos.noaa.gov>
Message-ID: <419CE76E.4070601@vanderbilt.edu>

Mike Prager wrote:
> At 11/18/2004 07:01 AM Thursday, Thomas Sch??nhoff wrote:
> 
>> To sum up, what I am in need to is an extensive example based 
>> help-system, focused on how to do things in R. In parts this is 
>> already there, i.e. SimpleR from Verzani (contributed docs area) etc.
>>
>> Hopefully I can  contribute to this in future, since it is seems to me 
>> invaluable to learn R by going through example-based lessons (some are 
>> found in vignette() ).
>> These are much more comprehensible to me than those short reference 
>> like entries in the current help-system, mostly due to their very 
>> technical approach (same is to be said about the official GNU R 
>> manuals, especially "The R Language", which wasn't a great help for me 
>> when I took my first look at GNU R). In this context something like 
>> the GuideMaps of Vista come to my mind!
>>
>> But to be as clear as possible, I think GNU R is great and I 
>> appreciate all the efforts done by the R core team and associates!
>>
>> Nevertheless it seems to be valuable to re-think the help-system in R 
>> with respect to those who may have a good understanding in statistics, 
>> but lacking some basic experiences in how to introduce themselves to 
>> sophisticated world of R/S languages.
> 
> 
> (I posted similar material before, but it was moved to R-devel, and I 
> wanted to express a bit of it here.)
> 
> I have frequently felt, like Thomas, that what could make R easier to 
> use is not a GUI, but a help system more focused on tasks and examples, 
> rather than on functions and packages.  This has obvious and large costs 
> of development, and I am unlikely to contribute much myself, for reasons 
> of time and ability.  Yet, I mention it for the sake of this discussion.
> 
> Such a help system could be a tree (or key) structure in which through 
> making choices, the user's description of the desired task is gradually 
> narrowed.  At the end of each twig of the tree would be a list of 
> suggested functions for solving the problem, hyperlinked into the 
> existing help system (which in many ways is outstanding and has evolved 
> just as fast as R itself).  This could be coupled with the continued 
> expansion of the number of examples in the help system.
> 
> Now I must express appreciation for what exists already that helps in 
> this regard:  MASS (in its many editions), Introductory Statistics with 
> R, Simple R, and the other free documentation that so many authors have 
> generously provided.  Not to mention the superlative contribution of R 
> itself, and the work of the R development team.  It is beyond my 
> understanding how something so valuable and well thought out has been 
> created by people with so many other responsibilities.
> 
> Mike

...

I second all of that.  What you are describing Mike could be done with 
a community-maintained wiki, with easy to add hyperlinks to other sites. 
  Just think what a great value it would be to the statistical community 
to have an ever-growing set of examples with all code and output, taking 
a cue from the BUGS examples guides.  The content could be broken down 
by major areas (data import examples, data manipulation examples, many 
analysis topics, many graphics topics, etc.).  Ultimately the more 
elaborate case studies could be peer-reviewied (a la the Journal of 
Statistical Software) and updated.

Frank

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From mmiller3 at iupui.edu  Thu Nov 18 19:39:05 2004
From: mmiller3 at iupui.edu (Michael A. Miller)
Date: Thu, 18 Nov 2004 13:39:05 -0500
Subject: [R] Re: The hidden costs of R?
In-Reply-To: <419C8F0D.1040601@swissinfo.org> (Thomas
	=?iso-8859-1?q?Sch=F6nhoff's?= message of "Thu,
	18 Nov 2004 13:01:17 +0100")
References: <200411170955.iAH9toV21278008@hedwig1.umh.ac.be>
	<419B2CB4.7010708@srres.com> <419B5FE5.8040009@pburns.seanet.com>
	<630CA0C2-394C-11D9-A2B1-000A95B2B140@sanger.ac.uk>
	<419C8F0D.1040601@swissinfo.org>
Message-ID: <878y8zato6.fsf_-_@lumen.indyrad.iupui.edu>

>>>>> "Thomas" == Thomas Sch??nhoff <tom_woody at swissinfo.org> writes:

    > To sum up, what I am in need to is an extensive example
    > based help-system, focused on how to do things in R. In
    > parts this is already there, i.e. SimpleR from Verzani
    > (contributed docs area) etc.

I have a nice set of extensive help with documentation sitting on
my shelf:

   - Peter Dalgaard. Introductory Statistics with R. Springer,
     2002. ISBN 0-387-9 

   - William N. Venables and Brian D. Ripley. Modern Applied
     Statistics with S. Fourth Edition. Springer, 2002. ISBN
     0-387-95457-0.  

   - Jose C. Pinheiro and Douglas M. Bates. Mixed-Effects Models
     in S and S-Plus. Springer, 2000. ISBN 0-387-98957-0.  

I suspect that I would have spent the money on these books even
if I'd started by spending money for S-plus, instead of R.  But
I've never seen the S-plus help system, so I may be wrong.

See http://www.r-project.org/doc/bib/R-publications.html and
http://www.r-project.org/doc/bib/R_bib.html for yet more.

Mike

-- 
Michael A. Miller                               mmiller3 at iupui.edu
  Imaging Sciences, Department of Radiology, IU School of Medicine



From matteo.ruggiero at phd.unibocconi.it  Thu Nov 18 19:41:31 2004
From: matteo.ruggiero at phd.unibocconi.it (matteo ruggiero)
Date: Thu, 18 Nov 2004 19:41:31 +0100
Subject: [R] gibbs sampling for mixture of normals
Message-ID: <1100803291.419cecdb7ef81@webmail.phd.unibocconi.it>

hi

i'm looking for a gibbs sampling algorithm for R for the case of mixture of K
normals, and in particular for the case of bivariate normals.
i'd be grateful if anyone could send its own R-routine, at least for the
univariate case.

thank you in advance
matteo



From abu3ammar at gmail.com  Thu Nov 18 19:53:20 2004
From: abu3ammar at gmail.com (Yasser El-Zein)
Date: Thu, 18 Nov 2004 13:53:20 -0500
Subject: [R] SJava
Message-ID: <b1d3150404111810536149badc@mail.gmail.com>

I failed to build SJava dure to teh error below. Any ideas?

# R CMD INSTALL -c SJava_0.68-0.tar.gz 
* Installing *source* package 'SJava' ...
checking for java... /opt/j2sdk1.4.2_06//bin/java
Java VM /opt/j2sdk1.4.2_06//bin/java
checking for javah... /opt/j2sdk1.4.2_06//bin/javah
Looking in /opt/j2sdk1.4.2_06/include
Looking in /opt/j2sdk1.4.2_06/include/linux
checking for g++... no
checking for c++... no
checking for gpp... gpp
checking for C++ compiler default output... b.out
checking whether the C++ compiler works... configure: error: cannot
run C++ compiled programs.
If you meant to cross compile, use `--host'.
See `config.log' for more details.
ERROR: configuration failed for package 'SJava'



From drf5n at maplepark.com  Thu Nov 18 20:03:10 2004
From: drf5n at maplepark.com (David Forrest)
Date: Thu, 18 Nov 2004 13:03:10 -0600 (CST)
Subject: [R] The hidden costs of GPL software?
In-Reply-To: <419CE76E.4070601@vanderbilt.edu>
References: <200411170955.iAH9toV21278008@hedwig1.umh.ac.be>
	<419B2CB4.7010708@srres.com> <419B5FE5.8040009@pburns.seanet.com>
	<630CA0C2-394C-11D9-A2B1-000A95B2B140@sanger.ac.uk>
	<419C8F0D.1040601@swissinfo.org>
	<6.1.2.0.2.20041118094347.01ef21d0@hermes.nos.noaa.gov>
	<419CE76E.4070601@vanderbilt.edu>
Message-ID: <Pine.LNX.4.58.0411181258160.14150@maplepark.com>

On Thu, 18 Nov 2004, Frank E Harrell Jr wrote:
...
> ...
> I second all of that.  What you are describing Mike could be done with
> a community-maintained wiki, with easy to add hyperlinks to other sites.

There is a wiki at http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl but it
doesn't seem to get much use.

Last time I was hunting for help on R, I made the page
http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl?SearchFunctions
 and in particular:

help.search.archive<-function(string){
   RURL="http://www.google.com/u/newcastlemaths"
   RSearchURL=paste(RURL,"?q=",string,sep='')
   browseURL(RSearchURL)
   return(invisible(0))
 }

help.search.archive('wiki') # example

Dave
-- 
 Dave Forrest
 drf at vims.edu                                    (804)684-7900w
 drf5n at maplepark.com                             (804)642-0662h
                                   http://maplepark.com/~drf5n/



From ripley at stats.ox.ac.uk  Thu Nov 18 20:07:10 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 18 Nov 2004 19:07:10 +0000 (GMT)
Subject: [R] hashing using named lists
In-Reply-To: <D9A95B4B7B20354992E165EEADA31999056A91D8@uswpmx00.merck.com>
References: <D9A95B4B7B20354992E165EEADA31999056A91D8@uswpmx00.merck.com>
Message-ID: <Pine.LNX.4.61.0411181859260.26866@gannet.stats>

On Thu, 18 Nov 2004, Huntsinger, Reid wrote:

> It seems that that behavior is hard-coded in the subscript code,

the behaviour being partial matching?

> but I bet you could fix it easily by changing the call to get1index
>
> offset = get1index(CAR(subs), getAttrib(x, R_NamesSymbol),
>                           length(x), /*partial ok*/TRUE, i);
>
> in src/main/subset.c (line 762 I think, R-2.0.0) to supply FALSE in place of
> TRUE and recompiling... I haven't tried yet though so maybe I'm quite badly
> wrong.

That is the [[ ]] code.  The $ code is at about line 968.

And if you did that a lot of R code may break, so please don't even think 
about it.  Partial matching is a long-standing feature.

There have been proposals from time to time for [ ] and [[ ]] to have an 
exact=TRUE argument for character indices.  That seems a good idea, except 
that they are generic and there are now many methods out there which would 
ignore the argument.


> Reid Huntsinger
>
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of ulas karaoz
> Sent: Thursday, November 18, 2004 11:30 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] hashing using named lists
>
>
> hi all,
> I am trying to use named list to hash a bunch of vector by name, for
> instance:
> test = list()
> test$name = c(1,2,3)
>
> the problem is that when i try to get the values back by using the
> name, the matching isn't done in an exact way, so
> test$na is not NULL.
>
> is there a way around this?
> Why by default all.equal.list doesnt require an exact match?
> How can I do hashing in R?
>
> thanks.
> ulas.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Nov 18 20:27:53 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 18 Nov 2004 19:27:53 +0000 (GMT)
Subject: [R] SJava
In-Reply-To: <b1d3150404111810536149badc@mail.gmail.com>
References: <b1d3150404111810536149badc@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0411181925180.4014@gannet.stats>

Seem pretty straightforward to me: either gpp is not a C++ compiler or it 
does not work (possibly because LD_LIBRARY_PATH does not contain its 
run-time libraries).

On Thu, 18 Nov 2004, Yasser El-Zein wrote:

> I failed to build SJava dure to teh error below. Any ideas?
>
> # R CMD INSTALL -c SJava_0.68-0.tar.gz
> * Installing *source* package 'SJava' ...
> checking for java... /opt/j2sdk1.4.2_06//bin/java
> Java VM /opt/j2sdk1.4.2_06//bin/java
> checking for javah... /opt/j2sdk1.4.2_06//bin/javah
> Looking in /opt/j2sdk1.4.2_06/include
> Looking in /opt/j2sdk1.4.2_06/include/linux
> checking for g++... no
> checking for c++... no
> checking for gpp... gpp
> checking for C++ compiler default output... b.out
> checking whether the C++ compiler works... configure: error: cannot
> run C++ compiled programs.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From abu3ammar at gmail.com  Thu Nov 18 20:40:00 2004
From: abu3ammar at gmail.com (Yasser El-Zein)
Date: Thu, 18 Nov 2004 14:40:00 -0500
Subject: [R] SJava
In-Reply-To: <Pine.LNX.4.61.0411181925180.4014@gannet.stats>
References: <b1d3150404111810536149badc@mail.gmail.com>
	<Pine.LNX.4.61.0411181925180.4014@gannet.stats>
Message-ID: <b1d31504041118114062e6d04c@mail.gmail.com>

Thank you Prof Ripley for your guidance. I re-installed gpp and SJava
installed correctly.
I am now running into this problem: When I load the SJava library I get:

> library(SJava)
Error in dyn.load(x, as.logical(local), as.logical(now)) : 
        unable to load shared library
"/usr/local/lib/R/site-library/SJava/libs/SJava.so":
  libRSNativeJava.so: cannot open shared object file: No such file or directory
Error in library(SJava) : .First.lib failed for 'SJava'

The file does exit:
$ ls -la /usr/local/lib/R/site-library/SJava/libs/SJava.so
-rwxr-xr-x  1 root staff 283388 Nov 18 13:27
/usr/local/lib/R/site-library/SJava/libs/SJava.so

I then ran:
$ R CMD ldd /usr/local/lib/R/site-library/SJava/libs/SJava.so
                libRSNativeJava.so => not found
        libjvm.so => not found
        libR.so => /usr/lib/R/lib/libR.so (0x40014000)
        libc.so.6 => /lib/tls/libc.so.6 (0x401df000)
        libblas.so.3 => /usr/lib/atlas/libblas.so.3 (0x4031a000)
        libg2c.so.0 => /usr/lib/libg2c.so.0 (0x40663000)
        libm.so.6 => /lib/tls/libm.so.6 (0x40681000)
        libgcc_s.so.1 => /lib/libgcc_s.so.1 (0x406a5000)
        libpcre.so.3 => /usr/lib/libpcre.so.3 (0x406ae000)
        libbz2.so.1.0 => /usr/lib/libbz2.so.1.0 (0x406be000)
        libz.so.1 => /usr/lib/libz.so.1 (0x406ce000)
        libreadline.so.4 => /lib/libreadline.so.4 (0x406df000)
        libdl.so.2 => /lib/tls/libdl.so.2 (0x4070b000)
        libncurses.so.5 => /lib/libncurses.so.5 (0x4070f000)
        /lib/ld-linux.so.2 => /lib/ld-linux.so.2 (0x80000000)

libjvm.so does exist in 4 locations in my intel box running debian:
/opt/splus/java/jre/lib/i386/client/libjvm.so
/opt/splus/java/jre/lib/i386/server/libjvm.so
/opt/j2sdk1.4.2_06/jre/lib/i386/server/libjvm.so
/opt/j2sdk1.4.2_06/jre/lib/i386/client/libjvm.so

I have JAVA_HOME defined as:
$ echo $JAVA_HOME
/opt/j2sdk1.4.2_06/

Your further help is greatly appreciated.

On Thu, 18 Nov 2004 19:27:53 +0000 (GMT), Prof Brian Ripley
<ripley at stats.ox.ac.uk> wrote:
> Seem pretty straightforward to me: either gpp is not a C++ compiler or it
> does not work (possibly because LD_LIBRARY_PATH does not contain its
> run-time libraries).
> 
> 
> 
> On Thu, 18 Nov 2004, Yasser El-Zein wrote:
> 
> > I failed to build SJava dure to teh error below. Any ideas?
> >
> > # R CMD INSTALL -c SJava_0.68-0.tar.gz
> > * Installing *source* package 'SJava' ...
> > checking for java... /opt/j2sdk1.4.2_06//bin/java
> > Java VM /opt/j2sdk1.4.2_06//bin/java
> > checking for javah... /opt/j2sdk1.4.2_06//bin/javah
> > Looking in /opt/j2sdk1.4.2_06/include
> > Looking in /opt/j2sdk1.4.2_06/include/linux
> > checking for g++... no
> > checking for c++... no
> > checking for gpp... gpp
> > checking for C++ compiler default output... b.out
> > checking whether the C++ compiler works... configure: error: cannot
> > run C++ compiled programs.
> 
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From andy_liaw at merck.com  Thu Nov 18 20:50:35 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 18 Nov 2004 14:50:35 -0500
Subject: [R] The hidden costs of GPL software?
Message-ID: <3A822319EB35174CA3714066D590DCD50994E33B@usrymx25.merck.com>

> From: David Forrest
> 
> On Wed, 17 Nov 2004, Mike Prager wrote:
> 
> ...
> > Using CLI software, an infrequent user has trouble 
> remembering the known
> > functions needed and trouble finding new ones (especially 
> as that user gets
> > older).  What might help is an added help facility more 
> oriented towards
> > tasks, rather than structured around functions or packages.
> ...
> 
> Another good (non-GUI) tool for the CLI is keyword 
> completion.  R in ESS
> does this, giving you lists of possible functions, variables 
> and objects,
> or feedback if there isn't any.  R's CLI completes, but only with
> filenames in the current directory.

That works only if R was compiled with readline.  Thus it doesn't work that
way on Windows, for example.  Completion still works on Windows under ESS
though.

Andy


 
> Dave
> -- 
>  Dave Forrest
>  drf at vims.edu                                    (804)684-7900w
>  drf5n at maplepark.com                             (804)642-0662h
>                                    http://maplepark.com/~drf5n/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From kingroi at hotmail.com  Thu Nov 18 21:06:10 2004
From: kingroi at hotmail.com (paul king)
Date: Thu, 18 Nov 2004 20:06:10 +0000
Subject: [R] Advanced R programming course in North Carolina (RTP area) ?
Message-ID: <BAY16-F2410BC24216CEDAA97B67CB0C20@phx.gbl>

Anyone knows of an upcomming Advanced R programming course in NC?
Would like to take this class before my project ends in December.

Thanks, Paul



From ripley at stats.ox.ac.uk  Thu Nov 18 21:08:30 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 18 Nov 2004 20:08:30 +0000 (GMT)
Subject: [R] SJava
In-Reply-To: <b1d31504041118114062e6d04c@mail.gmail.com>
References: <b1d3150404111810536149badc@mail.gmail.com>
	<Pine.LNX.4.61.0411181925180.4014@gannet.stats>
	<b1d31504041118114062e6d04c@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0411182002140.4858@gannet.stats>

You need to ensure that libRSNativeJava.so is in your LD_LIBRARY_PATH: 
last time I looked SJava came with scripts to achieve this, and I suspect 
you need to read the documentation about them.

As a further note, these are programming questions about a non-R product, 
and  R-help is _really not_ the appropriate forum.  Do please consult the 
posting guide.

On Thu, 18 Nov 2004, Yasser El-Zein wrote:

> Thank you Prof Ripley for your guidance. I re-installed gpp and SJava
> installed correctly.
> I am now running into this problem: When I load the SJava library I get:
>
>> library(SJava)
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>        unable to load shared library
> "/usr/local/lib/R/site-library/SJava/libs/SJava.so":
>  libRSNativeJava.so: cannot open shared object file: No such file or directory
> Error in library(SJava) : .First.lib failed for 'SJava'
>
> The file does exit:
> $ ls -la /usr/local/lib/R/site-library/SJava/libs/SJava.so
> -rwxr-xr-x  1 root staff 283388 Nov 18 13:27
> /usr/local/lib/R/site-library/SJava/libs/SJava.so
>
> I then ran:
> $ R CMD ldd /usr/local/lib/R/site-library/SJava/libs/SJava.so
>                libRSNativeJava.so => not found
>        libjvm.so => not found
>        libR.so => /usr/lib/R/lib/libR.so (0x40014000)
>        libc.so.6 => /lib/tls/libc.so.6 (0x401df000)
>        libblas.so.3 => /usr/lib/atlas/libblas.so.3 (0x4031a000)
>        libg2c.so.0 => /usr/lib/libg2c.so.0 (0x40663000)
>        libm.so.6 => /lib/tls/libm.so.6 (0x40681000)
>        libgcc_s.so.1 => /lib/libgcc_s.so.1 (0x406a5000)
>        libpcre.so.3 => /usr/lib/libpcre.so.3 (0x406ae000)
>        libbz2.so.1.0 => /usr/lib/libbz2.so.1.0 (0x406be000)
>        libz.so.1 => /usr/lib/libz.so.1 (0x406ce000)
>        libreadline.so.4 => /lib/libreadline.so.4 (0x406df000)
>        libdl.so.2 => /lib/tls/libdl.so.2 (0x4070b000)
>        libncurses.so.5 => /lib/libncurses.so.5 (0x4070f000)
>        /lib/ld-linux.so.2 => /lib/ld-linux.so.2 (0x80000000)
>
> libjvm.so does exist in 4 locations in my intel box running debian:
> /opt/splus/java/jre/lib/i386/client/libjvm.so
> /opt/splus/java/jre/lib/i386/server/libjvm.so
> /opt/j2sdk1.4.2_06/jre/lib/i386/server/libjvm.so
> /opt/j2sdk1.4.2_06/jre/lib/i386/client/libjvm.so
>
> I have JAVA_HOME defined as:
> $ echo $JAVA_HOME
> /opt/j2sdk1.4.2_06/
>
> Your further help is greatly appreciated.
>
> On Thu, 18 Nov 2004 19:27:53 +0000 (GMT), Prof Brian Ripley
> <ripley at stats.ox.ac.uk> wrote:
>> Seem pretty straightforward to me: either gpp is not a C++ compiler or it
>> does not work (possibly because LD_LIBRARY_PATH does not contain its
>> run-time libraries).
>>
>>
>>
>> On Thu, 18 Nov 2004, Yasser El-Zein wrote:
>>
>>> I failed to build SJava dure to teh error below. Any ideas?
>>>
>>> # R CMD INSTALL -c SJava_0.68-0.tar.gz
>>> * Installing *source* package 'SJava' ...
>>> checking for java... /opt/j2sdk1.4.2_06//bin/java
>>> Java VM /opt/j2sdk1.4.2_06//bin/java
>>> checking for javah... /opt/j2sdk1.4.2_06//bin/javah
>>> Looking in /opt/j2sdk1.4.2_06/include
>>> Looking in /opt/j2sdk1.4.2_06/include/linux
>>> checking for g++... no
>>> checking for c++... no
>>> checking for gpp... gpp
>>> checking for C++ compiler default output... b.out
>>> checking whether the C++ compiler works... configure: error: cannot
>>> run C++ compiled programs.
>>
>> --
>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>> University of Oxford,             Tel:  +44 1865 272861 (self)
>> 1 South Parks Road,                     +44 1865 272866 (PA)
>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From lauraholt_983 at hotmail.com  Thu Nov 18 21:10:05 2004
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Thu, 18 Nov 2004 14:10:05 -0600
Subject: [R] Off Topic: GSL installation for Windows
Message-ID: <BAY12-F9E4A6F7777BE4D37F1540D6C20@phx.gbl>

Hi R People!

Has anyone installed the Gnu Scientific Library on a Windows system, please?

I'm having a dreadful time with that.

Any advance would be much appreciated.

Sincerely,
Laura Holt
mailto: lauraholt_983 at hotmail.com



From THOMAS.VOLSCHO at huskymail.uconn.edu  Thu Nov 18 21:11:09 2004
From: THOMAS.VOLSCHO at huskymail.uconn.edu (Thomas W Volscho)
Date: Thu, 18 Nov 2004 15:11:09 -0500
Subject: [R] Enormous Datasets
Message-ID: <1aab4cb1aae597.1aae5971aab4cb@huskymail.uconn.edu>

Dear List,
I have some projects where I use enormous datasets.  For instance, the 5% PUMS microdata from the Census Bureau.  After deleting cases I may have a dataset with 7 million+ rows and 50+ columns.  Will R handle a datafile of this size?  If so, how?

Thank you in advance,
Tom Volscho

************************************        
Thomas W. Volscho
Graduate Student
Dept. of Sociology U-2068
University of Connecticut
Storrs, CT 06269
Phone: (860) 486-3882
http://vm.uconn.edu/~twv00001



From stats at psyctc.org  Thu Nov 18 21:21:04 2004
From: stats at psyctc.org (Chris Evans)
Date: Thu, 18 Nov 2004 20:21:04 +0000
Subject: [R] Where has the Debian respository gone?
In-Reply-To: <20041118151840.GA22159@sonny.eddelbuettel.com>
References: <cn8k7h$lin$1@sea.gmane.org>
	<20041115013527.GA17007@sonny.eddelbuettel.com>
	<793505964.20041118085238@psyctc.org>
	<20041118151840.GA22159@sonny.eddelbuettel.com>
Message-ID: <1796582187.20041118202104@psyctc.org>

Hello Dirk,

Thursday, November 18, 2004, 3:18:40 PM, you wrote:

DE> On Thu, Nov 18, 2004 at 08:52:38AM +0000, stats wrote:
>> I'm a bit puzzled.  I had
>>         deb http://cran.r-project.org/bin/linux/debian woody main
>> in /etc/apt/sources.list and had hoped, perhaps rather unwisely, that
>> this would look after the transition from 1.8.0 on my internet server (Debian
>> stable) where it serves up some cgi-bin work.  (Most of my R work is
>> on a Win2k machine, much though I'd like to go Debian all the way,
>> that isn't possible for my main job in near future.)
>> 
>> Is there an easy way of upgrading R on a Debian stable machine?  I
>> don't want to move off stable as the security side of that server is
>> too important.  I also don't really want to compile it myself if I can
>> avoid that, the server is pretty old iron and that might back up all
>> the Email stuff it does.
>> 
>> Advice anyone?

DE> More than advice, we need a volunteer to "backport" the current R package(s)
DE> for Debian to the Debian stable distribution. As I said, testing and
DE> unstable are taken care of (and yes, testing is still lagging because of the
DE> now much more formal interdependence of packages; R 2.0.* will appears once
DE> all dependent packages are available on all architectures)

I'm sure this is in itself proof that I'm not the person to do it but
can you say a bit more about what's involved Dirk?  I run a pretty low
powered Debian stable server on i386 hardware (an athlon if I remember
rightly) with pretty much the standard packages, GCC, perl etc. and I'm
not completely stupid.  However, debugging compiler and make complaints
is really not my area of competence and I do wonder about the likely load
on the machine and on my time.

In the not too distant future this machine should be replaced with a
much more powerful one and a somewhat more powerful backup machine so
hardware may not be a long term problem.

Any chance I can be useful?  Could I team up with someone who really
knows what s/he is doing but doesn't use Debian stable and work this
together?

Let me know, I'd love to put something very direct back into the R project.

Chris



From fhduan at gmail.com  Thu Nov 18 21:34:29 2004
From: fhduan at gmail.com (Frank Duan)
Date: Thu, 18 Nov 2004 15:34:29 -0500
Subject: [R] How to calculate the stratified means in a data frame?
Message-ID: <3b91723104111812343d837fd1@mail.gmail.com>

Dear R people,

I have a simple question to ask. Suppose I have a data.frame with two
variables: one factor (x) and one numeric (y), I want to calculate the
mean of y for each value of x. Although it's easy to do it within a
for a loop, I believe there may be a concise way by using some kinds
of "apply" functions. Could anyone tell me how to do that? Thank you.

Frank



From K.E.Vorloou at durham.ac.uk  Thu Nov 18 21:45:24 2004
From: K.E.Vorloou at durham.ac.uk (Costas Vorlow)
Date: Thu, 18 Nov 2004 20:45:24 +0000
Subject: [R] Time series plot orientation
Message-ID: <419D09E4.7030807@durham.ac.uk>

Hello,

I am trying to rotate by 90 degrees a time series plot. So I need the 
time axis to be the vertical one. Is there an easy way?

I couldn't guess anything from the help pages.

Apologies for a silly question.

Regards,
Costas

-- 
=================================================================
This e-mail contains information intended for the addressee only.
It may be confidential and may be the subject of legal and/or 
professional Privilege. Any dissemination, distribution, copyright 
or use of this communication without prior permission of the 
addressee is strictly prohibited.
-----------------------------------------------------------------
Dr. Costas Vorlow		| Tel: +44 (0)191 33 45727
Durham Business School	 	| Fax: +44 (0)191 33 45201
Room(324), University of Durham | email: K.E.Vorloou(at)durham.ac.uk
Mill Hill Lane, 		| or : costas(at)vorlow.org
Durham DH1 3LB, UK. 		| http://www.vorlow.org
-----------------------------------------------------------------
Fingerprint: B010 577A 9EC3 9185 08AE 8F22 1A48 B4E7 9FA6 C31A



From ripley at stats.ox.ac.uk  Thu Nov 18 21:56:20 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 18 Nov 2004 20:56:20 +0000 (GMT)
Subject: [R] Off Topic: GSL installation for Windows
In-Reply-To: <BAY12-F9E4A6F7777BE4D37F1540D6C20@phx.gbl>
References: <BAY12-F9E4A6F7777BE4D37F1540D6C20@phx.gbl>
Message-ID: <Pine.LNX.4.61.0411182052320.6842@gannet.stats>

See the ReadMe (which should be the first place to look) at

http://cran.r-project.org/bin/windows/contrib/2.0/@ReadMe

It's easy once you have gsl-1.5 built (the gnuwin32 project's version 
is too old), but getting gsl-1.5 built is not straightforward.

On Thu, 18 Nov 2004, Laura Holt wrote:

> Has anyone installed the Gnu Scientific Library on a Windows system, please?
>
> I'm having a dreadful time with that.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Nov 18 22:07:20 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 18 Nov 2004 21:07:20 +0000 (GMT)
Subject: [R] How to calculate the stratified means in a data frame?
In-Reply-To: <3b91723104111812343d837fd1@mail.gmail.com>
References: <3b91723104111812343d837fd1@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0411182103400.6842@gannet.stats>

On Thu, 18 Nov 2004, Frank Duan wrote:

> I have a simple question to ask. Suppose I have a data.frame with two
> variables: one factor (x) and one numeric (y), I want to calculate the
> mean of y for each value of x. Although it's easy to do it within a
> for a loop, I believe there may be a concise way by using some kinds
> of "apply" functions. Could anyone tell me how to do that? Thank you.

tapply(y, x, mean)  # which _is_ in `An Introduction to R', BTW

?by
?aggregate

for more sophisticated packaging of such ideas.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Thu Nov 18 22:07:23 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Nov 2004 22:07:23 +0100
Subject: [R] Enormous Datasets
In-Reply-To: <1aab4cb1aae597.1aae5971aab4cb@huskymail.uconn.edu>
References: <1aab4cb1aae597.1aae5971aab4cb@huskymail.uconn.edu>
Message-ID: <x2hdnmsw6s.fsf@biostat.ku.dk>

Thomas W Volscho <THOMAS.VOLSCHO at huskymail.uconn.edu> writes:

> Dear List, I have some projects where I use enormous datasets. For
> instance, the 5% PUMS microdata from the Census Bureau. After
> deleting cases I may have a dataset with 7 million+ rows and 50+
> columns. Will R handle a datafile of this size? If so, how?

With a big machine... If that is numeric, non-integer data, you are
looking at something like 

> 7e6*50*8
[1] 2.8e+09

i.e. roughly 3 GB of data for one copy of the data set. You easily
find yourself with multiple copies, so I suppose a machine with 16GB
RAM would cut it. These days that basically suggests x86_64
architecture running Linux (e.g. multiprocessor Opterons), but there
are also 64 bit Unix "big iron" solutions (Sun, IBM, HP,...).

If you can avoid dealing with the whole dataset at once, smaller
machines might get you there. Notice that 1 column is "only" 56MB, and
you may be able to work with aggregated data from some step onwards. 


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From vograno at evafunds.com  Thu Nov 18 22:13:12 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Thu, 18 Nov 2004 13:13:12 -0800
Subject: [R] Enormous Datasets
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A5751386@phost015.EVAFUNDS.intermedia.net>

Very unlikely R will be able to handle this. The problems are:

* the data set may simply not fit into the memory
* it will take forever to read from the ASCII file
* any meaningful analysis of a dataset in R typically require 5 - 10
times more memory than the size of the dataset (unless you are a real
insider and know all the knobs)


Your best strategy is probably to partition the file in meaningful
sub-categories and work with them. To save time on conversion from ASCII
you can read the sub-files into a data frame and then save the data
frame in .rda file using save(). Subsequent loading .rda files is much
faster than reading ASCII

Another strategy which is often advocated on the list is to put the data
into a data base and draw random samples of manageable size from the
database. I have no experience with this approach

HTH,
Vadim

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Thomas 
> W Volscho
> Sent: Thursday, November 18, 2004 12:11 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Enormous Datasets
> 
> Dear List,
> I have some projects where I use enormous datasets.  For 
> instance, the 5% PUMS microdata from the Census Bureau.  
> After deleting cases I may have a dataset with 7 million+ 
> rows and 50+ columns.  Will R handle a datafile of this size? 
>  If so, how?
> 
> Thank you in advance,
> Tom Volscho
> 
> ************************************        
> Thomas W. Volscho
> Graduate Student
> Dept. of Sociology U-2068
> University of Connecticut
> Storrs, CT 06269
> Phone: (860) 486-3882
> http://vm.uconn.edu/~twv00001
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From p.dalgaard at biostat.ku.dk  Thu Nov 18 22:11:49 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Nov 2004 22:11:49 +0100
Subject: [R] How to calculate the stratified means in a data frame?
In-Reply-To: <3b91723104111812343d837fd1@mail.gmail.com>
References: <3b91723104111812343d837fd1@mail.gmail.com>
Message-ID: <x2d5yasvze.fsf@biostat.ku.dk>

Frank Duan <fhduan at gmail.com> writes:

> Dear R people,
> 
> I have a simple question to ask. Suppose I have a data.frame with two
> variables: one factor (x) and one numeric (y), I want to calculate the
> mean of y for each value of x. Although it's easy to do it within a
> for a loop, I believe there may be a concise way by using some kinds
> of "apply" functions. Could anyone tell me how to do that? Thank you.

tapply() will do that. (help(tapply), look at the "presidents" example).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From fhduan at gmail.com  Thu Nov 18 22:07:43 2004
From: fhduan at gmail.com (Frank Duan)
Date: Thu, 18 Nov 2004 16:07:43 -0500
Subject: [R] How to calculate the stratified means in a data frame?
In-Reply-To: <419D0FDD.3090606@noaa.gov>
References: <3b91723104111812343d837fd1@mail.gmail.com>
	<419D0FDD.3090606@noaa.gov>
Message-ID: <3b917231041118130746c17f1f@mail.gmail.com>

Got it. Many thanks.


On Thu, 18 Nov 2004 13:10:53 -0800, Jeff Laake <jeff.laake at noaa.gov> wrote:
> look at "by"
> 
> 
> 
> Frank Duan wrote:
> 
> > Dear R people,
> >
> > I have a simple question to ask. Suppose I have a data.frame with two
> > variables: one factor (x) and one numeric (y), I want to calculate the
> > mean of y for each value of x. Although it's easy to do it within a
> > for a loop, I believe there may be a concise way by using some kinds
> > of "apply" functions. Could anyone tell me how to do that? Thank you.
> >
> > Frank
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From MSchwartz at MedAnalytics.com  Thu Nov 18 22:14:53 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 18 Nov 2004 15:14:53 -0600
Subject: [R] How to calculate the stratified means in a data frame?
In-Reply-To: <3b91723104111812343d837fd1@mail.gmail.com>
References: <3b91723104111812343d837fd1@mail.gmail.com>
Message-ID: <1100812493.837.56.camel@horizons.localdomain>

On Thu, 2004-11-18 at 15:34 -0500, Frank Duan wrote:
> Dear R people,
> 
> I have a simple question to ask. Suppose I have a data.frame with two
> variables: one factor (x) and one numeric (y), I want to calculate the
> mean of y for each value of x. Although it's easy to do it within a
> for a loop, I believe there may be a concise way by using some kinds
> of "apply" functions. Could anyone tell me how to do that? Thank you.
> 
> Frank


One way is to use by(). Using the 'iris' dataset to get the means for
Sepal.Length by Species:

> with(iris, by(Sepal.Length, Species, mean))
INDICES: setosa
[1] 5.006
------------------------------------------------------ 
INDICES: versicolor
[1] 5.936
------------------------------------------------------ 
INDICES: virginica
[1] 6.588

See ?by, also ?tapply and ?aggregate.

Note also the use of with() as a wrapper, in lieu of attach() here.

HTH,

Marc Schwartz



From andy_liaw at merck.com  Thu Nov 18 22:24:36 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 18 Nov 2004 16:24:36 -0500
Subject: [R] How to calculate the stratified means in a data frame?
Message-ID: <3A822319EB35174CA3714066D590DCD50994E33E@usrymx25.merck.com>

See ?tapply or ?by.

Andy

> From: Frank Duan
> 
> Dear R people,
> 
> I have a simple question to ask. Suppose I have a data.frame with two
> variables: one factor (x) and one numeric (y), I want to calculate the
> mean of y for each value of x. Although it's easy to do it within a
> for a loop, I believe there may be a concise way by using some kinds
> of "apply" functions. Could anyone tell me how to do that? Thank you.
> 
> Frank
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From friendly at yorku.ca  Thu Nov 18 22:29:16 2004
From: friendly at yorku.ca (Michael Friendly)
Date: Thu, 18 Nov 2004 16:29:16 -0500
Subject: [R] adjusting the map of France to 1830
Message-ID: <419D142C.1080003@yorku.ca>

I'm doing some analyses of historical data from France in 1830 on 'moral 
statistics' that I'd like to
show on a map.  I've done most of my analyses in SAS, but a few things 
would work better in R.
To do this, I have to adjust the modern map,

library(maps)
map('france')

to adjust for changes in departments (86 in 1830, to 97 now).  I've read 
the documentation
for the maps and maptools package, but there seems to be no functions to 
allow this, and
I can't find information on the exact structure of map datasets, but I 
understand them to
be delimited lists of polygon coordinates.

In SAS, all maps have (one or more) ID variables representing the 
geographical region,
and there is also a proc gremove that can remove internal boundaries 
inside the polygons
for regions with the same ID.  Is there some way I can do this in R?

Here's what I did in SAS:

*-- Fix the map of France to conform to Guerry:
    - adjust the 97 current departments to correspond to the 86 in 1830
    - delete those not part of France then
;

data gfrtemp;
    set maps.france;
    /* Corse was one dept - merge these to one area, new ID */
    if id in (201, 202)    then dept=200;

    /* Seine et Oise (78) was cut into
    Essonne (91), Val d'Oise (95) and Yvelines (78) */
    else if id in (91, 95)    then dept=78;

    /* Seine (75) now split into
    Hauts-de-Seine (92), Seine-Saint-Denis (93) et Val-de-Marne (94)*/
    else if id in (92, 93, 94)    then dept=75;
   
     /* departments not part of France in 1830 */
    else if id in (
        6,     /* Alpes-Maritimes */
        73,74, /* Savoie, Haute-Savoie */
        90)    /* Territore-de-Belfort */
        then delete;
    else                       dept=id;
    run;
   
*-- remove internal boundaries based on merged DEPT;
proc sort data=gfrtemp;
    by dept;

proc gremove data=gfrtemp out=gfrance;
   by dept;
   id id;
   run;



-- 
Michael Friendly     Email: friendly at yorku.ca 
Professor, Psychology Dept.
York University      Voice: 416 736-5115 x66249 Fax: 416 736-5814
4700 Keele Street    http://www.math.yorku.ca/SCS/friendly.html
Toronto, ONT  M3J 1P3 CANADA



From andy_liaw at merck.com  Thu Nov 18 22:33:14 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 18 Nov 2004 16:33:14 -0500
Subject: [R] Enormous Datasets
Message-ID: <3A822319EB35174CA3714066D590DCD50994E33F@usrymx25.merck.com>

It depends on what you want to do with that data in R.  If you want to play
with the whole data, just storing it in R will require more than 2.6GB of
memory (assuming all data are numeric and are stored as doubles):

> 7e6 * 50 * 8 / 1024^2
[1] 2670.288

That's not impossible, but you'll need to be on a computer with quite a bit
more memory than that, and running on an OS that supports it.  If that's not
feasible, you need to re-think what you want to do with that data in R
(e.g., read in and process a small chunk at a time, or read in a random
sample, etc.).

Andy


> From: Thomas W Volscho
> 
> Dear List,
> I have some projects where I use enormous datasets.  For 
> instance, the 5% PUMS microdata from the Census Bureau.  
> After deleting cases I may have a dataset with 7 million+ 
> rows and 50+ columns.  Will R handle a datafile of this size? 
>  If so, how?
> 
> Thank you in advance,
> Tom Volscho
> 
> ************************************        
> Thomas W. Volscho
> Graduate Student
> Dept. of Sociology U-2068
> University of Connecticut
> Storrs, CT 06269
> Phone: (860) 486-3882
> http://vm.uconn.edu/~twv00001
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From machud at intellektik.informatik.tu-darmstadt.de  Thu Nov 18 22:00:39 2004
From: machud at intellektik.informatik.tu-darmstadt.de (Marco Chiarandini)
Date: Thu, 18 Nov 2004 22:00:39 +0100 (CET)
Subject: [R] Re: R package installation
In-Reply-To: <DB78B092-389D-11D9-BA19-000A95A1DC28@umich.edu>
References: <DB78B092-389D-11D9-BA19-000A95A1DC28@umich.edu>
Message-ID: <Pine.LNX.4.58.0411182147540.13898@kika.intellektik.informatik.tu-darmstadt.de>

Dear Prof. Johnson,

sorry for posting the reply so late.

I am already using FINK. But you were right, R was looking for g77 and
the gcc libraries under the MacOs distributions.

Rather than creating symbolic links I updated FLIBS in
/Library/Frameworks/R.framework/Resources/etc/Makeconf
so that it searches in the right path
/sw/lib/gcc/powerpc-apple-darwin7.2.0/

For g77 I made instead the link you suggest under /usr/bin/.

I removed and reinstalled the package and everything run fine, in spite
of my versions of libraries and compilers (g77 was already up-date from
FINK).


Thank you all for the help,


Marco



> Dear Marco,
>
> I was given an excerpt with your problem about installing package on
> a MAC, such as Hmisc.
>
> I had the same problems and found a work around.
>
> I have not had any trouble loading in source packages since, include
> Hmisc and Design, acepack and vgam.
>
> First, I downloaded and installed the g77 compiler.
>
> I use a progam named FINK to find, download and intall g77 (so first I
> installed FINK
> then from within FINK I downloaded/installed the g77 compiler.) Do a
> "Google" search
> for FINK, it is easy to find and install.
>
> After g77 was installed I had to make a symbolic link so R could find
> it:
>
> ln -s \sw\bin\g77 \usr\bin\g77  (I think I had to make a link to my gcc
> compiler also)
> \n -s \sw\bin\gcc \usr\bin\g77
>
> It looks like you already have the g77 compiler from the message.  the
> next mesage you can also remedy by symbolic links.  Try
>
> ln -s /sw/lib/gcc   /usr/local/lib/gcc
> ln -s /sw/lib/gcc/powerpc-apple-darwin7.5.0
> \usr\local\lib\gcc\powerpc-apple-darwin6.8
> ln -s /sw/lib/gcc/powerpc-apple-darwin7.5.0/3.4.1
> \usr\local\lib\gcc\powerpc-apple-darwin6.8\3.4.2
>
> The first directory path in each of the above may be specific to your
> configuration for gcc.
>
> But this did work for me, and if you find the correct location for gcc/
> powerpc-apple-darwinX.Y.Z/U.V.W, you should have no trouble either.
>
> Good luck.
>
> Sincerely
>
> Tim Johnson
> Adjunct Asst. Professor
> University of Michigan



-------------------------------------------------------------------
Marco Chiarandini, Fachgebiet Intellektik, Fachbereich Informatik,
Technische Universit??t Darmstadt, Hochschulstra??e 10,
D-64289 Darmstadt - Germany, Office: S2/02 Raum E317
Tel: +49.(0)6151.166802 Fax: +49.(0)6151.165326
email: machud at intellektik.informatik.tu-darmstadt.de
web page: http://www.intellektik.informatik.tu-darmstadt.de/~machud



From john.maindonald at anu.edu.au  Thu Nov 18 22:48:31 2004
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Fri, 19 Nov 2004 08:48:31 +1100
Subject: [R] The hidden costs of GPL software?
Message-ID: <96B9B9A4-39AB-11D9-95C5-000A95CDA0F2@anu.edu.au>

The author of the article says nothing about the large
number of hours and weeks that he surely spent learning
S-plus!

There should be attention to the costs that arise from a wrong
or inappropriate analysis, perhaps because the software that
is in use makes it difficult to do anything better, perhaps
because of statistical skill limitations, often with these two
factors working together.  Analyses that misrepresent the
science, or designs and analyses that conspire together to
this end, have serious and costly implications for research.

I've refereed several papers recently, in broadly ecological
fields of endeavour, with seemingly quite reasonable data,
where the mix of author skill and abilities of the package was
clearly not up to the task in hand.  Relative to getting on top
of the statistical issues (for which they will probably end up
getting, as they need to, statistical help), the GUI/noGUI issue
will be a minor consideration, and hours or weeks spent
learning R will be at most a modest consideration.

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Bioinformation Science, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.


On 17 Nov 2004, at 10:27 PM, r-help-request at stat.math.ethz.ch wrote:

> From: "Philippe Grosjean" <phgrosjean at sciviews.org>
> Date: 17 November 2004 8:53:28 PM
> To: <r-help at stat.math.ethz.ch>, <r-sig-gui at stat.math.ethz.ch>
> Cc: Subject:
>
> Hello,
>
> In the latest 'Scientific Computing World' magazine (issue 78, p. 22), 
> there
> is a review on free statistical software by Felix Grant ("doesn't have 
> to
> pay good money to obtain good statistics software"). As far as I know, 
> this
> is the first time that R is even mentioned in this magazine, given 
> that it
> usually discuss commercial products.
>
> In this article, the analysis of R is interesting. It is admitted that 
> R is
> a great software with lots of potentials, but: "All in all, R was a 
> good
> lesson in the price that may have to be paid for free software: I 
> spent many
> hours relearning some quite basic things taken for granted in the 
> commercial
> package." Those basic things are releated with data import, obtention 
> of
> basic plots, etc... with a claim for a missing more intuitive GUI in 
> order
> to smooth a little bit the learning curve.
>
> There are several R GUI projects ongoing, but these are progressing 
> very
> slowly. The main reason is, I believe, that a relatively low number of
> programmers working on R are interested by this field. Most people 
> wanting
> such a GUI are basic user that do not (cannot) contribute... And if 
> they
> eventually become more knowledgeable, they tend to have other 
> interests.
>
> So, is this analysis correct: are there hidden costs for free software 
> like
> R in the time required to learn it? At least currently, for the people 
> I
> know (biologists, ecologists, oceanographers, ...), this is perfectly 
> true.
> This is even an insurmountable barrier for many of them I know, and 
> they
> have given up (they come back to Statistica, Systat, or S-PLUS using
> exclusively functions they can reach through menus/dialog boxes).
>
> Of course, the solution is to have a decent GUI for R, but this is a 
> lot of
> work, and I wonder if the intrinsic mechanism of GPL is not working 
> against
> such a development (leading to a very low pool of programmers actively
> involved in the elaboration of such a GUI, in comparison to the very 
> large
> pool of competent developers working on R itself).
>
> Do not misunderstand me: I don't give up with my GUI project, I am just
> wondering if there is a general, ineluctable mechanism that leads to 
> the
> current R / R GUI situation as it stands,... and consequently to a 
> "general
> rule" that there are indeed most of the time "hidden costs" in free
> software, due to the larger time required to learn it. I am sure there 
> are
> counter-examples, however, my feeling is that, for Linux, Apache, 
> etc... the
> GUI (if there is one) is often a way back in comparison to the 
> potentials in
> the software, leading to a steep learning curve in order to use all 
> these
> features.
>
> I would be interested by your impressions and ideas on this topic.
>
> Best regards,
>
> Philippe Grosjean



From davidr at rhotrading.com  Thu Nov 18 23:35:38 2004
From: davidr at rhotrading.com (davidr@rhotrading.com)
Date: Thu, 18 Nov 2004 16:35:38 -0600
Subject: [R] 3d scatter plot with drop line
Message-ID: <12AE52872B5C5348BE5CF47C707FF53A422225@rhosvr02.rhotrading.com>

Very nice!
I noticed that you wrote the function to drop points to any surface.
Is it possible to add the surface to the plot as a translucent surface,
so the points and drop lines still show?

David L. Reiner

-----Original Message-----
From: Robin Hankin [mailto:rksh at soc.soton.ac.uk] 
Sent: Thursday, November 18, 2004 3:39 AM
To: joel30000 at gmail.com
Cc: r-help at stat.math.ethz.ch
Subject: Fwd: Re: [R] 3d scatter plot with drop line



Hi

try this:




p3dpairs <- function(x,x1, 
xlim=NULL,ylim=NULL,zlim=NULL,col=par("col"), pch=par("pch"), 
cex=par("cex"), ...){
   if(is.matrix(x)){
     z <- x[,3]
     y <- x[,2]
     x <- x[,1]
   }

     if(is.matrix(x1)){
     z1 <- x1[,3]
     y1 <- x1[,2]
     x1 <- x1[,1]
   }

   if(missing(zlim)) {
     z.grid <- matrix(range(z),2,2)
   } else {
     z.grid <- matrix(zlim,2,2)
   }

   if(missing(xlim)){ xlim <- range(x) }
   if(missing(ylim)){ ylim <- range(y) }

   persp(xlim, ylim, z.grid, col = NA, border=NA, ...) -> res

   trans3d <- function(x,y,z, pmat) {
     tr <- cbind(x,y,z,1) %*% pmat
     list(x = tr[,1]/tr[,4], y= tr[,2]/tr[,4])
   }

   out <- trans3d(x,y,z,pm=res)
   out1 <- trans3d(x1,y1,z1,pm=res)
   points(out, col=col, pch=pch, cex=cex, ...)

   for(i in 1:length(out$x)){
     lines(c(out$x[i],out1$x[i]),c(out$y[i],out1$y[i]), col="gray", ...)
   }
   return(invisible(out))
}




then


a <- matrix(rnorm(60),20,3)
  b <- a
b[,3] <- 0
  p3dpairs(a,b)


gives you a good approximation to what you want

HTH

rksh
>
>
>>This is a follow up to my question from yesterday. I want to do in R
>>what is called a "3d scatter plot with drop lines" in S-PLUS.
>>
>>Basically, it's a 3dscatterplot with lines connecting the x-y grid to
>>the z points.
>>The lines give a better perspective on the shape of the data surface.
>>
>>How to?
>>
>>Joel Bremson
>>UC Davis Statistics
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>
>
>--
>
>Robin Hankin
>Uncertainty Analyst
>Southampton Oceanography Centre
>SO14 3ZH
>tel +44(0)23-8059-7743
>initialDOTsurname at soc.soton.ac.uk (edit in obvious way; spam
precaution)


-- 
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
SO14 3ZH
tel +44(0)23-8059-7743
initialDOTsurname at soc.soton.ac.uk (edit in obvious way; spam precaution)
	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From rpeng at jhsph.edu  Thu Nov 18 23:36:49 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 18 Nov 2004 17:36:49 -0500
Subject: [R] Enormous Datasets
In-Reply-To: <1aab4cb1aae597.1aae5971aab4cb@huskymail.uconn.edu>
References: <1aab4cb1aae597.1aae5971aab4cb@huskymail.uconn.edu>
Message-ID: <419D2401.2090705@jhsph.edu>

It depends on what you mean by 'handle', but probably not.  You'll 
likely have to split the file into multiple files unless you have some 
rather high end hardware.   However, in my limited experience, there's 
almost always a meaningful way to split the data (geographically, or 
by other categories).

A few things I've learned recently working with large datasets:

1.  Store files in .rda format using save() -- the load times are much 
faster and loading takes up less memory
2.  If your data are integers, store them as integers!
3.  Don't store character variables in dataframes -- use factors

-roger

Thomas W Volscho wrote:
> Dear List,
> I have some projects where I use enormous datasets.  For instance, the 5% PUMS microdata from the Census Bureau.  After deleting cases I may have a dataset with 7 million+ rows and 50+ columns.  Will R handle a datafile of this size?  If so, how?
> 
> Thank you in advance,
> Tom Volscho
> 
> ************************************        
> Thomas W. Volscho
> Graduate Student
> Dept. of Sociology U-2068
> University of Connecticut
> Storrs, CT 06269
> Phone: (860) 486-3882
> http://vm.uconn.edu/~twv00001
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From yeaman at zoology.ubc.ca  Thu Nov 18 23:38:02 2004
From: yeaman at zoology.ubc.ca (Sam Yeaman)
Date: Thu, 18 Nov 2004 14:38:02 -0800
Subject: [R] Memory problems...leaky?
Message-ID: <419D244A.3060305@zoology.ubc.ca>

Hi list folks,

I have been trying to figure out how I can run the script that I have 
included below. I have 1.2 gigs of memory on the computer, but this is 
not enough, given the size of the input datasets. The input matrix 
('input') is a little under 2400 x 2900 cells, and is about 60 megs when 
stored on hard drive...what is R doing, that this (small) size of file 
takes up so much memory? The script uses three variables of this size, 
but  I don't see why that would add up to over 1.2 gigs...Is the way 
that I call variables in the for-loops causing this problem, with some 
sort of leakiness? I had heard of leaky memory problems with version 
1.5.1, but I am using 1.9.1. Alternatively, I had wondered if it was 
just the way memory works in R, as I had read a post earlier today that 
suggested that R requires 5-10 times more memory than the input to 
meaninfully calculate anything...

Thanks...Sam



bflow <- 2
pie <- 3.141592654


input <-  read.table ("xxx.txt", sep="", header=FALSE, na.strings = "-9999")
rowsize <- dim(input)[1]
colsize <- dim(input)[2]

result <- matrix (NA, rowsize, colsize)

for (i in 1:rowsize)
for (j in 1:colsize)

    if (is.na (input [i,j]) == FALSE) {

        probability <- matrix (NA, rowsize, colsize)

        for (p in 1:rowsize) {
            or (q in 1:colsize){
            distance <- sqrt ((p - i)^2 + (q - j)^2)
            probability [p,q] <- (2 / (bflow * pie * (1 + (distance / 
bflow)^2)))   
            }   
        }
 
        kernsum <- sum ((probability * input / input), na = 
TRUE)                                               
        intmean <- sum (input * probability, na = TRUE) / 
kernsum                               
        result [i,j] <- sum (((input - intmean)^2 * probability), na = 
TRUE) / kernsum
    }

}
}



From edd at debian.org  Fri Nov 19 00:05:49 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 18 Nov 2004 17:05:49 -0600
Subject: [R] Where has the Debian respository gone?
In-Reply-To: <1796582187.20041118202104@psyctc.org>
References: <cn8k7h$lin$1@sea.gmane.org>
	<20041115013527.GA17007@sonny.eddelbuettel.com>
	<793505964.20041118085238@psyctc.org>
	<20041118151840.GA22159@sonny.eddelbuettel.com>
	<1796582187.20041118202104@psyctc.org>
Message-ID: <20041118230549.GA26346@sonny.eddelbuettel.com>

Chris,

On Thu, Nov 18, 2004 at 08:21:04PM +0000, Chris Evans wrote:
> DE> More than advice, we need a volunteer to "backport" the current R package(s)
> DE> for Debian to the Debian stable distribution. As I said, testing and
> DE> unstable are taken care of (and yes, testing is still lagging because of the
> DE> now much more formal interdependence of packages; R 2.0.* will appears once
> DE> all dependent packages are available on all architectures)
> 
> I'm sure this is in itself proof that I'm not the person to do it but
> can you say a bit more about what's involved Dirk?  I run a pretty low
[..]
> Any chance I can be useful?  Could I team up with someone who really
> knows what s/he is doing but doesn't use Debian stable and work this
> together?
> 
> Let me know, I'd love to put something very direct back into the R project.

Thanks a bunch -- I'll follow up off-list!

Dirk

-- 
If your hair is standing up, then you are in extreme danger.
      -- http://www.usafa.af.mil/dfp/cockpit-phys/fp1ex3.htm



From m.abdolell at utoronto.ca  Fri Nov 19 00:14:56 2004
From: m.abdolell at utoronto.ca (Mohamed Abdolell)
Date: Thu, 18 Nov 2004 18:14:56 -0500
Subject: [R] implementing a "loop" using by(x,x$factor,FUN)
Message-ID: <1100819696.419d2cf0e587a@webmail.utoronto.ca>

Hi,

I'm writing some R code that requires a massive amount of looping and would
ideally like to write it so that it avoid the use of "for" loop ... however I'm
having some trouble.

Very briefly, the basic idea is to implement a binary partitioning algorithm to
determine the optimal cutpoint based on deviance measures obtained from
likelihood estiamtes.  This is in the geo-spatial context so I'm actually using
the geoRglm package to obtain this likelihood fit.  I initially use the "variog"
function to help specify the initial parameter values passed to "likfit" to
ensure convergence.

Although not the most elegant solution, the code works ... I just want to
re-code it to avoid using the "for" loop.

Any help would be greatly appreciated.

This is what I've got ....


RootDev <- 600
splits <- NULL

for (cutpoint in cutpoints) {
   LRGdata <- split(gdata, gdata[,4] <= cutpoint)
   vgrmL <- variog(as.geodata(LRGdata$"TRUE"), covar.col=covcol)) 
   vgrmR <- variog(as.geodata(LRGdata$"FALSE"), covar.col=covcol)) 
   mlL<- likfit(as.geodata(LRGdata$"TRUE"),
	ini=expand.grid(seq(0, max(vgrmL$v), l=10), seq(0,max(vgrmL$u), l=10)))
   mlR<- likfit(as.geodata(LRGdata$"FALSE"),
   	ini=expand.grid(seq(0, max(vgrmR$v), l=10), seq(0,max(vgrmR$u), l=10)))
   LeftDev <- summary(mlL)[[8]]$log.L
   RightDev <- summary(mlR)[[8]]$log.L
   LeftN <- dim(LRGdata$"TRUE")[1] 
   RightN <- dim(LRGdata$"FALSE")[1] 
   splits <- rbind(splits, c(paste("<=",cutpoint),paste(">",cutpoint),
	LeftN, RightN, RootDev, LeftDev, RightDev,
        -2*(RootDev-(LeftDev+RightDev))))
}


- Mohamed



From dray at biomserv.univ-lyon1.fr  Fri Nov 19 00:25:20 2004
From: dray at biomserv.univ-lyon1.fr (Stephane DRAY)
Date: Thu, 18 Nov 2004 18:25:20 -0500
Subject: [R] adjusting the map of France to 1830
In-Reply-To: <419D142C.1080003@yorku.ca>
Message-ID: <5.2.1.1.0.20041118182350.00c43198@biomserv.univ-lyon1.fr>

Hello. I do not know if you can merge polygons, but you can select easily:

 > departements=map('france',namesonly=T) # returns a vector of names of 
regions
 > map('france',regions=departements[1:20],namesonly=T) # use what you need 
with regions argument

Hope this helps,


At 16:29 18/11/2004, Michael Friendly wrote:
>I'm doing some analyses of historical data from France in 1830 on 'moral 
>statistics' that I'd like to
>show on a map.  I've done most of my analyses in SAS, but a few things 
>would work better in R.
>To do this, I have to adjust the modern map,
>
>library(maps)
>map('france')
>
>to adjust for changes in departments (86 in 1830, to 97 now).  I've read 
>the documentation
>for the maps and maptools package, but there seems to be no functions to 
>allow this, and
>I can't find information on the exact structure of map datasets, but I 
>understand them to
>be delimited lists of polygon coordinates.
>
>In SAS, all maps have (one or more) ID variables representing the 
>geographical region,
>and there is also a proc gremove that can remove internal boundaries 
>inside the polygons
>for regions with the same ID.  Is there some way I can do this in R?
>
>Here's what I did in SAS:
>
>*-- Fix the map of France to conform to Guerry:
>    - adjust the 97 current departments to correspond to the 86 in 1830
>    - delete those not part of France then
>;
>
>data gfrtemp;
>    set maps.france;
>    /* Corse was one dept - merge these to one area, new ID */
>    if id in (201, 202)    then dept=200;
>
>    /* Seine et Oise (78) was cut into
>    Essonne (91), Val d'Oise (95) and Yvelines (78) */
>    else if id in (91, 95)    then dept=78;
>
>    /* Seine (75) now split into
>    Hauts-de-Seine (92), Seine-Saint-Denis (93) et Val-de-Marne (94)*/
>    else if id in (92, 93, 94)    then dept=75;
>
>     /* departments not part of France in 1830 */
>    else if id in (
>        6,     /* Alpes-Maritimes */
>        73,74, /* Savoie, Haute-Savoie */
>        90)    /* Territore-de-Belfort */
>        then delete;
>    else                       dept=id;
>    run;
>
>*-- remove internal boundaries based on merged DEPT;
>proc sort data=gfrtemp;
>    by dept;
>
>proc gremove data=gfrtemp out=gfrance;
>   by dept;
>   id id;
>   run;
>
>
>
>--
>Michael Friendly     Email: friendly at yorku.ca Professor, Psychology Dept.
>York University      Voice: 416 736-5115 x66249 Fax: 416 736-5814
>4700 Keele Street    http://www.math.yorku.ca/SCS/friendly.html
>Toronto, ONT  M3J 1P3 CANADA
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

St??phane DRAY
-------------------------------------------------------------------------------------------------- 

D??partement des Sciences Biologiques
Universit?? de Montr??al, C.P. 6128, succursale centre-ville
Montr??al, Qu??bec H3C 3J7, Canada

Tel : (514) 343-6111 poste 1233         Fax : (514) 343-2293
E-mail : stephane.dray at umontreal.ca
-------------------------------------------------------------------------------------------------- 

Web                                          http://www.steph280.freesurf.fr/



From ray at mcs.vuw.ac.nz  Fri Nov 19 01:07:41 2004
From: ray at mcs.vuw.ac.nz (Ray Brownrigg)
Date: Fri, 19 Nov 2004 13:07:41 +1300 (NZDT)
Subject: [R] adjusting the map of France to 1830
Message-ID: <200411190007.iAJ07fGA001575@tahi.mcs.vuw.ac.nz>

At 16:29 18/11/2004, Michael Friendly wrote:
> I'm doing some analyses of historical data from France in 1830 on 'moral 
> statistics' that I'd like to
> show on a map.  I've done most of my analyses in SAS, but a few things 
> would work better in R.
> To do this, I have to adjust the modern map,
> 
> library(maps)
> map('france')
> 
> to adjust for changes in departments (86 in 1830, to 97 now).  I've read 
> the documentation
> for the maps and maptools package, but there seems to be no functions to 
> allow this, and
> I can't find information on the exact structure of map datasets, but I 
> understand them to
> be delimited lists of polygon coordinates.
> 
> In SAS, all maps have (one or more) ID variables representing the 
> geographical region,
> and there is also a proc gremove that can remove internal boundaries 
> inside the polygons
> for regions with the same ID.  Is there some way I can do this in R?
>
Unfortunately not with the current implementation of several of the
'extra' databases in the mapdata package.  The map() function does have
the interior=FALSE option, which would normally do what you want, but
only when the data has been manipulated to allow it.  Currently this
option is only useful with the world and usa maps (and their
derivatives, such as world2 and state).

Currently every department is a complete polygon, and so every interior
line segment occurs twice in the data.  What has to happen to the data
is for it to be split up into non-overlapping line segments, and each
polygon reconstructed from a list of these line segments (with
direction being important).

If you are prepared to perform this somewhat tedious process, I am
happy to assist you with further details.

However even with the interior= option functioning, it would still not
be easy to produce the map you would require, since you would have to
build it up from many components (namely each of the 'combined'
departments, plus 'all the rest').

HTH,
Ray Brownrigg



From mlaflamme at regenstrief.org  Fri Nov 19 01:13:44 2004
From: mlaflamme at regenstrief.org (Mark R Laflamme, MD)
Date: Thu, 18 Nov 2004 19:13:44 -0500
Subject: [R] Date time Differences
Message-ID: <419D3AB8.7040405@regenstrief.org>

Greetings from an R newbie

Table imported with 4 columns containing Dates and Times, I desire to 
determine the differences between said columns
Have loaded the base and chron packages for R commander.  Have tried to 
use format but without success.  Have tried the Postix commands but 
error states "unused argument (s) (...)", even though some of the 
examples do not use (...) at the end of the command

Example:
Table is named GvY1
      Operation.Time.End      Operation.Time.Dictated   
Operation.Time.Verfiy
       7/10/2004 10:10:12         7/12/2004 9:18:03                  
7/28/2004 05:00:02

Information is mm/dd/year hh:mm:ss (time of day, e.g. the first column 
is July 10th 2004 10:10 AM at 12 seconds)
What is the best way to get the difference between any of the 2 columns? 
And eventually test statistics on the columns (eg. P values, if needed, 
but not a priority at this time)
Thanks in advance

Platform WinXP
IBM T40p
R v 1.8.1
R commander

-- 
Mark R Laflamme,MD
Regenstrief Institute	
1050 Wishard Blvd RG5
Indianapolis, IN 46202
317 630-7833



From mlaflamme at regenstrief.org  Fri Nov 19 01:14:52 2004
From: mlaflamme at regenstrief.org (Mark R Laflamme, MD)
Date: Thu, 18 Nov 2004 19:14:52 -0500
Subject: [R] Date Time differences
Message-ID: <419D3AFC.50700@regenstrief.org>

Greetings from an R newbie

Table imported with 4 columns containing Dates and Times, I desire to 
determine the differences between said columns
Have loaded the base and chron packages for R commander.  Have tried to 
use format but without success.  Have tried the Postix commands but 
error states "unused argument (s) (...)", even though some of the 
examples do not use (...) at the end of the command

Example:
Table is named GvY1
     Operation.Time.End      Operation.Time.Dictated   
Operation.Time.Verfiy
      7/10/2004 10:10:12         7/12/2004 9:18:03                  
7/28/2004 05:00:02

Information is mm/dd/year hh:mm:ss (time of day, e.g. the first column 
is July 10th 2004 10:10 AM at 12 seconds)
What is the best way to get the difference between any of the 2 columns? 
And eventually test statistics on the columns (eg. P values, if needed, 
but not a priority at this time)
Thanks in advance

Platform WinXP
IBM T40p
R v 1.8.1
R commander

-- 
Mark R Laflamme,MD
Regenstrief Institute	
1050 Wishard Blvd RG5
Indianapolis, IN 46202
317 630-7833



From dray at biomserv.univ-lyon1.fr  Fri Nov 19 01:24:21 2004
From: dray at biomserv.univ-lyon1.fr (Stephane DRAY)
Date: Thu, 18 Nov 2004 19:24:21 -0500
Subject: [R] adjusting the map of France to 1830
In-Reply-To: <200411190007.iAJ07fGA001575@tahi.mcs.vuw.ac.nz>
Message-ID: <5.2.1.1.0.20041118191436.00c353d0@biomserv.univ-lyon1.fr>


At 19:07 18/11/2004, Ray Brownrigg wrote:
>At 16:29 18/11/2004, Michael Friendly wrote:
> > I'm doing some analyses of historical data from France in 1830 on 'moral
> > statistics' that I'd like to
> > show on a map.  I've done most of my analyses in SAS, but a few things
> > would work better in R.
> > To do this, I have to adjust the modern map,
> >
> > library(maps)
> > map('france')
> >
> > to adjust for changes in departments (86 in 1830, to 97 now).  I've read
> > the documentation
> > for the maps and maptools package, but there seems to be no functions to
> > allow this, and
> > I can't find information on the exact structure of map datasets, but I
> > understand them to
> > be delimited lists of polygon coordinates.
> >
> > In SAS, all maps have (one or more) ID variables representing the
> > geographical region,
> > and there is also a proc gremove that can remove internal boundaries
> > inside the polygons
> > for regions with the same ID.  Is there some way I can do this in R?
> >
>Unfortunately not with the current implementation of several of the
>'extra' databases in the mapdata package.  The map() function does have
>the interior=FALSE option, which would normally do what you want, but
>only when the data has been manipulated to allow it.  Currently this
>option is only useful with the world and usa maps (and their
>derivatives, such as world2 and state).
>
>Currently every department is a complete polygon, and so every interior
>line segment occurs twice in the data.  What has to happen to the data
>is for it to be split up into non-overlapping line segments, and each
>polygon reconstructed from a list of these line segments (with
>direction being important).

There is the gpclib package which computes intersection, union... of 
polygons. I have try to play with its union function and the france data, 
but the results are good but a little bit complicate:
# i merge the two firts polygons:

 > departements=map('france')
 > which(is.na(departements$x))[1:2]
   [1]   66  122
 > gpcA <- as(cbind(departements$x[1:65],departements$y[1:65]),"gpc.poly")
 > gpcB <- as(cbind(departements$x[67:121],departements$y[67:121]),"gpc.poly")
 > union(gpcA,gpcB)
GPC Polygon
    Num. Contours:  1
    Num. Vertices:  74
    BBox (X):  1.563161 --> 4.225965
    BBox (Y):  49.97212 --> 51.09752
 > gpcAB<-union(gpcA,gpcB)


 > 
departements$x=c(attr(gpcAB,"pts")[[1]]$x,attr(gpcAB,"pts")[[1]]$x[1],departements$x[-(1:121)])
 > 
departements$y=c(attr(gpcAB,"pts")[[1]]$y,,attr(gpcAB,"pts")[[1]]$y[1],departements$y[-(1:121)])
 > map(departements)


Another solution is to do the job in a GIS and to import the new map with 
maptools.

The package gpclib is very intersting and I think that it can be used to 
develop some basic GIS tools. I have write a functions to compute 
intersections for objects of class polys easily.

The only thing is to know for which class of spatial objects these 
functions must be developed !



St??phane DRAY
-------------------------------------------------------------------------------------------------- 

D??partement des Sciences Biologiques
Universit?? de Montr??al, C.P. 6128, succursale centre-ville
Montr??al, Qu??bec H3C 3J7, Canada

Tel : (514) 343-6111 poste 1233         Fax : (514) 343-2293
E-mail : stephane.dray at umontreal.ca
-------------------------------------------------------------------------------------------------- 

Web                                          http://www.steph280.freesurf.fr/



From N.L.Pace at m.cc.utah.edu  Fri Nov 19 01:52:57 2004
From: N.L.Pace at m.cc.utah.edu (Nathan Leon Pace, MD, MStat)
Date: Thu, 18 Nov 2004 17:52:57 -0700
Subject: [R] Creating logical value from the difference of two absolute
	values
In-Reply-To: <5.2.1.1.0.20041118191436.00c353d0@biomserv.univ-lyon1.fr>
References: <5.2.1.1.0.20041118191436.00c353d0@biomserv.univ-lyon1.fr>
Message-ID: <5A80B9B4-39C5-11D9-B2EE-000393B3E9D0@utah.edu>

Hi,

Using R 2.0.1 on Mac g5 running Mac OS X 10.3.6.

I would expect that

abs(.7 - .5) >= abs(.3 - .5) should be returned TRUE.

Instead

 > www <- abs(.7 - .5) >= abs(.3 - .5)
 > www
[1] FALSE

Is this a result of floating point or the implementation of abs or 
something else?

In a function I need to compare two absolute values - each being of the 
form |variable - constant|.

Any suggestions for implementing this correctly?

Nathan



From andy_liaw at merck.com  Fri Nov 19 02:02:01 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 18 Nov 2004 20:02:01 -0500
Subject: [R] Creating logical value from the difference of two
	absolut e values
Message-ID: <3A822319EB35174CA3714066D590DCD50994E346@usrymx25.merck.com>

It's imprecision of floating point representation of real numbers:

> print(abs(.7 - .5) - abs(.3 - .5), digit=20)
[1] -5.551115123125783e-17

one of the first things to learn about programming: be extremely careful
testing for equality of floating point numbers.

Andy


> From: Nathan Leon Pace, MD, MStat
> 
> Hi,
> 
> Using R 2.0.1 on Mac g5 running Mac OS X 10.3.6.
> 
> I would expect that
> 
> abs(.7 - .5) >= abs(.3 - .5) should be returned TRUE.
> 
> Instead
> 
>  > www <- abs(.7 - .5) >= abs(.3 - .5)
>  > www
> [1] FALSE
> 
> Is this a result of floating point or the implementation of abs or 
> something else?
> 
> In a function I need to compare two absolute values - each 
> being of the 
> form |variable - constant|.
> 
> Any suggestions for implementing this correctly?
> 
> Nathan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From vograno at evafunds.com  Fri Nov 19 02:55:44 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Thu, 18 Nov 2004 17:55:44 -0800
Subject: [R] how to get to interesting part of pattern match
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A57513B9@phost015.EVAFUNDS.intermedia.net>

Hi,

I am looking for a way to extract an "interesting" part of the match to
a regular expression. For example the pattern "[./](*.)" matches a
substring that begins with either "." or "/" followed by anything. I am
interested in this "anything" w/o the "." or "/" prefix. If say I match
the pattern against "abc/foo" I want to get "foo", not "/foo". In Perl
one can simply wrap the "interesting" part in () and get it out of the
match. Is it possible to do a similar thing in R?

There seems to be a way to refer to the match, see below, but I couldn't
figure out how to make gsub return it.
> gsub("[./](*.)", "\\1", "abc/foo")
[1] "abcfoo"


Thanks,
Vadim



From sasprog474 at yahoo.com  Fri Nov 19 03:03:20 2004
From: sasprog474 at yahoo.com (Greg Tarpinian)
Date: Thu, 18 Nov 2004 18:03:20 -0800 (PST)
Subject: [R] NLME plottting and Confidence Intervals
Message-ID: <20041119020320.92408.qmail@web41404.mail.yahoo.com>

All,

I have been learning about mixed models and have been
able to successfully use lme( ) and nlme( ) to fit
some simple linear and 4PL logistic models.  As a 
relative "newbie" I am at a loss as to how I can do
the following:

(1) Import a SAS dataset with DATE9. formatted time
    values and get them converted into a convenient
    time variable for use with the nlme package.  In
    particular, I would like to use the lattice 
    package to produce panel plots for diagnostic and
    exploratory purposes.

(2) Plot the fitted model(s) along with appropriate
    95% confidence bounds for the model

(3) Obtain prediction intervals for given individuals
    in the datasets.

Sorry for what must be trivial questions!  I very
much appreciate any insight.

Thanks,
    Greg



From jeaneid at chass.utoronto.ca  Fri Nov 19 03:25:18 2004
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Thu, 18 Nov 2004 21:25:18 -0500
Subject: [R] how to get to interesting part of pattern match
In-Reply-To: <C698D707214E6F4AB39AB7096C3DE5A57513B9@phost015.EVAFUNDS.intermedia.net>
Message-ID: <Pine.SGI.4.40.0411182123520.54267817-100000@origin.chass.utoronto.ca>



sub(".*/", "", "abc/foo")
[1] "foo"


Jean

On Thu, 18 Nov 2004, Vadim Ogranovich wrote:

> Hi,
>
> I am looking for a way to extract an "interesting" part of the match to
> a regular expression. For example the pattern "[./](*.)" matches a
> substring that begins with either "." or "/" followed by anything. I am
> interested in this "anything" w/o the "." or "/" prefix. If say I match
> the pattern against "abc/foo" I want to get "foo", not "/foo". In Perl
> one can simply wrap the "interesting" part in () and get it out of the
> match. Is it possible to do a similar thing in R?
>
> There seems to be a way to refer to the match, see below, but I couldn't
> figure out how to make gsub return it.
> > gsub("[./](*.)", "\\1", "abc/foo")
> [1] "abcfoo"
>
>
> Thanks,
> Vadim
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From jfox at mcmaster.ca  Fri Nov 19 03:45:50 2004
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 18 Nov 2004 21:45:50 -0500
Subject: [R] The hidden costs of GPL software?
In-Reply-To: <419B2CB4.7010708@srres.com>
Message-ID: <20041119024548.EUZS1919.tomts22-srv.bellnexxia.net@JohnDesktop8300>

Dear list members,

This has been a stimulating discussion, now spread over three lists.
Although I'd like to address issues that have been raised on all three
lists, I expect that more or less everyone reads r-help, so I'm just posting
these comments there.

(1) As everyone else, I've had experience with a number of other statistical
packages and programming environments in addition to R (including, more
years ago than I care to say, the mainframe predecessor of the MicrOsiris
package mentioned positively in the SCW article cited by Philippe in his
original message). I don't believe that extensive point-and-click GUIs for
broad statistical packages/programming environments such as Stata, R,
S-PLUS, or SAS are very helpful: They tend to be labyrinths that are
difficult to navigate. Some of the suggestions for other kinds of GUIs
(e.g., aids to command specification) seem to me more promising. Moreover, I
don't think that one should expect to learn an extensive system such as R or
SAS without doing some reading. My own experience is that S (i.e.,
encompassing R and S-PLUS) is easier, not harder, to learn than its true
competitors. 

(2) On the other hand, one can build quite nice graphical interfaces to more
limited packages. A couple of examples that I particularly like are SAS JMP
and Cook's and Weisberg's Arc (built on Lisp-Stat).

(3) Similarly, my Rcmdr package was meant to be a limited-purpose GUI,
useful for basic-statistics classes. Its range has grown somewhat to cover
linear and generalized-linear models, and I plan a few more modest
extensions (including the ability to incorporate other classes of
statistical models more easily). As a technical matter, I don't think that
it would be hard (although it would be time-consuming) to produce a much
broader extension, but the result (in my opinion) would be as dubiously
useful as the GUIs for SAS or S-PLUS. By the way, if there were something I
could wish for here it would be a slightly broader set of Tk widgets to be
included with the Tcl/Tk that installs with R for Windows, since using
widgets outside of this set creates installation obstacles for lower-level
users.

(4) Several people have pointed once more to the difficulty that novice
users experience in locating functions to perform particular tasks or in
figuring out how to use them once found. I suspect that even people who have
been using R for a while occasionally have a brain-cramp that leads to a
search through documentation. I know that I do. In my experience, the
various facilities for searching documentation in R work pretty well. 

(5) I think that examples in help files and vignettes can be useful, but are
not substitutes for text-books, manuals, and journal articles. It certainly
should not be the job of statistical software to teach the statistics,
although of course it can be used to help do that. I doubt that many list
members would look favourably on the statistical-methods "decision tree" in
MicrOsiris, for example. One solution is to include PDF "manuals" with
packages. I've done this, for example, with my effects and Rcmdr packages.
The introductory manual supplied with Thomas Lumley's survey package is
another, similar example. Maybe there's a better way of integrating such
non-vignette manuals with the help system -- something like
help(manual=package).

(6) As has been pointed out, e.g., by Duncan Murdoch, solving the
function-locating problem is best done by a method or methods that
automatically accommodate the growing and changing set of contributed
packages on CRAN.  Why not, as previously has been proposed, replace the
current static (and, in my view, not very useful) set of keywords in R
documentation with the requirement that package authors supply their own
keywords for each documented object? I believe that this is the intent of
the concept entries in Rd files, but their use certainly is not required or
even actively encouraged. (They're just mentioned in passing in the Writing
R Extensions manual.)

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox



From ggrothendieck at myway.com  Fri Nov 19 04:17:02 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 19 Nov 2004 03:17:02 +0000 (UTC)
Subject: [R] how to get to interesting part of pattern match
References: <C698D707214E6F4AB39AB7096C3DE5A57513B9@phost015.EVAFUNDS.intermedia.net>
Message-ID: <loom.20041119T041434-742@post.gmane.org>

Vadim Ogranovich <vograno <at> evafunds.com> writes:


: I am looking for a way to extract an "interesting" part of the match to
: a regular expression. For example the pattern "[./](*.)" matches a
: substring that begins with either "." or "/" followed by anything. I am
: interested in this "anything" w/o the "." or "/" prefix. If say I match
: the pattern against "abc/foo" I want to get "foo", not "/foo". In Perl
: one can simply wrap the "interesting" part in () and get it out of the
: match. Is it possible to do a similar thing in R?
: 
: There seems to be a way to refer to the match, see below, but I couldn't
: figure out how to make gsub return it.
: > gsub("[./](*.)", "\\1", "abc/foo")
: [1] "abcfoo"

Assuming what was meant is the following (dot and star are 
transposed and gsub is sub):

	sub("[./](.*)", "\\1", "abc/foo")

then the regular expression matches /foo and the backreference
contains foo so it replaces /foo with foo which is why it returns
abcfoo .

To get just foo ensure that your regular expression matches 
the entire string so that the entire string is replaced with
the backreference:

	sub("[^./]*[./](.*)", "\\1", "abc/foo")



From ggrothendieck at myway.com  Fri Nov 19 04:40:03 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 19 Nov 2004 03:40:03 +0000 (UTC)
Subject: [R] Date time Differences
References: <419D3AB8.7040405@regenstrief.org>
Message-ID: <loom.20041119T041948-437@post.gmane.org>

Mark R Laflamme, MD <mlaflamme <at> regenstrief.org> writes:

: 
: Greetings from an R newbie
: 
: Table imported with 4 columns containing Dates and Times, I desire to 
: determine the differences between said columns
: Have loaded the base and chron packages for R commander.  Have tried to 
: use format but without success.  Have tried the Postix commands but 
: error states "unused argument (s) (...)", even though some of the 
: examples do not use (...) at the end of the command
: 
: Example:
: Table is named GvY1
:       Operation.Time.End      Operation.Time.Dictated   
: Operation.Time.Verfiy
:        7/10/2004 10:10:12         7/12/2004 9:18:03                  
: 7/28/2004 05:00:02
: 
: Information is mm/dd/year hh:mm:ss (time of day, e.g. the first column 
: is July 10th 2004 10:10 AM at 12 seconds)
: What is the best way to get the difference between any of the 2 columns? 
: And eventually test statistics on the columns (eg. P values, if needed, 
: but not a priority at this time)
: Thanks in advance
: 
: Platform WinXP
: IBM T40p
: R v 1.8.1
: R commander
: 


Often problems with data frames are due to storing the data
as factors rather than in a datetime class but without a 
reproduceable example its impossible to know for sure.  You might 
be able to solve this yourself by reading the article on dates and 
times in R News 4/1.



From monch1962 at gmail.com  Fri Nov 19 04:53:46 2004
From: monch1962 at gmail.com (David Mitchell)
Date: Thu, 18 Nov 2004 22:53:46 -0500
Subject: [R] Tools for data preparation?
Message-ID: <f6508a86041118195330d889d@mail.gmail.com>

Hello list,

I'm regularly in the position where I have to do a lot of data
manipulation, in order to get the data I have into a format R is happy
with.  This manipulation would generally be in one of two forms:
- getting data from e.g. text log files into a tabular format
- extracting sensible sample data from a very large data set (i.e. too
large for R to handle)

In general, I use Perl or Python to do the task; I'm curious as to
what others use when they hit the same problem.

Regards

Dave Mitchell



From Wanzare at HCJP.com  Fri Nov 19 06:17:37 2004
From: Wanzare at HCJP.com (Manoj - Hachibushu Capital)
Date: Fri, 19 Nov 2004 14:17:37 +0900
Subject: [R] Performing regression using R & C
Message-ID: <111C3CD246AB774790496EBB3CD220080334F4@mothra.hcjp.com>

Dear All,
	Is it possible to perform OLS using C code? I am trying to
optimize a n-period "moving window" OLS on a huge dataset hence was
wondering if such a thing is possible.

	Ideally the solution that I am looking for would involve a
C-code accepting two float arrays and returning back computed parameters
such as t-stat, coefficient etc. 

	I have glanced thru the FAQ's and tried to google this
information but wasn't able to find anything truly relevant, hence would
really appreciate your help on this one.


Thanks.

Manoj



From sasprog474 at yahoo.com  Fri Nov 19 07:19:19 2004
From: sasprog474 at yahoo.com (Greg Tarpinian)
Date: Thu, 18 Nov 2004 22:19:19 -0800 (PST)
Subject: [R] Performing regression using R & C
In-Reply-To: <111C3CD246AB774790496EBB3CD220080334F4@mothra.hcjp.com>
Message-ID: <20041119061919.27260.qmail@web41414.mail.yahoo.com>

I have found "Numerical Recipes in C" to be extremely
helpful in my statistical studies.  OLS is described
in detail in this book.

Best,

    Greg


--- Manoj - Hachibushu Capital <Wanzare at HCJP.com>
wrote:

> Dear All,
> 	Is it possible to perform OLS using C code? I am
> trying to
> optimize a n-period "moving window" OLS on a huge
> dataset hence was
> wondering if such a thing is possible.
> 
> 	Ideally the solution that I am looking for would
> involve a
> C-code accepting two float arrays and returning back
> computed parameters
> such as t-stat, coefficient etc. 
> 
> 	I have glanced thru the FAQ's and tried to google
> this
> information but wasn't able to find anything truly
> relevant, hence would
> really appreciate your help on this one.
> 
> 
> Thanks.
> 
> Manoj
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From Roger.Bivand at nhh.no  Fri Nov 19 08:48:21 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 19 Nov 2004 08:48:21 +0100 (CET)
Subject: [R] Performing regression using R & C
In-Reply-To: <20041119061919.27260.qmail@web41414.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0411190837400.17805-100000@reclus.nhh.no>

On Thu, 18 Nov 2004, Greg Tarpinian wrote:

> I have found "Numerical Recipes in C" to be extremely
> helpful in my statistical studies.  OLS is described
> in detail in this book.
> 

Maybe just read the R code: src/library/stats/R/lm.R contains function 
lm.fit(), which is a wrapper for the Fortran subroutine dqrls(), and is 
about as finely honed as you get. It won't give you the standard errors, 
but you get the coefficients and the qr object, from which the rest can be 
constructed. Using well-supported and maintained code does save time too!

But maybe there are moving window functions around that you can use
without having to "roll your own", searching on "moving window" at
http://finzi.psych.upenn.edu/search.html gave 85 hits, including the
following, which you may find fruitful:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/26904.html

Roger

> Best,
> 
>     Greg
> 
> 
> --- Manoj - Hachibushu Capital <Wanzare at HCJP.com>
> wrote:
> 
> > Dear All,
> > 	Is it possible to perform OLS using C code? I am
> > trying to
> > optimize a n-period "moving window" OLS on a huge
> > dataset hence was
> > wondering if such a thing is possible.
> > 
> > 	Ideally the solution that I am looking for would
> > involve a
> > C-code accepting two float arrays and returning back
> > computed parameters
> > such as t-stat, coefficient etc. 
> > 
> > 	I have glanced thru the FAQ's and tried to google
> > this
> > information but wasn't able to find anything truly
> > relevant, hence would
> > really appreciate your help on this one.
> > 
> > 
> > Thanks.
> > 
> > Manoj
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From phgrosjean at sciviews.org  Fri Nov 19 08:52:11 2004
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Fri, 19 Nov 2004 08:52:11 +0100
Subject: [R] Clipboard under Linux/Unix
Message-ID: <200411190752.iAJ7qCnD018061@outmx008.isp.belgacom.be>

Hello,

This may be a trivial question, but I don't find the answer in R online
help. Under Windows, I can copy/paste to the clipboard using
readClipboard()/writeClipboard(), or something like cat(..., file =
"clipboard"). Are there equivalent function for other platforms?
Best,

Philippe Grosjean

..............................................<??}))><........
 ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
 ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
 ) ) ) ) )   Mons-Hainaut University, Pentagone
( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
 ) ) ) ) )   6, av du Champ de Mars, 7000 Mons, Belgium  
( ( ( ( (       
 ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.33.12
( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
 ) ) ) ) )      
( ( ( ( (    web:   http://www.umh.ac.be/~econum
 ) ) ) ) )
..............................................................



From s-plus at wiwi.uni-bielefeld.de  Fri Nov 19 08:53:33 2004
From: s-plus at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Fri, 19 Nov 2004 08:53:33 +0100
Subject: [R] how to get to interesting part of pattern match
References: <C698D707214E6F4AB39AB7096C3DE5A57513B9@phost015.EVAFUNDS.intermedia.net>
Message-ID: <419DA67D.7060108@wiwi.uni-bielefeld.de>

Vadim Ogranovich wrote:

>Hi,
>
>I am looking for a way to extract an "interesting" part of the match to
>a regular expression. For example the pattern "[./](*.)" matches a
>substring that begins with either "." or "/" followed by anything. I am
>interested in this "anything" w/o the "." or "/" prefix. If say I match
>the pattern against "abc/foo" I want to get "foo", not "/foo". In Perl
>one can simply wrap the "interesting" part in () and get it out of the
>match. Is it possible to do a similar thing in R?
>
>There seems to be a way to refer to the match, see below, but I couldn't
>figure out how to make gsub return it.
>  
>
>>gsub("[./](*.)", "\\1", "abc/foo")
>>    
>>
>[1] "abcfoo"
>
>
>Thanks,
>Vadim
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>
what about:

gsub(".*[./](*.)", "\\1", "abc/foo")

output-start
[1] "foo"
output-end

or try:

strsplit("abc/foo","/")[[1]][2]

output-start
[1] "foo"
output-end

Peter Wolf



From ripley at stats.ox.ac.uk  Fri Nov 19 09:17:42 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 19 Nov 2004 08:17:42 +0000 (GMT)
Subject: [R] Clipboard under Linux/Unix
In-Reply-To: <200411190752.iAJ7qCnD018061@outmx008.isp.belgacom.be>
References: <200411190752.iAJ7qCnD018061@outmx008.isp.belgacom.be>
Message-ID: <Pine.LNX.4.61.0411190812050.24346@gannet.stats>

On Fri, 19 Nov 2004, Philippe Grosjean wrote:

> This may be a trivial question, but I don't find the answer in R online
> help. Under Windows, I can copy/paste to the clipboard using
> readClipboard()/writeClipboard(), or something like cat(..., file =
> "clipboard"). Are there equivalent function for other platforms?

No.  There is not necessarily even the equivalent of a clipboard; I am 
typing this in a terminal session on a remote computer that has no access 
to the clipboard on my local windowing system (nor even to my local 
display: for performance reasons the X11 connection is not forwarded, 
and even if it were, I do the same thing from my Windows laptop and 
there Exceed and Windows have separate clipboards in different formats).

Please do use R-devel and not R-help for programming questions: see the 
posting guide.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From phgrosjean at sciviews.org  Fri Nov 19 09:28:43 2004
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Fri, 19 Nov 2004 09:28:43 +0100
Subject: [R] The hidden costs of GPL software?
In-Reply-To: <20041119024548.EUZS1919.tomts22-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <200411190828.iAJ8SiUS016426@outmx018.isp.belgacom.be>

John Fox wrote:
> [...] (sorry, this is long mail, and I want to comment only details)
> By the way, if there were 
> something I could wish for here it would be a slightly 
> broader set of Tk widgets to be included with the Tcl/Tk that 
> installs with R for Windows, since using widgets outside of 
> this set creates installation obstacles for lower-level users.

Then, take a look at the tcltk2 package in the SciViews bundle (probably, in
the next version, I will take it out of the bundle). You have there tile
(themable widgets with notebook tabs, progress bar, and many more... and
very soon combo boxes and lists/trees). You have also the famous tkTable,
and a separate combobox and a tree, and a support for tooltips everywhere...
Just propose if you need more! All this runs under Windows, but I still got
problems to compile it under other platforms.

> I doubt that many list 
> members would look favourably on the statistical-methods 
> "decision tree" in MicrOsiris, for example. One solution is 
> to include PDF "manuals" with packages. I've done this, for 
> example, with my effects and Rcmdr packages.
> The introductory manual supplied with Thomas Lumley's survey 
> package is another, similar example. Maybe there's a better 
> way of integrating such non-vignette manuals with the help 
> system -- something like help(manual=package).

I tend to have the same opinion than John (although I thing that both a good
manual, and a better online help could be beneficial): a PDF manual is much
more readable than a wiki! Why not to propose PDF manuals in the \doc
section of CRAN which have a GNU Free Documentation License
(http://www.gnu.org/copyleft/fdl.html), so that the manual could be
progressively enhanced by many authors?

Best,

Philippe Grosjean



From w.northcott at unsw.edu.au  Fri Nov 19 10:22:29 2004
From: w.northcott at unsw.edu.au (Bill Northcott)
Date: Fri, 19 Nov 2004 20:22:29 +1100
Subject: [R] The hidden costs of GPL software?
In-Reply-To: <200411181113.iAIB6XId020702@hypatia.math.ethz.ch>
References: <200411181113.iAIB6XId020702@hypatia.math.ethz.ch>
Message-ID: <89046CC4-3A0C-11D9-9929-000393D3D676@unsw.edu.au>

>       If the GPL were not so tight on this point, someone could
> commercialize a GUI for R without having to offer their source code
> under the GPL.
>
There is nothing in GPL to stop a commercial GUI for R.  Have a look at 
what Apple do.  They have a complete commercial GUI and numerous 
applications built on a an almost completely GPL'ed operating system.  
There are loads of shareware GUIs which drive GPL utilities.  Most 
obviously there are plenty of commercial apps which run on GNU Linux.

Bill Northcott



From M.Mamin at intershop.de  Fri Nov 19 10:28:00 2004
From: M.Mamin at intershop.de (Marc Mamin)
Date: Fri, 19 Nov 2004 10:28:00 +0100
Subject: [R] Tools for data preparation?
Message-ID: <A03188C6623C0D46A703CB5AA59907F201C11C55@JENMAIL01.ad.intershop.net>

Hello David,


I had the same problem with log files containing many fields separated by the "|" character.

My task was to extract parts of some fields with regular expression and normalize the result to compact them (using R functions factor and table)

To reduce the data size, I first split the logfile into "subfiles" containing only one field from the original data.
So I could process one field after the other instead of loading the complete log file.

under Linux:

	cutfile<-function(index,afile,tmpdir,wd){
	#index: list of fields to keep
	#afile: logfile
	setwd(wd)
	system(paste('for n  in ',index,'; \n',
         'do sudo gzip -dc ',afile,' | cut -f$n -d"|" > ',tmpdir,'/',afile,'.$n \n',
         'done;',sep=''))
	return(1)
}

exampe: cutfile(c(1,5,8),'mylog',outputdir,sourcedir)

=> files mylog,1, mylog.5, mylog.8

HTH,

Marc Mamin


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of David Mitchell
Sent: Friday, November 19, 2004 4:54 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Tools for data preparation?


Hello list,

I'm regularly in the position where I have to do a lot of data
manipulation, in order to get the data I have into a format R is happy
with.  This manipulation would generally be in one of two forms:
- getting data from e.g. text log files into a tabular format
- extracting sensible sample data from a very large data set (i.e. too
large for R to handle)

In general, I use Perl or Python to do the task; I'm curious as to
what others use when they hit the same problem.

Regards

Dave Mitchell

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From stijn.lievens at ugent.be  Fri Nov 19 10:37:10 2004
From: stijn.lievens at ugent.be (Stijn Lievens)
Date: Fri, 19 Nov 2004 10:37:10 +0100
Subject: [R] how to rewrite this without a loop ?
In-Reply-To: <Pine.A41.4.61b.0411180804320.116790@homer11.u.washington.edu>
References: <419CB545.1000807@ugent.be>
	<Pine.A41.4.61b.0411180804320.116790@homer11.u.washington.edu>
Message-ID: <419DBEC6.4050301@ugent.be>

Thomas Lumley wrote:
> On Thu, 18 Nov 2004, Stijn Lievens wrote:
> 
>>
>> <code>
>> add.fun <- function(perf.data) {
>>   ss <- 0
>>   for (i in 0:29) {
>>       ss <- ss + cor(subset(perf.data, dataset == i)[3], 
>> subset(perf.data, dataset == i)[7], method = "kendall")
>>   }
>>   ss    }
>> </code>
>>
>> As one can see this function uses a for-loop.  Now chapter 9 of 'An 
>> introduction to R' tells us that we should avoid for-loops as much as 
>> possible.
> 
> 
> 
> You don't say whether `dataset' is the name of a column in `perf.data'. 
> Assuming it is, and assuming that 0:29 are all the values of `dataset'
> 
> sum(by(perf.data, list(perf.data$dataset),
>           function(d)  cor(d[,3],d[,7], method="kendall")))
> 
> would work.  

Indeed, this works.  The 'by' command is exactly what I was looking for.
As far as I can tell, this useful command it isn't mentioned in 'An 
introduction to R'.

> If this is faster it will be because you don't call 
> subset() twice per iteration, rather than because you are avoiding a 
> loop.  However it has other benefits: it doesn't have the variable `i', 
> it doesn't have to change the value of `ss', and it doesn't have the 
> range of `dataset' hard-coded into it.  These are all clarity 
> optimisations.
> 

In fact I don't care too much about speed at the moment, but a one-line 
statement is more convenient to type (and recall) in the command line 
interface then a multi-line statmement.

Your solution really does the trick for me.  Thanks,

Stijn.


>     -thomas



From michael.watson at bbsrc.ac.uk  Fri Nov 19 10:53:03 2004
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Fri, 19 Nov 2004 09:53:03 -0000
Subject: [R] More problems checking an R package
Message-ID: <8975119BCD0AC5419D61A9CF1A923E95E8989F@iahce2knas1.iah.bbsrc.reserved>

Hi

I am having a few more miscellaneous problems when I run R CMD check on
my package.

All of the early stuff works OK.  I get a WARNING at the "checking S3
generic/method consistency" stage, where it seems to think two of my
fucntions are inconsistent with plot() (I don't understand why it thinks
that).

I then get another WARNING indicating that I have an undocumented data
set called ".Traceback", but I can't find anything called .Traceback on
my entire system.

Finally, all my examples run fine, all my Latex is checked and that's
OK, and I get:

* creating mypackage-manual.tex ... OK
* checking mypackage-manual.tex ... ERROR
LaTeX errors when creating DVI version.
This typically indicates Rd problems.

This is in contrast to the first line of the output which states:

* checking for working latex ... OK

So I'm kind of stuck here.  I'd like to ask if there is something I can
use to check my Rd files WITHOUT having to run the examples, as the
examples all take about 30 minutes to run, and so it's not a quick
bug-tracking cycle...

Any ideas, comments welcome!

Thanks
Mick



From Ted.Harding at nessie.mcc.ac.uk  Fri Nov 19 09:56:47 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 19 Nov 2004 08:56:47 -0000 (GMT)
Subject: [R] Tools for data preparation?
In-Reply-To: <f6508a86041118195330d889d@mail.gmail.com>
Message-ID: <XFMail.041119085647.Ted.Harding@nessie.mcc.ac.uk>

On 19-Nov-04 David Mitchell wrote:
> Hello list,
> 
> I'm regularly in the position where I have to do a lot of data
> manipulation, in order to get the data I have into a format R
> is happy with.  This manipulation would generally be in one of
> two forms:
> - getting data from e.g. text log files into a tabular format
> - extracting sensible sample data from a very large data set
> (i.e. too large for R to handle)
> 
> In general, I use Perl or Python to do the task; I'm curious
> as to what others use when they hit the same problem.

I generally use 'awk' with help from 'sed' when needed.
This is on the same lines as your choice though lighter-weight
and less powerful (but I've never had a case that needed more).

Since the sort of task you describe is basically on a line-by-line
basis (and what's meant by a "line" can be pretty flexible in 'awk'),
this sort of thing can be done straightforwardly; but greater
flexibility is also possible.

E.g. it is easy to extract a line from the input, or apply a certain
transformation to fields in a line, if & only if it has already been
preceded by a line satisfying a certain condition, and so on.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 19-Nov-04                                       Time: 08:56:47
------------------------------ XFMail ------------------------------



From Ted.Harding at nessie.mcc.ac.uk  Fri Nov 19 10:49:19 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 19 Nov 2004 09:49:19 -0000 (GMT)
Subject: [R] Clipboard under Linux/Unix
In-Reply-To: <200411190752.iAJ7qCnD018061@outmx008.isp.belgacom.be>
Message-ID: <XFMail.041119094919.Ted.Harding@nessie.mcc.ac.uk>

On 19-Nov-04 Philippe Grosjean wrote:
> Hello,
> 
> This may be a trivial question, but I don't find the answer
> in R online help. Under Windows, I can copy/paste to the
> clipboard using readClipboard()/writeClipboard(), or something
> like cat(..., file = "clipboard"). Are there equivalent function
> for other platforms?
> Best,
> 
> Philippe Grosjean

Hi Philippe,
Leaving aside the situation described by Brian, where the network
is set up to prevent it, normally in Unix/Linux running X windows
you could in certain circumstances (see below) do it transparantly
over the net between machines, as well as within the machine you
are working on, under program control.

Of course there is the usual "block-copy using mouse left button
and paste using mouse middle button" manoeuvre, but -- provided
you have WindowMaker installed (part of GNUstep, the NeXTstep
emulator; you don't need to be running WindowMaker, only to
have the relevant binaries available as /usr/X11R6/bin/wxcopy
and /usr/X11R6/bin/wxpaste) -- then there are also two commands
which put it under program control, especially useful for larger
blocks of text.

1. From 'man wxcopy':

wxcopy(1x)                                             wxcopy(1x)
NAME
       wxcopy - copy stdin or file into cutbuffer
SYNOPSIS
       wxcopy [options] [filename]
DESCRIPTION
       wxcopy  copies stdin or filename into the cutbuffer. If no
       cutbuffer is specified, the data will be copied into  cut-
       buffer 0 and the other cutbuffers will be rotated, if pre-
       sent.  If a cutbuffer is specified,  the  data  is  copied
       into  that  cutbuffer,  and no rotation of buffers is per-
       formed.

2. From 'man wxpaste':

wxpaste(1x)                                           wxpaste(1x)
NAME
       wxpaste - output a cutbuffer to stdout
SYNOPSIS
       wxpaste [options]
DESCRIPTION
       wxpaste outputs the contents of the specified cutbuffer to
       stdout. If no cutbuffer is specified, the cutbuffer 0 will
       be used as default.

(Read the man pages in full for information on the options, etc.)

Using these two commands, you should be able to write an R function
(using 'system()' function to invoke these commands) which does
what you want; you will probably need to exploit the Unix-type
redirection methods as well, e.g. to redirect stdout from 'wpaste'
as in

  wpaste > file.to.paste.into

As a trivial example:

  wxcopy < file1
  wxpaste > file2

Then file2 will contain a copy of the contents of file1.

To illustrate how it works (as typed in):
-----------------------------------------
wxcopy << EOT
This is something  
I want to copy into the cut buffer
and then
paste into the file
"mycopy"
EOT

wxpaste > mycopy

and this gives the contents of the file mycopy:
-----------------------------------------------
cat mycopy
This is something  
I want to copy into the cut buffer
and then
paste into the file
"mycopy"


Hoping this helps,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 19-Nov-04                                       Time: 09:49:19
------------------------------ XFMail ------------------------------



From ripley at stats.ox.ac.uk  Fri Nov 19 11:02:59 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 19 Nov 2004 10:02:59 +0000 (GMT)
Subject: [R] The hidden costs of GPL software?
In-Reply-To: <89046CC4-3A0C-11D9-9929-000393D3D676@unsw.edu.au>
References: <200411181113.iAIB6XId020702@hypatia.math.ethz.ch>
	<89046CC4-3A0C-11D9-9929-000393D3D676@unsw.edu.au>
Message-ID: <Pine.LNX.4.61.0411190950050.27349@gannet.stats>

On Fri, 19 Nov 2004, Bill Northcott wrote:

>>       If the GPL were not so tight on this point, someone could
>> commercialize a GUI for R without having to offer their source code
>> under the GPL.
>> 
> There is nothing in GPL to stop a commercial GUI for R.  Have a look at what 
> Apple do.  They have a complete commercial GUI and numerous applications 
> built on a an almost completely GPL'ed operating system.  There are loads of 
> shareware GUIs which drive GPL utilities.  Most obviously there are plenty of 
> commercial apps which run on GNU Linux.

Perhaps you could look at earlier replies, for as Thomas Lumley said in

   https://stat.ethz.ch/pipermail/r-help/2004-November/059625.html

   `A GUI that ran R just by sending commands to stdin and getting results
   from stdout could clearly be proprietary without violating the GPL.  The
   question of exactly what level of closer integration is allowed would
   get complicated and I won't speculate.'

I will speculate as far as to say that the Free Software Foundation seems 
to regard the degree of integration that involves linking against libR.so 
or R.dll as subject to the `based on' provisions of GPL: for example

   http://www.gnu.org/licenses/gpl-faq.html#MereAggregation

says

   Combining two modules means connecting them together so that they form a
   single larger program. If either part is covered by the GPL, the whole
   combination must also be released under the GPL--if you can't, or won't,
   do that, you may not combine them.

   What constitutes combining two parts into one program? This is a legal
   question, which ultimately judges will decide. We believe that a proper
   criterion depends both on the mechanism of communication (exec, pipes,
   rpc, function calls within a shared address space, etc.) and the
   semantics of the communication (what kinds of information are interchanged).

   If the modules are included in the same executable file, they are
   definitely combined in one program. If modules are designed to run
   linked together in a shared address space, that almost surely means combining
   them into one program.

   By contrast, pipes, sockets and command-line arguments are communication
   mechanisms normally used between two separate programs. So when they are
   used for communication, the modules normally are separate programs. But if
   the semantics of the communication are intimate enough, exchanging
   complex internal data structures, that too could be a basis to consider the two
   parts as combined into a larger program.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From christoph.bier at web.de  Fri Nov 19 11:04:57 2004
From: christoph.bier at web.de (Christoph Bier)
Date: Fri, 19 Nov 2004 11:04:57 +0100
Subject: [R] Where has the Debian respository gone?
In-Reply-To: <20041118230549.GA26346@sonny.eddelbuettel.com>
References: <cn8k7h$lin$1@sea.gmane.org>	<20041115013527.GA17007@sonny.eddelbuettel.com>	<793505964.20041118085238@psyctc.org>	<20041118151840.GA22159@sonny.eddelbuettel.com>	<1796582187.20041118202104@psyctc.org>
	<20041118230549.GA26346@sonny.eddelbuettel.com>
Message-ID: <cnkggb$p0m$1@sea.gmane.org>

Dirk Eddelbuettel schrieb am 19.11.2004 00:05

> On Thu, Nov 18, 2004 at 08:21:04PM +0000, Chris Evans wrote:

[...]

>>Any chance I can be useful?  Could I team up with someone who really
>>knows what s/he is doing but doesn't use Debian stable and work this
>>together?
>>
>>Let me know, I'd love to put something very direct back into the R project.
> 
> Thanks a bunch -- I'll follow up off-list!

Include me, if you want. I have very less time at the moment (and
the near future) but maybe I can be useful, too (e.g. in compiling).

Regards,
    Christoph



From ligges at statistik.uni-dortmund.de  Fri Nov 19 11:14:30 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 19 Nov 2004 11:14:30 +0100
Subject: [R] More problems checking an R package
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E95E8989F@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E95E8989F@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <419DC786.40301@statistik.uni-dortmund.de>

michael watson (IAH-C) wrote:
> Hi
> 
> I am having a few more miscellaneous problems when I run R CMD check on
> my package.
> 
> All of the early stuff works OK.  I get a WARNING at the "checking S3
> generic/method consistency" stage, where it seems to think two of my
> fucntions are inconsistent with plot() (I don't understand why it thinks
> that).
> 
> I then get another WARNING indicating that I have an undocumented data
> set called ".Traceback", but I can't find anything called .Traceback on
> my entire system.
> 
> Finally, all my examples run fine, all my Latex is checked and that's
> OK, and I get:
> 
> * creating mypackage-manual.tex ... OK
> * checking mypackage-manual.tex ... ERROR
> LaTeX errors when creating DVI version.
> This typically indicates Rd problems.

See the log file in the check directory, it shows where the error comes 
from.


> This is in contrast to the first line of the output which states:
> 
> * checking for working latex ... OK
> 
> So I'm kind of stuck here.  I'd like to ask if there is something I can
> use to check my Rd files WITHOUT having to run the examples, as the
> examples all take about 30 minutes to run, and so it's not a quick
> bug-tracking cycle...

R CMD check --help  tells you:

R CMD check --no-examples .....

Uwe Ligges


> Any ideas, comments welcome!
> 
> Thanks
> Mick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Fri Nov 19 11:45:40 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 19 Nov 2004 11:45:40 +0100
Subject: [R] The hidden costs of GPL software?
In-Reply-To: <20041119024548.EUZS1919.tomts22-srv.bellnexxia.net@JohnDesktop8300>
References: <20041119024548.EUZS1919.tomts22-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <x24qjmazhn.fsf@biostat.ku.dk>

"John Fox" <jfox at mcmaster.ca> writes:

> I don't think that
> it would be hard (although it would be time-consuming) to produce a much
> broader extension, but the result (in my opinion) would be as dubiously
> useful as the GUIs for SAS or S-PLUS. 

Strategically, that might actually be a valid (and valiant) design
goal! From my limited experience with Rcmdr and SAS Analyst, I'd say
that Rcmdr is almost there, just a few little niggles like not
remembering values from the last time a form was filled in.

> By the way, if there were something I
> could wish for here it would be a slightly broader set of Tk widgets to be
> included with the Tcl/Tk that installs with R for Windows, since using
> widgets outside of this set creates installation obstacles for lower-level
> users.

Argh. Please stop poking at my guilty conscience.... Wrapping Tcl/Tk
extensions as R packages has been on my wish list too for some time,
with tktable as the obvious first candidate. (It's not just on
Windows; the default Unix/Linux installs of Tcl/Tk tend to be pretty
minimal too. On Windows we have this instructive twist on the BSD/GPL
debacle, that ActiveState made a very nice Tcl/Tk distribution with
all sorts of "batteries included", but we cannot bundle it with R as
they are restricting redistribution.)


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From phgrosjean at sciviews.org  Fri Nov 19 12:03:11 2004
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Fri, 19 Nov 2004 12:03:11 +0100
Subject: [R] The hidden costs of GPL software?
In-Reply-To: <x24qjmazhn.fsf@biostat.ku.dk>
Message-ID: <200411191103.iAJB3Cf0022376@outmx017.isp.belgacom.be>

Peter,

You don't need the ActiveState Tcl distribution to add extensions. If you
compile extensions yourself (and these extensions have a compatible
license), then you have no problems... (well, almost! You must make sure
those extensions compile correctly on all supproted platforms). This is
exactly what I do in the tcltk2 package.
Best,

Philippe Grosjean 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Peter Dalgaard
> Sent: Friday, November 19, 2004 11:46 AM
> To: John Fox
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] The hidden costs of GPL software?
> 
> "John Fox" <jfox at mcmaster.ca> writes:
> 
> > I don't think that
> > it would be hard (although it would be time-consuming) to produce a 
> > much broader extension, but the result (in my opinion) would be as 
> > dubiously useful as the GUIs for SAS or S-PLUS.
> 
> Strategically, that might actually be a valid (and valiant) 
> design goal! From my limited experience with Rcmdr and SAS 
> Analyst, I'd say that Rcmdr is almost there, just a few 
> little niggles like not remembering values from the last time 
> a form was filled in.
> 
> > By the way, if there were something I
> > could wish for here it would be a slightly broader set of 
> Tk widgets 
> > to be included with the Tcl/Tk that installs with R for 
> Windows, since 
> > using widgets outside of this set creates installation 
> obstacles for 
> > lower-level users.
> 
> Argh. Please stop poking at my guilty conscience.... Wrapping 
> Tcl/Tk extensions as R packages has been on my wish list too 
> for some time, with tktable as the obvious first candidate. 
> (It's not just on Windows; the default Unix/Linux installs of 
> Tcl/Tk tend to be pretty minimal too. On Windows we have this 
> instructive twist on the BSD/GPL debacle, that ActiveState 
> made a very nice Tcl/Tk distribution with all sorts of 
> "batteries included", but we cannot bundle it with R as they 
> are restricting redistribution.)
> 
> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: 
> (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: 
> (+45) 35327907
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From phgrosjean at sciviews.org  Fri Nov 19 12:17:32 2004
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Fri, 19 Nov 2004 12:17:32 +0100
Subject: [R] The hidden costs of GPL software?
In-Reply-To: <x24qjmazhn.fsf@biostat.ku.dk>
Message-ID: <200411191117.iAJBHXLF005650@outmx007.isp.belgacom.be>

 
Peter Dalgaard wrote:
> [...]
> Strategically, that might actually be a valid (and valiant) 
> design goal! From my limited experience with Rcmdr and SAS 
> Analyst, I'd say that Rcmdr is almost there, just a few 
> little niggles like not remembering values from the last time 
> a form was filled in.

Humm, in my view, this is a feature! The first time the user calls the form,
it creates the corresponding R script. If the user needs to make
corrections, or to run the analysis a second time, he is now supposed to
work with this script! By not remembering last values in a dialog box, Rcmdr
makes the script edition more obvious than recalling the dialog box
again,... even for lazy people! Otherwise, you are sure that those lazy
people will not switch to script: they will call the same dialog box again
and again.

Of course, one has to explain to the students that this is a feature.
Otherwise, it may look like a bad design, especially in comparison with
other "better designed" GUIs!

Best,

Philippe Grosjean

> [...]



From andy_liaw at merck.com  Fri Nov 19 12:39:18 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 19 Nov 2004 06:39:18 -0500
Subject: [R] Tools for data preparation?
Message-ID: <3A822319EB35174CA3714066D590DCD50994E347@usrymx25.merck.com>

My choices are (in the order of my preference):

- use connections and readLines()/strsplit()/etc. in R to process the file a
chunk at a time

- use cut/paste/grep/etc., perhaps within pipe() in R

- use awk, perhaps within pipe() in R

- Python is my last resort, as I'm not familiar with it

The first preference is to do it all in R, mostly for the reason that I can
keep track of what was done all in one place (the R script or function).

Andy

> From: David Mitchell
> 
> Hello list,
> 
> I'm regularly in the position where I have to do a lot of data
> manipulation, in order to get the data I have into a format R is happy
> with.  This manipulation would generally be in one of two forms:
> - getting data from e.g. text log files into a tabular format
> - extracting sensible sample data from a very large data set (i.e. too
> large for R to handle)
> 
> In general, I use Perl or Python to do the task; I'm curious as to
> what others use when they hit the same problem.
> 
> Regards
> 
> Dave Mitchell
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From p.dalgaard at biostat.ku.dk  Fri Nov 19 13:13:36 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 19 Nov 2004 13:13:36 +0100
Subject: [R] The hidden costs of GPL software?
In-Reply-To: <200411191103.iAJB3Cf0022376@outmx017.isp.belgacom.be>
References: <200411191103.iAJB3Cf0022376@outmx017.isp.belgacom.be>
Message-ID: <x2u0rm9gun.fsf@biostat.ku.dk>

"Philippe Grosjean" <phgrosjean at sciviews.org> writes:

> Peter,
> 
> You don't need the ActiveState Tcl distribution to add extensions. If you
> compile extensions yourself (and these extensions have a compatible
> license), then you have no problems... (well, almost! You must make sure
> those extensions compile correctly on all supproted platforms). This is
> exactly what I do in the tcltk2 package.
> Best,
> 
> Philippe Grosjean 

I know, it's just that it feels silly that we cannot build on the fine
work of ActiveState.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Mike.Prager at noaa.gov  Fri Nov 19 13:57:27 2004
From: Mike.Prager at noaa.gov (Mike Prager)
Date: Fri, 19 Nov 2004 07:57:27 -0500
Subject: [R] The hidden costs of GPL software?
In-Reply-To: <20041119024548.EUZS1919.tomts22-srv.bellnexxia.net@JohnDes
	ktop8300>
References: <419B2CB4.7010708@srres.com>
	<20041119024548.EUZS1919.tomts22-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <6.1.2.0.0.20041119075123.0198bd20@hermes.nos.noaa.gov>

At 09:45 PM 11/18/2004, John Fox wrote:
[...]
>6) As has been pointed out, e.g., by Duncan Murdoch, solving the
>function-locating problem is best done by a method or methods that
>automatically accommodate the growing and changing set of contributed
>packages on CRAN.  Why not, as previously has been proposed, replace the
>current static (and, in my view, not very useful) set of keywords in R
>documentation with the requirement that package authors supply their own
>keywords for each documented object? I believe that this is the intent of
>the concept entries in Rd files, but their use certainly is not required or
>even actively encouraged. (They're just mentioned in passing in the Writing
>R Extensions manual.) [...]

That should prove extremely helpful.  Would it be practical to have the 
complete index installed with base R?  That would help identify useful 
packages not yet in a user's installation.


-- 
Michael Prager, Ph.D.                <Mike.Prager at noaa.gov>
NOAA Beaufort Laboratory
Beaufort, North Carolina  28516
http://shrimp.ccfhrb.noaa.gov/~mprager/
***



From bates at stat.wisc.edu  Fri Nov 19 13:59:49 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 19 Nov 2004 06:59:49 -0600
Subject: [R] Creating logical value from the difference of two absolute
	values
In-Reply-To: <5A80B9B4-39C5-11D9-B2EE-000393B3E9D0@utah.edu>
References: <5.2.1.1.0.20041118191436.00c353d0@biomserv.univ-lyon1.fr>
	<5A80B9B4-39C5-11D9-B2EE-000393B3E9D0@utah.edu>
Message-ID: <419DEE45.3040504@stat.wisc.edu>

Nathan Leon Pace, MD, MStat wrote:
> Hi,
> 
> Using R 2.0.1 on Mac g5 running Mac OS X 10.3.6.
> 
> I would expect that
> 
> abs(.7 - .5) >= abs(.3 - .5) should be returned TRUE.
> 
> Instead
> 
>  > www <- abs(.7 - .5) >= abs(.3 - .5)
>  > www
> [1] FALSE
> 
> Is this a result of floating point or the implementation of abs or 
> something else?

Due to floating point arithmetic in general, not specifically the abs 
function.  The number .5 will have an exact floating point 
representation but .3 and .7 will not.  Making equality comparisons with 
floating point values is always risky.

> In a function I need to compare two absolute values - each being of the 
> form |variable - constant|.

On a Mac I get

 > abs(.7-.5) - abs(.3-.5)
[1] -5.551115e-17

so you need to make a relative comparison, not an absolute comparison. 
You could write the relative comparison using the all.equal function, 
such as

 > v1 <- abs(.7-.5)
 > v2 <- abs(.3-.5)
 > (v1 > v2) || all.equal(v1, v2)
[1] TRUE



From rksh at soc.soton.ac.uk  Fri Nov 19 14:10:36 2004
From: rksh at soc.soton.ac.uk (Robin Hankin)
Date: Fri, 19 Nov 2004 13:10:36 +0000
Subject: [R] [R-pkgs] new package gsl,
	a wrapper for the Gnu Scientific Library
Message-ID: <a06200710bdc39dd2eba3@[139.166.242.29]>

Dear list

R package "gsl" is now on  CRAN.  This  is a wrapper for
the special functions of the Gnu Scientific Library (GSL).  Functions include
elliptic integrals, Airy functions, hypergeometric functions, and so on.
Most functions optionally return error estimates.

This library is a little odd in that documentation is limited to a 
pointer to the GSL reference
manual, and occasional re-productions of tables and figures from 
Abramowitz and Stegun.

It passes R CMD check  under Fedora Linux, and MacOSX 10.3 if the GSL
(version 1.5)  is installed in the default locations.

A windows version is available at  http://www.stats.ox.ac.uk/pub/RWin/2.0.0/



(also, a big Thank You to Kurt Hornik for advice on configure.ac)



-- 
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
SO14 3ZH
tel +44(0)23-8059-7743
initialDOTsurname at soc.soton.ac.uk (edit in obvious way; spam precaution)

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From LosierR at mar.dfo-mpo.gc.ca  Fri Nov 19 14:28:38 2004
From: LosierR at mar.dfo-mpo.gc.ca (Losier, Randy J)
Date: Fri, 19 Nov 2004 09:28:38 -0400
Subject: [R] Legend
Message-ID: <E37EEC6DE3A0C5439B7E7B07406C24AE11BDDC@msgmarsta01.bio.dfo.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041119/6ab8bacc/attachment.pl

From Bernhard.Pfaff at drkw.com  Fri Nov 19 14:42:49 2004
From: Bernhard.Pfaff at drkw.com (Pfaff, Bernhard)
Date: Fri, 19 Nov 2004 14:42:49 +0100
Subject: [R] The hidden costs of GPL software?
Message-ID: <29E0BC0C716A584582941615CF9FFB0902585D39@ibfftce107.de.ad.drkw.net>

Dear list member,

this thread as well as the first one started by Philippe about the
usefulness of a GUI is interesting and overwhelming alike. IMHO, it
wittnesses the greatness and superiority of R compared to other statistical
programming environments and programs: the core team and all people involved
with it. Everyday I am flabbergasted and amazed anew; learning new concepts,
programming tricks, statistical methods I was unfamiliar with and much much
more: kudos to all of you. Let me now toss in my two cents:

First cent: Comments about a GUI
Although I use Emacs/ESS and R batch mode mostly, a GUI is beneficial in
terms of teaching statistics and/or econometrics with R. This conclusion
draws upon experience nine years back, while I was giving, beside
econometric classes, computer labs at university. At that time we had only
commercial products at hand: RATS, GAUSS and EViews. From a students
perspective EViews (menu-driven) was the most convenient one. The
econometric method comprehension and the interpretation of its application
is of utmost importance. Hence, novices should concentrate on this to
familiarise themselves with the subject. Most of the students got scared and
distracted by learning a command driven programming language too, i.e. this
was too much to swallow at one time. With other words: do not challenge
novices to statistics/econometrics programmatically. A point mentioned by
Phillipe in an earlier email too. 
Now given Rcmdr: I like its flexibility and that everybody can tailormade
his own `version' by adding new menus and functionalities. So to speak, a
very decent ground work has been provided by John Fox and I appreciate it
alot. I can only speak for myself: I am currently writing an `urca' add-in
to Rcmdr, such that the package is more amenable to novice users in a
computer lab for example; that is: they do not have to worry about the
correct syntax or what can be achieved with which function. In order to do
so, two files are needed: one is an addendum to the menu's file and the
other one contains the R functions to be executed within Rcmdr. It is at the
leisure of the instructure to include these into Rcmdr. They can be shipped
in the package's /inst directory, for example. This seems to be a feasible
approach for other package maintainers working in different fields too. Or
would such an approach be to simple?

Second cent: help system
As voiced in earlier emails in this thread the R documentation, contributed
tutorials and the likes as well as the help facilities are indeed great. The
only snag, is a lack of an `easy to find' approach to be taken. Surely there
is help.search(), apropos, help.start() etc. etc. But what would be nice,
would be something similar to `texdoctk' for LaTeX document retrieval. That
is: categorise the manuals, package manuals, vignettes and other contributed
docs with respect to the catergory they belong to. Well, the snag is: who
does this labour intensive work? Hm, I am sceptic, but it might turn out
that this is not a feasible approach to be taken, but maybe my second
suggestion is: making greater use of \concepts and/or \keyword by providing
a file for download on CRAN that contains the \concepts entries & the
function & the package in which it is contained. One could then download
this file and execute a `zgrep' on it, as could be done likewise with a
contents file from an apt repository to find out which file is contained in
which rpm. The advantage would be the decentralisation of the work. It does
not take that long when each package maintainer utilises \concepts in his
.Rd files. Once, a package is contributed to CRAN, one could scan the
tarballs and extract the relevant information into the above mentioned file.
Another advantage would be, that users would find functions of packages that
are currently not in their search path, because the packages have not been
installed. And not a few questions on this list are answered by: `This
functionality is contained in package xyz'. Anyway, I will introduce
\concepts within the next release of `urca'.

Best Regards,
Bernhard   

> 
> "Philippe Grosjean" <phgrosjean at sciviews.org> writes:
> 
> > Peter,
> > 
> > You don't need the ActiveState Tcl distribution to add 
> extensions. If you
> > compile extensions yourself (and these extensions have a compatible
> > license), then you have no problems... (well, almost! You 
> must make sure
> > those extensions compile correctly on all supproted 
> platforms). This is
> > exactly what I do in the tcltk2 package.
> > Best,
> > 
> > Philippe Grosjean 
> 
> I know, it's just that it feels silly that we cannot build on the fine
> work of ActiveState.
> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: 
> (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: 
> (+45) 35327907
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html


--------------------------------------------------------------------------------
The information contained herein is confidential and is inte...{{dropped}}



From michael.watson at bbsrc.ac.uk  Fri Nov 19 15:06:19 2004
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Fri, 19 Nov 2004 14:06:19 -0000
Subject: [R] Building package on Linux and Windows
Message-ID: <8975119BCD0AC5419D61A9CF1A923E95E898A5@iahce2knas1.iah.bbsrc.reserved>

Hi

OK, final stretch now, thank you to everyone who has helped!

(hopefully) final question - what does R CMD build actually do that
tar/gzip and WinZip do not?

I have successfully used R CMD build to create a .tar.gz version of my
package, which installs well and works under Linux - hurray!  I naively
thought that all I had to do now was create a .zip version (using
something like WinZip) and it would work on Windows too - wrong!

On Windows I get:

Error in file(file, "r") : unable to open connection
In addition: Warning messages: 
1: error -1 in extracting from zip file 
2: cannot open file `coXpress/DESCRIPTION' 

I didn't run R CMD build on Windows because I don't have "suitable
versions of Unix tools including make, sh, rm, sed, awk,mkdir, echo, cp
and cat; we have packaged a set at
http://www.murdoch-sutherland.com/Rtools/tools.zip.".  Do I really have
to install all of these on my Windows machine to build a package?  

Asked another way - I have a linux .tar.gz file which works, how do I
make a windows version?

Thanks in advance!

Mick



From michael.watson at bbsrc.ac.uk  Fri Nov 19 15:18:58 2004
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Fri, 19 Nov 2004 14:18:58 -0000
Subject: [R] Building package on Linux and Windows
Message-ID: <8975119BCD0AC5419D61A9CF1A923E95E898A6@iahce2knas1.iah.bbsrc.reserved>

No need, got it now :-)

Thanks
M

-----Original Message-----
From: michael watson (IAH-C) 
Sent: 19 November 2004 14:06
To: r-help at stat.math.ethz.ch
Subject: [R] Building package on Linux and Windows


Hi

OK, final stretch now, thank you to everyone who has helped!

(hopefully) final question - what does R CMD build actually do that
tar/gzip and WinZip do not?

I have successfully used R CMD build to create a .tar.gz version of my
package, which installs well and works under Linux - hurray!  I naively
thought that all I had to do now was create a .zip version (using
something like WinZip) and it would work on Windows too - wrong!

On Windows I get:

Error in file(file, "r") : unable to open connection
In addition: Warning messages: 
1: error -1 in extracting from zip file 
2: cannot open file `coXpress/DESCRIPTION' 

I didn't run R CMD build on Windows because I don't have "suitable
versions of Unix tools including make, sh, rm, sed, awk,mkdir, echo, cp
and cat; we have packaged a set at
http://www.murdoch-sutherland.com/Rtools/tools.zip.".  Do I really have
to install all of these on my Windows machine to build a package?  

Asked another way - I have a linux .tar.gz file which works, how do I
make a windows version?

Thanks in advance!

Mick

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From valeska at fiocruz.br  Fri Nov 19 16:58:04 2004
From: valeska at fiocruz.br (Valeska Andreozzi)
Date: Fri, 19 Nov 2004 12:58:04 -0300
Subject: [R] glm with Newton Raphson
Message-ID: <419E180C.7000408@fiocruz.br>

Hi,

Does anyone know if there is a function to find the maximum likelihood 
estimates of glm using Newton Raphson metodology instead of using IWLS.

Thanks
Valeska Andreozzi

--------------------------------------------------------
Department of Epidemiology and Quantitative Methods
FIOCRUZ - National School of Public Health
Tel: (55) 21 2598 2872
Rio de Janeiro - Brazil



From kheom at iastate.edu  Fri Nov 19 15:55:40 2004
From: kheom at iastate.edu (Kihong Eom)
Date: Fri, 19 Nov 2004 08:55:40 -0600
Subject: [R] Question on panel corrected standard errors by Beck and Katz
Message-ID: <200411191455.iAJEtfJ8006725@mailhub-5.iastate.edu>

Dear colleagues, 

I am looking for a program which can run OLS with panel corrected standard
errors developed by Beck and Katz.  Thanks for your help in advance.

Kihong Eom, Ph.D.
539 Ross Hall
Iowa State University
Ames, IA 50011



From adrian.alexa at gmail.com  Fri Nov 19 15:59:15 2004
From: adrian.alexa at gmail.com (Adrian Alexa)
Date: Fri, 19 Nov 2004 15:59:15 +0100
Subject: [R] accessing the attributes of a list inside lapply()
Message-ID: <4062eb8d0411190659385e615d@mail.gmail.com>

Hello R-users, 


I have the following problem, that I want to solve efficiently:


I have a named list, for example:

> l <- list(a = 1, b = 3, c = 'asd')
> l
$a
[1] 1

$b
[1] 3

$c
[1] "asd"


I know that I can iterate through it using lapply() function, but I
would also like to able to get the list names or some attributes of l
in the lapply(). For example if I use names() function in the call of
lapply() I get:


> lapply(l, names)
$a
NULL

$b
NULL

$c
NULL
 

My question is if I can get something like:


> lapply(l, get_attr)
$a
[1] a

$b
[1] b

$c
[1] c
 

I can do this very easy with a for() loop but my list is quite big and
I would like to get a decent running time. I don't need only the
attributes of the list(I can obtain them by using attributes() or
attr() function), but for my list the names of the elements are given
me information that I need. Also I must mention that the elements of
the list can by of any type.


Any solution is welcome.

 

Many thanks, 

Adrian 




-- 
Adrian Alexa             
Max-Planck-Institut fuer Informatik  
Stuhlsatzenhausweg 85 Room 514 
66123 Saarbruecken, Germany



From Amer.Siddique at ssa.gov  Fri Nov 19 16:18:50 2004
From: Amer.Siddique at ssa.gov (Siddique, Amer)
Date: Fri, 19 Nov 2004 10:18:50 -0500
Subject: [R] Re: The hidden costs of GPL software? 
Message-ID: <03EC0EF2887E0C4D84748557F82F2E5B068126C7@sdf0cd1.ph.ssa.gov>

my quick thought:
R is a programming language and shouldn't be wrapped up in a GUI to serve
the interests of those too complacent to learn to leverage its power. 

and to echo others: I feel an IDE approach with, say, a code editor and a
hyperlinked help system with a richer set of examples is sufficient. we
already have the former. cheers



From rpeng at jhsph.edu  Fri Nov 19 16:22:10 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Fri, 19 Nov 2004 10:22:10 -0500
Subject: [R] glm with Newton Raphson
In-Reply-To: <419E180C.7000408@fiocruz.br>
References: <419E180C.7000408@fiocruz.br>
Message-ID: <419E0FA2.3000906@jhsph.edu>

There are some examples of how to approach this in Modern Applied 
Statistics with S, 4th ed. (chap. 16) by Venebles & Ripley.  It's not 
Newton-Raphson but I think the code can be adapted.

-roger

Valeska Andreozzi wrote:
> Hi,
> 
> Does anyone know if there is a function to find the maximum likelihood 
> estimates of glm using Newton Raphson metodology instead of using IWLS.
> 
> Thanks
> Valeska Andreozzi
> 
> --------------------------------------------------------
> Department of Epidemiology and Quantitative Methods
> FIOCRUZ - National School of Public Health
> Tel: (55) 21 2598 2872
> Rio de Janeiro - Brazil
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From kahra at mpsgr.it  Fri Nov 19 16:28:20 2004
From: kahra at mpsgr.it (Kahra Hannu)
Date: Fri, 19 Nov 2004 16:28:20 +0100
Subject: [R] gibbs sampling for mixture of normals
Message-ID: <C9FC71F7E9356F40AFE2ACC2099DE14722517F@MAILSERVER-B.mpsgr.it>

Matteo,

have a look at http://www.mrc-bsu.cam.ac.uk/bugs/faqs/contents.shtml and
BUGS http://www.mrc-bsu.cam.ac.uk/bugs/welcome.shtml.

Hannu Kahra 
Progetti Speciali 
Monte Paschi Asset Management SGR S.p.A. 
Via San Vittore, 37
IT-20123 Milano, Italia 

Tel.: +39 02 43828 754 
Mobile: +39 333 876 1558 
Fax: +39 02 43828 247 
E-mail: kahra at mpsgr.it 
Web: www.mpsam.it 



-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of matteo ruggiero
Sent: Thursday, November 18, 2004 7:42 PM
To: r-help at stat.math.ethz.ch
Subject: [R] gibbs sampling for mixture of normals


hi

i'm looking for a gibbs sampling algorithm for R for the case of mixture of K
normals, and in particular for the case of bivariate normals.
i'd be grateful if anyone could send its own R-routine, at least for the
univariate case.

thank you in advance
matteo

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Fri Nov 19 16:28:00 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 19 Nov 2004 15:28:00 +0000 (GMT)
Subject: [R] glm with Newton Raphson
In-Reply-To: <419E180C.7000408@fiocruz.br>
References: <419E180C.7000408@fiocruz.br>
Message-ID: <Pine.LNX.4.61.0411191524240.12516@gannet.stats>

For canonical links they are the same thing (and the likelihood is 
log-concave), but in general NR is a poor optimizer without at least 
step-length adjustment.

MASS4 p.445 has an example of using a general optimizer for logistic 
regression.

On Fri, 19 Nov 2004, Valeska Andreozzi wrote:

> Does anyone know if there is a function to find the maximum likelihood 
> estimates of glm using Newton Raphson metodology instead of using IWLS.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jfox at mcmaster.ca  Fri Nov 19 16:33:43 2004
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 19 Nov 2004 10:33:43 -0500
Subject: [R] The hidden costs of GPL software?
In-Reply-To: <200411190828.iAJ8SiUS016426@outmx018.isp.belgacom.be>
Message-ID: <20041119153342.LFLS1836.tomts16-srv.bellnexxia.net@JohnDesktop8300>

Dear Philippe,

I was aware of your tcltk2 package and will likely use it (if the standard
widget set distributed with R for Windows is not expanded) when it becomes
cross-platform.

Thanks for this.
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: Philippe Grosjean [mailto:phgrosjean at sciviews.org] 
> Sent: Friday, November 19, 2004 3:29 AM
> To: 'John Fox'; r-help at stat.math.ethz.ch
> Subject: RE: [R] The hidden costs of GPL software?
> 
> John Fox wrote:
> > [...] (sorry, this is long mail, and I want to comment only 
> details) 
> > By the way, if there were something I could wish for here 
> it would be 
> > a slightly broader set of Tk widgets to be included with the Tcl/Tk 
> > that installs with R for Windows, since using widgets 
> outside of this 
> > set creates installation obstacles for lower-level users.
> 
> Then, take a look at the tcltk2 package in the SciViews 
> bundle (probably, in the next version, I will take it out of 
> the bundle). You have there tile (themable widgets with 
> notebook tabs, progress bar, and many more... and very soon 
> combo boxes and lists/trees). You have also the famous 
> tkTable, and a separate combobox and a tree, and a support 
> for tooltips everywhere...
> Just propose if you need more! All this runs under Windows, 
> but I still got problems to compile it under other platforms.
> 
> > I doubt that many list
> > members would look favourably on the statistical-methods "decision 
> > tree" in MicrOsiris, for example. One solution is to include PDF 
> > "manuals" with packages. I've done this, for example, with 
> my effects 
> > and Rcmdr packages.
> > The introductory manual supplied with Thomas Lumley's 
> survey package 
> > is another, similar example. Maybe there's a better way of 
> integrating 
> > such non-vignette manuals with the help system -- something like 
> > help(manual=package).
> 
> I tend to have the same opinion than John (although I thing 
> that both a good manual, and a better online help could be 
> beneficial): a PDF manual is much more readable than a wiki! 
> Why not to propose PDF manuals in the \doc section of CRAN 
> which have a GNU Free Documentation License 
> (http://www.gnu.org/copyleft/fdl.html), so that the manual 
> could be progressively enhanced by many authors?
> 
> Best,
> 
> Philippe Grosjean
> 
>



From jfox at mcmaster.ca  Fri Nov 19 16:40:53 2004
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 19 Nov 2004 10:40:53 -0500
Subject: [R] The hidden costs of GPL software?
In-Reply-To: <x24qjmazhn.fsf@biostat.ku.dk>
Message-ID: <20041119154052.USCD25979.tomts25-srv.bellnexxia.net@JohnDesktop8300>

Dear Peter,

> -----Original Message-----
> From: Peter Dalgaard [mailto:p.dalgaard at biostat.ku.dk] 
> Sent: Friday, November 19, 2004 5:46 AM
> To: John Fox
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] The hidden costs of GPL software?
> 
> "John Fox" <jfox at mcmaster.ca> writes:
> 
> > I don't think that
> > it would be hard (although it would be time-consuming) to produce a 
> > much broader extension, but the result (in my opinion) would be as 
> > dubiously useful as the GUIs for SAS or S-PLUS.
> 
> Strategically, that might actually be a valid (and valiant) 
> design goal! From my limited experience with Rcmdr and SAS 
> Analyst, I'd say that Rcmdr is almost there, just a few 
> little niggles like not remembering values from the last time 
> a form was filled in.
> 

I've thought about remembering dialog values, but I guess so far I've been
too lazy to do it or, to put a better construction on it, too distracted by
other things. It shouldn't be hard to do -- just a matter of maintaining a
data base of previous entries that are flushed when the active data set
changes. Actually, the linear-model and generalized-linear-model dialogs
already do this.

I'd be interested in the other little niggles as well.

> > By the way, if there were something I
> > could wish for here it would be a slightly broader set of 
> Tk widgets 
> > to be included with the Tcl/Tk that installs with R for 
> Windows, since 
> > using widgets outside of this set creates installation 
> obstacles for 
> > lower-level users.
> 
> Argh. Please stop poking at my guilty conscience.... Wrapping 
> Tcl/Tk extensions as R packages has been on my wish list too 
> for some time, with tktable as the obvious first candidate. 
> (It's not just on Windows; the default Unix/Linux installs of 
> Tcl/Tk tend to be pretty minimal too. On Windows we have this 
> instructive twist on the BSD/GPL debacle, that ActiveState 
> made a very nice Tcl/Tk distribution with all sorts of 
> "batteries included", but we cannot bundle it with R as they 
> are restricting redistribution.)
> 

I certainly don't want to press this issue, since I'm grateful for what
you've already done (and it's surprising how much mileage one can get from
the basic widget set). I see this as primarily a Windows problem because
users of other computing platforms (possibly with the exception of some Mac
users) tend to be more sophisticated about installing software.

Regards,
 John

> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: 
> (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: 
> (+45) 35327907



From a.quandt at web.de  Fri Nov 19 16:51:55 2004
From: a.quandt at web.de (Andreas Quandt)
Date: Fri, 19 Nov 2004 16:51:55 +0100
Subject: [R] problem with usepackage{} in Sweave under Quantian
Message-ID: <419E169B.9010908@web.de>

dear expeRts,

if i try to create a .tex file under Quantian 6.9.1 with sweave the 
command \usepackage{.../texmf/Sweave} will not added and the following 
compilation to a .pdf file failed. if i add the \usepackage line by 
myself all is going right. so what i am doing wrong because under mac os 
x 10.3 the \usepackage{} line is added automatically by sweave? In both 
cases i used xemacs/ess and R (all in the newest versions) to create the 
files.
thanks in advance for helping.

best wishes
andreas



From myzhaogong at yahoo.com  Fri Nov 19 16:55:18 2004
From: myzhaogong at yahoo.com (an ying)
Date: Fri, 19 Nov 2004 07:55:18 -0800 (PST)
Subject: [R] help! a urgent question
Message-ID: <20041119155518.10269.qmail@web53108.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041119/9e8d0332/attachment.pl

From ll9f at tetra.mail.virginia.edu  Fri Nov 19 17:01:53 2004
From: ll9f at tetra.mail.virginia.edu (Lei  Liu)
Date: Fri, 19 Nov 2004 11:01:53 -0500
Subject: [R] function 'vcov' for coxph in R 2.0.0
Message-ID: <web-98002552@cgatepro-4.mail.virginia.edu>

Hi there,

After I fitted a cox model, I used vcov to obtain the 
variance of the parameter estimate. It worked correctly in 
R 1.9.1. But it failed in R 2.0.0 and the error message is 

Error in vcov(cox.1) : no applicable method for "vcov"

I don't know if it is a bug or there is some update on 
this function. Thanks!

Lei Liu
Assistant Professor
Division of Biostatistics and Epidemiology
Dept. of Health Evaluation Sciences
School of Medicine
University of Virginia

3181 Hospital West Complex
Charlottesville, VA 22908-0717

1-434-982-3364 (o)
1-734-730-1395 (c)

liulei at virginia.edu
ll9f at virginia.edu



From spencer.graves at pdf.com  Fri Nov 19 17:06:11 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 19 Nov 2004 08:06:11 -0800
Subject: [R] The hidden costs of GPL software?
In-Reply-To: <x24qjmazhn.fsf@biostat.ku.dk>
References: <20041119024548.EUZS1919.tomts22-srv.bellnexxia.net@JohnDesktop8300>
	<x24qjmazhn.fsf@biostat.ku.dk>
Message-ID: <419E19F3.9060900@pdf.com>

My very dear Prof. Dalgaard:   

Peter Dalgaard wrote:

>Argh. Please stop poking at my guilty conscience.... Wrapping Tcl/Tk
>extensions as R packages has been on my wish list too for some time,
>...
>
      You, of all people, should hardly have a guilty conscience about 
not doing enough on R!  I, and many others, are continually awed by the 
accomplishments of you and the rest of the R Core Team. 

      spencer graves

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From lecoutre at stat.ucl.ac.be  Fri Nov 19 17:06:57 2004
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Fri, 19 Nov 2004 17:06:57 +0100
Subject: [R] accessing the attributes of a list inside lapply()
In-Reply-To: <4062eb8d0411190659385e615d@mail.gmail.com>
References: <4062eb8d0411190659385e615d@mail.gmail.com>
Message-ID: <6.0.1.1.2.20041119170618.03af89b0@stat4ux.stat.ucl.ac.be>


Hi,

What about:

 >  as.list(names(l))
[[1]]
[1] "a"

[[2]]
[1] "b"

[[3]]
[1] "c"


HTH,

Eric


At 15:59 19/11/2004, Adrian Alexa wrote:
>Hello R-users,
>
>
>I have the following problem, that I want to solve efficiently:
>
>
>I have a named list, for example:
>
> > l <- list(a = 1, b = 3, c = 'asd')
> > l
>$a
>[1] 1
>
>$b
>[1] 3
>
>$c
>[1] "asd"
>
>
>I know that I can iterate through it using lapply() function, but I
>would also like to able to get the list names or some attributes of l
>in the lapply(). For example if I use names() function in the call of
>lapply() I get:
>
>
> > lapply(l, names)
>$a
>NULL
>
>$b
>NULL
>
>$c
>NULL
>
>
>My question is if I can get something like:
>
>
> > lapply(l, get_attr)
>$a
>[1] a
>
>$b
>[1] b
>
>$c
>[1] c
>
>
>I can do this very easy with a for() loop but my list is quite big and
>I would like to get a decent running time. I don't need only the
>attributes of the list(I can obtain them by using attributes() or
>attr() function), but for my list the names of the elements are given
>me information that I need. Also I must mention that the elements of
>the list can by of any type.
>
>
>Any solution is welcome.
>
>
>
>Many thanks,
>
>Adrian
>
>
>
>
>--
>Adrian Alexa
>Max-Planck-Institut fuer Informatik
>Stuhlsatzenhausweg 85 Room 514
>66123 Saarbruecken, Germany
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Fri Nov 19 17:16:41 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 19 Nov 2004 08:16:41 -0800
Subject: [R] accessing the attributes of a list inside lapply()
In-Reply-To: <4062eb8d0411190659385e615d@mail.gmail.com>
References: <4062eb8d0411190659385e615d@mail.gmail.com>
Message-ID: <419E1C69.1020902@pdf.com>

      Maybe I misunderstand your problem, but I wonder if you've 
considered "names": 

 > l <- list(a = 1, b = 3, c = 'asd')
 > names(l)
[1] "a" "b" "c"    

      hope this helps.  spencer graves
p.s.  I can't parse your "lapply(l, get_attr)".  In R 1.9.1 and R 2.0.0 
Patched, I get the following: 

 > lapply(l, get_attr)
Error in match.fun(FUN) : Object "get_attr" not found

Adrian Alexa wrote:

>Hello R-users, 
>
>
>I have the following problem, that I want to solve efficiently:
>
>
>I have a named list, for example:
>
>  
>
>>l <- list(a = 1, b = 3, c = 'asd')
>>l
>>    
>>
>$a
>[1] 1
>
>$b
>[1] 3
>
>$c
>[1] "asd"
>
>
>I know that I can iterate through it using lapply() function, but I
>would also like to able to get the list names or some attributes of l
>in the lapply(). For example if I use names() function in the call of
>lapply() I get:
>
>
>  
>
>>lapply(l, names)
>>    
>>
>$a
>NULL
>
>$b
>NULL
>
>$c
>NULL
> 
>
>My question is if I can get something like:
>
>
>  
>
>>lapply(l, get_attr)
>>    
>>
>$a
>[1] a
>
>$b
>[1] b
>
>$c
>[1] c
> 
>
>I can do this very easy with a for() loop but my list is quite big and
>I would like to get a decent running time. I don't need only the
>attributes of the list(I can obtain them by using attributes() or
>attr() function), but for my list the names of the elements are given
>me information that I need. Also I must mention that the elements of
>the list can by of any type.
>
>
>Any solution is welcome.
>
> 
>
>Many thanks, 
>
>Adrian 
>
>
>
>
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From partha_bagchi at hgsi.com  Fri Nov 19 17:08:34 2004
From: partha_bagchi at hgsi.com (partha_bagchi@hgsi.com)
Date: Fri, 19 Nov 2004 11:08:34 -0500
Subject: [R] 3d Map with bars
Message-ID: <OFB67FA518.7A81C6D6-ON85256F51.0057F2A7-85256F51.0058AE2B@hgsi.com>

Apologies in advance for the question. I am trying to draw a map of the US 
as a surface plot so that I would be able to drop bars on the different 
states (something like Uwe Ligges' scatterplot3d example 4). I am not sure 
where to start looking for such a beast. If anyone has any pointers, 
ideas, I will be grateful.

TIA,
Partha



From jfox at mcmaster.ca  Fri Nov 19 17:46:57 2004
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 19 Nov 2004 11:46:57 -0500
Subject: [R] glm with Newton Raphson
In-Reply-To: <419E0FA2.3000906@jhsph.edu>
Message-ID: <20041119164655.KBBT1863.tomts10-srv.bellnexxia.net@JohnDesktop8300>

Dear Roger and Valeska,

I have example that uses Newton-Raphson for logistic regression in my R and
S-PLUS Companion that might be of help. It's in the chapter script at
<http://socserv.socsci.mcmaster.ca/jfox/Books/Companion/Ch8-script.txt> (the
function lreg). See also the notes at
<http://socserv.socsci.mcmaster.ca/jfox/Courses/S-course/Topic-4.pdf>.

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Roger D. Peng
> Sent: Friday, November 19, 2004 10:22 AM
> To: Valeska Andreozzi
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] glm with Newton Raphson
> 
> There are some examples of how to approach this in Modern 
> Applied Statistics with S, 4th ed. (chap. 16) by Venebles & 
> Ripley.  It's not Newton-Raphson but I think the code can be adapted.
> 
> -roger
> 
> Valeska Andreozzi wrote:
> > Hi,
> > 
> > Does anyone know if there is a function to find the maximum 
> likelihood 
> > estimates of glm using Newton Raphson metodology instead of 
> using IWLS.
> > 
> > Thanks
> > Valeska Andreozzi
> > 
> > --------------------------------------------------------
> > Department of Epidemiology and Quantitative Methods FIOCRUZ 
> - National 
> > School of Public Health
> > Tel: (55) 21 2598 2872
> > Rio de Janeiro - Brazil
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> 
> --
> Roger D. Peng
> http://www.biostat.jhsph.edu/~rpeng/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From Karen.Richardson at insightful.com  Fri Nov 19 17:47:11 2004
From: Karen.Richardson at insightful.com (Karen Richardson)
Date: Fri, 19 Nov 2004 16:47:11 -0000
Subject: [R] COURSE: Statistical Modelling - By Dr Bill Venables
Message-ID: <B796B8C05975394DA24E457D1985BDB42F0B85@uk2kexch01.insightful.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041119/9a53ac6f/attachment.pl

From mwgrant2001 at yahoo.com  Fri Nov 19 17:48:17 2004
From: mwgrant2001 at yahoo.com (Michael Grant)
Date: Fri, 19 Nov 2004 08:48:17 -0800 (PST)
Subject: [R] The hidden costs of GPL software?
In-Reply-To: <20041119153342.LFLS1836.tomts16-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <20041119164817.64244.qmail@web54409.mail.yahoo.com>

I inadvertently directed this response to the R-Gui
list this morning. To those receiving a double
receipt, I give my apologies. My intended list was the
R list. 

--- Duncan Murdoch <murdoch at stats.uwo.ca> wrote:

> On Thu, 18 Nov 2004 03:24:01 -0800 (PST), Michael
> Grant
> <mwgrant2001 at yahoo.com> wrote:
> 
> >Hmmmm, interesting thread and minds will not be
> >changed but regarding GUIs...I thought S (aka R)

...

> 
> I have to disagree with you.  What you say might be
> true about *bad*
> GUIs, but I find nothing more frustrating than the
> lack of programming
> support in R.
>
> What's a nice GUI for programming?
> 
> You should be able to edit code, and have R parse
> the code that you
> are editing
...
[snip] [snip] [snip]
... 
> All of these things have existed for years in IDEs
> (i.e. programming
> GUIs), but most are not in R's GUIs.  

I guess we'll just agree to disagree. :O)
1.)The LACK of programming support? Isn't that a bit
of an overstatement? There are materials available, as
of ciurse you are aware. At one time or another many
of us may find it difficult to determine some 'key'
programming information at the moment. But you know
something, I've  had that happen using the packages
like you describe--this includes wired IDE help,
original documentation, and 3rd party books. I accept
that as a condition for using both free and commercial
software. And if the particular burden is too great,
then I don't use the product. Such is life :O)

2.)As you indicate below, R doesn't not have a VB or
VC++ style IDE. R doesn't have the development
environment of Smalltalk or the commercial LISPs
(sigh...) But, really, an IDE is a bit more than a
GUI, wouldn't you agree? A GUI is just one component
of an IDE.

Perhaps part of our difference is how we view
programming. I view it more as a form of expression
using a LANGUAGE. Like any language, e.g., English,
French, Chinese, you have to develop a degree of
fluency to express yourself. Some people are
comfortable working with a phrase book and others put
more effort in to learn to converse sans book. Both
approaches are quite legitimate in that either can get
the job done. (And both can fail miserably!) 

>From another perspective, I can not deny that having a
real GUI would be nice at times even for a grump like
myself. And not having such is a cost. But in my case
that cost is not the deciding factor. The fact is, I
by preference do a lot of coding--both at the
quick/dirty scale and the project scale--in R that I
could do in C/C++, FORTRAN, BASIC. I have those tools
in commerical form with IDEs

Why R? The turn around is so fast by comparison. R/S
is language in which I can much more easily and
quickly express myself.  The development team has done
a lot of work developing my high-level language for me
:O). (Note--my second hacking language is  lisp-stat,
also an interpreted, higher functionality language.) I
don't use most of R's capabilities, and 'not knowing
that which I do not know' is not an issue. When I need
something new I am able to learn it incrementally on
top of what I already know.
...
> 
> That's one sort of GUI that R could have, but it's
> not the only one,
> and it's not the one that I'd use.  However, I might
> start out
> students on it.  There's a big benefit to a list of
> suggestions as
> opposed to a big blank space.
Did I suggest banning GUIs? I don't think so. Your
world is one where there are benefit for your
clients--the students. My world is turn around and
documentation. Coding is easier to document than a
complex sequence of menu actions. Indeed I would get
laughed out of Dodge City if I documented a set of
calculations: " next I clicked ...". It's that just
different requirements lead to different needs.

> 
> >GUIs encourage a passive approach to using
> computers
> >when solving problems. In addition, it is
> regretable
...
[snip]
...
> >gather 'electronic dust'.
> 
> A lot of people do incomplete or incorrect work
> because they don't
> know any better.  It doesn't matter if they're using
> a GUI or not,
> they'll do what they think they know, and get it
> wrong.

Of course that is the case, but the limitations in a
given GUI is one more thing that puts such people in
rationalized comfort-zone with their actions.
(Typically I see this with EXCEL apps--99.9% of the
people in my trade run away from statistical
software.)
More than once I have seen this occur in a senior
scientist review capacity after management has seen
the product and 'accepted' its results. Doom, doom,
doom...shoot the messenger! Oh woe, oh woe!:O(.

Best regards, Duncan
Michael



From gregor.gorjanc at bfro.uni-lj.si  Fri Nov 19 18:48:22 2004
From: gregor.gorjanc at bfro.uni-lj.si (Gregor GORJANC)
Date: Fri, 19 Nov 2004 17:48:22 +0000
Subject: [R] Plotting averages of y per x 
Message-ID: <419E31E6.3040302@bfro.uni-lj.si>

Hello!

I often plot average of y instead of all y values, since one can easily 
see the trend if there is to many points and/or x might be like 1, 2, 3, 
4, 5, ... and you might get a cloud (which can also be informative) 
and/or columns of points.

Anyway, learning with R i get stucked. I have the data in table 
burnins1, with the following names:

names(burnins1)
[1] "Model"       "Replication" "h2"          "burnin"

and some data
...
...
380    New          80  2     23
381    New          81  5     38
382    New          82 10     31
383    New          83 15     98
384    New          84 20     32
385    New          85 40     30
386    New          86  1     43
387    New          87  2     53
388    New          88  5     36
389    New          89 10     51
390    New          90 15     19
...
...

So I want to plot mean of variable burnin by variables model and h2. I 
compute means with

tmp <- as.data.frame(t(tapply(burnin, list(Model, h2), mean)))

and table tmp looks like this

      Old   New
1  31.00 29.36
2  30.30 28.34
5  32.92 30.66
10 39.00 37.54
15 40.66 34.07
20 39.29 35.94
40 28.63 28.51

Now I want to launch something like
plot(tmp[, 0], Old)
points(tmp[, 0], New, pch = 20)

and it gives me the error

Error in pairs.default(x, ...) : non-numeric argument to pairs

I guess that I am having problem with tmp[, 0]. If I use

plot(Old)
points(New, pch = 20)

I get the plot, but abscisa is numbered from 1 to 7 not 1, 2, 5, 10, 15, 
20 and 40. Does anyone have any sugestions?

I would also like to produce table like this, so plotting would be 
easier, since there would be no need to use points() and/or lines() 
commands to add another group.

   Model h2 mean
1  Old  1  31.00
2  Old  2  30.30
3  Old  5  32.92
4  Old  10 39.00
5  Old  15 40.66
6  Old  20 39.29
7  Old  40 28.63
8  Old  1  29.36
9  Old  2  28.34
10 Old  5  30.66
11 Old  10 37.54
12 Old  15 34.07
13 Old  20 35.94
14 Old  40 28.51


-- 
Lep pozdrav / With regards / Con respeto,
     Gregor GORJANC

---------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty       URI: http://www.bfro.uni-lj.si
Zootechnical Department    mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                  tel: +386 (0)1 72 17 861
SI-1230 Domzale            fax: +386 (0)1 72 41 005
Slovenia



From spencer.graves at pdf.com  Fri Nov 19 18:04:13 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 19 Nov 2004 09:04:13 -0800
Subject: [R] help! a urgent question
In-Reply-To: <20041119155518.10269.qmail@web53108.mail.yahoo.com>
References: <20041119155518.10269.qmail@web53108.mail.yahoo.com>
Message-ID: <419E278D.5050906@pdf.com>

  Have you tried "?help.search" in R?

Also, have you tried www.r-project.org -> search -> "R site search"?

Finally, have you tried "the posting guide! 
http://www.R-project.org/posting-guide.html"?

hope this helps. spencer graves

an ying wrote:

>Dear Sir/Madam,
> 
>I am doing a project related to R. 
>However, it is always difficult find some R functions.
>The R user guide seems not complete. 
>Is there any free document about all R functions ?
>Who knows ? please help me.
>My email is myzhaogong at yahoo.com 
> 
> 
>Thank you very much
>
>		
>---------------------------------
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From david.lennartsson at saida-med.com  Fri Nov 19 18:02:40 2004
From: david.lennartsson at saida-med.com (David Lennartsson)
Date: Fri, 19 Nov 2004 18:02:40 +0100
Subject: [R-gui] RE: [R] The hidden costs of GPL software?
In-Reply-To: <200411181017.iAIAHrI5024559@outmx014.isp.belgacom.be>
References: <200411181017.iAIAHrI5024559@outmx014.isp.belgacom.be>
Message-ID: <419E2730.8010103@saida-med.com>

Philippe Grosjean wrote:

>John W. Eaton wrote:
>  
>
>>On 17-Nov-2004, Philippe Grosjean <phgrosjean at sciviews.org> wrote:
>>
>>| - There is no possibility to make a commercial GUI for R (thanks to 
>>| the GPL),
>>
>>This is false.  Please don't confuse "commercial" (Red Hat 
>>and SuSE GNU/Linux distributions are commercial software) 
>>with "proprietary".
>>
>>jwe
>>    
>>
>
>Ooops! Sorry, and thank you for correcting me. I mean "proprietary", of
>course.
>Best,
>
>Philippe Grosjean
>  
>
This thread has gone to be centered around the GUI of R and what it is 
good and bad.

However, is the above statement correct? To me it seems like there is a 
fully working R-proxy dll for windows and other ways to interface 
against R that only binds to LGPL components. You can build completely 
proprietary packages and front-ends to R without having to make sources 
available, as long as you distribute changes to R itself as source.

In my opinion anyone can be to R what S+ is to S. Can any developer 
comment on this?

    Best regards,

    David



From MSchwartz at MedAnalytics.com  Fri Nov 19 18:06:07 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 19 Nov 2004 11:06:07 -0600
Subject: [R] help! a urgent question
In-Reply-To: <20041119155518.10269.qmail@web53108.mail.yahoo.com>
References: <20041119155518.10269.qmail@web53108.mail.yahoo.com>
Message-ID: <1100883967.28622.24.camel@horizons.localdomain>

On Fri, 2004-11-19 at 07:55 -0800, an ying wrote:
> Dear Sir/Madam,
>  
> I am doing a project related to R. 
> However, it is always difficult find some R functions.
> The R user guide seems not complete. 
> Is there any free document about all R functions ?
> Who knows ? please help me.
> My email is myzhaogong at yahoo.com 
> 
> 
> Thank you very much


Perhaps if you provided some insight into what exactly it is you are
looking to do, we can provide some specific guidance.

The R Manuals are typically available in your installation, accessible
via help.start().

However they are also online at the main R site:

http://www.r-project.org/

Along the left hand side of the page under "Documentation", there are
several links to available manuals, user contributed docs, FAQ's, etc.

HTH,

Marc Schwartz



From ym at climpact.com  Fri Nov 19 19:20:29 2004
From: ym at climpact.com (Yves Magliulo)
Date: 19 Nov 2004 19:20:29 +0100
Subject: [R] help! a urgent question
In-Reply-To: <20041119155518.10269.qmail@web53108.mail.yahoo.com>
References: <20041119155518.10269.qmail@web53108.mail.yahoo.com>
Message-ID: <1100888428.1471.246.camel@new-york.climpact.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041119/be811249/attachment.pl

From ripley at stats.ox.ac.uk  Fri Nov 19 18:22:51 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 19 Nov 2004 17:22:51 +0000 (GMT)
Subject: [R] function 'vcov' for coxph in R 2.0.0
In-Reply-To: <web-98002552@cgatepro-4.mail.virginia.edu>
References: <web-98002552@cgatepro-4.mail.virginia.edu>
Message-ID: <Pine.LNX.4.61.0411191721470.31530@gannet.stats>

Your version of survival is out of date: please use update.packages().

On Fri, 19 Nov 2004, Lei  Liu wrote:

> Hi there,
>
> After I fitted a cox model, I used vcov to obtain the variance of the 
> parameter estimate. It worked correctly in R 1.9.1. But it failed in R 2.0.0 
> and the error message is 
> Error in vcov(cox.1) : no applicable method for "vcov"
>
> I don't know if it is a bug or there is some update on this function. Thanks!

WHY don't you know if there is an update?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From kjetil at acelerate.com  Fri Nov 19 18:22:58 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Fri, 19 Nov 2004 13:22:58 -0400
Subject: [R] CRAN problems
Message-ID: <419E2BF2.2010900@acelerate.com>

The area

*Index of /bin/windows/contrib/2.1

(and also 2.0)

**at the main CRAN site is at the moment empty'
*

-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From drf5n at maplepark.com  Fri Nov 19 18:28:19 2004
From: drf5n at maplepark.com (David Forrest)
Date: Fri, 19 Nov 2004 11:28:19 -0600 (CST)
Subject: [R] help! a urgent question
In-Reply-To: <20041119155518.10269.qmail@web53108.mail.yahoo.com>
References: <20041119155518.10269.qmail@web53108.mail.yahoo.com>
Message-ID: <Pine.LNX.4.58.0411191107160.1640@maplepark.com>

On Fri, 19 Nov 2004, an ying wrote:

> Dear Sir/Madam,
>
> I am doing a project related to R.
> However, it is always difficult find some R functions.
> The R user guide seems not complete.
> Is there any free document about all R functions ?
> Who knows ? please help me.
> My email is myzhaogong at yahoo.com

There has been much discussion on this topic on this mailing list
recently, and an important result is that there is no complete user guide.
Like any computer language, R is extensible, and there is no complete
listing.  If you know what you are looking for, for example 'urgent
question', try this in R:

 help.search.archive<-function(string){
   RURL="http://www.google.com/u/newcastlemaths"
   RSearchURL=paste(RURL,"?q=",string,sep='')
   browseURL(RSearchURL)
   return(invisible(0))
 }
 # and

help.search.archive("urgent question") # to find urgent questions

Dave
-- 
 Dave Forrest
 drf at vims.edu                                    (804)684-7900w
 drf5n at maplepark.com                             (804)642-0662h
                                   http://maplepark.com/~drf5n/



From sdavis2 at mail.nih.gov  Fri Nov 19 19:08:37 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Fri, 19 Nov 2004 13:08:37 -0500
Subject: [R] Running sum
Message-ID: <09151670-3A56-11D9-AD50-000A95D7BA10@mail.nih.gov>

I have vector X of length N that I want to have a running sum for 
(called Y).  I just need max(Y).  I do this with a "for" loop like so:

     Y <- vector(length=N)
     Y[1] <- X[1]
     for (i in 2:N) {
       Y[i] <- Y[i-1]+X[i]
     }
     return(max(Y))

Is there a faster way to do this?

Thanks,
Sean



From jimmcloughlin at earthlink.net  Fri Nov 19 19:21:50 2004
From: jimmcloughlin at earthlink.net (Jim McLoughlin)
Date: Fri, 19 Nov 2004 10:21:50 -0800
Subject: [R] Performing regression using R & C
In-Reply-To: <111C3CD246AB774790496EBB3CD220080334F4@mothra.hcjp.com>
References: <111C3CD246AB774790496EBB3CD220080334F4@mothra.hcjp.com>
Message-ID: <E1D82A70-3A57-11D9-B879-000393B2DF14@earthlink.net>

> 	Is it possible to perform OLS using C code? I am trying to
> optimize a n-period "moving window" OLS on a huge dataset hence was
> wondering if such a thing is possible.
>
> 	Ideally the solution that I am looking for would involve a
> C-code accepting two float arrays and returning back computed 
> parameters
> such as t-stat, coefficient etc.

It is probably not worth trying to call R to do OLS from C.  I would 
look at GSL:

http://www.gnu.org/software/gsl/manual/gsl-ref_toc.html

GSL has a simple but elegant C interface for doing OLS and weighted 
least squares.  It will give you the covariance matrix of the model 
parameters, from which you can derive t-stats.

Jim M



From kjetil at acelerate.com  Fri Nov 19 18:56:57 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Fri, 19 Nov 2004 13:56:57 -0400
Subject: [R] help! a urgent question
In-Reply-To: <20041119155518.10269.qmail@web53108.mail.yahoo.com>
References: <20041119155518.10269.qmail@web53108.mail.yahoo.com>
Message-ID: <419E33E9.1040509@acelerate.com>

Go to
http://cran.r-project.org/

and find the contributed documentation section.

Kjetil

an ying wrote:

>Dear Sir/Madam,
> 
>I am doing a project related to R. 
>However, it is always difficult find some R functions.
>The R user guide seems not complete. 
>Is there any free document about all R functions ?
>Who knows ? please help me.
>My email is myzhaogong at yahoo.com 
> 
> 
>Thank you very much
>
>		
>---------------------------------
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From kjetil at acelerate.com  Fri Nov 19 19:44:03 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Fri, 19 Nov 2004 14:44:03 -0400
Subject: [R] Plotting averages of y per x
In-Reply-To: <419E31E6.3040302@bfro.uni-lj.si>
References: <419E31E6.3040302@bfro.uni-lj.si>
Message-ID: <419E3EF3.8040300@acelerate.com>

Gregor GORJANC wrote:

> Hello!
>
> I often plot average of y instead of all y values, since one can 
> easily see the trend if there is to many points and/or x might be like 
> 1, 2, 3, 4, 5, ... and you might get a cloud (which can also be 
> informative) and/or columns of points.
>
> Anyway, learning with R i get stucked. I have the data in table 
> burnins1, with the following names:
>
> names(burnins1)
> [1] "Model"       "Replication" "h2"          "burnin"
>
> and some data
> ...
> ...
> 380    New          80  2     23
> 381    New          81  5     38
> 382    New          82 10     31
> 383    New          83 15     98
> 384    New          84 20     32
> 385    New          85 40     30
> 386    New          86  1     43
> 387    New          87  2     53
> 388    New          88  5     36
> 389    New          89 10     51
> 390    New          90 15     19
> ...
> ...
>
> So I want to plot mean of variable burnin by variables model and h2. I 
> compute means with
>
> tmp <- as.data.frame(t(tapply(burnin, list(Model, h2), mean)))
>
> and table tmp looks like this
>
>      Old   New
> 1  31.00 29.36
> 2  30.30 28.34
> 5  32.92 30.66
> 10 39.00 37.54
> 15 40.66 34.07
> 20 39.29 35.94
> 40 28.63 28.51
>
> Now I want to launch something like
> plot(tmp[, 0], Old)
> points(tmp[, 0], New, pch = 20)
>
There is no column 0. R starts counting at 1. Try something like
plot(as.numeric(rownames(tmp)), tmp[,"Old"])
(not tested)

Kjetil

> and it gives me the error
>
> Error in pairs.default(x, ...) : non-numeric argument to pairs
>
> I guess that I am having problem with tmp[, 0]. If I use
>
> plot(Old)
> points(New, pch = 20)
>
> I get the plot, but abscisa is numbered from 1 to 7 not 1, 2, 5, 10, 
> 15, 20 and 40. Does anyone have any sugestions?
>
> I would also like to produce table like this, so plotting would be 
> easier, since there would be no need to use points() and/or lines() 
> commands to add another group.
>
>   Model h2 mean
> 1  Old  1  31.00
> 2  Old  2  30.30
> 3  Old  5  32.92
> 4  Old  10 39.00
> 5  Old  15 40.66
> 6  Old  20 39.29
> 7  Old  40 28.63
> 8  Old  1  29.36
> 9  Old  2  28.34
> 10 Old  5  30.66
> 11 Old  10 37.54
> 12 Old  15 34.07
> 13 Old  20 35.94
> 14 Old  40 28.51
>
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From kjetil at acelerate.com  Fri Nov 19 19:47:06 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Fri, 19 Nov 2004 14:47:06 -0400
Subject: [R] Running sum
In-Reply-To: <09151670-3A56-11D9-AD50-000A95D7BA10@mail.nih.gov>
References: <09151670-3A56-11D9-AD50-000A95D7BA10@mail.nih.gov>
Message-ID: <419E3FAA.3050707@acelerate.com>

Sean Davis wrote:

> I have vector X of length N that I want to have a running sum for 
> (called Y).  I just need max(Y).  I do this with a "for" loop like so:
>
>     Y <- vector(length=N)
>     Y[1] <- X[1]
>     for (i in 2:N) {
>       Y[i] <- Y[i-1]+X[i]
>     }
>     return(max(Y))
>
> Is there a faster way to do this?


max(cumsum(Y))

Kjetil

>
> Thanks,
> Sean
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From andy_liaw at merck.com  Fri Nov 19 19:51:17 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 19 Nov 2004 13:51:17 -0500
Subject: [R] Running sum
Message-ID: <3A822319EB35174CA3714066D590DCD50994E350@usrymx25.merck.com>

See cumsum:

> x <- rnorm(10)
> cs <- function(X) { 
+      N <- length(X)
+      Y <- vector(length=N)
+      Y[1] <- X[1]
+      for (i in 2:N) {
+        Y[i] <- Y[i-1]+X[i]
+      }
+      return(max(Y))
+ }
> cs(x)
[1] 3.228554
> max(cumsum(x))
[1] 3.228554

Andy

> From: Sean Davis
> 
> I have vector X of length N that I want to have a running sum for 
> (called Y).  I just need max(Y).  I do this with a "for" loop like so:
> 
>      Y <- vector(length=N)
>      Y[1] <- X[1]
>      for (i in 2:N) {
>        Y[i] <- Y[i-1]+X[i]
>      }
>      return(max(Y))
> 
> Is there a faster way to do this?
> 
> Thanks,
> Sean
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From abunn at whrc.org  Fri Nov 19 19:52:24 2004
From: abunn at whrc.org (Andy Bunn)
Date: Fri, 19 Nov 2004 13:52:24 -0500
Subject: [R] Running sum
In-Reply-To: <09151670-3A56-11D9-AD50-000A95D7BA10@mail.nih.gov>
Message-ID: <NEBBIPHDAMMOKDKPOFFIKEHKCMAA.abunn@whrc.org>

see ?cumsum
x <- 1:10
cumsum(x)
max(cumsum(x))

HTH, Andy


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Sean Davis
> Sent: Friday, November 19, 2004 1:09 PM
> To: r-help
> Subject: [R] Running sum
> 
> 
> I have vector X of length N that I want to have a running sum for 
> (called Y).  I just need max(Y).  I do this with a "for" loop like so:
> 
>      Y <- vector(length=N)
>      Y[1] <- X[1]
>      for (i in 2:N) {
>        Y[i] <- Y[i-1]+X[i]
>      }
>      return(max(Y))
> 
> Is there a faster way to do this?
> 
> Thanks,
> Sean
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From sdavis2 at mail.nih.gov  Fri Nov 19 19:53:36 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Fri, 19 Nov 2004 13:53:36 -0500
Subject: [R] Running sum
In-Reply-To: <419E3FAA.3050707@acelerate.com>
References: <09151670-3A56-11D9-AD50-000A95D7BA10@mail.nih.gov>
	<419E3FAA.3050707@acelerate.com>
Message-ID: <516AC824-3A5C-11D9-AD50-000A95D7BA10@mail.nih.gov>

Thanks all.

cumsum does the job

Sean



From MSchwartz at MedAnalytics.com  Fri Nov 19 19:57:24 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 19 Nov 2004 12:57:24 -0600
Subject: [R] Running sum
In-Reply-To: <09151670-3A56-11D9-AD50-000A95D7BA10@mail.nih.gov>
References: <09151670-3A56-11D9-AD50-000A95D7BA10@mail.nih.gov>
Message-ID: <1100890645.31008.16.camel@horizons.localdomain>

On Fri, 2004-11-19 at 13:08 -0500, Sean Davis wrote:
> I have vector X of length N that I want to have a running sum for 
> (called Y).  I just need max(Y).  I do this with a "for" loop like so:
> 
>      Y <- vector(length=N)
>      Y[1] <- X[1]
>      for (i in 2:N) {
>        Y[i] <- Y[i-1]+X[i]
>      }
>      return(max(Y))
> 
> Is there a faster way to do this?
> 
> Thanks,
> Sean


Something like:

> cumsum(1:10)
 [1]  1  3  6 10 15 21 28 36 45 55

> max(cumsum(1:10))
[1] 55

Does that help?

Marc Schwartz



From ripley at stats.ox.ac.uk  Fri Nov 19 19:58:14 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 19 Nov 2004 18:58:14 +0000 (GMT)
Subject: [R] Running sum
In-Reply-To: <09151670-3A56-11D9-AD50-000A95D7BA10@mail.nih.gov>
References: <09151670-3A56-11D9-AD50-000A95D7BA10@mail.nih.gov>
Message-ID: <Pine.LNX.4.61.0411191857240.3190@gannet.stats>

?cumsum

On Fri, 19 Nov 2004, Sean Davis wrote:

> I have vector X of length N that I want to have a running sum for (called Y). 
> I just need max(Y).  I do this with a "for" loop like so:
>
>    Y <- vector(length=N)
>    Y[1] <- X[1]
>    for (i in 2:N) {
>      Y[i] <- Y[i-1]+X[i]
>    }
>    return(max(Y))
>
> Is there a faster way to do this?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From spencer.graves at pdf.com  Fri Nov 19 20:00:38 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 19 Nov 2004 11:00:38 -0800
Subject: [R] Running sum
In-Reply-To: <09151670-3A56-11D9-AD50-000A95D7BA10@mail.nih.gov>
References: <09151670-3A56-11D9-AD50-000A95D7BA10@mail.nih.gov>
Message-ID: <419E42D6.7030404@pdf.com>

      Have you considered "cumsum"? 

      > cumsum(c(1, 2, 3, -9, 2))
      [1]  1  3  6 -3 -1

      hope this helps.  spencer graves

Sean Davis wrote:

> I have vector X of length N that I want to have a running sum for 
> (called Y).  I just need max(Y).  I do this with a "for" loop like so:
>
>     Y <- vector(length=N)
>     Y[1] <- X[1]
>     for (i in 2:N) {
>       Y[i] <- Y[i-1]+X[i]
>     }
>     return(max(Y))
>
> Is there a faster way to do this?
>
> Thanks,
> Sean
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html


-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From annhuxtable at hotmail.com  Fri Nov 19 20:01:48 2004
From: annhuxtable at hotmail.com (Ann Huxtable)
Date: Fri, 19 Nov 2004 19:01:48 +0000
Subject: [R] Selecting distinct values from a column
Message-ID: <BAY2-F341F4B36FA4DBEF5313417B5C30@phx.gbl>

Hello,

Is there an easy way (other than the obvious for loop?) to select distinct 
values from a column vector?

Many thanks

Ann



From rpeng at jhsph.edu  Fri Nov 19 20:05:02 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Fri, 19 Nov 2004 14:05:02 -0500
Subject: [R] Running sum
In-Reply-To: <09151670-3A56-11D9-AD50-000A95D7BA10@mail.nih.gov>
References: <09151670-3A56-11D9-AD50-000A95D7BA10@mail.nih.gov>
Message-ID: <419E43DE.8000308@jhsph.edu>

You could try using embed(), but I doubt it's faster.

-roger

Sean Davis wrote:
> I have vector X of length N that I want to have a running sum for 
> (called Y).  I just need max(Y).  I do this with a "for" loop like so:
> 
>     Y <- vector(length=N)
>     Y[1] <- X[1]
>     for (i in 2:N) {
>       Y[i] <- Y[i-1]+X[i]
>     }
>     return(max(Y))
> 
> Is there a faster way to do this?
> 
> Thanks,
> Sean
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From sdutky at starpower.net  Fri Nov 19 20:43:36 2004
From: sdutky at starpower.net (Steve Dutky)
Date: Fri, 19 Nov 2004 14:43:36 -0500
Subject: [R] R (unix) command line editing for native
 speakers of vi
Message-ID: <a7664fdc.a1d5974d.8618100@ms08.mrf.mail.rcn.net>

As an inveterate user of vi, I was pleased to stumble on how 
to use it for editing R commands.  

When an interactive R session is launched under unix, the 
command line editor most likely defaults to emacs. Typing 
<esc>,<ctrl>+j will switch this to vi editing mode (see below 
for possible exceptions).

excerpted from readline(3) manpage:
vi-editing-mode (M-C-j)
  When in emacs editing mode, this causes a switch to        
vi editing mode.

emacs-editing-mode (C-e)
  When in vi editing mode, this causes  a  switch  to emacs 
editing mode.


An  emacs-style  notation  is  used  to denote keystrokes. 
Control keys are denoted by C-key, e.g.,C-n  means  Control-
N.  Similarly, meta keys are denoted by M-key, so M-x means 
Meta-X.  (On keyboards without a meta key, M-x means ESC  x,  
i.e.,  press the Escape key then the x key.  This makes ESC 
the meta prefix.  The combination M-C-x means ESC-Control-x,  
or press the Escape key then hold the Control key while 
pressing the x key.)

see also:
An Introduction to R Appendix C: The command-line editor.
http://cran.r-project.org/doc/manuals/R-intro.html#The%
20command-line%20editor

Steve Dutky (sdutky at terpalum.umd.edu)



From phgrosjean at sciviews.org  Fri Nov 19 21:10:37 2004
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Fri, 19 Nov 2004 21:10:37 +0100
Subject: [R] Running sum
In-Reply-To: <1100890645.31008.16.camel@horizons.localdomain>
Message-ID: <200411192010.iAJKAcDM019929@outmx001.isp.belgacom.be>

?cumsum is not exactly the answer (as I understand it), but a part of it.
I propose:

runSum2 <- function(x)
	cumsum(x)[-1] - c(0, cumsum(x[1:(length(x) - 2)]))

# Example
a <- round(runif(10, 0, 10))
a
runSum2(a)
max(runSum2(a)) # To get only the max

Best,

Philippe

..............................................<??}))><........
 ) ) ) ) )
( ( ( ( (    Prof. Philippe Grosjean
 ) ) ) ) )
( ( ( ( (    Numerical Ecology of Aquatic Systems
 ) ) ) ) )   Mons-Hainaut University, Pentagone
( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
 ) ) ) ) )   6, av du Champ de Mars, 7000 Mons, Belgium  
( ( ( ( (       
 ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.33.12
( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
 ) ) ) ) )      
( ( ( ( (    web:   http://www.umh.ac.be/~econum
 ) ) ) ) )
..............................................................

 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Marc Schwartz
> Sent: Friday, November 19, 2004 7:57 PM
> To: Sean Davis
> Cc: R-Help
> Subject: Re: [R] Running sum
> 
> On Fri, 2004-11-19 at 13:08 -0500, Sean Davis wrote:
> > I have vector X of length N that I want to have a running sum for 
> > (called Y).  I just need max(Y).  I do this with a "for" 
> loop like so:
> > 
> >      Y <- vector(length=N)
> >      Y[1] <- X[1]
> >      for (i in 2:N) {
> >        Y[i] <- Y[i-1]+X[i]
> >      }
> >      return(max(Y))
> > 
> > Is there a faster way to do this?
> > 
> > Thanks,
> > Sean
> 
> 
> Something like:
> 
> > cumsum(1:10)
>  [1]  1  3  6 10 15 21 28 36 45 55
> 
> > max(cumsum(1:10))
> [1] 55
> 
> Does that help?
> 
> Marc Schwartz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From bob.ohara at helsinki.fi  Fri Nov 19 21:13:56 2004
From: bob.ohara at helsinki.fi (Anon.)
Date: Fri, 19 Nov 2004 22:13:56 +0200
Subject: [R] Running sum
In-Reply-To: <Pine.LNX.4.61.0411191857240.3190@gannet.stats>
References: <09151670-3A56-11D9-AD50-000A95D7BA10@mail.nih.gov>
	<Pine.LNX.4.61.0411191857240.3190@gannet.stats>
Message-ID: <419E5404.5060501@helsinki.fi>

Prof Brian Ripley wrote:

> ?cumsum
>
> On Fri, 19 Nov 2004, Sean Davis wrote:
>
>> I have vector X of length N that I want to have a running sum for 
>> (called Y). I just need max(Y).  I do this with a "for" loop like so:
>>
>>    Y <- vector(length=N)
>>    Y[1] <- X[1]
>>    for (i in 2:N) {
>>      Y[i] <- Y[i-1]+X[i]
>>    }
>>    return(max(Y))
>>
>> Is there a faster way to do this?
>
>
My apologies, I am not being entirely serious, but...

Please do read the posting guide! 
(particularly the third point in the "Responding to other posts" section).

Bob

-- 
Bob O'Hara
Department of Mathematics and Statistics
P.O. Box 68 (Gustaf H??llstr??min katu 2b)
FIN-00014 University of Helsinki
Finland

Telephone: +358-9-191 51479
Mobile: +358 50 599 0540
Fax:  +358-9-191 51400
WWW:  http://www.RNI.Helsinki.FI/~boh/
Journal of Negative Results - EEB: www.jnr-eeb.org



From strivens at bcm.tmc.edu  Fri Nov 19 21:46:14 2004
From: strivens at bcm.tmc.edu (strivens)
Date: Fri, 19 Nov 2004 14:46:14 -0600
Subject: [R] Rd and document formatting
Message-ID: <419E7358@webmail.bcm.tmc.edu>

I am trying to compose some documentation for a package I hope to release 
soon.

However when I do the following:

R CMD Rd2dvi --pdf mypackage.Rd

I get two mainly blank pages prepended to the top of the document - the only 
text on either (at the top of the first page) is as follows:
 hyperindex,colorlinks,pagebackref,linktocpage,plainpages=false,linkcolor=Blue
,citecolor=Blue,urlcolor=Red,pdnull null null

Curiously if you generate the dvi it is fine and if you generate the pdf from 
the dvi using dvipdf it is also fine...

Anyone got any ideas?

System: Linux 2.4.24 SMP i686 GNU/Linux, latex 3.14159-2.1 pdfTex 3.4.5

Thanks  Mark



From MSchwartz at MedAnalytics.com  Fri Nov 19 21:47:51 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 19 Nov 2004 14:47:51 -0600
Subject: [R] Selecting distinct values from a column
In-Reply-To: <BAY2-F341F4B36FA4DBEF5313417B5C30@phx.gbl>
References: <BAY2-F341F4B36FA4DBEF5313417B5C30@phx.gbl>
Message-ID: <1100897271.31008.38.camel@horizons.localdomain>

On Fri, 2004-11-19 at 19:01 +0000, Ann Huxtable wrote:
> Hello,
> 
> Is there an easy way (other than the obvious for loop?) to select distinct 
> values from a column vector?
> 
> Many thanks
> 
> Ann


Presuming that by 'distinct' you mean 'unique', see ?unique

> MyVec <- rep(1:5, times = c(1:5))

> MyVec
 [1] 1 2 2 3 3 3 4 4 4 4 5 5 5 5 5

> unique(MyVec)
[1] 1 2 3 4 5


HTH,

Marc Schwartz



From andy_liaw at merck.com  Fri Nov 19 21:55:06 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 19 Nov 2004 15:55:06 -0500
Subject: [R] R (unix) command line editing for native speakers of vi
Message-ID: <3A822319EB35174CA3714066D590DCD50994E356@usrymx25.merck.com>

If you have the following in your ~/.inputrc, you won't need to set it
manually every time (at least according to the readline manual):

set editing-mode vi

HTH,
Andy

> From: Steve Dutky
> 
> As an inveterate user of vi, I was pleased to stumble on how 
> to use it for editing R commands.  
> 
> When an interactive R session is launched under unix, the 
> command line editor most likely defaults to emacs. Typing 
> <esc>,<ctrl>+j will switch this to vi editing mode (see below 
> for possible exceptions).
> 
> excerpted from readline(3) manpage:
> vi-editing-mode (M-C-j)
>   When in emacs editing mode, this causes a switch to        
> vi editing mode.
> 
> emacs-editing-mode (C-e)
>   When in vi editing mode, this causes  a  switch  to emacs 
> editing mode.
> 
> 
> An  emacs-style  notation  is  used  to denote keystrokes. 
> Control keys are denoted by C-key, e.g.,C-n  means  Control-
> N.  Similarly, meta keys are denoted by M-key, so M-x means 
> Meta-X.  (On keyboards without a meta key, M-x means ESC  x,  
> i.e.,  press the Escape key then the x key.  This makes ESC 
> the meta prefix.  The combination M-C-x means ESC-Control-x,  
> or press the Escape key then hold the Control key while 
> pressing the x key.)
> 
> see also:
> An Introduction to R Appendix C: The command-line editor.
> http://cran.r-project.org/doc/manuals/R-intro.html#The%
> 20command-line%20editor
> 
> Steve Dutky (sdutky at terpalum.umd.edu)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From friendly at yorku.ca  Fri Nov 19 21:59:25 2004
From: friendly at yorku.ca (Michael Friendly)
Date: Fri, 19 Nov 2004 15:59:25 -0500
Subject: [R] adjusting the map of France to 1830
In-Reply-To: <5.2.1.1.0.20041118182350.00c43198@biomserv.univ-lyon1.fr>
References: <5.2.1.1.0.20041118182350.00c43198@biomserv.univ-lyon1.fr>
Message-ID: <419E5EAD.3040708@yorku.ca>

Here's what I tried.   I can plot a selection of regions, but I
can't seem to remove an arbitrary list of region numbers, unless I've 
done something wrong
by selecting the regions I want to plot with departements[-exclude].
I also get an error
when I try to use map.text to label a map with only the regions I'm  
selecting.

 > departements <- map('france',namesonly=T, plot=FALSE)
 > # returns a vector of names of regions
 >
 > exclude <- c(47,  #Alpes-Maritimes
+ 66,  # Haute-Savoie
+ 76,  # Savoie
+ 95,  # Territore-de-Belfort
+ 109, 110, 111, # Var: Iles d'Hyeres
+ 49, 53, 54, 55, # Moribhan: Isles
+ 62, 64,    # Vendee: Isles
+ 72, 75     # Charente-Maritime: Isles
+ )
 >
 > depts <- departements[-exclude]
 > gfrance <-map('france', regions=depts)
 > labels <- (as.character(1:length(departements)))[-exclude]
 > gfrance <-map.text('france', regions=depts, add=FALSE, labels=labels)
Error in map.text("france", regions = depts, add = FALSE, labels = labels) :
        map object must have polygons (fill=TRUE)

Another problem, potentially more difficult for mapping data on the map 
of France is that
the "departements" are actually just the polygons in the map, 
arbitrarily numbered from
east to west, and from north to south --- they don't correspond to the 
'official' administrative
region numbers.  As well, the departement names don't always match 
exactly (ignoring
accents, e.g., Val-d'Oise vs. Val-Doise) so it would be another 
challenge to plot my
historical data on the map of France.

-Michael


Stephane DRAY wrote:

> Hello. I do not know if you can merge polygons, but you can select 
> easily:
>
> > departements=map('france',namesonly=T) # returns a vector of names 
> of regions
> > map('france',regions=departements[1:20],namesonly=T) # use what you 
> need with regions argument
>
> Hope this helps,
>
>
> At 16:29 18/11/2004, Michael Friendly wrote:
>
>> I'm doing some analyses of historical data from France in 1830 on 
>> 'moral statistics' that I'd like to
>> show on a map.  I've done most of my analyses in SAS, but a few 
>> things would work better in R.
>> To do this, I have to adjust the modern map,
>>
>> library(maps)
>> map('france')
>>
>> to adjust for changes in departments (86 in 1830, to 97 now).  I've 
>> read the documentation
>> for the maps and maptools package, but there seems to be no functions 
>> to allow this, and
>> I can't find information on the exact structure of map datasets, but 
>> I understand them to
>> be delimited lists of polygon coordinates.
>>
>> In SAS, all maps have (one or more) ID variables representing the 
>> geographical region,
>> and there is also a proc gremove that can remove internal boundaries 
>> inside the polygons
>> for regions with the same ID.  Is there some way I can do this in R?
>>
>> Here's what I did in SAS:
>>
>> *-- Fix the map of France to conform to Guerry:
>>    - adjust the 97 current departments to correspond to the 86 in 1830
>>    - delete those not part of France then
>> ;
>>
>> data gfrtemp;
>>    set maps.france;
>>    /* Corse was one dept - merge these to one area, new ID */
>>    if id in (201, 202)    then dept=200;
>>
>>    /* Seine et Oise (78) was cut into
>>    Essonne (91), Val d'Oise (95) and Yvelines (78) */
>>    else if id in (91, 95)    then dept=78;
>>
>>    /* Seine (75) now split into
>>    Hauts-de-Seine (92), Seine-Saint-Denis (93) et Val-de-Marne (94)*/
>>    else if id in (92, 93, 94)    then dept=75;
>>
>>     /* departments not part of France in 1830 */
>>    else if id in (
>>        6,     /* Alpes-Maritimes */
>>        73,74, /* Savoie, Haute-Savoie */
>>        90)    /* Territore-de-Belfort */
>>        then delete;
>>    else                       dept=id;
>>    run;
>>
>> *-- remove internal boundaries based on merged DEPT;
>> proc sort data=gfrtemp;
>>    by dept;
>>
>> proc gremove data=gfrtemp out=gfrance;
>>   by dept;
>>   id id;
>>   run;
>>
>>
>>
>> -- 
>> Michael Friendly     Email: friendly at yorku.ca Professor, Psychology 
>> Dept.
>> York University      Voice: 416 736-5115 x66249 Fax: 416 736-5814
>> 4700 Keele Street    http://www.math.yorku.ca/SCS/friendly.html
>> Toronto, ONT  M3J 1P3 CANADA
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
>
> St??phane DRAY
> -------------------------------------------------------------------------------------------------- 
>
> D??partement des Sciences Biologiques
> Universit?? de Montr??al, C.P. 6128, succursale centre-ville
> Montr??al, Qu??bec H3C 3J7, Canada
>
> Tel : (514) 343-6111 poste 1233         Fax : (514) 343-2293
> E-mail : stephane.dray at umontreal.ca
> -------------------------------------------------------------------------------------------------- 
>
> Web                                          
> http://www.steph280.freesurf.fr/
> -------------------------------------------------------------------------------------------------- 
>
>


-- 
Michael Friendly     Email: friendly at yorku.ca 
Professor, Psychology Dept.
York University      Voice: 416 736-5115 x66249 Fax: 416 736-5814
4700 Keele Street    http://www.math.yorku.ca/SCS/friendly.html
Toronto, ONT  M3J 1P3 CANADA



From ligges at statistik.uni-dortmund.de  Fri Nov 19 22:20:51 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 19 Nov 2004 22:20:51 +0100
Subject: [R] Plotting averages of y per x
In-Reply-To: <419E31E6.3040302@bfro.uni-lj.si>
References: <419E31E6.3040302@bfro.uni-lj.si>
Message-ID: <419E63B3.6060908@statistik.uni-dortmund.de>

Gregor GORJANC wrote:
> Hello!
> 
> I often plot average of y instead of all y values, since one can easily 
> see the trend if there is to many points and/or x might be like 1, 2, 3, 
> 4, 5, ... and you might get a cloud (which can also be informative) 
> and/or columns of points.
> 
> Anyway, learning with R i get stucked. I have the data in table 
> burnins1, with the following names:
> 
> names(burnins1)
> [1] "Model"       "Replication" "h2"          "burnin"
> 
> and some data
> ...
> ...
> 380    New          80  2     23
> 381    New          81  5     38
> 382    New          82 10     31
> 383    New          83 15     98
> 384    New          84 20     32
> 385    New          85 40     30
> 386    New          86  1     43
> 387    New          87  2     53
> 388    New          88  5     36
> 389    New          89 10     51
> 390    New          90 15     19
> ...
> ...
> 
> So I want to plot mean of variable burnin by variables model and h2. I 
> compute means with
> 
> tmp <- as.data.frame(t(tapply(burnin, list(Model, h2), mean)))
> 
> and table tmp looks like this
> 
>      Old   New
> 1  31.00 29.36
> 2  30.30 28.34
> 5  32.92 30.66
> 10 39.00 37.54
> 15 40.66 34.07
> 20 39.29 35.94
> 40 28.63 28.51
> 
> Now I want to launch something like
> plot(tmp[, 0], Old)
> points(tmp[, 0], New, pch = 20)
> 
> and it gives me the error
> 
> Error in pairs.default(x, ...) : non-numeric argument to pairs
> 
> I guess that I am having problem with tmp[, 0]. If I use


Yes, R starts indexing with 1, but not with 0.


> plot(Old)
> points(New, pch = 20)
> 
> I get the plot, but abscisa is numbered from 1 to 7 not 1, 2, 5, 10, 15, 
> 20 and 40. Does anyone have any sugestions?

Omit the x axis at first (e.g. using argument xaxt="n") and add it with 
axis(2, ...) and correct labels afterwards.

Uwe Ligges


> I would also like to produce table like this, so plotting would be 
> easier, since there would be no need to use points() and/or lines() 
> commands to add another group.
> 
>   Model h2 mean
> 1  Old  1  31.00
> 2  Old  2  30.30
> 3  Old  5  32.92
> 4  Old  10 39.00
> 5  Old  15 40.66
> 6  Old  20 39.29
> 7  Old  40 28.63
> 8  Old  1  29.36
> 9  Old  2  28.34
> 10 Old  5  30.66
> 11 Old  10 37.54
> 12 Old  15 34.07
> 13 Old  20 35.94
> 14 Old  40 28.51
> 
>



From ligges at statistik.uni-dortmund.de  Fri Nov 19 22:30:45 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 19 Nov 2004 22:30:45 +0100
Subject: [R] Time series plot orientation
In-Reply-To: <419D09E4.7030807@durham.ac.uk>
References: <419D09E4.7030807@durham.ac.uk>
Message-ID: <419E6605.5010606@statistik.uni-dortmund.de>

Costas Vorlow wrote:

> Hello,
> 
> I am trying to rotate by 90 degrees a time series plot. So I need the 
> time axis to be the vertical one. Is there an easy way?


No, you have to do it manually, AFAIK.

Uwe Ligges


> I couldn't guess anything from the help pages.
> 
> Apologies for a silly question.
> 
> Regards,
> Costas
>



From murdoch at stats.uwo.ca  Fri Nov 19 22:31:33 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 19 Nov 2004 16:31:33 -0500
Subject: [R] Rd and document formatting
In-Reply-To: <419E7358@webmail.bcm.tmc.edu>
References: <419E7358@webmail.bcm.tmc.edu>
Message-ID: <v4psp09secqqvo140v1mtp9485qcgqca9n@4ax.com>

On Fri, 19 Nov 2004 14:46:14 -0600, strivens <strivens at bcm.tmc.edu>
wrote :

>I am trying to compose some documentation for a package I hope to release 
>soon.
>
>However when I do the following:
>
>R CMD Rd2dvi --pdf mypackage.Rd
>
>I get two mainly blank pages prepended to the top of the document - the only 
>text on either (at the top of the first page) is as follows:
> hyperindex,colorlinks,pagebackref,linktocpage,plainpages=false,linkcolor=Blue
>,citecolor=Blue,urlcolor=Red,pdnull null null
>
>Curiously if you generate the dvi it is fine and if you generate the pdf from 
>the dvi using dvipdf it is also fine...
>
>Anyone got any ideas?
>
>System: Linux 2.4.24 SMP i686 GNU/Linux, latex 3.14159-2.1 pdfTex 3.4.5

Looks like some LaTeX package is out of date, and doesn't understand
some of the tex that got generated.  Rd.sty has this:

  \hypersetup{%
    hyperindex,%
    colorlinks,%
    pagebackref,%
    linktocpage,%
    plainpages=false,%
    linkcolor=Blue,%
    citecolor=Blue,%
    urlcolor=Red,%
    pdfstartview=Fit,%
    pdfview={XYZ null null null}%
  }

so I think your system doesn't have the \hypersetup macro defined (or
maybe you're using an out of date Rd.sty or hyperref.sty?)

Duncan Murdoch



From ligges at statistik.uni-dortmund.de  Fri Nov 19 22:33:14 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 19 Nov 2004 22:33:14 +0100
Subject: [R] Re: 3d Map with bars
In-Reply-To: <OFB67FA518.7A81C6D6-ON85256F51.0057F2A7-85256F51.0058AE2B@hgsi.com>
References: <OFB67FA518.7A81C6D6-ON85256F51.0057F2A7-85256F51.0058AE2B@hgsi.com>
Message-ID: <419E669A.2080609@statistik.uni-dortmund.de>

partha_bagchi at hgsi.com wrote:

> Apologies in advance for the question. I am trying to draw a map of the US 
> as a surface plot so that I would be able to drop bars on the different 
> states (something like Uwe Ligges' scatterplot3d example 4). I am not sure 
> where to start looking for such a beast. If anyone has any pointers, 
> ideas, I will be grateful.
> 
> TIA,
> Partha

How to "drop bars" with persp() has been described on R-help yesterday 
or today, please check the mailing list's archives.

Uwe Ligges



From dray at biomserv.univ-lyon1.fr  Fri Nov 19 22:36:14 2004
From: dray at biomserv.univ-lyon1.fr (Stephane DRAY)
Date: Fri, 19 Nov 2004 16:36:14 -0500
Subject: [R] adjusting the map of France to 1830
In-Reply-To: <419E5EAD.3040708@yorku.ca>
References: <5.2.1.1.0.20041118182350.00c43198@biomserv.univ-lyon1.fr>
	<5.2.1.1.0.20041118182350.00c43198@biomserv.univ-lyon1.fr>
Message-ID: <5.2.1.1.0.20041119162629.03704d48@biomserv.univ-lyon1.fr>

Hello Michael,

you have made a small mystake in your code:

use

gfrance <-map.text('france', regions=depts, add=FALSE)

But I think that, it would be quite difficult to do what you need in R.
It is more a GIS problem.

I think that your problem can be better treated if you can create your map 
outside R and import it.
The map is freely available at:
http://www.ign.fr/affiche_rubrique.asp?rbr_id=1761&lng_id=FR#71023
it contains administrative numbers... You can modify the map in a GIS, 
(e.g. ArcView) and import it with maptools package.

Tell me if you can do the GIS part, otherwise I can do the job (it is is 
not too long ;-) ) and send you the "old france" data !

Sincerely,




At 15:59 19/11/2004, you wrote:
>Here's what I tried.   I can plot a selection of regions, but I
>can't seem to remove an arbitrary list of region numbers, unless I've done 
>something wrong
>by selecting the regions I want to plot with departements[-exclude].
>I also get an error
>when I try to use map.text to label a map with only the regions I'm
>selecting.
>
> > departements <- map('france',namesonly=T, plot=FALSE)
> > # returns a vector of names of regions
> >
> > exclude <- c(47,  #Alpes-Maritimes
>+ 66,  # Haute-Savoie
>+ 76,  # Savoie
>+ 95,  # Territore-de-Belfort
>+ 109, 110, 111, # Var: Iles d'Hyeres
>+ 49, 53, 54, 55, # Moribhan: Isles
>+ 62, 64,    # Vendee: Isles
>+ 72, 75     # Charente-Maritime: Isles
>+ )
> >
> > depts <- departements[-exclude]
> > gfrance <-map('france', regions=depts)
> > labels <- (as.character(1:length(departements)))[-exclude]
> > gfrance <-map.text('france', regions=depts, add=FALSE, labels=labels)
>Error in map.text("france", regions = depts, add = FALSE, labels = labels) :
>        map object must have polygons (fill=TRUE)
>
>Another problem, potentially more difficult for mapping data on the map of 
>France is that
>the "departements" are actually just the polygons in the map, arbitrarily 
>numbered from
>east to west, and from north to south --- they don't correspond to the 
>'official' administrative
>region numbers.  As well, the departement names don't always match exactly 
>(ignoring
>accents, e.g., Val-d'Oise vs. Val-Doise) so it would be another challenge 
>to plot my
>historical data on the map of France.
>
>-Michael
>
>
>Stephane DRAY wrote:
>
>>Hello. I do not know if you can merge polygons, but you can select easily:
>>
>> > departements=map('france',namesonly=T) # returns a vector of names of 
>> regions
>> > map('france',regions=departements[1:20],namesonly=T) # use what you 
>> need with regions argument
>>
>>Hope this helps,
>>
>>
>>At 16:29 18/11/2004, Michael Friendly wrote:
>>
>>>I'm doing some analyses of historical data from France in 1830 on 'moral 
>>>statistics' that I'd like to
>>>show on a map.  I've done most of my analyses in SAS, but a few things 
>>>would work better in R.
>>>To do this, I have to adjust the modern map,
>>>
>>>library(maps)
>>>map('france')
>>>
>>>to adjust for changes in departments (86 in 1830, to 97 now).  I've read 
>>>the documentation
>>>for the maps and maptools package, but there seems to be no functions to 
>>>allow this, and
>>>I can't find information on the exact structure of map datasets, but I 
>>>understand them to
>>>be delimited lists of polygon coordinates.
>>>
>>>In SAS, all maps have (one or more) ID variables representing the 
>>>geographical region,
>>>and there is also a proc gremove that can remove internal boundaries 
>>>inside the polygons
>>>for regions with the same ID.  Is there some way I can do this in R?
>>>
>>>Here's what I did in SAS:
>>>
>>>*-- Fix the map of France to conform to Guerry:
>>>    - adjust the 97 current departments to correspond to the 86 in 1830
>>>    - delete those not part of France then
>>>;
>>>
>>>data gfrtemp;
>>>    set maps.france;
>>>    /* Corse was one dept - merge these to one area, new ID */
>>>    if id in (201, 202)    then dept=200;
>>>
>>>    /* Seine et Oise (78) was cut into
>>>    Essonne (91), Val d'Oise (95) and Yvelines (78) */
>>>    else if id in (91, 95)    then dept=78;
>>>
>>>    /* Seine (75) now split into
>>>    Hauts-de-Seine (92), Seine-Saint-Denis (93) et Val-de-Marne (94)*/
>>>    else if id in (92, 93, 94)    then dept=75;
>>>
>>>     /* departments not part of France in 1830 */
>>>    else if id in (
>>>        6,     /* Alpes-Maritimes */
>>>        73,74, /* Savoie, Haute-Savoie */
>>>        90)    /* Territore-de-Belfort */
>>>        then delete;
>>>    else                       dept=id;
>>>    run;
>>>
>>>*-- remove internal boundaries based on merged DEPT;
>>>proc sort data=gfrtemp;
>>>    by dept;
>>>
>>>proc gremove data=gfrtemp out=gfrance;
>>>   by dept;
>>>   id id;
>>>   run;
>>>
>>>
>>>
>>>--
>>>Michael Friendly     Email: friendly at yorku.ca Professor, Psychology Dept.
>>>York University      Voice: 416 736-5115 x66249 Fax: 416 736-5814
>>>4700 Keele Street    http://www.math.yorku.ca/SCS/friendly.html
>>>Toronto, ONT  M3J 1P3 CANADA
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! 
>>>http://www.R-project.org/posting-guide.html
>>
>>
>>St??phane DRAY
>>-------------------------------------------------------------------------------------------------- 
>>
>>D??partement des Sciences Biologiques
>>Universit?? de Montr??al, C.P. 6128, succursale centre-ville
>>Montr??al, Qu??bec H3C 3J7, Canada
>>
>>Tel : (514) 343-6111 poste 1233         Fax : (514) 343-2293
>>E-mail : stephane.dray at umontreal.ca
>>-------------------------------------------------------------------------------------------------- 
>>
>>Web
>>http://www.steph280.freesurf.fr/
>>-------------------------------------------------------------------------------------------------- 
>>
>
>
>--
>Michael Friendly     Email: friendly at yorku.ca Professor, Psychology Dept.
>York University      Voice: 416 736-5115 x66249 Fax: 416 736-5814
>4700 Keele Street    http://www.math.yorku.ca/SCS/friendly.html
>Toronto, ONT  M3J 1P3 CANADA

St??phane DRAY
-------------------------------------------------------------------------------------------------- 

D??partement des Sciences Biologiques
Universit?? de Montr??al, C.P. 6128, succursale centre-ville
Montr??al, Qu??bec H3C 3J7, Canada

Tel : (514) 343-6111 poste 1233         Fax : (514) 343-2293
E-mail : stephane.dray at umontreal.ca
-------------------------------------------------------------------------------------------------- 

Web                                          http://www.steph280.freesurf.fr/



From MSchwartz at MedAnalytics.com  Fri Nov 19 22:42:01 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 19 Nov 2004 15:42:01 -0600
Subject: [R] Running sum
In-Reply-To: <200411192010.iAJKAcDM019929@outmx001.isp.belgacom.be>
References: <200411192010.iAJKAcDM019929@outmx001.isp.belgacom.be>
Message-ID: <1100900521.31008.65.camel@horizons.localdomain>

On Fri, 2004-11-19 at 21:10 +0100, Philippe Grosjean wrote:
> ?cumsum is not exactly the answer (as I understand it), but a part of it.
> I propose:
> 
> runSum2 <- function(x)
> 	cumsum(x)[-1] - c(0, cumsum(x[1:(length(x) - 2)]))
> 
> # Example
> a <- round(runif(10, 0, 10))
> a
> runSum2(a)
> max(runSum2(a)) # To get only the max


Phillipe, 

If you run Sean's original function for 1:10, you get:

X <- 1:10
Y <- vector(length = 10)
Y[1] <- X[1]

for (i in 2:10)
{
   Y[i] <- Y[i-1] + X[i]
}

> Y
 [1]  1  3  6 10 15 21 28 36 45 55

which is equivalent to cumsum(1:10)


Your function yields:

> runSum2(1:10)
[1]  3  5  7  9 11 13 15 17 19

Which is the sum of successive individual pairs of vector elements.

For future reference, the running() function in the gregmisc bundle's
gtools package offers a more general approach to 'moving window'
functions:

> running(1:10, width = 2, fun = sum)
 1:2  2:3  3:4  4:5  5:6  6:7  7:8  8:9 9:10 
   3    5    7    9   11   13   15   17   19 


Best regards,

Marc Schwartz



From ligges at statistik.uni-dortmund.de  Fri Nov 19 22:44:39 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 19 Nov 2004 22:44:39 +0100
Subject: [R] function 'vcov' for coxph in R 2.0.0
In-Reply-To: <web-98002552@cgatepro-4.mail.virginia.edu>
References: <web-98002552@cgatepro-4.mail.virginia.edu>
Message-ID: <419E6947.4030108@statistik.uni-dortmund.de>

Lei Liu wrote:

> Hi there,
> 
> After I fitted a cox model, I used vcov to obtain the variance of the 
> parameter estimate. It worked correctly in R 1.9.1. But it failed in R 
> 2.0.0 and the error message is
> Error in vcov(cox.1) : no applicable method for "vcov"

Please give us details and read the posting guide.

The first thing we can guess is that you are using function coxph() from 
package "survival". Is this correct?

If yes, the following extended exmaple from ?coxph works perfectly for 
me in R-2.0.1:

test1 <- list(time=  c(4, 3,1,1,2,2,3),
                 status=c(1,NA,1,0,1,1,0),
                 x=     c(0, 2,1,1,1,0,0),
                 sex=   c(0, 0,0,0,1,1,1))
vcov(coxph( Surv(time, status) ~ x + strata(sex), test1))


Uwe Ligges



> I don't know if it is a bug or there is some update on this function. 
> Thanks!
> 
> Lei Liu
> Assistant Professor
> Division of Biostatistics and Epidemiology
> Dept. of Health Evaluation Sciences
> School of Medicine
> University of Virginia
> 
> 3181 Hospital West Complex
> Charlottesville, VA 22908-0717
> 
> 1-434-982-3364 (o)
> 1-734-730-1395 (c)
> 
> liulei at virginia.edu
> ll9f at virginia.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From mstrivens at houston.rr.com  Fri Nov 19 22:48:11 2004
From: mstrivens at houston.rr.com (Mark Strivens)
Date: Fri, 19 Nov 2004 15:48:11 -0600
Subject: [R] Rd and document formatting
In-Reply-To: <v4psp09secqqvo140v1mtp9485qcgqca9n@4ax.com>
References: <419E7358@webmail.bcm.tmc.edu>
	<v4psp09secqqvo140v1mtp9485qcgqca9n@4ax.com>
Message-ID: <419E6A1B.5080205@houston.rr.com>

Thanks Duncan,

I really know nothing about Latex - however the macro you detailed
below does not exist in the file: /usr/lib/R/share/texmf/Rd.sty

The header of that file reads:

%%% Rd.sty ... Style for printing the R manual
%%%
%%% Modified 1998/01/05 by Friedrich.Leisch at ci.tuwien.ac.at
%%% Modified 1998/07/07 by Martin Maechler
%%% Modified 1999/11/20 by Brian Ripley
%%% Modified 1999/12/26 by Kurt Hornik

My R version is 1.8.0 - can update the support files (like Rd.sty) 
without doing
a total R upgrade?

Thanks for your help

Mark

>>I am trying to compose some documentation for a package I hope to release 
>>soon.
>>
>>However when I do the following:
>>
>>R CMD Rd2dvi --pdf mypackage.Rd
>>
>>I get two mainly blank pages prepended to the top of the document - the only 
>>text on either (at the top of the first page) is as follows:
>>hyperindex,colorlinks,pagebackref,linktocpage,plainpages=false,linkcolor=Blue
>>,citecolor=Blue,urlcolor=Red,pdnull null null
>>
>>Curiously if you generate the dvi it is fine and if you generate the pdf from 
>>the dvi using dvipdf it is also fine...
>>
>>Anyone got any ideas?
>>
>>System: Linux 2.4.24 SMP i686 GNU/Linux, latex 3.14159-2.1 pdfTex 3.4.5
>>    
>>
>
>Looks like some LaTeX package is out of date, and doesn't understand
>some of the tex that got generated.  Rd.sty has this:
>
>  \hypersetup{%
>    hyperindex,%
>    colorlinks,%
>    pagebackref,%
>    linktocpage,%
>    plainpages=false,%
>    linkcolor=Blue,%
>    citecolor=Blue,%
>    urlcolor=Red,%
>    pdfstartview=Fit,%
>    pdfview={XYZ null null null}%
>  }
>
>so I think your system doesn't have the \hypersetup macro defined (or
>maybe you're using an out of date Rd.sty or hyperref.sty?)
>
>Duncan Murdoch
>
>
>  
>



From partha_bagchi at hgsi.com  Fri Nov 19 22:53:12 2004
From: partha_bagchi at hgsi.com (partha_bagchi@hgsi.com)
Date: Fri, 19 Nov 2004 16:53:12 -0500
Subject: [R] Re: 3d Map with bars
Message-ID: <OF494BB0AC.DE6D3001-ON85256F51.0078286C-85256F51.00783B92@hgsi.com>

Thanks for reply. I need to first draw the map of USA a perspective plot. 
I guess thats where my problem was.

Partha





Uwe Ligges <ligges at statistik.uni-dortmund.de>
11/19/2004 04:33 PM

 
        To:     partha_bagchi at hgsi.com
        cc:     r-help at stat.math.ethz.ch
        Subject:        Re: 3d Map with bars


partha_bagchi at hgsi.com wrote:

> Apologies in advance for the question. I am trying to draw a map of the 
US
> as a surface plot so that I would be able to drop bars on the different
> states (something like Uwe Ligges' scatterplot3d example 4). I am not 
sure
> where to start looking for such a beast. If anyone has any pointers,
> ideas, I will be grateful.
>
> TIA,
> Partha

How to "drop bars" with persp() has been described on R-help yesterday
or today, please check the mailing list's archives.

Uwe Ligges



From cliff at ms.washington.edu  Fri Nov 19 22:59:23 2004
From: cliff at ms.washington.edu (Cliff Lunneborg)
Date: Fri, 19 Nov 2004 13:59:23 -0800
Subject: [R] The hidden costs of GPL software?
Message-ID: <00d801c4ce83$083dc500$6401a8c0@C56909A>

Could I voice my support for the sixth point raised by John Fox? Many
users would find such a development to be enormously useful.


"  (6) As has been pointed out, e.g., by Duncan Murdoch, solving the
function-locating problem is best done by a method or methods that
automatically accommodate the growing and changing set of contributed
packages on CRAN.  Why not, as previously has been proposed, replace the
current static (and, in my view, not very useful) set of keywords in R
documentation with the requirement that package authors supply their own
keywords for each documented object? I believe that this is the intent
of
the concept entries in Rd files, but their use certainly is not required
or
even actively encouraged. (They're just mentioned in passing in the
Writing
R Extensions manual.)"


**********************************************************
Cliff Lunneborg, Professor Emeritus, Statistics &
Psychology, University of Washington, Seattle
cliff at ms.washington.edu



From feferraz at ime.usp.br  Fri Nov 19 23:17:01 2004
From: feferraz at ime.usp.br (Fernando Henrique Ferraz P. da Rosa)
Date: Fri, 19 Nov 2004 20:17:01 -0200
Subject: [R] Difference between two correlation matrices
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950121B88A@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E950121B88A@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <20041119221701.GA17638@ime.usp.br>

michael watson (IAH-C) writes:
> Hi 
> 
> Now a more theoretical question.  I have two correlation matrices - one
> of a set of variables under a particular condition, the other of the
> same set of variables under a different condition.  Is there a
> statistical test I can use to see if these correlation matrices are
> "different"?
> 
> Thanks
> Mick
> 


        If you can assume multivariate normality, you can use a test of
hypothesis to test if the covariance+ matrices are equal. Such a test is
described for example in Anderson (1958)*.

        I am currently working on implementing some multivariate tests
on R (sphericity test, equality of covariance matrices, etc). Attached
follows a preliminary version of varcomp(): this function implements the
test for equality of covariance matrices under multivare normality
aforementioned. It takes as first argument a list with the covariance
matrices and a vector n indicating the sample size used to calculate
each of them.

        Example of use: Suppose you have 3 estimated covariance matrices
S1, S2, S3, from a sample of 3 bivariate normal populations with unkown
covariance matrices. The sample size for each Si, was 11,12,11.

        S1 <- matrix(c(7.17,19.47,19.47,113.38),byrow=T,ncol=2)
        S2 <- matrix(c(20.33,59.78,59.78,229.02),byrow=T,ncol=2)
        S3 <- matrix(c(5.22,17.33,17.33,112.88),byrow=T,ncol=2)
        varcomp(list(S1,S2,S3),n=c(11,12,11))

        
  Will test H0: Sigma1 = Sigma2 = Sigma3, vs. H1: at least two of
them are different from each other. 

        Beware though that it's still work in progress, I've tested it
for a few examples and it gave sensible results, but it still needs some
polishing.

* An Introduction to Multivariate Analysis, Wiley.
+ note that a correlation matrix is a special type of a covariance
matrix, so you can use a test of hypothesis designed for covariance
matrices. 


--
Fernando Henrique Ferraz P. da Rosa
http://www.ime.usp.br/~feferraz



From feferraz at ime.usp.br  Fri Nov 19 23:18:53 2004
From: feferraz at ime.usp.br (Fernando Henrique Ferraz P. da Rosa)
Date: Fri, 19 Nov 2004 20:18:53 -0200
Subject: [R] Difference between two correlation matrices
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950121B88A@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E950121B88A@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <20041119221853.GA18042@ime.usp.br>

  er.. forgot to attach the file. there it goes. (sorry)

michael watson (IAH-C) writes:
> Hi 
> 
> Now a more theoretical question.  I have two correlation matrices - one
> of a set of variables under a particular condition, the other of the
> same set of variables under a different condition.  Is there a
> statistical test I can use to see if these correlation matrices are
> "different"?
> 
> Thanks
> Mick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 
--
Fernando Henrique Ferraz P. da Rosa
http://www.ime.usp.br/~feferraz
-------------- next part --------------
varcomp <- function(covmat,n) {
   if (is.list(covmat)) {
	if (length(covmat) < 2) 
	    stop("covmat must be a list with at least 2 elements")
	ps <- as.vector(sapply(covmat,dim))
	if (sum(ps[1] == ps) != length(ps))
	    stop("all covariance matrices must have the same dimension")
	p <- ps[1]	
        q <- length(covmat)
        if (length(n) == 1)
	    Ng <- rep(n,q)
	else if (length(n) == q) 
	    Ng <- n
	else 
	    stop("n must be equal length(covmat) or 1")
            
	DNAME <- deparse(substitute(covmat))
   }

   else
	stop("covmat must be a list")

   ng <- Ng - 1
   Ag <- lapply(1:length(covmat),function(i,mat,n) { n[i] * mat[[i]] },mat=covmat,n=ng)
   A <- matrix(colSums(matrix(unlist(Ag),ncol=p^2,byrow=T)),ncol=p)
   detAg <- sapply(Ag,det)
   detA <- det(A)
   V1 <- prod(detAg^(ng/2))/(detA^(sum(ng)/2))
   kg <- ng/sum(ng)
   l1 <- prod((1/kg)^kg)^(p*sum(ng)/2) * V1
   rho <- 1 - (sum(1/ng) - 1/sum(ng))*(2*p^2+3*p-1)/(6*(p+1)*(q-1))
   w2 <- p*(p+1) * ((p-1)*(p+2) * (sum(1/ng^2) - 1/(sum(ng)^2)) - 6*(q-1)*(1-rho)^2) / (48*rho^2)
   f <- 0.5 * (q-1)*p*(p+1)
   STATISTIC <- -2*rho*log(l1)
   PVAL <- 1 - (pchisq(STATISTIC,f) + w2*(pchisq(STATISTIC,f+4) - pchisq(STATISTIC,f)))
   names(STATISTIC) <- "corrected lambda*"
   names(f) <- "df"
   RVAL <- structure(list(statistic = STATISTIC, parameter = f,p.value = PVAL, data.name = DNAME, method = "Equality of Covariances Matrices Test"),class="htest")
   return(RVAL)
}

From sblay at cs.sfu.ca  Fri Nov 19 23:25:20 2004
From: sblay at cs.sfu.ca (Sigal Blay)
Date: Fri, 19 Nov 2004 14:25:20 -0800
Subject: [R] ERROR: installing package indices failed
Message-ID: <20041119222520.GA502@stawlmihq.cs.sfu.ca>

Dear R-helpers,

I am developing a package named LDehatmap.
It depends on the "genetics" package
and includes two data files and a demo file.
When I'm trying to install it, I get the following messages:

* Installing *source* package 'LDheatmap' ...
** R
** data
** demo
** help
 >>> Building/Updating help pages for package 'LDheatmap'
     Formats: text html latex example
  LDheatmap                text    html    latex   example
  ldheatmap                text    html    latex   example
Error: object 'reorder' not found whilst loading namespace 'gdata'
Error: package 'gdata' could not be loaded
Execution halted
ERROR: installing package indices failed

Any ideas?
 
Thanks,
  
Sigal



From ligges at statistik.uni-dortmund.de  Fri Nov 19 23:27:51 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 19 Nov 2004 23:27:51 +0100
Subject: [R] Rd and document formatting
In-Reply-To: <419E6A1B.5080205@houston.rr.com>
References: <419E7358@webmail.bcm.tmc.edu>	<v4psp09secqqvo140v1mtp9485qcgqca9n@4ax.com>
	<419E6A1B.5080205@houston.rr.com>
Message-ID: <419E7367.5070503@statistik.uni-dortmund.de>

Mark Strivens wrote:
> Thanks Duncan,
> 
> I really know nothing about Latex - however the macro you detailed
> below does not exist in the file: /usr/lib/R/share/texmf/Rd.sty
> 
> The header of that file reads:
> 
> %%% Rd.sty ... Style for printing the R manual
> %%%
> %%% Modified 1998/01/05 by Friedrich.Leisch at ci.tuwien.ac.at
> %%% Modified 1998/07/07 by Martin Maechler
> %%% Modified 1999/11/20 by Brian Ripley
> %%% Modified 1999/12/26 by Kurt Hornik
> 
> My R version is 1.8.0 - can update the support files (like Rd.sty) 
> without doing
> a total R upgrade?

Please, do upgrade! You will also get problems when updating packages.

BTW: You should have told about your completely outdated R version in 
your first question!

Uwe Ligges



> Thanks for your help
> 
> Mark
> 
>>> I am trying to compose some documentation for a package I hope to 
>>> release soon.
>>>
>>> However when I do the following:
>>>
>>> R CMD Rd2dvi --pdf mypackage.Rd
>>>
>>> I get two mainly blank pages prepended to the top of the document - 
>>> the only text on either (at the top of the first page) is as follows:
>>> hyperindex,colorlinks,pagebackref,linktocpage,plainpages=false,linkcolor=Blue 
>>>
>>> ,citecolor=Blue,urlcolor=Red,pdnull null null
>>>
>>> Curiously if you generate the dvi it is fine and if you generate the 
>>> pdf from the dvi using dvipdf it is also fine...
>>>
>>> Anyone got any ideas?
>>>
>>> System: Linux 2.4.24 SMP i686 GNU/Linux, latex 3.14159-2.1 pdfTex 3.4.5
>>>   
>>
>>
>> Looks like some LaTeX package is out of date, and doesn't understand
>> some of the tex that got generated.  Rd.sty has this:
>>
>>  \hypersetup{%
>>    hyperindex,%
>>    colorlinks,%
>>    pagebackref,%
>>    linktocpage,%
>>    plainpages=false,%
>>    linkcolor=Blue,%
>>    citecolor=Blue,%
>>    urlcolor=Red,%
>>    pdfstartview=Fit,%
>>    pdfview={XYZ null null null}%
>>  }
>>
>> so I think your system doesn't have the \hypersetup macro defined (or
>> maybe you're using an out of date Rd.sty or hyperref.sty?)
>>
>> Duncan Murdoch
>>
>>
>>  
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From h.wickham at gmail.com  Fri Nov 19 23:44:18 2004
From: h.wickham at gmail.com (hadley wickham)
Date: Fri, 19 Nov 2004 16:44:18 -0600
Subject: [R] Running sum
In-Reply-To: <1100900521.31008.65.camel@horizons.localdomain>
References: <200411192010.iAJKAcDM019929@outmx001.isp.belgacom.be>
	<1100900521.31008.65.camel@horizons.localdomain>
Message-ID: <f8e6ff05041119144472b3854e@mail.gmail.com>

Perhaps I'm missing something, but isn't the maximum of the cumulative
sum simply the last value, ie. sum(x)?

Hadley



From MSchwartz at MedAnalytics.com  Sat Nov 20 00:50:23 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 19 Nov 2004 17:50:23 -0600
Subject: [R] Running sum
In-Reply-To: <f8e6ff05041119144472b3854e@mail.gmail.com>
References: <200411192010.iAJKAcDM019929@outmx001.isp.belgacom.be>
	<1100900521.31008.65.camel@horizons.localdomain>
	<f8e6ff05041119144472b3854e@mail.gmail.com>
Message-ID: <1100908223.31008.79.camel@horizons.localdomain>

On Fri, 2004-11-19 at 16:44 -0600, hadley wickham wrote:
> Perhaps I'm missing something, but isn't the maximum of the cumulative
> sum simply the last value, ie. sum(x)?
> 
> Hadley


Indeed!

And...going back to Sean's original post he did write:

"I just need max(Y)."

Thus, a whole group of us (independently) managed to over-engineer the
solution. 

Presuming of course, that Sean does not require the entire cumulative
sum vector for other purposes.

I wonder what that says about human cognition....

Thanks,

Marc
<Cleaning his bi-focals...>



From kjetil at acelerate.com  Sat Nov 20 01:00:06 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Fri, 19 Nov 2004 20:00:06 -0400
Subject: [R] Running sum
In-Reply-To: <1100908223.31008.79.camel@horizons.localdomain>
References: <200411192010.iAJKAcDM019929@outmx001.isp.belgacom.be>	<1100900521.31008.65.camel@horizons.localdomain>	<f8e6ff05041119144472b3854e@mail.gmail.com>
	<1100908223.31008.79.camel@horizons.localdomain>
Message-ID: <419E8906.1040702@acelerate.com>

Marc Schwartz wrote:

>On Fri, 2004-11-19 at 16:44 -0600, hadley wickham wrote:
>  
>
>>Perhaps I'm missing something, but isn't the maximum of the cumulative
>>sum simply the last value, ie. sum(x)?
>>
>>Hadley
>>    
>>
>
>
>Indeed!
>
>And...going back to Sean's original post he did write:
>
>"I just need max(Y)."
>
>Thus, a whole group of us (independently) managed to over-engineer the
>  
>
Please, Please, please, ... was it ever said that the numbers was only 
positive?

Kjetil

>solution. 
>
>Presuming of course, that Sean does not require the entire cumulative
>sum vector for other purposes.
>
>I wonder what that says about human cognition....
>
>Thanks,
>
>Marc
><Cleaning his bi-focals...>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From bates at stat.wisc.edu  Sat Nov 20 01:00:26 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 19 Nov 2004 18:00:26 -0600
Subject: [R] Running sum
In-Reply-To: <1100908223.31008.79.camel@horizons.localdomain>
References: <200411192010.iAJKAcDM019929@outmx001.isp.belgacom.be>	<1100900521.31008.65.camel@horizons.localdomain>	<f8e6ff05041119144472b3854e@mail.gmail.com>
	<1100908223.31008.79.camel@horizons.localdomain>
Message-ID: <419E891A.3090008@stat.wisc.edu>

Marc Schwartz wrote:
> On Fri, 2004-11-19 at 16:44 -0600, hadley wickham wrote:
> 
>>Perhaps I'm missing something, but isn't the maximum of the cumulative
>>sum simply the last value, ie. sum(x)?
>>
>>Hadley
> 
> 
> 
> Indeed!
> 
> And...going back to Sean's original post he did write:
> 
> "I just need max(Y)."
> 
> Thus, a whole group of us (independently) managed to over-engineer the
> solution. 
> 
> Presuming of course, that Sean does not require the entire cumulative
> sum vector for other purposes.
> 
> I wonder what that says about human cognition....
> 
> Thanks,
> 
> Marc
> <Cleaning his bi-focals...>

I have forgotten the details of the original question but it seems to me 
that if the elements of x could be both positive and negative then the 
maximum of the cumulative sum doesn't have to be the last sum.



From murdoch at stats.uwo.ca  Sat Nov 20 01:16:44 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 19 Nov 2004 19:16:44 -0500
Subject: [R] Rd and document formatting
In-Reply-To: <419E6A1B.5080205@houston.rr.com>
References: <419E7358@webmail.bcm.tmc.edu>
	<v4psp09secqqvo140v1mtp9485qcgqca9n@4ax.com>
	<419E6A1B.5080205@houston.rr.com>
Message-ID: <qu2tp0tlgdgt6a9idvsaeh0ucq0ij4vv5u@4ax.com>

On Fri, 19 Nov 2004 15:48:11 -0600, Mark Strivens
<mstrivens at houston.rr.com> wrote:

>Thanks Duncan,
>
>I really know nothing about Latex - however the macro you detailed
>below does not exist in the file: /usr/lib/R/share/texmf/Rd.sty
>
>The header of that file reads:
>
>%%% Rd.sty ... Style for printing the R manual
>%%%
>%%% Modified 1998/01/05 by Friedrich.Leisch at ci.tuwien.ac.at
>%%% Modified 1998/07/07 by Martin Maechler
>%%% Modified 1999/11/20 by Brian Ripley
>%%% Modified 1999/12/26 by Kurt Hornik
>
>My R version is 1.8.0 - can update the support files (like Rd.sty) 
>without doing
>a total R upgrade?

I don't know, but my instinct would be to avoid mixing files from
different versions.  What you have is a pretty old version, which had
a pretty long list of bugs that were fixed in 1.8.1.  I'd really
strongly recommend upgrading at least to there, and preferably to
2.0.1.  

Duncan Murdoch



From MSchwartz at MedAnalytics.com  Sat Nov 20 01:16:03 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 19 Nov 2004 18:16:03 -0600
Subject: [R] Running sum
In-Reply-To: <419E891A.3090008@stat.wisc.edu>
References: <200411192010.iAJKAcDM019929@outmx001.isp.belgacom.be>
	<1100900521.31008.65.camel@horizons.localdomain>
	<f8e6ff05041119144472b3854e@mail.gmail.com>
	<1100908223.31008.79.camel@horizons.localdomain>
	<419E891A.3090008@stat.wisc.edu>
Message-ID: <1100909763.31008.83.camel@horizons.localdomain>

On Fri, 2004-11-19 at 18:00 -0600, Douglas Bates wrote:

> I have forgotten the details of the original question but it seems to me 
> that if the elements of x could be both positive and negative then the 
> maximum of the cumulative sum doesn't have to be the last sum.

On Fri, 2004-11-19 at 20:00 -0400, Kjetil Brinchmann Halvorsen wrote:

> Please, Please, please, ... was it ever said that the numbers was only 
> positive?


Thanks to both Doug and Kjetil.

I stand corrected.

Marc



From graumann at caltech.edu  Sat Nov 20 01:21:43 2004
From: graumann at caltech.edu (Johannes Graumann)
Date: Fri, 19 Nov 2004 16:21:43 -0800
Subject: [R] annotation problems (conditional text())
Message-ID: <20041119162143.52d36a49@localhost>

Hello,

I'm trying to annotate my plots nicely and am running into trouble.
This example contains two problems:
a) the 'text()' arguments do not show the conditional behavior I'm
trying to give them. I try to test for the intercept of my regression
and reformat the output accordingly ('+ intercept' in the >= 0 case and
'- sqrt(intercept^2)' in the other case), which doesn't work this way.
b) my pasted substitute commands yield jolted output, which overwrites
itself ...

Can anyone give this newbie a nudge into the right direction?

Thanks, Joh

if(m >= 0){
  text(
    4,0.19,
    cex=0.75,
    adj=0,
    paste(
      substitute(
        y == m*x+b,
        list(
          m=round(m,digits=5),
          b=round(b,digits=5)
        )
      ),
      "\n",
      substitute(
        R^2==rsquared,
        list(
          rsquared=round(summary(fit)$r.squared,digits=3)
        )
      )
    )
  );
  text(
    4,0.19,
    cex=0.75,
    adj=0,
    paste(
      substitute(
        y == m*x-b,
        list(
          m=round(m,digits=5),
          b=round(sqrt(b^2),digits=5)
        )
      ),
      "\n",
      substitute(
        R^2==rsquared,
        list(
          rsquared=round(summary(fit)$r.squared,digits=3)
        )
      )
    )
  )
}



From lj22 at u.washington.edu  Sat Nov 20 01:54:44 2004
From: lj22 at u.washington.edu (Lei Jiang)
Date: Fri, 19 Nov 2004 16:54:44 -0800 (PST)
Subject: [R] subset on data frame
Message-ID: <Pine.A41.4.61b.0411191648490.335134@homer06.u.washington.edu>

I have a data frame. And I'd like to subset according to rownames.

subset(mydataframe, rownames(mydataframe) == myrow, select = mycols)

it turned out that "myrow" cannot be a vector. But I have multiple rows to 
pick. Is there a way to get around this problem??

Thank you for your help!!

Lei Jiang

Department of Chemsitry
University of Washington
Box 351700
Seattle, WA 98195
Phone: 206-616-6882
Fax: 206-685-8665



From tlumley at u.washington.edu  Sat Nov 20 02:01:05 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 19 Nov 2004 17:01:05 -0800 (PST)
Subject: [R] subset on data frame
In-Reply-To: <Pine.A41.4.61b.0411191648490.335134@homer06.u.washington.edu>
References: <Pine.A41.4.61b.0411191648490.335134@homer06.u.washington.edu>
Message-ID: <Pine.A41.4.61b.0411191700540.250526@homer11.u.washington.edu>

On Fri, 19 Nov 2004, Lei Jiang wrote:

> I have a data frame. And I'd like to subset according to rownames.
>
> subset(mydataframe, rownames(mydataframe) == myrow, select = mycols)
>
> it turned out that "myrow" cannot be a vector. But I have multiple rows to 
> pick. Is there a way to get around this problem??
>

?"%in%"

 	-thomas



From ripley at stats.ox.ac.uk  Sat Nov 20 08:46:03 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 20 Nov 2004 07:46:03 +0000 (GMT)
Subject: [R] ERROR: installing package indices failed
In-Reply-To: <20041119222520.GA502@stawlmihq.cs.sfu.ca>
References: <20041119222520.GA502@stawlmihq.cs.sfu.ca>
Message-ID: <Pine.LNX.4.61.0411200742560.12645@gannet.stats>

On Fri, 19 Nov 2004, Sigal Blay wrote:

> Dear R-helpers,
>
> I am developing a package named LDehatmap.
> It depends on the "genetics" package
> and includes two data files and a demo file.
> When I'm trying to install it, I get the following messages:
>
> * Installing *source* package 'LDheatmap' ...
> ** R
> ** data
> ** demo
> ** help
> >>> Building/Updating help pages for package 'LDheatmap'
>     Formats: text html latex example
>  LDheatmap                text    html    latex   example
>  ldheatmap                text    html    latex   example
> Error: object 'reorder' not found whilst loading namespace 'gdata'
> Error: package 'gdata' could not be loaded
> Execution halted
> ERROR: installing package indices failed
>
> Any ideas?

Yes.  You do not have gdata (part of gregmisc) installed, and genetics 
depends on it.  How did you get genetics installed? A binary install?

Install gregmisc ....

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From snvk4u at yahoo.co.in  Sat Nov 20 09:18:04 2004
From: snvk4u at yahoo.co.in (neela v)
Date: Sat, 20 Nov 2004 00:18:04 -0800 (PST)
Subject: [R] SAS or R software
Message-ID: <20041120081804.90157.qmail@web8403.mail.in.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041120/8cfdeda8/attachment.pl

From francoisromain at free.fr  Sat Nov 20 10:25:43 2004
From: francoisromain at free.fr (=?ISO-8859-1?Q?Romain_Fran=E7ois?=)
Date: Sat, 20 Nov 2004 10:25:43 +0100
Subject: [R] SAS or R software
In-Reply-To: <20041120081804.90157.qmail@web8403.mail.in.yahoo.com>
References: <20041120081804.90157.qmail@web8403.mail.in.yahoo.com>
Message-ID: <419F0D97.3040506@free.fr>

Hello,

I think you'll get some bias if you do a survey on what software is 
better SAS or R in a R mailing list.

Personnaly, what I like in R is :
    - the fact to you have to know what you are doing to do it,
    - the graphic potential is outstanding (only limited by the 
imagination of the user, just check the graph on the web page of R : 
http://www.r-project.org/ and try to do so with SAS, goooood luck !! )
    - the mailing list is great
    - and many many many other things

what I like in SAS :
    - nothing

So in my opinion, leaving or considering the commercial aspect (but that 
counts too) I think there is no competition.

Something else, go on http://www.sas.com/ and search for the word 
"statistic", missing, isn't it curious for a statistical software ?


neela v a ??crit :

>Hi all there
> 
>Can some one clarify me on this issue, features wise which is better R or SAS, leaving the commerical aspect associated with it. I suppose there are few people who have worked on both R and SAS and wish they would be able to help me in deciding on this.
> 
>THank you for the help
>  
>
-- 
Romain FRANCOIS : francoisromain at free.fr
06 18 39 14 69 / 01 46 80 65 60
_______________________________________________________
Etudiant en 3eme ann??e
Institut de Statistique de l'Universit?? de Paris (ISUP)
Fili??re Industrie et Services
http://www.isup.cicrp.jussieu.fr/



From phgrosjean at sciviews.org  Sat Nov 20 11:02:03 2004
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Sat, 20 Nov 2004 11:02:03 +0100
Subject: [R] Running sum
In-Reply-To: <200411192010.iAJKAcDM019929@outmx001.isp.belgacom.be>
Message-ID: <200411201002.iAKA25fF022065@outmx014.isp.belgacom.be>

Ooops! Sorry. I did not look carefully enough on the code and focused on the
term! My proposition is for a "running sum with a window width of 2
observations" (the title of the question is "Running sum", isn't it?), while
the true question was indeed about a *cumulative sum*, which is something
totally different.
Best,

Philippe Grosjean 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Philippe Grosjean
> Sent: Friday, November 19, 2004 9:11 PM
> To: MSchwartz at MedAnalytics.com; 'Sean Davis'
> Cc: 'R-Help'
> Subject: RE: [R] Running sum
> 
> ?cumsum is not exactly the answer (as I understand it), but a 
> part of it.
> I propose:
> 
> runSum2 <- function(x)
> 	cumsum(x)[-1] - c(0, cumsum(x[1:(length(x) - 2)]))
> 
> # Example
> a <- round(runif(10, 0, 10))
> a
> runSum2(a)
> max(runSum2(a)) # To get only the max
> 
> Best,
> 
> Philippe
> 
> ..............................................<??}))><........
>  ) ) ) ) )
> ( ( ( ( (    Prof. Philippe Grosjean
>  ) ) ) ) )
> ( ( ( ( (    Numerical Ecology of Aquatic Systems
>  ) ) ) ) )   Mons-Hainaut University, Pentagone
> ( ( ( ( (    Academie Universitaire Wallonie-Bruxelles
>  ) ) ) ) )   6, av du Champ de Mars, 7000 Mons, Belgium  
> ( ( ( ( (       
>  ) ) ) ) )   phone: + 32.65.37.34.97, fax: + 32.65.37.33.12
> ( ( ( ( (    email: Philippe.Grosjean at umh.ac.be
>  ) ) ) ) )      
> ( ( ( ( (    web:   http://www.umh.ac.be/~econum
>  ) ) ) ) )
> ..............................................................
> 
>  
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch 
> > [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Marc Schwartz
> > Sent: Friday, November 19, 2004 7:57 PM
> > To: Sean Davis
> > Cc: R-Help
> > Subject: Re: [R] Running sum
> > 
> > On Fri, 2004-11-19 at 13:08 -0500, Sean Davis wrote:
> > > I have vector X of length N that I want to have a running sum for 
> > > (called Y).  I just need max(Y).  I do this with a "for"
> > loop like so:
> > > 
> > >      Y <- vector(length=N)
> > >      Y[1] <- X[1]
> > >      for (i in 2:N) {
> > >        Y[i] <- Y[i-1]+X[i]
> > >      }
> > >      return(max(Y))
> > > 
> > > Is there a faster way to do this?
> > > 
> > > Thanks,
> > > Sean
> > 
> > 
> > Something like:
> > 
> > > cumsum(1:10)
> >  [1]  1  3  6 10 15 21 28 36 45 55
> > 
> > > max(cumsum(1:10))
> > [1] 55
> > 
> > Does that help?
> > 
> > Marc Schwartz
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Kevin.Wang at maths.anu.edu.au  Sat Nov 20 11:39:45 2004
From: Kevin.Wang at maths.anu.edu.au (Kevin Wang)
Date: Sat, 20 Nov 2004 21:39:45 +1100 (EST)
Subject: [R] SAS or R software
In-Reply-To: <20041120081804.90157.qmail@web8403.mail.in.yahoo.com>
References: <20041120081804.90157.qmail@web8403.mail.in.yahoo.com>
Message-ID: <Pine.GSO.4.58.0411202137230.2827@yin>

Hi,

On Sat, 20 Nov 2004, neela v wrote:

> Hi all there
>
> Can some one clarify me on this issue, features wise which is better R or SAS, leaving the commerical aspect associated with it. I suppose there are few people who have worked on both R and SAS and wish they would be able to help me in deciding on this.

R is THE one to use when it comes to graphics.  So far, I haven't seen any
other software that can produce better graphs (and I've used SAS, Minitab,
Excel, Genstat...etc).  It's programming feature, IMHO, is also neater
than SAS's procedure-oriented programming.

Cheers,

Kevin

--------------------------------
Ko-Kang Kevin Wang
PhD Student
Centre for Mathematics and its Applications
Building 27, Room 1004
Mathematical Sciences Institute (MSI)
Australian National University
Canberra, ACT 0200
Australia

Homepage: http://wwwmaths.anu.edu.au/~wangk/
Ph (W): +61-2-6125-2431
Ph (H): +61-2-6125-7407
Ph (M): +61-40-451-8301



From baron at psych.upenn.edu  Sat Nov 20 13:11:55 2004
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Sat, 20 Nov 2004 07:11:55 -0500
Subject: [R] SAS or R software
In-Reply-To: <419F0D97.3040506@free.fr>
References: <20041120081804.90157.qmail@web8403.mail.in.yahoo.com>
	<419F0D97.3040506@free.fr>
Message-ID: <20041120121155.GA20979@psych>

There was at least one previous discussion of SAS vs. R on this
list.  I searched my archives (below).  The thread begins at
http://finzi.psych.upenn.edu/R/Rhelp01/archive/5947.html
but then changes title.

Two arguments for SAS:

1. You may be working with other people who use it.  (That is why
   many of my colleagues use it, aside from inertia and [rational
   - since they have no choice] ignorance of alternatives.)

2. It used to be true, and it may still be true, that SAS
   compiles all code before running it, whereas R uses many
   compiled routines but does not compile the code you write
   yourself.  Thus, SAS may be faster for huge data sets, like
   census data.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron
R search page: http://finzi.psych.upenn.edu/



From feferraz at ime.usp.br  Sat Nov 20 14:33:42 2004
From: feferraz at ime.usp.br (Fernando Henrique Ferraz P. da Rosa)
Date: Sat, 20 Nov 2004 11:33:42 -0200
Subject: [R] SAS or R software
In-Reply-To: <20041120081804.90157.qmail@web8403.mail.in.yahoo.com>
References: <20041120081804.90157.qmail@web8403.mail.in.yahoo.com>
Message-ID: <20041120133342.GA30792@ime.usp.br>

neela v writes:
> Hi all there
>  
> Can some one clarify me on this issue, features wise which is better R or SAS, leaving the commerical aspect associated with it. I suppose there are few people who have worked on both R and SAS and wish they would be able to help me in deciding on this.
>  
> THank you for the help
> 

        I'm definitely biased towards R, but I'll try to be the devil's
advocate and point some advantages of SAS.

        - There's a huge collection of data-manipulation features. You
          can parse all sort of weird files using DATA/INPUT statements.
You can reshape, merge, combine, summarize, define and propagate missing
observations, (...) using the inumerous features of the 'SAS DATA Step
Language', as the manuals call it. Sure, there's much of it you can do
in plain R, but R comes from the Unix tradition of specifc tools for
specific tasks, so it doesn't try to be everything to everyone. In
order to match SAS' Data Step Lanaguage using R, you would have to use a
language more fitted for such tasks, like Perl/awk for parsing and a SQL
Database for storing and reshaping. These languages integrate nicely
with R, and if you can really exploit their potential, R + Perl/awk + SQL
will definetly surpass SAS data-manipulation features.

        - There are some statistical analysis which are more completely
          implemented in SAS. PROC VARCOMP comes to mind (I know you can
use lme for mixed models, but you have to use a cumbersome syntax when
you're not dealing with nested but crossed random effects). Sure,
there's nothing stoping you from using R and adapting it to your
needs or implementing the analysis you need to do. While at first it
will take you more time to actually think about the problem and how to
implement it, the flexibility you'll gain will probably pay off.  

        Summarizing, I think the main difference between SAS and R is
their philosophy and how they reflect on their implementations. While SAS
aims to "(...) provide data analysts one system to meet all their
computing needs*", R tries to be the best tool for the specific task of
statistical analysis. SAS's approach allows you to learn only one single
language and use it for almost all computing needs you'll have. While it
seems appealing on first sight, after you reach a certain level of
proficiency, the lack of flexibility of this approach starts to limit
what you can actually do. You'll be locked inside what you can do using
the PROC/DATA procedures provided by SAS; if you want to implement new
analysis etc, you will have a hard time. R's aproach on the other hand,
may seem harder at first, as you'll have to learn (if you don't know
some of them already, that is) specific languages for certain tasks,
such as* LaTeX or HTML for reports, SQL databases, Perl/Awk for data
parsing, C/C++/Fortran for implementing high performance functions, etc.
You'll be higly compensanted though by the gains in flexibility and
productivity. Not to mention that the more prociency you have in R and
the other tools of your choice, more flexibility and power you'll have
at hand to implement new analysis, manipulate data, create reports
dinamically and so on.

        It nails down to how much time/effort you are willing to spend
and for how long you're going to be using the languages. If you prefer
to spend at first a great amount of money, and less time, by learning
only one language, and don't mind being limited in the future on the
range of things you'll be able to do, SAS is the appropriate choice. If
you can spare the time to learn R and a set of appropriate tools for
each task you'll want to do, it'll take you more time at first but in
the end you'll have much more power and flexiblity at hands.



* SAS User's Guide: Basics.
* Strictly speaking you don't have to learn any of those. You can get
  along well using plain R in the beginning, but in order to exploit the
  power of it's approach, you'll find yourself in need to use one or
  more of them.


--
Fernando Henrique Ferraz P. da Rosa
http://www.ime.usp.br/~feferraz



From f.harrell at vanderbilt.edu  Sat Nov 20 14:42:34 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sat, 20 Nov 2004 08:42:34 -0500
Subject: [R] SAS or R software
In-Reply-To: <20041120081804.90157.qmail@web8403.mail.in.yahoo.com>
References: <20041120081804.90157.qmail@web8403.mail.in.yahoo.com>
Message-ID: <419F49CA.3060908@vanderbilt.edu>

neela v wrote:
> Hi all there
>  
> Can some one clarify me on this issue, features wise which is better R or SAS, leaving the commerical aspect associated with it. I suppose there are few people who have worked on both R and SAS and wish they would be able to help me in deciding on this.
>  
> THank you for the help

I estimate that SAS is about 11 years behind R in statistical analysis 
and graphics capabilities, and that gap is growing.  Also, code in SAS 
is not peer-reviewed as is code in R.  SAS has advantages in two areas: 
dealing with massive datasets (generally speaking, > 1GB) and getting 
more accurate P-values in mixed effect models.

See http://biostat.mc.vanderbilt.edu/twiki/pub/Main/RS/sintro.pdf for 
one comparison of SAS and S on technical features.

There are companies whose yearly license fees to SAS total millions of 
dollars.  Then those companies hire armies of SAS programmers to program 
an archaic macro language using old statistical methods to produce ugly 
tables and the worst graphics in the statistical software world.

Frank Harrell
SAS User, 1969-1991
-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From rolf at math.unb.ca  Sat Nov 20 14:44:40 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Sat, 20 Nov 2004 09:44:40 -0400 (AST)
Subject: [R] SAS or R software
Message-ID: <200411201344.iAKDierH008922@erdos.math.unb.ca>

neela v wrote:

> Can some one clarify me on this issue, features wise which is better
> R or SAS, leaving the commerical aspect associated with it. I suppose
> there are few people who have worked on both R and SAS and wish they
> would be able to help me in deciding on this.

My personal proclivities incline me to use SAS (or Minitab) for
analysing mixed effects models.  I have never been able to get my
head around the S/R syntax for such models, whereas the SAS/Minitab
syntax seems perfectly clear to me.

For ***ANYTHING*** else I would use R.

(In my work I don't encounter the multi-multi-megabyte data sets
for which SAS apparently has a pronounced advantage with respect
to speed.)

				cheers,

					Rolf Turner
					rolf at math.unb.ca



From michael.watson at bbsrc.ac.uk  Sat Nov 20 15:57:18 2004
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Sat, 20 Nov 2004 14:57:18 -0000
Subject: [R] subset on data frame
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950121B899@iahce2knas1.iah.bbsrc.reserved>

Try something like

mydataframe[rownames(mydataframe) %in% myrow,]

Mick

-----Original Message-----
From:	Lei Jiang [mailto:lj22 at u.washington.edu]
Sent:	Sat 11/20/2004 12:54 AM
To:	R-help at stat.math.ethz.ch
Cc:	
Subject:	[R] subset on data frame
I have a data frame. And I'd like to subset according to rownames.

subset(mydataframe, rownames(mydataframe) == myrow, select = mycols)

it turned out that "myrow" cannot be a vector. But I have multiple rows to 
pick. Is there a way to get around this problem??

Thank you for your help!!

Lei Jiang

Department of Chemsitry
University of Washington
Box 351700
Seattle, WA 98195
Phone: 206-616-6882
Fax: 206-685-8665

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Sat Nov 20 17:43:37 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 20 Nov 2004 17:43:37 +0100
Subject: [R] annotation problems (conditional text())
In-Reply-To: <20041119162143.52d36a49@localhost>
References: <20041119162143.52d36a49@localhost>
Message-ID: <419F7439.7080106@statistik.uni-dortmund.de>

Johannes Graumann wrote:

> Hello,
> 
> I'm trying to annotate my plots nicely and am running into trouble.
> This example contains two problems:
> a) the 'text()' arguments do not show the conditional behavior I'm
> trying to give them. I try to test for the intercept of my regression
> and reformat the output accordingly ('+ intercept' in the >= 0 case and
> '- sqrt(intercept^2)' in the other case), which doesn't work this way.
> b) my pasted substitute commands yield jolted output, which overwrites
> itself ...
> 
> Can anyone give this newbie a nudge into the right direction?

1. Multi-line mathematical annotation is not supported. You have to add 
them line by line.

2. You want to use a construct such as
   if(m >= 0){
     # code for text 1
   } else{
     # code for text 2
   }
rather than the one you speicfied below, which evaluates both text() 
calls if m > 0.

Uwe Ligges


> Thanks, Joh
> 
> if(m >= 0){
>   text(
>     4,0.19,
>     cex=0.75,
>     adj=0,
>     paste(
>       substitute(
>         y == m*x+b,
>         list(
>           m=round(m,digits=5),
>           b=round(b,digits=5)
>         )
>       ),
>       "\n",
>       substitute(
>         R^2==rsquared,
>         list(
>           rsquared=round(summary(fit)$r.squared,digits=3)
>         )
>       )
>     )
>   );
>   text(
>     4,0.19,
>     cex=0.75,
>     adj=0,
>     paste(
>       substitute(
>         y == m*x-b,
>         list(
>           m=round(m,digits=5),
>           b=round(sqrt(b^2),digits=5)
>         )
>       ),
>       "\n",
>       substitute(
>         R^2==rsquared,
>         list(
>           rsquared=round(summary(fit)$r.squared,digits=3)
>         )
>       )
>     )
>   )
> }
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From adiamond at fas.harvard.edu  Sat Nov 20 17:43:58 2004
From: adiamond at fas.harvard.edu (Alexis J. Diamond)
Date: Sat, 20 Nov 2004 11:43:58 -0500 (EST)
Subject: [R] how to suppress whiskers in a bwplot? 
Message-ID: <Pine.LNX.4.58.0411201143280.27736@ice3.fas.harvard.edu>


dear R-help,

i have looked carefully through the R-help archives for information on how
to suppress whiskers in a bwplot.  someone asked this question a while
ago, but the answer he received is not available in the archives.

but i did manage to get my hands on a panel function (called
"my.panel") that is supposed to do this (the function is reproduced at the
end of the email, below).  the problem is that i get an error message
when i use it in the following way:

####
data(singer)

bwplot(voice.part ~ height, data=singer, xlab="Height (inches)",
     panel = "my.panel")
####

### the error message is:

Error in segments(x1 = structure(c(NA, NA, NA, NA), .Names = c("", "",  :
        Argument "x0" is missing, with no default

###

i don't know if the problem is this panel function, or how I am
(mis)using it.
(i've never used a homegrown lattice panel function before.)

i realize that there is an excellent function in Hmisc that generates cool
bwplots sans whiskers, but those plots are too fancy for my
current needs.

all i need are regular boxy bwplots without whiskers
(or umbrellas, as i guess they're also called).

as a quick fix, i've also tried changing the color of the whiskers to the
background color to make the whiskers invisible, but this doesn't work
well when the whiskers perfectly align with the border of the box.

thank you for your help,

alexis diamond
adiamond at fas.harvard.edu


###  my.panel, the function that I've been told can suppress whiskers,
###  when outline = F

my.panel <- function(x, y, box.ratio = 1, font = box.dot$font, pch = box.dot$pch,
 cex = box.dot$cex, col = box.dot$col, outline = T, ...)
{
 ok <- !is.na(x) & !is.na(y)
 x <- x[ok]
 y <- y[ok]
 y.unique <- sort(unique(y))
 width <- box.ratio/(1 + box.ratio)
 w <- width/2
 e <- par("cxy")[1]
 for(Y in y.unique) {
  X <- x[y == Y]
  q <- quantile(X, c(0.75, 0.5, 0.25))
  iqr <- q[1] - q[3]
  d <- q[c(1, 3)] + c(1, -1) * 1.5 * iqr
  up.w <- max(X[X <= d[1]], q[1])
  lo.w <- min(X[X >= d[2]], q[3])
  outliers <- X[X < lo.w | X > up.w]
  X <- c(up.w, q, lo.w)
  median.value <- list(x = X[3], y = Y)
  Box <- list(x1 = X[c(2, 4, 4, 2)], y1 = Y + c( - w,
    - w, w, w), x2 = X[c(4, 4, 2, 2)], y2 = Y +
   c( - w, w, w,  - w))
  e <- par("cxy")[1]
  e.l <- min(e, (X[4] - X[5])/2)
  # prevent lower staple ends from touching box
  e.u <- min(e, (X[1] - X[2])/2)
  # prevent upper staple ends from touching box
  staple.ends <- list(x1 = rep(c(X[5], max(X[1] - e.u,
   X[2])), 2), y1 = c(rep(Y - w, 2), rep(Y + w,
   2)), x2 = rep(c(min(X[5] + e.l, X[4]), X[1]),
   2), y2 = c(rep(Y - w, 2), rep(Y + w, 2)))
  staple.body <- list(x1 = X[c(1, 5)], y1 = rep(Y - w,
   2), x2 = X[c(1, 5)], y2 = rep(Y + w, 2))
  dotted.line <- list(x1 = X[c(1, 4)], y1 = c(Y, Y),
   x2 = X[c(2, 5)], y2 = c(Y, Y))
  box.umbrella <- trellis.par.get("box.umbrella")
  box.dot <- trellis.par.get("box.dot")
  box.dot.par <- c(list(pch = pch, cex = cex, col = col,
   font = font), ...)
  do.call("segments", c(staple.ends, box.umbrella))
  do.call("segments", c(staple.body, box.umbrella))
  do.call("segments", c(dotted.line, box.umbrella))
  do.call("segments", c(Box, trellis.par.get(
   "box.rectangle")))
  do.call("points", c(median.value, box.dot.par))
  if(outline & length(outliers) > 0) {
   outliers <- list(x = outliers, y = rep(Y,
    length(outliers)))
   do.call("points", c(outliers, trellis.par.get(
    "plot.symbol"), cex = cex))
  }
 }
}



From raheem at gmail.com  Sat Nov 20 18:10:35 2004
From: raheem at gmail.com (Enayetur RAHEEM)
Date: Sat, 20 Nov 2004 12:10:35 -0500
Subject: [R] solving system of nonlinear equations
In-Reply-To: <419801B2.30409@pdf.com>
References: <8290920504111406452ab79603@mail.gmail.com>
	<419801B2.30409@pdf.com>
Message-ID: <8290920504112009105bcdef3@mail.gmail.com>

R version: 2.0.0
OS: WinXP, SP2

I am using "nls" to estimate parameters of a system of nonlinear
equations. Although, iteration is not converging, I would like to get
the final estimates and store them in the object, say, "RR".

Any help would be appreciated. Thanks. 

The following is not working ( I mean, I can not store it in "RR")

RR=nls(k~exp(-(q1-lam)/sig)+exp(-(q2-del)/tau),start=st,data=d1,trace=T,control=ctrl)
13.24433 :  30 30 15 10 
3.90071 :  33.00603 38.86974 20.18227 10.15656 
0.02326074 :  31.44802 36.80447 20.01395 10.07528 
1.209036e-06 :  31.39755 36.58548 19.99954 10.05675 
1.543785e-15 :  31.39752 36.58347 19.99947 10.05640 
1.030527e-29 :  31.39752 36.58347 19.99947 10.05640 
8.019572e-31 :  31.39752 36.58347 19.99947 10.05640 
8.019572e-31 :  31.39752 36.58347 19.99947 10.05640 
8.019572e-31 :  31.39752 36.58347 19.99947 10.05640 
8.019572e-31 :  31.39752 36.58347 19.99947 10.05640 
8.019572e-31 :  31.39752 36.58347 19.99947 10.05640 
Error in nls(k ~ exp(-(q1 - lam)/sig) + exp(-(q2 - del)/tau), start = st,  : 
        number of iterations exceeded maximum of 10



On Sun, 14 Nov 2004 17:09:06 -0800, Spencer Graves
<spencer.graves at pdf.com> wrote:
>       Have you considered "nls"?  If you read the help file and work
> through the examples, there is a good chance you can make it work, I
> think.  I think I would start trying "plinear" in "nls", parameterizing
> the problem in terms of alpha, beta, ln.sigma, and ln.tau, unless you
> think a solution might require sigma < 0 or tau < 0.  Using logarithms
> will get rid of the constraint and may make the problem numerically
> easier.  Using alpha and beta rather than lambda and delta transforms
> the problem into an ordinary least squares problem for alpha and beta
> given any two numbers for sigma and tau (or ln.sigma and ln.tau).
> 
>       If I had trouble with this, I might try two other things:
> 
>       (a) The "solver" in Excel.
> 
>       (b) I might generate a grid in ln.sigma and ln.tau using
> expand.grid.  For each combination of levels, I'd set up the linear
> regression problem and use "lm" to estimate alpha and beta and compute
> and store the sum of squares of residuals.  Then I'd use "contour" to
> visualize the sum of squares surface.
> 
>       I've done all these things with crudely similar problems in the
> past and been happy with the results.  If I only had this one problem,
> I'd be surprised if it would require more than a few hours.  If I wanted
> a general algorithm for other purposes, I might do it two or three
> different ways both to help select a good algorithm and to build
> confidence in the results.
> 
>       hope this helps.
>       spencer graves
> p.s.  Some of these techniques are discussed in Venables and Ripley
> (2002) Modern Applied Statistics with S, 4th ed. (Springer).  If you
> don't have this, I'd encourage you to consider spending some time with it.
> 
> 
> 
> Enayetur RAHEEM wrote:
> 
> >Hello there
> >
> >Can anybody please tell me if there is any package in R to solve the
> >following 4 nonlinear equations with 4 unknowns:
> >
> >alpha*exp(20/sigma)+ beta*exp(21/tau) = 2
> >alpha*exp(22/sigma)+ beta*exp(9/tau) = 4
> >alpha*exp(10/sigma)+ beta*exp(30/tau) = 6
> >alpha*exp(40/sigma)+ beta*exp(39/tau) = 5
> >
> >where
> >
> >alpha = exp(lambda/sigma)
> >beta= exp(delta/tau)
> >
> >I need to estimate lambda, sigma, delta, tau
> >
> >Thanks.
> >E Raheem
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> >
> 
> -- 
> Spencer Graves, PhD, Senior Development Engineer
> O:  (408)938-4420;  mobile:  (408)655-4567
> 
>



From mase at is.titech.ac.jp  Sat Nov 20 18:12:53 2004
From: mase at is.titech.ac.jp (Shigeru Mase)
Date: Sun, 21 Nov 2004 02:12:53 +0900
Subject: [R] How to change the significant codes default?
Message-ID: <419F7B15.7040503@is.titech.ac.jp>

Dear R experts,

I am posting this question on behalf of a Japanese R user
who wants to know how to change the siginificant codes default.
As you know, R's default significant codes are:

 Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1

But he says that it is usual in economics to give codes such as

 `***' for 0.01, `**' for 0.05 and `*' for 0.10

I don't know if this is true (common) or not, but what I as well
as he are puzzled is that, apparently, there is no part in the code,
say that of summary.lm, which produces these significant codes
as well as the annotation above. A quick search of "rking" using
keywords "significant codes star" gave me no information.

Thanks in advance.

I use R 2.0.0 on Debian GNU Linux (don't know his).

=======================================================
Shigeru MASE <mase at is.titech.ac.jp>
Tokyo Institute of Technology
Dept. of Math. and Comp. Sciences, Tokyo, Japan



From f.harrell at vanderbilt.edu  Sat Nov 20 16:37:55 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sat, 20 Nov 2004 09:37:55 -0600
Subject: [R] Error with strwidth after lattice graphic drawn
Message-ID: <419F64D3.1050109@vanderbilt.edu>

In

platform i386-pc-linux-gnu
arch     i386
os       linux-gnu
system   i386, linux-gnu
status
major    2
minor    0.1
year     2004
month    11
day      15
language R

I'm getting an error when using strwidth after a lattice graphic is drawn:

library(lattice)
xyplot(runif(20) ~ runif(20))
strwidth('xxx')

Error in strwidth("xxx") : invalid graphics state

Any help appreciated.  I have version 2.0.1 of grid and version 0.10-14 
of lattice.
-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From h.wickham at gmail.com  Sat Nov 20 18:20:06 2004
From: h.wickham at gmail.com (hadley wickham)
Date: Sat, 20 Nov 2004 11:20:06 -0600
Subject: [R] Running sum
In-Reply-To: <f8e6ff05041119144472b3854e@mail.gmail.com>
References: <200411192010.iAJKAcDM019929@outmx001.isp.belgacom.be>
	<1100900521.31008.65.camel@horizons.localdomain>
	<f8e6ff05041119144472b3854e@mail.gmail.com>
Message-ID: <f8e6ff050411200920712a31d8@mail.gmail.com>

> Perhaps I'm missing something, but isn't the maximum of the cumulative
> sum simply the last value, ie. sum(x)?

As many have mentioned, I was forgetting the negative numbers.  Thanks
to those who pointed that out.

Hadley



From ripley at stats.ox.ac.uk  Sat Nov 20 18:42:07 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 20 Nov 2004 17:42:07 +0000 (GMT)
Subject: [R] solving system of nonlinear equations
In-Reply-To: <8290920504112009105bcdef3@mail.gmail.com>
References: <8290920504111406452ab79603@mail.gmail.com>
	<419801B2.30409@pdf.com> <8290920504112009105bcdef3@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0411201738090.22215@gannet.stats>

On Sat, 20 Nov 2004, Enayetur RAHEEM wrote:

> R version: 2.0.0
> OS: WinXP, SP2
>
> I am using "nls" to estimate parameters of a system of nonlinear
> equations. Although, iteration is not converging, I would like to get
> the final estimates and store them in the object, say, "RR".

Please read the help page for nls: this is a misuse of the function:

      *Do not use 'nls' on artificial "zero-residual" data.*

You could do this, easily, using a general-purpose optimizer such as 
optim().


> Any help would be appreciated. Thanks.
>
> The following is not working ( I mean, I can not store it in "RR")
>
> RR=nls(k~exp(-(q1-lam)/sig)+exp(-(q2-del)/tau),start=st,data=d1,trace=T,control=ctrl)
> 13.24433 :  30 30 15 10
> 3.90071 :  33.00603 38.86974 20.18227 10.15656
> 0.02326074 :  31.44802 36.80447 20.01395 10.07528
> 1.209036e-06 :  31.39755 36.58548 19.99954 10.05675
> 1.543785e-15 :  31.39752 36.58347 19.99947 10.05640
> 1.030527e-29 :  31.39752 36.58347 19.99947 10.05640
> 8.019572e-31 :  31.39752 36.58347 19.99947 10.05640
> 8.019572e-31 :  31.39752 36.58347 19.99947 10.05640
> 8.019572e-31 :  31.39752 36.58347 19.99947 10.05640
> 8.019572e-31 :  31.39752 36.58347 19.99947 10.05640
> 8.019572e-31 :  31.39752 36.58347 19.99947 10.05640
> Error in nls(k ~ exp(-(q1 - lam)/sig) + exp(-(q2 - del)/tau), start = st,  :
>        number of iterations exceeded maximum of 10
>
>
>
> On Sun, 14 Nov 2004 17:09:06 -0800, Spencer Graves
> <spencer.graves at pdf.com> wrote:
>>       Have you considered "nls"?  If you read the help file and work
>> through the examples, there is a good chance you can make it work, I
>> think.  I think I would start trying "plinear" in "nls", parameterizing
>> the problem in terms of alpha, beta, ln.sigma, and ln.tau, unless you
>> think a solution might require sigma < 0 or tau < 0.  Using logarithms
>> will get rid of the constraint and may make the problem numerically
>> easier.  Using alpha and beta rather than lambda and delta transforms
>> the problem into an ordinary least squares problem for alpha and beta
>> given any two numbers for sigma and tau (or ln.sigma and ln.tau).
>>
>>       If I had trouble with this, I might try two other things:
>>
>>       (a) The "solver" in Excel.
>>
>>       (b) I might generate a grid in ln.sigma and ln.tau using
>> expand.grid.  For each combination of levels, I'd set up the linear
>> regression problem and use "lm" to estimate alpha and beta and compute
>> and store the sum of squares of residuals.  Then I'd use "contour" to
>> visualize the sum of squares surface.
>>
>>       I've done all these things with crudely similar problems in the
>> past and been happy with the results.  If I only had this one problem,
>> I'd be surprised if it would require more than a few hours.  If I wanted
>> a general algorithm for other purposes, I might do it two or three
>> different ways both to help select a good algorithm and to build
>> confidence in the results.
>>
>>       hope this helps.
>>       spencer graves
>> p.s.  Some of these techniques are discussed in Venables and Ripley
>> (2002) Modern Applied Statistics with S, 4th ed. (Springer).  If you
>> don't have this, I'd encourage you to consider spending some time with it.
>>
>>
>>
>> Enayetur RAHEEM wrote:
>>
>>> Hello there
>>>
>>> Can anybody please tell me if there is any package in R to solve the
>>> following 4 nonlinear equations with 4 unknowns:
>>>
>>> alpha*exp(20/sigma)+ beta*exp(21/tau) = 2
>>> alpha*exp(22/sigma)+ beta*exp(9/tau) = 4
>>> alpha*exp(10/sigma)+ beta*exp(30/tau) = 6
>>> alpha*exp(40/sigma)+ beta*exp(39/tau) = 5
>>>
>>> where
>>>
>>> alpha = exp(lambda/sigma)
>>> beta= exp(delta/tau)
>>>
>>> I need to estimate lambda, sigma, delta, tau
>>>
>>> Thanks.
>>> E Raheem
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>>
>>>
>>
>> --
>> Spencer Graves, PhD, Senior Development Engineer
>> O:  (408)938-4420;  mobile:  (408)655-4567
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Sat Nov 20 18:55:51 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 20 Nov 2004 18:55:51 +0100
Subject: [R] How to change the significant codes default?
In-Reply-To: <419F7B15.7040503@is.titech.ac.jp>
References: <419F7B15.7040503@is.titech.ac.jp>
Message-ID: <419F8527.90403@statistik.uni-dortmund.de>

Shigeru Mase wrote:
> Dear R experts,
> 
> I am posting this question on behalf of a Japanese R user
> who wants to know how to change the siginificant codes default.
> As you know, R's default significant codes are:
> 
>  Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
> 
> But he says that it is usual in economics to give codes such as
> 
>  `***' for 0.01, `**' for 0.05 and `*' for 0.10
> 
> I don't know if this is true (common) or not, but what I as well
> as he are puzzled is that, apparently, there is no part in the code,
> say that of summary.lm, which produces these significant codes
> as well as the annotation above. A quick search of "rking" using
> keywords "significant codes star" gave me no information.
> 
> Thanks in advance.

For example, calling summary(lmObject) dispatches on method summary.lm() 
hwich creates an object of class "summary.lm".
The latter is printed by method print.summary.lm() which calls 
printCoefmat().

The stars are hard-coded there, and I don't think anybody is going to 
change that. I suggest to turn of the printing of siginificant codes by 
specifying
   print(summary(.....), signif.stars = FALSE)
or by setting the corresponding option().

Uwe Ligges



> I use R 2.0.0 on Debian GNU Linux (don't know his).
> 
> =======================================================
> Shigeru MASE <mase at is.titech.ac.jp>
> Tokyo Institute of Technology
> Dept. of Math. and Comp. Sciences, Tokyo, Japan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Sat Nov 20 18:56:38 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 20 Nov 2004 17:56:38 +0000 (GMT)
Subject: [R] Error with strwidth after lattice graphic drawn
In-Reply-To: <419F64D3.1050109@vanderbilt.edu>
References: <419F64D3.1050109@vanderbilt.edu>
Message-ID: <Pine.LNX.4.61.0411201752410.22215@gannet.stats>

On Sat, 20 Nov 2004, Frank E Harrell Jr wrote:

> In
>
> platform i386-pc-linux-gnu
> arch     i386
> os       linux-gnu
> system   i386, linux-gnu
> status
> major    2
> minor    0.1
> year     2004
> month    11
> day      15
> language R
>
> I'm getting an error when using strwidth after a lattice graphic is drawn:
>
> library(lattice)
> xyplot(runif(20) ~ runif(20))
> strwidth('xxx')
>
> Error in strwidth("xxx") : invalid graphics state
>
> Any help appreciated.  I have version 2.0.1 of grid and version 0.10-14 of 
> lattice.

The advice is `don't do that'!

strwidth() is a base graphics command, and will only work if a device is 
currently plotting base graphics.  Lattice is built on grid, which has
stringWidth().

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Sat Nov 20 19:19:31 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 20 Nov 2004 19:19:31 +0100
Subject: [R] Error with strwidth after lattice graphic drawn
In-Reply-To: <Pine.LNX.4.61.0411201752410.22215@gannet.stats>
References: <419F64D3.1050109@vanderbilt.edu>
	<Pine.LNX.4.61.0411201752410.22215@gannet.stats>
Message-ID: <419F8AB3.9050203@statistik.uni-dortmund.de>

Prof Brian Ripley wrote:
> On Sat, 20 Nov 2004, Frank E Harrell Jr wrote:
> 
>> In
>>
>> platform i386-pc-linux-gnu
>> arch     i386
>> os       linux-gnu
>> system   i386, linux-gnu
>> status
>> major    2
>> minor    0.1
>> year     2004
>> month    11
>> day      15
>> language R
>>
>> I'm getting an error when using strwidth after a lattice graphic is 
>> drawn:
>>
>> library(lattice)
>> xyplot(runif(20) ~ runif(20))
>> strwidth('xxx')
>>
>> Error in strwidth("xxx") : invalid graphics state
>>
>> Any help appreciated.  I have version 2.0.1 of grid and version 
>> 0.10-14 of lattice.
> 
> 
> The advice is `don't do that'!
> 
> strwidth() is a base graphics command, and will only work if a device is 
> currently plotting base graphics.  Lattice is built on grid, which has
> stringWidth().
> 

... and convertWidth() is useful to display stuff afterwards in an 
interpretable way  ...

Uwe Ligges



From ligges at statistik.uni-dortmund.de  Sat Nov 20 19:25:51 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 20 Nov 2004 19:25:51 +0100
Subject: [R] how to suppress whiskers in a bwplot?
In-Reply-To: <Pine.LNX.4.58.0411201143280.27736@ice3.fas.harvard.edu>
References: <Pine.LNX.4.58.0411201143280.27736@ice3.fas.harvard.edu>
Message-ID: <419F8C2F.8050808@statistik.uni-dortmund.de>

The answer to your question is to use a minimal value > 0 for the 
argument "coef" of bwplot() such as coef=10^(-10)

Unfortunately, there is a bug in R-2.0.1, lattice 0.10-14, which shows 
negative length whiskers for positive values of "coef".
 From ?bwplot:

   library(lattice)
   bwplot(voice.part ~ height, data = singer,
      xlab = "Height (inches)", coef = 10^(-10))

[I'm CCing to Deepayan Sarkar, the maintainer of lattice.]

Uwe Ligges



Alexis J. Diamond wrote:

> dear R-help,
> 
> i have looked carefully through the R-help archives for information on how
> to suppress whiskers in a bwplot.  someone asked this question a while
> ago, but the answer he received is not available in the archives.
> 
> but i did manage to get my hands on a panel function (called
> "my.panel") that is supposed to do this (the function is reproduced at the
> end of the email, below).  the problem is that i get an error message
> when i use it in the following way:
> 
> ####
> data(singer)
> 
> bwplot(voice.part ~ height, data=singer, xlab="Height (inches)",
>      panel = "my.panel")
> ####
> 
> ### the error message is:
> 
> Error in segments(x1 = structure(c(NA, NA, NA, NA), .Names = c("", "",  :
>         Argument "x0" is missing, with no default
> 
> ###
> 
> i don't know if the problem is this panel function, or how I am
> (mis)using it.
> (i've never used a homegrown lattice panel function before.)
> 
> i realize that there is an excellent function in Hmisc that generates cool
> bwplots sans whiskers, but those plots are too fancy for my
> current needs.
> 
> all i need are regular boxy bwplots without whiskers
> (or umbrellas, as i guess they're also called).
> 
> as a quick fix, i've also tried changing the color of the whiskers to the
> background color to make the whiskers invisible, but this doesn't work
> well when the whiskers perfectly align with the border of the box.
> 
> thank you for your help,
> 
> alexis diamond
> adiamond at fas.harvard.edu
> 
> 
> ###  my.panel, the function that I've been told can suppress whiskers,
> ###  when outline = F
> 
> my.panel <- function(x, y, box.ratio = 1, font = box.dot$font, pch = box.dot$pch,
>  cex = box.dot$cex, col = box.dot$col, outline = T, ...)
> {
>  ok <- !is.na(x) & !is.na(y)
>  x <- x[ok]
>  y <- y[ok]
>  y.unique <- sort(unique(y))
>  width <- box.ratio/(1 + box.ratio)
>  w <- width/2
>  e <- par("cxy")[1]
>  for(Y in y.unique) {
>   X <- x[y == Y]
>   q <- quantile(X, c(0.75, 0.5, 0.25))
>   iqr <- q[1] - q[3]
>   d <- q[c(1, 3)] + c(1, -1) * 1.5 * iqr
>   up.w <- max(X[X <= d[1]], q[1])
>   lo.w <- min(X[X >= d[2]], q[3])
>   outliers <- X[X < lo.w | X > up.w]
>   X <- c(up.w, q, lo.w)
>   median.value <- list(x = X[3], y = Y)
>   Box <- list(x1 = X[c(2, 4, 4, 2)], y1 = Y + c( - w,
>     - w, w, w), x2 = X[c(4, 4, 2, 2)], y2 = Y +
>    c( - w, w, w,  - w))
>   e <- par("cxy")[1]
>   e.l <- min(e, (X[4] - X[5])/2)
>   # prevent lower staple ends from touching box
>   e.u <- min(e, (X[1] - X[2])/2)
>   # prevent upper staple ends from touching box
>   staple.ends <- list(x1 = rep(c(X[5], max(X[1] - e.u,
>    X[2])), 2), y1 = c(rep(Y - w, 2), rep(Y + w,
>    2)), x2 = rep(c(min(X[5] + e.l, X[4]), X[1]),
>    2), y2 = c(rep(Y - w, 2), rep(Y + w, 2)))
>   staple.body <- list(x1 = X[c(1, 5)], y1 = rep(Y - w,
>    2), x2 = X[c(1, 5)], y2 = rep(Y + w, 2))
>   dotted.line <- list(x1 = X[c(1, 4)], y1 = c(Y, Y),
>    x2 = X[c(2, 5)], y2 = c(Y, Y))
>   box.umbrella <- trellis.par.get("box.umbrella")
>   box.dot <- trellis.par.get("box.dot")
>   box.dot.par <- c(list(pch = pch, cex = cex, col = col,
>    font = font), ...)
>   do.call("segments", c(staple.ends, box.umbrella))
>   do.call("segments", c(staple.body, box.umbrella))
>   do.call("segments", c(dotted.line, box.umbrella))
>   do.call("segments", c(Box, trellis.par.get(
>    "box.rectangle")))
>   do.call("points", c(median.value, box.dot.par))
>   if(outline & length(outliers) > 0) {
>    outliers <- list(x = outliers, y = rep(Y,
>     length(outliers)))
>    do.call("points", c(outliers, trellis.par.get(
>     "plot.symbol"), cex = cex))
>   }
>  }
> }
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From a_izad at atrico.com  Sat Nov 20 19:46:16 2004
From: a_izad at atrico.com (A. Izad)
Date: Sat, 20 Nov 2004 10:46:16 -0800
Subject: [R] Restore sources
Message-ID: <20041120184627.JCPD26440.priv-edtnes57.telusplanet.net@vaio>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041120/9c35ca3c/attachment.pl

From Ted.Harding at nessie.mcc.ac.uk  Sat Nov 20 20:13:23 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sat, 20 Nov 2004 19:13:23 -0000 (GMT)
Subject: [R] How to change the significant codes default?
In-Reply-To: <419F8527.90403@statistik.uni-dortmund.de>
Message-ID: <XFMail.041120191323.Ted.Harding@nessie.mcc.ac.uk>

On 20-Nov-04 Uwe Ligges wrote:
> Shigeru Mase wrote:
>> Dear R experts,
>> 
>> I am posting this question on behalf of a Japanese R user
>> who wants to know how to change the siginificant codes default.
>> As you know, R's default significant codes are:
>> 
>>  Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
>> 
>> But he says that it is usual in economics to give codes such as
>> 
>>  `***' for 0.01, `**' for 0.05 and `*' for 0.10
>> 
>> I don't know if this is true (common) or not, but what I as well
>> as he are puzzled is that, apparently, there is no part in the code,
>> say that of summary.lm, which produces these significant codes
>> as well as the annotation above. A quick search of "rking" using
>> keywords "significant codes star" gave me no information.
>> 
>> Thanks in advance.
> 
> For example, calling summary(lmObject) dispatches on method
> summary.lm() 
> hwich creates an object of class "summary.lm".
> The latter is printed by method print.summary.lm() which calls 
> printCoefmat().
> 
> The stars are hard-coded there, and I don't think anybody is going to 
> change that. I suggest to turn of the printing of siginificant codes by
> specifying
>    print(summary(.....), signif.stars = FALSE)
> or by setting the corresponding option().
> 
> Uwe Ligges

It would be possible to re-define 'printCoefmat' privately
so as to change the lines

      cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1),
      symbols = c("***", "**", "*", ".", " "))

towards the end of its code into whatever you prefer, e.g.

      cutpoints = c(0, 0.01, 0.05, 0.1, 1),
      symbols = c("***", "**", "*", " "))

or

      cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1),
      symbols = c("****", "***", "**", "*", " "))

(both of which are compatible with your description of what
is needed).

The most straightforward way of redefining it is to copy
the code for 'printCoefmat' into a file, e.g.

  sink("printCoefmat.R")
  printCoefmat
  sink()

and then edit that file.
NOTE that the code written to the file does not include
the name of the function, i.e. it starts

  function (x, digits = max(3, getOption("digits") - 2),....

so the first modification has to be

  printCoefmat<-function(x, digits = .... )

Then, when you want your private version, simply do

  source("printCoefmat.R")

and it will overlay the original version. (Experts will have
to advise whether this clashes with any "namespace" issues.
On my reading of the code, it doesn't seem to; but I'm no
expert!)

If your friend wants to use this new definition all the time,
then one way to arrange this is to put the revised function
definition (as in the edited file) into his .Rprofile,
or put the command
  source("printCoefmat")
into that file.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 20-Nov-04                                       Time: 19:13:23
------------------------------ XFMail ------------------------------



From maechler at stat.math.ethz.ch  Sat Nov 20 21:07:43 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 20 Nov 2004 21:07:43 +0100
Subject: [R] Creating logical value from the difference of two absolute
	values
In-Reply-To: <419DEE45.3040504@stat.wisc.edu>
References: <5.2.1.1.0.20041118191436.00c353d0@biomserv.univ-lyon1.fr>
	<5A80B9B4-39C5-11D9-B2EE-000393B3E9D0@utah.edu>
	<419DEE45.3040504@stat.wisc.edu>
Message-ID: <16799.41999.521806.587155@gargle.gargle.HOWL>

>>>>> "DougB" == Douglas Bates <bates at stat.wisc.edu>
>>>>>     on Fri, 19 Nov 2004 06:59:49 -0600 writes:

    DougB> Nathan Leon Pace, MD, MStat wrote:
    >> Hi,
    >> 
    >> Using R 2.0.1 on Mac g5 running Mac OS X 10.3.6.
    >> 
    >> I would expect that
    >> 
    >> abs(.7 - .5) >= abs(.3 - .5) should be returned TRUE.
    >> 
    >> Instead
    >> 
    >> > www <- abs(.7 - .5) >= abs(.3 - .5)
    >> > www
    >> [1] FALSE
    >> 
    >> Is this a result of floating point or the implementation of abs or 
    >> something else?

    DougB> Due to floating point arithmetic in general, not specifically the abs 
    DougB> function.  The number .5 will have an exact floating point 
    DougB> representation but .3 and .7 will not.  Making equality comparisons with 
    DougB> floating point values is always risky.

    >> In a function I need to compare two absolute values - each being of the 
    >> form |variable - constant|.

    DougB> On a Mac I get

    >> abs(.7-.5) - abs(.3-.5)
    DougB> [1] -5.551115e-17

    DougB> so you need to make a relative comparison, not an absolute comparison. 
    DougB> You could write the relative comparison using the all.equal function, 

yes,

    DougB> such as

    >> v1 <- abs(.7-.5)
    >> v2 <- abs(.3-.5)
    >> (v1 > v2) || all.equal(v1, v2)

    DougB> [1] TRUE

but unfortunately,  in such situations,  all.equal() has to be
used more cautiously :

 >  v1 <- 1; v2 <- 10
 >  (v1 > v2) || all.equal(v1, v2)
 Error in (v1 > v2) || all.equal(v1, v2) : invalid `y' type in `x || y'

since it doesn't return 'FALSE' but a string in the non-TRUE case.
One "correct" way (*) to use it is

  (v1 > v2) || identical(TRUE, all.equal(v1, v2))

I've asked myself quite a few times in the past if it wasn't worth
defining an   is.all.equal(.)
function  that would really do what one might (and you implicitly
did) expect of all.equal(.).

Maybe we really should provide it, just for the sake of code
clarity {and teaching "how to"s}.

Martin

(*) a shorter but much less readable version is
      (v1 > v2) || is.logical(all.equal(v1, v2))



From bxc at steno.dk  Sat Nov 20 21:24:42 2004
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Sat, 20 Nov 2004 21:24:42 +0100
Subject: [R] SAS or R software
Message-ID: <0ABD88905D18E347874E0FB71C0B29E90237D2FC@exdkba022.novo.dk>

Two major advantages of SAS that seems to have been overlooked in
the previous replies are:

1) The data-set language is SAS for data manipulation is more
   human-readable than R-code in general. 
   R is not a definite write-only laguage as APL, but in particular
   in datamanipulation it is easy to write code that is impossible
   to decipher after few weeks. 
   You can also produce unreadable code in SAS, but it generally takes 
   more of an effort.

   Thus: Data manupulation is easier to document in SAS than in R.

2) proc tabulate.
   This procedure enables you to do extensive sensible tabulation
   of your data if you are prepared to read the manual.
   (This is not a product of the complexity of the software,
    but of the complexity of the tabulation features).
   Compared to this only rudimentay tools exist in R (afaik).

So if you want to do well documented data manipulation and clear
and compact tables go for SAS.

If you want to do statistical analyses and graphics (in finite time)
go for R.

Bendix Carstensen
----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc
----------------------

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of neela v
> Sent: Saturday, November 20, 2004 9:18 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] SAS or R software
> 
> 
> Hi all there
>  
> Can some one clarify me on this issue, features wise which is 
> better R or SAS, leaving the commerical aspect associated 
> with it. I suppose there are few people who have worked on 
> both R and SAS and wish they would be able to help me in 
> deciding on this.
>  
> THank you for the help
> 
> 		
> ---------------------------------
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From sblay at cs.sfu.ca  Sat Nov 20 21:57:12 2004
From: sblay at cs.sfu.ca (Sigal Blay)
Date: Sat, 20 Nov 2004 12:57:12 -0800
Subject: [R] ERROR: installing package indices failed
Message-ID: <20041120205712.GA13035@stawlmihq.cs.sfu.ca>

gregmisc is installed yet the problem persist.
I installed gregmisc using 
install.packages(c("combinat","gregmisc","genetics"),lib='/home/sblay/lib')
(on the same library path where I am trying to install LDheatmap)

> installed.packages(lib='/home/sblay/lib')
          Package     LibPath           Version Priority Bundle    
combinat  "combinat"  "/home/sblay/lib" "0.0-5" NA       NA        
gdata     "gdata"     "/home/sblay/lib" "2.0.0" NA       "gregmisc"
genetics  "genetics"  "/home/sblay/lib" "1.1.1" NA       NA        
gmodels   "gmodels"   "/home/sblay/lib" "2.0.0" NA       "gregmisc"
gplots    "gplots"    "/home/sblay/lib" "2.0.0" NA       "gregmisc"
gtools    "gtools"    "/home/sblay/lib" "2.0.0" NA       "gregmisc"
LDheatmap "LDheatmap" "/home/sblay/lib" "1.0"   NA       NA        

...


> I am developing a package named LDehatmap.
> It depends on the "genetics" package
> and includes two data files and a demo file.
> When I'm trying to install it, I get the following messages:
>
> * Installing *source* package 'LDheatmap' ...
> ** R
> ** data
> ** demo
> ** help
> >>> Building/Updating help pages for package 'LDheatmap'
>     Formats: text html latex example
>  LDheatmap                text    html    latex   example
>  ldheatmap                text    html    latex   example
> Error: object 'reorder' not found whilst loading namespace 'gdata'
> Error: package 'gdata' could not be loaded
> Execution halted
> ERROR: installing package indices failed
>
> Any ideas?

Yes.  You do not have gdata (part of gregmisc) installed, and genetics 
depends on it.  How did you get genetics installed? A binary install?

Install gregmisc ....



From f.harrell at vanderbilt.edu  Sat Nov 20 23:05:30 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sat, 20 Nov 2004 17:05:30 -0500
Subject: [R] SAS or R software
In-Reply-To: <0ABD88905D18E347874E0FB71C0B29E90237D2FC@exdkba022.novo.dk>
References: <0ABD88905D18E347874E0FB71C0B29E90237D2FC@exdkba022.novo.dk>
Message-ID: <419FBFAA.3050309@vanderbilt.edu>

BXC (Bendix Carstensen) wrote:
> Two major advantages of SAS that seems to have been overlooked in
> the previous replies are:
> 
> 1) The data-set language is SAS for data manipulation is more
>    human-readable than R-code in general. 
>    R is not a definite write-only laguage as APL, but in particular
>    in datamanipulation it is easy to write code that is impossible
>    to decipher after few weeks. 
>    You can also produce unreadable code in SAS, but it generally takes 
>    more of an effort.
> 
>    Thus: Data manupulation is easier to document in SAS than in R.

I agree with the readability of data manipulation code, especially for 
novice users.  As far as functionality for data manipulation is 
concerned, R has more flexibility and is faster to program once one has 
experience.  I do think that learning data manipulation techniques in R 
takes a while.

> 
> 2) proc tabulate.
>    This procedure enables you to do extensive sensible tabulation
>    of your data if you are prepared to read the manual.
>    (This is not a product of the complexity of the software,
>     but of the complexity of the tabulation features).
>    Compared to this only rudimentay tools exist in R (afaik).

I could not disagree more with that statement.  Look for example at 
http://biostat.mc.vanderbilt.edu/twiki/pub/Main/StatReport/summary.pdf . 
  With R you can customize tables by specifying your own function for 
computing any statistic of interest.  Let's see how to use PROC TABULATE 
to display stratified Kaplan-Meier 4-year survival estimates along with 
median and median life length, not to mention match the fine control of 
formatting available using the combination of R and LaTeX.

> 
> So if you want to do well documented data manipulation and clear
> and compact tables go for SAS.

Readability is different from being well-documented.  And for clear and 
compact tables, R is the winner hands down.

Frank Harrell

> 
> If you want to do statistical analyses and graphics (in finite time)
> go for R.
> 
> Bendix Carstensen
> ----------------------
> Bendix Carstensen
> Senior Statistician


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From john.maindonald at anu.edu.au  Sun Nov 21 02:41:49 2004
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Sun, 21 Nov 2004 12:41:49 +1100
Subject: [R] Location of grobs etc on lattice output
Message-ID: <82DA8727-3B5E-11D9-A5CC-000A95CDA0F2@anu.edu.au>

Is there any way, after use of print.trellis(), to obtain the
co-ordinates of the plot region, e.g., in what are then the
native co-ordinates?

e.g.
  library(DAAG)
  library(lattice); library(grid)
  data(cuckoos)
  pushViewport(viewport(layout=grid.layout(2, 1)))
  pushViewport(viewport(layout.pos.row=1))
  cuckoos.strip <- stripplot(species ~ length, data=cuckoos)
  print(cuckoos.strip, newpage=FALSE)
  grid.text("A", x=unit(0.18,"native"), y=unit(0.925,"native"))
   # This works, but is fiddly, and needs rejigging if width
   # or fontsize are changed.
  popViewport(1)

An alternative would of course be to access the co-ordinate
system used by the lattice function for locating the panels,
or for locating labelling.

As in the example above, I have been using grid.text() to
position text outside the plot region, but closer to the "top"
axis than the legend parameter to the lattice function will
allow.

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Bioinformation Science, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.



From deepayan at stat.wisc.edu  Sun Nov 21 05:41:31 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Sat, 20 Nov 2004 22:41:31 -0600
Subject: [R] Location of grobs etc on lattice output
In-Reply-To: <82DA8727-3B5E-11D9-A5CC-000A95CDA0F2@anu.edu.au>
References: <82DA8727-3B5E-11D9-A5CC-000A95CDA0F2@anu.edu.au>
Message-ID: <200411202241.31479.deepayan@stat.wisc.edu>

On Saturday 20 November 2004 19:41, John Maindonald wrote:
> Is there any way, after use of print.trellis(), to obtain the
> co-ordinates of the plot region, e.g., in what are then the
> native co-ordinates?

Have you read help(trellis.focus)? This is new in 2.0.0 and the 
recommended API for interacting with lattice plots (you can of course 
use grid tools directly, but details are more likely to change at that 
level). 

It hasn't had much testing, so I would appreciate reports of things that 
should be doable easily but isn't.

> e.g.
>   library(DAAG)
>   library(lattice); library(grid)
>   data(cuckoos)
>   pushViewport(viewport(layout=grid.layout(2, 1)))
>   pushViewport(viewport(layout.pos.row=1))
>   cuckoos.strip <- stripplot(species ~ length, data=cuckoos)
>   print(cuckoos.strip, newpage=FALSE)
>   grid.text("A", x=unit(0.18,"native"), y=unit(0.925,"native"))
>    # This works, but is fiddly, and needs rejigging if width
>    # or fontsize are changed.
>   popViewport(1)
>
> An alternative would of course be to access the co-ordinate
> system used by the lattice function for locating the panels,
> or for locating labelling.
>
> As in the example above, I have been using grid.text() to
> position text outside the plot region, but closer to the "top"
> axis than the legend parameter to the lattice function will
> allow.

trellis.focus("panel", row=1, column=1, clip.off=TRUE)

will put you in the plot region (panel), but will switch off clipping so 
you can write text outside.

You can also now control the amount of space between the axis and 
legend, see

str(trellis.par.get("layout.heights"))

Deepayan



From tom_woody at swissinfo.org  Sun Nov 21 09:12:37 2004
From: tom_woody at swissinfo.org (=?ISO-8859-15?Q?Thomas_Sch=F6nhoff?=)
Date: Sun, 21 Nov 2004 09:12:37 +0100
Subject: [R] SAS or R software
In-Reply-To: <0ABD88905D18E347874E0FB71C0B29E90237D2FC@exdkba022.novo.dk>
References: <0ABD88905D18E347874E0FB71C0B29E90237D2FC@exdkba022.novo.dk>
Message-ID: <41A04DF5.70208@swissinfo.org>

Hello,

BXC (Bendix Carstensen) schrieb:
> Two major advantages of SAS that seems to have been overlooked in
> the previous replies are:
> 
> 1) The data-set language is SAS for data manipulation is more
>    human-readable than R-code in general. 
>    R is not a definite write-only laguage as APL, but in particular
>    in datamanipulation it is easy to write code that is impossible
>    to decipher after few weeks. 

Not quite sure if this is a valid point! Mostly you'll have to comment 
on code as it is taught in programming course.
If your comments are clear, there shouldn't be any problems to 
understand your code after weeks , month or maybe...........


Thomas



From abitbol at sent.com  Sun Nov 21 12:31:13 2004
From: abitbol at sent.com (Jean-Louis Abitbol)
Date: Sun, 21 Nov 2004 12:31:13 +0100
Subject: [R] Help with ooplot(gplots) and error bars
Message-ID: <1101036673.696.209133539@webmail.messagingengine.com>

Dear All

I am trying to graph a proportion and CI95% by a factor with ooplot (any
other better solution ?)

It works well until I try to add the confidence interval.

this is the error message and and a description of the data:
  
 > dat1
        PointEst
TT1   1      3.6
TT2   2      5.0
TT3   3      5.8
TT4   4     11.5
TT5   5      7.5
TT5   6      8.7
TT7   7     17.4
> dat2
        Lower
TT1   1   1.0
TT2   2   2.2
TT3   3   2.7
TT4   4   6.7
TT5   5   3.9
TT5   6   4.6
TT7   7  11.5
> dat3
        Upper
TT1   1  12.3
TT2   2  11.2
TT3   3  12.1
TT4   4  19.1
TT5   5  14.2
TT5   6  15.6
TT7   7  25.6
> ooplot(dat1,type="barplot",col=rich.colors(7,"temperature"),names.arg=c("X","Y","Z","A","B","C","D"),plot.ci=T,
+ ci.l=dat2,ci.u=dat3, xlab="Treatment", ylab="Percent Normalized
Patients")
Error in ooplot.default(dat1, type = "barplot", col = rich.colors(7,
"temperature"),  : 
        'height' and 'ci.u' must have the same dimensions.

I have tried various ways of supplying ci.l and ci.u (including a
vector)


Thanks for the help that anyone can bring,

Regards, JL



From n.r.street at soton.ac.uk  Sun Nov 21 12:38:52 2004
From: n.r.street at soton.ac.uk (nat)
Date: Sun, 21 Nov 2004 11:38:52 +0000
Subject: [R] Two factor ANOVA in lme
In-Reply-To: <200411211115.iALB5f6s029937@hypatia.math.ethz.ch>
References: <200411211115.iALB5f6s029937@hypatia.math.ethz.ch>
Message-ID: <41A07E4C.7080902@soton.ac.uk>

I want to specify a two-factor model in lme, which should be easy? 
Here's what I have:

factor 1 - treatment FIXED (two levels)
factor 2 - genotype RANDOM (160 genotypes in total)

I need a model that tells me whether the treatment, genotype and 
interaction terms are significant. I have been reading 'Mixed effects 
models in S' but in all examples the random factor is not in the main 
model - it is a nesting factor etc to specify the error structure. Here 
i need the random factor in the model.

I have tried this:

height.aov<-lme(height~trt*genotype,data.reps,random=~1|genotype,na.action=na.exclude)

but the output is nothing like that from Minitab (my only previous 
experience of stats software). The results for the interaction term are 
the same but F values for the factors alone are very different between 
Minitab and R.

This is a very simple model but I can't figure out how to specify it. 
Help would be much appreciated.

As background: The data are from a QTL mapping population, which is why 
I must test to see if genotype is significant and also why genotype is a 
random factor.

Thanks



From f.harrell at vanderbilt.edu  Sun Nov 21 13:48:58 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sun, 21 Nov 2004 07:48:58 -0500
Subject: [R] Help with ooplot(gplots) and error bars
In-Reply-To: <1101036673.696.209133539@webmail.messagingengine.com>
References: <1101036673.696.209133539@webmail.messagingengine.com>
Message-ID: <41A08EBA.6080108@vanderbilt.edu>

Jean-Louis Abitbol wrote:
> Dear All
> 
> I am trying to graph a proportion and CI95% by a factor with ooplot (any
> other better solution ?)
> 
> It works well until I try to add the confidence interval.
> 
> this is the error message and and a description of the data:
>   
>  > dat1
>         PointEst
> TT1   1      3.6
> TT2   2      5.0
> TT3   3      5.8
> TT4   4     11.5
> TT5   5      7.5
> TT5   6      8.7
> TT7   7     17.4
> 
>>dat2
> 
>         Lower
> TT1   1   1.0
> TT2   2   2.2
> TT3   3   2.7
> TT4   4   6.7
> TT5   5   3.9
> TT5   6   4.6
> TT7   7  11.5
> 
>>dat3
> 
>         Upper
> TT1   1  12.3
> TT2   2  11.2
> TT3   3  12.1
> TT4   4  19.1
> TT5   5  14.2
> TT5   6  15.6
> TT7   7  25.6
> 
>>ooplot(dat1,type="barplot",col=rich.colors(7,"temperature"),names.arg=c("X","Y","Z","A","B","C","D"),plot.ci=T,
> 
> + ci.l=dat2,ci.u=dat3, xlab="Treatment", ylab="Percent Normalized
> Patients")
> Error in ooplot.default(dat1, type = "barplot", col = rich.colors(7,
> "temperature"),  : 
>         'height' and 'ci.u' must have the same dimensions.
> 
> I have tried various ways of supplying ci.l and ci.u (including a
> vector)
> 
> 
> Thanks for the help that anyone can bring,
> 
> Regards, JL

One way is to look at the examples for Dotplot in the Hmisc package. 
Those examples display bootstrap percentile confidence intervals for a mean.

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From dmb at mrc-dunn.cam.ac.uk  Sun Nov 21 15:35:07 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Sun, 21 Nov 2004 14:35:07 +0000 (GMT)
Subject: [R] Analysis of pre-calculated frequency distribution?
Message-ID: <Pine.LNX.4.21.0411211403060.26310-100000@mail.mrc-dunn.cam.ac.uk>


Sorry for the dumb question, but I cant work out how to do this. 

Quick version, 

How can I re-bin a given frequency distribution using new breaks without
reference to the original data? Given distribution has integer valued
bins.


Long version,

I am loading a frequency table into R from a file. The original data is
very large, and it is a very simple process to get a frequency
distribution from an SQL database, so in all this is a convenient method
for me. Point being I don't start with 'raw' data.

The data looks like this...

> dat
             COUNT FREQUENCY
1                1 5734
2                2 1625
3                3  793
4                4  480
5                5  294
6                6  237
7                7  205
8                8  200
9                9  123
10              10  108
11              11   90
12              12   62
13              13   60
14              14   68
15              15   64
16              16   56
17              17   68
18              18   45
19              19   38
20              20   37
21              21   29
22              22   39
23              23   35
24              24   33
25              25   36
...
148            153    5
149            156    2
150            157    3
151            158    2
152            159    2
153            162    1
154            163    3
155            164    3
156            165    2
157            166    1
158            168    2
159            169    4
160            170    1
...
354           2106    1
355           2189    1
356           2194    1
357           2217    1
358           2246    1
359           2474    1
360           2801    1
361           3697    1
362           3702    1
363           7353    1
364           8738    1
365           9442    1
366          12280    1



This is a tipical 'count / frequency' distribution in biology, where low
counts of a certain property are very frequent (across genomes, proteins,
ecosystems, etc...), and high counts of of a certain property are very
rare.

In the above example a certain property occurs 12280 times with a
frequency of 1, another property occurs 9442 times with the same
frequency. At the other end of the extreem, a certain property occurs once
with a frequency of 5734, and another property occurs twice with a
frequency of 1625. 

This kind of distribution is variously known as a "zipf", a "power law", a
"Pareto", "scale free", "heavy tailed" or a "80:20" distribution, or
coloquially "the dominance of the few over the many". The term I choose is
a "log linear" distribution, because that makes no assumptions about the
underlying cause of the overall shape.

People tipically quote the curve in the form of y ~ Cx^(-a). I want to use
the binning method of parameter estimation given here...

http://www.ece.uc.edu/~annexste/Courses/cs690/Zipf,%20Power-law,%20Pareto%20-%20a%20ranking%20tutorial.htm

(bin the data with exponentially increasing bin widths within the data
range).

But I can't work out how to re-bin my existing frequency data.

Sorry for the long question, 
all the best
Dan.



From f.harrell at vanderbilt.edu  Sun Nov 21 16:40:40 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sun, 21 Nov 2004 10:40:40 -0500
Subject: [R] Help with ooplot(gplots) and error bars
In-Reply-To: <1101044163.5921.209136609@webmail.messagingengine.com>
References: <1101036673.696.209133539@webmail.messagingengine.com>
	<41A08EBA.6080108@vanderbilt.edu>
	<1101044163.5921.209136609@webmail.messagingengine.com>
Message-ID: <41A0B6F8.8000507@vanderbilt.edu>

Jean-Louis Abitbol wrote:
> Dear Pr Harrel,
> 
> Thanks for your help at this occasion as well as for previous questions
> to the list.
> 
> I just looked at the example in your intro doc.
> 
> However I am dealing with proportions (ie % of patients responding to a
> given treatment).
> 
> In this case I am not sure I can use summarize and then the xYplot.
> 
> I am not aware of R graphing tools that can deal directly with
> proportions adding CI not to mention producing by factor/trellis plots.
> 
> This is why I why trying to do it "by hand" (using binconf) with ooplot,
> without much success I am afraid. 
> 
> Best regards,
> 
> JL Abitbol, MD

Jean-Louis,

Here is an example.

# Plot proportions and their Wilson confidence limits
set.seed(3)
d <- expand.grid(continent=c('USA','Europe'), year=1999:2001,
                  reps=1:100)
# Generate binary events from a population probability of 0.2
# of the event, same for all years and continents
d$y <- ifelse(runif(6*100) <= .2, 1, 0)
s <- with(d,
           summarize(y, llist(continent,year),
                     function(y) {
                      n <- sum(!is.na(y))
                      s <- sum(y, na.rm=T)
                      binconf(s, n)
                     }, type='matrix')
)

Dotplot(year ~ Cbind(y) | continent,  data=s, ylab='Year',
         xlab='Probability')

I did have to temporarily override a function in Hmisc to fix a problem. 
  This will be corrected in an upcoming release of Hmisc:

mApply <- function(X, INDEX, FUN=NULL, ..., simplify=TRUE) {
## Matrix tapply
## X: matrix with n rows; INDEX: vector or list of vectors of length n
## FUN: function to operate on submatrices of x by INDEX
## ...: arguments to FUN; simplify: see sapply
## Modification of code by Tony Plate <tplate at blackmesacapital.com> 10Oct02
## If FUN returns more than one number, mApply returns a matrix with
## rows corresponding to unique values of INDEX
nr <- nrow(X)
if(!length(nr)) {  ## X not a matrix
   r <- tapply(X, INDEX, FUN, ..., simplify=simplify)
   if(is.matrix(r)) r <- drop(t(r)) else
   if(simplify && is.list(r))
     r <- drop(matrix(unlist(r), nrow=length(r),
                      dimnames=list(names(r),names(r[[1]])), byrow=TRUE))
} else {
   idx.list <- tapply(1:nr, INDEX, c)
   r <- sapply(idx.list, function(idx,x,fun,...) 
fun(x[idx,,drop=FALSE],...),
               x=X, fun=FUN, ..., simplify=simplify)
   if(simplify) r <- drop(t(r))
}
dn <- dimnames(r)
if(length(dn) && !length(dn[[length(dn)]])) {
   fx <- FUN(X,...)
   dnl <- if(length(names(fx))) names(fx) else dimnames(fx)[[2]]
   dn[[length(dn)]] <- dnl
   dimnames(r) <- dn
}

if(simplify && is.list(r) && is.array(r)) {

   ll <- sapply(r, length)
   maxl <- max(ll)
   empty <- (1:length(ll))[ll==0]
   for(i in empty) r[[i]] <- rep(NA, maxl)
   ## unlist not keep place for NULL entries for nonexistent categories
   first.not.empty <- ((1:length(ll))[ll > 0])[1]
   nam <- names(r[[first.not.empty]])
   dr <- dim(r)

   r <- aperm(array(unlist(r), dim=c(maxl,dr),
                    dimnames=c(list(nam),dimnames(r))),
              c(1+seq(length(dr)), 1))
}
r
}



Frank

> 
> On Sun, 21 Nov 2004 07:48:58 -0500, "Frank E Harrell Jr"
> <f.harrell at vanderbilt.edu> said:
> 
>>Jean-Louis Abitbol wrote:
>>
>>>Dear All
>>>
>>>I am trying to graph a proportion and CI95% by a factor with ooplot (any
>>>other better solution ?)
>>>
>>>It works well until I try to add the confidence interval.
>>>
>>>this is the error message and and a description of the data:
>>>  
>>> > dat1
>>>        PointEst
>>>TT1   1      3.6
>>>TT2   2      5.0
>>>TT3   3      5.8
>>>TT4   4     11.5
>>>TT5   5      7.5
>>>TT5   6      8.7
>>>TT7   7     17.4
>>>
>>>
>>>>dat2
>>>
>>>        Lower
>>>TT1   1   1.0
>>>TT2   2   2.2
>>>TT3   3   2.7
>>>TT4   4   6.7
>>>TT5   5   3.9
>>>TT5   6   4.6
>>>TT7   7  11.5
>>>
>>>
>>>>dat3
>>>
>>>        Upper
>>>TT1   1  12.3
>>>TT2   2  11.2
>>>TT3   3  12.1
>>>TT4   4  19.1
>>>TT5   5  14.2
>>>TT5   6  15.6
>>>TT7   7  25.6
>>>
>>>
>>>>ooplot(dat1,type="barplot",col=rich.colors(7,"temperature"),names.arg=c("X","Y","Z","A","B","C","D"),plot.ci=T,
>>>
>>>+ ci.l=dat2,ci.u=dat3, xlab="Treatment", ylab="Percent Normalized
>>>Patients")
>>>Error in ooplot.default(dat1, type = "barplot", col = rich.colors(7,
>>>"temperature"),  : 
>>>        'height' and 'ci.u' must have the same dimensions.
>>>
>>>I have tried various ways of supplying ci.l and ci.u (including a
>>>vector)
>>>
>>>
>>>Thanks for the help that anyone can bring,
>>>
>>>Regards, JL
>>
>>One way is to look at the examples for Dotplot in the Hmisc package. 
>>Those examples display bootstrap percentile confidence intervals for a
>>mean.
>>
>>-- 
>>Frank E Harrell Jr   Professor and Chair           School of Medicine
>>                      Department of Biostatistics   Vanderbilt University
> 
>



From f.harrell at vanderbilt.edu  Sun Nov 21 16:52:33 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sun, 21 Nov 2004 10:52:33 -0500
Subject: [R] Error with strwidth after lattice graphic drawn
In-Reply-To: <419F8AB3.9050203@statistik.uni-dortmund.de>
References: <419F64D3.1050109@vanderbilt.edu>
	<Pine.LNX.4.61.0411201752410.22215@gannet.stats>
	<419F8AB3.9050203@statistik.uni-dortmund.de>
Message-ID: <41A0B9C1.5090801@vanderbilt.edu>

Uwe Ligges wrote:
> Prof Brian Ripley wrote:
> 
>> On Sat, 20 Nov 2004, Frank E Harrell Jr wrote:
>>
>>> In
>>>
>>> platform i386-pc-linux-gnu
>>> arch     i386
>>> os       linux-gnu
>>> system   i386, linux-gnu
>>> status
>>> major    2
>>> minor    0.1
>>> year     2004
>>> month    11
>>> day      15
>>> language R
>>>
>>> I'm getting an error when using strwidth after a lattice graphic is 
>>> drawn:
>>>
>>> library(lattice)
>>> xyplot(runif(20) ~ runif(20))
>>> strwidth('xxx')
>>>
>>> Error in strwidth("xxx") : invalid graphics state
>>>
>>> Any help appreciated.  I have version 2.0.1 of grid and version 
>>> 0.10-14 of lattice.
>>
>>
>>
>> The advice is `don't do that'!
>>
>> strwidth() is a base graphics command, and will only work if a device 
>> is currently plotting base graphics.  Lattice is built on grid, which has
>> stringWidth().
>>
> 
> ... and convertWidth() is useful to display stuff afterwards in an 
> interpretable way  ...
> 
> Uwe Ligges
> 

Thanks Uwe and Brian.  Before the latest versions, grid would let me use 
ordinary graphics functions to deal with the currently rendered plot 
after I did par(usr=c(0,1,0,1)), so I was lazy.  I changed my code to 
use specific grid functions.

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From ligges at statistik.uni-dortmund.de  Sun Nov 21 17:19:46 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 21 Nov 2004 17:19:46 +0100
Subject: [R] ERROR: installing package indices failed
In-Reply-To: <20041120205712.GA13035@stawlmihq.cs.sfu.ca>
References: <20041120205712.GA13035@stawlmihq.cs.sfu.ca>
Message-ID: <41A0C022.4090409@statistik.uni-dortmund.de>

Sigal Blay wrote:
> gregmisc is installed yet the problem persist.
> I installed gregmisc using 
> install.packages(c("combinat","gregmisc","genetics"),lib='/home/sblay/lib')
> (on the same library path where I am trying to install LDheatmap)
> 

Have you set the environment variable R_LIBS appropriately?

Uwe Ligges


>>installed.packages(lib='/home/sblay/lib')
> 
>           Package     LibPath           Version Priority Bundle    
> combinat  "combinat"  "/home/sblay/lib" "0.0-5" NA       NA        
> gdata     "gdata"     "/home/sblay/lib" "2.0.0" NA       "gregmisc"
> genetics  "genetics"  "/home/sblay/lib" "1.1.1" NA       NA        
> gmodels   "gmodels"   "/home/sblay/lib" "2.0.0" NA       "gregmisc"
> gplots    "gplots"    "/home/sblay/lib" "2.0.0" NA       "gregmisc"
> gtools    "gtools"    "/home/sblay/lib" "2.0.0" NA       "gregmisc"
> LDheatmap "LDheatmap" "/home/sblay/lib" "1.0"   NA       NA        
> 
> ...
> 
> 
> 
>>I am developing a package named LDehatmap.
>>It depends on the "genetics" package
>>and includes two data files and a demo file.
>>When I'm trying to install it, I get the following messages:
>>
>>* Installing *source* package 'LDheatmap' ...
>>** R
>>** data
>>** demo
>>** help
>>
>>>>>Building/Updating help pages for package 'LDheatmap'
>>
>>    Formats: text html latex example
>> LDheatmap                text    html    latex   example
>> ldheatmap                text    html    latex   example
>>Error: object 'reorder' not found whilst loading namespace 'gdata'
>>Error: package 'gdata' could not be loaded
>>Execution halted
>>ERROR: installing package indices failed
>>
>>Any ideas?
> 
> 
> Yes.  You do not have gdata (part of gregmisc) installed, and genetics 
> depends on it.  How did you get genetics installed? A binary install?
> 
> Install gregmisc ....
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Sun Nov 21 17:26:23 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 21 Nov 2004 17:26:23 +0100
Subject: [R] How to change the significant codes default?
In-Reply-To: <XFMail.041120191323.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.041120191323.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <41A0C1AF.4060903@statistik.uni-dortmund.de>

(Ted Harding) wrote:

> On 20-Nov-04 Uwe Ligges wrote:
> 
>>Shigeru Mase wrote:
>>
>>>Dear R experts,
>>>
>>>I am posting this question on behalf of a Japanese R user
>>>who wants to know how to change the siginificant codes default.
>>>As you know, R's default significant codes are:
>>>
>>> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
>>>
>>>But he says that it is usual in economics to give codes such as
>>>
>>> `***' for 0.01, `**' for 0.05 and `*' for 0.10
>>>
>>>I don't know if this is true (common) or not, but what I as well
>>>as he are puzzled is that, apparently, there is no part in the code,
>>>say that of summary.lm, which produces these significant codes
>>>as well as the annotation above. A quick search of "rking" using
>>>keywords "significant codes star" gave me no information.
>>>
>>>Thanks in advance.
>>
>>For example, calling summary(lmObject) dispatches on method
>>summary.lm() 
>>hwich creates an object of class "summary.lm".
>>The latter is printed by method print.summary.lm() which calls 
>>printCoefmat().
>>
>>The stars are hard-coded there, and I don't think anybody is going to 
>>change that. I suggest to turn of the printing of siginificant codes by
>>specifying
>>   print(summary(.....), signif.stars = FALSE)
>>or by setting the corresponding option().
>>
>>Uwe Ligges
> 
> 
> It would be possible to re-define 'printCoefmat' privately
> so as to change the lines
> 
>       cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1),
>       symbols = c("***", "**", "*", ".", " "))
> 
> towards the end of its code into whatever you prefer, e.g.
> 
>       cutpoints = c(0, 0.01, 0.05, 0.1, 1),
>       symbols = c("***", "**", "*", " "))
> 
> or
> 
>       cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1),
>       symbols = c("****", "***", "**", "*", " "))
> 
> (both of which are compatible with your description of what
> is needed).
> 
> The most straightforward way of redefining it is to copy
> the code for 'printCoefmat' into a file, e.g.
> 
>   sink("printCoefmat.R")
>   printCoefmat
>   sink()
> 
> and then edit that file.
> NOTE that the code written to the file does not include
> the name of the function, i.e. it starts
> 
>   function (x, digits = max(3, getOption("digits") - 2),....
> 
> so the first modification has to be
> 
>   printCoefmat<-function(x, digits = .... )
> 
> Then, when you want your private version, simply do
> 
>   source("printCoefmat.R")
> 
> and it will overlay the original version. (Experts will have
> to advise whether this clashes with any "namespace" issues.
> On my reading of the code, it doesn't seem to; but I'm no
> expert!)

Ted, it "clashes"! Functions in the namespace are looked up at first.

Uwe

> If your friend wants to use this new definition all the time,
> then one way to arrange this is to put the revised function
> definition (as in the edited file) into his .Rprofile,
> or put the command
>   source("printCoefmat")
> into that file.
> 
> Best wishes,
> Ted.
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
> Date: 20-Nov-04                                       Time: 19:13:23
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From friendly at yorku.ca  Sun Nov 21 17:35:30 2004
From: friendly at yorku.ca (Michael Friendly)
Date: Sun, 21 Nov 2004 11:35:30 -0500
Subject: [R] SAS or R software
Message-ID: <41A0C3D2.7030204@yorku.ca>

I can't resist dipping my oar in here:

For me, some significant advantages of SAS are

- Ability to input data in almost *any* conceivable form using the 
combination of features available through
input/infile statements, SAS informats and formats, data step 
programming, etc.  Dataset manipulation
(merge, join, stack, subset, summarize, etc.) also favors SAS in more 
complex cases, IMHO.
- Output delivery system (ODS):  *Every* piece of SAS output is an 
output object that can be captured as
a dataset, rendered in RTF, LaTeX, HTML, PDF, etc. with a relatively 
simple mechanism (including graphs)
    ods pdf file='mystuff.pdf'';
      << any sas stuff>>
    ods pdf close;
- If you think the output tables are ugly, it is not difficult to define 
a template for *any* output to display
it the way you like.
- ODS Graphics (new in SAS 9.1) will extend much of this so that 
statistical procedures will produce many
graphics themselves with ease.

One significant disadvantage for me is the difficulty of composing 
multipanel graphic displays
(trellis graphics, linked micromaps, etc.) due to the lack of an 
overall, top-down graphics environment.
As well, there are a variety of kinds of graphics I've found 
extraordinarily frustrating to try to do
in SAS because of lack of coherence or generality in the output 
available from procedures --- an
example would be effect displays, such as implemented in R in the 
effects package.  

I can't agree, however, with Frank Harrell that SAS produces 'the worst 
graphics in the statistical software world.'
One can get ugly graphs in R almost as easily in SAS just by accepting 
the 80-20 rule:  You can typically get 80% of
what you want with 20% of the effort.  To get what you really want takes 
the remaining 80% of effort.
On the other hand, the active hard work of many R developers has led to 
R graphics for which the *default*
results for many graphs avoid many of the egregious design errors 
introduced in SAS in the days of pen-plotters
(+ signs for points, cross-hatching for area fill).

-Michael

-- 
Michael Friendly     Email: friendly at yorku.ca 
Professor, Psychology Dept.
York University      Voice: 416 736-5115 x66249 Fax: 416 736-5814
4700 Keele Street    http://www.math.yorku.ca/SCS/friendly.html
Toronto, ONT  M3J 1P3 CANADA



From Ted.Harding at nessie.mcc.ac.uk  Sun Nov 21 17:47:05 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 21 Nov 2004 16:47:05 -0000 (GMT)
Subject: [R] Analysis of pre-calculated frequency distribution?
In-Reply-To: <Pine.LNX.4.21.0411211403060.26310-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <XFMail.041121164705.Ted.Harding@nessie.mcc.ac.uk>

On 21-Nov-04 Dan Bolser wrote:
> 
> Sorry for the dumb question, but I cant work out how to do this. 
> 
> Quick version, 
> 
> How can I re-bin a given frequency distribution using new breaks
> without reference to the original data? Given distribution has
> integer valued bins.
> 
> 
> Long version,
> 
> I am loading a frequency table into R from a file. The original
> data is very large, and it is a very simple process to get a
> frequency distribution from an SQL database, so in all this is
> a convenient method for me. Point being I don't start with 'raw' data.
> 
> The data looks like this...
> 
>> dat
>              COUNT FREQUENCY
> 1                1 5734
> 2                2 1625
> [...]
> 365           9442    1
> 366          12280    1
> 
> [...]
> 
> People tipically quote the curve in the form of y ~ Cx^(-a).
> I want to use the binning method of parameter estimation given here...
> 
> http://www.ece.uc.edu/~annexste/Courses/cs690/Zipf,%20Power-law,%20Paret
> o%20-%20a%20ranking%20tutorial.htm
> 
> (bin the data with exponentially increasing bin widths within the data
> range).
> 
> But I can't work out how to re-bin my existing frequency data.

Hi Dan,
Your starting point can be the fact that the number of cases
with property i ("in class i") is COUNT_i + FREQUENCY_I

So if you construct a vector with these numbers in it you have
in effect reconstructed the original data.

I.e.  N[i] <- COUNT[i]*FREQUENCY[i]

which can be done in one stroke with N <- COUNT*FREQUENCY

One way (and maybe others can suggest better) to bin these
classes non-uniformly could be:

  Say you have k "upper" breakpoints for your k bins,
  say BP, so that e.g. if BP[1] = 2 then there are N[1]+N[2]
  cases with class <= 2, and if BP[2] = 5 then there are
  N[3] + N[4] + N[5] cases with class > 2 and class <= 5,
  and so on. In your case BP[k] = 366.

  Let

    csN <- cumsum(N)

  Then (if I've not overlooked something)

    diff(c(0,csN[BP]))

  will give you the counts in yhour new bins.

E.g. (just to show it should work):

  > N<-rep(1,31)
  > BP<-c(1,3,7,15,31)
  > csN <- cumsum(N)
  > diff(c(0,csN[BP]))
  [1]  1  2  4  8 16


  > BP<-c(2,3,5,9,17,31)
  > diff(c(0,csN[BP]))
  [1]  2  1  2  4  8 14

I hope this matches the sort of thing you have in mind!
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 21-Nov-04                                       Time: 16:47:05
------------------------------ XFMail ------------------------------



From MSchwartz at MedAnalytics.com  Sun Nov 21 17:59:47 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Sun, 21 Nov 2004 10:59:47 -0600
Subject: [R] Help with ooplot(gplots) and error bars
In-Reply-To: <1101036673.696.209133539@webmail.messagingengine.com>
References: <1101036673.696.209133539@webmail.messagingengine.com>
Message-ID: <1101056387.16325.60.camel@horizons.localdomain>

On Sun, 2004-11-21 at 12:31 +0100, Jean-Louis Abitbol wrote:
> Dear All
> 
> I am trying to graph a proportion and CI95% by a factor with ooplot (any
> other better solution ?)
> 
> It works well until I try to add the confidence interval.
> 
> this is the error message and and a description of the data:
>   
>  > dat1
>         PointEst
> TT1   1      3.6
> TT2   2      5.0
> TT3   3      5.8
> TT4   4     11.5
> TT5   5      7.5
> TT5   6      8.7
> TT7   7     17.4
> > dat2
>         Lower
> TT1   1   1.0
> TT2   2   2.2
> TT3   3   2.7
> TT4   4   6.7
> TT5   5   3.9
> TT5   6   4.6
> TT7   7  11.5
> > dat3
>         Upper
> TT1   1  12.3
> TT2   2  11.2
> TT3   3  12.1
> TT4   4  19.1
> TT5   5  14.2
> TT5   6  15.6
> TT7   7  25.6
> > ooplot(dat1,type="barplot",col=rich.colors(7,"temperature"),names.arg=c("X","Y","Z","A","B","C","D"),plot.ci=T,
> + ci.l=dat2,ci.u=dat3, xlab="Treatment", ylab="Percent Normalized
> Patients")
> Error in ooplot.default(dat1, type = "barplot", col = rich.colors(7,
> "temperature"),  : 
>         'height' and 'ci.u' must have the same dimensions.
> 
> I have tried various ways of supplying ci.l and ci.u (including a
> vector)
> 
> 
> Thanks for the help that anyone can bring,
> 
> Regards, JL


It appears that ooplot() is built upon barplot2() to an extent.

When I wrote barplot2(), in the case of plotting CI's, it expects that
the primary data structure ('data' in ooplot, 'height' in barplot2) have
the same dimensions as 'ci.l' and 'ci.u'.

Thus, for example:

barplot2(dat1[, 2], col = rich.colors(7,"temperature"), 
       names.arg = c("X", "Y", "Z", "A", "B", "C", "D"), plot.ci = TRUE,
       ci.l = dat2[, 2], ci.u = dat3[, 2], xlab = "Treatment", 
       ylab = "Percent Normalized Patients")

will work. Note that I am explicitly passing the requisite data vectors
for 'height' and the CI's to the function.

In the case of ooplot(), it appears to require that the 'data' argument
have at least two columns, which requires that you pass 'dat1' as a two
dimensional structure and not dat1[, 2] as a vector.

In this case, since ooplot is built upon barplot2, your call to ooplot
fails when the check of the 'data' and 'ci.u' dimesional structure takes
place.

The reason for the failure (even though all three of your data
structures appear to be of the same shape initially), is that ooplot
does:

 if (by.row) 
    data <- as.matrix(data)
 else data <- t(as.matrix(data))

which results in your dat1 being changed from a 7 x 2 matrix to a 2 x 7
matrix.

So 'data' now looks like:

> data
         TT1 TT2 TT3  TT4 TT5 TT6  TT7
         1.0   2 3.0  4.0 5.0 6.0  7.0
PointEst 3.6   5 5.8 11.5 7.5 8.7 17.4


ooplot then does:

height <- data[-1, , drop = FALSE]


to create 'height' which is used later in the function, as it is in
barplot2().


So 'height' now looks like:

> height
         TT1 TT2 TT3  TT4 TT5 TT6  TT7
PointEst 3.6   5 5.8 11.5 7.5 8.7 17.4


The actual checks in the ooplot code (taken verbatim from barplot2) that
compare the dimensions of the 'height' argument and the CI's is:

  if (any(dim(height) != dim(ci.u))) 
     stop("'height' and 'ci.u' must have the same dimensions.")
  else if (any(dim(height) != dim(ci.l))) 
     stop("'height' and 'ci.l' must have the same dimensions.")


Due to the transformations above however, 'height' is now a 1 x 7
matrix, whereas your dat2 and dat3 are 7 x 2 matrices. Hence the
failure.

So, I suspect that Greg (who I have cc:'d here) needs to look at the
ooplot code to make similar transformations on the 'ci.l' and 'ci.u'
arguments as he is on the 'data' argument to remove the error.

Short term, you have (at least) four options:

1. Use barplot2() as I note above

2. You can modify your call to ooplot(), by using t() on the 'ci.l' and
'ci.u' arguments as follows:

ooplot(dat1, type = "barplot", col = rich.colors(7,"temperature"), 
       names.arg = c("X", "Y", "Z", "A", "B", "C", "D"), plot.ci = TRUE,
       ci.l = t(dat2[, 2]), ci.u = t(dat3[, 2]), xlab = "Treatment", 
       ylab = "Percent Normalized Patients")


3. As Frank has mentioned, you can use his Dotplot() function.

4. Similar to Dotplot() in a fashion, is the plotCI() function, which is
also in Greg's gplots package.


If you stay with the barplot type of graph, you should consider changing
your colors, as the CI's are difficult to discern in the first two
columns at least.

HTH,

Marc Schwartz



From f.harrell at vanderbilt.edu  Sun Nov 21 18:04:01 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sun, 21 Nov 2004 12:04:01 -0500
Subject: [R] SAS or R software
In-Reply-To: <41A0C3D2.7030204@yorku.ca>
References: <41A0C3D2.7030204@yorku.ca>
Message-ID: <41A0CA81.5000907@vanderbilt.edu>

Michael Friendly wrote:
> I can't resist dipping my oar in here:
> 
> For me, some significant advantages of SAS are
> 
> - Ability to input data in almost *any* conceivable form using the 
> combination of features available through
> input/infile statements, SAS informats and formats, data step 
> programming, etc.  Dataset manipulation
> (merge, join, stack, subset, summarize, etc.) also favors SAS in more 
> complex cases, IMHO.

I respectfully disagree Michael, in cases where the file sizes are not 
enormous.  And in many cases repetitive SAS data manipulation can be 
done much easier using vectors (e.g., vectors of recodes) and matrices 
in R, as shown in the examples in Alzola & Harrell 
(http://biostat.mc.vanderbilt.edu/twiki/pub/Main/RS/sintro.pdf).

We are working on a project in which a client sends us 50 SAS datasets. 
  Not only can we do all the complex data manipulations needed in R, but 
we can treat the 50 datasets as a array (actually a list( )) to handle 
repetitive operations over many datasets.

> - Output delivery system (ODS):  *Every* piece of SAS output is an 
> output object that can be captured as
> a dataset, rendered in RTF, LaTeX, HTML, PDF, etc. with a relatively 
> simple mechanism (including graphs)
>    ods pdf file='mystuff.pdf'';
>      << any sas stuff>>
>    ods pdf close;
> - If you think the output tables are ugly, it is not difficult to define 
> a template for *any* output to display
> it the way you like.

I would like to see SAS ODS duplicate Table 10 in 
http://biostat.mc.vanderbilt.edu/twiki/pub/Main/StatReport/summary.pdf

Cheers,

Frank

> - ODS Graphics (new in SAS 9.1) will extend much of this so that 
> statistical procedures will produce many
> graphics themselves with ease.
> 
> One significant disadvantage for me is the difficulty of composing 
> multipanel graphic displays
> (trellis graphics, linked micromaps, etc.) due to the lack of an 
> overall, top-down graphics environment.
> As well, there are a variety of kinds of graphics I've found 
> extraordinarily frustrating to try to do
> in SAS because of lack of coherence or generality in the output 
> available from procedures --- an
> example would be effect displays, such as implemented in R in the 
> effects package. 
> I can't agree, however, with Frank Harrell that SAS produces 'the worst 
> graphics in the statistical software world.'
> One can get ugly graphs in R almost as easily in SAS just by accepting 
> the 80-20 rule:  You can typically get 80% of
> what you want with 20% of the effort.  To get what you really want takes 
> the remaining 80% of effort.
> On the other hand, the active hard work of many R developers has led to 
> R graphics for which the *default*
> results for many graphs avoid many of the egregious design errors 
> introduced in SAS in the days of pen-plotters
> (+ signs for points, cross-hatching for area fill).
> 
> -Michael
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From ivo_welch-rstat8783 at mailblocks.com  Sun Nov 21 18:15:40 2004
From: ivo_welch-rstat8783 at mailblocks.com (ivo_welch-rstat8783@mailblocks.com)
Date: Sun, 21 Nov 2004 09:15:40 -0800
Subject: [R] sas vs. R
In-Reply-To: <200411211112.iALB5f6d029937@hypatia.math.ethz.ch>
References: <200411211112.iALB5f6d029937@hypatia.math.ethz.ch>
Message-ID: <200411211715.iALHFfbl019928@hypatia.math.ethz.ch>


SAS

* better manuals.
* tech support for most universities contracted into the price, thus 
for researchers.
* batch orientation.  if you have to handle data sets that are as large 
as your memory, SAS generally does it better.  It seems to be an 
"n-pass design."  Years ago, when memory was expensive, I could not use 
S/R even for simple problems.  Just a few simple operations, and I was 
disk thrashing.
* all sorts of corporate-oriented data base and ready-to-go application 
stuff, often not statistical in nature, at all.

R

* actually, I believe that perl---which can be used as R or SAS 
backend---beats even weird SAS input statements in its flexibility.  
though don't get me going on how crazy it is not to have in-code data 
set embedding.
* a real programming language and a real graphics language.
* some stuff (e.g., built-in statistical procedures) are a bit overly 
complex; other stuff is so beautifully simple and intuitive that it 
borders on a miracle.
* interactive design.

both suffer from weird mysteries---magic incantations that gurus know, 
and ordinary people cannot easily find.

and let me say---despite prof brian ripley's occasional grumpiness ( 
;-) ), he and the rest if the core R group have done absolutely amazing 
things for the community, both building the program and in helping 
support it on this forum.  I wish some of the corporations or 
universities that are using SAS would fund the R group a little, too.

regards,

/ivo
---
ivo welch
professor of finance and economics
brown / nber / yale



From Ted.Harding at nessie.mcc.ac.uk  Sun Nov 21 18:01:35 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 21 Nov 2004 17:01:35 -0000 (GMT)
Subject: [R] How to change the significant codes default?
In-Reply-To: <41A0C1AF.4060903@statistik.uni-dortmund.de>
Message-ID: <XFMail.041121170135.Ted.Harding@nessie.mcc.ac.uk>

On 21-Nov-04 Uwe Ligges wrote:
> (Ted Harding) wrote:
>> [...]
>> Then, when you want your private version, simply do
>> 
>>   source("printCoefmat.R")
>> 
>> and it will overlay the original version. (Experts will have
>> to advise whether this clashes with any "namespace" issues.
>> On my reading of the code, it doesn't seem to; but I'm no
>> expert!)
> 
> Ted, it "clashes"! Functions in the namespace are looked up at first.
> 
> Uwe

Oops! Thanks, Uwe; now I know!
Cheers,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 21-Nov-04                                       Time: 17:01:35
------------------------------ XFMail ------------------------------



From andy_liaw at merck.com  Sun Nov 21 18:39:08 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sun, 21 Nov 2004 12:39:08 -0500
Subject: [R] How to change the significant codes default?
Message-ID: <3A822319EB35174CA3714066D590DCD50994E35C@usrymx25.merck.com>

> From: Uwe Ligges
> 
> (Ted Harding) wrote:
> 
> > On 20-Nov-04 Uwe Ligges wrote:
> > 
> >>Shigeru Mase wrote:
> >>
> >>>Dear R experts,
> >>>
> >>>I am posting this question on behalf of a Japanese R user
> >>>who wants to know how to change the siginificant codes default.
> >>>As you know, R's default significant codes are:
> >>>
> >>> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
> >>>
> >>>But he says that it is usual in economics to give codes such as
> >>>
> >>> `***' for 0.01, `**' for 0.05 and `*' for 0.10
> >>>
> >>>I don't know if this is true (common) or not, but what I as well
> >>>as he are puzzled is that, apparently, there is no part in 
> the code,
> >>>say that of summary.lm, which produces these significant codes
> >>>as well as the annotation above. A quick search of "rking" using
> >>>keywords "significant codes star" gave me no information.
> >>>
> >>>Thanks in advance.
> >>
> >>For example, calling summary(lmObject) dispatches on method
> >>summary.lm() 
> >>hwich creates an object of class "summary.lm".
> >>The latter is printed by method print.summary.lm() which calls 
> >>printCoefmat().
> >>
> >>The stars are hard-coded there, and I don't think anybody 
> is going to 
> >>change that. I suggest to turn of the printing of 
> siginificant codes by
> >>specifying
> >>   print(summary(.....), signif.stars = FALSE)
> >>or by setting the corresponding option().
> >>
> >>Uwe Ligges
> > 
> > 
> > It would be possible to re-define 'printCoefmat' privately
> > so as to change the lines
> > 
> >       cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1),
> >       symbols = c("***", "**", "*", ".", " "))
> > 
> > towards the end of its code into whatever you prefer, e.g.
> > 
> >       cutpoints = c(0, 0.01, 0.05, 0.1, 1),
> >       symbols = c("***", "**", "*", " "))
> > 
> > or
> > 
> >       cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1),
> >       symbols = c("****", "***", "**", "*", " "))
> > 
> > (both of which are compatible with your description of what
> > is needed).
> > 
> > The most straightforward way of redefining it is to copy
> > the code for 'printCoefmat' into a file, e.g.
> > 
> >   sink("printCoefmat.R")
> >   printCoefmat
> >   sink()
> > 
> > and then edit that file.
> > NOTE that the code written to the file does not include
> > the name of the function, i.e. it starts
> > 
> >   function (x, digits = max(3, getOption("digits") - 2),....
> > 
> > so the first modification has to be
> > 
> >   printCoefmat<-function(x, digits = .... )
> > 
> > Then, when you want your private version, simply do
> > 
> >   source("printCoefmat.R")
> > 
> > and it will overlay the original version. (Experts will have
> > to advise whether this clashes with any "namespace" issues.
> > On my reading of the code, it doesn't seem to; but I'm no
> > expert!)
> 
> Ted, it "clashes"! Functions in the namespace are looked up at first.
> 
> Uwe

As well, try to count the number of times people modified functions in base
without renaming, then asked if there's a bug when that was forgotten, or
when behavior of the function changed in newer version of R...

Andy


 
> > If your friend wants to use this new definition all the time,
> > then one way to arrange this is to put the revised function
> > definition (as in the edited file) into his .Rprofile,
> > or put the command
> >   source("printCoefmat")
> > into that file.
> > 
> > Best wishes,
> > Ted.
> > 
> > 
> > --------------------------------------------------------------------
> > E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> > Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
> > Date: 20-Nov-04                                       Time: 19:13:23
> > ------------------------------ XFMail ------------------------------
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From MSchwartz at MedAnalytics.com  Sun Nov 21 18:53:45 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Sun, 21 Nov 2004 11:53:45 -0600
Subject: [R] sas vs. R
In-Reply-To: <200411211715.iALHFfbl019928@hypatia.math.ethz.ch>
References: <200411211112.iALB5f6d029937@hypatia.math.ethz.ch>
	<200411211715.iALHFfbl019928@hypatia.math.ethz.ch>
Message-ID: <1101059625.16325.82.camel@horizons.localdomain>

On Sun, 2004-11-21 at 09:15 -0800, ivo_welch-rstat8783 at mailblocks.com
wrote:
> I wish some of the corporations or 
> universities that are using SAS would fund the R group a little, too.

We do via the R Foundation!

http://www.r-project.org/nosvn/foundation/memberlist.html

Talk to the folks at your institutions...

Marc Schwartz
<No longer a SAS user>



From dmb at mrc-dunn.cam.ac.uk  Sun Nov 21 19:12:16 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Sun, 21 Nov 2004 18:12:16 +0000 (GMT)
Subject: [R] Analysis of pre-calculated frequency distribution?
In-Reply-To: <XFMail.041121164705.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.LNX.4.21.0411211753570.26310-100000@mail.mrc-dunn.cam.ac.uk>

On Sun, 21 Nov 2004 Ted.Harding at nessie.mcc.ac.uk wrote:

>On 21-Nov-04 Dan Bolser wrote:
>> 
>> Sorry for the dumb question, but I cant work out how to do this. 
>> 
>> Quick version, 
>> 
>> How can I re-bin a given frequency distribution using new breaks
>> without reference to the original data? Given distribution has
>> integer valued bins.
>> 
>> 
>> Long version,
>> 
>> I am loading a frequency table into R from a file. The original
>> data is very large, and it is a very simple process to get a
>> frequency distribution from an SQL database, so in all this is
>> a convenient method for me. Point being I don't start with 'raw' data.
>> 
>> The data looks like this...
>> 
>>> dat
>>              COUNT FREQUENCY
>> 1                1 5734
>> 2                2 1625
>> [...]
>> 365           9442    1
>> 366          12280    1
>> 
>> [...]
>> 
>> People tipically quote the curve in the form of y ~ Cx^(-a).
>> I want to use the binning method of parameter estimation given here...
>> 
>> http://www.ece.uc.edu/~annexste/Courses/cs690/Zipf,%20Power-law,%20Paret
>> o%20-%20a%20ranking%20tutorial.htm
>> 
>> (bin the data with exponentially increasing bin widths within the data
>> range).
>> 
>> But I can't work out how to re-bin my existing frequency data.
>
>Hi Dan,
>Your starting point can be the fact that the number of cases
>with property i ("in class i") is COUNT_i + FREQUENCY_I
>
>So if you construct a vector with these numbers in it you have
>in effect reconstructed the original data.
>
>I.e.  N[i] <- COUNT[i]*FREQUENCY[i]


Cheers for this, I was trying this, but my results looked wrong with
respect to the data shown on the webpage cited above.

Thanks to James Holtman for the other suggestion - 

My confusion was coming from thinking I had to use hist, but in fact cut +
tapply was the ticket.

Cheers,
Dan.

>
>which can be done in one stroke with N <- COUNT*FREQUENCY
>
>One way (and maybe others can suggest better) to bin these
>classes non-uniformly could be:
>
>  Say you have k "upper" breakpoints for your k bins,
>  say BP, so that e.g. if BP[1] = 2 then there are N[1]+N[2]
>  cases with class <= 2, and if BP[2] = 5 then there are
>  N[3] + N[4] + N[5] cases with class > 2 and class <= 5,
>  and so on. In your case BP[k] = 366.
>
>  Let
>
>    csN <- cumsum(N)
>
>  Then (if I've not overlooked something)
>
>    diff(c(0,csN[BP]))
>
>  will give you the counts in yhour new bins.
>
>E.g. (just to show it should work):
>
>  > N<-rep(1,31)
>  > BP<-c(1,3,7,15,31)
>  > csN <- cumsum(N)
>  > diff(c(0,csN[BP]))
>  [1]  1  2  4  8 16
>
>
>  > BP<-c(2,3,5,9,17,31)
>  > diff(c(0,csN[BP]))
>  [1]  2  1  2  4  8 14
>
>I hope this matches the sort of thing you have in mind!
>Ted.
>
>
>--------------------------------------------------------------------
>E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
>Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
>Date: 21-Nov-04                                       Time: 16:47:05
>------------------------------ XFMail ------------------------------
>



From ggrothendieck at myway.com  Sun Nov 21 19:41:41 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 21 Nov 2004 18:41:41 +0000 (UTC)
Subject: [R] How to change the significant codes default?
References: <XFMail.041120191323.Ted.Harding@nessie.mcc.ac.uk>
	<41A0C1AF.4060903@statistik.uni-dortmund.de>
Message-ID: <loom.20041121T193813-325@post.gmane.org>

Uwe Ligges <ligges <at> statistik.uni-dortmund.de> writes:

: 
: (Ted Harding) wrote:
: 
: > On 20-Nov-04 Uwe Ligges wrote:
: > 
: >>Shigeru Mase wrote:
: >>
: >>>Dear R experts,
: >>>
: >>>I am posting this question on behalf of a Japanese R user
: >>>who wants to know how to change the siginificant codes default.
: >>>As you know, R's default significant codes are:
: >>>
: >>> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
: >>>
: >>>But he says that it is usual in economics to give codes such as
: >>>
: >>> `***' for 0.01, `**' for 0.05 and `*' for 0.10
: >>>
: >>>I don't know if this is true (common) or not, but what I as well
: >>>as he are puzzled is that, apparently, there is no part in the code,
: >>>say that of summary.lm, which produces these significant codes
: >>>as well as the annotation above. A quick search of "rking" using
: >>>keywords "significant codes star" gave me no information.
: >>>
: >>>Thanks in advance.
: >>
: >>For example, calling summary(lmObject) dispatches on method
: >>summary.lm() 
: >>hwich creates an object of class "summary.lm".
: >>The latter is printed by method print.summary.lm() which calls 
: >>printCoefmat().
: >>
: >>The stars are hard-coded there, and I don't think anybody is going to 
: >>change that. I suggest to turn of the printing of siginificant codes by
: >>specifying
: >>   print(summary(.....), signif.stars = FALSE)
: >>or by setting the corresponding option().
: >>
: >>Uwe Ligges
: > 
: > 
: > It would be possible to re-define 'printCoefmat' privately
: > so as to change the lines
: > 
: >       cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1),
: >       symbols = c("***", "**", "*", ".", " "))
: > 
: > towards the end of its code into whatever you prefer, e.g.
: > 
: >       cutpoints = c(0, 0.01, 0.05, 0.1, 1),
: >       symbols = c("***", "**", "*", " "))
: > 
: > or
: > 
: >       cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1),
: >       symbols = c("****", "***", "**", "*", " "))
: > 
: > (both of which are compatible with your description of what
: > is needed).
: > 
: > The most straightforward way of redefining it is to copy
: > the code for 'printCoefmat' into a file, e.g.
: > 
: >   sink("printCoefmat.R")
: >   printCoefmat
: >   sink()
: > 
: > and then edit that file.
: > NOTE that the code written to the file does not include
: > the name of the function, i.e. it starts
: > 
: >   function (x, digits = max(3, getOption("digits") - 2),....
: > 
: > so the first modification has to be
: > 
: >   printCoefmat<-function(x, digits = .... )
: > 
: > Then, when you want your private version, simply do
: > 
: >   source("printCoefmat.R")
: > 
: > and it will overlay the original version. (Experts will have
: > to advise whether this clashes with any "namespace" issues.
: > On my reading of the code, it doesn't seem to; but I'm no
: > expert!)
: 
: Ted, it "clashes"! Functions in the namespace are looked up at first.
: 

True, but one can still get the effect by using assignInNamespace.
For example, run these two lines (the body(...) <- line is just for
illustration here.  You want to ultimately replace that line with your
redefined printCoefmat:  printCoefmat <- function... as discussed by Ted.)

body(printCoefmat) <- parse(text = "cat('Greetings from printCoefmat!!!')")
assignInNamespace("printCoefmat", printCoefmat, "stats")

Now running summary.lm as shown below displays the desired Greetings line:

R> example(lm)
...snip...
R> summary(lm.D90)

Call:
lm(formula = weight ~ group - 1)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.0710 -0.4938  0.0685  0.2462  1.3690 

Coefficients:
Greetings from printCoefmat!!!
Residual standard error: 0.6964 on 18 degrees of freedom
Multiple R-Squared: 0.9818,     Adjusted R-squared: 0.9798 
F-statistic: 485.1 on 2 and 18 DF,  p-value: < 2.2e-16



From wuertz at itp.phys.ethz.ch  Sun Nov 21 20:33:28 2004
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Sun, 21 Nov 2004 19:33:28 +0000
Subject: [R] Re: [R-sig-finance] Question about Exponential Weighted Moving
 Average (EWMA) in rmetrics.
In-Reply-To: <Pine.GSO.4.58.0411201850340.16076@play.cs.columbia.edu>
References: <Pine.GSO.4.58.0411201850340.16076@play.cs.columbia.edu>
Message-ID: <41A0ED88.6080801@itp.phys.ethz.ch>

With the next release of  Rmetrics the help page will extended in the 
following way:

\item{lambda}{
        a numeric value between zero and one giving the decay length
        of the exponential moving average. If an integer value greater
        than one is given, lambda is used as a lag of  "n" periods to
        calculate the decay parameter.
        }

Please note that you will find much more (still undocumented) indicators 
in the
example file xmpTradingIndicators.R. These include.

#        accelTA
#        adiTA
#        adoscillatorTA
#        bollingerTA
#        chaikinoTA
#        chaikinvTA
#        garmanKlassTA
#        macdTA
#        medpriceTA
#        momentumTA
#        nviTA
#        obvTA
#        pviTA
#        pvtrendTA
#        rocTA
#        rsiTA
#        stochasticTA
#        typicalPriceTA
#        wcloseTA
#        williamsadTA
#        williamsrTA


If you have written functions for further trading indicators, please let 
me know, so
I can add them to Rmetrics.


Diethelm Wuertz.


German G. Creamer wrote:

>Please disregard the previous message. I realized that in the emaTA
>equation,
>a lambda greater than one is used as a lag of n periods to calculate the
>decay parameter. A lambda less than one is used directly as the decay
>parameter.
>
>So, the functions are consistent.
>
>Thanks anyway,
>
>German
>
>_______________________________________________
>R-sig-finance at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-sig-finance
>
>  
>



From postmaster.par-msw at sgcib.com  Sun Nov 21 20:48:19 2004
From: postmaster.par-msw at sgcib.com (postmaster.par-msw@sgcib.com)
Date: Sun, 21 Nov 2004 20:48:19 +0100 (CET)
Subject: [R] Your mail have been blocked
Message-ID: <200411211948.iALJmJX4011867@parsnd02.testsgib.com>

Your mail have been blocked



From ssim at lic.co.nz  Sun Nov 21 22:28:17 2004
From: ssim at lic.co.nz (ssim@lic.co.nz)
Date: Mon, 22 Nov 2004 10:28:17 +1300
Subject: [R] RE : Create sequence for dataset
Message-ID: <OF803FD2FA.BB6F4645-ONCC256F53.0075711F-CC256F53.0075F1F2@livestock.org.nz>

Dear members,

I want to create a sequence of numbers for the multiple records of
individual animal in my dataset. The SAS code below will do the trick, but
I want to learn to do it in R. Can anyone help ?

data ht&ssn;
set ht&ssn;
by anml_key;
if first.anml_key then do;
seq_ht_rslt=0;
end;
seq_ht_rslt+1;

Thanks in advance.

Stella
___________________________________________________________________________
This message, including attachments, is confidential. If you are not the
intended recipient, please contact us as soon as possible and then destroy
the message. Do not copy, disclose or use the contents in any way.

The recipient should check this email and any attachments for viruses and
other defects. Livestock Improvement Corporation Limited and any of its
subsidiaries and associates are not responsible for the consequences of any
virus, data corruption, interception or unauthorised amendments to this
email.

Because of the many uncertainties of email transmission we cannot guarantee
that a reply to this email will be received even if correctly sent. Unless
specifically stated to the contrary, this email does not designate an
information system for the purposes of section 11(a) of the New Zealand
Electronic Transactions Act 2002.



From john.maindonald at anu.edu.au  Sun Nov 21 23:35:02 2004
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Mon, 22 Nov 2004 09:35:02 +1100
Subject: [R] Location of grobs etc on lattice output
In-Reply-To: <200411202241.31479.deepayan@stat.wisc.edu>
References: <82DA8727-3B5E-11D9-A5CC-000A95CDA0F2@anu.edu.au>
	<200411202241.31479.deepayan@stat.wisc.edu>
Message-ID: <95453B16-3C0D-11D9-A5CC-000A95CDA0F2@anu.edu.au>

I'm puzzled about side effects of trellis.unfocus():

The following runs without problem, though grid.text() does not
seem to do anything.  (I'd thought that I had it working at one point.)

     library(DAAG); library(lattice); library(grid)
     cuckoos.strip <- stripplot(species ~ length, xlab="", data=cuckoos)
     cuckoos.bw <- bwplot(species~length, xlab="Length of egg (mm)",
                          data=cuckoos)
     vp0 <- viewport(layout=grid.layout(2, 1))
     pushViewport(vp0)
     vp1 <- viewport(layout.pos.row=1)
     vp2 <- viewport(layout.pos.row=2)
     pushViewport(vp1)
     print(cuckoos.strip,newpage=FALSE)
#   trellis.focus("panel", row=1, column=1, clip.off=TRUE)
     grid.text("A", x=unit(0,"native"), y=unit(1.05,"native"),
               gp=gpar(fontsize=9))
#   trellis.unfocus()  ## & remove the following upViewport()
     upViewport()
     pushViewport(vp2)
     print(cuckoos.bw, newpage=FALSE)
     trellis.focus("panel", row=1, column=1, clip.off=TRUE)
     grid.text("B", x=unit(0,"native"), y=unit(1.05,"native"),
               gp=gpar(fontsize=9))
     trellis.unfocus()

If I remove the #'s, and remove the upViewport() that
follows the second #, I seem to lose the current tree,
as though the newpage=FALSE for the next print()
is ignored.  Should I be able to do something like this?
Clearly I do not understand what happens when
trellis.focus() is invoked.

This seems an area where an effective GUI, with a
graphical display of the viewport tree, could be very
helpful.

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Bioinformation Science, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.


On 21 Nov 2004, at 3:41 PM, Deepayan Sarkar wrote:

> On Saturday 20 November 2004 19:41, John Maindonald wrote:
>> Is there any way, after use of print.trellis(), to obtain the
>> co-ordinates of the plot region, e.g., in what are then the
>> native co-ordinates?
>
> Have you read help(trellis.focus)? This is new in 2.0.0 and the
> recommended API for interacting with lattice plots (you can of course
> use grid tools directly, but details are more likely to change at that
> level).
>
> It hasn't had much testing, so I would appreciate reports of things 
> that
> should be doable easily but isn't.
>
>> e.g.
>>   library(DAAG)
>>   library(lattice); library(grid)
>>   data(cuckoos)
>>   pushViewport(viewport(layout=grid.layout(2, 1)))
>>   pushViewport(viewport(layout.pos.row=1))
>>   cuckoos.strip <- stripplot(species ~ length, data=cuckoos)
>>   print(cuckoos.strip, newpage=FALSE)
>>   grid.text("A", x=unit(0.18,"native"), y=unit(0.925,"native"))
>>    # This works, but is fiddly, and needs rejigging if width
>>    # or fontsize are changed.
>>   popViewport(1)
>>
>> An alternative would of course be to access the co-ordinate
>> system used by the lattice function for locating the panels,
>> or for locating labelling.
>>
>> As in the example above, I have been using grid.text() to
>> position text outside the plot region, but closer to the "top"
>> axis than the legend parameter to the lattice function will
>> allow.
>
> trellis.focus("panel", row=1, column=1, clip.off=TRUE)
>
> will put you in the plot region (panel), but will switch off clipping 
> so
> you can write text outside.
>
> You can also now control the amount of space between the axis and
> legend, see
>
> str(trellis.par.get("layout.heights"))
>
> Deepayan
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ray at mcs.vuw.ac.nz  Sun Nov 21 23:53:54 2004
From: ray at mcs.vuw.ac.nz (Ray Brownrigg)
Date: Mon, 22 Nov 2004 11:53:54 +1300 (NZDT)
Subject: [R] adjusting the map of France to 1830
Message-ID: <200411212253.iALMrsG2002493@ihi.mcs.vuw.ac.nz>

> Date: Fri, 19 Nov 2004 15:59:25 -0500
> From: Michael Friendly <friendly at yorku.ca>
> 
> Here's what I tried.   I can plot a selection of regions, but I
> can't seem to remove an arbitrary list of region numbers, unless I've 
> done something wrong
> by selecting the regions I want to plot with departements[-exclude].

I think here the problemis not using exact=T in the call to map(), see
below.

> I also get an error
> when I try to use map.text to label a map with only the regions I'm  
> selecting.
> 
>  > departements <- map('france',namesonly=T, plot=FALSE)
>  > # returns a vector of names of regions
>  >
>  > exclude <- c(47,  #Alpes-Maritimes
> + 66,  # Haute-Savoie
> + 76,  # Savoie
> + 95,  # Territore-de-Belfort
> + 109, 110, 111, # Var: Iles d'Hyeres
> + 49, 53, 54, 55, # Moribhan: Isles
> + 62, 64,    # Vendee: Isles
> + 72, 75     # Charente-Maritime: Isles
> + )
>  >
>  > depts <- departements[-exclude]
>  > gfrance <-map('france', regions=depts)
>  > labels <- (as.character(1:length(departements)))[-exclude]
>  > gfrance <-map.text('france', regions=depts, add=FALSE, labels=labels)
> Error in map.text("france", regions = depts, add = FALSE, labels = labels) :
>         map object must have polygons (fill=TRUE)
> 
That error message is issued when regions= specifies less than the whole
database, in which case the alternate "list of 'x', 'y', and 'names'
obtained from a previous call to 'map'" is required as a first parameter.

So to do what you want (notwithstanding the next problem you raise), try
the following as a demonstration:

gfrance <- map('france', regions=depts, exact=T, fill=T, plot=F)
map.text(gfrance, regions=depts, labels=labels)  
map('france', regions=departements[exclude], fill=T, col=1, add=T)

> Another problem, potentially more difficult for mapping data on the map 
> of France is that
> the "departements" are actually just the polygons in the map, 
> arbitrarily numbered from
> east to west, and from north to south --- they don't correspond to the 
> 'official' administrative
> region numbers.  As well, the departement names don't always match 
> exactly (ignoring
> accents, e.g., Val-d'Oise vs. Val-Doise) so it would be another 
> challenge to plot my
> historical data on the map of France.
> 
Well, maps is a source package! [:-)].

You are most welcome to modify the source files
maps/src/france.{gon,line,name} to reorder the polygons (and correct
errors in the names).  If the relationship between those 3 files is not
obvious, contact me for further details.  Also, I am happy to fold your
changes back into the original maps package.

Ray Brownrigg



From p.dalgaard at biostat.ku.dk  Sun Nov 21 23:57:49 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 21 Nov 2004 23:57:49 +0100
Subject: [R] RE : Create sequence for dataset
In-Reply-To: <OF803FD2FA.BB6F4645-ONCC256F53.0075711F-CC256F53.0075F1F2@livestock.org.nz>
References: <OF803FD2FA.BB6F4645-ONCC256F53.0075711F-CC256F53.0075F1F2@livestock.org.nz>
Message-ID: <x21xemq07m.fsf@biostat.ku.dk>

ssim at lic.co.nz writes:

> Dear members,
> 
> I want to create a sequence of numbers for the multiple records of
> individual animal in my dataset. The SAS code below will do the trick, but
> I want to learn to do it in R. Can anyone help ?
> 
> data ht&ssn;
> set ht&ssn;
> by anml_key;
> if first.anml_key then do;
> seq_ht_rslt=0;
> end;
> seq_ht_rslt+1;
> 
> Thanks in advance.

Whoa. Who just said that SAS data step code was clearer than R? Quite
a bit of implicit knowledge in that one.

Here's one way (someone please think up a better name for ave()...):

> x <- numeric(nrow(airquality))
> ave(x, airquality$Month, FUN=function(z)seq(along=z))
  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18
 [19] 19 20 21 22 23 24 25 26 27 28 29 30 31  1  2  3  4  5
 [37]  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 [55] 24 25 26 27 28 29 30  1  2  3  4  5  6  7  8  9 10 11
 [73] 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29
 [91] 30 31  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16
[109] 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  1  2  3
[127]  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21
[145] 22 23 24 25 26 27 28 29 30

or, same basic idea but a little less cryptic:

> tb <- table(airquality$Month) 
> l <- lapply(tb, function(x)seq(length=x))
> unsplit(l, airquality$Month)   
  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18
 [19] 19 20 21 22 23 24 25 26 27 28 29 30 31  1  2  3  4  5
(etc.)

or, brute force and ignorance:

> x <- numeric(nrow(airquality))
> for (i in unique(airquality$Month)) {
+   ix <- airquality$Month == i
+   x[ix] <- seq(along=x[ix])
+ }
> x
  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18
 [19] 19 20 21 22 23 24 25 26 27 28 29 30 31  1  2  3  4  5
....

or, going to the opposite extreme (Gabor et al. are going to try and
beat me on this...):

> seq.factor <- function(f) ave(rep(1,length(f)),f,FUN=cumsum)
> seq(as.factor(airquality$Month))
  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18
 [19] 19 20 21 22 23 24 25 26 27 28 29 30 31  1  2  3  4  5
....

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From james.holtman at convergys.com  Mon Nov 22 00:18:56 2004
From: james.holtman at convergys.com (james.holtman@convergys.com)
Date: Sun, 21 Nov 2004 18:18:56 -0500
Subject: [R] RE : Create sequence for dataset
Message-ID: <OF6F10B51D.09BCDFD4-ON85256F53.007FFEA8@nd.convergys.com>





I think this might do it.

> x.1 <- data.frame(x=sample(1:3,20,T), y=sample(10:12,20,T))  # create
test data
> x.1  # print it out
   x  y
1  2 11
2  3 11
3  2 10
4  1 12
5  3 11
6  1 10
7  3 10
8  1 11
9  1 12
10 1 11
11 1 12
12 1 12
13 2 11
14 3 11
15 3 10
16 3 10
17 2 12
18 2 10
19 3 11
20 2 11
# split the data by the numbers in 'x' (would be your 'amnl_key)
# and add a column containing the sequence number
> x.s <- by(x.1, x.1$x, function(x){x$seq <- seq(along=x$x); x})
# the result in 'x.s' is a list and the rows have to be recombined (rbind)
to form the result
> x.s  # print out the data
x.1$x: 1
   x  y seq
4  1 12   1
6  1 10   2
8  1 11   3
9  1 12   4
10 1 11   5
11 1 12   6
12 1 12   7
------------------------------------------------------------
x.1$x: 2
   x  y seq
1  2 11   1
3  2 10   2
13 2 11   3
17 2 12   4
18 2 10   5
20 2 11   6
------------------------------------------------------------
x.1$x: 3
   x  y seq
2  3 11   1
5  3 11   2
7  3 10   3
14 3 11   4
15 3 10   5
16 3 10   6
19 3 11   7
> do.call('rbind', x.s)  # bind the rows and print out the result
     x  y seq
1.4  1 12   1
1.6  1 10   2
1.8  1 11   3
1.9  1 12   4
1.10 1 11   5
1.11 1 12   6
1.12 1 12   7
2.1  2 11   1
2.3  2 10   2
2.13 2 11   3
2.17 2 12   4
2.18 2 10   5
2.20 2 11   6
3.2  3 11   1
3.5  3 11   2
3.7  3 10   3
3.14 3 11   4
3.15 3 10   5
3.16 3 10   6
3.19 3 11   7
>
__________________________________________________________
James Holtman        "What is the problem you are trying to solve?"
Executive Technical Consultant  --  Office of Technology, Convergys
james.holtman at convergys.com
+1 (513) 723-2929


                                                                                                                                           
                      ssim at lic.co.nz                                                                                                       
                      Sent by:                     To:       r-help at stat.math.ethz.ch                                                      
                      r-help-bounces at stat.m        cc:                                                                                     
                      ath.ethz.ch                  Subject:  [R] RE : Create sequence for dataset                                          
                                                                                                                                           
                                                                                                                                           
                      11/21/2004 16:28                                                                                                     
                                                                                                                                           
                                                                                                                                           




Dear members,

I want to create a sequence of numbers for the multiple records of
individual animal in my dataset. The SAS code below will do the trick, but
I want to learn to do it in R. Can anyone help ?

data ht&ssn;
set ht&ssn;
by anml_key;
if first.anml_key then do;
seq_ht_rslt=0;
end;
seq_ht_rslt+1;

Thanks in advance.

Stella
___________________________________________________________________________
This message, including attachments, is confidential. If you are not the
intended recipient, please contact us as soon as possible and then destroy
the message. Do not copy, disclose or use the contents in any way.

The recipient should check this email and any attachments for viruses and
other defects. Livestock Improvement Corporation Limited and any of its
subsidiaries and associates are not responsible for the consequences of any
virus, data corruption, interception or unauthorised amendments to this
email.

Because of the many uncertainties of email transmission we cannot guarantee
that a reply to this email will be received even if correctly sent. Unless
specifically stated to the contrary, this email does not designate an
information system for the purposes of section 11(a) of the New Zealand
Electronic Transactions Act 2002.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ray at mcs.vuw.ac.nz  Mon Nov 22 00:38:47 2004
From: ray at mcs.vuw.ac.nz (Ray Brownrigg)
Date: Mon, 22 Nov 2004 12:38:47 +1300 (NZDT)
Subject: [R] Re: 3d Map with bars
Message-ID: <200411212338.iALNclQE002590@ihi.mcs.vuw.ac.nz>

> From: partha_bagchi at hgsi.com
> Date: Fri, 19 Nov 2004 16:53:12 -0500
> 
> Thanks for reply. I need to first draw the map of USA a perspective plot. 
> I guess thats where my problem was.
> 

Try something like this:

library(maps)
states <- map("state", plot=F)
x1 <- rep(0, 3)
x2 <- rep(0, 3)
maxz <- 1
z <- matrix(c(0, 0, 0, 0, 0, 0, maxz, maxz, maxz), 3, 3)
x1[-2] <- states$range[1:2]
x2[-2] <- states$range[3:4]
x1[2] <- x1[3] - 1e-6
x2[2] <- x2[3] - 1e-6
pmat <- persp(x1, x2, z, xlab = "", ylab = "", zlab="", axes = F,
  theta=0, phi=20, d=10)
lines(trans3d(states$x, states$y, 0, pmat))
latitude <- -90
longitude <- 37
myz <- 0.6
lines(trans3d(rep(latitude, 2), rep(longitude, 2), c(0, myz), pmat), col=2)

where trans3d is as defined in the examples for persp().

Hope this helps,
Ray Brownrigg

----
> Uwe Ligges <ligges at statistik.uni-dortmund.de>
> 11/19/2004 04:33 PM
> 
>         To:     partha_bagchi at hgsi.com
>         cc:     r-help at stat.math.ethz.ch
>         Subject:        Re: 3d Map with bars
> 
> 
> partha_bagchi at hgsi.com wrote:
> 
> > Apologies in advance for the question. I am trying to draw a map of the 
> US
> > as a surface plot so that I would be able to drop bars on the different
> > states (something like Uwe Ligges' scatterplot3d example 4). I am not 
> sure
> > where to start looking for such a beast. If anyone has any pointers,
> > ideas, I will be grateful.
> >
> > TIA,
> > Partha
> 
> How to "drop bars" with persp() has been described on R-help yesterday
> or today, please check the mailing list's archives.
>



From deepayan at stat.wisc.edu  Mon Nov 22 00:54:54 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Sun, 21 Nov 2004 17:54:54 -0600
Subject: [R] Location of grobs etc on lattice output
In-Reply-To: <95453B16-3C0D-11D9-A5CC-000A95CDA0F2@anu.edu.au>
References: <82DA8727-3B5E-11D9-A5CC-000A95CDA0F2@anu.edu.au>
	<200411202241.31479.deepayan@stat.wisc.edu>
	<95453B16-3C0D-11D9-A5CC-000A95CDA0F2@anu.edu.au>
Message-ID: <200411211754.54735.deepayan@stat.wisc.edu>

On Sunday 21 November 2004 16:35, John Maindonald wrote:
> I'm puzzled about side effects of trellis.unfocus():
>
> The following runs without problem, though grid.text() does not
> seem to do anything.  (I'd thought that I had it working at one
> point.)
>
>      library(DAAG); library(lattice); library(grid)
>      cuckoos.strip <- stripplot(species ~ length, xlab="",
> data=cuckoos) cuckoos.bw <- bwplot(species~length, xlab="Length of
> egg (mm)", data=cuckoos)
>      vp0 <- viewport(layout=grid.layout(2, 1))
>      pushViewport(vp0)
>      vp1 <- viewport(layout.pos.row=1)
>      vp2 <- viewport(layout.pos.row=2)
>      pushViewport(vp1)
>      print(cuckoos.strip,newpage=FALSE)
> #   trellis.focus("panel", row=1, column=1, clip.off=TRUE)
>      grid.text("A", x=unit(0,"native"), y=unit(1.05,"native"),
>                gp=gpar(fontsize=9))

I think you want "npc" rather than "native" here. x=0 on the native 
scale is outside the device area.

> #   trellis.unfocus()  ## & remove the following upViewport()
>      upViewport()
>      pushViewport(vp2)
>      print(cuckoos.bw, newpage=FALSE)
>      trellis.focus("panel", row=1, column=1, clip.off=TRUE)
>      grid.text("B", x=unit(0,"native"), y=unit(1.05,"native"),
>                gp=gpar(fontsize=9))
>      trellis.unfocus()
>
> If I remove the #'s, and remove the upViewport() that
> follows the second #, I seem to lose the current tree,
> as though the newpage=FALSE for the next print()
> is ignored.  Should I be able to do something like this?
> Clearly I do not understand what happens when
> trellis.focus() is invoked.

This is a bug in trellis.unfocus, caused by my not reading grid 
documentation carefully enough, I didn't notice that upViewport(0) 
jumps to the root viewport instead of going up 0 viewports. I'll post 
an update soon.

Quick fix:

assignInNamespace("trellis.unfocus", ns = "lattice",
                  value = function()
{
    if (lattice:::lattice.getStatus("vp.highlighted"))
    {
        grid.remove("lvp.highlight", warn = FALSE)
        lattice:::lattice.setStatus(vp.highlighted = FALSE)
    }
    lattice:::lattice.setStatus(current.focus.column = 0,
                      current.focus.row = 0)
    if (lattice:::lattice.getStatus("vp.depth") > 0)
        upViewport(lattice:::lattice.getStatus("vp.depth"))
    lattice:::lattice.setStatus(vp.depth = 0)
    invisible()
})



> This seems an area where an effective GUI, with a
> graphical display of the viewport tree, could be very
> helpful.

True, but it may be overkill for the amount of use it would get.

Deepayan



From zhliur at yahoo.com  Mon Nov 22 01:06:49 2004
From: zhliur at yahoo.com (yyan liu)
Date: Sun, 21 Nov 2004 16:06:49 -0800 (PST)
Subject: [R] adjacent category model in ordinal regression
Message-ID: <20041122000649.84342.qmail@web53108.mail.yahoo.com>

Hi:
  I want to analyze some multinomial data. And the
response has a natural ordinal sturcture. I want to
fit a adjacent category model to the data by logit,
probit and complementary log-log link functions. I
found a package "VGAM"
(www.stat.auckland.ac.nz/~yee/VGAM/ ) whose function
"acat" can fit the adjacent category model but it only
has "log" and "identity" link functions. Does anybody
know there is a package or function can do the
analysis I want?
  Thank you!

liu

__________________________________________________




From ok at cs.otago.ac.nz  Mon Nov 22 01:10:59 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Mon, 22 Nov 2004 13:10:59 +1300 (NZDT)
Subject: [R] Installing rgl in R2.0.1
Message-ID: <200411220010.iAM0AxIa275680@atlas.otago.ac.nz>

I'm running R2.0.1 under Solaris 2.9 on a SunBlade 100.
When I installed it, I set things up to use the Sun compilers
cc, CC, f95 with the options recommended in the installation and
administration guide.

Until today, no worries.

With all this discussion about R GUIs I thought I'd give R Commander a go.
The web page said to install a bunch of packages first, so I did
> install.packages(c("abind", "car", "effects", "lmtest", "multcomp",
+ "mvtnorm", "relimp", "rgl", "sandwich", "strucchange", "zoo"),
+ dependencies = TRUE)

Again, all went well up to a certain point.  That point was rgl.

* Installing *source* package 'rgl' ...
checking build system type... sparc-sun-solaris2.9
checking host system type... sparc-sun-solaris2.9
checking for gcc... gcc
checking for C compiler default output file name... a.out
checking whether the C compiler works... yes
checking whether we are cross compiling... no
checking for suffix of executables... 
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ANSI C... none needed
checking how to run the C preprocessor... gcc -E
checking for X... libraries /usr/openwin/lib, headers /usr/openwin/include

checking for libpng-config... yes
configure: creating ./config.status
config.status: creating src/Makevars
** libs
CC -I/users/local/lib/R/include -I/usr/openwin/include -DHAVE_PNG_H -I/usr/local/include  -Wall -pedantic -fno-exceptions -fno-rtti -KPIC  -xO4 -xlibmil -dalign -c x11lib.cpp -o x11lib.o
CC: Warning: Option -Wall passed to ld, if ld is invoked, ignored otherwise
CC: Warning: Option -pedantic passed to ld, if ld is invoked, ignored otherwise
CC: Warning: Option -fno-exceptions passed to ld, if ld is invoked, ignored otherwise
...
erwise
CC: Warning: Option -fno-rtti passed to ld, if ld is invoked, ignored otherwise
CC -G -L/usr/local/lib -o rgl.so x11lib.o x11gui.o types.o math.o fps.o pixmap.o gui.o api.o device.o devicemanager.o rglview.o scene.o glgui.o -L/usr/openwin/lib -L/users/local/lib -R/users/local/lib -lpng12 -lz -lm -lstdc++ -lX11 -lXext -lGL -lGLU -lpng12 -lz -lm  
ld: fatal: library -lstdc++: not found
ld: fatal: File processing errors. No output written to rgl.so
*** Error code 1
make: Fatal error: Command failed for target `rgl.so'
ERROR: compilation failed for package 'rgl'

Previous packages figured out from whatever information the R installation
squirrelled away that they should use f95 (not g77) and cc (not gcc) and
provided sensible options.  However, the rgl installation has decided to
do its own configuration, and has decided to use gcc.  That would probably
work, except that it is mixing up the Sun C++ compiler (CC) with the Gnu
command line options (-Wall -pedantic -fno-exceptions .....) AND the Sun
command line options (-xO4 -xlibmil -dalign).  

All my attempts to follow the
http://wsopuppenkiste.wiso.uni-goettingen.de/~dadler/rgl
link on the rgl catalogue card at CRAN have failed.

- Did I do something wrong?
- What if anything can I do about it?



From ok at cs.otago.ac.nz  Mon Nov 22 01:34:45 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Mon, 22 Nov 2004 13:34:45 +1300 (NZDT)
Subject: [R] How to change the significant codes default?
Message-ID: <200411220034.iAM0YjPR247264@atlas.otago.ac.nz>

Shigeru Mase <mase at is.titech.ac.jp> wrote:
	I am posting this question on behalf of a Japanese R user
	who wants to know how to change the siginificant codes default.

It's the line
                 symbols = c("***", "**", "*", ".", " "))
in printCoefmat(), isn't it?

(summary.lm makes an object, it doesn't do any printing;
 getAnywhere("print.summary.lm") turns out to call printCoefmat();
 look in printCoefmat and there it is.)



From br44114 at yahoo.com  Mon Nov 22 01:41:20 2004
From: br44114 at yahoo.com (bogdan romocea)
Date: Sun, 21 Nov 2004 16:41:20 -0800 (PST)
Subject: [R] Running R from CD?
Message-ID: <20041122004120.67125.qmail@web50306.mail.yahoo.com>

Better install and run R from a USB flash drive. This will save you
the trouble of re-writing the CD as you upgrade and install new
packages. Also, you can simply copy the R installation on your work
computer (no install rights needed); R will run.

HTH,
b.


From: Hans van Walen <hans_at_vanwalen.com>
Date: Fri 27 Aug 2004 - 23:54:53 EST


At work I have no permission to install R. So, would anyone know
whether it is possible to create a CD with a running R-installation
for a windows(XP) pc? And of course, how to?

Thank you for your help,
Hans van Walen



From ok at cs.otago.ac.nz  Mon Nov 22 02:38:54 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Mon, 22 Nov 2004 14:38:54 +1300 (NZDT)
Subject: [R] RE : Create sequence for dataset
Message-ID: <200411220138.iAM1csBR266460@atlas.otago.ac.nz>

ssim at lic.co.nz (Stella) asked
	I want to create a sequence of numbers for the multiple records of
	individual animal in my dataset. The SAS code below will do the trick, but
	I want to learn to do it in R. Can anyone help ?
	
	data ht&ssn;
	set ht&ssn;
	by anml_key;
	if first.anml_key then do;
	seq_ht_rslt=0;
	end;
	seq_ht_rslt+1;
	
Someone was saying how readable SAS data steps were.
I must say that as someone who has written code in more than 160
programming languages I find this _completely_ unreadable.
(Is the initial value for seq_ht_rslt 0 or 1?)
So I'm going to have to guess what was intended.

Suppose you have a data.frame ht_ssn and want to add a sequence number
column for it.  That's easy:

	ht_ssn$seqno <- seq(length = nrow(ht_ssn))

Now suppose that there is an ht_ssn$anml_key column which says which
individual animal each row corresponds to, and many rows may correspond
to the same animal.

	data_sequence_number <- function (data, column = "anml_key") {
	    # Extract the key column.
	    # If it is not already a factor, make it one.
	    # From this factor, extract the level numbers.
	    as.numeric(as.factor(data[[column]]))
	}

	ht_ssn$seq_ht_rslt <- data_sequence_number(ht_ssn)

Probably I have completely misunderstood the question.

One thing which will be different is the actual numeric values.
If I've understood the SAS version, it will assign numbers to keys
in the order in which the keys are encountered, while the R code
above will assign numbers to keys in increasing order of key.  So
if the input contains just "Sammy" then "Jumbo" the SAS version
might assign numbers 1, 2 while the R version would assign 2, 1.

If this really matters, use
	x <- data[[column]]
	as.numeric(as.factor(x, levels = unique(x)))



From Jin.Li at csiro.au  Mon Nov 22 03:07:15 2004
From: Jin.Li at csiro.au (Jin.Li@csiro.au)
Date: Mon, 22 Nov 2004 12:07:15 +1000
Subject: [R] How to correct this
Message-ID: <2BEE99D7F6F1484EBDD1D22167385E75375B8A@exqld1-ath.nexus.csiro.au>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041122/8676cd66/attachment.pl

From murdoch at stats.uwo.ca  Mon Nov 22 03:18:16 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 21 Nov 2004 21:18:16 -0500
Subject: [R] Installing rgl in R2.0.1
In-Reply-To: <200411220010.iAM0AxIa275680@atlas.otago.ac.nz>
References: <200411220010.iAM0AxIa275680@atlas.otago.ac.nz>
Message-ID: <kki2q0t5c4ffbhjqqshuigl579ff6a38cc@4ax.com>

On Mon, 22 Nov 2004 13:10:59 +1300 (NZDT), "Richard A. O'Keefe"
<ok at cs.otago.ac.nz> wrote:

>I'm running R2.0.1 under Solaris 2.9 on a SunBlade 100.
>When I installed it, I set things up to use the Sun compilers
>cc, CC, f95 with the options recommended in the installation and
>administration guide.
>
>Until today, no worries.
>
>With all this discussion about R GUIs I thought I'd give R Commander a go.
>The web page said to install a bunch of packages first, so I did
>> install.packages(c("abind", "car", "effects", "lmtest", "multcomp",
>+ "mvtnorm", "relimp", "rgl", "sandwich", "strucchange", "zoo"),
>+ dependencies = TRUE)
>
>Again, all went well up to a certain point.  That point was rgl.
>
>* Installing *source* package 'rgl' ...
>checking build system type... sparc-sun-solaris2.9
>checking host system type... sparc-sun-solaris2.9
>checking for gcc... gcc
>checking for C compiler default output file name... a.out
>checking whether the C compiler works... yes
>checking whether we are cross compiling... no
>checking for suffix of executables... 
>checking for suffix of object files... o
>checking whether we are using the GNU C compiler... yes
>checking whether gcc accepts -g... yes
>checking for gcc option to accept ANSI C... none needed
>checking how to run the C preprocessor... gcc -E
>checking for X... libraries /usr/openwin/lib, headers /usr/openwin/include
>
>checking for libpng-config... yes
>configure: creating ./config.status
>config.status: creating src/Makevars
>** libs
>CC -I/users/local/lib/R/include -I/usr/openwin/include -DHAVE_PNG_H -I/usr/local/include  -Wall -pedantic -fno-exceptions -fno-rtti -KPIC  -xO4 -xlibmil -dalign -c x11lib.cpp -o x11lib.o
>CC: Warning: Option -Wall passed to ld, if ld is invoked, ignored otherwise
>CC: Warning: Option -pedantic passed to ld, if ld is invoked, ignored otherwise
>CC: Warning: Option -fno-exceptions passed to ld, if ld is invoked, ignored otherwise
>...
>erwise
>CC: Warning: Option -fno-rtti passed to ld, if ld is invoked, ignored otherwise
>CC -G -L/usr/local/lib -o rgl.so x11lib.o x11gui.o types.o math.o fps.o pixmap.o gui.o api.o device.o devicemanager.o rglview.o scene.o glgui.o -L/usr/openwin/lib -L/users/local/lib -R/users/local/lib -lpng12 -lz -lm -lstdc++ -lX11 -lXext -lGL -lGLU -lpng12 -lz -lm  
>ld: fatal: library -lstdc++: not found
>ld: fatal: File processing errors. No output written to rgl.so
>*** Error code 1
>make: Fatal error: Command failed for target `rgl.so'
>ERROR: compilation failed for package 'rgl'
>
>Previous packages figured out from whatever information the R installation
>squirrelled away that they should use f95 (not g77) and cc (not gcc) and
>provided sensible options.  However, the rgl installation has decided to
>do its own configuration, and has decided to use gcc.  That would probably
>work, except that it is mixing up the Sun C++ compiler (CC) with the Gnu
>command line options (-Wall -pedantic -fno-exceptions .....) AND the Sun
>command line options (-xO4 -xlibmil -dalign).  
>
>All my attempts to follow the
>http://wsopuppenkiste.wiso.uni-goettingen.de/~dadler/rgl
>link on the rgl catalogue card at CRAN have failed.
>
>- Did I do something wrong?
>- What if anything can I do about it?

I don't think you did anything wrong, but I don't know what you can do
to fix it.  uni-goettingen.de hasn't been responding for a few days.

Duncan Murdoch



From hodgess at gator.uhd.edu  Mon Nov 22 05:22:30 2004
From: hodgess at gator.uhd.edu (Erin Hodgess)
Date: Sun, 21 Nov 2004 22:22:30 -0600
Subject: [R] rhyp function from fBasics
Message-ID: <200411220422.iAM4MUk15900@gator.dt.uh.edu>

Dear R People:

There is a function from the fBasics library to get the probability
and quantiles for the hyperbolic probability function.

Is there one that will estimate parms of the hyperbolic probability
function from a data set, please?

Thanks in advance!

Sincerely,
Erin Hodgess
mailto: hodgess at gator.uhd.edu
R Version 2.0.1 windows



From d.scott at auckland.ac.nz  Mon Nov 22 05:37:16 2004
From: d.scott at auckland.ac.nz (David Scott)
Date: Mon, 22 Nov 2004 17:37:16 +1300 (NZDT)
Subject: [R] rhyp function from fBasics
In-Reply-To: <200411220422.iAM4MUk15900@gator.dt.uh.edu>
References: <200411220422.iAM4MUk15900@gator.dt.uh.edu>
Message-ID: <Pine.LNX.4.60.0411221736370.5938@stat71.stat.auckland.ac.nz>

On Sun, 21 Nov 2004, Erin Hodgess wrote:

> Dear R People:
>
> There is a function from the fBasics library to get the probability
> and quantiles for the hyperbolic probability function.
>
> Is there one that will estimate parms of the hyperbolic probability
> function from a data set, please?
>

Look at the package HyperbolicDist

David Scott
_________________________________________________________________
David Scott	Department of Statistics, Tamaki Campus
 		The University of Auckland, PB 92019
 		Auckland	NEW ZEALAND
Phone: +64 9 373 7599 ext 86830		Fax: +64 9 373 7000
Email:	d.scott at auckland.ac.nz


Graduate Officer, Department of Statistics



From sundar.dorai-raj at pdf.com  Mon Nov 22 05:40:20 2004
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Sun, 21 Nov 2004 22:40:20 -0600
Subject: [R] How to correct this
In-Reply-To: <2BEE99D7F6F1484EBDD1D22167385E75375B8A@exqld1-ath.nexus.csiro.au>
References: <2BEE99D7F6F1484EBDD1D22167385E75375B8A@exqld1-ath.nexus.csiro.au>
Message-ID: <41A16DB4.6000909@pdf.com>



Jin.Li at csiro.au wrote:

> Hi there,
> 
>  
> 
> I tried to add a few circles on an existing figure using the following
> codes
> 
>  
> 
> grid.circle(x=0.5, y=0.5, r=0.1, draw=TRUE,  gp=gpar(col=5))
> 
> grid.circle(x=0.5, y=0.5, r=0.3, draw=TRUE, gp=gpar(col=5))
> 
> grid.circle(x=0.5, y=0.5, r=0.5, draw=TRUE, gp=gpar(col=5))
> 
> points(0.5, 0.5, col = 5) # centre of the circle
> 
>  
> 
> , but all circles moved away from the centre.  Could we do any
> corrections to this? Thanks.
> 
>  
> 
> Regards,
> 
>  
> 
> Jin
> 

If you are using Lattice (where grid is a more natural fit) then you can 
do the following:

library(lattice)
library(grid)
xyplot(0.5 ~ 0.5,
        panel = function(x, y, ...) {
          grid.circle(x, y, 0.5, default.units = "native")
          panel.xyplot(x, y, ...)
        },
        xlim = c(0, 1),
        ylim = c(0, 1))

I'm not sure how grid is supposed to behave in a non-trellis.device.

--sundar



From sundar.dorai-raj at pdf.com  Mon Nov 22 05:56:30 2004
From: sundar.dorai-raj at pdf.com (Sundar Dorai-Raj)
Date: Sun, 21 Nov 2004 22:56:30 -0600
Subject: [R] rhyp function from fBasics
In-Reply-To: <200411220422.iAM4MUk15900@gator.dt.uh.edu>
References: <200411220422.iAM4MUk15900@gator.dt.uh.edu>
Message-ID: <41A1717E.30206@pdf.com>



Erin Hodgess wrote:

> Dear R People:
> 
> There is a function from the fBasics library to get the probability
> and quantiles for the hyperbolic probability function.
> 
> Is there one that will estimate parms of the hyperbolic probability
> function from a data set, please?
> 
> Thanks in advance!
> 
> Sincerely,
> Erin Hodgess
> mailto: hodgess at gator.uhd.edu
> R Version 2.0.1 windows
> 

Erin,

You can use MASS::fitdistr to do this, I believe.

library(fBasics)
library(MASS)
x <- rhyp(1000, alpha = 2, beta = 1, delta = 1)
fitdistr(x, dhyp, list(alpha = 1, beta = 0.5, delta = 0.5))

--sundar



From Benjamin.Osborne at uvm.edu  Mon Nov 22 06:11:13 2004
From: Benjamin.Osborne at uvm.edu (Benjamin M. Osborne)
Date: Mon, 22 Nov 2004 00:11:13 -0500
Subject: [R] variable object naming
Message-ID: <1101100273.41a174f12a2e2@webmail.uvm.edu>

Is it possible to give a temporary object a name that varies with each run of a
foreloop?  For example, I want to fill a matrix every time I run a loop, and I
want a new matrix with each run, with an appropriate new name.
i.e.:
for(i in 1:5){...  matrix.i<-some values ...}

so that in the end I would have:
matrix.1
matrix.2
matrix.3
matrix.4
matrix.5

Thanks,
Ben Osborne

--
Botany Department
University of Vermont
109 Carrigan Drive
Burlington, VT 05405

benjamin.osborne at uvm.edu
phone: 802-656-0297
fax: 802-656-0440



From Jin.Li at csiro.au  Mon Nov 22 06:20:49 2004
From: Jin.Li at csiro.au (Jin.Li@csiro.au)
Date: Mon, 22 Nov 2004 15:20:49 +1000
Subject: [R] How to correct this
Message-ID: <2BEE99D7F6F1484EBDD1D22167385E75375B97@exqld1-ath.nexus.csiro.au>

Hi there,

I would like to add a few circles to the following image: 
x<-seq(0,1,0.2)
y<-x
pred<-matrix(c(0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.7, 0.7, 0.7, 0.7,
0.5, 0.5, 0.7, 0.9, 0.9, 0.7, 0.5, 0.5, 0.7, 0.9, 0.9, 0.7, 0.5, 0.5,
0.7, 0.7, 0.7, 0.7, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5), 6, 6)
image(x, y, pred, col = gray(20:100/100), asp='s', axes=F, xlab=" ",
ylab="")
points(0.5, 0.5, col = 5) # the centre of the image

The centre of these circles needs to be overlapped with the centre of
the image. Any helps are greatly appreciated.
Regards,
Jin


-----Original Message-----
From: Mulholland, Tom [mailto:Tom.Mulholland at dpi.wa.gov.au] 
Sent: Monday, 22 November 2004 12:29 P
To: Li, Jin (CSE, Atherton)
Subject: RE: [R] How to correct this

I think you need to create a complete set of code that can be replicated
by anyone trying to help.
I ran the three grid.circle commands on my current plot and it did what
I expected it to do. It plotted three circles centred in the current
viewport. See the jpeg.

The last command using points makes me think that you need to understand
about units and the setting up of viewports. I have not played around
with this much but I think thr newsletter had an article which may be of
use (although it uses old code I think the differences are minor)

Ciao, Tom

-----Original Message-----
From: Jin.Li at csiro.au [mailto:Jin.Li at csiro.au]
Sent: Monday, 22 November 2004 10:07 AM
To: r-help at stat.math.ethz.ch
Subject: [R] How to correct this


Hi there,

 

I tried to add a few circles on an existing figure using the following
codes

 

grid.circle(x=0.5, y=0.5, r=0.1, draw=TRUE,  gp=gpar(col=5))

grid.circle(x=0.5, y=0.5, r=0.3, draw=TRUE, gp=gpar(col=5))

grid.circle(x=0.5, y=0.5, r=0.5, draw=TRUE, gp=gpar(col=5))

points(0.5, 0.5, col = 5) # centre of the circle

 

, but all circles moved away from the centre.  Could we do any
corrections to this? Thanks.

 

Regards,

 

Jin

==========================

Jin Li, PhD

Climate Impacts Modeller

CSIRO Sustainable Ecosystems

Atherton, QLD 4883

Australia

Ph: 61 7 4091 8802

Email: jin.li at csiro.au <mailto:jin.li at csiro.au> 

==========================

 


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Mon Nov 22 06:24:50 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 22 Nov 2004 05:24:50 +0000 (UTC)
Subject: [R] variable object naming
References: <1101100273.41a174f12a2e2@webmail.uvm.edu>
Message-ID: <loom.20041122T062421-246@post.gmane.org>

Benjamin M. Osborne <Benjamin.Osborne <at> uvm.edu> writes:

: 
: Is it possible to give a temporary object a name that varies with each run 
of a
: foreloop?  For example, I want to fill a matrix every time I run a loop, and 
I
: want a new matrix with each run, with an appropriate new name.
: i.e.:
: for(i in 1:5){...  matrix.i<-some values ...}
: 
: so that in the end I would have:
: matrix.1
: matrix.2
: matrix.3
: matrix.4
: matrix.5


See 7.1 of the FAQ.



From Tom.Mulholland at dpi.wa.gov.au  Mon Nov 22 06:44:03 2004
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Mon, 22 Nov 2004 13:44:03 +0800
Subject: [R] How to correct this
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3C900@afhex01.dpi.wa.gov.au>

Taking note of the first post, this is what I assume you wish. Note Paul's caveat in the help file

"If you resize the device, all bets are off!"

require(gridBase)
x<-seq(0,1,0.2)
y<-x
pred<-matrix(c(0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.7, 0.7, 0.7, 0.7,
0.5, 0.5, 0.7, 0.9, 0.9, 0.7, 0.5, 0.5, 0.7, 0.9, 0.9, 0.7, 0.5, 0.5,
0.7, 0.7, 0.7, 0.7, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5), 6, 6)
image(x, y, pred, col = gray(20:100/100), asp='s', axes=F, xlab=" ",
ylab="")
points(0.5, 0.5, col = 5) # the centre of the image

vps <- baseViewports()
pushViewport(vps$plot)
grid.circle(x=0.5, y=0.5, r=0.1, draw=TRUE,  gp=gpar(col=5))
grid.circle(x=0.5, y=0.5, r=0.3, draw=TRUE, gp=gpar(col=5))
grid.circle(x=0.5, y=0.5, r=0.5, draw=TRUE, gp=gpar(col=5))



-----Original Message-----
From: Jin.Li at csiro.au [mailto:Jin.Li at csiro.au]
Sent: Monday, 22 November 2004 1:21 PM
To: r-help at stat.math.ethz.ch
Subject: RE: [R] How to correct this


Hi there,

I would like to add a few circles to the following image: 
x<-seq(0,1,0.2)
y<-x
pred<-matrix(c(0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.7, 0.7, 0.7, 0.7,
0.5, 0.5, 0.7, 0.9, 0.9, 0.7, 0.5, 0.5, 0.7, 0.9, 0.9, 0.7, 0.5, 0.5,
0.7, 0.7, 0.7, 0.7, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5), 6, 6)
image(x, y, pred, col = gray(20:100/100), asp='s', axes=F, xlab=" ",
ylab="")
points(0.5, 0.5, col = 5) # the centre of the image

The centre of these circles needs to be overlapped with the centre of
the image. Any helps are greatly appreciated.
Regards,
Jin


-----Original Message-----
From: Mulholland, Tom [mailto:Tom.Mulholland at dpi.wa.gov.au] 
Sent: Monday, 22 November 2004 12:29 P
To: Li, Jin (CSE, Atherton)
Subject: RE: [R] How to correct this

I think you need to create a complete set of code that can be replicated
by anyone trying to help.
I ran the three grid.circle commands on my current plot and it did what
I expected it to do. It plotted three circles centred in the current
viewport. See the jpeg.

The last command using points makes me think that you need to understand
about units and the setting up of viewports. I have not played around
with this much but I think thr newsletter had an article which may be of
use (although it uses old code I think the differences are minor)

Ciao, Tom

-----Original Message-----
From: Jin.Li at csiro.au [mailto:Jin.Li at csiro.au]
Sent: Monday, 22 November 2004 10:07 AM
To: r-help at stat.math.ethz.ch
Subject: [R] How to correct this


Hi there,

 

I tried to add a few circles on an existing figure using the following
codes

 

grid.circle(x=0.5, y=0.5, r=0.1, draw=TRUE,  gp=gpar(col=5))

grid.circle(x=0.5, y=0.5, r=0.3, draw=TRUE, gp=gpar(col=5))

grid.circle(x=0.5, y=0.5, r=0.5, draw=TRUE, gp=gpar(col=5))

points(0.5, 0.5, col = 5) # centre of the circle

 

, but all circles moved away from the centre.  Could we do any
corrections to this? Thanks.

 

Regards,

 

Jin

==========================

Jin Li, PhD

Climate Impacts Modeller

CSIRO Sustainable Ecosystems

Atherton, QLD 4883

Australia

Ph: 61 7 4091 8802

Email: jin.li at csiro.au <mailto:jin.li at csiro.au> 

==========================

 


	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Mon Nov 22 09:34:18 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 22 Nov 2004 08:34:18 +0000 (GMT)
Subject: [R] Installing rgl in R2.0.1
In-Reply-To: <kki2q0t5c4ffbhjqqshuigl579ff6a38cc@4ax.com>
References: <200411220010.iAM0AxIa275680@atlas.otago.ac.nz>
	<kki2q0t5c4ffbhjqqshuigl579ff6a38cc@4ax.com>
Message-ID: <Pine.LNX.4.61.0411220817470.23095@gannet.stats>

On Sun, 21 Nov 2004, Duncan Murdoch wrote:

> On Mon, 22 Nov 2004 13:10:59 +1300 (NZDT), "Richard A. O'Keefe"
> <ok at cs.otago.ac.nz> wrote:
>
>> I'm running R2.0.1 under Solaris 2.9 on a SunBlade 100.
>> When I installed it, I set things up to use the Sun compilers
>> cc, CC, f95 with the options recommended in the installation and
>> administration guide.
>>
>> Until today, no worries.
>>
>> With all this discussion about R GUIs I thought I'd give R Commander a go.
>> The web page said to install a bunch of packages first, so I did
>>> install.packages(c("abind", "car", "effects", "lmtest", "multcomp",
>> + "mvtnorm", "relimp", "rgl", "sandwich", "strucchange", "zoo"),
>> + dependencies = TRUE)
>>
>> Again, all went well up to a certain point.  That point was rgl.
>>
>> * Installing *source* package 'rgl' ...
>> checking build system type... sparc-sun-solaris2.9
>> checking host system type... sparc-sun-solaris2.9
>> checking for gcc... gcc
>> checking for C compiler default output file name... a.out
>> checking whether the C compiler works... yes
>> checking whether we are cross compiling... no
>> checking for suffix of executables...
>> checking for suffix of object files... o
>> checking whether we are using the GNU C compiler... yes
>> checking whether gcc accepts -g... yes
>> checking for gcc option to accept ANSI C... none needed
>> checking how to run the C preprocessor... gcc -E
>> checking for X... libraries /usr/openwin/lib, headers /usr/openwin/include
>>
>> checking for libpng-config... yes
>> configure: creating ./config.status
>> config.status: creating src/Makevars
>> ** libs
>> CC -I/users/local/lib/R/include -I/usr/openwin/include -DHAVE_PNG_H -I/usr/local/include  -Wall -pedantic -fno-exceptions -fno-rtti -KPIC  -xO4 -xlibmil -dalign -c x11lib.cpp -o x11lib.o
>> CC: Warning: Option -Wall passed to ld, if ld is invoked, ignored otherwise
>> CC: Warning: Option -pedantic passed to ld, if ld is invoked, ignored otherwise
>> CC: Warning: Option -fno-exceptions passed to ld, if ld is invoked, ignored otherwise
>> ...
>> erwise
>> CC: Warning: Option -fno-rtti passed to ld, if ld is invoked, ignored otherwise
>> CC -G -L/usr/local/lib -o rgl.so x11lib.o x11gui.o types.o math.o fps.o pixmap.o gui.o api.o device.o devicemanager.o rglview.o scene.o glgui.o -L/usr/openwin/lib -L/users/local/lib -R/users/local/lib -lpng12 -lz -lm -lstdc++ -lX11 -lXext -lGL -lGLU -lpng12 -lz -lm
>> ld: fatal: library -lstdc++: not found
>> ld: fatal: File processing errors. No output written to rgl.so
>> *** Error code 1
>> make: Fatal error: Command failed for target `rgl.so'
>> ERROR: compilation failed for package 'rgl'
>>
>> Previous packages figured out from whatever information the R installation
>> squirrelled away that they should use f95 (not g77) and cc (not gcc) and
>> provided sensible options.  However, the rgl installation has decided to
>> do its own configuration, and has decided to use gcc.  That would probably
>> work, except that it is mixing up the Sun C++ compiler (CC) with the Gnu
>> command line options (-Wall -pedantic -fno-exceptions .....) AND the Sun
>> command line options (-xO4 -xlibmil -dalign).
>>
>> All my attempts to follow the
>> http://wsopuppenkiste.wiso.uni-goettingen.de/~dadler/rgl
>> link on the rgl catalogue card at CRAN have failed.
>>
>> - Did I do something wrong?
>> - What if anything can I do about it?
>
> I don't think you did anything wrong, but I don't know what you can do
> to fix it.  uni-goettingen.de hasn't been responding for a few days.

This is quite a common error in user-written configure scripts.  (To see 
how it should be done look at e.g. foreign.  A quick look suggests more 
scripts are wrong than right.)

The way to fix it for now is to remove rgl/configure and create 
rgl/src/Makevars by hand.

The configuration is not right even for gcc/g++.  It should not be adding
-lstdc++ for example, and things like

## C language

CPPFLAGS=""
CFLAGS="-Wall -pedantic"


## C++ language

LIBS="-lstdc++"
CXXFLAGS="${CFLAGS} -fno-exceptions -fno-rtti"

are inappropriate.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mase at is.titech.ac.jp  Mon Nov 22 09:39:09 2004
From: mase at is.titech.ac.jp (Shigeru Mase)
Date: Mon, 22 Nov 2004 17:39:09 +0900
Subject: [R] How to change the significant codes default?
In-Reply-To: <loom.20041121T193813-325@post.gmane.org>
References: <XFMail.041120191323.Ted.Harding@nessie.mcc.ac.uk>	<41A0C1AF.4060903@statistik.uni-dortmund.de>
	<loom.20041121T193813-325@post.gmane.org>
Message-ID: <41A1A5AD.7080402@is.titech.ac.jp>

Dear Uwe, Ted and Gabor. Thanks for your quick and kind informations.
The suggestion of Gabor combined with that of Ted works fine.
Of course, I myself would not like to change R's significant codes 
default, but I could get a deeper insight of R language mechanism.



From jarioksa at sun3.oulu.fi  Mon Nov 22 09:42:27 2004
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: 22 Nov 2004 10:42:27 +0200
Subject: [R] Running R from CD?
In-Reply-To: <20041122004120.67125.qmail@web50306.mail.yahoo.com>
References: <20041122004120.67125.qmail@web50306.mail.yahoo.com>
Message-ID: <1101112947.3798.6.camel@biol102145.oulu.fi>

On Mon, 2004-11-22 at 02:41, bogdan romocea wrote:
> Better install and run R from a USB flash drive. This will save you
> the trouble of re-writing the CD as you upgrade and install new
> packages. Also, you can simply copy the R installation on your work
> computer (no install rights needed); R will run.
> 
I think there is a niche (= a hole in the wall) for a live CD: it is
cheaper to distribute 20 copies of CD's to your audience than 20 USB
memory sticks. Instructions would be welcome.
> 
> From: Hans van Walen <hans_at_vanwalen.com>

> At work I have no permission to install R. So, would anyone know
> whether it is possible to create a CD with a running R-installation
> for a windows(XP) pc? And of course, how to?
> 
Check the file Getting-Started-with-the-Rcmdr.pdf in John Fox's Rcmdr
package. You should be able to reach this package by launching
help.start(), and then browsing its directory in the help browser
window. Go to chapter "7. Some Suggestions for Instructors" which tells
you how to make a live CD of R in Windows. I haven't tried this, since I
don't have Windows, but I sure will when I got to be an "instructor" in
a Windows class.
 
cheers, jari oksanen 
-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From petr.pikal at precheza.cz  Mon Nov 22 10:08:21 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 22 Nov 2004 10:08:21 +0100
Subject: [R] Restore sources
In-Reply-To: <20041120184627.JCPD26440.priv-edtnes57.telusplanet.net@vaio>
Message-ID: <41A1BA95.25926.C8F763@localhost>

Hi

Well I did not see any reply to your question (which does not 
surprise me as you broke almost all rules from posting guide).

The answer partly depands on OS you use but your commands are 
probably stored in .Rhistory. You can view it by some text editor 
(Notepad?). I recommend you to spend some time reading FAQs 
and other documentation.

Cheers
Petr



On 20 Nov 2004 at 10:46, A. Izad wrote:

> How can I see my codes (syntax) after restoring a workspace?
> 
> Also " Summery"
> 
> 
> 
> tnx
> 
> Mike
> 
> 
> 
> 
> 
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From hodgess at gator.uhd.edu  Mon Nov 22 10:48:57 2004
From: hodgess at gator.uhd.edu (Erin Hodgess)
Date: Mon, 22 Nov 2004 03:48:57 -0600
Subject: [R] simulation of Gumbel copulas
Message-ID: <200411220948.iAM9mvX01957@gator.dt.uh.edu>

Dear R:

Is there a function or a reference to simulate Gumbel copulas, please?

Thanks in advance!

Sincerely,
Erin Hodgess
mailto: hodgess at gator.uhd.edu
R version 2.0.1 windows



From Ted.Harding at nessie.mcc.ac.uk  Mon Nov 22 11:13:12 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 22 Nov 2004 10:13:12 -0000 (GMT)
Subject: [R] How to change the significant codes default?
In-Reply-To: <41A1A5AD.7080402@is.titech.ac.jp>
Message-ID: <XFMail.041122093047.Ted.Harding@nessie.mcc.ac.uk>

On 22-Nov-04 Shigeru Mase wrote:
> Dear Uwe, Ted and Gabor. Thanks for your quick and kind informations.
> The suggestion of Gabor combined with that of Ted works fine.
> Of course, I myself would not like to change R's significant codes 
> default, but I could get a deeper insight of R language mechanism.

So could I! Thanks, Gabor, for bringing 'body(...) <- ...' to
the surface.

Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 22-Nov-04                                       Time: 09:30:47
------------------------------ XFMail ------------------------------



From STELAD01 at glostruphosp.kbhamt.dk  Mon Nov 22 11:26:05 2004
From: STELAD01 at glostruphosp.kbhamt.dk (Ladelund, Steen)
Date: Mon, 22 Nov 2004 11:26:05 +0100
Subject: [R] Danish characters i R2.0.1 vs R1.9.1 under winXP
Message-ID: <F9E47473E3BCD1118C0500204808C3900995A9EC@GLO_003>

Hi all.

After upgrading to R2.0.1 i get

> "??"
[1] "??"
> "??"
[1] "\370" 
> "??"
[1] "??"

Whereas under R1.9.1 i get

> "??"
[1] "??"
> "??"
[1] "??"
> "??"
[1] "??"

Any hints apreciated.

Steen

Steen Ladelund, statistician
+4543233275 stelad01.FUNNYAglostruphospDOTkbhamt.dk
Research Center for Prevention and Health
Glostrup University Hospital, Denmark
www.fcfs.kbhamt.dk



From Bernhard.Pfaff at drkw.com  Mon Nov 22 11:33:44 2004
From: Bernhard.Pfaff at drkw.com (Pfaff, Bernhard)
Date: Mon, 22 Nov 2004 11:33:44 +0100
Subject: [R] Rcmdr -> doItAndPrint -> summary method of S4-class object not
 in Rcmdr window but R Console
Message-ID: <29E0BC0C716A584582941615CF9FFB0902585D3D@ibfftce107.de.ad.drkw.net>

Dear list member,

John Fox proposed to me posting this problem to R-Help:

I have written a function suited for Rcmdr and included this in a R-file
located in Rcmdr/etc as well as an augmented `Rcmdr-menus.txt'.

Now, I am faced with the following problem:
method show() of a S4-class object works flawlessly, that is `doItAndPrint'
works flawlessly and its output is returned correctly into the Rcmdr-Window.
However, by using method summary() for the same S4-class object fails, in
the sense that the output is *not* printed in the Rcmdr window, but into the
R Console. The summary()-method contains cat() and slots of the S4-class
objects only.

My question is: How can it be achieved that method summary() of S4 objects
is printed in the RCmdr window and why, in the first instance, does it fail
to do so by using `doItAndPrint'.

Any help or pointer is much appreciated,

Best Regards
Bernhard

platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    0.1            
year     2004           
month    11             
day      15             
language R              
  



--------------------------------------------------------------------------------
The information contained herein is confidential and is inte...{{dropped}}



From ripley at stats.ox.ac.uk  Mon Nov 22 11:48:21 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 22 Nov 2004 10:48:21 +0000 (GMT)
Subject: [R] Danish characters i R2.0.1 vs R1.9.1 under winXP
In-Reply-To: <F9E47473E3BCD1118C0500204808C3900995A9EC@GLO_003>
References: <F9E47473E3BCD1118C0500204808C3900995A9EC@GLO_003>
Message-ID: <Pine.LNX.4.61.0411221040280.25248@gannet.stats>

The problem is in your OS, which thinks the character \370 is not 
printable in your locale.

Apparently this is a WinXP problem, for it also thinks \366 is not 
printable in German (and other versions of Windows thinks it is).
(Uwe Ligges pointed that out a few hours ago.)

R 1.9.1 did not check, and so made a mess of trying to print non-printable 
characters, in particular nuls and control characters.

There is no simple workaround, as you do want octal representation for 
non-printable characters (e.g. embedded nuls).  What I have proposed is 
that we override Windows' view for upper (>= 0x80) characters.

It does not seem to be common: the only reports before today were for
Chinese, which is not expected to work.

On Mon, 22 Nov 2004, Ladelund, Steen wrote:

> Hi all.
>
> After upgrading to R2.0.1 i get
>
>> ""
> [1] ""
>> ""
> [1] "\370"
>> ""
> [1] ""
>
> Whereas under R1.9.1 i get
>
>> ""
> [1] ""
>> ""
> [1] ""
>> ""
> [1] ""
>
> Any hints apreciated.
>
> Steen
>
> Steen Ladelund, statistician
> +4543233275 stelad01.FUNNYAglostruphospDOTkbhamt.dk
> Research Center for Prevention and Health
> Glostrup University Hospital, Denmark
> www.fcfs.kbhamt.dk
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From ripley at stats.ox.ac.uk  Mon Nov 22 11:56:48 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 22 Nov 2004 10:56:48 +0000 (GMT)
Subject: [R] Rcmdr -> doItAndPrint -> summary method of S4-class object
	not in Rcmdr window but R Console
In-Reply-To: <29E0BC0C716A584582941615CF9FFB0902585D3D@ibfftce107.de.ad.drkw.net>
References: <29E0BC0C716A584582941615CF9FFB0902585D3D@ibfftce107.de.ad.drkw.net>
Message-ID: <Pine.LNX.4.61.0411221050370.25248@gannet.stats>

The FAQ Q8.1 does recommend that summary() methods do not print themselves 
but returned a classed object to be printed.

What doItAndPrint does is to capture the result of an explicit print, so 
it will only work for summary methods that follow that recommendation.

I do think doItAndPrint should be capturing all the output from a function 
via sink(), and not just the print of the final result.

On Mon, 22 Nov 2004, Pfaff, Bernhard wrote:

> Dear list member,
>
> John Fox proposed to me posting this problem to R-Help:
>
> I have written a function suited for Rcmdr and included this in a R-file
> located in Rcmdr/etc as well as an augmented `Rcmdr-menus.txt'.
>
> Now, I am faced with the following problem:
> method show() of a S4-class object works flawlessly, that is `doItAndPrint'
> works flawlessly and its output is returned correctly into the Rcmdr-Window.
> However, by using method summary() for the same S4-class object fails, in
> the sense that the output is *not* printed in the Rcmdr window, but into the
> R Console. The summary()-method contains cat() and slots of the S4-class
> objects only.
>
> My question is: How can it be achieved that method summary() of S4 objects
> is printed in the RCmdr window and why, in the first instance, does it fail
> to do so by using `doItAndPrint'.
>
> Any help or pointer is much appreciated,
>
> Best Regards
> Bernhard
>
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    2
> minor    0.1
> year     2004
> month    11
> day      15
> language R
>
>
>
>
> --------------------------------------------------------------------------------
> The information contained herein is confidential and is inte...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From STELAD01 at glostruphosp.kbhamt.dk  Mon Nov 22 11:57:40 2004
From: STELAD01 at glostruphosp.kbhamt.dk (Ladelund, Steen)
Date: Mon, 22 Nov 2004 11:57:40 +0100
Subject: [R] Left justification af dimnames in tables.
Message-ID: <F9E47473E3BCD1118C0500204808C3900995A9ED@GLO_003>

Hi helpeRs.

When I do two-ways tables dimnames in columns are almost left-adjusted:
  <- factor(rbinom(30,1,.3),labels=c("first","second"))
>  b <- a
>  table(a,b)
        b
a        first second
  first  21     0    
  second  0     9 

If I do a oneway table however, dimnames are right-adjusted:

table(a)
a
 first second 
    21      9 

Actually I must admit I dont know if its the dimnames or/and the cell counts
that are adjusted ;-)

Is there a way that I can get left-adjusted dimnames i oneway tables.

Thans in advance

Steen


Steen Ladelund, statistician
+4543233275 stelad01FUNNYAglostruphospDOTkbhamt.dk
Research Center for Prevention and Health
Glostrup University Hospital, Denmark
www.fcfs.kbhamt.dk



From giovanna.jonalasinio at uniroma1.it  Mon Nov 22 12:43:04 2004
From: giovanna.jonalasinio at uniroma1.it (giovanna jona lasinio)
Date: Mon, 22 Nov 2004 12:43:04 +0100
Subject: [R] RWinEdt,
	other text editors and R2.01 a problem in pasting commands
Message-ID: <000001c4d088$6db1b930$65036497@sta.uniroma1.it>

Dear All,
In the last few days I started using the last version of R as I
encountered a problem with R1.9 and the use of RWinEdt, however the
problem shows with R2.01 as well. More precisely

1. I install R2.01 after removing old verions of R
2. I Install RWinEdt package versione RWinEdt_1.6-2 and
SWinRegistry_0.3-2 and everything seems fine
3. I recall the RWinEdt library (library(RWinEdt)) and WinEdt appears
4. now after loading a workspace I run the following command using the
paste icon in WinEdt: 
plot(x,xaxt="n",xlab="time", ylab=expression(alpha),main=nome,type="l")

And I get the following error message:
<<Error in stripchart(x, ...) : unused argument(s) (xaxt ...)>>
>From now on the plot command stops working, it doesn't recognize any
option not even type="l" or type="b" or anything.

If I type the intrusction directly on the command window without any
previous paste operation from WinEdt the command works properly.

Furthermore if I try to paste commands by simply coping it from a text
file and pasting it on the R command window I have the same problem. I
even tried:
source("prova.txt") containing the same command line and I got the same
error:
Error in stripchart(x, ...) : unused argument(s) (xaxt ...)

I'm not very good at using Windows XP, can anyone help?

Thanks
Giovanna Jona Lasinio



From s938611 at mail.yzu.edu.tw  Mon Nov 22 13:38:08 2004
From: s938611 at mail.yzu.edu.tw (pcscan)
Date: Mon, 22 Nov 2004 20:38:08 +0800
Subject: [R] Questions of Significance Analysis of Microarrays(SAM){siggenes}
Message-ID: <000b01c4d090$1f3aed00$e9918a8c@linuxkeyzthan9>

Dear All:
Significance Analysis of Microarrays(SAM)

As we know sam do multiple t.test as following
## Default S3 method:
t.test(x, y = NULL, alternative = c("two.sided", "less", "greater"),mu = 0,
paired = FALSE, var.equal = FALSE,conf.level = 0.95, ...)

 var.equal: a logical variable indicating whether to treat the two variances
as being equal. If 'TRUE' then the pooled variance is used to estimate the
variance otherwise the Welch (or Satterthwaite) approximation to the degrees
of freedom is  used.

We are curious why sam in package siggenes do not have var.equal option ?
Are there some reason ?

sam(data,cl,B=100,balanced=FALSE,mat.samp=NULL,delta=(1:10)/5,med.fdr=TRUE,s
0=NA,alpha.s0=seq(0,1,.05),include.s0=TRUE,p0=NA,lambda.p0=1,vec.lambda.p0=(
0:95)/100,
na.rm=FALSE,graphic.fdr=TRUE,thres.fdr=seq(0.5,2,0.5),ngenes=NA,iteration=3,
initial.delta=c(.1,seq(.2,2,.2),4),rand=NA)

Any help is greatly appreciated.

Sincerely. Liu Yu Ting



From ripley at stats.ox.ac.uk  Mon Nov 22 13:58:57 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 22 Nov 2004 12:58:57 +0000 (GMT)
Subject: [R] Questions of Significance Analysis of
	Microarrays(SAM){siggenes}
In-Reply-To: <000b01c4d090$1f3aed00$e9918a8c@linuxkeyzthan9>
References: <000b01c4d090$1f3aed00$e9918a8c@linuxkeyzthan9>
Message-ID: <Pine.LNX.4.61.0411221256570.5053@gannet.stats>

Please ask questions about Bioconductor packages of the authors or on the 
Bioconductor list.

In this case, only the author can tell you why he did not do something.

On Mon, 22 Nov 2004, pcscan wrote:

> Dear All:
> Significance Analysis of Microarrays(SAM)
>
> As we know sam do multiple t.test as following
> ## Default S3 method:
> t.test(x, y = NULL, alternative = c("two.sided", "less", "greater"),mu = 0,
> paired = FALSE, var.equal = FALSE,conf.level = 0.95, ...)
>
> var.equal: a logical variable indicating whether to treat the two variances
> as being equal. If 'TRUE' then the pooled variance is used to estimate the
> variance otherwise the Welch (or Satterthwaite) approximation to the degrees
> of freedom is  used.
>
> We are curious why sam in package siggenes do not have var.equal option ?
> Are there some reason ?
>
> sam(data,cl,B=100,balanced=FALSE,mat.samp=NULL,delta=(1:10)/5,med.fdr=TRUE,s
> 0=NA,alpha.s0=seq(0,1,.05),include.s0=TRUE,p0=NA,lambda.p0=1,vec.lambda.p0=(
> 0:95)/100,
> na.rm=FALSE,graphic.fdr=TRUE,thres.fdr=seq(0.5,2,0.5),ngenes=NA,iteration=3,
> initial.delta=c(.1,seq(.2,2,.2),4),rand=NA)
>
> Any help is greatly appreciated.
>
> Sincerely. Liu Yu Ting
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jfox at mcmaster.ca  Mon Nov 22 14:04:21 2004
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 22 Nov 2004 08:04:21 -0500
Subject: [R] Running R from CD?
In-Reply-To: <1101112947.3798.6.camel@biol102145.oulu.fi>
Message-ID: <20041122130419.EWAU2034.tomts20-srv.bellnexxia.net@JohnDesktop8300>

Dear Jari,

When I distribute an R Windows CD/ROM to students, I generally include both
the installer and an installed version. This allows students who don't want
to or can't install the software to use it. They pay a big penalty in speed,
however. A flash drive would provide better performance if this is a viable
option.

It's not really necessary to do anything as elaborate as in my Rcmdr
instructions if the CD is for one's own use: Just install to the CD (copy
over installed versions of whatever contributed packages you want to use)
and run rgui.exe from it.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Jari Oksanen
> Sent: Monday, November 22, 2004 3:42 AM
> To: bogdan romocea
> Cc: hans at vanwalen.com; R-News
> Subject: Re: [R] Running R from CD?
> 
> On Mon, 2004-11-22 at 02:41, bogdan romocea wrote:
> > Better install and run R from a USB flash drive. This will save you 
> > the trouble of re-writing the CD as you upgrade and install new 
> > packages. Also, you can simply copy the R installation on your work 
> > computer (no install rights needed); R will run.
> > 
> I think there is a niche (= a hole in the wall) for a live 
> CD: it is cheaper to distribute 20 copies of CD's to your 
> audience than 20 USB memory sticks. Instructions would be welcome.
> > 
> > From: Hans van Walen <hans_at_vanwalen.com>
> 
> > At work I have no permission to install R. So, would anyone know 
> > whether it is possible to create a CD with a running R-installation 
> > for a windows(XP) pc? And of course, how to?
> > 
> Check the file Getting-Started-with-the-Rcmdr.pdf in John 
> Fox's Rcmdr package. You should be able to reach this package 
> by launching help.start(), and then browsing its directory in 
> the help browser window. Go to chapter "7. Some Suggestions 
> for Instructors" which tells you how to make a live CD of R 
> in Windows. I haven't tried this, since I don't have Windows, 
> but I sure will when I got to be an "instructor" in a Windows class.
>  
> cheers, jari oksanen
> --
> Jari Oksanen <jarioksa at sun3.oulu.fi>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From jfox at mcmaster.ca  Mon Nov 22 14:08:07 2004
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 22 Nov 2004 08:08:07 -0500
Subject: [R] Rcmdr -> doItAndPrint -> summary method of S4-class objectnot
	in Rcmdr window but R Console
In-Reply-To: <Pine.LNX.4.61.0411221050370.25248@gannet.stats>
Message-ID: <20041122130805.DKUO1919.tomts22-srv.bellnexxia.net@JohnDesktop8300>

Dear Brian,

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Prof 
> Brian Ripley
> Sent: Monday, November 22, 2004 5:57 AM
> To: Pfaff, Bernhard
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Rcmdr -> doItAndPrint -> summary method of 
> S4-class objectnot in Rcmdr window but R Console
> 
> The FAQ Q8.1 does recommend that summary() methods do not 
> print themselves but returned a classed object to be printed.
> 
> What doItAndPrint does is to capture the result of an 
> explicit print, so it will only work for summary methods that 
> follow that recommendation.
> 

So that's it -- I should have seen that.

> I do think doItAndPrint should be capturing all the output 
> from a function via sink(), and not just the print of the 
> final result.
> 

That would be better, wouldn't it. I'll try to figure out how to make that
work.

Thanks,
 John

> On Mon, 22 Nov 2004, Pfaff, Bernhard wrote:
> 
> > Dear list member,
> >
> > John Fox proposed to me posting this problem to R-Help:
> >
> > I have written a function suited for Rcmdr and included this in a 
> > R-file located in Rcmdr/etc as well as an augmented 
> `Rcmdr-menus.txt'.
> >
> > Now, I am faced with the following problem:
> > method show() of a S4-class object works flawlessly, that 
> is `doItAndPrint'
> > works flawlessly and its output is returned correctly into 
> the Rcmdr-Window.
> > However, by using method summary() for the same S4-class 
> object fails, 
> > in the sense that the output is *not* printed in the Rcmdr 
> window, but 
> > into the R Console. The summary()-method contains cat() and 
> slots of 
> > the S4-class objects only.
> >
> > My question is: How can it be achieved that method summary() of S4 
> > objects is printed in the RCmdr window and why, in the 
> first instance, 
> > does it fail to do so by using `doItAndPrint'.
> >
> > Any help or pointer is much appreciated,
> >
> > Best Regards
> > Bernhard
> >
> > platform i386-pc-mingw32
> > arch     i386
> > os       mingw32
> > system   i386, mingw32
> > status
> > major    2
> > minor    0.1
> > year     2004
> > month    11
> > day      15
> > language R
> >
> >
> >
> >
> > 
> ----------------------------------------------------------------------
> > ---------- The information contained herein is confidential and is 
> > inte...{{dropped}}
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> >
> >
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From partha_bagchi at hgsi.com  Mon Nov 22 14:12:18 2004
From: partha_bagchi at hgsi.com (partha_bagchi@hgsi.com)
Date: Mon, 22 Nov 2004 08:12:18 -0500
Subject: [R] Re: 3d Map with bars
Message-ID: <OF4FB24550.32853E5D-ON85256F54.004854BD-85256F54.0048895D@hgsi.com>

Ray,

Thanks very much. That did it. 

Partha





Ray Brownrigg <ray at mcs.vuw.ac.nz>
11/21/2004 06:38 PM

 
        To:     ligges at statistik.uni-dortmund.de, partha_bagchi at hgsi.com
        cc:     r-help at stat.math.ethz.ch
        Subject:        Re: [R] Re: 3d Map with bars


> From: partha_bagchi at hgsi.com
> Date: Fri, 19 Nov 2004 16:53:12 -0500
>
> Thanks for reply. I need to first draw the map of USA a perspective 
plot.
> I guess thats where my problem was.
>

Try something like this:

library(maps)
states <- map("state", plot=F)
x1 <- rep(0, 3)
x2 <- rep(0, 3)
maxz <- 1
z <- matrix(c(0, 0, 0, 0, 0, 0, maxz, maxz, maxz), 3, 3)
x1[-2] <- states$range[1:2]
x2[-2] <- states$range[3:4]
x1[2] <- x1[3] - 1e-6
x2[2] <- x2[3] - 1e-6
pmat <- persp(x1, x2, z, xlab = "", ylab = "", zlab="", axes = F,
theta=0, phi=20, d=10)
lines(trans3d(states$x, states$y, 0, pmat))
latitude <- -90
longitude <- 37
myz <- 0.6
lines(trans3d(rep(latitude, 2), rep(longitude, 2), c(0, myz), pmat), 
col=2)

where trans3d is as defined in the examples for persp().

Hope this helps,
Ray Brownrigg

----
> Uwe Ligges <ligges at statistik.uni-dortmund.de>
> 11/19/2004 04:33 PM
>
>         To:     partha_bagchi at hgsi.com
>         cc:     r-help at stat.math.ethz.ch
>         Subject:        Re: 3d Map with bars
>
>
> partha_bagchi at hgsi.com wrote:
>
> > Apologies in advance for the question. I am trying to draw a map of 
the
> US
> > as a surface plot so that I would be able to drop bars on the 
different
> > states (something like Uwe Ligges' scatterplot3d example 4). I am not
> sure
> > where to start looking for such a beast. If anyone has any pointers,
> > ideas, I will be grateful.
> >
> > TIA,
> > Partha
>
> How to "drop bars" with persp() has been described on R-help yesterday
> or today, please check the mailing list's archives.
>



From vito_ricci at yahoo.com  Mon Nov 22 14:16:18 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Mon, 22 Nov 2004 14:16:18 +0100 (CET)
Subject: [R] R: simulation of Gumbel copulas
Message-ID: <20041122131618.57276.qmail@web41209.mail.yahoo.com>

Hi,

I found this document, but it concerns S+. If it could
interest you'll see:

http://faculty.washington.edu/ezivot/book/QuanCopula.pdf

Cordially
Vito




You wrote:

Dear R:

Is there a function or a reference to simulate Gumbel
copulas, please?

Thanks in advance!

Sincerely,
Erin Hodgess
mailto: hodgess at gator.uhd.edu
R version 2.0.1 windows

=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/



From vito_ricci at yahoo.com  Mon Nov 22 14:23:27 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Mon, 22 Nov 2004 14:23:27 +0100 (CET)
Subject: [R] R: simulation of Gumbel copulas
Message-ID: <20041122132327.73094.qmail@web41205.mail.yahoo.com>

I'm sorry for another posting, but I found also this
message in S news:

http://www.biostat.wustl.edu/archives/html/s-news/2000-01/msg00058.html

Bye 
Vito

you wrote:

Dear R:

Is there a function or a reference to simulate Gumbel
copulas, please?

Thanks in advance!

=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/



From ripley at stats.ox.ac.uk  Mon Nov 22 14:36:08 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 22 Nov 2004 13:36:08 +0000 (GMT)
Subject: [R] Running R from CD?
In-Reply-To: <1101112947.3798.6.camel@biol102145.oulu.fi>
References: <20041122004120.67125.qmail@web50306.mail.yahoo.com>
	<1101112947.3798.6.camel@biol102145.oulu.fi>
Message-ID: <Pine.LNX.4.61.0411220922490.24431@gannet.stats>

Let's be a little careful here.  An R for Windows installation is 
relocatable, so you can just install it into a temporary directory and 
burn a copy of that onto CD.  (That may not be true after installing 
packages into a non-default library.)  It is not true of a Unix or MacOS X 
installation, as far as I am aware, for they have absolute paths coded 
into the files.

However, an R session does need to be able to write to a temporary 
directory, and also needs a `home' directory and at a last resort the 
latter defaults to the current directory.  So you do need to be running on 
a machine on which you have a writable area.

A policy that says you cannot install a program, but you can run from a CD 
and you can let such a program write to your area seems full of holes to 
me.  (Ours does not allow low-privilege users to run programs from a CD.)
Also, many organizations ban the use of USB drives for security reasons.

BTW, I believe running R 2.0.x from a CD will be a lot slower than 1.9.1
because of lazy loading and frequent file accesses: that's a theoretical 
issue we intend to address for 2.1.0, but not one anyone has yet commented 
that it is a problem.


On Mon, 22 Nov 2004, Jari Oksanen wrote:

> On Mon, 2004-11-22 at 02:41, bogdan romocea wrote:
>> Better install and run R from a USB flash drive. This will save you
>> the trouble of re-writing the CD as you upgrade and install new
>> packages. Also, you can simply copy the R installation on your work
>> computer (no install rights needed); R will run.
>>
> I think there is a niche (= a hole in the wall) for a live CD: it is
> cheaper to distribute 20 copies of CD's to your audience than 20 USB
> memory sticks. Instructions would be welcome.
>>
>> From: Hans van Walen <hans_at_vanwalen.com>
>
>> At work I have no permission to install R. So, would anyone know
>> whether it is possible to create a CD with a running R-installation
>> for a windows(XP) pc? And of course, how to?
>>
> Check the file Getting-Started-with-the-Rcmdr.pdf in John Fox's Rcmdr
> package. You should be able to reach this package by launching
> help.start(), and then browsing its directory in the help browser
> window. Go to chapter "7. Some Suggestions for Instructors" which tells
> you how to make a live CD of R in Windows. I haven't tried this, since I
> don't have Windows, but I sure will when I got to be an "instructor" in
> a Windows class.
>
> cheers, jari oksanen
> -- 
> Jari Oksanen <jarioksa at sun3.oulu.fi>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.campbell at econ.bbk.ac.uk  Mon Nov 22 14:37:16 2004
From: p.campbell at econ.bbk.ac.uk (Campbell)
Date: Mon, 22 Nov 2004 13:37:16 +0000
Subject: [R] GCC
Message-ID: <s1a1eba7.039@markets.econ.bbk.ac.uk>

> version        
         _                   
platform sparc-sun-solaris2.9
arch     sparc               
os       solaris2.9          
system   sparc, solaris2.9   
status                       
major    1                   
minor    7.1                 
year     2003                
month    06                  
day      16                  
language R                   


I would like to upgrade to 2.0.1.  What is the most recent GCC compiler
for which there has been a succesful build?

Phineas



From p.dalgaard at biostat.ku.dk  Mon Nov 22 14:38:59 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 22 Nov 2004 14:38:59 +0100
Subject: [R] Left justification af dimnames in tables.
In-Reply-To: <F9E47473E3BCD1118C0500204808C3900995A9ED@GLO_003>
References: <F9E47473E3BCD1118C0500204808C3900995A9ED@GLO_003>
Message-ID: <x2vfbygg0c.fsf@biostat.ku.dk>

"Ladelund, Steen" <STELAD01 at glostruphosp.kbhamt.dk> writes:

> Hi helpeRs.
> 
> When I do two-ways tables dimnames in columns are almost left-adjusted:
>   <- factor(rbinom(30,1,.3),labels=c("first","second"))
> >  b <- a
> >  table(a,b)
>         b
> a        first second
>   first  21     0    
>   second  0     9 
> 
> If I do a oneway table however, dimnames are right-adjusted:
> 
> table(a)
> a
>  first second 
>     21      9 
> 
> Actually I must admit I dont know if its the dimnames or/and the cell counts
> that are adjusted ;-)
> 
> Is there a way that I can get left-adjusted dimnames i oneway tables.
> 
> Thans in advance

This was actually an inadvertent change in R 1.7.0. It is fixed in the
development version of R but it was decided to keep the fix out of the
patch versions because it would change a lot of printed output (and
some people run automatic checks for that). For a twoway table,
there's a simple workaround:

> print(table(a,b),right=T)
        b
a        first second
  first     21      0
  second     0      9

Unfortunately, it doesn't seem to work for multiway tables....

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From murdoch at stats.uwo.ca  Mon Nov 22 14:54:40 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 22 Nov 2004 08:54:40 -0500
Subject: [R] The hidden costs of GPL software?
In-Reply-To: <00d801c4ce83$083dc500$6401a8c0@C56909A>
References: <00d801c4ce83$083dc500$6401a8c0@C56909A>
Message-ID: <jtq3q0tu5fjd6lbf12f8rj5r3n9m93paqh@4ax.com>

On Fri, 19 Nov 2004 13:59:23 -0800, "Cliff Lunneborg"
<cliff at ms.washington.edu> quoted John Fox:

>Why not, as previously has been proposed, replace the
>current static (and, in my view, not very useful) set of keywords in R
>documentation with the requirement that package authors supply their own
>keywords for each documented object? I believe that this is the intent
>of
>the concept entries in Rd files, but their use certainly is not required
>or
>even actively encouraged. (They're just mentioned in passing in the
>Writing
>R Extensions manual.

That would not be easy and won't happen quickly.  There are some
problems:

 - The base packages mostly don't use  \concept. (E.g. base has 365
man pages, only about 15 of them use it).  Adding it to each file is a
fairly time-consuming task.

- Before we started, we'd need to agree as to what they are for.
Right now, I think they are mainly used when the name of a concept
doesn't match the name of the function that implements it, e.g.
"modulo", "remainder", "promise", "argmin", "assertion".  The need for
this usage is pretty rare.  If they were used for everything, what
would they contain?

 - Keywording in a useful way is hard.  There are spelling issues
(e.g. optimise versus optimize); our fuzzy matching helps with those.
But there are also multiple names for the same thing, and multiple
meanings for the same name.

Duncan Murdoch



From ripley at stats.ox.ac.uk  Mon Nov 22 15:18:33 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 22 Nov 2004 14:18:33 +0000 (GMT)
Subject: [R] GCC
In-Reply-To: <s1a1eba7.039@markets.econ.bbk.ac.uk>
References: <s1a1eba7.039@markets.econ.bbk.ac.uk>
Message-ID: <Pine.LNX.4.61.0411221417310.6021@gannet.stats>

On Mon, 22 Nov 2004, Campbell wrote:

>> version
>         _
> platform sparc-sun-solaris2.9
> arch     sparc
> os       solaris2.9
> system   sparc, solaris2.9
> status
> major    1
> minor    7.1
> year     2003
> month    06
> day      16
> language R
>
>
> I would like to upgrade to 2.0.1.  What is the most recent GCC compiler
> for which there has been a succesful build?

3.4.3, the current version of gcc (and indeed all versions since the buggy 
ones mentioned in R-admin.html).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tang_chalmers at hotmail.com  Mon Nov 22 15:43:40 2004
From: tang_chalmers at hotmail.com (jing tang)
Date: Mon, 22 Nov 2004 15:43:40 +0100
Subject: [R] How to insert one element into a vector?
Message-ID: <BAY21-F40441ECA345ACA58328DCE83C60@phx.gbl>

suppose I want to insert 5 into the vector (1,2,3,4,6) between 4 and 6.
thx!



From michael.watson at bbsrc.ac.uk  Mon Nov 22 15:57:18 2004
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Mon, 22 Nov 2004 14:57:18 -0000
Subject: [R] How to insert one element into a vector?
Message-ID: <8975119BCD0AC5419D61A9CF1A923E95E898C7@iahce2knas1.iah.bbsrc.reserved>

There must be a million ways of doing this!
If you want to keep the order:

v <- c(1,2,3,4,6)
sort(c(v,5))

Mick

-----Original Message-----
From: jing tang [mailto:tang_chalmers at hotmail.com] 
Sent: 22 November 2004 14:44
To: r-help at stat.math.ethz.ch
Subject: [R] How to insert one element into a vector?


suppose I want to insert 5 into the vector (1,2,3,4,6) between 4 and 6.
thx!

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From B.Rowlingson at lancaster.ac.uk  Mon Nov 22 16:19:40 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 22 Nov 2004 15:19:40 +0000
Subject: [R] How to insert one element into a vector?
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E95E898C7@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E95E898C7@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <41A2038C.5080002@lancaster.ac.uk>

michael watson (IAH-C) wrote:
> There must be a million ways of doing this!

  Actually I'd say there were no ways of doing this, since I dont think 
you can actually insert into a vector - you have to create a new vector 
that produces the illusion of insertion!

  Here's a Q+D function that fails if you try and insert at the start, 
or at the end. Its very D.

insert <- function(v,e,pos){
   return(c(v[1:(pos-1)],e,v[(pos):length(v)]))
}

  > v=c(1,2,3,4,6)

  > insert(v,5,5)
  [1] 1 2 3 4 5 6

Yay.

  > insert(v,5,1)
  [1] 1 5 1 2 3 4 6

Oops.

  Cant be bothered to fix it, the principle is there.

Baz



From rpeng at jhsph.edu  Mon Nov 22 16:29:24 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Mon, 22 Nov 2004 10:29:24 -0500
Subject: [R] How to insert one element into a vector?
In-Reply-To: <BAY21-F40441ECA345ACA58328DCE83C60@phx.gbl>
References: <BAY21-F40441ECA345ACA58328DCE83C60@phx.gbl>
Message-ID: <41A205D4.1010600@jhsph.edu>

?append --- look at the `after' argument.

-roger

jing tang wrote:
> suppose I want to insert 5 into the vector (1,2,3,4,6) between 4 and 6.
> thx!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From deepayan at stat.wisc.edu  Mon Nov 22 16:29:06 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Mon, 22 Nov 2004 09:29:06 -0600
Subject: [R] How to insert one element into a vector?
In-Reply-To: <41A2038C.5080002@lancaster.ac.uk>
References: <8975119BCD0AC5419D61A9CF1A923E95E898C7@iahce2knas1.iah.bbsrc.reserved>
	<41A2038C.5080002@lancaster.ac.uk>
Message-ID: <200411220929.06611.deepayan@stat.wisc.edu>

On Monday 22 November 2004 09:19, Barry Rowlingson wrote:
> michael watson (IAH-C) wrote:
> > There must be a million ways of doing this!
>
>   Actually I'd say there were no ways of doing this, since I dont
> think you can actually insert into a vector - you have to create a
> new vector that produces the illusion of insertion!
>
>   Here's a Q+D function that fails if you try and insert at the
> start, or at the end. Its very D.
>
> insert <- function(v,e,pos){
>    return(c(v[1:(pos-1)],e,v[(pos):length(v)]))
> }

Pretty much what 'append' does.

Deepayan



From p.dalgaard at biostat.ku.dk  Mon Nov 22 16:36:41 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 22 Nov 2004 16:36:41 +0100
Subject: [R] How to insert one element into a vector?
In-Reply-To: <41A2038C.5080002@lancaster.ac.uk>
References: <8975119BCD0AC5419D61A9CF1A923E95E898C7@iahce2knas1.iah.bbsrc.reserved>
	<41A2038C.5080002@lancaster.ac.uk>
Message-ID: <x2mzx9hp4m.fsf@biostat.ku.dk>

Barry Rowlingson <B.Rowlingson at lancaster.ac.uk> writes:

> michael watson (IAH-C) wrote:
> > There must be a million ways of doing this!
> 
>   Actually I'd say there were no ways of doing this, since I dont
> think you can actually insert into a vector - you have to create a new
> vector that produces the illusion of insertion!
> 
>   Here's a Q+D function that fails if you try and insert at the start,
> or at the end. Its very D.
> 
> insert <- function(v,e,pos){
>    return(c(v[1:(pos-1)],e,v[(pos):length(v)]))
> }

This is actually an exercise in a certain introductory book on R, with
the intention of having the student do the obvious c(v[1:4],5,v[5:5])
style computation and maybe think a little about the edge effects. I
only recently saw the somewhat unfortunately named append() function,
which has been in S/R ever since the blue book...:

> append(v,5,4)
[1] 1 2 3 4 5 6


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From tlumley at u.washington.edu  Mon Nov 22 16:39:19 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 22 Nov 2004 07:39:19 -0800 (PST)
Subject: [R] variable object naming
In-Reply-To: <loom.20041122T062421-246@post.gmane.org>
References: <1101100273.41a174f12a2e2@webmail.uvm.edu>
	<loom.20041122T062421-246@post.gmane.org>
Message-ID: <Pine.A41.4.61b.0411220737520.234728@homer05.u.washington.edu>

On Mon, 22 Nov 2004, Gabor Grothendieck wrote:

> Benjamin M. Osborne <Benjamin.Osborne <at> uvm.edu> writes:
>
> :
> : Is it possible to give a temporary object a name that varies with each run
> of a
> : foreloop?  For example, I want to fill a matrix every time I run a loop, and
> I
> : want a new matrix with each run, with an appropriate new name.
> : i.e.:
> : for(i in 1:5){...  matrix.i<-some values ...}
> :
> : so that in the end I would have:
> : matrix.1
> : matrix.2
> : matrix.3
> : matrix.4
> : matrix.5
>
>
> See 7.1 of the FAQ.
>

Especially the part that says this probably isn't what you want to do. You 
would likely be better off with a list, and doing
     matrixlist[[i]]<-some_values


 	-thomas



From B.Rowlingson at lancaster.ac.uk  Mon Nov 22 16:43:01 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 22 Nov 2004 15:43:01 +0000
Subject: [R] How to insert one element into a vector?
In-Reply-To: <200411220929.06611.deepayan@stat.wisc.edu>
References: <8975119BCD0AC5419D61A9CF1A923E95E898C7@iahce2knas1.iah.bbsrc.reserved>
	<41A2038C.5080002@lancaster.ac.uk>
	<200411220929.06611.deepayan@stat.wisc.edu>
Message-ID: <41A20905.3000405@lancaster.ac.uk>

Deepayan Sarkar wrote:

> Pretty much what 'append' does.

  A shame then, that help.search("insert") doesn't find 'append'! I cant 
think why anyone looking for a way of _inserting_ a value in the middle 
of a vector would think of looking at "append"!

  Python has separate insert and append methods for vectors.

  >>> x=[1,2,3,4,6]
  >>> x.insert(4,5)
  >>> x
  [1, 2, 3, 4, 5, 6]
  >>> x.append(99)
  >>> x
  [1, 2, 3, 4, 5, 6, 99]


Barry



From tlumley at u.washington.edu  Mon Nov 22 17:10:54 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 22 Nov 2004 08:10:54 -0800 (PST)
Subject: [R] How to insert one element into a vector?
In-Reply-To: <41A20905.3000405@lancaster.ac.uk>
References: <8975119BCD0AC5419D61A9CF1A923E95E898C7@iahce2knas1.iah.bbsrc.reserved>
	<41A2038C.5080002@lancaster.ac.uk>
	<200411220929.06611.deepayan@stat.wisc.edu>
	<41A20905.3000405@lancaster.ac.uk>
Message-ID: <Pine.A41.4.61b.0411220804500.234728@homer05.u.washington.edu>

On Mon, 22 Nov 2004, Barry Rowlingson wrote:

> Deepayan Sarkar wrote:
>
>> Pretty much what 'append' does.
>
> A shame then, that help.search("insert") doesn't find 'append'! I cant think 
> why anyone looking for a way of _inserting_ a value in the middle of a vector 
> would think of looking at "append"!

Yes, this should be fixed.

> Python has separate insert and append methods for vectors.

I don't think this is a good idea unless there are efficiency issues. 
You could always define one: the definition is fairly simple.
    insert<-append

 	-thomas



From abu3ammar at gmail.com  Mon Nov 22 18:45:23 2004
From: abu3ammar at gmail.com (Yasser El-Zein)
Date: Mon, 22 Nov 2004 12:45:23 -0500
Subject: [R] timeDate
Message-ID: <b1d315040411220945366432ab@mail.gmail.com>

what package should I include to use timeDate? I want to convert a
double (num of millis) into date object.



From ligges at statistik.uni-dortmund.de  Mon Nov 22 18:54:43 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 22 Nov 2004 18:54:43 +0100
Subject: a question on plot(), workspace or method dispatch, NOT about: Re:
	[R] RWinEdt, other text editors and R2.01 a problem in pasting commands
In-Reply-To: <000001c4d088$6db1b930$65036497@sta.uniroma1.it>
References: <000001c4d088$6db1b930$65036497@sta.uniroma1.it>
Message-ID: <41A227E3.4050305@statistik.uni-dortmund.de>

giovanna jona lasinio wrote:

> Dear All,
> In the last few days I started using the last version of R as I
> encountered a problem with R1.9 and the use of RWinEdt, however the
> problem shows with R2.01 as well. More precisely
> 
> 1. I install R2.01 after removing old verions of R
> 2. I Install RWinEdt package versione RWinEdt_1.6-2 and
> SWinRegistry_0.3-2 and everything seems fine
> 3. I recall the RWinEdt library (library(RWinEdt)) and WinEdt appears
> 4. now after loading a workspace I run the following command using the
> paste icon in WinEdt: 
> plot(x,xaxt="n",xlab="time", ylab=expression(alpha),main=nome,type="l")
> 
> And I get the following error message:
> <<Error in stripchart(x, ...) : unused argument(s) (xaxt ...)>>
>>From now on the plot command stops working, it doesn't recognize any
> option not even type="l" or type="b" or anything.
> 
> If I type the intrusction directly on the command window without any
> previous paste operation from WinEdt the command works properly.
> 
> Furthermore if I try to paste commands by simply coping it from a text
> file and pasting it on the R command window I have the same problem. I
> even tried:
> source("prova.txt") containing the same command line and I got the same
> error:
> Error in stripchart(x, ...) : unused argument(s) (xaxt ...)
> 
> I'm not very good at using Windows XP, can anyone help?
> 
> Thanks
> Giovanna Jona Lasinio

Please don't complain about R-WinEdt, WinEdt, other editors, or Windows 
if the point is obviously somewhere else!

1. Have you redefined plot() in your workspace?
2. What kind of object is x (in particular it's class, please!)?

Uwe Ligges



From HankeA at mar.dfo-mpo.gc.ca  Mon Nov 22 18:59:32 2004
From: HankeA at mar.dfo-mpo.gc.ca (Hanke, Alex)
Date: Mon, 22 Nov 2004 13:59:32 -0400
Subject: [R] RODBC and Table views
Message-ID: <E37EEC6DE3A0C5439B7E7B07406C24AE124A6B@msgmarsta01.bio.dfo.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041122/e96a54cc/attachment.pl

From jmoreira at fe.up.pt  Mon Nov 22 19:10:30 2004
From: jmoreira at fe.up.pt (=?iso-8859-1?Q?Jo=E3o_Mendes_Moreira?=)
Date: Mon, 22 Nov 2004 18:10:30 -0000
Subject: [R] timeDate
References: <b1d315040411220945366432ab@mail.gmail.com>
Message-ID: <00a501c4d0be$8ecd7650$5e7aa8c0@FEUPsig.fe.up.pt>

library("chron")

Joao
----- Original Message ----- 
From: "Yasser El-Zein" <abu3ammar at gmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Monday, November 22, 2004 5:45 PM
Subject: [R] timeDate


> what package should I include to use timeDate? I want to convert a
> double (num of millis) into date object.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ggrothendieck at myway.com  Mon Nov 22 19:18:03 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 22 Nov 2004 18:18:03 +0000 (UTC)
Subject: [R] timeDate
References: <b1d315040411220945366432ab@mail.gmail.com>
Message-ID: <loom.20041122T191352-664@post.gmane.org>

Yasser El-Zein <abu3ammar <at> gmail.com> writes:

: 
: what package should I include to use timeDate? I want to convert a
: double (num of millis) into date object.

The times class of the chron package stores times as fractions of
a day.  You could check whether or not that gives you the resolution you
need.  Install chron and issue the R commands:

library(chron)
?chron

If you only need resolution to the second then you could alternately
use the POSIXct class.

Also check out the article on dates and times in R News 4/1 which
discusses the various possibilities.



From ligges at statistik.uni-dortmund.de  Mon Nov 22 19:24:48 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 22 Nov 2004 19:24:48 +0100
Subject: [R] timeDate
In-Reply-To: <b1d315040411220945366432ab@mail.gmail.com>
References: <b1d315040411220945366432ab@mail.gmail.com>
Message-ID: <41A22EF0.60709@statistik.uni-dortmund.de>

Yasser El-Zein wrote:

> what package should I include to use timeDate? I want to convert a
> double (num of millis) into date object.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

You might want to read the Help Desk column in R News 4 (1).

Uwe Ligges



From drakegis at dacafe.com  Mon Nov 22 19:30:35 2004
From: drakegis at dacafe.com (DrakeGis)
Date: Mon, 22 Nov 2004 13:30:35 -0500 (EST)
Subject: [R] plot.Map Maptools
Message-ID: <33513.141.211.77.96.1101148235.squirrel@cafemail.giscafe.com>

Hi,
  Is there any way to plot a Map with different shading patterns, instead
of color intensity, in R2.0.1 over Linux ?

  Thanks.


D


-----------------------------------------
Stay ahead of the information curve.
Receive GIS news and jobs on your desktop daily.
Subscribe today to the GIS CafeNews newsletter.
[ http://www10.giscafe.com/nl/newsletter_subscribe.php ]
It's informative and essential.



From graumann at its.caltech.edu  Mon Nov 22 19:50:42 2004
From: graumann at its.caltech.edu (Johannes Graumann)
Date: Mon, 22 Nov 2004 10:50:42 -0800
Subject: [R] annotation problems (conditional text())
In-Reply-To: <419F7439.7080106@statistik.uni-dortmund.de>
References: <20041119162143.52d36a49@localhost>
	<419F7439.7080106@statistik.uni-dortmund.de>
Message-ID: <20041122105042.4fe97091@localhost>

Thanks! Major knot in brain unraveled ...

Joh

On Sat, 20 Nov 2004 17:43:37 +0100
Uwe Ligges <ligges at statistik.uni-dortmund.de> wrote:

> Johannes Graumann wrote:
> 
> > Hello,
> > 
> > I'm trying to annotate my plots nicely and am running into trouble.
> > This example contains two problems:
> > a) the 'text()' arguments do not show the conditional behavior I'm
> > trying to give them. I try to test for the intercept of my
> > regression and reformat the output accordingly ('+ intercept' in the
> > >= 0 case and
> > '- sqrt(intercept^2)' in the other case), which doesn't work this
> > way. b) my pasted substitute commands yield jolted output, which
> > overwrites itself ...
> > 
> > Can anyone give this newbie a nudge into the right direction?
> 
> 1. Multi-line mathematical annotation is not supported. You have to
> add them line by line.
> 
> 2. You want to use a construct such as
>    if(m >= 0){
>      # code for text 1
>    } else{
>      # code for text 2
>    }
> rather than the one you speicfied below, which evaluates both text() 
> calls if m > 0.
> 
> Uwe Ligges
> 
> 
> > Thanks, Joh
> > 
> > if(m >= 0){
> >   text(
> >     4,0.19,
> >     cex=0.75,
> >     adj=0,
> >     paste(
> >       substitute(
> >         y == m*x+b,
> >         list(
> >           m=round(m,digits=5),
> >           b=round(b,digits=5)
> >         )
> >       ),
> >       "\n",
> >       substitute(
> >         R^2==rsquared,
> >         list(
> >           rsquared=round(summary(fit)$r.squared,digits=3)
> >         )
> >       )
> >     )
> >   );
> >   text(
> >     4,0.19,
> >     cex=0.75,
> >     adj=0,
> >     paste(
> >       substitute(
> >         y == m*x-b,
> >         list(
> >           m=round(m,digits=5),
> >           b=round(sqrt(b^2),digits=5)
> >         )
> >       ),
> >       "\n",
> >       substitute(
> >         R^2==rsquared,
> >         list(
> >           rsquared=round(summary(fit)$r.squared,digits=3)
> >         )
> >       )
> >     )
> >   )
> > }
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 
>



From p.murrell at auckland.ac.nz  Mon Nov 22 20:04:49 2004
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Tue, 23 Nov 2004 08:04:49 +1300
Subject: [R] How to correct this
References: <33F91FB3FDF42E4180428AC66A5CF30B02D3C900@afhex01.dpi.wa.gov.au>
Message-ID: <41A23851.3000002@stat.auckland.ac.nz>

Hi


Mulholland, Tom wrote:
> Taking note of the first post, this is what I assume you wish. Note Paul's caveat in the help file
> 
> "If you resize the device, all bets are off!"
> 
> require(gridBase)
> x<-seq(0,1,0.2)
> y<-x
> pred<-matrix(c(0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.7, 0.7, 0.7, 0.7,
> 0.5, 0.5, 0.7, 0.9, 0.9, 0.7, 0.5, 0.5, 0.7, 0.9, 0.9, 0.7, 0.5, 0.5,
> 0.7, 0.7, 0.7, 0.7, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5), 6, 6)
> image(x, y, pred, col = gray(20:100/100), asp='s', axes=F, xlab=" ",
> ylab="")
> points(0.5, 0.5, col = 5) # the centre of the image


In this case, using grid (or gridBase) is probably overkill.  The 
symbols() function should do what you want.  For example, ...

symbols(rep(0.5, 4), rep(0.5, 4), circles=1:4, add=TRUE)

Paul


> vps <- baseViewports()
> pushViewport(vps$plot)
> grid.circle(x=0.5, y=0.5, r=0.1, draw=TRUE,  gp=gpar(col=5))
> grid.circle(x=0.5, y=0.5, r=0.3, draw=TRUE, gp=gpar(col=5))
> grid.circle(x=0.5, y=0.5, r=0.5, draw=TRUE, gp=gpar(col=5))
> 
> 
> 
> -----Original Message-----
> From: Jin.Li at csiro.au [mailto:Jin.Li at csiro.au]
> Sent: Monday, 22 November 2004 1:21 PM
> To: r-help at stat.math.ethz.ch
> Subject: RE: [R] How to correct this
> 
> 
> Hi there,
> 
> I would like to add a few circles to the following image: 
> x<-seq(0,1,0.2)
> y<-x
> pred<-matrix(c(0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.7, 0.7, 0.7, 0.7,
> 0.5, 0.5, 0.7, 0.9, 0.9, 0.7, 0.5, 0.5, 0.7, 0.9, 0.9, 0.7, 0.5, 0.5,
> 0.7, 0.7, 0.7, 0.7, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5), 6, 6)
> image(x, y, pred, col = gray(20:100/100), asp='s', axes=F, xlab=" ",
> ylab="")
> points(0.5, 0.5, col = 5) # the centre of the image
> 
> The centre of these circles needs to be overlapped with the centre of
> the image. Any helps are greatly appreciated.
> Regards,
> Jin
> 
> 
> -----Original Message-----
> From: Mulholland, Tom [mailto:Tom.Mulholland at dpi.wa.gov.au] 
> Sent: Monday, 22 November 2004 12:29 P
> To: Li, Jin (CSE, Atherton)
> Subject: RE: [R] How to correct this
> 
> I think you need to create a complete set of code that can be replicated
> by anyone trying to help.
> I ran the three grid.circle commands on my current plot and it did what
> I expected it to do. It plotted three circles centred in the current
> viewport. See the jpeg.
> 
> The last command using points makes me think that you need to understand
> about units and the setting up of viewports. I have not played around
> with this much but I think thr newsletter had an article which may be of
> use (although it uses old code I think the differences are minor)
> 
> Ciao, Tom
> 
> -----Original Message-----
> From: Jin.Li at csiro.au [mailto:Jin.Li at csiro.au]
> Sent: Monday, 22 November 2004 10:07 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] How to correct this
> 
> 
> Hi there,
> 
>  
> 
> I tried to add a few circles on an existing figure using the following
> codes
> 
>  
> 
> grid.circle(x=0.5, y=0.5, r=0.1, draw=TRUE,  gp=gpar(col=5))
> 
> grid.circle(x=0.5, y=0.5, r=0.3, draw=TRUE, gp=gpar(col=5))
> 
> grid.circle(x=0.5, y=0.5, r=0.5, draw=TRUE, gp=gpar(col=5))
> 
> points(0.5, 0.5, col = 5) # centre of the circle
> 
>  
> 
> , but all circles moved away from the centre.  Could we do any
> corrections to this? Thanks.
> 
>  
> 
> Regards,
> 
>  
> 
> Jin
> 
> ==========================
> 
> Jin Li, PhD
> 
> Climate Impacts Modeller
> 
> CSIRO Sustainable Ecosystems
> 
> Atherton, QLD 4883
> 
> Australia
> 
> Ph: 61 7 4091 8802
> 
> Email: jin.li at csiro.au <mailto:jin.li at csiro.au> 
> 
> ==========================
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From constant.depiereux at aqte.be  Mon Nov 22 20:35:27 2004
From: constant.depiereux at aqte.be (=?iso-8859-1?Q?Constant_Depi=E8reux?=)
Date: Mon, 22 Nov 2004 20:35:27 +0100
Subject: [R] R(D)Com
Message-ID: <200411221935.iAMJZk2H013159@outmx015.isp.belgacom.be>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041122/354522d7/attachment.pl

From sander at oomvanlieshout.net  Mon Nov 22 20:59:52 2004
From: sander at oomvanlieshout.net (Sander Oom)
Date: Mon, 22 Nov 2004 21:59:52 +0200
Subject: [R] IFELSE across a 3D array?
Message-ID: <6.1.2.0.0.20041122213322.026dee60@www.oomvanlieshout.net>

Dear all,

We are trying to clean multiple realizations of a pattern. Erroneous NODATA 
and spurious DATA occur in the realizations. As we have to do a 1000 
realizations for many patterns, efficiency of the code is important.

We need to correct the realizations with a 'mask' pattern of DATA/NODATA. 
We think an ifelse should do the job. Spurious DATA will be simply removed 
using the mask. Erroneous NODATA should be filled in by averaging across 
the cell's nearest neighbors.

The ifelse table is as follows:
IFELSE criteria to determine TARGET from realization and mask
Real  Mask  Target
10     10      10
10      1     0/1 determined through post processing function (average 
across neighbors)
0/1    10      10
0/1     1     0/1

We think that an APPLY on a multi dimensional array is the way to go, with 
each realization (2 dimensions) being a dimension of the array (like a 
stack of maps)

This is where we have got so far:

*********************
library(abind)

#Sim: 0=Absent; 1=Present; 10=NODATA
vectSim <- 
c(0,1,0,1,10,0,1,0,1,1,10,0,1,1,0,1,0,10,10,1,0,1,0,1,0,1,1,10,0,10,10,1,0,1,0,10)
#Mask: 1=DATA; 10=NODATA
vectMask <- c(10,1,10,1,1,10,1,10,1,10,10,1)
length(vectSim)
length(vectMask)

numRow<-3
numCol<-4
numReal<-3

Sim <- array(vectSim, c(numRow,numCol,numReal))
Sim
Mask <- array(vectMask, c(numRow,numCol))
Mask

SmoothSim <- apply(Sim, c(1,2), ifelse(MaskSim==10,33,99))
SmoothSim

********************

Any help is much appreciated!

Thanks,

Sander and Alessandro.



From hb at maths.lth.se  Mon Nov 22 20:41:49 2004
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Mon, 22 Nov 2004 20:41:49 +0100
Subject: [R] The hidden costs of GPL software?
In-Reply-To: <419B96C9.3030108@pdf.com>
Message-ID: <001a01c4d0cb$58b98bf0$860040d5@hblaptop>

FYI, just noticed that the GPL is about (="a draft of which is due next
year") to be revised into GPL v3. Maybe they will solve part of the problems
you mention. Not much substance yet, but see

 GPL 3 to Take on IP, Patents, eWeek, November 22, 2004
 http://www.eweek.com/article2/0,1759,1730102,00.asp

and the slashdot discussion

 GPL Revision Coming Soon, slashdot, November 22, 2004
 http://slashdot.org/article.pl?sid=04/11/22/1746225&tid=117

Question to Martin Maechler: Is it ok to change the subject title to, say,
"Problem with GPL (Was: RE: ...)" when replying to a message? This thread
has covered quite a wide range of topics this far.

Cheers

Henrik Bengtsson

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Spencer Graves
> Sent: Wednesday, November 17, 2004 7:22 PM
> To: Berton Gunter
> Cc: r-help at stat.math.ethz.ch; 'Patrick Burns'; 'Philippe Grosjean'
> Subject: Re: [R] The hidden costs of GPL software?
> 
> 
>       I agree with Bert.  Thanks to all who contributed.  I'd like to 
> add one comment I didn't see in the thread so far: 
> 
>       The corporate legal where I work is deathly afraid of the GNU 
> General Public License (GPL), because if we touch GPL software 
> inappropriately with our commercial software, our copyrights are 
> replaced by the GPL.  This in turn means we can't charge royalties, 
> which means we can't repay the investors who covered our initial 
> development costs, and we file for bankruptcy.  The rabid capitalists 
> meet the rabid socialists and walk away, shaking their heads. 
>  (Sec. 2.b 
> of the GPL:  "You must cause any work that you distribute or publish, 
> that in whole or in part contains or is derived from the 
> Program or any 
> part thereof, to be licensed as a whole at no charge to all third 
> parties under the terms of this License."  We can get around this by 
> packaging accesses to GPL software as separately installed add-on(s), 
> because then only the add-on(s) would be covered by the GPL.)  Our 
> corporate legal is more concerned about a possible law suit from a 
> possible competitor than from the R Foundation, but the 
> threat is still 
> real and still being adjudicated in other cases. 
> 
>       If the GPL were not so tight on this point, someone could 
> commercialize a GUI for R without having to offer their source code 
> under the GPL. 
> 
>       However, even without this change, R seems to be the 
> platform of 
> choice for new statistical algorithm development by a growing 
> portion of 
> the international scientific community.  Moreover, from my experience 
> with this listserve, the technical support here is far superior to 
> anything I've experienced with any other software in the 40+ 
> years since 
> I wrote my first Fortran code. 
> 
>       Best Wishes,
>       spencer graves
> 
> Berton Gunter wrote:
> 
> >All:
> >
> >I have much enjoyed the discussion. Thanks to all who have 
> contibuted.
> >
> >Two quick comments:
> >
> >1. The problem of designing a GUI to make R's functionality more 
> >accessible is, I believe just one component of the larger issue of 
> >making statistical/data analysis functionality available to 
> those who 
> >need to use it but do not have sufficient understanding and 
> background 
> >to do so properly. I certainly include myself in this 
> category in many 
> >circumstances. A willingness and commitment to learning ( = 
> hard work!) 
> >is the only rational solution here, and saying that one doesn't have 
> >the time really doesn't cut it for me. Ditto for R language 
> >functionality?
> >
> >2. However, R has many attractive features for data manipulation and 
> >graphics that make it attractive for common tasks that are now done 
> >most frequently with (ugh!) Excel (NOT Statistica, Systat, et. al.). 
> >For this subset of R's functionality a GUI would be attractive. 
> >However, writing a good GUI for graphing that even begins to take 
> >advantage of R's flexibility and power in this arena is an 
> enormous -- 
> >perhaps an impossible -- task. Witness the S-Plus graphics 
> GUI, which I 
> >think is truly awful (and appears to thwart more than it helps, at 
> >least from many of the queries one sees on that news list). 
> So I'm not 
> >sanguine.
> >
> >Again, thanks to all for a thoughful and enjoyable discussion.
> >
> >-- Bert Gunter
> >Genentech Non-Clinical Statistics
> >South San Francisco, CA
> > 
> >"The business of the statistician is to catalyze the scientific 
> >learning process."  - George E. P. Box
> > 
> > 
> >
> >  
> >
> >>-----Original Message-----
> >>From: r-help-bounces at stat.math.ethz.ch
> >>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Patrick Burns
> >>Sent: Wednesday, November 17, 2004 6:28 AM
> >>To: Jan P. Smit
> >>Cc: r-help at stat.math.ethz.ch; Philippe Grosjean; 
> >>r-sig-gui at stat.math.ethz.ch
> >>Subject: Re: [R] The hidden costs of GPL software?
> >>
> >>I'm a big advocate -- perhaps even fanatic -- of  making R 
> easier for 
> >>novices in order to spread its use, but I'm not convinced 
> that  a GUI 
> >>(at least in the traditional form) is the most valuable approach.
> >>
> >>Perhaps an overly harsh summary of some of Ted Harding's statements
> >>is: You can make a truck easier to get into by taking off the
> >>wheels, but
> >>that doesn't make it more useful.
> >>
> >>In terms of GUIs, I think what R should focus on is the
> >>ability for  user's
> >>to make their own specialized GUI.  So that a knowledgeable 
> programmer
> >>at an installation can create a system that is easy for 
> >>unsophisticated
> >>users for the limited number of tasks that are to be done.  
> >>The ultimate
> >>users may not even need to know that R exists.
> >>
> >>I think Ted Harding was on  the mark when he said that it 
> is the help 
> >>system that needs enhancement.  I can imagine a system that 
> gets the 
> >>user to the right function and then helps fill in the 
> arguments; all 
> >>of the time pointing them towards the command line rather than away 
> >>from it.
> >>
> >>The author of the referenced article highlighted some hidden
> >>costs of R,
> >>but did not highlight the hidden benefits (because they were 
> >>hidden from
> >>him).  A big benefit of R is all of the bugs that aren't in 
> >>it (which may or
> >>may not be due to its free status).
> >>
> >>Patrick Burns
> >>
> >>Burns Statistics
> >>patrick at burns-stat.com
> >>+44 (0)20 8525 0696
> >>http://www.burns-stat.com
> >>(home of S Poetry and "A Guide for the Unwilling S User")
> >>
> >>Jan P. Smit wrote:
> >>
> >>    
> >>
> >>>Dear Phillippe,
> >>>
> >>>Very interesting. The URL of the article is
> >>>http://www.scientific-computing.com/scwsepoct04free_statist
> ics.html.
> >>>
> >>>Best regards,
> >>>
> >>>Jan Smit
> >>>
> >>>
> >>>Philippe Grosjean wrote:
> >>>
> >>>      
> >>>
> >>>>Hello,
> >>>>
> >>>>In the latest 'Scientific Computing World' magazine (issue 78, p.
> >>>>22), there
> >>>>is a review on free statistical software by Felix Grant ("doesn't 
> >>>>have to
> >>>>pay good money to obtain good statistics software"). As far as I 
> >>>>know, this
> >>>>is the first time that R is even mentioned in this 
> magazine, given 
> >>>>that it
> >>>>usually discuss commercial products.
> >>>>
> >>>>        
> >>>>
> >>[ ...]
> >>
> >>    
> >>
> >>>      
> >>>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list 
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide!
> >>http://www.R-project.org/posting-guide.html
> >>
> >>    
> >>
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list 
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! 
> >http://www.R-project.org/posting-guide.html
> >  
> >
> 
> -- 
> Spencer Graves, PhD, Senior Development Engineer
> O:  (408)938-4420;  mobile:  (408)655-4567
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Roger.Bivand at nhh.no  Mon Nov 22 21:00:07 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 22 Nov 2004 21:00:07 +0100 (CET)
Subject: [R] plot.Map Maptools
In-Reply-To: <33513.141.211.77.96.1101148235.squirrel@cafemail.giscafe.com>
Message-ID: <Pine.LNX.4.44.0411222056490.718-100000@reclus.nhh.no>

On Mon, 22 Nov 2004, DrakeGis wrote:

> Hi,
>   Is there any way to plot a Map with different shading patterns, instead
> of color intensity, in R2.0.1 over Linux ?
> 

Yes, but not with plot.Map(). If you need to fill, or shade, you are 
working with polygons. Convert to the polylist object using Map2poly(), 
and use plot.polylist() - see help(plot.polylist) for an example with 
shading patterns (arguments angle= and density=).

Roger Bivand


>   Thanks.
> 
> 
> D
> 
> 
> -----------------------------------------
> Stay ahead of the information curve.
> Receive GIS news and jobs on your desktop daily.
> Subscribe today to the GIS CafeNews newsletter.
> [ http://www10.giscafe.com/nl/newsletter_subscribe.php ]
> It's informative and essential.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From spencer.graves at pdf.com  Mon Nov 22 21:07:42 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 22 Nov 2004 12:07:42 -0800
Subject: [R] R(D)Com
In-Reply-To: <200411221935.iAMJZk2H013159@outmx015.isp.belgacom.be>
References: <200411221935.iAMJZk2H013159@outmx015.isp.belgacom.be>
Message-ID: <41A2470E.8030409@pdf.com>

      Have you tried "www.r-project.org" -> CRAN -> (select a local 
mirror, e.g., http://mirror.internet.tp/cran/) -> "Software:  Other" -> 
"Non-standard packages:  R-(D)COM Interface"? 

      If yes, before replying, please "read the posting guide! 
http://www.R-project.org/posting-guide.html".  In particular, search -> 
"R site search" can be quite helpful.  Also, please provide more detail 
of what you want to do with R(D)Com, including which version of R, which 
operating system, and what you've tried already that didn't work. 

      Bon Chance!
      Spencer Graves

Constant Depi??reux wrote:

>Hello,
> 
>Brand new user of R that I want to interface with Windew
>(http://www.pcsoft.fr), I am looking for a detailed documentation of
>R(D)Com. 
> 
>Does anybody know where I can find it, or do you have any form of experience
>of the use of R with Windev?
> 
>Thanks for your help.
> 
> 
> 
>PS : As I am new to the group, people interested in some details about the
>requestor could find his resume at address
>http://e-doclib.aqte.be/docs-doc-view.php?doc_id=303.
> 
>Best regards.
> 
>Constant Depi??reux
>Managing Director
>Applied Quality Technologies Europe sprl
>Rue des D??port??s 123
>B-4800 Verviers
>+32 87 29 21 75
>www.aqte.be
>
>---- Ce message a ??t?? scann?? avec Norton anti-virus, derni??re mise ?? jour
>disponible en ligne -----
>  
>
> 
> 
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From abu3ammar at gmail.com  Mon Nov 22 22:08:17 2004
From: abu3ammar at gmail.com (Yasser El-Zein)
Date: Mon, 22 Nov 2004 16:08:17 -0500
Subject: [R] timeDate
In-Reply-To: <41A22EF0.60709@statistik.uni-dortmund.de>
References: <b1d315040411220945366432ab@mail.gmail.com>
	<41A22EF0.60709@statistik.uni-dortmund.de>
Message-ID: <b1d3150404112213085eda9b3f@mail.gmail.com>

>From the document it is apparent to me that I need as.POSIXct  (I have
a double representing the number of millis since 1/1/1970 and I need
to construct a datetime object). I see it showing how to construct the
time object from a string representing the time but now fro a double
of millis. Does anyone know hoe to do that?


On Mon, 22 Nov 2004 19:24:48 +0100, Uwe Ligges
<ligges at statistik.uni-dortmund.de> wrote:
> Yasser El-Zein wrote:
> 
> 
> 
> > what package should I include to use timeDate? I want to convert a
> > double (num of millis) into date object.
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> You might want to read the Help Desk column in R News 4 (1).
> 
> Uwe Ligges
> 
>



From ggrothendieck at myway.com  Mon Nov 22 22:48:20 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 22 Nov 2004 21:48:20 +0000 (UTC)
Subject: [R] timeDate
References: <b1d315040411220945366432ab@mail.gmail.com>
	<41A22EF0.60709@statistik.uni-dortmund.de>
	<b1d3150404112213085eda9b3f@mail.gmail.com>
Message-ID: <loom.20041122T224151-145@post.gmane.org>

Yasser El-Zein <abu3ammar <at> gmail.com> writes:

> 
> >From the document it is apparent to me that I need as.POSIXct  (I have
> a double representing the number of millis since 1/1/1970 and I need
> to construct a datetime object). I see it showing how to construct the
> time object from a string representing the time but now fro a double
> of millis. Does anyone know hoe to do that?
> 

If by millis you mean milliseconds (i.e. one thousandths of a second)
then POSIXct does not support that resolution, but if rounding to 
seconds is ok then 

  structure(round(x/1000), class = c("POSIXt", "POSIXct"))

should give it to you assuming x is the number of milliseconds.



From mwgrant2001 at yahoo.com  Mon Nov 22 22:56:33 2004
From: mwgrant2001 at yahoo.com (Michael Grant)
Date: Mon, 22 Nov 2004 13:56:33 -0800 (PST)
Subject: [R] variable object naming
In-Reply-To: <1101100273.41a174f12a2e2@webmail.uvm.edu>
Message-ID: <20041122215634.60565.qmail@web54401.mail.yahoo.com>

Is this, i.e., using 'assign', the direction you want
to go? If so, pay attention to the 'envir' parameter.

BEGIN FROM Stat-R-US:
http://www.ku.edu/~pauljohn/R/Rtips.html#2.2
...
2.2 Create variable names "on the fly"   (10/04/2001
Paul Johnson <pauljohn at ukans.edu>) 

Paste together a variable name, set it to a value. Use
assign. As in 

> assign(paste("file", 1, "max", sep=""), 1)
> ls()
[1] "file1max"

(Brian Ripley, June 18, 2001)
...

END FROM STAT-R-US

Regards,
Michael Grant

--- "Benjamin M. Osborne" <Benjamin.Osborne at uvm.edu>
wrote:

> Is it possible to give a temporary object a name
> that varies with each run of a
> foreloop?  For example, I want to fill a matrix
> every time I run a loop, and I
> want a new matrix with each run, with an appropriate
> new name.
> i.e.:
> for(i in 1:5){...  matrix.i<-some values ...}
> 
> so that in the end I would have:
> matrix.1
> matrix.2
> matrix.3
> matrix.4
> matrix.5
> 
> Thanks,
> Ben Osborne
> 
> --
> Botany Department
> University of Vermont
> 109 Carrigan Drive
> Burlington, VT 05405
> 
> benjamin.osborne at uvm.edu
> phone: 802-656-0297
> fax: 802-656-0440
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From macq at llnl.gov  Mon Nov 22 23:33:42 2004
From: macq at llnl.gov (Don MacQueen)
Date: Mon, 22 Nov 2004 14:33:42 -0800
Subject: [R] timeDate
In-Reply-To: <loom.20041122T224151-145@post.gmane.org>
References: <b1d315040411220945366432ab@mail.gmail.com>
	<41A22EF0.60709@statistik.uni-dortmund.de>
	<b1d3150404112213085eda9b3f@mail.gmail.com>
	<loom.20041122T224151-145@post.gmane.org>
Message-ID: <p06110406bdc818fbb50e@[128.115.153.6]>

In which case, be aware that R's round() function rounds to the 
nearest even number when the fraction is 0.5:

>  round(c(1.5,2.5))
[1] 2 2

which might not be desirable for times. Consider floor() or trunc() instead.

>  floor(c(1.5,2.5))
[1] 1 2
>  trunc(c(1.5,2.5))
[1] 1 2


At 9:48 PM +0000 11/22/04, Gabor Grothendieck wrote:
>Yasser El-Zein <abu3ammar <at> gmail.com> writes:
>
>>
>>  >From the document it is apparent to me that I need as.POSIXct  (I have
>>  a double representing the number of millis since 1/1/1970 and I need
>>  to construct a datetime object). I see it showing how to construct the
>>  time object from a string representing the time but now fro a double
>>  of millis. Does anyone know hoe to do that?
>>
>
>If by millis you mean milliseconds (i.e. one thousandths of a second)
>then POSIXct does not support that resolution, but if rounding to
>seconds is ok then
>
>   structure(round(x/1000), class = c("POSIXt", "POSIXct"))
>
>should give it to you assuming x is the number of milliseconds.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From tring at gvdnet.dk  Tue Nov 23 00:06:24 2004
From: tring at gvdnet.dk (Troels Ring)
Date: Tue, 23 Nov 2004 00:06:24 +0100
Subject: [R] optimize in very small values
Message-ID: <5.2.0.9.0.20041122235011.03942900@home.gvdnet.dk>

I hope you will forgive me this simple question on titration.
I'm trying to find very small values from the algorithm below, which I 
believe is
correctly formatted, and the constants are also correct. When SID goes over 
ATOT, fitted vales are
much too low compared to the literature. I guess I must be using optimize 
in a wrong way but cannot find out how to improve it. I'm on windows, R 
2.0. Best wishes
Troels Ring, Aalborg

ATOT <- 0.019
KA <- 3e-7
KW <- 4.4e-14
SID <- 0.01

ff <- function(H,KA,SID,ATOT)
{H^3 + (KA+SID)*H^2+(KA*(SID-ATOT)-KW)*H- KA*KW}
-log10(optimize(ff,c(0,.1),tol=.Machine$double.eps,KA=KA,SID=SID,ATOT=ATOT)$minimum)



From hans.gardfjell at eg.umu.se  Tue Nov 23 00:44:32 2004
From: hans.gardfjell at eg.umu.se (Hans Gardfjell)
Date: Tue, 23 Nov 2004 00:44:32 +0100
Subject: [R] Danish characters i R2.0.1 vs R1.9.1 under winXP
Message-ID: <41A279E0.7040207@eg.umu.se>

Sorry for not reporting earlier, but the same problem exists with Swedish verisons
of Windows XP together with R 2.0.0 patched (but not with R 1.9.1) and the last
characters in our alphabet.

> "??"
[1] "\344" 
> "??"
[1] "\366" 
> "??"
[1] "??"
> "??"
[1] "\326" 
> "??"
[1] "\304" 
> "?
"
[1] "\305" 


Hans Gardfjell
Ecology and Environmental science
Ume?? University, Sweden


>The problem is in your OS, which thinks the character \370 is not 
>printable in your locale.

>Apparently this is a WinXP problem, for it also thinks \366 is not 
>printable in German (and other versions of Windows thinks it is).
>(Uwe Ligges pointed that out a few hours ago.)

>R 1.9.1 did not check, and so made a mess of trying to print non-printable 
>characters, in particular nuls and control characters.

>There is no simple workaround, as you do want octal representation for 
>non-printable characters (e.g. embedded nuls).  What I have proposed is 
>that we override Windows' view for upper (>= 0x80) characters.

>It does not seem to be common: the only reports before today were for
>Chinese, which is not expected to work.

>On Mon, 22 Nov 2004, Ladelund, Steen wrote:

>>/ Hi all.
/>>/
/>>/ After upgrading to R2.0.1 i get
/>>/
/>>>/ "??"
/>>/ [1] "??"
/>>>/ "??"
/>>/ [1] "\370"
/>>>/ "??"
/>>/ [1] "??"
/>>/
/>>/ Whereas under R1.9.1 i get
/>>/
/>>>/ "??"
/>>/ [1] "??"
/>>>/ "??"
/>>/ [1] "??"
/>>>/ "??"
/>>/ [1] "??"
/>>/
/>>/ Any hints apreciated.
/>>/
/>>/ Steen
/>>/
/>>/ Steen Ladelund, statistician
/>>/ +4543233275 stelad01.FUNNYAglostruphospDOTkbhamt.dk
/>>/ Research Center for Prevention and Health
/>>/ Glostrup University Hospital, Denmark
/>>/ www.fcfs.kbhamt.dk
/>>/
/>>/ ______________________________________________
/>>/ R-help at stat.math.ethz.ch <https://stat.ethz.ch/mailman/listinfo/r-help> mailing list
/>>/ https://stat.ethz.ch/mailman/listinfo/r-help
/>>/ PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
/>>/
/>/
/
>-- 
>Brian D. Ripley,                  ripley at stats.ox.ac.uk <https://stat.ethz.ch/mailman/listinfo/r-help>
>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/ <http://www.stats.ox.ac.uk/%7Eripley/>
>University of Oxford,             Tel:  +44 1865 272861 (self)
>1 South Parks Road,                     +44 1865 272866 (PA)
>Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Tom.Mulholland at dpi.wa.gov.au  Tue Nov 23 03:29:34 2004
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Tue, 23 Nov 2004 10:29:34 +0800
Subject: [R] How to correct this
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA31@afhex01.dpi.wa.gov.au>

This raises the question of "best practice." My answer was predicated on the fact that Jin Li had been attempting to use grid.circle in the first place without success. I rashly made the assumption that there was already a move to try and use some of the more sophisticated techniques within R.

This is a good example of the comments in the "hidden costs" thread, where the pathways to learning R came under some scrutiny. It is also similar to the "[R] How to insert one element into a vector?" where it is noted that append can be used to insert the element. That is the function appears to be originally written for one purpose, but it is evident that it has a broader application that is not immediately recognizable from the function name. When you are new to R it can seem confusing that you use rect for rectangles but symbols for circles, or segments for lines and lines for not lines, but they really are lines.

I am not yet proficient enough to always know which is the best approach. That's even with defining best as quickest, most easily maintained or most readable etc etc.

Now to the point. I have formed a collection of graphics that I have prepared over the last two years which I use to remind myself of the little idiosyncrasies of the various techniques. These of course have evolved as I have. They mostly use data that I cannot make available. I thought it might be a good idea to produce reproducible code that shows the bewildering variety ways to skin the proverbial animal. That is to produce code that can create a PDF flipbook of plots. One of the first things that I do when I load a package, is to run the examples that produce graphical output. I tend to work backwards and understand processes better when I know what the final output looks like. I am mathematically challenged, but can often appreciate what is happening once I see the plot. Ideally the code would include all the bells and whistles. I say this because I have spent hours trying to figure out just exactly what something is supposed to do before finally figuring out that it was really much simpler than I had thought. The bells and whistles should also show how you sometimes have to use par outside of the function (or remember that the ... is there for a reason) to get the effect that you want. For example when I load the vcd package to do mosaicplots I think I have to use par(xpd = TRUE) to get my multi-line labels not to be clipped.

As an evolving beast I see this as a way of demonstrating the techniques that are generally regarded as being "best practice" in a comprehensive manner.

In short I am volunteering. What for? I am not quite sure, but it includes example plots using data that helps in clarifying how the plot should be used. The last point means that I am not capable of producing some plots (and the examples in some packages already do this well) as I have no idea what they mean even when I have plotted the example. 

Tom Mulholland




-----Original Message-----
From: Paul Murrell [mailto:p.murrell at auckland.ac.nz]
Sent: Tuesday, 23 November 2004 3:05 AM
To: Mulholland, Tom
Cc: Jin.Li at csiro.au; r-help at stat.math.ethz.ch
Subject: Re: [R] How to correct this


Hi


Mulholland, Tom wrote:
> Taking note of the first post, this is what I assume you wish. Note Paul's caveat in the help file
> 
> "If you resize the device, all bets are off!"
> 
> require(gridBase)
> x<-seq(0,1,0.2)
> y<-x
> pred<-matrix(c(0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.7, 0.7, 0.7, 0.7,
> 0.5, 0.5, 0.7, 0.9, 0.9, 0.7, 0.5, 0.5, 0.7, 0.9, 0.9, 0.7, 0.5, 0.5,
> 0.7, 0.7, 0.7, 0.7, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5), 6, 6)
> image(x, y, pred, col = gray(20:100/100), asp='s', axes=F, xlab=" ",
> ylab="")
> points(0.5, 0.5, col = 5) # the centre of the image


In this case, using grid (or gridBase) is probably overkill.  The 
symbols() function should do what you want.  For example, ...

symbols(rep(0.5, 4), rep(0.5, 4), circles=1:4, add=TRUE)

Paul


> vps <- baseViewports()
> pushViewport(vps$plot)
> grid.circle(x=0.5, y=0.5, r=0.1, draw=TRUE,  gp=gpar(col=5))
> grid.circle(x=0.5, y=0.5, r=0.3, draw=TRUE, gp=gpar(col=5))
> grid.circle(x=0.5, y=0.5, r=0.5, draw=TRUE, gp=gpar(col=5))
> 
> 
> 
> -----Original Message-----
> From: Jin.Li at csiro.au [mailto:Jin.Li at csiro.au]
> Sent: Monday, 22 November 2004 1:21 PM
> To: r-help at stat.math.ethz.ch
> Subject: RE: [R] How to correct this
> 
> 
> Hi there,
> 
> I would like to add a few circles to the following image: 
> x<-seq(0,1,0.2)
> y<-x
> pred<-matrix(c(0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.7, 0.7, 0.7, 0.7,
> 0.5, 0.5, 0.7, 0.9, 0.9, 0.7, 0.5, 0.5, 0.7, 0.9, 0.9, 0.7, 0.5, 0.5,
> 0.7, 0.7, 0.7, 0.7, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5), 6, 6)
> image(x, y, pred, col = gray(20:100/100), asp='s', axes=F, xlab=" ",
> ylab="")
> points(0.5, 0.5, col = 5) # the centre of the image
> 
> The centre of these circles needs to be overlapped with the centre of
> the image. Any helps are greatly appreciated.
> Regards,
> Jin
> 
> 
> -----Original Message-----
> From: Mulholland, Tom [mailto:Tom.Mulholland at dpi.wa.gov.au] 
> Sent: Monday, 22 November 2004 12:29 P
> To: Li, Jin (CSE, Atherton)
> Subject: RE: [R] How to correct this
> 
> I think you need to create a complete set of code that can be replicated
> by anyone trying to help.
> I ran the three grid.circle commands on my current plot and it did what
> I expected it to do. It plotted three circles centred in the current
> viewport. See the jpeg.
> 
> The last command using points makes me think that you need to understand
> about units and the setting up of viewports. I have not played around
> with this much but I think thr newsletter had an article which may be of
> use (although it uses old code I think the differences are minor)
> 
> Ciao, Tom
> 
> -----Original Message-----
> From: Jin.Li at csiro.au [mailto:Jin.Li at csiro.au]
> Sent: Monday, 22 November 2004 10:07 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] How to correct this
> 
> 
> Hi there,
> 
>  
> 
> I tried to add a few circles on an existing figure using the following
> codes
> 
>  
> 
> grid.circle(x=0.5, y=0.5, r=0.1, draw=TRUE,  gp=gpar(col=5))
> 
> grid.circle(x=0.5, y=0.5, r=0.3, draw=TRUE, gp=gpar(col=5))
> 
> grid.circle(x=0.5, y=0.5, r=0.5, draw=TRUE, gp=gpar(col=5))
> 
> points(0.5, 0.5, col = 5) # centre of the circle
> 
>  
> 
> , but all circles moved away from the centre.  Could we do any
> corrections to this? Thanks.
> 
>  
> 
> Regards,
> 
>  
> 
> Jin
> 
> ==========================
> 
> Jin Li, PhD
> 
> Climate Impacts Modeller
> 
> CSIRO Sustainable Ecosystems
> 
> Atherton, QLD 4883
> 
> Australia
> 
> Ph: 61 7 4091 8802
> 
> Email: jin.li at csiro.au <mailto:jin.li at csiro.au> 
> 
> ==========================
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From Tom.Mulholland at dpi.wa.gov.au  Tue Nov 23 03:49:25 2004
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Tue, 23 Nov 2004 10:49:25 +0800
Subject: [R] Running R from CD?
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA32@afhex01.dpi.wa.gov.au>

I have noticed that R 2.0 did run slower than I thought it should. It's only now that you've raised the issue that I realise how much slower. However since I only use the CD when I am working on other people's machines I can't really say if there are other factors impacting upon the performance. I'll dig up the old disk, make some comparisons and forward the results.

The bottom line is that it is not a big issue for me.

Tom Mulholland

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
...
Subject: Re: [R] Running R from CD?
...
BTW, I believe running R 2.0.x from a CD will be a lot slower than 1.9.1
because of lazy loading and frequent file accesses: that's a theoretical 
issue we intend to address for 2.1.0, but not one anyone has yet commented 
that it is a problem.



From eric.lim at cvsnet.org  Tue Nov 23 07:56:56 2004
From: eric.lim at cvsnet.org (Eric Lim)
Date: Tue, 23 Nov 2004 06:56:56 -0000
Subject: [R] Weibull survival regression
Message-ID: <000001c4d129$a9086320$778f2bd9@youru989fckmtn>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041123/4316fab2/attachment.pl

From ripley at stats.ox.ac.uk  Tue Nov 23 08:25:01 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 23 Nov 2004 07:25:01 +0000 (GMT)
Subject: [R] Danish characters i R2.0.1 vs R1.9.1 under winXP
In-Reply-To: <41A279E0.7040207@eg.umu.se>
References: <41A279E0.7040207@eg.umu.se>
Message-ID: <Pine.LNX.4.61.0411230723070.11218@gannet.stats>

Try R 2.0.1 patched instead: the bug in (some versions of?) Windows XP has 
been worked around now.

On Tue, 23 Nov 2004, Hans Gardfjell wrote:

> Sorry for not reporting earlier, but the same problem exists with Swedish 
> verisons
> of Windows XP together with R 2.0.0 patched (but not with R 1.9.1) and the 
> last
> characters in our alphabet.
>
>> "?"
> [1] "\344" 
>> "?"
> [1] "\366" 
>> "?"
> [1] "?"
>> "?"
> [1] "\326" 
>> "?"
> [1] "\304" 
>> "?"
> [1] "\305" 
>
> Hans Gardfjell
> Ecology and Environmental science
> Ume? University, Sweden
>
>
>> The problem is in your OS, which thinks the character \370 is not printable 
>> in your locale.
>
>> Apparently this is a WinXP problem, for it also thinks \366 is not 
>> printable in German (and other versions of Windows thinks it is).
>> (Uwe Ligges pointed that out a few hours ago.)
>
>> R 1.9.1 did not check, and so made a mess of trying to print non-printable 
>> characters, in particular nuls and control characters.
>
>> There is no simple workaround, as you do want octal representation for 
>> non-printable characters (e.g. embedded nuls).  What I have proposed is 
>> that we override Windows' view for upper (>= 0x80) characters.
>
>> It does not seem to be common: the only reports before today were for
>> Chinese, which is not expected to work.
>
>> On Mon, 22 Nov 2004, Ladelund, Steen wrote:
>
>>> / Hi all.
> />>/
> />>/ After upgrading to R2.0.1 i get
> />>/
> />>>/ "?"
> />>/ [1] "?"
> />>>/ "?"
> />>/ [1] "\370"
> />>>/ "?"
> />>/ [1] "?"
> />>/
> />>/ Whereas under R1.9.1 i get
> />>/
> />>>/ "?"
> />>/ [1] "?"
> />>>/ "?"
> />>/ [1] "?"
> />>>/ "?"
> />>/ [1] "?"
> />>/
> />>/ Any hints apreciated.
> />>/
> />>/ Steen
> />>/
> />>/ Steen Ladelund, statistician
> />>/ +4543233275 stelad01.FUNNYAglostruphospDOTkbhamt.dk
> />>/ Research Center for Prevention and Health
> />>/ Glostrup University Hospital, Denmark
> />>/ www.fcfs.kbhamt.dk
> />>/
> />>/ ______________________________________________
> />>/ R-help at stat.math.ethz.ch 
> <https://stat.ethz.ch/mailman/listinfo/r-help> mailing list
> />>/ https://stat.ethz.ch/mailman/listinfo/r-help
> />>/ PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> />>/
> />/
> /
>> -- 
>> Brian D. Ripley,                  ripley at stats.ox.ac.uk 
>> <https://stat.ethz.ch/mailman/listinfo/r-help>
>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/ 
>> <http://www.stats.ox.ac.uk/%7Eripley/>
>> University of Oxford,             Tel:  +44 1865 272861 (self)
>> 1 South Parks Road,                     +44 1865 272866 (PA)
>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From jarioksa at sun3.oulu.fi  Tue Nov 23 09:04:16 2004
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: 23 Nov 2004 10:04:16 +0200
Subject: [R] How to insert one element into a vector?
In-Reply-To: <41A20905.3000405@lancaster.ac.uk>
References: <8975119BCD0AC5419D61A9CF1A923E95E898C7@iahce2knas1.iah.bbsrc.reserved>
	<41A2038C.5080002@lancaster.ac.uk>
	<200411220929.06611.deepayan@stat.wisc.edu>
	<41A20905.3000405@lancaster.ac.uk>
Message-ID: <1101197056.8482.5.camel@biol102145.oulu.fi>

On Mon, 2004-11-22 at 17:43, Barry Rowlingson wrote:
> Deepayan Sarkar wrote:
> 
> > Pretty much what 'append' does.
> 
>   A shame then, that help.search("insert") doesn't find 'append'! I cant 
> think why anyone looking for a way of _inserting_ a value in the middle 
> of a vector would think of looking at "append"!
> 
>   Python has separate insert and append methods for vectors.
> 
>   >>> x=[1,2,3,4,6]
>   >>> x.insert(4,5)
>   >>> x
>   [1, 2, 3, 4, 5, 6]
>   >>> x.append(99)
>   >>> x
>   [1, 2, 3, 4, 5, 6, 99]

So has R. R's 'insert' is called 'append', and R's 'append' is called
'c'. Counter-intuitively though, and I'm happy that Peter Dalgaard
didn't know that 'append' inserts: it gives some hope to us ordinary
mortals.

cheers, jazza
-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From ylong at smu.edu.sg  Tue Nov 23 09:19:09 2004
From: ylong at smu.edu.sg (LONG Yu)
Date: Tue, 23 Nov 2004 16:19:09 +0800
Subject: [R] Mode?
Message-ID: <CF99BBA00A3F5D4183C2FA33089FBC3403788434@EX01.staff.smu.edu.sg>


Dear all, 

I want to find out the mode for a data set, anyone knows how to do it in
R? 
I tried the codes below, but it seems too long:

tt<-table(data1)         #get the frequency tables of data1
oo<-order(tt);            #get the order of frequencies 
len<-length(tt)           #the length of the tables
mm<-oo[len];            #the last number of oo is the position  
tt[mm]                     #we can get the mode 


Thank you everybody first!

Regards, 
Long Yu



From ligges at statistik.uni-dortmund.de  Tue Nov 23 09:32:49 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 23 Nov 2004 09:32:49 +0100
Subject: [R] optimize in very small values
In-Reply-To: <5.2.0.9.0.20041122235011.03942900@home.gvdnet.dk>
References: <5.2.0.9.0.20041122235011.03942900@home.gvdnet.dk>
Message-ID: <41A2F5B1.7050208@statistik.uni-dortmund.de>

Troels Ring wrote:

> I hope you will forgive me this simple question on titration.
> I'm trying to find very small values from the algorithm below, which I 
> believe is
> correctly formatted, and the constants are also correct. When SID goes 
> over ATOT, fitted vales are
> much too low compared to the literature. I guess I must be using 
> optimize in a wrong way but cannot find out how to improve it. I'm on 
> windows, R 2.0. Best wishes
> Troels Ring, Aalborg
> 
> ATOT <- 0.019
> KA <- 3e-7
> KW <- 4.4e-14
> SID <- 0.01
> 
> ff <- function(H,KA,SID,ATOT)
> {H^3 + (KA+SID)*H^2+(KA*(SID-ATOT)-KW)*H- KA*KW}
> -log10(optimize(ff,c(0,.1),tol=.Machine$double.eps,KA=KA,SID=SID,ATOT=ATOT)$minimum) 

Well, whether the usage is appropriate depends on the problem. At least 
I have not understood what you are going to minimize...

Uwe Ligges



> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Tue Nov 23 09:35:06 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 23 Nov 2004 09:35:06 +0100
Subject: [R] Mode?
In-Reply-To: <CF99BBA00A3F5D4183C2FA33089FBC3403788434@EX01.staff.smu.edu.sg>
References: <CF99BBA00A3F5D4183C2FA33089FBC3403788434@EX01.staff.smu.edu.sg>
Message-ID: <41A2F63A.1000608@statistik.uni-dortmund.de>

LONG Yu wrote:

> Dear all, 
> 
> I want to find out the mode for a data set, anyone knows how to do it in
> R? 
> I tried the codes below, but it seems too long:
> 
> tt<-table(data1)         #get the frequency tables of data1
> oo<-order(tt);            #get the order of frequencies 
> len<-length(tt)           #the length of the tables
> mm<-oo[len];            #the last number of oo is the position  
> tt[mm]                     #we can get the mode 
> 
> 
> Thank you everybody first!
> 
> Regards, 
> Long Yu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


See ?which.max

Uwe Ligges



From tring at gvdnet.dk  Tue Nov 23 09:41:13 2004
From: tring at gvdnet.dk (Troels Ring)
Date: Tue, 23 Nov 2004 09:41:13 +0100
Subject: [R] optimize in very small values -   uniroot solved it
In-Reply-To: <5.2.0.9.0.20041122235011.03942900@home.gvdnet.dk>
Message-ID: <5.2.0.9.0.20041123093812.038f2080@home.gvdnet.dk>

Dear friends - just to prevent further loss of time on the problem below - 
I realized that it was really a root finding exercise and not a 
minimization - uniroot solved the problem.
Sorry for the confusion.
Best wishes
Troels

At 00:06 11/23/04, you wrote:
>I hope you will forgive me this simple question on titration.
>I'm trying to find very small values from the algorithm below, which I 
>believe is
>correctly formatted, and the constants are also correct. When SID goes 
>over ATOT, fitted vales are
>much too low compared to the literature. I guess I must be using optimize 
>in a wrong way but cannot find out how to improve it. I'm on windows, R 
>2.0. Best wishes
>Troels Ring, Aalborg
>
>ATOT <- 0.019
>KA <- 3e-7
>KW <- 4.4e-14
>SID <- 0.01
>
>ff <- function(H,KA,SID,ATOT)
>{H^3 + (KA+SID)*H^2+(KA*(SID-ATOT)-KW)*H- KA*KW}
>-log10(optimize(ff,c(0,.1),tol=.Machine$double.eps,KA=KA,SID=SID,ATOT=ATOT)$minimum)
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From dimitris.rizopoulos at med.kuleuven.ac.be  Tue Nov 23 09:33:33 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Tue, 23 Nov 2004 09:33:33 +0100
Subject: [R] Mode?
References: <CF99BBA00A3F5D4183C2FA33089FBC3403788434@EX01.staff.smu.edu.sg>
Message-ID: <007901c4d137$1e735d60$0540210a@www.domain>

Hi Yu,

this has been recently discussed in the list, look at:

http://tolstoy.newcastle.edu.au/R/help/04/11/7054.html


Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "LONG Yu" <ylong at smu.edu.sg>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, November 23, 2004 9:19 AM
Subject: [R] Mode?


>
> Dear all,
>
> I want to find out the mode for a data set, anyone knows how to do 
> it in
> R?
> I tried the codes below, but it seems too long:
>
> tt<-table(data1)         #get the frequency tables of data1
> oo<-order(tt);            #get the order of frequencies
> len<-length(tt)           #the length of the tables
> mm<-oo[len];            #the last number of oo is the position
> tt[mm]                     #we can get the mode
>
>
> Thank you everybody first!
>
> Regards,
> Long Yu
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From claudiapaladini at web.de  Tue Nov 23 09:54:17 2004
From: claudiapaladini at web.de (Claudia Paladini)
Date: Tue, 23 Nov 2004 09:54:17 +0100
Subject: [R] Batch
Message-ID: <1648891890@web.de>

Content-Type: text/plain; charset="iso-8859-1"
Content-Transfer-Encoding: 7bit
Received-SPF: none (hypatia: domain of claudiapaladini at web.de does not designate permitted sender hosts)
X-Virus-Scanned: by amavisd-new at stat.math.ethz.ch
X-Spam-Checker-Version: SpamAssassin 3.0.1 (2004-10-22) on hypatia.math.ethz.ch
X-Spam-Level: 
X-Spam-Status: No, score=0.0 required=5.0 tests=BAYES_50 autolearn=no version=3.0.1

Dear ladies and gentlemen,
I want to automatise my data-analysis with R. What I exactally want to do, is to start  R - functions out of a perl-skript and do some batch plotting.
But I have no idea how to realise  this. Can anybody help me?
Thank you very much.
With kind regards

Claudia Paladini
__________________________________________________________
Mit WEB.DE FreePhone mit hoechster Qualitaet ab 0 Ct./Min.



From giovanna.jonalasinio at uniroma1.it  Tue Nov 23 09:55:44 2004
From: giovanna.jonalasinio at uniroma1.it (giovanna jona lasinio)
Date: Tue, 23 Nov 2004 09:55:44 +0100
Subject: R: a question on plot(), workspace or method dispatch,
	NOT about: Re: [R] RWinEdt,
	other text editors and R2.01 a problem in pasting commands
In-Reply-To: <41A227E3.4050305@statistik.uni-dortmund.de>
Message-ID: <000301c4d13a$37e9bcf0$65036497@sta.uniroma1.it>

I'm using R since version 1.6, and  I didn't redefine plot in my
workspace (I would never even dream of doing anything like that), I
didn't change any R-environment variable or option in the last 4 month. 
In my example x is a vector of 30 real values ...

I installed the windows service pack2 lately and I removed it now as the
microsoft help desk suggested, but the problem is still there and it is
there even with a new computer on which I installed R for the first time
yesterday. The real problem is that the copy-paste procedure is not
recognized any more on these two computers. If I try to paste any
command using other editors then WinEdt the problem is still there it
extends to almost all R commands, only assign and basic arithmetic is
performed correctly

 in the last two days I tried the following editors:
1. the display file window in the RGUI
2. RWinEdt
3. Xemacs
3. Word pad
4. Block notes
With no changes.

Conclusion: How it may happen that the copy and paste procedure stop
working?

Giovanna
-----Messaggio originale-----
Da: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de] 
Inviato: luned??, 22. novembre 2004 18:55
A: giovanna jona lasinio
Cc: r-help at stat.math.ethz.ch
Oggetto: a question on plot(), workspace or method dispatch, NOT about:
Re: [R] RWinEdt, other text editors and R2.01 a problem in pasting
commands


giovanna jona lasinio wrote:

> Dear All,
> In the last few days I started using the last version of R as I 
> encountered a problem with R1.9 and the use of RWinEdt, however the 
> problem shows with R2.01 as well. More precisely
> 
> 1. I install R2.01 after removing old verions of R
> 2. I Install RWinEdt package versione RWinEdt_1.6-2 and 
> SWinRegistry_0.3-2 and everything seems fine 3. I recall the RWinEdt 
> library (library(RWinEdt)) and WinEdt appears 4. now after loading a 
> workspace I run the following command using the paste icon in WinEdt:
> plot(x,xaxt="n",xlab="time",
ylab=expression(alpha),main=nome,type="l")
> 
> And I get the following error message:
> <<Error in stripchart(x, ...) : unused argument(s) (xaxt ...)>>
>>From now on the plot command stops working, it doesn't recognize any
> option not even type="l" or type="b" or anything.
> 
> If I type the intrusction directly on the command window without any 
> previous paste operation from WinEdt the command works properly.
> 
> Furthermore if I try to paste commands by simply coping it from a text

> file and pasting it on the R command window I have the same problem. I

> even tried:
> source("prova.txt") containing the same command line and I got the 
> same
> error:
> Error in stripchart(x, ...) : unused argument(s) (xaxt ...)
> 
> I'm not very good at using Windows XP, can anyone help?
> 
> Thanks
> Giovanna Jona Lasinio

Please don't complain about R-WinEdt, WinEdt, other editors, or Windows 
if the point is obviously somewhere else!

1. Have you redefined plot() in your workspace?
2. What kind of object is x (in particular it's class, please!)?

Uwe Ligges



From ligges at statistik.uni-dortmund.de  Tue Nov 23 10:19:31 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 23 Nov 2004 10:19:31 +0100
Subject: R: a question on plot(), workspace or method dispatch, NOT about:
	Re: [R] RWinEdt,
	other text editors and R2.01 a problem in pasting commands
In-Reply-To: <000301c4d13a$37e9bcf0$65036497@sta.uniroma1.it>
References: <000301c4d13a$37e9bcf0$65036497@sta.uniroma1.it>
Message-ID: <41A300A3.5010101@statistik.uni-dortmund.de>

giovanna jona lasinio wrote:

> I'm using R since version 1.6, and  I didn't redefine plot in my
> workspace (I would never even dream of doing anything like that), I
> didn't change any R-environment variable or option in the last 4 month. 
> In my example x is a vector of 30 real values ...
> 
> I installed the windows service pack2 lately and I removed it now as the
> microsoft help desk suggested, but the problem is still there and it is
> there even with a new computer on which I installed R for the first time
> yesterday. The real problem is that the copy-paste procedure is not
> recognized any more on these two computers. If I try to paste any
> command using other editors then WinEdt the problem is still there it
> extends to almost all R commands, only assign and basic arithmetic is
> performed correctly

And the same command works when typed in the Console in exactly the same 
way?

>  in the last two days I tried the following editors:
> 1. the display file window in the RGUI
> 2. RWinEdt
> 3. Xemacs
> 3. Word pad
> 4. Block notes
> With no changes.
> 
> Conclusion: How it may happen that the copy and paste procedure stop
> working?

Please specify a reproducible example, i.e. the data, and the code!
What is the output of str(x)?
Does it still happen when R has been started with --vanilla?

Uwe Ligges


> Giovanna
> -----Messaggio originale-----
> Da: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de] 
> Inviato: luned??, 22. novembre 2004 18:55
> A: giovanna jona lasinio
> Cc: r-help at stat.math.ethz.ch
> Oggetto: a question on plot(), workspace or method dispatch, NOT about:
> Re: [R] RWinEdt, other text editors and R2.01 a problem in pasting
> commands
> 
> 
> giovanna jona lasinio wrote:
> 
> 
>>Dear All,
>>In the last few days I started using the last version of R as I 
>>encountered a problem with R1.9 and the use of RWinEdt, however the 
>>problem shows with R2.01 as well. More precisely
>>
>>1. I install R2.01 after removing old verions of R
>>2. I Install RWinEdt package versione RWinEdt_1.6-2 and 
>>SWinRegistry_0.3-2 and everything seems fine 3. I recall the RWinEdt 
>>library (library(RWinEdt)) and WinEdt appears 4. now after loading a 
>>workspace I run the following command using the paste icon in WinEdt:
>>plot(x,xaxt="n",xlab="time",
> 
> ylab=expression(alpha),main=nome,type="l")
> 
>>And I get the following error message:
>><<Error in stripchart(x, ...) : unused argument(s) (xaxt ...)>>
>>>From now on the plot command stops working, it doesn't recognize any
>>option not even type="l" or type="b" or anything.
>>
>>If I type the intrusction directly on the command window without any 
>>previous paste operation from WinEdt the command works properly.
>>
>>Furthermore if I try to paste commands by simply coping it from a text
> 
> 
>>file and pasting it on the R command window I have the same problem. I
> 
> 
>>even tried:
>>source("prova.txt") containing the same command line and I got the 
>>same
>>error:
>>Error in stripchart(x, ...) : unused argument(s) (xaxt ...)
>>
>>I'm not very good at using Windows XP, can anyone help?
>>
>>Thanks
>>Giovanna Jona Lasinio
> 
> 
> Please don't complain about R-WinEdt, WinEdt, other editors, or Windows 
> if the point is obviously somewhere else!
> 
> 1. Have you redefined plot() in your workspace?
> 2. What kind of object is x (in particular it's class, please!)?
> 
> Uwe Ligges
> 
> 
>



From M.Mamin at intershop.de  Tue Nov 23 10:58:14 2004
From: M.Mamin at intershop.de (Marc Mamin)
Date: Tue, 23 Nov 2004 10:58:14 +0100
Subject: [R] sorting without order
Message-ID: <A03188C6623C0D46A703CB5AA59907F201C11C59@JENMAIL01.ad.intershop.net>

Hello,


In order to increase the performance of a script I'd like to sort very large vectors containing repeated integer values. 
I'm not interesting in having the values sorted, but only grouped.
I also need the equivalent of index.return from the standard "sort" function:

  f(c(10,1,10,100,1,10))

  =>

  grouped: c(10,10,10,1,1,100)
  ix:	  c(1,3,6,2,5,4)


is there a way to achieve this which would be faster than the standard sort function?

Thanks for any hints,

Marc Mamin



From giovanna.jonalasinio at uniroma1.it  Tue Nov 23 11:03:48 2004
From: giovanna.jonalasinio at uniroma1.it (giovanna jona lasinio)
Date: Tue, 23 Nov 2004 11:03:48 +0100
Subject: R: R: a question on plot(), workspace or method dispatch,
	NOT about: Re: [R] RWinEdt,
	other text editors and R2.01 a problem in pasting commands
In-Reply-To: <41A300A3.5010101@statistik.uni-dortmund.de>
Message-ID: <000901c4d143$b9febcf0$65036497@sta.uniroma1.it>

The problem I'm pointing out do not happen if I type commands directly
on the command window (as I wrote in the yesterday email) but it shows
when I source any ascii file containing R-command lines, even files I
used in several of my classes and always worked before.
I'm starting to suspect a virus however my antivirus didn't find
anything.
Yesterday I removed R2.0 and started over with R1.9 but nothing changed.

Well just now I tried, in a new workspace
x<-rnorm(30)
u<-c(510,520,530,540,550,600,610,620,630,640,650,700,710,720,730,740,750
,800,810,820,830,840,850,900,910,920,930,940,950,1000)
plot(x,xaxt="n",xlab="time",main="trying",ylab="x",type="l")
axis(1,1:30,labels=as.character(u))

Everything seems to work if I type on the command window, if sourcing
from a file with the same lines and if I cut and paste. Yesterday the
same happened for sometime on new and old workspaces, then the problem
started over again and this morning the problem was there always in new
and old workspaces... I really do not understand what is causing this
unstable behavior. I used to work under Windows NT and I moved to XP
recently then I do not already have a good knowledge of this OS and then
I do not know if the problem may depend on the R-XP interface.... Or
where to look...

Right now I installed only the basic R packages + Rwinedt packages, I'm
going to reinstall the other packages now...
Thanks for any help you can give me :)
Giovanna

-----Messaggio originale-----
Da: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de] 
Inviato: marted??, 23. novembre 2004 10:20
A: giovanna jona lasinio
Cc: r-help at stat.math.ethz.ch
Oggetto: Re: R: a question on plot(), workspace or method dispatch, NOT
about: Re: [R] RWinEdt, other text editors and R2.01 a problem in
pasting commands


giovanna jona lasinio wrote:

> I'm using R since version 1.6, and  I didn't redefine plot in my 
> workspace (I would never even dream of doing anything like that), I 
> didn't change any R-environment variable or option in the last 4 
> month. In my example x is a vector of 30 real values ...
> 
> I installed the windows service pack2 lately and I removed it now as 
> the microsoft help desk suggested, but the problem is still there and 
> it is there even with a new computer on which I installed R for the 
> first time yesterday. The real problem is that the copy-paste 
> procedure is not recognized any more on these two computers. If I try 
> to paste any command using other editors then WinEdt the problem is 
> still there it extends to almost all R commands, only assign and basic

> arithmetic is performed correctly

And the same command works when typed in the Console in exactly the same

way?

>  in the last two days I tried the following editors:
> 1. the display file window in the RGUI
> 2. RWinEdt
> 3. Xemacs
> 3. Word pad
> 4. Block notes
> With no changes.
> 
> Conclusion: How it may happen that the copy and paste procedure stop 
> working?

Please specify a reproducible example, i.e. the data, and the code! What
is the output of str(x)? Does it still happen when R has been started
with --vanilla?

Uwe Ligges


> Giovanna
> -----Messaggio originale-----
> Da: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de]
> Inviato: luned??, 22. novembre 2004 18:55
> A: giovanna jona lasinio
> Cc: r-help at stat.math.ethz.ch
> Oggetto: a question on plot(), workspace or method dispatch, NOT
about:
> Re: [R] RWinEdt, other text editors and R2.01 a problem in pasting
> commands
> 
> 
> giovanna jona lasinio wrote:
> 
> 
>>Dear All,
>>In the last few days I started using the last version of R as I
>>encountered a problem with R1.9 and the use of RWinEdt, however the 
>>problem shows with R2.01 as well. More precisely
>>
>>1. I install R2.01 after removing old verions of R
>>2. I Install RWinEdt package versione RWinEdt_1.6-2 and
>>SWinRegistry_0.3-2 and everything seems fine 3. I recall the RWinEdt 
>>library (library(RWinEdt)) and WinEdt appears 4. now after loading a 
>>workspace I run the following command using the paste icon in WinEdt:
>>plot(x,xaxt="n",xlab="time",
> 
> ylab=expression(alpha),main=nome,type="l")
> 
>>And I get the following error message:
>><<Error in stripchart(x, ...) : unused argument(s) (xaxt ...)>>
>>>From now on the plot command stops working, it doesn't recognize any
>>option not even type="l" or type="b" or anything.
>>
>>If I type the intrusction directly on the command window without any
>>previous paste operation from WinEdt the command works properly.
>>
>>Furthermore if I try to paste commands by simply coping it from a text
> 
> 
>>file and pasting it on the R command window I have the same problem. I
> 
> 
>>even tried:
>>source("prova.txt") containing the same command line and I got the
>>same
>>error:
>>Error in stripchart(x, ...) : unused argument(s) (xaxt ...)
>>
>>I'm not very good at using Windows XP, can anyone help?
>>
>>Thanks
>>Giovanna Jona Lasinio
> 
> 
> Please don't complain about R-WinEdt, WinEdt, other editors, or 
> Windows
> if the point is obviously somewhere else!
> 
> 1. Have you redefined plot() in your workspace?
> 2. What kind of object is x (in particular it's class, please!)?
> 
> Uwe Ligges
> 
> 
>



From ales.ziberna at guest.arnes.si  Tue Nov 23 11:31:16 2004
From: ales.ziberna at guest.arnes.si (=?windows-1250?Q?Ale=9A_=8Eiberna?=)
Date: Tue, 23 Nov 2004 11:31:16 +0100
Subject: [R] Weibull survival regression
References: <000001c4d129$a9086320$778f2bd9@youru989fckmtn>
Message-ID: <005e01c4d147$955e1270$1109f9c2@ales>

I do not belive there is a plot method for survreg object.

#You can plot survuval using the coeficents it returns:
curve(exp(-(exp(-lung.wbs$coef[1])*x)^(1/lung.wbs$scale)))

#Here is an exemple for "lung" data from library "survival"
library(survival)
data(lung)
lung.wbs <- survreg( Surv(time, status)~1, data=lung, dist='weibull')
plot(survfit(Surv(time,status),data=lung))
curve(exp(-(exp(-lung.wbs$coef[1])*x)^(1/lung.wbs$scale)),col="red",add=T)


----- Original Message ----- 
From: "Eric Lim" <eric.lim at cvsnet.org>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, November 23, 2004 7:56 AM
Subject: [R] Weibull survival regression


Dear R users,

Please can you help me with a relatively straightforward problem that I
am struggling with? I am simply trying to plot a baseline survivor and
hazard function for a simple data set of lung cancer survival where
`futime' is follow up time in months and status is 1=dead and 0=alive.

Using the survival package:

lung.wbs <- survreg( Surv(futime, status)~ 1, data=lung, dist='weibull')

plot (lung.wbs)

Returns the error msg:

Error in xy.coords(x, y, xlabel, ylabel, log) :
        x and y lengths differ

Using the Design package:

lung.wbd <- psm (Surv (futime, status)~ 1, dist="weibull", data=lung,
na.action=na.omit)

survplot(lung.wbd)

Returns the error msg:

Error in survplot.Design(lung.wbd) : fit does not have design
information

Using the eha package (I have not figured out how to get baseline
function only, but have used ht=0/1 hypertension as a covariate):

lung.wbe <- weibreg (Surv (futime, status)~ ht, y=T, data=audit,
na.action=na.omit)

plot (lung.wbe)

I get a plot with hazard (y) against age (x) ??? I cannot control the
axes with labeling and any other covariate gets the same plot.

I have tried using covariates in the Design and Survival packages, but
they always return the same error msgs. I realise that the former 2 are
AFT survival models, but even when I convert the object with pphsm() in
the Design package, I get the same errors.

Please can anyone enlighten me as to what I seem to be doing /
understanding wrongly?

Regards,

Eric Lim
Papworth Hospital
Cambridge, UK


           futime status
 [1,]  0.40000000      1
 [2,]  0.80000000      1
 [3,]  7.10000000      1
 [4,]  3.00000000      1
 [5,]  0.63333333      1
 [6,]  9.93333333      1
 [7,]  0.50000000      1
 [8,]  6.93333333      1
 [9,] 23.33333333      1
[10,]  1.73333333      1
[11,] 24.66666667      1
[12,]  0.06666667      1
[13,] 14.43333333      1
[14,]  8.83333333      0
[15,] 27.53333333      0



[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html



From ales.ziberna at guest.arnes.si  Tue Nov 23 11:32:18 2004
From: ales.ziberna at guest.arnes.si (=?windows-1250?Q?Ale=9A_=8Eiberna?=)
Date: Tue, 23 Nov 2004 11:32:18 +0100
Subject: [R] Weibull survival regression
References: <000001c4d129$a9086320$778f2bd9@youru989fckmtn>
Message-ID: <006701c4d147$b9c8e310$1109f9c2@ales>

PS: You can plot hazard in a similar way

Best regards,
Ale. .iberna

----- Original Message ----- 
From: "Eric Lim" <eric.lim at cvsnet.org>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, November 23, 2004 7:56 AM
Subject: [R] Weibull survival regression


Dear R users,

Please can you help me with a relatively straightforward problem that I
am struggling with? I am simply trying to plot a baseline survivor and
hazard function for a simple data set of lung cancer survival where
`futime' is follow up time in months and status is 1=dead and 0=alive.

Using the survival package:

lung.wbs <- survreg( Surv(futime, status)~ 1, data=lung, dist='weibull')

plot (lung.wbs)

Returns the error msg:

Error in xy.coords(x, y, xlabel, ylabel, log) :
        x and y lengths differ

Using the Design package:

lung.wbd <- psm (Surv (futime, status)~ 1, dist="weibull", data=lung,
na.action=na.omit)

survplot(lung.wbd)

Returns the error msg:

Error in survplot.Design(lung.wbd) : fit does not have design
information

Using the eha package (I have not figured out how to get baseline
function only, but have used ht=0/1 hypertension as a covariate):

lung.wbe <- weibreg (Surv (futime, status)~ ht, y=T, data=audit,
na.action=na.omit)

plot (lung.wbe)

I get a plot with hazard (y) against age (x) ??? I cannot control the
axes with labeling and any other covariate gets the same plot.

I have tried using covariates in the Design and Survival packages, but
they always return the same error msgs. I realise that the former 2 are
AFT survival models, but even when I convert the object with pphsm() in
the Design package, I get the same errors.

Please can anyone enlighten me as to what I seem to be doing /
understanding wrongly?

Regards,

Eric Lim
Papworth Hospital
Cambridge, UK


           futime status
 [1,]  0.40000000      1
 [2,]  0.80000000      1
 [3,]  7.10000000      1
 [4,]  3.00000000      1
 [5,]  0.63333333      1
 [6,]  9.93333333      1
 [7,]  0.50000000      1
 [8,]  6.93333333      1
 [9,] 23.33333333      1
[10,]  1.73333333      1
[11,] 24.66666667      1
[12,]  0.06666667      1
[13,] 14.43333333      1
[14,]  8.83333333      0
[15,] 27.53333333      0



[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From chabanet at dijon.inra.fr  Tue Nov 23 11:38:53 2004
From: chabanet at dijon.inra.fr (Claire Chabanet #3354)
Date: Tue, 23 Nov 2004 11:38:53 +0100 (CET)
Subject: [R] Re: adjacent category model in ordinal regression
Message-ID: <200411231038.iANAcrh21238@dijon.inra.fr>

Hello,

I suspect that vglm would do the job for the logit link.  Try :

     data(pneumo)
     pneumo$let = log(pneumo$exposure.time)
     vglm(cbind(normal, mild, severe) ~ let, multinomial, pneumo)

     vglm(cbind(normal, mild, severe) ~ let, acat, pneumo)

This is an adjacent category model with logit link, although the
help page speaks about log link (log of odds = logit). 
But I think the model is, in fact, the same as the previous one.
(same deviance ...). It seems that only the parametrisation is different.

Hope this help. 


>   I want to analyze some multinomial data. And the
> response has a natural ordinal sturcture. I want to
> fit a adjacent category model to the data by logit,
> probit and complementary log-log link functions. I
> found a package "VGAM"
> (www.stat.auckland.ac.nz/~yee/VGAM/ ) whose function
> "acat" can fit the adjacent category model but it only
> has "log" and "identity" link functions. Does anybody
> know there is a package or function can do the
> analysis I want?

Claire Chabanet



From marwan.khawaja at aub.edu.lb  Mon Nov 22 23:47:02 2004
From: marwan.khawaja at aub.edu.lb (Marwan Khawaja)
Date: Tue, 23 Nov 2004 00:47:02 +0200
Subject: [R] Search enginge
Message-ID: <E1CWX5L-0001dz-00@spool.aub.edu.lb>

Hello,
The search engine in Netscape 7.1 does not seem to work for me -- it used to
work fine before.  I do get the html help page but nothing happens when I enter
a keyword for search. 
Java plug in and it is enabled -- so this is not the problem. 
Any help would be appreciated.
Thanks Marwan

> version

 platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    0.1            
year     2004           
month    11             
day      15             
language R        

-------------------------------------------------------------------
Marwan Khawaja         http://staff.aub.edu.lb/~mk36/



From petr.pikal at precheza.cz  Tue Nov 23 11:50:57 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 23 Nov 2004 11:50:57 +0100
Subject: [R] Restore sources
In-Reply-To: <1101159795.41a25d7346303@ssl.mecca.ca>
References: <41A1BA95.25926.C8F763@localhost>
Message-ID: <41A32421.9553.9927F2@localhost>



On 22 Nov 2004 at 16:43, a_izad at atrico.com wrote:

> I didn't know about rules, I found Email link in search engine as I
> remember. Anyhow, thanks for answer. I did use Hitory, just I was
> wondering if there is a command to use it in R and bring up everything

On Windows each line of saved history can be listed by up/down 
arrow on command window, but I do not know any command 
which could invoke all previous commands from .Rhistory file. 
Therefore I mentioned text editor. I personally use Tinn-R 
(www.sciviews.org) to view, edit and send commands to R 
console.

Cheers
Petr


> for review, it could be helpful somtime.
> 
> Thanks again
> Mike
> 
> 
> Quoting Petr Pikal <petr.pikal at precheza.cz>:
> 
> > Hi
> > 
> > Well I did not see any reply to your question (which does not 
> > surprise me as you broke almost all rules from posting guide).
> > 
> > The answer partly depands on OS you use but your commands are 
> > probably stored in .Rhistory. You can view it by some text editor
> > (Notepad?). I recommend you to spend some time reading FAQs and
> > other documentation.
> > 
> > Cheers
> > Petr
> > 
> > 
> > 
> > On 20 Nov 2004 at 10:46, A. Izad wrote:
> > 
> > > How can I see my codes (syntax) after restoring a workspace?
> > > 
> > > Also " Summery"
> > > 
> > > 
> > > 
> > > tnx
> > > 
> > > Mike
> > > 
> > > 
> > > 
> > > 
> > > 
> > > 
> > >  [[alternative HTML version deleted]]
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > 
> > Petr Pikal
> > petr.pikal at precheza.cz
> > 
> > 
> > 
> 
> 

Petr Pikal
petr.pikal at precheza.cz



From petr.pikal at precheza.cz  Tue Nov 23 12:04:10 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 23 Nov 2004 12:04:10 +0100
Subject: R: a question on plot(), workspace or method dispatch,
	NOT about: Re: [R] RWinEdt, other text editors and R2.01 a problem i
In-Reply-To: <000301c4d13a$37e9bcf0$65036497@sta.uniroma1.it>
References: <41A227E3.4050305@statistik.uni-dortmund.de>
Message-ID: <41A3273A.32643.A53FF4@localhost>

Hi Giovanna

I have encountered such problem some time ago (so it had to be R 
1.8-1.9 approximately). It was connected with some complicated 
plot statements and usually also with **non english** characters in 
annotation texts. I did not found any traceable or repeatable 
pattern and sometime it helped to copy from my Salamander 
viewer window to notepad. But usually I had to retype a command 
in command window (not very positive answer :-(

Cheers
Petr

On 23 Nov 2004 at 9:55, giovanna jona lasinio wrote:

> I'm using R since version 1.6, and  I didn't redefine plot in my
> workspace (I would never even dream of doing anything like that), I
> didn't change any R-environment variable or option in the last 4
> month. In my example x is a vector of 30 real values ...
> 
> I installed the windows service pack2 lately and I removed it now as
> the microsoft help desk suggested, but the problem is still there and
> it is there even with a new computer on which I installed R for the
> first time yesterday. The real problem is that the copy-paste
> procedure is not recognized any more on these two computers. If I try
> to paste any command using other editors then WinEdt the problem is
> still there it extends to almost all R commands, only assign and basic
> arithmetic is performed correctly
> 
>  in the last two days I tried the following editors:
> 1. the display file window in the RGUI
> 2. RWinEdt
> 3. Xemacs
> 3. Word pad
> 4. Block notes
> With no changes.
> 
> Conclusion: How it may happen that the copy and paste procedure stop
> working?
> 
> Giovanna
> -----Messaggio originale-----
> Da: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de] 
> Inviato: luned??, 22. novembre 2004 18:55
> A: giovanna jona lasinio
> Cc: r-help at stat.math.ethz.ch
> Oggetto: a question on plot(), workspace or method dispatch, NOT
> about: Re: [R] RWinEdt, other text editors and R2.01 a problem in
> pasting commands
> 
> 
> giovanna jona lasinio wrote:
> 
> > Dear All,
> > In the last few days I started using the last version of R as I
> > encountered a problem with R1.9 and the use of RWinEdt, however the
> > problem shows with R2.01 as well. More precisely
> > 
> > 1. I install R2.01 after removing old verions of R
> > 2. I Install RWinEdt package versione RWinEdt_1.6-2 and 
> > SWinRegistry_0.3-2 and everything seems fine 3. I recall the RWinEdt
> > library (library(RWinEdt)) and WinEdt appears 4. now after loading a
> > workspace I run the following command using the paste icon in
> > WinEdt: plot(x,xaxt="n",xlab="time",
> ylab=expression(alpha),main=nome,type="l")
> > 
> > And I get the following error message:
> > <<Error in stripchart(x, ...) : unused argument(s) (xaxt ...)>>
> >>From now on the plot command stops working, it doesn't recognize any
> > option not even type="l" or type="b" or anything.
> > 
> > If I type the intrusction directly on the command window without any
> > previous paste operation from WinEdt the command works properly.
> > 
> > Furthermore if I try to paste commands by simply coping it from a
> > text
> 
> > file and pasting it on the R command window I have the same problem.
> > I
> 
> > even tried:
> > source("prova.txt") containing the same command line and I got the
> > same error: Error in stripchart(x, ...) : unused argument(s) (xaxt
> > ...)
> > 
> > I'm not very good at using Windows XP, can anyone help?
> > 
> > Thanks
> > Giovanna Jona Lasinio
> 
> Please don't complain about R-WinEdt, WinEdt, other editors, or
> Windows if the point is obviously somewhere else!
> 
> 1. Have you redefined plot() in your workspace?
> 2. What kind of object is x (in particular it's class, please!)?
> 
> Uwe Ligges
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From giovanna.jonalasinio at uniroma1.it  Tue Nov 23 12:10:24 2004
From: giovanna.jonalasinio at uniroma1.it (giovanna jona lasinio)
Date: Tue, 23 Nov 2004 12:10:24 +0100
Subject: R: R: a question on plot(), workspace or method dispatch,
	NOT about: Re: [R] RWinEdt, other text editors and R2.01 a problem i
In-Reply-To: <41A3273A.32643.A53FF4@localhost>
Message-ID: <000b01c4d14d$07d2f7d0$65036497@sta.uniroma1.it>

Thanks, at least this crazy behavior is not my exclusive problem :-)
I think that if we understand how the OS operates when copying and
pasting we may find a solution...

-----Messaggio originale-----
Da: Petr Pikal [mailto:petr.pikal at precheza.cz] 
Inviato: marted, 23. novembre 2004 12:04
A: giovanna jona lasinio; r-help-bounces at stat.math.ethz.ch; r-help at stat.
math.ethz.ch
Oggetto: Re: R: a question on plot(), workspace or method dispatch, NOT
about: Re: [R] RWinEdt, other text editors and R2.01 a problem i


Hi Giovanna

I have encountered such problem some time ago (so it had to be R 
1.8-1.9 approximately). It was connected with some complicated 
plot statements and usually also with **non english** characters in 
annotation texts. I did not found any traceable or repeatable 
pattern and sometime it helped to copy from my Salamander 
viewer window to notepad. But usually I had to retype a command 
in command window (not very positive answer :-(

Cheers
Petr

On 23 Nov 2004 at 9:55, giovanna jona lasinio wrote:

> I'm using R since version 1.6, and  I didn't redefine plot in my 
> workspace (I would never even dream of doing anything like that), I 
> didn't change any R-environment variable or option in the last 4 
> month. In my example x is a vector of 30 real values ...
> 
> I installed the windows service pack2 lately and I removed it now as 
> the microsoft help desk suggested, but the problem is still there and 
> it is there even with a new computer on which I installed R for the 
> first time yesterday. The real problem is that the copy-paste 
> procedure is not recognized any more on these two computers. If I try 
> to paste any command using other editors then WinEdt the problem is 
> still there it extends to almost all R commands, only assign and basic

> arithmetic is performed correctly
> 
>  in the last two days I tried the following editors:
> 1. the display file window in the RGUI
> 2. RWinEdt
> 3. Xemacs
> 3. Word pad
> 4. Block notes
> With no changes.
> 
> Conclusion: How it may happen that the copy and paste procedure stop 
> working?
> 
> Giovanna
> -----Messaggio originale-----
> Da: Uwe Ligges [mailto:ligges at statistik.uni-dortmund.de]
> Inviato: luned, 22. novembre 2004 18:55
> A: giovanna jona lasinio
> Cc: r-help at stat.math.ethz.ch
> Oggetto: a question on plot(), workspace or method dispatch, NOT
> about: Re: [R] RWinEdt, other text editors and R2.01 a problem in
> pasting commands
> 
> 
> giovanna jona lasinio wrote:
> 
> > Dear All,
> > In the last few days I started using the last version of R as I 
> > encountered a problem with R1.9 and the use of RWinEdt, however the 
> > problem shows with R2.01 as well. More precisely
> > 
> > 1. I install R2.01 after removing old verions of R
> > 2. I Install RWinEdt package versione RWinEdt_1.6-2 and
> > SWinRegistry_0.3-2 and everything seems fine 3. I recall the RWinEdt
> > library (library(RWinEdt)) and WinEdt appears 4. now after loading a
> > workspace I run the following command using the paste icon in
> > WinEdt: plot(x,xaxt="n",xlab="time",
> ylab=expression(alpha),main=nome,type="l")
> > 
> > And I get the following error message:
> > <<Error in stripchart(x, ...) : unused argument(s) (xaxt ...)>>
> >>From now on the plot command stops working, it doesn't recognize any
> > option not even type="l" or type="b" or anything.
> > 
> > If I type the intrusction directly on the command window without any

> > previous paste operation from WinEdt the command works properly.
> > 
> > Furthermore if I try to paste commands by simply coping it from a 
> > text
> 
> > file and pasting it on the R command window I have the same problem.

> > I
> 
> > even tried:
> > source("prova.txt") containing the same command line and I got the 
> > same error: Error in stripchart(x, ...) : unused argument(s) (xaxt
> > ...)
> > 
> > I'm not very good at using Windows XP, can anyone help?
> > 
> > Thanks
> > Giovanna Jona Lasinio
> 
> Please don't complain about R-WinEdt, WinEdt, other editors, or 
> Windows if the point is obviously somewhere else!
> 
> 1. Have you redefined plot() in your workspace?
> 2. What kind of object is x (in particular it's class, please!)?
> 
> Uwe Ligges
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From ripley at stats.ox.ac.uk  Tue Nov 23 12:31:24 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 23 Nov 2004 11:31:24 +0000 (GMT)
Subject: [R] Restore sources
In-Reply-To: <41A32421.9553.9927F2@localhost>
References: <41A1BA95.25926.C8F763@localhost> <41A32421.9553.9927F2@localhost>
Message-ID: <Pine.LNX.4.61.0411231129340.24609@gannet.stats>

On Tue, 23 Nov 2004, Petr Pikal wrote:

> On 22 Nov 2004 at 16:43, a_izad at atrico.com wrote:
>
>> I didn't know about rules, I found Email link in search engine as I
>> remember. Anyhow, thanks for answer. I did use Hitory, just I was
>> wondering if there is a command to use it in R and bring up everything
>> for review, it could be helpful somtime.

> On Windows each line of saved history can be listed by up/down
> arrow on command window, but I do not know any command
> which could invoke all previous commands from .Rhistory file.
> Therefore I mentioned text editor. I personally use Tinn-R
> (www.sciviews.org) to view, edit and send commands to R
> console.

R for Windows (>= 2.0.0) has a script editor for just that purpose: just 
open .Rhistory in the script editor.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Nov 23 12:35:42 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 23 Nov 2004 11:35:42 +0000 (GMT)
Subject: [R] Search enginge
In-Reply-To: <E1CWX5L-0001dz-00@spool.aub.edu.lb>
References: <E1CWX5L-0001dz-00@spool.aub.edu.lb>
Message-ID: <Pine.LNX.4.61.0411231131360.24609@gannet.stats>

On Tue, 23 Nov 2004, Marwan Khawaja wrote:

> The search engine in Netscape 7.1 does not seem to work for me -- it used to
> work fine before.

Perhaps you need to undo what you changed, but we can't guess what that is.

> I do get the html help page but nothing happens when I enter
> a keyword for search.
> Java plug in and it is enabled -- so this is not the problem.

> Any help would be appreciated.

That page contains a link to the description in the R-admin manual.
In particular, do you have the current Java jre 1.5.0?

We did test Netscape 7.2 (7.1 is obselete) under Windows with R 
2.0.1-to-be.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Tue Nov 23 12:41:12 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 23 Nov 2004 12:41:12 +0100
Subject: [R] sorting without order
In-Reply-To: <A03188C6623C0D46A703CB5AA59907F201C11C59@JENMAIL01.ad.intershop.net>
References: <A03188C6623C0D46A703CB5AA59907F201C11C59@JENMAIL01.ad.intershop.net>
Message-ID: <x21xek3i93.fsf@biostat.ku.dk>

"Marc Mamin" <M.Mamin at intershop.de> writes:

> Hello,
> 
> 
> In order to increase the performance of a script I'd like to sort very large vectors containing repeated integer values. 
> I'm not interesting in having the values sorted, but only grouped.
> I also need the equivalent of index.return from the standard "sort" function:
> 
>   f(c(10,1,10,100,1,10))
> 
>   =>
> 
>   grouped: c(10,10,10,1,1,100)
>   ix:	  c(1,3,6,2,5,4)
> 
> 
> is there a way to achieve this which would be faster than the standard sort function?
> 
> Thanks for any hints,

Here's one way:

v <- c(10,1,10,100,1,10)
ix <- do.call("c",split(seq(along=v),v))
grouped <- v[ix]

Not sure about the speed though. Should be O(N) if the number of
groups is small, but the multiplier could be large because of various
formalities (such as adding names to ix).


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From francoisromain at free.fr  Tue Nov 23 12:55:28 2004
From: francoisromain at free.fr (francoisromain@free.fr)
Date: Tue, 23 Nov 2004 12:55:28 +0100
Subject: [R] Mode?
In-Reply-To: <41A2F63A.1000608@statistik.uni-dortmund.de>
References: <CF99BBA00A3F5D4183C2FA33089FBC3403788434@EX01.staff.smu.edu.sg>
	<41A2F63A.1000608@statistik.uni-dortmund.de>
Message-ID: <1101210928.41a32530796b3@imp1-q.free.fr>

Yhe question of finding the mode of a dataset has been discussed previously in
the list not only for categorical data :

search "How can I get the mode" in the search engine of the list.

Hope this helps.

Romain.

Selon Uwe Ligges <ligges at statistik.uni-dortmund.de>:

> LONG Yu wrote:
>
> > Dear all,
> >
> > I want to find out the mode for a data set, anyone knows how to do it in
> > R?
> > I tried the codes below, but it seems too long:
> >
> > tt<-table(data1)         #get the frequency tables of data1
> > oo<-order(tt);            #get the order of frequencies
> > len<-length(tt)           #the length of the tables
> > mm<-oo[len];            #the last number of oo is the position
> > tt[mm]                     #we can get the mode
> >
> >
> > Thank you everybody first!
> >
> > Regards,
> > Long Yu
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
>
> See ?which.max
>
> Uwe Ligges
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From gb at stat.umu.se  Tue Nov 23 12:59:40 2004
From: gb at stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Tue, 23 Nov 2004 12:59:40 +0100
Subject: [R] Weibull survival regression
In-Reply-To: <000001c4d129$a9086320$778f2bd9@youru989fckmtn>
References: <000001c4d129$a9086320$778f2bd9@youru989fckmtn>
Message-ID: <20041123115940.GA859@tal.stat.umu.se>

On Tue, Nov 23, 2004 at 06:56:56AM -0000, Eric Lim wrote:
> Dear R users,
>  
> Please can you help me with a relatively straightforward problem that I
> am struggling with? I am simply trying to plot a baseline survivor and
> hazard function for a simple data set of lung cancer survival where
> `futime' is follow up time in months and status is 1=dead and 0=alive.
>  
> Using the survival package:
>  
> lung.wbs <- survreg( Surv(futime, status)~ 1, data=lung, dist='weibull')
>  
> plot (lung.wbs)
>  
> Returns the error msg:
>  
> Error in xy.coords(x, y, xlabel, ylabel, log) : 
>         x and y lengths differ
>  
> Using the Design package:
>  
> lung.wbd <- psm (Surv (futime, status)~ 1, dist="weibull", data=lung,
> na.action=na.omit)
>  
> survplot(lung.wbd)
>  
> Returns the error msg:
>  
> Error in survplot.Design(lung.wbd) : fit does not have design
> information
>  
> Using the eha package (I have not figured out how to get baseline
> function only, but have used ht=0/1 hypertension as a covariate):
>  
> lung.wbe <- weibreg (Surv (futime, status)~ ht, y=T, data=audit,
> na.action=na.omit)
>  
> plot (lung.wbe)
>  
> I get a plot with hazard (y) against age (x) ??? I cannot control the
> axes with labeling and any other covariate gets the same plot.

Right, plot.weibreg in eha is very poorly implemented, newdata does not
work, you cannot change labels of the plot, etc. I hope to get time to do
it right soon, but...

Anyway, the problem is simple to solve: You get the estimates of scale and
shape of the Weibull distribution (or their logarithms). Since eha uses
"standard R parametrization", you can simply plot the functions 
"dweibull / pweibull( , lower.tail = FALSE)" with the estimated parameters
inserted for the hazard function, and "pweibull( , lower.tail = FALSE)" for
the survival function.

[...]

G??ran


-- 
 G??ran Brostr??m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume?? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume??, Sweden             e-mail: gb at stat.umu.se



From f.harrell at vanderbilt.edu  Tue Nov 23 13:11:22 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 23 Nov 2004 07:11:22 -0500
Subject: [R] Weibull survival regression
In-Reply-To: <000001c4d129$a9086320$778f2bd9@youru989fckmtn>
References: <000001c4d129$a9086320$778f2bd9@youru989fckmtn>
Message-ID: <41A328EA.1070903@vanderbilt.edu>

Eric Lim wrote:
> Dear R users,
>  
> Please can you help me with a relatively straightforward problem that I
> am struggling with? I am simply trying to plot a baseline survivor and
> hazard function for a simple data set of lung cancer survival where
> `futime' is follow up time in months and status is 1=dead and 0=alive.
>  
> Using the survival package:
>  
> lung.wbs <- survreg( Surv(futime, status)~ 1, data=lung, dist='weibull')
>  
> plot (lung.wbs)
>  
> Returns the error msg:
>  
> Error in xy.coords(x, y, xlabel, ylabel, log) : 
>         x and y lengths differ
>  
> Using the Design package:
>  
> lung.wbd <- psm (Surv (futime, status)~ 1, dist="weibull", data=lung,
> na.action=na.omit)

You don't need to specify na.action here.

>  
> survplot(lung.wbd)
>  
> Returns the error msg:
>  
> Error in survplot.Design(lung.wbd) : fit does not have design
> information

survplot only works when there is at least one covariate.  Sorry.  Maybe 
someday ...

  -Frank Harrell

> Regards,
>  
> Eric Lim
> Papworth Hospital
> Cambridge, UK

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From edd at debian.org  Tue Nov 23 13:22:23 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 23 Nov 2004 06:22:23 -0600
Subject: [R] Re: R problems [Debian testing]
In-Reply-To: <20041123120414.51253.qmail@web51309.mail.yahoo.com>
References: <20041123120414.51253.qmail@web51309.mail.yahoo.com>
Message-ID: <20041123122223.GA8398@sonny.eddelbuettel.com>

Zoltan,

On Tue, Nov 23, 2004 at 04:04:14AM -0800, Zoltan Nagy wrote:
> Hi,
> 
> I wrote you this email because I have some problems
> with
> the R library's. I use R Version 2.0.0 and it cannot
> load
> some libraries, eg:
> library();
> MASS                    ** No title available
> (pre-2.0.0 install?) **

You need a version of the r-cran-vr package compiled under R 2.0.0. These
have been out for weeks, but will only migrate from Debian unstable to Debian
testing once the most current package is 
-- at least ten days old without any bug reports
-- compiled on all architectures
-- and these two conditions hold for all packages it depends on.

The combination of these three requirements has prevented the package from
migrating to testing.  It will, however, work there -- so in the interim,
please install the package manually from 

       http://packages.debian.org/r-cran-vr
       
and likewise for other packages you may need.  The "apt-pinning" method is
nice too, there are some HOWTOs available.

I'm sorry for the inconvenience. I'll cc r-help as other people may have the
same issue.

Regards,  Dirk

> Will this problem are solved in the near future, or
> shall I use an older (stable) version?
> Thank you.
> Best regards,
> 
> Zoltan Nagy
> 
> 
> 		
> __________________________________ 
>  

-- 
If your hair is standing up, then you are in extreme danger.
      -- http://www.usafa.af.mil/dfp/cockpit-phys/fp1ex3.htm



From marwan.khawaja at aub.edu.lb  Tue Nov 23 01:47:08 2004
From: marwan.khawaja at aub.edu.lb (Marwan Khawaja)
Date: Tue, 23 Nov 2004 02:47:08 +0200
Subject: [R] Search enginge
In-Reply-To: <Pine.LNX.4.61.0411231131360.24609@gannet.stats>
Message-ID: <E1CWYxZ-000285-00@spool.aub.edu.lb>

Hi 
The problem was with my Netscape 7.1.
Many thanks to Prof Ripley for pointing me to the right direction.

Best Marwan
 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Prof 
> Brian Ripley
> Sent: Tuesday, November 23, 2004 1:36 PM
> To: Marwan Khawaja
> Cc: R
> Subject: Re: [R] Search enginge
> 
> On Tue, 23 Nov 2004, Marwan Khawaja wrote:
> 
> > The search engine in Netscape 7.1 does not seem to work for 
> me -- it 
> > used to work fine before.
> 
> Perhaps you need to undo what you changed, but we can't guess 
> what that is.
> 
> > I do get the html help page but nothing happens when I 
> enter a keyword 
> > for search.
> > Java plug in and it is enabled -- so this is not the problem.
> 
> > Any help would be appreciated.
> 
> That page contains a link to the description in the R-admin manual.
> In particular, do you have the current Java jre 1.5.0?
> 
> We did test Netscape 7.2 (7.1 is obselete) under Windows with 
> R 2.0.1-to-be.
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From dimitris.rizopoulos at med.kuleuven.ac.be  Tue Nov 23 13:52:52 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Tue, 23 Nov 2004 13:52:52 +0100
Subject: [R] sorting without order
References: <A03188C6623C0D46A703CB5AA59907F201C11C59@JENMAIL01.ad.intershop.net>
Message-ID: <00f901c4d15b$58813b70$0540210a@www.domain>

Hi Marc,

continuing on Prof. Dalgaard's proposal, you could use:

ix <- unlist(split(seq(along=v), v), use.names=FALSE)

but even with this, `sort()' seems faster if you are interseted only 
in grouping:

v <- sample(1:25000, 50000, TRUE)
######
system.time(ix <- do.call("c",split(seq(along=v),v)), gcFirst=TRUE)
[1] 0.13 0.00 0.13   NA   NA

system.time(ix <- unlist(split(seq(along=v), v), use.names=FALSE), 
gcFirst=TRUE)
[1] 0.06 0.00 0.07   NA   NA

system.time(x <- sort(v), gcFirst=TRUE)
[1] 0.01 0.00 0.02   NA   NA


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "Marc Mamin" <M.Mamin at intershop.de>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, November 23, 2004 10:58 AM
Subject: [R] sorting without order


> Hello,
>
>
> In order to increase the performance of a script I'd like to sort 
> very large vectors containing repeated integer values.
> I'm not interesting in having the values sorted, but only grouped.
> I also need the equivalent of index.return from the standard "sort" 
> function:
>
>  f(c(10,1,10,100,1,10))
>
>  =>
>
>  grouped: c(10,10,10,1,1,100)
>  ix:   c(1,3,6,2,5,4)
>
>
> is there a way to achieve this which would be faster than the 
> standard sort function?
>
> Thanks for any hints,
>
> Marc Mamin
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Tue Nov 23 13:58:30 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 23 Nov 2004 12:58:30 +0000 (GMT)
Subject: [R] sorting without order
In-Reply-To: <x21xek3i93.fsf@biostat.ku.dk>
References: <A03188C6623C0D46A703CB5AA59907F201C11C59@JENMAIL01.ad.intershop.net>
	<x21xek3i93.fsf@biostat.ku.dk>
Message-ID: <Pine.LNX.4.61.0411231249210.25709@gannet.stats>

On Tue, 23 Nov 2004, Peter Dalgaard wrote:

> "Marc Mamin" <M.Mamin at intershop.de> writes:
>
>> Hello,
>>
>>
>> In order to increase the performance of a script I'd like to sort very large vectors containing repeated integer values.
>> I'm not interesting in having the values sorted, but only grouped.
>> I also need the equivalent of index.return from the standard "sort" function:
>>
>>   f(c(10,1,10,100,1,10))
>>
>>   =>
>>
>>   grouped: c(10,10,10,1,1,100)
>>   ix:	  c(1,3,6,2,5,4)
>>
>>
>> is there a way to achieve this which would be faster than the standard sort function?
>>
>> Thanks for any hints,
>
> Here's one way:
>
> v <- c(10,1,10,100,1,10)
> ix <- do.call("c",split(seq(along=v),v))
> grouped <- v[ix]
>
> Not sure about the speed though. Should be O(N) if the number of
> groups is small, but the multiplier could be large because of various
> formalities (such as adding names to ix).

Radix sorting as implemented in sort.list will be hard to beat:

ix <- sort.list(unclass(factor(v)), method="radix")

although in your case as.integer() will do.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From partha_bagchi at hgsi.com  Tue Nov 23 13:53:30 2004
From: partha_bagchi at hgsi.com (partha_bagchi@hgsi.com)
Date: Tue, 23 Nov 2004 07:53:30 -0500
Subject: [R] Batch
Message-ID: <OF518C789E.7EDCE1F2-ON85256F55.004607EE-85256F55.0046D0F4@hgsi.com>

You haven't given much detail to go on. So I have to make some assumptions 
(feel free to correct me if I am wrong):

1. You already know how to plot something in R from the R command line.
2. You know how to save your plotting commands in a text file (say 
fancyplots.R).

If these are true, then say you define 
$r_file = "/path/to/fancyplots.R";

Let
$Rbin = "/usr/local/R/bin/R"; #I am assuming you are on *nix

Then this should work (untested - so use at your own risk!!)[modify to 
your needs]:

  open(R, "| $Rbin --no-save > temp.out");
  print R "source(\"$r_file\")\n";
  close R;

Obviously there may be other sophisticated ways to do this.

HTH,
Partha





"Claudia Paladini" <claudiapaladini at web.de>
Sent by: r-help-bounces at stat.math.ethz.ch
11/23/2004 03:54 AM

 
        To:     R-help at stat.math.ethz.ch
        cc: 
        Subject:        [R] Batch


Content-Type: text/plain; charset="iso-8859-1"
Content-Transfer-Encoding: 7bit
Received-SPF: none (hypatia: domain of claudiapaladini at web.de does not 
designate permitted sender hosts)
X-Virus-Scanned: by amavisd-new at stat.math.ethz.ch
X-Spam-Checker-Version: SpamAssassin 3.0.1 (2004-10-22) on 
hypatia.math.ethz.ch
X-Spam-Level:
X-Spam-Status: No, score=0.0 required=5.0 tests=BAYES_50 autolearn=no 
version=3.0.1

Dear ladies and gentlemen,
I want to automatise my data-analysis with R. What I exactally want to do, 
is to start  R - functions out of a perl-skript and do some batch 
plotting.
But I have no idea how to realise  this. Can anybody help me?
Thank you very much.
With kind regards

Claudia Paladini
__________________________________________________________
Mit WEB.DE FreePhone mit hoechster Qualitaet ab 0 Ct./Min.

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From sunny.ho at gmail.com  Tue Nov 23 14:16:16 2004
From: sunny.ho at gmail.com (Sunny Ho)
Date: Tue, 23 Nov 2004 21:16:16 +0800
Subject: [R] ROracle: fetch return zero rows or empty dataset (a workaround!)
Message-ID: <ba0addaf041123051679591ca7@mail.gmail.com>

Hi All,

If you had run into the problem with ROracle on Linux: "fetch()
returns zero rows or empty dataset", here is an easy & safe
work-around for you to try out. It works for me, and very likely it
will work for you too. You must have root privilege to do this on your
machine.

Quick Instructions
Part 1 - Check to see if this is the work-around for you
a) log in as "root"
b) cd /usr/include
c) grep sqlerrml sqlca.h
   NOTE: If you see something like below, then bingo! This is the
work-around for you!!
   int              sqlerrml;
Part 2 - Re-install ROracle properly
a) log in as "root"
b) cd /usr/include
c) mv sqlca.h sqlca.h.xxx
   NOTE: This is just to rename the file temporarily
d) Follow all the original steps to re-install your "ROracle" package.
   NOTE: For my system "WhiteBox Linux (Respin)" on x86 hardware with
Oracle 10g and R.2.0, I used:
      R CMD INSTALL --configure-args='--enable-extralibs
--with-oracle=10' ROracle_0.5-5.tar.gz
   NOTE: You should read the ROracle/inst/README* files if you have not.
e) After ROracle is successfully installed, restore the sqlca.h file:
   cd /usr/include
   mv sqlca.h.xxx sqlca.h
f) All done! Drop a note here for the others to share if your ROracle
works now, or if you
   have a better work-around.

---

Technical details for those who are interested...
First, I believe your Linux should also have "PostGreSQL" installed.
Otherwise, you may not run into this problem at all!  I'm also a
victim annoyed by this problem for months.  After searching and
waiting for months, I see no one seems to know what is wrong, so I
decided to trouble-shooting the problem myself.  After a lot of
investigation and debugging, I found that the ROracle Pro*C module
does not get a proper return code in sqlca.sqlerrd[2].  Those who know
embedded SQL should know that this field gives the number of "rows
returned from the database".  In fact, the return code is returned to
a wrong place: sqlca.sqlerrd[1]. This problem is a result of a
non-matching data definition of the data structure "sqlca" in ORACLE &
POSTGRESQL.

In oracle, sqlca is defined in $ORACLE_HOME/precomp/public/sqlca.h, as:
struct   sqlca
{
         /* ub1 */ char    sqlcaid[8];
         /* b4  */ int     sqlabc;
         /* b4  */ int     sqlcode;
         struct
           {
           /* ub2 */ unsigned short sqlerrml;
           /* ub1 */ char           sqlerrmc[70];
           } sqlerrm;
         /* ub1 */ char    sqlerrp[8];
         /* b4  */ int     sqlerrd[6];
         /* ub1 */ char    sqlwarn[8];
         /* ub1 */ char    sqlext[8];
};

In rh-postgresql-devel-7.3.6-1 package, it defines sqlca in
/usr/include/sqlca.h, as:
#define SQLERRMC_LEN    70
struct sqlca
{
        char            sqlcaid[8];
        long            sqlabc;
        long            sqlcode;
        struct
        {
                int             sqlerrml;
                char            sqlerrmc[SQLERRMC_LEN];
        }                       sqlerrm;
        char            sqlerrp[8];
        long            sqlerrd[6];
        char            sqlwarn[8];
        char            sqlext[8];
};

The difference in "sqlerrml" causes the subsequent component "sqlerrd"
shifted by 4 bytes. When the ROracle package is being installed, it
runs Oracle Pro*C to compile the source code. Unfortunately, the Pro*C
include search path causes it to pick up /usr/include/sqlca.h, instead
of the correct one in $ORACLE_HOME/precomp/public/sqlca.h, therefore
the ROracle module uses the incorrect sqlca and always picks up a
"zero" as the number of returned rows. The workaround above forces
ROracle to use the sqlca.h from Oracle, hence it should work well with
Oracle Embedded SQL.

For the code maintainer, I have no idea of whether Oracle and
PostGreSQL should use the same "sqlca", or who's right who's wrong.
But I believe this is a problem of PostGreSQL & Oracle co-exisitence,
but not a R problem.  My work-around just helps me to get my ROracle
working. I hope that those who have good insight into PostGreSQL and
Oracle could provide some ideas of what we should do to get this
PostGreSQL & Oracle problem fixed in Linux.

Regards,
Sunny Ho  (sunny.ho at gmail.com)



From andreas.wolf at uni-jena.de  Tue Nov 23 14:42:55 2004
From: andreas.wolf at uni-jena.de (Andreas Wolf)
Date: Tue, 23 Nov 2004 14:42:55 +0100
Subject: [R] number of pairwise present data in matrix with missings
Message-ID: <AF874507D8BE4945941321229E2AAF3906BE86@cat.app.metpsy.uni-jena.de>

is there a smart way of determining the number of pairwise present data
in a data matrix with missings (maybe as a by-product of some
statistical function?)

so far, i used several loops like:

for (column1 in 1:99) {
  for (column2 in 2:100) {
    for (row in 1:500) {
      if (!is.na(matrix[row,column1]) & !is.na(matrix[row,column2])) {
        pairs[col1,col2] <- pairs[col1,col2]+1
      }
    }
  }
}

but this seems neither the most elegant nor an utterly fast solution.

thanks for suggestions.
andreas wolf



From ripley at stats.ox.ac.uk  Tue Nov 23 14:46:10 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 23 Nov 2004 13:46:10 +0000 (GMT)
Subject: [R] sorting without order
In-Reply-To: <00f901c4d15b$58813b70$0540210a@www.domain>
References: <A03188C6623C0D46A703CB5AA59907F201C11C59@JENMAIL01.ad.intershop.net>
	<00f901c4d15b$58813b70$0540210a@www.domain>
Message-ID: <Pine.LNX.4.61.0411231341520.26648@gannet.stats>

Try a `very long' vector as originally specified:

> v <- sample(1:25000, 1e6, TRUE)
> system.time(ix <- sort.list(v, method="radix"), gcFirst=TRUE)
[1] 0.14 0.01 0.15 0.00 0.00
> system.time(x <- sort(v), gcFirst=TRUE)
[1] 0.42 0.02 0.44 0.00 0.00
> system.time(x <- sort(v, method="quick", index.return=TRUE), gcFirst=TRUE)
[1] 0.27 0.03 0.30 0.00 0.00
> system.time(ix <- unlist(split(seq(along=v), v), use.names=FALSE),
+ gcFirst=TRUE)
[1] 1.18 0.11 1.30 0.00 0.00

so sort can be beatened quite easily, even on a level playing field.


On Tue, 23 Nov 2004, Dimitris Rizopoulos wrote:

> Hi Marc,
>
> continuing on Prof. Dalgaard's proposal, you could use:
>
> ix <- unlist(split(seq(along=v), v), use.names=FALSE)
>
> but even with this, `sort()' seems faster if you are interseted only in 
> grouping:
>
> v <- sample(1:25000, 50000, TRUE)
> ######
> system.time(ix <- do.call("c",split(seq(along=v),v)), gcFirst=TRUE)
> [1] 0.13 0.00 0.13   NA   NA
>
> system.time(ix <- unlist(split(seq(along=v), v), use.names=FALSE), 
> gcFirst=TRUE)
> [1] 0.06 0.00 0.07   NA   NA
>
> system.time(x <- sort(v), gcFirst=TRUE)
> [1] 0.01 0.00 0.02   NA   NA
>
>
> I hope it helps.
>
> Best,
> Dimitris
>
> ----
> Dimitris Rizopoulos
> Ph.D. Student
> Biostatistical Centre
> School of Public Health
> Catholic University of Leuven
>
> Address: Kapucijnenvoer 35, Leuven, Belgium
> Tel: +32/16/336899
> Fax: +32/16/337015
> Web: http://www.med.kuleuven.ac.be/biostat
>    http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
>
>
>
> ----- Original Message ----- From: "Marc Mamin" <M.Mamin at intershop.de>
> To: <r-help at stat.math.ethz.ch>
> Sent: Tuesday, November 23, 2004 10:58 AM
> Subject: [R] sorting without order
>
>
>> Hello,
>> 
>> 
>> In order to increase the performance of a script I'd like to sort very 
>> large vectors containing repeated integer values.
>> I'm not interesting in having the values sorted, but only grouped.
>> I also need the equivalent of index.return from the standard "sort" 
>> function:
>> 
>>  f(c(10,1,10,100,1,10))
>> 
>>  =>
>> 
>>  grouped: c(10,10,10,1,1,100)
>>  ix:   c(1,3,6,2,5,4)
>> 
>> 
>> is there a way to achieve this which would be faster than the standard sort 
>> function?
>> 
>> Thanks for any hints,
>> 
>> Marc Mamin
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>> 
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jfox at mcmaster.ca  Tue Nov 23 14:48:28 2004
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 23 Nov 2004 08:48:28 -0500
Subject: [R] The hidden costs of GPL software?
In-Reply-To: <jtq3q0tu5fjd6lbf12f8rj5r3n9m93paqh@4ax.com>
Message-ID: <20041123134825.SNEN1863.tomts10-srv.bellnexxia.net@JohnDesktop8300>

Dear Duncan,

I don't think that there is an automatic, nearly costless way of providing
an effective solution to locating R resources. The problem seems to me to be
analogous to indexing a book. There's an excellent description of what that
process *should* look like in the Chicago Manual of Style, and it's a lot of
work. In my experience, most book indexes are quite poor, and automatically
generated indexes, while not useless, are even worse, since one should index
concepts, not words. The ideal indexer is therefore the author of the book.

I guess that the question boils down to how important is it to provide an
analogue of a good index to R? As I said in a previous message, I believe
that the current search facilities work pretty well -- about as well as one
could expect of an automatic approach. I don't believe that there's an
effective centralized solution, so doing something more ambitious than is
currently available implies farming out the process to package authors. Of
course, there's no guarantee that all package authors will be diligent
indexers. 

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Duncan Murdoch
> Sent: Monday, November 22, 2004 8:55 AM
> To: Cliff Lunneborg
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] The hidden costs of GPL software?
> 
> On Fri, 19 Nov 2004 13:59:23 -0800, "Cliff Lunneborg"
> <cliff at ms.washington.edu> quoted John Fox:
> 
> >Why not, as previously has been proposed, replace the current static 
> >(and, in my view, not very useful) set of keywords in R 
> documentation 
> >with the requirement that package authors supply their own 
> keywords for 
> >each documented object? I believe that this is the intent of the 
> >concept entries in Rd files, but their use certainly is not 
> required or 
> >even actively encouraged. (They're just mentioned in passing in the 
> >Writing R Extensions manual.
> 
> That would not be easy and won't happen quickly.  There are some
> problems:
> 
>  - The base packages mostly don't use  \concept. (E.g. base 
> has 365 man pages, only about 15 of them use it).  Adding it 
> to each file is a fairly time-consuming task.
> 
> - Before we started, we'd need to agree as to what they are for.
> Right now, I think they are mainly used when the name of a 
> concept doesn't match the name of the function that 
> implements it, e.g.
> "modulo", "remainder", "promise", "argmin", "assertion".  The 
> need for this usage is pretty rare.  If they were used for 
> everything, what would they contain?
> 
>  - Keywording in a useful way is hard.  There are spelling 
> issues (e.g. optimise versus optimize); our fuzzy matching 
> helps with those.
> But there are also multiple names for the same thing, and 
> multiple meanings for the same name.
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Tue Nov 23 15:06:12 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 23 Nov 2004 14:06:12 +0000 (UTC)
Subject: [R] number of pairwise present data in matrix with missings
References: <AF874507D8BE4945941321229E2AAF3906BE86@cat.app.metpsy.uni-jena.de>
Message-ID: <loom.20041123T150450-872@post.gmane.org>

Andreas Wolf <andreas.wolf <at> uni-jena.de> writes:

: 
: is there a smart way of determining the number of pairwise present data
: in a data matrix with missings (maybe as a by-product of some
: statistical function?)
: 
: so far, i used several loops like:
: 
: for (column1 in 1:99) {
:   for (column2 in 2:100) {
:     for (row in 1:500) {
:       if (!is.na(matrix[row,column1]) & !is.na(matrix[row,column2])) {
:         pairs[col1,col2] <- pairs[col1,col2]+1
:       }
:     }
:   }
: }
: 
: but this seems neither the most elegant nor an utterly fast solution.

This is just matrix multiplication of the !na(x) matrix:

R> x <- matrix(1:12,4,3)
R> x[c(1,10)] <- NA
R> x
     [,1] [,2] [,3]
[1,]   NA    5    9
[2,]    2    6   NA
[3,]    3    7   11
[4,]    4    8   12
R> crossprod(!is.na(x))
     [,1] [,2] [,3]
[1,]    3    3    2
[2,]    3    4    3
[3,]    2    3    3



From ripley at stats.ox.ac.uk  Tue Nov 23 15:18:17 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 23 Nov 2004 14:18:17 +0000 (GMT)
Subject: [R] number of pairwise present data in matrix with missings
In-Reply-To: <AF874507D8BE4945941321229E2AAF3906BE86@cat.app.metpsy.uni-jena.de>
References: <AF874507D8BE4945941321229E2AAF3906BE86@cat.app.metpsy.uni-jena.de>
Message-ID: <Pine.LNX.4.61.0411231406310.26885@gannet.stats>

Suppose your matrix is called A (`matrix' is not a good name). Then 
crossprod(!is.na(A)) is pretty efficient.  Test:

> A <- matrix(1, 6, 3)
> A[1,1] <- A[3, 1] <- A[2,2] <- NA
> A
      [,1] [,2] [,3]
[1,]   NA    1    1
[2,]    1   NA    1
[3,]   NA    1    1
[4,]    1    1    1
[5,]    1    1    1
[6,]    1    1    1
> crossprod(!is.na(A))
      [,1] [,2] [,3]
[1,]    4    3    4
[2,]    3    5    5
[3,]    4    5    6


On Tue, 23 Nov 2004, Andreas Wolf wrote:

> is there a smart way of determining the number of pairwise present data
> in a data matrix with missings (maybe as a by-product of some
> statistical function?)
>
> so far, i used several loops like:
>
> for (column1 in 1:99) {
>  for (column2 in 2:100) {
>    for (row in 1:500) {
>      if (!is.na(matrix[row,column1]) & !is.na(matrix[row,column2])) {
>        pairs[col1,col2] <- pairs[col1,col2]+1
>      }
>    }
>  }
> }
>
> but this seems neither the most elegant nor an utterly fast solution.
>
> thanks for suggestions.
> andreas wolf
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From f.harrell at vanderbilt.edu  Tue Nov 23 15:25:52 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 23 Nov 2004 09:25:52 -0500
Subject: [R] number of pairwise present data in matrix with missings
In-Reply-To: <AF874507D8BE4945941321229E2AAF3906BE86@cat.app.metpsy.uni-jena.de>
References: <AF874507D8BE4945941321229E2AAF3906BE86@cat.app.metpsy.uni-jena.de>
Message-ID: <41A34870.3030008@vanderbilt.edu>

Andreas Wolf wrote:
> is there a smart way of determining the number of pairwise present data
> in a data matrix with missings (maybe as a by-product of some
> statistical function?)
> 
> so far, i used several loops like:
> 
> for (column1 in 1:99) {
>   for (column2 in 2:100) {
>     for (row in 1:500) {
>       if (!is.na(matrix[row,column1]) & !is.na(matrix[row,column2])) {
>         pairs[col1,col2] <- pairs[col1,col2]+1
>       }
>     }
>   }
> }
> 
> but this seems neither the most elegant nor an utterly fast solution.
> 
> thanks for suggestions.
> andreas wolf

library(Hmisc)
n <- naclus(mydataframe)
plot(n)   # show pairwise missingness in a dendogram
naplot(n) # show more details in multiple plots

Frank

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From dimitris.rizopoulos at med.kuleuven.ac.be  Tue Nov 23 15:44:30 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Tue, 23 Nov 2004 15:44:30 +0100
Subject: [R] number of pairwise present data in matrix with missings
References: <AF874507D8BE4945941321229E2AAF3906BE86@cat.app.metpsy.uni-jena.de>
Message-ID: <01c001c4d16a$f0653680$0540210a@www.domain>

Hi Andreas,

maybe something like this could do it:

mat <- sample(0:3, 20*2, TRUE); dim(mat) <- c(20,2)
mat[sample(1:20, 4),] <- NA
########
mat
sum(rowMeans(mat)==mat[,1], na.rm=TRUE)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Andreas Wolf" <andreas.wolf at uni-jena.de>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, November 23, 2004 2:42 PM
Subject: [R] number of pairwise present data in matrix with missings


> is there a smart way of determining the number of pairwise present 
> data
> in a data matrix with missings (maybe as a by-product of some
> statistical function?)
>
> so far, i used several loops like:
>
> for (column1 in 1:99) {
>  for (column2 in 2:100) {
>    for (row in 1:500) {
>      if (!is.na(matrix[row,column1]) & !is.na(matrix[row,column2])) 
> {
>        pairs[col1,col2] <- pairs[col1,col2]+1
>      }
>    }
>  }
> }
>
> but this seems neither the most elegant nor an utterly fast 
> solution.
>
> thanks for suggestions.
> andreas wolf
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From abu3ammar at gmail.com  Tue Nov 23 15:55:44 2004
From: abu3ammar at gmail.com (Yasser El-Zein)
Date: Tue, 23 Nov 2004 09:55:44 -0500
Subject: [R] timeDate
In-Reply-To: <loom.20041122T224151-145@post.gmane.org>
References: <b1d315040411220945366432ab@mail.gmail.com>
	<41A22EF0.60709@statistik.uni-dortmund.de>
	<b1d3150404112213085eda9b3f@mail.gmail.com>
	<loom.20041122T224151-145@post.gmane.org>
Message-ID: <b1d3150404112306557eb13ab0@mail.gmail.com>

I am looking for up to the millisecond resolution. Is there a package
that has that?


On Mon, 22 Nov 2004 21:48:20 +0000 (UTC), Gabor Grothendieck
<ggrothendieck at myway.com> wrote:
> Yasser El-Zein <abu3ammar <at> gmail.com> writes:
> 
> >
> > >From the document it is apparent to me that I need as.POSIXct  (I have
> > a double representing the number of millis since 1/1/1970 and I need
> > to construct a datetime object). I see it showing how to construct the
> > time object from a string representing the time but now fro a double
> > of millis. Does anyone know hoe to do that?
> >
> 
> If by millis you mean milliseconds (i.e. one thousandths of a second)
> then POSIXct does not support that resolution, but if rounding to
> seconds is ok then
> 
>   structure(round(x/1000), class = c("POSIXt", "POSIXct"))
> 
> should give it to you assuming x is the number of milliseconds.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From dimitris.rizopoulos at med.kuleuven.ac.be  Tue Nov 23 16:06:05 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Tue, 23 Nov 2004 16:06:05 +0100
Subject: [R] number of pairwise present data in matrix with missings
References: <AF874507D8BE4945941321229E2AAF3906BE86@cat.app.metpsy.uni-jena.de>
Message-ID: <01d501c4d16d$f467f2b0$0540210a@www.domain>

Sorry my first reply was not relevant, I understood a different thing.

Dimitris

----- Original Message ----- 
From: "Andreas Wolf" <andreas.wolf at uni-jena.de>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, November 23, 2004 2:42 PM
Subject: [R] number of pairwise present data in matrix with missings


> is there a smart way of determining the number of pairwise present 
> data
> in a data matrix with missings (maybe as a by-product of some
> statistical function?)
>
> so far, i used several loops like:
>
> for (column1 in 1:99) {
>  for (column2 in 2:100) {
>    for (row in 1:500) {
>      if (!is.na(matrix[row,column1]) & !is.na(matrix[row,column2])) 
> {
>        pairs[col1,col2] <- pairs[col1,col2]+1
>      }
>    }
>  }
> }
>
> but this seems neither the most elegant nor an utterly fast 
> solution.
>
> thanks for suggestions.
> andreas wolf
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From tlumley at u.washington.edu  Tue Nov 23 16:21:41 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 23 Nov 2004 07:21:41 -0800 (PST)
Subject: [R] Weibull survival regression
In-Reply-To: <000001c4d129$a9086320$778f2bd9@youru989fckmtn>
References: <000001c4d129$a9086320$778f2bd9@youru989fckmtn>
Message-ID: <Pine.A41.4.61b.0411230716460.308234@homer08.u.washington.edu>

On Tue, 23 Nov 2004, Eric Lim wrote:

> Dear R users,
>
> Please can you help me with a relatively straightforward problem that I
> am struggling with? I am simply trying to plot a baseline survivor and
> hazard function for a simple data set of lung cancer survival where
> `futime' is follow up time in months and status is 1=dead and 0=alive.
>
> Using the survival package:
>
> lung.wbs <- survreg( Surv(futime, status)~ 1, data=lung, dist='weibull')
>
> plot (lung.wbs)
>
> Returns the error msg:
>
> Error in xy.coords(x, y, xlabel, ylabel, log) :
>        x and y lengths differ

Yes. There isn't a plot method for survreg() (and if there were, it 
wouldn't do this).

The right thing to try would have been
   plot(survfit(lung.wbs)
but that doesn't work either.

You can get the curve you want with
   curve(pweibull(x, scale=exp(coef(lung.wbs)), shape=1/lung.wbs$scale,
    lower.tail=FALSE),from=0, to=max(lung$futime))

where most of the complications come from the fact that survreg() and 
pweibull() parametrise the Weibull distribution differently.

Incidentally, this works quite nicely on the built-in lung cancer example 
data set, showing surprisingly good fit to a Weibull.

   data(lung)
   lung.wbs <- survreg( Surv(time, status)~ 1, data=lung, dist='weibull')
   curve(pweibull(x, scale=1/coef(lung.wbs), shape=1/lung.wbs$scale,
      lower.tail=FALSE),from=0, to=max(lung$time))
   lines(survfit(Surv(time,status)~1, data=lung), col="red")


 	-thomas



From james.holtman at convergys.com  Tue Nov 23 16:39:04 2004
From: james.holtman at convergys.com (james.holtman@convergys.com)
Date: Tue, 23 Nov 2004 10:39:04 -0500
Subject: [R] timeDate
Message-ID: <OFBA4A6C22.4AAA94E7-ON85256F55.005581ED@nd.convergys.com>





You might want to check out 'chron'.  This stores the time as days and
fractions of a day.

If you take the current date,

> as.numeric(chron(dates.="11/23/2004"))
[1] 12745
>

you get the value above.  If you change this to millisecond, you get

> as.numeric(chron(dates.="11/23/2004")) * 86400 * 1000
[1] 1.101168e+12
>

this value requires 46 bits and since a floating point number has 54 bits
of value, it should be enough to give you millisecond resolution and still
maintain the 'date'
__________________________________________________________
James Holtman        "What is the problem you are trying to solve?"
Executive Technical Consultant  --  Office of Technology, Convergys
james.holtman at convergys.com
+1 (513) 723-2929


                                                                                                                                           
                      Yasser El-Zein                                                                                                       
                      <abu3ammar at gmail.com>        To:       r-help at stat.math.ethz.ch                                                      
                      Sent by:                     cc:                                                                                     
                      r-help-bounces at stat.m        Subject:  Re: [R] timeDate                                                              
                      ath.ethz.ch                                                                                                          
                                                                                                                                           
                                                                                                                                           
                      11/23/2004 09:55                                                                                                     
                      Please respond to                                                                                                    
                      Yasser El-Zein                                                                                                       
                                                                                                                                           
                                                                                                                                           




I am looking for up to the millisecond resolution. Is there a package
that has that?


On Mon, 22 Nov 2004 21:48:20 +0000 (UTC), Gabor Grothendieck
<ggrothendieck at myway.com> wrote:
> Yasser El-Zein <abu3ammar <at> gmail.com> writes:
>
> >
> > >From the document it is apparent to me that I need as.POSIXct  (I have
> > a double representing the number of millis since 1/1/1970 and I need
> > to construct a datetime object). I see it showing how to construct the
> > time object from a string representing the time but now fro a double
> > of millis. Does anyone know hoe to do that?
> >
>
> If by millis you mean milliseconds (i.e. one thousandths of a second)
> then POSIXct does not support that resolution, but if rounding to
> seconds is ok then
>
>   structure(round(x/1000), class = c("POSIXt", "POSIXct"))
>
> should give it to you assuming x is the number of milliseconds.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From rkoenker at uiuc.edu  Tue Nov 23 16:40:16 2004
From: rkoenker at uiuc.edu (roger koenker)
Date: Tue, 23 Nov 2004 09:40:16 -0600
Subject: [R] The hidden costs of GPL software?
In-Reply-To: <20041123134825.SNEN1863.tomts10-srv.bellnexxia.net@JohnDesktop8300>
References: <20041123134825.SNEN1863.tomts10-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <F932D17E-3D65-11D9-BB0F-000A95A7E3AA@uiuc.edu>

Having just finished an index I would like to second John's comments.
Even as an author, it is  difficult to achieve some degree of
completeness and consistency.

Of course, maybe a real whizz at clustering could assemble something
very useful quite easily.  All of us who have had the frustration of 
searching
for a forgotten function would be grateful.


url:	www.econ.uiuc.edu/~roger        	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820

On Nov 23, 2004, at 7:48 AM, John Fox wrote:

> Dear Duncan,
>
> I don't think that there is an automatic, nearly costless way of 
> providing
> an effective solution to locating R resources. The problem seems to me 
> to be
> analogous to indexing a book. There's an excellent description of what 
> that
> process *should* look like in the Chicago Manual of Style, and it's a 
> lot of
> work. In my experience, most book indexes are quite poor, and 
> automatically
> generated indexes, while not useless, are even worse, since one should 
> index
> concepts, not words. The ideal indexer is therefore the author of the 
> book.
>
> I guess that the question boils down to how important is it to provide 
> an
> analogue of a good index to R? As I said in a previous message, I 
> believe
> that the current search facilities work pretty well -- about as well 
> as one
> could expect of an automatic approach. I don't believe that there's an
> effective centralized solution, so doing something more ambitious than 
> is
> currently available implies farming out the process to package 
> authors. Of
> course, there's no guarantee that all package authors will be diligent
> indexers.
>
> Regards,
>  John
>
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox
> --------------------------------
>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Duncan Murdoch
>> Sent: Monday, November 22, 2004 8:55 AM
>> To: Cliff Lunneborg
>> Cc: r-help at stat.math.ethz.ch
>> Subject: Re: [R] The hidden costs of GPL software?
>>
>> On Fri, 19 Nov 2004 13:59:23 -0800, "Cliff Lunneborg"
>> <cliff at ms.washington.edu> quoted John Fox:
>>
>>> Why not, as previously has been proposed, replace the current static
>>> (and, in my view, not very useful) set of keywords in R
>> documentation
>>> with the requirement that package authors supply their own
>> keywords for
>>> each documented object? I believe that this is the intent of the
>>> concept entries in Rd files, but their use certainly is not
>> required or
>>> even actively encouraged. (They're just mentioned in passing in the
>>> Writing R Extensions manual.
>>
>> That would not be easy and won't happen quickly.  There are some
>> problems:
>>
>>  - The base packages mostly don't use  \concept. (E.g. base
>> has 365 man pages, only about 15 of them use it).  Adding it
>> to each file is a fairly time-consuming task.
>>
>> - Before we started, we'd need to agree as to what they are for.
>> Right now, I think they are mainly used when the name of a
>> concept doesn't match the name of the function that
>> implements it, e.g.
>> "modulo", "remainder", "promise", "argmin", "assertion".  The
>> need for this usage is pretty rare.  If they were used for
>> everything, what would they contain?
>>
>>  - Keywording in a useful way is hard.  There are spelling
>> issues (e.g. optimise versus optimize); our fuzzy matching
>> helps with those.
>> But there are also multiple names for the same thing, and
>> multiple meanings for the same name.
>>
>> Duncan Murdoch
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From h.andersson at nioo.knaw.nl  Tue Nov 23 16:40:59 2004
From: h.andersson at nioo.knaw.nl (Henrik Andersson)
Date: Tue, 23 Nov 2004 16:40:59 +0100
Subject: [R] Create a vector of  combinations based on a table column names
Message-ID: <cnvlod$q4c$1@sea.gmane.org>

I want to create a character vector based on the table (shortened for 
display) below:
Where there are ones in the matrix I want the column name to appear and 
where there are zeros nothing, which would make the vector in this 
shortened case:

combinations <- ("A B","A C","A E H","A F G","B C D","E G H",A C D E 
H","A C D F G")

     no  value  A  B  C  D  E  F  G   H
1    2  3.095  1  1  0  0  0  0  0   0
2    2  1.687  1  0  1  0  0  0  0   0
46   3  3.470  1  0  0  0  1  0  0   1
47   3  1.563  1  0  0  0  0  1  1   0
50   3  6.234  0  1  1  1  0  0  0   0
148  4  3.663  0  0  1  0  1  0  1   1
151  4  3.470  0  0  0  1  1  1  0   1
177  5  5.411  1  0  1  1  1  0  0   1
178  5  6.829  1  0  1  1  0  1  1   0

Question is how to make this not so manually?

---------------------------------------------
Henrik Andersson
Netherlands Institute of Ecology -
Centre for Estuarine and Marine Ecology
P.O. Box 140
4400 AC Yerseke
Phone: +31 113 577473
h.andersson at nioo.knaw.nl
http://www.nioo.knaw.nl/ppages/handersson



From maechler at stat.math.ethz.ch  Tue Nov 23 17:04:41 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 23 Nov 2004 17:04:41 +0100
Subject: [R] timeDate
In-Reply-To: <OFBA4A6C22.4AAA94E7-ON85256F55.005581ED@nd.convergys.com>
References: <OFBA4A6C22.4AAA94E7-ON85256F55.005581ED@nd.convergys.com>
Message-ID: <16803.24473.589089.159051@gargle.gargle.HOWL>

>>>>> "james" == james holtman <james.holtman at convergys.com>
>>>>>     on Tue, 23 Nov 2004 10:39:04 -0500 writes:

    james> You might want to check out 'chron'.  This stores the
    james> time as days and fractions of a day.

    james> If you take the current date,

    >> as.numeric(chron(dates.="11/23/2004"))
    james> [1] 12745
    >>

    james> you get the value above.  If you change this to
    james> millisecond, you get

    >> as.numeric(chron(dates.="11/23/2004")) * 86400 * 1000
    james> [1] 1.101168e+12
    >>

    james> this value requires 46 bits and since a floating
    james> point number has 54 bits of value, 

no, only 52 bits  (64 = 52+1+12+1) with sign bits for exponent
and mantissa.

    james> it should be enough to give you millisecond resolution and still
    james> maintain the 'date'



From jarioksa at sun3.oulu.fi  Tue Nov 23 17:17:37 2004
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: 23 Nov 2004 18:17:37 +0200
Subject: [R] The hidden costs of GPL software?
In-Reply-To: <F932D17E-3D65-11D9-BB0F-000A95A7E3AA@uiuc.edu>
References: <20041123134825.SNEN1863.tomts10-srv.bellnexxia.net@JohnDesktop8300>
	<F932D17E-3D65-11D9-BB0F-000A95A7E3AA@uiuc.edu>
Message-ID: <1101226657.10908.9.camel@biol102145.oulu.fi>

On Tue, 2004-11-23 at 17:40, roger koenker wrote:
> Having just finished an index I would like to second John's comments.
> Even as an author, it is  difficult to achieve some degree of
> completeness and consistency.
> 
> Of course, maybe a real whizz at clustering could assemble something
> very useful quite easily.  All of us who have had the frustration of 
> searching
> for a forgotten function would be grateful.
> 
You mean SOM?
-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From lecoutre at stat.ucl.ac.be  Tue Nov 23 17:12:19 2004
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Tue, 23 Nov 2004 17:12:19 +0100
Subject: [R] Create a vector of  combinations based on a table column names
In-Reply-To: <cnvlod$q4c$1@sea.gmane.org>
References: <cnvlod$q4c$1@sea.gmane.org>
Message-ID: <6.0.1.1.2.20041123171113.01fefec0@stat4ux.stat.ucl.ac.be>


Hi,

Here is something that does the job (though I am sure other people will 
find smarter solutions!):


 > samp=matrix(sample(0:1,size=100,replace=TRUE ,prob=c(0.8,0.2)),ncol=10)
 > colnames(samp)<-LETTERS[1:10]
 > unlist(lapply(apply(samp,1,FUN=function(vec) 
colnames(samp)[as.logical(vec)] ),paste,collapse=" "))
  [1] "A F J"   "D I"     "B I"     "A"       "G H"     "B C E H" "E 
H"     "B C E G" "E"
[10] "B C E I"

HTH,

Eric


At 16:40 23/11/2004, Henrik Andersson wrote:
>I want to create a character vector based on the table (shortened for 
>display) below:
>Where there are ones in the matrix I want the column name to appear and 
>where there are zeros nothing, which would make the vector in this 
>shortened case:
>
>combinations <- ("A B","A C","A E H","A F G","B C D","E G H",A C D E H","A 
>C D F G")
>
>     no  value  A  B  C  D  E  F  G   H
>1    2  3.095  1  1  0  0  0  0  0   0
>2    2  1.687  1  0  1  0  0  0  0   0
>46   3  3.470  1  0  0  0  1  0  0   1
>47   3  1.563  1  0  0  0  0  1  1   0
>50   3  6.234  0  1  1  1  0  0  0   0
>148  4  3.663  0  0  1  0  1  0  1   1
>151  4  3.470  0  0  0  1  1  1  0   1
>177  5  5.411  1  0  1  1  1  0  0   1
>178  5  6.829  1  0  1  1  0  1  1   0
>
>Question is how to make this not so manually?
>
>---------------------------------------------
>Henrik Andersson
>Netherlands Institute of Ecology -
>Centre for Estuarine and Marine Ecology
>P.O. Box 140
>4400 AC Yerseke
>Phone: +31 113 577473
>h.andersson at nioo.knaw.nl
>http://www.nioo.knaw.nl/ppages/handersson
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Eric Lecoutre
UCL /  Institut de Statistique
Voie du Roman Pays, 20
1348 Louvain-la-Neuve
Belgium

tel: (+32)(0)10473050
lecoutre at stat.ucl.ac.be
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre

If the statistics are boring, then you've got the wrong numbers. -Edward 
Tufte



From sander at oomvanlieshout.net  Tue Nov 23 17:23:36 2004
From: sander at oomvanlieshout.net (Sander Oom)
Date: Tue, 23 Nov 2004 18:23:36 +0200
Subject: [R] IFELSE across large array?
Message-ID: <6.1.2.0.0.20041123181516.0245df88@www.oomvanlieshout.net>

Dear all,

As our previous email did not get any response, we try again with a 
reformulated question!

We are trying to do something which needs an efficient loop over a huge 
array, possibly functions such as apply and related (tapply,
lapply...?), but can't really understand syntax and examples in 
practice...i.e. cant' make it work.

to be more specific:
we are trying to apply a mask to a 3D array.
By this I mean that when "overlaying" [i.e. comparing element by element] 
the mask on to the array the mask should change array elements according to 
the values of both array and mask elements

the mask has two values: 1 and 10.

the array elements have 3 values: 0, 1,  or 10

sticking for the moment to the single 2d array case

for example:
[A= array]  10    0 10 1  10  0
                  1   10   1 0   0 10

[ M=mask]     1  10  10 1   1  1
                 10    1   1  1 10 10

I would like the array elements to:

a) IF A(ij) !=10 and  Mij = 1
              leave A(ij) unchanged

b)  IF   A(ij) != 10 and M(ij) =10
                change A(ij) to M(ij) i.e mask value (10)

c)IF A(ij) = 10 and M(ij) = 10
               leave (Aij) unchanged

d) IF A(ij) = 10 and M(ij) !=10
            replace A(ij) with the majority value in the 8-neighborhood

(or whatever if it is an edge element) BUT ignoring 10s in this 
neighborhood (i.e. with either 1 or 0, whichever is in majority)

because the array is 3d I would like to repeat the thing with all the k 
elements (2d sub-arrays) of the array in question, using the same mask for 
al k elements

Would you be able to suggest a strategy to do this?

thanks very much

Alessandro and Sander.



From michal.blazejczyk at gmail.com  Tue Nov 23 17:21:28 2004
From: michal.blazejczyk at gmail.com (Michal Blazejczyk)
Date: Tue, 23 Nov 2004 11:21:28 -0500
Subject: [R] Interactive plots in R?
Message-ID: <f419e62404112308211eec79f6@mail.gmail.com>

Hi all,

As part of a research project we are creating a statistical software
tool and will be using R as the computational engine.  I was wandering
whether we should also use R for plotting.  R has a good plotting
flexibility and an extensive library of available plot types. However,
our application is supposed to give our users an advanced
look-and-feel as well. For the plotting we will therefore need things
like:
 - interactivity (the possibility to interact with the plot using a
mouse, e.g. select data points or data bars)
 - zoom / scroll 
 - custom colors for lines, data points etc. 
 - conditional formatting (e.g. data points above a given threshold
are red and bigger while others are regular)
 - possibility to draw marker lines, custom-color bands and areas 
 - flexible axes, titles and legends 
 - axis grids 
 - export to image files (e.g. BMP) in high resolution 
 
Does anyone know how many of these things are achievable with R, to
what extent and using which package?
 
Thanks in advance, 
 
Michal Blazejczyk 
Lead Programmer 
Genome Quebec



From tiago17 at socrates.Berkeley.EDU  Tue Nov 23 17:27:03 2004
From: tiago17 at socrates.Berkeley.EDU (Tiago R Magalhaes)
Date: Tue, 23 Nov 2004 16:27:03 +0000
Subject: [R] data.frame into vector
Message-ID: <p06100508bdc915092d0e@[83.132.28.79]>

Hi

I want to extract a row from a data.frame but I want that object to 
be a vector . After trying some different ways I end up always with a 
data.frame or with the wrong vector. Any pointers?

  x <- data.frame(a = factor(c('a',2,'b')), b = c(4,5,6))
I want to get
"a" "4"

I tried:

as.vector(x[1,])
   a b
1 a 4
(resulting in a data.frame even after in my mind having coerced it 
into a vector!)

as.vector(c[1,], numeric='character')
[1] "2" "4"
(almost what I want, except that "2" instead of "a" - I guess this as 
to do with levels and factors)

Thanks for any help

>  R.Version()
$platform
[1] "powerpc-apple-darwin6.8"

$arch
[1] "powerpc"

$os
[1] "darwin6.8"

$system
[1] "powerpc, darwin6.8"

$status
[1] ""

$major
[1] "2"

$minor
[1] "0.1"

$year
[1] "2004"

$month
[1] "11"

$day
[1] "15"

$language
[1] "R"



From maechler at stat.math.ethz.ch  Tue Nov 23 17:30:35 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 23 Nov 2004 17:30:35 +0100
Subject: [R] timeDate
In-Reply-To: <16803.24473.589089.159051@gargle.gargle.HOWL>
References: <OFBA4A6C22.4AAA94E7-ON85256F55.005581ED@nd.convergys.com>
	<16803.24473.589089.159051@gargle.gargle.HOWL>
Message-ID: <16803.26027.198480.655681@gargle.gargle.HOWL>

>>>>> "MM" == Martin Maechler <maechler at stat.math.ethz.ch>
>>>>>     on Tue, 23 Nov 2004 17:04:41 +0100 writes:

>>>>> "james" == james holtman <james.holtman at convergys.com>
>>>>>     on Tue, 23 Nov 2004 10:39:04 -0500 writes:

    james> You might want to check out 'chron'.  This stores the
    james> time as days and fractions of a day.

    james> If you take the current date,

    >>> as.numeric(chron(dates.="11/23/2004"))
    james> [1] 12745
    >>> 

    james> you get the value above.  If you change this to
    james> millisecond, you get

    >>> as.numeric(chron(dates.="11/23/2004")) * 86400 * 1000
    james> [1] 1.101168e+12
    >>> 

    james> this value requires 46 bits and since a floating
    james> point number has 54 bits of value, 

    MM> no, only 52 bits  (64 = 52+1+12+1) with sign bits for exponent
    MM> and mantissa.

so much on the theme of mathematicians and arithmetic !
   64 = 52 + 1 + 10 + 1

Martin



From ripley at stats.ox.ac.uk  Tue Nov 23 17:32:40 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 23 Nov 2004 16:32:40 +0000 (GMT)
Subject: [R] timeDate
In-Reply-To: <16803.24473.589089.159051@gargle.gargle.HOWL>
References: <OFBA4A6C22.4AAA94E7-ON85256F55.005581ED@nd.convergys.com>
	<16803.24473.589089.159051@gargle.gargle.HOWL>
Message-ID: <Pine.LNX.4.61.0411231627160.28805@gannet.stats>

On Tue, 23 Nov 2004, Martin Maechler wrote:

>>>>>> "james" == james holtman <james.holtman at convergys.com>
>>>>>>     on Tue, 23 Nov 2004 10:39:04 -0500 writes:
>
>    james> You might want to check out 'chron'.  This stores the
>    james> time as days and fractions of a day.
>
>    james> If you take the current date,
>
>    >> as.numeric(chron(dates.="11/23/2004"))
>    james> [1] 12745
>    >>
>
>    james> you get the value above.  If you change this to
>    james> millisecond, you get
>
>    >> as.numeric(chron(dates.="11/23/2004")) * 86400 * 1000
>    james> [1] 1.101168e+12
>    >>
>
>    james> this value requires 46 bits and since a floating
>    james> point number has 54 bits of value,
>
> no, only 52 bits  (64 = 52+1+12+1) with sign bits for exponent
> and mantissa.

But with an implicit '1' for the first digit in a normalized number.

http://docs.sun.com/source/806-3568/ncg_math.html is one source.  E.g.

`The 52-bit fraction combined with the implicit leading significand bit 
provides 53 bits of precision in double-format normal numbers.'

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ramasamy at cancer.org.uk  Tue Nov 23 17:38:09 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Tue, 23 Nov 2004 16:38:09 +0000
Subject: [Fwd: [R] Create a vector of  combinations based on a table column
	names]
Message-ID: <1101227888.3093.251.camel@ndmpc126.ihs.ox.ac.uk>

x <- c(1,2,3.095,1,1,0,0,0,0,0,0,
       2,2,1.687,1,0,1,0,0,0,0,0,
       46,3,3.47,1,0,0,0,1,0,0,1,
       47,3,1.563,1,0,0,0,0,1,1,0,
       50,3,6.234,0,1,1,1,0,0,0,0,
       148,4,3.663,0,0,1,0,1,0,1,1,
       151,4,3.47,0,0,0,1,1,1,0,1,
       177,5,5.411,1,0,1,1,1,0,0,1,
       178,5,6.829,1,0,1,1,0,1,1,0)

mat <- matrix(x, nc=11, byrow=T); rm(x)
colnames(mat) <- c("index", "no", "value", LETTERS[1:8])


apply( mat[ , 4:11], 1, function(x) paste( names(x)[ which(x==1) ],
collapse=" " ))
[1] "A B"       "A C"       "A E H"     "A F G"   "B C D"   "C E G H"
[7] "D E F H"   "A C D E H" "A C D F G"

mat[ , 4:11] uses data from 4th to 11 th columns. 

names(x)[ which(x==1) ] finds the names of all elements with values 1 in
the rows

paste( ..., collapse=" ") collapses all the names of the elements with
value 1

apply( mat[ , 4:9], 1, ... ) applies the above process for every row

Regards, Adai


-----Forwarded Message-----
From: Henrik Andersson <h.andersson at nioo.knaw.nl>
To: r-help at stat.math.ethz.ch
Subject: [R] Create a vector of  combinations based on a table column names
Date: Tue, 23 Nov 2004 16:40:59 +0100

I want to create a character vector based on the table (shortened for 
display) below:
Where there are ones in the matrix I want the column name to appear and 
where there are zeros nothing, which would make the vector in this 
shortened case:

combinations <- ("A B","A C","A E H","A F G","B C D","E G H",A C D E 
H","A C D F G")

     no  value  A  B  C  D  E  F  G   H
1    2  3.095  1  1  0  0  0  0  0   0
2    2  1.687  1  0  1  0  0  0  0   0
46   3  3.470  1  0  0  0  1  0  0   1
47   3  1.563  1  0  0  0  0  1  1   0
50   3  6.234  0  1  1  1  0  0  0   0
148  4  3.663  0  0  1  0  1  0  1   1
151  4  3.470  0  0  0  1  1  1  0   1
177  5  5.411  1  0  1  1  1  0  0   1
178  5  6.829  1  0  1  1  0  1  1   0

Question is how to make this not so manually?

---------------------------------------------
Henrik Andersson
Netherlands Institute of Ecology -
Centre for Estuarine and Marine Ecology
P.O. Box 140
4400 AC Yerseke
Phone: +31 113 577473
h.andersson at nioo.knaw.nl
http://www.nioo.knaw.nl/ppages/handersson

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
-- 
Adaikalavan Ramasamy                    ramasamy at cancer.org.uk
Centre for Statistics in Medicine       http://www.ihs.ox.ac.uk/csm/
Cancer Research UK                      Tel : 01865 226 677
Old Road Campus, Headington, Oxford     Fax : 01865 226 962



From pburns at pburns.seanet.com  Tue Nov 23 17:45:53 2004
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Tue, 23 Nov 2004 16:45:53 +0000
Subject: [R] The hidden costs of GPL software?
In-Reply-To: <20041123134825.SNEN1863.tomts10-srv.bellnexxia.net@JohnDesktop8300>
References: <20041123134825.SNEN1863.tomts10-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <41A36941.6010406@pburns.seanet.com>

I think John has exactly the right image -- index to a book --
but I disagree with his conclusions.

I read somewhere that an index should not be done by the
author.  It was probably written by someone who was bored
of indexing, but the logic was precisely because indices should
be about concepts.  The author of a package will have one
concept for a function but not all of the concepts that come
from various fields of study.  I suspect that no one outside of
finance would think to index "sd" with "volatility" for (a not very
good) example.

There could be an index builder that accepts a search phrase and
the function or package that is the successful answer to the search.
If this were open, then R users could contribute to the index who
don't feel qualified to submit code. It could also help diffuse the
frustration of taking too long to find a function by allowing a way
to insure that the exact same thing doesn't happen to others.

Amazon has a function that says those who bought "The Chicago
Manual of Style" also bought Strunk and White.  In the same way,
the R index could provide a list of terms that overlap the given
search term.  For example if we search for "goodness of fit", then
"hypothesis test" might be one of the related terms that pops up.

No, I'm not volunteering to build the system.

Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

John Fox wrote:

>Dear Duncan,
>
>I don't think that there is an automatic, nearly costless way of providing
>an effective solution to locating R resources. The problem seems to me to be
>analogous to indexing a book. There's an excellent description of what that
>process *should* look like in the Chicago Manual of Style, and it's a lot of
>work. In my experience, most book indexes are quite poor, and automatically
>generated indexes, while not useless, are even worse, since one should index
>concepts, not words. The ideal indexer is therefore the author of the book.
>
>I guess that the question boils down to how important is it to provide an
>analogue of a good index to R? As I said in a previous message, I believe
>that the current search facilities work pretty well -- about as well as one
>could expect of an automatic approach. I don't believe that there's an
>effective centralized solution, so doing something more ambitious than is
>currently available implies farming out the process to package authors. Of
>course, there's no guarantee that all package authors will be diligent
>indexers. 
>
>Regards,
> John
>
>--------------------------------
>John Fox
>Department of Sociology
>McMaster University
>Hamilton, Ontario
>Canada L8S 4M4
>905-525-9140x23604
>http://socserv.mcmaster.ca/jfox 
>-------------------------------- 
>
>  
>
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Duncan Murdoch
>>Sent: Monday, November 22, 2004 8:55 AM
>>To: Cliff Lunneborg
>>Cc: r-help at stat.math.ethz.ch
>>Subject: Re: [R] The hidden costs of GPL software?
>>
>>On Fri, 19 Nov 2004 13:59:23 -0800, "Cliff Lunneborg"
>><cliff at ms.washington.edu> quoted John Fox:
>>
>>    
>>
>>>Why not, as previously has been proposed, replace the current static 
>>>(and, in my view, not very useful) set of keywords in R 
>>>      
>>>
>>documentation 
>>    
>>
>>>with the requirement that package authors supply their own 
>>>      
>>>
>>keywords for 
>>    
>>
>>>each documented object? I believe that this is the intent of the 
>>>concept entries in Rd files, but their use certainly is not 
>>>      
>>>
>>required or 
>>    
>>
>>>even actively encouraged. (They're just mentioned in passing in the 
>>>Writing R Extensions manual.
>>>      
>>>
>>That would not be easy and won't happen quickly.  There are some
>>problems:
>>
>> - The base packages mostly don't use  \concept. (E.g. base 
>>has 365 man pages, only about 15 of them use it).  Adding it 
>>to each file is a fairly time-consuming task.
>>
>>- Before we started, we'd need to agree as to what they are for.
>>Right now, I think they are mainly used when the name of a 
>>concept doesn't match the name of the function that 
>>implements it, e.g.
>>"modulo", "remainder", "promise", "argmin", "assertion".  The 
>>need for this usage is pretty rare.  If they were used for 
>>everything, what would they contain?
>>
>> - Keywording in a useful way is hard.  There are spelling 
>>issues (e.g. optimise versus optimize); our fuzzy matching 
>>helps with those.
>>But there are also multiple names for the same thing, and 
>>multiple meanings for the same name.
>>
>>Duncan Murdoch
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
>  
>



From stephane.dray at umontreal.ca  Tue Nov 23 17:47:08 2004
From: stephane.dray at umontreal.ca (Stephane DRAY)
Date: Tue, 23 Nov 2004 11:47:08 -0500
Subject: [R] locator and multiple plots
Message-ID: <5.2.1.1.0.20041123113544.00bec980@magellan.umontreal.ca>

Hello,

I would like to know if it possible to use locator() only on one region of 
a graphic device.
I would like to fragment a graphic device into sub-regions (using layout or 
split.screen or any other functions that I do not know) and then use 
locator. but I want that the locator is only used for one of the sub-plot. 
In my view, I would like is that the cursor is an arrow on the graphical 
window and become an arrow only on the region that I specify. Is it 
possible ??
My problem is to do a kind of interactive plot. One region of the device is 
a plot, another part is a kind of buttons menu.
I create some kinds of buttons on the second region (use rect) and then 
wait that the user click with one of these buttons to upadte the plot on 
the first region.
It seems to me that when I use locator after split.screen, coordinates are 
returned for the last region invoked by screen(), is that ok ?


Another question, I want to open a new graphical device.
windows() do it for windows version. Is something like :
eval(parse(text=paste(options()$device,"(width=3,height=3)"))) will work 
for all platforms ?


Thanks in advance !
St??phane DRAY
-------------------------------------------------------------------------------------------------- 

D??partement des Sciences Biologiques
Universit?? de Montr??al, C.P. 6128, succursale centre-ville
Montr??al, Qu??bec H3C 3J7, Canada

Tel : (514) 343-6111 poste 1233         Fax : (514) 343-2293
E-mail : stephane.dray at umontreal.ca
-------------------------------------------------------------------------------------------------- 

Web                                          http://www.steph280.freesurf.fr/



From andy_liaw at merck.com  Tue Nov 23 17:52:03 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 23 Nov 2004 11:52:03 -0500
Subject: [R] Create a vector of combinations based on a table column n ames
Message-ID: <3A822319EB35174CA3714066D590DCD50994E36D@usrymx25.merck.com>

Here's another way:

> dat <- read.table("clipboard", header=TRUE)[,-(1:3)]
> dat
  A B C D E F G H
1 1 1 0 0 0 0 0 0
2 1 0 1 0 0 0 0 0
3 1 0 0 0 1 0 0 1
4 1 0 0 0 0 1 1 0
5 0 1 1 1 0 0 0 0
6 0 0 1 0 1 0 1 1
7 0 0 0 1 1 1 0 1
8 1 0 1 1 1 0 0 1
9 1 0 1 1 0 1 1 0
> apply(dat > 0, 1, function(idx) paste(names(dat)[idx], collapse=" "))
          1           2           3           4           5           6
7 
      "A B"       "A C"     "A E H"     "A F G"     "B C D"   "C E G H"   "D
E F H" 
          8           9 
"A C D E H" "A C D F G" 

HTH,
Andy

> From: Eric Lecoutre
> 
> Hi,
> 
> Here is something that does the job (though I am sure other 
> people will 
> find smarter solutions!):
> 
> 
>  > samp=matrix(sample(0:1,size=100,replace=TRUE 
> ,prob=c(0.8,0.2)),ncol=10)
>  > colnames(samp)<-LETTERS[1:10]
>  > unlist(lapply(apply(samp,1,FUN=function(vec) 
> colnames(samp)[as.logical(vec)] ),paste,collapse=" "))
>   [1] "A F J"   "D I"     "B I"     "A"       "G H"     "B C E H" "E 
> H"     "B C E G" "E"
> [10] "B C E I"
> 
> HTH,
> 
> Eric
> 
> 
> At 16:40 23/11/2004, Henrik Andersson wrote:
> >I want to create a character vector based on the table 
> (shortened for 
> >display) below:
> >Where there are ones in the matrix I want the column name to 
> appear and 
> >where there are zeros nothing, which would make the vector in this 
> >shortened case:
> >
> >combinations <- ("A B","A C","A E H","A F G","B C D","E G 
> H",A C D E H","A 
> >C D F G")
> >
> >     no  value  A  B  C  D  E  F  G   H
> >1    2  3.095  1  1  0  0  0  0  0   0
> >2    2  1.687  1  0  1  0  0  0  0   0
> >46   3  3.470  1  0  0  0  1  0  0   1
> >47   3  1.563  1  0  0  0  0  1  1   0
> >50   3  6.234  0  1  1  1  0  0  0   0
> >148  4  3.663  0  0  1  0  1  0  1   1
> >151  4  3.470  0  0  0  1  1  1  0   1
> >177  5  5.411  1  0  1  1  1  0  0   1
> >178  5  6.829  1  0  1  1  0  1  1   0
> >
> >Question is how to make this not so manually?
> >
> >---------------------------------------------
> >Henrik Andersson
> >Netherlands Institute of Ecology -
> >Centre for Estuarine and Marine Ecology
> >P.O. Box 140
> >4400 AC Yerseke
> >Phone: +31 113 577473
> >h.andersson at nioo.knaw.nl
> >http://www.nioo.knaw.nl/ppages/handersson
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> Eric Lecoutre
> 
> UCL /  Institut de Statistique
> Voie du Roman Pays, 20
> 1348 Louvain-la-Neuve
> Belgium
> 
> tel: (+32)(0)10473050
> lecoutre at stat.ucl.ac.be
> http://www.stat.ucl.ac.be/ISpersonnel/lecoutre
> 
> If the statistics are boring, then you've got the wrong 
> numbers. -Edward 
> Tufte
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From abunn at whrc.org  Tue Nov 23 17:39:31 2004
From: abunn at whrc.org (Andy Bunn)
Date: Tue, 23 Nov 2004 11:39:31 -0500
Subject: [R] Create a vector of combinations based on a table column names
In-Reply-To: <cnvlod$q4c$1@sea.gmane.org>
Message-ID: <NEBBIPHDAMMOKDKPOFFIGEKHCMAA.abunn@whrc.org>

There has to be a better (more readable) way, but this works...

> set.seed(323)
> foo.df <- data.frame(A = round(runif(5)), B = round(runif(5)), C =
round(runif(5)))
> foo.df
  A B C
1 0 1 1
2 1 1 1
3 1 1 1
4 0 1 1
5 1 1 0
> names.list <- lapply( apply( foo.df, 1, function( x ) colnames(
foo.df )[ as.logical( x ) ] ), paste, collapse = ", " )
> names.vect <- unlist(names.list)
> foo.df
  A B C
1 0 1 1
2 1 1 1
3 1 1 1
4 0 1 1
5 1 1 0
> names.vect
        1         2         3         4         5
   "B, C" "A, B, C" "A, B, C"    "B, C"    "A, B"
>

Those nested applies make my head hurt.

HTH, Andy


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Henrik Andersson
> Sent: Tuesday, November 23, 2004 10:41 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Create a vector of combinations based on a table column
> names
>
>
> I want to create a character vector based on the table (shortened for
> display) below:
> Where there are ones in the matrix I want the column name to appear and
> where there are zeros nothing, which would make the vector in this
> shortened case:
>
> combinations <- ("A B","A C","A E H","A F G","B C D","E G H",A C D E
> H","A C D F G")
>
>      no  value  A  B  C  D  E  F  G   H
> 1    2  3.095  1  1  0  0  0  0  0   0
> 2    2  1.687  1  0  1  0  0  0  0   0
> 46   3  3.470  1  0  0  0  1  0  0   1
> 47   3  1.563  1  0  0  0  0  1  1   0
> 50   3  6.234  0  1  1  1  0  0  0   0
> 148  4  3.663  0  0  1  0  1  0  1   1
> 151  4  3.470  0  0  0  1  1  1  0   1
> 177  5  5.411  1  0  1  1  1  0  0   1
> 178  5  6.829  1  0  1  1  0  1  1   0
>
> Question is how to make this not so manually?
>
> ---------------------------------------------
> Henrik Andersson
> Netherlands Institute of Ecology -
> Centre for Estuarine and Marine Ecology
> P.O. Box 140
> 4400 AC Yerseke
> Phone: +31 113 577473
> h.andersson at nioo.knaw.nl
> http://www.nioo.knaw.nl/ppages/handersson
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From jahernan at umn.edu  Tue Nov 23 17:57:42 2004
From: jahernan at umn.edu (Jose A. Hernandez)
Date: Tue, 23 Nov 2004 10:57:42 -0600
Subject: [R] Quadratic curve on a loop
Message-ID: <41A36C06.6000500@umn.edu>

Dear R community, once again I request your generous help on an R issue 
I cannot seem to figure out.

I am trying to do some analysis for multiple sites, in my example I am 
using only two, but the same will done on many sites. It's just ONE 
line, so I hope somebody can give a me a hand by email.

Look at the loop to make the plot x ~ y. If the curve function is 
removed from the loop everything works fine, but when I tried to add the 
curve to the plot, I get the following error:

Error in xy.coords(x, y) : x and y lengths differ
In addition: Warning message:
'newdata' had 101 rows but variable(s) found have 6 rows

I know this error should be a hint about how to proceed but I am stuck 
figuring ways to make the curve to print in the plots.

Any hints on how to solve this issue would be greatly appreciated.

Sincerely,

Jose

PS.
 > version
          _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    2
minor    0.1
year     2004
month    11
day      15
language R



###################################################################
# some data
site <- c(1,1,1,1,1,1,2,2,2,2,2,2)
nrate <- c(0,60,90,120,150,180,0,60,90,120,150,180)
yield <- 
c(161.7,187.1,188.5,196.6,196.0,196.5,160.7,186.1,189.5,194.6,195.0,198.5)
site <- as.factor(site)

# file name
fname <- levels(site)
i <- nchar(fname) == 1
fname[i] <- paste("0", fname[i], sep = "")
fname <- paste("site", fname, ".eps", sep = "")

###################################################################
###################################################################

for (i in 1:length(levels(site))) {
     j <- site == levels(site)[i]
     x <- nrate[j]
     y <- yield[j]
     out <- try(lm(y ~ x + I(x^2)), silent = TRUE)
     plot(y ~ x,
          pch = 16,
          xlab = expression(paste("Nitrogen rate [lbs ac" ^-1,"]")),
          ylab = expression(paste("Corn yield [bu ac"^-1,"]")))
          curve(predict(out, data.frame(nrate = x)), add = T)
          title(paste("Site", i))
          # dev.copy2eps(file = fname[i], horizontal = TRUE)
}
###################################################################
###################################################################



-- 
Jose A. Hernandez
Department of Soil, Water, and Climate
University of Minnesota
1991 Upper Buford Circle
St. Paul, MN 55108

Ph. (612) 625-0445, Fax. (612) 625-2208



From ripley at stats.ox.ac.uk  Tue Nov 23 18:14:01 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 23 Nov 2004 17:14:01 +0000 (GMT)
Subject: [R] data.frame into vector
In-Reply-To: <p06100508bdc915092d0e@[83.132.28.79]>
References: <p06100508bdc915092d0e@[83.132.28.79]>
Message-ID: <Pine.LNX.4.61.0411231708450.29221@gannet.stats>

A data frame is a list, and a list is a vector.  Once you understand that, 
yoy may understand what you are seeing.

as.matrix(x)[1,]  seems to be one of the easiest ways to get what you want

On Tue, 23 Nov 2004, Tiago R Magalhaes wrote:

> Hi
>
> I want to extract a row from a data.frame but I want that object to be a 
> vector . After trying some different ways I end up always with a data.frame 
> or with the wrong vector. Any pointers?
>
> x <- data.frame(a = factor(c('a',2,'b')), b = c(4,5,6))
> I want to get
> "a" "4"
>
> I tried:
>
> as.vector(x[1,])
>  a b
> 1 a 4
> (resulting in a data.frame even after in my mind having coerced it into a 
> vector!)
>
> as.vector(c[1,], numeric='character')

Eh?  I get an error there.

> [1] "2" "4"
> (almost what I want, except that "2" instead of "a" - I guess this as to do 
> with levels and factors)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jfox at mcmaster.ca  Tue Nov 23 18:14:46 2004
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 23 Nov 2004 12:14:46 -0500
Subject: [R] Convergence problem in GLMM
Message-ID: <20041123171443.VKXM2034.tomts20-srv.bellnexxia.net@JohnDesktop8300>

Dear list members,

In re-running with GLMM() from the lme4 package a generalized-linear mixed
model that I had previously fit with glmmPQL() from MASS, I'm getting a
warning of a convergence failure, even when I set the method argument of
GLMM() to "PQL":

> bang.mod.1 <- glmmPQL(contraception ~ as.factor(children) + cage + urban,
+     random=~as.factor(children) + cage + urban|district,
+     family=binomial, data=Bangladesh)  # works!

> bang.mod.1 <- GLMM(contraception ~ as.factor(children) + cage + urban,
+     random=~as.factor(children) + cage + urban|district,
+     family=binomial, data=Bangladesh, method="PQL")
Warning message: 
IRLS iterations for glmm did not converge in: GLMM(formula = contraception ~
as.factor(children) + cage + urban,  

Despite the indicated convergence failure, the two sets of estimates are
quite close, as one would expect. I've also played around with various
arguments to lmeControl(), but without success.

I'm curious about the source of the difference, since (as I understand it),
essentially the same algorithm is used by the two functions. I'm using R
2.0.1 under Windows XP with the current version of lme4.

For anyone who's interested, the data are at
<http://socserv.socsci.mcmaster.ca/jfox/Courses/soc761/Bangladesh.txt>, and
are originally from Goldstein, Multilevel Statistical Models, Third Edition.

Any suggestions would be appreciated.

John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox



From andy_liaw at merck.com  Tue Nov 23 18:28:48 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 23 Nov 2004 12:28:48 -0500
Subject: [R] IFELSE across large array?
Message-ID: <3A822319EB35174CA3714066D590DCD50994E36F@usrymx25.merck.com>

I'll give it half a crack:

Steps a through c can be done via nested ifelse(), treating A and M as
vectors (as they really are).  Step d is the hard one.  I'd write a simple
Fortran code and use .Fortran() for that.

I don't see how any of the *apply() functions can help here, as your
operations are element-wise, not dimension-wise.

Andy


> From: Sander Oom
> 
> Dear all,
> 
> As our previous email did not get any response, we try again with a 
> reformulated question!
> 
> We are trying to do something which needs an efficient loop 
> over a huge 
> array, possibly functions such as apply and related (tapply,
> lapply...?), but can't really understand syntax and examples in 
> practice...i.e. cant' make it work.
> 
> to be more specific:
> we are trying to apply a mask to a 3D array.
> By this I mean that when "overlaying" [i.e. comparing element 
> by element] 
> the mask on to the array the mask should change array 
> elements according to 
> the values of both array and mask elements
> 
> the mask has two values: 1 and 10.
> 
> the array elements have 3 values: 0, 1,  or 10
> 
> sticking for the moment to the single 2d array case
> 
> for example:
> [A= array]  10    0 10 1  10  0
>                   1   10   1 0   0 10
> 
> [ M=mask]     1  10  10 1   1  1
>                  10    1   1  1 10 10
> 
> I would like the array elements to:
> 
> a) IF A(ij) !=10 and  Mij = 1
>               leave A(ij) unchanged
> 
> b)  IF   A(ij) != 10 and M(ij) =10
>                 change A(ij) to M(ij) i.e mask value (10)
> 
> c)IF A(ij) = 10 and M(ij) = 10
>                leave (Aij) unchanged
> 
> d) IF A(ij) = 10 and M(ij) !=10
>             replace A(ij) with the majority value in the 
> 8-neighborhood
> 
> (or whatever if it is an edge element) BUT ignoring 10s in this 
> neighborhood (i.e. with either 1 or 0, whichever is in majority)
> 
> because the array is 3d I would like to repeat the thing with 
> all the k 
> elements (2d sub-arrays) of the array in question, using the 
> same mask for 
> al k elements
> 
> Would you be able to suggest a strategy to do this?
> 
> thanks very much
> 
> Alessandro and Sander.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From jahernan at umn.edu  Tue Nov 23 18:33:26 2004
From: jahernan at umn.edu (Jose A. Hernandez)
Date: Tue, 23 Nov 2004 11:33:26 -0600
Subject: [R] SAS or R software
Message-ID: <41A37466.8090403@umn.edu>

Hi Neela,

Just my ??2 regarding the R vs. SAS issue. I started to use SAS since my 
undergraduate studies in 1993, and I am now a migrating to R user, and I 
hope not to ever go back to SAS.

My comments are:

- SAS is huge it takes over 1GB of space of my PC while R takes just 
over 100 MB, this days that may not be an important issue, but I'd 
consider this in your analysis.

- One of the things that bother me the most about SAS was their "system 
update" protocol, it was a mess (at least last time I tried downloading 
the updates, like 2 years ago). R has a great community and I love the 
update.packages() function.

- Regarding the large datasets discussion, I've had some experience 
working with mixed models on large dataset, and both programs end up 
taking a lot of time or crashing.

- The learning curve for a non-stats non-programmer guy was steeper for 
SAS, but again SAS was my first. However, R still does not come so 
naturally to me when I am coding, I have to look at my old codes, 
manuals and google. If I ever end up teaching applied stats I'll teach 
using R.

Sincerely,

Jose
... migrating to R ... and one of this days to linux

-- 
Jose A. Hernandez
Ph.D. Candidate
Precision Agriculture Center

Department of Soil, Water, and Climate
University of Minnesota
1991 Upper Buford Circle
St. Paul, MN 55108

Ph. (612) 625-0445, Fax. (612) 625-2208



From andy_liaw at merck.com  Tue Nov 23 18:38:24 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 23 Nov 2004 12:38:24 -0500
Subject: [R] data.frame into vector
Message-ID: <3A822319EB35174CA3714066D590DCD50994E370@usrymx25.merck.com>

Is this what you want?

> as.matrix(x[1,])[1,]
  a   b 
"a" "4" 

HTH,
Andy

> From: Tiago R Magalhaes
> 
> Hi
> 
> I want to extract a row from a data.frame but I want that object to 
> be a vector . After trying some different ways I end up always with a 
> data.frame or with the wrong vector. Any pointers?
> 
>   x <- data.frame(a = factor(c('a',2,'b')), b = c(4,5,6))
> I want to get
> "a" "4"
> 
> I tried:
> 
> as.vector(x[1,])
>    a b
> 1 a 4
> (resulting in a data.frame even after in my mind having coerced it 
> into a vector!)
> 
> as.vector(c[1,], numeric='character')
> [1] "2" "4"
> (almost what I want, except that "2" instead of "a" - I guess this as 
> to do with levels and factors)
> 
> Thanks for any help
> 
> >  R.Version()
> $platform
> [1] "powerpc-apple-darwin6.8"
> 
> $arch
> [1] "powerpc"
> 
> $os
> [1] "darwin6.8"
> 
> $system
> [1] "powerpc, darwin6.8"
> 
> $status
> [1] ""
> 
> $major
> [1] "2"
> 
> $minor
> [1] "0.1"
> 
> $year
> [1] "2004"
> 
> $month
> [1] "11"
> 
> $day
> [1] "15"
> 
> $language
> [1] "R"
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From greg.snow at ihc.com  Tue Nov 23 18:40:24 2004
From: greg.snow at ihc.com (Greg Snow)
Date: Tue, 23 Nov 2004 10:40:24 -0700
Subject: [R] Interactive plots in R?
Message-ID: <s1a313af.025@lp-msg1.co.ihc.com>

>>> Michal Blazejczyk <michal.blazejczyk at gmail.com> 11/23/04 09:21AM
>>>
> Hi all,
> 
> As part of a research project we are creating a statistical software
> tool and will be using R as the computational engine.  I was
wandering
> whether we should also use R for plotting.  R has a good plotting
> flexibility and an extensive library of available plot types.
However,
> our application is supposed to give our users an advanced
> look-and-feel as well.

R allows you quite a bit of flexibility and customization in plotting,
but you will 
have to decide if the look and feel matches with what you want.  I
address your
individual questions below.

>  For the plotting we will therefore need things
> like:
>  - interactivity (the possibility to interact with the plot using a
> mouse, e.g. select data points or data bars)

The identify command will identify points (optionally label them on the
plot, then tell you which point(s) were selected).

The locator command will give you the location where the user clicked
on the plot, you could use this with the output from barplot to figure
out which bar was clicked on.  If you imediatly replot the graph with
the selected bar/point drawn differently then it will show the
selecection any way you want. 

The one thing I don't know how to make it do, is to differentiate
selection by clicking vs. 
ctrl-clicking like some packages allow.

>  - zoom / scroll 

This is not automatic, but easily implemented.  One of the easiest ways
is using the tcltk
package.  I have code (most of it modified from code at 
http://www.wiwi.uni-bielefeld.de/~wolf/) that will plot a 3d surface,
then pop up a window
with sliders, moving the sliders will change the viewing angles, light
angles, etc. of the plot.
You can also easily write code that has the user click on the plot,
then you redraw the plot 
zoomed or panned (R is quick enough that I don't notice the redraw).

>  - custom colors for lines, data points etc. 

There are 657 named colors, or you can specify the color you want using
rgb, or hsv values.
see the col option on the help page for "par", you can specify a
different color for each
line/point if you really want to.

>  - conditional formatting (e.g. data points above a given threshold
> are red and bigger while others are regular)

simple, use something like: 
 plot(x,y, col=ifelse( z>M, 'red','black'), cex=ifelse( z>M, 1.5, 1) )

now points corresponding to variable z being greater than the value in
M will
be plotted 50% larger and red, others will be normal size and black.

>  - possibility to draw marker lines, custom-color bands and areas 

Functions points, abline, lines, segments, arrows, polygon, text (and
maybe some others) will
plot additions to the current plot, use these to create bands, areas,
or whatever you want.


>  - flexible axes, titles and legends 

The defaults are usually pretty good, but you can specify anything you
want (including 
math markup like fractions, integral signs, greek letters, etc.  see
the help on plotmath).

You can suppress the initial axis, then use the axis command to
customize the axis, telling it
exactly where to draw tickmarks and how to label them (or let it figure
out something that looks nice).

>  - axis grids 

see the tck option under the help for "par".  The command:

axis(side=2, tck=1, col='lightgrey', lty=4)
box()

will draw horizontal grid lines that are light grey and dashed, more
options are available to 
further customize this.

>  - export to image files (e.g. BMP) in high resolution 

see the bmp and dev.copy functions (also can save to postscript, pdf,
jpeg, png, and other formats).

>  
> Does anyone know how many of these things are achievable with R, to
> what extent and using which package?
>  
> Thanks in advance, 
>  
> Michal Blazejczyk 
> Lead Programmer 
> Genome Quebec




______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

Greg Snow, Ph.D.
Statistical Data Center
greg.snow at ihc.com
(801) 408-8111



From jahernan at umn.edu  Tue Nov 23 18:53:58 2004
From: jahernan at umn.edu (Jose A. Hernandez)
Date: Tue, 23 Nov 2004 11:53:58 -0600
Subject: [R] Quadratic curve on a loop
Message-ID: <41A37936.7000703@umn.edu>

Please disregard my message I've seen the light now...

curve(predict(out, data.frame(x = x)), add = T)

Thanks,

Jose

-- 
Jose A. Hernandez
Department of Soil, Water, and Climate
University of Minnesota
1991 Upper Buford Circle
St. Paul, MN 55108

Ph. (612) 625-0445, Fax. (612) 625-2208



From MSchwartz at MedAnalytics.com  Tue Nov 23 19:04:17 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Tue, 23 Nov 2004 12:04:17 -0600
Subject: [R] data.frame into vector
In-Reply-To: <p06100508bdc915092d0e@[83.132.28.79]>
References: <p06100508bdc915092d0e@[83.132.28.79]>
Message-ID: <1101233057.7049.11.camel@horizons.localdomain>

On Tue, 2004-11-23 at 16:27 +0000, Tiago R Magalhaes wrote:
> Hi
> 
> I want to extract a row from a data.frame but I want that object to 
> be a vector . After trying some different ways I end up always with a 
> data.frame or with the wrong vector. Any pointers?
> 
>   x <- data.frame(a = factor(c('a',2,'b')), b = c(4,5,6))
> I want to get
> "a" "4"
> 
> I tried:
> 
> as.vector(x[1,])
>    a b
> 1 a 4
> (resulting in a data.frame even after in my mind having coerced it 
> into a vector!)
> 
> as.vector(c[1,], numeric='character')
> [1] "2" "4"
> (almost what I want, except that "2" instead of "a" - I guess this as 
> to do with levels and factors)
> 
> Thanks for any help


Part of the problem that you are having, as you seem to pick up, is that
the first column in 'x' is a factor and you need to coerce it to a
character.

If you review the help for as.matrix, you will note in the details
section:

"as.matrix is a generic function. The method for data frames will
convert any non-numeric/complex column into a character vector using
format and so return a character matrix, except that all-logical data
frames will be coerced to a logical matrix."

Thus, one approach is:

> as.matrix(x)[1, ]
  a   b
"a" "4"

This coerces the data frame to a character matrix, which is then
subsetted to the first row only.

HTH,

Marc Schwartz



From elvis at xlsolutions-corp.com  Tue Nov 23 19:09:19 2004
From: elvis at xlsolutions-corp.com (elvis@xlsolutions-corp.com)
Date: Tue, 23 Nov 2004 11:09:19 -0700
Subject: [R] R/S System: Advanced Programming,
	San Francisco ***December 20-21, 2004
Message-ID: <20041123180919.9941.qmail@webmail03.mesa1.secureserver.net>

R/S Advanced Programming course in San Francisco on December 20-21st,
2004
Please check out the full description and Agenda of the 2-day class on
the website:

www.xlsolutions-corp.com/Radv.htm


And let us know if we should hold seats for you.
Ask for group discount!


Here's the outline:

Course outline:
- Overview of R/S fundamentals: Syntax and Semantics
- Class and Inheritance in R/S-Plus
- Concepts, Construction and good use of language objects
- Coercion and efficiency
- Object-oriented programming in R and S-Plus
- Advanced manipulation tools: Parse, Deparse, Substitute, etc.
- How to fully take advantage of Vectorization
- Generic and Method Functions; S4 (S-Plus 6)
- Search path, databases and frames Visibility
- Working with large objects
- Handling Properly Recursion and iterative calculations
- Managing loops; For (S-Plus) and for() loops
- Consequences of Lazy Evaluation
- Efficient Code practices for large computations
- Memory management and Resource monitoring
- Writing R/S-Plus functions to call compiled code
- Writing and debugging compiled code for R/S-Plus system
- Connecting R/S-Plus to External Data Sources
- Understanding the structure of model fitting functions in R/S-Plus
- Designing and Packaging efficiently 


This course will also deal with lots of S-Plus efficiency issues and
any special topics from participants is welcome.
Please let us know if you and your colleagues are interested in this
class to take advantage of group discount. Register now to secure your
seat 
in this course!

Cheers,
Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com



From MSchwartz at MedAnalytics.com  Tue Nov 23 19:47:57 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Tue, 23 Nov 2004 12:47:57 -0600
Subject: [R] IFELSE across large array?
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E36F@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E36F@usrymx25.merck.com>
Message-ID: <1101235677.21009.27.camel@horizons.localdomain>

On Tue, 2004-11-23 at 12:28 -0500, Liaw, Andy wrote:
> I'll give it half a crack:
> 
> Steps a through c can be done via nested ifelse(), treating A and M as
> vectors (as they really are).  Step d is the hard one.  I'd write a simple
> Fortran code and use .Fortran() for that.
> 
> I don't see how any of the *apply() functions can help here, as your
> operations are element-wise, not dimension-wise.
> 
> Andy

I'll toss in one additional thought here in follow up to Andy's, which
is that you could feasibly use mapply() to do element-wise operations. 

In fact, this is how I compute concordant and discordant pairs in a 2D
matrix for some of the measures of association that I want to include in
the CrossTable() function in the gregmisc bundle.

I'll paste the code for the concordant() function here, to give you an
idea of the approach:

# Calculate CONcordant Pairs in a table
# cycle through x[r, c] and multiply by
# sum(x elements below and to the right of x[r, c])
# x = table
concordant <- function(x)
{
  # get sum(matrix values > r AND > c)
  # for each matrix[r, c]
  mat.lr <- function(r, c)
  { 
    lr <- x[(r.x > r) & (c.x > c)]
    sum(lr)
  }

  # get row and column index for each
  # matrix element
  r.x <- row(x)
  c.x <- col(x)

  # return the sum of each matrix[r, c] * sums
  # using mapply to sequence thru each matrix[r, c]
  sum(x * mapply(mat.lr, r = r.x, c = c.x))
}


Presumably, you could extend the basic approach to a 3D structure, by
using a third index argument to the mapply() call and use something like
expand.grid() to create the three vectors for the indices into the 3D
array:

# Set up the index combinations for a 4 x 4 x 4 array
Index <- expand.grid(1:4, 1:4, 1:4)

YourFunction <- function(a, b, c)
{
   DoSomethingHere(YourArray[a, b, c])
}

YourReturnVals <- mapply(YourFunction, a = Index[, 1], b = Index[, 2], 
                         c = Index[, 3])


I am hoping that this approach might be applicable here, at least in
basic concept.  Also, keep in mind any scoping issues with respect to
your data structures.

HTH,

Marc Schwartz



From p.murrell at auckland.ac.nz  Tue Nov 23 21:05:21 2004
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Wed, 24 Nov 2004 09:05:21 +1300
Subject: [R] How to correct this
References: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA31@afhex01.dpi.wa.gov.au>
Message-ID: <41A39801.9080400@stat.auckland.ac.nz>

Hi


Mulholland, Tom wrote:
 > This raises the question of "best practice." My answer was predicated
 > on the fact that Jin Li had been attempting to use grid.circle in the
 > first place without success. I rashly made the assumption that there
 > was already a move to try and use some of the more sophisticated
 > techniques within R.
 >
 > This is a good example of the comments in the "hidden costs" thread,
 > where the pathways to learning R came under some scrutiny. It is also
 > similar to the "[R] How to insert one element into a vector?" where
 > it is noted that append can be used to insert the element. That is
 > the function appears to be originally written for one purpose, but it
 > is evident that it has a broader application that is not immediately
 > recognizable from the function name. When you are new to R it can
 > seem confusing that you use rect for rectangles but symbols for
 > circles, or segments for lines and lines for not lines, but they
 > really are lines.
 >
 > I am not yet proficient enough to always know which is the best
 > approach. That's even with defining best as quickest, most easily
 > maintained or most readable etc etc.
 >
 > Now to the point. I have formed a collection of graphics that I have
 > prepared over the last two years which I use to remind myself of the
 > little idiosyncrasies of the various techniques. These of course have
 > evolved as I have. They mostly use data that I cannot make available.
 > I thought it might be a good idea to produce reproducible code that
 > shows the bewildering variety ways to skin the proverbial animal.
 > That is to produce code that can create a PDF flipbook of plots. One
 > of the first things that I do when I load a package, is to run the
 > examples that produce graphical output. I tend to work backwards and
 > understand processes better when I know what the final output looks
 > like. I am mathematically challenged, but can often appreciate what
 > is happening once I see the plot. Ideally the code would include all
 > the bells and whistles. I say this because I have spent hours trying
 > to figure out just exactly what something is supposed to do before
 > finally figuring out that it was really much simpler than I had
 > thought. The bells and whistles should also show how you sometimes
 > have to use par outside of the function (or remember that the ... is
 > there for a reason) to get the effect that you want. For example when
 > I load the vcd package to do mosaicplots I think I have to use
 > par(xpd = TRUE) to get my multi-line labels not to be clipped.
 >
 > As an evolving beast I see this as a way of demonstrating the
 > techniques that are generally regarded as being "best practice" in a
 > comprehensive manner.
 >
 > In short I am volunteering. What for? I am not quite sure, but it
 > includes example plots using data that helps in clarifying how the
 > plot should be used. The last point means that I am not capable of
 > producing some plots (and the examples in some packages already do
 > this well) as I have no idea what they mean even when I have plotted
 > the example.


What this sounds like to me is an "R graphics cookbook", which I think 
would be a good idea, though have you looked at, for example, the 
"Graphiques avec R" section of Vincent Zoonekynd's "Statistiques avec R"
(http://zoonek2.free.fr/UNIX/48_R/all.html)
or the "Graphing" section of Paul Johnson's "R tips" page 
(http://www.ku.edu/~pauljohn/R/Rtips.html)?

Paul


 > -----Original Message----- From: Paul Murrell
 > [mailto:p.murrell at auckland.ac.nz] Sent: Tuesday, 23 November 2004
 > 3:05 AM To: Mulholland, Tom Cc: Jin.Li at csiro.au;
 > r-help at stat.math.ethz.ch Subject: Re: [R] How to correct this
 >
 >
 > Hi
 >
 >
 > Mulholland, Tom wrote:
 >
 >> Taking note of the first post, this is what I assume you wish. Note
 >> Paul's caveat in the help file
 >>
 >> "If you resize the device, all bets are off!"
 >>
 >> require(gridBase) x<-seq(0,1,0.2) y<-x pred<-matrix(c(0.5, 0.5,
 >> 0.5, 0.5, 0.5, 0.5, 0.5, 0.7, 0.7, 0.7, 0.7, 0.5, 0.5, 0.7, 0.9,
 >> 0.9, 0.7, 0.5, 0.5, 0.7, 0.9, 0.9, 0.7, 0.5, 0.5, 0.7, 0.7, 0.7,
 >> 0.7, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5), 6, 6) image(x, y, pred,
 >> col = gray(20:100/100), asp='s', axes=F, xlab=" ", ylab="")
 >> points(0.5, 0.5, col = 5) # the centre of the image
 >
 >
 >
 > In this case, using grid (or gridBase) is probably overkill.  The
 > symbols() function should do what you want.  For example, ...
 >
 > symbols(rep(0.5, 4), rep(0.5, 4), circles=1:4, add=TRUE)
 >
 > Paul
 >
 >
 >
 >> vps <- baseViewports() pushViewport(vps$plot) grid.circle(x=0.5,
 >> y=0.5, r=0.1, draw=TRUE,  gp=gpar(col=5)) grid.circle(x=0.5, y=0.5,
 >> r=0.3, draw=TRUE, gp=gpar(col=5)) grid.circle(x=0.5, y=0.5, r=0.5,
 >> draw=TRUE, gp=gpar(col=5))
 >>
 >>
 >>
 >> -----Original Message----- From: Jin.Li at csiro.au
 >> [mailto:Jin.Li at csiro.au] Sent: Monday, 22 November 2004 1:21 PM To:
 >> r-help at stat.math.ethz.ch Subject: RE: [R] How to correct this
 >>
 >>
 >> Hi there,
 >>
 >> I would like to add a few circles to the following image:
 >> x<-seq(0,1,0.2) y<-x pred<-matrix(c(0.5, 0.5, 0.5, 0.5, 0.5, 0.5,
 >> 0.5, 0.7, 0.7, 0.7, 0.7, 0.5, 0.5, 0.7, 0.9, 0.9, 0.7, 0.5, 0.5,
 >> 0.7, 0.9, 0.9, 0.7, 0.5, 0.5, 0.7, 0.7, 0.7, 0.7, 0.5, 0.5, 0.5,
 >> 0.5, 0.5, 0.5, 0.5), 6, 6) image(x, y, pred, col =
 >> gray(20:100/100), asp='s', axes=F, xlab=" ", ylab="") points(0.5,
 >> 0.5, col = 5) # the centre of the image
 >>
 >> The centre of these circles needs to be overlapped with the centre
 >> of the image. Any helps are greatly appreciated. Regards, Jin
 >>
 >>
 >> -----Original Message----- From: Mulholland, Tom
 >> [mailto:Tom.Mulholland at dpi.wa.gov.au] Sent: Monday, 22 November
 >> 2004 12:29 P To: Li, Jin (CSE, Atherton) Subject: RE: [R] How to
 >> correct this
 >>
 >> I think you need to create a complete set of code that can be
 >> replicated by anyone trying to help. I ran the three grid.circle
 >> commands on my current plot and it did what I expected it to do. It
 >> plotted three circles centred in the current viewport. See the
 >> jpeg.
 >>
 >> The last command using points makes me think that you need to
 >> understand about units and the setting up of viewports. I have not
 >> played around with this much but I think thr newsletter had an
 >> article which may be of use (although it uses old code I think the
 >> differences are minor)
 >>
 >> Ciao, Tom
 >>
 >> -----Original Message----- From: Jin.Li at csiro.au
 >> [mailto:Jin.Li at csiro.au] Sent: Monday, 22 November 2004 10:07 AM
 >> To: r-help at stat.math.ethz.ch Subject: [R] How to correct this
 >>
 >>
 >> Hi there,
 >>
 >>
 >>
 >> I tried to add a few circles on an existing figure using the
 >> following codes
 >>
 >>
 >>
 >> grid.circle(x=0.5, y=0.5, r=0.1, draw=TRUE,  gp=gpar(col=5))
 >>
 >> grid.circle(x=0.5, y=0.5, r=0.3, draw=TRUE, gp=gpar(col=5))
 >>
 >> grid.circle(x=0.5, y=0.5, r=0.5, draw=TRUE, gp=gpar(col=5))
 >>
 >> points(0.5, 0.5, col = 5) # centre of the circle
 >>
 >>
 >>
 >> , but all circles moved away from the centre.  Could we do any
 >> corrections to this? Thanks.
 >>
 >>
 >>
 >> Regards,
 >>
 >>
 >>
 >> Jin
 >>
 >> ==========================
 >>
 >> Jin Li, PhD
 >>
 >> Climate Impacts Modeller
 >>
 >> CSIRO Sustainable Ecosystems
 >>
 >> Atherton, QLD 4883
 >>
 >> Australia
 >>
 >> Ph: 61 7 4091 8802
 >>
 >> Email: jin.li at csiro.au <mailto:jin.li at csiro.au>
 >>
 >> ==========================
 >>
 >>
 >>
 >>
 >> [[alternative HTML version deleted]]
 >>
 >> ______________________________________________
 >> R-help at stat.math.ethz.ch mailing list
 >> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the
 >> posting guide! http://www.R-project.org/posting-guide.html
 >>
 >> ______________________________________________
 >> R-help at stat.math.ethz.ch mailing list
 >> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the
 >> posting guide! http://www.R-project.org/posting-guide.html
 >>
 >> ______________________________________________
 >> R-help at stat.math.ethz.ch mailing list
 >> https://stat.ethz.ch/mailman/listinfo/r-help PLEASE do read the
 >> posting guide! http://www.R-project.org/posting-guide.html
 >
 >
 >


-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From arshia22 at yahoo.com  Tue Nov 23 21:54:40 2004
From: arshia22 at yahoo.com (ebashi)
Date: Tue, 23 Nov 2004 12:54:40 -0800 (PST)
Subject: [R] How to extract data?
Message-ID: <20041123205440.37889.qmail@web81002.mail.yahoo.com>

I appreciate if anyone can help me,
I have a table as follow,
> rate
          DATE VALUE
1   1997-01-10  5.30
2   1997-01-17  5.30
3   1997-01-24  5.28
4   1997-01-31  5.30
5   1997-02-07  5.29
6   1997-02-14  5.26
7   1997-02-21  5.24
8   1997-02-28  5.26
9   1997-03-07  5.30
10  1997-03-14  5.30
.    ......     ...
.    ......     ...
.    ......     ...
I want to extract the DATE(s) on which the VALUE has
already dropped twice and the DATE(s) that VALUE has
already increased for three times,( ignore where
VALUE(i+1)-VALUE(i)=0),I try to use diff() function,
however that works only for one increase or decrease.

Sincerely,

Sean



From jahernan at umn.edu  Tue Nov 23 22:18:18 2004
From: jahernan at umn.edu (Jose A. Hernandez)
Date: Tue, 23 Nov 2004 15:18:18 -0600
Subject: [R] Help me to migrate from SAS
Message-ID: <41A3A91A.5070409@umn.edu>

R-community,

Assuming a file with 3 columns: site, nrate, yield. In SAS I can easily
run the analysis BY SITE using the following code:

*--------------------------------------------------*;
proc nlin method=MARQUARDT;
by farm;
parms b0=100 b1=0.33 b2=-0.00111 x0=120;
model yield = (b0 + b1 * nrate + b2 * nrate * nrate) * (nrate <= x0) +
(b0 + b1 * x0 + b2 * x0 * x0) * (nrate>x0);
output out=out predicted=pred ess=rss parms=b0 b1 b2 x0;
*--------------------------------------------------*;

In R I tried using nls in a loop, but it appears the analysis is very 
sensitive to the initial values and I keep getting the "singular 
gradient matrix at initial parameter estimates" error.

Any ideas on how to work around this issue would be greatly appreciated.

Thanks,

# Example data
site <-  c(1,1,1,1,1,1,2,2,2,2,2,2)
nrate <- c(0,60,90,120,150,180,0,60,90,120,150,180)
yield <- 
c(161.7,187.1,188.5,196.6,196.0,196.5,160.7,186.1,189.5,194.6,195.0,198.5)
site <- as.factor(site)
bar_1 <- NULL
for (i in 1:length(levels(site))) {
     j <- site == levels(site)[i]
     x <- nrate[j]
     y <- yield[j]
     nls.st <- list(b0=140, b1=0.5, b2=-0.002, x0=125)
     out<- try(nls(y ~ (b0 + b1*x + b2*I(x^2))*(x <= x0)
               + (b0 + b1*x0 + b2*I(x0^2))*(x > x0),
               start = nls.st))
     fit1 <- summary(out)$parameters
     qp.resi <- summary(out)$residuals
     rss <- sum(qp.resi^2)
     bar_1 <- rbind(bar_1, c(i, fit1[1, 1:2], fit1[2, 1:2], fit1[3, 
1:2],
     fit1[4, 1:2], rss))
}
dimnames(bar_1) <- list(NULL, c("Site", "b0", "s.e.", "b1", "s.e.", 
"b2", "s.e.", "x0", "s.e.", "RSS"))
print(bar_1)



From james.holtman at convergys.com  Tue Nov 23 22:27:45 2004
From: james.holtman at convergys.com (james.holtman@convergys.com)
Date: Tue, 23 Nov 2004 16:27:45 -0500
Subject: [R] How to extract data?
Message-ID: <OFBBCB2ECC.F8077073-ON85256F55.00755F8B@nd.convergys.com>





By 'ignore', can we delete those from the list of data?  I would then
assume that if you have a sequence of +0+0+ that you would want the last
"+" for the increase of three.

If that is the case, then do a 'diff' and delete the entries that are 0.
Then create a new 'diff' and then use 'rle' to see what the length of the
sequences are:

> x <- c(1,2,2,3,3,4,3,3,2,2,2,1)
> x
 [1] 1 2 2 3 3 4 3 3 2 2 2 1
> x.d <- diff(x)
> x.d
 [1]  1  0  1  0  1 -1  0 -1  0  0 -1
> x.new <- x[c(x.d,1) != 0]
> x.new
[1] 1 2 3 4 3 2 1
> x.d1 <- diff(x.new)
> x.d1
[1]  1  1  1 -1 -1 -1
> rle(x.d1)
Run Length Encoding
  lengths: int [1:2] 3 3
  values : num [1:2] 1 -1
>

you can check the results of 'rle' to determine where the changes are.
__________________________________________________________
James Holtman        "What is the problem you are trying to solve?"
Executive Technical Consultant  --  Office of Technology, Convergys
james.holtman at convergys.com
+1 (513) 723-2929


                                                                                                                                           
                      ebashi                                                                                                               
                      <arshia22 at yahoo.com>         To:       r-sig-finance at stat.math.ethz.ch, r-help at stat.math.ethz.ch                     
                      Sent by:                     cc:                                                                                     
                      r-help-bounces at stat.m        Subject:  [R] How to extract data?                                                      
                      ath.ethz.ch                                                                                                          
                                                                                                                                           
                                                                                                                                           
                      11/23/2004 15:54                                                                                                     
                                                                                                                                           
                                                                                                                                           




I appreciate if anyone can help me,
I have a table as follow,
> rate
          DATE VALUE
1   1997-01-10  5.30
2   1997-01-17  5.30
3   1997-01-24  5.28
4   1997-01-31  5.30
5   1997-02-07  5.29
6   1997-02-14  5.26
7   1997-02-21  5.24
8   1997-02-28  5.26
9   1997-03-07  5.30
10  1997-03-14  5.30
.    ......     ...
.    ......     ...
.    ......     ...
I want to extract the DATE(s) on which the VALUE has
already dropped twice and the DATE(s) that VALUE has
already increased for three times,( ignore where
VALUE(i+1)-VALUE(i)=0),I try to use diff() function,
however that works only for one increase or decrease.

Sincerely,

Sean

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Tue Nov 23 22:27:56 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 23 Nov 2004 16:27:56 -0500
Subject: [R] Help me to migrate from SAS
Message-ID: <3A822319EB35174CA3714066D590DCD50994E374@usrymx25.merck.com>

Perhaps you would be interested in the nlsList() function in the nlme
package.

HTH,
Andy

> From: Jose A. Hernandez
> 
> R-community,
> 
> Assuming a file with 3 columns: site, nrate, yield. In SAS I 
> can easily
> run the analysis BY SITE using the following code:
> 
> *--------------------------------------------------*;
> proc nlin method=MARQUARDT;
> by farm;
> parms b0=100 b1=0.33 b2=-0.00111 x0=120;
> model yield = (b0 + b1 * nrate + b2 * nrate * nrate) * (nrate <= x0) +
> (b0 + b1 * x0 + b2 * x0 * x0) * (nrate>x0);
> output out=out predicted=pred ess=rss parms=b0 b1 b2 x0;
> *--------------------------------------------------*;
> 
> In R I tried using nls in a loop, but it appears the analysis is very 
> sensitive to the initial values and I keep getting the "singular 
> gradient matrix at initial parameter estimates" error.
> 
> Any ideas on how to work around this issue would be greatly 
> appreciated.
> 
> Thanks,
> 
> # Example data
> site <-  c(1,1,1,1,1,1,2,2,2,2,2,2)
> nrate <- c(0,60,90,120,150,180,0,60,90,120,150,180)
> yield <- 
> c(161.7,187.1,188.5,196.6,196.0,196.5,160.7,186.1,189.5,194.6,
> 195.0,198.5)
> site <- as.factor(site)
> bar_1 <- NULL
> for (i in 1:length(levels(site))) {
>      j <- site == levels(site)[i]
>      x <- nrate[j]
>      y <- yield[j]
>      nls.st <- list(b0=140, b1=0.5, b2=-0.002, x0=125)
>      out<- try(nls(y ~ (b0 + b1*x + b2*I(x^2))*(x <= x0)
>                + (b0 + b1*x0 + b2*I(x0^2))*(x > x0),
>                start = nls.st))
>      fit1 <- summary(out)$parameters
>      qp.resi <- summary(out)$residuals
>      rss <- sum(qp.resi^2)
>      bar_1 <- rbind(bar_1, c(i, fit1[1, 1:2], fit1[2, 1:2], fit1[3, 
> 1:2],
>      fit1[4, 1:2], rss))
> }
> dimnames(bar_1) <- list(NULL, c("Site", "b0", "s.e.", "b1", "s.e.", 
> "b2", "s.e.", "x0", "s.e.", "RSS"))
> print(bar_1)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From andy_liaw at merck.com  Tue Nov 23 22:31:23 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 23 Nov 2004 16:31:23 -0500
Subject: [R] The hidden costs of GPL software?
Message-ID: <3A822319EB35174CA3714066D590DCD50994E375@usrymx25.merck.com>

> From: Patrick Burns
> 
> I think John has exactly the right image -- index to a book --
> but I disagree with his conclusions.
> 
> I read somewhere that an index should not be done by the
> author.  It was probably written by someone who was bored
> of indexing, but the logic was precisely because indices should
> be about concepts.  The author of a package will have one
> concept for a function but not all of the concepts that come
> from various fields of study.  I suspect that no one outside of
> finance would think to index "sd" with "volatility" for (a not very
> good) example.
> 
> There could be an index builder that accepts a search phrase and
> the function or package that is the successful answer to the search.
> If this were open, then R users could contribute to the index who
> don't feel qualified to submit code. It could also help diffuse the
> frustration of taking too long to find a function by allowing a way
> to insure that the exact same thing doesn't happen to others.
> 
> Amazon has a function that says those who bought "The Chicago
> Manual of Style" also bought Strunk and White.

Would that be the same function that suggested bunch of books on fashion
modeling when I look up Frank's book (`Regression Modeling Strategies')? 8-)

Andy

>  In the same way,
> the R index could provide a list of terms that overlap the given
> search term.  For example if we search for "goodness of fit", then
> "hypothesis test" might be one of the related terms that pops up.
> 
> No, I'm not volunteering to build the system.
> 
> Patrick Burns
> 
> Burns Statistics
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
> 
> John Fox wrote:
> 
> >Dear Duncan,
> >
> >I don't think that there is an automatic, nearly costless 
> way of providing
> >an effective solution to locating R resources. The problem 
> seems to me to be
> >analogous to indexing a book. There's an excellent 
> description of what that
> >process *should* look like in the Chicago Manual of Style, 
> and it's a lot of
> >work. In my experience, most book indexes are quite poor, 
> and automatically
> >generated indexes, while not useless, are even worse, since 
> one should index
> >concepts, not words. The ideal indexer is therefore the 
> author of the book.
> >
> >I guess that the question boils down to how important is it 
> to provide an
> >analogue of a good index to R? As I said in a previous 
> message, I believe
> >that the current search facilities work pretty well -- about 
> as well as one
> >could expect of an automatic approach. I don't believe that 
> there's an
> >effective centralized solution, so doing something more 
> ambitious than is
> >currently available implies farming out the process to 
> package authors. Of
> >course, there's no guarantee that all package authors will 
> be diligent
> >indexers. 
> >
> >Regards,
> > John
> >
> >--------------------------------
> >John Fox
> >Department of Sociology
> >McMaster University
> >Hamilton, Ontario
> >Canada L8S 4M4
> >905-525-9140x23604
> >http://socserv.mcmaster.ca/jfox 
> >-------------------------------- 
> >
> >  
> >
> >>-----Original Message-----
> >>From: r-help-bounces at stat.math.ethz.ch 
> >>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Duncan Murdoch
> >>Sent: Monday, November 22, 2004 8:55 AM
> >>To: Cliff Lunneborg
> >>Cc: r-help at stat.math.ethz.ch
> >>Subject: Re: [R] The hidden costs of GPL software?
> >>
> >>On Fri, 19 Nov 2004 13:59:23 -0800, "Cliff Lunneborg"
> >><cliff at ms.washington.edu> quoted John Fox:
> >>
> >>    
> >>
> >>>Why not, as previously has been proposed, replace the 
> current static 
> >>>(and, in my view, not very useful) set of keywords in R 
> >>>      
> >>>
> >>documentation 
> >>    
> >>
> >>>with the requirement that package authors supply their own 
> >>>      
> >>>
> >>keywords for 
> >>    
> >>
> >>>each documented object? I believe that this is the intent of the 
> >>>concept entries in Rd files, but their use certainly is not 
> >>>      
> >>>
> >>required or 
> >>    
> >>
> >>>even actively encouraged. (They're just mentioned in 
> passing in the 
> >>>Writing R Extensions manual.
> >>>      
> >>>
> >>That would not be easy and won't happen quickly.  There are some
> >>problems:
> >>
> >> - The base packages mostly don't use  \concept. (E.g. base 
> >>has 365 man pages, only about 15 of them use it).  Adding it 
> >>to each file is a fairly time-consuming task.
> >>
> >>- Before we started, we'd need to agree as to what they are for.
> >>Right now, I think they are mainly used when the name of a 
> >>concept doesn't match the name of the function that 
> >>implements it, e.g.
> >>"modulo", "remainder", "promise", "argmin", "assertion".  The 
> >>need for this usage is pretty rare.  If they were used for 
> >>everything, what would they contain?
> >>
> >> - Keywording in a useful way is hard.  There are spelling 
> >>issues (e.g. optimise versus optimize); our fuzzy matching 
> >>helps with those.
> >>But there are also multiple names for the same thing, and 
> >>multiple meanings for the same name.
> >>
> >>Duncan Murdoch
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide! 
> >>http://www.R-project.org/posting-guide.html
> >>    
> >>
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://stat.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> >
> >
> >
> >  
> >
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From phgrosjean at sciviews.org  Tue Nov 23 22:36:12 2004
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Tue, 23 Nov 2004 22:36:12 +0100
Subject: [R] The hidden costs of GPL software?
In-Reply-To: <41A36941.6010406@pburns.seanet.com>
Message-ID: <200411232136.iANLaFVW010276@outmx010.isp.belgacom.be>

Patrick Burns wrote:
> [....]
> No, I'm not volunteering to build the system.

Too bad! ;-)

Indeed, the idea to index tens of thousands of functions could not be
appealing to many of us! Why not to consider to test such ideas at the
package level? I mean, building a system that points out the packages of
interest (those in CRAN, of course), given a search phrase would be a more
resonable work. Then, looking at online help of that particular package
would be the small additional effort required by the user. The problem here
is with heterogeneous packages (the XXXXmisc, and the like)...

And... No I'm not volunteering to build the system either.

Best,

Philippe Grosjean



From andy_liaw at merck.com  Tue Nov 23 22:40:09 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 23 Nov 2004 16:40:09 -0500
Subject: [R] Quadratic curve on a loop
Message-ID: <3A822319EB35174CA3714066D590DCD50994E376@usrymx25.merck.com>

AFAICS, there are two errors:

1. curve() is probably not what you want: it plots functions, and that's not
what predict() gives you.

2. predict() needs to be given the correct data frame: The name of the
variable(s) need to match those used in the formula.  The formula has `x' on
the righthand side, so the data frame given to predict() needs to have a
variable named `x', otherwise `x' will be looked up elsewhere, and if it
finds one, it's unlikely to be the one you intended.

Try something like:

lines(predict(out, newdata=data.frame(x=seq(min(x), max(x), length=51)))

HTH,
Andy

> From: Jose A. Hernandez
> 
> Dear R community, once again I request your generous help on 
> an R issue 
> I cannot seem to figure out.
> 
> I am trying to do some analysis for multiple sites, in my 
> example I am 
> using only two, but the same will done on many sites. It's just ONE 
> line, so I hope somebody can give a me a hand by email.
> 
> Look at the loop to make the plot x ~ y. If the curve function is 
> removed from the loop everything works fine, but when I tried 
> to add the 
> curve to the plot, I get the following error:
> 
> Error in xy.coords(x, y) : x and y lengths differ
> In addition: Warning message:
> 'newdata' had 101 rows but variable(s) found have 6 rows
> 
> I know this error should be a hint about how to proceed but I 
> am stuck 
> figuring ways to make the curve to print in the plots.
> 
> Any hints on how to solve this issue would be greatly appreciated.
> 
> Sincerely,
> 
> Jose
> 
> PS.
>  > version
>           _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    2
> minor    0.1
> year     2004
> month    11
> day      15
> language R
> 
> 
> 
> ###################################################################
> # some data
> site <- c(1,1,1,1,1,1,2,2,2,2,2,2)
> nrate <- c(0,60,90,120,150,180,0,60,90,120,150,180)
> yield <- 
> c(161.7,187.1,188.5,196.6,196.0,196.5,160.7,186.1,189.5,194.6,
> 195.0,198.5)
> site <- as.factor(site)
> 
> # file name
> fname <- levels(site)
> i <- nchar(fname) == 1
> fname[i] <- paste("0", fname[i], sep = "")
> fname <- paste("site", fname, ".eps", sep = "")
> 
> ###################################################################
> ###################################################################
> 
> for (i in 1:length(levels(site))) {
>      j <- site == levels(site)[i]
>      x <- nrate[j]
>      y <- yield[j]
>      out <- try(lm(y ~ x + I(x^2)), silent = TRUE)
>      plot(y ~ x,
>           pch = 16,
>           xlab = expression(paste("Nitrogen rate [lbs ac" ^-1,"]")),
>           ylab = expression(paste("Corn yield [bu ac"^-1,"]")))
>           curve(predict(out, data.frame(nrate = x)), add = T)
>           title(paste("Site", i))
>           # dev.copy2eps(file = fname[i], horizontal = TRUE)
> }
> ###################################################################
> ###################################################################
> 
> 
> 
> -- 
> Jose A. Hernandez
> Department of Soil, Water, and Climate
> University of Minnesota
> 1991 Upper Buford Circle
> St. Paul, MN 55108
> 
> Ph. (612) 625-0445, Fax. (612) 625-2208
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From deepayan at stat.wisc.edu  Tue Nov 23 23:10:16 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Tue, 23 Nov 2004 16:10:16 -0600
Subject: [R] Convergence problem in GLMM
In-Reply-To: <20041123171443.VKXM2034.tomts20-srv.bellnexxia.net@JohnDesktop8300>
References: <20041123171443.VKXM2034.tomts20-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <200411231610.16682.deepayan@stat.wisc.edu>

On Tuesday 23 November 2004 11:14, John Fox wrote:
> Dear list members,
>
> In re-running with GLMM() from the lme4 package a generalized-linear
> mixed model that I had previously fit with glmmPQL() from MASS, I'm
> getting a warning of a convergence failure, even when I set the
> method argument of
>
> GLMM() to "PQL":
> > bang.mod.1 <- glmmPQL(contraception ~ as.factor(children) + cage +
> > urban,
>
> +     random=~as.factor(children) + cage + urban|district,
> +     family=binomial, data=Bangladesh)  # works!
>
> > bang.mod.1 <- GLMM(contraception ~ as.factor(children) + cage +
> > urban,
>
> +     random=~as.factor(children) + cage + urban|district,
> +     family=binomial, data=Bangladesh, method="PQL")
> Warning message:
> IRLS iterations for glmm did not converge in: GLMM(formula =
> contraception ~ as.factor(children) + cage + urban,

This dataset is also in lme4 as 'Contraception'.

> Despite the indicated convergence failure, the two sets of estimates
> are quite close, as one would expect. I've also played around with
> various arguments to lmeControl(), but without success.
>
> I'm curious about the source of the difference, since (as I
> understand it), essentially the same algorithm is used by the two
> functions. I'm using R 2.0.1 under Windows XP with the current
> version of lme4.

Probably because the convergence criterion is different. glmmPQL has 

        if (sum((eta - etaold)^2) < 1e-06 * sum(eta^2)) 
            break

GLMM has 

         crit <- max(abs(eta - etaold)) / (0.1 + max(abs(eta)))
         ## use this to determine convergence
         if (crit < controlvals$tolerance) {
              conv <- TRUE
              break
         }

I'm not sure why we chose that. Setting 

control = list(tolerance = 1e-3, PQLmaxIt = 100)

converges for me, but that's probably a bit extreme.

Deepayan



From deepayan at stat.wisc.edu  Tue Nov 23 23:10:16 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Tue, 23 Nov 2004 16:10:16 -0600
Subject: [R] Convergence problem in GLMM
In-Reply-To: <20041123171443.VKXM2034.tomts20-srv.bellnexxia.net@JohnDesktop8300>
References: <20041123171443.VKXM2034.tomts20-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <200411231610.16682.deepayan@stat.wisc.edu>

On Tuesday 23 November 2004 11:14, John Fox wrote:
> Dear list members,
>
> In re-running with GLMM() from the lme4 package a generalized-linear
> mixed model that I had previously fit with glmmPQL() from MASS, I'm
> getting a warning of a convergence failure, even when I set the
> method argument of
>
> GLMM() to "PQL":
> > bang.mod.1 <- glmmPQL(contraception ~ as.factor(children) + cage +
> > urban,
>
> +     random=~as.factor(children) + cage + urban|district,
> +     family=binomial, data=Bangladesh)  # works!
>
> > bang.mod.1 <- GLMM(contraception ~ as.factor(children) + cage +
> > urban,
>
> +     random=~as.factor(children) + cage + urban|district,
> +     family=binomial, data=Bangladesh, method="PQL")
> Warning message:
> IRLS iterations for glmm did not converge in: GLMM(formula =
> contraception ~ as.factor(children) + cage + urban,

This dataset is also in lme4 as 'Contraception'.

> Despite the indicated convergence failure, the two sets of estimates
> are quite close, as one would expect. I've also played around with
> various arguments to lmeControl(), but without success.
>
> I'm curious about the source of the difference, since (as I
> understand it), essentially the same algorithm is used by the two
> functions. I'm using R 2.0.1 under Windows XP with the current
> version of lme4.

Probably because the convergence criterion is different. glmmPQL has 

        if (sum((eta - etaold)^2) < 1e-06 * sum(eta^2)) 
            break

GLMM has 

         crit <- max(abs(eta - etaold)) / (0.1 + max(abs(eta)))
         ## use this to determine convergence
         if (crit < controlvals$tolerance) {
              conv <- TRUE
              break
         }

I'm not sure why we chose that. Setting 

control = list(tolerance = 1e-3, PQLmaxIt = 100)

converges for me, but that's probably a bit extreme.

Deepayan



From Mike.Prager at noaa.gov  Tue Nov 23 23:10:01 2004
From: Mike.Prager at noaa.gov (Mike Prager)
Date: Tue, 23 Nov 2004 17:10:01 -0500
Subject: [R] The hidden costs of GPL software?
In-Reply-To: <41A36941.6010406@pburns.seanet.com>
References: <20041123134825.SNEN1863.tomts10-srv.bellnexxia.net@JohnDesktop8300>
	<41A36941.6010406@pburns.seanet.com>
Message-ID: <6.1.2.0.2.20041123170737.01ec5380@hermes.nos.noaa.gov>

At 11/23/2004 11:45 AM Tuesday, Patrick Burns wrote:

>...There could be an index builder that accepts a search phrase and
>the function or package that is the successful answer to the search.
>If this were open, then R users could contribute to the index who
>don't feel qualified to submit code. It could also help diffuse the
>frustration of taking too long to find a function by allowing a way
>to insure that the exact same thing doesn't happen to others.
>
>[...] No, I'm not volunteering to build the system.

Nor am I, but as one of those users, I would very gladly contribute to it.


-- 
Michael Prager, Ph.D.
Population Dynamics Team, NMFS SE Fisheries Science Center
NOAA Center for Coastal Fisheries and Habitat Research
Beaufort, North Carolina  28516
http://shrimp.ccfhrb.noaa.gov/~mprager/



From eric.lim at cvsnet.org  Tue Nov 23 23:21:01 2004
From: eric.lim at cvsnet.org (Eric Lim)
Date: Tue, 23 Nov 2004 22:21:01 -0000
Subject: [R] Weibull survival regression
Message-ID: <000801c4d1aa$c0f47bf0$836a9951@youru989fckmtn>

A big thank you to everyone who responded...

Using Ales Ziberna solution with the lung dataset, I obtained:

> plot(survfit(Surv(time,status),data=lung))

Error in Surv(time, status) : Time variable is not numeric
In addition: Warning message: 
is.na() applied to non-(list or vector) in: is.na(time)


Using Thomas Lumley's solution with the lung dataset, I obtained:

> curve(pweibull(x, scale=1/coef(lung.wbs), shape=1/lung.wbs$scale,
      lower.tail=FALSE),from=0, to=max(lung$time))

A Weibull survival plot appearing as a 90 degree `L' at the extremes of
the plot

I managed to obtain the solution by combining the two solutions:

lung.wbs <- survreg( Surv(futime, status)~ 1, data=lung, dist='weibull')

plot (survfit(Surv(futime,status)~1, data=lung))

curve (exp (- (exp( lung.wbs$coef[1])*x)^(1/lung.wbs$scale)),
col="black", add=T, from=0, to=30)

Kind regards,

Eric Lim
Papworth Hospital 
Cambridge, UK



From jfox at mcmaster.ca  Wed Nov 24 00:18:34 2004
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 23 Nov 2004 18:18:34 -0500
Subject: [R] Convergence problem in GLMM
In-Reply-To: <200411231610.16682.deepayan@stat.wisc.edu>
Message-ID: <20041123231834.YGSB1919.tomts22-srv.bellnexxia.net@JohnDesktop8300>

Dear Deepayan,

Thanks for the explanation (and for pointing out that the data set is also
in the lme4 package). I had tried control=lmeControl(tolerance=1e-4,
PQLmaxIt=100) and control=lmeControl(PQLmaxIt=1000), but gave up at that
point. I just tried lmeControl(tolerance=1e-4, PQLmaxIt=1000), and that
works. I guess I just was insufficiently persistent.

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: Deepayan Sarkar [mailto:deepayan at stat.wisc.edu] 
> Sent: Tuesday, November 23, 2004 5:10 PM
> To: r-help at stat.math.ethz.ch
> Cc: John Fox; 'R-Help'
> Subject: Re: [R] Convergence problem in GLMM
> 
> On Tuesday 23 November 2004 11:14, John Fox wrote:
> > Dear list members,
> >
> > In re-running with GLMM() from the lme4 package a 
> generalized-linear 
> > mixed model that I had previously fit with glmmPQL() from MASS, I'm 
> > getting a warning of a convergence failure, even when I set 
> the method 
> > argument of
> >
> > GLMM() to "PQL":
> > > bang.mod.1 <- glmmPQL(contraception ~ as.factor(children) 
> + cage + 
> > > urban,
> >
> > +     random=~as.factor(children) + cage + urban|district,
> > +     family=binomial, data=Bangladesh)  # works!
> >
> > > bang.mod.1 <- GLMM(contraception ~ as.factor(children) + cage + 
> > > urban,
> >
> > +     random=~as.factor(children) + cage + urban|district,
> > +     family=binomial, data=Bangladesh, method="PQL")
> > Warning message:
> > IRLS iterations for glmm did not converge in: GLMM(formula = 
> > contraception ~ as.factor(children) + cage + urban,
> 
> This dataset is also in lme4 as 'Contraception'.
> 
> > Despite the indicated convergence failure, the two sets of 
> estimates 
> > are quite close, as one would expect. I've also played around with 
> > various arguments to lmeControl(), but without success.
> >
> > I'm curious about the source of the difference, since (as I 
> understand 
> > it), essentially the same algorithm is used by the two 
> functions. I'm 
> > using R 2.0.1 under Windows XP with the current version of lme4.
> 
> Probably because the convergence criterion is different. glmmPQL has 
> 
>         if (sum((eta - etaold)^2) < 1e-06 * sum(eta^2)) 
>             break
> 
> GLMM has 
> 
>          crit <- max(abs(eta - etaold)) / (0.1 + max(abs(eta)))
>          ## use this to determine convergence
>          if (crit < controlvals$tolerance) {
>               conv <- TRUE
>               break
>          }
> 
> I'm not sure why we chose that. Setting 
> 
> control = list(tolerance = 1e-3, PQLmaxIt = 100)
> 
> converges for me, but that's probably a bit extreme.
> 
> Deepayan



From tlumley at u.washington.edu  Wed Nov 24 00:27:49 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 23 Nov 2004 15:27:49 -0800 (PST)
Subject: [R] Weibull survival regression
In-Reply-To: <000801c4d1aa$c0f47bf0$836a9951@youru989fckmtn>
References: <000801c4d1aa$c0f47bf0$836a9951@youru989fckmtn>
Message-ID: <Pine.A41.4.61b.0411231526470.284326@homer12.u.washington.edu>

On Tue, 23 Nov 2004, Eric Lim wrote:
>
> Using Thomas Lumley's solution with the lung dataset, I obtained:
>
>> curve(pweibull(x, scale=1/coef(lung.wbs), shape=1/lung.wbs$scale,
>      lower.tail=FALSE),from=0, to=max(lung$time))
>

That's because you read the part of the message where I got the 
reparametrization wrong, rather than the part where I got right.

It should be
  curve(pweibull(x, scale=exp(coef(lung.wbs)), shape=1/lung.wbs$scale,
       lower.tail=FALSE),from=0, to=max(lung$time))


 	-thomas



From ray at mcs.vuw.ac.nz  Wed Nov 24 00:55:13 2004
From: ray at mcs.vuw.ac.nz (Ray Brownrigg)
Date: Wed, 24 Nov 2004 12:55:13 +1300 (NZDT)
Subject: [R] IFELSE across large array?
Message-ID: <200411232355.iANNtD1G007436@tahi.mcs.vuw.ac.nz>

> From: "Liaw, Andy" <andy_liaw at merck.com>
> Date: Tue, 23 Nov 2004 12:28:48 -0500
> 
> I'll give it half a crack:
> 
> Steps a through c can be done via nested ifelse(), treating A and M as
> vectors (as they really are).  Step d is the hard one.  I'd write a simple
> Fortran code and use .Fortran() for that.
> 
> I don't see how any of the *apply() functions can help here, as your
> operations are element-wise, not dimension-wise.
> 
> Andy
> 
The original message mentions that the value 10 is actually "NODATA",
and if one recodes 10 as NA, steps a) to c) become trivial, namely:

A[A == 10] <- NA
M[M == 10] <- NA
return(A + M - 1)

Then if step d) is performed first (i.e. appropriate values in A are
replaced by the 'most common neighbour' [perhaps using
round(mean(.., na.rm=T))] this still works, but would have to be repeated
for each replication (the third dimension).

Ray Brownrigg

> > From: Sander Oom
> > 
> > Dear all,
> > 
> > As our previous email did not get any response, we try again with a 
> > reformulated question!
> > 
> > We are trying to do something which needs an efficient loop 
> > over a huge 
> > array, possibly functions such as apply and related (tapply,
> > lapply...?), but can't really understand syntax and examples in 
> > practice...i.e. cant' make it work.
> > 
> > to be more specific:
> > we are trying to apply a mask to a 3D array.
> > By this I mean that when "overlaying" [i.e. comparing element 
> > by element] 
> > the mask on to the array the mask should change array 
> > elements according to 
> > the values of both array and mask elements
> > 
> > the mask has two values: 1 and 10.
> > 
> > the array elements have 3 values: 0, 1,  or 10
> > 
> > sticking for the moment to the single 2d array case
> > 
> > for example:
> > [A= array]  10    0 10 1  10  0
> >                   1   10   1 0   0 10
> > 
> > [ M=mask]     1  10  10 1   1  1
> >                  10    1   1  1 10 10
> > 
> > I would like the array elements to:
> > 
> > a) IF A(ij) !=10 and  Mij = 1
> >               leave A(ij) unchanged
> > 
> > b)  IF   A(ij) != 10 and M(ij) =10
> >                 change A(ij) to M(ij) i.e mask value (10)
> > 
> > c)IF A(ij) = 10 and M(ij) = 10
> >                leave (Aij) unchanged
> > 
> > d) IF A(ij) = 10 and M(ij) !=10
> >             replace A(ij) with the majority value in the 
> > 8-neighborhood
> > 
> > (or whatever if it is an edge element) BUT ignoring 10s in this 
> > neighborhood (i.e. with either 1 or 0, whichever is in majority)
> > 
> > because the array is 3d I would like to repeat the thing with 
> > all the k 
> > elements (2d sub-arrays) of the array in question, using the 
> > same mask for 
> > al k elements
> > 
> > Would you be able to suggest a strategy to do this?
> > 
> > thanks very much
> > 
> > Alessandro and Sander.



From tmulholland at bigpond.com  Wed Nov 24 01:06:04 2004
From: tmulholland at bigpond.com (Tom Mulholland)
Date: Wed, 24 Nov 2004 08:06:04 +0800
Subject: [R] How to correct this
In-Reply-To: <41A39801.9080400@stat.auckland.ac.nz>
References: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA31@afhex01.dpi.wa.gov.au>
	<41A39801.9080400@stat.auckland.ac.nz>
Message-ID: <41A3D06C.5040001@bigpond.com>

It has been a while since I looked at those two sources. My main impetus 
has been the detail. I think that one of the aims of my project has been 
  to document the variants that might not be given a name. I have a 
function called tufte158 (Tufte, The Visual Display of Quantitative 
Information p. 158) In essence it's just a very simple line plot. 
However I have done quite a lot of work automating label positioning, so 
they do not to overlap. So while it is dead easy to produce the basic 
plot, getting it to publication standard, is not as easy. I have quite a 
lot of hybrid graphics that do not immediately look like plots. I guess 
these are best described as customised plots, which often are only good 
for the specific purpose, even though the components often reappear.

I think I would be trying to add to these sources, although it's not 
easy to see what would be the best method.

I will go back to my idea and after a thorough search through the 
existing stuff I might try to verbalise the idea again.

Thanks

Tom


Paul Murrell wrote:
...
> 
> What this sounds like to me is an "R graphics cookbook", which I think 
> would be a good idea, though have you looked at, for example, the 
> "Graphiques avec R" section of Vincent Zoonekynd's "Statistiques avec R"
> (http://zoonek2.free.fr/UNIX/48_R/all.html)
> or the "Graphing" section of Paul Johnson's "R tips" page 
> (http://www.ku.edu/~pauljohn/R/Rtips.html)?
> 
> Paul
...



From jeff.hamann at forestinformatics.com  Wed Nov 24 01:50:06 2004
From: jeff.hamann at forestinformatics.com (Jeff D. Hamann)
Date: Tue, 23 Nov 2004 16:50:06 -0800 (PST)
Subject: [R] route generation capabilites?
Message-ID: <1244.128.193.137.106.1101257406.squirrel@www.forestinformatics.com>

I have a short simulation problem (well it's not *that* short) that
requires I create all the possible routes amongst a set of destinations. I
didn't seem to find anything regarding route generation on the website and
wanted to know if it's possible to generate routes using R (via
dist,rdist,AllShortestPaths, and extractPath from e1701) or using some
other method from within R.

Thanks,
Jeff.

-- 
Jeff D. Hamann
Forest Informatics, Inc.
PO Box 1421
Corvallis, Oregon 97339-1421
phone 541-754-1428
fax 541-752-0288
jeff.hamann at forestinformatics.com
http://www.forestinformatics.com



From drf5n at maplepark.com  Wed Nov 24 03:21:52 2004
From: drf5n at maplepark.com (David Forrest)
Date: Tue, 23 Nov 2004 20:21:52 -0600 (CST)
Subject: [R] The hidden costs of GPL software?
In-Reply-To: <200411232136.iANLaFVW010276@outmx010.isp.belgacom.be>
References: <200411232136.iANLaFVW010276@outmx010.isp.belgacom.be>
Message-ID: <Pine.LNX.4.58.0411232003300.2728@maplepark.com>

On Tue, 23 Nov 2004, Philippe Grosjean wrote:

> Patrick Burns wrote:
> > [....]
> > No, I'm not volunteering to build the system.
>
> Too bad! ;-)
>
> Indeed, the idea to index tens of thousands of functions could not be
> appealing to many of us! Why not to consider to test such ideas at the
> package level? I mean, building a system that points out the packages of
> interest (those in CRAN, of course), given a search phrase would be a more
> resonable work. Then, looking at online help of that particular package
> would be the small additional effort required by the user. The problem here
> is with heterogeneous packages (the XXXXmisc, and the like)...

This mail archive works well if the questions are well posed and answered:

help.search.archive<-function(string){
   RURL="http://www.google.com/u/newcastlemaths"
   RSearchURL=paste(RURL,"?q=",string,sep='')
   browseURL(RSearchURL)
   return(invisible(0))
 }
help.search.google<-function(string){
   RURL="http://www.google.com/search"
   RSearchURL=paste(RURL,"?sitesearch=r-project.org&q=",string,sep='')
   browseURL(RSearchURL)
   return(invisible(0))
 }

help.search.archive('volatility') # may soon show Dr. Harrell's example
help.search.google('volatility') # may show enough

Is there package data that is not searchable through the google search?

Dave
-- 
 Dave Forrest
 drf at vims.edu                                    (804)684-7900w
 drf5n at maplepark.com                             (804)642-0662h
                                   http://maplepark.com/~drf5n/



From ggrothendieck at myway.com  Wed Nov 24 03:38:56 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 24 Nov 2004 02:38:56 +0000 (UTC)
Subject: [R] timeDate
References: <b1d315040411220945366432ab@mail.gmail.com>
	<41A22EF0.60709@statistik.uni-dortmund.de>
	<b1d3150404112213085eda9b3f@mail.gmail.com>
	<loom.20041122T224151-145@post.gmane.org>
	<b1d3150404112306557eb13ab0@mail.gmail.com>
Message-ID: <loom.20041124T033659-681@post.gmane.org>


Yasser El-Zein <abu3ammar <at> gmail.com> writes:

: 
: I am looking for up to the millisecond resolution. Is there a package
: that has that?
: 
: On Mon, 22 Nov 2004 21:48:20 +0000 (UTC), Gabor Grothendieck
: <ggrothendieck <at> myway.com> wrote:
: > Yasser El-Zein <abu3ammar <at> gmail.com> writes:
: > 
: > >
: > > >From the document it is apparent to me that I need as.POSIXct  (I have
: > > a double representing the number of millis since 1/1/1970 and I need
: > > to construct a datetime object). I see it showing how to construct the
: > > time object from a string representing the time but now fro a double
: > > of millis. Does anyone know hoe to do that?
: > >
: > 
: > If by millis you mean milliseconds (i.e. one thousandths of a second)
: > then POSIXct does not support that resolution, but if rounding to
: > seconds is ok then
: > 
: >   structure(round(x/1000), class = c("POSIXt", "POSIXct"))
: > 
: > should give it to you assuming x is the number of milliseconds.

There is no package/class that represents times and dates
internally as milliseoncds since Jan 1, 1970.   You can
rework your data into chron's internal representation, viz.
day number plus fraction of day, like this:

	# x is vector of milliseconds since Jan 1/70
	# x.chron is corresponding chron date/time
	# untested
	library(chron) 
	ms.in.day <- 1000*24*60*60 
	day <- floor(x/ms.in.day) 
	frac <- (x-1000*day)/ms.in.day
	x.chron <- chron(day+frac)

If you need to take leap seconds into account (which the above
does not) then note that R comes with a builtin vector called
leap.seconds.



From ksartor at montana.edu  Wed Nov 24 03:43:30 2004
From: ksartor at montana.edu (Karla Sartor)
Date: Tue, 23 Nov 2004 19:43:30 -0700
Subject: [R] list of lists question
Message-ID: <41A3F552.109@montana.edu>

Hello all,
As a general programming question I can't seem to figure out how to make 
a list of lists in R.
As matrices won't work as they have to be rectangular.

I am sure that there is an easy solution but...

the specific situation is this:
- I have created a Tukey confidence interval table and have listed the 
means that are not significantly different
- then using these not significantly different pairs I have created the 
groups of means that are not significantly different from each other
the issue then is that many of these lists are subsets of other lists 
and I need to check for this.

Below is a little program is illustrate the issue

 > a=c(1,1,1,1,1)                  # generate the first list
 > b=c(2,2,2)                        # generate a second list
 > c=c(a,b)                           #combine them
 > cat(c, "\n")                        # and print
1 1 1 1 1 2 2 2                   #  this is 1-D!!! ahh
 > d=list(a,b)                        # make a list of a and b
 > d                                       #  and print
[[1]]                                      #this is exactly what I want, 
but continue
[1] 1 1 1 1 1

[[2]]
[1] 2 2 2

 > e=list(d,a)                           # now on the next iteration I 
need to add another list to this list of lists
 > e                                         # and print
[[1]]                                       # ahh all hell has broken 
loose and this is not what I want
[[1]][[1]]                               #  desired result below
[1] 1 1 1 1 1

[[1]][[2]]
[1] 2 2 2


[[2]]
[1] 1 1 1 1 1

-------------
desired result

#wrong code but this is what I want to happen
a=c(1,1,1,1,1,1,1)
for(i in 1:5) {
    a=list(a,1:5)
}

output I want is (something like)
[1] 1 1 1 1 1 1 1
[2] 1 2 3 4 5
[3] 1 2 3 4 5
[4] 1 2 3 4 5
[5] 1 2 3 4 5
[6] 1 2 3 4 5

so then I could call cat(a[1]) and get 1 1 1 1 1 1 1 1 and cat(a[2]) and 
get 1 2 3 4 5

Anyone know the answer (hopefully simple)

Cheers,

Karla Sartor



----------------------------------------------------------
Karla Sartor
Montana State University - LRES
ksartor at montana.edu



From andy_liaw at merck.com  Wed Nov 24 04:02:35 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 23 Nov 2004 22:02:35 -0500
Subject: [R] list of lists question
Message-ID: <3A822319EB35174CA3714066D590DCD50994E37A@usrymx25.merck.com>

> From: Karla Sartor
> 
> Hello all,
> As a general programming question I can't seem to figure out 
> how to make 
> a list of lists in R.
> As matrices won't work as they have to be rectangular.
> 
> I am sure that there is an easy solution but...
> 
> the specific situation is this:
> - I have created a Tukey confidence interval table and have 
> listed the 
> means that are not significantly different
> - then using these not significantly different pairs I have 
> created the 
> groups of means that are not significantly different from each other
> the issue then is that many of these lists are subsets of other lists 
> and I need to check for this.
> 
> Below is a little program is illustrate the issue
> 
>  > a=c(1,1,1,1,1)                  # generate the first list
>  > b=c(2,2,2)                        # generate a second list
>  > c=c(a,b)                           #combine them
>  > cat(c, "\n")                        # and print
> 1 1 1 1 1 2 2 2                   #  this is 1-D!!! ahh
>  > d=list(a,b)                        # make a list of a and b
>  > d                                       #  and print
> [[1]]                                      #this is exactly 
> what I want, 
> but continue
> [1] 1 1 1 1 1
> 
> [[2]]
> [1] 2 2 2
> 
>  > e=list(d,a)                           # now on the next 
> iteration I 
> need to add another list to this list of lists
>  > e                                         # and print
> [[1]]                                       # ahh all hell has broken 
> loose and this is not what I want
> [[1]][[1]]                               #  desired result below
> [1] 1 1 1 1 1
> 
> [[1]][[2]]
> [1] 2 2 2
> 
> 
> [[2]]
> [1] 1 1 1 1 1
> 
> -------------
> desired result
> 
> #wrong code but this is what I want to happen
> a=c(1,1,1,1,1,1,1)
> for(i in 1:5) {
>     a=list(a,1:5)
> }
> 
> output I want is (something like)
> [1] 1 1 1 1 1 1 1
> [2] 1 2 3 4 5
> [3] 1 2 3 4 5
> [4] 1 2 3 4 5
> [5] 1 2 3 4 5
> [6] 1 2 3 4 5
> 
> so then I could call cat(a[1]) and get 1 1 1 1 1 1 1 1 and 
> cat(a[2]) and 
> get 1 2 3 4 5
> 
> Anyone know the answer (hopefully simple)

Indeed:  Lists are vectors.  You use c() to concatenate vectors, so you also
use it for lists.  E.g.,

> a <- list(rep(1, 5))
> for (i in 1:2) a <- c(a, list(1:5))
> a
[[1]]
[1] 1 1 1 1 1

[[2]]
[1] 1 2 3 4 5

[[3]]
[1] 1 2 3 4 5

Andy


 
> Cheers,
> 
> Karla Sartor
> 
> 
> 
> ----------------------------------------------------------
> Karla Sartor
> Montana State University - LRES
> ksartor at montana.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From spencer.graves at pdf.com  Wed Nov 24 04:04:27 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 23 Nov 2004 19:04:27 -0800
Subject: [R] list of lists question
In-Reply-To: <41A3F552.109@montana.edu>
References: <41A3F552.109@montana.edu>
Message-ID: <41A3FA3B.4000302@pdf.com>

      Is the following more like what you want: 


a=c(1,1,1,1,1)                  # generate the first list
b=c(2,2,2)                        # generate a second list
d=list(a,b)                        # make a list of a and b
(
e=c(d,a)
 )
[[1]]
[1] 1 1 1 1 1

[[2]]
[1] 2 2 2

[[3]]
[1] 1

[[4]]
[1] 1

[[5]]
[1] 1

[[6]]
[1] 1

[[7]]
[1] 1

      If no, have you read sec. 6 in "An Introduction to R" [the first 
option available from help.start()]? 

      In this example, note that e[1] is a list with only one 
attributes, namely the vector 1 1 1 1 1;  e[[1]] is not a list but that 
vector itself. 

      hope this helps.  spencer graves

Karla Sartor wrote:

> Hello all,
> As a general programming question I can't seem to figure out how to 
> make a list of lists in R.
> As matrices won't work as they have to be rectangular.
>
> I am sure that there is an easy solution but...
>
> the specific situation is this:
> - I have created a Tukey confidence interval table and have listed the 
> means that are not significantly different
> - then using these not significantly different pairs I have created 
> the groups of means that are not significantly different from each other
> the issue then is that many of these lists are subsets of other lists 
> and I need to check for this.
>
> Below is a little program is illustrate the issue
>
> > a=c(1,1,1,1,1)                  # generate the first list
> > b=c(2,2,2)                        # generate a second list
> > c=c(a,b)                           #combine them
> > cat(c, "\n")                        # and print
> 1 1 1 1 1 2 2 2                   #  this is 1-D!!! ahh
> > d=list(a,b)                        # make a list of a and b
> > d                                       #  and print
> [[1]]                                      #this is exactly what I 
> want, but continue
> [1] 1 1 1 1 1
>
> [[2]]
> [1] 2 2 2
>
> > e=list(d,a)                           # now on the next iteration I 
> need to add another list to this list of lists
> > e                                         # and print
> [[1]]                                       # ahh all hell has broken 
> loose and this is not what I want
> [[1]][[1]]                               #  desired result below
> [1] 1 1 1 1 1
>
> [[1]][[2]]
> [1] 2 2 2
>
>
> [[2]]
> [1] 1 1 1 1 1
>
> -------------
> desired result
>
> #wrong code but this is what I want to happen
> a=c(1,1,1,1,1,1,1)
> for(i in 1:5) {
>    a=list(a,1:5)
> }
>
> output I want is (something like)
> [1] 1 1 1 1 1 1 1
> [2] 1 2 3 4 5
> [3] 1 2 3 4 5
> [4] 1 2 3 4 5
> [5] 1 2 3 4 5
> [6] 1 2 3 4 5
>
> so then I could call cat(a[1]) and get 1 1 1 1 1 1 1 1 and cat(a[2]) 
> and get 1 2 3 4 5
>
> Anyone know the answer (hopefully simple)
>
> Cheers,
>
> Karla Sartor
>
>
>
> ----------------------------------------------------------
> Karla Sartor
> Montana State University - LRES
> ksartor at montana.edu
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html


-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From andy_liaw at merck.com  Wed Nov 24 04:57:25 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 23 Nov 2004 22:57:25 -0500
Subject: [R] an R function to search on Prof. Baron's site
Message-ID: <3A822319EB35174CA3714066D590DCD50994E37B@usrymx25.merck.com>

Inspired by the functions that Barry Rawlingson and Dave Forrest posted for
searching Rwiki and R-help archive, I've made up a function that does the
search on Prof. Baron's site (Thanks to Prof. Baron's help on setting up the
query string!):

RSiteSearch <- function(string, restrict="Rhelp", format="long",
                        sortby="score", matchesPerPage=10) {
    URL <- "http://finzi.psych.upenn.edu/cgi-bin/htsearch"
    qstring <- paste(URL, "?config=htdigrun1", sep="")
    ## replace spaces with "%20" in the query
    string <- paste("words=", gsub(" ", "%20", string), sep="")
    mpp <- paste("matchesperpage=", matchesPerPage, sep="")

    format <- charmatch(format, c("long", "short"))
    if (format == 0) stop("format must be either long or short")
    format <- paste("format=builtin-", switch(format, "long", "short"),
sep="")

    sortby <- charmatch(sortby, c("score", "time", "title", "revtime"))
    if (sortby == 0) stop("wrong sortby specified")
    sortby <- paste("sort=",
                    switch(sortby, "score", "time", "title", "revtime"),
                    sep="")
                    
    res <- charmatch(restrict, c("Rhelp", "doc", "function", "doc/fun"))
    if (res == 0) stop("wrong restriction specified")
    res <- switch(res, "Rhelp00/archive|Rhelp01/archive|Rhelp02a/archive",
                  "finzi.psych.upenn.edu/R/doc",
                  "finzi.psych.upenn.edu/R/library",
 
"finzi.psych.upenn.edu/R/doc|finzi.psych.upenn.edu/R/library")
    res <- paste("restrict=", res, sep="")
    qstring <- paste(qstring, res, format, sortby, string, mpp, sep=";")
    browseURL(qstring)
    invisible(qstring)
}

The options roughly correspond to the options on that search page.  Hope
some people find this somewhat helpful.  Comments/suggestions for
improvement are much appreciated.

Best,
Andy



From steven.f.freeman at verizon.net  Wed Nov 24 05:28:58 2004
From: steven.f.freeman at verizon.net (Steve Freeman)
Date: Tue, 23 Nov 2004 23:28:58 -0500
Subject: [R] T-test syntax question 
Message-ID: <20041124042902.IKBF1432.out012.verizon.net@TOSHIBA>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041123/c985353b/attachment.pl

From ok at cs.otago.ac.nz  Wed Nov 24 05:53:52 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Wed, 24 Nov 2004 17:53:52 +1300 (NZDT)
Subject: [R] Convergence problem in GLMM
Message-ID: <200411240453.iAO4rqua303386@atlas.otago.ac.nz>

I was trying to install some more packages and ran into a problem
I hadn't seen before.

Version:

    platform sparc-sun-solaris2.9
    arch     sparc               
    os       solaris2.9          
    system   sparc, solaris2.9   
    status                       
    major    2                   
    minor    0.1                 
    year     2004                
    month    11                  
    day      15                  
    language R                   

Fortran compilers available to me:

    f77: Sun WorkShop 6 update 2 FORTRAN 77 5.3 2001/05/15
    f90: Sun WorkShop 6 update 2 Fortran 95 6.2 2001/05/15
    f95: Sun WorkShop 6 update 2 Fortran 95 6.2 2001/05/15

Package:

    gam

    In fact I didn't ask for this one specifically, I had
    "dependencies=TRUE" in a call to install.packages().

Problem:

    Following the installation instructions for R, I had selected F95
    as my Fortran compiler.

    The f95 compiler complained about nearly every line of
    gam/src/bsplvd.f

    From the error messages as displayed on the screen, I could see no
    reason for complaint.  However, looking at the file with a text
    editor immediately revealed the problem.  The files

	bsplvd.f	bvalue.f	bvalus.f	loessf.f
	qsbart.f	sgram.f		sinerp.f	sslvrg.f
	stxwx.f

    all use CR-LF line termination.  The files

	linear.f	lo.f		splsm.f

    all use LF line termination expected on UNIX.

    It turns out that the g77 and f77 compilers don't mind CR at the
    end of a line, but f90 and f95 hate them like poison.

    Removing the CRs makes f90 and f95 happy again.

Second-order problem:

    I know how to fix the immediate problem.  What I don't know is how
    to intervene in the installation process.  What I need to do is
	- get and unpack files (steps normally done by install.packages)
	- make changes (remove CR, edit configuration, whatever)
	- resume whatever install.packages normally does



From ggrothendieck at myway.com  Wed Nov 24 05:55:34 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 24 Nov 2004 04:55:34 +0000 (UTC)
Subject: [R] an R function to search on Prof. Baron's site
References: <3A822319EB35174CA3714066D590DCD50994E37B@usrymx25.merck.com>
Message-ID: <loom.20041124T054946-210@post.gmane.org>

Liaw, Andy <andy_liaw <at> merck.com> writes:

: 
: Inspired by the functions that Barry Rawlingson and Dave Forrest posted for
: searching Rwiki and R-help archive, I've made up a function that does the
: search on Prof. Baron's site (Thanks to Prof. Baron's help on setting up the
: query string!):

It would be nice if this and the other search functions recently
posted were collected into a package or even integrated into
R itself.  In the case of the Windows Rgui, it would be nice if they 
appeared on a menu with the other search and help functions.



From ripley at stats.ox.ac.uk  Wed Nov 24 07:04:53 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 24 Nov 2004 06:04:53 +0000 (GMT)
Subject: [R] CRLF-terminated Fortran files
In-Reply-To: <200411240453.iAO4rqua303386@atlas.otago.ac.nz>
References: <200411240453.iAO4rqua303386@atlas.otago.ac.nz>
Message-ID: <Pine.LNX.4.61.0411240553100.4979@gannet.stats>

What did this have to do with GLMM?  I've changed the subject line.

On Wed, 24 Nov 2004, Richard A. O'Keefe wrote:

> I was trying to install some more packages and ran into a problem
> I hadn't seen before.

We've seen it for C, and test it for C in R CMD check.  I think we should 
check C++ and Fortran files too.

> Version:
>
>    platform sparc-sun-solaris2.9
>    arch     sparc
>    os       solaris2.9
>    system   sparc, solaris2.9
>    status
>    major    2
>    minor    0.1
>    year     2004
>    month    11
>    day      15
>    language R
>
> Fortran compilers available to me:
>
>    f77: Sun WorkShop 6 update 2 FORTRAN 77 5.3 2001/05/15
>    f90: Sun WorkShop 6 update 2 Fortran 95 6.2 2001/05/15
>    f95: Sun WorkShop 6 update 2 Fortran 95 6.2 2001/05/15
>
> Package:
>
>    gam
>
>    In fact I didn't ask for this one specifically, I had
>    "dependencies=TRUE" in a call to install.packages().
>
> Problem:
>
>    Following the installation instructions for R, I had selected F95
>    as my Fortran compiler.
>
>    The f95 compiler complained about nearly every line of
>    gam/src/bsplvd.f
>
>    From the error messages as displayed on the screen, I could see no
>    reason for complaint.  However, looking at the file with a text
>    editor immediately revealed the problem.  The files
>
> 	bsplvd.f	bvalue.f	bvalus.f	loessf.f
> 	qsbart.f	sgram.f		sinerp.f	sslvrg.f
> 	stxwx.f
>
>    all use CR-LF line termination.  The files
>
> 	linear.f	lo.f		splsm.f
>
>    all use LF line termination expected on UNIX.
>
>    It turns out that the g77 and f77 compilers don't mind CR at the
>    end of a line, but f90 and f95 hate them like poison.
>
>    Removing the CRs makes f90 and f95 happy again.

BTW, in that version of Sun Workshop f90 and f95 are the same compiler, 
and in later versions so is f77.  (I think these compilers are on version 
9 now.)  Even in version 7, there is no problem with line endings.

I did get a warning:

       call dchdc(a,p,p,work,jpvt,job,info)
                                      ^
"linear.f", Line = 408, Column = 38: WARNING: Procedure "DCHDC" is defined 
at line 194 (linear.f).  Illegal association of array actual argument with 
scalar dummy argument "INFO".

which seems genuine (make it info(1) in the call).


> Second-order problem:
>
>    I know how to fix the immediate problem.  What I don't know is how
>    to intervene in the installation process.  What I need to do is
> 	- get and unpack files (steps normally done by install.packages)
> 	- make changes (remove CR, edit configuration, whatever)
> 	- resume whatever install.packages normally does

- Use install.packages(destdir=) to retain the tarballs which are 
downloaded.

- Unpack the package tarball, make changes.

- Run R CMD INSTALL on the changed sources.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From kee at wehi.EDU.AU  Wed Nov 24 07:09:56 2004
From: kee at wehi.EDU.AU (Thuan-Jin Kee)
Date: Wed, 24 Nov 2004 17:09:56 +1100 (EST)
Subject: [R] R 2.0.1 jpeg() png() pdf() Mac OSX 10.3.5 - empty files being 
 created
Message-ID: <3057.192.168.20.199.1101276596.squirrel@192.168.20.199>

Hi everyone,

I've been opening up R 2.0.1 from the dock and then starting the Mac OSX
X11 server.

After that I call

>jpeg("test.jpg",plot(rnorm(10)))

and r pops up a quartz window with the appropriate graph in it and puts a
file called test.jpg into the HOME which is 0kb in size and is completely
empty.

There is no error message.

I've tried
>png("test.png", plot(rnorm(10)))

and
>pdf(plot(rnorm(10)))

with the same result.

I'm sure that I've got the appropriate libpng and jpeg libraries installed
correctly from sourceforge as linked to by http://www.economia.unimi.it/R/

Thanks in advance
Yours

Jin Kee



From ripley at stats.ox.ac.uk  Wed Nov 24 07:58:26 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 24 Nov 2004 06:58:26 +0000 (GMT)
Subject: [R] R 2.0.1 jpeg() png() pdf() Mac OSX 10.3.5 - empty files
	being  created
In-Reply-To: <3057.192.168.20.199.1101276596.squirrel@192.168.20.199>
References: <3057.192.168.20.199.1101276596.squirrel@192.168.20.199>
Message-ID: <Pine.LNX.4.61.0411240655540.9600@gannet.stats>

On Wed, 24 Nov 2004, Thuan-Jin Kee wrote:

> I've been opening up R 2.0.1 from the dock and then starting the Mac OSX
> X11 server.
>
> After that I call
>
>> jpeg("test.jpg",plot(rnorm(10)))

Please do read the help page: the syntax is

jpeg("test.jpg")
plot(rnorm(10))
dev.off()

> and r pops up a quartz window with the appropriate graph in it and puts a
> file called test.jpg into the HOME which is 0kb in size and is completely
> empty.
>
> There is no error message.
>
> I've tried
>> png("test.png", plot(rnorm(10)))
>
> and
>> pdf(plot(rnorm(10)))
>
> with the same result.

Also not as specified on their help pages.

You never plotted anything on the devices you opened, and you did not 
close them.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jwdougherty at mcihispeed.net  Wed Nov 24 08:48:24 2004
From: jwdougherty at mcihispeed.net (John)
Date: Tue, 23 Nov 2004 23:48:24 -0800
Subject: [R] The hidden costs of GPL software? - None
In-Reply-To: <96B9B9A4-39AB-11D9-95C5-000A95CDA0F2@anu.edu.au>
References: <96B9B9A4-39AB-11D9-95C5-000A95CDA0F2@anu.edu.au>
Message-ID: <200411232348.24621.jwdougherty@mcihispeed.net>

Off hand, the costs of GPL'd software are not hidden at all.  R for instance 
demands that a would be user sit down and learn the language.  This in turn 
pushes a user into learning more about statistics than the simple overview 
that Stat 1 presents a student.

In contrast, any program that simplifies use also tends to encourage a 
simplified understanding.  So, I believe it can be legitimately argued that 
the real hidden costs lurk in "easy to use" software, especially commeercial 
software with GUI interfaces.

JDougherty



From angelare at to.infn.it  Wed Nov 24 08:50:44 2004
From: angelare at to.infn.it (angelare@to.infn.it)
Date: Wed, 24 Nov 2004 08:50:44 +0100 (MET)
Subject: [R] (no subject)
Message-ID: <1058.193.205.64.125.1101282644.squirrel@193.205.64.125>

Good morning,
I have to apply the Ks test with the the t distribution.
I know I have to write ks.test(data_name,"distribution_name", parameters..)
but I don't know what is the name fot t distribution and which parameters
to introduce? may be mean=0 and freedom degrees in my case?
Thank you for helping me.
Angela Re



From ligges at statistik.uni-dortmund.de  Wed Nov 24 09:32:27 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 24 Nov 2004 09:32:27 +0100
Subject: [R] The hidden costs of GPL software? - None
In-Reply-To: <200411232348.24621.jwdougherty@mcihispeed.net>
References: <96B9B9A4-39AB-11D9-95C5-000A95CDA0F2@anu.edu.au>
	<200411232348.24621.jwdougherty@mcihispeed.net>
Message-ID: <41A4471B.6030502@statistik.uni-dortmund.de>

Martin,

what about setting up a new mailing list R-hcgs?
(acronym for "R - The hidden costs of GPL software?")
Seems to be worth given the amount of messages in this thread(s). ;-)

Uwe



From vito_ricci at yahoo.com  Wed Nov 24 09:33:42 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Wed, 24 Nov 2004 09:33:42 +0100 (CET)
Subject: [R] T-test syntax question
Message-ID: <20041124083342.67294.qmail@web41202.mail.yahoo.com>

Hi,

You did not specify if data are paired or not, as data
are paired you should use option paired=TRUE in
t.test(). Variances of the two samples have to be not
significatevely different, (see ? var.test) to use
t.test, if not you should specify var.equal=FALSE.

var.equal: a logical variable indicating whether to
treat the two ariances as being equal. If 'TRUE' then
the pooled variance is used to estimate the variance
otherwise the Welch (or Satterthwaite) approximation
to the degrees of freedom is used.

paired: a logical indicating whether you want a paired
t-test.

If 'paired' is 'TRUE' then both 'x' and 'y' must be
specified and they must be the same length.  Missing
values are removed (in pairs if 'paired' is 'TRUE'). 
If 'var.equal' is 'TRUE' then the pooled estimate of
the variance is used.  By default, if 'var.equal' is
'FALSE' then the variance is estimated separately for
both groups and the Welch modification to the degrees
of freedom is used.
 
>From the output of your test you're sending I
understand that variances of the two samples are
significatively different (Welch Two Sample t-test)
and delta values are also significatively different
from 0.

See ? t.test

Regards
Vito


You wrote:

Hi. 

I'd like to do a t-test to compare the Delta values of
items with Crit=1 with Delta values of items with
Crit=0. What is the t.test syntax?

It should produce a result like this below (I can't
get in touch with the person who originally did this
for me)

    Welch Two Sample t-test

data:  t1$Delta by Crit
t = -3.4105, df = 8.674, p-value = 0.008173
alternative hypothesis: true
difference in means is not equal to 0
95 percent confidence interval:
 -0.04506155 -0.00899827
sample estimates:
mean in group FALSE  mean in group TRUE 
         0.03331391          0.06034382 

Thanks.

=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/



From ligges at statistik.uni-dortmund.de  Wed Nov 24 09:38:41 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 24 Nov 2004 09:38:41 +0100
Subject: [R] How to extract data?
In-Reply-To: <OFBBCB2ECC.F8077073-ON85256F55.00755F8B@nd.convergys.com>
References: <OFBBCB2ECC.F8077073-ON85256F55.00755F8B@nd.convergys.com>
Message-ID: <41A44891.5090300@statistik.uni-dortmund.de>

james.holtman at convergys.com wrote:

> 
> 
> 
> By 'ignore', can we delete those from the list of data?  I would then
> assume that if you have a sequence of +0+0+ that you would want the last
> "+" for the increase of three.
> 
> If that is the case, then do a 'diff' and delete the entries that are 0.
> Then create a new 'diff' and then use 'rle' to see what the length of the
> sequences are:
> 
> 
>>x <- c(1,2,2,3,3,4,3,3,2,2,2,1)
>>x
> 
>  [1] 1 2 2 3 3 4 3 3 2 2 2 1
> 
>>x.d <- diff(x)
>>x.d
> 
>  [1]  1  0  1  0  1 -1  0 -1  0  0 -1
> 
>>x.new <- x[c(x.d,1) != 0]
>>x.new
> 
> [1] 1 2 3 4 3 2 1
> 
>>x.d1 <- diff(x.new)
>>x.d1
> 
> [1]  1  1  1 -1 -1 -1
> 
>>rle(x.d1)
> 
> Run Length Encoding
>   lengths: int [1:2] 3 3
>   values : num [1:2] 1 -1


Additionally, if there are differences are > 1, you might want apply 
sign() before rle().

Uwe Ligges

> 
> you can check the results of 'rle' to determine where the changes are.
> __________________________________________________________
> James Holtman        "What is the problem you are trying to solve?"
> Executive Technical Consultant  --  Office of Technology, Convergys
> james.holtman at convergys.com
> +1 (513) 723-2929
> 
> 
>                                                                                                                                            
>                       ebashi                                                                                                               
>                       <arshia22 at yahoo.com>         To:       r-sig-finance at stat.math.ethz.ch, r-help at stat.math.ethz.ch                     
>                       Sent by:                     cc:                                                                                     
>                       r-help-bounces at stat.m        Subject:  [R] How to extract data?                                                      
>                       ath.ethz.ch                                                                                                          
>                                                                                                                                            
>                                                                                                                                            
>                       11/23/2004 15:54                                                                                                     
>                                                                                                                                            
>                                                                                                                                            
> 
> 
> 
> 
> I appreciate if anyone can help me,
> I have a table as follow,
> 
>>rate
> 
>           DATE VALUE
> 1   1997-01-10  5.30
> 2   1997-01-17  5.30
> 3   1997-01-24  5.28
> 4   1997-01-31  5.30
> 5   1997-02-07  5.29
> 6   1997-02-14  5.26
> 7   1997-02-21  5.24
> 8   1997-02-28  5.26
> 9   1997-03-07  5.30
> 10  1997-03-14  5.30
> .    ......     ...
> .    ......     ...
> .    ......     ...
> I want to extract the DATE(s) on which the VALUE has
> already dropped twice and the DATE(s) that VALUE has
> already increased for three times,( ignore where
> VALUE(i+1)-VALUE(i)=0),I try to use diff() function,
> however that works only for one increase or decrease.
> 
> Sincerely,
> 
> Sean
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From vito_ricci at yahoo.com  Wed Nov 24 09:40:10 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Wed, 24 Nov 2004 09:40:10 +0100 (CET)
Subject: [R] Ks.test (was (no subject))
Message-ID: <20041124084010.24351.qmail@web41204.mail.yahoo.com>

Hi Angela,

I believe you should introduce only df as parameters;
t distribution as by default mean=0; see this example.

> x<-rt(100,10)
> ks.test(x, "pt",10)

        One-sample Kolmogorov-Smirnov test

data:  x 
D = 0.1414, p-value = 0.03671
alternative hypothesis: two.sided 

Ciao
Vito

you wrote:

Good morning,
I have to apply the Ks test with the the t
distribution.
I know I have to write
ks.test(data_name,"distribution_name", parameters..)
but I don't know what is the name fot t distribution
and which parameters
to introduce? may be mean=0 and freedom degrees in my
case?
Thank you for helping me.
Angela Re




=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/



From ligges at statistik.uni-dortmund.de  Wed Nov 24 09:59:27 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 24 Nov 2004 09:59:27 +0100
Subject: [R] (no subject)
In-Reply-To: <1058.193.205.64.125.1101282644.squirrel@193.205.64.125>
References: <1058.193.205.64.125.1101282644.squirrel@193.205.64.125>
Message-ID: <41A44D6F.5080100@statistik.uni-dortmund.de>

angelare at to.infn.it wrote:

> Good morning,
> I have to apply the Ks test with the the t distribution.
> I know I have to write ks.test(data_name,"distribution_name", parameters..)
> but I don't know what is the name fot t distribution and which parameters
> to introduce? may be mean=0 and freedom degrees in my case?

For example:
   ks.test(x, "pt", df = 4)

See ?ks.test and ?pt

Uwe Ligges



> Thank you for helping me.
> Angela Re
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From fhduan at gmail.com  Wed Nov 24 10:03:17 2004
From: fhduan at gmail.com (Frank Duan)
Date: Wed, 24 Nov 2004 04:03:17 -0500
Subject: [R] Is there a package in R that lets me fit a robust linear mixed
	model?
Message-ID: <3b91723104112401034206eeb9@mail.gmail.com>

Dear R people,

Happy Thanksgiving! 

I just wonder if there is a R package that can supply some kind of
"robust" way to fit a linear mixed model. I mean assigning small
weights to those observations with large residuals, like
iteratively-reweighted-least-squares approach.

Many thanks,

Frank



From Ted.Harding at nessie.mcc.ac.uk  Wed Nov 24 10:21:36 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 24 Nov 2004 09:21:36 -0000 (GMT)
Subject: [R] The hidden costs of GPL software? - None
In-Reply-To: <200411232348.24621.jwdougherty@mcihispeed.net>
Message-ID: <XFMail.041124092136.Ted.Harding@nessie.mcc.ac.uk>

On 24-Nov-04 John wrote:
> Off hand, the costs of GPL'd software are not hidden at all.
> R for instance demands that a would be user sit down and
> learn the language. This in turn pushes a user into learning
> more about statistics than the simple overview that Stat 1
> presents a student.

I'd see this as less a cost than a benefit!

> In contrast, any program that simplifies use also tends to
> encourage a simplified understanding.

Agreed!

> So, I believe it can be legitimately argued that the real
> hidden costs lurk in "easy to use" software, especially
> commeercial software with GUI interfaces.

Well put; though it's not obvious whom these costs fall on.
The people who actually use the "easy to use" software, or
the organisations that employ them, can all too often get
away with sloppy or invalid analysis. It may often be the
consumer of their results or of products based on them who
ultimately loses.

Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 24-Nov-04                                       Time: 09:21:35
------------------------------ XFMail ------------------------------



From maechler at stat.math.ethz.ch  Wed Nov 24 10:47:16 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 24 Nov 2004 10:47:16 +0100
Subject: [R] Posting to 2 mailing lists {was "How to extract data?"}
In-Reply-To: <20041123205440.37889.qmail@web81002.mail.yahoo.com>
References: <20041123205440.37889.qmail@web81002.mail.yahoo.com>
Message-ID: <16804.22692.289556.105233@gargle.gargle.HOWL>

You posted the referenced message to both
R-help and R-sig-finance.

Please do *not* post to more than one R-list!
Please do *not* post to more than one R-list!
Please do *not* post to more than one R-list!
	  
Please decide if something is specific for an R-SIG-<foo> list
or if it belongs to R-help (or R-devel) and post to one and only
one mailing list!

We had another mess recently with this by a posting to both
R-help and R-sig-gui (and I think another one where
even 3 mailing lists where affected). The whole thing is a
particular impoliteness to all those people -- often the nice
helpers! -- who are subscribed to more than one of the implied
lists.

Chosing one list, the discussion thread will be archived/seen/read
consistently both on the server archives and people's mail/news boxes.

- If the thread should be *diverted* to another list, there
  could be *one* overlap message (posting to both), 
  where the move should be announced

- If you deem it relevant, you can still alert the readers of
  one list to a "hot topic" on another list, e.g., by posting an
  URL to the starting message in an (online) archive.

Martin Maechler, ETH Zurich

PS: Of course, I've been tempted for a moment
    to post this to all R- mailing lists  ;-) :-)



From andrew.c at bu.ac.th  Wed Nov 24 10:50:23 2004
From: andrew.c at bu.ac.th (Andrew R. Criswell)
Date: Wed, 24 Nov 2004 16:50:23 +0700 (ICT)
Subject: [R] problem with anova and glmmPQL
Message-ID: <33853.10.9.9.16.1101289823.squirrel@email.bu.ac.th>

Hello:

I am getting an error message when appplying anova() to two equations
estimated using glmmPQL. I did look through the archives but didn't
finding anything relevant to my problem. The R-code and results follow.

Hope someone can help.

ANDREW
____________________________

> fm1 <- glmmPQL(choice ~ day + stereotypy,
+                random = ~ 1 | bear, data = learning, family = binomial)
iteration 1
iteration 2
iteration 3
iteration 4
> fm2 <- glmmPQL(choice ~ day + envir + stereotypy,
+                random = ~ 1 | bear, data = learning, family = binomial)
iteration 1
iteration 2
iteration 3
iteration 4
> anova(fm1)
            numDF denDF   F-value p-value
(Intercept)     1  2032   7.95709  0.0048
day             1  2032 213.98391  <.0001
stereotypy      1  2032   0.42810  0.5130
>
> anova(fm2)
            numDF denDF   F-value p-value
(Intercept)     1  2031   5.70343  0.0170
day             1  2031 213.21673  <.0001
envir           1  2031  12.50388  0.0004
stereotypy      1  2031   0.27256  0.6017
>
> anova(fm1, fm2)
Error in anova.lme(fm1, fm2) : Objects must inherit from classes "gls",
"gnls" "lm","lmList", "lme","nlme","nlsList", or "nls"
>
> version
         _
platform i586-mandrake-linux-gnu
arch     i586
os       linux-gnu
system   i586, linux-gnu
status
major    2
minor    0.0
year     2004
month    10
day      04
language R






-- 
Andrew R. Criswell, Ph.D.
Graduate School, Bangkok University

mailto:andrew.c at bu.ac.th
mailto:andrew at arcriswell.com



From christoph.lehmann at gmx.ch  Wed Nov 24 11:16:57 2004
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Wed, 24 Nov 2004 11:16:57 +0100
Subject: [R] LDA with previous PCA for dimensionality reduction
Message-ID: <41A45F99.8030808@gmx.ch>

Dear all, not really a R question but:

If I want to check for the classification accuracy of a LDA with 
previous PCA for dimensionality reduction by means of the LOOCV method:

Is it ok to do the PCA on the WHOLE dataset ONCE and then run the LDA 
with the CV option set to TRUE (runs LOOCV)

-- OR--

do I need
- to compute for each 'test-bag' (the n-1 observations) a PCA 
(my.princomp.1),
- then run the LDA on the test-bag scores (-> my.lda.1)
- then compute the scores of the left-out-observation using 
my.princomp.1 (-> my.scores.2)
- and only then use predict.lda(my.lda.1, my.scores.2) on the scores of 
the left-out-observation

?
I read some articles, where they choose procedure 1, but I am not sure, 
if this is really correct?

many thanks for a hint

Christoph



From Ted.Harding at nessie.mcc.ac.uk  Wed Nov 24 11:36:35 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 24 Nov 2004 10:36:35 -0000 (GMT)
Subject: [R] Grumble ...
Message-ID: <XFMail.041124103635.Ted.Harding@nessie.mcc.ac.uk>

Hi Folks,

A Grumble ...

The message I just sent to R-help about "The hidden costs of GPL ..."
has evoked a "Challenge" response:

  Hi,
  You??ve just sent a message to diagnosticando at uol.com.br
  In order to confirm the sent message, please click here

  This confirmation is necessary because diagnosticando at uol.com.br
  uses Antispam UOL, a service that avoids unwanted messages like
  advertising, pornography, viruses, and spams.

  Other messages sent to diagnosticando at uol.com.br won't need to
  be confirmed*.
  *If you receive another confirmation request, please ask
  diagnosticando at uol.com.br to include you in his/her authorized
  e-mail list.

I won't be responding to this. Let the recipient simply not receive
the mail. Of no great importance in this case, but a disadvantage
to the recipient in the long run.

I disapprove strongly of this mechanism, and want to oppose it.
There must be a few thousand subscribers to R-help. If the
"Challenge" mechanism became widespread, then I would receive
thousands of such messages. Rather than respond to all these,
I would quit the list (and of course probably many others).
The "Challenge" mechanism would destroy the mailing-list community
if it became widely adopted.

One reason I am posting this grumble to R-help is in the hope
that I get a challenge to this one too. In that case, once and
for all, I shall respond, so that the recipient will see this
message and (I hope) do something about it, to eliminate the
"Challenge" responder (I can't find the true recipient's
email address from the "Challenge").

The recipient may be able to recognise themselves from the
fact that they receive this message but not the message which
triggered the response, which began:
=======================================
On 24-Nov-04 John wrote:
> Off hand, the costs of GPL'd software are not hidden at all.
> R for instance demands that a would be user sit down and
> learn the language. This in turn pushes a user into learning
> more about statistics than the simple overview that Stat 1
> presents a student.

I'd see this as less a cost than a benefit!
=======================================

My apologies for bothering you with this if you didn't want to
know about it.

Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 24-Nov-04                                       Time: 10:36:35
------------------------------ XFMail ------------------------------



From ramasamy at cancer.org.uk  Wed Nov 24 11:55:32 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Wed, 24 Nov 2004 10:55:32 +0000
Subject: [R] T-test syntax question
In-Reply-To: <20041124042902.IKBF1432.out012.verizon.net@TOSHIBA>
References: <20041124042902.IKBF1432.out012.verizon.net@TOSHIBA>
Message-ID: <1101293693.3158.16.camel@ndmpc126.ihs.ox.ac.uk>

As Vito Ricci has already pointed out, the Welsh test is for two group
unpaired data with unequal variance assumption.

If you have the original data, say x and y, then you can simply do
t.test( x, y, paired=FALSE, var.equal=FALSE ).

If you do not have the original data, you can still calculate the
relevant statistics and p-value as long as you know the group length and
variance. 'stats:::t.test.default' shows you the code behind t-test. I
think the relevant bits are as follows

mx <- 0
my <- 2
mu <- 0

# You will need to fill these with your observed values
vy <- var(y)
vx <- var(x)
ny <- length(y)
ny <- length(y)


stderrx <- sqrt(vx/nx)
stderry <- sqrt(vy/ny)
stderr  <- sqrt(stderrx^2 + stderry^2)
df      <- stderr^4/(stderrx^4/(nx - 1) + stderry^4/(ny - 1))
tstat   <- (mx - my - mu)/stderr

# for two sided alternative
pval  <- 2 * pt(-abs(tstat), df)
alpha <- 1 - conf.level
cint  <- qt(1 - alpha/2, df)
cint  <- tstat + c(-cint, cint)
cint  <- mu + cint * stderr



On Wed, 2004-11-24 at 04:28, Steve Freeman wrote:
> Hi. 
> 
> I'd like to do a t-test to compare the Delta values of items with Crit=1
> with Delta values of items with Crit=0. What is the t.test syntax?
> 
> It should produce a result like this below (I can't get in touch with the
> person who originally did this for me)
> 
>     Welch Two Sample t-test
>
> data:  t1$Delta by Crit
> t = -3.4105, df = 8.674, p-value = 0.008173 alternative hypothesis: true
> difference in means is not equal to 0
> 95 percent confidence interval:
>  -0.04506155 -0.00899827
> sample estimates:
> mean in group FALSE  mean in group TRUE 
>          0.03331391          0.06034382 
> 
> Thanks.
> 
> ------------------------------------
> Steven F. Freeman * Center for Organizational Dynamics * University of
> Pennsylvania * (215) 898-6967 * Fax: (215) 898-8934 * Cell: (215) 802-4680 *
> stfreema at sas.upenn.edu * http://center.grad.upenn.edu/faculty/freeman.html
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
-- 
Adaikalavan Ramasamy                    ramasamy at cancer.org.uk
Centre for Statistics in Medicine       http://www.ihs.ox.ac.uk/csm/
Cancer Research UK                      Tel : 01865 226 677
Old Road Campus, Headington, Oxford     Fax : 01865 226 962



From andersm at maths.lth.se  Wed Nov 24 12:10:18 2004
From: andersm at maths.lth.se (Anders Malmberg)
Date: Wed, 24 Nov 2004 12:10:18 +0100
Subject: [R] Automatic file reading
Message-ID: <41A46C1A.2030105@maths.lth.se>

Hi,

I want to do automatic reading of a number of tables (files) stored in 
ascii format
without having to specify the variable name in R each time.  Below is an 
example
of how I would like to use it (I assume files pair1,...,pair8 exist in 
spec. dire.)

for (i in 1:8){
  name <- paste("pair",i,sep="")
  ? ? ? <- read.table(paste("/home/andersm/tmp/",name,sep=""))
}

after which I want to have pair1,...,pair8 as tables.

But I can not get it right. Anybody having a smart solution?

Best regards,
Anders Malmberg



From vito_ricci at yahoo.com  Wed Nov 24 12:32:06 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Wed, 24 Nov 2004 12:32:06 +0100 (CET)
Subject: [R] Re: T-test syntax question
Message-ID: <20041124113206.95023.qmail@web41202.mail.yahoo.com>

Hi,

In case of paired data, if you have only differencies
and not original data you can get this t test based on
differencies:

Say d is the vector with differencies data and suppose
you wish to test if the mean of differency is equal to
zero:

md<-mean(d) ## sample mean of differencies
sdd<-sd(d) ## sample sd of differencies
n<-length(d) ## sample size
t.value<-(md/(sdd/sqrt(n))) ## sample t-value with n-1
df
pt(t.value,n-1,lower.tail=FALSE) ## p-value of test

> set.seed(13)
> d<-rnorm(50)
> md<-mean(d) ## sample mean of differencies
> sdd<-sd(d) ## sample sd of differencies
> n<-length(d) ## sample size
> t.value<-(md/(sdd/sqrt(n))) ## sample t-value with
n-1 df
> pt(t.value,n-1,lower.tail=FALSE) ## p-value of test
[1] 0.5755711

Best regards,
Vito



Steven F. Freeman wrote:

I'd like to do a t-test to compare the Delta values of
items with Crit=1
with Delta values of items with Crit=0. What is the
t.test syntax?

It should produce a result like this below (I can't
get in touch with the
person who originally did this for me)

    Welch Two Sample t-test

data:  t1$Delta by Crit
t = -3.4105, df = 8.674, p-value = 0.008173
alternative hypothesis: true
difference in means is not equal to 0
95 percent confidence interval:
 -0.04506155 -0.00899827
sample estimates:
mean in group FALSE  mean in group TRUE 
         0.03331391          0.06034382 

Thanks.

=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/



From ripley at stats.ox.ac.uk  Wed Nov 24 12:32:40 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 24 Nov 2004 11:32:40 +0000 (GMT)
Subject: [R] Re: CRLF-terminated Fortran files
In-Reply-To: <Pine.LNX.4.61.0411240553100.4979@gannet.stats>
References: <200411240453.iAO4rqua303386@atlas.otago.ac.nz>
	<Pine.LNX.4.61.0411240553100.4979@gannet.stats>
Message-ID: <Pine.LNX.4.61.0411241115150.24979@gannet.stats>

I added a check for CRLF termination of Fortran and C++ source files to R
CMD check and found potential problems in packages

BsMD
MCMCpack (C++)
asypow
aws
bayesSurv (C++)
eha
fBasics/fOptions/fSeries
gam
mclust
ncomplete
noverlap
pan
rrcov
subselect (C++)
survrec

I'd be interested to know if C++ gives your problems too.  (Sun cc used to 
object to CRLF, but does not in Forte 7.)  I don't think I've ever tried 
one of the above before on Solaris, but had no CRLF problems with Forte 7, 
just plenty of other problems in MCMCpack and bayesSurv.  Could you please 
try installing subselect.


On Wed, 24 Nov 2004, Prof Brian Ripley wrote:

> What did this have to do with GLMM?  I've changed the subject line.
>
> On Wed, 24 Nov 2004, Richard A. O'Keefe wrote:
>
>> I was trying to install some more packages and ran into a problem
>> I hadn't seen before.
>
> We've seen it for C, and test it for C in R CMD check.  I think we should 
> check C++ and Fortran files too.
>
>> Version:
>> 
>>    platform sparc-sun-solaris2.9
>>    arch     sparc
>>    os       solaris2.9
>>    system   sparc, solaris2.9
>>    status
>>    major    2
>>    minor    0.1
>>    year     2004
>>    month    11
>>    day      15
>>    language R
>> 
>> Fortran compilers available to me:
>> 
>>    f77: Sun WorkShop 6 update 2 FORTRAN 77 5.3 2001/05/15
>>    f90: Sun WorkShop 6 update 2 Fortran 95 6.2 2001/05/15
>>    f95: Sun WorkShop 6 update 2 Fortran 95 6.2 2001/05/15
>> 
>> Package:
>> 
>>    gam
>> 
>>    In fact I didn't ask for this one specifically, I had
>>    "dependencies=TRUE" in a call to install.packages().
>> 
>> Problem:
>> 
>>    Following the installation instructions for R, I had selected F95
>>    as my Fortran compiler.
>> 
>>    The f95 compiler complained about nearly every line of
>>    gam/src/bsplvd.f
>> 
>>    From the error messages as displayed on the screen, I could see no
>>    reason for complaint.  However, looking at the file with a text
>>    editor immediately revealed the problem.  The files
>> 
>> 	bsplvd.f	bvalue.f	bvalus.f	loessf.f
>> 	qsbart.f	sgram.f		sinerp.f	sslvrg.f
>> 	stxwx.f
>> 
>>    all use CR-LF line termination.  The files
>> 
>> 	linear.f	lo.f		splsm.f
>> 
>>    all use LF line termination expected on UNIX.
>> 
>>    It turns out that the g77 and f77 compilers don't mind CR at the
>>    end of a line, but f90 and f95 hate them like poison.
>> 
>>    Removing the CRs makes f90 and f95 happy again.
>
> BTW, in that version of Sun Workshop f90 and f95 are the same compiler, and 
> in later versions so is f77.  (I think these compilers are on version 9 now.) 
> Even in version 7, there is no problem with line endings.
>
> I did get a warning:
>
>      call dchdc(a,p,p,work,jpvt,job,info)
>                                     ^
> "linear.f", Line = 408, Column = 38: WARNING: Procedure "DCHDC" is defined at 
> line 194 (linear.f).  Illegal association of array actual argument with 
> scalar dummy argument "INFO".
>
> which seems genuine (make it info(1) in the call).
>
>
>> Second-order problem:
>> 
>>    I know how to fix the immediate problem.  What I don't know is how
>>    to intervene in the installation process.  What I need to do is
>> 	- get and unpack files (steps normally done by install.packages)
>> 	- make changes (remove CR, edit configuration, whatever)
>> 	- resume whatever install.packages normally does
>
> - Use install.packages(destdir=) to retain the tarballs which are downloaded.
>
> - Unpack the package tarball, make changes.
>
> - Run R CMD INSTALL on the changed sources.
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From murdoch at stats.uwo.ca  Wed Nov 24 12:54:07 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 24 Nov 2004 06:54:07 -0500
Subject: [R] Grumble ...
In-Reply-To: <XFMail.041124103635.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.041124103635.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <qct8q0pi21bi4hoa9p64cr2ahcr2ie4e6a@4ax.com>

On Wed, 24 Nov 2004 10:36:35 -0000 (GMT), (Ted Harding)
<Ted.Harding at nessie.mcc.ac.uk> wrote:

>Hi Folks,
>
>A Grumble ...
>
>The message I just sent to R-help about "The hidden costs of GPL ..."
>has evoked a "Challenge" response:
>
>  Hi,
>  You??ve just sent a message to diagnosticando at uol.com.br
>  In order to confirm the sent message, please click here

Here's a strategy that I hope subverts this irritating mechanism:
Every now and then I get a challenge about a message that I didn't
send, because someone (or some virus) forged me into the "From:"
address.  Those are the only ones I confirm.

Duncan Murdoch



From ramasamy at cancer.org.uk  Wed Nov 24 12:55:08 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Wed, 24 Nov 2004 11:55:08 +0000
Subject: [R] Automatic file reading
In-Reply-To: <41A46C1A.2030105@maths.lth.se>
References: <41A46C1A.2030105@maths.lth.se>
Message-ID: <1101297307.3158.23.camel@ndmpc126.ihs.ox.ac.uk>

for(i in 1:10){ assign( paste("data", i), i ) }
> data1
[1] 1
> data5
[1] 5
> data8 + data5
[1] 13

See help("assign") for more details and examples.


On Wed, 2004-11-24 at 11:10, Anders Malmberg wrote:
> Hi,
> 
> I want to do automatic reading of a number of tables (files) stored in 
> ascii format
> without having to specify the variable name in R each time.  Below is an 
> example
> of how I would like to use it (I assume files pair1,...,pair8 exist in 
> spec. dire.)
> 
> for (i in 1:8){
>   name <- paste("pair",i,sep="")
>   ? ? ? <- read.table(paste("/home/andersm/tmp/",name,sep=""))
> }
> 
> after which I want to have pair1,...,pair8 as tables.
> 
> But I can not get it right. Anybody having a smart solution?
> 
> Best regards,
> Anders Malmberg
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
-- 
Adaikalavan Ramasamy                    ramasamy at cancer.org.uk
Centre for Statistics in Medicine       http://www.ihs.ox.ac.uk/csm/
Cancer Research UK                      Tel : 01865 226 677
Old Road Campus, Headington, Oxford     Fax : 01865 226 962



From ligges at statistik.uni-dortmund.de  Wed Nov 24 12:58:00 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 24 Nov 2004 12:58:00 +0100
Subject: [R] Automatic file reading
In-Reply-To: <41A46C1A.2030105@maths.lth.se>
References: <41A46C1A.2030105@maths.lth.se>
Message-ID: <41A47748.6060206@statistik.uni-dortmund.de>

Anders Malmberg wrote:

> Hi,
> 
> I want to do automatic reading of a number of tables (files) stored in 
> ascii format
> without having to specify the variable name in R each time.  Below is an 
> example
> of how I would like to use it (I assume files pair1,...,pair8 exist in 
> spec. dire.)
> 
> for (i in 1:8){
>  name <- paste("pair",i,sep="")
>  ? ? ? <- read.table(paste("/home/andersm/tmp/",name,sep=""))
> }

pairlist <- vector(8, mode = "list")
for (i in 1:8){
   name <- paste("pair",i,sep="")
   pairlist[[i]] <- read.table(paste("/home/andersm/tmp/",name,sep=""))
}

or use assign(), but you don't want to do that really.

Uwe Ligges




> after which I want to have pair1,...,pair8 as tables.
> 
> But I can not get it right. Anybody having a smart solution?
> 
> Best regards,
> Anders Malmberg
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ahenningsen at email.uni-kiel.de  Wed Nov 24 13:31:33 2004
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Wed, 24 Nov 2004 13:31:33 +0100
Subject: [R] Automatic file reading
In-Reply-To: <41A46C1A.2030105@maths.lth.se>
References: <41A46C1A.2030105@maths.lth.se>
Message-ID: <200411241331.33276.ahenningsen@email.uni-kiel.de>

Hi Andreas,

what's about:
pair <- list()
for (i in 1:8){
   name <- paste("pair",i,sep="")
   pair[[ i ]] <- read.table(paste("/home/andersm/tmp/",name,sep=""))
}

Arne

On Wednesday 24 November 2004 12:10, Anders Malmberg wrote:
> Hi,
>
> I want to do automatic reading of a number of tables (files) stored in
> ascii format
> without having to specify the variable name in R each time.  Below is an
> example
> of how I would like to use it (I assume files pair1,...,pair8 exist in
> spec. dire.)
>
> for (i in 1:8){
>   name <- paste("pair",i,sep="")
>   ? ? ? <- read.table(paste("/home/andersm/tmp/",name,sep=""))
> }
>
> after which I want to have pair1,...,pair8 as tables.
>
> But I can not get it right. Anybody having a smart solution?
>
> Best regards,
> Anders Malmberg
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From sdavis2 at mail.nih.gov  Wed Nov 24 13:42:04 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Wed, 24 Nov 2004 07:42:04 -0500
Subject: [R] Automatic file reading
In-Reply-To: <200411241331.33276.ahenningsen@email.uni-kiel.de>
References: <41A46C1A.2030105@maths.lth.se>
	<200411241331.33276.ahenningsen@email.uni-kiel.de>
Message-ID: <3E838AED-3E16-11D9-9A35-000D933565E8@mail.nih.gov>

If you simply want read all files in a given directory, you can do 
something like:

fullpath = "/home/andersm/tmp"
filenames <- dir(fullpath,pattern="*")
pair <- sapply(filenames,function(x) 
{read.table(paste(fullpath,'/',x,sep=""))})

Sorry, untested.  But the point is that you can use dir to get all of 
the filenames specified by pattern from a directory specified by 
fullpath.

Sean

On Nov 24, 2004, at 7:31 AM, Arne Henningsen wrote:

> Hi Andreas,
>
> what's about:
> pair <- list()
> for (i in 1:8){
>    name <- paste("pair",i,sep="")
>    pair[[ i ]] <- read.table(paste("/home/andersm/tmp/",name,sep=""))
> }
>
> Arne
>
> On Wednesday 24 November 2004 12:10, Anders Malmberg wrote:
>> Hi,
>>
>> I want to do automatic reading of a number of tables (files) stored in
>> ascii format
>> without having to specify the variable name in R each time.  Below is 
>> an
>> example
>> of how I would like to use it (I assume files pair1,...,pair8 exist in
>> spec. dire.)
>>
>> for (i in 1:8){
>>   name <- paste("pair",i,sep="")
>>   ? ? ? <- read.table(paste("/home/andersm/tmp/",name,sep=""))
>> }
>>
>> after which I want to have pair1,...,pair8 as tables.
>>
>> But I can not get it right. Anybody having a smart solution?
>>
>> Best regards,
>> Anders Malmberg
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>
> -- 
> Arne Henningsen
> Department of Agricultural Economics
> University of Kiel
> Olshausenstr. 40
> D-24098 Kiel (Germany)
> Tel: +49-431-880 4445
> Fax: +49-431-880 1397
> ahenningsen at agric-econ.uni-kiel.de
> http://www.uni-kiel.de/agrarpol/ahenningsen/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From jgoebel at diw.de  Wed Nov 24 13:50:20 2004
From: jgoebel at diw.de (Jan Goebel)
Date: Wed, 24 Nov 2004 13:50:20 +0100
Subject: [R] data.frame into vector
In-Reply-To: <p06100508bdc915092d0e@[83.132.28.79]>
References: <p06100508bdc915092d0e@[83.132.28.79]>
Message-ID: <20041124125020.GA6894@diw138134.diw-berlin.de>

Hi,

as other already pointed out as.matrix is what you need.
Just one comment:

as.matrix(x[1,]) 

should be much faster for larger data frames compared to

as.matrix(x)[1,]

Best 

jan


On Tue, 23 Nov 2004, Tiago R Magalhaes wrote:

> Hi
> 
> I want to extract a row from a data.frame but I want that object to 
> be a vector . After trying some different ways I end up always with a 
> data.frame or with the wrong vector. Any pointers?
> 
>  x <- data.frame(a = factor(c('a',2,'b')), b = c(4,5,6))
> I want to get
> "a" "4"
> 
> I tried:
> 
> as.vector(x[1,])
>   a b
> 1 a 4
> (resulting in a data.frame even after in my mind having coerced it 
> into a vector!)
> 
> as.vector(c[1,], numeric='character')
> [1] "2" "4"
> (almost what I want, except that "2" instead of "a" - I guess this as 
> to do with levels and factors)
> 
> Thanks for any help
> 
> > R.Version()
> $platform
> [1] "powerpc-apple-darwin6.8"
> 
> $arch
> [1] "powerpc"
> 
> $os
> [1] "darwin6.8"
> 
> $system
> [1] "powerpc, darwin6.8"
> 
> $status
> [1] ""
> 
> $major
> [1] "2"
> 
> $minor
> [1] "0.1"
> 
> $year
> [1] "2004"
> 
> $month
> [1] "11"
> 
> $day
> [1] "15"
> 
> $language
> [1] "R"
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html

-- 
+-----------------------------------------
 Jan Goebel 
 j g o e b e l @ d i w . d e

 DIW Berlin 
 German Socio-Economic Panel Study (GSOEP) 
 K??nigin-Luise-Str. 5
 D-14195 Berlin -- Germany --
 phone: 49 30 89789-377
+-----------------------------------------



From ggrothendieck at myway.com  Wed Nov 24 13:49:18 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 24 Nov 2004 12:49:18 +0000 (UTC)
Subject: [R] timeDate
References: <b1d315040411220945366432ab@mail.gmail.com>
	<41A22EF0.60709@statistik.uni-dortmund.de>
	<b1d3150404112213085eda9b3f@mail.gmail.com>
	<loom.20041122T224151-145@post.gmane.org>
	<b1d3150404112306557eb13ab0@mail.gmail.com>
	<loom.20041124T033659-681@post.gmane.org>
Message-ID: <loom.20041124T134347-878@post.gmane.org>

Gabor Grothendieck <ggrothendieck <at> myway.com> writes:

: 
: Yasser El-Zein <abu3ammar <at> gmail.com> writes:
: 
: : 
: : I am looking for up to the millisecond resolution. Is there a package
: : that has that?
: : 
: : On Mon, 22 Nov 2004 21:48:20 +0000 (UTC), Gabor Grothendieck
: : <ggrothendieck <at> myway.com> wrote:
: : > Yasser El-Zein <abu3ammar <at> gmail.com> writes:
: : > 
: : > >
: : > > >From the document it is apparent to me that I need as.POSIXct  (I have
: : > > a double representing the number of millis since 1/1/1970 and I need
: : > > to construct a datetime object). I see it showing how to construct the
: : > > time object from a string representing the time but now fro a double
: : > > of millis. Does anyone know hoe to do that?
: : > >
: : > 
: : > If by millis you mean milliseconds (i.e. one thousandths of a second)
: : > then POSIXct does not support that resolution, but if rounding to
: : > seconds is ok then
: : > 
: : >   structure(round(x/1000), class = c("POSIXt", "POSIXct"))
: : > 
: : > should give it to you assuming x is the number of milliseconds.
: 
: There is no package/class that represents times and dates
: internally as milliseoncds since Jan 1, 1970.   You can
: rework your data into chron's internal representation, viz.
: day number plus fraction of day, like this:
: 
: 	# x is vector of milliseconds since Jan 1/70
: 	# x.chron is corresponding chron date/time
: 	# untested
: 	library(chron) 
: 	ms.in.day <- 1000*24*60*60 
: 	day <- floor(x/ms.in.day) 
: 	frac <- (x-1000*day)/ms.in.day
: 	x.chron <- chron(day+frac)

Not sure why I made the above so complicated but it can
be written just as:

        library(chron)
        ms.in.day <- 1000*24*60*60
        x.chron <- chron(x/ms.in.day)

: If you need to take leap seconds into account (which the above
: does not) then note that R comes with a builtin vector called
: leap.seconds.



From maechler at stat.math.ethz.ch  Wed Nov 24 13:53:17 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 24 Nov 2004 13:53:17 +0100
Subject: [R] Grumble ...
In-Reply-To: <XFMail.041124103635.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.041124103635.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <16804.33853.572143.292962@gargle.gargle.HOWL>

>>>>> "Ted" == Ted Harding <Ted.Harding at nessie.mcc.ac.uk>
>>>>>     on Wed, 24 Nov 2004 10:36:35 -0000 (GMT) writes:

    Ted> Hi Folks, A Grumble ...

    Ted> The message I just sent to R-help about "The hidden
    Ted> costs of GPL ..."  has evoked a "Challenge" response:

    Ted>   Hi, You??ve just sent a message to
    Ted> diagnosticando at uol.com.br In order to confirm the sent
    Ted> message, please click here

    Ted>   This confirmation is necessary because
    Ted> diagnosticando at uol.com.br uses Antispam UOL, a service
    Ted> that avoids unwanted messages like advertising,
    Ted> pornography, viruses, and spams.

    Ted>   Other messages sent to diagnosticando at uol.com.br
    Ted> won't need to be confirmed*.  *If you receive another
    Ted> confirmation request, please ask
    Ted> diagnosticando at uol.com.br to include you in his/her
    Ted> authorized e-mail list.

    Ted> I won't be responding to this. Let the recipient simply
    Ted> not receive the mail. Of no great importance in this
    Ted> case, but a disadvantage to the recipient in the long
    Ted> run.

    Ted> I disapprove strongly of this mechanism, and want to
    Ted> oppose it.  There must be a few thousand subscribers to
    Ted> R-help. If the "Challenge" mechanism became widespread,
    Ted> then I would receive thousands of such messages. Rather
    Ted> than respond to all these, I would quit the list (and
    Ted> of course probably many others).  The "Challenge"
    Ted> mechanism would destroy the mailing-list community if
    Ted> it became widely adopted.

Exactly.
I've received such a message myself from the same "machine" and
-- as mailing list manager -- tried to find out more.

The problem is that diagnosticando at uol.com.br is not subscribed
to R-help. One other person is and I have written e-mail to that
address withOUT getting such a message back..

Again, I completely agree that it is absolutely inacceptable 
to subscribe from such a spam-blocking address.


    Ted> One reason I am posting this grumble to R-help is in
    Ted> the hope that I get a challenge to this one too. In
    Ted> that case, once and for all, I shall respond, so that
    Ted> the recipient will see this message and (I hope) do
    Ted> something about it, to eliminate the "Challenge"
    Ted> responder (I can't find the true recipient's email
    Ted> address from the "Challenge").

please let me (or R-help too) know what you find out.

Martin Maechler, ETH Zurich
(R-help mailing list maintainer)

    Ted> The recipient may be able to recognise themselves from
    Ted> the fact that they receive this message but not the
    Ted> message which triggered the response, which began:
    Ted> ======================================= On 24-Nov-04
    Ted> John wrote:
    >> Off hand, the costs of GPL'd software are not hidden at
    >> all.  R for instance demands that a would be user sit
    >> down and learn the language. This in turn pushes a user
    >> into learning more about statistics than the simple
    >> overview that Stat 1 presents a student.

    Ted> I'd see this as less a cost than a benefit!
    Ted> =======================================

    Ted> My apologies for bothering you with this if you didn't
    Ted> want to know about it.

    Ted> Best wishes to all, Ted.



From p.dalgaard at biostat.ku.dk  Wed Nov 24 13:59:41 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 24 Nov 2004 13:59:41 +0100
Subject: [R] Grumble ...
In-Reply-To: <qct8q0pi21bi4hoa9p64cr2ahcr2ie4e6a@4ax.com>
References: <XFMail.041124103635.Ted.Harding@nessie.mcc.ac.uk>
	<qct8q0pi21bi4hoa9p64cr2ahcr2ie4e6a@4ax.com>
Message-ID: <x2act7z9ky.fsf@biostat.ku.dk>

Duncan Murdoch <murdoch at stats.uwo.ca> writes:

> On Wed, 24 Nov 2004 10:36:35 -0000 (GMT), (Ted Harding)
> <Ted.Harding at nessie.mcc.ac.uk> wrote:
> 
> >Hi Folks,
> >
> >A Grumble ...
> >
> >The message I just sent to R-help about "The hidden costs of GPL ..."
> >has evoked a "Challenge" response:
> >
> >  Hi,
> >  You??ve just sent a message to diagnosticando at uol.com.br
> >  In order to confirm the sent message, please click here
> 
> Here's a strategy that I hope subverts this irritating mechanism:
> Every now and then I get a challenge about a message that I didn't
> send, because someone (or some virus) forged me into the "From:"
> address.  Those are the only ones I confirm.

Hehe... But don't you risk getting listed as an "active spammer" or
something that way? Personally I just send them to the bogus folder
for later update to the spamfilter.

Imagine if everyone had this challenge stuff installed and we had to
confirm every message ~1e3 times (how many subscribers are we these
days). The vacation messages are annoying enough. 

I wonder how this guy got on the list in the first place. I suspect
that he couldn't actually have completed the subscription process
unless the mechanism was installed after the subscription.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From rdiaz at cnio.es  Wed Nov 24 14:01:35 2004
From: rdiaz at cnio.es (Ramon Diaz-Uriarte)
Date: Wed, 24 Nov 2004 15:01:35 +0200
Subject: [R] LDA with previous PCA for dimensionality reduction
In-Reply-To: <41A45F99.8030808@gmx.ch>
References: <41A45F99.8030808@gmx.ch>
Message-ID: <200411241401.35329.rdiaz@cnio.es>

Dear Cristoph,

I guess you want to assess the error rate of a LDA that has been fitted to a 
set of currently existing training data, and that in the future you will get 
some new observation(s) for which you want to make a prediction.
Then, I'd say that you want to use the second approach. You might find that 
the first step turns out to be crucial and, after all, your whole subsequent 
LDA is contingent on the PC scores you obtain on the previous step. Somewhat 
similar issues have been discussed in the microarray literature. Two 
references are:


@ARTICLE{ambroise-02,
  author = {Ambroise, C. and McLachlan, G. J.},
  title = {Selection bias in gene extraction on the basis of microarray 
gene-expression data},
  journal = {Proc Natl Acad Sci USA},
  year = {2002},
  volume = {99},
  pages = {6562--6566},
  number = {10},
}


@ARTICLE{simon-03,
  author = {Simon, R. and Radmacher, M. D. and Dobbin, K. and McShane, L. M.},
  title = {Pitfalls in the use of DNA microarray data for diagnostic and 
prognostic classification},
  journal = {Journal of the National Cancer Institute},
  year = {2003},
  volume = {95},
  pages = {14--18},
  number = {1},
}


I am not sure, though, why you use PCA followed by LDA. But that's another 
story.

Best,


R.

On Wednesday 24 November 2004 11:16, Christoph Lehmann wrote:
> Dear all, not really a R question but:
>
> If I want to check for the classification accuracy of a LDA with
> previous PCA for dimensionality reduction by means of the LOOCV method:
>
> Is it ok to do the PCA on the WHOLE dataset ONCE and then run the LDA
> with the CV option set to TRUE (runs LOOCV)
>
> -- OR--
>
> do I need
> - to compute for each 'test-bag' (the n-1 observations) a PCA
> (my.princomp.1),
> - then run the LDA on the test-bag scores (-> my.lda.1)
> - then compute the scores of the left-out-observation using
> my.princomp.1 (-> my.scores.2)
> - and only then use predict.lda(my.lda.1, my.scores.2) on the scores of
> the left-out-observation
>
> ?
> I read some articles, where they choose procedure 1, but I am not sure,
> if this is really correct?
>
> many thanks for a hint
>
> Christoph
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Ram??n D??az-Uriarte
Bioinformatics Unit
Centro Nacional de Investigaciones Oncol??gicas (CNIO)
(Spanish National Cancer Center)
Melchor Fern??ndez Almagro, 3
28029 Madrid (Spain)
Fax: +-34-91-224-6972
Phone: +-34-91-224-6900

http://ligarto.org/rdiaz
PGP KeyID: 0xE89B3462
(http://ligarto.org/rdiaz/0xE89B3462.asc)



From Ted.Harding at nessie.mcc.ac.uk  Wed Nov 24 14:28:49 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 24 Nov 2004 13:28:49 -0000 (GMT)
Subject: [R] Grumble ...
In-Reply-To: <16804.33853.572143.292962@gargle.gargle.HOWL>
Message-ID: <XFMail.041124132849.Ted.Harding@nessie.mcc.ac.uk>

On 24-Nov-04 Martin Maechler wrote:
>>>>>> "Ted" == Ted Harding <Ted.Harding at nessie.mcc.ac.uk>
> [...]
>     Ted> [...]  The "Challenge"
>     Ted> mechanism would destroy the mailing-list community if
>     Ted> it became widely adopted.
> 
> Exactly.
> I've received such a message myself from the same "machine" and
> -- as mailing list manager -- tried to find out more.
> 
> The problem is that diagnosticando at uol.com.br is not subscribed
> to R-help. One other person is and I have written e-mail to that
> address withOUT getting such a message back..
> 
> Again, I completely agree that it is absolutely inacceptable 
> to subscribe from such a spam-blocking address.
> 
> 
>     Ted> One reason I am posting this grumble to R-help is in
>     Ted> the hope that I get a challenge to this one too. In
>     Ted> that case, once and for all, I shall respond, so that
>     Ted> the recipient will see this message and (I hope) do
>     Ted> something about it, to eliminate the "Challenge"
>     Ted> responder (I can't find the true recipient's email
>     Ted> address from the "Challenge").
> 
> please let me (or R-help too) know what you find out.
> 
> Martin Maechler, ETH Zurich
> (R-help mailing list maintainer)

Hi Martin,
I'm pleased to receive such positive support! (I was apprehensive
about annoying the list with this issue).

I did get a "Challenge" to the Grumble, so I confirmed that one.
I have heard nothing yet from anyone who might be connected to
uol.com.br but of course may do so later.

I visited the uol.com.br website and found the following on it:

  http://antispam.uol.com.br/autorizados.jhtm#discussao

  4. Como autorizar listas de discuss??o?
  As mensagens de algumas listas/grupos de discuss??o
  chegam com o campo "Para:" preenchido com um e-mail
  que n??o ?? o seu (e sim o e-mail geral da lista).
  Para receber estas mensagens em sua pasta de entrada,
  acesse o Web Mail e clique no op????o Grupos/Listas do
  AntiSpam UOL. Em seguida, autorize o e-mail das listas
  que voc?? participa.

which, as well as I can understand it, suggests that someone
subscribed to a mailing list can set that mailing-list as an
acceptable sender. What's not clear to me is whether, having
done that, it over-rides challenging the originator (e.g. me)
of the message.

Thanks, and best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 24-Nov-04                                       Time: 13:28:49
------------------------------ XFMail ------------------------------



From feferraz at ime.usp.br  Wed Nov 24 15:03:00 2004
From: feferraz at ime.usp.br (Fernando Henrique Ferraz P. da Rosa)
Date: Wed, 24 Nov 2004 12:03:00 -0200
Subject: [R] Two factor ANOVA in lme
In-Reply-To: <41A07E4C.7080902@soton.ac.uk>
References: <200411211115.iALB5f6s029937@hypatia.math.ethz.ch>
	<41A07E4C.7080902@soton.ac.uk>
Message-ID: <20041124140300.GB986@ime.usp.br>

nat writes:
> I want to specify a two-factor model in lme, which should be easy? 
> Here's what I have:
> 
> factor 1 - treatment FIXED (two levels)
> factor 2 - genotype RANDOM (160 genotypes in total)
> 
> I need a model that tells me whether the treatment, genotype and 
> interaction terms are significant. I have been reading 'Mixed effects 
> models in S' but in all examples the random factor is not in the main 
> model - it is a nesting factor etc to specify the error structure. Here 
> i need the random factor in the model.
> 
> I have tried this:
> 
> height.aov<-lme(height~trt*genotype,data.reps,random=~1|genotype,na.action=na.exclude)
> 
> but the output is nothing like that from Minitab (my only previous 
> experience of stats software). The results for the interaction term are 
> the same but F values for the factors alone are very different between 
> Minitab and R.
> 
> This is a very simple model but I can't figure out how to specify it. 
> Help would be much appreciated.
> 
> As background: The data are from a QTL mapping population, which is why 
> I must test to see if genotype is significant and also why genotype is a 
> random factor.
> 
> Thanks

        It seems your message didn't get any replies (at least none
posted to r-help). 

        I recentely adjusted such a model  (two effects, one fixed,
another random, with interaction effects) using lme. I used the
following command: 

 z1 <- lme(reacao ~ posicao,data=memoria,random=~1|subject/posicao)

        Where my model is 

         reacao = mu + posicao (fixed) + posicao*subject (random) +
subject (random)

         Beware though that minitab uses different estimation methods (in lme
itself you may use maximum likelihood other restricted m.l) and the
results need not to be the same.

 


--
Fernando Henrique Ferraz P. da Rosa
http://www.ime.usp.br/~feferraz



From murdoch at stats.uwo.ca  Wed Nov 24 15:09:28 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 24 Nov 2004 09:09:28 -0500
Subject: [R] Grumble ...
In-Reply-To: <x2act7z9ky.fsf@biostat.ku.dk>
References: <XFMail.041124103635.Ted.Harding@nessie.mcc.ac.uk>
	<qct8q0pi21bi4hoa9p64cr2ahcr2ie4e6a@4ax.com>
	<x2act7z9ky.fsf@biostat.ku.dk>
Message-ID: <2659q0lnfa8epodough2n9u8b62tt9p3qj@4ax.com>

On 24 Nov 2004 13:59:41 +0100, Peter Dalgaard
<p.dalgaard at biostat.ku.dk> wrote :

>Duncan Murdoch <murdoch at stats.uwo.ca> writes:

>> Here's a strategy that I hope subverts this irritating mechanism:
>> Every now and then I get a challenge about a message that I didn't
>> send, because someone (or some virus) forged me into the "From:"
>> address.  Those are the only ones I confirm.
>
>Hehe... But don't you risk getting listed as an "active spammer" or
>something that way? 

That could happen, but that's really a benefit:  it means the user of
the challenge response system will find his filters don't work
properly, another reason to drop it.

>Personally I just send them to the bogus folder
>for later update to the spamfilter.

I do that too, which may be why I only see these "now and then".

Duncan Murdoch



From Torsten.Hothorn at rzmail.uni-erlangen.de  Wed Nov 24 15:43:13 2004
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Wed, 24 Nov 2004 15:43:13 +0100 (CET)
Subject: [R] LDA with previous PCA for dimensionality reduction
In-Reply-To: <200411241401.35329.rdiaz@cnio.es>
References: <41A45F99.8030808@gmx.ch> <200411241401.35329.rdiaz@cnio.es>
Message-ID: <Pine.LNX.4.51.0411241535040.19391@artemis.imbe.med.uni-erlangen.de>


On Wed, 24 Nov 2004, Ramon Diaz-Uriarte wrote:

> Dear Cristoph,
>
> I guess you want to assess the error rate of a LDA that has been fitted to a
> set of currently existing training data, and that in the future you will get
> some new observation(s) for which you want to make a prediction.
> Then, I'd say that you want to use the second approach. You might find that
> the first step turns out to be crucial and, after all, your whole subsequent
> LDA is contingent on the PC scores you obtain on the previous step.

Ramon,

as long as one does not use the information in the response (the class
variable, in this case) I don't think that one ends up with an
optimistically biased estimate of the error (although leave-one-out is
a suboptimal choice). Of course, when one starts to "tune" the method
used for dimension reduction, a selection of the procedure with minimal
error will produce a bias. Or am I missing something important?

Btw, `ipred::slda' implements something not completely unlike the
procedure Christoph is interested in.

Best,

Torsten

> Somewhat
> similar issues have been discussed in the microarray literature. Two
> references are:
>
>
> @ARTICLE{ambroise-02,
>   author = {Ambroise, C. and McLachlan, G. J.},
>   title = {Selection bias in gene extraction on the basis of microarray
> gene-expression data},
>   journal = {Proc Natl Acad Sci USA},
>   year = {2002},
>   volume = {99},
>   pages = {6562--6566},
>   number = {10},
> }
>
>
> @ARTICLE{simon-03,
>   author = {Simon, R. and Radmacher, M. D. and Dobbin, K. and McShane, L. M.},
>   title = {Pitfalls in the use of DNA microarray data for diagnostic and
> prognostic classification},
>   journal = {Journal of the National Cancer Institute},
>   year = {2003},
>   volume = {95},
>   pages = {14--18},
>   number = {1},
> }
>
>
> I am not sure, though, why you use PCA followed by LDA. But that's another
> story.
>
> Best,
>
>
> R.
>
> On Wednesday 24 November 2004 11:16, Christoph Lehmann wrote:
> > Dear all, not really a R question but:
> >
> > If I want to check for the classification accuracy of a LDA with
> > previous PCA for dimensionality reduction by means of the LOOCV method:
> >
> > Is it ok to do the PCA on the WHOLE dataset ONCE and then run the LDA
> > with the CV option set to TRUE (runs LOOCV)
> >
> > -- OR--
> >
> > do I need
> > - to compute for each 'test-bag' (the n-1 observations) a PCA
> > (my.princomp.1),
> > - then run the LDA on the test-bag scores (-> my.lda.1)
> > - then compute the scores of the left-out-observation using
> > my.princomp.1 (-> my.scores.2)
> > - and only then use predict.lda(my.lda.1, my.scores.2) on the scores of
> > the left-out-observation
> >
> > ?
> > I read some articles, where they choose procedure 1, but I am not sure,
> > if this is really correct?
> >
> > many thanks for a hint
> >
> > Christoph
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
>
> --
> Ram??n D??az-Uriarte
> Bioinformatics Unit
> Centro Nacional de Investigaciones Oncol??gicas (CNIO)
> (Spanish National Cancer Center)
> Melchor Fern??ndez Almagro, 3
> 28029 Madrid (Spain)
> Fax: +-34-91-224-6972
> Phone: +-34-91-224-6900
>
> http://ligarto.org/rdiaz
> PGP KeyID: 0xE89B3462
> (http://ligarto.org/rdiaz/0xE89B3462.asc)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From abunn at whrc.org  Wed Nov 24 15:48:35 2004
From: abunn at whrc.org (Andy Bunn)
Date: Wed, 24 Nov 2004 09:48:35 -0500
Subject: [R] an R function to search on Prof. Baron's site
In-Reply-To: <loom.20041124T054946-210@post.gmane.org>
Message-ID: <NEBBIPHDAMMOKDKPOFFIOELICMAA.abunn@whrc.org>

Using this function with 2.0.0 XP and Firefox 1.0 (I've rediscovered the
internet) produces a curious result.

> myString <- RSiteSearch(string = 'Ripley')
> myString
[1]
"http://finzi.psych.upenn.edu/cgi-bin/htsearch?config=htdigrun1;restrict=Rhe
lp00/archive|Rhelp01/archive|Rhelp02a/archive;format=builtin-long;sort=score
;words=Ripley;matchesperpage=10"
> version
         _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    2
minor    0.0
year     2004
month    10
day      04
language R

If no browser is open, then this is the URL that is browsed in Firefox:
http://finzi.psych.upenn.edu/cgi-bin/htsearch?config=htdigrun1;restrict=Rhel
p00/archive

Oddly, these two other windows are opened too:
http://finzi.psych.upenn.edu/R/Rhelp01/archive/1000.html

and:
http://www.mail-archive.com/r-help at stat.math.ethz.ch/msg17461.html

This happens regardless of what the search string is. If a browser window is
open then everything works as planned. The sticky bit, obviously, is parsing
browseURL which has the same behavior if I try:
> browseURL(myString)

However, the searches:
> RSiteSearch(string = 'browseURL Firefox')
> RSiteSearch(string = 'browseURL Mozilla')

don't turn up much help! If I change browseURL to use IE then browseURL
behaves as expected:

> browseURL(myString, browser="C:/Program Files/Internet
Explorer/iexplore.exe")

Specifying Firefox explicitly in browseURL doesn't help - It still opens
three windows as above (if no browser is open):

> browseURL(myString, browser="C:/Program Files/Mozilla
Firefox/firefox.exe")

So, under Windows the 'NULL' argument in 'browser' which determines the
browser via file association isn't the problem.

Anybody know how I can make Firefox work a little more smoothly?

Thanks, Andy



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Gabor Grothendieck
> Sent: Tuesday, November 23, 2004 11:56 PM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] an R function to search on Prof. Baron's site
>
>
> Liaw, Andy <andy_liaw <at> merck.com> writes:
>
> :
> : Inspired by the functions that Barry Rawlingson and Dave
> Forrest posted for
> : searching Rwiki and R-help archive, I've made up a function
> that does the
> : search on Prof. Baron's site (Thanks to Prof. Baron's help on
> setting up the
> : query string!):
>
> It would be nice if this and the other search functions recently
> posted were collected into a package or even integrated into
> R itself.  In the case of the Windows Rgui, it would be nice if they
> appeared on a menu with the other search and help functions.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From christoph.lehmann at gmx.ch  Wed Nov 24 15:52:51 2004
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Wed, 24 Nov 2004 15:52:51 +0100
Subject: [R] LDA with previous PCA for dimensionality reduction
In-Reply-To: <Pine.LNX.4.51.0411241535040.19391@artemis.imbe.med.uni-erlangen.de>
References: <41A45F99.8030808@gmx.ch> <200411241401.35329.rdiaz@cnio.es>
	<Pine.LNX.4.51.0411241535040.19391@artemis.imbe.med.uni-erlangen.de>
Message-ID: <41A4A043.9070905@gmx.ch>

Thank you, Torsten; that's what I thought, as long as one does not use 
the 'class label' as a constraint in the dimension reduction, the 
procedure is ok. Of course it is computationally more demanding, since 
for each new (unknown in respect of the class label) observation one has 
to compute a new PCA as well.

Cheers

Christoph

Torsten Hothorn wrote:
> On Wed, 24 Nov 2004, Ramon Diaz-Uriarte wrote:
> 
> 
>>Dear Cristoph,
>>
>>I guess you want to assess the error rate of a LDA that has been fitted to a
>>set of currently existing training data, and that in the future you will get
>>some new observation(s) for which you want to make a prediction.
>>Then, I'd say that you want to use the second approach. You might find that
>>the first step turns out to be crucial and, after all, your whole subsequent
>>LDA is contingent on the PC scores you obtain on the previous step.
> 
> 
> Ramon,
> 
> as long as one does not use the information in the response (the class
> variable, in this case) I don't think that one ends up with an
> optimistically biased estimate of the error (although leave-one-out is
> a suboptimal choice). Of course, when one starts to "tune" the method
> used for dimension reduction, a selection of the procedure with minimal
> error will produce a bias. Or am I missing something important?
> 
> Btw, `ipred::slda' implements something not completely unlike the
> procedure Christoph is interested in.
> 
> Best,
> 
> Torsten
> 
> 
>>Somewhat
>>similar issues have been discussed in the microarray literature. Two
>>references are:
>>
>>
>>@ARTICLE{ambroise-02,
>>  author = {Ambroise, C. and McLachlan, G. J.},
>>  title = {Selection bias in gene extraction on the basis of microarray
>>gene-expression data},
>>  journal = {Proc Natl Acad Sci USA},
>>  year = {2002},
>>  volume = {99},
>>  pages = {6562--6566},
>>  number = {10},
>>}
>>
>>
>>@ARTICLE{simon-03,
>>  author = {Simon, R. and Radmacher, M. D. and Dobbin, K. and McShane, L. M.},
>>  title = {Pitfalls in the use of DNA microarray data for diagnostic and
>>prognostic classification},
>>  journal = {Journal of the National Cancer Institute},
>>  year = {2003},
>>  volume = {95},
>>  pages = {14--18},
>>  number = {1},
>>}
>>
>>
>>I am not sure, though, why you use PCA followed by LDA. But that's another
>>story.
>>
>>Best,
>>
>>
>>R.
>>
>>On Wednesday 24 November 2004 11:16, Christoph Lehmann wrote:
>>
>>>Dear all, not really a R question but:
>>>
>>>If I want to check for the classification accuracy of a LDA with
>>>previous PCA for dimensionality reduction by means of the LOOCV method:
>>>
>>>Is it ok to do the PCA on the WHOLE dataset ONCE and then run the LDA
>>>with the CV option set to TRUE (runs LOOCV)
>>>
>>>-- OR--
>>>
>>>do I need
>>>- to compute for each 'test-bag' (the n-1 observations) a PCA
>>>(my.princomp.1),
>>>- then run the LDA on the test-bag scores (-> my.lda.1)
>>>- then compute the scores of the left-out-observation using
>>>my.princomp.1 (-> my.scores.2)
>>>- and only then use predict.lda(my.lda.1, my.scores.2) on the scores of
>>>the left-out-observation
>>>
>>>?
>>>I read some articles, where they choose procedure 1, but I am not sure,
>>>if this is really correct?
>>>
>>>many thanks for a hint
>>>
>>>Christoph
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide!
>>>http://www.R-project.org/posting-guide.html
>>
>>--
>>Ram??n D??az-Uriarte
>>Bioinformatics Unit
>>Centro Nacional de Investigaciones Oncol??gicas (CNIO)
>>(Spanish National Cancer Center)
>>Melchor Fern??ndez Almagro, 3
>>28029 Madrid (Spain)
>>Fax: +-34-91-224-6972
>>Phone: +-34-91-224-6900
>>
>>http://ligarto.org/rdiaz
>>PGP KeyID: 0xE89B3462
>>(http://ligarto.org/rdiaz/0xE89B3462.asc)
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>
> 
> 
>



From infocd at i-network.com  Wed Nov 24 16:09:29 2004
From: infocd at i-network.com (infocd@i-network.com)
Date: Wed, 24 Nov 2004 16:09:29 +0100 (CET)
Subject: [R] Respuesta Automatica CorreoDirect.
Message-ID: <20041124150929.9C96853E2E@mail.i-network.com>

###################################################
Este email esta generado de manera automatica
###################################################

Ha escrito usted a una cuenta de correo generica de CorreoDirect.

Si desea conocer nuestra pol??tica de privacidad, pulse aqu??.
http://www.correodirect.com/public/nosotros/privacidad.php

Si usted esta dado de alta en nuestro servicio, tiene la posibilidad de
darse f??cilmente de baja o bien modificar su perfil para que las ofertas
que le enviamos se ajusten mejor a sus intereses.

Si lo que desea es modificar su perfil pulse aqu??
http://www.correodirect.com/usuarios/area/modif.php

Si desea darse de baja de nuestro servicio pulse aqu??
http://www.correodirect.com/usuarios/area/baja.php

Si cree que sus datos se encuentran por error en nuestra Base de Datos, 
escriba un email a reclamaciones at correodirect.com

Si tiene dudas sobre nuestro funcionamiento, acuda a
http://www.correodirect.com/public/ayuda/faqusuario.php

Atentamente,
Atenci??n al Usuario, CorreoDirect

www.correodirect.com

CorreoDirect, el lider en permission email marketing.



From dle at aber.ac.uk  Wed Nov 24 16:18:14 2004
From: dle at aber.ac.uk (David Enot)
Date: Wed, 24 Nov 2004 15:18:14 +0000
Subject: [R] LDA with previous PCA for dimensionality reduction
In-Reply-To: <41A45F99.8030808@gmx.ch>
References: <41A45F99.8030808@gmx.ch>
Message-ID: <0FC4CF09-3E2C-11D9-9517-000A95BBEB3C@aber.ac.uk>


On 24 Nov 2004, at 10:16, Christoph Lehmann wrote:

> Dear all, not really a R question but:
>
> If I want to check for the classification accuracy of a LDA with 
> previous PCA for dimensionality reduction by means of the LOOCV 
> method:
>
> Is it ok to do the PCA on the WHOLE dataset ONCE and then run the LDA 
> with the CV option set to TRUE (runs LOOCV)
>
> -- OR--
>
> do I need
> - to compute for each 'test-bag' (the n-1 observations) a PCA 
> (my.princomp.1),
> - then run the LDA on the test-bag scores (-> my.lda.1)
> - then compute the scores of the left-out-observation using 
> my.princomp.1 (-> my.scores.2)
> - and only then use predict.lda(my.lda.1, my.scores.2) on the scores 
> of the left-out-observation
>
> ?
> I read some articles, where they choose procedure 1, but I am not 
> sure, if this is really correct?


As far as understand your problem (assessing the predictive ability of 
your model), the second solution should be done: the test set is 
something that should be never seen by the training data. If you run 
your PCA on the whole set, then you will take into account your test 
bag while forming your training data. Keep in mind that your classifier 
is made up with 2 components: PCA followed by LDA. This is fine if you 
build your model with a given number of PC's: the procedure to get an 
optimal number of PC's would be similar as above but considering the 
(n-1) examples. A proper validation of the model can become quickly 
tricky: this requires a bit of computing skills and this may take 
longer (especially with LOO)!

  Hope it helps


   David



>
> many thanks for a hint
>
> Christoph
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From wolski at molgen.mpg.de  Wed Nov 24 16:34:46 2004
From: wolski at molgen.mpg.de (Witold Eryk Wolski)
Date: Wed, 24 Nov 2004 16:34:46 +0100
Subject: [R] scatterplot of 100000 points and pdf file format
Message-ID: <41A4AA16.7070008@molgen.mpg.de>

Hi,

I want to draw a scatter plot with 1M  and more points and save it as pdf.
This makes the pdf file large.
So i tried to save the file first as png and than convert it to pdf. 
This looks OK if printed but if viewed e.g. with acrobat as document 
figure the quality is bad.

Anyone knows a way to reduce the size but keep the quality?


/E

-- 
Dipl. bio-chem. Witold Eryk Wolski
MPI-Moleculare Genetic
Ihnestrasse 63-73 14195 Berlin
tel: 0049-30-83875219                 __("<    _
http://www.molgen.mpg.de/~wolski      \__/    'v'
http://r4proteomics.sourceforge.net    ||    /   \
mail: witek96 at users.sourceforge.net    ^^     m m
      wolski at molgen.mpg.de



From apollo_w at yahoo.com  Wed Nov 24 16:37:18 2004
From: apollo_w at yahoo.com (apollo wong)
Date: Wed, 24 Nov 2004 07:37:18 -0800 (PST)
Subject: [R] >2GB dataset
Message-ID: <20041124153718.77378.qmail@web51402.mail.yahoo.com>

Hi, do any one have experience with loading dataset
that is larger than 2GB into R. My organization is a
SAS oriented shop and I'm in the process of switching
it to R. One of the complain about R has always been
it's inability to handle large dataset (>GB)
efficiently. I would like some comments from someone
with experience of working on >2GB dataset in R.
Thanks.
Apollo



From drcarbon at gmail.com  Wed Nov 24 16:48:41 2004
From: drcarbon at gmail.com (Dr Carbon)
Date: Wed, 24 Nov 2004 10:48:41 -0500
Subject: [R] tsdiag for ar?
Message-ID: <e89bb7ac041124074871ab8d01@mail.gmail.com>

Is there a way to have the ar function work with tsdiag for on-the-fly
visualization of ar fits? I have to fit a great many models of varying
order and would like to save the diagnostic graphs.

For instance, 
tsdiag(ar(lh))
tsdiag(arima(lh, order = c(3,0,0)))

Thanks...



From ripley at stats.ox.ac.uk  Wed Nov 24 17:02:05 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 24 Nov 2004 16:02:05 +0000 (GMT)
Subject: [R] >2GB dataset
In-Reply-To: <20041124153718.77378.qmail@web51402.mail.yahoo.com>
References: <20041124153718.77378.qmail@web51402.mail.yahoo.com>
Message-ID: <Pine.LNX.4.61.0411241558280.2129@gannet.stats>

Absolutely no problem on 64-bit OSes with enough memory.  Many 32-bit OSes 
have problems with > 2Gb files.

Please do read the posting guide and tell us basic facts like which OS you 
are running on, so we don't have to speculate to answer your question.

Also, what you want to do with the dataset?   This matters crucially.

On Wed, 24 Nov 2004, apollo wong wrote:

> Hi, do any one have experience with loading dataset
> that is larger than 2GB into R. My organization is a
> SAS oriented shop and I'm in the process of switching
> it to R. One of the complain about R has always been
> it's inability to handle large dataset (>GB)
> efficiently. I would like some comments from someone
> with experience of working on >2GB dataset in R.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Nov 24 17:05:03 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 24 Nov 2004 16:05:03 +0000 (GMT)
Subject: [R] tsdiag for ar?
In-Reply-To: <e89bb7ac041124074871ab8d01@mail.gmail.com>
References: <e89bb7ac041124074871ab8d01@mail.gmail.com>
Message-ID: <Pine.LNX.4.61.0411241603020.2129@gannet.stats>

On Wed, 24 Nov 2004, Dr Carbon wrote:

> Is there a way to have the ar function work with tsdiag for on-the-fly
> visualization of ar fits? I have to fit a great many models of varying
> order and would like to save the diagnostic graphs.

First you have to produce them, surely?

> For instance,
> tsdiag(ar(lh))

That gives an error. All you have to do is to write an "ar" method for 
tsdiag -- a good exercise in R programming for you.

> tsdiag(arima(lh, order = c(3,0,0)))


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From MSchwartz at MedAnalytics.com  Wed Nov 24 17:22:56 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Wed, 24 Nov 2004 10:22:56 -0600
Subject: [R] scatterplot of 100000 points and pdf file format
In-Reply-To: <41A4AA16.7070008@molgen.mpg.de>
References: <41A4AA16.7070008@molgen.mpg.de>
Message-ID: <1101313376.18939.25.camel@horizons.localdomain>

On Wed, 2004-11-24 at 16:34 +0100, Witold Eryk Wolski wrote:
> Hi,
> 
> I want to draw a scatter plot with 1M  and more points and save it as pdf.
> This makes the pdf file large.
> So i tried to save the file first as png and than convert it to pdf. 
> This looks OK if printed but if viewed e.g. with acrobat as document 
> figure the quality is bad.
> 
> Anyone knows a way to reduce the size but keep the quality?

Hi Eryk!

Part of the problem is that in a pdf file, the vector based instructions
will need to be defined for each of your 10 ^ 6 points in order to draw
them.

When trying to create a simple example:

pdf()
plot(rnorm(1000000), rnorm(1000000))
dev.off()

The pdf file is 55 Mb in size.

One immediate thought was to try a ps file and using the above plot, the
ps file was "only" 23 Mb in size. So note that ps can be more efficient.

Going to a bitmap might result in a much smaller file, but as you note,
the quality does degrade as compared to a vector based image.

I tried the above to a png, then converted to a pdf (using 'convert')
and as expected, the image both viewed and printed was "pixelated",
since the pdf instructions are presumably drawing pixels and not vector
based objects.

Depending upon what you plan to do with the image, you may have to
choose among several options, resulting in tradeoffs between image
quality and file size.

If you can create the bitmap file explicitly in the size that you
require for printing or incorporating in a document, that is one way to
go and will preserve, to an extent, the overall fixed size image
quality, while keeping file size small.

Another option to consider for the pdf approach, if it does not
compromise the integrity of your plot, is to remove any duplicate data
points if any exist. Thus, you will not need what are in effect
redundant instructions in the pdf file. This may not be possible
depending upon the nature of your data (ie. doubles) without considering
some tolerance level for "equivalence".

Perhaps others will have additional ideas.

HTH,

Marc Schwartz



From Ted.Harding at nessie.mcc.ac.uk  Wed Nov 24 17:16:28 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 24 Nov 2004 16:16:28 -0000 (GMT)
Subject: [R] scatterplot of 100000 points and pdf file format
In-Reply-To: <41A4AA16.7070008@molgen.mpg.de>
Message-ID: <XFMail.041124161628.Ted.Harding@nessie.mcc.ac.uk>

On 24-Nov-04 Witold Eryk Wolski wrote:
> Hi,
> I want to draw a scatter plot with 1M  and more points
> and save it as pdf.
> This makes the pdf file large.
> So i tried to save the file first as png and than convert
> it to pdf. This looks OK if printed but if viewed e.g. with
> acrobat as document figure the quality is bad.
> 
> Anyone knows a way to reduce the size but keep the quality?

If you want the PDF file to preserve the info about all the
1M points then the problem has no solution. The png file
will already have suppressed most of this (which is one
reason for poor quality).

I think you should give thought to reducing what you need
to plot.

Think about it: suppose you plot with a resolution of
1/200 points per inch (about the limit at which the eye
begins to see rough edges). Then you have 40000 points
per square inch. If your 1M points are separate but as
closely packed as possible, this requires 25 square inches,
or a 5x5 inch (= 12.7x12.7 cm) square. And this would be
solid black!

Presumably in your plot there is a very large number of
points which are effectively indistinguisable from other
points, so these could be eliminated without spoiling
the plot.

I don't have an obviously best strategy for reducing what
you actually plot, but perhaps one line to think along
might be the following:

1. Multiply the data by some factor and then round the
   results to an integer (to avoid problems in step 2).
   Factor chosen so that the result of (4) below is
   satisfactory.

2. Eliminate duplicates in the result of (1).

3. Divide by the factor you used in (1).

4. Plot the result; save plot to PDF.

As to how to do it in R: the critical step is (2),
which with so many points could be very heavy unless
done by a well-chosen procedure. I'm not expert enough
to advise about that, but no doubt others are.

Good luck!
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 24-Nov-04                                       Time: 16:16:28
------------------------------ XFMail ------------------------------



From kjetil at acelerate.com  Wed Nov 24 17:34:03 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Wed, 24 Nov 2004 12:34:03 -0400
Subject: [R] OOT: frailty-multinivel
Message-ID: <41A4B7FB.8070503@acelerate.com>

Hola!

I started to search for information about multilevel survival models, and
found frailty in R. This seems to be something of the same, is it the same?

Then: why the name frailty (weekness?)

-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From andy_liaw at merck.com  Wed Nov 24 17:37:29 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 24 Nov 2004 11:37:29 -0500
Subject: [R] scatterplot of 100000 points and pdf file format
Message-ID: <3A822319EB35174CA3714066D590DCD50994E381@usrymx25.merck.com>

Marc/Eryk,

I have no experience with it, but I believe the hexbin package in BioC was
there for this purpose: avoid heavy over-plotting lots of points.  You might
want to look into that, if you have not done so yet.

Best,
Andy

> From: Marc Schwartz
> 
> On Wed, 2004-11-24 at 16:34 +0100, Witold Eryk Wolski wrote:
> > Hi,
> > 
> > I want to draw a scatter plot with 1M  and more points and 
> save it as pdf.
> > This makes the pdf file large.
> > So i tried to save the file first as png and than convert 
> it to pdf. 
> > This looks OK if printed but if viewed e.g. with acrobat as 
> document 
> > figure the quality is bad.
> > 
> > Anyone knows a way to reduce the size but keep the quality?
> 
> Hi Eryk!
> 
> Part of the problem is that in a pdf file, the vector based 
> instructions
> will need to be defined for each of your 10 ^ 6 points in 
> order to draw
> them.
> 
> When trying to create a simple example:
> 
> pdf()
> plot(rnorm(1000000), rnorm(1000000))
> dev.off()
> 
> The pdf file is 55 Mb in size.
> 
> One immediate thought was to try a ps file and using the 
> above plot, the
> ps file was "only" 23 Mb in size. So note that ps can be more 
> efficient.
> 
> Going to a bitmap might result in a much smaller file, but as 
> you note,
> the quality does degrade as compared to a vector based image.
> 
> I tried the above to a png, then converted to a pdf (using 'convert')
> and as expected, the image both viewed and printed was "pixelated",
> since the pdf instructions are presumably drawing pixels and 
> not vector
> based objects.
> 
> Depending upon what you plan to do with the image, you may have to
> choose among several options, resulting in tradeoffs between image
> quality and file size.
> 
> If you can create the bitmap file explicitly in the size that you
> require for printing or incorporating in a document, that is 
> one way to
> go and will preserve, to an extent, the overall fixed size image
> quality, while keeping file size small.
> 
> Another option to consider for the pdf approach, if it does not
> compromise the integrity of your plot, is to remove any duplicate data
> points if any exist. Thus, you will not need what are in effect
> redundant instructions in the pdf file. This may not be possible
> depending upon the nature of your data (ie. doubles) without 
> considering
> some tolerance level for "equivalence".
> 
> Perhaps others will have additional ideas.
> 
> HTH,
> 
> Marc Schwartz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From wolski at molgen.mpg.de  Wed Nov 24 17:43:31 2004
From: wolski at molgen.mpg.de (Witold Eryk Wolski)
Date: Wed, 24 Nov 2004 17:43:31 +0100
Subject: [R] scatterplot of 100000 points and pdf file format
In-Reply-To: <1101313376.18939.25.camel@horizons.localdomain>
References: <41A4AA16.7070008@molgen.mpg.de>
	<1101313376.18939.25.camel@horizons.localdomain>
Message-ID: <41A4BA33.9060004@molgen.mpg.de>

Hi,

I tried the ps idea. But I am using pdflatex.
You get a even larger size reduction if you convert the ps into a pdf 
using ps2pdf.
But unfortunately there is a quality loss.

I have found almost a working solution:
a) Save the scatterplot without axes and with par(mar=c(0,0,0,0)) as png .
b) convert it using any program to pnm
c) read the pnm file using pixmap
d) Add axes labels and lines afterwards with par(new=TRUE)

And this looks like I would like that it looks like. But unfortunately 
acroread and gv on window is crashing when I try to print the file.

png(file="pepslop.png",width=500,height=500)
par(mar=c(0,0,0,0))
X2<-rnorm(100000)
Y2<-X2*10+rnorm(100000)
plot(X2,Y2,pch=".",xlab="",ylab="",main="",axes=F)
dev.off()

pdf(file="pepslop.pdf",width=7,height=7)
par(mar=c(3.2,3.2,1,1))
x <- read.pnm("pepslop.pnm" )
plot(x)
par(new=TRUE)
par(mar=c(3.2,3.2,1,1))
plot(X2,Y2,pch=".",xlab="",ylab="",main="",type="n")
mtext(expression(m[nominal]),side=1,line=2)
mtext(expression(mod(m[monoisotopic],1)),side=2,line=2)
legend(1000,4,expression(paste(lambda[DB],"=",0.000495)),col=2,lty=1,lwd=1)
abline(test,col=2,lwd=2)
dev.off()


 

Marc Schwartz wrote:

>On Wed, 2004-11-24 at 16:34 +0100, Witold Eryk Wolski wrote:
>  
>
>>Hi,
>>
>>I want to draw a scatter plot with 1M  and more points and save it as pdf.
>>This makes the pdf file large.
>>So i tried to save the file first as png and than convert it to pdf. 
>>This looks OK if printed but if viewed e.g. with acrobat as document 
>>figure the quality is bad.
>>
>>Anyone knows a way to reduce the size but keep the quality?
>>    
>>
>
>Hi Eryk!
>
>Part of the problem is that in a pdf file, the vector based instructions
>will need to be defined for each of your 10 ^ 6 points in order to draw
>them.
>
>When trying to create a simple example:
>
>pdf()
>plot(rnorm(1000000), rnorm(1000000))
>dev.off()
>
>The pdf file is 55 Mb in size.
>
>One immediate thought was to try a ps file and using the above plot, the
>ps file was "only" 23 Mb in size. So note that ps can be more efficient.
>
>Going to a bitmap might result in a much smaller file, but as you note,
>the quality does degrade as compared to a vector based image.
>
>I tried the above to a png, then converted to a pdf (using 'convert')
>and as expected, the image both viewed and printed was "pixelated",
>since the pdf instructions are presumably drawing pixels and not vector
>based objects.
>
>Depending upon what you plan to do with the image, you may have to
>choose among several options, resulting in tradeoffs between image
>quality and file size.
>
>If you can create the bitmap file explicitly in the size that you
>require for printing or incorporating in a document, that is one way to
>go and will preserve, to an extent, the overall fixed size image
>quality, while keeping file size small.
>
>Another option to consider for the pdf approach, if it does not
>compromise the integrity of your plot, is to remove any duplicate data
>points if any exist. Thus, you will not need what are in effect
>redundant instructions in the pdf file. This may not be possible
>depending upon the nature of your data (ie. doubles) without considering
>some tolerance level for "equivalence".
>
>Perhaps others will have additional ideas.
>
>HTH,
>
>Marc Schwartz
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>


-- 
Dipl. bio-chem. Witold Eryk Wolski
MPI-Moleculare Genetic
Ihnestrasse 63-73 14195 Berlin
tel: 0049-30-83875219                 __("<    _
http://www.molgen.mpg.de/~wolski      \__/    'v'
http://r4proteomics.sourceforge.net    ||    /   \
mail: witek96 at users.sourceforge.net    ^^     m m
      wolski at molgen.mpg.de



From tlumley at u.washington.edu  Wed Nov 24 17:48:00 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 24 Nov 2004 08:48:00 -0800 (PST)
Subject: [R] scatterplot of 100000 points and pdf file format
In-Reply-To: <41A4AA16.7070008@molgen.mpg.de>
References: <41A4AA16.7070008@molgen.mpg.de>
Message-ID: <Pine.A41.4.61b.0411240846380.204476@homer11.u.washington.edu>

On Wed, 24 Nov 2004, Witold Eryk Wolski wrote:

> Hi,
>
> I want to draw a scatter plot with 1M  and more points and save it as pdf.

Try the "hexbin" Bioconductor package, which gives hexagonally-binned 
density scatterplots. Even for tens of thousands of points this is often 
much better than a scatterplot.

 	-thomas



From wolski at molgen.mpg.de  Wed Nov 24 17:49:44 2004
From: wolski at molgen.mpg.de (Witold Eryk Wolski)
Date: Wed, 24 Nov 2004 17:49:44 +0100
Subject: [R] scatterplot of 100000 points and pdf file format
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E381@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E381@usrymx25.merck.com>
Message-ID: <41A4BBA8.5030504@molgen.mpg.de>

Hi,
Yes, indeed the hexbin package generates very cool pix. They look great. 
I was using it already.
But this time I am interested in visualizing exactly the _scatter_ of 
some extreme points.

Eryk

Liaw, Andy wrote:

>Marc/Eryk,
>
>I have no experience with it, but I believe the hexbin package in BioC was
>there for this purpose: avoid heavy over-plotting lots of points.  You might
>want to look into that, if you have not done so yet.
>
>Best,
>Andy
>
>  
>
>>From: Marc Schwartz
>>
>>On Wed, 2004-11-24 at 16:34 +0100, Witold Eryk Wolski wrote:
>>    
>>
>>>Hi,
>>>
>>>I want to draw a scatter plot with 1M  and more points and 
>>>      
>>>
>>save it as pdf.
>>    
>>
>>>This makes the pdf file large.
>>>So i tried to save the file first as png and than convert 
>>>      
>>>
>>it to pdf. 
>>    
>>
>>>This looks OK if printed but if viewed e.g. with acrobat as 
>>>      
>>>
>>document 
>>    
>>
>>>figure the quality is bad.
>>>
>>>Anyone knows a way to reduce the size but keep the quality?
>>>      
>>>
>>Hi Eryk!
>>
>>Part of the problem is that in a pdf file, the vector based 
>>instructions
>>will need to be defined for each of your 10 ^ 6 points in 
>>order to draw
>>them.
>>
>>When trying to create a simple example:
>>
>>pdf()
>>plot(rnorm(1000000), rnorm(1000000))
>>dev.off()
>>
>>The pdf file is 55 Mb in size.
>>
>>One immediate thought was to try a ps file and using the 
>>above plot, the
>>ps file was "only" 23 Mb in size. So note that ps can be more 
>>efficient.
>>
>>Going to a bitmap might result in a much smaller file, but as 
>>you note,
>>the quality does degrade as compared to a vector based image.
>>
>>I tried the above to a png, then converted to a pdf (using 'convert')
>>and as expected, the image both viewed and printed was "pixelated",
>>since the pdf instructions are presumably drawing pixels and 
>>not vector
>>based objects.
>>
>>Depending upon what you plan to do with the image, you may have to
>>choose among several options, resulting in tradeoffs between image
>>quality and file size.
>>
>>If you can create the bitmap file explicitly in the size that you
>>require for printing or incorporating in a document, that is 
>>one way to
>>go and will preserve, to an extent, the overall fixed size image
>>quality, while keeping file size small.
>>
>>Another option to consider for the pdf approach, if it does not
>>compromise the integrity of your plot, is to remove any duplicate data
>>points if any exist. Thus, you will not need what are in effect
>>redundant instructions in the pdf file. This may not be possible
>>depending upon the nature of your data (ie. doubles) without 
>>considering
>>some tolerance level for "equivalence".
>>
>>Perhaps others will have additional ideas.
>>
>>HTH,
>>
>>Marc Schwartz
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>
>>    
>>
>
>
>------------------------------------------------------------------------------
>Notice:  This e-mail message, together with any attachments, contains information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New Jersey, USA 08889), and/or its affiliates (which may be known outside the United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as Banyu) that may be confidential, proprietary copyrighted and/or legally privileged. It is intended solely for the use of the individual or entity named on this message.  If you are not the intended recipient, and have received this message in error, please notify us immediately by reply e-mail and then delete it from your system.
>------------------------------------------------------------------------------
>
>  
>


-- 
Dipl. bio-chem. Witold Eryk Wolski
MPI-Moleculare Genetic
Ihnestrasse 63-73 14195 Berlin
tel: 0049-30-83875219                 __("<    _
http://www.molgen.mpg.de/~wolski      \__/    'v'
http://r4proteomics.sourceforge.net    ||    /   \
mail: witek96 at users.sourceforge.net    ^^     m m
      wolski at molgen.mpg.de



From ripley at stats.ox.ac.uk  Wed Nov 24 17:50:13 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 24 Nov 2004 16:50:13 +0000 (GMT)
Subject: [R] scatterplot of 100000 points and pdf file format
In-Reply-To: <XFMail.041124161628.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.041124161628.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.LNX.4.61.0411241646080.2681@gannet.stats>

On Wed, 24 Nov 2004 Ted.Harding at nessie.mcc.ac.uk wrote:

> On 24-Nov-04 Witold Eryk Wolski wrote:
>> Hi,
>> I want to draw a scatter plot with 1M  and more points
>> and save it as pdf.
>> This makes the pdf file large.
>> So i tried to save the file first as png and than convert
>> it to pdf. This looks OK if printed but if viewed e.g. with
>> acrobat as document figure the quality is bad.
>>
>> Anyone knows a way to reduce the size but keep the quality?
>
> If you want the PDF file to preserve the info about all the
> 1M points then the problem has no solution. The png file
> will already have suppressed most of this (which is one
> reason for poor quality).
>
> I think you should give thought to reducing what you need
> to plot.
>
> Think about it: suppose you plot with a resolution of
> 1/200 points per inch (about the limit at which the eye
> begins to see rough edges). Then you have 40000 points
> per square inch. If your 1M points are separate but as
> closely packed as possible, this requires 25 square inches,
> or a 5x5 inch (= 12.7x12.7 cm) square. And this would be
> solid black!
>
> Presumably in your plot there is a very large number of
> points which are effectively indistinguisable from other
> points, so these could be eliminated without spoiling
> the plot.
>
> I don't have an obviously best strategy for reducing what
> you actually plot, but perhaps one line to think along
> might be the following:
>
> 1. Multiply the data by some factor and then round the
>   results to an integer (to avoid problems in step 2).
>   Factor chosen so that the result of (4) below is
>   satisfactory.
>
> 2. Eliminate duplicates in the result of (1).
>
> 3. Divide by the factor you used in (1).
>
> 4. Plot the result; save plot to PDF.
>
> As to how to do it in R: the critical step is (2),
> which with so many points could be very heavy unless
> done by a well-chosen procedure. I'm not expert enough
> to advise about that, but no doubt others are.

unique will eat that for breakfast

> x <- runif(1e6)
> system.time(xx <- unique(round(x, 4)))
[1] 0.55 0.09 0.64 0.00 0.00
> length(xx)
[1] 10001


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From MNelson at sequenom.com  Wed Nov 24 17:53:01 2004
From: MNelson at sequenom.com (Matt Nelson)
Date: Wed, 24 Nov 2004 08:53:01 -0800
Subject: [R] scatterplot of 100000 points and pdf file format
Message-ID: <9F171A0DEB645643BD848C4CF0FB3684052C19F0@dna.sequenom.com>

Witold,

I have found that plotting more than a few thousand data points at a time
quickly becomes a loosing proposition.  That is, the dense overlap of data
points tends to obscure the patterns of interest, with only outliers
distinctly visible.  I typically deal with this in two ways.  

The most straight forward is to select a much smaller subset data points to
plot, say on the order of 100-1000, depending on the nature of the data and
the features you want to illustrate.  How you sample depends on the
structure of your data set.  E.g. you may want to sample fixed proportions
within subgroups.  You can add loess lines or confidence ellipses estimated
from the complete data.

Another approach is to estimate the two dimensional density using kde2d()
(MASS package) and represent the result with a contour or image plot.  See
?kde2d for an example.  

Both of these will result in much more manageable (and likely more
informative) figures.

Regards,
Matt

Matthew R. Nelson, Ph.D.
Director, Biostatistics
Sequenom, Inc.


> -----Original Message-----
> From: Witold Eryk Wolski [mailto:wolski at molgen.mpg.de]
> Sent: Wednesday, November 24, 2004 7:35 AM
> To: R Help Mailing List
> Subject: [R] scatterplot of 100000 points and pdf file format
> 
> 
> Hi,
> 
> I want to draw a scatter plot with 1M  and more points and 
> save it as pdf.
> This makes the pdf file large.
> So i tried to save the file first as png and than convert it to pdf. 
> This looks OK if printed but if viewed e.g. with acrobat as document 
> figure the quality is bad.
> 
> Anyone knows a way to reduce the size but keep the quality?
> 
> 
> /E
> 
> -- 
> Dipl. bio-chem. Witold Eryk Wolski
> MPI-Moleculare Genetic
> Ihnestrasse 63-73 14195 Berlin
> tel: 0049-30-83875219                 __("<    _
> http://www.molgen.mpg.de/~wolski      \__/    'v'
> http://r4proteomics.sourceforge.net    ||    /   \
> mail: witek96 at users.sourceforge.net    ^^     m m
>       wolski at molgen.mpg.de
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From jeff.hamann at forestinformatics.com  Wed Nov 24 17:55:39 2004
From: jeff.hamann at forestinformatics.com (Jeff D. Hamann)
Date: Wed, 24 Nov 2004 08:55:39 -0800 (PST)
Subject: [R] coplot =? gannt chart + bargraph
Message-ID: <1057.128.193.136.240.1101315339.squirrel@www.forestinformatics.com>

I would like to display some results from simulations in the form of a
Gantt chart (progress) with a barchart (production) of another variable
below (something very similar to coplot charts). I'm not sure if I should
attempt to build this from scratch (using grid or some of the basic
graphics features) or if there's a similar feature in one of the existing
packages.

I need to take the following (truncated) results,

unit,week,machine,volume,pdxratio
0,14,1,925.402525,1.000000
0,15,1,925.402525,1.000000
0,16,1,925.402525,1.000000
0,17,1,702.792425,0.759445
1,46,1,1007.664896,1.000000
1,47,1,1007.664896,1.000000
1,48,1,1007.664896,1.000000
1,49,1,563.005311,0.558723
2,33,1,1019.781108,1.000000
2,34,1,1019.781108,1.000000
2,35,1,1019.781108,1.000000
2,36,1,697.656677,0.684124
3,41,2,1043.451341,1.000000
3,42,2,1043.451341,1.000000
3,43,2,1043.451341,1.000000
3,44,2,741.645977,0.710762
4,7,2,1048.494508,1.000000
4,8,2,1048.494508,1.000000


and generate charts over unit and week (both as factors?) and think I
should be using aggregate, but wanted to find out if there's a better
method.

Thanks,
Jeff.


-- 
Jeff D. Hamann
Forest Informatics, Inc.
PO Box 1421
Corvallis, Oregon 97339-1421
phone 541-754-1428
fax 541-752-0288
jeff.hamann at forestinformatics.com
http://www.forestinformatics.com



From tlumley at u.washington.edu  Wed Nov 24 18:00:11 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 24 Nov 2004 09:00:11 -0800 (PST)
Subject: [R] OOT: frailty-multinivel
In-Reply-To: <41A4B7FB.8070503@acelerate.com>
References: <41A4B7FB.8070503@acelerate.com>
Message-ID: <Pine.A41.4.61b.0411240848130.204476@homer11.u.washington.edu>

On Wed, 24 Nov 2004, Kjetil Brinchmann Halvorsen wrote:

> Hola!
>
> I started to search for information about multilevel survival models, and
> found frailty in R. This seems to be something of the same, is it the same?

More or less. "Shared frailty" models are the same as hierarchical/mixed 
survival models.  R uses a fitting method that is equivalent to maximum 
likelihood only when exp(random effects) has a Gamma distribution.  The 
"survival" package can fit random intercept models; the new "kinship" 
package fits much more general mixed models.

[I will put in my usual objection to the term "multilevel model"
being used to refer solely to models that include unmeasured 
variables]

>
> Then: why the name frailty (weekness?)
>

The idea is that 'weaker' individuals fail earlier than `stronger' 
individuals for the same values of covariates.  The concept has been used 
both for modelling correlation between survival times and to motivate 
parametric models that give an initially decreasing hazard.  I have a 
vague impression that the term originated in Scandinavia somewhere.


 	-thomas



From hathanassiou at automatedcell.com  Wed Nov 24 18:01:47 2004
From: hathanassiou at automatedcell.com (Harry Athanassiou)
Date: Wed, 24 Nov 2004 12:01:47 -0500
Subject: [R] what does order() stand for in an lme formula?
Message-ID: <004701c4d247$4899b380$6601a8c0@SALAMINIA>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041124/82afbd90/attachment.pl

From jwdougherty at mcihispeed.net  Wed Nov 24 18:37:05 2004
From: jwdougherty at mcihispeed.net (John)
Date: Wed, 24 Nov 2004 09:37:05 -0800
Subject: [R] scatterplot of 100000 points and pdf file format
In-Reply-To: <41A4AA16.7070008@molgen.mpg.de>
References: <41A4AA16.7070008@molgen.mpg.de>
Message-ID: <200411240937.05820.jwdougherty@mcihispeed.net>

On Wednesday 24 November 2004 07:34, Witold Eryk Wolski wrote:
> Hi,
>
> I want to draw a scatter plot with 1M  and more points and save it as pdf.
> This makes the pdf file large.
> So i tried to save the file first as png and than convert it to pdf.
> This looks OK if printed but if viewed e.g. with acrobat as document
> figure the quality is bad.
>
> Anyone knows a way to reduce the size but keep the quality?
>
I would strongly suggest a different method to present the data such as a 
contour plot or 3D bar plot.  An XY plot with a million points is unlikely to 
be readable unless it is produced as a large format print.  At 200 DPI 
printed, 1,000,000 discrete points requires a minimum of a 5 inch (12.7          
cm) by 5 inch area.  Besides, other than being visually overwhelming, what 
information would such a plot offer a viewer?

John



From sdavis2 at mail.nih.gov  Wed Nov 24 18:44:05 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Wed, 24 Nov 2004 12:44:05 -0500
Subject: [R] scatterplot of 100000 points and pdf file format
In-Reply-To: <9F171A0DEB645643BD848C4CF0FB3684052C19F0@dna.sequenom.com>
References: <9F171A0DEB645643BD848C4CF0FB3684052C19F0@dna.sequenom.com>
Message-ID: <6F8483B9-3E40-11D9-9A35-000D933565E8@mail.nih.gov>

Do you have a measures of "scatter" or can you pick "outliers" that 
could allow you to produce a "mixed" plot using either density or 
hexbinned data with only outliers placed after-the-fact using points()?

Sean

>> -----Original Message-----
>> From: Witold Eryk Wolski [mailto:wolski at molgen.mpg.de]
>> Sent: Wednesday, November 24, 2004 7:35 AM
>> To: R Help Mailing List
>> Subject: [R] scatterplot of 100000 points and pdf file format
>>
>>
>> Hi,
>>
>> I want to draw a scatter plot with 1M  and more points and
>> save it as pdf.
>> This makes the pdf file large.
>> So i tried to save the file first as png and than convert it to pdf.
>> This looks OK if printed but if viewed e.g. with acrobat as document
>> figure the quality is bad.
>>
>> Anyone knows a way to reduce the size but keep the quality?



From james.holtman at convergys.com  Wed Nov 24 18:09:57 2004
From: james.holtman at convergys.com (james.holtman@convergys.com)
Date: Wed, 24 Nov 2004 12:09:57 -0500
Subject: [R] scatterplot of 100000 points and pdf file format
Message-ID: <OF1993D56D.63ACB475-ON85256F56.005E01E1@nd.convergys.com>





Have you tried

plot(...,pch='.')

This will use the period as the plotting character instead of the 'circle'
which is drawn.  This should reduce the size of the PDF file.

I have done scatter plots with 2M points and they are typically meaningless
with that many points overlaid.  Check out 'hexbin' on Bioconductor (you
can download the package from the RGUI window.  This is a much better way
of showing some information since it will plot the number of points that
are within a hexagon.  I have found this to be a better way of looking at
some data.
__________________________________________________________
James Holtman        "What is the problem you are trying to solve?"
Executive Technical Consultant  --  Office of Technology, Convergys
james.holtman at convergys.com
+1 (513) 723-2929


                                                                                                                                           
                      Witold Eryk Wolski                                                                                                   
                      <wolski at molgen.mpg.de        To:       R Help Mailing List <r-help at stat.math.ethz.ch>                                
                      >                            cc:                                                                                     
                      Sent by:                     Subject:  [R] scatterplot of 100000 points and pdf file format                          
                      r-help-bounces at stat.m                                                                                                
                      ath.ethz.ch                                                                                                          
                                                                                                                                           
                                                                                                                                           
                      11/24/2004 10:34                                                                                                     
                                                                                                                                           
                                                                                                                                           




Hi,

I want to draw a scatter plot with 1M  and more points and save it as pdf.
This makes the pdf file large.
So i tried to save the file first as png and than convert it to pdf.
This looks OK if printed but if viewed e.g. with acrobat as document
figure the quality is bad.

Anyone knows a way to reduce the size but keep the quality?


/E

--
Dipl. bio-chem. Witold Eryk Wolski
MPI-Moleculare Genetic
Ihnestrasse 63-73 14195 Berlin
tel: 0049-30-83875219                 __("<    _
http://www.molgen.mpg.de/~wolski      \__/    'v'
http://r4proteomics.sourceforge.net    ||    /   \
mail: witek96 at users.sourceforge.net    ^^     m m
      wolski at molgen.mpg.de

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From MSchwartz at MedAnalytics.com  Wed Nov 24 18:46:56 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Wed, 24 Nov 2004 11:46:56 -0600
Subject: [R] scatterplot of 100000 points and pdf file format
In-Reply-To: <41A4BA33.9060004@molgen.mpg.de>
References: <41A4AA16.7070008@molgen.mpg.de>
	<1101313376.18939.25.camel@horizons.localdomain>
	<41A4BA33.9060004@molgen.mpg.de>
Message-ID: <1101318416.18939.40.camel@horizons.localdomain>

On Wed, 2004-11-24 at 17:43 +0100, Witold Eryk Wolski wrote:
> Hi,
> 
> I tried the ps idea. But I am using pdflatex.
> You get a even larger size reduction if you convert the ps into a pdf 
> using ps2pdf.
> But unfortunately there is a quality loss.
> 
> I have found almost a working solution:
> a) Save the scatterplot without axes and with par(mar=c(0,0,0,0)) as png .
> b) convert it using any program to pnm
> c) read the pnm file using pixmap
> d) Add axes labels and lines afterwards with par(new=TRUE)
> 
> And this looks like I would like that it looks like. But unfortunately 
> acroread and gv on window is crashing when I try to print the file.
> 
> png(file="pepslop.png",width=500,height=500)
> par(mar=c(0,0,0,0))
> X2<-rnorm(100000)
> Y2<-X2*10+rnorm(100000)
> plot(X2,Y2,pch=".",xlab="",ylab="",main="",axes=F)
> dev.off()
> 
> pdf(file="pepslop.pdf",width=7,height=7)
> par(mar=c(3.2,3.2,1,1))
> x <- read.pnm("pepslop.pnm" )
> plot(x)
> par(new=TRUE)
> par(mar=c(3.2,3.2,1,1))
> plot(X2,Y2,pch=".",xlab="",ylab="",main="",type="n")
> mtext(expression(m[nominal]),side=1,line=2)
> mtext(expression(mod(m[monoisotopic],1)),side=2,line=2)
> legend(1000,4,expression(paste(lambda[DB],"=",0.000495)),col=2,lty=1,lwd=1)
> abline(test,col=2,lwd=2)
> dev.off()

Eryk,

I tried this approach and was able to print without problem here under
FC3, using acroread.

Also, I think that you left out:

test <- lm(Y2 ~ X2)

in the code above, lest abline() will fail. Also I changed the x,y
coordinates of the legend, since (1000, 4) is outside the plot range for
the points that I generated here.

Interesting approach, it reduced the pdf file size to about 7 Mb.

BTW, any chance that there is a huge black hole in the center of
that...  ;-)

Best,

Marc



From sfan_2004 at hotmail.com  Wed Nov 24 19:15:59 2004
From: sfan_2004 at hotmail.com (Jessie F)
Date: Wed, 24 Nov 2004 10:15:59 -0800
Subject: [R] Re: T-test syntax question
In-Reply-To: <20041124113206.95023.qmail@web41202.mail.yahoo.com>
Message-ID: <BAY22-F11488599DDE55D81722B1F8CB80@phx.gbl>

Actually, you can still use t.test with one vector of data. Say, the 
differences is d ( a vector (or an array of numbers), you can use t.test(d), 
then by default, it testing whether mu=0, you can also specify confidence 
level by adding conf.level = 0.95 etc.

You can also type ?t.test in R command to get more information with R.help.

Hope this helps!

S Fan


>From: Vito Ricci <vito_ricci at yahoo.com>
>To: r-help at stat.math.ethz.ch
>CC: steven.f.freeman at verizon.net
>Subject: [R] Re: T-test syntax question
>Date: Wed, 24 Nov 2004 12:32:06 +0100 (CET)
>
>Hi,
>
>In case of paired data, if you have only differencies
>and not original data you can get this t test based on
>differencies:
>
>Say d is the vector with differencies data and suppose
>you wish to test if the mean of differency is equal to
>zero:
>
>md<-mean(d) ## sample mean of differencies
>sdd<-sd(d) ## sample sd of differencies
>n<-length(d) ## sample size
>t.value<-(md/(sdd/sqrt(n))) ## sample t-value with n-1
>df
>pt(t.value,n-1,lower.tail=FALSE) ## p-value of test
>
> > set.seed(13)
> > d<-rnorm(50)
> > md<-mean(d) ## sample mean of differencies
> > sdd<-sd(d) ## sample sd of differencies
> > n<-length(d) ## sample size
> > t.value<-(md/(sdd/sqrt(n))) ## sample t-value with
>n-1 df
> > pt(t.value,n-1,lower.tail=FALSE) ## p-value of test
>[1] 0.5755711
>
>Best regards,
>Vito
>
>
>
>Steven F. Freeman wrote:
>
>I'd like to do a t-test to compare the Delta values of
>items with Crit=1
>with Delta values of items with Crit=0. What is the
>t.test syntax?
>
>It should produce a result like this below (I can't
>get in touch with the
>person who originally did this for me)
>
>     Welch Two Sample t-test
>
>data:  t1$Delta by Crit
>t = -3.4105, df = 8.674, p-value = 0.008173
>alternative hypothesis: true
>difference in means is not equal to 0
>95 percent confidence interval:
>  -0.04506155 -0.00899827
>sample estimates:
>mean in group FALSE  mean in group TRUE
>          0.03331391          0.06034382
>
>Thanks.
>
>=====
>Diventare costruttori di soluzioni
>Became solutions' constructors
>
>"The business of the statistician is to catalyze
>the scientific learning process."
>George E. P. Box
>
>
>Visitate il portale http://www.modugno.it/
>e in particolare la sezione su Palese  
>http://www.modugno.it/archivio/palese/
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html


From B.Rowlingson at lancaster.ac.uk  Wed Nov 24 19:19:38 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Wed, 24 Nov 2004 18:19:38 +0000
Subject: [R] scatterplot of 100000 points and pdf file format
In-Reply-To: <200411240937.05820.jwdougherty@mcihispeed.net>
References: <41A4AA16.7070008@molgen.mpg.de>
	<200411240937.05820.jwdougherty@mcihispeed.net>
Message-ID: <41A4D0BA.9060507@lancaster.ac.uk>


> 
> I would strongly suggest a different method to present the data such as a 
> contour plot or 3D bar plot.  An XY plot with a million points is unlikely to 
> be readable unless it is produced as a large format print.  At 200 DPI 
> printed, 1,000,000 discrete points requires a minimum of a 5 inch (12.7          
> cm) by 5 inch area.  Besides, other than being visually overwhelming, what 
> information would such a plot offer a viewer?

  I recall some of our extreme value statistics people printing things 
like this. Several million points on a plot. Most of which were in a 
big, thick block of toner, and then a few hundred at the extremes which 
was where they where interested in looking.

  Of course these things took an hour to print on a PostScript printer 
at the time. I think I suggested only plotting points for which X > 
someThreshold. Saved on toner and time. Got a bit tricky in the 
bivariate case though, where you really needed to plot points outside 
some ellipse that you knew would otherwise be a big black blob, and then 
you filled that in with a black ellipse.

  Contours or aggregation wasn't any use, since they were interested in 
the point patterns of the extreme value data.

Baz



From abitbol at sent.com  Wed Nov 24 19:24:57 2004
From: abitbol at sent.com (Jean-Louis Abitbol)
Date: Wed, 24 Nov 2004 19:24:57 +0100
Subject: [R] reshaping of data for barplot2
Message-ID: <1101320697.25550.209392197@webmail.messagingengine.com>

Dear All,

I have  the following data coming out from 

s <- with(final,
           summarize(norm, llist(gtt,fdiab),
                     function(norm) {
                      n <- sum(!is.na(norm))
                      s <- sum(norm, na.rm=T)
                      binconf(s, n)
                     }, type='matrix')
)
ie 

     gtt fdiab   norm.norm  norm.norm2  norm.norm3
18    PL    No  3.70370370  0.18997516 18.28346593
19    PL   Yes  3.57142857  0.18319034 17.71219774
13    TT1   No  9.09090909  3.59221932 21.15923917
14    TT1  Yes  1.81818182  0.09326054  9.60577606
...
10  HIGH    No 26.53061224 16.21128213 40.26228897
11  HIGH   Yes 10.00000000  4.66428345 20.14946472

I would like to reshape the data so that I can barplot2 treatments (gtt)
with 2 beside bars for fdiab  yes/no and add CI.

Various attemps have been unsuccessful as I have not understood both the
logic of beside and the nature of structures to be passed to barplot2.
Not enough know-how with reshape and transpose either.

Needless to say Dotplot works great with this kind of data but some
"Authority" requests side:side bars with CI.

Thanks for any help.

JL



From hjb at pdq.com  Wed Nov 24 18:26:46 2004
From: hjb at pdq.com (Heather J. Branton)
Date: Wed, 24 Nov 2004 12:26:46 -0500
Subject: [R] Searching for antilog function
Message-ID: <41A4C456.308@pdq.com>

Dear R-users,

I have a basic question about how to determine the antilog of a variable.

Say I have some number, x, which is a factor of 2 such that x = 2^y. I 
want to figure out what y is, i.e. I am looking for the antilog base 2 of x.

I have found log2 in the Reference Manual. But I am struggling how to 
get the antilog of that.

Any help will be appreciated!

 > version

platform i386-pc-mingw32
arch     i386          
os       mingw32       
system   i386, mingw32 
status                 
major    1             
minor    9.1           
year     2004          
month    06            
day      21            
language R

...heather



From p.dalgaard at biostat.ku.dk  Wed Nov 24 19:35:45 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 24 Nov 2004 19:35:45 +0100
Subject: [R] what does order() stand for in an lme formula?
In-Reply-To: <004701c4d247$4899b380$6601a8c0@SALAMINIA>
References: <004701c4d247$4899b380$6601a8c0@SALAMINIA>
Message-ID: <x2u0rf3xj2.fsf@biostat.ku.dk>

"Harry Athanassiou" <hathanassiou at automatedcell.com> writes:

> I'm a beginner in R, and trying to fit linear models with different
> intercepts per group, of the type y ~ A*x1 + B, where x1 is a numerical
> variable. I cannot understand whether I should use
>             y1 ~ x1 +1
> or
>             y1 ~ order(x1) + 1
> Although in the toy example included it makes a small difference, in models
> with many groups the models without order() converge slower if at all!

Er?

What gave you the idea of using order in the first place? To the best
of my knowledge, order(x) is also in this context just a function,
which for the nth observation returns the position of the nth largest
observation in x. This is not likely to make sense as a predictor in a
model. 


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From br44114 at yahoo.com  Wed Nov 24 19:57:38 2004
From: br44114 at yahoo.com (bogdan romocea)
Date: Wed, 24 Nov 2004 10:57:38 -0800 (PST)
Subject: [R] SAS or R software
Message-ID: <20041124185738.1141.qmail@web50303.mail.yahoo.com>

neela v writes:
> Hi all there
>  
> Can some one clarify me on this issue, features wise which is
better R or SAS, leaving the commerical aspect associated with it. I
suppose there are few people who have worked on both R and SAS and
wish they would be able to help me in deciding on this.
>  
> THank you for the help
> 


I very much doubt you can make an informed decision if you leave the
commercial aspect (license) aside. A single Base SAS installation
(server) can cost tens of thousands of [[your currency here; may need
to multiply by 10 or 100 or more]] in the first year, then a
percentage of that in the following years. (SAS software is not
purchased, but licensed on a yearly basis.) Want more than Base SAS?
Prepare your wallet: thousands upon thousands (per year) for
regression, anova, clustering (SAS/Stat), graphics (SAS/Graph), time
series (SAS/ETS), optimizations (SAS/OR) etc. Then, if you want
decision trees and neural networks (Enterprise Miner), I warmly
recommend you to quickly find a chair and sit down before you hear
the price tag. 

Will you always work for an organization that licenses SAS software?
Will the organization license all the modules you'll need? Will those
modules do everything you want? As others have said, R is a lot more
flexible, and the GPL ensures that whatever you can do today will
continue to be expanded and improved (much faster than SAS Institute
would want or be able to expand/improve SAS). 

All in all, if you're primarily interested in data analysis (and
don't want, for example, to get a job as a SAS programmer) and still
choose SAS, you will regret it one day. The benefits are few (such as
robust manipulation of massive data sets - I mean in excess of
hundreds of millions of rows) and the risks are high (whatever you do
is dependent on proprietary, very expensive software). With R, almost
the opposite is true: lots of benefits and no risks (nothing can take
R away from you).

HTH,
b.
		
__________________________________ 





From adslvdll at tpg.com.au  Wed Nov 24 20:10:12 2004
From: adslvdll at tpg.com.au (stephenc)
Date: Thu, 25 Nov 2004 06:10:12 +1100
Subject: [R] SMVs
Message-ID: <001c01c4d259$3c6c9d90$0d01a8c0@tablet>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041125/5576b732/attachment.pl

From andy_liaw at merck.com  Wed Nov 24 20:14:25 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 24 Nov 2004 14:14:25 -0500
Subject: [R] Searching for antilog function
Message-ID: <3A822319EB35174CA3714066D590DCD50994E386@usrymx25.merck.com>

What's wrong with log2()?

> log2(16)
[1] 4

Isn't that exactly what you asked for?

Andy

> From: Heather J. Branton
> 
> Dear R-users,
> 
> I have a basic question about how to determine the antilog of 
> a variable.
> 
> Say I have some number, x, which is a factor of 2 such that x 
> = 2^y. I 
> want to figure out what y is, i.e. I am looking for the 
> antilog base 2 of x.
> 
> I have found log2 in the Reference Manual. But I am struggling how to 
> get the antilog of that.
> 
> Any help will be appreciated!
> 
>  > version
> 
> platform i386-pc-mingw32
> arch     i386          
> os       mingw32       
> system   i386, mingw32 
> status                 
> major    1             
> minor    9.1           
> year     2004          
> month    06            
> day      21            
> language R
> 
> ...heather
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From spencer.graves at pdf.com  Wed Nov 24 20:17:55 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 24 Nov 2004 11:17:55 -0800
Subject: [R] Searching for antilog function
In-Reply-To: <41A4C456.308@pdq.com>
References: <41A4C456.308@pdq.com>
Message-ID: <41A4DE63.1050501@pdf.com>

      Consider: 

 > exp(log(1:11))
 [1]  1  2  3  4  5  6  7  8  9 10 11
 > 2^log(1:11, 2)
 [1]  1  2  3  4  5  6  7  8  9 10 11
 > 2^logb(1:11, 2)
 [1]  1  2  3  4  5  6  7  8  9 10 11
 > 10^log10(1:11)
 [1]  1  2  3  4  5  6  7  8  9 10 11
 > 2^log2(1:11)
 [1]  1  2  3  4  5  6  7  8  9 10 11

      Does this answer the question? 

      hope this helps. spencer graves

Heather J. Branton wrote:

> Dear R-users,
>
> I have a basic question about how to determine the antilog of a variable.
>
> Say I have some number, x, which is a factor of 2 such that x = 2^y. I 
> want to figure out what y is, i.e. I am looking for the antilog base 2 
> of x.
>
> I have found log2 in the Reference Manual. But I am struggling how to 
> get the antilog of that.
>
> Any help will be appreciated!
>
> > version
>
> platform i386-pc-mingw32
> arch     i386          os       mingw32       system   i386, mingw32 
> status                 major    1             minor    9.1           
> year     2004          month    06            day      21            
> language R
>
> ...heather
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html


-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From murdoch at stats.uwo.ca  Wed Nov 24 20:31:41 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 24 Nov 2004 14:31:41 -0500
Subject: [R] Searching for antilog function
In-Reply-To: <41A4C456.308@pdq.com>
References: <41A4C456.308@pdq.com>
Message-ID: <l9o9q0lm2j1fi046pb3irlo1vhfcv76v9l@4ax.com>

On Wed, 24 Nov 2004 12:26:46 -0500, "Heather J. Branton" <hjb at pdq.com>
wrote :

>Dear R-users,
>
>I have a basic question about how to determine the antilog of a variable.
>
>Say I have some number, x, which is a factor of 2 such that x = 2^y. I 
>want to figure out what y is, i.e. I am looking for the antilog base 2 of x.
>
>I have found log2 in the Reference Manual. But I am struggling how to 
>get the antilog of that.

You seem to be confusing log with antilog, but log2(x) and 2^y are
inverses of each other, i.e.

log2(2^y) equals y

and 

2^log2(x) equals x

(up to rounding error, of course).

Duncan Murdoch



From hjb at pdq.com  Wed Nov 24 20:18:53 2004
From: hjb at pdq.com (Heather J. Branton)
Date: Wed, 24 Nov 2004 14:18:53 -0500
Subject: [R] Searching for antilog function
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E386@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E386@usrymx25.merck.com>
Message-ID: <41A4DE9D.7040804@pdq.com>

Yes! Somehow I must have made an entry error when I tried that before as 
I was getting something completely different!

Thank you.

...heather

Liaw, Andy wrote:

>What's wrong with log2()?
>
>  
>
>>log2(16)
>>    
>>
>[1] 4
>
>Isn't that exactly what you asked for?
>
>Andy
>
>  
>
>>From: Heather J. Branton
>>
>>Dear R-users,
>>
>>I have a basic question about how to determine the antilog of 
>>a variable.
>>
>>Say I have some number, x, which is a factor of 2 such that x 
>>= 2^y. I 
>>want to figure out what y is, i.e. I am looking for the 
>>antilog base 2 of x.
>>
>>I have found log2 in the Reference Manual. But I am struggling how to 
>>get the antilog of that.
>>
>>Any help will be appreciated!
>>
>> > version
>>
>>platform i386-pc-mingw32
>>arch     i386          
>>os       mingw32       
>>system   i386, mingw32 
>>status                 
>>major    1             
>>minor    9.1           
>>year     2004          
>>month    06            
>>day      21            
>>language R
>>
>>...heather
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>
>>    
>>
>
>
>------------------------------------------------------------------------------
>Notice:  This e-mail message, together with any attachments, contains information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New Jersey, USA 08889), and/or its affiliates (which may be known outside the United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as Banyu) that may be confidential, proprietary copyrighted and/or legally privileged. It is intended solely for the use of the individual or entity named on this message.  If you are not the intended recipient, and have received this message in error, please notify us immediately by reply e-mail and then delete it from your system.
>------------------------------------------------------------------------------
>
>
>  
>

-- 
_______________________________________________________________________
Heather J. Branton                                  Public Data Queries
Data Specialist                                 310 Depot Street, Ste C
734.213.4964 x312                                  Ann Arbor, MI  48104

               U.S. Census Microdata At Your Fingertips
                          http://www.pdq.com



From abu3ammar at gmail.com  Wed Nov 24 21:29:53 2004
From: abu3ammar at gmail.com (Yasser El-Zein)
Date: Wed, 24 Nov 2004 15:29:53 -0500
Subject: [R] seriesMerge
Message-ID: <b1d31504041124122950a6efce@mail.gmail.com>

Is there a function in R that is equivalent to S-PLUS's 
seriesMerge(x1, x2, pos="union")
where x1, and x2 are of class timeSeries

seriesMerge is in S-PLUS's finmetrics. I looked into R's mergeSeries
(in fSeries part of Rmetrics) but I could not make it behave quite the
same. In R it expected a timeSeries object and a matrix of the same
row count. In S-PLUS when using the union option both objects can be
of different lengths.



From greg.snow at ihc.com  Wed Nov 24 21:32:22 2004
From: greg.snow at ihc.com (Greg Snow)
Date: Wed, 24 Nov 2004 13:32:22 -0700
Subject: [R] scatterplot of 100000 points and pdf file format
Message-ID: <s1a48d77.044@lp-msg1.co.ihc.com>

How about the following to plot only the 1,000 or so most extreem points
(the outliers):

x <- rnorm(1e6)
y <- 2*x+rnorm(1e6)

plot(x,y,pch='.')

tmp <- chull(x,y)

while( length(tmp) < 1000 ){
	tmp <- c(tmp, seq(along=x)[-tmp][ chull(x[-tmp],y[-tmp]) ] )
}

points(x[tmp],y[tmp], col='red')

now just replace the initial plot with a hexbin or contour plot and you
should have something that takes a lot less room but still shows the
locations of the outer points.



Greg Snow, Ph.D.
Statistical Data Center
greg.snow at ihc.com
(801) 408-8111



From hjb at pdq.com  Wed Nov 24 20:38:19 2004
From: hjb at pdq.com (Heather J. Branton)
Date: Wed, 24 Nov 2004 14:38:19 -0500
Subject: [R] Searching for antilog function
In-Reply-To: <l9o9q0lm2j1fi046pb3irlo1vhfcv76v9l@4ax.com>
References: <41A4C456.308@pdq.com> <l9o9q0lm2j1fi046pb3irlo1vhfcv76v9l@4ax.com>
Message-ID: <41A4E32B.30300@pdq.com>

Thank you so much for each of your responses. But to make sure I am 
clear (in my own mind), is this correct?

If  x = 2^y
Then  y = log2(x)

Thanks again. I know this is basic.

...heather

Duncan Murdoch wrote:

>On Wed, 24 Nov 2004 12:26:46 -0500, "Heather J. Branton" <hjb at pdq.com>
>wrote :
>
>  
>
>>Dear R-users,
>>
>>I have a basic question about how to determine the antilog of a variable.
>>
>>Say I have some number, x, which is a factor of 2 such that x = 2^y. I 
>>want to figure out what y is, i.e. I am looking for the antilog base 2 of x.
>>
>>I have found log2 in the Reference Manual. But I am struggling how to 
>>get the antilog of that.
>>    
>>
>
>You seem to be confusing log with antilog, but log2(x) and 2^y are
>inverses of each other, i.e.
>
>log2(2^y) equals y
>
>and 
>
>2^log2(x) equals x
>
>(up to rounding error, of course).
>
>Duncan Murdoch
>
>
>  
>

-- 
_______________________________________________________________________
Heather J. Branton                                  Public Data Queries
Data Specialist                                 310 Depot Street, Ste C
734.213.4964 x312                                  Ann Arbor, MI  48104

               U.S. Census Microdata At Your Fingertips
                          http://www.pdq.com



From MSchwartz at MedAnalytics.com  Wed Nov 24 21:58:00 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Wed, 24 Nov 2004 14:58:00 -0600
Subject: [R] reshaping of data for barplot2
In-Reply-To: <1101320697.25550.209392197@webmail.messagingengine.com>
References: <1101320697.25550.209392197@webmail.messagingengine.com>
Message-ID: <1101329880.22170.71.camel@horizons.localdomain>

On Wed, 2004-11-24 at 19:24 +0100, Jean-Louis Abitbol wrote:
> Dear All,
> 
> I have  the following data coming out from 
> 
> s <- with(final,
>            summarize(norm, llist(gtt,fdiab),
>                      function(norm) {
>                       n <- sum(!is.na(norm))
>                       s <- sum(norm, na.rm=T)
>                       binconf(s, n)
>                      }, type='matrix')
> )
> ie 
> 
>      gtt fdiab   norm.norm  norm.norm2  norm.norm3
> 18    PL    No  3.70370370  0.18997516 18.28346593
> 19    PL   Yes  3.57142857  0.18319034 17.71219774
> 13    TT1   No  9.09090909  3.59221932 21.15923917
> 14    TT1  Yes  1.81818182  0.09326054  9.60577606
> ...
> 10  HIGH    No 26.53061224 16.21128213 40.26228897
> 11  HIGH   Yes 10.00000000  4.66428345 20.14946472
> 
> I would like to reshape the data so that I can barplot2 treatments (gtt)
> with 2 beside bars for fdiab  yes/no and add CI.
> 
> Various attemps have been unsuccessful as I have not understood both the
> logic of beside and the nature of structures to be passed to barplot2.
> Not enough know-how with reshape and transpose either.
> 
> Needless to say Dotplot works great with this kind of data but some
> "Authority" requests side:side bars with CI.
> 
> Thanks for any help.

Jean-Louis,

For an easy example, see the help in barplot2, which uses the VADeaths
dataset. The dataset looks like:

> VADeaths
      Rural Male Rural Female Urban Male Urban Female
50-54       11.7          8.7       15.4          8.4
55-59       18.1         11.7       24.3         13.6
60-64       26.9         20.3       37.0         19.3
65-69       41.0         30.9       54.6         35.1
70-74       66.0         54.3       71.1         50.0

Now use:

barplot2(VADeaths)

This will yield a stacked bar plot, where there are 4 bars (one for each
column in the matrix). Each bar then consists of 5 stacked sections,
with each section representing the row values in each column.

Now try:

barplot(VADeaths, beside = TRUE)

This now yields 4 groups of bars, with one group for each column. Each
group then consists of 5 bars, one bar for each row value.

Hopefully, that gives you some insight into how the matrix structure
interacts with the 'beside' argument.

In the case of your data above, I read the few rows into a data frame
called 'df'. So 'df' looks like:

> df
   gtt fdiab norm.norm  norm.norm2 norm.norm3
1   PL    No  3.703704  0.18997516  18.283466
2   PL   Yes  3.571429  0.18319034  17.712198
3  TT1    No  9.090909  3.59221932  21.159239
4  TT1   Yes  1.818182  0.09326054   9.605776
5 HIGH    No 26.530612 16.21128213  40.262289
6 HIGH   Yes 10.000000  4.66428345  20.149465


To follow the VADeaths example above, you need to re-shape the required
columns, each as three column matrices, as follows:

height <- matrix(df$norm.norm, ncol = 3)
ci.l <- matrix(df$norm.norm2, ncol = 3)
ci.u <- matrix(df$norm.norm3, ncol = 3)
bars <- matrix(df$fdiab, ncol = 3)

Now, 'height' looks like:

> height
         [,1]     [,2]     [,3]
[1,] 3.703704 9.090909 26.53061
[2,] 3.571429 1.818182 10.00000

ci.l and ci.u and bars will of course look similar.


So, now you could use barplot2 as follows:

mp <- barplot2(height, plot.ci = TRUE, 
               ci.l = ci.l, ci.u = ci.u, beside = TRUE,
               names.arg = bars)


Note that I save the bar midpoints in 'mp'.

Now, you can go back and put in the bar group labels as follows. First
break out the unique values of 'gtt' keeping the order intact by using
matrix():

labels <- matrix(df$gtt, ncol = 2, byrow = TRUE)
mtext(side = 1, at = colMeans(mp), text = labels[, 1], line = 3)

Note that I use 'byrow = TRUE' in the call to matrix() so that the order
of the matrix is set properly. Thus, each column contains the group
labels and looks like:

> labels
     [,1]   [,2]  
[1,] "PL"   "PL"  
[2,] "TT1"  "TT1" 
[3,] "HIGH" "HIGH"

So we just use the first column above in the call to mtext().


So that should do it and can be extended to your full dataset if the
format is consistent with what you have above.


One final (and important) note.  There is another approach here that can
be used, which is to keep your data in its initial state and specify the
'space' argument explicitly in the call to barplot2. This is actually
less work than what we did above. In this case, we use the 'space'
argument to group the bars explicitly, which is in effect, what the
'beside' argument does internally.

We use each column from 'df' directly and set the 'space' argument to a
repeating sequence of c(1, 0) for each of the 3 groups. Note that here
we need to explicitly define the colors to use, since barplot2 uses
'grey' by default when 'height' is a vector (as does barplot). We also
need to convert df$diab to a vector, otherwise the numeric factor codes
will be used.

The sequence then goes like this:

mp <- barplot2(df$norm.norm, plot.ci = TRUE, 
               ci.l = df$norm.norm2, ci.u = df$norm.norm3,
               space = rep(c(1, 0), 3),
               col = rep(c("red", "yellow"), 3),
               names.arg = as.vector(df$fdiab))

Now, as before, we create the 'labels' matrix.

labels <- matrix(df$gtt, ncol = 2, byrow = TRUE)

Now, use mtext() for the group. Note that since 'height' was a vector
and not a matrix, 'mp' will be as well. Thus, we need to convert the
'mp' vector to a matrix to get the group midpoints:

mtext(side = 1, at = colMeans(matrix(mp, ncol = 3)), 
      text = labels[, 1], line = 3)


Which approach you take is up to you. As you see, either one will work.

HTH,

Marc Schwartz
<Will be away from e-mail for a while. Will check back later>



From edd at debian.org  Wed Nov 24 21:58:28 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 24 Nov 2004 14:58:28 -0600
Subject: [R] seriesMerge
In-Reply-To: <b1d31504041124122950a6efce@mail.gmail.com>
References: <b1d31504041124122950a6efce@mail.gmail.com>
Message-ID: <20041124205828.GA25839@sonny.eddelbuettel.com>

On Wed, Nov 24, 2004 at 03:29:53PM -0500, Yasser El-Zein wrote:
> Is there a function in R that is equivalent to S-PLUS's 
> seriesMerge(x1, x2, pos="union")
> where x1, and x2 are of class timeSeries
> 
> seriesMerge is in S-PLUS's finmetrics. I looked into R's mergeSeries
> (in fSeries part of Rmetrics) but I could not make it behave quite the
> same. In R it expected a timeSeries object and a matrix of the same
> row count. In S-PLUS when using the union option both objects can be
> of different lengths.

The its (short for irregular timeseries) package has union() and
intersect(). The zoo package also some functions for this, I think.

Topics like this get discussed a bit on the r-sig-finance list, you may want
to glance at the archives or do some conditional googleing.

Hth, Dirk

-- 
If your hair is standing up, then you are in extreme danger.
      -- http://www.usafa.af.mil/dfp/cockpit-phys/fp1ex3.htm



From dunc_harris at hotmail.com  Wed Nov 24 22:08:45 2004
From: dunc_harris at hotmail.com (Duncan Harris)
Date: Wed, 24 Nov 2004 21:08:45 +0000
Subject: [R] confidence interval of a average...
Message-ID: <BAY101-F20D3D2CA65B53DAF285C949FB80@phx.gbl>

I have a sample of lung capacities from a population measured against 
height.  I need to know the 95% CI of the lung capacity of a person of 
average height.

I have fitted a regression line.

How do I get a minimum and maximum values of the 95% CI?

My thinking was that this has something to do with covariance, but how?

My other thinking was that I could derive the 0.975 (sqrt 0.95) CI for the 
height.  Then I could take the lower height 0.975 CI value and calculate 
from that the lower 0.975 value from the lung capacity. And then do the same 
for the taller people.  That is bound to be wrong though.

Dunc



From murdoch at stats.uwo.ca  Wed Nov 24 22:42:46 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Wed, 24 Nov 2004 16:42:46 -0500
Subject: [R] Searching for antilog function
In-Reply-To: <41A4E32B.30300@pdq.com>
References: <41A4C456.308@pdq.com> <l9o9q0lm2j1fi046pb3irlo1vhfcv76v9l@4ax.com>
	<41A4E32B.30300@pdq.com>
Message-ID: <t10aq09891sola387glkops8sfv1dfduuq@4ax.com>

On Wed, 24 Nov 2004 14:38:19 -0500, "Heather J. Branton" <hjb at pdq.com>
wrote :

>Thank you so much for each of your responses. But to make sure I am 
>clear (in my own mind), is this correct?
>
>If  x = 2^y
>Then  y = log2(x)

Yes, that's right.

Duncan Murdoch



From rbaer at atsu.edu  Wed Nov 24 22:48:22 2004
From: rbaer at atsu.edu (Robert W. Baer, Ph.D.)
Date: Wed, 24 Nov 2004 15:48:22 -0600
Subject: [R] Installing gregmisc under windows 2000
References: <1101320697.25550.209392197@webmail.messagingengine.com>
	<1101329880.22170.71.camel@horizons.localdomain>
Message-ID: <001201c4d26f$54ccd790$2781010a@BigBaer>

I went through the following steps using RGUI menus to install gregmisc from
CRAN.  It appears to install but at the end R does not seem to be able to
find it.  Any idea what I'm doing wrong?

Thankjs,
Rob
----------------------------
> local({a <- CRAN.packages()
+ install.packages(select.list(a[,1],,TRUE), .libPaths()[1], available=a,
dependencies=TRUE)})
trying URL `http://cran.r-project.org/bin/windows/contrib/2.0/PACKAGES'
Content type `text/plain; charset=iso-8859-1' length 23113 bytes
opened URL
downloaded 22Kb

trying URL
`http://cran.r-project.org/bin/windows/contrib/2.0/gregmisc_2.0.0.zip'
Content type `application/zip' length 687958 bytes
opened URL
downloaded 671Kb

bundle 'gregmisc' successfully unpacked and MD5 sums checked

Delete downloaded files (y/N)? y

updating HTML package descriptions
> library(gregmisc)
Error in library(gregmisc) : There is no package called 'gregmisc'
>
> version
         _
platform i386-pc-mingw32
arch     i386
os       mingw32
system   i386, mingw32
status
major    2
minor    0.1
year     2004
month    11
day      15
language R



From sasprog474 at yahoo.com  Wed Nov 24 22:58:27 2004
From: sasprog474 at yahoo.com (Greg Tarpinian)
Date: Wed, 24 Nov 2004 13:58:27 -0800 (PST)
Subject: [R] Modeling censored binomial / Poisson data
Message-ID: <20041124215827.73811.qmail@web41411.mail.yahoo.com>

I have some count data (0 - 1 at each time point for
each test subject) that I would like to model.  Since
the 1's are rather sparse, the Poisson distribution
comes to mind but I would also consider the binomial.

The data are censored as the data come from a clinical
trial and subjects were able to leave the study and
some were therefore lost to follow-up.

I am aware of the capabilities of lme( ) and nlme( )
through the excellent book by Pinheiro and Bates, but
am at a loss as to what to do with these count data.
Ideally, I would like to compare the placebo and 
treatment groups in a meaningful way.

Any input would be greatly appreciated.

Thanks,

     Greg
		
__________________________________ 


From muster at gmail.com  Wed Nov 24 23:13:58 2004
From: muster at gmail.com (Terry Mu)
Date: Wed, 24 Nov 2004 17:13:58 -0500
Subject: [R] how to remove time series trend in R?
Message-ID: <b68812e70411241413411b52bc@mail.gmail.com>

I got a set of data which has seasonal trend in form of sinx, cosx, I
don't have any idea on how to deal with it.

Can you give me a starting point? Thanks,

Terry



From ripley at stats.ox.ac.uk  Wed Nov 24 23:14:14 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 24 Nov 2004 22:14:14 +0000 (GMT)
Subject: [R] Installing gregmisc under windows 2000
In-Reply-To: <001201c4d26f$54ccd790$2781010a@BigBaer>
References: <1101320697.25550.209392197@webmail.messagingengine.com>
	<1101329880.22170.71.camel@horizons.localdomain>
	<001201c4d26f$54ccd790$2781010a@BigBaer>
Message-ID: <Pine.LNX.4.61.0411242211300.9604@gannet.stats>

gregmisc is a bundle, not a package.  Its description on CRAN is

gregmisc	Bundle of gtools, gdata, gmodels, gplots

so try one of those packages.


On Wed, 24 Nov 2004, Robert W. Baer, Ph.D. wrote:

> I went through the following steps using RGUI menus to install gregmisc from
> CRAN.  It appears to install but at the end R does not seem to be able to
> find it.  Any idea what I'm doing wrong?
>
> Thankjs,
> Rob
> ----------------------------
>> local({a <- CRAN.packages()
> + install.packages(select.list(a[,1],,TRUE), .libPaths()[1], available=a,
> dependencies=TRUE)})
> trying URL `http://cran.r-project.org/bin/windows/contrib/2.0/PACKAGES'
> Content type `text/plain; charset=iso-8859-1' length 23113 bytes
> opened URL
> downloaded 22Kb
>
> trying URL
> `http://cran.r-project.org/bin/windows/contrib/2.0/gregmisc_2.0.0.zip'
> Content type `application/zip' length 687958 bytes
> opened URL
> downloaded 671Kb
>
> bundle 'gregmisc' successfully unpacked and MD5 sums checked
  ^^^^^^^

> Delete downloaded files (y/N)? y
>
> updating HTML package descriptions
>> library(gregmisc)
> Error in library(gregmisc) : There is no package called 'gregmisc'
                                            ^^^^^^^

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Wed Nov 24 23:13:11 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 24 Nov 2004 23:13:11 +0100
Subject: [R] Installing gregmisc under windows 2000
In-Reply-To: <001201c4d26f$54ccd790$2781010a@BigBaer>
References: <1101320697.25550.209392197@webmail.messagingengine.com>
	<1101329880.22170.71.camel@horizons.localdomain>
	<001201c4d26f$54ccd790$2781010a@BigBaer>
Message-ID: <x2pt225214.fsf@biostat.ku.dk>

"Robert W. Baer, Ph.D." <rbaer at atsu.edu> writes:

> I went through the following steps using RGUI menus to install gregmisc from
> CRAN.  It appears to install but at the end R does not seem to be able to
> find it.  Any idea what I'm doing wrong?

It's a bundle nowadays, so you need to load one of it's constituent
packages. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From nielssteenkrogh at zitelab.dk  Wed Nov 24 23:28:07 2004
From: nielssteenkrogh at zitelab.dk (Niels Steen Krogh)
Date: Wed, 24 Nov 2004 23:28:07 +0100
Subject: [R] RE: RODBC and Table views
Message-ID: <20041124222427.M51997@zitelab.dk>

channel2<- odbcConnectAccess("C:\\Documents and
Settings\\F??lles\\Journal\\DATASUPERMARKED\\DANBIONOVEMBER2004.mdb", uid="")
sqlQuery(channel2,"select * from Afdelinger_output_tabel1B order by antal desc")

Does take views and tables!
Niels Steen Krogh
Konsulent
ZiteLab

Mail: ---------- nielssteenkrogh at zitelab.dk
Telefon: ------- +45 38 88 86 13
Mobil: --------- +45 22 67 37 38
Adresse: ------- Zitelab
---------------- Solsortvej 44
---------------- 2000 F.

ZiteLab
-Let's Empower Your Data with Webservices



From rbaer at atsu.edu  Wed Nov 24 23:45:19 2004
From: rbaer at atsu.edu (Robert W. Baer, Ph.D.)
Date: Wed, 24 Nov 2004 16:45:19 -0600
Subject: [R] Installing gregmisc under windows 2000
References: <1101320697.25550.209392197@webmail.messagingengine.com><1101329880.22170.71.camel@horizons.localdomain><001201c4d26f$54ccd790$2781010a@BigBaer>
	<x2pt225214.fsf@biostat.ku.dk>
Message-ID: <004901c4d277$495e1420$2781010a@BigBaer>

Thanks for the clarification.

Pursuant to the recent dicussion of GUI promoting ignornce among users, I
plead guilty for CRAN installs  but they have generally saved so much
time.<g>.  This does raise the question as to whether gregmisc and other
bundles should appear on the "install packages from CRAN" pop-up in RGUI.
It also leaves me wondering what exactly was the REAL result of the
apparently successful  gregmisc install .

I did help.search("bundle") and coming away with nada.  I am not sure where
I should head to de-dumb myself.  I found a little in writing R extensions,
but this did not clarify the interaction with the RGUI install procedure for
me.

Thanks again.

Rob

-----------------------------------------
----- Original Message ----- 
From: "Peter Dalgaard" <p.dalgaard at biostat.ku.dk>
To: "Robert W. Baer, Ph.D." <rbaer at atsu.edu>
Cc: "R-Help" <r-help at stat.math.ethz.ch>
Sent: Wednesday, November 24, 2004 4:13 PM
Subject: Re: [R] Installing gregmisc under windows 2000


> "Robert W. Baer, Ph.D." <rbaer at atsu.edu> writes:
>
> > I went through the following steps using RGUI menus to install gregmisc
from
> > CRAN.  It appears to install but at the end R does not seem to be able
to
> > find it.  Any idea what I'm doing wrong?
>
> It's a bundle nowadays, so you need to load one of it's constituent
> packages.
>
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907
>



From rbaer at atsu.edu  Wed Nov 24 23:56:00 2004
From: rbaer at atsu.edu (Robert W. Baer, Ph.D.)
Date: Wed, 24 Nov 2004 16:56:00 -0600
Subject: [R] confidence interval of a average...
Message-ID: <001101c4d278$c483ffb0$2781010a@BigBaer>

It depends on whether you want to do 95% ocnfidence intervals on the
predicition or the mean vital capacity.  Try the following and see if it
gets you started:
#Simulate data
height=48:72
vc=height*10+20*rnorm(72-48+1)
# Do regression
lm.vc=lm(vc~height)

# Confidence interval on mean vc
predict.lm(lm.vc,interval="confidence")
#confidence interval on prediced vc
predict.lm(lm.vc,interval="prediction")

#plot everything
plot(vc~height)

matlines(height,predict.lm(lm.vc,interval="c"), lty=c(1,2,2),col='blue')
matlines(height,predict.lm(lm.vc,interval="p"),lty=c(1,3,3),col=c('black','r
ed','red'))> Rob
> ----------------------
> Fom: "Duncan Harris" <dunc_harris at hotmail.com>
> > I have a sample of lung capacities from a population measured against
> > height.  I need to know the 95% CI of the lung capacity of a person of
> > average height.
> >
> > I have fitted a regression line.
> >
> > How do I get a minimum and maximum values of the 95% CI?
> >
> > My thinking was that this has something to do with covariance, but how?
> >
> > My other thinking was that I could derive the 0.975 (sqrt 0.95) CI for
the
> > height.  Then I could take the lower height 0.975 CI value and calculate
> > from that the lower 0.975 value from the lung capacity. And then do the
> same
> > for the taller people.  That is bound to be wrong though.
> >
> > Dunc
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> >
>



From dunc_harris at hotmail.com  Thu Nov 25 00:05:32 2004
From: dunc_harris at hotmail.com (Duncan Harris)
Date: Wed, 24 Nov 2004 23:05:32 +0000
Subject: [R] confidence interval of a average...
In-Reply-To: <003a01c4d274$929cb7c0$2781010a@BigBaer>
Message-ID: <BAY101-F3438A00EDE9F6C6ADC471A9FB80@phx.gbl>

Sorry if this was not clear.  This is more of a theoreticla question rather 
than a R-coding question.  I need to calculate

"The predicted response and 95% prediction interval for a man of average 
height"

So I need to predict the average response, which is easily done by taking 
the mean height and using the regression formula.

However, "average height" has to be calculated from the sample, and thus I 
have confidence in that.  Let's say the mean is 163cm, I think that I can't 
take the 163cm value and calculate the CI from just the sd of the lung 
capacity because that would be too narrow; I think covariance must come into 
it somehow, or can I just do a 97.5% CI on the height and take those extreme 
values and do a 97.% CI on them?

Dunc

>From: "Robert W. Baer, Ph.D." <rbaer at atsu.edu>
>To: "Duncan Harris" <dunc_harris at hotmail.com>
>Subject: Re: [R] confidence interval of a average...
>Date: Wed, 24 Nov 2004 16:25:53 -0600
>
>It depends on whether you want to do 95% ocnfidence intervals on the
>predicition or the mean vital capacity.  Try the following and see if it
>gets you started:
>#Simulate data
>height=48:72
>vc=height*10+20*rnorm(72-48+1)
># Do regression
>lm.vc=lm(vc~height)
>
># Confidence interval on mean vc
>predict.lm(lm.vc,interval="confidence")
>#confidence interval on prediced vc
>predict.lm(lm.vc,interval="prediction")
>
>#plot everything
>plot(vc~height)
>
>matlines(height,predict.lm(lm.vc,interval="c"), lty=c(1,2,2),col='blue')
>matlines(height,predict.lm(lm.vc,interval="p"),lty=c(1,3,3),col=c('black','r
>ed','red'))
>
>Rob
>----------------------
>Fom: "Duncan Harris" <dunc_harris at hotmail.com>
> > I have a sample of lung capacities from a population measured against
> > height.  I need to know the 95% CI of the lung capacity of a person of
> > average height.
> >
> > I have fitted a regression line.
> >
> > How do I get a minimum and maximum values of the 95% CI?
> >
> > My thinking was that this has something to do with covariance, but how?
> >
> > My other thinking was that I could derive the 0.975 (sqrt 0.95) CI for 
>the
> > height.  Then I could take the lower height 0.975 CI value and calculate
> > from that the lower 0.975 value from the lung capacity. And then do the
>same
> > for the taller people.  That is bound to be wrong though.
> >
> > Dunc
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
>http://www.R-project.org/posting-guide.html
> >
>



From rbaer at atsu.edu  Thu Nov 25 00:06:38 2004
From: rbaer at atsu.edu (Robert W. Baer, Ph.D.)
Date: Wed, 24 Nov 2004 17:06:38 -0600
Subject: [R] confidence interval of a average...
References: <001101c4d278$c483ffb0$2781010a@BigBaer>
Message-ID: <002701c4d27a$436c9890$2781010a@BigBaer>

Sorry.  The last code line got destroyed by my emailer and should read:
> matlines(height,predict.lm(lm.vc,interval="p"),
+ lty=c(1,3,3),col=c('black','red','red'))


----- Original Message ----- 
From: "Robert W. Baer, Ph.D." <rbaer at atsu.edu>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, November 24, 2004 4:56 PM
Subject: Re: [R] confidence interval of a average...


> It depends on whether you want to do 95% ocnfidence intervals on the
> predicition or the mean vital capacity.  Try the following and see if it
> gets you started:
> #Simulate data
> height=48:72
> vc=height*10+20*rnorm(72-48+1)
> # Do regression
> lm.vc=lm(vc~height)
>
> # Confidence interval on mean vc
> predict.lm(lm.vc,interval="confidence")
> #confidence interval on prediced vc
> predict.lm(lm.vc,interval="prediction")
>
> #plot everything
> plot(vc~height)
>
> matlines(height,predict.lm(lm.vc,interval="c"), lty=c(1,2,2),col='blue')
>
matlines(height,predict.lm(lm.vc,interval="p"),lty=c(1,3,3),col=c('black','r
> ed','red'))> Rob
> > ----------------------
> > Fom: "Duncan Harris" <dunc_harris at hotmail.com>
> > > I have a sample of lung capacities from a population measured against
> > > height.  I need to know the 95% CI of the lung capacity of a person of
> > > average height.
> > >
> > > I have fitted a regression line.
> > >
> > > How do I get a minimum and maximum values of the 95% CI?
> > >
> > > My thinking was that this has something to do with covariance, but
how?
> > >
> > > My other thinking was that I could derive the 0.975 (sqrt 0.95) CI for
> the
> > > height.  Then I could take the lower height 0.975 CI value and
calculate
> > > from that the lower 0.975 value from the lung capacity. And then do
the
> > same
> > > for the taller people.  That is bound to be wrong though.
> > >
> > > Dunc
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>



From yzhang4 at turing.une.edu.au  Thu Nov 25 00:25:07 2004
From: yzhang4 at turing.une.edu.au (Yuandan Zhang)
Date: Thu, 25 Nov 2004 10:25:07 +1100
Subject: [R] Installing gregmisc under windows 2000
In-Reply-To: <Pine.LNX.4.61.0411242211300.9604@gannet.stats>
References: <1101320697.25550.209392197@webmail.messagingengine.com>
	<1101329880.22170.71.camel@horizons.localdomain>
	<001201c4d26f$54ccd790$2781010a@BigBaer>
	<Pine.LNX.4.61.0411242211300.9604@gannet.stats>
Message-ID: <20041125102507.5274bfa3.yzhang4@turing.une.edu.au>

That seems not the case under linux in term of installation. You can install this bundle in the same way as installing an individul package.

eg 

R CMD INSTALL  CRAN/contrib/main/gregmisc_2.0.0.tar.gz

to get all constituent packages installed. 
 
 
On Wed, 24 Nov 2004 22:14:14 +0000 (GMT)
Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:

> gregmisc is a bundle, not a package.  Its description on CRAN is
> 
> gregmisc	Bundle of gtools, gdata, gmodels, gplots
> 
> so try one of those packages.


-- 

--
Yuandan Zhang, PhD

Animal Genetics and Breeding Unit
The University of New England
Armidale, NSW, Australia, 2351

E-mail:   yzhang4 at metz.une.edu.au
Phone:    (61) 02 6773 3786
Fax:      (61) 02 6773 3266
http://agbu.une.edu.au
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  AGBU is a joint venture of NSW Primary Industries 
  and The University of New England to undertake 
  genetic R&D for Australia's Livestock Industries           
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



From p.connolly at hortresearch.co.nz  Thu Nov 25 00:35:34 2004
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Thu, 25 Nov 2004 12:35:34 +1300
Subject: [R] scatterplot of 100000 points and pdf file format
In-Reply-To: <1101313376.18939.25.camel@horizons.localdomain>
References: <41A4AA16.7070008@molgen.mpg.de> 
	<1101313376.18939.25.camel@horizons.localdomain>
Message-ID: <20041124233534.GG18464@hortresearch.co.nz>

On Wed, 24-Nov-2004 at 10:22AM -0600, Marc Schwartz wrote:

|> On Wed, 2004-11-24 at 16:34 +0100, Witold Eryk Wolski wrote:
|> > Hi,
|> > 
|> > I want to draw a scatter plot with 1M  and more points and save it as pdf.
|> > This makes the pdf file large.
|> > So i tried to save the file first as png and than convert it to pdf. 
|> > This looks OK if printed but if viewed e.g. with acrobat as document 
|> > figure the quality is bad.
|> > 
|> > Anyone knows a way to reduce the size but keep the quality?
|> 
|> Hi Eryk!
|> 
|> Part of the problem is that in a pdf file, the vector based instructions
|> will need to be defined for each of your 10 ^ 6 points in order to draw
|> them.
|> 
|> When trying to create a simple example:
|> 
|> pdf()
|> plot(rnorm(1000000), rnorm(1000000))
|> dev.off()
|> 
|> The pdf file is 55 Mb in size.
|> 
|> One immediate thought was to try a ps file and using the above plot, the
|> ps file was "only" 23 Mb in size. So note that ps can be more efficient.
|> 
|> Going to a bitmap might result in a much smaller file, but as you note,
|> the quality does degrade as compared to a vector based image.
|> 
|> I tried the above to a png, then converted to a pdf (using 'convert')
|> and as expected, the image both viewed and printed was "pixelated",
|> since the pdf instructions are presumably drawing pixels and not vector
|> based objects.

Using bitmap( ... , res = 300), I get a bitmap file of 56 Kb.

It's rather slow, most of the time being taken up using gs which is
converting the vector image I suspect.  Time would be much shorter if,
say a circle of diameter of 4 is left unplotted in the middle and
others have mentioned other ways of reducing redundant points.

A pdf file slightly larger than the png file can be made directly from
OpenOffice that has the png imported into it.  For a plot of 160mm
square, this pdf printed unpixelated.

Depending on what size (dimensions) you need to finish up with, you
might find you could get away with a lower resolution than 300 dpi,
but I usually find 200 too ragged.

HTH

-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From andy_liaw at merck.com  Thu Nov 25 01:03:07 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 24 Nov 2004 19:03:07 -0500
Subject: [R] Installing gregmisc under windows 2000
Message-ID: <3A822319EB35174CA3714066D590DCD50994E387@usrymx25.merck.com>

If I'm not mistaken, "bundle" is really only useful as a concept for
distribution and installation.  You distribute and install a bundle, but
load the individual packages when you want to use them.  Once you install
the bundle, you won't see the name of the bundle in the list of installed
packages, but you see the constituent packages, and those are what you load
when you want to use them.  [This is the same on all platforms, BTW.]

Andy

> From: Robert W. Baer, Ph.D.
> Sent: Wednesday, November 24, 2004 5:45 PM
> To: Peter Dalgaard
> Cc: R-Help
> Subject: Re: [R] Installing gregmisc under windows 2000
> 
> 
> Thanks for the clarification.
> 
> Pursuant to the recent dicussion of GUI promoting ignornce 
> among users, I
> plead guilty for CRAN installs  but they have generally saved so much
> time.<g>.  This does raise the question as to whether 
> gregmisc and other
> bundles should appear on the "install packages from CRAN" 
> pop-up in RGUI.
> It also leaves me wondering what exactly was the REAL result of the
> apparently successful  gregmisc install .
> 
> I did help.search("bundle") and coming away with nada.  I am 
> not sure where
> I should head to de-dumb myself.  I found a little in writing 
> R extensions,
> but this did not clarify the interaction with the RGUI 
> install procedure for
> me.
> 
> Thanks again.
> 
> Rob
> 
> -----------------------------------------
> ----- Original Message ----- 
> From: "Peter Dalgaard" <p.dalgaard at biostat.ku.dk>
> To: "Robert W. Baer, Ph.D." <rbaer at atsu.edu>
> Cc: "R-Help" <r-help at stat.math.ethz.ch>
> Sent: Wednesday, November 24, 2004 4:13 PM
> Subject: Re: [R] Installing gregmisc under windows 2000
> 
> 
> > "Robert W. Baer, Ph.D." <rbaer at atsu.edu> writes:
> >
> > > I went through the following steps using RGUI menus to 
> install gregmisc
> from
> > > CRAN.  It appears to install but at the end R does not 
> seem to be able
> to
> > > find it.  Any idea what I'm doing wrong?
> >
> > It's a bundle nowadays, so you need to load one of it's constituent
> > packages.
> >
> > -- 
> >    O__  ---- Peter Dalgaard             Blegdamsvej 3
> >   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N
> >  (*) \(*) -- University of Copenhagen   Denmark      Ph: 
> (+45) 35327918
> > ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: 
> (+45) 35327907
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Ted.Harding at nessie.mcc.ac.uk  Thu Nov 25 01:37:15 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 25 Nov 2004 00:37:15 -0000 (GMT)
Subject: [R] scatterplot of 100000 points and pdf file format
In-Reply-To: <Pine.LNX.4.61.0411241646080.2681@gannet.stats>
Message-ID: <XFMail.041125003715.Ted.Harding@nessie.mcc.ac.uk>

On 24-Nov-04 Prof Brian Ripley wrote:
> On Wed, 24 Nov 2004 Ted.Harding at nessie.mcc.ac.uk wrote:
> 
>> 1. Multiply the data by some factor and then round the
>>   results to an integer (to avoid problems in step 2).
>>   Factor chosen so that the result of (4) below is
>>   satisfactory.
>>
>> 2. Eliminate duplicates in the result of (1).
>>
>> 3. Divide by the factor you used in (1).
>>
>> 4. Plot the result; save plot to PDF.
>>
>> As to how to do it in R: the critical step is (2),
>> which with so many points could be very heavy unless
>> done by a well-chosen procedure. I'm not expert enough
>> to advise about that, but no doubt others are.
> 
> unique will eat that for breakfast
> 
>> x <- runif(1e6)
>> system.time(xx <- unique(round(x, 4)))
> [1] 0.55 0.09 0.64 0.00 0.00
>> length(xx)
> [1] 10001

'unique' will eat x for breakfast, indeed, but will have some
trouble chewing (x,y).

I still can't think of a neat way of doing that.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 25-Nov-04                                       Time: 00:37:15
------------------------------ XFMail ------------------------------



From maustin at amgen.com  Thu Nov 25 02:51:53 2004
From: maustin at amgen.com (Austin, Matt)
Date: Wed, 24 Nov 2004 17:51:53 -0800
Subject: [R] scatterplot of 100000 points and pdf file format
Message-ID: <E7D5AB4811D20B489622AABA9C53859104E0DBB7@teal-exch.amgen.com>



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of
> Ted.Harding at nessie.mcc.ac.uk
> Sent: Wednesday, November 24, 2004 16:37 PM
> To: R Help Mailing List
> Subject: RE: [R] scatterplot of 100000 points and pdf file format
> 
> 
> On 24-Nov-04 Prof Brian Ripley wrote:
> > On Wed, 24 Nov 2004 Ted.Harding at nessie.mcc.ac.uk wrote:
> > 
> >> 1. Multiply the data by some factor and then round the
> >>   results to an integer (to avoid problems in step 2).
> >>   Factor chosen so that the result of (4) below is
> >>   satisfactory.
> >>
> >> 2. Eliminate duplicates in the result of (1).
> >>
> >> 3. Divide by the factor you used in (1).
> >>
> >> 4. Plot the result; save plot to PDF.
> >>
> >> As to how to do it in R: the critical step is (2),
> >> which with so many points could be very heavy unless
> >> done by a well-chosen procedure. I'm not expert enough
> >> to advise about that, but no doubt others are.
> > 
> > unique will eat that for breakfast
> > 
> >> x <- runif(1e6)
> >> system.time(xx <- unique(round(x, 4)))
> > [1] 0.55 0.09 0.64 0.00 0.00
> >> length(xx)
> > [1] 10001
> 
> 'unique' will eat x for breakfast, indeed, but will have some
> trouble chewing (x,y).
> 


>  xx <- data.frame(x=round(runif(1000000),4), y=round(runif(1000000),4))
>  system.time(xx2 <- unique(xx))
[1] 14.23  0.06 14.34    NA    NA

The time does not seem too bad, depending on how many times it has to be
performed.
--Matt

Matt Austin
Statistician

Amgen 
One Amgen Center Drive
M/S 24-2-C
Thousand Oaks CA 93021
(805) 447 - 7431

> I still can't think of a neat way of doing that.
> 
> Best wishes,
> Ted.
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
> Date: 25-Nov-04                                       Time: 00:37:15
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Ted.Harding at nessie.mcc.ac.uk  Thu Nov 25 02:45:48 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 25 Nov 2004 01:45:48 -0000 (GMT)
Subject: [R] scatterplot of 100000 points and pdf file format
In-Reply-To: <XFMail.041125003715.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <XFMail.041125014548.Ted.Harding@nessie.mcc.ac.uk>

On 25-Nov-04 Ted Harding wrote:
> 'unique' will eat x for breakfast, indeed, but will have some
> trouble chewing (x,y).
> 
> I still can't think of a neat way of doing that.
> 
> Best wishes,
> Ted.

Sorry, I don't want to be misunderstood.
I didn't mean that 'unique' won't work for arrays.
What I meant was:

> X<-round(rnorm(1e6),3);Y<-round(rnorm(1e6),3)
> system.time(unique(X))
[1] 0.74 0.07 0.81 0.00 0.00
> system.time(unique(cbind(X,Y)))
[1] 350.81   4.56 356.54   0.00   0.00

However, still rounding to 3 d.p. we can try packing:

> Z<-100000000*X + 1000*Y
> system.time(W<-unique(Z))
[1] 0.83 0.05 0.88 0.00 0.00
> length(W)
[1] 961523

Though the runtime is small we don't get much reduction
and still W has to be unpacked.

With rounding to 2 d.p.

> X<-round(rnorm(1e6),2);Y<-round(rnorm(1e6),2)
> Z<-100000000*X + 1000*Y
> system.time(W<-unique(Z))
[1] 1.31 0.01 1.32 0.00 0.00
> length(W)
[1] 209882

so now it's about 1/5, but visible discretisation must be
getting close.

With 1 d.p.

> X<-round(rnorm(1e6),1);Y<-round(rnorm(1e6),1)
> Z<-100000000*X + 1000*Y
> system.time(W<-unique(Z))
[1] 0.92 0.01 0.93 0.00 0.00
> length(W)
[1] 4953

there's a good reduction (about 1/200) but the discretisation
would definitely now be visible. However, as I suggested before,
there's an issue of choice of constant (i.e. of the resolution
of the discretisation so that there's a useful reduction and
also the plot is acceptable).

I'd still like to learn of a method which avoids the
above method of packing, which strikes me as clumsy
(but maybe it's the best way after all).

Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 25-Nov-04                                       Time: 01:45:48
------------------------------ XFMail ------------------------------



From rbaer at atsu.edu  Thu Nov 25 03:05:37 2004
From: rbaer at atsu.edu (Robert W. Baer, Ph.D.)
Date: Wed, 24 Nov 2004 20:05:37 -0600
Subject: [R] confidence interval of a average...
References: <BAY101-F3438A00EDE9F6C6ADC471A9FB80@phx.gbl>
Message-ID: <004201c4d293$41f72d40$6501a8c0@ALKAID>

> Sorry if this was not clear.  This is more of a theoreticla question 
> rather than a R-coding question.  I need to calculate
>
> "The predicted response and 95% prediction interval for a man of average 
> height"
>
> So I need to predict the average response, which is easily done by taking 
> the mean height and using the regression formula.
>
> However, "average height" has to be calculated from the sample, and thus I 
> have confidence in that.  Let's say the mean is 163cm, I think that I 
> can't take the 163cm value and calculate the CI from just the sd of the 
> lung capacity because that would be too narrow; I think covariance must 
> come into it somehow, or can I just do a 97.5% CI on the height and take 
> those extreme values and do a 97.% CI on them?

Then, you want the predition interval on the mean VC which is the thighter 
of the two confidence intervals and does not include the extra variability 
of VC about its mean.  As always with confidence intevals, you are free to 
look at either 95% CI or 97.5% CI depending on what kind of satement you'd 
like to make about your confidence.  I don't not understand you comment 
about covariance at all.

Let me try again with data in your units.  Note that CI varies with height 
and is smallest at the mean height whether you are talking about CI on the 
mean VC or CI on the predicted VC.  For comparison, the red lines are the 
95% CI on mean regression fit VC and the blue lines are 95% CI on 
"predicted" VC.   The simulated data is set to have a mean height that 
varies around 163 cm.


# Make simulated data with mean height near 163
# vc approximately in liter values with scatter
height=sort(rnorm(50,mean=163,sd=35))
vc=0.03*height+.5*rnorm(50)

#Plot the simulated data
plot(vc~height,ylab='vital capacity (l)',xlab='Height (cm)')

# Set up data frame with values of height you wish a ci on
# column heading must be same as for lm() fit x variable
# in this case, dataframe contains only mean height
mean.height.fit.ci=data.frame(height=mean(height))

#print out the mean height
mean.height.fit.ci

# fit the regression model
vc.lm=lm(vc~height)

#Draw 95% confidence intervals on mean vc at various heights(red) (min at 
mean(height)
matlines(height,predict.lm(vc.lm,interval="c"),lty=c(1,2,2), 
col=c('black','red','red'))

#Draw 95% confidence intervals on new vc at various heights(blue) (min again 
at mean(height)
matlines(height,predict.lm(vc.lm,interval="p"),lty=c(1,3,3), 
col=c('black','blue','blue'))

# Determine 95% CI on mean vc at mean height
predict.lm(vc.lm,mean.height.fit.ci,interval="confidence")

# Determine 97.5 5% CI on mean vc at mean height
predict.lm(vc.lm,mean.height.fit.ci,interval="confidence", level=0.975)


You might wish to read a little more about regression CIs in a good 
statistics book.

HTH,
Rob



From apha at laser-registration.com  Thu Nov 25 03:06:31 2004
From: apha at laser-registration.com (APHA Registration Services)
Date: Wed, 24 Nov 2004 21:06:31 -0500
Subject: [R] Re:Hi!
Message-ID: <A03FD9EE-3E86-11D9-89BB-0030654D391A@laser-registration.com>

Thank you for your interest in APHA 2004. This is an automated reply 
confirming that we have received your email inquiry.

General questions, requests, modifications and/or new registrations 
received by email, fax or mail will be processed within 7 business 
days. At that time, you will receive either a letter of confirmation 
reflecting your requested modifications or a response to your inquiry 
via email.

Confirmation letters will be sent to the email address you had provided 
on your registration form, or by fax, if no email address was provided.

Meanwhile, the most up-to-date meeting and program information is 
online! Visit www.APHA.org and register at the same time! Registration 
does not get any more convenient than with One-Stop-Registration.

Should you have any questions, please do not hesitate to contact us.

Sincerely,

APHA Registrar.
Phone: (514) 228-3009
Fax: (514) 228-3148
Email: apha at laser-registration.com

APHA c/o Laser Registration
1200 G Street, NW
Suite 800
Washington, DC 20005-3967

On Nov 24, 2004, at 9:03 PM, r-help at lists.r-project.org wrote:


From Ted.Harding at nessie.mcc.ac.uk  Thu Nov 25 03:12:20 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 25 Nov 2004 02:12:20 -0000 (GMT)
Subject: [R] scatterplot of 100000 points and pdf file format
In-Reply-To: <E7D5AB4811D20B489622AABA9C53859104E0DBB7@teal-exch.amgen.com>
Message-ID: <XFMail.041125021220.Ted.Harding@nessie.mcc.ac.uk>

On 25-Nov-04 Austin, Matt wrote:
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of
>> Ted.Harding at nessie.mcc.ac.uk
>> Sent: Wednesday, November 24, 2004 16:37 PM
>> To: R Help Mailing List
>> Subject: RE: [R] scatterplot of 100000 points and pdf file format
>> 
>> 
>> On 24-Nov-04 Prof Brian Ripley wrote:
>> > On Wed, 24 Nov 2004 Ted.Harding at nessie.mcc.ac.uk wrote:
>> > 
>> >> 1. Multiply the data by some factor and then round the
>> >>   results to an integer (to avoid problems in step 2).
>> >>   Factor chosen so that the result of (4) below is
>> >>   satisfactory.
>> >>
>> >> 2. Eliminate duplicates in the result of (1).
>> >>
>> >> 3. Divide by the factor you used in (1).
>> >>
>> >> 4. Plot the result; save plot to PDF.
>> >>
>> >> As to how to do it in R: the critical step is (2),
>> >> which with so many points could be very heavy unless
>> >> done by a well-chosen procedure. I'm not expert enough
>> >> to advise about that, but no doubt others are.
>> > 
>> > unique will eat that for breakfast
>> > 
>> >> x <- runif(1e6)
>> >> system.time(xx <- unique(round(x, 4)))
>> > [1] 0.55 0.09 0.64 0.00 0.00
>> >> length(xx)
>> > [1] 10001
>> 
>> 'unique' will eat x for breakfast, indeed, but will have some
>> trouble chewing (x,y).
>> 
> 
> 
>>  xx <- data.frame(x=round(runif(1000000),4),
>>  y=round(runif(1000000),4))
>>  system.time(xx2 <- unique(xx))
> [1] 14.23  0.06 14.34    NA    NA
> 
> The time does not seem too bad, depending on how many times it
> has to be performed.
> --Matt

Interesting! Let's see:

Starting again,

  X<-round(rnorm(1e6),3);Y<-round(rnorm(1e6),3)
  XY<-cbind(X,Y)
  system.time(unique(XY))
  [1] 288.22   3.00 291.38   0.00   0.00

  XY<-data.frame(x=X,y=Y)
  system.time(unique(XY))
  [1] 72.38  0.84 74.44  0.00  0.00


Data Frames Are Fast Food!!!

Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 25-Nov-04                                       Time: 02:12:20
------------------------------ XFMail ------------------------------



From r-stats at arcriswell.com  Thu Nov 25 03:47:12 2004
From: r-stats at arcriswell.com (Andrew Criswell)
Date: Thu, 25 Nov 2004 09:47:12 +0700
Subject: [R] Error in anova(): objects must inherit from classes
Message-ID: <41A547B0.90508@arcriswell.com>

Hello:

Let me rephrase my question to attract interest in the problem I'm having. When I appply anova() to two equations
estimated using glmmPQL, I get a complaint,

> anova(fm1, fm2)
Error in anova.lme(fm1, fm2) : Objects must inherit from classes "gls",
"gnls" "lm","lmList", "lme","nlme","nlsList", or "nls"
>

The two equations I estimated are these:

> fm1 <- glmmPQL(choice ~ day + stereotypy,
+                random = ~ 1 | bear, data = learning, family = binomial)
> fm2 <- glmmPQL(choice ~ day + envir + stereotypy,
+                random = ~ 1 | bear, data = learning, family = binomial)

Individually, I get results from anova():

> anova(fm1)
            numDF denDF   F-value p-value
(Intercept)     1  2032   7.95709  0.0048
day             1  2032 213.98391  <.0001
stereotypy      1  2032   0.42810  0.5130
>
> anova(fm2)
            numDF denDF   F-value p-value
(Intercept)     1  2031   5.70343  0.0170
day             1  2031 213.21673  <.0001
envir           1  2031  12.50388  0.0004
stereotypy      1  2031   0.27256  0.6017
>

I did look through the archives but didn't finding anything relevant to my problem.

Hope someone can help.

ANDREW
____________________________
         _
platform i586-mandrake-linux-gnu
arch     i586
os       linux-gnu
system   i586, linux-gnu
status
major    2
minor    0.0
year     2004
month    10
day      04
language R



-- 
Andrew R. Criswell, Ph.D.
Graduate School, Bangkok University

mailto:andrew.c at bu.ac.th <http://email.bu.ac.th/src/compose.php?send_to=andrew.c%40bu.ac.th>
mailto:andrew at arcriswell.com <http://email.bu.ac.th/src/compose.php?send_to=andrew%40arcriswell.com>



From andy_liaw at merck.com  Thu Nov 25 03:45:27 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 24 Nov 2004 21:45:27 -0500
Subject: [R] scatterplot of 100000 points and pdf file format
Message-ID: <3A822319EB35174CA3714066D590DCD50994E38A@usrymx25.merck.com>

> From: Ted.Harding at nessie.mcc.ac.uk
> 
> On 25-Nov-04 Ted Harding wrote:
> > 'unique' will eat x for breakfast, indeed, but will have some
> > trouble chewing (x,y).
> > 
> > I still can't think of a neat way of doing that.
> > 
> > Best wishes,
> > Ted.
> 
> Sorry, I don't want to be misunderstood.
> I didn't mean that 'unique' won't work for arrays.
> What I meant was:
> 
> > X<-round(rnorm(1e6),3);Y<-round(rnorm(1e6),3)
> > system.time(unique(X))
> [1] 0.74 0.07 0.81 0.00 0.00
> > system.time(unique(cbind(X,Y)))
> [1] 350.81   4.56 356.54   0.00   0.00

Do you know if majority of that time is spent in unique() itself?  If so,
which method?  What I see is:

> X<-round(rnorm(1e6),3);Y<-round(rnorm(1e6),3)
> system.time(unique(X), gcFirst=TRUE)
[1] 0.25 0.01 0.26   NA   NA
> system.time(unique(cbind(X,Y)), gcFirst=TRUE)
[1] 101.80   0.34 104.61     NA     NA
> system.time(dat <- data.frame(x=X, y=Y), gcFirst=TRUE)
[1] 10.17  0.00 10.24    NA    NA
> system.time(unique(dat), gcFirst=TRUE)
[1] 23.94  0.11 24.15    NA    NA

Andy

 
> However, still rounding to 3 d.p. we can try packing:
> 
> > Z<-100000000*X + 1000*Y
> > system.time(W<-unique(Z))
> [1] 0.83 0.05 0.88 0.00 0.00
> > length(W)
> [1] 961523
> 
> Though the runtime is small we don't get much reduction
> and still W has to be unpacked.
> 
> With rounding to 2 d.p.
> 
> > X<-round(rnorm(1e6),2);Y<-round(rnorm(1e6),2)
> > Z<-100000000*X + 1000*Y
> > system.time(W<-unique(Z))
> [1] 1.31 0.01 1.32 0.00 0.00
> > length(W)
> [1] 209882
> 
> so now it's about 1/5, but visible discretisation must be
> getting close.
> 
> With 1 d.p.
> 
> > X<-round(rnorm(1e6),1);Y<-round(rnorm(1e6),1)
> > Z<-100000000*X + 1000*Y
> > system.time(W<-unique(Z))
> [1] 0.92 0.01 0.93 0.00 0.00
> > length(W)
> [1] 4953
> 
> there's a good reduction (about 1/200) but the discretisation
> would definitely now be visible. However, as I suggested before,
> there's an issue of choice of constant (i.e. of the resolution
> of the discretisation so that there's a useful reduction and
> also the plot is acceptable).
> 
> I'd still like to learn of a method which avoids the
> above method of packing, which strikes me as clumsy
> (but maybe it's the best way after all).
> 
> Ted.
> 
> 
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
> Date: 25-Nov-04                                       Time: 01:45:48
> ------------------------------ XFMail ------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From h.wickham at gmail.com  Thu Nov 25 04:21:28 2004
From: h.wickham at gmail.com (hadley wickham)
Date: Wed, 24 Nov 2004 21:21:28 -0600
Subject: [R] scatterplot of 100000 points and pdf file format
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E38A@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E38A@usrymx25.merck.com>
Message-ID: <f8e6ff0504112419219b6fb8b@mail.gmail.com>

Another possibility might be to use a 2d kernel density estimate (eg.
kde2d from library(MASS).  Then for the high density areas plot the
density contours, for the low density areas plot the individual
points.

Hadley



From china_man at hotmail.com  Thu Nov 25 04:26:22 2004
From: china_man at hotmail.com (Michael Lau)
Date: Wed, 24 Nov 2004 22:26:22 -0500
Subject: [R] logistic regression and 3PL model
Message-ID: <BAY1-DAV18752B378D942FD7000ECA9FB90@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041124/64ca075a/attachment.pl

From maustin at amgen.com  Thu Nov 25 04:34:05 2004
From: maustin at amgen.com (Austin, Matt)
Date: Wed, 24 Nov 2004 19:34:05 -0800
Subject: [R] Error in anova(): objects must inherit from classes
Message-ID: <E7D5AB4811D20B489622AABA9C53859104E0DBBA@teal-exch.amgen.com>

The lme method for anova() checks the inheritance of the object when a
single object is supplied, which is why there is no error when you use one
object at a time.  When two objects are supplied, the method uses the class
of the object by invoking the data.class function (which does not list
glmmPQL class).  If you replace the check of the class with a check of
inheritance it should work.

Following is a check from the example listed in MASS (Venables and Ripley)

>  library(MASS)
>  library(nlme) 
>  x1 <- glmmPQL(y ~ I(week > 2), random = ~ 1 | ID,
+                  family = binomial, data = bacteria)
iteration 1 
iteration 2 
iteration 3 
iteration 4 
iteration 5 
iteration 6 
>  x2 <- glmmPQL(y ~ trt + I(week > 2), random = ~ 1 | ID,
+                  family = binomial, data = bacteria)
iteration 1 
iteration 2 
iteration 3 
iteration 4 
iteration 5 
iteration 6 
>  anova(x1)
            numDF denDF F-value p-value
(Intercept)     1   169      35  <.0001
I(week > 2)     1   169      21  <.0001
>  anova(x2)
            numDF denDF F-value p-value
(Intercept)     1   169      35  <.0001
trt             2    47       2    0.22
I(week > 2)     1   169      20  <.0001

>  anova(x1, x2)
Error in anova.lme(x1, x2) : Objects must inherit from classes "gls", "gnls"
"lm","lmList", "lme","nlme","nlsList", or "nls"

After replacement:

>  anovaLME(x1, x2)
   Model df  AIC  BIC logLik   Test L.Ratio p-value
x1     1  4 1107 1121   -550                       
x2     2  6 1114 1134   -551 1 vs 2     2.6    0.28


Matt Austin
Statistician

Amgen 
One Amgen Center Drive
M/S 24-2-C
Thousand Oaks CA 93021
(805) 447 - 7431


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Andrew Criswell
> Sent: Wednesday, November 24, 2004 18:47 PM
> To: R-help
> Subject: [R] Error in anova(): objects must inherit from classes
> 
> 
> Hello:
> 
> Let me rephrase my question to attract interest in the 
> problem I'm having. When I appply anova() to two equations
> estimated using glmmPQL, I get a complaint,
> 
> > anova(fm1, fm2)
> Error in anova.lme(fm1, fm2) : Objects must inherit from 
> classes "gls",
> "gnls" "lm","lmList", "lme","nlme","nlsList", or "nls"
> >
> 
> The two equations I estimated are these:
> 
> > fm1 <- glmmPQL(choice ~ day + stereotypy,
> +                random = ~ 1 | bear, data = learning, family 
> = binomial)
> > fm2 <- glmmPQL(choice ~ day + envir + stereotypy,
> +                random = ~ 1 | bear, data = learning, family 
> = binomial)
> 
> Individually, I get results from anova():
> 
> > anova(fm1)
>             numDF denDF   F-value p-value
> (Intercept)     1  2032   7.95709  0.0048
> day             1  2032 213.98391  <.0001
> stereotypy      1  2032   0.42810  0.5130
> >
> > anova(fm2)
>             numDF denDF   F-value p-value
> (Intercept)     1  2031   5.70343  0.0170
> day             1  2031 213.21673  <.0001
> envir           1  2031  12.50388  0.0004
> stereotypy      1  2031   0.27256  0.6017
> >
> 
> I did look through the archives but didn't finding anything 
> relevant to my problem.
> 
> Hope someone can help.
> 
> ANDREW
> ____________________________
>          _
> platform i586-mandrake-linux-gnu
> arch     i586
> os       linux-gnu
> system   i586, linux-gnu
> status
> major    2
> minor    0.0
> year     2004
> month    10
> day      04
> language R
> 
> 
> 
> -- 
> Andrew R. Criswell, Ph.D.
> Graduate School, Bangkok University
> 
> mailto:andrew.c at bu.ac.th 
> <http://email.bu.ac.th/src/compose.php?send_to=andrew.c%40bu.ac.th>
> mailto:andrew at arcriswell.com 
> <http://email.bu.ac.th/src/compose.php?send_to=andrew%40arcris
well.com>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Thu Nov 25 04:50:28 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 25 Nov 2004 03:50:28 +0000 (UTC)
Subject: [R] seriesMerge
References: <b1d31504041124122950a6efce@mail.gmail.com>
Message-ID: <loom.20041125T042831-247@post.gmane.org>

Yasser El-Zein <abu3ammar <at> gmail.com> writes:

: 
: Is there a function in R that is equivalent to S-PLUS's 
: seriesMerge(x1, x2, pos="union")
: where x1, and x2 are of class timeSeries
: 
: seriesMerge is in S-PLUS's finmetrics. I looked into R's mergeSeries
: (in fSeries part of Rmetrics) but I could not make it behave quite the
: same. In R it expected a timeSeries object and a matrix of the same
: row count. In S-PLUS when using the union option both objects can be
: of different lengths.

merge.zoo in package zoo handles union, intersection, left
and right join of unequal length time series according to the 
setting of the all= argument.  zoo can also work with chron dates 
and times which would allow you to work with your millisecond data
and can also merge more than two series at a time.   (The its 
package (see ?itsJoin) and for regular time series, cbind.ts, also 
support merging unequal length series but neither of these support 
chron which I gather is a requirement for you.)

eg. zoo example.
In the following x has  length 8 and y has length 6 and
they overlap for chron(5:8).  chron(1:4) only belongs
to x and chron(9:10) only belongs to y.

library(chron)
library(zoo)
x <- zoo(1:8, chron(1:8)) 
y <- zoo(5:10, chron(5:10))
merge(x,y) # union
merge(x,y,all=FALSE) # intersection
merge(x,y,all=c(FALSE, TRUE)) # right join
merge(x,y,all=c(TRUE, FALSE)) # left join



From ggrothendieck at myway.com  Thu Nov 25 05:22:58 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 25 Nov 2004 04:22:58 +0000 (UTC)
Subject: [R] Searching for antilog function
References: <41A4C456.308@pdq.com> <l9o9q0lm2j1fi046pb3irlo1vhfcv76v9l@4ax.com>
	<41A4E32B.30300@pdq.com>
Message-ID: <loom.20041125T051716-875@post.gmane.org>

Heather J. Branton <hjb <at> pdq.com> writes:

: 
: Thank you so much for each of your responses. But to make sure I am 
: clear (in my own mind), is this correct?
: 
: If  x = 2^y
: Then  y = log2(x)
: 
: Thanks again. I know this is basic.

Although its not a proof, you can still use R to help you
verify such hypotheses.  Just use actual vectors of numbers 
and check that your hypothesis, in this case y equals log2(x),
holds.

For example,

R> # try it out with the vector 1,2,3,...,10
R> y <- 1:10
R> y
 [1]  1  2  3  4  5  6  7  8  9 10
R> # now calculate x
R> x <- log2(y)
R> # lets see what 2^x looks like:
R> 2^x 
 [1]  1  2  3  4  5  6  7  8  9 10
R> # it gave back y!



From ripley at stats.ox.ac.uk  Thu Nov 25 08:31:08 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 25 Nov 2004 07:31:08 +0000 (GMT)
Subject: [R] Installing gregmisc under windows 2000
In-Reply-To: <20041125102507.5274bfa3.yzhang4@turing.une.edu.au>
References: <1101320697.25550.209392197@webmail.messagingengine.com>
	<1101329880.22170.71.camel@horizons.localdomain>
	<001201c4d26f$54ccd790$2781010a@BigBaer>
	<Pine.LNX.4.61.0411242211300.9604@gannet.stats>
	<20041125102507.5274bfa3.yzhang4@turing.une.edu.au>
Message-ID: <Pine.LNX.4.61.0411250728310.15247@gannet.stats>

On Thu, 25 Nov 2004, Yuandan Zhang wrote:

> That seems not the case under linux in term of installation. You can install this bundle in the same way as installing an individul package.
>
> eg
>
> R CMD INSTALL  CRAN/contrib/main/gregmisc_2.0.0.tar.gz
>
> to get all constituent packages installed.

And the same under Windows.  Please read the rest of the message you 
silently excised.

As gregmisc is not one of the constituent packages, library(gregmisc) does 
not work under Linux.

What exactly were you trying to `correct'?


> On Wed, 24 Nov 2004 22:14:14 +0000 (GMT)
> Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
>
>> gregmisc is a bundle, not a package.  Its description on CRAN is
>>
>> gregmisc	Bundle of gtools, gdata, gmodels, gplots
>>
>> so try one of those packages.

[Important details removed here.]

>
>
> -- 
>
> --
> Yuandan Zhang, PhD
>
> Animal Genetics and Breeding Unit
> The University of New England
> Armidale, NSW, Australia, 2351
>
> E-mail:   yzhang4 at metz.une.edu.au
> Phone:    (61) 02 6773 3786
> Fax:      (61) 02 6773 3266
> http://agbu.une.edu.au
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>  AGBU is a joint venture of NSW Primary Industries
>  and The University of New England to undertake
>  genetic R&D for Australia's Livestock Industries
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From wolski at molgen.mpg.de  Thu Nov 25 09:33:46 2004
From: wolski at molgen.mpg.de (Witold Eryk Wolski)
Date: Thu, 25 Nov 2004 09:33:46 +0100
Subject: [R] scatterplot of 100000 points and pdf file format
In-Reply-To: <Pine.LNX.4.61.0411241646080.2681@gannet.stats>
References: <XFMail.041124161628.Ted.Harding@nessie.mcc.ac.uk>
	<Pine.LNX.4.61.0411241646080.2681@gannet.stats>
Message-ID: <41A598EA.2080301@molgen.mpg.de>

Prof Brian Ripley wrote:

> On Wed, 24 Nov 2004 Ted.Harding at nessie.mcc.ac.uk wrote:
>
>> On 24-Nov-04 Witold Eryk Wolski wrote:
>>
>>> Hi,
>>> I want to draw a scatter plot with 1M  and more points
>>> and save it as pdf.
>>> This makes the pdf file large.
>>> So i tried to save the file first as png and than convert
>>> it to pdf. This looks OK if printed but if viewed e.g. with
>>> acrobat as document figure the quality is bad.
>>>
>>> Anyone knows a way to reduce the size but keep the quality?
>>
>>
>> If you want the PDF file to preserve the info about all the
>> 1M points then the problem has no solution. The png file
>> will already have suppressed most of this (which is one
>> reason for poor quality).
>>
>> I think you should give thought to reducing what you need
>> to plot.
>>
>> Think about it: suppose you plot with a resolution of
>> 1/200 points per inch (about the limit at which the eye
>> begins to see rough edges). Then you have 40000 points
>> per square inch. If your 1M points are separate but as
>> closely packed as possible, this requires 25 square inches,
>> or a 5x5 inch (= 12.7x12.7 cm) square. And this would be
>> solid black!
>>
>> Presumably in your plot there is a very large number of
>> points which are effectively indistinguisable from other
>> points, so these could be eliminated without spoiling
>> the plot.
>>
>> I don't have an obviously best strategy for reducing what
>> you actually plot, but perhaps one line to think along
>> might be the following:
>>
>> 1. Multiply the data by some factor and then round the
>>   results to an integer (to avoid problems in step 2).
>>   Factor chosen so that the result of (4) below is
>>   satisfactory.
>>
>> 2. Eliminate duplicates in the result of (1).
>>
>> 3. Divide by the factor you used in (1).
>>
>> 4. Plot the result; save plot to PDF.
>>
>> As to how to do it in R: the critical step is (2),
>> which with so many points could be very heavy unless
>> done by a well-chosen procedure. I'm not expert enough
>> to advise about that, but no doubt others are.
>
>
> unique will eat that for breakfast
>
>> x <- runif(1e6)
>> system.time(xx <- unique(round(x, 4)))
>
> [1] 0.55 0.09 0.64 0.00 0.00
>
>> length(xx)
>
> [1] 10001
>
>


?table -> reduces the data
and
?image -> shows it.
And this is doing exactly what I need. (not my idea but one of Thomas 
Untern??her).  Thanks Thomas.


/E

-- 
Dipl. bio-chem. Witold Eryk Wolski
MPI-Moleculare Genetic
Ihnestrasse 63-73 14195 Berlin
tel: 0049-30-83875219                 __("<    _
http://www.molgen.mpg.de/~wolski      \__/    'v'
http://r4proteomics.sourceforge.net    ||    /   \
mail: witek96 at users.sourceforge.net    ^^     m m
      wolski at molgen.mpg.de



From maechler at stat.math.ethz.ch  Thu Nov 25 09:47:29 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 25 Nov 2004 09:47:29 +0100
Subject: [R] what does order() stand for in an lme formula?
In-Reply-To: <x2u0rf3xj2.fsf@biostat.ku.dk>
References: <004701c4d247$4899b380$6601a8c0@SALAMINIA>
	<x2u0rf3xj2.fsf@biostat.ku.dk>
Message-ID: <16805.39969.709630.570500@gargle.gargle.HOWL>

>>>>> "PD" == Peter Dalgaard <p.dalgaard at biostat.ku.dk>
>>>>>     on 24 Nov 2004 19:35:45 +0100 writes:

    PD> "Harry Athanassiou" <hathanassiou at automatedcell.com>
    PD> writes:
    >> I'm a beginner in R, and trying to fit linear models with
    >> different intercepts per group, of the type y ~ A*x1 + B,
    >> where x1 is a numerical variable. I cannot understand
    >> whether I should use y1 ~ x1 +1 or y1 ~ order(x1) + 1

    >> Although in the toy example included it makes a small
    >> difference, in models with many groups the models without
    >> order() converge slower if at all!

    PD> Er?

    PD> What gave you the idea of using order in the first
    PD> place? To the best of my knowledge, order(x) is also in
    PD> this context just a function, which for the nth
    PD> observation returns the position of the nth largest
    PD> observation in x. This is not likely to make sense as a
    PD> predictor in a model.

where on the other hand,  
      rank(x1)
may make sense and what Harry really intended to use.

Martin Maechler, ETH Zurich



From alexandersokol at ofir.dk  Thu Nov 25 10:12:28 2004
From: alexandersokol at ofir.dk (Alexander Sokol)
Date: Thu, 25 Nov 2004 10:12:28 +0100
Subject: [R] Creating lists from matrices
Message-ID: <41A752C2@webmail2.ofir.dk>

Hello,

I am using R 1.9.1 on Windows 2000 SP4. I have the following problem:

Say I have a matrix,

>my.matrix
       [,1]   [,2]   [,3]
[1,]   "A"   "B"   "C"
[2,]   "D"   "E"   "F"
[3,]   "G"   "H"   "I"

I would like to apply an operation to this matrix which returns a list my.list 
containing the following 3 elements,

>my.list
[[1]]
[1] "A" "B" "C"
[[2]]
[2] "D" "E" "F"
[[3]]
[3] "G" "H" "I"

That is, each row of the original matrix is turned into a vector and these 
vectors are collected to a list. How do I do this?

Thanks,
 Alexander



From Jean.Coursol at math.u-psud.fr  Thu Nov 25 10:23:51 2004
From: Jean.Coursol at math.u-psud.fr (Jean Coursol)
Date: Thu, 25 Nov 2004 10:23:51 +0100 (CET)
Subject: [R] Danish characters i R2.0.1 vs R1.9.1 under winXP
In-Reply-To: <BAY1-DAV18752B378D942FD7000ECA9FB90@phx.gbl>
Message-ID: <200411250923.KAA16928@jacaranda.math.u-psud.fr>

The same is true in french under linux. Something changed
from 1.9.1 to 2.0.0.

First, it is necessary to have .inputrc (in $HOME) 
(or $INPUTRC defined) to enter and display 8-bits 
characters under bash and R.

#.inputrc (for readline library)
set input-meta on
set output-meta on
set convert-meta off

Then

Under R2.0.1, I have:

> ??l??ment <- "??"           # error for object name
Error: syntax error
> element <- "??"           
> element 
[1] "\351"                 # different from R1.9.1 (="??")

> Sys.setlocale('LC_ALL','fr_FR')
[1] "fr_FR"
> ??l??ment <- "??"           # OK for object name
> ??l??ment     
[1] "??"                    # OK for display

Another solution:

export LC_ALL='fr_FR'      # before loading R

and then Sys.setlocale becomes useless.

Jean Coursol



From p.dalgaard at biostat.ku.dk  Thu Nov 25 10:22:28 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 25 Nov 2004 10:22:28 +0100
Subject: [R] Creating lists from matrices
In-Reply-To: <41A752C2@webmail2.ofir.dk>
References: <41A752C2@webmail2.ofir.dk>
Message-ID: <x2ekiinuzv.fsf@biostat.ku.dk>

Alexander Sokol <alexandersokol at ofir.dk> writes:

> [1] "A" "B" "C"
> [[2]]
> [2] "D" "E" "F"
> [[3]]
> [3] "G" "H" "I"
> 
> That is, each row of the original matrix is turned into a vector and these 
> vectors are collected to a list. How do I do this?

split(my.matrix, row(my.matrix))

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From dimitris.rizopoulos at med.kuleuven.ac.be  Thu Nov 25 10:28:11 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Thu, 25 Nov 2004 10:28:11 +0100
Subject: [R] Creating lists from matrices
References: <41A752C2@webmail2.ofir.dk>
Message-ID: <00e001c4d2d1$1503b9a0$0540210a@www.domain>

Hi Alexander,

you could try this:

(my.matrix <- matrix(LETTERS[1:9], 3, byrow=TRUE))
split(my.matrix, row(my.matrix))


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Alexander Sokol" <alexandersokol at ofir.dk>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, November 25, 2004 10:12 AM
Subject: [R] Creating lists from matrices


> Hello,
>
> I am using R 1.9.1 on Windows 2000 SP4. I have the following 
> problem:
>
> Say I have a matrix,
>
>>my.matrix
>       [,1]   [,2]   [,3]
> [1,]   "A"   "B"   "C"
> [2,]   "D"   "E"   "F"
> [3,]   "G"   "H"   "I"
>
> I would like to apply an operation to this matrix which returns a 
> list my.list
> containing the following 3 elements,
>
>>my.list
> [[1]]
> [1] "A" "B" "C"
> [[2]]
> [2] "D" "E" "F"
> [[3]]
> [3] "G" "H" "I"
>
> That is, each row of the original matrix is turned into a vector and 
> these
> vectors are collected to a list. How do I do this?
>
> Thanks,
> Alexander
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Thu Nov 25 10:31:11 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 25 Nov 2004 09:31:11 +0000 (GMT)
Subject: [R] Creating lists from matrices
In-Reply-To: <41A752C2@webmail2.ofir.dk>
References: <41A752C2@webmail2.ofir.dk>
Message-ID: <Pine.LNX.4.61.0411250928510.25197@gannet.stats>

my.matrix <- matrix(LETTERS[1:9],3,3,byrow=TRUE)
split(my.matrix, row(my.matrix))

$"1"
[1] "A" "B" "C"

$"2"
[1] "D" "E" "F"

$"3"
[1] "G" "H" "I"

which even names the rows for you.

On Thu, 25 Nov 2004, Alexander Sokol wrote:

> Hello,
>
> I am using R 1.9.1 on Windows 2000 SP4. I have the following problem:
>
> Say I have a matrix,
>
>> my.matrix
>       [,1]   [,2]   [,3]
> [1,]   "A"   "B"   "C"
> [2,]   "D"   "E"   "F"
> [3,]   "G"   "H"   "I"
>
> I would like to apply an operation to this matrix which returns a list my.list
> containing the following 3 elements,
>
>> my.list
> [[1]]
> [1] "A" "B" "C"
> [[2]]
> [2] "D" "E" "F"
> [[3]]
> [3] "G" "H" "I"
>
> That is, each row of the original matrix is turned into a vector and these
> vectors are collected to a list. How do I do this?
>
> Thanks,
> Alexander
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From lecoutre at stat.ucl.ac.be  Thu Nov 25 10:24:52 2004
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Thu, 25 Nov 2004 10:24:52 +0100
Subject: [R] Creating lists from matrices
In-Reply-To: <41A752C2@webmail2.ofir.dk>
References: <41A752C2@webmail2.ofir.dk>
Message-ID: <6.0.1.1.2.20041125102113.02075ec0@stat4ux.stat.ucl.ac.be>


Hi,

One could use the following:

 >  mm=matrix(letters[1:9],ncol=3,byrow=TRUE)
 >  lapply(apply(mm,1,list),function(el)el[[1]])
[[1]]
[1] "a" "b" "c"

[[2]]
[1] "d" "e" "f"

[[3]]
[1] "g" "h" "i"

You could also have a look at as.data.frame.matrix, which transform a 
matrix into a data.frame efficiently. data.frames are internaly structured 
as lists...

Eric


At 10:12 25/11/2004, Alexander Sokol wrote:
>Hello,
>
>I am using R 1.9.1 on Windows 2000 SP4. I have the following problem:
>
>Say I have a matrix,
>
> >my.matrix
>        [,1]   [,2]   [,3]
>[1,]   "A"   "B"   "C"
>[2,]   "D"   "E"   "F"
>[3,]   "G"   "H"   "I"
>
>I would like to apply an operation to this matrix which returns a list 
>my.list
>containing the following 3 elements,
>
> >my.list
>[[1]]
>[1] "A" "B" "C"
>[[2]]
>[2] "D" "E" "F"
>[[3]]
>[3] "G" "H" "I"
>
>That is, each row of the original matrix is turned into a vector and these
>vectors are collected to a list. How do I do this?
>
>Thanks,
>  Alexander
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Eric Lecoutre
UCL /  Institut de Statistique
Voie du Roman Pays, 20
1348 Louvain-la-Neuve
Belgium

tel: (+32)(0)10473050
lecoutre at stat.ucl.ac.be
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre

If the statistics are boring, then you've got the wrong numbers. -Edward 
Tufte



From ripley at stats.ox.ac.uk  Thu Nov 25 10:48:30 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 25 Nov 2004 09:48:30 +0000 (GMT)
Subject: [R] Danish characters i R2.0.1 vs R1.9.1 under winXP
In-Reply-To: <200411250923.KAA16928@jacaranda.math.u-psud.fr>
References: <200411250923.KAA16928@jacaranda.math.u-psud.fr>
Message-ID: <Pine.LNX.4.61.0411250931410.25197@gannet.stats>

On Thu, 25 Nov 2004, Jean Coursol wrote:

> The same is true in french under linux.

No, it is not the same.  Windows XP does this even in the Danish locale, 
and that is a problem (in Windows).

Your problem is simply that you should be using a French locale to use 
French characters.  You should not expect French characters to be 
recognised in a non-French locale, and if they were, that was a bug in R 
1.9.1.

From ?Sys.setlocale

      The locale describes aspects of the internationalization of a
      program. Initially most aspects of the locale of R are set to
      '"C"' (which is the default for the C language and reflects
      North-American usage). R does set '"LC_CTYPE"' and '"LC_COLLATE"',
      which allow the use of a different character set (typically ISO
      Latin 1) and alphabetic comparisons in that character set
      (including the use of 'sort') ....

> Something changed
> from 1.9.1 to 2.0.0.

Yes, that has already be explained in this thread, so please read earlier 
replies.

To summarize: a bug has been corrected so R now works as it has always 
been documented to do, print()ing only the printable characters of the 
current locale.  Unfortunately for German and Scandinavian locales (at 
least), Windows XP does not correctly identify some of the characters in 
their locales as used in the locale.  As from 2.0.1 patched, we no longer 
believe Windows, but we do still believe other OSes.


> First, it is necessary to have .inputrc (in $HOME)
> (or $INPUTRC defined) to enter and display 8-bits
> characters under bash and R.

Enter from the console, yes, but not e.g. from a file.

> #.inputrc (for readline library)
> set input-meta on
> set output-meta on
> set convert-meta off
>
> Then
>
> Under R2.0.1, I have:

In what locale?  It matters!

>> ?l?ment <- "?"           # error for object name
> Error: syntax error
>> element <- "?"
>> element
> [1] "\351"                 # different from R1.9.1 (="?")
>
>> Sys.setlocale('LC_ALL','fr_FR')

The setting you need is LC_CTYPE: see the help page.

> [1] "fr_FR"
>> ?l?ment <- "?"           # OK for object name
>> ?l?ment
> [1] "?"                    # OK for display

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From christian.hoffmann at wsl.ch  Thu Nov 25 11:17:28 2004
From: christian.hoffmann at wsl.ch (Christian Hoffmann)
Date: Thu, 25 Nov 2004 11:17:28 +0100
Subject: [R] eval in correct frame?
Message-ID: <41A5B138.8020001@wsl.ch>

I am trying, without success, to find out how to formulate correctly the 
parameters of "eval".

My code snippet looks like:

proutside <- function(txt) {
   cat("\n",txt,"\n");
   print(eval(parse(text = txt)))
}

vari <- function(Ob) {
   prininside <- function(txt) {
     cat("\n",txt,"\n");
     print(eval(parse(text = txt)))
   }
   prininside("'inside'; 2*Ob")
   proutside("'outside'; 2*Ob")
}
Obs <- matrix(rnorm(9),nrow=3,ncol=3)
 > vari(Obs)

  'inside'; 2*Ob
           [,1]     [,2]     [,3]
[1,] -1.566331 -1.98257 0.127522
[2,]  3.161932 -4.88416 2.355412
[3,] -0.763759  2.33552 2.165868

  'outside'; 2*Ob
Error in eval(expr, envir, enclos) : Object "Ob" not found
 >

My aim is to be able to use a function of type "proutside" as deep 
inside a function within a function... as I wish. I suppose, that the 
call of "eval" within "proutside" should have "envir = parent.frame(), 
enclos = ", but I cannot figure that out correctly.

Thank for any help.
Christian

-- 
Dr.sc.math.Christian W. Hoffmann, 
http://www.wsl.ch/staff/christian.hoffmann
Mathematics + Statistical Computing   e-mail: christian.hoffmann at wsl.ch
Swiss Federal Research Institute WSL  Tel: ++41-44-73922-   -77  (office)
CH-8903 Birmensdorf, Switzerland             -11(exchange), -15  (fax)



From p.dalgaard at biostat.ku.dk  Thu Nov 25 11:29:23 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 25 Nov 2004 11:29:23 +0100
Subject: [R] eval in correct frame?
In-Reply-To: <41A5B138.8020001@wsl.ch>
References: <41A5B138.8020001@wsl.ch>
Message-ID: <x2pt22te64.fsf@biostat.ku.dk>

Christian Hoffmann <christian.hoffmann at wsl.ch> writes:

> I am trying, without success, to find out how to formulate correctly
> the parameters of "eval".
> 
> My code snippet looks like:
> 
> proutside <- function(txt) {
>    cat("\n",txt,"\n");
>    print(eval(parse(text = txt)))
> }
> 
> vari <- function(Ob) {
>    prininside <- function(txt) {
>      cat("\n",txt,"\n");
>      print(eval(parse(text = txt)))
>    }
>    prininside("'inside'; 2*Ob")
>    proutside("'outside'; 2*Ob")
> }
> Obs <- matrix(rnorm(9),nrow=3,ncol=3)
>  > vari(Obs)
> 
>   'inside'; 2*Ob
>            [,1]     [,2]     [,3]
> [1,] -1.566331 -1.98257 0.127522
> [2,]  3.161932 -4.88416 2.355412
> [3,] -0.763759  2.33552 2.165868
> 
>   'outside'; 2*Ob
> Error in eval(expr, envir, enclos) : Object "Ob" not found
>  >
> 
> My aim is to be able to use a function of type "proutside" as deep
> inside a function within a function... as I wish. I suppose, that the
> call of "eval" within "proutside" should have "envir = parent.frame(),
> enclos = ", but I cannot figure that out correctly.

I think you are looking for eval.parent() (in both cases; try using
prinside on something called "txt")

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Thu Nov 25 11:53:03 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 25 Nov 2004 10:53:03 +0000 (GMT)
Subject: [R] Running R from CD?
In-Reply-To: <Pine.LNX.4.61.0411220922490.24431@gannet.stats>
References: <20041122004120.67125.qmail@web50306.mail.yahoo.com>
	<1101112947.3798.6.camel@biol102145.oulu.fi>
	<Pine.LNX.4.61.0411220922490.24431@gannet.stats>
Message-ID: <Pine.LNX.4.61.0411251032450.25900@gannet.stats>

On Mon, 22 Nov 2004, Prof Brian Ripley wrote:

[...]

> BTW, I believe running R 2.0.x from a CD will be a lot slower than 1.9.1
> because of lazy loading and frequent file accesses: that's a theoretical 
> issue we intend to address for 2.1.0, but not one anyone has yet commented 
> that it is a problem.

I collected some data (under Windows XP).

On a modern desktop, running R from a CD-R or from a USB 2.0 thumbdrive 
was perfectably acceptable, with startup times of about 5 secs and little 
delay when running.

On a 2.5year old laptop with a USB 1.1 port (but the same thumbdrive) it 
took about 15secs to start and with frequent delays the first time an 
object was used -- I would not find that tolerable.  The laptop's CD drive 
was slower than the desktop and there were delays when it powered down, 
but it was acceptable.

This was less performance penalty than I was expecting, and less than I 
have seen on a high-latency network file system. So it looks as if all we 
can do is trade a slower startup time (by caching files) for removing 
hiatuses when running.  (Caching the pkg.rdb and pkg.rdx files when a 
package is opened would probably only take up a little over 1Mb in a 
typical session.)

Writing to the thumbdrive took about 20mins, as R has so many small files
and the drive has a VFAT file system.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From manuel_gutierrez_lopez at yahoo.es  Thu Nov 25 12:17:25 2004
From: manuel_gutierrez_lopez at yahoo.es (Manuel Gutierrez)
Date: Thu, 25 Nov 2004 12:17:25 +0100 (CET)
Subject: [R] substitute accents
Message-ID: <20041125111725.37782.qmail@web25109.mail.ukl.yahoo.com>

I have an openoffice spreadsheet with a column of
character strings.
Some of them contain accents.
I want to read it in R so I have saved it as a csv
file using Western Europe (ISO-8859-1) character set
(the default, I've tried other sets but it doesn't
help).
R reads it fine with 

CharMatrix<-read.csv("test.csv",header=F,sep=",",as.is=TRUE);
Say I wan't to replace the 'o' with accent in the
first cell.
I've tried:
gsub('??','o', CharMatrix[1,1])
But, It doesn't make any substitution

Trying to find a solution I input the character string
in R and do the substitution:
CharMatrix[1,1]<-"h??la"
gsub('??','o', CharMatrix[1,1])
And it works. I think the difference is that when I
now print the content of CharMatrix I get a \201
before the ?? while I didn't get it with the openoffice
imported csv file.
I'm sure it is a problem with my understanding of how
accents can be specified. Can someone give me any
solutions / references?
Thanks,
M

         _                
platform i686-pc-linux-gnu
arch     i686             
os       linux-gnu        
system   i686, linux-gnu  
status                    
major    2                
minor    0.0              
year     2004             
month    10               
day      04               
language R   
		
______________________________________________


From Michael.Griffiths at lgc.co.uk  Thu Nov 25 12:20:56 2004
From: Michael.Griffiths at lgc.co.uk (Michael Griffiths)
Date: Thu, 25 Nov 2004 11:20:56 +0000
Subject: [R] help with error message
Message-ID: <s1a5c027.053@tedmail2.lgc.co.uk>

Before I receive a barrage of 'try looking in the help file' messages, I
have and to no avail. For a new user of R I would like to point out that
in order to be able to use the help files/manuals effectively one must
know the correct question and that only comes with using R!

Could someone please direct me to why I keep getting the following
error message

"Error: subscript out of bounds"

when the following code is run

z.score<-function (group)	{
	for (t in levels(group$Country))	{ 	# this will give
t countries
		y<-subset(group,factor(Country)==t)	#particular
analyte over all countries equal to y

			#calculate overall huber mean and sigma for a
particular analyte over all countries
			ov.mu<-hubers(group$X)$mu
			ov.sigma<-hubers(group$X)$s

			#define arrays
			#p.mu<-array()
			#p.sigma<-array()
			#z.value<-array()

			#calculate huber mean and sigma for given
analyte (defined by group) by selected country (y)
			p.mu<-hubers(y$X)$mu
			#p.sigma<-hubers(y$X)$s

			#calculate z score for particular
analyte:country combination			
			#z.value<-as.vector((p.mu[t]-ov.mu)/ov.sigma)
	}
		#data<-list(mean=ov.mu,sd=ov.sigma)
		return(p.mu)
}

	

# group entered as group=subset(sub2, factor(Analyte)=="Cholesterol")
for example
#
#
#
#
#


Please ignore the commented out lines, these were put in for my own
use. The code gave the same error message with them removed.

Thankyou for your help

Mike Griffiths


Michael Griffiths, Ph.D.
Chemometrician
Training, Quality and Statistics Group
LGC Limited
Queens Road
Teddington
Middlesex, TW11 0LY, UK
Tel: +44 (0)20 8943 7352
Fax: +44 (0)20 8943 2767
e-mail: michael.griffiths at lgc.co.uk
*******************************************************************
This email and any attachments are confidential. Any use, co...{{dropped}}



From ripley at stats.ox.ac.uk  Thu Nov 25 12:45:58 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 25 Nov 2004 11:45:58 +0000 (GMT)
Subject: [R] substitute accents
In-Reply-To: <20041125111725.37782.qmail@web25109.mail.ukl.yahoo.com>
References: <20041125111725.37782.qmail@web25109.mail.ukl.yahoo.com>
Message-ID: <Pine.LNX.4.61.0411251142050.26493@gannet.stats>

Can you please tell us what locale you are working in?

This looks as if the problem might be the use of a UTF-8 locale, which R 
does not currently support and which some Linux distros have made their 
default.  However, R does issue a warning -- so did you get one?

On Thu, 25 Nov 2004, Manuel Gutierrez wrote:

> I have an openoffice spreadsheet with a column of
> character strings.
> Some of them contain accents.
> I want to read it in R so I have saved it as a csv
> file using Western Europe (ISO-8859-1) character set
> (the default, I've tried other sets but it doesn't
> help).
> R reads it fine with
>
> CharMatrix<-read.csv("test.csv",header=F,sep=",",as.is=TRUE);
> Say I wan't to replace the 'o' with accent in the
> first cell.
> I've tried:
> gsub('?','o', CharMatrix[1,1])
> But, It doesn't make any substitution
> Trying to find a solution I input the character string
> in R and do the substitution:
> CharMatrix[1,1]<-"h?la"
> gsub('?','o', CharMatrix[1,1])
> And it works. I think the difference is that when I
> now print the content of CharMatrix I get a \201
> before the ? while I didn't get it with the openoffice
> imported csv file.
> I'm sure it is a problem with my understanding of how
> accents can be specified. Can someone give me any
> solutions / references?
> Thanks,
> M
>
>         _
> platform i686-pc-linux-gnu
> arch     i686
> os       linux-gnu
> system   i686, linux-gnu
> status
> major    2
> minor    0.0
> year     2004
> month    10
> day      04
> language R
>
>
>
>
>
> ______________________________________________
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From Michael.Griffiths at lgc.co.uk  Thu Nov 25 12:56:25 2004
From: Michael.Griffiths at lgc.co.uk (Michael Griffiths)
Date: Thu, 25 Nov 2004 11:56:25 +0000
Subject: [R] help with error message - problem solved
Message-ID: <s1a5c878.017@tedmail2.lgc.co.uk>

Apologies to the listing, the problem was with the data set and not the
code

Thanks


Michael Griffiths, Ph.D.
Chemometrician
Training, Quality and Statistics Group
LGC Limited
Queens Road
Teddington
Middlesex, TW11 0LY, UK
Tel: +44 (0)20 8943 7352
Fax: +44 (0)20 8943 2767
e-mail: michael.griffiths at lgc.co.uk

>>> "Michael Griffiths" <Michael.Griffiths at lgc.co.uk> 25/11/2004
11:20:56 >>>
Before I receive a barrage of 'try looking in the help file' messages,
I
have and to no avail. For a new user of R I would like to point out
that
in order to be able to use the help files/manuals effectively one must
know the correct question and that only comes with using R!

Could someone please direct me to why I keep getting the following
error message

"Error: subscript out of bounds"

when the following code is run

z.score<-function (group)	{
	for (t in levels(group$Country))	{ 	# this will
give
t countries
		y<-subset(group,factor(Country)==t)	#particular
analyte over all countries equal to y

			#calculate overall huber mean and sigma for a
particular analyte over all countries
			ov.mu<-hubers(group$X)$mu
			ov.sigma<-hubers(group$X)$s

			#define arrays
			#p.mu<-array()
			#p.sigma<-array()
			#z.value<-array()

			#calculate huber mean and sigma for given
analyte (defined by group) by selected country (y)
			p.mu<-hubers(y$X)$mu
			#p.sigma<-hubers(y$X)$s

			#calculate z score for particular
analyte:country combination			
			#z.value<-as.vector((p.mu[t]-ov.mu)/ov.sigma)
	}
		#data<-list(mean=ov.mu,sd=ov.sigma)
		return(p.mu)
}

	

# group entered as group=subset(sub2, factor(Analyte)=="Cholesterol")
for example
#
#
#
#
#


Please ignore the commented out lines, these were put in for my own
use. The code gave the same error message with them removed.

Thankyou for your help

Mike Griffiths


Michael Griffiths, Ph.D.
Chemometrician
Training, Quality and Statistics Group
LGC Limited
Queens Road
Teddington
Middlesex, TW11 0LY, UK
Tel: +44 (0)20 8943 7352
Fax: +44 (0)20 8943 2767
e-mail: michael.griffiths at lgc.co.uk 
*******************************************************************
This email and any attachments are confidential. Any use,\ c...{{dropped}}



From f.gherardini at pigrecodata.net  Thu Nov 25 14:00:49 2004
From: f.gherardini at pigrecodata.net (Federico Gherardini)
Date: Thu, 25 Nov 2004 14:00:49 +0100
Subject: [R] Error using glm with poisson family and identity link
Message-ID: <41A5D781.70400@pigrecodata.net>

Hi all
I'm trying to use the function glm from the MASS package to do the 
following fit.

fit <- glm(FP ~ rand, data = tab, family = poisson(link = "identity"), 
subset = rand >= 1)
(FP is >= 0)

but I get the following error

Error: no valid set of coefficients has been found:please supply 
starting values
In addition: Warning message:
NaNs produced in: log(x)

in contrast if I fit a model without intercept

fit <- glm(FP ~ rand - 1, data = tab, family = poisson(link = 
"identity"), subset = rand >= 1)

everything goes fine.
Now my guess is that the points "naturally" have a negative intercept so 
the error is produced because I'm using the poisson distribution for the 
y and negative values are of course not admitted. Am I right?
Also if this is the cause, shouldn't the function always try to do the 
best fit given the parameters? I mean shouldn't it fit a model with 
intercept 0 anyway and report it as a bad fit?

Thanks

Federico



From vito_ricci at yahoo.com  Thu Nov 25 13:15:37 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Thu, 25 Nov 2004 13:15:37 +0100 (CET)
Subject: [R] R vs SPSS
Message-ID: <20041125121537.22285.qmail@web41212.mail.yahoo.com>

Dear all,

in last weeks you discussed about R vs SAS. 
I want to ask your opinion about a comparison between
R and SPSS. I don't know this software, but some weeks
ago I went to a presentation of this product. I found
it really user-friendly with GUI (even if I'd prefer
command line) and very usefull and simple to use in
creation and managing tables, OLAP tecniques, pivot
table.
What you think about?
Cordially
Vito

=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/



From manuel_gutierrez_lopez at yahoo.es  Thu Nov 25 13:18:08 2004
From: manuel_gutierrez_lopez at yahoo.es (Manuel Gutierrez)
Date: Thu, 25 Nov 2004 13:18:08 +0100 (CET)
Subject: [R] substitute accents
In-Reply-To: <Pine.LNX.4.61.0411251142050.26493@gannet.stats>
Message-ID: <20041125121808.27990.qmail@web25103.mail.ukl.yahoo.com>

$ locale
LANG=en_GB
LC_CTYPE="en_GB"
LC_NUMERIC="en_GB"
LC_TIME="en_GB"
LC_COLLATE="en_GB"
LC_MONETARY="en_GB"
LC_MESSAGES="en_GB"
LC_PAPER="en_GB"
LC_NAME="en_GB"
LC_ADDRESS="en_GB"
LC_TELEPHONE="en_GB"
LC_MEASUREMENT="en_GB"
LC_IDENTIFICATION="en_GB"
LC_ALL=

  
$ locale charmap
ISO-8859-1

I have tried changing the locales with no difference.
Is this fine?
And, no, I didn't get any warning message.
My sistem is a debian sid under kde 3.3.
Thanks,
M

 --- Prof Brian Ripley <ripley at stats.ox.ac.uk>
escribi??: 
> Can you please tell us what locale you are working
> in?
> 
> This looks as if the problem might be the use of a
> UTF-8 locale, which R 
> does not currently support and which some Linux
> distros have made their 
> default.  However, R does issue a warning -- so did
> you get one?
> 
> On Thu, 25 Nov 2004, Manuel Gutierrez wrote:
> 
> > I have an openoffice spreadsheet with a column of
> > character strings.
> > Some of them contain accents.
> > I want to read it in R so I have saved it as a csv
> > file using Western Europe (ISO-8859-1) character
> set
> > (the default, I've tried other sets but it doesn't
> > help).
> > R reads it fine with
> >
> >
>
CharMatrix<-read.csv("test.csv",header=F,sep=",",as.is=TRUE);
> > Say I wan't to replace the 'o' with accent in the
> > first cell.
> > I've tried:
> > gsub('??','o', CharMatrix[1,1])
> > But, It doesn't make any substitution
> > Trying to find a solution I input the character
> string
> > in R and do the substitution:
> > CharMatrix[1,1]<-"h??la"
> > gsub('??','o', CharMatrix[1,1])
> > And it works. I think the difference is that when
> I
> > now print the content of CharMatrix I get a \201
> > before the ?? while I didn't get it with the
> openoffice
> > imported csv file.
> > I'm sure it is a problem with my understanding of
> how
> > accents can be specified. Can someone give me any
> > solutions / references?
> > Thanks,
> > M
> >
> >         _
> > platform i686-pc-linux-gnu
> > arch     i686
> > os       linux-gnu
> > system   i686, linux-gnu
> > status
> > major    2
> > minor    0.0
> > year     2004
> > month    10
> > day      04
> > language R
> >
> >
> >
> >
> >
> > ______________________________________________
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> >
> >
> 
> -- 
> Brian D. Ripley,                 
> ripley at stats.ox.ac.uk
> Professor of Applied Statistics, 
> http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865
> 272861 (self)
> 1 South Parks Road,                     +44 1865
> 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865
272595 


		
______________________________________________


From andrew.c at bu.ac.th  Thu Nov 25 13:44:02 2004
From: andrew.c at bu.ac.th (Andrew R. Criswell)
Date: Thu, 25 Nov 2004 19:44:02 +0700 (ICT)
Subject: [R] MASS problem -- glmmPQL and anova
Message-ID: <38530.10.9.9.16.1101386642.squirrel@email.bu.ac.th>

Hello:

I am really stuck on this problem. Why do I get an error message with
anova() when I compare these two equations?

Hope someone can help.

ANDREW
____________________________

> fm1 <- glmmPQL(choice ~ day + stereotypy,
+                random = ~ 1 | bear, data = learning, family = binomial)

> fm2 <- glmmPQL(choice ~ day + envir + stereotypy,
+                random = ~ 1 | bear, data = learning, family = binomial)

> anova(fm1, fm2)

Error in anova.lme(fm1, fm2) : Objects must inherit from classes "gls",
"gnls" "lm","lmList", "lme","nlme","nlsList", or "nls"

> anova(fm1)

            numDF denDF   F-value p-value
(Intercept)     1  2032   7.95709  0.0048
day             1  2032 213.98391  <.0001
stereotypy      1  2032   0.42810  0.5130

> anova(fm2)

            numDF denDF   F-value p-value
(Intercept)     1  2031   5.70343  0.0170
day             1  2031 213.21673  <.0001
envir           1  2031  12.50388  0.0004
stereotypy      1  2031   0.27256  0.6017

-- 
Andrew R. Criswell, Ph.D.
Graduate School, Bangkok University

mailto:andrew.c at bu.ac.th
mailto:andrew at arcriswell.com



From valderama at gmail.com  Thu Nov 25 14:09:04 2004
From: valderama at gmail.com (Laurent Valdes)
Date: Thu, 25 Nov 2004 14:09:04 +0100
Subject: [R] R vs SPSS
In-Reply-To: <20041125121537.22285.qmail@web41212.mail.yahoo.com>
References: <20041125121537.22285.qmail@web41212.mail.yahoo.com>
Message-ID: <2E5E378E-3EE3-11D9-9427-000393B0109E@gmail.com>


Hi,

Le 25 nov. 04, ?? 13:15, Vito Ricci a ??crit :

> command line) and very usefull and simple to use in
>
I do not know R so much, nor SPSS.
Then I appreciate SPSS, because tools are very practical to use.
Every transformation, model analysis are easily made.

In the other hand, let me say it doesn't run on Mac OS X 10.3 (only on 
Mac OS X 10.2), then the software editor didn't manage to update its 
product. R's communauty does.

R is really made to make big computations on big servers, that's not 
SPSS cup of tea.

Laurent



From valderama at gmail.com  Thu Nov 25 14:11:12 2004
From: valderama at gmail.com (Laurent Valdes)
Date: Thu, 25 Nov 2004 14:11:12 +0100
Subject: [R] R multithreading skills
Message-ID: <7B245803-3EE3-11D9-9427-000393B0109E@gmail.com>

Hi folks,

is it possible to read more things about R behavior in multiprocessor / 
multihost environment ?
Is there any distributed computation project associated to it ?

Laurent



From alexandersokol at ofir.dk  Thu Nov 25 14:09:14 2004
From: alexandersokol at ofir.dk (Alexander Sokol)
Date: Thu, 25 Nov 2004 14:09:14 +0100
Subject: [R] Turning strings into expressions
Message-ID: <41A9E892@webmail2.ofir.dk>

Hello,

I am running R 1.9.1 om Windows 2000 SP4. My problem is as follows:

Say I have a dataframe my.frame with column names A and B. I have a string,

>my.string
[1] "A==1 & B==2"

And I would like to retrieve the subset corresponding to my.string, that is, 
from my.frame and my.string I would like to get the result of

subset(my.frame,A==1 & B==2)

So I need to find a way to convert

"A==1 & B==2"

to

A==1 & B==2

I at first hoped that get() could do the job, but this does not work. Does 
anyone know how to do this?

Thanks,
 Alexander



From murdoch at stats.uwo.ca  Thu Nov 25 14:37:38 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 25 Nov 2004 08:37:38 -0500
Subject: [R] Turning strings into expressions
In-Reply-To: <41A9E892@webmail2.ofir.dk>
References: <41A9E892@webmail2.ofir.dk>
Message-ID: <7tnbq0tticis340gu6up2bhsdqrkeumc76@4ax.com>

On Thu, 25 Nov 2004 14:09:14 +0100, Alexander Sokol
<alexandersokol at ofir.dk> wrote :

>Hello,
>
>I am running R 1.9.1 om Windows 2000 SP4. My problem is as follows:
>
>Say I have a dataframe my.frame with column names A and B. I have a string,
>
>>my.string
>[1] "A==1 & B==2"
>
>And I would like to retrieve the subset corresponding to my.string, that is, 
>from my.frame and my.string I would like to get the result of
>
>subset(my.frame,A==1 & B==2)
>
>So I need to find a way to convert
>
>"A==1 & B==2"
>
>to
>
>A==1 & B==2
>
>I at first hoped that get() could do the job, but this does not work. Does 
>anyone know how to do this?

parse() does the conversion to an expression, but doesn't evaluate it.
So you probably want 

  eval(parse(text = "A == 1 & B == 2"))

but you may want to set the envir argument to eval, to tell R where to
go looking for A and B.

Duncan Murdoch



From angelare at to.infn.it  Thu Nov 25 14:43:24 2004
From: angelare at to.infn.it (Angela Re)
Date: Thu, 25 Nov 2004 14:43:24 +0100
Subject: [R] (no subject)
Message-ID: <41A5E17C.1060702@to.infn.it>

Good morning,
I'd like to know how to superimpose to a distribution of Pearson 
coefficient the Student cumulative distribution function.
Thank you of helping me.
Angela



From dimitris.rizopoulos at med.kuleuven.ac.be  Thu Nov 25 14:51:57 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Thu, 25 Nov 2004 14:51:57 +0100
Subject: [R] Turning strings into expressions
References: <41A9E892@webmail2.ofir.dk>
Message-ID: <003f01c4d2f5$ee283750$0540210a@www.domain>

Hi Alexander,

you could try:

my.string <- "A==1 & B==2"
(my.frame <- data.frame(A=sample(1:2, 20, TRUE), B=sample(1:2, 20, 
TRUE)))
subset(my.frame, eval(parse(text=my.string)))

I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Alexander Sokol" <alexandersokol at ofir.dk>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, November 25, 2004 2:09 PM
Subject: [R] Turning strings into expressions


> Hello,
>
> I am running R 1.9.1 om Windows 2000 SP4. My problem is as follows:
>
> Say I have a dataframe my.frame with column names A and B. I have a 
> string,
>
>>my.string
> [1] "A==1 & B==2"
>
> And I would like to retrieve the subset corresponding to my.string, 
> that is,
> from my.frame and my.string I would like to get the result of
>
> subset(my.frame,A==1 & B==2)
>
> So I need to find a way to convert
>
> "A==1 & B==2"
>
> to
>
> A==1 & B==2
>
> I at first hoped that get() could do the job, but this does not 
> work. Does
> anyone know how to do this?
>
> Thanks,
> Alexander
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From angelare at to.infn.it  Thu Nov 25 14:55:39 2004
From: angelare at to.infn.it (Angela Re)
Date: Thu, 25 Nov 2004 14:55:39 +0100
Subject: [R] (no subject)
Message-ID: <41A5E45B.6090808@to.infn.it>

Good  morning,
I tried to apply the ks test to a Student distribution by ks.test(input, 
"pt", ncp = 0, df = 58) or ks.test(input, "pt", df = 58) without success 
where input contains my data and 58 is the fredoom degree number. Why?
Thank you, Angela



From dmb at mrc-dunn.cam.ac.uk  Thu Nov 25 15:01:54 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Thu, 25 Nov 2004 14:01:54 +0000 (GMT)
Subject: [R] Making legend() look like my plot()
Message-ID: <Pine.LNX.4.21.0411251352430.17102-100000@mail.mrc-dunn.cam.ac.uk>


Hello,

I am using code like the following to create as simple plot...



plot(x,y,type='b')
lines(lowess(x,y),lwd=3,lty=3,col=2)

I want to add a legend which shows lines looking exactly like those used
in my plot, i.e. a thin black line with gaps taken up by circles (the
default for type='b', and a thick dashed red line with no pch at all).

I have two problems, 

1) making the pch on the first like look like type = 'b' (gaps around pch)
2) surpressing a pch on for the second line


Any help with these two problems would be greatly appreciated.

Any archive of plots and code to browse which could help me visually find
what I want and then copy the code?


An online user contributable database of 'graphics in R' would be
smashing.


How come some smart people dont just let me do something like

legend(xpos,ypos,legend=add)

to add a legend to the current plot for all the relevant points and lines
which have been added so far?



From vito_ricci at yahoo.com  Thu Nov 25 15:03:24 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Thu, 25 Nov 2004 15:03:24 +0100 (CET)
Subject: [R] Re: how to remove time series trend in R?
Message-ID: <20041125140324.57384.qmail@web41201.mail.yahoo.com>

Hi Terry,

If I understood your problem you would estimate trend
and seasonal (as sum of sin and cos) in a ts.

If t is time, Y is your ts, T=f(t) is trend function
of time (it could be linear, quadratic, etc. as better
is for your data), e=errors/residuals  

Your model to fit will'be:

Y(t)=T(t)+a*cos(2*pi*t/12)+b*sin(2*pi*t/12)+e(t) 

using lm() function to estimate a linear/polinomial
trend and sin/cos seasonal:

cos.t <- cos(2*pi*t/12)
sin.t <- sin(2*pi*t/12)
gfit<-lm(y~t+cos.t+sin.t, data=yourdf)

see this example:

> t<-seq(1:48)
> t
 [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16
17 18 19 20 21 22 23 24 25
[26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41
42 43 44 45 46 47 48

y<-10+5*t+0.5*cos(2*pi*t/12)+0.2*sin(2*pi*t/12)+rnorm(48)
> cos.t <- cos(2*pi*t/12)
> sin.t <- sin(2*pi*t/12)
> gfit<-lm(y~t+cos.t+sin.t)
> summary(gfit)

Call:
lm(formula = y ~ t + cos.t + sin.t)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.10222 -0.62184 -0.09387  0.50586  2.74299 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 10.30466    0.30009  34.339   <2e-16 ***
t            4.98987    0.01071 465.793   <2e-16 ***
cos.t        0.30207    0.20604   1.466    0.150    
sin.t        0.08699    0.20961   0.415    0.680    
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.'
0.1 ` ' 1 

Residual standard error: 1.008 on 44 degrees of
freedom
Multiple R-Squared: 0.9998,     Adjusted R-squared:
0.9998 
F-statistic: 7.525e+04 on 3 and 44 DF,  p-value: <
2.2e-16 

I hope I helped you.

Best
Vito
 



=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/



From bhx2 at mevik.net  Thu Nov 25 15:05:08 2004
From: bhx2 at mevik.net (=?iso-8859-1?q?Bj=F8rn-Helge_Mevik?=)
Date: Thu, 25 Nov 2004 15:05:08 +0100
Subject: [R] LDA with previous PCA for dimensionality reduction
In-Reply-To: <Pine.LNX.4.51.0411241535040.19391@artemis.imbe.med.uni-erlangen.de>
	(Torsten
	Hothorn's message of "Wed, 24 Nov 2004 15:43:13 +0100 (CET)")
References: <41A45F99.8030808@gmx.ch> <200411241401.35329.rdiaz@cnio.es>
	<Pine.LNX.4.51.0411241535040.19391@artemis.imbe.med.uni-erlangen.de>
Message-ID: <m0act63tyj.fsf@bar.nemo-project.org>

Torsten Hothorn writes:

> as long as one does not use the information in the response (the class
> variable, in this case) I don't think that one ends up with an
> optimistically biased estimate of the error

I would be a little careful, though.  The left-out sample in the
LDA-cross-validation, will still have influenced the PCA used to build
the LDA on the rest of the samples.  The sample will have a tendency
to lie closer to the centre of the "complete" PCA than of a PCA on the
remaining samples.  Also, if the sample has a high leverage on the
PCA, the directions of the two PCAs can be quite different.  Thus, the
LDA is built on data that "fits" better to the left-out sample than if
the sample was a completely new sample.

I have no proofs or numerical studies showing that this gives
over-optimistic error rates, but I would not recommend placing the PCA
"outside" the cross-validation.  (The same for any resampling-based
validation.)

-- 
Bj??rn-Helge Mevik



From edd at debian.org  Thu Nov 25 15:11:48 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 25 Nov 2004 08:11:48 -0600
Subject: [R] R multiprocessor/multihost skills
In-Reply-To: <7B245803-3EE3-11D9-9427-000393B0109E@gmail.com>
References: <7B245803-3EE3-11D9-9427-000393B0109E@gmail.com>
Message-ID: <20041125141148.GA1669@sonny.eddelbuettel.com>

On Thu, Nov 25, 2004 at 02:11:12PM +0100, Laurent Valdes wrote:
> Hi folks,
> 
> is it possible to read more things about R behavior in multiprocessor / 
> multihost environment ?
> Is there any distributed computation project associated to it ?

Sure. I'd start with the paper 'Simple Parallel Statistical Computing in R' by
Tony Rossini, Luke Tierney and Na Li:

   http://www.bepress.com/uwbiostat/paper193/

and the references therein.  The underlying software (SNOW, Rmpi, Rpvm, ...)
is readily available on CRAN, but you may have to do some work to get the
communications libraries needed (lam, mpi, pvm, ...) built.  

One way to get a head start on deployment is to grab a Quantian dvd image:

   http://dirk.eddelbuettel.com/quantian
   
which not only contains R alongside Snow, Rmpi, Rpvm for explicit parallelism
using message parsing, but also provides a ready-to-use openMosix clustering
environment where you can boot nodes 2, 3, ... right over the net off the
first machine booted from the dvd.

Hth, Dirk

PS I amended your subject line as you really asked about multiprocessor and
mulithost rather than multithreading (which is typically inside one cpu, and
which R doesn't do, see http://developer.r-project.org).

-- 
If your hair is standing up, then you are in extreme danger.
      -- http://www.usafa.af.mil/dfp/cockpit-phys/fp1ex3.htm



From ligges at statistik.uni-dortmund.de  Thu Nov 25 15:18:20 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 25 Nov 2004 15:18:20 +0100
Subject: [R] (no subject)
In-Reply-To: <41A5E45B.6090808@to.infn.it>
References: <41A5E45B.6090808@to.infn.it>
Message-ID: <41A5E9AC.1090804@statistik.uni-dortmund.de>

Angela Re wrote:

> Good  morning,
> I tried to apply the ks test to a Student distribution by ks.test(input, 
> "pt", ncp = 0, df = 58) or ks.test(input, "pt", df = 58) without success 
> where input contains my data and 58 is the fredoom degree number. Why?
> Thank you, Angela
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html


Please do what the appended messages tells you to do:
read the posting guide and learn to use a sensible subject line!

The following works for me, so please also specify a reproducible 
example that shows what does not work....

  input <- rnorm(100)
  ks.test(input, "pt", ncp = 0, df = 58)


Uwe Ligges



From bates at stat.wisc.edu  Thu Nov 25 15:28:02 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: Thu, 25 Nov 2004 08:28:02 -0600
Subject: [R] Automatic file reading
In-Reply-To: <3E838AED-3E16-11D9-9A35-000D933565E8@mail.nih.gov>
References: <41A46C1A.2030105@maths.lth.se>	<200411241331.33276.ahenningsen@email.uni-kiel.de>
	<3E838AED-3E16-11D9-9A35-000D933565E8@mail.nih.gov>
Message-ID: <41A5EBF2.3030600@stat.wisc.edu>

Sean Davis wrote:
> If you simply want read all files in a given directory, you can do 
> something like:
> 
> fullpath = "/home/andersm/tmp"
> filenames <- dir(fullpath,pattern="*")
> pair <- sapply(filenames,function(x) 
> {read.table(paste(fullpath,'/',x,sep=""))})

Slightly off-topic but it is more portable to use the file.path function 
instead of paste when creating a file name.



From vito_ricci at yahoo.com  Thu Nov 25 15:35:46 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Thu, 25 Nov 2004 15:35:46 +0100 (CET)
Subject: [R] Re: (no subject)
Message-ID: <20041125143546.70186.qmail@web41206.mail.yahoo.com>

Angela Re wrote:

> Good  morning,
> I tried to apply the ks test to a Student
distribution by ks.test(input, 
> "pt", ncp = 0, df = 58) or ks.test(input, "pt", df =
58) without success 
> where input contains my data and 58 is the fredoom
degree number. Why?
> Thank you, Angela

It runs also for me:

> input<-rt(100,58)
> ks.test(input, "pt", ncp = 0, df = 58)

        One-sample Kolmogorov-Smirnov test

data:  input 
D = 0.0827, p-value = 0.5003
alternative hypothesis: two.sided 

Vito


=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/



From Ted.Harding at nessie.mcc.ac.uk  Thu Nov 25 15:30:39 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 25 Nov 2004 14:30:39 -0000 (GMT)
Subject: [R] scatterplot of 100000 points and pdf file format
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E38A@usrymx25.merck.com>
Message-ID: <XFMail.041125143039.Ted.Harding@nessie.mcc.ac.uk>

Hi Andy,

On 25-Nov-04 Liaw, Andy wrote:
>> From: Ted.Harding at nessie.mcc.ac.uk
>> [...]
>> > X<-round(rnorm(1e6),3);Y<-round(rnorm(1e6),3)
>> > system.time(unique(X))
>> [1] 0.74 0.07 0.81 0.00 0.00
>> > system.time(unique(cbind(X,Y)))
>> [1] 350.81   4.56 356.54   0.00   0.00
> 
> Do you know if majority of that time is spent in unique() itself?
>  If so, which method?  What I see is:
> 
>> X<-round(rnorm(1e6),3);Y<-round(rnorm(1e6),3)
>> system.time(unique(X), gcFirst=TRUE)
> [1] 0.25 0.01 0.26   NA   NA
>> system.time(unique(cbind(X,Y)), gcFirst=TRUE)
> [1] 101.80   0.34 104.61     NA     NA
>> system.time(dat <- data.frame(x=X, y=Y), gcFirst=TRUE)
> [1] 10.17  0.00 10.24    NA    NA
>> system.time(unique(dat), gcFirst=TRUE)
> [1] 23.94  0.11 24.15    NA    NA
> 
> Andy

I want to look into this a bit more systematically (I have
an idea why 'unique' may be taking longer on the array from
'cbind' than on the dataframe), but I will be doing this on
a much faster machine than I immediately have to hand, so
will report results (if interesting) later.

Meanwhile, I'm not sure what you mean by "which method?",
and I'm also wondering what "gcFirst" is about.

Thanks,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 25-Nov-04                                       Time: 14:30:39
------------------------------ XFMail ------------------------------



From f.harrell at vanderbilt.edu  Thu Nov 25 15:56:52 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 25 Nov 2004 09:56:52 -0500
Subject: [R] R vs SPSS
In-Reply-To: <20041125121537.22285.qmail@web41212.mail.yahoo.com>
References: <20041125121537.22285.qmail@web41212.mail.yahoo.com>
Message-ID: <41A5F2B4.5040201@vanderbilt.edu>

Vito Ricci wrote:
> Dear all,
> 
> in last weeks you discussed about R vs SAS. 
> I want to ask your opinion about a comparison between
> R and SPSS. I don't know this software, but some weeks
> ago I went to a presentation of this product. I found
> it really user-friendly with GUI (even if I'd prefer
> command line) and very usefull and simple to use in
> creation and managing tables, OLAP tecniques, pivot
> table.
> What you think about?
> Cordially
> Vito
> 

What worries me about SPSS is that it often results in poor statistical 
practice.  The defaults in dialog boxes are not very good in some cases, 
and like SAS, SPSS tends to lead users to make to many assumptions 
(linearity in regression being one of the key ones).
-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From murdoch at stats.uwo.ca  Thu Nov 25 16:37:49 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Thu, 25 Nov 2004 10:37:49 -0500
Subject: [R] Searching for a string in RSQLite
Message-ID: <fbubq0984n88mh6eb4aq12pdk3jfmgjbfg@4ax.com>

I'd like to search for a particular string in an SQLite database using
RSQLite, but I'm running into problems constructing the query
properly, because of embedded quotes and parens in the string.

Is there a function that escapes these for me, or some other fixup
that would let me do the queries below?  In the real situation I don't
have control over what strings get searched for.

Example based on ?SQLite:

> library(RSQLite)
> m <- dbDriver("SQLite")
> con <- dbConnect(m, dbname = "base.dbms")
> data(USArrests)
> dbWriteTable(con, "USArrests", USArrests, overwrite = T)
[1] TRUE
> state <- "Wyoming"

# this works fine:

> dbGetQuery(con, paste("SELECT * from USArrests where row_names='",state,"'",sep=""))
  row_names Murder Assault UrbanPop Rape
1   Wyoming    6.8     161       60 15.6

# Buf if the search string contains characters that SQL interprets, I
# get an error

> state <- "messy:  ' ("
> dbGetQuery(con, paste("SELECT * from USArrests where row_names='",state,"'",sep=""))
Error in sqliteExecStatement(con, statement) : 
        RS-DBI driver: (error in statement: near "(": syntax error)

Duncan Murdoch



From jfox at mcmaster.ca  Thu Nov 25 16:40:56 2004
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 25 Nov 2004 10:40:56 -0500
Subject: [R] logistic regression and 3PL model
In-Reply-To: <BAY1-DAV18752B378D942FD7000ECA9FB90@phx.gbl>
Message-ID: <20041125154056.TKIQ2034.tomts20-srv.bellnexxia.net@JohnDesktop8300>

Dear Mike,

Pinheiro and Bates discuss a three-parameter logistic growth model in their
Mixed Effects Models in S and S-PLUS, but as far as I know there's no direct
way to fit the 3PL IRT model in R. It should be possible to fit such a model
using one of the general optimisers in R, such as nlm() or optimise(), and I
think that it would be a nice project to produce an IRT package for R. 

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Michael Lau
> Sent: Wednesday, November 24, 2004 10:26 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] logistic regression and 3PL model
> 
> Hello colleagues,
> 
>  
> 
> I am a novice with R and am stuck with an analysis I am 
> trying to conduct.
> Any suggestions or feedback would be very much appreciated.
> 
>  
> 
> I am analyzing a data set of psi (ESP) ganzfeld trials.  The 
> response variable is binary (correct/incorrect), with a 25% 
> base rate.  I've looked around the documentation and other 
> online resources and cannot find how I can correct for that 
> base rate when I conduct a logistic regression.  I understand 
> that the correction would be equivalent to the three 
> parameter logistic model (3PL) in IRT but am unsure how to 
> best fit it from a logistic regression in R.
> 
>  
> 
> Thanks much,
> 
>  
> 
> Mike Lau
> 
>  
> 
> __________________________________
> Michael Y. Lau, M.A. 
> 118 Haggar Hall
> Department of Psychology
> University of Notre Dame
> Notre Dame, IN 46556 
>  
>   
> 
>  
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Thu Nov 25 16:41:54 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 25 Nov 2004 15:41:54 +0000 (GMT)
Subject: [R] Error using glm with poisson family and identity link
In-Reply-To: <41A5D781.70400@pigrecodata.net>
References: <41A5D781.70400@pigrecodata.net>
Message-ID: <Pine.LNX.4.61.0411251535190.28898@gannet.stats>

On Thu, 25 Nov 2004, Federico Gherardini wrote:

> Hi all
> I'm trying to use the function glm from the MASS package to do the following

It's in the stats package.

> fit.
>
> fit <- glm(FP ~ rand, data = tab, family = poisson(link = "identity"), subset 
> = rand >= 1)
> (FP is >= 0)
>
> but I get the following error
>
> Error: no valid set of coefficients has been found:please supply starting 
> values
> In addition: Warning message:
> NaNs produced in: log(x)

And did you follow the advice in the error message?

> in contrast if I fit a model without intercept
>
> fit <- glm(FP ~ rand - 1, data = tab, family = poisson(link = "identity"), 
> subset = rand >= 1)
>
> everything goes fine.
> Now my guess is that the points "naturally" have a negative intercept so the 
> error is produced because I'm using the poisson distribution for the y and 
> negative values are of course not admitted. Am I right?

We don't have your data, but it is plausible.

> Also if this is the cause, shouldn't the function always try to do the best 
> fit given the parameters? I mean shouldn't it fit a model with intercept 0 
> anyway and report it as a bad fit?

Well, I believe functions should do what they say on the box (and the help 
page), and not what some user hopes they might do by mind-reading.

You do have a suitable set of starting values from the second fit, so why 
not just follow the rather explicit advice?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From nassar at noos.fr  Thu Nov 25 16:40:03 2004
From: nassar at noos.fr (Nassar)
Date: Thu, 25 Nov 2004 16:40:03 +0100
Subject: [R] R vs SPSS
In-Reply-To: <41A5F2B4.5040201@vanderbilt.edu>
Message-ID: <002701c4d305$089339a0$6501a8c0@NajiNassar>

Vito,


I use SPSS mainly for descriptive analysis (tables, graphs, factor
analysis..) and for data manipulation (you can see your data and
verify/control each step of your manipulation), mainly exploring the
analysis I need to develop in R (advanced clustering modelling,
simulations..).
SPSS huge worry : ITS VALUE.. Just actualize the annual fees ..
If you have a lot of data manipulation and table/easy graphs production, you
can consider it..
For statistical issues, spend the money into R trainings and R contribution


Best regards
Naji
-----Message d'origine-----
De : r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]De la part de Frank E Harrell
Jr
Envoy?? : jeudi 25 novembre 2004 15:57
?? : Vito Ricci
Cc : r-help at stat.math.ethz.ch
Objet : Re: [R] R vs SPSS


Vito Ricci wrote:
> Dear all,
>
> in last weeks you discussed about R vs SAS.
> I want to ask your opinion about a comparison between
> R and SPSS. I don't know this software, but some weeks
> ago I went to a presentation of this product. I found
> it really user-friendly with GUI (even if I'd prefer
> command line) and very usefull and simple to use in
> creation and managing tables, OLAP tecniques, pivot
> table.
> What you think about?
> Cordially
> Vito
>

What worries me about SPSS is that it often results in poor statistical
practice.  The defaults in dialog boxes are not very good in some cases,
and like SAS, SPSS tends to lead users to make to many assumptions
(linearity in regression being one of the key ones).
--
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From k.lindveld at imperial.ac.uk  Thu Nov 25 16:49:44 2004
From: k.lindveld at imperial.ac.uk (Lindveld, Charles)
Date: Thu, 25 Nov 2004 15:49:44 -0000
Subject: [R] Problem with ODBC access to SQL database
Message-ID: <28AD4F9DFF3DD649B26B3027C78FB6732D74B9@icex1.ic.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041125/77a22626/attachment.pl

From ripley at stats.ox.ac.uk  Thu Nov 25 17:13:31 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 25 Nov 2004 16:13:31 +0000 (GMT)
Subject: [R] logistic regression and 3PL model
In-Reply-To: <20041125154056.TKIQ2034.tomts20-srv.bellnexxia.net@JohnDesktop8300>
References: <20041125154056.TKIQ2034.tomts20-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <Pine.LNX.4.61.0411251604410.29054@gannet.stats>

On Thu, 25 Nov 2004, John Fox wrote:

> Pinheiro and Bates discuss a three-parameter logistic growth model in their
> Mixed Effects Models in S and S-PLUS, but as far as I know there's no direct
> way to fit the 3PL IRT model in R. It should be possible to fit such a model
> using one of the general optimisers in R, such as nlm() or optimise(), and I

optim(), not optimize() as there are at least two free parameters, I 
believe.

> think that it would be a nice project to produce an IRT package for R.

As I understand it this is a logistic regression and not a logistic growth 
curve, the latter being fitted by least squares.

For a known baseline (which is thus a 2-free PL model but what seems asked 
for here), a glm family can be constructed to allow glm() to do the 
fitting.  This is model described at

http://work.psych.uiuc.edu/irt/modeling_dich1.asp

with c known to be 0.25.  It would certainly be worth having an 
implementation of that in R, with c=0.5 being the most common case.

It is quite straightforward to fit such models by direct optimization of 
the likelihood, and MASS4 p. 445 gives you a template for logistic 
regression that could easily be modified.


>> -----Original Message-----
>> To: r-help at stat.math.ethz.ch
>> Subject: [R] logistic regression and 3PL model
>>
>> Hello colleagues,
>>
>> I am a novice with R and am stuck with an analysis I am
>> trying to conduct.
>> Any suggestions or feedback would be very much appreciated.
>>
>> I am analyzing a data set of psi (ESP) ganzfeld trials.  The
>> response variable is binary (correct/incorrect), with a 25%
>> base rate.  I've looked around the documentation and other
>> online resources and cannot find how I can correct for that
>> base rate when I conduct a logistic regression.  I understand
>> that the correction would be equivalent to the three
>> parameter logistic model (3PL) in IRT but am unsure how to
>> best fit it from a logistic regression in R.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rconroy at rcsi.ie  Thu Nov 25 17:21:44 2004
From: rconroy at rcsi.ie (=?ISO-8859-1?Q?Ron=E1n_Conroy?=)
Date: Thu, 25 Nov 2004 16:21:44 +0000
Subject: [R] R vs SPSS
In-Reply-To: <41A5F2B4.5040201@vanderbilt.edu>
References: <20041125121537.22285.qmail@web41212.mail.yahoo.com>
	<41A5F2B4.5040201@vanderbilt.edu>
Message-ID: <41A60698.2030501@rcsi.ie>

Frank E Harrell Jr wrote:

> What worries me about SPSS is that it often results in poor statistical
> practice.  The defaults in dialog boxes are not very good in some cases,
> and like SAS, SPSS tends to lead users to make to many assumptions
> (linearity in regression being one of the key ones).
> -- 
> Frank E Harrell Jr   Professor and Chair           School of Medicine
>                       Department of Biostatistics   Vanderbilt University
>
> ______________________________________________
>
My worry about SPSS is that it encourages people to do analysis and 
dataset manipulation 'on-the-fly', without leaving behind an audit trail 
that can be used to reconstruct the dataset and results. Certainly, SPSS 
has a 'paste' button which allows you to save a 'syntax' file of 
commands, but most users appear to ignore it. And post-hoc editing of 
graphs and tables cannot be saved thus (unless I'm missing out something 
here).

-- 

Ronan M Conroy (rconroy at rcsi.ie) 
Senior Lecturer in Biostatistics 
Royal College of Surgeons 
Dublin 2, Ireland 
+353 1 402 2431 (fax 2764) 
-------------------- 
Just say no to drug reps 
http://www.nofreelunch.org/



From dj at research.bell-labs.com  Thu Nov 25 17:25:05 2004
From: dj at research.bell-labs.com (David James)
Date: Thu, 25 Nov 2004 11:25:05 -0500
Subject: [R] Searching for a string in RSQLite
In-Reply-To: <fbubq0984n88mh6eb4aq12pdk3jfmgjbfg@4ax.com>;
	from murdoch@stats.uwo.ca on Thu, Nov 25, 2004 at 10:37:49AM -0500
References: <fbubq0984n88mh6eb4aq12pdk3jfmgjbfg@4ax.com>
Message-ID: <20041125112505.B21088@jessie.research.bell-labs.com>

Single quotes in a string are escaped by putting two single quotes in
a row.  E.g., 

   state <- "mess: '' ("

Regards,

--
David

Duncan Murdoch wrote:
> I'd like to search for a particular string in an SQLite database using
> RSQLite, but I'm running into problems constructing the query
> properly, because of embedded quotes and parens in the string.
> 
> Is there a function that escapes these for me, or some other fixup
> that would let me do the queries below?  In the real situation I don't
> have control over what strings get searched for.
> 
> Example based on ?SQLite:
> 
> > library(RSQLite)
> > m <- dbDriver("SQLite")
> > con <- dbConnect(m, dbname = "base.dbms")
> > data(USArrests)
> > dbWriteTable(con, "USArrests", USArrests, overwrite = T)
> [1] TRUE
> > state <- "Wyoming"
> 
> # this works fine:
> 
> > dbGetQuery(con, paste("SELECT * from USArrests where row_names='",state,"'",sep=""))
>   row_names Murder Assault UrbanPop Rape
> 1   Wyoming    6.8     161       60 15.6
> 
> # Buf if the search string contains characters that SQL interprets, I
> # get an error
> 
> > state <- "messy:  ' ("
> > dbGetQuery(con, paste("SELECT * from USArrests where row_names='",state,"'",sep=""))
> Error in sqliteExecStatement(con, statement) : 
>         RS-DBI driver: (error in statement: near "(": syntax error)
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From michael.waters at dtn.ntl.com  Thu Nov 25 17:33:46 2004
From: michael.waters at dtn.ntl.com (Dr Mike Waters)
Date: Thu, 25 Nov 2004 16:33:46 -0000
Subject: [R] Searching for a string in RSQLite
In-Reply-To: <fbubq0984n88mh6eb4aq12pdk3jfmgjbfg@4ax.com>
Message-ID: <00c501c4d30c$89d34850$6600a8c0@mainman>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Duncan Murdoch
> Sent: 25 November 2004 15:38
> To: r-help at stat.math.ethz.ch
> Subject: [R] Searching for a string in RSQLite
> 
> 
> I'd like to search for a particular string in an SQLite 
> database using RSQLite, but I'm running into problems 
> constructing the query properly, because of embedded quotes 
> and parens in the string.
> 
> Is there a function that escapes these for me, or some other 
> fixup that would let me do the queries below?  In the real 
> situation I don't have control over what strings get searched for.
> 
> Example based on ?SQLite:
> 
> > library(RSQLite)
> > m <- dbDriver("SQLite")
> > con <- dbConnect(m, dbname = "base.dbms")
> > data(USArrests)
> > dbWriteTable(con, "USArrests", USArrests, overwrite = T)
> [1] TRUE
> > state <- "Wyoming"
> 
> # this works fine:
> 
> > dbGetQuery(con, paste("SELECT * from USArrests where 
> > row_names='",state,"'",sep=""))
>   row_names Murder Assault UrbanPop Rape
> 1   Wyoming    6.8     161       60 15.6
> 
> # Buf if the search string contains characters that SQL 
> interprets, I # get an error
> 
> > state <- "messy:  ' ("
> > dbGetQuery(con, paste("SELECT * from USArrests where 
> > row_names='",state,"'",sep=""))
> Error in sqliteExecStatement(con, statement) : 
>         RS-DBI driver: (error in statement: near "(": syntax error)
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 

The normal character for escaping the next character to prevent it being
interpreted in SQL (including SQLite) is the backslash (i.e. \). Unless, of
course, I'm not understanding the precise nature of your request.

Regards

Mike



From jenniferwilson99 at yahoo.co.uk  Thu Nov 25 17:46:02 2004
From: jenniferwilson99 at yahoo.co.uk (Jennifer Wilson)
Date: Thu, 25 Nov 2004 16:46:02 +0000
Subject: [R] urgent
Message-ID: <E1CXMks-000PY9-47@server2.hostultra.com>

Hello,

My name is Mrs. Jennifer Wilson i am a dying woman who have decided to donate what i have to you/ church.I am 59 years old and i was diagnosed for cancer for about 2 years ago,immediately after the death of my husband, who has left me everything he worked for.

I have been touched by God to donate from what i have inherited from my late husband to the you for the god work of God,rather than allow my relatives to use my husband hard earned funds ungodly.Please pray,that the good Lord forgive me my sins.I have asked God to forgive me and i beleive he has because He is a merciful God. I will be going in for an operation in less than one hour.

I decided to WILL/donate the sum of $1,500,000 (One million five hundred thousand dollars) to you for the good work of the lord, and also to help the motherless and less privilege and also for the assistance of the widows according to (JAMES 1:27).

At the moment i cannot take any telephone calls right now due to the fact that my relatives are around me and my health status.I have adjusted my WILL and my lawyer is aware i have changed my will you and he will arrange the transfer of the funds from my account to you.

I wish you all the best and may the good Lord bless you abundantly, and please use the funds well and always extend the good work to others. Contact my lawyer with this specified email   jjbbsolicitor at netscape.net   and tell him that i have WILLED ($1,500,000.00) to you and i have also notified him that i am WILLING that amount to you for a specific and good work.I know i dont know you but i have been directed to do this.Thanks and God bless.

NB: I will appreciate your utmost confidentiality in this matter until the task is accomplished as I don't want anything that will Jeopardize my last wish. And Also I will be contacting with you by email as I don't want my relation or anybody to know because they are always around me. 

Regards,
Jennifer Wilson



From dimitris.rizopoulos at med.kuleuven.ac.be  Thu Nov 25 17:51:30 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Thu, 25 Nov 2004 17:51:30 +0100
Subject: [R] logistic regression and 3PL model
References: <20041125154056.TKIQ2034.tomts20-srv.bellnexxia.net@JohnDesktop8300>
	<Pine.LNX.4.61.0411251604410.29054@gannet.stats>
Message-ID: <00cd01c4d30f$03483310$0540210a@www.domain>

I don't know if I am missing something, but isn't there also a latent 
variable (trait) that must be integrated out using maybe Gauss-Hermite 
which might complicate a bit the calculations? So is this possible 
with `glm()'?

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
To: "John Fox" <jfox at mcmaster.ca>
Cc: <r-help at stat.math.ethz.ch>
Sent: Thursday, November 25, 2004 5:13 PM
Subject: RE: [R] logistic regression and 3PL model


> On Thu, 25 Nov 2004, John Fox wrote:
>
>> Pinheiro and Bates discuss a three-parameter logistic growth model 
>> in their
>> Mixed Effects Models in S and S-PLUS, but as far as I know there's 
>> no direct
>> way to fit the 3PL IRT model in R. It should be possible to fit 
>> such a model
>> using one of the general optimisers in R, such as nlm() or 
>> optimise(), and I
>
> optim(), not optimize() as there are at least two free parameters, I 
> believe.
>
>> think that it would be a nice project to produce an IRT package for 
>> R.
>
> As I understand it this is a logistic regression and not a logistic 
> growth curve, the latter being fitted by least squares.
>
> For a known baseline (which is thus a 2-free PL model but what seems 
> asked for here), a glm family can be constructed to allow glm() to 
> do the fitting.  This is model described at
>
> http://work.psych.uiuc.edu/irt/modeling_dich1.asp
>
> with c known to be 0.25.  It would certainly be worth having an 
> implementation of that in R, with c=0.5 being the most common case.
>
> It is quite straightforward to fit such models by direct 
> optimization of the likelihood, and MASS4 p. 445 gives you a 
> template for logistic regression that could easily be modified.
>
>
>>> -----Original Message-----
>>> To: r-help at stat.math.ethz.ch
>>> Subject: [R] logistic regression and 3PL model
>>>
>>> Hello colleagues,
>>>
>>> I am a novice with R and am stuck with an analysis I am
>>> trying to conduct.
>>> Any suggestions or feedback would be very much appreciated.
>>>
>>> I am analyzing a data set of psi (ESP) ganzfeld trials.  The
>>> response variable is binary (correct/incorrect), with a 25%
>>> base rate.  I've looked around the documentation and other
>>> online resources and cannot find how I can correct for that
>>> base rate when I conduct a logistic regression.  I understand
>>> that the correction would be equivalent to the three
>>> parameter logistic model (3PL) in IRT but am unsure how to
>>> best fit it from a logistic regression in R.
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From h.wickham at gmail.com  Thu Nov 25 18:12:11 2004
From: h.wickham at gmail.com (hadley wickham)
Date: Thu, 25 Nov 2004 11:12:11 -0600
Subject: [R] Searching for a string in RSQLite
In-Reply-To: <fbubq0984n88mh6eb4aq12pdk3jfmgjbfg@4ax.com>
References: <fbubq0984n88mh6eb4aq12pdk3jfmgjbfg@4ax.com>
Message-ID: <f8e6ff0504112509126eeb3aa4@mail.gmail.com>

> I'd like to search for a particular string in an SQLite database using
> RSQLite, but I'm running into problems constructing the query
> properly, because of embedded quotes and parens in the string.

You may find dQuote() and sQuote() to be helpful, but a better
solution might be to ask the developers of the RSQLite package to add
R functions that call the SQLite api functions sqlite_exec_printf()
and sqlite_get_table_printf() which automatically escape single quotes
(among other things, http://www.sqlite.org/c_interface.html)

Hadley



From cyracules at yahoo.co.uk  Thu Nov 25 18:15:50 2004
From: cyracules at yahoo.co.uk (John)
Date: Thu, 25 Nov 2004 17:15:50 +0000 (GMT)
Subject: [R] Avoiding for-loops
Message-ID: <20041125171550.81380.qmail@web26306.mail.ukl.yahoo.com>

Hello R-users,

I have a symmetric matrix of numerical values and I
want to obtain those values in the upper or lower
triangle of the matrix in a vector. I tried to do the
job by using two for-loops but it doens't seem to be a
clever way, and I'd like to know a more efficient code
for a large matrix of thousands of rows and columns.
Below is my code for your reference. 

Thanks a lot.

John

####################

# mtx.sym is a symmetric matrix

> my.ftn <- function(size_mtx, mtx) {
+ my.vector <- c()
+ for ( i in 1:size_mtx ) {
+ cat(".")
+ for ( j in 1:size_mtx ) {
+ if ( upper.tri(mtx)[i,j] ) {
+ my.vector <- c(my.vector, mtx[i,j])
+ }}}
+ cat("\n")
+ }
>
# if I have a matrix, mtx.sym, of 100x100
> my.ftn(100, mtx.sym)



From spencer.graves at pdf.com  Thu Nov 25 18:22:59 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 25 Nov 2004 09:22:59 -0800
Subject: [R] Error using glm with poisson family and identity link
In-Reply-To: <Pine.LNX.4.61.0411251535190.28898@gannet.stats>
References: <41A5D781.70400@pigrecodata.net>
	<Pine.LNX.4.61.0411251535190.28898@gannet.stats>
Message-ID: <41A614F3.4000102@pdf.com>

Dear Federico: 

      Why do you use the "identity" link?  That can produce situations 
with an average of (-2) Poisson defects per unit, for example.  That's 
physical nonsense.  Also, it seems essentially to be generating your 
error message. 

      Also, have you considered the following: 

fit <- glm(FP ~ offset(log(rand)), data = tab, family = poisson, subset 
= rand >= 1)

      I haven't tried this, but it looks like this model is virtually 
equivalent to the one you wrote: 

      "FP ~ rand-1" with link = "identity" says "estimate 'b' in 
PoissonMean = b*rand". 

      "FP ~ offset(log(rand))" with link = "log" (the default) says 
"estimate 'a' in log(PoissonMean)-log(rand) = a". 

      If I haven't made an error, then log(b) = a. 

      In more general situations, if you really need the "identity" 
link, have you considered searching for good starting values, as Prof. 
Ripley suggested?  You could build up to your final model by estimating 
simpler models and obtaining trial fits using the default "log" link?  
With those results and a little thought, you should be able to obtain 
reasonable starting values. 

      hope this helps. 
      spencer graves

Prof Brian Ripley wrote:

> On Thu, 25 Nov 2004, Federico Gherardini wrote:
>
>> Hi all
>> I'm trying to use the function glm from the MASS package to do the 
>> following
>
>
> It's in the stats package.
>
>> fit.
>>
>> fit <- glm(FP ~ rand, data = tab, family = poisson(link = 
>> "identity"), subset = rand >= 1)
>> (FP is >= 0)
>>
>> but I get the following error
>>
>> Error: no valid set of coefficients has been found:please supply 
>> starting values
>> In addition: Warning message:
>> NaNs produced in: log(x)
>
>
> And did you follow the advice in the error message?
>
>> in contrast if I fit a model without intercept
>>
>> fit <- glm(FP ~ rand - 1, data = tab, family = poisson(link = 
>> "identity"), subset = rand >= 1)
>>
>> everything goes fine.
>> Now my guess is that the points "naturally" have a negative intercept 
>> so the error is produced because I'm using the poisson distribution 
>> for the y and negative values are of course not admitted. Am I right?
>
>
> We don't have your data, but it is plausible.
>
>> Also if this is the cause, shouldn't the function always try to do 
>> the best fit given the parameters? I mean shouldn't it fit a model 
>> with intercept 0 anyway and report it as a bad fit?
>
>
> Well, I believe functions should do what they say on the box (and the 
> help page), and not what some user hopes they might do by mind-reading.
>
> You do have a suitable set of starting values from the second fit, so 
> why not just follow the rather explicit advice?
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From h.wickham at gmail.com  Thu Nov 25 18:33:21 2004
From: h.wickham at gmail.com (hadley wickham)
Date: Thu, 25 Nov 2004 11:33:21 -0600
Subject: [R] Searching for a string in RSQLite
In-Reply-To: <f8e6ff0504112509126eeb3aa4@mail.gmail.com>
References: <fbubq0984n88mh6eb4aq12pdk3jfmgjbfg@4ax.com>
	<f8e6ff0504112509126eeb3aa4@mail.gmail.com>
Message-ID: <f8e6ff0504112509333c2d5df3@mail.gmail.com>

> You may find dQuote() and sQuote() to be helpful, but a better

Ooops, dQuote() and sQuote() won't be of much use as they escape
quotes with quotes.

A regular expression should do the trick: gsub("'", "\\\\'", "Hi
y'all").  (Note that this looks like it has too many backslashes, but
this is just the way R prints escaped strings, use str() to see the
unescaped string)

Hadley



From p.dalgaard at biostat.ku.dk  Thu Nov 25 18:53:16 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 25 Nov 2004 18:53:16 +0100
Subject: [R] Turning strings into expressions
In-Reply-To: <003f01c4d2f5$ee283750$0540210a@www.domain>
References: <41A9E892@webmail2.ofir.dk>
	<003f01c4d2f5$ee283750$0540210a@www.domain>
Message-ID: <x2brdlbysz.fsf@biostat.ku.dk>

"Dimitris Rizopoulos" <dimitris.rizopoulos at med.kuleuven.ac.be> writes:

> Hi Alexander,
> 
> you could try:
> 
> my.string <- "A==1 & B==2"
> (my.frame <- data.frame(A=sample(1:2, 20, TRUE), B=sample(1:2, 20,
> TRUE)))
> subset(my.frame, eval(parse(text=my.string)))

Hmm, considering the nonstandard evaluation that is going on inside
subset(), I think I'd rather try

 my.frame <- data.frame(A=sample(1:2, 20, TRUE), B=sample(1:2, 20,TRUE))
 my.string <- "A==1 & B==2"
 l <- as.list(parse(text=my.string))
 names(l)<-"sub"
 eval(substitute(subset(my.frame, sub), l))
   A B
18 1 2


(or perhaps l <- list(sub=parse(text=my.string)[[1]]) is less
cryptic).

Point being that this way you'll literally evaluate

> substitute(subset(my.frame, sub), l)
subset(my.frame, A == 1 & B == 2)


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From maechler at stat.math.ethz.ch  Thu Nov 25 18:58:55 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 25 Nov 2004 18:58:55 +0100
Subject: Spam {Re: [R] urgent}
In-Reply-To: <E1CXMks-000PY9-47@server2.hostultra.com>
References: <E1CXMks-000PY9-47@server2.hostultra.com>
Message-ID: <16806.7519.534080.731020@gargle.gargle.HOWL>

I'm sorry for this spam that astonishingly came through to the
mailing list.

It seems these guys have been "exercising" against known spam
filters and achieved more than in the past.
Also, recent versions of our spamfilter have been tuned such as
to rather produce a few false negatives {spam not detected}
with hardly ever any false positive {non-spam not delivered to recipient},
which makes a lot of sense.

We (I and local e-mail administrators) do keep an eye on this, 
me spending a little time for "manual tuning", but we do not
want to allocate too much time for this. 

PLEASE do not reply to this e-mail (at least not to R-help!). 
It is *not* relevant to R and not worth the time (also since too
many people think they know what they are talking about :-).

If you want, reply to me privately.

Martin Maechler, ETH Zurich



From p.dalgaard at biostat.ku.dk  Thu Nov 25 19:02:32 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 25 Nov 2004 19:02:32 +0100
Subject: [R] scatterplot of 100000 points and pdf file format
In-Reply-To: <XFMail.041125143039.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.041125143039.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <x27jo9bydj.fsf@biostat.ku.dk>

(Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:

> I want to look into this a bit more systematically (I have
> an idea why 'unique' may be taking longer on the array from
> 'cbind' than on the dataframe),

Just look inside the functions. One is pasting columns together, the
other is using a paste() construct inside an apply() function. So with
two columns by 1e6 rows, one is doing one large paste and the other a
million small ones.


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From roger at ysidro.econ.uiuc.edu  Thu Nov 25 19:15:13 2004
From: roger at ysidro.econ.uiuc.edu (roger koenker)
Date: Thu, 25 Nov 2004 12:15:13 -0600
Subject: [R] Avoiding for-loops
In-Reply-To: <20041125171550.81380.qmail@web26306.mail.ukl.yahoo.com>
References: <20041125171550.81380.qmail@web26306.mail.ukl.yahoo.com>
Message-ID: <F349E5A8-3F0D-11D9-B3FE-000393A361A2@ysidro.econ.uiuc.edu>

lower triangle can be obtained by

	A[row(A)>col(A)]

url:	www.econ.uiuc.edu/~roger        	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820

On Nov 25, 2004, at 11:15 AM, John wrote:

> Hello R-users,
>
> I have a symmetric matrix of numerical values and I
> want to obtain those values in the upper or lower
> triangle of the matrix in a vector. I tried to do the
> job by using two for-loops but it doens't seem to be a
> clever way, and I'd like to know a more efficient code
> for a large matrix of thousands of rows and columns.
> Below is my code for your reference.
>
> Thanks a lot.
>
> John
>
> ####################
>
> # mtx.sym is a symmetric matrix
>
>> my.ftn <- function(size_mtx, mtx) {
> + my.vector <- c()
> + for ( i in 1:size_mtx ) {
> + cat(".")
> + for ( j in 1:size_mtx ) {
> + if ( upper.tri(mtx)[i,j] ) {
> + my.vector <- c(my.vector, mtx[i,j])
> + }}}
> + cat("\n")
> + }
>>
> # if I have a matrix, mtx.sym, of 100x100
>> my.ftn(100, mtx.sym)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Thu Nov 25 19:19:55 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 25 Nov 2004 19:19:55 +0100
Subject: [R] Avoiding for-loops
In-Reply-To: <20041125171550.81380.qmail@web26306.mail.ukl.yahoo.com>
References: <20041125171550.81380.qmail@web26306.mail.ukl.yahoo.com>
Message-ID: <41A6224B.8010706@statistik.uni-dortmund.de>

John wrote:

> Hello R-users,
> 
> I have a symmetric matrix of numerical values and I
> want to obtain those values in the upper or lower
> triangle of the matrix in a vector. I tried to do the
> job by using two for-loops but it doens't seem to be a
> clever way, and I'd like to know a more efficient code
> for a large matrix of thousands of rows and columns.
> Below is my code for your reference. 


See ?upper.tri


Uwe Ligges



> Thanks a lot.
> 
> John
> 
> ####################
> 
> # mtx.sym is a symmetric matrix
> 
> 
>>my.ftn <- function(size_mtx, mtx) {
> 
> + my.vector <- c()
> + for ( i in 1:size_mtx ) {
> + cat(".")
> + for ( j in 1:size_mtx ) {
> + if ( upper.tri(mtx)[i,j] ) {
> + my.vector <- c(my.vector, mtx[i,j])
> + }}}
> + cat("\n")
> + }
> 
> # if I have a matrix, mtx.sym, of 100x100
> 
>>my.ftn(100, mtx.sym)
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From deepayan at stat.wisc.edu  Thu Nov 25 19:22:26 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 25 Nov 2004 12:22:26 -0600
Subject: [R] Avoiding for-loops
In-Reply-To: <20041125171550.81380.qmail@web26306.mail.ukl.yahoo.com>
References: <20041125171550.81380.qmail@web26306.mail.ukl.yahoo.com>
Message-ID: <200411251222.26290.deepayan@stat.wisc.edu>

On Thursday 25 November 2004 11:15, John wrote:
> Hello R-users,
>
> I have a symmetric matrix of numerical values and I
> want to obtain those values in the upper or lower
> triangle of the matrix in a vector. I tried to do the
> job by using two for-loops but it doens't seem to be a
> clever way, and I'd like to know a more efficient code
> for a large matrix of thousands of rows and columns.
> Below is my code for your reference.

Try 

mtx[lower.tri(mtx)]

which should give you the same order as your code (untested).

HTH,

Deepayan


>
> Thanks a lot.
>
> John
>
> ####################
>
> # mtx.sym is a symmetric matrix
>
> > my.ftn <- function(size_mtx, mtx) {
>
> + my.vector <- c()
> + for ( i in 1:size_mtx ) {
> + cat(".")
> + for ( j in 1:size_mtx ) {
> + if ( upper.tri(mtx)[i,j] ) {
> + my.vector <- c(my.vector, mtx[i,j])
> + }}}
> + cat("\n")
> + }
>
> # if I have a matrix, mtx.sym, of 100x100
>
> > my.ftn(100, mtx.sym)



From rrsilva at ib.usp.br  Thu Nov 25 17:29:11 2004
From: rrsilva at ib.usp.br (Rogerio Rosa da Silva)
Date: Thu, 25 Nov 2004 16:29:11 +0000
Subject: [R] arrows in histograms
Message-ID: <200411251629.11254.rrsilva@ib.usp.br>

Dear all,

I have a null frequency histogram of  pairwise associations values.
How can I put  an arrow on the histogram to indicate the critical value
obtained from the null distribution? [In this case, quantile 0.05].

Thanks
-- 
Rog??rio R. Silva
MZUSP http://www.mz.usp.br
Linux User # 354364
Linux counter http://counter.li.org



From p.dalgaard at biostat.ku.dk  Thu Nov 25 19:40:23 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 25 Nov 2004 19:40:23 +0100
Subject: [R] Error using glm with poisson family and identity link
In-Reply-To: <41A614F3.4000102@pdf.com>
References: <41A5D781.70400@pigrecodata.net>
	<Pine.LNX.4.61.0411251535190.28898@gannet.stats>
	<41A614F3.4000102@pdf.com>
Message-ID: <x2y8gpai20.fsf@biostat.ku.dk>

Spencer Graves <spencer.graves at pdf.com> writes:

> Dear Federico:     Why do you use the "identity" link?  That can
> produce situations with an average of (-2) Poisson defects per unit,
> for example.  That's physical nonsense. 

So is _not_ using the identity link when the model is manifestly
additive on the identity scale. E.g. calibrating differential
spectrofluorometry with photon counters recording linear combinations
of intensities at different wavelengths.

I've bumped into similar situations before (binomial(link=identity), I
think it was then) and the glm.fit algorithm could use improvement in
dealing with the parameter constraints in these cases. With the
standard IRLS algorithm, if the maximum is on the boundary, you
basically hit a random point on the boundary and get stuck there with
a search direction pointing out of the valid region.
 
-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Ted.Harding at nessie.mcc.ac.uk  Thu Nov 25 19:54:20 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 25 Nov 2004 18:54:20 -0000 (GMT)
Subject: [R] Avoiding for-loops
In-Reply-To: <20041125171550.81380.qmail@web26306.mail.ukl.yahoo.com>
Message-ID: <XFMail.041125185420.Ted.Harding@nessie.mcc.ac.uk>

On 25-Nov-04 John wrote:
> Hello R-users,
> 
> I have a symmetric matrix of numerical values and I
> want to obtain those values in the upper or lower
> triangle of the matrix in a vector. I tried to do the
> job by using two for-loops but it doens't seem to be a
> clever way, and I'd like to know a more efficient code
> for a large matrix of thousands of rows and columns.

The two functions uuper.tri and lower.tri do just this job!

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 25-Nov-04                                       Time: 18:54:20
------------------------------ XFMail ------------------------------



From dmb at mrc-dunn.cam.ac.uk  Thu Nov 25 20:25:30 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Thu, 25 Nov 2004 19:25:30 +0000 (GMT)
Subject: [R] Making legend() look like my plot()
In-Reply-To: <Pine.LNX.4.21.0411251352430.17102-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <Pine.LNX.4.21.0411251924260.21943-100000@mail.mrc-dunn.cam.ac.uk>


Is this an impossible task?

How about just problem 2 below, having one pch in one legend entry, but no
pch in the second?




On Thu, 25 Nov 2004, Dan Bolser wrote:

>
>Hello,
>
>I am using code like the following to create as simple plot...
>
>
>
>plot(x,y,type='b')
>lines(lowess(x,y),lwd=3,lty=3,col=2)
>
>I want to add a legend which shows lines looking exactly like those used
>in my plot, i.e. a thin black line with gaps taken up by circles (the
>default for type='b', and a thick dashed red line with no pch at all).
>
>I have two problems, 
>
>1) making the pch on the first like look like type = 'b' (gaps around pch)
>2) surpressing a pch on for the second line
>
>
>Any help with these two problems would be greatly appreciated.
>
>Any archive of plots and code to browse which could help me visually find
>what I want and then copy the code?
>
>
>An online user contributable database of 'graphics in R' would be
>smashing.
>
>
>How come some smart people dont just let me do something like
>
>legend(xpos,ypos,legend=add)
>
>to add a legend to the current plot for all the relevant points and lines
>which have been added so far?
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From spencer.graves at pdf.com  Thu Nov 25 20:35:24 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 25 Nov 2004 11:35:24 -0800
Subject: [R] Error using glm with poisson family and identity link
In-Reply-To: <x2y8gpai20.fsf@biostat.ku.dk>
References: <41A5D781.70400@pigrecodata.net>	<Pine.LNX.4.61.0411251535190.28898@gannet.stats>	<41A614F3.4000102@pdf.com>
	<x2y8gpai20.fsf@biostat.ku.dk>
Message-ID: <41A633FC.5010401@pdf.com>

Hi, Peter: 

      What do you do in such situations? 

      Sundar Dorai-Raj and I have extended "glm" concepts to models 
driven by a sum of k independent Poissons, with the a linear model for 
log(defectRate[i]) for each source (i = 1:k).  To handle convergence 
problems, etc., I think we need to use informative Bayes, but we're not 
there yet.  In any context where things are done more than once [which 
covers most human activities], informative Bayes seems sensible. 

      A related question comes with data representing the differences 
between Poisson counts, e.g., with d[i] = X[i]-X[i-1] = the number of 
new defects added between steps i-1 and i in a manufacturing process.  
Most of the time, d[i] is nonnegative.  However, in some cases, it can 
be negative, either because of metrology errors in X[i] or because of 
defect removal between steps i-1 and i. 

      Comments?
      Best Wishes,
      Spencer Graves

Peter Dalgaard wrote:

>Spencer Graves <spencer.graves at pdf.com> writes:
>
>  
>
>>Dear Federico:     Why do you use the "identity" link?  That can
>>produce situations with an average of (-2) Poisson defects per unit,
>>for example.  That's physical nonsense. 
>>    
>>
>
>So is _not_ using the identity link when the model is manifestly
>additive on the identity scale. E.g. calibrating differential
>spectrofluorometry with photon counters recording linear combinations
>of intensities at different wavelengths.
>
>I've bumped into similar situations before (binomial(link=identity), I
>think it was then) and the glm.fit algorithm could use improvement in
>dealing with the parameter constraints in these cases. With the
>standard IRLS algorithm, if the maximum is on the boundary, you
>basically hit a random point on the boundary and get stuck there with
>a search direction pointing out of the valid region.
> 
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From leroy at ucsd.edu  Thu Nov 25 20:39:45 2004
From: leroy at ucsd.edu (Anthony Westerling)
Date: Thu, 25 Nov 2004 11:39:45 -0800
Subject: [R] R-2.0.1 reinstall non-CRAN pkg 
Message-ID: <C2434B72-3F19-11D9-8F0F-000A959EA8AA@ucsd.edu>


I am trying to upgrade to R-2.0.1 from R-1.9 on a Mac running OS X 10.3.

I have some simple packages I wrote myself that have to be reinstalled 
to be recognized as valid packages.  I have been using them for a while 
on earlier versions of R, so didn't expect to have any problems.

I am probably going about this the wrong way?  I simply used

R CMD build mypkgdir

and then

R CMD install mypkgdir.tar.gz

the package installs without any error messages.

however, library(mypkgname) still generates spiteful

	Error in library(mypkgname) : 'mypkgname' is not a valid package -- 
installed < 2.0.0?

messages.

My apologies if answers to this kind of question have already been 
posted.  I have looked over the archived r-help threads for the last 
couple of months.

Best

Anthony Westerling



From Ted.Harding at nessie.mcc.ac.uk  Thu Nov 25 20:46:15 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Thu, 25 Nov 2004 19:46:15 -0000 (GMT)
Subject: [R] arrows in histograms
In-Reply-To: <200411251629.11254.rrsilva@ib.usp.br>
Message-ID: <XFMail.041125194615.Ted.Harding@nessie.mcc.ac.uk>

On 25-Nov-04 Rogerio Rosa da Silva wrote:
> Dear all,
> 
> I have a null frequency histogram of _pairwise associations values.
> How can I put _an arrow on the histogram to indicate the critical value
> obtained from the null distribution? [In this case, quantile 0.05].

?Something like

  q05 <- qnorm(0.95)  ## or qnorm(0.05) -- not sure which you really want
  hist(x,xlim=c(-5,5))
  arrows(q05,400,q05,0,length=0.1)
  text(q05+0.1,400,"5 per cent point",adj=0)

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 25-Nov-04                                       Time: 19:46:15
------------------------------ XFMail ------------------------------



From ligges at statistik.uni-dortmund.de  Thu Nov 25 21:28:17 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 25 Nov 2004 21:28:17 +0100
Subject: [R] Making legend() look like my plot()
In-Reply-To: <Pine.LNX.4.21.0411251924260.21943-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0411251924260.21943-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <41A64061.60904@statistik.uni-dortmund.de>

Dan Bolser wrote:
> Is this an impossible task?
> 
> How about just problem 2 below, having one pch in one legend entry, but no
> pch in the second?

Please be at least a little bit patient! This is not a hotline! People 
are not working 24 hours a day just to answer your questions at once - 
they are answering questions on a voluntary basis!

answer 1) is not straightforward, but you might want to use one of 
fillable symbols mentioned in ?points, e.g. number 21

answer 2) pch = c(1, NA) should do the trick.

legend(....., pch=c(21,NA), lwd=c(1,3), lty=c(1,3), pt.bg="white", col=1:2)

Uwe Ligges


> 
> 
> 
> On Thu, 25 Nov 2004, Dan Bolser wrote:
> 
> 
>>Hello,
>>
>>I am using code like the following to create as simple plot...
>>
>>
>>
>>plot(x,y,type='b')
>>lines(lowess(x,y),lwd=3,lty=3,col=2)
>>
>>I want to add a legend which shows lines looking exactly like those used
>>in my plot, i.e. a thin black line with gaps taken up by circles (the
>>default for type='b', and a thick dashed red line with no pch at all).
>>
>>I have two problems, 
>>
>>1) making the pch on the first like look like type = 'b' (gaps around pch)
>>2) surpressing a pch on for the second line
>>
>>
>>Any help with these two problems would be greatly appreciated.
>>
>>Any archive of plots and code to browse which could help me visually find
>>what I want and then copy the code?
>>
>>
>>An online user contributable database of 'graphics in R' would be
>>smashing.
>>
>>
>>How come some smart people dont just let me do something like
>>
>>legend(xpos,ypos,legend=add)
>>
>>to add a legend to the current plot for all the relevant points and lines
>>which have been added so far?
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Thu Nov 25 21:49:03 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 25 Nov 2004 21:49:03 +0100
Subject: [R] Error using glm with poisson family and identity link
In-Reply-To: <41A633FC.5010401@pdf.com>
References: <41A5D781.70400@pigrecodata.net>
	<Pine.LNX.4.61.0411251535190.28898@gannet.stats>
	<41A614F3.4000102@pdf.com> <x2y8gpai20.fsf@biostat.ku.dk>
	<41A633FC.5010401@pdf.com>
Message-ID: <x2u0rdac3k.fsf@biostat.ku.dk>

Spencer Graves <spencer.graves at pdf.com> writes:

> Hi, Peter:     What do you do in such situations?     Sundar Dorai-Raj
> and I have extended "glm" concepts to models driven by a sum of k
> independent Poissons, with the a linear model for log(defectRate[i])
> for each source (i = 1:k).  To handle convergence problems, etc., I
> think we need to use informative Bayes, but we're not there yet.  In
> any context where things are done more than once [which covers most
> human activities], informative Bayes seems sensible.     A related
> question comes with data representing the differences between Poisson
> counts, e.g., with d[i] = X[i]-X[i-1] = the number of new defects
> added between steps i-1 and i in a manufacturing process.  Most of the
> time, d[i] is nonnegative.  However, in some cases, it can be
> negative, either because of metrology errors in X[i] or because of
> defect removal between steps i-1 and i.     Comments?

I haven't got all that much experience with it, but obviously, the
various algorithms for constrained optimization (box- or otherwise) at
least allow you to find a proper maximum likelihood estimator.


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From spencer.graves at pdf.com  Thu Nov 25 23:28:56 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 25 Nov 2004 14:28:56 -0800
Subject: [R] Error using glm with poisson family and identity link
In-Reply-To: <x2u0rdac3k.fsf@biostat.ku.dk>
References: <41A5D781.70400@pigrecodata.net>	<Pine.LNX.4.61.0411251535190.28898@gannet.stats>	<41A614F3.4000102@pdf.com>
	<x2y8gpai20.fsf@biostat.ku.dk>	<41A633FC.5010401@pdf.com>
	<x2u0rdac3k.fsf@biostat.ku.dk>
Message-ID: <41A65CA8.8090508@pdf.com>

Hi, Peter: 

      Thanks for the comment and reply. 

      I generally avoid constrained optimizers for three reasons: 

      1.  My experience with them has included many cases where the 
optimizer would stop with an error when testing parameter values that 
violate the constraints.  If I transform the parameter space to remove 
the constraints, that never happens.  The constrained optimizers in R 
2.0.1 may not exhibit this behavior, but I have not checked. 

      2.  In a few cases, I've plotted the log(likelihood) vs. parameter 
values using various transformations.  When I've done that, I typically 
found that the most nearly parabolic performance used unconstrained 
parameterizations.  This makes asymptotic normality more useful and 
increases the accuracy of simple, approximate sequential Bayesian 
procedures. 

      3.  When I think carefully about a particular application, I often 
find a rationale for claiming that a certain unconstrained 
parameterization provides a better description of the application.  For 
example, interest income on investments is essentially additive on the 
log scale.  Similarly, the concept of "materiality" in Accounting is 
closer to being constant in log space:  One might look for an error of a 
few Euros in the accounts of a very small business, but in auditing some 
major government accounts, errors on the order of a few Euros might not 
be investigated.  Also, measurement errors with microvolts are much 
smaller than with megavolts;  expressing the measurements in decibels 
(i.e., on the log scale) makes the measurement errors more nearly 
comparable. 

      Thanks again for your comments. 
      Best Wishes,
      Spencer Graves        

Peter Dalgaard wrote:

>Spencer Graves <spencer.graves at pdf.com> writes:
>
>  
>
>>Hi, Peter:     What do you do in such situations?     Sundar Dorai-Raj
>>and I have extended "glm" concepts to models driven by a sum of k
>>independent Poissons, with the a linear model for log(defectRate[i])
>>for each source (i = 1:k).  To handle convergence problems, etc., I
>>think we need to use informative Bayes, but we're not there yet.  In
>>any context where things are done more than once [which covers most
>>human activities], informative Bayes seems sensible.     A related
>>question comes with data representing the differences between Poisson
>>counts, e.g., with d[i] = X[i]-X[i-1] = the number of new defects
>>added between steps i-1 and i in a manufacturing process.  Most of the
>>time, d[i] is nonnegative.  However, in some cases, it can be
>>negative, either because of metrology errors in X[i] or because of
>>defect removal between steps i-1 and i.     Comments?
>>    
>>
>
>I haven't got all that much experience with it, but obviously, the
>various algorithms for constrained optimization (box- or otherwise) at
>least allow you to find a proper maximum likelihood estimator.
>
>
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From dmb at mrc-dunn.cam.ac.uk  Thu Nov 25 23:35:22 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Thu, 25 Nov 2004 22:35:22 +0000 (GMT)
Subject: [R] Making legend() look like my plot()
In-Reply-To: <41A64061.60904@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.21.0411252234270.21943-100000@mail.mrc-dunn.cam.ac.uk>

On Thu, 25 Nov 2004, Uwe Ligges wrote:

>Dan Bolser wrote:
>> Is this an impossible task?
>> 
>> How about just problem 2 below, having one pch in one legend entry, but no
>> pch in the second?
>
>Please be at least a little bit patient! This is not a hotline! People 
>are not working 24 hours a day just to answer your questions at once - 
>they are answering questions on a voluntary basis!
>
>answer 1) is not straightforward, but you might want to use one of 
>fillable symbols mentioned in ?points, e.g. number 21
>
>answer 2) pch = c(1, NA) should do the trick.
>
>legend(....., pch=c(21,NA), lwd=c(1,3), lty=c(1,3), pt.bg="white", col=1:2)

Ahhh... I tried pch=c(1,NULL), pt.bg='white' I couldn't work out what was
going on..

thanks very much for the info


>
>Uwe Ligges
>
>
>> 
>> 
>> 
>> On Thu, 25 Nov 2004, Dan Bolser wrote:
>> 
>> 
>>>Hello,
>>>
>>>I am using code like the following to create as simple plot...
>>>
>>>
>>>
>>>plot(x,y,type='b')
>>>lines(lowess(x,y),lwd=3,lty=3,col=2)
>>>
>>>I want to add a legend which shows lines looking exactly like those used
>>>in my plot, i.e. a thin black line with gaps taken up by circles (the
>>>default for type='b', and a thick dashed red line with no pch at all).
>>>
>>>I have two problems, 
>>>
>>>1) making the pch on the first like look like type = 'b' (gaps around pch)
>>>2) surpressing a pch on for the second line
>>>
>>>
>>>Any help with these two problems would be greatly appreciated.
>>>
>>>Any archive of plots and code to browse which could help me visually find
>>>what I want and then copy the code?
>>>
>>>
>>>An online user contributable database of 'graphics in R' would be
>>>smashing.
>>>
>>>
>>>How come some smart people dont just let me do something like
>>>
>>>legend(xpos,ypos,legend=add)
>>>
>>>to add a legend to the current plot for all the relevant points and lines
>>>which have been added so far?
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>>
>> 
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From adslvdll at tpg.com.au  Fri Nov 26 01:12:48 2004
From: adslvdll at tpg.com.au (stephenc)
Date: Fri, 26 Nov 2004 11:12:48 +1100
Subject: [R] support vector machine
Message-ID: <001401c4d34c$aced8900$0d01a8c0@tablet>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041126/d2d5c0b8/attachment.pl

From r-stats at arcriswell.com  Fri Nov 26 03:31:40 2004
From: r-stats at arcriswell.com (Andrew Criswell)
Date: Fri, 26 Nov 2004 09:31:40 +0700
Subject: [R] help with glmmPQL
Message-ID: <41A6958C.1050407@arcriswell.com>

Hello:

Will someone PLEASE help me with this problem. This is the third time 
I've posted it.

When I appply anova() to two equations estimated using glmmPQL, I get a 
complaint,

> anova(fm1, fm2)

Error in anova.lme(fm1, fm2) : Objects must inherit from classes "gls",
"gnls" "lm","lmList", "lme","nlme","nlsList", or "nls"

>

The two equations I estimated are these:

> fm1 <- glmmPQL(choice ~ day + stereotypy,

+                random = ~ 1 | bear, data = learning, family = binomial)

> fm2 <- glmmPQL(choice ~ day + envir + stereotypy,

+                random = ~ 1 | bear, data = learning, family = binomial)

Individually, I get results from anova():

> anova(fm1)

           numDF denDF   F-value p-value
(Intercept)     1  2032   7.95709  0.0048
day             1  2032 213.98391  <.0001
stereotypy      1  2032   0.42810  0.5130

>
> anova(fm2)

           numDF denDF   F-value p-value
(Intercept)     1  2031   5.70343  0.0170
day             1  2031 213.21673  <.0001
envir           1  2031  12.50388  0.0004
stereotypy      1  2031   0.27256  0.6017

>

I did look through the archives but didn't finding anything relevant to 
my problem.

Hope someone can help.

ANDREW
____________________________
        _
platform i586-mandrake-linux-gnu
arch     i586
os       linux-gnu
system   i586, linux-gnu
status
major    2
minor    0.0
year     2004
month    10
day      04
language R



-- 
Andrew R. Criswell, Ph.D.
Graduate School, Bangkok University



From jfox at mcmaster.ca  Fri Nov 26 04:28:50 2004
From: jfox at mcmaster.ca (John Fox)
Date: Thu, 25 Nov 2004 22:28:50 -0500
Subject: [R] Testing for S4 objects
Message-ID: <20041126032849.FSW1899.tomts13-srv.bellnexxia.net@JohnDesktop8300>

Dear r-help list members,

Is there a way to test whether an object is an S4 object? The best that I've
been able to come up with is

	isS4object <- function(object) !(is.null(slotNames(object)))

which assumes that an S4 object has at least one slot. I think this is safe,
but perhaps I'm missing something.

Thanks,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox



From G.Qian at latrobe.edu.au  Fri Nov 26 04:42:31 2004
From: G.Qian at latrobe.edu.au (Guoqi Qian)
Date: Fri, 26 Nov 2004 14:42:31 +1100
Subject: [R] How to expand the size limit of a vector?
Message-ID: <2AF8E46266CFBB489F83E0041360B79804309C85@exchange1.ltu.edu.au>

Hi everybody on this list,

Could somebody please tell me how to expand the size limit of a vector
in R?
In my simulation I get the following error message:

> x=kronecker(diag(1,100),matrix(1,100,100))
Error: cannot allocate vector of size 781250 Kb
In addition: Warning message: 
Reached total allocation of 511Mb: see help(memory.size)

Thanks in advance

Yours sincerely,

Guoqi Qian

-----------------------------------------------------------------
Guoqi Qian                
Department of Statistics  
La Trobe University       
Bundoora, VIC 3086             
AUSTRALIA                 
Email:g.qian at latrobe.edu.au
Tel:  +61 3 9479 2609
Fax:  +61 3 9479 2466
http://www.latrobe.edu.au/www/statistic/



From bitwrit at ozemail.com.au  Fri Nov 26 23:15:45 2004
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Sat, 27 Nov 2004 09:15:45 +1100
Subject: [R] R vs SPSS
In-Reply-To: <41A60698.2030501@rcsi.ie>
References: <20041125121537.22285.qmail@web41212.mail.yahoo.com>
	<41A5F2B4.5040201@vanderbilt.edu> <41A60698.2030501@rcsi.ie>
Message-ID: <20041126040357.MRQZ13237.smta02.mail.ozemail.net@there>

As I started out using SPSS when there was no GUI (in fact, no interactive 
interface at all), I automatically open up the syntax editing window when I 
have to use it. It's a workable text editor, you can run all or part of the 
code at will, and build up a code file in much the same way as R.

On the other hand, it does encourage the user who has not taken Pope to heart 
("A little learning...") to put their data through a high-powered analysis 
while convincing themselves that they know what they are doing. I confess to 
having done it more than once in the past. It was when I began reviewing 
other researcher's papers, and thinking 'This guy didn't know what he was 
doing.' and then, 'And you've done it too, brother.' that I resolved to be 
more circumspect.

Jim



From dvrecko at sfu.ca  Fri Nov 26 05:35:36 2004
From: dvrecko at sfu.ca (dvrecko@sfu.ca)
Date: Thu, 25 Nov 2004 20:35:36 -0800
Subject: [R] Response Surface
Message-ID: <200411260435.iAQ4ZaO2001043@rm-rstar.sfu.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041125/f35ac798/attachment.pl

From deepayan at stat.wisc.edu  Fri Nov 26 06:05:17 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 25 Nov 2004 23:05:17 -0600
Subject: [R] Response Surface
In-Reply-To: <200411260435.iAQ4ZaO2001043@rm-rstar.sfu.ca>
References: <200411260435.iAQ4ZaO2001043@rm-rstar.sfu.ca>
Message-ID: <200411252305.17388.deepayan@stat.wisc.edu>

On Thursday 25 November 2004 22:35, dvrecko at sfu.ca wrote:
> Hi. I'm a student at Simon Fraser University in British Columbia,
> Canada. I can't for the life of me figure out how to plot a 3D
> surface (A 3D response surface to be more specific) in R. I found
> your email address on a web board, and saw someone mention
> wireframe(), but using the help in R yielded no results. Any
> suggestions?

Read

> help(persp)

and run

> example(persp)
> demo(persp)

Deepayan



From maustin at amgen.com  Fri Nov 26 06:05:24 2004
From: maustin at amgen.com (Austin, Matt)
Date: Thu, 25 Nov 2004 21:05:24 -0800
Subject: [R] Response Surface
Message-ID: <E7D5AB4811D20B489622AABA9C53859104E0DBC0@teal-exch.amgen.com>

wireframe() is available in the package lattice.

--Matt

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of dvrecko at sfu.ca
> Sent: Thursday, November 25, 2004 20:36 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Response Surface
> 
> 
> Hi. I'm a student at Simon Fraser University in British 
> Columbia, Canada. I
> can't for the life of me figure out how to plot a 3D surface 
> (A 3D response
> surface to be more specific) in R. I found your email address on a web
> board, and saw someone mention wireframe(), but using the 
> help in R yielded
> no results. Any suggestions?
> 
> Thanks.
> 
> Dean Vrecko
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From Tom.Mulholland at dpi.wa.gov.au  Fri Nov 26 06:09:16 2004
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Fri, 26 Nov 2004 13:09:16 +0800
Subject: [R] Response Surface
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3C910@afhex01.dpi.wa.gov.au>

type ?wireframe rather than wireframe()

Tom Mulholland

-----Original Message-----
From: dvrecko at sfu.ca [mailto:dvrecko at sfu.ca]
Sent: Friday, 26 November 2004 12:36 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Response Surface


Hi. I'm a student at Simon Fraser University in British Columbia, Canada. I
can't for the life of me figure out how to plot a 3D surface (A 3D response
surface to be more specific) in R. I found your email address on a web
board, and saw someone mention wireframe(), but using the help in R yielded
no results. Any suggestions?

Thanks.

Dean Vrecko

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From blindglobe at gmail.com  Fri Nov 26 07:28:03 2004
From: blindglobe at gmail.com (A.J. Rossini)
Date: Fri, 26 Nov 2004 07:28:03 +0100
Subject: [R] help with glmmPQL
In-Reply-To: <41A6958C.1050407@arcriswell.com>
References: <41A6958C.1050407@arcriswell.com>
Message-ID: <1abe3fa9041125222824b799da@mail.gmail.com>

For better or worse, it's holidays in the states.  Very amusing for me
being in a non-Thanksgiving celebrating country.

In addition, it's not a problem.  The complaint is valid.  Probably no
one has coded up the right solution yet for comparison.

I can't recall if one would want those statistics for a binomial
random effects model, but I do recall some issues with model
comparison in that setting, though they are a bit dated (say, 2 years
or so).

On Fri, 26 Nov 2004 09:31:40 +0700, Andrew Criswell
<r-stats at arcriswell.com> wrote:
> Hello:
> 
> Will someone PLEASE help me with this problem. This is the third time
> I've posted it.
> 
> When I appply anova() to two equations estimated using glmmPQL, I get a
> complaint,
> 
> > anova(fm1, fm2)
> 
> Error in anova.lme(fm1, fm2) : Objects must inherit from classes "gls",
> "gnls" "lm","lmList", "lme","nlme","nlsList", or "nls"
> 
> >
> 
> The two equations I estimated are these:
> 
> > fm1 <- glmmPQL(choice ~ day + stereotypy,
> 
> +                random = ~ 1 | bear, data = learning, family = binomial)
> 
> > fm2 <- glmmPQL(choice ~ day + envir + stereotypy,
> 
> +                random = ~ 1 | bear, data = learning, family = binomial)
> 
> Individually, I get results from anova():
> 
> > anova(fm1)
> 
>           numDF denDF   F-value p-value
> (Intercept)     1  2032   7.95709  0.0048
> day             1  2032 213.98391  <.0001
> stereotypy      1  2032   0.42810  0.5130
> 
> >
> > anova(fm2)
> 
>           numDF denDF   F-value p-value
> (Intercept)     1  2031   5.70343  0.0170
> day             1  2031 213.21673  <.0001
> envir           1  2031  12.50388  0.0004
> stereotypy      1  2031   0.27256  0.6017
> 
> >
> 
> I did look through the archives but didn't finding anything relevant to
> my problem.
> 
> Hope someone can help.
> 
> ANDREW
> ____________________________
>        _
> platform i586-mandrake-linux-gnu
> arch     i586
> os       linux-gnu
> system   i586, linux-gnu
> status
> major    2
> minor    0.0
> year     2004
> month    10
> day      04
> language R
> 
> --
> Andrew R. Criswell, Ph.D.
> Graduate School, Bangkok University
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 

best,
-tony

---
A.J. Rossini
blindglobe at gmail.com



From abitbol at sent.com  Fri Nov 26 07:56:02 2004
From: abitbol at sent.com (Jean-Louis Abitbol)
Date: Fri, 26 Nov 2004 07:56:02 +0100
Subject: [R] barplot(2?) with CI from a zero reference line
Message-ID: <1101452162.29558.209490571@webmail.messagingengine.com>

Dear R Users, (and dear Marc)

First of all many thanks for the answers to my previous questions.

I would like to barplot the mean percent change of a variate with it's
CI. Bars should start from the zero reference line to height (in
barplot2).

Is there a way to tweak barplot2, for example,  to do that ? 

I have tried to see what the function was but unlike other functions was
not able to list it by > barplot2. Is it because it is called through
UseMethods ? 

Thanks for any help.

Jean-Louis



From ozric at web.de  Fri Nov 26 08:13:12 2004
From: ozric at web.de (Christian Schulz)
Date: Fri, 26 Nov 2004 08:13:12 +0100
Subject: [R] support vector machine
In-Reply-To: <001401c4d34c$aced8900$0d01a8c0@tablet>
References: <001401c4d34c$aced8900$0d01a8c0@tablet>
Message-ID: <41A6D788.9070608@web.de>


http://www.maths.lth.se/help/R/.R/library/e1071/doc/svmdoc.pdf

regards, christian

stephenc wrote:

>Hi Everyone
> 
>Thanks to those who responded last time.
> 
>I am still having problems.  I really want to find one of those
>tutorials on how to use svm() so I can then get going using it myself.
>Issues are which kernel to choose, how to tune the parameters.  If
>anyone know of a tutorial please let me know.
> 
>Stephen
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From ripley at stats.ox.ac.uk  Fri Nov 26 08:30:08 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 26 Nov 2004 07:30:08 +0000 (GMT)
Subject: [R] Error using glm with poisson family and identity link
In-Reply-To: <41A65CA8.8090508@pdf.com>
References: <41A5D781.70400@pigrecodata.net>
	<Pine.LNX.4.61.0411251535190.28898@gannet.stats>
	<41A614F3.4000102@pdf.com> <x2y8gpai20.fsf@biostat.ku.dk>
	<41A633FC.5010401@pdf.com>
	<x2u0rdac3k.fsf@biostat.ku.dk> <41A65CA8.8090508@pdf.com>
Message-ID: <Pine.LNX.4.61.0411260725330.10707@gannet.stats>

On Thu, 25 Nov 2004, Spencer Graves wrote:

>     I generally avoid constrained optimizers for three reasons: 
>     1.  My experience with them has included many cases where the optimizer 
> would stop with an error when testing parameter values that violate the 
> constraints.  If I transform the parameter space to remove the constraints, 
> that never happens.  The constrained optimizers in R 2.0.1 may not exhibit 
> this behavior, but I have not checked.

They do not, and OTOH if the MLE really does lie on the boundary (and here 
it may well, with one data point fitted with mean zero) transformation 
will often not find a good solution.

Constrained optimization is a hard problem, good methods are very 
complex and good code to implement them is usually expensive.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Nov 26 08:50:09 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 26 Nov 2004 07:50:09 +0000 (GMT)
Subject: [R] How to expand the size limit of a vector?
In-Reply-To: <2AF8E46266CFBB489F83E0041360B79804309C85@exchange1.ltu.edu.au>
References: <2AF8E46266CFBB489F83E0041360B79804309C85@exchange1.ltu.edu.au>
Message-ID: <Pine.LNX.4.61.0411260743160.10707@gannet.stats>

On Fri, 26 Nov 2004, Guoqi Qian wrote:

> Hi everybody on this list,
>
> Could somebody please tell me how to expand the size limit of a vector
> in R?

You must be using Windows, without telling us. The rw-FAQ tells you the 
answer, as well as the help page that message refers to.  It's better for 
you that you learn to use these resources than ask 2000 people to read 
them for you.

You may also need to get a bigger computer, since you are trying to create 
a single object bigger than your computer's RAM memory and a high 
proportion of the address space of a Windows machine.  It might be more 
sensible to ask your more senior colleagues for advice on your statistical 
computing and see if you can avoid this computation.

> In my simulation I get the following error message:
>
>> x=kronecker(diag(1,100),matrix(1,100,100))
> Error: cannot allocate vector of size 781250 Kb
> In addition: Warning message:
> Reached total allocation of 511Mb: see help(memory.size)

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

which asks you to read the rw-FAQ before posting.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Tom.Mulholland at dpi.wa.gov.au  Fri Nov 26 08:55:13 2004
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Fri, 26 Nov 2004 15:55:13 +0800
Subject: [R] barplot(2?) with CI from a zero reference line
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3C912@afhex01.dpi.wa.gov.au>

I didn't know how to do this but I knew it had to been asked about.

Try getS3method("barplot2","default")

Make sure you've loaded gplots. I guessed "default", but I wonder how you would find out the class if it had been something else. I guess that's something to work on when I'm next twiddling my thumbs.

Tom Mulholland

-----Original Message-----
From: Jean-Louis Abitbol [mailto:abitbol at sent.com]
Sent: Friday, 26 November 2004 2:56 PM
To: r-help at stat.math.ethz.ch
Subject: [R] barplot(2?) with CI from a zero reference line


Dear R Users, (and dear Marc)

First of all many thanks for the answers to my previous questions.

I would like to barplot the mean percent change of a variate with it's
CI. Bars should start from the zero reference line to height (in
barplot2).

Is there a way to tweak barplot2, for example,  to do that ? 

I have tried to see what the function was but unlike other functions was
not able to list it by > barplot2. Is it because it is called through
UseMethods ? 

Thanks for any help.

Jean-Louis

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From maechler at stat.math.ethz.ch  Fri Nov 26 09:17:59 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 26 Nov 2004 09:17:59 +0100
Subject: [R] Testing for S4 objects
In-Reply-To: <20041126032849.FSW1899.tomts13-srv.bellnexxia.net@JohnDesktop8300>
References: <20041126032849.FSW1899.tomts13-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <16806.59063.246234.858186@gargle.gargle.HOWL>

>>>>> "JohnF" == John Fox <jfox at mcmaster.ca>
>>>>>     on Thu, 25 Nov 2004 22:28:50 -0500 writes:

    JohnF> Dear r-help list members, Is there a way to test
    JohnF> whether an object is an S4 object? The best that I've
    JohnF> been able to come up with is

    JohnF> 	isS4object <- function(object) !(is.null(slotNames(object)))

you can drop one pair of "(..)" to give

  isS4object <- function(object) !is.null(slotNames(object))


    JohnF> which assumes that an S4 object has at least one
    JohnF> slot. I think this is safe, but perhaps I'm missing
    JohnF> something.

The question is a very good one -- that I have posed to R-core a
while ago myself.

Inside  utils:::str.default  {which doesn't show the many
commments in the *source* of str.default()}, I have wanted a way
that even works when the 'methods' package is not attached and
use the more obscure 

    #NOT yet:if(has.class <- !is.null(cl <- class(object)))
    if(has.class <- !is.null(cl <- attr(object, "class")))# S3 or S4 class
	S4 <- !is.null(attr(cl, "package"))## <<<'kludge' FIXME!
	##or length(methods::getSlots(cl)) > 0

For the time being, I'd keep your function, but I don't think
we'd guarantee that it will remain the appropriate test in all
future.  But till then many things will have happened (if not
all of them ;-).

Martin Maechler, ETH Zurich



From Anne.Olga.Piotet at omsv.vd.ch  Fri Nov 26 09:29:41 2004
From: Anne.Olga.Piotet at omsv.vd.ch (Anne Piotet)
Date: Fri, 26 Nov 2004 09:29:41 +0100
Subject: [R] multiple logistic regression
Message-ID: <004901c4d392$131f7090$83dad10a@prod.omsv.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041126/e7fcb77d/attachment.pl

From maechler at stat.math.ethz.ch  Fri Nov 26 09:29:29 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 26 Nov 2004 09:29:29 +0100
Subject: [R] Making legend() look like my plot()
In-Reply-To: <Pine.LNX.4.21.0411252234270.21943-100000@mail.mrc-dunn.cam.ac.uk>
References: <41A64061.60904@statistik.uni-dortmund.de>
	<Pine.LNX.4.21.0411252234270.21943-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <16806.59753.614470.553594@gargle.gargle.HOWL>

>>>>> "Dan" == Dan Bolser <dmb at mrc-dunn.cam.ac.uk>
>>>>>     on Thu, 25 Nov 2004 22:35:22 +0000 (GMT) writes:

    Dan> On Thu, 25 Nov 2004, Uwe Ligges wrote:
    >> Dan Bolser wrote:
    >>> Is this an impossible task?
    >>> 
    >>> How about just problem 2 below, having one pch in one
    >>> legend entry, but no pch in the second?
    >>  Please be at least a little bit patient! This is not a
    >> hotline! People are not working 24 hours a day just to
    >> answer your questions at once - they are answering
    >> questions on a voluntary basis!
    >> 
    >> answer 1) is not straightforward, but you might want to
    >> use one of fillable symbols mentioned in ?points,
    >> e.g. number 21
    >> 
    >> answer 2) pch = c(1, NA) should do the trick.
    >> 
    >> legend(....., pch=c(21,NA), lwd=c(1,3), lty=c(1,3),
    >>               pt.bg="white", col=1:2)

    Dan> Ahhh... I tried pch=c(1,NULL), pt.bg='white' 
    Dan> I couldn't work out what was going on..

hmm, really,... at least you could have tried to see  what
c(1,NULL) is, by just "typing it at the prompt" !

Martin



From gregor.gorjanc at bfro.uni-lj.si  Fri Nov 26 10:45:39 2004
From: gregor.gorjanc at bfro.uni-lj.si (Gregor GORJANC)
Date: Fri, 26 Nov 2004 09:45:39 +0000
Subject: [R] [Fwd: modes again]
Message-ID: <41A6FB43.6070003@bfro.uni-lj.si>

Hello!

Ross thanks for the code. I have made a comparison with SAS, and your 
code (simpleMode, which is also pasted bellow) works well for 
categorical as well as for continous data. However there are problems 
with multiple modes.

I have sent this mail also to the R-help list so everybody can benefit. 
I hope you do not mind.

---------------------------------------------------------------

#returns the lowest mode of a value set
#finds the mode of a data set using the sort function
#call with source simplemode(var)

simpleMode=function(var)
{
#create a table with the values sorted by frequency
sortvar=sort(table(var))

#put the most freq value (the last one) and its count into a variable
modeTable=sortvar[length(sortvar)]

#get the name (value) of the most frequent value
themoderesult=names(modeTable)

#get the count of the most frequent value (one could use min or max, but 
since there is only one, this method is simpler)
maxfreq=as.vector(modeTable)

cat("Mode: ",themoderesult, "(frequency: ",maxfreq,")")
}

-------- Original Message --------
Subject: modes again
Date: Mon, 22 Nov 2004 08:49:54 -0800
From: Ross Nelson <rnelson at cariboo.bc.ca>
To: gregor.gorjanc at bfro.uni-lj.si

Attached are two short functions to generate modes.

Place them in your working directory.

call the simpleMode file with

source("simpleMode")
simpleMode(var)

where var is a vector variable.


call the mode2 file with

source("mode2")
mode2(var)

where var is a vector variable.


A description of the mode2 function is available from
http://cran.r-project.org/doc/contrib/Lemon-kickstart/kr_dstat.html

I wrote the other.


-- 
Lep pozdrav / With regards / Con respeto,
     Gregor GORJANC

---------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty       URI: http://www.bfro.uni-lj.si
Zootechnical Department    mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                  tel: +386 (0)1 72 17 861
SI-1230 Domzale            fax: +386 (0)1 72 41 005
Slovenia



From ozric at web.de  Fri Nov 26 09:58:10 2004
From: ozric at web.de (Christian Schulz)
Date: Fri, 26 Nov 2004 09:58:10 +0100
Subject: [R] conditional replacement - readLines
Message-ID: <41A6F022.9000907@web.de>

Hi,

i have list with 2575  cgi files which i have read with readLines - all 
Lines have the type "chr".
Now i want replace the 2. line with a condition in 4. line and  write  
the files back  to dir.

How i could avoid the incompatible problem, or is it better
doing something with awk!?

 for (i in 1:2575){
datenrep <- 
ifelse(daten[[i]][4]=="type,1",cat(substr(as.character(paste(daten[[i]][2])),1,5),as.character(100)
,substr(as.character(paste(daten[[i]][2])),6,12),sep=""),daten[[i]][2])}
lfdn,1001081Error in "[<-"(`*tmp*`, test & !nas, value = NULL) :
        incompatible types

many thanks, christian



From ripley at stats.ox.ac.uk  Fri Nov 26 10:09:03 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 26 Nov 2004 09:09:03 +0000 (GMT)
Subject: [R] multiple logistic regression
In-Reply-To: <004901c4d392$131f7090$83dad10a@prod.omsv.ch>
References: <004901c4d392$131f7090$83dad10a@prod.omsv.ch>
Message-ID: <Pine.LNX.4.61.0411260907290.11918@gannet.stats>

multinom() in package nnet.

On Fri, 26 Nov 2004, Anne Piotet wrote:

> Is there an utility for multiple logistic regression where the response 
> is not ordinal? As the predictors do not satisfy the necessary 
> hypothesis, I cannot use discriminant analysis.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From stuart.leask at nottingham.ac.uk  Fri Nov 26 10:09:18 2004
From: stuart.leask at nottingham.ac.uk (Stuart Leask)
Date: Fri, 26 Nov 2004 09:09:18 -0000
Subject: [R] R vs SPSS
References: <20041125121537.22285.qmail@web41212.mail.yahoo.com><41A5F2B4.5040201@vanderbilt.edu>
	<41A60698.2030501@rcsi.ie>
Message-ID: <006101c4d397$9c062f70$f2e1f380@OPENZAURUS>

> > What worries me about SPSS is that it often results in poor statistical
> > practice.  The defaults in dialog boxes are not very good in some cases,
> > and like SAS, SPSS tends to lead users to make to many assumptions
> > (linearity in regression being one of the key ones).
> > -- 
> > Frank E Harrell Jr   Professor and Chair           School of Medicine
> >                       Department of Biostatistics   Vanderbilt
University
> >
> > ______________________________________________
> >
> My worry about SPSS is that it encourages people to do analysis and
> dataset manipulation 'on-the-fly', without leaving behind an audit trail
> that can be used to reconstruct the dataset and results. Certainly, SPSS
> has a 'paste' button which allows you to save a 'syntax' file of
> commands, but most users appear to ignore it. And post-hoc editing of
> graphs and tables cannot be saved thus (unless I'm missing out something
> here).

Hear hear! I found that using SPSS left me knee-deep in un-documented
'intermediate datasets' and graphs I couldn't reproduce unless I spent an
age fiddling. And as for auto-labelled graph axes running from eg. -0.5-6.5
by units of 2, when I'd rather prefer 0-10... ugh, the memories are not
good. And if I see one more dissertation with dense, graphics-rich but
almost unreadable SPSS tables that have clearly just not been thought out...

For me it's not about features, it is the sloppy working styles SPSS
encourages. Take off the GUI and it's probably not too bad(!).

Stuart


This message has been scanned but we cannot guarantee that it and any
attachments are free from viruses or other damaging content: you are
advised to perform your own checks.  Email communications with the
University of Nottingham may be monitored as permitted by UK legislation.



From gregor.gorjanc at bfro.uni-lj.si  Fri Nov 26 11:25:11 2004
From: gregor.gorjanc at bfro.uni-lj.si (Gregor GORJANC)
Date: Fri, 26 Nov 2004 10:25:11 +0000
Subject: [R] Modes again
Message-ID: <41A70487.3030502@bfro.uni-lj.si>

Hello!

I write once more. I slightly modified your code Ross and now it returns 
all modes from the distribution. I changed the name of the funtion to 
distinguish the difference. I do not like the name simpleMode, however 
mode is already taken by another function (The (Storage) Mode of an Object).

It would be nice to have some additional options for number of 
observations in modes, however this is above my current programming 
skills in R. I don't like the use of cat for returning the result. That 
is the reason I am sending this mail also to R-devel list. Maybe anybody 
there can solve the issue. Quite some users asked for mode calculation 
function and it would really nice to have it in R by default.

modus <- function (x) {

     # Returns the modes of a value set
     # Based on Ross NELSONs code from 2004-11-16 (on R-help mailling list)

     # create a table with the values sorted by frequency
     sortvar <- sort(table(x))
     # get the max frequency
     maxfreq <- max(sortvar)
     # find values with high frequency
     modeTable <- which(sortvar == maxfreq)
     # get the name (value) of the most frequent value
     themoderesult <- names(modeTable)
     # Report
     return(themoderesult)

     # data(CO2)
     # modus(CO2$uptake)
     # rm(CO2)

}

-- 
Lep pozdrav / With regards / Con respeto,
     Gregor GORJANC

---------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty       URI: http://www.bfro.uni-lj.si
Zootechnical Department    mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                  tel: +386 (0)1 72 17 861
SI-1230 Domzale            fax: +386 (0)1 72 41 005
Slovenia



From dmb at mrc-dunn.cam.ac.uk  Fri Nov 26 10:32:26 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Fri, 26 Nov 2004 09:32:26 +0000 (GMT)
Subject: [R] Making legend() look like my plot()
In-Reply-To: <16806.59753.614470.553594@gargle.gargle.HOWL>
Message-ID: <Pine.LNX.4.21.0411260930170.21943-100000@mail.mrc-dunn.cam.ac.uk>

On Fri, 26 Nov 2004, Martin Maechler wrote:

>>>>>> "Dan" == Dan Bolser <dmb at mrc-dunn.cam.ac.uk>
>>>>>>     on Thu, 25 Nov 2004 22:35:22 +0000 (GMT) writes:
>
>    Dan> On Thu, 25 Nov 2004, Uwe Ligges wrote:
>    >> Dan Bolser wrote:
>    >>> Is this an impossible task?
>    >>> 
>    >>> How about just problem 2 below, having one pch in one
>    >>> legend entry, but no pch in the second?
>    >>  Please be at least a little bit patient! This is not a
>    >> hotline! People are not working 24 hours a day just to
>    >> answer your questions at once - they are answering
>    >> questions on a voluntary basis!
>    >> 
>    >> answer 1) is not straightforward, but you might want to
>    >> use one of fillable symbols mentioned in ?points,
>    >> e.g. number 21
>    >> 
>    >> answer 2) pch = c(1, NA) should do the trick.
>    >> 
>    >> legend(....., pch=c(21,NA), lwd=c(1,3), lty=c(1,3),
>    >>               pt.bg="white", col=1:2)
>
>    Dan> Ahhh... I tried pch=c(1,NULL), pt.bg='white' 
>    Dan> I couldn't work out what was going on..
>
>hmm, really,... at least you could have tried to see  what
>c(1,NULL) is, by just "typing it at the prompt" !

Too long at the SQL prompt made me think it was immutably correct.

That and the fact that c(1,'') was making both pch dissapear made me think
the whole thing was messed up.


How come legend isn't ablt to auto detect the line types in my plot and
add them automatically?

Is this just an issue of 'that would be cool if I had time', or is it more
fundamental?

Cheers,
Dan.


>
>Martin
>



From Ted.Harding at nessie.mcc.ac.uk  Fri Nov 26 10:39:08 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 26 Nov 2004 09:39:08 -0000 (GMT)
Subject: [R] Error using glm with poisson family and identity link
In-Reply-To: <Pine.LNX.4.61.0411260725330.10707@gannet.stats>
Message-ID: <XFMail.041126093908.Ted.Harding@nessie.mcc.ac.uk>

On 26-Nov-04 Prof Brian Ripley wrote:
> On Thu, 25 Nov 2004, Spencer Graves wrote:
> 
>>  I generally avoid constrained optimizers for three reasons: 
>>  1.  My experience with them has included many cases where the
>>  optimizer would stop with an error when testing parameter
>>  values that violate the constraints.  If I transform the
>>  parameter space to remove the constraints, that never happens. 
>>  The constrained optimizers in R 2.0.1 may not exhibit 
>>  this behavior, but I have not checked.
> 
> They do not, and OTOH if the MLE really does lie on the boundary
> (and here it may well, with one data point fitted with mean zero)
> transformation will often not find a good solution.
> 
> Constrained optimization is a hard problem, good methods are very 
> complex and good code to implement them is usually expensive.

I've had success with awkward constrained optimisation using the
Nelder-Mead procedure -- simply make the region beyond the constraint
hot enough for the simplex to recoil on touching it.

I once tested it in what I thought might be a tough case by
planting "trees" all over a shallow quadratic bowl. More
precisely, many circular regions which were not to be entered.
The objective was to locate the minimum of the bowl.

The function was defined so as to take a high value inside
each circular region.

The simplex took a long time to locate the minimum, spending
a lot of the time feeling its way slowly round the "trunks"
of the "trees", and some of the time trotting downhill over
open space, but it did get there. Nelder-Mead is pretty robust.

Best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 26-Nov-04                                       Time: 09:39:08
------------------------------ XFMail ------------------------------



From Stefano.Guazzetti at ausl.re.it  Fri Nov 26 11:07:29 2004
From: Stefano.Guazzetti at ausl.re.it (Guazzetti Stefano)
Date: Fri, 26 Nov 2004 11:07:29 +0100
Subject: [R] unexpected behaviour of 'curve' function
Message-ID: <B8A1EED732379B44A7E59D22E82E4442365380@IMHOTEP.ausl.org>

Dear all, 

curve(x^3*(1-x)^7, from = 0, to = 1)

works as  expected but, omitting the "xlim" or the "to" 
and "from" arguments and calling "curve" more than once:

par(mfrow = c(2,2))
for (i in 1:4)
curve(x^3*(1-x)^7)

gives an expected (al least to me) result.
Note also that a "pu" object is returned by curve

> pu
[1] -0.1802445  1.1802445

The behaviour is reproducible with both  R 2.0.0
and R 2.0.1

I can see that a promise of evaluation of "pu"
is made within curve but I cannot understand completely
what happens.

Thanks in advance, 

Stefano Guazzetti


platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    0.0            
year     2004           
month    10             
day      04             
language R



From Anne.Olga.Piotet at omsv.vd.ch  Fri Nov 26 11:21:38 2004
From: Anne.Olga.Piotet at omsv.vd.ch (Anne Piotet)
Date: Fri, 26 Nov 2004 11:21:38 +0100
Subject: [R] multiple logistic regression
References: <004901c4d392$131f7090$83dad10a@prod.omsv.ch>
	<Pine.LNX.4.61.0411260907290.11918@gannet.stats>
Message-ID: <006901c4d3a1$b6c6be10$83dad10a@prod.omsv.ch>

Thank you very much!
Anne

----- Original Message ----- 
From: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
To: "Anne Piotet" <Anne.Olga.Piotet at omsv.vd.ch>
Cc: <r-help at stat.math.ethz.ch>
Sent: Friday, November 26, 2004 10:09 AM
Subject: Re: [R] multiple logistic regression


> multinom() in package nnet.
>
> On Fri, 26 Nov 2004, Anne Piotet wrote:
>
> > Is there an utility for multiple logistic regression where the response
> > is not ordinal? As the predictors do not satisfy the necessary
> > hypothesis, I cannot use discriminant analysis.
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From rdiaz at cnio.es  Fri Nov 26 11:38:51 2004
From: rdiaz at cnio.es (Ramon Diaz-Uriarte)
Date: Fri, 26 Nov 2004 12:38:51 +0200
Subject: [R] LDA with previous PCA for dimensionality reduction
In-Reply-To: <m0act63tyj.fsf@bar.nemo-project.org>
References: <41A45F99.8030808@gmx.ch>
	<Pine.LNX.4.51.0411241535040.19391@artemis.imbe.med.uni-erlangen.de>
	<m0act63tyj.fsf@bar.nemo-project.org>
Message-ID: <200411261138.51926.rdiaz@cnio.es>

Dear Cristoph, David, Torsten and Bj??rn-Helge,

I think that Bj??rn-Helge has made more explicit what I had in mind (which I 
think is close also to what David mentioned). As well, at the very least, not 
placing the PCA inside the cross-validation will underestimate the variance 
in the predictions.

Best,

R.


On Thursday 25 November 2004 15:05, Bj??rn-Helge Mevik wrote:
> Torsten Hothorn writes:
> > as long as one does not use the information in the response (the class
> > variable, in this case) I don't think that one ends up with an
> > optimistically biased estimate of the error
>
> I would be a little careful, though.  The left-out sample in the
> LDA-cross-validation, will still have influenced the PCA used to build
> the LDA on the rest of the samples.  The sample will have a tendency
> to lie closer to the centre of the "complete" PCA than of a PCA on the
> remaining samples.  Also, if the sample has a high leverage on the
> PCA, the directions of the two PCAs can be quite different.  Thus, the
> LDA is built on data that "fits" better to the left-out sample than if
> the sample was a completely new sample.
>
> I have no proofs or numerical studies showing that this gives
> over-optimistic error rates, but I would not recommend placing the PCA
> "outside" the cross-validation.  (The same for any resampling-based
> validation.)

-- 
Ram??n D??az-Uriarte
Bioinformatics Unit
Centro Nacional de Investigaciones Oncol??gicas (CNIO)
(Spanish National Cancer Center)
Melchor Fern??ndez Almagro, 3
28029 Madrid (Spain)
Fax: +-34-91-224-6972
Phone: +-34-91-224-6900

http://ligarto.org/rdiaz
PGP KeyID: 0xE89B3462
(http://ligarto.org/rdiaz/0xE89B3462.asc)



From ligges at statistik.uni-dortmund.de  Fri Nov 26 11:45:35 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 26 Nov 2004 11:45:35 +0100
Subject: [R] Making legend() look like my plot()
In-Reply-To: <Pine.LNX.4.21.0411260930170.21943-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0411260930170.21943-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <41A7094F.5050908@statistik.uni-dortmund.de>

Dan Bolser wrote:

> On Fri, 26 Nov 2004, Martin Maechler wrote:
> 
> 
>>>>>>>"Dan" == Dan Bolser <dmb at mrc-dunn.cam.ac.uk>
>>>>>>>    on Thu, 25 Nov 2004 22:35:22 +0000 (GMT) writes:
>>
>>   Dan> On Thu, 25 Nov 2004, Uwe Ligges wrote:
>>   >> Dan Bolser wrote:
>>   >>> Is this an impossible task?
>>   >>> 
>>   >>> How about just problem 2 below, having one pch in one
>>   >>> legend entry, but no pch in the second?
>>   >>  Please be at least a little bit patient! This is not a
>>   >> hotline! People are not working 24 hours a day just to
>>   >> answer your questions at once - they are answering
>>   >> questions on a voluntary basis!
>>   >> 
>>   >> answer 1) is not straightforward, but you might want to
>>   >> use one of fillable symbols mentioned in ?points,
>>   >> e.g. number 21
>>   >> 
>>   >> answer 2) pch = c(1, NA) should do the trick.
>>   >> 
>>   >> legend(....., pch=c(21,NA), lwd=c(1,3), lty=c(1,3),
>>   >>               pt.bg="white", col=1:2)
>>
>>   Dan> Ahhh... I tried pch=c(1,NULL), pt.bg='white' 
>>   Dan> I couldn't work out what was going on..
>>
>>hmm, really,... at least you could have tried to see  what
>>c(1,NULL) is, by just "typing it at the prompt" !
> 
> 
> Too long at the SQL prompt made me think it was immutably correct.
> 
> That and the fact that c(1,'') was making both pch dissapear made me think
> the whole thing was messed up.
> 
> 
> How come legend isn't ablt to auto detect the line types in my plot and
> add them automatically?
> 
> Is this just an issue of 'that would be cool if I had time', or is it more
> fundamental?

It is really fundamental!
[Maybe not that fundamental for lattice as for base graphics.]

Uwe Ligges



> Cheers,
> Dan.
> 
> 
> 
>>Martin
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From f.gherardini at pigrecodata.net  Fri Nov 26 12:54:17 2004
From: f.gherardini at pigrecodata.net (Federico Gherardini)
Date: Fri, 26 Nov 2004 12:54:17 +0100
Subject: [R] Error using glm with poisson family and identity link
In-Reply-To: <XFMail.041126093908.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.041126093908.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <41A71969.5040008@pigrecodata.net>

Thanks very much to everybody for your replies and comments!

Cheers,

Federico



From Catherine.Eng at scbiol.uhp-nancy.fr  Fri Nov 26 12:11:29 2004
From: Catherine.Eng at scbiol.uhp-nancy.fr (Catherine Eng)
Date: Fri, 26 Nov 2004 12:11:29 +0100
Subject: [R] R information
Message-ID: <1101467489.41a70f61c9c9f@poincare.uhp-nancy.fr>

Hello everybody,

I'm a beginner to R language. That's why I have some questions deal with
graphs.
In fact, I would like to know if it is possible to draw a plot with 2 y axis
against x axis (like gnuplot with the function replot)? 
In fact, I would like to plot in y1 axis, a temperature value for x values and
in the same plot, plot y2 axis which represents the density for x values. And
so  analyse the data supperposition .

I search this information into the fullmanual, but nothing help me! 
Someboby could tell me if this plot is possible? 

Thanks a lot for your attention,

Best regards,

                Catherine ENG



From p.dalgaard at biostat.ku.dk  Fri Nov 26 12:27:35 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 26 Nov 2004 12:27:35 +0100
Subject: [R] unexpected behaviour of 'curve' function
In-Reply-To: <B8A1EED732379B44A7E59D22E82E4442365380@IMHOTEP.ausl.org>
References: <B8A1EED732379B44A7E59D22E82E4442365380@IMHOTEP.ausl.org>
Message-ID: <x2oehkx32w.fsf@biostat.ku.dk>

"Guazzetti Stefano" <Stefano.Guazzetti at ausl.re.it> writes:

> Dear all, 
> 
> curve(x^3*(1-x)^7, from = 0, to = 1)
> 
> works as  expected but, omitting the "xlim" or the "to" 
> and "from" arguments and calling "curve" more than once:
> 
> par(mfrow = c(2,2))
> for (i in 1:4)
> curve(x^3*(1-x)^7)
> 
> gives an expected (al least to me) result.
> Note also that a "pu" object is returned by curve
> 
> > pu
> [1] -0.1802445  1.1802445
> 
> The behaviour is reproducible with both  R 2.0.0
> and R 2.0.1
> 
> I can see that a promise of evaluation of "pu"
> is made within curve but I cannot understand completely
> what happens.

I think the upshot is just that it is taking the "from" and "to" from
par("usr") from the previous plot and every time you do so, it expands
a little (about 4%). Notice that it does not happen if you do this:

 par(mfrow = c(2,2), xaxs="i")
 for (i in 1:4)
    curve(x^3*(1-x)^7)

Having pu end up in the global environment looks like a bug in
delay() but I don't think it has any influence on this particular
effect. 


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From chencheva at gmail.com  Fri Nov 26 12:38:24 2004
From: chencheva at gmail.com (Hu Chen)
Date: Fri, 26 Nov 2004 19:38:24 +0800
Subject: [R] problem of writting large data into files
Message-ID: <6f3fc9ee0411260338158b5384@mail.gmail.com>

Hi all.
I used to use write.table() to write data into file. However , it
seems that write.table() waste a lot of memory on caching temporary
variables. Thus it ofen fails on writting larg matrix/frame into file.
I noticed that write.matrix() may be better. But how could I write the
column names and row names into file, such as "date" "buy" "sell",
instead of "[,1]" , "[,2]" , "[,3]"?
Anyone has some experience about this? Any other functions work better?
Thanks in advance.
                             Tiger



From mkondrin at hppi.troitsk.ru  Sat Nov 27 15:53:21 2004
From: mkondrin at hppi.troitsk.ru (M.Kondrin)
Date: Sat, 27 Nov 2004 06:53:21 -0800
Subject: [R] Generating R wrapper for C++ code.
Message-ID: <41A894E1.7050305@hppi.troitsk.ru>

Hello!
I have an intention of making a R-wrapper for VTK library ( 
www.kitware.com ). The libray itself is quite extensive  so write an R/C 
code for the  wrapper by hand seems to be impossible. VTK library has 
automatically generated wrappers for Python and Tcl so generating code 
for just another language would not be hard. The question is - what tool 
I should use for that? There is a SWIG tool, but using it means I have 
to insert wrappers for R into it. Another possibility is using 
RGccTranscript from Omegahat (but it looks to me it is suitable only for 
interfacing C-code - not C++ one). Is there an example of building 
generators of R code from C++ source? If someone has it, would not you 
share it, please?



From ligges at statistik.uni-dortmund.de  Fri Nov 26 12:49:04 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 26 Nov 2004 12:49:04 +0100
Subject: [R] problem of writting large data into files
In-Reply-To: <6f3fc9ee0411260338158b5384@mail.gmail.com>
References: <6f3fc9ee0411260338158b5384@mail.gmail.com>
Message-ID: <41A71830.9040002@statistik.uni-dortmund.de>

Hu Chen wrote:

> Hi all.
> I used to use write.table() to write data into file. However , it
> seems that write.table() waste a lot of memory on caching temporary
> variables. Thus it ofen fails on writting larg matrix/frame into file.
> I noticed that write.matrix() may be better. But how could I write the
> column names and row names into file, such as "date" "buy" "sell",
> instead of "[,1]" , "[,2]" , "[,3]"?

What about specifying colnames for the matrix / data frame?
[I guess we are talking about package MASS.]

Uwe Ligges





> Anyone has some experience about this? Any other functions work better?
> Thanks in advance.
>                              Tiger
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Fri Nov 26 12:50:30 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 26 Nov 2004 12:50:30 +0100
Subject: [R] R information
In-Reply-To: <1101467489.41a70f61c9c9f@poincare.uhp-nancy.fr>
References: <1101467489.41a70f61c9c9f@poincare.uhp-nancy.fr>
Message-ID: <41A71886.3050302@statistik.uni-dortmund.de>

Catherine Eng wrote:

> Hello everybody,
> 
> I'm a beginner to R language. That's why I have some questions deal with
> graphs.
> In fact, I would like to know if it is possible to draw a plot with 2 y axis
> against x axis (like gnuplot with the function replot)? 
> In fact, I would like to plot in y1 axis, a temperature value for x values and
> in the same plot, plot y2 axis which represents the density for x values. And
> so  analyse the data supperposition .
> 
> I search this information into the fullmanual, but nothing help me! 
> Someboby could tell me if this plot is possible? 
> 
> Thanks a lot for your attention,

Please search in the R-help mail archives (as the posting guide suggests).
This topic has been discussed hundreds of times during the last years.

Uwe Ligges


> Best regards,
> 
>                 Catherine ENG
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jfox at mcmaster.ca  Fri Nov 26 13:29:52 2004
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 26 Nov 2004 07:29:52 -0500
Subject: [R] Testing for S4 objects
In-Reply-To: <16806.59063.246234.858186@gargle.gargle.HOWL>
Message-ID: <20041126122951.GNAS2026.tomts5-srv.bellnexxia.net@JohnDesktop8300>

Dear Martin,

In the context I had in mind (for the Rcmdr package), I can insure that the
methods package is present.

Thanks for the information.

John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: Martin Maechler [mailto:maechler at stat.math.ethz.ch] 
> Sent: Friday, November 26, 2004 3:18 AM
> To: John Fox
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Testing for S4 objects
> 
> >>>>> "JohnF" == John Fox <jfox at mcmaster.ca>
> >>>>>     on Thu, 25 Nov 2004 22:28:50 -0500 writes:
> 
>     JohnF> Dear r-help list members, Is there a way to test
>     JohnF> whether an object is an S4 object? The best that I've
>     JohnF> been able to come up with is
> 
>     JohnF> 	isS4object <- function(object) 
> !(is.null(slotNames(object)))
> 
> you can drop one pair of "(..)" to give
> 
>   isS4object <- function(object) !is.null(slotNames(object))
> 
> 
>     JohnF> which assumes that an S4 object has at least one
>     JohnF> slot. I think this is safe, but perhaps I'm missing
>     JohnF> something.
> 
> The question is a very good one -- that I have posed to 
> R-core a while ago myself.
> 
> Inside  utils:::str.default  {which doesn't show the many 
> commments in the *source* of str.default()}, I have wanted a 
> way that even works when the 'methods' package is not 
> attached and use the more obscure 
> 
>     #NOT yet:if(has.class <- !is.null(cl <- class(object)))
>     if(has.class <- !is.null(cl <- attr(object, "class")))# 
> S3 or S4 class
> 	S4 <- !is.null(attr(cl, "package"))## <<<'kludge' FIXME!
> 	##or length(methods::getSlots(cl)) > 0
> 
> For the time being, I'd keep your function, but I don't think 
> we'd guarantee that it will remain the appropriate test in 
> all future.  But till then many things will have happened (if 
> not all of them ;-).
> 
> Martin Maechler, ETH Zurich
>



From Zack.Apoian at sac.com  Fri Nov 26 15:34:14 2004
From: Zack.Apoian at sac.com (Apoian, Zack)
Date: Fri, 26 Nov 2004 09:34:14 -0500
Subject: [R] plotting multiple series in one plot
Message-ID: <D35A7FAE1220DB49A018F51A225565E8059FE5@mailisny3.saccap.int>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041126/8e9b42af/attachment.pl

From jonsen at mathstat.dal.ca  Fri Nov 26 16:08:31 2004
From: jonsen at mathstat.dal.ca (Ian Jonsen)
Date: Fri, 26 Nov 2004 11:08:31 -0400
Subject: [R] Multivariate t - parameter estimation
Message-ID: <41A746EF.7060105@mathstat.dal.ca>

Hi There,

I would like to estimate the parameters of a multi-variate 
t-distribution from a series of bivariate data. Can anyone suggest how 
this might be done in R? I typically use nlm() for fitting univariate 
distributions.

Thanks,

Ian

-- 
Ian Jonsen, Postdoctoral Fellow

Biology Dept., Dalhousie University, Halifax, NS CANADA B2H 4J1
office: 804 LSC
voice:  +1 902 494 3910
fax:    +1 902 494 3736
email:  jonsen at mathstat.dal.ca



From dimitris.rizopoulos at med.kuleuven.ac.be  Fri Nov 26 16:11:58 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Fri, 26 Nov 2004 16:11:58 +0100
Subject: [R] plotting multiple series in one plot
References: <D35A7FAE1220DB49A018F51A225565E8059FE5@mailisny3.saccap.int>
Message-ID: <01c001c4d3ca$46603060$0540210a@www.domain>

Hi Zack,

you mean something like:

matplot(x$a, as.matrix(cbind(x$b, y$b)), lty=1:2, type="l")
legend(1, 2.5, c("b.x", "b.y"), lty=1:2, col=1:2)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Apoian, Zack" <Zack.Apoian at sac.com>
To: "'R-help at lists.R-project.org'" <R-help at stat.math.ethz.ch>
Sent: Friday, November 26, 2004 3:34 PM
Subject: [R] plotting multiple series in one plot


> say we have:
>
>> x<-data.frame(a=c(1,2,3,4,5),b=c(1,1,1.5,2,2))
>> y<-data.frame(a=c(1,2,3,4,5),b=c(1,2,2,3,3))
>
> How would I plot this so that, with the shared $a as the x-axis 
> values, I
> have both $b columns plotted together?  (a comparison of the two?)
>
> thanks.
>
>
> DISCLAIMER: This e-mail message and any attachments are 
> inte...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From stefaan.lhermitte at agr.kuleuven.ac.be  Fri Nov 26 16:19:51 2004
From: stefaan.lhermitte at agr.kuleuven.ac.be (Stefaan Lhermitte)
Date: Fri, 26 Nov 2004 16:19:51 +0100
Subject: [R] Multi-figure plotting
Message-ID: <41A74997.1020902@agr.kuleuven.ac.be>

Dear R-ians,

I have a question concerning plotting different plots on one figure. I 
have written a script to plot an image, a legend (based on different 
rectangles) and a timeseries plot on one figure.
In my R-lagnuage it looks like this (without arguments that are not 
usefull for my question):

#---> I first define the functions
image.data<-function(...){
    ....
    #-->Plot the data
    par(fig=c(0,1,0.25,1),new=TRUE)
    par(pin=c(1.5,(num.y/num.x*1.5)))
    image(c(1:(num.x+1)),c(1:(num.y+1)),output.matrix,xlab="X 
coord",ylab="Y coord",main='NDVI',las =1,cex.axis=0.7, cex.lab=0.7, 
col=topo.colors(255), asp=1)

    #---> Add a legend (colored rectangles) + legend axis (=values)
    par(fig=c(0,1,0.3,0.4),new=TRUE)
    plot.window(xlim = c(0,1), ylim = c(0,1), yaxs="i")
    rect((1:254)/255,0,(2:255)/255,0.1,col=topo.colors(255),border=FALSE)
    axis(1,padj=-1,cex.axis=0.7)
    ...
}

profile.data<-function(...){
    ...
    #-->Plot the data
    par(fig=c(0,1,0,0.3),new=TRUE)
    par(pin=c(1.5,num.y/num.x*0.5))
    plot(NDVI, type="l",cex.axis=0.7, cex.lab=0.7)
    ...
}

When I subsequently use the functions I get problems with the legend. It 
plots my image nice in the upper part of  my pixture, adds the 
timeseries below, but it does not plot the rectangles of the legend. I 
don't know what I do wrong, because it plots perfectly the axis of the 
legend, but the colorbar is invisible. All suggestions would be wonderfull!

Thanx in advance,
Stef



From gregor.gorjanc at bfro.uni-lj.si  Fri Nov 26 17:21:45 2004
From: gregor.gorjanc at bfro.uni-lj.si (Gregor GORJANC)
Date: Fri, 26 Nov 2004 16:21:45 +0000
Subject: [R] hist and truehist
Message-ID: <41A75819.1070300@bfro.uni-lj.si>

Hello!

Up to now I have been using hist() to display the distributions. 
Howevere, I noteiced strange numbers on y (vertical) axis, if I used 
probability = T or freq = F option. I thought it is a bug and launched 
the R-bug system and found some posts on that matter. Brian Ripley 
responded to one, that one should look at truehist() for that. Ok I can 
use truehist() if I want to see the ratios or probabilities, but what is 
then the "density or probability" in hist()?

For example:

# some data
mydata <- c(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,3,4,5)

# histogram with frequencies
hist(mydata)

# histogram with ratios or probabilities
hist(mydata, freq = F) # what are that values on vertical axis

# lets take a look at values behind
x <-hist(mydata, freq = F, plot = F); x

$breaks
[1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0

$counts
[1] 22  1  0  1  0  1  0  1

$intensities
[1] 1.69230735 0.07692308 0.00000000 0.07692308 0.00000000 0.07692308 
0.00000000
[8] 0.07692308

$density
[1] 1.69230735 0.07692308 0.00000000 0.07692308 0.00000000 0.07692308 
0.00000000
[8] 0.07692308

$mids
[1] 1.25 1.75 2.25 2.75 3.25 3.75 4.25 4.75

$xname
[1] "mydata"

$equidist
[1] TRUE

attr(,"class")
[1] "histogram"

# HOW are this intensities and density values calculated? What they 
actually represent?

# MASS packages
library(MASS)

# again histogram with prob = T by default
truehist(mydata) # looks OK

-- 
Lep pozdrav / With regards / Con respeto,
     Gregor GORJANC

---------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty       URI: http://www.bfro.uni-lj.si
Zootechnical Department    mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                  tel: +386 (0)1 72 17 861
SI-1230 Domzale            fax: +386 (0)1 72 41 005
Slovenia



From ggrothendieck at myway.com  Fri Nov 26 16:43:51 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 26 Nov 2004 15:43:51 +0000 (UTC)
Subject: [R] plotting multiple series in one plot
References: <D35A7FAE1220DB49A018F51A225565E8059FE5@mailisny3.saccap.int>
Message-ID: <loom.20041126T163842-917@post.gmane.org>

Apoian, Zack <Zack.Apoian <at> sac.com> writes:

: 
: say we have:
: 
: > x<-data.frame(a=c(1,2,3,4,5),b=c(1,1,1.5,2,2))
: > y<-data.frame(a=c(1,2,3,4,5),b=c(1,2,2,3,3))
: 
: How would I plot this so that, with the shared $a as the x-axis values, I
: have both $b columns plotted together?  (a comparison of the two?)

I have converted your data to ts class since you have
described them as time series and they appear to be regularly
spaced. I have modified your example slightly so that time scales
are not the same just to show that the code still works
in that case.  The code does rely on the series being
regularly spaced (if they are irregular see plot.zoo in the zoo
package).

x<-data.frame(a=c(2,3,4,5,6),b=c(1,1,1.5,2,2))
y<-data.frame(a=c(1,2,3,4),b=c(1,2,2,3))

xts <- ts(x$b,start=x$a[1])
yts <- ts(y$b,start=y$a[1])
ts.plot(xts,yts,col=c("red","blue"))



From MSchwartz at MedAnalytics.com  Fri Nov 26 16:42:04 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 26 Nov 2004 09:42:04 -0600
Subject: [R] barplot(2?) with CI from a zero reference line
In-Reply-To: <1101452162.29558.209490571@webmail.messagingengine.com>
References: <1101452162.29558.209490571@webmail.messagingengine.com>
Message-ID: <1101483724.3049.26.camel@horizons.localdomain>

On Fri, 2004-11-26 at 07:56 +0100, Jean-Louis Abitbol wrote:
> Dear R Users, (and dear Marc)
> 
> First of all many thanks for the answers to my previous questions.
> 
> I would like to barplot the mean percent change of a variate with it's
> CI. Bars should start from the zero reference line to height (in
> barplot2).
> 
> Is there a way to tweak barplot2, for example,  to do that ? 
> 
> I have tried to see what the function was but unlike other functions was
> not able to list it by > barplot2. Is it because it is called through
> UseMethods ? 
> 
> Thanks for any help.
> 
> Jean-Louis

Jean-Louis,

I may be mis-understanding what you are trying to do here, but do you
want to have a horizontal zero reference line in the middle of the plot,
such that you can have positive change bars going up from the line and
negative change bars going down from the line?

If so, the default mechanism will work:

barplot2(c(-1, 2, -3, 5, 4, -4))
abline(h = 0)


If that is the correct basic plot, just adjust the values for the CI's
accordingly.

HTH,

Marc Schwartz



From p.dalgaard at biostat.ku.dk  Fri Nov 26 16:48:47 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 26 Nov 2004 16:48:47 +0100
Subject: [R] hist and truehist
In-Reply-To: <41A75819.1070300@bfro.uni-lj.si>
References: <41A75819.1070300@bfro.uni-lj.si>
Message-ID: <x2fz2wwqzk.fsf@biostat.ku.dk>

Gregor GORJANC <gregor.gorjanc at bfro.uni-lj.si> writes:

> Hello!
> 
> Up to now I have been using hist() to display the distributions.
> Howevere, I noteiced strange numbers on y (vertical) axis, if I used
> probability = T or freq = F option. I thought it is a bug and launched
> the R-bug system and found some posts on that matter. Brian Ripley
> responded to one, that one should look at truehist() for that. Ok I
> can use truehist() if I want to see the ratios or probabilities, but
> what is then the "density or probability" in hist()?

...

> truehist(mydata) # looks OK

And truehist(mydata, h=.5)?

It is a density estimate. The sum of the bar _areas_ should be 1. 

> sum(x$intensities * .5)
[1] 0.9999998


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From mdowle at concordiafunds.com  Fri Nov 26 17:00:13 2004
From: mdowle at concordiafunds.com (Matthew Dowle)
Date: Fri, 26 Nov 2004 16:00:13 -0000
Subject: [R] Tcl error - brace in argument?
Message-ID: <78166BFC5165D811AA0400065BF0324B6B93D0@wisconsin.concordia>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041126/f493be37/attachment.pl

From angelare at to.infn.it  Fri Nov 26 17:25:56 2004
From: angelare at to.infn.it (Angela Re)
Date: Fri, 26 Nov 2004 17:25:56 +0100
Subject: [R] (no subject)
Message-ID: <41A75914.8060608@to.infn.it>

Good afternoon,
I'd like to know how to superimpose a Student distribution pt on a 
histogram.  I think I have to use the plot function but I don,t know the 
details.
Other question: what is a quntile function?
Can you help me?
Thank you.



From p.dalgaard at biostat.ku.dk  Fri Nov 26 17:36:55 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 26 Nov 2004 17:36:55 +0100
Subject: [R] Tcl error - brace in argument?
In-Reply-To: <78166BFC5165D811AA0400065BF0324B6B93D0@wisconsin.concordia>
References: <78166BFC5165D811AA0400065BF0324B6B93D0@wisconsin.concordia>
Message-ID: <x2y8gova6w.fsf@biostat.ku.dk>

Matthew Dowle <mdowle at concordiafunds.com> writes:

> Hi all,
>  
> Does anyone know a solution for this error ?
>  
> > tkwidget(dlg, "iwidgets::spinint", range="{0 23}")

I suspect you want range=as.tclObj(c(0,23)) or something like that,
i.e. a Tcl list of two numbers, not a five-character string.

> Error in structure(.External("dotTclObjv", objv, PACKAGE = "tcltk"), class =
> "tclObj") : 
>         [tcl] wrong # args: should be ".31.1.19 configure -range {begin
> end}".



-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Fri Nov 26 17:39:14 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 26 Nov 2004 17:39:14 +0100
Subject: No answer, Was Re: [R] (no subject)
In-Reply-To: <41A75914.8060608@to.infn.it>
References: <41A75914.8060608@to.infn.it>
Message-ID: <x2u0rcva31.fsf@biostat.ku.dk>

Angela Re <angelare at to.infn.it> writes:

> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Please do. You have been told before.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Rau at demogr.mpg.de  Fri Nov 26 17:43:53 2004
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Fri, 26 Nov 2004 17:43:53 +0100
Subject: [R] (no subject)
Message-ID: <71E4EAD7CBE44248908143EE3C6F835317A0C9@poseidon.demogr.mpg.de>

Hi
-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Angela Re
Sent: Friday, November 26, 2004 5:26 PM
To: r-help at stat.math.ethz.ch
Subject: [R] (no subject)

I'd like to know how to superimpose a Student distribution pt on a 
histogram.  I think I have to use the plot function but I don,t know the

details.
Other question: what is a quntile function?

Since I am not a statistician, I hope my answers are correct.

The quantile function is the inverse of the cumulative distribution
function. Maybe it is easier with an example:
pnorm(0)
asks how much of the probability mass (bounded between 0 and 1) is
smaller or equal to this value. In this case of the standard normal
distribution (mean 0, var=sd=1), it is just half of it.
If you reverse this process you get back to the "x-value", which is what
the quantile function is doing.
qnorm(0.5)
Does this help for the last question?

Code for superimposing the data:

deg.of.free <- 4
hist(rt(n=500, df=deg.of.free), freq=FALSE, breaks=75, xlim=c(-5,5))
xvalues <- seq(from=-5, to=5, length=200)
lines(x=xvalues, y=dt(xvalues, df=deg.of.free), col="blue", lwd=3)

Finally, just two remarks:
1) Please use a sensible subject line in your message.
2) Is this some kind of homework assignment? (looks like a typical one)

Ciao,
Roland




+++++
This mail has been sent through the MPI for Demographic Rese...{{dropped}}



From p.dalgaard at biostat.ku.dk  Fri Nov 26 17:43:14 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 26 Nov 2004 17:43:14 +0100
Subject: [R] Tcl error - brace in argument?
In-Reply-To: <x2y8gova6w.fsf@biostat.ku.dk>
References: <78166BFC5165D811AA0400065BF0324B6B93D0@wisconsin.concordia>
	<x2y8gova6w.fsf@biostat.ku.dk>
Message-ID: <x2pt20v9wd.fsf@biostat.ku.dk>

Peter Dalgaard <p.dalgaard at biostat.ku.dk> writes:

> Matthew Dowle <mdowle at concordiafunds.com> writes:
> 
> > Hi all,
> >  
> > Does anyone know a solution for this error ?
> >  
> > > tkwidget(dlg, "iwidgets::spinint", range="{0 23}")
> 
> I suspect you want range=as.tclObj(c(0,23)) or something like that,
> i.e. a Tcl list of two numbers, not a five-character string.

On second thoughts: I think range=c(0,23) should do.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Ted.Harding at nessie.mcc.ac.uk  Fri Nov 26 17:47:29 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 26 Nov 2004 16:47:29 -0000 (GMT)
Subject: [R] (no subject)
In-Reply-To: <41A75914.8060608@to.infn.it>
Message-ID: <XFMail.041126164729.Ted.Harding@nessie.mcc.ac.uk>

On 26-Nov-04 Angela Re wrote:
> Good afternoon,
> I'd like to know how to superimpose a Student distribution pt on a 
> histogram.  I think I have to use the plot function but I don,t know
> the details.

  x<-rt(5000,4)               # draw a sample from t with df=4
  hist(x,breaks=0.5*(-40:40)) # draw histogram with bin-width=0.5
  x0<-0.1*(-200,200)          # breakpoints for pt plot
  lines(x0,dt(x0,4)*5000*0.5) # superimpose the pt plot

DON'T use 'plot' for the last one: it will create a new plot,
while 'lines' (like 'points') adds a plot to an existing plot.

> Other question: what is a quntile function?

(?quantile) Given a distribution P(X <= x) for a random
variable X, the quantile of the distribution corresponding
to probability p (or percentage 100*p) is the value x_p
such that P(X <= x_p) = p (when using percentages it is usually
called the percentile).

Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 26-Nov-04                                       Time: 16:47:29
------------------------------ XFMail ------------------------------



From tkirsten at izbi.uni-leipzig.de  Fri Nov 26 18:12:22 2004
From: tkirsten at izbi.uni-leipzig.de (Toralf Kirsten)
Date: Fri, 26 Nov 2004 18:12:22 +0100
Subject: [R] sorting a data.frame using a vector
Message-ID: <41A763F6.9080502@izbi.uni-leipzig.de>

Hi all,
I'm looking for an efficient solution (speed and memory) for the 
following problem:
Given
- a data.frame x containing numbers of type double
   with nrow(x)>ncol(x) and unique row lables and
- a character vector y containing a sorted order labels

Now, I'd like to sort the rows of the data.frame x w.r.t. the order of 
labels in y.

example:
x <- data.frame(c(1:4),c(5:8))
row.names(x)<-LETTERS[1:4]
y <- c("C","A","D","B")


My current solution is like this:
if(!is.null(y) && is.vector(y)) {
    nObj <- length(y)
    for (i in 1:nObj) {
      sObj <- y[i]
      k <- c(1:nrow(x))[row.names(x)==sObj]
      if (i != k) {
        names <- row.names(x)
        tObj <- row.names(x[i,])
        temp <- x[i,]
        x[i,] <- x[k,]
        x[k,] <- temp
        names[i] <- sObj
        names[k] <- tObj
        row.names(x) <- names
     }
   }
}

But I'm not happy with it because it is not really efficient. Any other 
suggestions are welcome!

Thanks, Toralf



From p.dalgaard at biostat.ku.dk  Fri Nov 26 18:22:41 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 26 Nov 2004 18:22:41 +0100
Subject: [R] sorting a data.frame using a vector
In-Reply-To: <41A763F6.9080502@izbi.uni-leipzig.de>
References: <41A763F6.9080502@izbi.uni-leipzig.de>
Message-ID: <x2hdncv82m.fsf@biostat.ku.dk>

Toralf Kirsten <tkirsten at izbi.uni-leipzig.de> writes:

> Hi all,
> I'm looking for an efficient solution (speed and memory) for the
> following problem:
> Given
> - a data.frame x containing numbers of type double
>    with nrow(x)>ncol(x) and unique row lables and
> - a character vector y containing a sorted order labels
> 
> Now, I'd like to sort the rows of the data.frame x w.r.t. the order of
> labels in y.
> 
> example:
> x <- data.frame(c(1:4),c(5:8))
> row.names(x)<-LETTERS[1:4]
> y <- c("C","A","D","B")
> 
> 
> My current solution is like this:
> if(!is.null(y) && is.vector(y)) {
>     nObj <- length(y)
>     for (i in 1:nObj) {
>       sObj <- y[i]
>       k <- c(1:nrow(x))[row.names(x)==sObj]
>       if (i != k) {
>         names <- row.names(x)
>         tObj <- row.names(x[i,])
>         temp <- x[i,]
>         x[i,] <- x[k,]
>         x[k,] <- temp
>         names[i] <- sObj
>         names[k] <- tObj
>         row.names(x) <- names
>      }
>    }
> }
> 
> But I'm not happy with it because it is not really efficient. Any
> other suggestions are welcome!

Anything wrong with x[y,] ???

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From m_nica at hotmail.com  Fri Nov 26 18:30:49 2004
From: m_nica at hotmail.com (Mihai Nica)
Date: Fri, 26 Nov 2004 11:30:49 -0600
Subject: [R] Coplot Given text
Message-ID: <BAY18-DAV6956514E36AC3539BCAA4F8BA0@phx.gbl>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041126/73b99a87/attachment.pl

From Carlisle.Thacker at noaa.gov  Fri Nov 26 18:58:23 2004
From: Carlisle.Thacker at noaa.gov (Carlisle Thacker)
Date: Fri, 26 Nov 2004 12:58:23 -0500
Subject: [R] problem with xyplot
Message-ID: <41A76EBF.8060501@noaa.gov>

Dear Rlisters,

When trying to indicate which data belong to which of 7 groups, the
following plot shows only 5 groups:

ss <- trellis.par.get("superpose.symbol")
ss$col <- c("red","blue","green","yellow","orange","magenta","cyan")
ss$pch <- format(1:7)
ss$cex <- 1.2
ss <- trellis.par.get("background")
ss$col <- "white"
trellis.par.set("background",ss)

xyplot(t~s,data=P0,
  subset = p==1500 & whichLonP==2 & whichLatP==3 & t<6 & id>100000000,
  groups = id%/%1000)
# don't see groups 6 and 7 ?????????????

But groups can be put into separate panels:

xyplot(t~s|factor(id%/%1000),data=P0,as.table=TRUE,
  subset = p==1500 & whichLonP==2 & whichLatP==3 & t<6 & id>100000000)

And using a smaller dataframe helps:

temp <- P0[P0$p==1500 & whichLonP==2 & whichLatP==3 & P0$t<6,]
xyplot(t~s,data=temp,
  subset = id>100000000,
  groups = id%/%1000)
# get different labels but all groups

Also, for other plots involving large dataframes, specifying groups made
the plotting very slow.

Is there a bug, or am I screwing up?

Thanks,

Carlisle



From deepayan at stat.wisc.edu  Fri Nov 26 19:32:57 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Fri, 26 Nov 2004 12:32:57 -0600
Subject: [R] problem with xyplot
In-Reply-To: <41A76EBF.8060501@noaa.gov>
References: <41A76EBF.8060501@noaa.gov>
Message-ID: <200411261232.57198.deepayan@stat.wisc.edu>

On Friday 26 November 2004 11:58, Carlisle Thacker wrote:
> Dear Rlisters,
>
> When trying to indicate which data belong to which of 7 groups, the
> following plot shows only 5 groups:
>
> ss <- trellis.par.get("superpose.symbol")
> ss$col <- c("red","blue","green","yellow","orange","magenta","cyan")
> ss$pch <- format(1:7)
> ss$cex <- 1.2
> ss <- trellis.par.get("background")

At which point, all the changes you made to 'superpose/symbol' are lost.

> ss$col <- "white"
> trellis.par.set("background",ss)
>
> xyplot(t~s,data=P0,
>   subset = p==1500 & whichLonP==2 & whichLatP==3 & t<6 &
> id>100000000, groups = id%/%1000)
> # don't see groups 6 and 7 ?????????????

xyplot(1:7 ~ 1:7, groups = 1:7)

works for me (produces 7 distinct colors). I don't see how we can 
diagnose your problem without a (preferably small) reproducible 
example.

> But groups can be put into separate panels:
>
> xyplot(t~s|factor(id%/%1000),data=P0,as.table=TRUE,
>   subset = p==1500 & whichLonP==2 & whichLatP==3 & t<6 &
> id>100000000)
>
> And using a smaller dataframe helps:
>
> temp <- P0[P0$p==1500 & whichLonP==2 & whichLatP==3 & P0$t<6,]
> xyplot(t~s,data=temp,
>   subset = id>100000000,
>   groups = id%/%1000)
> # get different labels but all groups
>
> Also, for other plots involving large dataframes, specifying groups
> made the plotting very slow.

Every level in groups causes a call to grid.points (within each panel), 
so a large number of levels in groups could well make things slow.

Deepayan



From spencer.graves at pdf.com  Fri Nov 26 19:35:49 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 26 Nov 2004 10:35:49 -0800
Subject: [R] help with glmmPQL
In-Reply-To: <1abe3fa9041125222824b799da@mail.gmail.com>
References: <41A6958C.1050407@arcriswell.com>
	<1abe3fa9041125222824b799da@mail.gmail.com>
Message-ID: <41A77785.1020603@pdf.com>

HI, DOUG & JOSE: 

      Is there some reason that "anova.lme" should NOT accept an object 
of class "glmmPQL" in the example below?  If you don't see one either, 
then I suggest you consider modifying the code as described below. 

HI, ANDREW: 

      I couldn't find your data "learning" in my Windows installation of 
R 2.0.0pat, which meant that I had to take the time to find another 
example before I could get the error message you described.  I got it 
from modifying the example in the documentation for "glmmPQL" as follows: 

fit1 <- glmmPQL(y ~ trt, random = ~ 1 | ID,
                     family = binomial, data = bacteria)
fit2 <- glmmPQL(y ~ trt + I(week > 2), random = ~ 1 | ID,
                     family = binomial, data = bacteria)
anova(fit1, fit2)
    Error in anova.lme(fit1, fit2) : Objects must inherit from classes 
"gls", "gnls" "lm","lmList", "lme","nlme","nlsList", or "nls"

      I then checked the class of fit1 and fit2: 
 > class(fit1)
[1] "glmmPQL" "lme"   
 > class(fit2)
[1] "glmmPQL" "lme"   

      As an experiment, I changed the class of fit1 and fit2: 
 > class(fit1) <- "lme"
 > class(fit2) <- "lme"
 > anova(fit1, fit2)
     Model df      AIC      BIC    logLik   Test  L.Ratio p-value
fit1     1  5 1054.623 1071.592 -522.3117                       
fit2     2  6 1113.622 1133.984 -550.8111 1 vs 2 56.99884  <.0001

      Unless someone like Doug or Jose tells us, "Don't do that", I 
would use these answers. 

      I looked briefly at the code for "anova.lme", and it looks to me 
like "glmmPQL" could be added to the list of allowed options in the 
following test roughly half way through the code: 

       if (!all(match(termsClass, c("gls", "gnls", "lm", "lmList",
            "lme", "nlme", "nlsList", "nls"), 0))) {
            stop(paste("Objects must inherit from classes \"gls\", 
\"gnls\"",
                "\"lm\",\"lmList\", \"lme\",\"nlme\",\"nlsList\", or 
\"nls\""))
        }

      hope this helps. 
      spencer graves

A.J. Rossini wrote:

>For better or worse, it's holidays in the states.  Very amusing for me
>being in a non-Thanksgiving celebrating country.
>
>In addition, it's not a problem.  The complaint is valid.  Probably no
>one has coded up the right solution yet for comparison.
>
>I can't recall if one would want those statistics for a binomial
>random effects model, but I do recall some issues with model
>comparison in that setting, though they are a bit dated (say, 2 years
>or so).
>
>On Fri, 26 Nov 2004 09:31:40 +0700, Andrew Criswell
><r-stats at arcriswell.com> wrote:
>  
>
>>Hello:
>>
>>Will someone PLEASE help me with this problem. This is the third time
>>I've posted it.
>>
>>When I appply anova() to two equations estimated using glmmPQL, I get a
>>complaint,
>>
>>    
>>
>>>anova(fm1, fm2)
>>>      
>>>
>>Error in anova.lme(fm1, fm2) : Objects must inherit from classes "gls",
>>"gnls" "lm","lmList", "lme","nlme","nlsList", or "nls"
>>
>>    
>>
>>The two equations I estimated are these:
>>
>>    
>>
>>>fm1 <- glmmPQL(choice ~ day + stereotypy,
>>>      
>>>
>>+                random = ~ 1 | bear, data = learning, family = binomial)
>>
>>    
>>
>>>fm2 <- glmmPQL(choice ~ day + envir + stereotypy,
>>>      
>>>
>>+                random = ~ 1 | bear, data = learning, family = binomial)
>>
>>Individually, I get results from anova():
>>
>>    
>>
>>>anova(fm1)
>>>      
>>>
>>          numDF denDF   F-value p-value
>>(Intercept)     1  2032   7.95709  0.0048
>>day             1  2032 213.98391  <.0001
>>stereotypy      1  2032   0.42810  0.5130
>>
>>    
>>
>>>anova(fm2)
>>>      
>>>
>>          numDF denDF   F-value p-value
>>(Intercept)     1  2031   5.70343  0.0170
>>day             1  2031 213.21673  <.0001
>>envir           1  2031  12.50388  0.0004
>>stereotypy      1  2031   0.27256  0.6017
>>
>>    
>>
>>I did look through the archives but didn't finding anything relevant to
>>my problem.
>>
>>Hope someone can help.
>>
>>ANDREW
>>____________________________
>>       _
>>platform i586-mandrake-linux-gnu
>>arch     i586
>>os       linux-gnu
>>system   i586, linux-gnu
>>status
>>major    2
>>minor    0.0
>>year     2004
>>month    10
>>day      04
>>language R
>>
>>--
>>Andrew R. Criswell, Ph.D.
>>Graduate School, Bangkok University
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>    
>>
>
>
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From deepayan at stat.wisc.edu  Fri Nov 26 20:09:30 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Fri, 26 Nov 2004 13:09:30 -0600
Subject: [R] help with glmmPQL
In-Reply-To: <41A77785.1020603@pdf.com>
References: <41A6958C.1050407@arcriswell.com>
	<1abe3fa9041125222824b799da@mail.gmail.com>
	<41A77785.1020603@pdf.com>
Message-ID: <200411261309.30388.deepayan@stat.wisc.edu>

On Friday 26 November 2004 12:35, Spencer Graves wrote:
> HI, DOUG & JOSE:
>
>       Is there some reason that "anova.lme" should NOT accept an
> object of class "glmmPQL" in the example below?  If you don't see one
> either, then I suggest you consider modifying the code as described
> below.
>
> HI, ANDREW:
>
>       I couldn't find your data "learning" in my Windows installation
> of R 2.0.0pat, which meant that I had to take the time to find
> another example before I could get the error message you described. 
> I got it from modifying the example in the documentation for
> "glmmPQL" as follows:
>
> fit1 <- glmmPQL(y ~ trt, random = ~ 1 | ID,
>                      family = binomial, data = bacteria)
> fit2 <- glmmPQL(y ~ trt + I(week > 2), random = ~ 1 | ID,
>                      family = binomial, data = bacteria)
> anova(fit1, fit2)
>     Error in anova.lme(fit1, fit2) : Objects must inherit from
> classes "gls", "gnls" "lm","lmList", "lme","nlme","nlsList", or "nls"
>
>       I then checked the class of fit1 and fit2:
>  > class(fit1)
>
> [1] "glmmPQL" "lme"
>
>  > class(fit2)
>
> [1] "glmmPQL" "lme"
>
>       As an experiment, I changed the class of fit1 and fit2:
>  > class(fit1) <- "lme"
>  > class(fit2) <- "lme"
>  > anova(fit1, fit2)
>
>      Model df      AIC      BIC    logLik   Test  L.Ratio p-value
> fit1     1  5 1054.623 1071.592 -522.3117
> fit2     2  6 1113.622 1133.984 -550.8111 1 vs 2 56.99884  <.0001
>
>       Unless someone like Doug or Jose tells us, "Don't do that", I
> would use these answers.

These likelihoods are (AFAIK) NOT the likelihoods of the models fitted, 
they are likelihoods of an lme model that approximates it. Thus, the 
test may not be appropriate. Having 'anova(fit1, fit2)' silently 
producing an answer would certainly be misleading.

E.g., lme4 produces the following:

> library(lme4)
> data(bacteria, package = "MASS")
> fit1 <- GLMM(y ~ trt, random = ~ 1 | ID, 
+              family = binomial, data = bacteria) 
> fit2 <- GLMM(y ~ trt + I(week > 2), random = ~ 1 | ID, 
+              family = binomial, data = bacteria) 
> logLik(fit1)
`log Lik.' -104.3780 (df=5)
> logLik(fit2)
`log Lik.' -97.68162 (df=6)

Deepayan



From spencer.graves at pdf.com  Fri Nov 26 20:48:25 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 26 Nov 2004 11:48:25 -0800
Subject: [R] help with glmmPQL
In-Reply-To: <200411261309.30388.deepayan@stat.wisc.edu>
References: <41A6958C.1050407@arcriswell.com>
	<1abe3fa9041125222824b799da@mail.gmail.com>
	<41A77785.1020603@pdf.com>
	<200411261309.30388.deepayan@stat.wisc.edu>
Message-ID: <41A78889.2090906@pdf.com>

Hi, Deepayan: 

      Thanks much.  That's very helpful. 

ANDREW:  How difficult would it be for you to generate a Monte Carlo to 
simulate data according to your two models, e.g., "y ~ trt" and "y ~ trt 
+ I(week > 2)"?  If you did that, you could then fit both models to each 
set of simulated data, and compute and store logLR <- 
fit2$logLik-fit1$logLik for each one.  This will give you a reference 
distribution, from which you can estimate both the p-value and the 
statistical power of this analysis against your chosen alternatives. 

      If you do that, I suggest you use "GLMM" in library(lme4), not 
glmmPQL.  The "logLik" produced by glmmPLQ for model 2 was LESS THAN 
that for model 1.  If the function were maximizing the likelihood, 
fit2$logLik should be greater than fit1$logLik, not less. 

      Of course, of you simulate both models and compute the 
distribution of your favorite test statistic, you can get a p-value that 
is as good as your simulation.  I've done this kind of thing before, and 
it should be relatively easy in R using rnorm and rbinom. 

      hope this helps. 
      spencer graves

Deepayan Sarkar wrote:

>On Friday 26 November 2004 12:35, Spencer Graves wrote:
>  
>
>>HI, DOUG & JOSE:
>>
>>      Is there some reason that "anova.lme" should NOT accept an
>>object of class "glmmPQL" in the example below?  If you don't see one
>>either, then I suggest you consider modifying the code as described
>>below.
>>
>>HI, ANDREW:
>>
>>      I couldn't find your data "learning" in my Windows installation
>>of R 2.0.0pat, which meant that I had to take the time to find
>>another example before I could get the error message you described. 
>>I got it from modifying the example in the documentation for
>>"glmmPQL" as follows:
>>
>>fit1 <- glmmPQL(y ~ trt, random = ~ 1 | ID,
>>                     family = binomial, data = bacteria)
>>fit2 <- glmmPQL(y ~ trt + I(week > 2), random = ~ 1 | ID,
>>                     family = binomial, data = bacteria)
>>anova(fit1, fit2)
>>    Error in anova.lme(fit1, fit2) : Objects must inherit from
>>classes "gls", "gnls" "lm","lmList", "lme","nlme","nlsList", or "nls"
>>
>>      I then checked the class of fit1 and fit2:
>> > class(fit1)
>>
>>[1] "glmmPQL" "lme"
>>
>> > class(fit2)
>>
>>[1] "glmmPQL" "lme"
>>
>>      As an experiment, I changed the class of fit1 and fit2:
>> > class(fit1) <- "lme"
>> > class(fit2) <- "lme"
>> > anova(fit1, fit2)
>>
>>     Model df      AIC      BIC    logLik   Test  L.Ratio p-value
>>fit1     1  5 1054.623 1071.592 -522.3117
>>fit2     2  6 1113.622 1133.984 -550.8111 1 vs 2 56.99884  <.0001
>>
>>      Unless someone like Doug or Jose tells us, "Don't do that", I
>>would use these answers.
>>    
>>
>
>These likelihoods are (AFAIK) NOT the likelihoods of the models fitted, 
>they are likelihoods of an lme model that approximates it. Thus, the 
>test may not be appropriate. Having 'anova(fit1, fit2)' silently 
>producing an answer would certainly be misleading.
>
>E.g., lme4 produces the following:
>
>  
>
>>library(lme4)
>>data(bacteria, package = "MASS")
>>fit1 <- GLMM(y ~ trt, random = ~ 1 | ID, 
>>    
>>
>+              family = binomial, data = bacteria) 
>  
>
>>fit2 <- GLMM(y ~ trt + I(week > 2), random = ~ 1 | ID, 
>>    
>>
>+              family = binomial, data = bacteria) 
>  
>
>>logLik(fit1)
>>    
>>
>`log Lik.' -104.3780 (df=5)
>  
>
>>logLik(fit2)
>>    
>>
>`log Lik.' -97.68162 (df=6)
>
>Deepayan
>
>  
>

-- 
Spencer Graves, PhD, Senior Development Engineer
O:  (408)938-4420;  mobile:  (408)655-4567



From rbaer at atsu.edu  Fri Nov 26 20:51:13 2004
From: rbaer at atsu.edu (Robert W. Baer, Ph.D.)
Date: Fri, 26 Nov 2004 13:51:13 -0600
Subject: [R] Coplot Given text
References: <BAY18-DAV6956514E36AC3539BCAA4F8BA0@phx.gbl>
Message-ID: <005f01c4d3f1$5e71ec80$6401a8c0@meadow>

Does this do what you want?
coplot(myvar~myvar | myvar, show.given=FALSE, ylab="....", xlab=c("...","A 
Title on Top"))

Rob
----- Original Message ----- 
From: "Mihai Nica" <m_nica at hotmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, November 26, 2004 11:30 AM
Subject: [R] Coplot Given text


> Greetings:
>
> I am unsuccessful in suppressing "Given : myvariable" from a coplot. There 
> was such a question in the past but the thread breaks down. I am sure this 
> is a "for dummies" question :-). I tried:
>
> coplot(myvar~myvar | myvar, show.given=FALSE, xlab="....", ylab="...", 
> main=" ")
> and some other variations (including without main=" ") but I have to go to 
> the lower level and that is beyond me yet. I know, trellis is an option, 
> but I want to be able to handle basics first.
>
> Win2000, R 2.0.0
>
> Many thanks,
>
> Mihai Nica
> Jackson State University
> 155 B Parkhurst Dr.
> Jackson, MS 39202
> 601 969 5423
> 601 914 0361
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From bates at stat.wisc.edu  Fri Nov 26 21:12:36 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 26 Nov 2004 14:12:36 -0600
Subject: [R] help with glmmPQL
In-Reply-To: <41A77785.1020603@pdf.com>
References: <41A6958C.1050407@arcriswell.com>
	<1abe3fa9041125222824b799da@mail.gmail.com>
	<41A77785.1020603@pdf.com>
Message-ID: <41A78E34.8080400@stat.wisc.edu>

Spencer Graves wrote:
> HI, DOUG & JOSE:
>      Is there some reason that "anova.lme" should NOT accept an object 
> of class "glmmPQL" in the example below?  If you don't see one either, 
> then I suggest you consider modifying the code as described below.

I don't think that would be appropriate.  AFAIK the logLik generic 
applied to a glmmPQL object does not return the likelihood of the fitted 
model or an approximation to that likelihood.

That's why there is a difference between the values of the likelihood 
for the same model fit by glmmPQL and fit using the PQL method in GLMM 
from the lme4 package.

I will add anova methods for GLMM objects whenever I manage to dig 
myself out of the current rewrite of the representation of linear 
mixed-effects models.

> HI, ANDREW:
>      I couldn't find your data "learning" in my Windows installation of 
> R 2.0.0pat, which meant that I had to take the time to find another 
> example before I could get the error message you described.  I got it 
> from modifying the example in the documentation for "glmmPQL" as follows:
> fit1 <- glmmPQL(y ~ trt, random = ~ 1 | ID,
>                     family = binomial, data = bacteria)
> fit2 <- glmmPQL(y ~ trt + I(week > 2), random = ~ 1 | ID,
>                     family = binomial, data = bacteria)
> anova(fit1, fit2)
>    Error in anova.lme(fit1, fit2) : Objects must inherit from classes 
> "gls", "gnls" "lm","lmList", "lme","nlme","nlsList", or "nls"
> 
>      I then checked the class of fit1 and fit2: > class(fit1)
> [1] "glmmPQL" "lme"   > class(fit2)
> [1] "glmmPQL" "lme"  
>      As an experiment, I changed the class of fit1 and fit2: > 
> class(fit1) <- "lme"
>  > class(fit2) <- "lme"
>  > anova(fit1, fit2)
>     Model df      AIC      BIC    logLik   Test  L.Ratio p-value
> fit1     1  5 1054.623 1071.592 -522.3117                       fit2     
> 2  6 1113.622 1133.984 -550.8111 1 vs 2 56.99884  <.0001
> 
>      Unless someone like Doug or Jose tells us, "Don't do that", I would 
> use these answers.
>      I looked briefly at the code for "anova.lme", and it looks to me 
> like "glmmPQL" could be added to the list of allowed options in the 
> following test roughly half way through the code:
>       if (!all(match(termsClass, c("gls", "gnls", "lm", "lmList",
>            "lme", "nlme", "nlsList", "nls"), 0))) {
>            stop(paste("Objects must inherit from classes \"gls\", 
> \"gnls\"",
>                "\"lm\",\"lmList\", \"lme\",\"nlme\",\"nlsList\", or 
> \"nls\""))
>        }
> 
>      hope this helps.      spencer graves
> 
> A.J. Rossini wrote:
> 
>> For better or worse, it's holidays in the states.  Very amusing for me
>> being in a non-Thanksgiving celebrating country.
>>
>> In addition, it's not a problem.  The complaint is valid.  Probably no
>> one has coded up the right solution yet for comparison.
>>
>> I can't recall if one would want those statistics for a binomial
>> random effects model, but I do recall some issues with model
>> comparison in that setting, though they are a bit dated (say, 2 years
>> or so).
>>
>> On Fri, 26 Nov 2004 09:31:40 +0700, Andrew Criswell
>> <r-stats at arcriswell.com> wrote:
>>  
>>
>>> Hello:
>>>
>>> Will someone PLEASE help me with this problem. This is the third time
>>> I've posted it.
>>>
>>> When I appply anova() to two equations estimated using glmmPQL, I get a
>>> complaint,
>>>
>>>   
>>>
>>>> anova(fm1, fm2)
>>>>     
>>>
>>> Error in anova.lme(fm1, fm2) : Objects must inherit from classes "gls",
>>> "gnls" "lm","lmList", "lme","nlme","nlsList", or "nls"
>>>
>>>   
>>> The two equations I estimated are these:
>>>
>>>   
>>>
>>>> fm1 <- glmmPQL(choice ~ day + stereotypy,
>>>>     
>>>
>>> +                random = ~ 1 | bear, data = learning, family = 
>>> binomial)
>>>
>>>   
>>>
>>>> fm2 <- glmmPQL(choice ~ day + envir + stereotypy,
>>>>     
>>>
>>> +                random = ~ 1 | bear, data = learning, family = 
>>> binomial)
>>>
>>> Individually, I get results from anova():
>>>
>>>   
>>>
>>>> anova(fm1)
>>>>     
>>>
>>>          numDF denDF   F-value p-value
>>> (Intercept)     1  2032   7.95709  0.0048
>>> day             1  2032 213.98391  <.0001
>>> stereotypy      1  2032   0.42810  0.5130
>>>
>>>   
>>>
>>>> anova(fm2)
>>>>     
>>>
>>>          numDF denDF   F-value p-value
>>> (Intercept)     1  2031   5.70343  0.0170
>>> day             1  2031 213.21673  <.0001
>>> envir           1  2031  12.50388  0.0004
>>> stereotypy      1  2031   0.27256  0.6017
>>>
>>>   
>>> I did look through the archives but didn't finding anything relevant to
>>> my problem.
>>>
>>> Hope someone can help.
>>>
>>> ANDREW
>>> ____________________________
>>>       _
>>> platform i586-mandrake-linux-gnu
>>> arch     i586
>>> os       linux-gnu
>>> system   i586, linux-gnu
>>> status
>>> major    2
>>> minor    0.0
>>> year     2004
>>> month    10
>>> day      04
>>> language R
>>>
>>> -- 
>>> Andrew R. Criswell, Ph.D.
>>> Graduate School, Bangkok University
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>>
>>>   
>>
>>
>>
>>  
>>
>



From stephane.dray at umontreal.ca  Fri Nov 26 22:30:42 2004
From: stephane.dray at umontreal.ca (Stephane DRAY)
Date: Fri, 26 Nov 2004 16:30:42 -0500
Subject: [R] Arguments macthing
Message-ID: <5.2.1.1.0.20041126161941.022688c0@magellan.umontreal.ca>

Hello list,
I have a question concerning argument matching I have read R Language 
Definition before).
I have a function multxby2 which takes as argument the call to another 
function f1.
I would like to take the values of xx from the call to f1 and use it in 
multxby2.

Here is an example:


multxby2 <- function(callf) {
     appel1<-match.call()
     appel<-as.list(as.call(appel1$callf))

     px<-pmatch(names(appel),"xx")

     if(sum(is.na(px))!=length(appel)) print(appel[[which(!is.na(px))]]*2)
     # else ...

}

f1=function(xx=2,y=3){}

multxby2(f1(x=3))

My problem is due to the various ways that a user can enter its arguments, 
how to be sure to get xx from various call such as:

multxby2(f1(3,2))
multxby2(f1(y=2,3))

Is there a way to do that avoiding a lot of "if" and how to solve the last 
case ? My practical case takes functions with around 20 arguments.

Thanks in advance,

Sincerely.
St??phane DRAY
-------------------------------------------------------------------------------------------------- 

D??partement des Sciences Biologiques
Universit?? de Montr??al, C.P. 6128, succursale centre-ville
Montr??al, Qu??bec H3C 3J7, Canada

Tel : (514) 343-6111 poste 1233         Fax : (514) 343-2293
E-mail : stephane.dray at umontreal.ca
-------------------------------------------------------------------------------------------------- 

Web                                          http://www.steph280.freesurf.fr/



From dray at biomserv.univ-lyon1.fr  Fri Nov 26 22:58:34 2004
From: dray at biomserv.univ-lyon1.fr (Stephane DRAY)
Date: Fri, 26 Nov 2004 16:58:34 -0500
Subject: [R] Arguments matching
In-Reply-To: <5.2.1.1.0.20041126161941.022688c0@magellan.umontreal.ca>
Message-ID: <5.2.1.1.0.20041126165202.02266330@biomserv.univ-lyon1.fr>

I just precise my problem (and correct the subject).

Is there a way to obtain a list where each element has the name of the 
argument (as interpreted by R). The problem with my approach is that if the 
user do not type the name of the argument, this element of the list 
(returned by as.list(as.call(appel1$callf))) has no name.
I suppose that a solution could be obtained using formals, match.args... 
but I did not find it.

Thanks





At 16:30 26/11/2004, Stephane DRAY wrote:
>Hello list,
>I have a question concerning argument matching I have read R Language 
>Definition before).
>I have a function multxby2 which takes as argument the call to another 
>function f1.
>I would like to take the values of xx from the call to f1 and use it in 
>multxby2.
>
>Here is an example:
>
>
>multxby2 <- function(callf) {
>     appel1<-match.call()
>     appel<-as.list(as.call(appel1$callf))
>
>     px<-pmatch(names(appel),"xx")
>
>     if(sum(is.na(px))!=length(appel)) print(appel[[which(!is.na(px))]]*2)
>     # else ...
>
>}
>
>f1=function(xx=2,y=3){}
>
>multxby2(f1(x=3))
>
>My problem is due to the various ways that a user can enter its arguments, 
>how to be sure to get xx from various call such as:
>
>multxby2(f1(3,2))
>multxby2(f1(y=2,3))
>
>Is there a way to do that avoiding a lot of "if" and how to solve the last 
>case ? My practical case takes functions with around 20 arguments.
>
>Thanks in advance,
>
>Sincerely.
>St??phane DRAY
>-------------------------------------------------------------------------------------------------- 
>
>D??partement des Sciences Biologiques
>Universit?? de Montr??al, C.P. 6128, succursale centre-ville
>Montr??al, Qu??bec H3C 3J7, Canada
>
>Tel : (514) 343-6111 poste 1233         Fax : (514) 343-2293
>E-mail : stephane.dray at umontreal.ca
>-------------------------------------------------------------------------------------------------- 
>
>Web                                          http://www.steph280.freesurf.fr/
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

St??phane DRAY
-------------------------------------------------------------------------------------------------- 

D??partement des Sciences Biologiques
Universit?? de Montr??al, C.P. 6128, succursale centre-ville
Montr??al, Qu??bec H3C 3J7, Canada

Tel : (514) 343-6111 poste 1233         Fax : (514) 343-2293
E-mail : stephane.dray at umontreal.ca
-------------------------------------------------------------------------------------------------- 

Web                                          http://www.steph280.freesurf.fr/



From dray at biomserv.univ-lyon1.fr  Fri Nov 26 23:14:14 2004
From: dray at biomserv.univ-lyon1.fr (Stephane DRAY)
Date: Fri, 26 Nov 2004 17:14:14 -0500
Subject: [R] Arguments matching
In-Reply-To: <5.2.1.1.0.20041126165202.02266330@biomserv.univ-lyon1.fr>
References: <5.2.1.1.0.20041126161941.022688c0@magellan.umontreal.ca>
Message-ID: <5.2.1.1.0.20041126171042.0228fc40@biomserv.univ-lyon1.fr>

I have the answer using match.call:



multxby2 <- function(callf) {

     appel1<-match.call()

     appel<-as.list(match.call(eval(appel1$callf[[1]]),call=as.call(appel1$callf)))
     print(appel)

     px<-pmatch(names(appel),"xx")

     if(sum(is.na(px))!=length(appel)) print(appel[[which(!is.na(px))]]*2)
     # else ...

}

f1=function(x=2,y=3){}

multxby2(f1(x=3))



At 16:58 26/11/2004, Stephane DRAY wrote:
>I just precise my problem (and correct the subject).
>
>Is there a way to obtain a list where each element has the name of the 
>argument (as interpreted by R). The problem with my approach is that if 
>the user do not type the name of the argument, this element of the list 
>(returned by as.list(as.call(appel1$callf))) has no name.
>I suppose that a solution could be obtained using formals, match.args... 
>but I did not find it.
>
>Thanks
>
>
>
>
>
>At 16:30 26/11/2004, Stephane DRAY wrote:
>>Hello list,
>>I have a question concerning argument matching I have read R Language 
>>Definition before).
>>I have a function multxby2 which takes as argument the call to another 
>>function f1.
>>I would like to take the values of xx from the call to f1 and use it in 
>>multxby2.
>>
>>Here is an example:
>>
>>
>>multxby2 <- function(callf) {
>>     appel1<-match.call()
>>     appel<-as.list(as.call(appel1$callf))
>>
>>     px<-pmatch(names(appel),"xx")
>>
>>     if(sum(is.na(px))!=length(appel)) print(appel[[which(!is.na(px))]]*2)
>>     # else ...
>>
>>}
>>
>>f1=function(xx=2,y=3){}
>>
>>multxby2(f1(x=3))
>>
>>My problem is due to the various ways that a user can enter its 
>>arguments, how to be sure to get xx from various call such as:
>>
>>multxby2(f1(3,2))
>>multxby2(f1(y=2,3))
>>
>>Is there a way to do that avoiding a lot of "if" and how to solve the 
>>last case ? My practical case takes functions with around 20 arguments.
>>
>>Thanks in advance,
>>
>>Sincerely.
>>St??phane DRAY
>>-------------------------------------------------------------------------------------------------- 
>>
>>D??partement des Sciences Biologiques
>>Universit?? de Montr??al, C.P. 6128, succursale centre-ville
>>Montr??al, Qu??bec H3C 3J7, Canada
>>
>>Tel : (514) 343-6111 poste 1233         Fax : (514) 343-2293
>>E-mail : stephane.dray at umontreal.ca
>>-------------------------------------------------------------------------------------------------- 
>>
>>Web                                          http://www.steph280.freesurf.fr/
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>St??phane DRAY
>-------------------------------------------------------------------------------------------------- 
>
>D??partement des Sciences Biologiques
>Universit?? de Montr??al, C.P. 6128, succursale centre-ville
>Montr??al, Qu??bec H3C 3J7, Canada
>
>Tel : (514) 343-6111 poste 1233         Fax : (514) 343-2293
>E-mail : stephane.dray at umontreal.ca
>-------------------------------------------------------------------------------------------------- 
>
>Web                                          http://www.steph280.freesurf.fr/
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

St??phane DRAY
-------------------------------------------------------------------------------------------------- 

D??partement des Sciences Biologiques
Universit?? de Montr??al, C.P. 6128, succursale centre-ville
Montr??al, Qu??bec H3C 3J7, Canada

Tel : (514) 343-6111 poste 1233         Fax : (514) 343-2293
E-mail : stephane.dray at umontreal.ca
-------------------------------------------------------------------------------------------------- 

Web                                          http://www.steph280.freesurf.fr/



From murdoch at stats.uwo.ca  Sat Nov 27 00:48:45 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 26 Nov 2004 18:48:45 -0500
Subject: [R] unexpected behaviour of 'curve' function
In-Reply-To: <x2oehkx32w.fsf@biostat.ku.dk>
References: <B8A1EED732379B44A7E59D22E82E4442365380@IMHOTEP.ausl.org>
	<x2oehkx32w.fsf@biostat.ku.dk>
Message-ID: <soffq0pbd8tmjjf8rads8546n86i939nie@4ax.com>

On 26 Nov 2004 12:27:35 +0100, Peter Dalgaard
<p.dalgaard at biostat.ku.dk> wrote:

>"Guazzetti Stefano" <Stefano.Guazzetti at ausl.re.it> writes:
>
>> Dear all, 
>> 
>> curve(x^3*(1-x)^7, from = 0, to = 1)
>> 
>> works as  expected but, omitting the "xlim" or the "to" 
>> and "from" arguments and calling "curve" more than once:
>> 
>> par(mfrow = c(2,2))
>> for (i in 1:4)
>> curve(x^3*(1-x)^7)
>> 
>> gives an expected (al least to me) result.
>> Note also that a "pu" object is returned by curve
>> 
>> > pu
>> [1] -0.1802445  1.1802445
>> 
>> The behaviour is reproducible with both  R 2.0.0
>> and R 2.0.1
>> 
>> I can see that a promise of evaluation of "pu"
>> is made within curve but I cannot understand completely
>> what happens.
>
>I think the upshot is just that it is taking the "from" and "to" from
>par("usr") from the previous plot and every time you do so, it expands
>a little (about 4%). 

It's not documented what "from" and "to" will be when add=FALSE, so
this isn't strictly speaking a bug, but I'd say it's undesirable
behaviour.  I'd much rather see from=0 and to=1 in this case.

> Notice that it does not happen if you do this:
>
> par(mfrow = c(2,2), xaxs="i")
> for (i in 1:4)
>    curve(x^3*(1-x)^7)
>
>Having pu end up in the global environment looks like a bug in
>delay() but I don't think it has any influence on this particular
>effect. 

That's a pretty serious bug in delay().  It's been there at least
since 1.9.1.

Duncan Murdoch



From murdoch at stats.uwo.ca  Sat Nov 27 01:19:03 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Fri, 26 Nov 2004 19:19:03 -0500
Subject: [R] unexpected behaviour of 'curve' function
In-Reply-To: <soffq0pbd8tmjjf8rads8546n86i939nie@4ax.com>
References: <B8A1EED732379B44A7E59D22E82E4442365380@IMHOTEP.ausl.org>
	<x2oehkx32w.fsf@biostat.ku.dk>
	<soffq0pbd8tmjjf8rads8546n86i939nie@4ax.com>
Message-ID: <sqhfq0tv8iq4viv8ldmhhqspa085u516r5@4ax.com>

On Fri, 26 Nov 2004 18:48:45 -0500, Duncan Murdoch
<murdoch at stats.uwo.ca> wrote:

>On 26 Nov 2004 12:27:35 +0100, Peter Dalgaard
><p.dalgaard at biostat.ku.dk> wrote:

>>Having pu end up in the global environment looks like a bug in
>>delay() but I don't think it has any influence on this particular
>>effect. 
>
>That's a pretty serious bug in delay().  It's been there at least
>since 1.9.1.

Oops, not a bug, behaving as documented (but maybe a design flaw)?
delay() is documented as defaulting to the global environment.
curve() should have used "env=environment()".

Duncan Murdoch



From p.dalgaard at biostat.ku.dk  Sat Nov 27 02:28:49 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 27 Nov 2004 02:28:49 +0100
Subject: [R] unexpected behaviour of 'curve' function
In-Reply-To: <sqhfq0tv8iq4viv8ldmhhqspa085u516r5@4ax.com>
References: <B8A1EED732379B44A7E59D22E82E4442365380@IMHOTEP.ausl.org>
	<x2oehkx32w.fsf@biostat.ku.dk>
	<soffq0pbd8tmjjf8rads8546n86i939nie@4ax.com>
	<sqhfq0tv8iq4viv8ldmhhqspa085u516r5@4ax.com>
Message-ID: <x27jo8ulke.fsf@biostat.ku.dk>

Duncan Murdoch <murdoch at stats.uwo.ca> writes:

> Oops, not a bug, behaving as documented (but maybe a design flaw)?
> delay() is documented as defaulting to the global environment.
> curve() should have used "env=environment()".

Or not used delay() at all. That code looks a bit warped.

AFAICT, you could have 'from' defaulting to lims[1] and 'to' to
lims[2], then in the body let 

    if(missing(from) || missing(to))
        lims <- if(missing(xlim)) 
            if (par("xlog")) 
                10 ^ par("usr")[1:2] 
            else 
                 par("usr")[1:2] 
        else
            xlim

and get the same result (notwithstanding that you probably don't want
to use par("usr") in that way)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From agripinolima at yahoo.com  Sat Nov 27 02:52:32 2004
From: agripinolima at yahoo.com (agripino lima)
Date: Fri, 26 Nov 2004 17:52:32 -0800 (PST)
Subject: [R] Problems
Message-ID: <20041127015232.22003.qmail@web53702.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041126/688bc36e/attachment.pl

From tmulholland at bigpond.com  Sat Nov 27 02:53:33 2004
From: tmulholland at bigpond.com (Tom Mulholland)
Date: Sat, 27 Nov 2004 09:53:33 +0800
Subject: [R] an R function to search on Prof. Baron's site
In-Reply-To: <NEBBIPHDAMMOKDKPOFFIOELICMAA.abunn@whrc.org>
References: <NEBBIPHDAMMOKDKPOFFIOELICMAA.abunn@whrc.org>
Message-ID: <41A7DE1D.5000000@bigpond.com>

The problem appears to be associated with the "|"  in 
"Rhelp00/archive|Rhelp01/archive|Rhelp02a/archive". When you run the 
function in Firefox with restrict set to "doc" it gives the right response.

This function eventually uses system. So I decided to see if I could get 
this to work by hand using the code from browseURL

 > cmd
[1] "\"C:/Program Files/Internet Explorer/iexplore.exe\" 
http://finzi.psych.upenn.edu/cgi-bin/htsearch?config=htdigrun1;restrict=Rhelp00/archive|Rhelp01/archive|Rhelp02a/archive;format=builtin-long;sort=score;words=Ripley;matchesperpage=10"
 > system(cmd, wait = FALSE)

Works as expected since it does anyway. I make the assumption that I at 
least have the code correct.

 > cmd
[1] "\"C:/Program Files/Mozilla Firefox/firefox.exe\" 
http://finzi.psych.upenn.edu/cgi-bin/htsearch?config=htdigrun1;restrict=Rhelp00/archive|Rhelp01/archive|Rhelp02a/archive;format=builtin-long;sort=score;words=Ripley;matchesperpage=10"
 > system(cmd, wait = FALSE)

Falls over as previously described

I then thought that maybe quoting the search term would work, but this 
appears not to be the case.

As is noted in ?system "The command is run directly as a Windows command 
by the Windows API call 'CreateProcess'" Who knows what goes on in these 
processes. This is as far as I can go. It looks as if the API connection 
between system and Mozilla does not function the same way as with IE, 
and it is not obvious to me where the problem might be.

Tom Mulholland

R       R version 2.0.1, 2004-11-15
OS.type windows
GUI     Rgui

Andy Bunn wrote:
> Using this function with 2.0.0 XP and Firefox 1.0 (I've rediscovered the
> internet) produces a curious result.
> 
> 
>>myString <- RSiteSearch(string = 'Ripley')
>>myString
> 
> [1]
> "http://finzi.psych.upenn.edu/cgi-bin/htsearch?config=htdigrun1;restrict=Rhe
> lp00/archive|Rhelp01/archive|Rhelp02a/archive;format=builtin-long;sort=score
> ;words=Ripley;matchesperpage=10"
> 
>>version
> 
>          _
> platform i386-pc-mingw32
> arch     i386
> os       mingw32
> system   i386, mingw32
> status
> major    2
> minor    0.0
> year     2004
> month    10
> day      04
> language R
> 
> If no browser is open, then this is the URL that is browsed in Firefox:
> http://finzi.psych.upenn.edu/cgi-bin/htsearch?config=htdigrun1;restrict=Rhel
> p00/archive
> 
> Oddly, these two other windows are opened too:
> http://finzi.psych.upenn.edu/R/Rhelp01/archive/1000.html
> 
> and:
> http://www.mail-archive.com/r-help at stat.math.ethz.ch/msg17461.html
> 
> This happens regardless of what the search string is. If a browser window is
> open then everything works as planned. The sticky bit, obviously, is parsing
> browseURL which has the same behavior if I try:
> 
>>browseURL(myString)
> 
> 
> However, the searches:
> 
>>RSiteSearch(string = 'browseURL Firefox')
>>RSiteSearch(string = 'browseURL Mozilla')
> 
> 
> don't turn up much help! If I change browseURL to use IE then browseURL
> behaves as expected:
> 
> 
>>browseURL(myString, browser="C:/Program Files/Internet
> 
> Explorer/iexplore.exe")
> 
> Specifying Firefox explicitly in browseURL doesn't help - It still opens
> three windows as above (if no browser is open):
> 
> 
>>browseURL(myString, browser="C:/Program Files/Mozilla
> 
> Firefox/firefox.exe")
> 
> So, under Windows the 'NULL' argument in 'browser' which determines the
> browser via file association isn't the problem.
> 
> Anybody know how I can make Firefox work a little more smoothly?
> 
> Thanks, Andy
> 
> 
> 
> 
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch
>>[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Gabor Grothendieck
>>Sent: Tuesday, November 23, 2004 11:56 PM
>>To: r-help at stat.math.ethz.ch
>>Subject: Re: [R] an R function to search on Prof. Baron's site
>>
>>
>>Liaw, Andy <andy_liaw <at> merck.com> writes:
>>
>>:
>>: Inspired by the functions that Barry Rawlingson and Dave
>>Forrest posted for
>>: searching Rwiki and R-help archive, I've made up a function
>>that does the
>>: search on Prof. Baron's site (Thanks to Prof. Baron's help on
>>setting up the
>>: query string!):
>>
>>It would be nice if this and the other search functions recently
>>posted were collected into a package or even integrated into
>>R itself.  In the case of the Windows Rgui, it would be nice if they
>>appeared on a menu with the other search and help functions.
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>>http://www.R-project.org/posting-guide.html
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From jfox at mcmaster.ca  Sat Nov 27 03:10:19 2004
From: jfox at mcmaster.ca (John Fox)
Date: Fri, 26 Nov 2004 21:10:19 -0500
Subject: [R] help with glmmPQL
In-Reply-To: <41A78E34.8080400@stat.wisc.edu>
Message-ID: <20041127021018.NZBY1836.tomts16-srv.bellnexxia.net@JohnDesktop8300>

Dear Doug,

I assume that in the absence of a suitable anova() method, there's nothing
wrong with testing the difference between two GLMM objects nested in their
random effects by manually comparing the log-likelihoods as printed or
returned by logLik(). Is that correct?

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Douglas Bates
> Sent: Friday, November 26, 2004 3:13 PM
> To: Spencer Graves
> Cc: R-help; bates at cs.wisc.edu; Jose.Pinheiro at pharma.novartis.com
> Subject: Re: [R] help with glmmPQL
> 
> Spencer Graves wrote:
> > HI, DOUG & JOSE:
> >      Is there some reason that "anova.lme" should NOT 
> accept an object 
> > of class "glmmPQL" in the example below?  If you don't see 
> one either, 
> > then I suggest you consider modifying the code as described below.
> 
> I don't think that would be appropriate.  AFAIK the logLik 
> generic applied to a glmmPQL object does not return the 
> likelihood of the fitted model or an approximation to that likelihood.
> 
> That's why there is a difference between the values of the 
> likelihood for the same model fit by glmmPQL and fit using 
> the PQL method in GLMM from the lme4 package.
> 
> I will add anova methods for GLMM objects whenever I manage 
> to dig myself out of the current rewrite of the 
> representation of linear mixed-effects models.
> 
> > HI, ANDREW:
> >      I couldn't find your data "learning" in my Windows 
> installation 
> > of R 2.0.0pat, which meant that I had to take the time to 
> find another 
> > example before I could get the error message you described. 
>  I got it 
> > from modifying the example in the documentation for 
> "glmmPQL" as follows:
> > fit1 <- glmmPQL(y ~ trt, random = ~ 1 | ID,
> >                     family = binomial, data = bacteria)
> > fit2 <- glmmPQL(y ~ trt + I(week > 2), random = ~ 1 | ID,
> >                     family = binomial, data = bacteria) anova(fit1, 
> > fit2)
> >    Error in anova.lme(fit1, fit2) : Objects must inherit 
> from classes 
> > "gls", "gnls" "lm","lmList", "lme","nlme","nlsList", or "nls"
> > 
> >      I then checked the class of fit1 and fit2: > class(fit1)
> > [1] "glmmPQL" "lme"   > class(fit2)
> > [1] "glmmPQL" "lme"  
> >      As an experiment, I changed the class of fit1 and fit2: >
> > class(fit1) <- "lme"
> >  > class(fit2) <- "lme"
> >  > anova(fit1, fit2)
> >     Model df      AIC      BIC    logLik   Test  L.Ratio p-value
> > fit1     1  5 1054.623 1071.592 -522.3117                   
>     fit2     
> > 2  6 1113.622 1133.984 -550.8111 1 vs 2 56.99884  <.0001
> > 
> >      Unless someone like Doug or Jose tells us, "Don't do that", I 
> > would use these answers.
> >      I looked briefly at the code for "anova.lme", and it 
> looks to me 
> > like "glmmPQL" could be added to the list of allowed options in the 
> > following test roughly half way through the code:
> >       if (!all(match(termsClass, c("gls", "gnls", "lm", "lmList",
> >            "lme", "nlme", "nlsList", "nls"), 0))) {
> >            stop(paste("Objects must inherit from classes \"gls\", 
> > \"gnls\"",
> >                "\"lm\",\"lmList\", \"lme\",\"nlme\",\"nlsList\", or
> > \"nls\""))
> >        }
> > 
> >      hope this helps.      spencer graves
> > 
> > A.J. Rossini wrote:
> > 
> >> For better or worse, it's holidays in the states.  Very 
> amusing for 
> >> me being in a non-Thanksgiving celebrating country.
> >>
> >> In addition, it's not a problem.  The complaint is valid.  
> Probably 
> >> no one has coded up the right solution yet for comparison.
> >>
> >> I can't recall if one would want those statistics for a binomial 
> >> random effects model, but I do recall some issues with model 
> >> comparison in that setting, though they are a bit dated 
> (say, 2 years 
> >> or so).
> >>
> >> On Fri, 26 Nov 2004 09:31:40 +0700, Andrew Criswell 
> >> <r-stats at arcriswell.com> wrote:
> >>  
> >>
> >>> Hello:
> >>>
> >>> Will someone PLEASE help me with this problem. This is the third 
> >>> time I've posted it.
> >>>
> >>> When I appply anova() to two equations estimated using glmmPQL, I 
> >>> get a complaint,
> >>>
> >>>   
> >>>
> >>>> anova(fm1, fm2)
> >>>>     
> >>>
> >>> Error in anova.lme(fm1, fm2) : Objects must inherit from classes 
> >>> "gls", "gnls" "lm","lmList", "lme","nlme","nlsList", or "nls"
> >>>
> >>>   
> >>> The two equations I estimated are these:
> >>>
> >>>   
> >>>
> >>>> fm1 <- glmmPQL(choice ~ day + stereotypy,
> >>>>     
> >>>
> >>> +                random = ~ 1 | bear, data = learning, family =
> >>> binomial)
> >>>
> >>>   
> >>>
> >>>> fm2 <- glmmPQL(choice ~ day + envir + stereotypy,
> >>>>     
> >>>
> >>> +                random = ~ 1 | bear, data = learning, family =
> >>> binomial)
> >>>
> >>> Individually, I get results from anova():
> >>>
> >>>   
> >>>
> >>>> anova(fm1)
> >>>>     
> >>>
> >>>          numDF denDF   F-value p-value
> >>> (Intercept)     1  2032   7.95709  0.0048
> >>> day             1  2032 213.98391  <.0001
> >>> stereotypy      1  2032   0.42810  0.5130
> >>>
> >>>   
> >>>
> >>>> anova(fm2)
> >>>>     
> >>>
> >>>          numDF denDF   F-value p-value
> >>> (Intercept)     1  2031   5.70343  0.0170
> >>> day             1  2031 213.21673  <.0001
> >>> envir           1  2031  12.50388  0.0004
> >>> stereotypy      1  2031   0.27256  0.6017
> >>>
> >>>   
> >>> I did look through the archives but didn't finding 
> anything relevant 
> >>> to my problem.
> >>>
> >>> Hope someone can help.
> >>>
> >>> ANDREW
> >>> ____________________________
> >>>       _
> >>> platform i586-mandrake-linux-gnu
> >>> arch     i586
> >>> os       linux-gnu
> >>> system   i586, linux-gnu
> >>> status
> >>> major    2
> >>> minor    0.0
> >>> year     2004
> >>> month    10
> >>> day      04
> >>> language R
> >>>
> >>> --
> >>> Andrew R. Criswell, Ph.D.
> >>> Graduate School, Bangkok University
> >>>
> >>> ______________________________________________
> >>> R-help at stat.math.ethz.ch mailing list 
> >>> https://stat.ethz.ch/mailman/listinfo/r-help
> >>> PLEASE do read the posting guide! 
> >>> http://www.R-project.org/posting-guide.html
> >>>
> >>>   
> >>
> >>
> >>
> >>  
> >>
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From m_nica at hotmail.com  Sat Nov 27 03:42:24 2004
From: m_nica at hotmail.com (Mihai Nica)
Date: Fri, 26 Nov 2004 20:42:24 -0600
Subject: [R] Coplot Given text
References: <BAY18-DAV6956514E36AC3539BCAA4F8BA0@phx.gbl>
	<005f01c4d3f1$5e71ec80$6401a8c0@meadow>
Message-ID: <BAY18-DAV14287D36677E79B88092BDF8BB0@phx.gbl>

Oh yes, many thanks! It does appear if I type coplot but I just didn't put
it together... I never imagined it has anything to do with xlab...

Mihai Nica
Jackson State University
155 B Parkhurst Dr.
Jackson, MS 39202
601 969 5423
601 914 0361
----- Original Message ----- 
From: "Robert W. Baer, Ph.D." <rbaer at atsu.edu>
To: "Mihai Nica" <m_nica at hotmail.com>; <r-help at stat.math.ethz.ch>
Sent: Friday, November 26, 2004 1:51 PM
Subject: Re: [R] Coplot Given text


> Does this do what you want?
> coplot(myvar~myvar | myvar, show.given=FALSE, ylab="....", xlab=c("...","A
> Title on Top"))
>
> Rob
> ----- Original Message ----- 
> From: "Mihai Nica" <m_nica at hotmail.com>
> To: <r-help at stat.math.ethz.ch>
> Sent: Friday, November 26, 2004 11:30 AM
> Subject: [R] Coplot Given text
>
>
> > Greetings:
> >
> > I am unsuccessful in suppressing "Given : myvariable" from a coplot.
There
> > was such a question in the past but the thread breaks down. I am sure
this
> > is a "for dummies" question :-). I tried:
> >
> > coplot(myvar~myvar | myvar, show.given=FALSE, xlab="....", ylab="...",
> > main=" ")
> > and some other variations (including without main=" ") but I have to go
to
> > the lower level and that is beyond me yet. I know, trellis is an option,
> > but I want to be able to handle basics first.
> >
> > Win2000, R 2.0.0
> >
> > Many thanks,
> >
> > Mihai Nica
> > Jackson State University
> > 155 B Parkhurst Dr.
> > Jackson, MS 39202
> > 601 969 5423
> > 601 914 0361
> >
> > [[alternative HTML version deleted]]
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
>
>



From andy_liaw at merck.com  Sat Nov 27 05:39:42 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 26 Nov 2004 23:39:42 -0500
Subject: [R] sorting a data.frame using a vector
Message-ID: <3A822319EB35174CA3714066D590DCD50994E38E@usrymx25.merck.com>

> From: Peter Dalgaard
> 
> Toralf Kirsten <tkirsten at izbi.uni-leipzig.de> writes:
> 
> > Hi all,
> > I'm looking for an efficient solution (speed and memory) for the
> > following problem:
> > Given
> > - a data.frame x containing numbers of type double
> >    with nrow(x)>ncol(x) and unique row lables and
> > - a character vector y containing a sorted order labels
> > 
> > Now, I'd like to sort the rows of the data.frame x w.r.t. 
> the order of
> > labels in y.
> > 
> > example:
> > x <- data.frame(c(1:4),c(5:8))
> > row.names(x)<-LETTERS[1:4]
> > y <- c("C","A","D","B")
> > 
> > 
> > My current solution is like this:
> > if(!is.null(y) && is.vector(y)) {
> >     nObj <- length(y)
> >     for (i in 1:nObj) {
> >       sObj <- y[i]
> >       k <- c(1:nrow(x))[row.names(x)==sObj]
> >       if (i != k) {
> >         names <- row.names(x)
> >         tObj <- row.names(x[i,])
> >         temp <- x[i,]
> >         x[i,] <- x[k,]
> >         x[k,] <- temp
> >         names[i] <- sObj
> >         names[k] <- tObj
> >         row.names(x) <- names
> >      }
> >    }
> > }
> > 
> > But I'm not happy with it because it is not really efficient. Any
> > other suggestions are welcome!
> 
> Anything wrong with x[y,] ???

Well... sometimes:

> nm <- as.character(sample(1:1e5))
> x <- data.frame(x1=rnorm(1e5), row.names=1:1e5)
> system.time(x[nm, , drop=FALSE], gcFirst=TRUE)
[1] 155.13   0.01 156.10     NA     NA
> system.time(x2<-x[match(nm, rownames(x)), , drop=FALSE], gcFirst=TRUE)
[1] 0.37 0.00 0.37   NA   NA
> all(rownames(x2) == nm)
[1] TRUE
> R.version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    2              
minor    0.1            
year     2004           
month    11             
day      15             
language R              

Cheers,
Andy 
 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: 
> (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: 
> (+45) 35327907
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From andy_liaw at merck.com  Sat Nov 27 06:08:15 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 27 Nov 2004 00:08:15 -0500
Subject: [R] R-2.0.1 reinstall non-CRAN pkg
Message-ID: <3A822319EB35174CA3714066D590DCD50994E38F@usrymx25.merck.com>

Have you checked where R is looking for that package (.libPaths() would tell
you), and whether that's where you've installed it?

HTH,
Andy

> From: Anthony Westerling
> 
> 
> I am trying to upgrade to R-2.0.1 from R-1.9 on a Mac running 
> OS X 10.3.
> 
> I have some simple packages I wrote myself that have to be 
> reinstalled 
> to be recognized as valid packages.  I have been using them 
> for a while 
> on earlier versions of R, so didn't expect to have any problems.
> 
> I am probably going about this the wrong way?  I simply used
> 
> R CMD build mypkgdir
> 
> and then
> 
> R CMD install mypkgdir.tar.gz
> 
> the package installs without any error messages.
> 
> however, library(mypkgname) still generates spiteful
> 
> 	Error in library(mypkgname) : 'mypkgname' is not a 
> valid package -- 
> installed < 2.0.0?
> 
> messages.
> 
> My apologies if answers to this kind of question have already been 
> posted.  I have looked over the archived r-help threads for the last 
> couple of months.
> 
> Best
> 
> Anthony Westerling
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From akniss at uwyo.edu  Sat Nov 27 06:16:52 2004
From: akniss at uwyo.edu (Andrew Kniss)
Date: Fri, 26 Nov 2004 22:16:52 -0700
Subject: [R] Storing loop output from a function
Message-ID: <000001c4d440$54301810$103d510a@Andrew>

I am attempting to write an R function to aid in time series diagnostics.
The tsdiag() works well, but I would prefer to get a plot with ACF, PACF,
and Ljung-Box p-values of residuals.  In my attempt to get to that point, I
am writing a function to calculate Ljung-Box p-values for residuals at
various lag distances.

ljung<-function(fit) 
       for(i in c(1:24,36,48))   
          {box<-(Box.test(fit$residuals, lag=i, type="Ljung-Box")) 
           print(c(i, box$p.value))}


This is one of my first R function writing attempts, so my apologies if
there is an obvious mistake.  The above function produces the desired effect
in printing the lags and p-values to be plotted [where fit is the result of
arima()]; however I cannot seem to get the output stored in a data.frame for
subsequent plotting.  I have tried storing the output using various methods
including data.frame, write.table, cat, capture.output, all with no success.


e.g:
ljung.out<-capture.output(print(c(i, box$p.value)))  
#I saw a suggestion similar to this one in a previous post to the list...
  
Any hints on how I can get the output stored so that I may plot it later? I
am using v 1.9.1, RGui for Windows.
Thanks,
 
Andrew Kniss
Assistant Research Scientist
University of Wyoming
Dept. 3354  
1000 E. University Ave.
Laramie, WY  82071
(307) 766-3949
akniss at uwyo.edu


Below is an arima fit that I have used in testing the function.

> dump("coal.fit", file=stdout())
coal.fit <-
structure(list(coef = structure(c(0.69539198614687, 484.903589344614
), .Names = c("ar1", "intercept")), sigma2 = 3109.45476265185, 
    var.coef = structure(c(0.0104024111201598, 0.302125085158484, 
    0.302125085158484, 634.39981868771), .Dim = as.integer(c(2, 
    2)), .Dimnames = list(c("ar1", "intercept"), c("ar1", "intercept"
    ))), mask = c(TRUE, TRUE), loglik = -266.892361376605, aic =
539.784722753209, 
    arma = as.integer(c(1, 0, 0, 0, 1, 0, 0)), residuals =
structure(c(60.4342567589239, 
    -127.383559378086, -14.9885854976147, 123.839062585504,
-56.6019914334983, 
    35.7247594443981, 63.6906479431108, -28.1651273226733,
-6.91856808459544, 
    8.90309567990135, -30.8784722646861, -91.1489687772519,
-103.345257968621, 
    -29.2770349660464, -20.9664426335713, -25.3512422872431, 
    32.6086618928476, -6.98260117899269, -108.850345082021,
4.60267757422564, 
    38.6146462114696, 42.7187751257762, 79.9491758184327, 36.880952815858, 
    62.0132089128299, -0.848550671576206, -15.6420872534077, 
    111.955160137055, 13.5021374808082, -126.940710948639, 63.7127908071542,

    27.4722158876983, -52.0448398629454, -15.4535767911051,
-73.4996569296364, 
    46.7008221699102, 27.5464232088949, -2.40151233395178,
-80.5337684309237, 
    -20.8162335807335, -18.2070175530272, -33.9885854976147, 
    -5.94848967770536, 17.8390625855041, 0.109559098069901,
39.5464232088949, 
    30.2537838322858, 32.9551601370546, 13.4381043864110), .Tsp = c(1, 
    49, 1), class = "ts"), call = stats:::arima(x = x, order = order, 
        seasonal = seasonal, include.mean = include.mean), series = "x", 
    code = as.integer(0), n.cond = 0, model = structure(list(
        phi = 0.69539198614687, theta = numeric(0), Delta = numeric(0), 
        Z = 1, a = 60.0964106553856, P = structure(0, .Dim = as.integer(c(1,

        1))), T = structure(0.69539198614687, .Dim = as.integer(c(1, 
        1))), V = structure(1, .Dim = as.integer(c(1, 1))), h = 0, 
        Pn = structure(1, .Dim = as.integer(c(1, 1)))), .Names = c("phi", 
    "theta", "Delta", "Z", "a", "P", "T", "V", "h", "Pn")), x =
structure(as.integer(c(569, 
    416, 422, 565, 484, 520, 573, 518, 501, 505, 468, 382, 310, 
    334, 359, 372, 439, 446, 349, 395, 461, 511, 583, 590, 620, 
    578, 534, 631, 600, 438, 516, 534, 467, 457, 392, 467, 500, 
    493, 410, 412, 416, 403, 422, 459, 467, 512, 534, 552, 545
    )), .Dim = as.integer(c(49, 1)), .Dimnames = list(NULL, "Coal"), .Tsp =
c(1, 
    49, 1), class = "ts")), .Names = c("coef", "sigma2", "var.coef", 
"mask", "loglik", "aic", "arma", "residuals", "call", "series", 
"code", "n.cond", "model", "x"), class = "Arima")



From leroy at ucsd.edu  Sat Nov 27 08:21:47 2004
From: leroy at ucsd.edu (Anthony Westerling)
Date: Fri, 26 Nov 2004 23:21:47 -0800
Subject: [R] R-2.0.1 reinstall non-CRAN pkg
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E38F@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E38F@usrymx25.merck.com>
Message-ID: <FFD740CC-4044-11D9-B3D5-000A959EA8AA@ucsd.edu>

Yes.  It is looking the correct directory.  Note that the package is  
installed without errors.  The problem is that, even after being newly  
reinstalled, it is still not recognized as a valid package for R-2.0

Tony


On Nov 26, 2004, at 9:08 PM, Liaw, Andy wrote:

> Have you checked where R is looking for that package (.libPaths()  
> would tell
> you), and whether that's where you've installed it?
>
> HTH,
> Andy
>
>> From: Anthony Westerling
>>
>>
>> I am trying to upgrade to R-2.0.1 from R-1.9 on a Mac running
>> OS X 10.3.
>>
>> I have some simple packages I wrote myself that have to be
>> reinstalled
>> to be recognized as valid packages.  I have been using them
>> for a while
>> on earlier versions of R, so didn't expect to have any problems.
>>
>> I am probably going about this the wrong way?  I simply used
>>
>> R CMD build mypkgdir
>>
>> and then
>>
>> R CMD install mypkgdir.tar.gz
>>
>> the package installs without any error messages.
>>
>> however, library(mypkgname) still generates spiteful
>>
>> 	Error in library(mypkgname) : 'mypkgname' is not a
>> valid package --
>> installed < 2.0.0?
>>
>> messages.
>>
>> My apologies if answers to this kind of question have already been
>> posted.  I have looked over the archived r-help threads for the last
>> couple of months.
>>
>> Best
>>
>> Anthony Westerling
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>>
>
>
> ----------------------------------------------------------------------- 
> -------
> Notice:  This e-mail message, together with any attachment...{{dropped}}



From R.P.Clement at westminster.ac.uk  Sat Nov 27 09:59:56 2004
From: R.P.Clement at westminster.ac.uk (Ross Clement)
Date: Sat, 27 Nov 2004 08:59:56 GMT
Subject: [R] labelling barplot
Message-ID: <200411270859.iAR8xuBw005531@tiger.wmin.ac.uk>


Hi. I'd like to produce a barplot where only the lowest value and the 
highest value are labelled on the x-axis. E.g. I might have a list
of numbers and frequencies:

barplot( c( 2, 2, 0, 4, 2 ), names.arg=c( 1, 2, 3, 4, 5 ) )

where the data is a set of counts for some values between 1 and 5. I'd
like to have a barplot where only the extremes 1 and 5 are labelled.

How do I do this?

Thanks in anticipation,

Ross Clement



From p.dalgaard at biostat.ku.dk  Sat Nov 27 12:14:47 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 27 Nov 2004 12:14:47 +0100
Subject: [R] sorting a data.frame using a vector
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E38E@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E38E@usrymx25.merck.com>
Message-ID: <x2mzx31r2w.fsf@biostat.ku.dk>

"Liaw, Andy" <andy_liaw at merck.com> writes:

> > > But I'm not happy with it because it is not really efficient. Any
> > > other suggestions are welcome!
> > 
> > Anything wrong with x[y,] ???
> 
> Well... sometimes:
> 
> > nm <- as.character(sample(1:1e5))
> > x <- data.frame(x1=rnorm(1e5), row.names=1:1e5)
> > system.time(x[nm, , drop=FALSE], gcFirst=TRUE)
> [1] 155.13   0.01 156.10     NA     NA
> > system.time(x2<-x[match(nm, rownames(x)), , drop=FALSE], gcFirst=TRUE)
> [1] 0.37 0.00 0.37   NA   NA
> > all(rownames(x2) == nm)
> [1] TRUE

Yes, the internals are using  

   pmatch(i, rows, duplicates.ok = TRUE)

and pmatch() is a horrible lot slower than match(). Anyone for a spot
of hardcore optimization?

(Partial matching of character indices is a feature long regretted by
its inventors, but every time we consider killing it, we tend to recoil
in horror upon realizing what it would break...) 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From bates at stat.wisc.edu  Sat Nov 27 15:22:02 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sat, 27 Nov 2004 08:22:02 -0600
Subject: [R] help with glmmPQL
In-Reply-To: <20041127021018.NZBY1836.tomts16-srv.bellnexxia.net@JohnDesktop8300>
References: <20041127021018.NZBY1836.tomts16-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <41A88D8A.3070201@stat.wisc.edu>

John Fox wrote:
> Dear Doug,
> 
> I assume that in the absence of a suitable anova() method, there's nothing
> wrong with testing the difference between two GLMM objects nested in their
> random effects by manually comparing the log-likelihoods as printed or
> returned by logLik(). Is that correct?
> 
> Regards,
>  John
> 

Yes.

I should have thought of that myself.  At this time of year those in 
Canada where they celebrate Thanksgiving in October are obviously 
thinking more clearly than those in the United States whose minds are 
fogged by too much turkey and televised football games.



From partha at kent.edu  Sat Nov 27 15:44:41 2004
From: partha at kent.edu (partha@kent.edu)
Date: Sat, 27 Nov 2004 09:44:41 -0500
Subject: [R] I will be on vacation
Message-ID: <33413b33227b.33227b33413b@kent.edu>

Sir:
Is it possible to stop sending me email till January 12,2005.
I will be away from my workplace.
Partha



From MSchwartz at MedAnalytics.com  Sat Nov 27 16:22:36 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Sat, 27 Nov 2004 09:22:36 -0600
Subject: [R] labelling barplot
In-Reply-To: <200411270859.iAR8xuBw005531@tiger.wmin.ac.uk>
References: <200411270859.iAR8xuBw005531@tiger.wmin.ac.uk>
Message-ID: <1101568956.13352.6.camel@horizons.localdomain>

On Sat, 2004-11-27 at 08:59 +0000, Ross Clement wrote:
> Hi. I'd like to produce a barplot where only the lowest value and the 
> highest value are labelled on the x-axis. E.g. I might have a list
> of numbers and frequencies:
> 
> barplot( c( 2, 2, 0, 4, 2 ), names.arg=c( 1, 2, 3, 4, 5 ) )
> 
> where the data is a set of counts for some values between 1 and 5. I'd
> like to have a barplot where only the extremes 1 and 5 are labelled.
> 
> How do I do this?
> 
> Thanks in anticipation,
> 
> Ross Clement


Draw your barplot, saving the bar midpoints in 'mp':

mp <- barplot( c( 2, 2, 0, 4, 2 ))


Now using axis(), label the x axis. Set 'at' to be the first and last
values in the 'mp' vector:

axis(1, labels = c(1, 5), at = c(mp[1], mp[length(mp)]))


Another alternative is to use mtext() instead of axis() like this:

mtext(side = 1, text = c(1, 5), at = c(mp[1], mp[length(mp)]), line = 1)


HTH,

Marc Schwartz



From ligges at statistik.uni-dortmund.de  Sat Nov 27 16:54:46 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 27 Nov 2004 16:54:46 +0100
Subject: [R] R-2.0.1 reinstall non-CRAN pkg
In-Reply-To: <FFD740CC-4044-11D9-B3D5-000A959EA8AA@ucsd.edu>
References: <3A822319EB35174CA3714066D590DCD50994E38F@usrymx25.merck.com>
	<FFD740CC-4044-11D9-B3D5-000A959EA8AA@ucsd.edu>
Message-ID: <41A8A346.5050707@statistik.uni-dortmund.de>

Anthony Westerling wrote:
> Yes.  It is looking the correct directory.  Note that the package is  
> installed without errors.  The problem is that, even after being newly  
> reinstalled, it is still not recognized as a valid package for R-2.0
> 
> Tony
> 
> 
> On Nov 26, 2004, at 9:08 PM, Liaw, Andy wrote:
> 
>> Have you checked where R is looking for that package (.libPaths()  
>> would tell
>> you), and whether that's where you've installed it?
>>
>> HTH,
>> Andy
>>
>>> From: Anthony Westerling
>>>
>>>
>>> I am trying to upgrade to R-2.0.1 from R-1.9 on a Mac running
>>> OS X 10.3.
>>>
>>> I have some simple packages I wrote myself that have to be
>>> reinstalled
>>> to be recognized as valid packages.  I have been using them
>>> for a while
>>> on earlier versions of R, so didn't expect to have any problems.
>>>
>>> I am probably going about this the wrong way?  I simply used
>>>
>>> R CMD build mypkgdir
>>>
>>> and then
>>>
>>> R CMD install mypkgdir.tar.gz
>>>
>>> the package installs without any error messages.
>>>
>>> however, library(mypkgname) still generates spiteful
>>>
>>>     Error in library(mypkgname) : 'mypkgname' is not a
>>> valid package --
>>> installed < 2.0.0?

So it is not packaged correctly, and thinks it has been installed under 
R < 2.0.0.
Please check the DESCRIPTION file carefully and rebuild the package, 
after that installating is should work.

Uwe Ligges


>>> messages.
>>>
>>> My apologies if answers to this kind of question have already been
>>> posted.  I have looked over the archived r-help threads for the last
>>> couple of months.
>>>
>>> Best
>>>
>>> Anthony Westerling
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide!
>>> http://www.R-project.org/posting-guide.html
>>>
>>>
>>
>>
>> ----------------------------------------------------------------------- 
>> -------
>> Notice:  This e-mail message, together with any attachment...{{dropped}}
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From maechler at stat.math.ethz.ch  Sat Nov 27 16:55:33 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Sat, 27 Nov 2004 16:55:33 +0100
Subject: [R] inappropriate posting (".. vacation ..")
In-Reply-To: <33413b33227b.33227b33413b@kent.edu>
References: <33413b33227b.33227b33413b@kent.edu>
Message-ID: <16808.41845.290445.21159@gargle.gargle.HOWL>

Huuh ???

>>>>> "Partha" ==   <partha at kent.edu>
>>>>>     on Sat, 27 Nov 2004 09:44:41 -0500 writes:

    Partha> Sir: Is it possible to stop sending me email till
    Partha> January 12,2005.  I will be away from my workplace.
    Partha> Partha

What did you think when sending this to the 2500 readers of
R-help?

Can you think what would happen if everyone told R-help
(i.e. everyone else) about their vacations ????

PLEASE!

BTW, even if you only had wanted to send this to the list
maintainer, it would not have been appropriate:  This is a
volunteer job, and relies on subscribers taking care of these
things themselves: Namely, by reading {the posting guide,
mailing list pages,...} on your own.

Now that this has cost too much time anyway, I'm putting this to
general consideration.

Martin Maechler, ETH Zurich
R mailing lists' maintainer



From ligges at statistik.uni-dortmund.de  Sat Nov 27 17:09:26 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 27 Nov 2004 17:09:26 +0100
Subject: [R] Storing loop output from a function
In-Reply-To: <000001c4d440$54301810$103d510a@Andrew>
References: <000001c4d440$54301810$103d510a@Andrew>
Message-ID: <41A8A6B6.3050504@statistik.uni-dortmund.de>

Andrew Kniss wrote:

> I am attempting to write an R function to aid in time series diagnostics.
> The tsdiag() works well, but I would prefer to get a plot with ACF, PACF,
> and Ljung-Box p-values of residuals.  In my attempt to get to that point, I
> am writing a function to calculate Ljung-Box p-values for residuals at
> various lag distances.
> 
> ljung<-function(fit) 
>        for(i in c(1:24,36,48))   
>           {box<-(Box.test(fit$residuals, lag=i, type="Ljung-Box")) 
>            print(c(i, box$p.value))}
> 

You need to return() rather than print() the object.

Uwe Ligges


> This is one of my first R function writing attempts, so my apologies if
> there is an obvious mistake.  The above function produces the desired effect
> in printing the lags and p-values to be plotted [where fit is the result of
> arima()]; however I cannot seem to get the output stored in a data.frame for
> subsequent plotting.  I have tried storing the output using various methods
> including data.frame, write.table, cat, capture.output, all with no success.
> 
> 
> e.g:
> ljung.out<-capture.output(print(c(i, box$p.value)))  
> #I saw a suggestion similar to this one in a previous post to the list...
>   
> Any hints on how I can get the output stored so that I may plot it later? I
> am using v 1.9.1, RGui for Windows.
> Thanks,
>  
> Andrew Kniss
> Assistant Research Scientist
> University of Wyoming
> Dept. 3354  
> 1000 E. University Ave.
> Laramie, WY  82071
> (307) 766-3949
> akniss at uwyo.edu
> 
> 
> Below is an arima fit that I have used in testing the function.
> 
> 
>>dump("coal.fit", file=stdout())
> 
> coal.fit <-
> structure(list(coef = structure(c(0.69539198614687, 484.903589344614
> ), .Names = c("ar1", "intercept")), sigma2 = 3109.45476265185, 
>     var.coef = structure(c(0.0104024111201598, 0.302125085158484, 
>     0.302125085158484, 634.39981868771), .Dim = as.integer(c(2, 
>     2)), .Dimnames = list(c("ar1", "intercept"), c("ar1", "intercept"
>     ))), mask = c(TRUE, TRUE), loglik = -266.892361376605, aic =
> 539.784722753209, 
>     arma = as.integer(c(1, 0, 0, 0, 1, 0, 0)), residuals =
> structure(c(60.4342567589239, 
>     -127.383559378086, -14.9885854976147, 123.839062585504,
> -56.6019914334983, 
>     35.7247594443981, 63.6906479431108, -28.1651273226733,
> -6.91856808459544, 
>     8.90309567990135, -30.8784722646861, -91.1489687772519,
> -103.345257968621, 
>     -29.2770349660464, -20.9664426335713, -25.3512422872431, 
>     32.6086618928476, -6.98260117899269, -108.850345082021,
> 4.60267757422564, 
>     38.6146462114696, 42.7187751257762, 79.9491758184327, 36.880952815858, 
>     62.0132089128299, -0.848550671576206, -15.6420872534077, 
>     111.955160137055, 13.5021374808082, -126.940710948639, 63.7127908071542,
> 
>     27.4722158876983, -52.0448398629454, -15.4535767911051,
> -73.4996569296364, 
>     46.7008221699102, 27.5464232088949, -2.40151233395178,
> -80.5337684309237, 
>     -20.8162335807335, -18.2070175530272, -33.9885854976147, 
>     -5.94848967770536, 17.8390625855041, 0.109559098069901,
> 39.5464232088949, 
>     30.2537838322858, 32.9551601370546, 13.4381043864110), .Tsp = c(1, 
>     49, 1), class = "ts"), call = stats:::arima(x = x, order = order, 
>         seasonal = seasonal, include.mean = include.mean), series = "x", 
>     code = as.integer(0), n.cond = 0, model = structure(list(
>         phi = 0.69539198614687, theta = numeric(0), Delta = numeric(0), 
>         Z = 1, a = 60.0964106553856, P = structure(0, .Dim = as.integer(c(1,
> 
>         1))), T = structure(0.69539198614687, .Dim = as.integer(c(1, 
>         1))), V = structure(1, .Dim = as.integer(c(1, 1))), h = 0, 
>         Pn = structure(1, .Dim = as.integer(c(1, 1)))), .Names = c("phi", 
>     "theta", "Delta", "Z", "a", "P", "T", "V", "h", "Pn")), x =
> structure(as.integer(c(569, 
>     416, 422, 565, 484, 520, 573, 518, 501, 505, 468, 382, 310, 
>     334, 359, 372, 439, 446, 349, 395, 461, 511, 583, 590, 620, 
>     578, 534, 631, 600, 438, 516, 534, 467, 457, 392, 467, 500, 
>     493, 410, 412, 416, 403, 422, 459, 467, 512, 534, 552, 545
>     )), .Dim = as.integer(c(49, 1)), .Dimnames = list(NULL, "Coal"), .Tsp =
> c(1, 
>     49, 1), class = "ts")), .Names = c("coef", "sigma2", "var.coef", 
> "mask", "loglik", "aic", "arma", "residuals", "call", "series", 
> "code", "n.cond", "model", "x"), class = "Arima")
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Sat Nov 27 17:17:39 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 27 Nov 2004 17:17:39 +0100
Subject: [R] Multi-figure plotting
In-Reply-To: <41A74997.1020902@agr.kuleuven.ac.be>
References: <41A74997.1020902@agr.kuleuven.ac.be>
Message-ID: <41A8A8A3.9030204@statistik.uni-dortmund.de>

Stefaan Lhermitte wrote:

> Dear R-ians,
> 
> I have a question concerning plotting different plots on one figure. I 
> have written a script to plot an image, a legend (based on different 
> rectangles) and a timeseries plot on one figure.
> In my R-lagnuage it looks like this (without arguments that are not 
> usefull for my question):
> 
> #---> I first define the functions
> image.data<-function(...){
>    ....
>    #-->Plot the data
>    par(fig=c(0,1,0.25,1),new=TRUE)
>    par(pin=c(1.5,(num.y/num.x*1.5)))
>    image(c(1:(num.x+1)),c(1:(num.y+1)),output.matrix,xlab="X 
> coord",ylab="Y coord",main='NDVI',las =1,cex.axis=0.7, cex.lab=0.7, 
> col=topo.colors(255), asp=1)
> 
>    #---> Add a legend (colored rectangles) + legend axis (=values)
>    par(fig=c(0,1,0.3,0.4),new=TRUE)
>    plot.window(xlim = c(0,1), ylim = c(0,1), yaxs="i")
>    rect((1:254)/255,0,(2:255)/255,0.1,col=topo.colors(255),border=FALSE)
>    axis(1,padj=-1,cex.axis=0.7)
>    ...
> }
> 
> profile.data<-function(...){
>    ...
>    #-->Plot the data
>    par(fig=c(0,1,0,0.3),new=TRUE)
>    par(pin=c(1.5,num.y/num.x*0.5))
>    plot(NDVI, type="l",cex.axis=0.7, cex.lab=0.7)
>    ...
> }
> 
> When I subsequently use the functions I get problems with the legend. It 
> plots my image nice in the upper part of  my pixture, adds the 
> timeseries below, but it does not plot the rectangles of the legend. I 
> don't know what I do wrong, because it plots perfectly the axis of the 
> legend, but the colorbar is invisible. All suggestions would be wonderfull!


I guess it is either clipped or there is not enough space left in the 
plot region. But since the example is not directly reproducable, I 
cannot tell you (well, I don't want to waste time to make your example 
reproducible for me, that's your job!).

Uwe Ligges


> Thanx in advance,
> Stef
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Sat Nov 27 17:20:09 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 27 Nov 2004 17:20:09 +0100
Subject: [R] conditional replacement - readLines
In-Reply-To: <41A6F022.9000907@web.de>
References: <41A6F022.9000907@web.de>
Message-ID: <41A8A939.5070705@statistik.uni-dortmund.de>

Christian Schulz wrote:

> Hi,
> 
> i have list with 2575  cgi files which i have read with readLines - all 
> Lines have the type "chr".
> Now i want replace the 2. line with a condition in 4. line and  write  
> the files back  to dir.
> 
> How i could avoid the incompatible problem, or is it better
> doing something with awk!?
> 
> for (i in 1:2575){
> datenrep <- 
> ifelse(daten[[i]][4]=="type,1",cat(substr(as.character(paste(daten[[i]][2])),1,5),as.character(100) 

You don't want to cat() here!
(Also, you do want to use "if(){} else{}" rather than "ifelse()".)

Uwe Ligges


> ,substr(as.character(paste(daten[[i]][2])),6,12),sep=""),daten[[i]][2])}
> lfdn,1001081Error in "[<-"(`*tmp*`, test & !nas, value = NULL) :
>        incompatible types
> 
> many thanks, christian
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Sat Nov 27 17:24:35 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 27 Nov 2004 11:24:35 -0500
Subject: [R] Storing loop output from a function
Message-ID: <3A822319EB35174CA3714066D590DCD50994E391@usrymx25.merck.com>

> From: Uwe Ligges
> 
> Andrew Kniss wrote:
> 
> > I am attempting to write an R function to aid in time 
> series diagnostics.
> > The tsdiag() works well, but I would prefer to get a plot 
> with ACF, PACF,
> > and Ljung-Box p-values of residuals.  In my attempt to get 
> to that point, I
> > am writing a function to calculate Ljung-Box p-values for 
> residuals at
> > various lag distances.
> > 
> > ljung<-function(fit) 
> >        for(i in c(1:24,36,48))   
> >           {box<-(Box.test(fit$residuals, lag=i, type="Ljung-Box")) 
> >            print(c(i, box$p.value))}
> > 
> 
> You need to return() rather than print() the object.

Yes, but print() is supposed to return (invisibly) its argument(s)...

Andy
 
> Uwe Ligges
> 
> 
> > This is one of my first R function writing attempts, so my 
> apologies if
> > there is an obvious mistake.  The above function produces 
> the desired effect
> > in printing the lags and p-values to be plotted [where fit 
> is the result of
> > arima()]; however I cannot seem to get the output stored in 
> a data.frame for
> > subsequent plotting.  I have tried storing the output using 
> various methods
> > including data.frame, write.table, cat, capture.output, all 
> with no success.
> > 
> > 
> > e.g:
> > ljung.out<-capture.output(print(c(i, box$p.value)))  
> > #I saw a suggestion similar to this one in a previous post 
> to the list...
> >   
> > Any hints on how I can get the output stored so that I may 
> plot it later? I
> > am using v 1.9.1, RGui for Windows.
> > Thanks,
> >  
> > Andrew Kniss
> > Assistant Research Scientist
> > University of Wyoming
> > Dept. 3354  
> > 1000 E. University Ave.
> > Laramie, WY  82071
> > (307) 766-3949
> > akniss at uwyo.edu
> > 
> > 
> > Below is an arima fit that I have used in testing the function.
> > 
> > 
> >>dump("coal.fit", file=stdout())
> > 
> > coal.fit <-
> > structure(list(coef = structure(c(0.69539198614687, 484.903589344614
> > ), .Names = c("ar1", "intercept")), sigma2 = 3109.45476265185, 
> >     var.coef = structure(c(0.0104024111201598, 0.302125085158484, 
> >     0.302125085158484, 634.39981868771), .Dim = as.integer(c(2, 
> >     2)), .Dimnames = list(c("ar1", "intercept"), c("ar1", 
> "intercept"
> >     ))), mask = c(TRUE, TRUE), loglik = -266.892361376605, aic =
> > 539.784722753209, 
> >     arma = as.integer(c(1, 0, 0, 0, 1, 0, 0)), residuals =
> > structure(c(60.4342567589239, 
> >     -127.383559378086, -14.9885854976147, 123.839062585504,
> > -56.6019914334983, 
> >     35.7247594443981, 63.6906479431108, -28.1651273226733,
> > -6.91856808459544, 
> >     8.90309567990135, -30.8784722646861, -91.1489687772519,
> > -103.345257968621, 
> >     -29.2770349660464, -20.9664426335713, -25.3512422872431, 
> >     32.6086618928476, -6.98260117899269, -108.850345082021,
> > 4.60267757422564, 
> >     38.6146462114696, 42.7187751257762, 79.9491758184327, 
> 36.880952815858, 
> >     62.0132089128299, -0.848550671576206, -15.6420872534077, 
> >     111.955160137055, 13.5021374808082, -126.940710948639, 
> 63.7127908071542,
> > 
> >     27.4722158876983, -52.0448398629454, -15.4535767911051,
> > -73.4996569296364, 
> >     46.7008221699102, 27.5464232088949, -2.40151233395178,
> > -80.5337684309237, 
> >     -20.8162335807335, -18.2070175530272, -33.9885854976147, 
> >     -5.94848967770536, 17.8390625855041, 0.109559098069901,
> > 39.5464232088949, 
> >     30.2537838322858, 32.9551601370546, 13.4381043864110), 
> .Tsp = c(1, 
> >     49, 1), class = "ts"), call = stats:::arima(x = x, 
> order = order, 
> >         seasonal = seasonal, include.mean = include.mean), 
> series = "x", 
> >     code = as.integer(0), n.cond = 0, model = structure(list(
> >         phi = 0.69539198614687, theta = numeric(0), Delta = 
> numeric(0), 
> >         Z = 1, a = 60.0964106553856, P = structure(0, .Dim 
> = as.integer(c(1,
> > 
> >         1))), T = structure(0.69539198614687, .Dim = 
> as.integer(c(1, 
> >         1))), V = structure(1, .Dim = as.integer(c(1, 1))), h = 0, 
> >         Pn = structure(1, .Dim = as.integer(c(1, 1)))), 
> .Names = c("phi", 
> >     "theta", "Delta", "Z", "a", "P", "T", "V", "h", "Pn")), x =
> > structure(as.integer(c(569, 
> >     416, 422, 565, 484, 520, 573, 518, 501, 505, 468, 382, 310, 
> >     334, 359, 372, 439, 446, 349, 395, 461, 511, 583, 590, 620, 
> >     578, 534, 631, 600, 438, 516, 534, 467, 457, 392, 467, 500, 
> >     493, 410, 412, 416, 403, 422, 459, 467, 512, 534, 552, 545
> >     )), .Dim = as.integer(c(49, 1)), .Dimnames = list(NULL, 
> "Coal"), .Tsp =
> > c(1, 
> >     49, 1), class = "ts")), .Names = c("coef", "sigma2", 
> "var.coef", 
> > "mask", "loglik", "aic", "arma", "residuals", "call", "series", 
> > "code", "n.cond", "model", "x"), class = "Arima")
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From muster at gmail.com  Sat Nov 27 17:42:15 2004
From: muster at gmail.com (Terry Mu)
Date: Sat, 27 Nov 2004 11:42:15 -0500
Subject: [R] how to pause between plots running scripts?
Message-ID: <b68812e704112708422b5228a4@mail.gmail.com>

I used pause() from library(DAAG) to pasue between plots. This works
when I source a script, but seems don't work when I run (ctrl + R) the
script in R.

Did I do something wrong? Thanks.



From andy_liaw at merck.com  Sat Nov 27 17:44:19 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 27 Nov 2004 11:44:19 -0500
Subject: [R] an R function to search on Prof. Baron's site
Message-ID: <3A822319EB35174CA3714066D590DCD50994E392@usrymx25.merck.com>

Thanks to Tom and Andy for pointing this out and chasing it down.  I tried
replacing "|" by "%7C" and that seems to fix the problem (at least on a
somewhat outdated release of Firefox).  Please let me know if the following
works.

With regard to Gabor's comment:  I'd guess R-core would be reluctant to have
functions in base that depends on something outside the official R site, the
excellent service that Prof. Baron provide not withstanding.  Although I'd
vote for suggesting the use of these in the FAQ or the R-help posting guide.

Best,
Andy

RSiteSearch <- function(string, restrict="Rhelp", format="long",
                        sortby="score", matchesPerPage=10) {
    URL <- "http://finzi.psych.upenn.edu/cgi-bin/htsearch"
    qstring <- paste(URL, "?config=htdigrun1", sep="")
    ## replace spaces with "%20" in the query
    string <- paste("words=", gsub(" ", "%20", string), sep="")
    mpp <- paste("matchesperpage=", matchesPerPage, sep="")

    format <- charmatch(format, c("long", "short"))
    if (format == 0) stop("format must be either long or short")
    format <- paste("format=builtin-", switch(format, "long", "short"),
sep="")

    sortby <- charmatch(sortby, c("score", "time", "title", "revtime"))
    if (sortby == 0) stop("wrong sortby specified")
    sortby <- paste("sort=",
                    switch(sortby, "score", "time", "title", "revtime"),
                    sep="")
                    
    res <- charmatch(restrict, c("Rhelp", "doc", "function", "doc/fun"))
    if (res == 0) stop("wrong restriction specified")
    res <- switch(res,
"Rhelp00/archive%7CRhelp01/archive%7CRhelp02a/archive",
                  "finzi.psych.upenn.edu/R/doc",
                  "finzi.psych.upenn.edu/R/library",
 
"finzi.psych.upenn.edu/R/doc%7Cfinzi.psych.upenn.edu/R/library")
    res <- paste("restrict=", res, sep="")
    qstring <- paste(qstring, res, format, sortby, string, mpp, sep=";")
    browseURL(qstring)
    invisible(qstring)
}

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Tom Mulholland
> Sent: Friday, November 26, 2004 8:54 PM
> To: Andy Bunn
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] an R function to search on Prof. Baron's site
> 
> 
> The problem appears to be associated with the "|"  in 
> "Rhelp00/archive|Rhelp01/archive|Rhelp02a/archive". When you run the 
> function in Firefox with restrict set to "doc" it gives the 
> right response.
> 
> This function eventually uses system. So I decided to see if 
> I could get 
> this to work by hand using the code from browseURL
> 
>  > cmd
> [1] "\"C:/Program Files/Internet Explorer/iexplore.exe\" 
> http://finzi.psych.upenn.edu/cgi-bin/htsearch?config=htdigrun1
> ;restrict=Rhelp00/archive|Rhelp01/archive|Rhelp02a/archive;for
> mat=builtin-long;sort=score;words=Ripley;matchesperpage=10"
>  > system(cmd, wait = FALSE)
> 
> Works as expected since it does anyway. I make the assumption 
> that I at 
> least have the code correct.
> 
>  > cmd
> [1] "\"C:/Program Files/Mozilla Firefox/firefox.exe\" 
> http://finzi.psych.upenn.edu/cgi-bin/htsearch?config=htdigrun1
> ;restrict=Rhelp00/archive|Rhelp01/archive|Rhelp02a/archive;for
> mat=builtin-long;sort=score;words=Ripley;matchesperpage=10"
>  > system(cmd, wait = FALSE)
> 
> Falls over as previously described
> 
> I then thought that maybe quoting the search term would work, 
> but this 
> appears not to be the case.
> 
> As is noted in ?system "The command is run directly as a 
> Windows command 
> by the Windows API call 'CreateProcess'" Who knows what goes 
> on in these 
> processes. This is as far as I can go. It looks as if the API 
> connection 
> between system and Mozilla does not function the same way as with IE, 
> and it is not obvious to me where the problem might be.
> 
> Tom Mulholland
> 
> R       R version 2.0.1, 2004-11-15
> OS.type windows
> GUI     Rgui
> 
> Andy Bunn wrote:
> > Using this function with 2.0.0 XP and Firefox 1.0 (I've 
> rediscovered the
> > internet) produces a curious result.
> > 
> > 
> >>myString <- RSiteSearch(string = 'Ripley')
> >>myString
> > 
> > [1]
> > 
> "http://finzi.psych.upenn.edu/cgi-bin/htsearch?config=htdigrun
> 1;restrict=Rhe
> > 
> lp00/archive|Rhelp01/archive|Rhelp02a/archive;format=builtin-l
> ong;sort=score
> > ;words=Ripley;matchesperpage=10"
> > 
> >>version
> > 
> >          _
> > platform i386-pc-mingw32
> > arch     i386
> > os       mingw32
> > system   i386, mingw32
> > status
> > major    2
> > minor    0.0
> > year     2004
> > month    10
> > day      04
> > language R
> > 
> > If no browser is open, then this is the URL that is browsed 
> in Firefox:
> > 
> http://finzi.psych.upenn.edu/cgi-bin/htsearch?config=htdigrun1
> ;restrict=Rhel
> > p00/archive
> > 
> > Oddly, these two other windows are opened too:
> > http://finzi.psych.upenn.edu/R/Rhelp01/archive/1000.html
> > 
> > and:
> > http://www.mail-archive.com/r-help at stat.math.ethz.ch/msg17461.html
> > 
> > This happens regardless of what the search string is. If a 
> browser window is
> > open then everything works as planned. The sticky bit, 
> obviously, is parsing
> > browseURL which has the same behavior if I try:
> > 
> >>browseURL(myString)
> > 
> > 
> > However, the searches:
> > 
> >>RSiteSearch(string = 'browseURL Firefox')
> >>RSiteSearch(string = 'browseURL Mozilla')
> > 
> > 
> > don't turn up much help! If I change browseURL to use IE 
> then browseURL
> > behaves as expected:
> > 
> > 
> >>browseURL(myString, browser="C:/Program Files/Internet
> > 
> > Explorer/iexplore.exe")
> > 
> > Specifying Firefox explicitly in browseURL doesn't help - 
> It still opens
> > three windows as above (if no browser is open):
> > 
> > 
> >>browseURL(myString, browser="C:/Program Files/Mozilla
> > 
> > Firefox/firefox.exe")
> > 
> > So, under Windows the 'NULL' argument in 'browser' which 
> determines the
> > browser via file association isn't the problem.
> > 
> > Anybody know how I can make Firefox work a little more smoothly?
> > 
> > Thanks, Andy
> > 
> > 
> > 
> > 
> >>-----Original Message-----
> >>From: r-help-bounces at stat.math.ethz.ch
> >>[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Gabor 
> Grothendieck
> >>Sent: Tuesday, November 23, 2004 11:56 PM
> >>To: r-help at stat.math.ethz.ch
> >>Subject: Re: [R] an R function to search on Prof. Baron's site
> >>
> >>
> >>Liaw, Andy <andy_liaw <at> merck.com> writes:
> >>
> >>:
> >>: Inspired by the functions that Barry Rawlingson and Dave
> >>Forrest posted for
> >>: searching Rwiki and R-help archive, I've made up a function
> >>that does the
> >>: search on Prof. Baron's site (Thanks to Prof. Baron's help on
> >>setting up the
> >>: query string!):
> >>
> >>It would be nice if this and the other search functions recently
> >>posted were collected into a package or even integrated into
> >>R itself.  In the case of the Windows Rgui, it would be nice if they
> >>appeared on a menu with the other search and help functions.
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://stat.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide!
> >>http://www.R-project.org/posting-guide.html
> >>
> > 
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> >
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ramasamy at cancer.org.uk  Sat Nov 27 19:31:08 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Sat, 27 Nov 2004 18:31:08 +0000
Subject: [R] inappropriate posting (".. vacation ..")
In-Reply-To: <16808.41845.290445.21159@gargle.gargle.HOWL>
References: <33413b33227b.33227b33413b@kent.edu>
	<16808.41845.290445.21159@gargle.gargle.HOWL>
Message-ID: <1101580230.3059.7.camel@ramasamy.stats>

In appropriate yes, but at least responders will not be getting as many
out of office replies. I am replying for future reference.

Go to https://stat.ethz.ch/mailman/listinfo/r-help and enter your email
address on the "unsubscribe or edit options" button (the last textbox).
You can also get the password (which you will need to unsubscribe)
emailed to you via this option. I think you will be sent an email to
confirm unsubscribe which you need to reply to.

There may be other ways, but I know this one works for sure.

And when you get back, you can subscribe in the same way. BTW, I found
the webpage for subscribing, unsubscribing etc to be much more pleasant
than some other mailing lists.

Regards, Adai



On Sat, 2004-11-27 at 15:55, Martin Maechler wrote:
> Huuh ???
> 
> >>>>> "Partha" ==   <partha at kent.edu>
> >>>>>     on Sat, 27 Nov 2004 09:44:41 -0500 writes:
> 
>     Partha> Sir: Is it possible to stop sending me email till
>     Partha> January 12,2005.  I will be away from my workplace.
>     Partha> Partha
> 
> What did you think when sending this to the 2500 readers of
> R-help?
> 
> Can you think what would happen if everyone told R-help
> (i.e. everyone else) about their vacations ????
> 
> PLEASE!
> 
> BTW, even if you only had wanted to send this to the list
> maintainer, it would not have been appropriate:  This is a
> volunteer job, and relies on subscribers taking care of these
> things themselves: Namely, by reading {the posting guide,
> mailing list pages,...} on your own.
> 
> Now that this has cost too much time anyway, I'm putting this to
> general consideration.
> 
> Martin Maechler, ETH Zurich
> R mailing lists' maintainer
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
-- 
Adaikalavan Ramasamy                    ramasamy at cancer.org.uk
Centre for Statistics in Medicine       http://www.ihs.ox.ac.uk/csm/
Cancer Research UK                      Tel : 01865 226 677
Old Road Campus, Headington, Oxford     Fax : 01865 226 962



From leroy at ucsd.edu  Sat Nov 27 20:02:09 2004
From: leroy at ucsd.edu (Anthony Westerling)
Date: Sat, 27 Nov 2004 11:02:09 -0800
Subject: [R] R-2.0.1 reinstall non-CRAN pkg
In-Reply-To: <41A8A346.5050707@statistik.uni-dortmund.de>
References: <3A822319EB35174CA3714066D590DCD50994E38F@usrymx25.merck.com>
	<FFD740CC-4044-11D9-B3D5-000A959EA8AA@ucsd.edu>
	<41A8A346.5050707@statistik.uni-dortmund.de>
Message-ID: <D6F223A5-40A6-11D9-B3D5-000A959EA8AA@ucsd.edu>

Thank you Uwe

That was indeed the problem.  I revised the depends line and removed  
the old built line, and then rebuilt the package.  Now it is recognized  
as valid.

Thanks

Tony

On Nov 27, 2004, at 7:54 AM, Uwe Ligges wrote:

> Anthony Westerling wrote:
>> Yes.  It is looking the correct directory.  Note that the package is   
>> installed without errors.  The problem is that, even after being  
>> newly  reinstalled, it is still not recognized as a valid package for  
>> R-2.0
>> Tony
>> On Nov 26, 2004, at 9:08 PM, Liaw, Andy wrote:
>>> Have you checked where R is looking for that package (.libPaths()   
>>> would tell
>>> you), and whether that's where you've installed it?
>>>
>>> HTH,
>>> Andy
>>>
>>>> From: Anthony Westerling
>>>>
>>>>
>>>> I am trying to upgrade to R-2.0.1 from R-1.9 on a Mac running
>>>> OS X 10.3.
>>>>
>>>> I have some simple packages I wrote myself that have to be
>>>> reinstalled
>>>> to be recognized as valid packages.  I have been using them
>>>> for a while
>>>> on earlier versions of R, so didn't expect to have any problems.
>>>>
>>>> I am probably going about this the wrong way?  I simply used
>>>>
>>>> R CMD build mypkgdir
>>>>
>>>> and then
>>>>
>>>> R CMD install mypkgdir.tar.gz
>>>>
>>>> the package installs without any error messages.
>>>>
>>>> however, library(mypkgname) still generates spiteful
>>>>
>>>>     Error in library(mypkgname) : 'mypkgname' is not a
>>>> valid package --
>>>> installed < 2.0.0?
>
> So it is not packaged correctly, and thinks it has been installed  
> under R < 2.0.0.
> Please check the DESCRIPTION file carefully and rebuild the package,  
> after that installating is should work.
>
> Uwe Ligges
>
>
>>>> messages.
>>>>
>>>> My apologies if answers to this kind of question have already been
>>>> posted.  I have looked over the archived r-help threads for the last
>>>> couple of months.
>>>>
>>>> Best
>>>>
>>>> Anthony Westerling
>>>>
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://stat.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide!
>>>> http://www.R-project.org/posting-guide.html
>>>>
>>>>
>>>
>>>
>>> --------------------------------------------------------------------- 
>>> -- -------
>>> Notice:  This e-mail message, together with any  
>>> attachment...{{dropped}}
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!  
>> http://www.R-project.org/posting-guide.html
>
>



From MSchwartz at MedAnalytics.com  Sat Nov 27 20:07:58 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Sat, 27 Nov 2004 13:07:58 -0600
Subject: [R] how to pause between plots running scripts?
In-Reply-To: <b68812e704112708422b5228a4@mail.gmail.com>
References: <b68812e704112708422b5228a4@mail.gmail.com>
Message-ID: <1101582479.13352.37.camel@horizons.localdomain>

On Sat, 2004-11-27 at 11:42 -0500, Terry Mu wrote:
> I used pause() from library(DAAG) to pasue between plots. This works
> when I source a script, but seems don't work when I run (ctrl + R) the
> script in R.
> 
> Did I do something wrong? Thanks.

The standard way to pause between plots is to set 'par(ask = TRUE)'
before your first plot, which will then prompt you to continue after
each new plot. See ?par for more information.

I am presuming that you are using Windows, which you do not indicate,
with CTRL-R being the command to paste and run code from the R Windows
GUI editor.

>From a scan of the code for DAAG's pause(), it appears to use the
readline() function to prompt for user input as the basis for the
pausing.

A scan of the source code in .../gnuwin32/editor.c indicates that the
copy and paste operation to the Rgui console from the editor takes place
via the clipboard in function editorrunselection, if my read is correct.

I would defer to Duncan Murdoch and Prof. Ripley here, but I suspect
that there may be some interaction between readline() and the
copy/paste/run mechanism that precludes pause() from running in this
situation.

A quick test you might want to run would be to replace pause() with:

readline(prompt = "Pause. Press <Enter> to continue...")

in your code to see if this behavior continues, independent of the DAAG
package. You should also notify the DAAG package authors/maintainer of
the issue.

HTH,

Marc Schwartz



From ernesto at ipimar.pt  Sat Nov 27 23:49:36 2004
From: ernesto at ipimar.pt (Ernesto Jardim)
Date: Sat, 27 Nov 2004 22:49:36 +0000
Subject: [R] >2GB dataset
In-Reply-To: <20041124153718.77378.qmail@web51402.mail.yahoo.com>
References: <20041124153718.77378.qmail@web51402.mail.yahoo.com>
Message-ID: <1101595776.3617.5.camel@moria.ipimar.pt>

Hi,

I've been using large datasets (>GB) and I've stored them in MySQL
databases and use RMySQL to access them. My feeling is that most of the
times you don't need to keep the dataset in your workspace, but you need
to access parts of it or aggregate it in some way, before run some
analysis. So use what is best from each world, databases to store and
perform partial selections and aggregations, and R to statistical
analysis.

You'll be amazed with the speed of this 2 together (R & MySQL).

Regards

EJ
 

On Wed, 2004-11-24 at 15:37, apollo wong wrote:
> Hi, do any one have experience with loading dataset
> that is larger than 2GB into R. My organization is a
> SAS oriented shop and I'm in the process of switching
> it to R. One of the complain about R has always been
> it's inability to handle large dataset (>GB)
> efficiently. I would like some comments from someone
> with experience of working on >2GB dataset in R.
> Thanks.
> Apollo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jfox at mcmaster.ca  Sat Nov 27 23:48:30 2004
From: jfox at mcmaster.ca (John Fox)
Date: Sat, 27 Nov 2004 17:48:30 -0500
Subject: [R] Testing for S4 objects
In-Reply-To: <16806.59063.246234.858186@gargle.gargle.HOWL>
Message-ID: <20041127224829.WDSK25979.tomts25-srv.bellnexxia.net@JohnDesktop8300>

Dear Martin,

As it turns out, the test that I proposed (i.e., testing for NULL slotNames)
sometimes fails. For example:

> library(car)
> data(Prestige)
> sum <- summary(lm(prestige ~ income + education, data=Prestige))
> slotNames(sum)
character(0)

The following, however, seems to work (at least as far as I've been able to
ascertain):

isS4object <- function(object) length(slotNames(object)) != 0

I hope that this is a more robust test.

John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: Martin Maechler [mailto:maechler at stat.math.ethz.ch] 
> Sent: Friday, November 26, 2004 3:18 AM
> To: John Fox
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Testing for S4 objects
> 
> >>>>> "JohnF" == John Fox <jfox at mcmaster.ca>
> >>>>>     on Thu, 25 Nov 2004 22:28:50 -0500 writes:
> 
>     JohnF> Dear r-help list members, Is there a way to test
>     JohnF> whether an object is an S4 object? The best that I've
>     JohnF> been able to come up with is
> 
>     JohnF> 	isS4object <- function(object) 
> !(is.null(slotNames(object)))
> 
> you can drop one pair of "(..)" to give
> 
>   isS4object <- function(object) !is.null(slotNames(object))
> 
> 
>     JohnF> which assumes that an S4 object has at least one
>     JohnF> slot. I think this is safe, but perhaps I'm missing
>     JohnF> something.
> 
> The question is a very good one -- that I have posed to 
> R-core a while ago myself.
> 
> Inside  utils:::str.default  {which doesn't show the many 
> commments in the *source* of str.default()}, I have wanted a 
> way that even works when the 'methods' package is not 
> attached and use the more obscure 
> 
>     #NOT yet:if(has.class <- !is.null(cl <- class(object)))
>     if(has.class <- !is.null(cl <- attr(object, "class")))# 
> S3 or S4 class
> 	S4 <- !is.null(attr(cl, "package"))## <<<'kludge' FIXME!
> 	##or length(methods::getSlots(cl)) > 0
> 
> For the time being, I'd keep your function, but I don't think 
> we'd guarantee that it will remain the appropriate test in 
> all future.  But till then many things will have happened (if 
> not all of them ;-).
> 
> Martin Maechler, ETH Zurich
>



From xianggui01 at yahoo.com  Sun Nov 28 01:46:36 2004
From: xianggui01 at yahoo.com (Xianggui QU)
Date: Sat, 27 Nov 2004 16:46:36 -0800 (PST)
Subject: [R] R program for row-column designs!
Message-ID: <20041128004636.99294.qmail@web51804.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041127/89ab9ee2/attachment.pl

From muster at gmail.com  Sun Nov 28 03:03:16 2004
From: muster at gmail.com (Terry Mu)
Date: Sat, 27 Nov 2004 21:03:16 -0500
Subject: [R] how to pause between plots running scripts?
In-Reply-To: <1101582479.13352.37.camel@horizons.localdomain>
References: <b68812e704112708422b5228a4@mail.gmail.com>
	<1101582479.13352.37.camel@horizons.localdomain>
Message-ID: <b68812e704112718032d3cf7d8@mail.gmail.com>

Thank you.

I am using R 2.0.0 under Windows 2000.

I've tried
1. par(ask=T)
2. readline(prompt = "Pause. Press <Enter> to continue...")

1. script:
x <- 0:20
y <- 5:25
plot(y)
plot(x)
plot(y~x)

output:
> par()$ask
[1] TRUE
> x <- 0:20
> y <- 5:25
> plot(y)
Hit <Return> to see next plot: plot(x) ####R does not pause after plot(y)
#### it seems R eats the second plot(x)
> plot(y~x)
Hit <Return> to see next plot: ####R does pause here

2.

script:
x <- 0:20
y <- 5:25
plot(y)
readline(prompt = "Pause. Press <Enter> to continue...")
plot(x)
pause()
plot(y~x)

output: ####R does not pause at all
> par()$ask
[1] TRUE
> plot(y)
Hit <Return> to see next plot: readline(promp #seems plot() eats some characters
>  = "Pause. Press <Enter> to continue...")
Error: syntax error
> plot(x)
Hit <Return> to see next plot: pause()
> plot(y~x)
Hit <Return> to see next plot: 

I don't know if I have made some stupid mistakes or messed up some of
configurations.
*************************************************************

On Sat, 27 Nov 2004 13:07:58 -0600, Marc Schwartz
<mschwartz at medanalytics.com> wrote:
> On Sat, 2004-11-27 at 11:42 -0500, Terry Mu wrote:
> 
> 
> > I used pause() from library(DAAG) to pasue between plots. This works
> > when I source a script, but seems don't work when I run (ctrl + R) the
> > script in R.
> >
> > Did I do something wrong? Thanks.
> 
> The standard way to pause between plots is to set 'par(ask = TRUE)'
> before your first plot, which will then prompt you to continue after
> each new plot. See ?par for more information.
> 
> I am presuming that you are using Windows, which you do not indicate,
> with CTRL-R being the command to paste and run code from the R Windows
> GUI editor.
> 
> >From a scan of the code for DAAG's pause(), it appears to use the
> readline() function to prompt for user input as the basis for the
> pausing.
> 
> A scan of the source code in .../gnuwin32/editor.c indicates that the
> copy and paste operation to the Rgui console from the editor takes place
> via the clipboard in function editorrunselection, if my read is correct.
> 
> I would defer to Duncan Murdoch and Prof. Ripley here, but I suspect
> that there may be some interaction between readline() and the
> copy/paste/run mechanism that precludes pause() from running in this
> situation.
> 
> A quick test you might want to run would be to replace pause() with:
> 
> readline(prompt = "Pause. Press <Enter> to continue...")
> 
> in your code to see if this behavior continues, independent of the DAAG
> package. You should also notify the DAAG package authors/maintainer of
> the issue.
> 
> HTH,
> 
> Marc Schwartz
> 
>



From murdoch at stats.uwo.ca  Sun Nov 28 04:38:34 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sat, 27 Nov 2004 22:38:34 -0500
Subject: [R] how to pause between plots running scripts?
In-Reply-To: <b68812e704112718032d3cf7d8@mail.gmail.com>
References: <b68812e704112708422b5228a4@mail.gmail.com>
	<1101582479.13352.37.camel@horizons.localdomain>
	<b68812e704112718032d3cf7d8@mail.gmail.com>
Message-ID: <vuhiq095lke0trblge8161b3dakaubg776@4ax.com>


>> > I used pause() from library(DAAG) to pasue between plots. This works
>> > when I source a script, but seems don't work when I run (ctrl + R) the
>> > script in R.

This sounds like a bug to me, but I'm not sure it's going to be an
easy one to fix.  

Duncan Murdoch



From sdimhoff at wisc.edu  Sun Nov 28 05:08:57 2004
From: sdimhoff at wisc.edu (Seth Imhoff)
Date: Sat, 27 Nov 2004 22:08:57 -0600
Subject: [R] lm help: using lm when one point is known (not y intercept)
Message-ID: <41A94F59.6040702@wisc.edu>

Hello-

My question is a short one.  How can I specify a single point which 
through the fitted linear model has to go through?  To illustrate my 
problem, the fit to following data must go through the point 
(-37.25(effect), 50(prob)).  Note: you can ignore the label column.

    Effect      Prob Label

1 -1143.75  7.142857     L

2  -572.75 21.428571     D

3  -223.75 35.714286    GL

4   123.25 50.000000    DG

5   359.75 64.285714     G

6   374.75 78.571429   DGL

7   821.75 92.857143    DL


Thanks in advance!

Seth Imhoff



From gongwuming at yahoo.com  Sun Nov 28 05:39:15 2004
From: gongwuming at yahoo.com (Wuming Gong)
Date: Sat, 27 Nov 2004 20:39:15 -0800 (PST)
Subject: [R] How can I generate a random network with given cluster
	coefficient using R?
Message-ID: <20041128043915.33999.qmail@web13008.mail.yahoo.com>

Hi list, 

I have read the mannual of package graph, but it looks that three
functions randomEGraph, randomGraph and randomNodeGraph cannot generate
a random network with given cluster coefficient. 
Is there any exisiting R package for such kind job and if no, how can I
perform this job with existing graph package?

Thanks

Wuming



From jfox at mcmaster.ca  Sun Nov 28 05:49:08 2004
From: jfox at mcmaster.ca (John Fox)
Date: Sat, 27 Nov 2004 23:49:08 -0500
Subject: [R] lm help: using lm when one point is known (not y intercept)
In-Reply-To: <41A94F59.6040702@wisc.edu>
Message-ID: <20041128044907.VOUE1836.tomts16-srv.bellnexxia.net@JohnDesktop8300>

Dear Seth,

You don't say which variable is the explanatory variable and which is the
response, but assuming that prob is to be regressed on effect, you can fit
lm(prob - 50 ~ I(effect + 37.25) - 1). That is you can shift the point
through which the regression is to go to the origin and then force the
regression through the origin.

I hope this helps,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Seth Imhoff
> Sent: Saturday, November 27, 2004 11:09 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] lm help: using lm when one point is known (not y 
> intercept)
> 
> Hello-
> 
> My question is a short one.  How can I specify a single point 
> which through the fitted linear model has to go through?  To 
> illustrate my problem, the fit to following data must go 
> through the point (-37.25(effect), 50(prob)).  Note: you can 
> ignore the label column.
> 
>     Effect      Prob Label
> 
> 1 -1143.75  7.142857     L
> 
> 2  -572.75 21.428571     D
> 
> 3  -223.75 35.714286    GL
> 
> 4   123.25 50.000000    DG
> 
> 5   359.75 64.285714     G
> 
> 6   374.75 78.571429   DGL
> 
> 7   821.75 92.857143    DL
> 
> 
> Thanks in advance!
> 
> Seth Imhoff



From gongwuming at gmail.com  Sun Nov 28 06:15:09 2004
From: gongwuming at gmail.com (Wuming Gong)
Date: Sun, 28 Nov 2004 13:15:09 +0800
Subject: [R] How can I generate a random network with given cluster
	coefficient using R?
Message-ID: <24d6fd0504112721157b06af95@mail.gmail.com>

Hi list, 

I have read the mannual of package graph, but it looks that three
functions randomEGraph, randomGraph and randomNodeGraph cannot generate
a random network with given cluster coefficient. 
Is there any exisiting R package for such kind job and if no, how can I
perform this job with existing graph package?

Thanks

Wuming



From arshia22 at yahoo.com  Sun Nov 28 12:21:26 2004
From: arshia22 at yahoo.com (ebashi)
Date: Sun, 28 Nov 2004 03:21:26 -0800 (PST)
Subject: [R] Correct Syntax for a Loop
Message-ID: <20041128112126.13093.qmail@web81005.mail.yahoo.com>

I'll appreciate if some one can help me with the
following loop. This is the logic of the loop,
if we have the following data;
> x.df
    x.dif
 .    .
 .    .
102  0.00
103  0.42
104  0.08
105  0.00
106  0.00
107  0.00
108 -0.16
109 -0.34
110  0.00
111 -0.17
112 -0.33
113  0.00
114  0.00
115  0.00
116  0.33
117  0.17
118  0.00 
 .    .
 .    .
I'm trying to find i's where 
  for (i in 2:length(x.dif))
  if (x.dif[i-1]<=0 and x.dif[i]>0 and x.dif[i+2]>0)
  it would return i+2 to me,
How can I turn this to a right format as a loop.(I
can't figure out the syntax)

Cheers,

Sean



From dledoux at chu.ulg.ac.be  Sun Nov 28 12:34:15 2004
From: dledoux at chu.ulg.ac.be (HOME - Didier Ledoux)
Date: Sun, 28 Nov 2004 12:34:15 +0100
Subject: [R] =?iso-8859-1?q?Question_d=27un_d=E9butant?=
Message-ID: <001101c4d53e$30a9cea0$0801a8c0@LDX20041022>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041128/9e598250/attachment.pl

From francoisromain at free.fr  Sun Nov 28 13:01:32 2004
From: francoisromain at free.fr (francoisromain@free.fr)
Date: Sun, 28 Nov 2004 13:01:32 +0100
Subject: [R] Question d'un =?iso-8859-1?b?ZOlidXRhbnQ=?=
In-Reply-To: <001101c4d53e$30a9cea0$0801a8c0@LDX20041022>
References: <001101c4d53e$30a9cea0$0801a8c0@LDX20041022>
Message-ID: <1101643292.41a9be1c044ef@imp1-q.free.fr>

Hello Didier,

Please read the posting guide, it says that the R mailing list speaks in english
: http://www.R-project.org/posting-guide.html
And it also says to read the FAQ and An introduction to R. That covers your
question.


After that, try :

?mean
?by

Hope this helps.

Selon HOME - Didier Ledoux <dledoux at chu.ulg.ac.be>:

> Je suis d??butant en R. Je voudrais faire un tableau de statistiques
> descriptives ou les moyennes seraient calcul??es en fonction de deux crit??res:
> sexe, r??gion
> Cela donnerait ceci:
> SEXE    REGION    MOYENNE
> femme    A                moy1
> femme    B                moy2
> Homme    A              moy3
> Homme    B              moy4
>
> Comment dois-je faire?
> Quelles commandes utiliser?
>
> Merci pour votre aide
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Sun Nov 28 13:07:06 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 28 Nov 2004 13:07:06 +0100
Subject: [R] Question d'un =?ISO-8859-1?Q?d=E9butant?=
In-Reply-To: <001101c4d53e$30a9cea0$0801a8c0@LDX20041022>
References: <001101c4d53e$30a9cea0$0801a8c0@LDX20041022>
Message-ID: <41A9BF6A.8040303@statistik.uni-dortmund.de>

HOME - Didier Ledoux wrote:
> Je suis d??butant en R. Je voudrais faire un tableau de statistiques descriptives ou les moyennes seraient calcul??es en fonction de deux crit??res: sexe, r??gion
> Cela donnerait ceci:
> SEXE    REGION    MOYENNE
> femme    A                moy1
> femme    B                moy2
> Homme    A              moy3
> Homme    B              moy4
> 
> Comment dois-je faire?
> Quelles commandes utiliser?
> 
> Merci pour votre aide

Hallo,

bitte lesen Sie den "Posting Guide", der unten angegeben ist.
Vermutlich wird Ihnen die Funktion table() weiterhelfen. Siehe ?table.

Uebrigens: Die Sprache auf dieser Liste ist eigentlich Englisch!

Gruss,
Uwe Ligges


> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Sun Nov 28 13:10:47 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 28 Nov 2004 13:10:47 +0100
Subject: [R] Correct Syntax for a Loop
In-Reply-To: <20041128112126.13093.qmail@web81005.mail.yahoo.com>
References: <20041128112126.13093.qmail@web81005.mail.yahoo.com>
Message-ID: <41A9C047.2010801@statistik.uni-dortmund.de>

ebashi wrote:
> I'll appreciate if some one can help me with the
> following loop. This is the logic of the loop,
> if we have the following data;
> 
>>x.df
> 
>     x.dif
>  .    .
>  .    .
> 102  0.00
> 103  0.42
> 104  0.08
> 105  0.00
> 106  0.00
> 107  0.00
> 108 -0.16
> 109 -0.34
> 110  0.00
> 111 -0.17
> 112 -0.33
> 113  0.00
> 114  0.00
> 115  0.00
> 116  0.33
> 117  0.17
> 118  0.00 
>  .    .
>  .    .
> I'm trying to find i's where 
>   for (i in 2:length(x.dif))
>   if (x.dif[i-1]<=0 and x.dif[i]>0 and x.dif[i+2]>0)

Solution to the question:

a) you cannot loop to length(x.dif), x.dif[i+2] does not exist for 
i==length(x.dif).
b) Use && instead of "and" in this case
c) vector operations are much mor

Hint: If you lengthen the vectors appropriately, you can apply the 
comparisons vectorized  rather than in a loop...

Uwe Ligges

>   it would return i+2 to me,
> How can I turn this to a right format as a loop.(I
> can't figure out the syntax)
> 
> Cheers,
> 
> Sean
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Ted.Harding at nessie.mcc.ac.uk  Sun Nov 28 13:10:00 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Sun, 28 Nov 2004 12:10:00 -0000 (GMT)
Subject: [R] Correct Syntax for a Loop
In-Reply-To: <20041128112126.13093.qmail@web81005.mail.yahoo.com>
Message-ID: <XFMail.041128121000.Ted.Harding@nessie.mcc.ac.uk>

On 28-Nov-04 ebashi wrote:
> I'll appreciate if some one can help me with the
> following loop. This is the logic of the loop,
> if we have the following data;
>> x.df
>     x.dif
>  .    .
>  .    .
> 102  0.00
> 103  0.42
> 104  0.08
> 105  0.00
> 106  0.00
> 107  0.00
> 108 -0.16
> 109 -0.34
> 110  0.00
> 111 -0.17
> 112 -0.33
> 113  0.00
> 114  0.00
> 115  0.00
> 116  0.33
> 117  0.17
> 118  0.00 
>  .    .
>  .    .
> I'm trying to find i's where 
>   for (i in 2:length(x.dif))
>   if (x.dif[i-1]<=0 and x.dif[i]>0 and x.dif[i+2]>0)
>   it would return i+2 to me,
> How can I turn this to a right format as a loop.(I
> can't figure out the syntax)

In this sort of case you can do it without a loop. The
following code (which as written is specific to the precise
question you have asked) is the sort of thing you can use:

  n<-length(x.dif)
  y2<-x.dif[4:n]
  y0<-x.dif[2:(n-2)]
  y1<-x.dif[1:(n-3)]
  which((y1<=0)&(y0>0)&(y2>0))+3

Hoping this helps,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 28-Nov-04                                       Time: 12:10:00
------------------------------ XFMail ------------------------------



From m.dewey at iop.kcl.ac.uk  Sun Nov 28 16:27:57 2004
From: m.dewey at iop.kcl.ac.uk (Michael Dewey)
Date: Sun, 28 Nov 2004 15:27:57 +0000
Subject: [R] Tetrachoric and polychoric ceofficients (for sem) - any tips?
Message-ID: <6.1.0.6.0.20041128152028.026432e0@pop.freeserve.net>

About two years ago there was a thread about this which suggested that at 
that time nobody had these coefficients ready to go.
(a) has anyone in the meanwhile programmed them?
(b) I think I can see how to do the tetrachoric one with mvtnorm on similar 
lines to an example on the help page so will try that if nobody else 
already has
(c) looking at the polychoric one makes me realise yet again that I wish I 
knew more mathematics. I would also find it difficult to test it as I do 
not have any worked examples. Does anyone have any tips about how to 
program it and how to test the resultant code?
(d) I appreciate this last item is not strictly an R question, but my 
intention is to use these as input into the sem package for structural 
equation models. If anyone thinks that is misguided I would be intersted to 
here.


Michael Dewey
m.dewey at iop.kcl.ac.uk



From ggrothendieck at myway.com  Sun Nov 28 16:51:01 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 28 Nov 2004 15:51:01 +0000 (UTC)
Subject: [R] Correct Syntax for a Loop
References: <20041128112126.13093.qmail@web81005.mail.yahoo.com>
Message-ID: <loom.20041128T164848-835@post.gmane.org>

ebashi <arshia22 <at> yahoo.com> writes:

: 
: I'll appreciate if some one can help me with the
: following loop. This is the logic of the loop,
: if we have the following data;
: > x.df
:     x.dif
:  .    .
:  .    .
: 102  0.00
: 103  0.42
: 104  0.08
: 105  0.00
: 106  0.00
: 107  0.00
: 108 -0.16
: 109 -0.34
: 110  0.00
: 111 -0.17
: 112 -0.33
: 113  0.00
: 114  0.00
: 115  0.00
: 116  0.33
: 117  0.17
: 118  0.00 
:  .    .
:  .    .
: I'm trying to find i's where 
:   for (i in 2:length(x.dif))
:   if (x.dif[i-1]<=0 and x.dif[i]>0 and x.dif[i+2]>0)
:   it would return i+2 to me,
: How can I turn this to a right format as a loop.(I
: can't figure out the syntax)

One way is to convert it to a ts time series so you can use the lag
operator and have everything automatically aligned for you:

x.dif.ts <- ts(x.dif)
bool <- lag(x.dif.ts,-3) <= 0 & lag(x.dif.ts,-2) > 0 & x.dif.ts > 0
time(bool)[bool]



From cyracules at yahoo.co.uk  Sun Nov 28 18:06:51 2004
From: cyracules at yahoo.co.uk (John)
Date: Sun, 28 Nov 2004 17:06:51 +0000 (GMT)
Subject: [R] creating a sequence of object names
Message-ID: <20041128170651.71853.qmail@web26309.mail.ukl.yahoo.com>

Hello R-users,

I wanted to generate objects named 'my.ftn1',
'my.ftn2', ... , 'my.ftn10', and tried the following
code without success. How can I do this?

> for ( i in 1:10 ) {
+ sub(" ", "", paste("my.ftn", i)) <- NULL
+ }
Error: Target of assignment expands to non-language
object
> 

Many thanks.

John



From ligges at statistik.uni-dortmund.de  Sun Nov 28 18:22:00 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 28 Nov 2004 18:22:00 +0100
Subject: [R] creating a sequence of object names
In-Reply-To: <20041128170651.71853.qmail@web26309.mail.ukl.yahoo.com>
References: <20041128170651.71853.qmail@web26309.mail.ukl.yahoo.com>
Message-ID: <41AA0938.5010905@statistik.uni-dortmund.de>

John wrote:

> Hello R-users,
> 
> I wanted to generate objects named 'my.ftn1',
> 'my.ftn2', ... , 'my.ftn10', and tried the following
> code without success. How can I do this?
> 
> 
>>for ( i in 1:10 ) {
> 
> + sub(" ", "", paste("my.ftn", i)) <- NULL
> + }
> Error: Target of assignment expands to non-language
> object
> 
> 
> Many thanks.
> 
> John
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


Please do as suggested above, read the posting guide!
It suggests to read the FAQs. FAQ 7.21 is what you are looking for: "How 
can I turn a string into a variable?".

Uwe Ligges



From HDoran at air.org  Sun Nov 28 18:22:45 2004
From: HDoran at air.org (Doran, Harold)
Date: Sun, 28 Nov 2004 12:22:45 -0500
Subject: [R] Modifications to an abline
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7404044D97@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041128/ae4eeff9/attachment.pl

From HDoran at air.org  Sun Nov 28 18:53:11 2004
From: HDoran at air.org (Doran, Harold)
Date: Sun, 28 Nov 2004 12:53:11 -0500
Subject: [R] paste command
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7404044D99@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041128/00a338c2/attachment.pl

From bates at stat.wisc.edu  Sun Nov 28 18:57:17 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sun, 28 Nov 2004 11:57:17 -0600
Subject: [R] Modifications to an abline
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F7404044D97@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F7404044D97@dc1ex2.air.org>
Message-ID: <41AA117D.9@stat.wisc.edu>

Doran, Harold wrote:
> Dear List:
> 
> I am working to generate graphs for individual students that will be
> created through a series of loops in Sweave. Before doing so, I am
> still trying to design the graph. The code for creating the barplot
> is below with some sample datapoints just made up for now.
> 
> Ultimately, this chart will take data from an lme object using
> longitudinal student data. So, the dots represent the observed
> student scores at each occasion and the abline will be added using
> the data from coef(), which gives the intercept and slope for each
> individual student.
> 
> Now, the abline currently extends the entire graph, which I do not
> want exactly. Assume I only have data at grades 1,2, and 3 for this
> individual. I would want the abline to extend only as far as the
> observed data using a lty=1.
> 
> However, I want to show an extrapolated line from the current score
> to the end of the graph. I want for this line to be dashed, something
> like lty=2.
> 
> So, while the abline should extend the length of the chart, I want
> for it to change in style at a specified point. Can this be done?
> Last, I'm not sure I want the abline to extend the entire region of
> the chart, but maybe say to the middle of the barplot.
> 
> I would appreciate any thoughts on this. Any thoughts on code to
> improve the chart is also appreciated.

One approach is to use abline to put the dashed line on the plot then 
use segments to add the line covering the observed range with lty = 1.
You will need to calculate the end points of the solid line but those 
are just the values of the fitted line at the minimum and maximum 
observed grade levels.



From bates at stat.wisc.edu  Sun Nov 28 19:00:58 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: Sun, 28 Nov 2004 12:00:58 -0600
Subject: [R] paste command
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F7404044D99@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F7404044D99@dc1ex2.air.org>
Message-ID: <41AA125A.103@stat.wisc.edu>

Doran, Harold wrote:
> In a previous post, I mentioned a loop being used to generate graphs. I have some sample code partially put together but have found one offending line of code that I cannot figure out what to do with.
>  
> I have one data frame called grade4. If I do something like
>  
> hist(grade4$math)
>  
> I get the appropriate chart.
>  
> Within the loop, however, I am doing this for multiple files and grades, so I use the paste command, but it isn't working. For example, the code would perform something similar to:
>  
> hist(paste("grade", "4", "$math",sep=""))
>  
> which doesn't work. The code below is what I hope will ultimately be the working loop.
>  
> grade.list<-as.vector(unique(egsingle$grade))
> for(grade.number in grade.list){
> pdf(paste("grade",grade.number,".pdf",sep=""))
> hist(paste("grade",grade.number,"$math",sep=""))
> dev.off()}
>  
> I'm a little confused on how to get this work properly. Any thoughts are much appreciated.

Well, you could read the section of the FAQ that Uwe quoted in an 
earlier response today :-)

A short version of that section of the FAQ is that if you use

get(paste("grade", "4", sep = ""))[["math"]]

then you can generalize the "4" and the "math" to variable names.



From jfox at mcmaster.ca  Sun Nov 28 19:07:30 2004
From: jfox at mcmaster.ca (John Fox)
Date: Sun, 28 Nov 2004 13:07:30 -0500
Subject: [R] Tetrachoric and polychoric ceofficients (for sem) - any tips?
In-Reply-To: <6.1.0.6.0.20041128152028.026432e0@pop.freeserve.net>
Message-ID: <20041128180729.UZHW1899.tomts13-srv.bellnexxia.net@JohnDesktop8300>

Dear Michael,

I'm not aware of pre-existing R code for tetrachoric or polychoric
correlations. I may at some point incorporate such functions into the sem
package but I don't have concrete plans for doing so. On the other hand, I
don't think that it would be very hard to do so. (A discussion, references,
and an example are in Kotz and Johnson, eds., Encyclopedia of Statistics,
Vol 7.)

Tetrachoric and polychoric correlations are estimates, and in subsequently
estimating a SEM from these, one should take that into account. 

Regards,
 John

--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario
Canada L8S 4M4
905-525-9140x23604
http://socserv.mcmaster.ca/jfox 
-------------------------------- 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Michael Dewey
> Sent: Sunday, November 28, 2004 10:28 AM
> To: r-help-stat.math.ethz.ch
> Subject: [R] Tetrachoric and polychoric ceofficients (for 
> sem) - any tips?
> 
> About two years ago there was a thread about this which 
> suggested that at that time nobody had these coefficients ready to go.
> (a) has anyone in the meanwhile programmed them?
> (b) I think I can see how to do the tetrachoric one with 
> mvtnorm on similar lines to an example on the help page so 
> will try that if nobody else already has
> (c) looking at the polychoric one makes me realise yet again 
> that I wish I knew more mathematics. I would also find it 
> difficult to test it as I do not have any worked examples. 
> Does anyone have any tips about how to program it and how to 
> test the resultant code?
> (d) I appreciate this last item is not strictly an R 
> question, but my intention is to use these as input into the 
> sem package for structural equation models. If anyone thinks 
> that is misguided I would be intersted to here.
> 
> 
> Michael Dewey
> m.dewey at iop.kcl.ac.uk
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From obuhard at yahoo.fr  Sun Nov 28 20:42:06 2004
From: obuhard at yahoo.fr (BUHARD Olivier)
Date: Sun, 28 Nov 2004 20:42:06 +0100
Subject: [R] use of file.show() under linux session....
Message-ID: <1101655292.2321.0.camel@localhost.localdomain>

Hi all,

I know this is a stupid question, but I can't display a text file in a
separate console window under a R session in X11 (gnome with MDK 9.2).
I use R 2.0.0. When I enter :

> file.show("/path/to/my/file/filename.txt" ,pager="gnome-terminal")

the new window opens and then I just obtain the blinking cursor after
the console prompt :-(
I always obtain the same result, wathever the pager used (konsole,
gnome-terminal...)
When I open the file in the current window, it is displayed as any
normal text file, so I don't understand.
Well, it's trivial, but it could help me if I could see the file while
working in another window with the R session... of course, I could use
another soft (Gedit, or open a new console manually...) : well, I find
it more convinient without many different programs running in the
session.
Thanks in advance for help.

Best regards

BUHARD Olivier



From p.murrell at auckland.ac.nz  Sun Nov 28 20:46:36 2004
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Mon, 29 Nov 2004 08:46:36 +1300
Subject: [R] Making legend() look like my plot()
References: <Pine.LNX.4.21.0411260930170.21943-100000@mail.mrc-dunn.cam.ac.uk>
	<41A7094F.5050908@statistik.uni-dortmund.de>
Message-ID: <41AA2B1C.5050204@stat.auckland.ac.nz>

Hi


Uwe Ligges wrote:
> Dan Bolser wrote:
> 
>> On Fri, 26 Nov 2004, Martin Maechler wrote:
>>
>>
>>>>>>>> "Dan" == Dan Bolser <dmb at mrc-dunn.cam.ac.uk>
>>>>>>>>    on Thu, 25 Nov 2004 22:35:22 +0000 (GMT) writes:
>>>>>>>
>>>
>>>   Dan> On Thu, 25 Nov 2004, Uwe Ligges wrote:
>>>   >> Dan Bolser wrote:
>>>   >>> Is this an impossible task?
>>>   >>>   >>> How about just problem 2 below, having one pch in one
>>>   >>> legend entry, but no pch in the second?
>>>   >>  Please be at least a little bit patient! This is not a
>>>   >> hotline! People are not working 24 hours a day just to
>>>   >> answer your questions at once - they are answering
>>>   >> questions on a voluntary basis!
>>>   >>   >> answer 1) is not straightforward, but you might want to
>>>   >> use one of fillable symbols mentioned in ?points,
>>>   >> e.g. number 21
>>>   >>   >> answer 2) pch = c(1, NA) should do the trick.
>>>   >>   >> legend(....., pch=c(21,NA), lwd=c(1,3), lty=c(1,3),
>>>   >>               pt.bg="white", col=1:2)
>>>
>>>   Dan> Ahhh... I tried pch=c(1,NULL), pt.bg='white'   Dan> I couldn't 
>>> work out what was going on..
>>>
>>> hmm, really,... at least you could have tried to see  what
>>> c(1,NULL) is, by just "typing it at the prompt" !
>>
>>
>>
>> Too long at the SQL prompt made me think it was immutably correct.
>>
>> That and the fact that c(1,'') was making both pch dissapear made me 
>> think
>> the whole thing was messed up.
>>
>>
>> How come legend isn't ablt to auto detect the line types in my plot and
>> add them automatically?
>>
>> Is this just an issue of 'that would be cool if I had time', or is it 
>> more
>> fundamental?
> 
> 
> It is really fundamental!
> [Maybe not that fundamental for lattice as for base graphics.]


For example, ...

     xyplot(Sepal.Length + Sepal.Width ~ Petal.Length + Petal.Width |
            Species,
            data = iris, scales = "free", layout = c(2, 2),
            auto.key = TRUE)

... and even in traditional graphics there are some automated legends ...

       a <- expand.grid(1:20, 1:20)
       b <- matrix(a[,1] + a[,2], 20)
       filled.contour(x = 1:20, y = 1:20, z = b)

... but this is only really possible when an entire plot is produced 
from a single function call (so everything is known about the plot at 
once).  The general problem is that R tries to be useful by allowing you 
to add extra lines, points, ... to any part of a plot, but the price of 
this flexibility is that R has no idea what is actually a data series in 
a plot and what is just some piece of decoration.  With the grid 
graphics package, it is possible to build up a graphical object 
representing a plot, so it might then be possible to make the 
distinction between output representing data series and other stuff; 
and here we encounter your previous statement, 'that would be cool if I 
had time' :)

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From deepayan at stat.wisc.edu  Sun Nov 28 21:01:13 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Sun, 28 Nov 2004 14:01:13 -0600
Subject: [R] use of file.show() under linux session....
In-Reply-To: <1101655292.2321.0.camel@localhost.localdomain>
References: <1101655292.2321.0.camel@localhost.localdomain>
Message-ID: <200411281401.13351.deepayan@stat.wisc.edu>

On Sunday 28 November 2004 13:42, BUHARD Olivier wrote:
> Hi all,
>
> I know this is a stupid question, but I can't display a text file in
> a separate console window under a R session in X11 (gnome with MDK
> 9.2).
>
> I use R 2.0.0. When I enter :
> > file.show("/path/to/my/file/filename.txt" ,pager="gnome-terminal")
>
> the new window opens and then I just obtain the blinking cursor after
> the console prompt :-(
> I always obtain the same result, wathever the pager used (konsole,
> gnome-terminal...)

Why are you expecting it to do anything else? Does 

$ gnome-terminal /path/to/my/file/filename.txt

display your file? gnome-terminal / konsole are not pagers.

> When I open the file in the current window, it is displayed as any
> normal text file, so I don't understand.

What do you mean by 'open'?

> Well, it's trivial, but it could help me if I could see the file
> while working in another window with the R session... of course, I
> could use another soft (Gedit, or open a new console manually...) :
> well, I find it more convinient without many different programs
> running in the session.

On my system at least, if there's an already running gedit, 

> file.show("myfile", pager = "gedit")

opens the file in a new tab (note, not a new program or even a new 
window) in gedit, and returns immediately to the R prompt, where you 
can do other stuff while the file is being displayed.

Deepayan



From cyracules at yahoo.co.uk  Sun Nov 28 22:26:24 2004
From: cyracules at yahoo.co.uk (John)
Date: Sun, 28 Nov 2004 21:26:24 +0000 (GMT)
Subject: [R] creating a sequence of object names
In-Reply-To: <41AA0938.5010905@statistik.uni-dortmund.de>
Message-ID: <20041128212624.81477.qmail@web26306.mail.ukl.yahoo.com>

Thank you, Uwe. I've found a way to do the job by
reading the FAQ 7.21 although it is not giving a
precise explanation to a novice or casual user at
first reading. For example, if you type the first two
lines in the FAQ, you get an error as you do not have
the variable, a, initially.

I am sure that more and more people get interested in
and serious about using R if advanced users are kind
enough to answer simple and silly questions as well
which are already explained in basic documentations.
Or is this community for highly motivated and advanced
R users only?

Regards,

John


 --- Uwe Ligges <ligges at statistik.uni-dortmund.de>
wrote: 
> John wrote:
> 
> > Hello R-users,
> > 
> > I wanted to generate objects named 'my.ftn1',
> > 'my.ftn2', ... , 'my.ftn10', and tried the
> following
> > code without success. How can I do this?
> > 
> > 
> >>for ( i in 1:10 ) {
> > 
> > + sub(" ", "", paste("my.ftn", i)) <- NULL
> > + }
> > Error: Target of assignment expands to
> non-language
> > object
> > 
> > 
> > Many thanks.
> > 
> > John
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 
> Please do as suggested above, read the posting
> guide!
> It suggests to read the FAQs. FAQ 7.21 is what you
> are looking for: "How 
> can I turn a string into a variable?".
> 
> Uwe Ligges
>



From fhduan at gmail.com  Mon Nov 29 00:13:20 2004
From: fhduan at gmail.com (Frank Duan)
Date: Sun, 28 Nov 2004 18:13:20 -0500
Subject: [R] Could anyone help me reshape this "wide" data into
	"longitudinal" one? Thanks
Message-ID: <3b91723104112815133b7e64ca@mail.gmail.com>

Dear R people,

I have a matrix like this:

      var1   var2    var3    var4 
a1   7.1   7.2       8.1    8.2 
a2  10.5  10.6    ...       ...
a3
b1
b2
b3
b4
c1
c2
...

The matrix row names are "a1", "a2", ......  and the matrix column
names are "var1", "var2", "var3" and "var4". Now I want to reshape
this data into a longitudinal-formatted one like this:


subject    seq   time    resp        
a              1      1        7.1
a              1      2        7.2
a              1      3        8.1
a              1      4        8.2
a              2      1       10.5 
a              2      2       10.6
a              2      3         ...
a              2      4         ...
a              3      1         ...
a              3      2
a              3      3
a              3      4
b              1      1
b              1      2
b              1      3
b              1      4
b              2      1
b              2      2
b              2      3
b              2      4
...             ...    ...
...             ...    ...
...             ...    ...


I always met errors when using "reshape" function. Could anyone give
me an idea how to do it?

Many thanks to you,

Frank



From rbaer at atsu.edu  Mon Nov 29 02:19:11 2004
From: rbaer at atsu.edu (Robert W. Baer, Ph.D.)
Date: Sun, 28 Nov 2004 19:19:11 -0600
Subject: [R] Could anyone help me reshape this "wide" data
	into"longitudinal" one? Thanks
References: <3b91723104112815133b7e64ca@mail.gmail.com>
Message-ID: <001001c4d5b1$73a509e0$2781010a@BigBaer>

This might get you started:
# make data in original format
a=matrix(1:48,12,4)
colnames(a)<-c('var1','var2','var3','var4')
rownames(a)<-c('a1','a2','a3','b1','b2','b3','c1','c2','c3','d1','d2','d3')
a

#rehape the data
a=data.frame(a)
reshape(a[1:4],idvar="row",
timevar="column",varying=list(names(a[1:4])),direction="long")

Rob

----- Original Message ----- 
From: "Frank Duan" <fhduan at gmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Sunday, November 28, 2004 5:13 PM
Subject: [R] Could anyone help me reshape this "wide" data
into"longitudinal" one? Thanks


> Dear R people,
>
> I have a matrix like this:
>
>       var1   var2    var3    var4
> a1   7.1   7.2       8.1    8.2
> a2  10.5  10.6    ...       ...
> a3
> b1
> b2
> b3
> b4
> c1
> c2
> ...
>
> The matrix row names are "a1", "a2", ......  and the matrix column
> names are "var1", "var2", "var3" and "var4". Now I want to reshape
> this data into a longitudinal-formatted one like this:
>
>
> subject    seq   time    resp
> a              1      1        7.1
> a              1      2        7.2
> a              1      3        8.1
> a              1      4        8.2
> a              2      1       10.5
> a              2      2       10.6
> a              2      3         ...
> a              2      4         ...
> a              3      1         ...
> a              3      2
> a              3      3
> a              3      4
> b              1      1
> b              1      2
> b              1      3
> b              1      4
> b              2      1
> b              2      2
> b              2      3
> b              2      4
> ...             ...    ...
> ...             ...    ...
> ...             ...    ...
>
>
> I always met errors when using "reshape" function. Could anyone give
> me an idea how to do it?
>
> Many thanks to you,
>
> Frank
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>



From dvrecko at sfu.ca  Mon Nov 29 02:35:11 2004
From: dvrecko at sfu.ca (dvrecko@sfu.ca)
Date: Sun, 28 Nov 2004 17:35:11 -0800
Subject: [R]: Adding a line in the graph of  'plot()'
Message-ID: <200411290135.iAT1ZBO2013387@rm-rstar.sfu.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041128/acd5d4b0/attachment.pl

From Tom.Mulholland at dpi.wa.gov.au  Mon Nov 29 03:02:29 2004
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Mon, 29 Nov 2004 10:02:29 +0800
Subject: [R] RE: Adding a line in the graph of  'plot()'
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA36@afhex01.dpi.wa.gov.au>

The general nature of your question means there are a multitude of answers.

if you type '?abline' it will give you an example of a line drawn over a plot.

type ?segments, ?lines, ?points, ?text and go through the examples and try them out

The help pages to the base package 'grid' includes an introduction to using the various drawing mechanisms

see ?par for the 'new' argument

logical, defaulting to 'FALSE'.  If set to 'TRUE', the next
high-level plotting command (actually 'plot.new') should _not
clean_ the frame before drawing "as if it was on a *_new_*
device".

Alternatively the 'lattice' package can produce a wide range of plots.

Then try the various contributed documents that exist on the CRAN mirrors to further your knowledge. It's all there. 

Tom Mulholland

-----Original Message-----
From: dvrecko at sfu.ca [mailto:dvrecko at sfu.ca]
Sent: Monday, 29 November 2004 9:35 AM
To: r-help at stat.math.ethz.ch
Subject: [R]: Adding a line in the graph of 'plot()'


Hello.

I'm looking for a way to add a line in a plot of a points that should lie
along a particular line. Can I add a line to the 'plot()' function, maybe
using 'abline()' so that the line is visible in the graph of 'plot()'? How?
More generally, can I overlay plots over one another?

Thanks.

Dean Vrecko
Simon Fraser University

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ihok at hotmail.com  Mon Nov 29 03:25:24 2004
From: ihok at hotmail.com (Jack Tanner)
Date: Sun, 28 Nov 2004 21:25:24 -0500
Subject: [R] proper way to process dataframe by rows
Message-ID: <BAY102-DAV101A6E49CC92511482A9F9CABD0@phx.gbl>

This is a best practices / style question.

The way I use RODBC is I something like this:

 > foo <- sqlQuery(db, "select * from foo")
 > apply(foo, 1, function{...})

That is, I use apply to iterate over each result -- row -- in the 
RODBC-produced dataframe. Is this how one generally wants to do this?

My concern is that when apply iterates over the rows, it uses 
as.matrix() to convert the dataframe to a character representation of 
itself. Thus my database's carefully planned data types (that RODBC 
carefully preserved when returning query results) get completely lost as 
I process the data. I've taken to judiciously sprinkling as.numeric() 
and friends here and there, but this is just begging for bugs.

In other words, what is the smart way to process a dataframe by rows? Or 
is there, by chance, a specific technique or practice that is available 
for RODBC results but not for dataframes in general?

Thank you for your thoughts.



From ggrothendieck at myway.com  Mon Nov 29 03:44:11 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 29 Nov 2004 02:44:11 +0000 (UTC)
Subject: [R] proper way to process dataframe by rows
References: <BAY102-DAV101A6E49CC92511482A9F9CABD0@phx.gbl>
Message-ID: <loom.20041129T034230-722@post.gmane.org>

Jack Tanner <ihok <at> hotmail.com> writes:

: 
: This is a best practices / style question.
: 
: The way I use RODBC is I something like this:
: 
:  > foo <- sqlQuery(db, "select * from foo")
:  > apply(foo, 1, function{...})
: 
: That is, I use apply to iterate over each result -- row -- in the 
: RODBC-produced dataframe. Is this how one generally wants to do this?
: 
: My concern is that when apply iterates over the rows, it uses 
: as.matrix() to convert the dataframe to a character representation of 
: itself. Thus my database's carefully planned data types (that RODBC 
: carefully preserved when returning query results) get completely lost as 
: I process the data. I've taken to judiciously sprinkling as.numeric() 
: and friends here and there, but this is just begging for bugs.
: 
: In other words, what is the smart way to process a dataframe by rows? Or 
: is there, by chance, a specific technique or practice that is available 
: for RODBC results but not for dataframes in general?
: 

Don't know about the best way but here is one way that does not 
convert to charadter:

R> data(iris)
R> irish <- head(iris)
R> irish
  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1          5.1         3.5          1.4         0.2  setosa
2          4.9         3.0          1.4         0.2  setosa
3          4.7         3.2          1.3         0.2  setosa
4          4.6         3.1          1.5         0.2  setosa
5          5.0         3.6          1.4         0.2  setosa
6          5.4         3.9          1.7         0.4  setosa
R> f <- function(i) with(irish[i,], Sepal.Length + Sepal.Width)
R> sapply(1:nrow(irish), f)
[1] 8.6 7.9 7.9 7.7 8.6 9.3



From murdoch at stats.uwo.ca  Mon Nov 29 04:38:23 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Sun, 28 Nov 2004 22:38:23 -0500
Subject: [R] proper way to process dataframe by rows
In-Reply-To: <BAY102-DAV101A6E49CC92511482A9F9CABD0@phx.gbl>
	<41AA8894.1070703@hotmail.com>
References: <BAY102-DAV101A6E49CC92511482A9F9CABD0@phx.gbl>
	<41AA8894.1070703@hotmail.com>
Message-ID: <196lq0p2lgj22ajuvc8g3laatk5quc019k@4ax.com>

On Sun, 28 Nov 2004 21:25:24 -0500, Jack Tanner <ihok at hotmail.com>
wrote:

>This is a best practices / style question.
>
>The way I use RODBC is I something like this:
>
> > foo <- sqlQuery(db, "select * from foo")
> > apply(foo, 1, function{...})
>
>That is, I use apply to iterate over each result -- row -- in the 
>RODBC-produced dataframe. Is this how one generally wants to do this?
>
>My concern is that when apply iterates over the rows, it uses 
>as.matrix() to convert the dataframe to a character representation of 
>itself. Thus my database's carefully planned data types (that RODBC 
>carefully preserved when returning query results) get completely lost as 
>I process the data. I've taken to judiciously sprinkling as.numeric() 
>and friends here and there, but this is just begging for bugs.
>
>In other words, what is the smart way to process a dataframe by rows? Or 
>is there, by chance, a specific technique or practice that is available 
>for RODBC results but not for dataframes in general?

I would just use a for() loop if I didn't care about the speed too
much.  If I did, I'd avoid dealing with rows of dataframes:  access
using dataframe indexing is slow.  Depending what your function is,
you're probably better off extracting the columns of the dataframe as
vectors, and working with those.

Duncan Murdoch



From mcneney at stat.sfu.ca  Mon Nov 29 06:49:14 2004
From: mcneney at stat.sfu.ca (Brad McNeney)
Date: Sun, 28 Nov 2004 21:49:14 -0800
Subject: [R]: Adding a line in the graph of  'plot()'
In-Reply-To: <200411290135.iAT1ZBO2013387@rm-rstar.sfu.ca>
References: <200411290135.iAT1ZBO2013387@rm-rstar.sfu.ca>
Message-ID: <41AAB85A.8030704@stat.sfu.ca>

dvrecko at sfu.ca wrote:

>Hello.
>
>I'm looking for a way to add a line in a plot of a points that should lie
>along a particular line. Can I add a line to the 'plot()' function, maybe
>using 'abline()' so that the line is visible in the graph of 'plot()'? How?
>  
>
It sounds like abline will do what you want.

>More generally, can I overlay plots over one another?
>
>  
>
Try help("points"), help("lines"), etc. You should read the section on 
graphical procedures in the manual "An Introduction to R" (as it 
suggests in the posting guide, which you can access through the URL at 
the bottom of your message).

Brad

>Thanks.
>
>Dean Vrecko
>Simon Fraser University
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From ligges at statistik.uni-dortmund.de  Mon Nov 29 09:05:55 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 29 Nov 2004 09:05:55 +0100
Subject: Reasons not to answer very basic questions in a straightforward way; 
	was: Re: [R] creating a sequence of object names
In-Reply-To: <20041128212624.81477.qmail@web26306.mail.ukl.yahoo.com>
References: <20041128212624.81477.qmail@web26306.mail.ukl.yahoo.com>
Message-ID: <41AAD863.1070401@statistik.uni-dortmund.de>

John wrote:
> Thank you, Uwe. I've found a way to do the job by
> reading the FAQ 7.21 although it is not giving a
> precise explanation to a novice or casual user at
> first reading. For example, if you type the first two

But the corresponding help files do so, for sure, and the FAQ 7.21 
points you to ?assign and ?get.


> lines in the FAQ, you get an error as you do not have
> the variable, a, initially.
>
> I am sure that more and more people get interested in
> and serious about using R if advanced users are kind
> enough to answer simple and silly questions as well
> which are already explained in basic documentations.
> Or is this community for highly motivated and advanced
> R users only?

No, of course it is for novices as well!

BUT we do expect that novices do read basic documentation such as "An 
Introduction to R" and the R FAQ before asking question.
If there are too many silly questions from thousands of R users, nobody 
is able to manage the questions any more. And note that those people 
answering questions do it on a voluntary basis, and (at least partially) 
in their spare time!
Nobody would be subscribed to R-help any more, if there are 1000 mails a 
day, 900 of them containing silly questions! It is yet already hard 
enough to get through the huge amount of messages in a reasonable amount 
of time!


I have answered your question in a way,

  1) so that it is up to you to read some documentation. Now you have 
seen the FAQs and some help files. And you have learned much more than 
you would have learned if I had said "Use assign()"

  2) so that nobody feels too encouraged to ask questions before reading 
basic documentation - and my answer still saved you a lot of time!

Uwe Ligges




> Regards,
> 
> John
> 
> 
>  --- Uwe Ligges <ligges at statistik.uni-dortmund.de>
> wrote: 
> 
>>John wrote:
>>
>>
>>>Hello R-users,
>>>
>>>I wanted to generate objects named 'my.ftn1',
>>>'my.ftn2', ... , 'my.ftn10', and tried the
>>
>>following
>>
>>>code without success. How can I do this?
>>>
>>>
>>>
>>>>for ( i in 1:10 ) {
>>>
>>>+ sub(" ", "", paste("my.ftn", i)) <- NULL
>>>+ }
>>>Error: Target of assignment expands to
>>
>>non-language
>>
>>>object
>>>
>>>
>>>Many thanks.
>>>
>>>John
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://stat.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide!
>>
>>http://www.R-project.org/posting-guide.html
>>
>>
>>Please do as suggested above, read the posting
>>guide!
>>It suggests to read the FAQs. FAQ 7.21 is what you
>>are looking for: "How 
>>can I turn a string into a variable?".
>>
>>Uwe Ligges
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From gregor.gorjanc at bfro.uni-lj.si  Mon Nov 29 10:27:55 2004
From: gregor.gorjanc at bfro.uni-lj.si (Gregor GORJANC)
Date: Mon, 29 Nov 2004 09:27:55 +0000
Subject: [R] hist and truehist
Message-ID: <41AAEB9B.7050902@bfro.uni-lj.si>

Peter thanks for the response.

So the results from hist(mydata) and truehist(mydata, h = .5) are the 
same. OK, but the sum of densities or intensities, for case that I gave, 
don't sum to 1 but to 2. Look bellow. I have an example where these 
density values are also up to 4 and sum to 5 (I have attached the PDF of 
that plot).

This is really frustrating for me. What are actually these intensisties 
and densities, how are they calculated. Why are they the same?

mydata <- c(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,3,4,5)
# histogram with frequencies
hist(mydata)
# histogram with ratios or probabilities
hist(mydata, freq = F) # what are that values on vertical axis
# lets take a look at values behind
x <-hist(mydata, freq = F, plot = F); x
# Sum values
sum(x$intensities)
[1] 2.000000
R > sum(x$density)
[1] 2.000000

When I think of histogram (the one not with frequencies) I think of
gathering records into "some classes" and then divide the number of
records in each "class" by total number of all records. This is not the
case in hist().

Sorry for being pain in the ..., but this is really weird. Above that 
R-team is really doing a great job. Thanks for such a good tool!

-- 
Lep pozdrav / With regards / Con respeto,
     Gregor GORJANC

---------------------------------------------------------------
University of Ljubljana
Biotechnical Faculty       URI: http://www.bfro.uni-lj.si
Zootechnical Department    mail: gregor.gorjanc <at> bfro.uni-lj.si
Groblje 3                  tel: +386 (0)1 72 17 861
SI-1230 Domzale            fax: +386 (0)1 72 41 005
Slovenia
---------------------------------------------------------------
-------------- next part --------------
A non-text attachment was scrubbed...
Name: Rplots.pdf
Type: application/pdf
Size: 4018 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20041129/affffeb4/Rplots.pdf

From maechler at stat.math.ethz.ch  Mon Nov 29 09:32:12 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 29 Nov 2004 09:32:12 +0100
Subject: [R] Storing loop output from a function
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E391@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E391@usrymx25.merck.com>
Message-ID: <16810.56972.371924.912420@gargle.gargle.HOWL>

>>>>> "UweL" == Uwe Ligges <ligges at statistik.uni-dortmund.de>
>>>>>     on Sat, 27 Nov 2004 17:09:26 +0100 writes:

    UweL> Andrew Kniss wrote:
    >> I am attempting to write an R function to aid in time series diagnostics.
    >> The tsdiag() works well, but I would prefer to get a plot with ACF, PACF,
    >> and Ljung-Box p-values of residuals.  In my attempt to get to that point, I
    >> am writing a function to calculate Ljung-Box p-values for residuals at
    >> various lag distances.
    >> 
    >> ljung <- function(fit) 
    >>   for(i in c(1:24,36,48))   
    >>    {box<-(Box.test(fit$residuals, lag=i, type="Ljung-Box")) 
    >>     print(c(i, box$p.value))}
    >> 

    UweL> You need to return() rather than print() the object.

and Andy Liaw correctly remarked that

    AndyL> Yes, but print() is supposed to return (invisibly) its argument(s)...

Yes.
But I'm pretty sure, Andrew's problem is to get all the p-values back
from his function and not just the last one

Which -- (together with making 'lags' a function argument)
would be something like

    ljung <- function(fit, lags = c(1:24,36,48)) 
    {
	nl <- length(lags)
        r <- numeric(nl)
	for(i in 1:nl)
	   r[i] <- Box.test(fit$residuals, lag= lags[i], type="Ljung-Box")
        r
    }

or, simply, more elegantly and efficiently, 
but a bit harder to understand for a beginner,

    ljung <- function(fit, lags = c(1:24,36,48)) 
        sapply(lags, function(ll) 
               Box.test(fit$residuals, lag = ll, type="Ljung-Box"))

Martin Maechler, ETH Zurich



From ligges at statistik.uni-dortmund.de  Mon Nov 29 09:49:28 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 29 Nov 2004 09:49:28 +0100
Subject: [R] hist and truehist
In-Reply-To: <41AAEB9B.7050902@bfro.uni-lj.si>
References: <41AAEB9B.7050902@bfro.uni-lj.si>
Message-ID: <41AAE298.1050803@statistik.uni-dortmund.de>

Gregor GORJANC wrote:

> Peter thanks for the response.
> 
> So the results from hist(mydata) and truehist(mydata, h = .5) are the 
> same. OK, but the sum of densities or intensities, for case that I gave, 
> don't sum to 1 but to 2. Look bellow. I have an example where these 
> density values are also up to 4 and sum to 5 (I have attached the PDF of 
> that plot).
> 
> This is really frustrating for me. What are actually these intensisties 
> and densities, how are they calculated. Why are they the same?
> 
> mydata <- c(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,3,4,5)
> # histogram with frequencies
> hist(mydata)
> # histogram with ratios or probabilities
> hist(mydata, freq = F) # what are that values on vertical axis
> # lets take a look at values behind
> x <-hist(mydata, freq = F, plot = F); x
> # Sum values
> sum(x$intensities)
> [1] 2.000000
> R > sum(x$density)
> [1] 2.000000


Well, look at the breaks! Each has with 0.5!!!!
Hence the you have to calculate sum(x$density)*0.5

Uwe Ligges


> When I think of histogram (the one not with frequencies) I think of
> gathering records into "some classes" and then divide the number of
> records in each "class" by total number of all records. This is not the
> case in hist().
> 
> Sorry for being pain in the ..., but this is really weird. Above that 
> R-team is really doing a great job. Thanks for such a good tool!
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From maechler at stat.math.ethz.ch  Mon Nov 29 10:36:22 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 29 Nov 2004 10:36:22 +0100
Subject: [R] lm help: using lm when one point is known (not y intercept)
In-Reply-To: <20041128044907.VOUE1836.tomts16-srv.bellnexxia.net@JohnDesktop8300>
References: <41A94F59.6040702@wisc.edu>
	<20041128044907.VOUE1836.tomts16-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <16810.60822.357119.180166@gargle.gargle.HOWL>

>>>>> "JohnF" == John Fox <jfox at mcmaster.ca>
>>>>>     on Sat, 27 Nov 2004 23:49:08 -0500 writes:

    JohnF> Dear Seth,

    JohnF> You don't say which variable is the explanatory
    JohnF> variable and which is the response, but assuming that
    JohnF> prob is to be regressed on effect, you can fit
    JohnF> lm(prob - 50 ~ I(effect + 37.25) - 1). That is you
    JohnF> can shift the point through which the regression is
    JohnF> to go to the origin and then force the regression
    JohnF> through the origin.

    JohnF> I hope this helps,
yes, nice!

Even a bit more useful {though slightly uglier} is to use offset():

     mfit <- lm(prob ~ offset(50+ 0*effect) + I(effect + 37.25) - 1)

such that e.g. predict(mfit, ...) will still predict 'prob'

Note however that for both solutions, the regression abline()
will look wrong {and I hoped it would also be ok when using offset()},
plot(prob ~ effect)  ; abline(mfit)

Martin


    JohnF> John

    JohnF> --------------------------------
    JohnF> John Fox
    JohnF> Department of Sociology
    JohnF> McMaster University
    JohnF> Hamilton, Ontario
    JohnF> Canada L8S 4M4
    JohnF> 905-525-9140x23604
    JohnF> http://socserv.mcmaster.ca/jfox 
    JohnF> -------------------------------- 

    >> -----Original Message-----
    >> To: r-help at stat.math.ethz.ch
    >> Subject: [R] lm help: using lm when one point is known (not y intercept)
    >> 
    >> Hello-
    >> 
    >> My question is a short one.  How can I specify a single point 
    >> which through the fitted linear model has to go through?  To 
    >> illustrate my problem, the fit to following data must go 
    >> through the point (-37.25(effect), 50(prob)).  Note: you can 
    >> ignore the label column.
    >> 
    >> Effect      Prob Label
    >> 
    >> 1 -1143.75  7.142857     L
    >> 2  -572.75 21.428571     D
    >> 3  -223.75 35.714286    GL
    >> 4   123.25 50.000000    DG
    >> 5   359.75 64.285714     G
    >> 6   374.75 78.571429   DGL
    >> 7   821.75 92.857143    DL
    >> 
    >> Thanks in advance!
    >> 
    >> Seth Imhoff



From mdowle at concordiafunds.com  Mon Nov 29 10:58:00 2004
From: mdowle at concordiafunds.com (Matthew Dowle)
Date: Mon, 29 Nov 2004 09:58:00 -0000
Subject: [R] Tcl error - brace in argument?
Message-ID: <78166BFC5165D811AA0400065BF0324B6B93D6@wisconsin.concordia>


Peter,

Yes c(0,23) works.

Many thanks!
Matthew


-----Original Message-----
From: Peter Dalgaard [mailto:p.dalgaard at biostat.ku.dk] 
Sent: 26 November 2004 16:43
To: Peter Dalgaard
Cc: Matthew Dowle; 'r-help at stat.math.ethz.ch'
Subject: Re: [R] Tcl error - brace in argument?


Peter Dalgaard <p.dalgaard at biostat.ku.dk> writes:

> Matthew Dowle <mdowle at concordiafunds.com> writes:
> 
> > Hi all,
> >  
> > Does anyone know a solution for this error ?
> >  
> > > tkwidget(dlg, "iwidgets::spinint", range="{0 23}")
> 
> I suspect you want range=as.tclObj(c(0,23)) or something like that, 
> i.e. a Tcl list of two numbers, not a five-character string.

On second thoughts: I think range=c(0,23) should do.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From gregoire.thomas at ugent.be  Mon Nov 29 10:59:15 2004
From: gregoire.thomas at ugent.be (Gregoire Thomas)
Date: Mon, 29 Nov 2004 10:59:15 +0100
Subject: [R] PlotML
Message-ID: <41AAF2F3.30807@ugent.be>

Dear all,
Has anybody ever written some plot / hist functions that would return 
PlotML code? [http://ptolemy.eecs.berkeley.edu/]
Regards,
Gregoire

From cyracules at yahoo.co.uk  Mon Nov 29 11:12:54 2004
From: cyracules at yahoo.co.uk (John)
Date: Mon, 29 Nov 2004 10:12:54 +0000 (GMT)
Subject: Reasons not to answer very basic questions in a straightforward
	way; was: Re: [R] creating a sequence of object names
In-Reply-To: <41AAD863.1070401@statistik.uni-dortmund.de>
Message-ID: <20041129101254.38226.qmail@web26308.mail.ukl.yahoo.com>

Dear Uwe,

I must say that I had thanked you for referring me to
the "specific and exact" FAQ 7.21 and I had solved my
simple problem from it. I alreadly had looked at some
of basic materials like 'An Introduction to R', 'R for
Beginners', 'R Data Import/Export as well as the
FAQ(that is, I know how to use ?assign and ?get). But,
because I am not going to be an expert in R I assume
that I have missed something (even very trivial) in
those documents. Of course, I can read them again and
again until I know everything in them. That is for
more interested enthusiasts, however. 

I know very well that it is basic manners to read
those materials before asking questions here, but you
should also understand that people sometimes get stuck
with very simple problems if they are driven by stress
or run down. They can save a lot of time and
concentrate on and develop their primary jobs instead.
And I don't think you should be worried about 900
silly questions out of 1000 messages posted because
they are at least well-educated people who know what
reading basic materials before posting questions
means.

People can learn diverse solutions about their simple
questions, from advanced experienced users, that
sometimes contain much more informations and tips.
It is up to users(not necessarily advanced users)
whether or not they are willing to answer questions
and share their precious (even little) findings in
programming. Volunteers can simply ignore silly
questions if they are not appropriate for answering.
Or I would let them know what to do with their
improper questions in a personal email.

Finally, I do appreciate your answer again and other
people's active replies too. It was really useful to
point me to the specific FAQ rather than to just say
'look at the FAQ'. It simply occurred to my mind that
"kindness" is the best policy for good education.

I beg your pardon if this message is not relevant to
this help list.

With kind regards,

John


 --- Uwe Ligges <ligges at statistik.uni-dortmund.de>
wrote: 
> John wrote:
> > Thank you, Uwe. I've found a way to do the job by
> > reading the FAQ 7.21 although it is not giving a
> > precise explanation to a novice or casual user at
> > first reading. For example, if you type the first
> two
> 
> But the corresponding help files do so, for sure,
> and the FAQ 7.21 
> points you to ?assign and ?get.
> 
> 
> > lines in the FAQ, you get an error as you do not
> have
> > the variable, a, initially.
> >
> > I am sure that more and more people get interested
> in
> > and serious about using R if advanced users are
> kind
> > enough to answer simple and silly questions as
> well
> > which are already explained in basic
> documentations.
> > Or is this community for highly motivated and
> advanced
> > R users only?
> 
> No, of course it is for novices as well!
> 
> BUT we do expect that novices do read basic
> documentation such as "An 
> Introduction to R" and the R FAQ before asking
> question.
> If there are too many silly questions from thousands
> of R users, nobody 
> is able to manage the questions any more. And note
> that those people 
> answering questions do it on a voluntary basis, and
> (at least partially) 
> in their spare time!
> Nobody would be subscribed to R-help any more, if
> there are 1000 mails a 
> day, 900 of them containing silly questions! It is
> yet already hard 
> enough to get through the huge amount of messages in
> a reasonable amount 
> of time!
> 
> 
> I have answered your question in a way,
> 
>   1) so that it is up to you to read some
> documentation. Now you have 
> seen the FAQs and some help files. And you have
> learned much more than 
> you would have learned if I had said "Use assign()"
> 
>   2) so that nobody feels too encouraged to ask
> questions before reading 
> basic documentation - and my answer still saved you
> a lot of time!
> 
> Uwe Ligges
> 
> 
> 
> 
> > Regards,
> > 
> > John
> > 
> > 
> >  --- Uwe Ligges <ligges at statistik.uni-dortmund.de>
> > wrote: 
> > 
> >>John wrote:
> >>
> >>
> >>>Hello R-users,
> >>>
> >>>I wanted to generate objects named 'my.ftn1',
> >>>'my.ftn2', ... , 'my.ftn10', and tried the
> >>
> >>following
> >>
> >>>code without success. How can I do this?
> >>>
> >>>
> >>>
> >>>>for ( i in 1:10 ) {
> >>>
> >>>+ sub(" ", "", paste("my.ftn", i)) <- NULL
> >>>+ }
> >>>Error: Target of assignment expands to
> >>
> >>non-language
> >>
> >>>object
> >>>
> >>>
> >>>Many thanks.
> >>>
> >>>John
> >>>
> >>>______________________________________________
> >>>R-help at stat.math.ethz.ch mailing list
> >>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>PLEASE do read the posting guide!
> >>
> >>http://www.R-project.org/posting-guide.html
> >>
> >>
> >>Please do as suggested above, read the posting
> >>guide!
> >>It suggests to read the FAQs. FAQ 7.21 is what you
> >>are looking for: "How 
> >>can I turn a string into a variable?".
> >>
> >>Uwe Ligges
> >>
> > 
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
>



From chancs at gis.a-star.edu.sg  Mon Nov 29 11:12:57 2004
From: chancs at gis.a-star.edu.sg (CHAN Chee Seng)
Date: Mon, 29 Nov 2004 18:12:57 +0800
Subject: [R] core dump during make check when building 64-bit R on Solaris
	8/9
Message-ID: <6D9E9B9DF347EF4385F6271C64FB8D56012FE0DD@BIONIC.biopolis.one-north.com>

Hi,

I am building a 64-bit R 2.0.1 on Solaris 9.  The compiler is Sun Studio
8.  Make was successful but I have a core dump during a make check.  By
the way, this problem also happens on my Solaris 8 machine though I did
not get a core dump.  I do not have 64-bit versions of the readline,
tcl/tk, libncurses, etc libraries so these caused configure not to use
them with ELFCLASS32 errors.

I use the following flags in config.site:
CC="cc -xarch=v9"
CFLAGS="-xO5 -xlibmil -dalign"
F77="f95 -xarch=v9"
FFLAGS="-xO5 -xlibmil -dalign"
CXX="CC -xarch=v9"


I pasted the make check error messages below:
$ make check
collecting examples for package 'base' ...
 >>> Building/Updating help pages for package 'base'
     Formats: text html latex example 
running code in 'base-Ex.R' ...*** Error code 1
make: Fatal error: Command failed for target `base-Ex.Rout'
Current working directory /export/home/cheeseng/R-2.0.1/tests/Examples
*** Error code 1
make: Fatal error: Command failed for target `test-Examples-Base'
Current working directory /export/home/cheeseng/R-2.0.1/tests/Examples
*** Error code 1
make: Fatal error: Command failed for target `test-Examples'
Current working directory /export/home/cheeseng/R-2.0.1/tests
*** Error code 1
make: Fatal error: Command failed for target `test-all-basics'
Current working directory /export/home/cheeseng/R-2.0.1/tests
*** Error code 1
make: Fatal error: Command failed for target `check'


The last few lines in base-Ex.Rout.fail is:
> ### * kappa
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: kappa
> ### Title: Estimate the Condition Number
> ### Aliases: kappa kappa.default kappa.lm kappa.qr kappa.tri
> ### Keywords: math
> 
> ### ** Examples
> 
> kappa(x1 <- cbind(1,1:10))# 15.71
[1] 15.70590


Doing a dbx on the core dump shows that the following:
program terminated by signal SEGV (no mapping at the fault address)
0xffffffff790fdf54: ___pl_dgesdd_64_+0x1654:    std      %f4, [%o1]

I hope some one have a more successful build and can show how you did
it.

Thanks,
Chee Seng
UNIX Administrator
Genome Institute of Singapore
60 Biopolis Street, Genome
#02-01 Singapore 138672
DID 64788065
------------------------------- 
This email is confidential and may be privileged.  If you are not the
intended recipient, please delete it and notify us immediately. Please
do not copy or use it for any purpose, or disclose its contents to any
other person. Thank you.



From p.dalgaard at biostat.ku.dk  Mon Nov 29 11:32:53 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 29 Nov 2004 11:32:53 +0100
Subject: [R] PlotML
In-Reply-To: <41AAF2F3.30807@ugent.be>
References: <41AAF2F3.30807@ugent.be>
Message-ID: <x2fz2tkkru.fsf@biostat.ku.dk>

Gregoire Thomas <gregoire.thomas at ugent.be> writes:

> Dear all,
> Has anybody ever written some plot / hist functions that would return
> PlotML code? [http://ptolemy.eecs.berkeley.edu/]

Not that I know of, and looking at the design of PlotML, it doesn't
look like a nice fit as an R device driver. PlotML works at the level
of data sets, bar graphs, axes, etc., not with low-level items like
lines, polygons, and text. There are general XML handling tools in the
XML package, though.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From john.maindonald at anu.edu.au  Mon Nov 29 11:46:00 2004
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Mon, 29 Nov 2004 21:46:00 +1100
Subject: [R] Call to trellis.focus(); thenpanel.superpose()
Message-ID: <DC12507A-41F3-11D9-92B8-000A95CDA0F2@anu.edu.au>

The following works fine with the x11 device, though it
may well be that an initial plot is overwritten.  With a pdf
or postscript device, I get two plots, the first of which
still has the red border from having the focus, while the
second is the plot that I want.

         library(lattice); library(grid)
         plt <- xyplot(uptake ~ conc, groups=Plant, data=CO2)
         print(plt)
         trellis.focus("panel", row=1, column=1)
         arglist=trellis.panelArgs()
         arglist$type <- "l"
         do.call("panel.superpose", args=arglist)
         trellis.unfocus()

Should I be able to use panel.superpose() in this way?

The new abilities provided by trellis.focus() etc add
greatly to the flexibility of what can be done with lattice
plots.  The grid-lattice combination is a great piece of
software.

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Bioinformation Science, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.



From Allan at STATS.uct.ac.za  Mon Nov 29 11:48:02 2004
From: Allan at STATS.uct.ac.za (Clark Allan)
Date: Mon, 29 Nov 2004 12:48:02 +0200
Subject: [R] R: nnet questions
Message-ID: <41AAFE62.9B6D6470@STATS.uct.ac.za>

hi all

i'm new to the area of neural networks. i've been reading some
references and seem to understand some of the learning algorithms. i am
very familiar with regression and would just like to see how neural nets
handle this problem so i've been using the nnet package.

i simply want to use a 3 layer neural net, ie 1 input, 1 hidden layer
(where the hidden layer is linear, since i want to basically perform
regression analysis by means of neural nets) and 1 output layer.

the x and y vector was simulated as follows:
x<-1:100
x
  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16 
17  18
 [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34 
35  36
 [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52 
53  54
 [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70 
71  72
 [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88 
89  90
 [91]  91  92  93  94  95  96  97  98  99 100

y<-2+5*x+rnorm(100)*5

y
  [1]   8.789605  11.151109  14.622276  30.381379  19.328647  29.317038 
33.793720  39.557390  51.939294  45.045965
 [11]  58.783991  63.191745  72.882202  79.184778  85.034551  94.446000 
89.243004  88.223547 106.327683 104.424668
 [21] 103.057648 112.855778 111.777823 108.359485 128.956152 127.369102
128.784481 139.760279 151.959887 152.014623
 [31] 158.869586 167.030970 166.711160 177.415680 173.542293 182.484224
179.767128 192.284343 196.173830 202.353030
 [41] 220.449623 213.410307 216.746041 219.812526 230.440402 230.759429
239.311279 244.151390 248.637023 254.648298
 [51] 262.694237 253.619539 276.975714 280.395284 280.173787 286.813617
284.766870 296.705692 295.110064 304.709464
 [61] 305.650793 310.128992 314.035624 314.649213 322.958865 333.640203
342.538307 340.546359 342.433629 344.720633
 [71] 354.115051 363.631246 371.479886 367.066764 377.184512 386.634677
392.310577 386.151325 400.345393 408.831710
 [81] 413.999148 405.009358 418.679828 418.388427 419.282955 432.329471
433.448313 444.166060 447.773185 455.103503
 [91] 448.588598 464.410358 465.565875 478.677403 478.306390 479.565728
487.681689 491.422090 502.468491 500.385458


i then went about to use the nnet function:

> a.net<-nnet(y~x,linout=T,size=1)
# weights:  4
initial  value 8587786.130082 
iter  10 value 2086583.979646
final  value 2079743.529111 
converged

NOTE: the function said that four weights were estimated. This is
correct as shown below. the model can be represented as:

input   ---(w1)--->    hidden   ---(w2)--->   output

x       ---------->    a1+w1*x  ---------->   a2+w2*(a1+w1*x) 


where:
wi	are the weights, i=1,2
x	is an input pattern


further results were:

summary(a.net)
a 1-1-1 network with 4 weights
options were - linear output units 
  b->h1  i1->h1 
-276.48 -295.11 
   b->o   h1->o 
 254.92  764.72 


is the following statement correct? (i think that it is!)
a1=	"b->h1"
w1=	"i1->h1"
a2=	"b->o"
w2=	"h1->o"


If the hidden layer and the output layers are both LINEAR then the
following should be true:
1.	2= a2+a1*w2
2.	5= w1*w2

THIS IS NOT THE CASE, see the results. The only thing that i can think
of thats happening is that the activation function from the hidden layer
is not linear. Is this correct since i used the linout=T arguement? are
we able to change the activation function from the hidden layer?



two other questions:
1.	with regard to the size arguement, how does one know how many nodes
are in the hidden layer? (this might be a silly question.) e.g we 	might
have 2 hidden layers both with 3 nodes.
2.	are we able to plot the drop in the error function as a function of
the epochs?


hope someone can help

thanking you!!!

From HDoran at air.org  Mon Nov 29 12:00:29 2004
From: HDoran at air.org (Doran, Harold)
Date: Mon, 29 Nov 2004 06:00:29 -0500
Subject: [R] RE: Adding a line in the graph of  'plot()'
Message-ID: <88EAF3512A55DF46B06B1954AEF73F7404044DA0@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041129/2bac501f/attachment.pl

From vito_ricci at yahoo.com  Mon Nov 29 13:20:19 2004
From: vito_ricci at yahoo.com (Vito Ricci)
Date: Mon, 29 Nov 2004 13:20:19 +0100 (CET)
Subject: [R] Ts analysis with R: a contribute in Italian language
Message-ID: <20041129122019.36941.qmail@web41214.mail.yahoo.com>

Dear All,

I wish to inform, especially Italian speaking R-users,
that on CRAN web site is now available a contribute
(in Italian language) about using R in ts analysis.

Any comments would be appreciated.

Best regards,
Vito


=====
Diventare costruttori di soluzioni
Became solutions' constructors

"The business of the statistician is to catalyze 
the scientific learning process."  
George E. P. Box


Visitate il portale http://www.modugno.it/
e in particolare la sezione su Palese  http://www.modugno.it/archivio/palese/



From billthebrute at yahoo.fr  Mon Nov 29 13:47:35 2004
From: billthebrute at yahoo.fr (william ritchie)
Date: Mon, 29 Nov 2004 13:47:35 +0100 (CET)
Subject: [R] plot problem
Message-ID: <20041129124735.54892.qmail@web86910.mail.ukl.yahoo.com>

Dear all,

I am having trouble plotting a PCA result. The plot doesn't appear!!!
R goes through without any errors but doesn't make a plot appear!!
Could it be wrong window parameters? In this case how do I change them?
I am under red hat 9 with the latest version of R!

Thanks.


	

	
		
Vous manquez d??espace pour stocker vos mails ?



From michael.watson at bbsrc.ac.uk  Mon Nov 29 14:21:10 2004
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Mon, 29 Nov 2004 13:21:10 -0000
Subject: [R] Building latest version of package
Message-ID: <8975119BCD0AC5419D61A9CF1A923E95E898F9@iahce2knas1.iah.bbsrc.reserved>

Hi

I have a package which was built using R 1.9.1 and everything worked
fine.  I recently upgraded to R 2.0.1 and tried to re-install my package
- and I got:

Error in library(mypackage) : 'mypackage' is not a valid package --
installed < 2.0.0?

So I tried rebuilding it using my new version of R:

R CMD BUILD --binary mypackage

hhc: not found
cp: cannot stat `mypackage.chm': No such file or directory
make[1]: *** [chm-mypackage] Error 1
make: *** [pkg-mypackage] Error 2
*** Installation of mypackage failed ***

Removing 'f:/tmp/Rbuild.2972/mypackage'
 ERROR
* installation failed

I didn't have these problems before.  What is "hhc" and why can't R find
it?

In general, will I have to re-build my package everytime a new version
of R is released?

Many thanks

Mick



From stu44414 at mail.uni-kiel.de  Mon Nov 29 14:29:52 2004
From: stu44414 at mail.uni-kiel.de (Andreas Franke)
Date: Mon, 29 Nov 2004 14:29:52 +0100
Subject: [R] plotting data in non-orthogonal coords.
Message-ID: <200411291429.52432.stu44414@mail.uni-kiel.de>

Hi !
I am wondering how to plot data (e.g.  f(x,y) ) in a coordinate system spanned 
by two non-orthogonal basis vectors (e.g. hexagonal symmetry). The data is 
given on an equally spaced grid in theses coords and i would like to do a 
contour plot (e.g. with filled.contour).

Thanks for your help. Andreas



From stevehoughton1111 at hotmail.co.uk  Mon Nov 29 14:43:48 2004
From: stevehoughton1111 at hotmail.co.uk (steve houghton)
Date: Mon, 29 Nov 2004 13:43:48 +0000
Subject: [R] "non-visible" functions in return to methods()
Message-ID: <BAY23-F10C5C287BB981FC7D81074EBBD0@phx.gbl>

Please point me to the documentation explaining why some of the functions 
returned
by calling methods() are marked as "non-visible" and whether there is indeed 
no way of
viewing the R code of such functions

thanks

Steve

_________________________________________________________________



From rpeng at jhsph.edu  Mon Nov 29 14:48:04 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Mon, 29 Nov 2004 08:48:04 -0500
Subject: [R] "non-visible" functions in return to methods()
In-Reply-To: <BAY23-F10C5C287BB981FC7D81074EBBD0@phx.gbl>
References: <BAY23-F10C5C287BB981FC7D81074EBBD0@phx.gbl>
Message-ID: <41AB2894.8020005@jhsph.edu>

"non-visible" functions are hidden in a namespace.  You can view the 
code by using getS3method().

-roger

steve houghton wrote:
> Please point me to the documentation explaining why some of the 
> functions returned
> by calling methods() are marked as "non-visible" and whether there is 
> indeed no way of
> viewing the R code of such functions
> 
> thanks
> 
> Steve
> 
> _________________________________________________________________
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From anne.piotet at urbanet.ch  Mon Nov 29 14:51:26 2004
From: anne.piotet at urbanet.ch (anne.piotet@urbanet.ch)
Date: Mon, 29 Nov 2004 14:51:26 +0100
Subject: [R] problem with using transace
Message-ID: <200411291351.iATDpQik006820@smtp.hispeed.ch>

>I am trying to use the Hmisc function transace to transform predictors
>
> test<-cbind(flowstress,pressres,alloy)
> xtrans<-transace(x,binary=pressres',monotonic='flowstress', categorical='alloy')
>
>
>and I am getting the following message??
>Error in ace(x[, -i], x[, i], monotone = im, categorical = ic) : 
>	unused argument(s) (monotone ...)
>
>Any idea?

thanks anne
>thank for your help
>Anne
>
>



From andy_liaw at merck.com  Mon Nov 29 16:04:38 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 29 Nov 2004 10:04:38 -0500
Subject: Reasons not to answer very basic questions in a
	straightforwa rd way; was: Re: [R] creating a sequence of object names
Message-ID: <3A822319EB35174CA3714066D590DCD50994E396@usrymx25.merck.com>

I'd like to make just a couple of points:

R-help is considered by quite a few people to be "high-traffic".  As such,
many have low appetite for very basic questions.  (I wouldn't call them
"silly".)  In many cases such questions are answered by pointing to a
particular function help page or manual section.  In this particular case,
it's probably not at the very basic level that would be covered in "An
Introduction to R".  However, it _is_ a bona fide FAQ, thus the entry 7.21
in the R-FAQ.  Now, if there's ever one type of questions that many do not
like to see in a mailing list, it's one that can be found in the list FAQ,
as one of the main reasons for having the FAQ is to prevent such questions
from being asked over and over again on the list.

Now, if after reading the FAQ entry, you still can't solve the problem, then
you should tell us that, as well as how you tried and failed, so people have
a much better idea where you went off track, and are more likely to give you
more useful help.  This is in the Posting Guide, which suggest ways to ask
question that maximize the probability of getting useful replies.  Reading
that is to your own benefit, as well as others on the list.

As Duncan Murdoch said in a reply to a poster complaining (essentially about
being told to RTFM) on R-devel, we are not asking you to read these things
over and over again, nor on a periodic basis, but please do try to at least
take a glance before posting.  Posts that get less than enthusiastic
response are usually ones that showed that posters' unwillingness to do the
minimal work to help themselves, not because they are considered `dumb' or
`silly'.  In such cases people are much less willing to help.

Cheers,
Andy

> From: John
> 
> Dear Uwe,
> 
> I must say that I had thanked you for referring me to
> the "specific and exact" FAQ 7.21 and I had solved my
> simple problem from it. I alreadly had looked at some
> of basic materials like 'An Introduction to R', 'R for
> Beginners', 'R Data Import/Export as well as the
> FAQ(that is, I know how to use ?assign and ?get). But,
> because I am not going to be an expert in R I assume
> that I have missed something (even very trivial) in
> those documents. Of course, I can read them again and
> again until I know everything in them. That is for
> more interested enthusiasts, however. 
> 
> I know very well that it is basic manners to read
> those materials before asking questions here, but you
> should also understand that people sometimes get stuck
> with very simple problems if they are driven by stress
> or run down. They can save a lot of time and
> concentrate on and develop their primary jobs instead.
> And I don't think you should be worried about 900
> silly questions out of 1000 messages posted because
> they are at least well-educated people who know what
> reading basic materials before posting questions
> means.
> 
> People can learn diverse solutions about their simple
> questions, from advanced experienced users, that
> sometimes contain much more informations and tips.
> It is up to users(not necessarily advanced users)
> whether or not they are willing to answer questions
> and share their precious (even little) findings in
> programming. Volunteers can simply ignore silly
> questions if they are not appropriate for answering.
> Or I would let them know what to do with their
> improper questions in a personal email.
> 
> Finally, I do appreciate your answer again and other
> people's active replies too. It was really useful to
> point me to the specific FAQ rather than to just say
> 'look at the FAQ'. It simply occurred to my mind that
> "kindness" is the best policy for good education.
> 
> I beg your pardon if this message is not relevant to
> this help list.
> 
> With kind regards,
> 
> John
> 
> 
>  --- Uwe Ligges <ligges at statistik.uni-dortmund.de>
> wrote: 
> > John wrote:
> > > Thank you, Uwe. I've found a way to do the job by
> > > reading the FAQ 7.21 although it is not giving a
> > > precise explanation to a novice or casual user at
> > > first reading. For example, if you type the first
> > two
> > 
> > But the corresponding help files do so, for sure,
> > and the FAQ 7.21 
> > points you to ?assign and ?get.
> > 
> > 
> > > lines in the FAQ, you get an error as you do not
> > have
> > > the variable, a, initially.
> > >
> > > I am sure that more and more people get interested
> > in
> > > and serious about using R if advanced users are
> > kind
> > > enough to answer simple and silly questions as
> > well
> > > which are already explained in basic
> > documentations.
> > > Or is this community for highly motivated and
> > advanced
> > > R users only?
> > 
> > No, of course it is for novices as well!
> > 
> > BUT we do expect that novices do read basic
> > documentation such as "An 
> > Introduction to R" and the R FAQ before asking
> > question.
> > If there are too many silly questions from thousands
> > of R users, nobody 
> > is able to manage the questions any more. And note
> > that those people 
> > answering questions do it on a voluntary basis, and
> > (at least partially) 
> > in their spare time!
> > Nobody would be subscribed to R-help any more, if
> > there are 1000 mails a 
> > day, 900 of them containing silly questions! It is
> > yet already hard 
> > enough to get through the huge amount of messages in
> > a reasonable amount 
> > of time!
> > 
> > 
> > I have answered your question in a way,
> > 
> >   1) so that it is up to you to read some
> > documentation. Now you have 
> > seen the FAQs and some help files. And you have
> > learned much more than 
> > you would have learned if I had said "Use assign()"
> > 
> >   2) so that nobody feels too encouraged to ask
> > questions before reading 
> > basic documentation - and my answer still saved you
> > a lot of time!
> > 
> > Uwe Ligges
> > 
> > 
> > 
> > 
> > > Regards,
> > > 
> > > John
> > > 
> > > 
> > >  --- Uwe Ligges <ligges at statistik.uni-dortmund.de>
> > > wrote: 
> > > 
> > >>John wrote:
> > >>
> > >>
> > >>>Hello R-users,
> > >>>
> > >>>I wanted to generate objects named 'my.ftn1',
> > >>>'my.ftn2', ... , 'my.ftn10', and tried the
> > >>
> > >>following
> > >>
> > >>>code without success. How can I do this?
> > >>>
> > >>>
> > >>>
> > >>>>for ( i in 1:10 ) {
> > >>>
> > >>>+ sub(" ", "", paste("my.ftn", i)) <- NULL
> > >>>+ }
> > >>>Error: Target of assignment expands to
> > >>
> > >>non-language
> > >>
> > >>>object
> > >>>
> > >>>
> > >>>Many thanks.
> > >>>
> > >>>John
> > >>>
> > >>>______________________________________________
> > >>>R-help at stat.math.ethz.ch mailing list
> > >>>https://stat.ethz.ch/mailman/listinfo/r-help
> > >>>PLEASE do read the posting guide!
> > >>
> > >>http://www.R-project.org/posting-guide.html
> > >>
> > >>
> > >>Please do as suggested above, read the posting
> > >>guide!
> > >>It suggests to read the FAQs. FAQ 7.21 is what you
> > >>are looking for: "How 
> > >>can I turn a string into a variable?".
> > >>
> > >>Uwe Ligges
> > >>
> > > 
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From deepayan at stat.wisc.edu  Mon Nov 29 16:21:34 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Mon, 29 Nov 2004 09:21:34 -0600
Subject: [R] Call to trellis.focus(); thenpanel.superpose()
In-Reply-To: <DC12507A-41F3-11D9-92B8-000A95CDA0F2@anu.edu.au>
References: <DC12507A-41F3-11D9-92B8-000A95CDA0F2@anu.edu.au>
Message-ID: <200411290921.34595.deepayan@stat.wisc.edu>

On Monday 29 November 2004 04:46, John Maindonald wrote:
> The following works fine with the x11 device, though it
> may well be that an initial plot is overwritten.  With a pdf
> or postscript device, I get two plots, the first of which
> still has the red border from having the focus, while the
> second is the plot that I want.
>
>          library(lattice); library(grid)
>          plt <- xyplot(uptake ~ conc, groups=Plant, data=CO2)
>          print(plt)
>          trellis.focus("panel", row=1, column=1)
>          arglist=trellis.panelArgs()
>          arglist$type <- "l"
>          do.call("panel.superpose", args=arglist)
>          trellis.unfocus()
>
> Should I be able to use panel.superpose() in this way?

Yes. The red border should be 'removed', but that's done by grid.remove 
and maybe it doesn't work on PDF devices.

The solution is to use 

trellis.focus("panel", row=1, column=1, highlight = FALSE)

(which happens automatically for non-interactive sessions).

> The new abilities provided by trellis.focus() etc add
> greatly to the flexibility of what can be done with lattice
> plots.  The grid-lattice combination is a great piece of
> software.

Yes, it can be especially useful for tasks similar to identify(). 
Everything else can probably be done by clever enough use of the panel 
function (though perhaps not as naturally).

Deepayan



From andy_liaw at merck.com  Mon Nov 29 16:19:21 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 29 Nov 2004 10:19:21 -0500
Subject: [R] "non-visible" functions in return to methods()
Message-ID: <3A822319EB35174CA3714066D590DCD50994E397@usrymx25.merck.com>

You mean something like ?methods, which says:

Value:

     An object of class '"MethodsFunction"', a character vector of
     function names with an '"info"' attribute. There is a 'print'
     method which marks with an asterisk any methods which are not
     visible: such functions can be examined by 'getS3method' or
     'getAnywhere'.

??

Andy

> From: steve houghton
> 
> Please point me to the documentation explaining why some of 
> the functions 
> returned
> by calling methods() are marked as "non-visible" and whether 
> there is indeed 
> no way of
> viewing the R code of such functions
> 
> thanks
> 
> Steve
> 
> _________________________________________________________________

> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From blindglobe at gmail.com  Mon Nov 29 16:29:47 2004
From: blindglobe at gmail.com (A.J. Rossini)
Date: Mon, 29 Nov 2004 16:29:47 +0100
Subject: Reasons not to answer very basic questions in a straightforwa rd
	way; was: Re: [R] creating a sequence of object names
In-Reply-To: <3A822319EB35174CA3714066D590DCD50994E396@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD50994E396@usrymx25.merck.com>
Message-ID: <1abe3fa9041129072971f8ee4f@mail.gmail.com>

<lots of good points from Andy and Uwe deleted>

and perhaps the most important reason for the particular socratic form
of teaching on this list are the number of to-be, current, and former
faculty members who feel compelled to teach general solutions to the
problems (reading the FAQ is a rather general solution, eh?) rather
than spoonfeed answers.  This is a bit unlike some of the program
language mailing lists where the care and feeding of personal egos is
also part of the scene.  (of course, I'm not referring to the wizards
lists...!).

best,
-tony

---
A.J. Rossini
blindglobe at gmail.com



From maechler at stat.math.ethz.ch  Mon Nov 29 16:30:04 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 29 Nov 2004 16:30:04 +0100
Subject: [R] "non-visible" functions in return to methods()
In-Reply-To: <41AB2894.8020005@jhsph.edu>
References: <BAY23-F10C5C287BB981FC7D81074EBBD0@phx.gbl>
	<41AB2894.8020005@jhsph.edu>
Message-ID: <16811.16508.348499.357382@gargle.gargle.HOWL>

>>>>> "Roger" == Roger D Peng <rpeng at jhsph.edu>
>>>>>     on Mon, 29 Nov 2004 08:48:04 -0500 writes:

    Roger> "non-visible" functions are hidden in a namespace.  
yes.

    Roger> You can view the  code by using getS3method().

not always {namely when the hidden function is not an S3 method}

getAnywhere() is more generally useful, but please consider my
post from half a year ago
   
   https://stat.ethz.ch/pipermail/r-help/2004-May/050112.html

which gives more explanations and possibilities.

    Roger> steve houghton wrote:
    >> Please point me to the documentation explaining why some of the 
    >> functions returned
    >> by calling methods() are marked as "non-visible" and whether there is 
    >> indeed no way of
    >> viewing the R code of such functions
    >> 
    >> thanks
    >> 
    >> Steve



From contact at thomasalmer.com  Mon Nov 29 16:42:01 2004
From: contact at thomasalmer.com (contact@thomasalmer.com)
Date: Mon, 29 Nov 2004 16:42:01 +0100
Subject: [R] systemfit - SUR
Message-ID: <27129039$110174209141ab400be54d90.69596918@config17.schlund.de>


Hello to everyone,

I have 2 problems and would be very pleased if anyone can help me:

1) When I use the package "systemfit" for SUR regressions, I get two
different variance-covariance matrices when I firstly do the SUR
regression ("The covariance matrix of the residuals used for
estimation") and secondly do the OLS regressions. In the manual for
"systemfit" on page 14 I see however, that the variance-covariance
matrix for SUR is obtained from OLS. How can this be explained?

2) Is there an easy possibility to test a) the OLS equations, and b) the
SUR system for SUR structures? In other words: Is the LM-Test from
Breusch and Pagan available in R?

Thanks for the attention!

Best Regards,
Thomas Almer



From ahenningsen at email.uni-kiel.de  Mon Nov 29 17:02:12 2004
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Mon, 29 Nov 2004 17:02:12 +0100
Subject: [R] systemfit - SUR
In-Reply-To: <27129039$110174209141ab400be54d90.69596918@config17.schlund.de>
References: <27129039$110174209141ab400be54d90.69596918@config17.schlund.de>
Message-ID: <200411291702.12416.ahenningsen@email.uni-kiel.de>

On Monday 29 November 2004 16:42, contact at thomasalmer.com wrote:
> Hello to everyone,
>
> I have 2 problems and would be very pleased if anyone can help me:
>
> 1) When I use the package "systemfit" for SUR regressions, I get two
> different variance-covariance matrices when I firstly do the SUR
> regression ("The covariance matrix of the residuals used for
> estimation") and secondly do the OLS regressions. In the manual for
> "systemfit" on page 14 I see however, that the variance-covariance
> matrix for SUR is obtained from OLS. How can this be explained?

Hi Thomas,
I get identical residual covariance matrices:

R> library(systemfit)
R> data( kmenta )
R> demand <- q ~ p + d
R> supply <- q ~ p + f + a
R> labels <- list( "demand", "supply" )
R> system <- list( demand, supply )
R> fitols <- systemfit("OLS", system, labels, data=kmenta )
R> fitols$rcov
         [,1]     [,2]
[1,] 3.725391 4.136963
[2,] 4.136963 5.784441
R> fitsur <- systemfit("SUR", system, labels, data=kmenta )
R> fitsur$rcovest
         [,1]     [,2]
[1,] 3.725391 4.136963
[2,] 4.136963 5.784441

Did you do _iterated_ SUR?

Best wishes,
Arne

> 2) Is there an easy possibility to test a) the OLS equations, and b) the
> SUR system for SUR structures? In other words: Is the LM-Test from
> Breusch and Pagan available in R?
>
> Thanks for the attention!
>
> Best Regards,
> Thomas Almer
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From tati.fernandes at infolink.com.br  Mon Nov 29 17:19:38 2004
From: tati.fernandes at infolink.com.br (Tatiana Fernandes)
Date: Mon, 29 Nov 2004 14:19:38 -0200
Subject: [R] Citation
Message-ID: <000801c4d62f$3dc2cc80$6fd2fea9@laptop2>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041129/4c30cbcb/attachment.pl

From ahenningsen at email.uni-kiel.de  Mon Nov 29 17:20:21 2004
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Mon, 29 Nov 2004 17:20:21 +0100
Subject: [R] plot problem
In-Reply-To: <20041129124735.54892.qmail@web86910.mail.ukl.yahoo.com>
References: <20041129124735.54892.qmail@web86910.mail.ukl.yahoo.com>
Message-ID: <200411291720.21239.ahenningsen@email.uni-kiel.de>

Hi William,

the 1st example given in ?screeplot works for me (R 2.0.0 on SuSE Linux 9.0):
> (pc.cr <- princomp(USArrests, cor = TRUE))  # inappropriate
> screeplot(pc.cr)
plot appears

To help you we need more details. 
Does normal plotting work? e.g.: plot(rnorm(20),rnorm(20))
Can you plot to a file? (see ?ps, ?pdf or ?png and don't forget dev.off())

Arne

On Monday 29 November 2004 13:47, william ritchie wrote:
> Dear all,
>
> I am having trouble plotting a PCA result. The plot doesn't appear!!!
> R goes through without any errors but doesn't make a plot appear!!
> Could it be wrong window parameters? In this case how do I change them?
> I am under red hat 9 with the latest version of R!
>
> Thanks.
>
> Vous manquez d??espace pour stocker vos mails ?
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From abunn at whrc.org  Mon Nov 29 17:23:45 2004
From: abunn at whrc.org (Andy Bunn)
Date: Mon, 29 Nov 2004 11:23:45 -0500
Subject: [R] Citation
In-Reply-To: <000801c4d62f$3dc2cc80$6fd2fea9@laptop2>
Message-ID: <NEBBIPHDAMMOKDKPOFFIOENACMAA.abunn@whrc.org>

See the function ?citation under 2.0.0

R > citation()

To cite R in publications use:

  R Development Core Team (2004). R: A language and environment for
  statistical computing. R Foundation for Statistical Computing,
  Vienna, Austria. ISBN 3-900051-07-0, URL http://www.R-project.org.
....
HTH, Andy


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Tatiana Fernandes
> Sent: Monday, November 29, 2004 11:20 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] Citation
>
>
> Hello!
>
> I would like to know how do I citate R?
> I have used it during my Master thesis but I don??t know how to citate
> during the text and on the references.
> I??ve looked for it on the web page but only found how to citate the FAQ.
>
> Thank you in advance.
>
> Tatiana Fernandes
> Universidade Estadual do Norte Fluminense
> Laborat??rio de Ci??ncias Ambientais - CBB
> Av. Alberto Lamego, 2.000 - Campos dos Goytacazes - RJ
> 28013-600 - BRASIL
> Tel: +55 (22) 2726-1469
> Fax: +55 (22) 2726-1472
> e-mail:  <mailto:tati.fernandes at infolink.com.br>
> tati.fernandes at infolink.com.br
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ccleland at optonline.net  Mon Nov 29 17:24:30 2004
From: ccleland at optonline.net (Chuck Cleland)
Date: Mon, 29 Nov 2004 11:24:30 -0500
Subject: [R] Citation
In-Reply-To: <000801c4d62f$3dc2cc80$6fd2fea9@laptop2>
References: <000801c4d62f$3dc2cc80$6fd2fea9@laptop2>
Message-ID: <41AB4D3E.2020508@optonline.net>

   Did you see item 2.8 in the R FAQ?

http://cran.r-project.org/doc/FAQ/R-FAQ.html#Citing-R

Tatiana Fernandes wrote:
> Hello!
>  
> I would like to know how do I citate R?
> I have used it during my Master thesis but I don??t know how to citate
> during the text and on the references.
> I??ve looked for it on the web page but only found how to citate the FAQ.
>  
> Thank you in advance.
>  
> Tatiana Fernandes
> Universidade Estadual do Norte Fluminense
> Laborat??rio de Ci??ncias Ambientais - CBB
> Av. Alberto Lamego, 2.000 - Campos dos Goytacazes - RJ
> 28013-600 - BRASIL
> Tel: +55 (22) 2726-1469
> Fax: +55 (22) 2726-1472
> e-mail:  <mailto:tati.fernandes at infolink.com.br>
> tati.fernandes at infolink.com.br
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From ligges at statistik.uni-dortmund.de  Mon Nov 29 17:28:00 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 29 Nov 2004 17:28:00 +0100
Subject: [R] Citation
In-Reply-To: <000801c4d62f$3dc2cc80$6fd2fea9@laptop2>
References: <000801c4d62f$3dc2cc80$6fd2fea9@laptop2>
Message-ID: <41AB4E10.8000809@statistik.uni-dortmund.de>

Tatiana Fernandes wrote:

> Hello!
>  
> I would like to know how do I citate R?
> I have used it during my Master thesis but I don??t know how to citate
> during the text and on the references.
> I??ve looked for it on the web page but only found how to citate the FAQ.

Please the FAQ more carefully and find how it is cited therein. Or just type
   citation()
at the prompt.

Uwe Ligges




> Thank you in advance.
>  
> Tatiana Fernandes
> Universidade Estadual do Norte Fluminense
> Laborat??rio de Ci??ncias Ambientais - CBB
> Av. Alberto Lamego, 2.000 - Campos dos Goytacazes - RJ
> 28013-600 - BRASIL
> Tel: +55 (22) 2726-1469
> Fax: +55 (22) 2726-1472
> e-mail:  <mailto:tati.fernandes at infolink.com.br>
> tati.fernandes at infolink.com.br
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From susana.barbosa at fc.up.pt  Mon Nov 29 17:29:08 2004
From: susana.barbosa at fc.up.pt (susana barbosa)
Date: Mon, 29 Nov 2004 16:29:08 +0000
Subject: [R] Citation
In-Reply-To: <000801c4d62f$3dc2cc80$6fd2fea9@laptop2>
References: <000801c4d62f$3dc2cc80$6fd2fea9@laptop2>
Message-ID: <200411291629.09115.susana.barbosa@fc.up.pt>


citation()

> Hello!
>
> I would like to know how do I citate R?
> I have used it during my Master thesis but I don?t know how to citate
> during the text and on the references.
> I?ve looked for it on the web page but only found how to citate the FAQ.
>
> Thank you in advance.
>
> Tatiana Fernandes
> Universidade Estadual do Norte Fluminense
> Laborat?rio de Ci?ncias Ambientais - CBB
> Av. Alberto Lamego, 2.000 - Campos dos Goytacazes - RJ
> 28013-600 - BRASIL
> Tel: +55 (22) 2726-1469
> Fax: +55 (22) 2726-1472
> e-mail:  <mailto:tati.fernandes at infolink.com.br>
> tati.fernandes at infolink.com.br
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Mon Nov 29 17:31:27 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 29 Nov 2004 17:31:27 +0100
Subject: [R] Building latest version of package
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E95E898F9@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E95E898F9@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <41AB4EDF.4070602@statistik.uni-dortmund.de>

michael watson (IAH-C) wrote:

> Hi
> 
> I have a package which was built using R 1.9.1 and everything worked
> fine.  I recently upgraded to R 2.0.1 and tried to re-install my package
> - and I got:
> 
> Error in library(mypackage) : 'mypackage' is not a valid package --
> installed < 2.0.0?
> 
> So I tried rebuilding it using my new version of R:
> 
> R CMD BUILD --binary mypackage
> 
> hhc: not found
> cp: cannot stat `mypackage.chm': No such file or directory
> make[1]: *** [chm-mypackage] Error 1
> make: *** [pkg-mypackage] Error 2
> *** Installation of mypackage failed ***
> 
> Removing 'f:/tmp/Rbuild.2972/mypackage'
>  ERROR
> * installation failed
> 
> I didn't have these problems before.  What is "hhc" and why can't R find
> it?

hhc is Microsoft's compiled html help compiler. Either edit your MkRules 
file not to generate chm files, or download and install the required 
software as mentioned in file readme.packages.


> In general, will I have to re-build my package everytime a new version
> of R is released?

Well, not always, but yes, if considerable changes have taken place, as 
it has happend for the change from 1.y.z to 2.0.0.

Uwe Ligges




> Many thanks
> 
> Mick
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Mon Nov 29 17:31:16 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 29 Nov 2004 17:31:16 +0100
Subject: [R] Citation
In-Reply-To: <000801c4d62f$3dc2cc80$6fd2fea9@laptop2>
References: <000801c4d62f$3dc2cc80$6fd2fea9@laptop2>
Message-ID: <x2ekick46j.fsf@biostat.ku.dk>

"Tatiana Fernandes" <tati.fernandes at infolink.com.br> writes:

> Hello!
>  
> I would like to know how do I citate R?
> I have used it during my Master thesis but I don??t know how to citate
> during the text and on the references.
> I??ve looked for it on the web page but only found how to citate the FAQ.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R in publications.
^^^^^^^^^^^^

...and if you don't see that when you start R, you need to upgrade!

It is in the FAQ too (Q2.8).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From andy_liaw at merck.com  Mon Nov 29 17:33:31 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 29 Nov 2004 11:33:31 -0500
Subject: [R] Citation
Message-ID: <3A822319EB35174CA3714066D590DCD50994E399@usrymx25.merck.com>

> From: Chuck Cleland
> 
>    Did you see item 2.8 in the R FAQ?
> 
> http://cran.r-project.org/doc/FAQ/R-FAQ.html#Citing-R

Or the start-up message, which has, in part:

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Note that last line!

Andy

 
> Tatiana Fernandes wrote:
> > Hello!
> >  
> > I would like to know how do I citate R?
> > I have used it during my Master thesis but I don't know how 
> to citate
> > during the text and on the references.
> > I've looked for it on the web page but only found how to 
> citate the FAQ.
> >  
> > Thank you in advance.
> >  
> > Tatiana Fernandes
> > Universidade Estadual do Norte Fluminense
> > Laborat??rio de Ci??ncias Ambientais - CBB
> > Av. Alberto Lamego, 2.000 - Campos dos Goytacazes - RJ
> > 28013-600 - BRASIL
> > Tel: +55 (22) 2726-1469
> > Fax: +55 (22) 2726-1472
> > e-mail:  <mailto:tati.fernandes at infolink.com.br>
> > tati.fernandes at infolink.com.br
> >  
> > 
> > 	[[alternative HTML version deleted]]
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> > 
> 
> -- 
> Chuck 
> Cleland, Ph.D.
> NDRI, Inc.
> 71 West 23rd Street, 8th floor
> New York, NY 10010
> tel: (212) 845-4495 (Tu, Th)
> tel: (732) 452-1424 (M, W, F)
> fax: (917) 438-0894
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From tiago17 at socrates.Berkeley.EDU  Mon Nov 29 17:41:30 2004
From: tiago17 at socrates.Berkeley.EDU (Tiago R Magalhaes)
Date: Mon, 29 Nov 2004 16:41:30 +0000
Subject: [R] scan; 1 items
Message-ID: <p06100500bdd10116e3ab@[83.132.29.55]>

HI

the scan function when only one item is read says:

scan()
1: 3.3
2:
Read 1 items
[1] 3.3

I hope my english is not playing a trick on me, but "1 items" sounds 
very strange
this makes me feel very anal, and it's really not important and I 
apologize for it but here it goes anyway



From tlumley at u.washington.edu  Mon Nov 29 17:53:04 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 29 Nov 2004 08:53:04 -0800 (PST)
Subject: [R] Error using glm with poisson family and identity link
In-Reply-To: <x2u0rdac3k.fsf@biostat.ku.dk>
References: <41A5D781.70400@pigrecodata.net>
	<Pine.LNX.4.61.0411251535190.28898@gannet.stats>
	<41A614F3.4000102@pdf.com> <x2y8gpai20.fsf@biostat.ku.dk>
	<41A633FC.5010401@pdf.com> <x2u0rdac3k.fsf@biostat.ku.dk>
Message-ID: <Pine.A41.4.61b.0411290840400.88302@homer09.u.washington.edu>

On Thu, 25 Nov 2004, Peter Dalgaard wrote:

>
> I haven't got all that much experience with it, but obviously, the
> various algorithms for constrained optimization (box- or otherwise) at
> least allow you to find a proper maximum likelihood estimator.
>
>

It's harder than it looks (well, my experience is with the log-binomial 
model, but it should be similar).  The constraints are not box 
constraints, but a more general set of linear constraints, and you either 
have to find the convex hull of the data or use a constraint for every 
data point.

Also, the algorithm in glm.fit, while not perfect, is a little smarter 
than a simple IRLS. It uses step-halving to back away from the edge, and 
when the parameter space is convex it has a reasonable chance of creeping 
along the boundary to the true MLE.

I think better glm fitting is worth pursuing: computational difficulties 
with the log-binomial model have forced many epidemiologists turn to 
estimators other than the MLE (or contrasts other than the relative risk), 
which is a pity.


 	-thomas



From gregoire.thomas at ugent.be  Mon Nov 29 18:00:54 2004
From: gregoire.thomas at ugent.be (Gregoire Thomas)
Date: Mon, 29 Nov 2004 18:00:54 +0100
Subject: [R] PlotML
In-Reply-To: <41AB52AC.3000904@ugent.be>
References: <41AB52AC.3000904@ugent.be>
Message-ID: <41AB55C6.8020305@ugent.be>



> > Has anybody ever written some plot / hist functions that would return
> > PlotML code? [http://ptolemy.eecs.berkeley.edu/] 
> <http://ptolemy.eecs.berkeley.edu/%5D>
>
> Not that I know of, and looking at the design of PlotML, it doesn't
> look like a nice fit as an R device driver. PlotML works at the level
> of data sets, bar graphs, axes, etc., not with low-level items like
> lines, polygons, and text. There are general XML handling tools in the
> XML package, though.

Thx. I've wrapped up a couple of functions into a package that does what 
I need. In case somebody is interested I've put it with an example at: 
http://penyfan.ugent.be/ptplot/ . Note that you might also need to 
install R2HTML.
Regards, Gregoire


From p.dalgaard at biostat.ku.dk  Mon Nov 29 18:07:40 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 29 Nov 2004 18:07:40 +0100
Subject: [R] Error using glm with poisson family and identity link
In-Reply-To: <Pine.A41.4.61b.0411290840400.88302@homer09.u.washington.edu>
References: <41A5D781.70400@pigrecodata.net>
	<Pine.LNX.4.61.0411251535190.28898@gannet.stats>
	<41A614F3.4000102@pdf.com> <x2y8gpai20.fsf@biostat.ku.dk>
	<41A633FC.5010401@pdf.com> <x2u0rdac3k.fsf@biostat.ku.dk>
	<Pine.A41.4.61b.0411290840400.88302@homer09.u.washington.edu>
Message-ID: <x28y8kk2hv.fsf@biostat.ku.dk>

Thomas Lumley <tlumley at u.washington.edu> writes:

> Also, the algorithm in glm.fit, while not perfect, is a little smarter
> than a simple IRLS. It uses step-halving to back away from the edge,
> and when the parameter space is convex it has a reasonable chance of
> creeping along the boundary to the true MLE.

Hmm. That wasn't my experience. I had a situation where there was like
a (virtual) maximum outside the boundary, and the algorithm would
basically stay on the path to that peak, banging its little head
into the same point of the wall repeatedly, so to speak.

(If you make a little drawing of an elliptical contour intersecting a
linear boundary, I'm sure you'll see that this process leads to a
point-of-no-progress that can be quite far from the maximum along the
edge.)

 
> I think better glm fitting is worth pursuing: computational
> difficulties with the log-binomial model have forced many
> epidemiologists turn to estimators other than the MLE (or contrasts
> other than the relative risk), which is a pity.

Yup...

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From tlumley at u.washington.edu  Mon Nov 29 18:11:14 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 29 Nov 2004 09:11:14 -0800 (PST)
Subject: [R] =?iso-8859-1?q?Question_d=27un_d=E9butant?=
In-Reply-To: <001101c4d53e$30a9cea0$0801a8c0@LDX20041022>
References: <001101c4d53e$30a9cea0$0801a8c0@LDX20041022>
Message-ID: <Pine.A41.4.61b.0411290910250.88302@homer09.u.washington.edu>

On Sun, 28 Nov 2004, HOME - Didier Ledoux wrote:

> Je suis dbutant en R. Je voudrais faire un tableau de statistiques descriptives ou les moyennes seraient calcules en fonction de deux critres: sexe, rgion
> Cela donnerait ceci:
> SEXE    REGION    MOYENNE
> femme    A                moy1
> femme    B                moy2
> Homme    A              moy3
> Homme    B              moy4
>
> Comment dois-je faire?
> Quelles commandes utiliser?
>

?by

 	-thomas

From danlipsitt at gmail.com  Mon Nov 29 18:20:02 2004
From: danlipsitt at gmail.com (Dan Lipsitt)
Date: Mon, 29 Nov 2004 12:20:02 -0500
Subject: [R] escaping backslash in a string
Message-ID: <b3a7efa904112909201373cd6@mail.gmail.com>

How can I get a single backslash in a character string?

My goal is to escape dots in a string that will be used as a regular
expression. I thought I could do it this way:

gsub(".", "\\.", x)

Unfortunately, "\\" does not represent a literal backslash as I
expected, but rather a pair of backslashes:

> "\\."
[1] "\\."
> "\\"
[1] "\\"

Just a backslash and a dot fails too, since that represents an escaped dot:

> "\."
[1] "."

A single backslash works in the middle of strings sometimes,but it
depends on what the character following it is (presumably depending on
whether the pair of characters represents an escape sequence):

> "a\b"
[1] "a\b"
> "x\y"
[1] "xy"

Is there a way to represent "\"? This seems like a design problem in
the interpreter.

> R.version
         _                    
platform i386-redhat-linux-gnu
arch     i386                 
os       linux-gnu            
system   i386, linux-gnu      
status                        
major    2                    
minor    0.1                  
year     2004                 
month    11                   
day      15                   
language R



From tlumley at u.washington.edu  Mon Nov 29 18:29:25 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 29 Nov 2004 09:29:25 -0800 (PST)
Subject: [R] "non-visible" functions in return to methods()
In-Reply-To: <BAY23-F10C5C287BB981FC7D81074EBBD0@phx.gbl>
References: <BAY23-F10C5C287BB981FC7D81074EBBD0@phx.gbl>
Message-ID: <Pine.A41.4.61b.0411290928480.88302@homer09.u.washington.edu>

On Mon, 29 Nov 2004, steve houghton wrote:

> Please point me to the documentation explaining why some of the functions 
> returned
> by calling methods() are marked as "non-visible" and whether there is indeed 
> no way of
> viewing the R code of such functions
>

Luke Tierney's article in Volume 3 No 1 of the R Newsletter explains why. 
How to get the functions is FAQ 7.26.


 	-thomas



From jfox at al.noaa.gov  Mon Nov 29 18:31:38 2004
From: jfox at al.noaa.gov (Jenny Fox)
Date: Mon, 29 Nov 2004 10:31:38 -0700
Subject: [R] library(fields) world shift function not working anymore
Message-ID: <8659F1CA-422C-11D9-975A-00039366A72C@al.noaa.gov>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041129/776fcc5c/attachment.pl

From p.dalgaard at biostat.ku.dk  Mon Nov 29 18:35:34 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 29 Nov 2004 18:35:34 +0100
Subject: [R] escaping backslash in a string
In-Reply-To: <b3a7efa904112909201373cd6@mail.gmail.com>
References: <b3a7efa904112909201373cd6@mail.gmail.com>
Message-ID: <x24qj8k17d.fsf@biostat.ku.dk>

Dan Lipsitt <danlipsitt at gmail.com> writes:

> How can I get a single backslash in a character string?
> 
> My goal is to escape dots in a string that will be used as a regular
> expression. I thought I could do it this way:
> 
> gsub(".", "\\.", x)
> 
> Unfortunately, "\\" does not represent a literal backslash as I
> expected, but rather a pair of backslashes:
> 
> > "\\."
> [1] "\\."
> > "\\"
> [1] "\\"

Nononononono.... If you want to know what is inside a string, use
cat() not (implicitly) print()

> cat( "\\.")
\.>

The thing is that print() itself escapes "weird" characters, including
the escape character:

> x <- readLines() # ctr-D terminates (on Linux anyway)
\.
> x
[1] "\\."


> Is there a way to represent "\"? This seems like a design problem in
> the interpreter.

Yes. Not at all.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From f.harrell at vanderbilt.edu  Mon Nov 29 18:57:08 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Mon, 29 Nov 2004 12:57:08 -0500
Subject: [R] problem with using transace
In-Reply-To: <200411291351.iATDpQik006820@smtp.hispeed.ch>
References: <200411291351.iATDpQik006820@smtp.hispeed.ch>
Message-ID: <41AB62F4.701@vanderbilt.edu>

anne.piotet at urbanet.ch wrote:
>>I am trying to use the Hmisc function transace to transform predictors
>>
>>test<-cbind(flowstress,pressres,alloy)
>>xtrans<-transace(x,binary=pressres',monotonic='flowstress', categorical='alloy')
>>
>>
>>and I am getting the following message??
>>Error in ace(x[, -i], x[, i], monotone = im, categorical = ic) : 
>>	unused argument(s) (monotone ...)
>>
>>Any idea?
> 
> 
> thanks anne
> 
>>thank for your help
>>Anne

The corrected version (below) will fix that problem but note that there 
is a bug in ace causing it not to allow a monotonicity constraint when a 
variable is on the left hand side.  This is inconsistent with the ace 
documentation.  There are other problems in ace in which it checks 
column numbers against the number of rows in the x matrix instead of the 
number of columns.  The internal version of ace defined inside areg.boot 
fixes the latter problem.  Note that I reported these problems a long 
time ago.

Frank

transace <- function(x, monotonic=NULL, categorical=NULL, binary=NULL, 
pl=TRUE) {
   if(.R.) require('acepack')  # provides ace, avas

nam <- dimnames(x)[[2]]
omit <- is.na(x %*% rep(1,ncol(x)))
omitted <- (1:nrow(x))[omit]
if(length(omitted)) x <- x[!omit,]
p <- ncol(x)
xt <- x  # binary variables retain original coding
if(!length(nam)) stop("x must have column names")
rsq <- rep(NA, p)
names(rsq) <- nam


for(i in (1:p)[!(nam %in% binary)])	{
   lab <- nam[-i]
   w <- 1:(p-1)
   im <- w[lab %in% monotonic]
   ic <- w[lab %in% categorical]
   if(nam[i] %in% monotonic) im <- c(0, im)
   if(nam[i] %in% categorical) ic <- c(0, ic)
   m <- 10*(length(im)>0)+(length(ic)>0)
   if(m==11) a <- ace(x[,-i], x[,i], mon=im, cat=ic)
   else if (m==10) a <- ace(x[,-i], x[,i], mon=im)
   else if(m==1) a <- ace(x[,-i], x[,i], cat=ic)
   else a <- ace(x[,-i], x[,i])
   xt[,i] <- a$ty
   rsq[i] <- a$rsq
   if(pl)plot(x[,i], xt[,i], xlab=nam[i], ylab=paste("Transformed",nam[i]))
}	

cat("R-squared achieved in predicting each variable:\n\n")
print(rsq)

attr(xt, "rsq") <- rsq
attr(xt, "omitted") <- omitted
invisible(xt)
}

>>
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From GBLEVINS at marketsolutionsgroup.com  Mon Nov 29 19:09:13 2004
From: GBLEVINS at marketsolutionsgroup.com (Greg Blevins)
Date: Mon, 29 Nov 2004 12:09:13 -0600
Subject: [R] Seeking help with a simple loop construction
Message-ID: <s1ab1177.040@mail.marketsolutionsgroup.com>

Hello,

I have a df, pp, with five variables:

> nobs(pp)
  q10_1   q10_2   q10_3   q10_4 actcode 
   1620    1620    1620    1620    1620 

I want to create a loop to run four xtabs (the first four variables above by the fifth) and then store the results in a matrix.  Below I make my intent clear by showing the output of one xtab which is inserted into a matrix.

> a <- xtabs(q10_1 ~ actcode)
> a
actcode
 1  2  3  4  5  6  7  8  9 10 
 7 11  3 60 66 56 21 40  7  8 

> freq.mat <- matrix(0, 4, 10, byrow = TRUE)
> freq.mat[1,] <- a
> freq.mat
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
[1,]    7   11    3   60   66   56   21   40    7     8
[2,]    0    0    0    0    0    0    0    0    0     0
[3,]    0    0    0    0    0    0    0    0    0     0
[4,]    0    0    0    0    0    0    0    0    0     0
===============================================================
I have spent a couple of hours searching the web and my texts but continue to strike out in my attempts to construct a correct formulation of this simple loop. Help would be appreciated.

Greg Blevins
The Market Solutions Group, Inc.
Windows XP
R 2.0.1
Pentium 4
512 memory



From srini_iyyer_bio at yahoo.com  Mon Nov 29 19:24:40 2004
From: srini_iyyer_bio at yahoo.com (Srinivas Iyyer)
Date: Mon, 29 Nov 2004 10:24:40 -0800 (PST)
Subject: [R] Log(2) values to log(10) values
In-Reply-To: <x24qj8k17d.fsf@biostat.ku.dk>
Message-ID: <20041129182441.53812.qmail@web53502.mail.yahoo.com>

Dear group, 
 I am using justRMA in bioconductor to get expression
values.  Now I computed fold changes.  the fold
changes i get are log2 values.  

How can I convert these to log10 values to see them as
actual fold change values. 

thank you.



From danlipsitt at gmail.com  Mon Nov 29 19:26:16 2004
From: danlipsitt at gmail.com (Dan Lipsitt)
Date: Mon, 29 Nov 2004 13:26:16 -0500
Subject: [R] escaping backslash in a string
In-Reply-To: <x24qj8k17d.fsf@biostat.ku.dk>
References: <b3a7efa904112909201373cd6@mail.gmail.com>
	<x24qj8k17d.fsf@biostat.ku.dk>
Message-ID: <b3a7efa904112910265462a0ca@mail.gmail.com>

Ah, I see. Thanks. ?print.default and ?cat do not mention this.



From andy_liaw at merck.com  Mon Nov 29 19:32:24 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 29 Nov 2004 13:32:24 -0500
Subject: [R] Log(2) values to log(10) values
Message-ID: <3A822319EB35174CA3714066D590DCD50994E39B@usrymx25.merck.com>

Multiply by log(2)/log(10); e.g., 

> log2(1024) * log(2)/log(10)
[1] 3.0103
> log(1024, 10)
[1] 3.0103

Andy

> From: Srinivas Iyyer
> 
> Dear group, 
>  I am using justRMA in bioconductor to get expression
> values.  Now I computed fold changes.  the fold
> changes i get are log2 values.  
> 
> How can I convert these to log10 values to see them as
> actual fold change values. 
> 
> thank you.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From p.dalgaard at biostat.ku.dk  Mon Nov 29 19:30:30 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 29 Nov 2004 19:30:30 +0100
Subject: [R] Log(2) values to log(10) values
In-Reply-To: <20041129182441.53812.qmail@web53502.mail.yahoo.com>
References: <20041129182441.53812.qmail@web53502.mail.yahoo.com>
Message-ID: <x2zn10ik3d.fsf@biostat.ku.dk>

Srinivas Iyyer <srini_iyyer_bio at yahoo.com> writes:

> Dear group, 
>  I am using justRMA in bioconductor to get expression
> values.  Now I computed fold changes.  the fold
> changes i get are log2 values.  
> 
> How can I convert these to log10 values to see them as
> actual fold change values. 

Divide by log2(10) or multiply by log10(2).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From abunn at whrc.org  Mon Nov 29 19:33:21 2004
From: abunn at whrc.org (Andy Bunn)
Date: Mon, 29 Nov 2004 13:33:21 -0500
Subject: [R] Seeking help with a simple loop construction
In-Reply-To: <s1ab1177.040@mail.marketsolutionsgroup.com>
Message-ID: <NEBBIPHDAMMOKDKPOFFIKENGCMAA.abunn@whrc.org>

Does this do what you want?

foo.df <- data.frame(x = rnorm(12), y = runif(12), z = factor(rep(1:3,4)))
bar.mat <- matrix(NA,  nrow = ncol(foo.df)-1, ncol = nlevels(foo.df$z))
for(i in 1:(ncol(foo.df)-1))
{
    bar.mat[i,] <- xtabs(foo.df[,i] ~ foo.df$z)
}
bar.mat

There's probably a slicker way with apply...

HTH, Andy

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Greg Blevins
> Sent: Monday, November 29, 2004 1:09 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Seeking help with a simple loop construction
> 
> 
> Hello,
> 
> I have a df, pp, with five variables:
> 
> > nobs(pp)
>   q10_1   q10_2   q10_3   q10_4 actcode 
>    1620    1620    1620    1620    1620 
> 
> I want to create a loop to run four xtabs (the first four 
> variables above by the fifth) and then store the results in a 
> matrix.  Below I make my intent clear by showing the output of 
> one xtab which is inserted into a matrix.
> 
> > a <- xtabs(q10_1 ~ actcode)
> > a
> actcode
>  1  2  3  4  5  6  7  8  9 10 
>  7 11  3 60 66 56 21 40  7  8 
> 
> > freq.mat <- matrix(0, 4, 10, byrow = TRUE)
> > freq.mat[1,] <- a
> > freq.mat
>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
> [1,]    7   11    3   60   66   56   21   40    7     8
> [2,]    0    0    0    0    0    0    0    0    0     0
> [3,]    0    0    0    0    0    0    0    0    0     0
> [4,]    0    0    0    0    0    0    0    0    0     0
> ===============================================================
> I have spent a couple of hours searching the web and my texts but 
> continue to strike out in my attempts to construct a correct 
> formulation of this simple loop. Help would be appreciated.
> 
> Greg Blevins
> The Market Solutions Group, Inc.
> Windows XP
> R 2.0.1
> Pentium 4
> 512 memory
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html



From jmacdon at med.umich.edu  Mon Nov 29 19:49:03 2004
From: jmacdon at med.umich.edu (James W. MacDonald)
Date: Mon, 29 Nov 2004 13:49:03 -0500
Subject: [R] Log(2) values to log(10) values
In-Reply-To: <20041129182441.53812.qmail@web53502.mail.yahoo.com>
References: <20041129182441.53812.qmail@web53502.mail.yahoo.com>
Message-ID: <41AB6F1F.8070504@med.umich.edu>

Srinivas Iyyer wrote:
> Dear group, 
>  I am using justRMA in bioconductor to get expression
> values.  Now I computed fold changes.  the fold
> changes i get are log2 values.  
> 
> How can I convert these to log10 values to see them as
> actual fold change values. 

First, a bit of etiquette. It is considered impolite to post the same 
question to more than one listserv. Second, although this is not a 
BioC-specific question, that is probably a better place to ask.

To answer; log10 values will *not* give you actual fold changes. These 
will be the same as log2, to a multiplicitive constant. You want to 
convert back to the natural scale e.g., 2^FC, where FC = your fold changes.

Also note that you compute fold change on the log scale by subtraction, 
not division.

HTH,

Jim


> 
> thank you.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
James W. MacDonald
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109



From p.murrell at auckland.ac.nz  Mon Nov 29 20:02:08 2004
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Tue, 30 Nov 2004 08:02:08 +1300
Subject: [R] Call to trellis.focus(); thenpanel.superpose()
References: <DC12507A-41F3-11D9-92B8-000A95CDA0F2@anu.edu.au>
	<200411290921.34595.deepayan@stat.wisc.edu>
Message-ID: <41AB7230.8010303@stat.auckland.ac.nz>

Hi


Deepayan Sarkar wrote:
> On Monday 29 November 2004 04:46, John Maindonald wrote:
> 
>>The following works fine with the x11 device, though it
>>may well be that an initial plot is overwritten.  With a pdf
>>or postscript device, I get two plots, the first of which
>>still has the red border from having the focus, while the
>>second is the plot that I want.
>>
>>         library(lattice); library(grid)
>>         plt <- xyplot(uptake ~ conc, groups=Plant, data=CO2)
>>         print(plt)
>>         trellis.focus("panel", row=1, column=1)
>>         arglist=trellis.panelArgs()
>>         arglist$type <- "l"
>>         do.call("panel.superpose", args=arglist)
>>         trellis.unfocus()
>>
>>Should I be able to use panel.superpose() in this way?
> 
> 
> Yes. The red border should be 'removed', but that's done by grid.remove 
> and maybe it doesn't work on PDF devices.


It works, but it works by removing the red rectangle from the list of 
objects representing the scene and then redrawing the scene, hence a new 
page.

Paul


> The solution is to use 
> 
> trellis.focus("panel", row=1, column=1, highlight = FALSE)
> 
> (which happens automatically for non-interactive sessions).
> 
> 
>>The new abilities provided by trellis.focus() etc add
>>greatly to the flexibility of what can be done with lattice
>>plots.  The grid-lattice combination is a great piece of
>>software.
> 
> 
> Yes, it can be especially useful for tasks similar to identify(). 
> Everything else can probably be done by clever enough use of the panel 
> function (though perhaps not as naturally).
> 
> Deepayan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From adslvdll at tpg.com.au  Mon Nov 29 20:07:38 2004
From: adslvdll at tpg.com.au (stephenc)
Date: Tue, 30 Nov 2004 06:07:38 +1100
Subject: [R] (no subject)
Message-ID: <000001c4d646$b67a0000$0d01a8c0@tablet>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041130/d167047b/attachment.pl

From danlipsitt at gmail.com  Mon Nov 29 20:12:33 2004
From: danlipsitt at gmail.com (Dan Lipsitt)
Date: Mon, 29 Nov 2004 14:12:33 -0500
Subject: [R] escaping backslash in a string
In-Reply-To: <x24qj8k17d.fsf@biostat.ku.dk>
References: <b3a7efa904112909201373cd6@mail.gmail.com>
	<x24qj8k17d.fsf@biostat.ku.dk>
Message-ID: <b3a7efa90411291112a9c3a12@mail.gmail.com>

I have it working now, I think. Since it's going into a regular
expression, I have to escape each of the escape characters, resulting
in four backslashes altogether:

> sub("[.]", "x", "a.b")
[1] "axb"
> sub("[.]", "\.", "a.b")
[1] "a.b"
> sub("[.]", "\\.", "a.b")
[1] "a.b"
> sub("[.]", "\\\.", "a.b")
[1] "a.b"
> sub("[.]", "\\\\.", "a.b")
[1] "a\\.b"
> cat(sub("[.]", "\\\\.", "a.b"))
a\.b>

or

> cat(sub("\\.", "\\\\.", "a.b"))
a\.b> 

Dan



From bitwrit at ozemail.com.au  Tue Nov 30 19:24:19 2004
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Wed, 1 Dec 2004 05:24:19 +1100
Subject: [R] Re: Reasons not to answer very basic questions...
Message-ID: <20041129192247.YRQE24735.smta04.mail.ozemail.net@there>

A.J. Rossini wrote:
>
> and perhaps the most important reason for the particular socratic form
> of teaching on this list...

Golly, anyone who read Plato's Dialogues would realize that the Socratic 
method involves patiently leading the questioner stepwise through the 
solution, not simply writing RTFMeno.

Jim



From ray at mcs.vuw.ac.nz  Mon Nov 29 21:07:01 2004
From: ray at mcs.vuw.ac.nz (Ray Brownrigg)
Date: Tue, 30 Nov 2004 09:07:01 +1300 (NZDT)
Subject: [R] library(fields) world shift function not working anymore
Message-ID: <200411292007.iATK71Kr017636@tahi.mcs.vuw.ac.nz>

> I just upgraded to 2.01 on Mac OS 10.3.6.  I used to use the command 
> (on R 1.9.x):
> 
> world(ylim=c(-30,30), xlim = c(0,360), shift=TRUE, add=TRUE)
> 
> to draw a world outline over my image plots.  My data uses longitude 
> from (0, 360) so I need to use the shift function.  After I upgraded, I 
> get the following error:
> 
>  > world(ylim=c(-30,30), xlim = c(0,360), shift=TRUE, add=TRUE)
> Error in world(ylim = c(-30, 30), xlim = c(0, 360), shift = TRUE, add = 
> TRUE) :
> 	NAs are not allowed in subscripted assignments
> 
> Does anyone know a workaround for this?
> 
Well, a workaround would be:
library(maps)
map("world2", ylim=c(-30,30), xlim = c(0,360), add = TRUE)

(at least until fields is updated).

                CHANGES IN R VERSION 2.0.0

    o   Subassignments involving NAs and with a replacement value of
        length > 1 are now disallowed.  (They were handled
        inconsistently in R < 2.0.0, see PR#7210.)  For data frames
        they are disallowed altogether, even for logical matrix indices
        (the only case which used to work).

Ray Brownrigg



From adslvdll at tpg.com.au  Mon Nov 29 21:09:19 2004
From: adslvdll at tpg.com.au (stephenc)
Date: Tue, 30 Nov 2004 07:09:19 +1100
Subject: [R] tune()
Message-ID: <000301c4d64f$530c9330$0d01a8c0@tablet>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041130/2b0a250f/attachment.pl

From Achim.Zeileis at wu-wien.ac.at  Mon Nov 29 21:34:37 2004
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Mon, 29 Nov 2004 21:34:37 +0100
Subject: [R] tune()
In-Reply-To: <000301c4d64f$530c9330$0d01a8c0@tablet>
References: <000301c4d64f$530c9330$0d01a8c0@tablet>
Message-ID: <20041129213437.0b2a630d.Achim.Zeileis@wu-wien.ac.at>

Sending a help request once is enough! Even if you don't have an answer
after 1 hour.

> I am trying to tune an svm by doing the following:
>  
> tune(svm, similarity ~., data = training, degree = 2^(1:2), gamma =
> 2^(-1:1), coef0 = 2^(-1:1), cost = 2^(2:4), type = "polynomial")

I think you want to set `kernel' not `type'...
  
> but I am getting 
>  
> Error in svm.default(x, y, scale = scale, ...) : 
>         wrong type specification!

...checking the argument `type' on ?svm would have told you that.

> I have to admit I am not sure what I am doing wrong.  Could anyone
> tell me why the parameters I am using are wrong? 
>
> Plus could anyone tell me how to go about picking the correct ranges
> for my tuning?

Surprisingly, you have to set `ranges' to specify the ranges of the
parameters, e.g.,

  obj <- tune(svm, Species ~ ., data = iris,
              ranges = list(degree =2^(1:2), gamma = 2^(-1:1),
                            coef0 = 2^(-1:1), cost = 2^(2:4)),
              kernel = "polynomial")

I'm not sure what good ranges are to tune an SVM with a polynomial
kernel...

hth,
Z



From ggrothendieck at myway.com  Mon Nov 29 21:47:50 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 29 Nov 2004 20:47:50 +0000 (UTC)
Subject: [R] escaping backslash in a string
References: <b3a7efa904112909201373cd6@mail.gmail.com>
	<x24qj8k17d.fsf@biostat.ku.dk>
	<b3a7efa90411291112a9c3a12@mail.gmail.com>
Message-ID: <loom.20041129T214319-896@post.gmane.org>

Dan Lipsitt <danlipsitt <at> gmail.com> writes:

: 
: I have it working now, I think. Since it's going into a regular
: expression, I have to escape each of the escape characters, resulting
: in four backslashes altogether:
: 
: > sub("[.]", "x", "a.b")
: [1] "axb"
: > sub("[.]", "\.", "a.b")
: [1] "a.b"
: > sub("[.]", "\\.", "a.b")
: [1] "a.b"
: > sub("[.]", "\\\.", "a.b")
: [1] "a.b"
: > sub("[.]", "\\\\.", "a.b")
: [1] "a\\.b"
: > cat(sub("[.]", "\\\\.", "a.b"))
: a\.b>
: 
: or
: 
: > cat(sub("\\.", "\\\\.", "a.b"))
: a\.b> 
: 

You can use \134 in place of the double backslash 
if that makes more sense to you.  

Another possibility is to create a variable 
   backslash <- "\\" 
and paste together each string in terms of that variable.

Also its sometimes helpful to use nchar(s) on string s
just to check how many characters it has.



From cyracules at yahoo.co.uk  Mon Nov 29 22:03:22 2004
From: cyracules at yahoo.co.uk (John)
Date: Mon, 29 Nov 2004 21:03:22 +0000 (GMT)
Subject: [R] [BASIC] Solution of creating a sequence of object names
Message-ID: <20041129210322.89856.qmail@web26306.mail.ukl.yahoo.com>

Dear R-users,

I state that this is for beginners, so you may ignore
this in order not to be irritated.

By the way, "patience" is another important thing,
together with "kindness", we should keep in mind when
we teach students and our own children as Jim Lemon
pointed out well in the context of the Socratic
method. You may know that being kind does not mean 
giving "spoonfed" answers to questioners.

---------------------

I was asked for the solution of my problem, and a
couple of answers were given to me in private emails.
I am not sure if it was a mere accident. I post them
now, without their permission, for those who are
interested in learning them. So if you're happy to
know the solution, thanks should go to the person
concerned. I thank all the three people named below.

(1) my solution after reading the R-FAQ 7.21 as Uwe
Ligges pointed out

> for ( i in 1:10 ) {
+ assign(paste("my.file.", i, sep=""), NULL)
+ }
>

(2) Adai Ramasamy's solution

> for(obj in paste("my.ftn", 1:10, sep=""))
assign(obj, NULL)
> 
### or 
> 
> for(i in 1:10) assign(paste("my.ftn", i, sep=""),
NULL)
>

(3) James Holtman's solution

# For example, if you want to generate 10 groups 
# of 5 random numbers and store them 
# under then names "GRPn" where n is 1 -> 10, 
# the following can be used:
#
> Result <- list()      # create the list
> for (i in 1:10) Result[[paste("GRP", i, sep='')]] <-
runif(5)   # store each result
> Result    # print out the data
$GRP1
[1] 0.2655087 0.3721239 0.5728534 0.9082078 0.2016819

$GRP2
[1] 0.89838968 0.94467527 0.66079779 0.62911404
0.06178627

$GRP3
[1] 0.2059746 0.1765568 0.6870228 0.3841037 0.7698414

$GRP4
[1] 0.4976992 0.7176185 0.9919061 0.3800352 0.7774452

$GRP5
[1] 0.9347052 0.2121425 0.6516738 0.1255551 0.2672207

$GRP6
[1] 0.38611409 0.01339033 0.38238796 0.86969085
0.34034900

$GRP7
[1] 0.4820801 0.5995658 0.4935413 0.1862176 0.8273733

$GRP8
[1] 0.6684667 0.7942399 0.1079436 0.7237109 0.4112744

$GRP9
[1] 0.8209463 0.6470602 0.7829328 0.5530363 0.5297196

$GRP10
[1] 0.78935623 0.02333120 0.47723007 0.73231374
0.69273156

>

Regards,

John



From br44114 at yahoo.com  Mon Nov 29 22:27:36 2004
From: br44114 at yahoo.com (bogdan romocea)
Date: Mon, 29 Nov 2004 13:27:36 -0800 (PST)
Subject: [R] [BASIC] Solution of creating a sequence of object names
Message-ID: <20041129212736.84184.qmail@web50307.mail.yahoo.com>

You may be missing something. After you create all those objects,
you'll want to use them. Use get():
for (i in 1:10) ... get(paste("object",i,sep="")) ... 
It took me about a week to find out how to do this. I waited for a
few days, but before I got to ask this basic/rtfm question, someone
else - fortunately :-) - did.

HTH,
b.


-----Original Message-----
From: John [mailto:cyracules at yahoo.co.uk]
Sent: Monday, November 29, 2004 4:03 PM
To: r-help at stat.math.ethz.ch
Subject: [R] [BASIC] Solution of creating a sequence of object names


Dear R-users,

I state that this is for beginners, so you may ignore
this in order not to be irritated.

By the way, "patience" is another important thing,
together with "kindness", we should keep in mind when
we teach students and our own children as Jim Lemon
pointed out well in the context of the Socratic
method. You may know that being kind does not mean 
giving "spoonfed" answers to questioners.

---------------------

I was asked for the solution of my problem, and a
couple of answers were given to me in private emails.
I am not sure if it was a mere accident. I post them
now, without their permission, for those who are
interested in learning them. So if you're happy to
know the solution, thanks should go to the person
concerned. I thank all the three people named below.

(1) my solution after reading the R-FAQ 7.21 as Uwe
Ligges pointed out

> for ( i in 1:10 ) {
+ assign(paste("my.file.", i, sep=""), NULL)
+ }
>

(2) Adai Ramasamy's solution

> for(obj in paste("my.ftn", 1:10, sep=""))
assign(obj, NULL)
> 
### or 
> 
> for(i in 1:10) assign(paste("my.ftn", i, sep=""),
NULL)
>

(3) James Holtman's solution

# For example, if you want to generate 10 groups 
# of 5 random numbers and store them 
# under then names "GRPn" where n is 1 -> 10, 
# the following can be used:
#
> Result <- list()      # create the list
> for (i in 1:10) Result[[paste("GRP", i, sep='')]] <-
runif(5)   # store each result
> Result    # print out the data
$GRP1
[1] 0.2655087 0.3721239 0.5728534 0.9082078 0.2016819

$GRP2
[1] 0.89838968 0.94467527 0.66079779 0.62911404
0.06178627

$GRP3
[1] 0.2059746 0.1765568 0.6870228 0.3841037 0.7698414

$GRP4
[1] 0.4976992 0.7176185 0.9919061 0.3800352 0.7774452

$GRP5
[1] 0.9347052 0.2121425 0.6516738 0.1255551 0.2672207

$GRP6
[1] 0.38611409 0.01339033 0.38238796 0.86969085
0.34034900

$GRP7
[1] 0.4820801 0.5995658 0.4935413 0.1862176 0.8273733

$GRP8
[1] 0.6684667 0.7942399 0.1079436 0.7237109 0.4112744

$GRP9
[1] 0.8209463 0.6470602 0.7829328 0.5530363 0.5297196

$GRP10
[1] 0.78935623 0.02333120 0.47723007 0.73231374
0.69273156

>

Regards,

John

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From huh at rti.org  Mon Nov 29 22:57:28 2004
From: huh at rti.org (Huh, Seungho)
Date: Mon, 29 Nov 2004 16:57:28 -0500
Subject: [R] Kernel Fisher Discriminant in R?
Message-ID: <54535CBB16ABDA469D72C7614960429B097405@rtpwexc04.RCC_NT.RTI.ORG>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041129/5971601d/attachment.pl

From cyracules at yahoo.co.uk  Mon Nov 29 23:53:32 2004
From: cyracules at yahoo.co.uk (John)
Date: Mon, 29 Nov 2004 22:53:32 +0000 (GMT)
Subject: [R] [BASIC] Solution of creating a sequence of object names
In-Reply-To: <20041129212736.84184.qmail@web50307.mail.yahoo.com>
Message-ID: <20041129225332.10292.qmail@web26305.mail.ukl.yahoo.com>

It was enough for me to use the 'assign' function
alone. But I'll remember the 'get' function for future
reference. Thanks a lot for the note.

John
 

 --- bogdan romocea <br44114 at yahoo.com> wrote: 
> You may be missing something. After you create all
> those objects,
> you'll want to use them. Use get():
> for (i in 1:10) ... get(paste("object",i,sep=""))
> ... 
> It took me about a week to find out how to do this.
> I waited for a
> few days, but before I got to ask this basic/rtfm
> question, someone
> else - fortunately :-) - did.
> 
> HTH,
> b.
> 
> 
> -----Original Message-----
> From: John [mailto:cyracules at yahoo.co.uk]
> Sent: Monday, November 29, 2004 4:03 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] [BASIC] Solution of creating a sequence
> of object names
> 
> 
> Dear R-users,
> 
> I state that this is for beginners, so you may
> ignore
> this in order not to be irritated.
> 
> By the way, "patience" is another important thing,
> together with "kindness", we should keep in mind
> when
> we teach students and our own children as Jim Lemon
> pointed out well in the context of the Socratic
> method. You may know that being kind does not mean 
> giving "spoonfed" answers to questioners.
> 
> ---------------------
> 
> I was asked for the solution of my problem, and a
> couple of answers were given to me in private
> emails.
> I am not sure if it was a mere accident. I post them
> now, without their permission, for those who are
> interested in learning them. So if you're happy to
> know the solution, thanks should go to the person
> concerned. I thank all the three people named below.
> 
> (1) my solution after reading the R-FAQ 7.21 as Uwe
> Ligges pointed out
> 
> > for ( i in 1:10 ) {
> + assign(paste("my.file.", i, sep=""), NULL)
> + }
> >
> 
> (2) Adai Ramasamy's solution
> 
> > for(obj in paste("my.ftn", 1:10, sep=""))
> assign(obj, NULL)
> > 
> ### or 
> > 
> > for(i in 1:10) assign(paste("my.ftn", i, sep=""),
> NULL)
> >
> 
> (3) James Holtman's solution
> 
> # For example, if you want to generate 10 groups 
> # of 5 random numbers and store them 
> # under then names "GRPn" where n is 1 -> 10, 
> # the following can be used:
> #
> > Result <- list()      # create the list
> > for (i in 1:10) Result[[paste("GRP", i, sep='')]]
> <-
> runif(5)   # store each result
> > Result    # print out the data
> $GRP1
> [1] 0.2655087 0.3721239 0.5728534 0.9082078
> 0.2016819
> 
> $GRP2
> [1] 0.89838968 0.94467527 0.66079779 0.62911404
> 0.06178627
> 
> $GRP3
> [1] 0.2059746 0.1765568 0.6870228 0.3841037
> 0.7698414
> 
> $GRP4
> [1] 0.4976992 0.7176185 0.9919061 0.3800352
> 0.7774452
> 
> $GRP5
> [1] 0.9347052 0.2121425 0.6516738 0.1255551
> 0.2672207
> 
> $GRP6
> [1] 0.38611409 0.01339033 0.38238796 0.86969085
> 0.34034900
> 
> $GRP7
> [1] 0.4820801 0.5995658 0.4935413 0.1862176
> 0.8273733
> 
> $GRP8
> [1] 0.6684667 0.7942399 0.1079436 0.7237109
> 0.4112744
> 
> $GRP9
> [1] 0.8209463 0.6470602 0.7829328 0.5530363
> 0.5297196
> 
> $GRP10
> [1] 0.78935623 0.02333120 0.47723007 0.73231374
> 0.69273156
> 
> >
> 
> Regards,
> 
> John
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 
> 
> 	
> 		
> __________________________________ 



From HDoran at air.org  Mon Nov 29 23:55:21 2004
From: HDoran at air.org (Doran, Harold)
Date: Mon, 29 Nov 2004 17:55:21 -0500
Subject: [R] Labeling charts within a loop
Message-ID: <88EAF3512A55DF46B06B1954AEF73F74069362B1@dc1ex2.air.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041129/45206422/attachment.pl

From p.dalgaard at biostat.ku.dk  Tue Nov 30 01:32:24 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Nov 2004 01:32:24 +0100
Subject: [R] Labeling charts within a loop
In-Reply-To: <88EAF3512A55DF46B06B1954AEF73F74069362B1@dc1ex2.air.org>
References: <88EAF3512A55DF46B06B1954AEF73F74069362B1@dc1ex2.air.org>
Message-ID: <x2act0ch2f.fsf@biostat.ku.dk>

"Doran, Harold" <HDoran at air.org> writes:

> Hi All:
> 
> This may turn out to be very simply, but I can't seem to add the name of
> the school to a chart. The loop I created is below that subsets a
> dataframe and creates a chart for each school based on certain
> variables. As it stands now, they title includes the school's ID number.
> Instead, I want to replace this with the school's actual name, which is
> stored in a variable called schname.
> 
> But so far, my attempts have been messy and ugly? Can anyone see a
> mistake in my code?

No overt mistakes, but multiple infelicities. Don't listen to guys who
want to create sequences of variable names, it is (almost) invariably
the wrong idea.

> #This portion of code takes the subset
> school.list<-as.vector(unique(mansfield.dat$schirn))
> for(school.number in school.list){
> assign(paste("school", school.number, sep=""),
> subset(mansfield.dat,mansfield.dat$schirn==school.number))}

Lessee, that was basically

byschool <- split(mansfield.dat, mansfield.dat$schirn) 

below, you use only 4 variables from the data frame and under
different names, so maybe first extract and rename them and have

#--------------
# code should be executable from here on

vars <- c("bpra3ccf", "bprr3ccf", "bprl3ccf", "bpri3ccf")
d2 <- mansfield.dat[vars]
names(d2) <- c("AV", "RP", "LT", "IT")
byschool <- split(d2, mansfield.dat$schirn)

# now to get the averages, you can do

mnList <- lapply(byschool, colMeans)

# now, let's do a little function to do one of your plots. It will have
# three arguments, the mean vector, the number of the school, and the
# name of the school

doPlot <- function(mn, no, name)
{
   # (I won't argue with your filename scheme)
   pdf(file=paste("school", no, "rel.pdf", sep="")
   # The barplot should get the names right automagically now:
   barplot(mn, width=c(.5,.5,.5,.5), ylim = c(0, 4), col=("salmon"))
   legend(1.0,3.8,legend=c("1=Relative Weakness", 
                "2=Similar to District", "3=Relative Strength"))
   title(main=(paste("Grade 3 Relative Strengths and Weaknesses", "\n",
      "School:", name, sep="")))
  
    dev.off()
}

# All set, now

num <- names(mnList)
mapply(doPlot, mnList, num, schname[num])

# done! End executable code
#--------------------------

Obviously, not having your data, all the above is untested, and most
likely not entirely bugfree.

> 
> for(school.number in school.list){
> AV<-mean(get(paste("school",school.number,sep=""))[["bpra3ccf"]])
> RP<-mean(get(paste("school",school.number,sep=""))[["bprr3ccf"]])
> LT<-mean(get(paste("school",school.number,sep=""))[["bprl3ccf"]])
> IT<-mean(get(paste("school",school.number,sep=""))[["bpri3ccf"]])
> 
> 
> strengthbar<-cbind(AV,RP,LT,IT)
> 
> pdf(paste("school",school.number,"rel", ".pdf",sep=""))
> barplot(strengthbar,names=c("AV", "RP", "LT",
> "IT"),width=c(.5,.5,.5,.5),ylim = c(0, 4),col=("salmon"))
> legend(1.0,3.8,legend=c("1=Relative Weakness", "2=Similar to District",
> "3=Relative Strength"))
> 
> 
> 
> title(main=(paste("Grade 3 Relative Strengths and Weaknesses", "\n",
> "School ", school.number,sep="")))
> 
> 
> dev.off()
> 
> }


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Tom.Mulholland at dpi.wa.gov.au  Tue Nov 30 02:33:18 2004
From: Tom.Mulholland at dpi.wa.gov.au (Mulholland, Tom)
Date: Tue, 30 Nov 2004 09:33:18 +0800
Subject: Reasons not to answer very basic questions in a straightforward
	way; was: Re: [R] creating a sequence of object names
Message-ID: <33F91FB3FDF42E4180428AC66A5CF30B02D3BA37@afhex01.dpi.wa.gov.au>

Your statement seems innocent enough on the face of it, but there are two facets that I think are worthy of note.

The first is that of time, and more specifically who's time. As a user of other lists I can say that this is the best list in terms of getting the answer to my problem, albeit sometime's obliquely. I intermittently respond to questions generally of the type you refer to. I say intermittently because I don't have the time to do more than that. Why do I respond to these questions? Well I made some of the same basic errors. As a much more knowledgeable user, I think twice (well more like six times) before I post because I understand the amount of time it takes to create a response that is worthwhile. I'll get to the reason for not creating simple answers in the next point. If I had to pay for the "quality" of support that I get on this list, there is no way that I could afford it. I take what I get and I am grateful for the time given by so many. To assume that my time is more important than those who will give me the answer is disrespectful.

Secondly is a process referred to as "crowding out." With reference to the list there is a danger that it would cease to be a source of wisdom and start being a repetitive FAQ. As the list stands now I learn much more from other people's questions than I do from my own. I read about different ways of approaching various tasks and while I barely comprehend some of the more difficult questions they provoke my curiosity. I can read an FAQ anytime, I can read all of the manuals, they won't go away. At the moment the list is full of variation with the odd thread like this, which sparks more of a philosophical content. If 90% of the list was full of questions that are "tiresome because of dullness" or more succinctly "tedious," why would I continue to either ask questions of it or respond to them. In essence what I find useful on the list would be crowded out by repetitive questions. 

Experience has shown me that where you have a demand for quick solutions from people busy getting on with their lives, it can overwhelm your own life. One such experience happened  the last time I was in London, I happened to be standing next to one of those little currency exchange booths waiting for a friend. I heard some people having trouble working out where the British Museum was. I gave them some help. It was only after a while that I thought to start counting how many requests I received (well I was on Tottenham Court Rd) but eventually I counted 35. One can maintain that sort of help for a while, but I couldn't stand there all day. I was abused by a couple for eventually leaving and not answering their question. I know there are users of R who will not use the mailing list because they are intimidated by the manner of the list, but the users I have talked to acknowledge that they are looking for an easy solution and are not interested in contributing to the list. They have also pointed out that they can see why the list does what it does.

I get the feeling that a lot of subscribers to this list would understand where you are coming from, even though they may not look at the list the same way that you do. The bottom line is that I have had a reply to every question that I have put on the list and those replies have always helped me to solve my problem. Show that you've put some effort in and people will match that effort "and more*." Your note had effort and consequently was treated as meritorious, although the answers may not have been what you wished.

Tom Mulholland


* K9, Dr Who, BBC Television
-----Original Message-----
...
I know very well that it is basic manners to read
those materials before asking questions here, but you
should also understand that people sometimes get stuck
with very simple problems if they are driven by stress
or run down. They can save a lot of time and
concentrate on and develop their primary jobs instead.
And I don't think you should be worried about 900
silly questions out of 1000 messages posted because
they are at least well-educated people who know what
reading basic materials before posting questions
means.

...

I beg your pardon if this message is not relevant to
this help list.

With kind regards,

John


 --- Uwe Ligges <ligges at statistik.uni-dortmund.de>
wrote: 
> John wrote:
> > Thank you, Uwe. I've found a way to do the job by
> > reading the FAQ 7.21 although it is not giving a
> > precise explanation to a novice or casual user at
> > first reading. For example, if you type the first
> two
> 
> But the corresponding help files do so, for sure,
> and the FAQ 7.21 
> points you to ?assign and ?get.
> 
> 
> > lines in the FAQ, you get an error as you do not
> have
> > the variable, a, initially.
> >
> > I am sure that more and more people get interested
> in
> > and serious about using R if advanced users are
> kind
> > enough to answer simple and silly questions as
> well
> > which are already explained in basic
> documentations.
> > Or is this community for highly motivated and
> advanced
> > R users only?
> 
> No, of course it is for novices as well!
> 
> BUT we do expect that novices do read basic
> documentation such as "An 
> Introduction to R" and the R FAQ before asking
> question.
> If there are too many silly questions from thousands
> of R users, nobody 
> is able to manage the questions any more. And note
> that those people 
> answering questions do it on a voluntary basis, and
> (at least partially) 
> in their spare time!
> Nobody would be subscribed to R-help any more, if
> there are 1000 mails a 
> day, 900 of them containing silly questions! It is
> yet already hard 
> enough to get through the huge amount of messages in
> a reasonable amount 
> of time!
> 
> 
> I have answered your question in a way,
> 
>   1) so that it is up to you to read some
> documentation. Now you have 
> seen the FAQs and some help files. And you have
> learned much more than 
> you would have learned if I had said "Use assign()"
> 
>   2) so that nobody feels too encouraged to ask
> questions before reading 
> basic documentation - and my answer still saved you
> a lot of time!
> 
> Uwe Ligges
> 
> 
> 
> 
> > Regards,
> > 
> > John
> > 
> > 
> >  --- Uwe Ligges <ligges at statistik.uni-dortmund.de>
> > wrote: 
> > 
> >>John wrote:
> >>
> >>
> >>>Hello R-users,
> >>>
> >>>I wanted to generate objects named 'my.ftn1',
> >>>'my.ftn2', ... , 'my.ftn10', and tried the
> >>
> >>following
> >>
> >>>code without success. How can I do this?
> >>>
> >>>
> >>>
> >>>>for ( i in 1:10 ) {
> >>>
> >>>+ sub(" ", "", paste("my.ftn", i)) <- NULL
> >>>+ }
> >>>Error: Target of assignment expands to
> >>
> >>non-language
> >>
> >>>object
> >>>
> >>>
> >>>Many thanks.
> >>>
> >>>John
> >>>
> >>>______________________________________________
> >>>R-help at stat.math.ethz.ch mailing list
> >>>https://stat.ethz.ch/mailman/listinfo/r-help
> >>>PLEASE do read the posting guide!
> >>
> >>http://www.R-project.org/posting-guide.html
> >>
> >>
> >>Please do as suggested above, read the posting
> >>guide!
> >>It suggests to read the FAQs. FAQ 7.21 is what you
> >>are looking for: "How 
> >>can I turn a string into a variable?".
> >>
> >>Uwe Ligges
> >>
> > 
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://stat.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jfox at mcmaster.ca  Tue Nov 30 03:25:34 2004
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 29 Nov 2004 21:25:34 -0500
Subject: [R] Tetrachoric and polychoric ceofficients (for sem) - any tips?
In-Reply-To: <6.1.0.6.0.20041129183439.02658eb0@mailbox.iop.kcl.ac.uk>
Message-ID: <20041130022533.KMHD1919.tomts22-srv.bellnexxia.net@JohnDesktop8300>

Dear Michael,

I had some time this evening, so I programmed the two-step procedure
described in the article that I mentioned. This isn't the ML estimate of the
correlation, but apparently it performs reasonably well and it is quite
fast. I checked the function on some examples and it appears to work
correctly. I'd be curious to see how the this function compares with the one
that you received.

A more complete solution would take a data frame and, as appropriate,
calculate polychoric, polyserial, and product-moment correlations among its
columns. I don't think that writing a polyserial-correlation function would
be any more difficult. Perhaps I'll add this to the sem package; I hesitate
to do so because the resulting standard errors and likelihoods from sem()
won't be right.

I'm taking the liberty of copying this reply to the r-help list, since the
question was originally raised there. I hope that's OK.

Regards,
 John 


-------------- snip ------------

polychor <- function (x, y){
# x: a contingency table of counts or an ordered categorical variable
# y: if x is a variable, a second ordered categorical variable
# returns the polychoric correlation for the table
#  or between x and y, using the two-step approximation
#  to the ML estimate described in F. Drasgow, "Polychoric and
#  polyserial correlations," in S. Kotz and N. Johnson, eds.
#  The Encyclopedia of Statistics, Volume 7, New York: Wiley,
#  1986.
  f <- function(rho) {
    P <- matrix(0, r, c)
    R <- matrix(c(1, rho, rho, 1), 2, 2)
    for (i in 1:r){
      for (j in 1:c){
        P[i,j] <- pmvnorm(lower=c(row.cuts[i], col.cuts[j]),
                          upper=c(row.cuts[i+1], col.cuts[j+1]),
                          corr=R)
        }
      }
     - sum(tab * log(P))
    }
  tab <- if (missing(y)) x else table(x, y)
  r <- nrow(tab)
  c <- ncol(tab)
  n <- sum(tab)
  row.cuts <- c(-Inf, qnorm(cumsum(rowSums(tab))/n))
  col.cuts <- c(-Inf, qnorm(cumsum(colSums(tab))/n))
  optimise(f, interval=c(0, 1))$minimum
  }


> -----Original Message-----
> From: Michael Dewey [mailto:m.dewey at iop.kcl.ac.uk] 
> Sent: Monday, November 29, 2004 1:38 PM
> To: John Fox
> Subject: RE: [R] Tetrachoric and polychoric ceofficients (for 
> sem) - any tips?
> 
> At 18:07 28/11/04, you wrote:
> >Dear Michael,
> >
> >I'm not aware of pre-existing R code for tetrachoric or polychoric 
> >correlations. I may at some point incorporate such functions 
> into the 
> >sem package but I don't have concrete plans for doing so. On 
> the other 
> >hand, I don't think that it would be very hard to do so. (A 
> discussion, 
> >references, and an example are in Kotz and Johnson, eds., 
> Encyclopedia 
> >of Statistics, Vol 7.)
> 
> Someone has kindly emailed me some code for the polychoric 
> case (which he says is quite slow) I will try it out, it may 
> be better to treat the 2 by 2 case separately. If it is OK I 
> plan to write an equivalent to cor for matrices of 
> tetrachoric/polychoric. It will not happen any time soon now, 
> but if I do manage t would you be interested in it for sem? 
> As you say there remains the problem that they are estimates 
> and introduce further uncertainties.
> 
> 
> >Tetrachoric and polychoric correlations are estimates, and in 
> >subsequently estimating a SEM from these, one should take 
> that into account.
> 
> Michael Dewey
> m.dewey at iop.kcl.ac.uk
>



From murdoch at stats.uwo.ca  Tue Nov 30 03:53:02 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Mon, 29 Nov 2004 21:53:02 -0500
Subject: [R] Error using glm with poisson family and identity link
In-Reply-To: <x28y8kk2hv.fsf@biostat.ku.dk>
References: <41A5D781.70400@pigrecodata.net>
	<Pine.LNX.4.61.0411251535190.28898@gannet.stats>
	<41A614F3.4000102@pdf.com> <x2y8gpai20.fsf@biostat.ku.dk>
	<41A633FC.5010401@pdf.com> <x2u0rdac3k.fsf@biostat.ku.dk>
	<Pine.A41.4.61b.0411290840400.88302@homer09.u.washington.edu>
	<x28y8kk2hv.fsf@biostat.ku.dk>
Message-ID: <rsnnq0t0hfajlrtq4tlskh15pjefbrtpkc@4ax.com>

On 29 Nov 2004 18:07:40 +0100, Peter Dalgaard
<p.dalgaard at biostat.ku.dk> wrote:

> I had a situation where there was like
>a (virtual) maximum outside the boundary, and the algorithm would
>basically stay on the path to that peak, banging its little head
>into the same point of the wall repeatedly, so to speak.

I love that image:  the little optimizer that couldn't.

I hope the fortunes maintainer is listening...

Duncan Murdoch



From peterwyang at gmail.com  Tue Nov 30 04:07:27 2004
From: peterwyang at gmail.com (Peter Yang)
Date: Mon, 29 Nov 2004 22:07:27 -0500
Subject: [R] How to plot a 3D picture
Message-ID: <003801c4d689$ba7cef30$3e02a8c0@Peter>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041129/e3816a85/attachment.pl

From deepayan at stat.wisc.edu  Tue Nov 30 05:25:51 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Mon, 29 Nov 2004 22:25:51 -0600
Subject: [R] How to plot a 3D picture
In-Reply-To: <003801c4d689$ba7cef30$3e02a8c0@Peter>
References: <003801c4d689$ba7cef30$3e02a8c0@Peter>
Message-ID: <200411292225.51442.deepayan@stat.wisc.edu>

On Monday 29 November 2004 21:07, Peter Yang wrote:
> Hi,
>
> Suppose I have a 4 by 4 matrix as following:
>
> 10 6 3 1
>
>  8 4 3 2
>
>  6 2 4 3
>
>  9 3 4 2
>
> The x axis is the column index of the matrix, the y axis is the row
> index of the matrix and the z axis is the value in the corresponding
> position of the matrix.
>
> I tried to use contour and image. They are not the ones I want.

You are probably looking for ?persp.

Deepayan



From dhinds at sonic.net  Tue Nov 30 05:39:55 2004
From: dhinds at sonic.net (dhinds@sonic.net)
Date: Tue, 30 Nov 2004 04:39:55 +0000 (UTC)
Subject: [R] New trellis settings
Message-ID: <cogtiq$ovm$1@sea.gmane.org>

I've just upgraded to 2.0.1 and was taken by surprise by the changes
in graphical parameter handling for lattice plots.  I'd previously
been using 1.9.1.  The old settings seem to have been replaced by a
daunting number of new options.  I've poked around a bit and have not
seen any discussion of the changes in the newsletter or on r-help but
maybe I'm overlooking something?

Specifically, I have some code that in the past changed the relative
proportions of text versus the plot area using:

  trellis.par.set('fontsize', list(text=8))

which caused all text to get smaller, and the plot area to grow to
fill the larger available space.  Now, the plot area does not grow on
its own.  I've tried fiddling with layout.heights, layout.widths, and
axis.components but there are dozens of settings available.  For many
of these settings, I have no idea what quantity I'm actually changing,
other than by trial and error.  Is there an easier way?

-- Dave



From xxq5 at case.edu  Tue Nov 30 05:47:18 2004
From: xxq5 at case.edu (Xin Qi)
Date: Mon, 29 Nov 2004 23:47:18 -0500
Subject: [R] New trellis settings
In-Reply-To: <cogtiq$ovm$1@sea.gmane.org>
Message-ID: <5.1.1.6.0.20041129234501.02a473d0@mail.cwru.edu>

Hi, Dear all R users:

Does someone know whether R can calculate the Receiver Operating 
Characteristic (ROC) Curves? I didn't find it from the packages.

Thanks a lot.

--- Xin



From xxq5 at case.edu  Tue Nov 30 05:49:08 2004
From: xxq5 at case.edu (Xin Qi)
Date: Mon, 29 Nov 2004 23:49:08 -0500
Subject: [R] About ROC curves
Message-ID: <5.1.1.6.0.20041129234835.02a3fec0@mail.cwru.edu>

Hi, Dear all R users:

Does someone know whether R can calculate the Receiver Operating 
Characteristic (ROC) Curves? I didn't find it from the packages.

Thanks a lot.

--- Xin



From deepayan at stat.wisc.edu  Tue Nov 30 06:23:35 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Mon, 29 Nov 2004 23:23:35 -0600
Subject: [R] New trellis settings
In-Reply-To: <cogtiq$ovm$1@sea.gmane.org>
References: <cogtiq$ovm$1@sea.gmane.org>
Message-ID: <200411292323.36010.deepayan@stat.wisc.edu>

On Monday 29 November 2004 22:39, dhinds at sonic.net wrote:
> I've just upgraded to 2.0.1 and was taken by surprise by the changes
> in graphical parameter handling for lattice plots.  I'd previously
> been using 1.9.1.  The old settings seem to have been replaced by a
> daunting number of new options.  

Not really. The old settings are essentially what they were, the new 
'options' are for things that used to be hard-coded in previous 
versions.

> I've poked around a bit and have not 
> seen any discussion of the changes in the newsletter or on r-help but
> maybe I'm overlooking something?
> 
> Specifically, I have some code that in the past changed the relative
> proportions of text versus the plot area using:
>
>   trellis.par.set('fontsize', list(text=8))
>
> which caused all text to get smaller, and the plot area to grow to
> fill the larger available space.  Now, the plot area does not grow on
> its own.  

That does seem to have been a side-effect in 1.9.1, but it was never 
intended. I would consider that behaviour a bug, not a feature.

> I've tried fiddling with layout.heights, layout.widths, and 
> axis.components but there are dozens of settings available.  For many
> of these settings, I have no idea what quantity I'm actually
> changing, other than by trial and error.  Is there an easier way?

Not really. The recommended thing to change is the settings (not the 
options) which all default to 1:

> str(trellis.par.get()$layout.heights)
List of 18
 $ top.padding      : num 1
 $ main             : num 1
 $ main.key.padding : num 1
 $ key.top          : num 1
 $ key.axis.padding : num 1
 ...

I was hoping that the names would be enough of a hint. Anything with 
'padding' in the name is space, and probably the ones you want to 
change.

Deepayan



From christoph.lehmann at gmx.ch  Tue Nov 30 06:33:35 2004
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Tue, 30 Nov 2004 06:33:35 +0100
Subject: [R] About ROC curves
In-Reply-To: <5.1.1.6.0.20041129234835.02a3fec0@mail.cwru.edu>
References: <5.1.1.6.0.20041129234835.02a3fec0@mail.cwru.edu>
Message-ID: <41AC062F.3090902@gmx.ch>

package ROC from bioconductor, eg:

http://www.bioconductor.org/repository/release1.5/package/Win32/

Cheers!
Christoph

Xin Qi wrote:
> Hi, Dear all R users:
> 
> Does someone know whether R can calculate the Receiver Operating 
> Characteristic (ROC) Curves? I didn't find it from the packages.
> 
> Thanks a lot.
> 
> --- Xin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From bxc at steno.dk  Tue Nov 30 08:26:08 2004
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Tue, 30 Nov 2004 08:26:08 +0100
Subject: [R] About ROC curves
Message-ID: <0ABD88905D18E347874E0FB71C0B29E90237D4C8@exdkba022.novo.dk>

There is a package, Lexis, not officil though, which contains
a function ROC (and some other stuff for epidemiology). 
You can find it in:
http://biostat.ku.dk/~bxc/SPE/library/

Bendix Carstensen
----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc
----------------------



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Xin Qi
> Sent: Tuesday, November 30, 2004 5:49 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] About ROC curves
> 
> 
> Hi, Dear all R users:
> 
> Does someone know whether R can calculate the Receiver Operating 
> Characteristic (ROC) Curves? I didn't find it from the packages.
> 
> Thanks a lot.
> 
> --- Xin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Tue Nov 30 09:14:55 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 30 Nov 2004 09:14:55 +0100
Subject: [R] Re: Reasons not to answer very basic questions...
In-Reply-To: <20041129192247.YRQE24735.smta04.mail.ozemail.net@there>
References: <20041129192247.YRQE24735.smta04.mail.ozemail.net@there>
Message-ID: <41AC2BFF.2010407@statistik.uni-dortmund.de>

Jim Lemon wrote:

> A.J. Rossini wrote:
> 
>>and perhaps the most important reason for the particular socratic form
>>of teaching on this list...
> 
> 
> Golly, anyone who read Plato's Dialogues would realize that the Socratic 
> method involves patiently leading the questioner stepwise through the 
> solution, not simply writing RTFMeno.
> 
> Jim
> 

Jim, yes, that way works as long as the number of questioners does not 
sum up to "a lot":
It is "easy" to teach courses for 20 people and answer individual 
questions, even if these questions are very basic and have been 
explained more than once during the course.
Courses for 700 people (e.g. basic statistics for economists) in the 
biggest auditory aren't that funny and you have to say "read what 
I/others have written!" - or next month almost each student will be in 
your office asking question he/she could not work out in half an hour 
him/herself ...

So it is my very serious opinion that we need to point people to the 
basic documentation and the FAQ, if they post their first very basic 
question to the list. If the questioner asks the next basic questions as 
well, and with him many others, we will have a flood of many hundreds of 
messages a day!

Uwe



From Yan.Zhao at cea.fr  Tue Nov 30 09:16:35 2004
From: Yan.Zhao at cea.fr (yan zhao)
Date: Tue, 30 Nov 2004 09:16:35 +0100
Subject: [R] about cancor.R
Message-ID: <41AC2C63.1040205@dsm-mail.cea.fr>

Hello,
I'm a beginning user of R, now I have a question about canonical correlation analysis (cca). 
In R,there is a function "cancor.R" used for cca; For example X(n*p1) and Y(n*p2)are the two matrix to be analyzed. In the example given by R, when n> max(p1,n2), cancor(X,Y) works; but when n<p1 or n<p2, cancor(X,Y) doesn't work well because cancor$cor == 1; how to cope with this case? maybe before apply the two matrix to cancor.R, some time-space transformation should be done? could you give some suggestion? Thanks you in advance!
Best regards,
Yan ZHAO



From ripley at stats.ox.ac.uk  Tue Nov 30 09:35:17 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 30 Nov 2004 08:35:17 +0000 (GMT)
Subject: [R] about cancor.R
In-Reply-To: <41AC2C63.1040205@dsm-mail.cea.fr>
References: <41AC2C63.1040205@dsm-mail.cea.fr>
Message-ID: <Pine.LNX.4.61.0411300828000.6939@gannet.stats>

On Tue, 30 Nov 2004, yan zhao wrote:

> Hello,
> I'm a beginning user of R, now I have a question about canonical correlation 
> analysis (cca). In R,there is a function "cancor.R" used for cca; For example

There is no such function in R, and I can't find it in any add-on package.
Do you mean the function cancor() in the stats package?

> X(n*p1) and Y(n*p2)are the two matrix to be analyzed. In the example given by 
> R, when n> max(p1,n2), cancor(X,Y) works; but when n<p1 or n<p2, cancor(X,Y) 
> doesn't work well because cancor$cor == 1; how to cope with this case? maybe 
> before apply the two matrix to cancor.R, some time-space transformation 
> should be done? could you give some suggestion? Thanks you in advance!

You need to understand the underlying theory: the problem is not with the 
function but your use of the technique.  Assuming non-degenerate matrices, 
you have a situation in which one of the matrices spans the entire space 
of observations and so imposes no restriction.

R help pages come with references: please consult them.  Since you have 
not told us why you are using CCA, we can't help you with the 
methodological questions, even if they are off topic for the R-help list.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From contact at thomasalmer.com  Tue Nov 30 09:34:02 2004
From: contact at thomasalmer.com (contact@thomasalmer.com)
Date: Tue, 30 Nov 2004 09:34:02 +0100
Subject: =?iso-8859-1?Q?Re:_Re:_[R]_systemfit_-_SUR?=
Message-ID: <27129039$110180207841ac2a5e5d2635.18302846@config6.schlund.de>


Arne Henningsen <ahenningsen at email.uni-kiel.de> schrieb am 29.11.2004,
17:02:12:
> On Monday 29 November 2004 16:42, contact at thomasalmer.com wrote:
> > Hello to everyone,
> >
> > I have 2 problems and would be very pleased if anyone can help me:
> >
> > 1) When I use the package "systemfit" for SUR regressions, I get two
> > different variance-covariance matrices when I firstly do the SUR
> > regression ("The covariance matrix of the residuals used for
> > estimation") and secondly do the OLS regressions. In the manual for
> > "systemfit" on page 14 I see however, that the variance-covariance
> > matrix for SUR is obtained from OLS. How can this be explained?
> 
> Hi Thomas,
> I get identical residual covariance matrices:
> 
> R> library(systemfit)
> R> data( kmenta )
> R> demand  supply  labels  system  fitols  fitols$rcov
>          [,1]     [,2]
> [1,] 3.725391 4.136963
> [2,] 4.136963 5.784441
> R> fitsur  fitsur$rcovest
>          [,1]     [,2]
> [1,] 3.725391 4.136963
> [2,] 4.136963 5.784441
> 
It is a pity, but my matrices are not as nice :-(
An excerpt:
fitsur$rcovest
            [,1]         [,2]         [,3]       ...
 [1,] 0.015097517  0.018005050 
 [2,] 0.018005050  0.276259834
 ...

fitols$rcov
               [,1]          [,2] 	[,3]	 ... 
 [1,]  1.010326e-02  0.0096103837  
 [2,]  9.610384e-03  0.2329884378  
 ...

fitsur "The covariance matrix of the residuals used for estimation":
    eq1         eq2        eq3         ...  
eq1 0.01317429  0.01504719 0.007981307  
eq2 0.01504719  0.25233860 
...

fitols "The covariance matrix of the residuals":
    eq1          eq2          eq3      ...
eq1 9.51154e-03  0.009137884  0.002648577  
eq2 9.13788e-03  0.220435063  
...

By the way: Why are the figures larger for SUR?
> Did you do _iterated_ SUR?
> 
Yes:
"systemfit results 
method: iterated SUR 
convergence achieved after 30 iterations"

I do not know how to change that. 

> Best wishes,
> Arne
THANKS A LOT FOR YOUR IMMEDIATE HELP!!!

> 
> > 2) Is there an easy possibility to test a) the OLS equations, and b) the
> > SUR system for SUR structures? In other words: Is the LM-Test from
> > Breusch and Pagan available in R?
> >
> > Thanks for the attention!
> >
> > Best Regards,
> > Thomas Almer
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 
> -- 
> Arne Henningsen
> Department of Agricultural Economics
> University of Kiel
> Olshausenstr. 40
> D-24098 Kiel (Germany)
> Tel: +49-431-880 4445
> Fax: +49-431-880 1397
> ahenningsen at agric-econ.uni-kiel.de
> http://www.uni-kiel.de/agrarpol/ahenningsen/



From kbrinkm at ump.gwdg.de  Tue Nov 30 09:42:53 2004
From: kbrinkm at ump.gwdg.de (Kevin Brinkmann)
Date: Tue, 30 Nov 2004 09:42:53 +0100 (CET)
Subject: [R] Problem with print() and backslashes.
Message-ID: <Pine.LNX.4.53.0411300934510.11919@mp71.material.physik.uni-goettingen.de>

Dear R List

I have a small problem concerning the output of print().

My version:

> R.version
         _
platform i386-portbld-freebsd5.2
arch     i386
os       freebsd5.2
system   i386, freebsd5.2
status
major    1
minor    9.0
year     2004
month    04
day      12
language R

Consider this: I want to print a backslash with an exclamation mark. Here
is the output.

> print( "\!" )
[1] "!"

Now I try it differently...

> print( "\\!" )
[1] "\\!"

The output contains two backslashes. Why?

Regards,

Kevin



From ripley at stats.ox.ac.uk  Tue Nov 30 10:00:32 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 30 Nov 2004 09:00:32 +0000 (GMT)
Subject: [R] Problem with print() and backslashes.
In-Reply-To: <Pine.LNX.4.53.0411300934510.11919@mp71.material.physik.uni-goettingen.de>
References: <Pine.LNX.4.53.0411300934510.11919@mp71.material.physik.uni-goettingen.de>
Message-ID: <Pine.LNX.4.61.0411300859450.7783@gannet.stats>

On Tue, 30 Nov 2004, Kevin Brinkmann wrote:

> Dear R List
>
> I have a small problem concerning the output of print().
>
> My version:
>
>> R.version
>         _
> platform i386-portbld-freebsd5.2
> arch     i386
> os       freebsd5.2
> system   i386, freebsd5.2
> status
> major    1
> minor    9.0
> year     2004
> month    04
> day      12
> language R
>
> Consider this: I want to print a backslash with an exclamation mark. Here
> is the output.
>
>> print( "\!" )
> [1] "!"
>
> Now I try it differently...
>
>> print( "\\!" )
> [1] "\\!"
>
> The output contains two backslashes. Why?

Because it is documented to escape backslashes: use cat() if that is not 
what you want.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Tue Nov 30 09:59:52 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Nov 2004 09:59:52 +0100
Subject: [R] Problem with print() and backslashes.
In-Reply-To: <Pine.LNX.4.53.0411300934510.11919@mp71.material.physik.uni-goettingen.de>
References: <Pine.LNX.4.53.0411300934510.11919@mp71.material.physik.uni-goettingen.de>
Message-ID: <x2y8gjn247.fsf@biostat.ku.dk>

Kevin Brinkmann <kbrinkm at ump.gwdg.de> writes:

> Consider this: I want to print a backslash with an exclamation mark. Here
> is the output.
> 
> > print( "\!" )
> [1] "!"
> 
> Now I try it differently...
> 
> > print( "\\!" )
> [1] "\\!"
> 
> The output contains two backslashes. Why?

(Didn't we do that one only yesterday?)

The answer is: For the same reason that you need them on input. R
likes to line things up in columns when printing vectors so cannot
just print special characters like newline, carriage return, etc.
Instead it represents them as \n, \r just like on input. To
distinguish from backslash-followed-by-n the backslash is itself
escaped with a backslash. Use cat() to output a string as raw
characters. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From hb at maths.lth.se  Tue Nov 30 10:10:58 2004
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Tue, 30 Nov 2004 10:10:58 +0100
Subject: [R] Problem with print() and backslashes.
In-Reply-To: <Pine.LNX.4.53.0411300934510.11919@mp71.material.physik.uni-goettingen.de>
Message-ID: <006201c4d6bc$81608a80$e502eb82@hblaptop>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Kevin Brinkmann
> Sent: Tuesday, November 30, 2004 9:43 AM
> To: R-help at stat.math.ethz.ch
> Subject: [R] Problem with print() and backslashes.
> 
> 
> Dear R List
> 
> I have a small problem concerning the output of print().
>
<snip></snip> 
> 
> Consider this: I want to print a backslash with an
> exclamation mark. Here is the output.
> 
> > print( "\!" )
> [1] "!"
> 
> Now I try it differently...
> 
> > print( "\\!" )
> [1] "\\!"
> 
> The output contains two backslashes. Why?

> cat("\\!")
\!

also

> str("\\!")
 chr "\!"


What is important to keep in mind is that the string "\\!" has *two*
characters (when read by R), not three (as on the screen or in your editor);

> nchar("\\!")
[1] 2

The first character is "\\", which needs to be escaped in order for the R
parser to recognize it as '\'. The second is of course '!'. I agree that

> nchar("\!")
[1] 1

might be confusing. The thing is that some characters remain the same
escaped or not, whereas others have certain mappings to non-printable ASCII
codes (0-255).

> identical("\!", "!")
[1] TRUE

and the well known(?) newline character

> identical("\n", "n")
[1] FALSE

Another good example:
> identical("\"", '"')
[1] TRUE

Given a string, print() gives you the escaped version of the string, which
can be useful for debugging, if you want to cut'n'paste and so on. Thus,

> print("Hello world\n!\n")
[1] "Hello world\n!\n"

and

> cat("Hello world\n!\n")
Hello world
!

Hope this helps!

Henrik Bengtsson

> Regards,
> 
> Kevin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From brostaux.y at fsagx.ac.be  Tue Nov 30 10:12:28 2004
From: brostaux.y at fsagx.ac.be (Yves Brostaux)
Date: Tue, 30 Nov 2004 10:12:28 +0100
Subject: [R] Creating a factor from a combination of vectors
Message-ID: <41AC397C.7020404@fsagx.ac.be>

Dear list,

Here's a little problem I already solved with my own coding style, but I 
feel there is a more efficient and cleaner way to write it, but had no 
success finding the "clever" solution.

I want to produce a factor from a subset of the combination of two 
vectors. I have the vectors a et b in a data-frame :

 > df <- expand.grid(a=c(0, 5, 10, 25, 50), b=c(0, 25, 50, 100, 200))
 > fac.df
    a   b
1   0   0
2   5   0
3  10   0
4  25   0
5  50   0
6   0  25
7   5  25
<snip>

and want to create a factor which levels correspond to particular 
combinations of a and b (let's say Low for a=0 & b=0, Medium for a=10 & 
b=50, High for a=50 & b=200, others levels set to NA), reading them from 
a data-frame which describes the desired subset and corresponding levels.

Here's my own solution (inputs are data-frames df and cas, output is the 
sub factor):

 > cas <- as.data.frame(matrix(c(0, 10,50, 0, 50, 200), 3, 
2,dimnames=list(c("Low", "Medium", "High"), c("a", "b"))))
 > cas
        a   b
Low     0   0
Medium 10  50
High   50 200

 > sub <- character(length(df$a))
 > for (i in 1:length(df$a)) {
+   temp <- rownames(cas)[cas$a==df$a[i] & cas$b==df$b[i]]
+   sub[i] <- ifelse(length(temp)>0, temp, NA)
+ }
 > sub <- ordered(sub, levels=c("Low", "Medium", "High"))
 > sub
 [1] Low    <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   
<NA>   <NA>   <NA>   Medium <NA>   <NA>   <NA>   <NA> 
[18] <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   High 
Levels: Low < Medium < High

I was looking for a vectorized solution (apply style) binding 
data-frames df and cas, but didn't succeed avoiding the for loop. Could 
anybody bring me the ligths over the darkness of my ignorance ? Thank 
you very much in advance.

-- 
Ir. Yves BROSTAUX
Unit?? de Statistique et Informatique
Facult?? universitaire des Sciences agronomiques de Gembloux (FUSAGx)
8, avenue de la Facult??
B-5030 Gembloux
Belgique
T??l: +32 81 62 24 69
Email: brostaux.y at fsagx.ac.be



From ahenningsen at email.uni-kiel.de  Tue Nov 30 10:20:29 2004
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Tue, 30 Nov 2004 10:20:29 +0100
Subject: [R] systemfit - SUR
In-Reply-To: <27129039$110180207841ac2a5e5d2635.18302846@config6.schlund.de>
References: <27129039$110180207841ac2a5e5d2635.18302846@config6.schlund.de>
Message-ID: <200411301020.29165.ahenningsen@email.uni-kiel.de>

On Tuesday 30 November 2004 09:34, contact at thomasalmer.com wrote:
> Arne Henningsen <ahenningsen at email.uni-kiel.de> schrieb am 29.11.2004,
>
> 17:02:12:
> > On Monday 29 November 2004 16:42, contact at thomasalmer.com wrote:
> > > Hello to everyone,
> > >
> > > I have 2 problems and would be very pleased if anyone can help me:
> > >
> > > 1) When I use the package "systemfit" for SUR regressions, I get two
> > > different variance-covariance matrices when I firstly do the SUR
> > > regression ("The covariance matrix of the residuals used for
> > > estimation") and secondly do the OLS regressions. In the manual for
> > > "systemfit" on page 14 I see however, that the variance-covariance
> > > matrix for SUR is obtained from OLS. How can this be explained?
> >
> > Hi Thomas,
> > I get identical residual covariance matrices:
> >
> > R> library(systemfit)
> > R> data( kmenta )
> > R> demand  supply  labels  system  fitols  fitols$rcov
> >          [,1]     [,2]
> > [1,] 3.725391 4.136963
> > [2,] 4.136963 5.784441
> > R> fitsur  fitsur$rcovest
> >          [,1]     [,2]
> > [1,] 3.725391 4.136963
> > [2,] 4.136963 5.784441
>
> It is a pity, but my matrices are not as nice :-(

Please show how you obtained these results. 

This is what I did:
R> data( kmenta )
R> demand <- q ~ p + d
R> supply <- q ~ p + f + a
R> labels <- list( "demand", "supply" )
R> system <- list( demand, supply )
R>
R> # OLS estimation:
R> fitols <- systemfit("OLS", system, labels, data=kmenta )
R> # (non-iterated) SUR estimation
R> fitsur <- systemfit("SUR", system, labels, data=kmenta )
R> iterated SUR estimation
R> fitsurit <- systemfit("SUR", system, labels, data=kmenta, maxit=100 )
R>
R> fitols$rcov
         [,1]     [,2]
[1,] 3.725391 4.136963
[2,] 4.136963 5.784441
R> fitsur$rcovest
         [,1]     [,2]
[1,] 3.725391 4.136963
[2,] 4.136963 5.784441
R> fitsurit$rcovest
         [,1]     [,2]
[1,] 6.199071 7.493383
[2,] 7.493383 9.128547


> An excerpt:
> fitsur$rcovest
>             [,1]         [,2]         [,3]       ...
>  [1,] 0.015097517  0.018005050
>  [2,] 0.018005050  0.276259834
>  ...
>
> fitols$rcov
>                [,1]          [,2] 	[,3]	 ...
>  [1,]  1.010326e-02  0.0096103837
>  [2,]  9.610384e-03  0.2329884378
>  ...
>
> fitsur "The covariance matrix of the residuals used for estimation":
>     eq1         eq2        eq3         ...
> eq1 0.01317429  0.01504719 0.007981307
> eq2 0.01504719  0.25233860
> ...
>
> fitols "The covariance matrix of the residuals":
>     eq1          eq2          eq3      ...
> eq1 9.51154e-03  0.009137884  0.002648577
> eq2 9.13788e-03  0.220435063
> ...
>
> By the way: Why are the figures larger for SUR?

OLS minimizes the residuals and, thus, also the variance of the residuals 
(=diagonal of the residual covariance matrix).
Iterated SUR is equivalent to a maximum likelihood estimation. Maximizing the 
likelihood value is equivalent to minimizing the determinant of the residual 
covariance matrix. Thus, the determinant of the residual covariance matrix 
and not the residuals itself are minimized:

R> det(fitols$rcov)
[1] 4.434845
R> det(fitsurit$rcov)
[1] 0.4376941
R> det(fitsurit$rcovest)
[1] 0.4377184

> > Did you do _iterated_ SUR?
>
> Yes:
> "systemfit results
> method: iterated SUR
> convergence achieved after 30 iterations"

If you use iterated SUR, the SUR estimations are iterated. In the first SUR 
estimation the residual covariance matrix of the OLS estimation is used. In 
all following iterations the residual covariance matrix of the previous step 
SUR estimation is used. 

> I do not know how to change that.

Please read the documentation. It says to set argument "maxit" to 1 - or do 
not provide this argument, since 1 is the default.

> > Best wishes,
> > Arne
>
> THANKS A LOT FOR YOUR IMMEDIATE HELP!!!
>
> > > 2) Is there an easy possibility to test a) the OLS equations, and b)
> > > the SUR system for SUR structures? In other words: Is the LM-Test from
> > > Breusch and Pagan available in R?

I don't understand what you want to test. Does the hausman test what you are 
looking for (see ?hausman.systemfit). If you have questions regarding this 
test, you might ask my co-author of systemfit, Jeff Hamann.

Best wishes,
Arne

> > > Thanks for the attention!
> > >
> > > Best Regards,
> > > Thomas Almer
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> >
> > --
> > Arne Henningsen
> > Department of Agricultural Economics
> > University of Kiel
> > Olshausenstr. 40
> > D-24098 Kiel (Germany)
> > Tel: +49-431-880 4445
> > Fax: +49-431-880 1397
> > ahenningsen at agric-econ.uni-kiel.de
> > http://www.uni-kiel.de/agrarpol/ahenningsen/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From contact at thomasalmer.com  Tue Nov 30 10:50:01 2004
From: contact at thomasalmer.com (contact@thomasalmer.com)
Date: Tue, 30 Nov 2004 10:50:01 +0100
Subject: =?iso-8859-1?Q?Re:_Re:_[R]_systemfit_-_SUR?=
Message-ID: <27129039$110180804441ac41acef00a7.13238566@config18.schlund.de>


Arne Henningsen <ahenningsen at email.uni-kiel.de> schrieb am 30.11.2004,
10:20:29:
> On Tuesday 30 November 2004 09:34, contact at thomasalmer.com wrote:
> > Arne Henningsen  schrieb am 29.11.2004,
> >
> > 17:02:12:
> > > On Monday 29 November 2004 16:42, contact at thomasalmer.com wrote:
> > > > Hello to everyone,
> > > >
> > > > I have 2 problems and would be very pleased if anyone can help me:
> > > >
> > > > 1) When I use the package "systemfit" for SUR regressions, I get two
> > > > different variance-covariance matrices when I firstly do the SUR
> > > > regression ("The covariance matrix of the residuals used for
> > > > estimation") and secondly do the OLS regressions. In the manual for
> > > > "systemfit" on page 14 I see however, that the variance-covariance
> > > > matrix for SUR is obtained from OLS. How can this be explained?
> > >
> > > Hi Thomas,
> > > I get identical residual covariance matrices:
> > >
> > > R> library(systemfit)
> > > R> data( kmenta )
> > > R> demand  supply  labels  system  fitols  fitols$rcov
> > >          [,1]     [,2]
> > > [1,] 3.725391 4.136963
> > > [2,] 4.136963 5.784441
> > > R> fitsur  fitsur$rcovest
> > >          [,1]     [,2]
> > > [1,] 3.725391 4.136963
> > > [2,] 4.136963 5.784441
> >
> > It is a pity, but my matrices are not as nice :-(
> 
> Please show how you obtained these results. 
I did not provide the steps, because the data is not public. But that is
what I did after defining the system and labels (like you & the
documentation):
fitols<-systemfit("OLS", system, labels)
fitsur<-systemfit("SUR", system, labels, maxit=100)

But after all I think your answers are sufficient. Thanks once again for
your support. R and the community are really excellent and a big thread
to competitors! 

> This is what I did:
> R> data( kmenta )
> R> demand  supply  labels  system 
> R> # OLS estimation:
> R> fitols  # (non-iterated) SUR estimation
> R> fitsur  iterated SUR estimation
> R> fitsurit 
> R> fitols$rcov
>          [,1]     [,2]
> [1,] 3.725391 4.136963
> [2,] 4.136963 5.784441
> R> fitsur$rcovest
>          [,1]     [,2]
> [1,] 3.725391 4.136963
> [2,] 4.136963 5.784441
> R> fitsurit$rcovest
>          [,1]     [,2]
> [1,] 6.199071 7.493383
> [2,] 7.493383 9.128547
> 
> 
> > An excerpt:
> > fitsur$rcovest
> >             [,1]         [,2]         [,3]       ...
> >  [1,] 0.015097517  0.018005050
> >  [2,] 0.018005050  0.276259834
> >  ...
> >
> > fitols$rcov
> >                [,1]          [,2] 	[,3]	 ...
> >  [1,]  1.010326e-02  0.0096103837
> >  [2,]  9.610384e-03  0.2329884378
> >  ...
> >
> > fitsur "The covariance matrix of the residuals used for estimation":
> >     eq1         eq2        eq3         ...
> > eq1 0.01317429  0.01504719 0.007981307
> > eq2 0.01504719  0.25233860
> > ...
> >
> > fitols "The covariance matrix of the residuals":
> >     eq1          eq2          eq3      ...
> > eq1 9.51154e-03  0.009137884  0.002648577
> > eq2 9.13788e-03  0.220435063
> > ...
> >
> > By the way: Why are the figures larger for SUR?
> 
> OLS minimizes the residuals and, thus, also the variance of the residuals 
> (=diagonal of the residual covariance matrix).
> Iterated SUR is equivalent to a maximum likelihood estimation. Maximizing the 
> likelihood value is equivalent to minimizing the determinant of the residual 
> covariance matrix. Thus, the determinant of the residual covariance matrix 
> and not the residuals itself are minimized:
> 
> R> det(fitols$rcov)
> [1] 4.434845
> R> det(fitsurit$rcov)
> [1] 0.4376941
> R> det(fitsurit$rcovest)
> [1] 0.4377184
> 
> > > Did you do _iterated_ SUR?
> >
> > Yes:
> > "systemfit results
> > method: iterated SUR
> > convergence achieved after 30 iterations"
> 
> If you use iterated SUR, the SUR estimations are iterated. In the first SUR 
> estimation the residual covariance matrix of the OLS estimation is used. In 
> all following iterations the residual covariance matrix of the previous step 
> SUR estimation is used. 
> 
> > I do not know how to change that.
> 
> Please read the documentation. It says to set argument "maxit" to 1 - or do 
> not provide this argument, since 1 is the default.
> 
> > > Best wishes,
> > > Arne
> >
> > THANKS A LOT FOR YOUR IMMEDIATE HELP!!!
> >
> > > > 2) Is there an easy possibility to test a) the OLS equations, and b)
> > > > the SUR system for SUR structures? In other words: Is the LM-Test from
> > > > Breusch and Pagan available in R?
> 
> I don't understand what you want to test. Does the hausman test what you are 
> looking for (see ?hausman.systemfit). If you have questions regarding this 
> test, you might ask my co-author of systemfit, Jeff Hamann.
Primarily I want to test if the variance covariance matrix of the OLS
residuals is diagonal. But this can be done manually, of course. 


> Best wishes,
> Arne
> 
> > > > Thanks for the attention!
> > > >
> > > > Best Regards,
> > > > Thomas Almer
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://stat.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide!
> > > > http://www.R-project.org/posting-guide.html
> > >
> > > --
> > > Arne Henningsen
> > > Department of Agricultural Economics
> > > University of Kiel
> > > Olshausenstr. 40
> > > D-24098 Kiel (Germany)
> > > Tel: +49-431-880 4445
> > > Fax: +49-431-880 1397
> > > ahenningsen at agric-econ.uni-kiel.de
> > > http://www.uni-kiel.de/agrarpol/ahenningsen/
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 
> -- 
> Arne Henningsen
> Department of Agricultural Economics
> University of Kiel
> Olshausenstr. 40
> D-24098 Kiel (Germany)
> Tel: +49-431-880 4445
> Fax: +49-431-880 1397
> ahenningsen at agric-econ.uni-kiel.de
> http://www.uni-kiel.de/agrarpol/ahenningsen/



From stu44414 at mail.uni-kiel.de  Tue Nov 30 11:06:59 2004
From: stu44414 at mail.uni-kiel.de (Andreas Franke)
Date: Tue, 30 Nov 2004 11:06:59 +0100
Subject: [R] plotting data in non-orthogonal coords.
In-Reply-To: <41AC4AEB.15710.955B5A@localhost>
References: <41AC4AEB.15710.955B5A@localhost>
Message-ID: <200411301106.59500.stu44414@mail.uni-kiel.de>

Am Dienstag, 30. November 2004 10:26 schrieb Petr Pikal:
> Hallo Andreas
>
> There is probably no one who can give you some answer as you
> did provide almost no facts what you really did and what is wrong.
> You probably need to transform your coordinates to orthogonal
> and plot them as you wish. But you have to do it yourself.
>
> I had some data in polar coordinates some time ago and I did the
> same (see attached picture).
>
> Cheers
> Petr
>
> On 29 Nov 2004 at 14:29, Andreas Franke wrote:
> > Hi !
> > I am wondering how to plot data (e.g.  f(x,y) ) in a coordinate system
> > spanned by two non-orthogonal basis vectors (e.g. hexagonal symmetry).
> > The data is given on an equally spaced grid in theses coords and i
> > would like to do a contour plot (e.g. with filled.contour).
> >
> > Thanks for your help. Andreas
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://stat.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
>
> Petr Pikal
> petr.pikal at precheza.cz

Hi !
Thanks for replying to my post and sorry for not being specific enough.
Maybe I am the one who didnt get the point , but as far as I understand R 
plots filled.contour(x,y,z,...) in the following way:

x,y define a grid in cartesian coords, i.e. the angle between x and y is 90??. 
So if I have data on such a grid I am fine.
My data is on an equally spaced grid in a coordsystem where x and y are at an 
angle of 60??. If you transform into cartesiancoords this isnt an equally 
spaced grid anymore.  You could interpret it as an equally spaced grid on 
which there is only data given on every second grid point.

It would be nice if one could just plot data given as F(x,y) where you supply 
x and y for every data point seperatly so that you dont need any specific 
grid.

I hope that someone can help.
Thanks in advance. Andreas



From ahenningsen at email.uni-kiel.de  Tue Nov 30 12:04:33 2004
From: ahenningsen at email.uni-kiel.de (Arne Henningsen)
Date: Tue, 30 Nov 2004 12:04:33 +0100
Subject: [R] systemfit - SUR
In-Reply-To: <27129039$110180804441ac41acef00a7.13238566@config18.schlund.de>
References: <27129039$110180804441ac41acef00a7.13238566@config18.schlund.de>
Message-ID: <200411301204.33609.ahenningsen@email.uni-kiel.de>

On Tuesday 30 November 2004 10:50, contact at thomasalmer.com wrote:
> [ . . .]
> > Please show how you obtained these results.
>
> I did not provide the steps, because the data is not public. 

A general remark:
In most cases a questioner can use the example data provided with a package or 
construct simple data, e.g. with matrix(1:9,3). See the posting guide, 
section "Examples". (If the surprising behavior does not appear with the 
sample data, the questioner has to take a deeper look in _his_ data.)

> But that is 
> what I did after defining the system and labels (like you & the
> documentation):
> fitols<-systemfit("OLS", system, labels)
> fitsur<-systemfit("SUR", system, labels, maxit=100)
>
> But after all I think your answers are sufficient. Thanks once again for
> your support. R and the community are really excellent and a big thread
> to competitors!
>
> [ . . . ]
> > > > > 2) Is there an easy possibility to test a) the OLS equations, and
> > > > > b) the SUR system for SUR structures? In other words: Is the
> > > > > LM-Test from Breusch and Pagan available in R?
> >
> > I don't understand what you want to test. Does the hausman test what you
> > are looking for (see ?hausman.systemfit). If you have questions regarding
> > this test, you might ask my co-author of systemfit, Jeff Hamann.
>
> Primarily I want to test if the variance covariance matrix of the OLS
> residuals is diagonal. But this can be done manually, of course.

Please provide the code and a reference to the test. Then, I will add this 
test to systemfit. This is how R works: useRs become developerR :-)

Best wishes,
Arne

-- 
Arne Henningsen
Department of Agricultural Economics
University of Kiel
Olshausenstr. 40
D-24098 Kiel (Germany)
Tel: +49-431-880 4445
Fax: +49-431-880 1397
ahenningsen at agric-econ.uni-kiel.de
http://www.uni-kiel.de/agrarpol/ahenningsen/



From pkhomski at wiwi.uni-bielefeld.de  Tue Nov 30 12:08:09 2004
From: pkhomski at wiwi.uni-bielefeld.de (Pavel Khomski)
Date: Tue, 30 Nov 2004 12:08:09 +0100
Subject: [R] lme in R-2.0.0: Problem with lmeControl
Message-ID: <41AC5499.9090100@wiwi.uni-bielefeld.de>

Hello!

One note/question hier about specification of control-parameters in the 
lme(...,control=list(...)) function call:

i tried to specify tne number of iteration needed via 
lme(....,control=list(maxIter=..., niterEM=...,msVerbose=TRUE))
but every time i change the defualt values maxIter (e.g. maxIter=1, 
niterEM=0)  on ones specified by me, the call returns all the iterations 
needed until it's converged.
and this is exactly the problem i will to get round.  (    e.g. in 
example on p.81 of Pinheiro/Bates,2000: 
fm1Rail.lme<-lme(...,control=list(maxIter=1,...))            )
so i have tried with option msMaxIter=... and this works.

The other problem is, that i even can not see (in R !!!) the output from 
iterations, despite the msVerbose=TRUE  specification and setting 
options(verbose=TRUE) (The S-plus can do it but also ignoring the 
maxIter=... specification)

Thank you for your hint



From petr.pikal at precheza.cz  Tue Nov 30 10:26:51 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 30 Nov 2004 10:26:51 +0100
Subject: [R] plotting data in non-orthogonal coords.
In-Reply-To: <200411291429.52432.stu44414@mail.uni-kiel.de>
Message-ID: <41AC4AEB.15710.955B5A@localhost>

Hallo Andreas

There is probably no one who can give you some answer as you 
did provide almost no facts what you really did and what is wrong. 
You probably need to transform your coordinates to orthogonal 
and plot them as you wish. But you have to do it yourself. 

I had some data in polar coordinates some time ago and I did the 
same (see attached picture). 

Cheers
Petr 


On 29 Nov 2004 at 14:29, Andreas Franke wrote:

> Hi !
> I am wondering how to plot data (e.g.  f(x,y) ) in a coordinate system
> spanned by two non-orthogonal basis vectors (e.g. hexagonal symmetry).
> The data is given on an equally spaced grid in theses coords and i
> would like to do a contour plot (e.g. with filled.contour).
> 
> Thanks for your help. Andreas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz


From lecoutre at stat.ucl.ac.be  Tue Nov 30 12:48:34 2004
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Tue, 30 Nov 2004 12:48:34 +0100
Subject: [R] Creating a factor from a combination of vectors
In-Reply-To: <41AC397C.7020404@fsagx.ac.be>
References: <41AC397C.7020404@fsagx.ac.be>
Message-ID: <6.0.1.1.2.20041130124634.0205e8e0@stat4ux.stat.ucl.ac.be>


Hi Yves,

Using your objects, here is a way:


 > cascombo=do.call("paste",c(cas,sep="."))
 > factor(do.call("paste",c(df,sep=".")),levels=cascombo,labels=rownames(cas))
[1] 
Low    <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA> 
  <NA>   Medium <NA>   <NA>
[16] <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   High
Levels: Low Medium High


It uses:
? paste (sep=.) to create the combinations ie 0.0, 10.50, etc.
? do.call to invoke the paste on the columns of the data.frames
? factor specifying existing levels (only those defined by cas data.frame) 
anbd labels

Eric



At 10:12 30/11/2004, Yves Brostaux wrote:
>Dear list,
>
>Here's a little problem I already solved with my own coding style, but I 
>feel there is a more efficient and cleaner way to write it, but had no 
>success finding the "clever" solution.
>
>I want to produce a factor from a subset of the combination of two 
>vectors. I have the vectors a et b in a data-frame :
>
> > df <- expand.grid(a=c(0, 5, 10, 25, 50), b=c(0, 25, 50, 100, 200))
> > fac.df
>    a   b
>1   0   0
>2   5   0
>3  10   0
>4  25   0
>5  50   0
>6   0  25
>7   5  25
><snip>
>
>and want to create a factor which levels correspond to particular 
>combinations of a and b (let's say Low for a=0 & b=0, Medium for a=10 & 
>b=50, High for a=50 & b=200, others levels set to NA), reading them from a 
>data-frame which describes the desired subset and corresponding levels.
>
>Here's my own solution (inputs are data-frames df and cas, output is the 
>sub factor):
>
> > cas <- as.data.frame(matrix(c(0, 10,50, 0, 50, 200), 3, 
> 2,dimnames=list(c("Low", "Medium", "High"), c("a", "b"))))
> > cas
>        a   b
>Low     0   0
>Medium 10  50
>High   50 200
>
> > sub <- character(length(df$a))
> > for (i in 1:length(df$a)) {
>+   temp <- rownames(cas)[cas$a==df$a[i] & cas$b==df$b[i]]
>+   sub[i] <- ifelse(length(temp)>0, temp, NA)
>+ }
> > sub <- ordered(sub, levels=c("Low", "Medium", "High"))
> > sub
>[1] Low    <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>
><NA>   <NA>   <NA>   Medium <NA>   <NA>   <NA>   <NA> [18] 
><NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   High Levels: Low < Medium 
>< High
>
>I was looking for a vectorized solution (apply style) binding data-frames 
>df and cas, but didn't succeed avoiding the for loop. Could anybody bring 
>me the ligths over the darkness of my ignorance ? Thank you very much in 
>advance.
>
>--
>Ir. Yves BROSTAUX
>Unit?? de Statistique et Informatique
>Facult?? universitaire des Sciences agronomiques de Gembloux (FUSAGx)
>8, avenue de la Facult??
>B-5030 Gembloux
>Belgique
>T??l: +32 81 62 24 69
>Email: brostaux.y at fsagx.ac.be
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Eric Lecoutre
UCL /  Institut de Statistique
Voie du Roman Pays, 20
1348 Louvain-la-Neuve
Belgium

tel: (+32)(0)10473050
lecoutre at stat.ucl.ac.be
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre

If the statistics are boring, then you've got the wrong numbers. -Edward 
Tufte



From ggrothendieck at myway.com  Tue Nov 30 12:55:15 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 30 Nov 2004 11:55:15 +0000 (UTC)
Subject: [R] Creating a factor from a combination of vectors
References: <41AC397C.7020404@fsagx.ac.be>
Message-ID: <loom.20041130T124405-528@post.gmane.org>

Yves Brostaux <brostaux.y <at> fsagx.ac.be> writes:

: 
: Dear list,
: 
: Here's a little problem I already solved with my own coding style, but I 
: feel there is a more efficient and cleaner way to write it, but had no 
: success finding the "clever" solution.
: 
: I want to produce a factor from a subset of the combination of two 
: vectors. I have the vectors a et b in a data-frame :
: 
:  > df <- expand.grid(a=c(0, 5, 10, 25, 50), b=c(0, 25, 50, 100, 200))
:  > fac.df
:     a   b
: 1   0   0
: 2   5   0
: 3  10   0
: 4  25   0
: 5  50   0
: 6   0  25
: 7   5  25
: <snip>
: 
: and want to create a factor which levels correspond to particular 
: combinations of a and b (let's say Low for a=0 & b=0, Medium for a=10 & 
: b=50, High for a=50 & b=200, others levels set to NA), reading them from 
: a data-frame which describes the desired subset and corresponding levels.
: 
: Here's my own solution (inputs are data-frames df and cas, output is the 
: sub factor):
: 
:  > cas <- as.data.frame(matrix(c(0, 10,50, 0, 50, 200), 3, 
: 2,dimnames=list(c("Low", "Medium", "High"), c("a", "b"))))
:  > cas
:         a   b
: Low     0   0
: Medium 10  50
: High   50 200
: 
:  > sub <- character(length(df$a))
:  > for (i in 1:length(df$a)) {
: +   temp <- rownames(cas)[cas$a==df$a[i] & cas$b==df$b[i]]
: +   sub[i] <- ifelse(length(temp)>0, temp, NA)
: + }
:  > sub <- ordered(sub, levels=c("Low", "Medium", "High"))
:  > sub
:  [1] Low    <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   
: <NA>   <NA>   <NA>   Medium <NA>   <NA>   <NA>   <NA> 
: [18] <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   <NA>   High 
: Levels: Low < Medium < High
: 
: I was looking for a vectorized solution (apply style) binding 
: data-frames df and cas, but didn't succeed avoiding the for loop. Could 
: anybody bring me the ligths over the darkness of my ignorance ? Thank 
: you very much in advance.
: 


Use interaction() and factor() like this:

factor( interaction(df), lev = c("0.0", "10.50", "50.200"),
  lab = c("Low", "Medium", "High"), ordered = TRUE)



From krcabrer at epm.net.co  Tue Nov 30 12:58:57 2004
From: krcabrer at epm.net.co (Kenneth)
Date: Tue, 30 Nov 2004 06:58:57 -0500
Subject: [R] A basic question
In-Reply-To: <20041129192247.YRQE24735.smta04.mail.ozemail.net@there>
References: <20041129192247.YRQE24735.smta04.mail.ozemail.net@there>
Message-ID: <opsh90wjhdezww6l@perseus.unalmed.edu.co>

Hi R users:

I want to know any experience compiling R in other LINUX distributions
besides FEDORA (Red Hat) or Mandrake, for example in BSD, Debian,
Gentoo, Slackware, vector LINUX, Knoppix, Yopper or CERN linux?

Hope this is not a "basic question"

Thank you for your help.

Kenneth



From svuwie at gmx.de  Tue Nov 30 12:59:39 2004
From: svuwie at gmx.de (Sven)
Date: Tue, 30 Nov 2004 12:59:39 +0100
Subject: [R] 2k-factorial design with 10 parameters
Message-ID: <41AC60AB.2040806@gmx.de>

Hi,

I'd like to apply a 2^k factorial design with k=10 parameters. Obviously 
this results in a quite long term for the model equation due to the high 
number of combinations of parameters.

How can I specify the equation for the linear model (lm) without writing 
all combinations explicitly down by hand? Does a R command exist for 
this problematic?

Thanks for your help in advance,
Sven



From ripley at stats.ox.ac.uk  Tue Nov 30 13:01:42 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 30 Nov 2004 12:01:42 +0000 (GMT)
Subject: [R] Building latest version of package
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E95E898F9@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E95E898F9@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <Pine.LNX.4.61.0411301157170.13214@gannet.stats>

On Mon, 29 Nov 2004, michael watson (IAH-C) wrote:

> Hi
>
> I have a package which was built using R 1.9.1 and everything worked
> fine.  I recently upgraded to R 2.0.1 and tried to re-install my package
> - and I got:
>
> Error in library(mypackage) : 'mypackage' is not a valid package --
> installed < 2.0.0?

I don't think you re-installed it, rather unzipped the zip file.

> So I tried rebuilding it using my new version of R:
>
> R CMD BUILD --binary mypackage
>
> hhc: not found
> cp: cannot stat `mypackage.chm': No such file or directory
> make[1]: *** [chm-mypackage] Error 1
> make: *** [pkg-mypackage] Error 2
> *** Installation of mypackage failed ***
>
> Removing 'f:/tmp/Rbuild.2972/mypackage'
> ERROR
> * installation failed
>
> I didn't have these problems before.  What is "hhc" and why can't R find
> it?

README.packages says what it is, and R is not looking for it, but make is.
Probably because you didn't check the checklist in README.packages.  This 
is not a new requirement.

> In general, will I have to re-build my package everytime a new version
> of R is released?

No.  But we are talking about installing, not building, here.  Please look 
in `Writing R Extensions' to see the difference.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From petr.pikal at precheza.cz  Tue Nov 30 13:13:26 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 30 Nov 2004 13:13:26 +0100
Subject: [R] plotting data in non-orthogonal coords.
In-Reply-To: <200411301106.59500.stu44414@mail.uni-kiel.de>
References: <41AC4AEB.15710.955B5A@localhost>
Message-ID: <41AC71F6.15899.12DDE03@localhost>



On 30 Nov 2004 at 11:06, Andreas Franke wrote:

<snip>
> 
> Hi !
> Thanks for replying to my post and sorry for not being specific
> enough. Maybe I am the one who didnt get the point , but as far as I
> understand R plots filled.contour(x,y,z,...) in the following way:
> 
> x,y define a grid in cartesian coords, i.e. the angle between x and y
> is 90??. So if I have data on such a grid I am fine. My data is on an
> equally spaced grid in a coordsystem where x and y are at an angle of
> 60??. If you transform into cartesiancoords this isnt an equally spaced
> grid anymore.  You could interpret it as an equally spaced grid on
> which there is only data given on every second grid point.
> 
> It would be nice if one could just plot data given as F(x,y) where you
> supply x and y for every data point seperatly so that you dont need
> any specific grid.

Maybe interp() in akima package can help you, but as I said it 
strongly depends on what you really want to achieve.

Cheers
Petr

> 
> I hope that someone can help.
> Thanks in advance. Andreas

Petr Pikal
petr.pikal at precheza.cz



From dimitris.rizopoulos at med.kuleuven.ac.be  Tue Nov 30 13:14:57 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Tue, 30 Nov 2004 13:14:57 +0100
Subject: [R] 2k-factorial design with 10 parameters
References: <41AC60AB.2040806@gmx.de>
Message-ID: <000801c4d6d6$34fb0cf0$0540210a@www.domain>

Hi Sven,

just use:

lm(y~(x1+x2+x3+...+x10)^10)

e.g.,

y <- rnorm(5000)
x1 <- factor(sample(0:1, 5000, TRUE))
x2 <- factor(sample(0:1, 5000, TRUE))
x3 <- factor(sample(0:1, 5000, TRUE))
x4 <- factor(sample(0:1, 5000, TRUE))

lm1 <- lm(y~(x1+x2+x3+x4)^4)
summary(lm1)


I hope it helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Ph.D. Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/336899
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Sven" <svuwie at gmx.de>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, November 30, 2004 12:59 PM
Subject: [R] 2k-factorial design with 10 parameters


> Hi,
>
> I'd like to apply a 2^k factorial design with k=10 parameters. 
> Obviously this results in a quite long term for the model equation 
> due to the high number of combinations of parameters.
>
> How can I specify the equation for the linear model (lm) without 
> writing all combinations explicitly down by hand? Does a R command 
> exist for this problematic?
>
> Thanks for your help in advance,
> Sven
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Tue Nov 30 13:20:33 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 30 Nov 2004 12:20:33 +0000 (GMT)
Subject: [R] 2k-factorial design with 10 parameters
In-Reply-To: <41AC60AB.2040806@gmx.de>
References: <41AC60AB.2040806@gmx.de>
Message-ID: <Pine.LNX.4.61.0411301215080.13214@gannet.stats>

On Tue, 30 Nov 2004, Sven wrote:

> I'd like to apply a 2^k factorial design with k=10 parameters. Obviously this 
> results in a quite long term for the model equation due to the high number of 
> combinations of parameters.
>
> How can I specify the equation for the linear model (lm) without writing all 
> combinations explicitly down by hand? Does a R command exist for this 
> problematic?

I assume you mean k=10 factors (there are a lot more parameters).

 	aov(y ~ .^10, data=mydata)

will do what you are probably asking, if you have a data frame with 
response y and the ten factors.  I'm not sure how you could analyse the 
1000 odd lines of output, so reduce 10 to something sensible (like 2 or 3)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tuechler at gmx.at  Tue Nov 30 13:24:43 2004
From: tuechler at gmx.at (Heinz Tuechler)
Date: Tue, 30 Nov 2004 13:24:43 +0100
Subject: [R] How to know if a bug was recognised
Message-ID: <3.0.6.32.20041130132443.007a2a30@pop.gmx.net>

Hello!

A problem with special characters seemed to me to be a bug. I sent a mail
to R-windows at r-project.org concerning the problem (see below).
How can I find out, if this is considered as a bug or an error of myself?
Which part of FAQs or documentation did I miss to find the answer?

thanks in advance

Heinz T??chler

-------------------- copy of abovementioned mail ----------
to: R-windows at r-project.org
subject: problem with special characters (??,??,??)
Dear Developers!

Using special characters I found a strange behaviour in R 2.0.1 and equally
in 
R : Copyright 2004, The R Foundation for Statistical Computing
Version 2.0.1  (2004-11-15), ISBN 3-900051-07-0

Operating System: Windows 98SE

example:
factor1<-as.factor(c("weiblich","m??nnlich","??sterreichisch","fr??hreif","Gru??
"))
factor1
> factor1
[1] weiblich           m\344nnlich        \366sterreichisch  fr\374hreif
   
[5] Gru\337           
Levels: fr??hreif Gru?? m??nnlich ??sterreichisch weiblich

with best wishes

Heinz T??chler



From petr.pikal at precheza.cz  Tue Nov 30 13:30:41 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 30 Nov 2004 13:30:41 +0100
Subject: [R] 2k-factorial design with 10 parameters
In-Reply-To: <41AC60AB.2040806@gmx.de>
Message-ID: <41AC7601.9560.13DA850@localhost>



On 30 Nov 2004 at 12:59, Sven wrote:

> Hi,
> 
> I'd like to apply a 2^k factorial design with k=10 parameters.
> Obviously this results in a quite long term for the model equation due
> to the high number of combinations of parameters.
> 
> How can I specify the equation for the linear model (lm) without
> writing all combinations explicitly down by hand? Does a R command
> exist for this problematic?

Hi Sven

from
?lm

The specification 'first*second'
     indicates the _cross_ of 'first' and 'second'.  This is the same
     as 'first + second + first:second'

and from

?formula

## Create a formula for a model with a large number of variables:
     xnam <- paste("x", 1:25, sep="")
     (fmla <- as.formula(paste("y ~ ", paste(xnam, collapse= "+"))))

If you change 1:25 to 1:10 and collapse to * and use fmla in your 
model you will get what you want (I suppose). But I woder if it 
has any sense.

Cheers
Petr

> 
> Thanks for your help in advance,
> Sven
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From ripley at stats.ox.ac.uk  Tue Nov 30 13:55:05 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 30 Nov 2004 12:55:05 +0000 (GMT)
Subject: [R] How to know if a bug was recognised
In-Reply-To: <3.0.6.32.20041130132443.007a2a30@pop.gmx.net>
References: <3.0.6.32.20041130132443.007a2a30@pop.gmx.net>
Message-ID: <Pine.LNX.4.61.0411301241320.13695@gannet.stats>

You have already been sent an answer.  It is a bug in your copy of 
Windows, and it will be worked around in the current version of R-patched. 
From the CHANGES file:

   R 2.0.1 patched
   ===============

   We work around reported bugs in Windows XP as to which characters are
   printable by attempting to print all non-control characters when using
   print().

Note: no one else has reported a problem on 98SE, despite this having been 
the behaviour of R for three months, and it seemed only XP SP2 was 
affected.  Indeed, no one reported the problem for 2.0.0 at all, including 
all the alpha and beta test versions (except for Chinese where the 
characters are invalid and so the behaviour was correct), and so the 
workaround did not make 2.0.1.

BTW, your expectation that an email you sent at the weekend will be 
answered by Tuesday is completely unreasonable.  R is a volunteer project, 
and the developers do have other commitments (and occasionally make 
attempts to have a life).

[You could have read the archives of the R mailing lists to find that this 
was a known issue that had already been addressed.]

On Tue, 30 Nov 2004, Heinz Tuechler wrote:

> Hello!
>
> A problem with special characters seemed to me to be a bug. I sent a mail
> to R-windows at r-project.org concerning the problem (see below).
> How can I find out, if this is considered as a bug or an error of myself?
> Which part of FAQs or documentation did I miss to find the answer?
>
> thanks in advance
>
> Heinz T?chler
>
> -------------------- copy of abovementioned mail ----------
> to: R-windows at r-project.org
> subject: problem with special characters (?,?,?)
> Dear Developers!
>
> Using special characters I found a strange behaviour in R 2.0.1 and equally
> in
> R : Copyright 2004, The R Foundation for Statistical Computing
> Version 2.0.1  (2004-11-15), ISBN 3-900051-07-0
>
> Operating System: Windows 98SE
>
> example:
> factor1<-as.factor(c("weiblich","m?nnlich","?sterreichisch","fr?hreif","Gru?
> "))
> factor1
>> factor1
> [1] weiblich           m\344nnlich        \366sterreichisch  fr\374hreif
>
> [5] Gru\337
> Levels: fr?hreif Gru? m?nnlich ?sterreichisch weiblich
>
> with best wishes
>
> Heinz T?chler
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

From hb at maths.lth.se  Tue Nov 30 14:01:55 2004
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Tue, 30 Nov 2004 14:01:55 +0100
Subject: [R] How to know if a bug was recognised
In-Reply-To: <3.0.6.32.20041130132443.007a2a30@pop.gmx.net>
Message-ID: <001601c4d6dc$ca21d2e0$e502eb82@hblaptop>

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Heinz Tuechler
> Sent: Tuesday, November 30, 2004 1:25 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] How to know if a bug was recognised
> 
> 
> Hello!
> 
> A problem with special characters seemed to me to be a bug. I 
> sent a mail to R-windows at r-project.org concerning the problem 
> (see below). How can I find out, if this is considered as a 
> bug or an error of myself? Which part of FAQs or 
> documentation did I miss to find the answer?

This will not answer your question on what is a bug or not, but if you don't
know, the "R team" has kindly made a fix for this problem. What I remember
from an earlier thread, this was not really due to R, but to Windows. For
the latest R v2.0.1 patch it now seems to work as before/expected:

R : Copyright 2004, The R Foundation for Statistical Computing
Version 2.0.1 Patched (2004-11-27), ISBN 3-900051-07-0

> "??"
[1] "??"
> "??"
[1] "??"
> "??"
[1] "??"

Cheers

Henrik Bengtsson


> thanks in advance
> 
> Heinz T??chler
> 
> -------------------- copy of abovementioned mail ----------
> to: R-windows at r-project.org
> subject: problem with special characters (??,??,??)
> Dear Developers!
> 
> Using special characters I found a strange behaviour in R 
> 2.0.1 and equally in 
> R : Copyright 2004, The R Foundation for Statistical 
> Computing Version 2.0.1  (2004-11-15), ISBN 3-900051-07-0
> 
> Operating System: Windows 98SE
> 
> example: 
> factor1<-as.factor(c("weiblich","m??nnlich","??sterreichisch","f
> r??hreif","Gru??
> "))
> factor1
> > factor1
> [1] weiblich           m\344nnlich        \366sterreichisch  
> fr\374hreif
>    
> [5] Gru\337           
> Levels: fr??hreif Gru?? m??nnlich ??sterreichisch weiblich
> 
> with best wishes
> 
> Heinz T??chler
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From michael.watson at bbsrc.ac.uk  Tue Nov 30 14:25:16 2004
From: michael.watson at bbsrc.ac.uk (michael watson (IAH-C))
Date: Tue, 30 Nov 2004 13:25:16 -0000
Subject: [R] Opening connection to FTP site
Message-ID: <8975119BCD0AC5419D61A9CF1A923E950121B8AC@iahce2knas1.iah.bbsrc.reserved>

Hi

Is it possible to open a connection to an FTP site such that I can read
the directory listing?  Eg:

URL <- url("ftp://ftp.ensembl.org", open="r")
Error in url("ftp://ftp.ensembl.org", open="r") :
	unable to open connection

URL <- url("ftp://ftp.ensembl.org")
open(URL)
Error in open.connection(URL) : unable to open connection

I think perhaps it is because there isn't actually a file there to
connect to, eg this works fine:

URL <- url("ftp://ftp.ensembl.org/pub/README", open="r")

Thanks in advance

Mick



From ripley at stats.ox.ac.uk  Tue Nov 30 14:51:17 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 30 Nov 2004 13:51:17 +0000 (GMT)
Subject: [R] Opening connection to FTP site
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E950121B8AC@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E950121B8AC@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <Pine.LNX.4.61.0411301347120.26457@gannet.stats>

On Tue, 30 Nov 2004, michael watson (IAH-C) wrote:

> Hi
>
> Is it possible to open a connection to an FTP site such that I can read
> the directory listing?  Eg:
>
> URL <- url("ftp://ftp.ensembl.org", open="r")
> Error in url("ftp://ftp.ensembl.org", open="r") :
> 	unable to open connection
>
> URL <- url("ftp://ftp.ensembl.org")
> open(URL)
> Error in open.connection(URL) : unable to open connection
>
> I think perhaps it is because there isn't actually a file there to
> connect to, eg this works fine:
>
> URL <- url("ftp://ftp.ensembl.org/pub/README", open="r")

It is not possible using url().  Try, e.g.

tmp <- tempfile()
download.file("ftp://ftp.ensembl.org", tmp, method="lynx") # or "wget"
readLines(tmp)


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bates at stat.wisc.edu  Tue Nov 30 14:59:11 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 30 Nov 2004 07:59:11 -0600
Subject: [R] lme in R-2.0.0: Problem with lmeControl
In-Reply-To: <41AC5499.9090100@wiwi.uni-bielefeld.de>
References: <41AC5499.9090100@wiwi.uni-bielefeld.de>
Message-ID: <41AC7CAF.4000506@stat.wisc.edu>

Pavel Khomski wrote:
> Hello!
> 
> One note/question hier about specification of control-parameters in the 
> lme(...,control=list(...)) function call:
> 
> i tried to specify tne number of iteration needed via 
> lme(....,control=list(maxIter=..., niterEM=...,msVerbose=TRUE))
> but every time i change the defualt values maxIter (e.g. maxIter=1, 
> niterEM=0)  on ones specified by me, the call returns all the iterations 
> needed until it's converged.
> and this is exactly the problem i will to get round.  (    e.g. in 
> example on p.81 of Pinheiro/Bates,2000: 
> fm1Rail.lme<-lme(...,control=list(maxIter=1,...))            )
> so i have tried with option msMaxIter=... and this works.
> 
> The other problem is, that i even can not see (in R !!!) the output from 
> iterations, despite the msVerbose=TRUE  specification and setting 
> options(verbose=TRUE) (The S-plus can do it but also ignoring the 
> maxIter=... specification)
> 
> Thank you for your hint
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html

One difference between the S-PLUS and the R versions of lme is the 
optimizer that is used.  In S-PLUS the ms() function is used to optimize 
the log-likelihood.  In R the most reasonable choices are optim or nlm.

The version of lme in the current nlme package uses optim, which has a 
more sophisticated "verbose" option than just an on/off switch.  It has 
several levels of verbosity.  If we say that msVerbose corresponds to 
the lowest non-silent level then it only prints out intermediate results 
every five iterations and that example converges before 5 iterations.

 > library(nlme)
 > fm1Rail.lme <- lme(travel ~ 1, Rail, ~ 1|Rail,
+    control=list(niterEM=0,msVerbose=TRUE))
initial  value 67.893737
final  value 61.048859
converged
 >

If we were to set msVerbose=TRUE to give the highest level of verbosity 
you would be innundated with output.

The version of lme in the current lme4 package allows the optimizer to 
be chosen.  (The lme4 package cannot be loaded in the same session with 
the nlme package.  You must quit R and restart to be able to load the 
lme4 package).

 > library(lme4)
Loading required package: Matrix
Loading required package: latticeExtra
 > fm1Rail.lme <- lme(travel ~ 1, Rail, ~ 1 | Rail,
+   control = list(msVerbose = TRUE, optimizer = "nlm", niterEM = 0))
iteration = 0
Step:
[1] 0
Parameter:
[1] 0.1177830
Function Value
[1] 143.9911
Gradient:
[1] 7.849722

iteration = 1
Step:
[1] -7.849722
Parameter:
[1] -7.731939
Function Value
[1] 136.7956
Gradient:
[1] -4.880857

iteration = 2
Step:
[1] 3.009554
Parameter:
[1] -4.722385
Function Value
[1] 123.8982
Gradient:
[1] -2.879458

iteration = 3
Step:
[1] 0.9505343
Parameter:
[1] -3.771850
Function Value
[1] 122.207
Gradient:
[1] -0.4481287

iteration = 4
Step:
[1] 0.1751970
Parameter:
[1] -3.596653
Function Value
[1] 122.1802
Gradient:
[1] 0.1486718

iteration = 5
Step:
[1] -0.04364416
Parameter:
[1] -3.640298
Function Value
[1] 122.177
Gradient:
[1] -0.003887335

iteration = 6
Step:
[1] 0.00111209
Parameter:
[1] -3.639185
Function Value
[1] 122.177
Gradient:
[1] -3.100244e-05

iteration = 7
Parameter:
[1] -3.639177
Function Value
[1] 122.177
Gradient:
[1] 6.643475e-09

Relative gradient close to zero.
Current iterate is probably solution.

Those results do not match the output in our book because the version in 
  lme4 uses the deviance as the objective function.

(One of the difficulties of writing a book about a particular piece of 
software is that the software is likely to change much more rapidly than 
  the book.)

I show this with some trepidation because I am in the process of 
changing the optimization within lme yet again.  The development version 
ignores optimizer = 'nlm' because it phrases the problem as a 
constrained optimization problem and uses optim unconditionally.  The 
full level of verbosity of optim, which I occasionally use for 
development purposes, is very verbose

 > fm1Rail.lme <- lme1(travel ~ 1, Rail, ~ 1 | Rail,
+ control = list(msVerbose = TRUE, optimizer = "nlm", niterEM = 0))
N = 1, M = 5 machine precision = 2.22045e-16
L = 1e-10
X0 = 0.888889
U = inf
At X0, 0 variables are exactly at the bounds
At iterate     0  f=       143.99  |proj g|=       8.8309
Iteration     0

---------------- CAUCHY entered-------------------

There are 0  breakpoints

GCP found in this segment
Piece      1 f1, f2 at start point -7.7985e+01  7.7985e+01
Distance to the stationary point =   1.0000e+00
Cauchy X =  9.71983

---------------- exit CAUCHY----------------------

1  variables are free at GCP on iteration 1
LINE SEARCH 0 times; norm of step = 1
X = 1.88889
G = -4.46694
Iteration     1

---------------- CAUCHY entered-------------------

There are 0  breakpoints

GCP found in this segment
Piece      1 f1, f2 at start point -1.9954e+01  8.7077e+01
Distance to the stationary point =   2.2915e-01
Cauchy X =  2.91248

---------------- exit CAUCHY----------------------

1  variables are free at GCP on iteration 2
LINE SEARCH 0 times; norm of step = 1.02359
X = 2.91248
G = -2.81364
Iteration     2

---------------- CAUCHY entered-------------------

There are 0  breakpoints

GCP found in this segment
Piece      1 f1, f2 at start point -7.9166e+00  1.2787e+01
Distance to the stationary point =   6.1912e-01
Cauchy X =  4.65446

---------------- exit CAUCHY----------------------

1  variables are free at GCP on iteration 3
LINE SEARCH 0 times; norm of step = 1.74198
X = 4.65446
G = -1.59578
Iteration     3

---------------- CAUCHY entered-------------------

There are 0  breakpoints

GCP found in this segment
Piece      1 f1, f2 at start point -2.5465e+00  1.7803e+00
Distance to the stationary point =   1.4304e+00
Cauchy X =  6.937

---------------- exit CAUCHY----------------------

1  variables are free at GCP on iteration 4
LINE SEARCH 0 times; norm of step = 2.28253
X = 6.937
G = -0.919919
Iteration     4

---------------- CAUCHY entered-------------------

There are 0  breakpoints

GCP found in this segment
Piece      1 f1, f2 at start point -8.4625e-01  2.5058e-01
Distance to the stationary point =   3.3772e+00
Cauchy X =  10.0438

---------------- exit CAUCHY----------------------

1  variables are free at GCP on iteration 5
LINE SEARCH 0 times; norm of step = 3.10678
X = 10.0438
G = -0.51183
Iteration     5

---------------- CAUCHY entered-------------------

There are 0  breakpoints

GCP found in this segment
Piece      1 f1, f2 at start point -2.6197e-01  3.4411e-02
Distance to the stationary point =   7.6130e+00
Cauchy X =  13.9403

---------------- exit CAUCHY----------------------

1  variables are free at GCP on iteration 6
LINE SEARCH 0 times; norm of step = 3.89656
X = 13.9403
G = -0.279117
Iteration     6

---------------- CAUCHY entered-------------------

There are 0  breakpoints

GCP found in this segment
Piece      1 f1, f2 at start point -7.7906e-02  4.6528e-03
Distance to the stationary point =   1.6744e+01
Cauchy X =  18.6139

---------------- exit CAUCHY----------------------

1  variables are free at GCP on iteration 7
LINE SEARCH 0 times; norm of step = 4.67355
X = 18.6139
G = -0.146855
Iteration     7

---------------- CAUCHY entered-------------------

There are 0  breakpoints

GCP found in this segment
Piece      1 f1, f2 at start point -2.1566e-02  6.1033e-04
Distance to the stationary point =   3.5335e+01
Cauchy X =  23.8031

---------------- exit CAUCHY----------------------

1  variables are free at GCP on iteration 8
LINE SEARCH 0 times; norm of step = 5.18917
X = 23.8031
G = -0.0735916
Iteration     8

---------------- CAUCHY entered-------------------

There are 0  breakpoints

GCP found in this segment
Piece      1 f1, f2 at start point -5.4157e-03  7.6462e-05
Distance to the stationary point =   7.0829e+01
Cauchy X =  29.0155

---------------- exit CAUCHY----------------------

1  variables are free at GCP on iteration 9
LINE SEARCH 0 times; norm of step = 5.21245
X = 29.0155
G = -0.0339817
Iteration     9

---------------- CAUCHY entered-------------------

There are 0  breakpoints

GCP found in this segment
Piece      1 f1, f2 at start point -1.1548e-03  8.7751e-06
Distance to the stationary point =   1.3159e+02
Cauchy X =  33.4873

---------------- exit CAUCHY----------------------

1  variables are free at GCP on iteration 10
LINE SEARCH 0 times; norm of step = 4.4718
X = 33.4873
G = -0.0135712
Iteration    10

---------------- CAUCHY entered-------------------

There are 0  breakpoints

GCP found in this segment
Piece      1 f1, f2 at start point -1.8418e-04  8.4064e-07
Distance to the stationary point =   2.1909e+02
Cauchy X =  36.4607

---------------- exit CAUCHY----------------------

1  variables are free at GCP on iteration 11
LINE SEARCH 0 times; norm of step = 2.97337
X = 36.4607
G = -0.0041181
Iteration    11

---------------- CAUCHY entered-------------------

There are 0  breakpoints

GCP found in this segment
Piece      1 f1, f2 at start point -1.6959e-05  5.3916e-08
Distance to the stationary point =   3.1454e+02
Cauchy X =  37.756

---------------- exit CAUCHY----------------------

1  variables are free at GCP on iteration 12
LINE SEARCH 0 times; norm of step = 1.2953
X = 37.756
G = -0.000739049
Iteration    12

---------------- CAUCHY entered-------------------

There are 0  breakpoints

GCP found in this segment
Piece      1 f1, f2 at start point -5.4619e-07  1.4249e-09
Distance to the stationary point =   3.8333e+02
Cauchy X =  38.0393

---------------- exit CAUCHY----------------------

1  variables are free at GCP on iteration 13
LINE SEARCH 0 times; norm of step = 0.283301
X = 38.0393
G = -5.08201e-05
Iteration    13

---------------- CAUCHY entered-------------------

There are 0  breakpoints

GCP found in this segment
Piece      1 f1, f2 at start point -2.5827e-09  6.2742e-12
Distance to the stationary point =   4.1164e+02
Cauchy X =  38.0602

---------------- exit CAUCHY----------------------

1  variables are free at GCP on iteration 14
LINE SEARCH 0 times; norm of step = 0.0209195
X = 38.0602
G = -6.84527e-07
Iteration    14

---------------- CAUCHY entered-------------------

There are 0  breakpoints

GCP found in this segment
Piece      1 f1, f2 at start point -4.6858e-13  1.1230e-15
Distance to the stationary point =   4.1726e+02
Cauchy X =  38.0605

---------------- exit CAUCHY----------------------

1  variables are free at GCP on iteration 15
LINE SEARCH 0 times; norm of step = 0.000285625
X = 38.0605
G = -6.44794e-10

iterations 15
function evaluations 16
segments explored during Cauchy searches 15
BFGS updates skipped 0
active bounds at final generalized Cauchy point 0
norm of the final projected gradient 6.44794e-10
final function value 122.177

X = 38.0605
F = 122.177
final  value 122.177001
converged

The curious reader can try to work out what the current parameterization 
  is.  :-)  You win the prize if you can also explain why.



From DempseyC at kssg.com  Tue Nov 30 13:30:40 2004
From: DempseyC at kssg.com (Catherine Dempsey)
Date: Tue, 30 Nov 2004 12:30:40 -0000
Subject: [R] Info
Message-ID: <6B5A9304046AD411BD0200508BDFB6CB032EA9DB@gimli.middleearth.kssg.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041130/61370aac/attachment.pl

From bates at stat.wisc.edu  Tue Nov 30 15:11:44 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 30 Nov 2004 08:11:44 -0600
Subject: [R] A basic question
In-Reply-To: <opsh90wjhdezww6l@perseus.unalmed.edu.co>
References: <20041129192247.YRQE24735.smta04.mail.ozemail.net@there>
	<opsh90wjhdezww6l@perseus.unalmed.edu.co>
Message-ID: <41AC7FA0.90609@stat.wisc.edu>

Kenneth wrote:

> I want to know any experience compiling R in other LINUX distributions
> besides FEDORA (Red Hat) or Mandrake, for example in BSD, Debian,
> Gentoo, Slackware, vector LINUX, Knoppix, Yopper or CERN linux?
> 
> Hope this is not a "basic question"
> 
> Thank you for your help.

There is considerable experience compiling R under Debian Linux.  Many 
of the developers use Debian and if you check the sources you will find 
that there is a debian directory in the official source distribution. 
The daily package checking is done on a system running Debian and there 
are dozens of R packages in the Debian distribution.  See 
http://packages.qa.debian.org/r/r-base-core for a history of the 
uploads.  The link to the buildd logs shows compilation of R under 
Debian on many different architectures.

Because Knoppix is derived from Debian and uses the Debian packaging 
system the compilation of R under Knoppix is essentially the same as 
under Debian.  The Quantian distribution comes with R installed.



From p.dalgaard at biostat.ku.dk  Tue Nov 30 15:17:05 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Nov 2004 15:17:05 +0100
Subject: [R] A basic question
In-Reply-To: <opsh90wjhdezww6l@perseus.unalmed.edu.co>
References: <20041129192247.YRQE24735.smta04.mail.ozemail.net@there>
	<opsh90wjhdezww6l@perseus.unalmed.edu.co>
Message-ID: <x2ekibfmla.fsf@biostat.ku.dk>

Kenneth <krcabrer at epm.net.co> writes:

> Hi R users:
> 
> I want to know any experience compiling R in other LINUX distributions
> besides FEDORA (Red Hat) or Mandrake, for example in BSD, Debian,
> Gentoo, Slackware, vector LINUX, Knoppix, Yopper or CERN linux?
> 
> Hope this is not a "basic question"

BSD is not Linux... 

I'm fairly sure we have people checking BSD (perhaps even on the core
team. Ross?), and also SuSE and Gentoo. We definitely have core
members using Debian, and Knoppix is just a Debian variant (see also
Quantian). CERN Linux is a RedHat variant. Don't know about Yoper,
Slackware, and vector Linux (a Slackware derivative), but building
frome source is not generally a problem on Linux.


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From abunn at whrc.org  Tue Nov 30 15:22:20 2004
From: abunn at whrc.org (Andy Bunn)
Date: Tue, 30 Nov 2004 09:22:20 -0500
Subject: [R] Info
In-Reply-To: <6B5A9304046AD411BD0200508BDFB6CB032EA9DB@gimli.middleearth.kssg.com>
Message-ID: <NEBBIPHDAMMOKDKPOFFICEOECMAA.abunn@whrc.org>

It looks like factanal is unable to optimize from these starting values
(kinda like the error message says). So, factanal.fit.mle isn't converging
and you have problems with your analysis. Try putting control = list(trace =
T) in your code to see what happenens. E.g.,

R >
R >      v1 <- c(1,1,1,1,1,1,1,1,1,1,3,3,3,3,3,4,5,6)
R >      v2 <- c(1,2,1,1,1,1,2,1,2,1,3,4,3,3,3,4,6,5)
R >      v3 <- c(3,3,3,3,3,1,1,1,1,1,1,1,1,1,1,5,4,6)
R >      v4 <- c(3,3,4,3,3,1,1,2,1,1,1,1,2,1,1,5,6,4)
R >      v5 <- c(1,1,1,1,1,3,3,3,3,3,1,1,1,1,1,6,4,5)
R >      v6 <- c(1,1,1,2,1,3,3,3,4,3,1,1,1,2,1,6,5,4)
R >      m1 <- cbind(v1,v2,v3,v4,v5,v6)
R >      factanal(m1, factors=3, control = list(trace = T))
start 1 value: 0.4755156 uniqs: 0.0050 0.1009 0.0050 0.2241 0.0843 0.0050

Call:
factanal(x = m1, factors = 3, control = list(trace = T))

Uniquenesses:
   v1    v2    v3    v4    v5    v6
0.005 0.101 0.005 0.224 0.084 0.005

Loadings:
   Factor1 Factor2 Factor3
v1 0.944   0.182   0.267
v2 0.905   0.235   0.159
v3 0.236   0.210   0.946
v4 0.180   0.242   0.828
v5 0.242   0.881   0.286
v6 0.193   0.959   0.196

               Factor1 Factor2 Factor3
SS loadings      1.893   1.886   1.797
Proportion Var   0.316   0.314   0.300
Cumulative Var   0.316   0.630   0.929

The degrees of freedom for the model is 0 and the fit was 0.4755
R >

If you think the model should be OK and there isn't an error somewhere else
then you'll want to fiddle with the control options. From the help for
fractanal:

 control: A list of control values,

          nstart The number of starting values to be tried if 'start =
               NULL'. Default 1.

          trace logical. Output tracing information? Default 'FALSE'.

          lower The lower bound for uniquenesses during optimization.
               Should be > 0. Default 0.005.

          opt A list of control values to be passed to 'optim''s
               'control' argument.

          rotate a list of additional arguments for the rotation
               function.

     ...: Components of 'control' can also be supplied as named
          arguments to 'factanal'.


HTH, Andy


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Catherine Dempsey
> Sent: Tuesday, November 30, 2004 7:31 AM
> To: 'r-help at lists.R-project.org'
> Subject: [R] Info
>
>
> I am having difficulty obtaining the scores from my principal component
> analysis.  I have used this method before and have had no problems.  The
> data set that I am using this time is similar to what I have used in the
> past.  What do I need to do to my dataset in order for me to obtain these
> scores?
>
> R screen says the following message
>
> Error in factanal(covmat = pasa.cov, factors = 4) :
>         Unable to optimize from these starting value(s)
>
> many thanks
>
> Catherine
>
>
> KSS Ltd
> Seventh Floor  St James's Buildings  79 Oxford Street  Manchester
>  M1 6SS  England
> Company Registration Number 2800886
> Tel: +44 (0) 161 228 0040	Fax: +44 (0) 161 236 6305
> mailto:kssg at kssg.com		http://www.kssg.com
>
>
> The information in this Internet email is confidential and m...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From jarioksa at sun3.oulu.fi  Tue Nov 30 15:26:43 2004
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: 30 Nov 2004 16:26:43 +0200
Subject: [R] A basic question
In-Reply-To: <opsh90wjhdezww6l@perseus.unalmed.edu.co>
References: <20041129192247.YRQE24735.smta04.mail.ozemail.net@there>
	<opsh90wjhdezww6l@perseus.unalmed.edu.co>
Message-ID: <1101824802.8293.14.camel@biol102145.oulu.fi>

On Tue, 2004-11-30 at 13:58, Kenneth wrote:
> Hi R users:
> 
> I want to know any experience compiling R in other LINUX distributions
> besides FEDORA (Red Hat) or Mandrake, for example in BSD, Debian,
> Gentoo, Slackware, vector LINUX, Knoppix, Yopper or CERN linux?
> 
> Hope this is not a "basic question"
> 
> Thank you for your help.
> 
I assume that the following will typically work:
Get the source file, gunzip and untar, cd to the created directory and
type:

./configure
make
sudo make install

It is best to check the resulting configuration after ./configure and
get the software (compilers, libraries, packages, utilities) you need
for the missing functionality you want to have. It is also wise to run
'make check' after 'make' so that you see if you can trust your
compilation. This make check fails in some cases: at least standard
package 'foreign' failed 'make check' in ppc architecture both in Red
Hat/Fedora  based (Yellowdog) and Debian based (Ubuntu) Linuxes when I
tried last time. Otherwise the compilation seems to run smoothly (and
you may not need 'foreign').

BSD is not Linux, but R is officially supported at least for one version
of BSD with GNU tools: MacOS X.

cheers,jari oksanen
-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From johnmchambers at gmail.com  Tue Nov 30 15:40:03 2004
From: johnmchambers at gmail.com (John Chambers)
Date: Tue, 30 Nov 2004 09:40:03 -0500
Subject: [R] Testing for S4 objects
In-Reply-To: <20041127224829.WDSK25979.tomts25-srv.bellnexxia.net@JohnDesktop8300>
References: <16806.59063.246234.858186@gargle.gargle.HOWL>
	<20041127224829.WDSK25979.tomts25-srv.bellnexxia.net@JohnDesktop8300>
Message-ID: <61842888041130064036da550b@mail.gmail.com>

Let me suggest a different test, because slotNames was written to work
differently when given a string or a class definition.  With your
definition,

R> x <- "classRepresentation"
R> isS4object(x)
[1] TRUE

which I assume is not what you wanted.  (Given a single string, 
slotNames() tries to look up the class definition of that name.)

How about the following?  The logic is that an S4 object must have an
actual class attribute of length 1 (that rules out basic data types,
where class(x) is a string but there is no actual attribute, and also
rules out some S3 objects).  Then if that's true, try to look up the
class definition.  If it is non-null, seems like an S4 object.

R> isS4object <- function(object)(length(attr(object, "class"))==1 &&
+     !is.null(getClass(class(object))))
R> isS4object(x)
[1] FALSE
R> isS4object(getClass(class(x)))
[1] TRUE

This definition seems to work, at least on the examples I could think
of right away.  Notice though, that some classes, such as "ts", that
have been around for a long while are nevertheless legitimate S4
classes, so:

R> t1 = ts(1:12)
R> isS4object(t1)
[1] TRUE

(this applies to either version of isS4object).

There are a couple of details, more appropriate for the r-devel list. 
Seems  a good candidate for a function to add to R.


On Sat, 27 Nov 2004 17:48:30 -0500, John Fox <jfox at mcmaster.ca> wrote:
> Dear Martin,
> 
> As it turns out, the test that I proposed (i.e., testing for NULL slotNames)
> sometimes fails. For example:
> 
> > library(car)
> > data(Prestige)
> > sum <- summary(lm(prestige ~ income + education, data=Prestige))
> > slotNames(sum)
> character(0)
> 
> The following, however, seems to work (at least as far as I've been able to
> ascertain):
> 
> isS4object <- function(object) length(slotNames(object)) != 0
> 
> I hope that this is a more robust test.
> 
> 
> 
> John
> 
> --------------------------------
> John Fox
> Department of Sociology
> McMaster University
> Hamilton, Ontario
> Canada L8S 4M4
> 905-525-9140x23604
> http://socserv.mcmaster.ca/jfox
> --------------------------------
> 
> > -----Original Message-----
> > From: Martin Maechler [mailto:maechler at stat.math.ethz.ch]
> > Sent: Friday, November 26, 2004 3:18 AM
> > To: John Fox
> > Cc: r-help at stat.math.ethz.ch
> > Subject: Re: [R] Testing for S4 objects
> >
> > >>>>> "JohnF" == John Fox <jfox at mcmaster.ca>
> > >>>>>     on Thu, 25 Nov 2004 22:28:50 -0500 writes:
> >
> >     JohnF> Dear r-help list members, Is there a way to test
> >     JohnF> whether an object is an S4 object? The best that I've
> >     JohnF> been able to come up with is
> >
> >     JohnF>    isS4object <- function(object)
> > !(is.null(slotNames(object)))
> >
> > you can drop one pair of "(..)" to give
> >
> >   isS4object <- function(object) !is.null(slotNames(object))
> >
> >
> >     JohnF> which assumes that an S4 object has at least one
> >     JohnF> slot. I think this is safe, but perhaps I'm missing
> >     JohnF> something.
> >
> > The question is a very good one -- that I have posed to
> > R-core a while ago myself.
> >
> > Inside  utils:::str.default  {which doesn't show the many
> > commments in the *source* of str.default()}, I have wanted a
> > way that even works when the 'methods' package is not
> > attached and use the more obscure
> >
> >     #NOT yet:if(has.class <- !is.null(cl <- class(object)))
> >     if(has.class <- !is.null(cl <- attr(object, "class")))#
> > S3 or S4 class
> >       S4 <- !is.null(attr(cl, "package"))## <<<'kludge' FIXME!
> >       ##or length(methods::getSlots(cl)) > 0
> >
> > For the time being, I'd keep your function, but I don't think
> > we'd guarantee that it will remain the appropriate test in
> > all future.  But till then many things will have happened (if
> > not all of them ;-).
> >
> > Martin Maechler, ETH Zurich
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From fciclone at bol.com.br  Tue Nov 30 16:03:23 2004
From: fciclone at bol.com.br (Alex)
Date: Tue, 30 Nov 2004 12:03:23 -0300
Subject: [R] glmmPQL
Message-ID: <I7ZZ5N$6CC54E3A7D2A0E3717B58B23F1502CE2@bol.com.br>

Dear listmembers,

I've adjusted a mixed model with glmmPQL:

nulo<-glmmPQL(POS~1,random=~1|GRUPO, family=binomial, data=new)

and I've reched the following results:

Linear mixed-effects model fit by maximum likelihood
 Data: new 
      AIC      BIC    logLik
  53238.5 53260.74 -26616.25

Random effects:
 Formula: ~1 | GRUPO
        (Intercept)  Residual
StdDev:   0.3402137 0.9952645

Variance function:
 Structure: fixed weights
 Formula: ~invwt 
Fixed effects: POS ~ 1 
                Value  Std.Error    DF  t-value p-value
(Intercept) 0.6214472 0.03723763 12025 16.68869       0

Standardized Within-Group Residuals:
       Min         Q1        Med         Q3        Max 
-1.7831032 -1.2642113  0.6508504  0.7058691  1.1945426 

Number of Observations: 12227
Number of Groups: 202 

Could someone clarify me? I don't understand why DF=12025 in the intercept t-test above, if the objective is to estimate only one mean for all groups (mean=0,6214472). I expected that this DF would equal 201, as I have 202 groups.

Thanks in advance, Alex



 




 
__________________________________________________________________________
Acabe com aquelas janelinhas que pulam na sua tela.
AntiPop-up UOL - ?? gr??tis!
http://antipopup.uol.com.br/



From tlumley at u.washington.edu  Tue Nov 30 16:20:38 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 30 Nov 2004 07:20:38 -0800 (PST)
Subject: [R] About ROC curves
In-Reply-To: <5.1.1.6.0.20041129234835.02a3fec0@mail.cwru.edu>
References: <5.1.1.6.0.20041129234835.02a3fec0@mail.cwru.edu>
Message-ID: <Pine.A41.4.61b.0411300719030.356622@homer08.u.washington.edu>

On Mon, 29 Nov 2004, Xin Qi wrote:

> Hi, Dear all R users:
>
> Does someone know whether R can calculate the Receiver Operating 
> Characteristic (ROC) Curves? I didn't find it from the packages.

Issue 1 of the R Newsletter this year used ROC curve calculation as an 
example of simple S3 and S4 classes.

 	-thomas



From murdoch at stats.uwo.ca  Tue Nov 30 16:27:01 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 30 Nov 2004 10:27:01 -0500
Subject: [R] =?iso-8859-1?q?Attn_Heinz_Tuechler=3A_Re=3A__problem_with_sp?=
 =?iso-8859-1?q?ecial_characters_=28=E4=2C=F6=2C=FC=29?=
In-Reply-To: <3.0.6.32.20041127233123.00794d20@pop.gmx.net>
References: <3.0.6.32.20041127233123.00794d20@pop.gmx.net>
Message-ID: <l64pq01252g98aki53brfihdfipa96drg5@4ax.com>

[I tried to send this message privately, but the return address
bounced.]

I think this has been fixed in R-patched, but I doubt if the fix has
been tested in Win98.  Could you please download a copy from
<http://cran.r-project.org/bin/windows/base/rpatched.html> and confirm
that it has been fixed?

Duncan Murdoch

On Sat, 27 Nov 2004 23:31:23 +0100, Heinz Tuechler <tuechler at gmx.at>
wrote :

>Dear Developers!
>
>Using special characters I found a strange behaviour in R 2.0.1 and equally
>in 
>R : Copyright 2004, The R Foundation for Statistical Computing
>Version 2.0.1  (2004-11-15), ISBN 3-900051-07-0
>
>Operating System: Windows 98SE
>
>example:
>factor1<-as.factor(c("weiblich","m??nnlich","??sterreichisch","fr??hreif","Gru??
>"))
>factor1
>> factor1
>[1] weiblich           m\344nnlich        \366sterreichisch  fr\374hreif
>   
>[5] Gru\337           
>Levels: fr??hreif Gru?? m??nnlich ??sterreichisch weiblich
>
>with best wishes
>
>Heinz T??chler



From ripley at stats.ox.ac.uk  Tue Nov 30 16:28:21 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 30 Nov 2004 15:28:21 +0000 (GMT)
Subject: [R] Info
In-Reply-To: <NEBBIPHDAMMOKDKPOFFICEOECMAA.abunn@whrc.org>
References: <NEBBIPHDAMMOKDKPOFFICEOECMAA.abunn@whrc.org>
Message-ID: <Pine.LNX.4.61.0411301525050.26457@gannet.stats>

Also, factanal() does not do `principal component analysis', and it may 
well be that the data are inappropriate for factor analysis if they are 
appropriate for PCA.

If PCA was really intended, prcomp() and princomp() are appropriate tools.

On Tue, 30 Nov 2004, Andy Bunn wrote:

> It looks like factanal is unable to optimize from these starting values
> (kinda like the error message says). So, factanal.fit.mle isn't converging
> and you have problems with your analysis. Try putting control = list(trace =
> T) in your code to see what happenens. E.g.,
>
> R >
> R >      v1 <- c(1,1,1,1,1,1,1,1,1,1,3,3,3,3,3,4,5,6)
> R >      v2 <- c(1,2,1,1,1,1,2,1,2,1,3,4,3,3,3,4,6,5)
> R >      v3 <- c(3,3,3,3,3,1,1,1,1,1,1,1,1,1,1,5,4,6)
> R >      v4 <- c(3,3,4,3,3,1,1,2,1,1,1,1,2,1,1,5,6,4)
> R >      v5 <- c(1,1,1,1,1,3,3,3,3,3,1,1,1,1,1,6,4,5)
> R >      v6 <- c(1,1,1,2,1,3,3,3,4,3,1,1,1,2,1,6,5,4)
> R >      m1 <- cbind(v1,v2,v3,v4,v5,v6)
> R >      factanal(m1, factors=3, control = list(trace = T))
> start 1 value: 0.4755156 uniqs: 0.0050 0.1009 0.0050 0.2241 0.0843 0.0050
>
> Call:
> factanal(x = m1, factors = 3, control = list(trace = T))
>
> Uniquenesses:
>   v1    v2    v3    v4    v5    v6
> 0.005 0.101 0.005 0.224 0.084 0.005
>
> Loadings:
>   Factor1 Factor2 Factor3
> v1 0.944   0.182   0.267
> v2 0.905   0.235   0.159
> v3 0.236   0.210   0.946
> v4 0.180   0.242   0.828
> v5 0.242   0.881   0.286
> v6 0.193   0.959   0.196
>
>               Factor1 Factor2 Factor3
> SS loadings      1.893   1.886   1.797
> Proportion Var   0.316   0.314   0.300
> Cumulative Var   0.316   0.630   0.929
>
> The degrees of freedom for the model is 0 and the fit was 0.4755
> R >
>
> If you think the model should be OK and there isn't an error somewhere else
> then you'll want to fiddle with the control options. From the help for
> fractanal:
>
> control: A list of control values,
>
>          nstart The number of starting values to be tried if 'start =
>               NULL'. Default 1.
>
>          trace logical. Output tracing information? Default 'FALSE'.
>
>          lower The lower bound for uniquenesses during optimization.
>               Should be > 0. Default 0.005.
>
>          opt A list of control values to be passed to 'optim''s
>               'control' argument.
>
>          rotate a list of additional arguments for the rotation
>               function.
>
>     ...: Components of 'control' can also be supplied as named
>          arguments to 'factanal'.
>
>
> HTH, Andy
>
>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Catherine Dempsey
>> Sent: Tuesday, November 30, 2004 7:31 AM
>> To: 'r-help at lists.R-project.org'
>> Subject: [R] Info
>>
>>
>> I am having difficulty obtaining the scores from my principal component
>> analysis.  I have used this method before and have had no problems.  The
>> data set that I am using this time is similar to what I have used in the
>> past.  What do I need to do to my dataset in order for me to obtain these
>> scores?
>>
>> R screen says the following message
>>
>> Error in factanal(covmat = pasa.cov, factors = 4) :
>>         Unable to optimize from these starting value(s)
>>
>> many thanks
>>
>> Catherine
>>
>>
>> KSS Ltd
>> Seventh Floor  St James's Buildings  79 Oxford Street  Manchester
>>  M1 6SS  England
>> Company Registration Number 2800886
>> Tel: +44 (0) 161 228 0040	Fax: +44 (0) 161 236 6305
>> mailto:kssg at kssg.com		http://www.kssg.com
>>
>>
>> The information in this Internet email is confidential and m...{{dropped}}
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From deepayan at stat.wisc.edu  Tue Nov 30 16:32:59 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Tue, 30 Nov 2004 09:32:59 -0600
Subject: [R] plotting data in non-orthogonal coords.
In-Reply-To: <200411301106.59500.stu44414@mail.uni-kiel.de>
References: <41AC4AEB.15710.955B5A@localhost>
	<200411301106.59500.stu44414@mail.uni-kiel.de>
Message-ID: <200411300932.59942.deepayan@stat.wisc.edu>

On Tuesday 30 November 2004 04:06, Andreas Franke wrote:

[...]

> It would be nice if one could just plot data given as F(x,y) where
> you supply x and y for every data point seperatly so that you dont
> need any specific grid.

This is one solution, but it's not exactly what you want:


library(lattice)
u <- row(matrix(0, nrow = 10, ncol = 15))
v <- col(matrix(0, nrow = 10, ncol = 15))
x <- u
y <- .5 * u + .3 * v
z <- log(u + v)
wireframe(z ~ x * y)


Deepayan



From murdoch at stats.uwo.ca  Tue Nov 30 16:43:24 2004
From: murdoch at stats.uwo.ca (Duncan Murdoch)
Date: Tue, 30 Nov 2004 10:43:24 -0500
Subject: [R] How to know if a bug was recognised
In-Reply-To: <3.0.6.32.20041130132443.007a2a30@pop.gmx.net>
References: <3.0.6.32.20041130132443.007a2a30@pop.gmx.net>
Message-ID: <c65pq0h6rmcrrjtegoluesvvniroat4f64@4ax.com>

On Tue, 30 Nov 2004 13:24:43 +0100, Heinz Tuechler <tuechler at gmx.at>
wrote :

>Hello!
>
>A problem with special characters seemed to me to be a bug. I sent a mail
>to R-windows at r-project.org concerning the problem (see below).
>How can I find out, if this is considered as a bug or an error of myself?
>Which part of FAQs or documentation did I miss to find the answer?

If you send a private email, please use a return address that works.
I got messages that tuechler at gmx.at has been disabled when I tried to
respond there.

Duncan Murdoch



From lynnerbaker at yahoo.com  Tue Nov 30 16:50:31 2004
From: lynnerbaker at yahoo.com (Lynne Baker)
Date: Tue, 30 Nov 2004 07:50:31 -0800 (PST)
Subject: [R] Package for multivariate binary logistic regression?
Message-ID: <20041130155031.52848.qmail@web60801.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041130/6ae98a8a/attachment.pl

From jruiz99 at ensae.org  Tue Nov 30 16:55:27 2004
From: jruiz99 at ensae.org (jruiz99@ensae.org)
Date: Tue, 30 Nov 2004 15:55:27 GMT
Subject: [R] (no subject)
Message-ID: <20041130155527.8897386600@mail.fr.clara.net>



Hello,

I am trying to estimate a choice model with varying choice set for each
individual.

I would like to fit different kinds of model (logit ,nested logit,
probit...).

So far I have found that package *mnp* allows me to estimate a probit model
with varying choice set.

But for estimation of a logit model, I have only found function *multinom*
of package *nnet* which does not seem to allow for varying choice set.

I could incorporate indicators of choice availability as explanotary
variables, but it does not seem a very good way to do it.
Instead, for a logit model, I have coded a likelihood computation of the
underlying model with varying choice set and I use optim function to get
the "maximum".

I wondered if there were more user friendly functions available and/or
other choice model related package.

Thanks,
	Julien



From jruiz99 at ensae.org  Tue Nov 30 16:55:27 2004
From: jruiz99 at ensae.org (jruiz99@ensae.org)
Date: Tue, 30 Nov 2004 15:55:27 GMT
Subject: [R] (no subject)
Message-ID: <20041130155527.8897386600@mail.fr.clara.net>



Hello,

I am trying to estimate a choice model with varying choice set for each
individual.

I would like to fit different kinds of model (logit ,nested logit,
probit...).

So far I have found that package *mnp* allows me to estimate a probit model
with varying choice set.

But for estimation of a logit model, I have only found function *multinom*
of package *nnet* which does not seem to allow for varying choice set.

I could incorporate indicators of choice availability as explanotary
variables, but it does not seem a very good way to do it.
Instead, for a logit model, I have coded a likelihood computation of the
underlying model with varying choice set and I use optim function to get
the "maximum".

I wondered if there were more user friendly functions available and/or
other choice model related package.

Thanks,
	Julien



From Carlisle.Thacker at noaa.gov  Tue Nov 30 17:09:12 2004
From: Carlisle.Thacker at noaa.gov (Carlisle Thacker)
Date: Tue, 30 Nov 2004 11:09:12 -0500
Subject: [R] adding regression curve to xyplot
Message-ID: <41AC9B28.1010401@noaa.gov>

Dear R-listers,

It seems that predict() behaves differently within panel.xyplot.  Am I
doing something stupid?

Thanks,

Carlisle

First, without xyplot():
> lmtest <- lm(t~s,data=subset(P100,whichLon100==1 & whichLat100==1))
> lmtest

Call:
lm(formula = s ~ t, data = subset(P100, whichLon100 == 1 & whichLat100 ==
   1))

Coefficients:
(Intercept)            t
    33.3307       0.1393
> range(P100$t)
[1]  4.050469 24.514543
>> predict(lmtest,newdata=data.frame(t=range(P100$t)))
       1        2
33.89501 36.74620

As expected, predict gives two values.  But inside xyplot() predict gives
300 values:

> xyplot(t~s|factor(lonLabels[whichLon100])*factor(latLabels[whichLat100]),
+   data=P100,pch=".",
+   panel=function(x,y,...){panel.xyplot(x,y,...)
+                           thislm <- lm(x~y)
+                      print(thislm)
+                           newt <- range(P100$t)
+                      print(newt)
+                           news <-
as.vector(predict(thislm,newdata=data.frame(t=newt)))
+                      print(news)
+                           llines(news,newt,col="red")})

Call:
lm(formula = x ~ y)

Coefficients:
(Intercept)            y
    33.3307       0.1393

[1]  4.050469 24.514543
  [1] 34.16173 35.31284 34.95317 35.62314 34.64448 34.06225 34.19688 34.03391
  [9] 33.90860 35.28849 35.29911 34.61618 35.19837 35.28966 34.91978 35.32388
 [17] 35.39351 35.54533 35.58760 34.77039 35.07226 35.35643 35.40594 33.98609
 [25] 33.96461 33.91158 34.12389 34.33804 34.72235 34.63541 36.20420 36.13635
 [33] 34.60623 34.32401 35.59768 34.68576 34.64661 34.58804 34.74548 35.09062
 [41] 34.70290 34.22428 35.12743 34.90726 34.52325 34.90041 35.46984 34.79572
 [49] 35.47026 35.35481 34.70722 34.08883 33.97586 33.94797 33.96165 33.97715
 [57] 33.94886 33.99715 34.10887 34.72597 34.20534 34.45016 35.00769 34.56742
 [65] 34.76119 34.87190 34.60760 34.50113 34.56594 34.65954 34.57076 34.91072
 [73] 34.97322 34.99036 34.48229 34.58436 34.56767 34.94204 34.95929 34.89107
 [81] 34.98846 34.51328 34.74252 34.61337 34.62212 34.70953 34.74080 34.71630
 [89] 34.68599 35.58629 34.77031 34.49366 34.49873 34.48112 35.11327 34.62598
 [97] 34.50436 34.50705 34.70170 35.01278 35.12273 35.15541 35.06295 35.15366
[105] 35.15089 35.11312 35.09834 35.05271 34.84838 34.52225 34.34650 34.32637
[113] 34.42911 34.79282 35.06830 35.11559 35.25736 34.43281 34.45208 34.58407
[121] 35.28598 34.34246 34.32532 34.47483 34.79309 34.02772 34.01553 35.33372
[129] 34.19273 34.07622 34.52820 35.27539 35.37703 34.70560 34.77501 34.52084
[137] 34.68773 35.19217 35.69538 34.21886 34.17226 34.94784 34.03079 33.95669
[145] 33.92002 33.92972 33.96568 33.92996 33.94236 33.93887 33.92059 33.91746
[153] 33.98369 34.17190 34.05327 33.96500 34.16136 33.98172 34.02463 34.00802
[161] 34.05238 34.05088 34.16828 34.14888 33.94711 33.96567 34.12450 33.98988
[169] 33.98670 33.98489 34.05447 34.03689 34.17137 34.31777 34.05027 33.97821
[177] 33.99281 34.00209 34.07560 33.98840 33.98930 33.98548 33.99127 35.23652
[185] 35.17034 35.53546 35.53861 34.02384 34.03402 33.93877 33.93540 33.90636
[193] 33.92267 34.57678 34.49457 34.46038 34.64318 35.06561 35.06680 34.76466
[201] 34.00418 35.48020 35.12687 35.28180 35.07281 35.23499 35.14693 34.07914
[209] 33.99047 34.00648 34.10073 34.00784 33.99185 33.98191 34.03089 33.96755
[217] 34.17042 34.10803 34.12671 34.09942 34.25003 34.06542 34.07293 34.19048
[225] 34.06594 34.01929 34.00155 34.11555 33.89754 33.89527 33.89501 33.93433
[233] 33.89881 33.94577 33.96603 33.95199 34.00284 34.12974 33.98116 34.08882
[241] 34.00953 34.98977 34.72824 34.68920 35.15207 34.59716 34.64580 34.25087
[249] 34.96008 34.62607 35.32346 35.16993 34.46759 34.47157 34.46978 34.46092
[257] 34.46561 34.46489 34.49970 34.49599 34.55441 34.52048 34.58480 34.59267
[265] 34.50082 34.57311 34.60421 34.57043 34.53131 34.46365 34.41946 34.74566
[273] 34.64593 34.67921 34.65706 34.71625 34.61487 34.61687 34.58899 34.47081
[281] 34.58366 35.66823 35.61883 34.71477 34.72518 34.69732 34.66265 34.65186
[289] 34.81468 34.74725 34.77658 34.77191 34.81481 34.76427 34.70751 34.96763
[297] 35.11322 35.16852 34.94393 34.89605
Error in xy.coords(x, y) : x and y lengths differ



From ripley at stats.ox.ac.uk  Tue Nov 30 17:10:39 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 30 Nov 2004 16:10:39 +0000 (GMT)
Subject: [R] Package for multivariate binary logistic regression?
In-Reply-To: <20041130155031.52848.qmail@web60801.mail.yahoo.com>
References: <20041130155031.52848.qmail@web60801.mail.yahoo.com>
Message-ID: <Pine.LNX.4.61.0411301601490.8867@gannet.stats>

You'll have to help us a bit here.  `McFadden' comes up in connection with 
conditional logistic regression, which package survival supports.  Is that 
what you mean?

On Tue, 30 Nov 2004, Lynne Baker wrote:

> I am trying to find out if someone has implemented a (McFadden-type) 
> multivariate binary logistic regresssion package for R? From what I can 
> tell, this is not available for R.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tuechler at gmx.at  Tue Nov 30 17:14:57 2004
From: tuechler at gmx.at (Heinz Tuechler)
Date: Tue, 30 Nov 2004 17:14:57 +0100
Subject: [R] Attn Heinz Tuechler: Re: problem with sp ecial characters (
	=?iso-8859-1?Q?=E4?=  =?iso-8859-1?Q?,=F6,=FC)?=
Message-ID: <3.0.6.32.20041130171457.007a7100@pop.gmx.net>

Thank you for the information. I did already download the 27-11-2004
version and I have the intention to try it within a day and report on the
result, if this is of interest.

My address (tuechler at gmx.at) should work but I will check that too.

Heinz T??chler

At 10:27 30.11.2004 -0500, you wrote:
>[I tried to send this message privately, but the return address
>bounced.]
>
>I think this has been fixed in R-patched, but I doubt if the fix has
>been tested in Win98.  Could you please download a copy from
><http://cran.r-project.org/bin/windows/base/rpatched.html> and confirm
>that it has been fixed?
>
>Duncan Murdoch
>
>On Sat, 27 Nov 2004 23:31:23 +0100, Heinz Tuechler <tuechler at gmx.at>
>wrote :
>
>>Dear Developers!
>>
>>Using special characters I found a strange behaviour in R 2.0.1 and equally
>>in 
>>R : Copyright 2004, The R Foundation for Statistical Computing
>>Version 2.0.1  (2004-11-15), ISBN 3-900051-07-0
>>
>>Operating System: Windows 98SE
>>
>>example:
>>factor1<-as.factor(c("weiblich","m??nnlich","??sterreichisch","fr??hreif","Gru??
>>"))
>>factor1
>>> factor1
>>[1] weiblich           m\344nnlich        \366sterreichisch  fr\374hreif
>>   
>>[5] Gru\337           
>>Levels: fr??hreif Gru?? m??nnlich ??sterreichisch weiblich
>>
>>with best wishes
>>
>>Heinz T??chler
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From tuechler at gmx.at  Tue Nov 30 17:15:57 2004
From: tuechler at gmx.at (Heinz Tuechler)
Date: Tue, 30 Nov 2004 17:15:57 +0100
Subject: [R] How to know if a bug was recognised
Message-ID: <3.0.6.32.20041130171557.007aee40@pop.gmx.net>

Dear Henrik Bengtsson!

Thank you for your kind answer. I am sorry that at the time of writing my
inital message the last version of R was 2004-11-15. As soon as possible I
will try the new version.

with many thanks

Heinz T??chler


At 14:01 30.11.2004 +0100, you wrote:
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch 
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Heinz Tuechler
>> Sent: Tuesday, November 30, 2004 1:25 PM
>> To: r-help at stat.math.ethz.ch
>> Subject: [R] How to know if a bug was recognised
>> 
>> 
>> Hello!
>> 
>> A problem with special characters seemed to me to be a bug. I 
>> sent a mail to R-windows at r-project.org concerning the problem 
>> (see below). How can I find out, if this is considered as a 
>> bug or an error of myself? Which part of FAQs or 
>> documentation did I miss to find the answer?
>
>This will not answer your question on what is a bug or not, but if you don't
>know, the "R team" has kindly made a fix for this problem. What I remember
>from an earlier thread, this was not really due to R, but to Windows. For
>the latest R v2.0.1 patch it now seems to work as before/expected:
>
>R : Copyright 2004, The R Foundation for Statistical Computing
>Version 2.0.1 Patched (2004-11-27), ISBN 3-900051-07-0
>
>> "??"
>[1] "??"
>> "??"
>[1] "??"
>> "??"
>[1] "??"
>
>Cheers
>
>Henrik Bengtsson
>
>
>> thanks in advance
>> 
>> Heinz T??chler
>> 
>> -------------------- copy of abovementioned mail ----------
>> to: R-windows at r-project.org
>> subject: problem with special characters (??,??,??)
>> Dear Developers!
>> 
>> Using special characters I found a strange behaviour in R 
>> 2.0.1 and equally in 
>> R : Copyright 2004, The R Foundation for Statistical 
>> Computing Version 2.0.1  (2004-11-15), ISBN 3-900051-07-0
>> 
>> Operating System: Windows 98SE
>> 
>> example: 
>> factor1<-as.factor(c("weiblich","m??nnlich","??sterreichisch","f
>> r??hreif","Gru??
>> "))
>> factor1
>> > factor1
>> [1] weiblich           m\344nnlich        \366sterreichisch  
>> fr\374hreif
>>    
>> [5] Gru\337           
>> Levels: fr??hreif Gru?? m??nnlich ??sterreichisch weiblich
>> 
>> with best wishes
>> 
>> Heinz T??chler
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list 
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>> 
>> 
>
>



From tuechler at gmx.at  Tue Nov 30 17:15:36 2004
From: tuechler at gmx.at (Heinz Tuechler)
Date: Tue, 30 Nov 2004 17:15:36 +0100
Subject: [R] How to know if a bug was recognised
Message-ID: <3.0.6.32.20041130171536.00796620@pop.gmx.net>

Thank you for your answer.

BTW for a beginner like me it seems very difficult to ask a question
without being criticised.
Maybe that for a beginner it is not that easy to find an issue
which, as you say, "Indeed, no one reported the problem for 2.0.0 at all,
including all the alpha and beta test versions (except for Chinese where
the characters are invalid and so the behaviour was correct), and so the
workaround did not make 2.0.1." in the archives of the R mailing lists.
And believe me I was searching for several hours, obvoiusly not with the
right keywords.
I did not express any expectation about an answer, I hoped to get an answer
about what kind of reaction I should expect. Studying the "R Bug Tracking
System" was not successful.
I am aware of the character of the R project, I highly appreciate the work
of this group and of course I do not want to inhibit your occasional
attempts to have a life.

Heinz T??chler

At 12:55 30.11.2004 +0000, you wrote:
>You have already been sent an answer.  It is a bug in your copy of 
>Windows, and it will be worked around in the current version of R-patched. 
>From the CHANGES file:
>
>   R 2.0.1 patched
>   ===============
>
>   We work around reported bugs in Windows XP as to which characters are
>   printable by attempting to print all non-control characters when using
>   print().
>
>Note: no one else has reported a problem on 98SE, despite this having been 
>the behaviour of R for three months, and it seemed only XP SP2 was 
>affected.  Indeed, no one reported the problem for 2.0.0 at all, including 
>all the alpha and beta test versions (except for Chinese where the 
>characters are invalid and so the behaviour was correct), and so the 
>workaround did not make 2.0.1.
>
>BTW, your expectation that an email you sent at the weekend will be 
>answered by Tuesday is completely unreasonable.  R is a volunteer project, 
>and the developers do have other commitments (and occasionally make 
>attempts to have a life).
>
>[You could have read the archives of the R mailing lists to find that this 
>was a known issue that had already been addressed.]
>
>On Tue, 30 Nov 2004, Heinz Tuechler wrote:
>
>> Hello!
>>
>> A problem with special characters seemed to me to be a bug. I sent a mail
>> to R-windows at r-project.org concerning the problem (see below).
>> How can I find out, if this is considered as a bug or an error of myself?
>> Which part of FAQs or documentation did I miss to find the answer?
>>
>> thanks in advance
>>
>> Heinz T??chler
>>
>> -------------------- copy of abovementioned mail ----------
>> to: R-windows at r-project.org
>> subject: problem with special characters (??,??,??)
>> Dear Developers!
>>
>> Using special characters I found a strange behaviour in R 2.0.1 and equally
>> in
>> R : Copyright 2004, The R Foundation for Statistical Computing
>> Version 2.0.1  (2004-11-15), ISBN 3-900051-07-0
>>
>> Operating System: Windows 98SE
>>
>> example:
>>
factor1<-as.factor(c("weiblich","m??nnlich","??sterreichisch","fr??hreif","Gru??
>> "))
>> factor1
>>> factor1
>> [1] weiblich           m\344nnlich        \366sterreichisch  fr\374hreif
>>
>> [5] Gru\337
>> Levels: fr??hreif Gru?? m??nnlich ??sterreichisch weiblich
>>
>> with best wishes
>>
>> Heinz T??chler
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://stat.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>>
>>
>
>-- 
>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>University of Oxford,             Tel:  +44 1865 272861 (self)
>1 South Parks Road,                     +44 1865 272866 (PA)
>Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maustin at amgen.com  Tue Nov 30 17:18:41 2004
From: maustin at amgen.com (Austin, Matt)
Date: Tue, 30 Nov 2004 08:18:41 -0800
Subject: [R] adding regression curve to xyplot
Message-ID: <E7D5AB4811D20B489622AABA9C53859104E0DBD9@teal-exch.amgen.com>

Inside the panel, you have to only use the parts of x and y for each unique
combination of factor(lonLabels[whichLon100])*factor(latLabels[whichLat100])



So use   "thislm <- lm(x[subscripts]~y[subscripts])" in the panel function.


Matt Austin
Statistician

Amgen 
One Amgen Center Drive
M/S 24-2-C
Thousand Oaks CA 93021
(805) 447 - 7431


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Carlisle Thacker
> Sent: Tuesday, November 30, 2004 8:9 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] adding regression curve to xyplot
> 
> 
> Dear R-listers,
> 
> It seems that predict() behaves differently within panel.xyplot.  Am I
> doing something stupid?
> 
> Thanks,
> 
> Carlisle
> 
> First, without xyplot():
> > lmtest <- lm(t~s,data=subset(P100,whichLon100==1 & whichLat100==1))
> > lmtest
> 
> Call:
> lm(formula = s ~ t, data = subset(P100, whichLon100 == 1 & 
> whichLat100 ==
>    1))
> 
> Coefficients:
> (Intercept)            t
>     33.3307       0.1393
> > range(P100$t)
> [1]  4.050469 24.514543
> >> predict(lmtest,newdata=data.frame(t=range(P100$t)))
>        1        2
> 33.89501 36.74620
> 
> As expected, predict gives two values.  But inside xyplot() 
> predict gives
> 300 values:
> 
> > 
> xyplot(t~s|factor(lonLabels[whichLon100])*factor(latLabels[whi
> chLat100]),
> +   data=P100,pch=".",
> +   panel=function(x,y,...){panel.xyplot(x,y,...)
> +                           thislm <- lm(x~y)
> +                      print(thislm)
> +                           newt <- range(P100$t)
> +                      print(newt)
> +                           news <-
> as.vector(predict(thislm,newdata=data.frame(t=newt)))
> +                      print(news)
> +                           llines(news,newt,col="red")})
> 
> Call:
> lm(formula = x ~ y)
> 
> Coefficients:
> (Intercept)            y
>     33.3307       0.1393
> 
> [1]  4.050469 24.514543
>   [1] 34.16173 35.31284 34.95317 35.62314 34.64448 34.06225 
> 34.19688 34.03391
>   [9] 33.90860 35.28849 35.29911 34.61618 35.19837 35.28966 
> 34.91978 35.32388
>  [17] 35.39351 35.54533 35.58760 34.77039 35.07226 35.35643 
> 35.40594 33.98609
>  [25] 33.96461 33.91158 34.12389 34.33804 34.72235 34.63541 
> 36.20420 36.13635
>  [33] 34.60623 34.32401 35.59768 34.68576 34.64661 34.58804 
> 34.74548 35.09062
>  [41] 34.70290 34.22428 35.12743 34.90726 34.52325 34.90041 
> 35.46984 34.79572
>  [49] 35.47026 35.35481 34.70722 34.08883 33.97586 33.94797 
> 33.96165 33.97715
>  [57] 33.94886 33.99715 34.10887 34.72597 34.20534 34.45016 
> 35.00769 34.56742
>  [65] 34.76119 34.87190 34.60760 34.50113 34.56594 34.65954 
> 34.57076 34.91072
>  [73] 34.97322 34.99036 34.48229 34.58436 34.56767 34.94204 
> 34.95929 34.89107
>  [81] 34.98846 34.51328 34.74252 34.61337 34.62212 34.70953 
> 34.74080 34.71630
>  [89] 34.68599 35.58629 34.77031 34.49366 34.49873 34.48112 
> 35.11327 34.62598
>  [97] 34.50436 34.50705 34.70170 35.01278 35.12273 35.15541 
> 35.06295 35.15366
> [105] 35.15089 35.11312 35.09834 35.05271 34.84838 34.52225 
> 34.34650 34.32637
> [113] 34.42911 34.79282 35.06830 35.11559 35.25736 34.43281 
> 34.45208 34.58407
> [121] 35.28598 34.34246 34.32532 34.47483 34.79309 34.02772 
> 34.01553 35.33372
> [129] 34.19273 34.07622 34.52820 35.27539 35.37703 34.70560 
> 34.77501 34.52084
> [137] 34.68773 35.19217 35.69538 34.21886 34.17226 34.94784 
> 34.03079 33.95669
> [145] 33.92002 33.92972 33.96568 33.92996 33.94236 33.93887 
> 33.92059 33.91746
> [153] 33.98369 34.17190 34.05327 33.96500 34.16136 33.98172 
> 34.02463 34.00802
> [161] 34.05238 34.05088 34.16828 34.14888 33.94711 33.96567 
> 34.12450 33.98988
> [169] 33.98670 33.98489 34.05447 34.03689 34.17137 34.31777 
> 34.05027 33.97821
> [177] 33.99281 34.00209 34.07560 33.98840 33.98930 33.98548 
> 33.99127 35.23652
> [185] 35.17034 35.53546 35.53861 34.02384 34.03402 33.93877 
> 33.93540 33.90636
> [193] 33.92267 34.57678 34.49457 34.46038 34.64318 35.06561 
> 35.06680 34.76466
> [201] 34.00418 35.48020 35.12687 35.28180 35.07281 35.23499 
> 35.14693 34.07914
> [209] 33.99047 34.00648 34.10073 34.00784 33.99185 33.98191 
> 34.03089 33.96755
> [217] 34.17042 34.10803 34.12671 34.09942 34.25003 34.06542 
> 34.07293 34.19048
> [225] 34.06594 34.01929 34.00155 34.11555 33.89754 33.89527 
> 33.89501 33.93433
> [233] 33.89881 33.94577 33.96603 33.95199 34.00284 34.12974 
> 33.98116 34.08882
> [241] 34.00953 34.98977 34.72824 34.68920 35.15207 34.59716 
> 34.64580 34.25087
> [249] 34.96008 34.62607 35.32346 35.16993 34.46759 34.47157 
> 34.46978 34.46092
> [257] 34.46561 34.46489 34.49970 34.49599 34.55441 34.52048 
> 34.58480 34.59267
> [265] 34.50082 34.57311 34.60421 34.57043 34.53131 34.46365 
> 34.41946 34.74566
> [273] 34.64593 34.67921 34.65706 34.71625 34.61487 34.61687 
> 34.58899 34.47081
> [281] 34.58366 35.66823 35.61883 34.71477 34.72518 34.69732 
> 34.66265 34.65186
> [289] 34.81468 34.74725 34.77658 34.77191 34.81481 34.76427 
> 34.70751 34.96763
> [297] 35.11322 35.16852 34.94393 34.89605
> Error in xy.coords(x, y) : x and y lengths differ
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html



From adrian.mincu at from.ro  Tue Nov 30 17:16:47 2004
From: adrian.mincu at from.ro (adrian mincu)
Date: Tue, 30 Nov 2004 18:16:47 +0200
Subject: [R] combinations min problem
Message-ID: <200411301622.iAUGMSSO025751@hypatia.math.ethz.ch>

Hello,
as I was not yet able to find a good solution to the following 
problem, I'd appreciate some help.

 

I am working on finding

the minimum of a list of  vectors, each comprised of x parameters

to include

all combinations of n parameters taken k at a time,

x>k

thx. and rgds.

a/m



From ripley at stats.ox.ac.uk  Tue Nov 30 17:24:44 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 30 Nov 2004 16:24:44 +0000 (GMT)
Subject: [R] adding regression curve to xyplot
In-Reply-To: <41AC9B28.1010401@noaa.gov>
References: <41AC9B28.1010401@noaa.gov>
Message-ID: <Pine.LNX.4.61.0411301621130.10751@gannet.stats>

On Tue, 30 Nov 2004, Carlisle Thacker wrote:

> Dear R-listers,
>
> It seems that predict() behaves differently within panel.xyplot.  Am I
> doing something stupid?

You fitted the model lm(x ~ y) and supplied new values for t, not y.

Using panel.abline would be a bit easier: just call panel.abline(thislm) 
from your panel function.

> Thanks,
>
> Carlisle
>
> First, without xyplot():
>> lmtest <- lm(t~s,data=subset(P100,whichLon100==1 & whichLat100==1))
>> lmtest
>
> Call:
> lm(formula = s ~ t, data = subset(P100, whichLon100 == 1 & whichLat100 ==
>   1))
>
> Coefficients:
> (Intercept)            t
>    33.3307       0.1393
>> range(P100$t)
> [1]  4.050469 24.514543
>>> predict(lmtest,newdata=data.frame(t=range(P100$t)))
>       1        2
> 33.89501 36.74620
>
> As expected, predict gives two values.  But inside xyplot() predict gives
> 300 values:
>
>> xyplot(t~s|factor(lonLabels[whichLon100])*factor(latLabels[whichLat100]),
> +   data=P100,pch=".",
> +   panel=function(x,y,...){panel.xyplot(x,y,...)
> +                           thislm <- lm(x~y)
> +                      print(thislm)
> +                           newt <- range(P100$t)
> +                      print(newt)
> +                           news <-
> as.vector(predict(thislm,newdata=data.frame(t=newt)))
> +                      print(news)
> +                           llines(news,newt,col="red")})
>
> Call:
> lm(formula = x ~ y)
>
> Coefficients:
> (Intercept)            y
>    33.3307       0.1393
>
> [1]  4.050469 24.514543
>  [1] 34.16173 35.31284 34.95317 35.62314 34.64448 34.06225 34.19688 34.03391
>  [9] 33.90860 35.28849 35.29911 34.61618 35.19837 35.28966 34.91978 35.32388
> [17] 35.39351 35.54533 35.58760 34.77039 35.07226 35.35643 35.40594 33.98609
> [25] 33.96461 33.91158 34.12389 34.33804 34.72235 34.63541 36.20420 36.13635
> [33] 34.60623 34.32401 35.59768 34.68576 34.64661 34.58804 34.74548 35.09062
> [41] 34.70290 34.22428 35.12743 34.90726 34.52325 34.90041 35.46984 34.79572
> [49] 35.47026 35.35481 34.70722 34.08883 33.97586 33.94797 33.96165 33.97715
> [57] 33.94886 33.99715 34.10887 34.72597 34.20534 34.45016 35.00769 34.56742
> [65] 34.76119 34.87190 34.60760 34.50113 34.56594 34.65954 34.57076 34.91072
> [73] 34.97322 34.99036 34.48229 34.58436 34.56767 34.94204 34.95929 34.89107
> [81] 34.98846 34.51328 34.74252 34.61337 34.62212 34.70953 34.74080 34.71630
> [89] 34.68599 35.58629 34.77031 34.49366 34.49873 34.48112 35.11327 34.62598
> [97] 34.50436 34.50705 34.70170 35.01278 35.12273 35.15541 35.06295 35.15366
> [105] 35.15089 35.11312 35.09834 35.05271 34.84838 34.52225 34.34650 34.32637
> [113] 34.42911 34.79282 35.06830 35.11559 35.25736 34.43281 34.45208 34.58407
> [121] 35.28598 34.34246 34.32532 34.47483 34.79309 34.02772 34.01553 35.33372
> [129] 34.19273 34.07622 34.52820 35.27539 35.37703 34.70560 34.77501 34.52084
> [137] 34.68773 35.19217 35.69538 34.21886 34.17226 34.94784 34.03079 33.95669
> [145] 33.92002 33.92972 33.96568 33.92996 33.94236 33.93887 33.92059 33.91746
> [153] 33.98369 34.17190 34.05327 33.96500 34.16136 33.98172 34.02463 34.00802
> [161] 34.05238 34.05088 34.16828 34.14888 33.94711 33.96567 34.12450 33.98988
> [169] 33.98670 33.98489 34.05447 34.03689 34.17137 34.31777 34.05027 33.97821
> [177] 33.99281 34.00209 34.07560 33.98840 33.98930 33.98548 33.99127 35.23652
> [185] 35.17034 35.53546 35.53861 34.02384 34.03402 33.93877 33.93540 33.90636
> [193] 33.92267 34.57678 34.49457 34.46038 34.64318 35.06561 35.06680 34.76466
> [201] 34.00418 35.48020 35.12687 35.28180 35.07281 35.23499 35.14693 34.07914
> [209] 33.99047 34.00648 34.10073 34.00784 33.99185 33.98191 34.03089 33.96755
> [217] 34.17042 34.10803 34.12671 34.09942 34.25003 34.06542 34.07293 34.19048
> [225] 34.06594 34.01929 34.00155 34.11555 33.89754 33.89527 33.89501 33.93433
> [233] 33.89881 33.94577 33.96603 33.95199 34.00284 34.12974 33.98116 34.08882
> [241] 34.00953 34.98977 34.72824 34.68920 35.15207 34.59716 34.64580 34.25087
> [249] 34.96008 34.62607 35.32346 35.16993 34.46759 34.47157 34.46978 34.46092
> [257] 34.46561 34.46489 34.49970 34.49599 34.55441 34.52048 34.58480 34.59267
> [265] 34.50082 34.57311 34.60421 34.57043 34.53131 34.46365 34.41946 34.74566
> [273] 34.64593 34.67921 34.65706 34.71625 34.61487 34.61687 34.58899 34.47081
> [281] 34.58366 35.66823 35.61883 34.71477 34.72518 34.69732 34.66265 34.65186
> [289] 34.81468 34.74725 34.77658 34.77191 34.81481 34.76427 34.70751 34.96763
> [297] 35.11322 35.16852 34.94393 34.89605
> Error in xy.coords(x, y) : x and y lengths differ
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andreas.cordes at stud.uni-goettingen.de  Tue Nov 30 17:28:40 2004
From: andreas.cordes at stud.uni-goettingen.de (Andreas Cordes)
Date: Tue, 30 Nov 2004 17:28:40 +0100
Subject: [R] Using mimR
Message-ID: <41AC9FB8.3090504@stud.uni-goettingen.de>

Hi,
I am trying to use the mimR Package. According to its help pages it 
needs the RDCOMClient package to run.
When I try to evaluate a model I get

 > m.sat<-mim("..",data=gm.dat)
Error in mim.cmd("clear; clear output") : couldn't find function "COMCreate"
 > m2.sat<-emfit(m.sat)
Error in toMIM(mim$data) : Object "m.sat" not found
 > imputeMissing()
Error in mim.cmd("impute") : couldn't find function "COMCreate"
 > imp.dat<-retrieveData()
Error in mim.cmd("pf 12,8", look.nice = FALSE) :
        couldn't find function "COMCreate"

I first thought mimR might not automatically load RDCOMClient, but it 
turned out that RDCOMClient may not be called vie library at al, even 
though it was installed correctly.

 > library(RDCOMClient)
Error in library(RDCOMClient) : 'RDCOMClient' is not a valid package -- 
installed < 2.0.0?
 >

Does anybody have an idea what the source of the problem might be?

thanks and best regards
Andreas



From deepayan at stat.wisc.edu  Tue Nov 30 17:31:56 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Tue, 30 Nov 2004 10:31:56 -0600
Subject: [R] adding regression curve to xyplot
In-Reply-To: <41AC9B28.1010401@noaa.gov>
References: <41AC9B28.1010401@noaa.gov>
Message-ID: <200411301031.56665.deepayan@stat.wisc.edu>

On Tuesday 30 November 2004 10:09, Carlisle Thacker wrote:
> Dear R-listers,
>
> It seems that predict() behaves differently within panel.xyplot.  Am
> I doing something stupid?
>
> Thanks,
>
> Carlisle
>
> First, without xyplot():
> > lmtest <- lm(t~s,data=subset(P100,whichLon100==1 & whichLat100==1))
> > lmtest
>
> Call:
> lm(formula = s ~ t, data = subset(P100, whichLon100 == 1 &
> whichLat100 == 1))
>
> Coefficients:
> (Intercept)            t
>     33.3307       0.1393
>
> > range(P100$t)
>
> [1]  4.050469 24.514543
>
> >> predict(lmtest,newdata=data.frame(t=range(P100$t)))
>
>        1        2
> 33.89501 36.74620
>
> As expected, predict gives two values.  But inside xyplot() predict
> gives
>
> 300 values:
> > xyplot(t~s|factor(lonLabels[whichLon100])*factor(latLabels[whichLat
> >100]),
>
> +   data=P100,pch=".",
> +   panel=function(x,y,...){panel.xyplot(x,y,...)
> +                           thislm <- lm(x~y)
> +                      print(thislm)
> +                           newt <- range(P100$t)
> +                      print(newt)
> +                           news <-
> as.vector(predict(thislm,newdata=data.frame(t=newt)))

Why 't' here? The variables involved in 'thislm' are 'x' and 'y'.

Deepayan

> +                      print(news)
> +                           llines(news,newt,col="red")})

[...]



From h.wickham at gmail.com  Tue Nov 30 17:32:06 2004
From: h.wickham at gmail.com (hadley wickham)
Date: Tue, 30 Nov 2004 10:32:06 -0600
Subject: [R] adding regression curve to xyplot
In-Reply-To: <41AC9B28.1010401@noaa.gov>
References: <41AC9B28.1010401@noaa.gov>
Message-ID: <f8e6ff0504113008326538b2ef@mail.gmail.com>

If you only want a simple linear regression, you might also want to try

 xyplot(t~s|factor(lonLabels[whichLon100])*factor(latLabels[whichLat100]),
+   data=P100,pch=".", type =c("p","r")

 - type "r" will automatically fit and plot the regression line for you.

Hadley



From deepayan at stat.wisc.edu  Tue Nov 30 17:43:04 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Tue, 30 Nov 2004 10:43:04 -0600
Subject: [R] adding regression curve to xyplot
In-Reply-To: <Pine.LNX.4.61.0411301621130.10751@gannet.stats>
References: <41AC9B28.1010401@noaa.gov>
	<Pine.LNX.4.61.0411301621130.10751@gannet.stats>
Message-ID: <200411301043.04179.deepayan@stat.wisc.edu>

On Tuesday 30 November 2004 10:24, Prof Brian Ripley wrote:
> On Tue, 30 Nov 2004, Carlisle Thacker wrote:
> > Dear R-listers,
> >
> > It seems that predict() behaves differently within panel.xyplot. 
> > Am I doing something stupid?
>
> You fitted the model lm(x ~ y) and supplied new values for t, not y.
>
> Using panel.abline would be a bit easier: just call
> panel.abline(thislm) from your panel function.

That wouldn't quite work, since the response and predictor are switched 
in the lm fit.

Deepayan



From bwheeler at echip.com  Tue Nov 30 17:41:39 2004
From: bwheeler at echip.com (Bob Wheeler)
Date: Tue, 30 Nov 2004 11:41:39 -0500
Subject: [R] 2k-factorial design with 10 parameters
In-Reply-To: <Pine.LNX.4.61.0411301215080.13214@gannet.stats>
References: <41AC60AB.2040806@gmx.de>
	<Pine.LNX.4.61.0411301215080.13214@gannet.stats>
Message-ID: <41ACA2C3.4020907@echip.com>



> On Tue, 30 Nov 2004, Sven wrote:
> 
>> I'd like to apply a 2^k factorial design with k=10 parameters. 
>> Obviously this results in a quite long term for the model equation due 
>> to the high number of combinations of parameters.
>>
>> How can I specify the equation for the linear model (lm) without 
>> writing all combinations explicitly down by hand? Does a R command 
>> exist for this problematic?
> 

Use expand.formula() in the AlgDesign package.

-- 
Bob Wheeler --- http://www.bobwheeler.com/
         ECHIP, Inc. ---
Randomness comes in bunches.



From deepayan at stat.wisc.edu  Tue Nov 30 17:47:40 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Tue, 30 Nov 2004 10:47:40 -0600
Subject: [R] adding regression curve to xyplot
In-Reply-To: <E7D5AB4811D20B489622AABA9C53859104E0DBD9@teal-exch.amgen.com>
References: <E7D5AB4811D20B489622AABA9C53859104E0DBD9@teal-exch.amgen.com>
Message-ID: <200411301047.40509.deepayan@stat.wisc.edu>

On Tuesday 30 November 2004 10:18, Austin, Matt wrote:
> Inside the panel, you have to only use the parts of x and y for each
> unique combination of
> factor(lonLabels[whichLon100])*factor(latLabels[whichLat100])
>
>
>
> So use   "thislm <- lm(x[subscripts]~y[subscripts])" in the panel
> function.

That's not true (at least in this situation). The 'x' and 'y' passed to 
the panel function by xyplot are already the appropriate subsets. 
('groups' would have been passed whole, had it been there.)

Deepayan



From baron at psych.upenn.edu  Tue Nov 30 17:52:15 2004
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Tue, 30 Nov 2004 11:52:15 -0500
Subject: [R] impute missing values in correlated variables: transcan?
Message-ID: <20041130165215.GA21580@psych>

I would like to impute missing data in a set of correlated
variables (columns of a matrix).  It looks like transcan() from
Hmisc is roughly what I want.  It says, "transcan automatically
transforms continuous and categorical variables to have maximum
correlation with the best linear combination of the other
variables." And, "By default, transcan imputes NAs with "best
guess" expected values of transformed variables, back transformed
to the original scale."

But I can't get it to work.  I say

m1 <- matrix(1:20+rnorm(20),5,)  # four correlated variables
colnames(m1) <- paste("R",1:4,sep="")
m1[c(2,19)] <- NA                # simulate some missing data
library(Hmisc)
transcan(m1,data=m1)

and I get

Error in rcspline.eval(y, nk = nk, inclx = TRUE) : 
      fewer than 6 non-missing observations with knots omitted

I've tried a few other things, but I think it is time to ask for
help.

The specific problem is a real one.  Our graduate admissions
committee (4 members) rates applications, and we average the
ratings to get an overall rating for each applicant.  Sometimes
one of the committee members is absent, or late; hence the
missing data.  The members differ in the way they use the rating
scale, in both slope and intercept (if you regress each on the
mean).  Many decisions end up depending on the second decimal
place of the averages, so we want to do better than just averging
the non-missing ratings.

Maybe I'm just not seeing something really simple.  In fact, the
problem is simpler than transcan assumes, since we are willing to
assume linearity of the regression of each variable on the other
variables.  Other members proposed solutions that assumed this,
but they did not take into account the fact that missing data at
the high or low end of each variable (each member's ratings)
would change its mean.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron
R search page: http://finzi.psych.upenn.edu/



From HankeA at mar.dfo-mpo.gc.ca  Tue Nov 30 17:54:54 2004
From: HankeA at mar.dfo-mpo.gc.ca (Hanke, Alex)
Date: Tue, 30 Nov 2004 12:54:54 -0400
Subject: [R] Google for scientists: search engine
Message-ID: <E37EEC6DE3A0C5439B7E7B07406C24AE124A73@msgmarsta01.bio.dfo.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041130/1aa72982/attachment.pl

From Carlisle.Thacker at noaa.gov  Tue Nov 30 17:59:28 2004
From: Carlisle.Thacker at noaa.gov (Carlisle Thacker)
Date: Tue, 30 Nov 2004 11:59:28 -0500
Subject: [R] adding regression curve to xyplot
In-Reply-To: <200411301043.04179.deepayan@stat.wisc.edu>
References: <41AC9B28.1010401@noaa.gov>
	<Pine.LNX.4.61.0411301621130.10751@gannet.stats>
	<200411301043.04179.deepayan@stat.wisc.edu>
Message-ID: <41ACA6F0.2030902@noaa.gov>

The axes are switched in xyplot.  Oceanographers like to have temperature
as the vertical axis and salinity horizontal.  But temperature is used to
predict salinity.  Confusing, and makes everything complicated.

So panel.abline(thislm) would give the wrong line, as would
type=c("p","r").  And neither would handle a parabolic fit.

The problem was my using t rather than y when getting news.

Thanks for all the help.

Deepayan Sarkar wrote:
> On Tuesday 30 November 2004 10:24, Prof Brian Ripley wrote:
> 
>>On Tue, 30 Nov 2004, Carlisle Thacker wrote:
>>
>>>Dear R-listers,
>>>
>>>It seems that predict() behaves differently within panel.xyplot. 
>>>Am I doing something stupid?
>>
>>You fitted the model lm(x ~ y) and supplied new values for t, not y.
>>
>>Using panel.abline would be a bit easier: just call
>>panel.abline(thislm) from your panel function.
> 
> 
> That wouldn't quite work, since the response and predictor are switched 
> in the lm fit.
> 
> Deepayan
>



From ligges at statistik.uni-dortmund.de  Tue Nov 30 18:02:47 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 30 Nov 2004 18:02:47 +0100
Subject: [R] Using mimR
In-Reply-To: <41AC9FB8.3090504@stud.uni-goettingen.de>
References: <41AC9FB8.3090504@stud.uni-goettingen.de>
Message-ID: <41ACA7B7.4020701@statistik.uni-dortmund.de>

Andreas Cordes wrote:

> Hi,
> I am trying to use the mimR Package. According to its help pages it 
> needs the RDCOMClient package to run.
> When I try to evaluate a model I get
> 
>  > m.sat<-mim("..",data=gm.dat)
> Error in mim.cmd("clear; clear output") : couldn't find function 
> "COMCreate"
>  > m2.sat<-emfit(m.sat)
> Error in toMIM(mim$data) : Object "m.sat" not found
>  > imputeMissing()
> Error in mim.cmd("impute") : couldn't find function "COMCreate"
>  > imp.dat<-retrieveData()
> Error in mim.cmd("pf 12,8", look.nice = FALSE) :
>        couldn't find function "COMCreate"
> 
> I first thought mimR might not automatically load RDCOMClient, but it 
> turned out that RDCOMClient may not be called vie library at al, even 
> though it was installed correctly.
> 
>  > library(RDCOMClient)
> Error in library(RDCOMClient) : 'RDCOMClient' is not a valid package -- 
> installed < 2.0.0?
>  >
> 
> Does anybody have an idea what the source of the problem might be?

That your (binary!) version of the RDCOMClient package has been 
installed with R < 2.0.0 ?

Please install a recent version compiled for R-2.0.x. A binary for 
R-2.0.x is available from the Omegahat webpage, or compile from sources 
yourself (you just need to replace one line in Makefile.win, AFAIR).


Uwe Ligges




> thanks and best regards
> Andreas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From berthoulyanne at hotmail.com  Tue Nov 30 18:15:03 2004
From: berthoulyanne at hotmail.com (anne berthouly)
Date: Tue, 30 Nov 2004 17:15:03 +0000
Subject: [R] (no subject)
Message-ID: <BAY23-F169DE9A05DDB5904BFFC93ADBE0@phx.gbl>

Dear all,

I??ve got problems concerning crossed random effects and nested effects using 
the function  "lme": I have two crossed random effects and I would like to 
nest each of them in a fixed factor and also test for the effects of these 
fixed factors.
At the moment my model looks like this:
Lme1<-lme(x~(A+B+C+D+E)^2, random=list(grp 
=pdBlocked(list(pdIdent(~Z-1),pdIdent(~W-1)))))
I would like to nest Z in A and C (C/A/Z), an W in B and C (C/B/W) (A,B,C 
being 3 fixed factors with two levels).

If  you have a response to this kind of problem, could you please help me.

Thank you,
Anne Berthouly



From rkoenker at uiuc.edu  Tue Nov 30 18:23:26 2004
From: rkoenker at uiuc.edu (roger koenker)
Date: Tue, 30 Nov 2004 11:23:26 -0600
Subject: [R] impute missing values in correlated variables: transcan?
In-Reply-To: <20041130165215.GA21580@psych>
References: <20041130165215.GA21580@psych>
Message-ID: <8B634752-42F4-11D9-86C7-000A95A7E3AA@uiuc.edu>

At the risk of stirring up a hornet's nest , I'd suggest that
means are dangerous in such applications.  A nice paper
on combining ratings is:  Gilbert Bassett and Joseph  Persky,
Rating Skating,  JASA, 1994,  1075-1079.


url:	www.econ.uiuc.edu/~roger        	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820

On Nov 30, 2004, at 10:52 AM, Jonathan Baron wrote:

> I would like to impute missing data in a set of correlated
> variables (columns of a matrix).  It looks like transcan() from
> Hmisc is roughly what I want.  It says, "transcan automatically
> transforms continuous and categorical variables to have maximum
> correlation with the best linear combination of the other
> variables." And, "By default, transcan imputes NAs with "best
> guess" expected values of transformed variables, back transformed
> to the original scale."
>
> But I can't get it to work.  I say
>
> m1 <- matrix(1:20+rnorm(20),5,)  # four correlated variables
> colnames(m1) <- paste("R",1:4,sep="")
> m1[c(2,19)] <- NA                # simulate some missing data
> library(Hmisc)
> transcan(m1,data=m1)
>
> and I get
>
> Error in rcspline.eval(y, nk = nk, inclx = TRUE) :
>       fewer than 6 non-missing observations with knots omitted
>
> I've tried a few other things, but I think it is time to ask for
> help.
>
> The specific problem is a real one.  Our graduate admissions
> committee (4 members) rates applications, and we average the
> ratings to get an overall rating for each applicant.  Sometimes
> one of the committee members is absent, or late; hence the
> missing data.  The members differ in the way they use the rating
> scale, in both slope and intercept (if you regress each on the
> mean).  Many decisions end up depending on the second decimal
> place of the averages, so we want to do better than just averging
> the non-missing ratings.
>
> Maybe I'm just not seeing something really simple.  In fact, the
> problem is simpler than transcan assumes, since we are willing to
> assume linearity of the regression of each variable on the other
> variables.  Other members proposed solutions that assumed this,
> but they did not take into account the fact that missing data at
> the high or low end of each variable (each member's ratings)
> would change its mean.
>
> Jon
> -- 
> Jonathan Baron, Professor of Psychology, University of Pennsylvania
> Home page: http://www.sas.upenn.edu/~baron
> R search page: http://finzi.psych.upenn.edu/
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From zk_h1 at yahoo.ca  Tue Nov 30 18:50:02 2004
From: zk_h1 at yahoo.ca (kh zu)
Date: Tue, 30 Nov 2004 12:50:02 -0500 (EST)
Subject: [R] about the subscript assignment porblem
Message-ID: <20041130175002.82127.qmail@web54102.mail.yahoo.com>

Hi, Dear,

  I have a problem for the new version 2.0 of R
When I put this code in it give me the error:

uis$ivhx3[uis$ivhx>0] <- 1*(uis$ivhx==3)
Error: NAs are not allowed in subscripted assignments

but this would happen in version 1.91
there are missing data in the data set and it is
represented as NA
the data set can be find on line 
at the ucla web
actually this is a example from its web site

the c code is at
https://svn.r-project.org/R/trunk/src/main/subassign.c

:
if(length(y) > 1)
	for(i = 0; i < n; i++)
	    if(INTEGER(indx)[i] == NA_INTEGER)
		error("NAs are not allowed in subscripted
assignments");
 So maybe we need get rid of this conditional code.

  

  Best Wishes
  Ku



From baron at psych.upenn.edu  Tue Nov 30 18:53:21 2004
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Tue, 30 Nov 2004 12:53:21 -0500
Subject: [R] impute missing values in correlated variables: transcan?
In-Reply-To: <8B634752-42F4-11D9-86C7-000A95A7E3AA@uiuc.edu>
References: <20041130165215.GA21580@psych>
	<8B634752-42F4-11D9-86C7-000A95A7E3AA@uiuc.edu>
Message-ID: <20041130175321.GA20849@psych>

On 11/30/04 11:23, roger koenker wrote:
>At the risk of stirring up a hornet's nest , I'd suggest that
>means are dangerous in such applications.  A nice paper
>on combining ratings is:  Gilbert Bassett and Joseph  Persky,
>Rating Skating,  JASA, 1994,  1075-1079.

Here is the abstract, which seems to capture what the article
says:

"Among judged sports, figure skating uses a unique method of
median ranks for determining placement. This system responds
positively to increased marks by each judge and follows majority
rule when a majority of judges agree on a skater's rank. It is
demonstrated that this is the only aggregation system possessing
these two properties. Median ranks provide strong safeguards
against manipulation by a minority of judges. These positive
features do not require the sacrifice of efficiency in
controlling measurement error. In a Monte Carlo study, the median
rank system consistently outperforms alternatives when judges'
marks are significantly skewed toward an upper limit."

I think this is irrelevant.  We are using ratings, not rankings.

(And there was a small error in my original post.  The disturbing
effect of missing data at the high or low end would be on the
slope rather than the intercept or mean.)

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron



From Jean.Coursol at math.u-psud.fr  Tue Nov 30 18:57:11 2004
From: Jean.Coursol at math.u-psud.fr (Jean Coursol)
Date: Tue, 30 Nov 2004 18:57:11 +0100 (CET)
Subject: [R] RecordPlot
Message-ID: <200411301757.SAA18807@jacaranda.math.u-psud.fr>

I want to do a zoom with recordPlot(). I have problems with lists.
(R-2.0.1 patched 2004-11-30 , various linux). I have problems 
with RecordPlot class structure.

> plot(1:10)
> saveP <- recordPlot()
> dev.off()

> sx <- saveP[[1]][[2]][[2]] 
> saveP[[1]][[2]][[2]] <- sx 
Error in "[[<-"(`*tmp*`, 1, value = list(list(
.Primitive("plot.new")), list(.Primitive("plot
.window"), c(1,  : 
        incompatible types

> typeof(saveP[[1]][[2]][[2]])
[1] "double"
> typeof(sx)
[1] "double"
        

But:

> s1 <- saveP[[1]]
> s1[[2]][[2]] <- c(4,6)
> saveP[1] <- list(s1)
> saveP                  # zoom is OK

Also, I don't understand recursive indexing.
With not modified saveP, 

> saveP[[c(1,2)]]
[[1]]
.Primitive("plot.window")

[[2]]
[1]  1 10

[[3]]
[1]  1 10

[[4]]
[1] ""

[[5]]
[1] NA

# OK

> saveP[[c(1,2,2)]]
Error: recursive indexing failed at level 2  

# why not [1]  1 10 ??

Jean Coursol



From asemeria at cramont.it  Tue Nov 30 19:10:45 2004
From: asemeria at cramont.it (asemeria@cramont.it)
Date: Tue, 30 Nov 2004 19:10:45 +0100
Subject: [R] A basic question
Message-ID: <OF81F8FF7F.54390430-ONC1256F5C.0063656B-C1256F5C.0062B1F2@tomware.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041130/239635c2/attachment.pl

From elvis at xlsolutions-corp.com  Tue Nov 30 19:13:37 2004
From: elvis at xlsolutions-corp.com (elvis@xlsolutions-corp.com)
Date: Tue, 30 Nov 2004 11:13:37 -0700
Subject: [R] Course ****R/S System: Advanced Programming,
	Seattle***December 20-21 
Message-ID: <20041130181337.9359.qmail@webmail03.mesa1.secureserver.net>

R/S Advanced Programming course in Seattle on 
December 20-21
Please check out the full description and Agenda of 
the 2-day class on the website:

www.xlsolutions-corp.com/Radv.htm


And let us know if we should hold seats for you.
Ask for group discount!


Here's the outline:

Course outline:
- Overview of R/S fundamentals: Syntax and Semantics
- Class and Inheritance in R/S-Plus
- Concepts, Construction and good use of language objects
- Coercion and efficiency
- Object-oriented programming in R and S-Plus
- Advanced manipulation tools: Parse, Deparse, Substitute, etc.
- How to fully take advantage of Vectorization
- Generic and Method Functions; S4 (S-Plus 6)
- Search path, databases and frames Visibility
- Working with large objects
- Handling Properly Recursion and iterative calculations
- Managing loops; For (S-Plus) and for() loops
- Consequences of Lazy Evaluation
- Efficient Code practices for large computations
- Memory management and Resource monitoring
- Writing R/S-Plus functions to call compiled code
- Writing and debugging compiled code for R/S-Plus system
- Connecting R/S-Plus to External Data Sources
- Understanding the structure of model fitting functions in R/S-Plus
- Designing and Packaging efficiently 


This course will also deal with lots of S-Plus efficiency issues and
any special topics from participants is welcome.
Please let us know if you and your colleagues are interested in this
class to take advantage of group discount. Register now to secure your
seat 
in this course!

Cheers,
Elvis Miller, PhD
Manager Training.
XLSolutions Corporation
206 686 1578
www.xlsolutions-corp.com



From tlumley at u.washington.edu  Tue Nov 30 19:56:27 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 30 Nov 2004 10:56:27 -0800 (PST)
Subject: [R] about the subscript assignment porblem
In-Reply-To: <20041130175002.82127.qmail@web54102.mail.yahoo.com>
References: <20041130175002.82127.qmail@web54102.mail.yahoo.com>
Message-ID: <Pine.A41.4.61b.0411301033040.206220@homer11.u.washington.edu>

On Tue, 30 Nov 2004, kh zu wrote:

> Hi, Dear,
>
>  I have a problem for the new version 2.0 of R
> When I put this code in it give me the error:
>
> uis$ivhx3[uis$ivhx>0] <- 1*(uis$ivhx==3)
> Error: NAs are not allowed in subscripted assignments
>
> but this would happen in version 1.91

What would happen?  In version 1.9.0 (I don't have a 1.9.1 lying around) 
you get a warning if any of the subscripts are FALSE
   > x<-1:3
   > y<-7:9
   > x[c(TRUE,NA,FALSE)]<-y
   Warning message:
   number of items to replace is not a multiple of replacement length

and if all the subscripts are TRUE or NA the NAs are treated as FALSE

   > x<-1:3
   > y<-7:9
   > x[c(TRUE,NA,TRUE)]<-y
   > x
   [1] 7 2 9


The change was largely motivated by numerical indices, where the problem 
is worse.   In particular if x is 1,2,3, i is 1, NA, 2  and y is 7,8,9, 
should
    x[i] <- y
set the second element of x to 8 or 9?  The answer was different for 
vectors and matrices in 1.9.1.

It was clearly a deliberate change, not a mistake (it's hard to see how it 
could have happened accidentally, and it was documented), and the previous 
behaviour was wrong.


 	-thomas



From f.harrell at vanderbilt.edu  Tue Nov 30 20:21:26 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 30 Nov 2004 13:21:26 -0600
Subject: [R] impute missing values in correlated variables: transcan?
In-Reply-To: <20041130165215.GA21580@psych>
References: <20041130165215.GA21580@psych>
Message-ID: <41ACC836.2070107@vanderbilt.edu>

Jonathan Baron wrote:
> I would like to impute missing data in a set of correlated
> variables (columns of a matrix).  It looks like transcan() from
> Hmisc is roughly what I want.  It says, "transcan automatically
> transforms continuous and categorical variables to have maximum
> correlation with the best linear combination of the other
> variables." And, "By default, transcan imputes NAs with "best
> guess" expected values of transformed variables, back transformed
> to the original scale."
> 
> But I can't get it to work.  I say
> 
> m1 <- matrix(1:20+rnorm(20),5,)  # four correlated variables
> colnames(m1) <- paste("R",1:4,sep="")
> m1[c(2,19)] <- NA                # simulate some missing data
> library(Hmisc)
> transcan(m1,data=m1)
> 
> and I get
> 
> Error in rcspline.eval(y, nk = nk, inclx = TRUE) : 
>       fewer than 6 non-missing observations with knots omitted

Jonathan - you would need many more observations to be able to fit 
flexible additive models as transcan does.  Also note that single 
imputation has problems and you may want to consider multiple imputation 
as done by the Hmisc aregImpute function, if you had more data.

Frank

> 
> I've tried a few other things, but I think it is time to ask for
> help.
> 
> The specific problem is a real one.  Our graduate admissions
> committee (4 members) rates applications, and we average the
> ratings to get an overall rating for each applicant.  Sometimes
> one of the committee members is absent, or late; hence the
> missing data.  The members differ in the way they use the rating
> scale, in both slope and intercept (if you regress each on the
> mean).  Many decisions end up depending on the second decimal
> place of the averages, so we want to do better than just averging
> the non-missing ratings.
> 
> Maybe I'm just not seeing something really simple.  In fact, the
> problem is simpler than transcan assumes, since we are willing to
> assume linearity of the regression of each variable on the other
> variables.  Other members proposed solutions that assumed this,
> but they did not take into account the fact that missing data at
> the high or low end of each variable (each member's ratings)
> would change its mean.
> 
> Jon


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From arshia22 at yahoo.com  Tue Nov 30 20:31:21 2004
From: arshia22 at yahoo.com (ebashi)
Date: Tue, 30 Nov 2004 11:31:21 -0800 (PST)
Subject: [R] xy_plot
Message-ID: <20041130193121.49492.qmail@web81010.mail.yahoo.com>

R users; 
Does anyone have a recommendation for a faster
plotting function rather than plot(x,y)?
when I use plot(x,y) for a large number of variables,
it take couple of moments to plot the graph?

Sincerely,

Sean



From dhinds at sonic.net  Tue Nov 30 20:33:13 2004
From: dhinds at sonic.net (David Hinds)
Date: Tue, 30 Nov 2004 11:33:13 -0800
Subject: [R] New trellis settings
In-Reply-To: <200411292323.36010.deepayan@stat.wisc.edu>
References: <cogtiq$ovm$1@sea.gmane.org>
	<200411292323.36010.deepayan@stat.wisc.edu>
Message-ID: <20041130193313.GA3743@sonic.net>

On Mon, Nov 29, 2004 at 11:23:35PM -0600, Deepayan Sarkar wrote:
> > 
> > Specifically, I have some code that in the past changed the relative
> > proportions of text versus the plot area using:
> >
> >   trellis.par.set('fontsize', list(text=8))
> >
> > which caused all text to get smaller, and the plot area to grow to
> > fill the larger available space.  Now, the plot area does not grow on
> > its own.  
> 
> That does seem to have been a side-effect in 1.9.1, but it was never 
> intended. I would consider that behaviour a bug, not a feature.

It was a pretty convenient bug! ;)

> > str(trellis.par.get()$layout.heights)
> List of 18
>  $ top.padding      : num 1
>  $ main             : num 1
>  $ main.key.padding : num 1
>  $ key.top          : num 1
>  $ key.axis.padding : num 1
>  ...
> 
> I was hoping that the names would be enough of a hint. Anything with 
> 'padding' in the name is space, and probably the ones you want to 
> change.

I've fiddled with these and can pretty much get what I want.  But
without really understanding what I'm doing.  It isn't clear to me,
for instance, why the "key" settings affect my plot when no key is
drawn.

-- Dave



From andrewr at uidaho.edu  Tue Nov 30 20:38:43 2004
From: andrewr at uidaho.edu (Andrew Robinson)
Date: Wed, 1 Dec 2004 06:38:43 +1100
Subject: [R] A basic question
In-Reply-To: <opsh90wjhdezww6l@perseus.unalmed.edu.co>
References: <20041129192247.YRQE24735.smta04.mail.ozemail.net@there>
	<opsh90wjhdezww6l@perseus.unalmed.edu.co>
Message-ID: <20041130193843.GA653@uidaho.edu>

R has compiled with no problem in FreeBSD.  It is also included in
the ports, thanks to its FreeBSD maintainer.

Andrew


On Tue, Nov 30, 2004 at 06:58:57AM -0500, Kenneth wrote:
> Hi R users:
> 
> I want to know any experience compiling R in other LINUX distributions
> besides FEDORA (Red Hat) or Mandrake, for example in BSD, Debian,
> Gentoo, Slackware, vector LINUX, Knoppix, Yopper or CERN linux?
> 
> Hope this is not a "basic question"
> 
> Thank you for your help.
> 
> Kenneth
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html

-- 
Andrew Robinson                      Ph: 208 885 7115
Department of Forest Resources       Fa: 208 885 6226
University of Idaho                  E : andrewr at uidaho.edu
PO Box 441133                        W : http://www.uidaho.edu/~andrewr
Moscow ID 83843                      Or: http://www.biometrics.uidaho.edu
No statement above necessarily represents my employer's opinion.



From p.murrell at auckland.ac.nz  Tue Nov 30 20:42:34 2004
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Wed, 01 Dec 2004 08:42:34 +1300
Subject: [R] RecordPlot
References: <200411301757.SAA18807@jacaranda.math.u-psud.fr>
Message-ID: <41ACCD2A.60505@stat.auckland.ac.nz>

Hi

The main problem here is that you really shouldn't be poking around 
inside a "recordedplot" object.  From the help page:

WARNING:

      The format of recorded plots may change between R versions.
      Recorded plots should *not* be used as a permanent storage format
      for R plots.

      R will always attempt to replay a recorded plot, but if the plot
      was  recorded with a different R version then bad things may
      happen.

This could (should) be more explicit.  In particular, it might be worth 
adding a comment from the C source ...

  * The graphics engine assumes that it is getting a snapshot
  * that was created in THE CURRENT R SESSION

... [snapshot == recordedplot].  The basic message is:  don't rely on 
the internal structure of a recordedplot because we can (and have [and 
will]) change it at any time.

That leaves the problem of how to achieve your "zoom", but I'm not sure 
exactly what your zoom is doing (just modifying the axis ranges?). 
Could you (privately) send me screen shots of before and after the zoom? 
Then I can try to suggest a "correct" way to zoom.

Paul


Jean Coursol wrote:
> I want to do a zoom with recordPlot(). I have problems with lists.
> (R-2.0.1 patched 2004-11-30 , various linux). I have problems 
> with RecordPlot class structure.
> 
> 
>>plot(1:10)
>>saveP <- recordPlot()
>>dev.off()
> 
> 
>>sx <- saveP[[1]][[2]][[2]] 
>>saveP[[1]][[2]][[2]] <- sx 
> 
> Error in "[[<-"(`*tmp*`, 1, value = list(list(
> .Primitive("plot.new")), list(.Primitive("plot
> .window"), c(1,  : 
>         incompatible types
> 
> 
>>typeof(saveP[[1]][[2]][[2]])
> 
> [1] "double"
> 
>>typeof(sx)
> 
> [1] "double"
>         
> 
> But:
> 
> 
>>s1 <- saveP[[1]]
>>s1[[2]][[2]] <- c(4,6)
>>saveP[1] <- list(s1)
>>saveP                  # zoom is OK
> 
> 
> Also, I don't understand recursive indexing.
> With not modified saveP, 
> 
> 
>>saveP[[c(1,2)]]
> 
> [[1]]
> .Primitive("plot.window")
> 
> [[2]]
> [1]  1 10
> 
> [[3]]
> [1]  1 10
> 
> [[4]]
> [1] ""
> 
> [[5]]
> [1] NA
> 
> # OK
> 
> 
>>saveP[[c(1,2,2)]]
> 
> Error: recursive indexing failed at level 2  
> 
> # why not [1]  1 10 ??
> 
> Jean Coursol
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From baron at psych.upenn.edu  Tue Nov 30 20:50:26 2004
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Tue, 30 Nov 2004 14:50:26 -0500
Subject: [R] impute missing values in correlated variables: transcan?
In-Reply-To: <41ACC836.2070107@vanderbilt.edu>
References: <20041130165215.GA21580@psych> <41ACC836.2070107@vanderbilt.edu>
Message-ID: <20041130195026.GA9176@psych>

On 11/30/04 13:21, Frank E Harrell Jr wrote:
>Jonathan Baron wrote:
>> I would like to impute missing data in a set of correlated
>> variables (columns of a matrix).  It looks like transcan() from
>> Hmisc is roughly what I want.  It says, "transcan automatically
>> transforms continuous and categorical variables to have maximum
>> correlation with the best linear combination of the other
>> variables." And, "By default, transcan imputes NAs with "best
>> guess" expected values of transformed variables, back transformed
>> to the original scale."
>>
>> But I can't get it to work.  I say
>>
>> m1 <- matrix(1:20+rnorm(20),5,)  # four correlated variables
>> colnames(m1) <- paste("R",1:4,sep="")
>> m1[c(2,19)] <- NA                # simulate some missing data
>> library(Hmisc)
>> transcan(m1,data=m1)
>>
>> and I get
>>
>> Error in rcspline.eval(y, nk = nk, inclx = TRUE) :
>>       fewer than 6 non-missing observations with knots omitted
>
>Jonathan - you would need many more observations to be able to fit
>flexible additive models as transcan does.  Also note that single
>imputation has problems and you may want to consider multiple imputation
>as done by the Hmisc aregImpute function, if you had more data.

Thanks.  But they don't _need_ to be so flexible as what transcan
does.  Linear would be OK, but I can't find an option for that in
transcan.

We _will_ have more data, about 50 applicants rated by the time
we start making decisions.  So I tried my little simulation with
more data, and it didn't give an error message.  So that was the
problem.  Here is the new one:

m1 <- matrix(1:80+rnorm(80),,4)
colnames(m1) <- paste("R",1:4,sep="")
m1[c(2,19)] <- NA
library(Hmisc)
t1 <- transcan(m1,data=m1,long=T,imputed=T)

I've used aregImpute, and I notice it has a "defaultlinear"
option, which is good.  Thus, it may work better once I figure
out how to get a single value out of it for each missing datum
(which doesn't look too hard).

This is not about statistical inference, which seems to me to be
where the main advantage of multiple imputation lies.  But
probably it won't do any harm.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page: http://www.sas.upenn.edu/~baron
R search page: http://finzi.psych.upenn.edu/



From mike_saunders at umenfa.maine.edu  Tue Nov 30 20:56:35 2004
From: mike_saunders at umenfa.maine.edu (Mike Saunders)
Date: Tue, 30 Nov 2004 14:56:35 -0500
Subject: [R] xy_plot
References: <20041130193121.49492.qmail@web81010.mail.yahoo.com>
Message-ID: <000d01c4d716$b2894750$9ba76f82@CFRU0104>

Sean:

You need to provide more information on your problem for anyone to help you 
much.  One of the "apply" functions working in tandem with "points" or 
"lines" (or others) can often do multiple item quickly, but again I don't 
know the context of your problem.

Mike


----- Original Message ----- 
From: "ebashi" <arshia22 at yahoo.com>
To: "R" <r-help at stat.math.ethz.ch>
Sent: Tuesday, November 30, 2004 2:31 PM
Subject: [R] xy_plot


>R users;
> Does anyone have a recommendation for a faster
> plotting function rather than plot(x,y)?
> when I use plot(x,y) for a large number of variables,
> it take couple of moments to plot the graph?
>
> Sincerely,
>
> Sean
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From TCerto at mays.tamu.edu  Tue Nov 30 21:10:19 2004
From: TCerto at mays.tamu.edu (Certo, Trevis)
Date: Tue, 30 Nov 2004 14:10:19 -0600
Subject: [R] Random effects and non-linear models
Message-ID: <33A19A6E6AC77141A5148EABFC80830BA525F6@email.mbs.tamu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20041130/0a8ffe24/attachment.pl

From Manuel.A.Morales at williams.edu  Tue Nov 30 21:12:34 2004
From: Manuel.A.Morales at williams.edu (Manuel Morales)
Date: Tue, 30 Nov 2004 15:12:34 -0500
Subject: [R] augPred with lme(...,subset=...)
Message-ID: <1101845555.9608.4.camel@localhost.localdomain>

Hello,

Is there a way to get augPred to work with lme if a subset statement is
used? For example, if I modify the example from ?augPred.lme to include
a subset statement, I get the following error:


fm1 <- lme(Orthodont, random = ~1, subset=distance>19)
augPred(fm1, length.out = 2, level = c(0,1))
Error in tapply(object[[nm]], groups, FUN[["numeric"]], ...) :
        arguments must have same length

Thanks!
Manuel



From srini_iyyer_bio at yahoo.com  Tue Nov 30 21:18:11 2004
From: srini_iyyer_bio at yahoo.com (Srinivas Iyyer)
Date: Tue, 30 Nov 2004 12:18:11 -0800 (PST)
Subject: [R] Reading a tab delimitted text
Message-ID: <20041130201811.65277.qmail@web53509.mail.yahoo.com>

Dear group,
 I have a tab delimitted text file with 21 column and
1988 rows. When I read the file:

D1 <-
read.table(file="gist_data1.txt",sep="\t",header=T)
> dim(D1)
[1] 1811   20

It reads only 1811 rows from that file.

I saved this file as CSV file and I read it again
using:

> D1 <-
read.csv(file="gist_data1.txt",sep=",",header=T)


> dim(D1)
[1] 1987    1
> 


This timer it reaad correct number of rows and wrong
column numbers. 

Can any one suggest how to read a tab-delimited text
file and read in corect number of lines. 


thank you.
		
__________________________________ 


From beracah at studentsreview.com  Tue Nov 30 21:36:40 2004
From: beracah at studentsreview.com (Director, Beracah Yankama)
Date: Tue, 30 Nov 2004 15:36:40 -0500
Subject: [R] bitmap (blank image) plot rendering without X11
Message-ID: <41ACD9D8.5010400@studentsreview.com>

Hi,
I am trying to generate plots on our unix (no X) server to be included 
in our web-pages -> specifically pngs.

Without X, png() obviously doesn't work, so I have been trying to define 
the graphics print device bitmap as described in the help:

 > bitmap(file="plot.png", type = "png256", height = 6, width=6,res=72)
 > pie(c(12,5))
 > dev2bitmap(file="plot.png", type = "png256", height = 6, width = 
6,res=72)

I end up with "plot.png" as a nonzero png file, but it is always 
blank... I have gs 6.51 installed & R_GSCMD=/usr/bin/gs set.

generating Rplot.ps; pstoimg -antialias (or pstopnm)  would also be an 
acceptable solution if I knew how to control the size of the generated 
image, or if the ps generated by R was consistently compatible with 
pstopnm.  (ps2ps causes me to lose parts of the image)..

So I am asking know-alls of R... if they know of a solution?  We are 
generating thousands of images, so work on a windows or slower machine 
with X is not really an option...


any help much appreciated!
Beracah





-- 

Beracah Yankama
Director, StudentsReview
StudentsReview.com
beracah at studentsreview.com



From t.muhlhofer at lse.ac.uk  Tue Nov 30 21:45:40 2004
From: t.muhlhofer at lse.ac.uk (Tobias Muhlhofer)
Date: Tue, 30 Nov 2004 20:45:40 +0000
Subject: [R] Relative subscripting
Message-ID: <41ACDBF4.7070804@lse.ac.uk>

Hi!

I'm trying to do the following.

I have monthly a dataset with, among other things, "marketcap", and 
"return".

I want to multiply return by the marketcap of the previous month. How do 
I do this?

In STATA (which I have used frequently in the past) I would simply use 
an expression of the form

return[_n]*marketcap[_n-1]

I believe they call this relative subscripting. Is there something like 
this in R?

On a completely different note, are there books on R? I read the 
"Getting Started" notes, but, while helpful, I would need more than those.

Thanks,
	Toby



From abu3ammar at gmail.com  Tue Nov 30 22:00:26 2004
From: abu3ammar at gmail.com (Yasser El-Zein)
Date: Tue, 30 Nov 2004 16:00:26 -0500
Subject: [R] SJava
Message-ID: <b1d3150404113013007c9a1423@mail.gmail.com>

How can I increase the JVM's memory using teh SJava library?



From schaffer at scripps.edu  Tue Nov 30 22:13:24 2004
From: schaffer at scripps.edu (Lana Schaffer)
Date: Tue, 30 Nov 2004 13:13:24 -0800
Subject: [R] calling R from Java
Message-ID: <41ACF252@neo>

Greetings,
I would like to call a small R function from Java (actually
to Bioconductor).  From a little reading, looks like I can
only do this on a Unix system and not on a PC.  Is this indeed
the case?  Does anyone have experience with calling a R
function from Java?
Lana

Lana Schaffer
Research Specialist
The Scripps Research Institute
DNA Array Core Facility



From kjetil at acelerate.com  Tue Nov 30 20:59:37 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Tue, 30 Nov 2004 15:59:37 -0400
Subject: [R] Package for multivariate binary logistic regression?
In-Reply-To: <20041130155031.52848.qmail@web60801.mail.yahoo.com>
References: <20041130155031.52848.qmail@web60801.mail.yahoo.com>
Message-ID: <41ACD129.40102@acelerate.com>

Lynne Baker wrote:

>I am trying to find out if someone has implemented a (McFadden-type) multivariate 
>binary logistic regresssion package for R? From what I can tell, this is not available for R.
> 
>Thank you,
> 
>Lynne Baker
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>
You can have a look at package VGAM (I think still not on CRAN)
which has binom2.or, for instance.

Kjetil Halvorsen

-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From kjetil at acelerate.com  Tue Nov 30 22:09:57 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Tue, 30 Nov 2004 17:09:57 -0400
Subject: [R] Choice modelling (was:(no subject))
In-Reply-To: <20041130155527.8897386600@mail.fr.clara.net>
References: <20041130155527.8897386600@mail.fr.clara.net>
Message-ID: <41ACE1A5.1050401@acelerate.com>

jruiz99 at ensae.org wrote:

>Hello,
>
>I am trying to estimate a choice model with varying choice set for each
>individual.
>
>I would like to fit different kinds of model (logit ,nested logit,
>probit...).
>
>So far I have found that package *mnp* allows me to estimate a probit model
>with varying choice set.
>
>But for estimation of a logit model, I have only found function *multinom*
>of package *nnet* which does not seem to allow for varying choice set.
>
>I could incorporate indicators of choice availability as explanotary
>variables, but it does not seem a very good way to do it.
>Instead, for a logit model, I have coded a likelihood computation of the
>underlying model with varying choice set and I use optim function to get
>the "maximum".
>  
>
Could you post the code?

>I wondered if there were more user friendly functions available and/or
>other choice model related package.
>  
>
Look at CRAN package eba (elimination by aspects).

Kjetil

>Thanks,
>	Julien
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://stat.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From p.dalgaard at biostat.ku.dk  Tue Nov 30 22:23:07 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Nov 2004 22:23:07 +0100
Subject: [R] bitmap (blank image) plot rendering without X11
In-Reply-To: <41ACD9D8.5010400@studentsreview.com>
References: <41ACD9D8.5010400@studentsreview.com>
Message-ID: <x21xeb2fr8.fsf@biostat.ku.dk>

"Director, Beracah Yankama" <beracah at studentsreview.com> writes:

> Hi,
> I am trying to generate plots on our unix (no X) server to be included
> in our web-pages -> specifically pngs.
> 
> Without X, png() obviously doesn't work, so I have been trying to
> define the graphics print device bitmap as described in the help:
> 
>  > bitmap(file="plot.png", type = "png256", height = 6, width=6,res=72)
>  > pie(c(12,5))
>  > dev2bitmap(file="plot.png", type = "png256", height = 6, width =
> 6,res=72)
> 
> I end up with "plot.png" as a nonzero png file, but it is always
> blank... I have gs 6.51 installed & R_GSCMD=/usr/bin/gs set.

Try forgetting the dev2bitmap() line and just terminate the device
with dev.off(). 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From abu3ammar at gmail.com  Tue Nov 30 22:38:24 2004
From: abu3ammar at gmail.com (Yasser El-Zein)
Date: Tue, 30 Nov 2004 16:38:24 -0500
Subject: [R] seriesMerge
In-Reply-To: <loom.20041125T042831-247@post.gmane.org>
References: <b1d31504041124122950a6efce@mail.gmail.com>
	<loom.20041125T042831-247@post.gmane.org>
Message-ID: <b1d315040411301338360345b5@mail.gmail.com>

I want to merge two timeSeries (union) and get a resulting timeSeries.
using union or merge I got a data.frame and when I tried to convert it
to timeSeries the data was corrupt. Is there a more straight forward
way to take two timeSeries and merege them for a resulting timeSeries
of the union of both?


On Thu, 25 Nov 2004 03:50:28 +0000 (UTC), Gabor Grothendieck
<ggrothendieck at myway.com> wrote:
> Yasser El-Zein <abu3ammar <at> gmail.com> writes:
> 
> :
> : Is there a function in R that is equivalent to S-PLUS's
> 
> 
> : seriesMerge(x1, x2, pos="union")
> : where x1, and x2 are of class timeSeries
> :
> : seriesMerge is in S-PLUS's finmetrics. I looked into R's mergeSeries
> : (in fSeries part of Rmetrics) but I could not make it behave quite the
> : same. In R it expected a timeSeries object and a matrix of the same
> : row count. In S-PLUS when using the union option both objects can be
> : of different lengths.
> 
> merge.zoo in package zoo handles union, intersection, left
> and right join of unequal length time series according to the
> setting of the all= argument.  zoo can also work with chron dates
> and times which would allow you to work with your millisecond data
> and can also merge more than two series at a time.   (The its
> package (see ?itsJoin) and for regular time series, cbind.ts, also
> support merging unequal length series but neither of these support
> chron which I gather is a requirement for you.)
> 
> eg. zoo example.
> In the following x has  length 8 and y has length 6 and
> they overlap for chron(5:8).  chron(1:4) only belongs
> to x and chron(9:10) only belongs to y.
> 
> library(chron)
> library(zoo)
> x <- zoo(1:8, chron(1:8))
> y <- zoo(5:10, chron(5:10))
> merge(x,y) # union
> merge(x,y,all=FALSE) # intersection
> merge(x,y,all=c(FALSE, TRUE)) # right join
> merge(x,y,all=c(TRUE, FALSE)) # left join
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From jeaneid at chass.utoronto.ca  Tue Nov 30 22:51:13 2004
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Tue, 30 Nov 2004 16:51:13 -0500
Subject: [R] Relative subscripting
In-Reply-To: <41ACDBF4.7070804@lse.ac.uk>
Message-ID: <Pine.SGI.4.40.0411301620340.57336654-100000@origin.chass.utoronto.ca>

 wouldn't it be return[2:NROW(return)]*marketcap[1:(NROW(return)-1)]

(note that return is also a function in R so maybe you should stay away
from calling your variable return. Anyways if you have a data frame you
can add another variable to it but attach an NA to the first element
(since you are lagging variables) i.e.

mydata$myvar<-c(NA, mydata$return[2:nrow(mydata)]*mydata$marketcap[1:(nrow(mydata)-1)]


Jean



On Tue, 30 Nov 2004, Tobias Muhlhofer wrote:

> Hi!
>
> I'm trying to do the following.
>
> I have monthly a dataset with, among other things, "marketcap", and
> "return".
>
> I want to multiply return by the marketcap of the previous month. How do
> I do this?
>
> In STATA (which I have used frequently in the past) I would simply use
> an expression of the form
>
> return[_n]*marketcap[_n-1]
>
> I believe they call this relative subscripting. Is there something like
> this in R?
>
> On a completely different note, are there books on R? I read the
> "Getting Started" notes, but, while helpful, I would need more than those.
>
> Thanks,
> 	Toby
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From kjetil at acelerate.com  Tue Nov 30 22:44:57 2004
From: kjetil at acelerate.com (Kjetil Brinchmann Halvorsen)
Date: Tue, 30 Nov 2004 17:44:57 -0400
Subject: [R] Relative subscripting
In-Reply-To: <41ACDBF4.7070804@lse.ac.uk>
References: <41ACDBF4.7070804@lse.ac.uk>
Message-ID: <41ACE9D9.5080902@acelerate.com>

Tobias Muhlhofer wrote:

> Hi!
>
> I'm trying to do the following.
>
> I have monthly a dataset with, among other things, "marketcap", and 
> "return".
>
> I want to multiply return by the marketcap of the previous month. How 
> do I do this?
>
> In STATA (which I have used frequently in the past) I would simply use 
> an expression of the form
>
> return[_n]*marketcap[_n-1]
>
> I believe they call this relative subscripting. Is there something 
> like this in R?
>
> On a completely different note, are there books on R? I read the 
> "Getting Started" notes, but, while helpful, I would need more than 
> those.

Q 2.7 in the R faq (yes, there are many)!

Kjetil

>
> Thanks,
>     Toby
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>


-- 

Kjetil Halvorsen.

Peace is the most effective weapon of mass construction.
               --  Mahdi Elmandjra



From t.muhlhofer at lse.ac.uk  Tue Nov 30 22:50:01 2004
From: t.muhlhofer at lse.ac.uk (Tobias Muhlhofer)
Date: Tue, 30 Nov 2004 21:50:01 +0000
Subject: [R] Relative subscripting
In-Reply-To: <Pine.SGI.4.40.0411301620340.57336654-100000@origin.chass.utoronto.ca>
References: <Pine.SGI.4.40.0411301620340.57336654-100000@origin.chass.utoronto.ca>
Message-ID: <41ACEB09.7070905@lse.ac.uk>

OK, yeah. I did think of that in the meantime, but I was also wondering 
if there was some other convenience function to do it. But this will do 
in any case.

Toby


Jean Eid wrote:
>  wouldn't it be return[2:NROW(return)]*marketcap[1:(NROW(return)-1)]
> 
> (note that return is also a function in R so maybe you should stay away
> from calling your variable return. Anyways if you have a data frame you
> can add another variable to it but attach an NA to the first element
> (since you are lagging variables) i.e.
> 
> mydata$myvar<-c(NA, mydata$return[2:nrow(mydata)]*mydata$marketcap[1:(nrow(mydata)-1)]
> 
> 
> Jean
> 
> 
> 
> On Tue, 30 Nov 2004, Tobias Muhlhofer wrote:
> 
> 
>>Hi!
>>
>>I'm trying to do the following.
>>
>>I have monthly a dataset with, among other things, "marketcap", and
>>"return".
>>
>>I want to multiply return by the marketcap of the previous month. How do
>>I do this?
>>
>>In STATA (which I have used frequently in the past) I would simply use
>>an expression of the form
>>
>>return[_n]*marketcap[_n-1]
>>
>>I believe they call this relative subscripting. Is there something like
>>this in R?
>>
>>On a completely different note, are there books on R? I read the
>>"Getting Started" notes, but, while helpful, I would need more than those.
>>
>>Thanks,
>>	Toby
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://stat.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
> 
> 
> 

-- 
**************************************************************************
When Thomas Edison invented the light bulb he tried over 2000
experiments before he got it to work. A young reporter asked
him how it felt to have failed so many times. He said
"I never failed once. I invented the light bulb.
It just happened to be a 2000-step process."



From bates at stat.wisc.edu  Tue Nov 30 22:57:47 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 30 Nov 2004 15:57:47 -0600
Subject: [R] Random effects and non-linear models
In-Reply-To: <33A19A6E6AC77141A5148EABFC80830BA525F6@email.mbs.tamu.edu>
References: <33A19A6E6AC77141A5148EABFC80830BA525F6@email.mbs.tamu.edu>
Message-ID: <41ACECDB.1000201@stat.wisc.edu>

Certo, Trevis wrote:
> I am trying to model growth over time for a number of companies (my unit
> of analysis). My dependent variable is a count (the number of
> acquisitions each firm made each year), and I'm having some trouble
> figuring out exactly how to model this. My assumption is that modeling
> the effect of time on this dependent variable with a linear model is
> inappropriate. Does anyone have any guidance regarding how to model such
> a dependent variable such that I could include both random and fixed
> effects? Thanks in advance.

You may want to consider a generalized linear mixed model (GLMM) for the 
Poisson family.  These can be fit with the GLMM function in the lme4 
package.



From Ted.Harding at nessie.mcc.ac.uk  Tue Nov 30 22:34:02 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 30 Nov 2004 21:34:02 -0000 (GMT)
Subject: [R] Relative subscripting
In-Reply-To: <41ACDBF4.7070804@lse.ac.uk>
Message-ID: <XFMail.041130213402.Ted.Harding@nessie.mcc.ac.uk>

On 30-Nov-04 Tobias Muhlhofer wrote:
> [...]
> I have monthly a dataset with, among other things, "marketcap",
> and "return".
> 
> I want to multiply return by the marketcap of the previous month.
> How do I do this?
> 
> In STATA (which I have used frequently in the past) I would simply
> use an expression of the form
> 
> return[_n]*marketcap[_n-1]
> 
> I believe they call this relative subscripting. Is there something
> like this in R?

Not as such, as far as I know. But there's an easy way to achieve
the same effect:

Let

  N<-length(return)
  new.var <- return[2:N]*marketcap[1:(N-1)]

Notes:
1. Note the parantheses in "1:(N-1)".
     > 1:(6-1)
     [1] 1 2 3 4 5
     > 1:6-1
     [1] 0 1 2 3 4 5
   (i.e. ":" is evaluated before "-")

2. You could also define n <- 2:(N-1) in which case you could
   definitely write

     return[n]*marketcap[n-1]

   which looks very similar to what you wrote, but I don't
   know whether it implies the same underlying mechanism.


3. I'm not sure you should use "return" as the name of a variable,
   since it's a predefined function in R, which you can put in a
   function definition to tell it what to return:
     > sq<-function(x){return(x^2)}
     > sq(4)
     [1] 16

   In my experiments, it didn't seem to change this behaviour
   to assign something to "return", e.g. return<-2, even inside
   the function definition; but I'd recommend avoiding the practice!

   You could avoid it here by using "Return" instead of "return"
   for the name of your variable.

Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 094 0861  [NB: New number!]
Date: 30-Nov-04                                       Time: 21:34:02
------------------------------ XFMail ------------------------------



From bates at stat.wisc.edu  Tue Nov 30 22:59:08 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 30 Nov 2004 15:59:08 -0600
Subject: [R] augPred with lme(...,subset=...)
In-Reply-To: <1101845555.9608.4.camel@localhost.localdomain>
References: <1101845555.9608.4.camel@localhost.localdomain>
Message-ID: <41ACED2C.3040700@stat.wisc.edu>

Manuel Morales wrote:
> Hello,
> 
> Is there a way to get augPred to work with lme if a subset statement is
> used? For example, if I modify the example from ?augPred.lme to include
> a subset statement, I get the following error:
> 
> 
> fm1 <- lme(Orthodont, random = ~1, subset=distance>19)
> augPred(fm1, length.out = 2, level = c(0,1))
> Error in tapply(object[[nm]], groups, FUN[["numeric"]], ...) :
>         arguments must have same length
> 
> Thanks!
> Manuel
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Well you could debug the augPred method and send the package maintainer 
a patch :-)



From bates at stat.wisc.edu  Tue Nov 30 23:03:05 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 30 Nov 2004 16:03:05 -0600
Subject: [R] Reading a tab delimitted text
In-Reply-To: <20041130201811.65277.qmail@web53509.mail.yahoo.com>
References: <20041130201811.65277.qmail@web53509.mail.yahoo.com>
Message-ID: <41ACEE19.40006@stat.wisc.edu>

Srinivas Iyyer wrote:
> Dear group,
>  I have a tab delimitted text file with 21 column and
> 1988 rows. When I read the file:
> 
> D1 <-
> read.table(file="gist_data1.txt",sep="\t",header=T)
> 
>>dim(D1)
> 
> [1] 1811   20
> 
> It reads only 1811 rows from that file.
> 
> I saved this file as CSV file and I read it again
> using:
> 
> 
>>D1 <-
> 
> read.csv(file="gist_data1.txt",sep=",",header=T)
> 
> 
> 
>>dim(D1)
> 
> [1] 1987    1
> 
> 
> 
> This timer it reaad correct number of rows and wrong
> column numbers. 
> 
> Can any one suggest how to read a tab-delimited text
> file and read in corect number of lines. 
> 
> 
> thank you.

Check for the presence of unexpected comment characters or quote 
characters in the file you are reading.  Alternatively, try setting 
quote='', comment='' in the call to read.table.



From deepayan at stat.wisc.edu  Tue Nov 30 23:33:50 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Tue, 30 Nov 2004 16:33:50 -0600
Subject: [R] New trellis settings
In-Reply-To: <20041130193313.GA3743@sonic.net>
References: <cogtiq$ovm$1@sea.gmane.org>
	<200411292323.36010.deepayan@stat.wisc.edu>
	<20041130193313.GA3743@sonic.net>
Message-ID: <200411301633.50366.deepayan@stat.wisc.edu>

On Tuesday 30 November 2004 13:33, David Hinds wrote:
> On Mon, Nov 29, 2004 at 11:23:35PM -0600, Deepayan Sarkar wrote:
> > > Specifically, I have some code that in the past changed the
> > > relative proportions of text versus the plot area using:
> > >
> > >   trellis.par.set('fontsize', list(text=8))
> > >
> > > which caused all text to get smaller, and the plot area to grow
> > > to fill the larger available space.  Now, the plot area does not
> > > grow on its own.
> >
> > That does seem to have been a side-effect in 1.9.1, but it was
> > never intended. I would consider that behaviour a bug, not a
> > feature.
>
> It was a pretty convenient bug! ;)
>
> > > str(trellis.par.get()$layout.heights)
> >
> > List of 18
> >  $ top.padding      : num 1
> >  $ main             : num 1
> >  $ main.key.padding : num 1
> >  $ key.top          : num 1
> >  $ key.axis.padding : num 1
> >  ...
> >
> > I was hoping that the names would be enough of a hint. Anything
> > with 'padding' in the name is space, and probably the ones you want
> > to change.
>
> I've fiddled with these and can pretty much get what I want.  But
> without really understanding what I'm doing.  It isn't clear to me,
> for instance, why the "key" settings affect my plot when no key is
> drawn.

It shouldn't (unless you have messed with lattice.options()). Can you 
provide an example? The following doesn't do anything unusual for me:

xyplot(1~1, 
       par.settings = 
       list(layout.heights = list(key.top = 100, key.bottom = 100)))

(I'm running r-devel though, so there's a small chance something might 
be different.)

Deepayan



From t.muhlhofer at lse.ac.uk  Tue Nov 30 23:42:10 2004
From: t.muhlhofer at lse.ac.uk (Tobias Muhlhofer)
Date: Tue, 30 Nov 2004 22:42:10 +0000
Subject: [R] Combined variable names
Message-ID: <41ACF742.1020408@lse.ac.uk>

I am trying to define a large number of variables through a loop construct.

I have my loop variable i being cycled through 1:100 and I would like 
the variables produced by this to be called

vi (i.e. v1 v2 v3 etc)

so, for example I'm going:

for(i in 1:100) {

<blank> <- a[i:N] # or whatever else you want to put on the right side 


}

where N is previously defined.

What goes in for <blank>?

Thanks,
	Toby



From dyang at NRCan.gc.ca  Tue Nov 30 23:43:12 2004
From: dyang at NRCan.gc.ca (Yang, Richard)
Date: Tue, 30 Nov 2004 17:43:12 -0500
Subject: [R] nls() error: Object not found
Message-ID: <F0E0B899CB43D5118D220002A55113CF04FE56B9@s2-edm-r1.nofc.cfs.nrcan.gc.ca>

Dear R-helpers;

	Using nls() to fit a function,Rdum, defined below I stumbled on an
error: "Error in eval(expr, envir, enclos): Object "s0" not found." 

	The function Rdum is defined as

     Rdum <- deriv(~ h1 * (s0 + sl0*sl + sm0*sm  + sp01*sp1 + sp02*sp2 +
sp03*sp3+sp04*sp4) *
      ((1 - exp(-(s1 + sl1*sl + sm1*sm  + sp11*sp1 + sp12*sp2  +
sp14*sp4)*t2)) /
       (1 - exp(-(s1 + sl1*sl + sm1*sm  + sp11*sp1 + sp12*sp2 +
sp14*sp4)*t1)))^
       (s2 + sl2*sl+sm2*sm+ sp21*sp1+sp22*sp2+sp23*sp3+sp24*sp4),
  c("s0", "s10","sm0", "sp01","sp02","sp03","sp04",
     "s1", "sl1","sm1", "sp11", "sp12","sp14",
     "s2","sl2","sm2", "sp21","sp22","sp23","sp24"),
     function(s0,sl0,sm0,sp01,sp02,sp03,sp04, s1, sl1, sm1, sp11,sp12, sp14,

    s2, sl2, sm2, sp21,sp22, sp23, sp24, h1, t1,t2, sl, sm,sp1, sp2, sp3,
sp4){})

and variables t1, t2, h1, h2, sl, sm, sp1, sp2, sp3, and sp4 in the gbht10D
data frame are fitted to the function by

   Dum.nls <- nls(h2 ~ Rdum(s0, sl0, sm0, sp01, sp02, sp03, sp04, sl,
sl1,sm1,sp11,sp12, sp14,
   s2, sl2, sm2, sp21, sp22, sp23, sp24,h1, t1, t2, sl, sm, sp1, sp2, sp3,
sp4), data=gbht10D,
   start=c(0.8413,-0.1602,-0.0156,0.0527,0.0513,0.00314,0.0378,
   -0.00598,-0.0125,0.00328,0.00989,0.0113,0.00583,
   1.9255,0.9427,0.2408,-0.0643,0.0047,0.0016,-0.00519))

    The complained object "s0" is one of the 20 parameters in the function
to be estimated from the dataset gbht10D. 

    The nls() script is relatively short. Stepping into the source code, I
located the offending line:

Browse[1]> start
 [1]  0.84130 -0.16020 -0.01560  0.05270  0.05130  0.00314  0.03780 -0.00598
-0.01250
[10]  0.00328  0.00989  0.01130  0.00583  1.92550  0.94270  0.24080 -0.06430
0.00470
[19]  0.00160 -0.00519
Browse[1]> n
debug: pnames <- names(start)
Browse[1]> n
debug: varNames <- varNames[is.na(match(varNames, pnames, nomatch = NA))]
Browse[1]> n
debug: varIndex <- sapply(varNames, function(varName, data, respLength) {
    length(eval(as.name(varName), data))%%respLength == 0
}, data, length(eval(formula[[2]], data)))
Browse[1]> varNames
 [1] "h2"   "s0"   "sl0"  "sm0"  "sp01" "sp02" "sp03" "sp04" "sl"   "sl1"
"sm1"  "sp11"
[13] "sp12" "sp14" "s2"   "sl2"  "sm2"  "sp21" "sp22" "sp23" "sp24" "h1"
"t1"   "t2"  
[25] "sm"   "sp1"  "sp2"  "sp3"  "sp4" 
Browse[1]> n
Error in eval(expr, envir, enclos) : Object "s0" not found. 

The "s0" is shown in varNames. Is the error message misleading? Or did I
miss something? Any iedas and suggestions?

I am using R 2.0.0 on Windows XP.

TIA,

Richard Yang



From tlumley at u.washington.edu  Tue Nov 30 23:48:30 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 30 Nov 2004 14:48:30 -0800 (PST)
Subject: [R] Combined variable names
In-Reply-To: <41ACF742.1020408@lse.ac.uk>
References: <41ACF742.1020408@lse.ac.uk>
Message-ID: <Pine.A41.4.61b.0411301447350.206220@homer11.u.washington.edu>

On Tue, 30 Nov 2004, Tobias Muhlhofer wrote:

> I am trying to define a large number of variables through a loop construct.
>
> I have my loop variable i being cycled through 1:100 and I would like the 
> variables produced by this to be called
>
> vi (i.e. v1 v2 v3 etc)
>

Look at FAQ 7.21, which explains how to do it and suggest that you might 
not really want to.


 	-thomas



From jes at luretanker.no  Tue Nov 30 23:48:49 2004
From: jes at luretanker.no (Jon Egil Strand)
Date: Tue, 30 Nov 2004 23:48:49 +0100 (CET)
Subject: [R] predict.gam problem
Message-ID: <Pine.LNX.4.44.0411302324060.28991-100000@ani.mywh.net>


-----------------
Mac OS X 10.3
R 2.0.1
gam 0.9.2
-----------------

Greetings

I am facing difficulties in prediction on using Trevor Hasties GAM package. 

How can one interpret the response from predict.gam?

Documentation on the type-parameter:
  The default (link) produces predictions on the scale of the additive 
  predictors, ... 

  If "response" is selected, the predictions are on the scale of the
  response, and are monotone trans-formations of the additive predictors,
  using the inverse link function.

Does "on the scale of the response" ammount to residals? 

All I want to do is get the predicted response from running my GAM model
on new data. The results given are way off, but I am not able to dechipher 
the documentation, nor use any scale information.


Any help will be highly appreciated.


All the best

Jon Egil Strand




-- 

Jon Egil Strand
jes at luretanker.no
Phone: +47 45030081



From sdavis2 at mail.nih.gov  Tue Nov 30 23:49:19 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Tue, 30 Nov 2004 17:49:19 -0500
Subject: [R] Combined variable names
In-Reply-To: <41ACF742.1020408@lse.ac.uk>
References: <41ACF742.1020408@lse.ac.uk>
Message-ID: <124CBC30-4322-11D9-8962-000D933565E8@mail.nih.gov>

See  the R-FAQ here:

http://cran.r-project.org/doc/FAQ/R-FAQ.html

Number 7.21.

However, as the FAQ also points out, you might be better served using 
lists.

a <- list()
for (i in 1:100) {
   a[[i]] <- some stuff
}

Sean

On Nov 30, 2004, at 5:42 PM, Tobias Muhlhofer wrote:

> I am trying to define a large number of variables through a loop 
> construct.
>
> I have my loop variable i being cycled through 1:100 and I would like 
> the variables produced by this to be called
>
> vi (i.e. v1 v2 v3 etc)
>
> so, for example I'm going:
>
> for(i in 1:100) {
>
> <blank> <- a[i:N] # or whatever else you want to put on the right side
>
> }
>
> where N is previously defined.
>
> What goes in for <blank>?
>
> Thanks,
> 	Toby
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From jeaneid at chass.utoronto.ca  Tue Nov 30 23:59:55 2004
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Tue, 30 Nov 2004 17:59:55 -0500
Subject: [R] Combined variable names
In-Reply-To: <41ACF742.1020408@lse.ac.uk>
Message-ID: <Pine.SGI.4.40.0411301756560.52988256-100000@origin.chass.utoronto.ca>

see ?assign and ?get
i,e  instead ob <blank> below have something like assign(paste("v", i,
sep=""), a[i:N])
and if you need to loop over calling them say get(paste("v", i, sep=""))

of course typing v1 will call the variable...



Jean

On Tue, 30 Nov 2004, Tobias Muhlhofer wrote:

> I am trying to define a large number of variables through a loop construct.
>
> I have my loop variable i being cycled through 1:100 and I would like
> the variables produced by this to be called
>
> vi (i.e. v1 v2 v3 etc)
>
> so, for example I'm going:
>
> for(i in 1:100) {
>
> <blank> <- a[i:N] # or whatever else you want to put on the right side
>
>
> }
>
> where N is previously defined.
>
> What goes in for <blank>?
>
> Thanks,
> 	Toby
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://stat.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From bates at stat.wisc.edu  Tue Nov 30 23:53:50 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 30 Nov 2004 16:53:50 -0600
Subject: [R] nls() error: Object not found
In-Reply-To: <F0E0B899CB43D5118D220002A55113CF04FE56B9@s2-edm-r1.nofc.cfs.nrcan.gc.ca>
References: <F0E0B899CB43D5118D220002A55113CF04FE56B9@s2-edm-r1.nofc.cfs.nrcan.gc.ca>
Message-ID: <41ACF9FE.3020909@stat.wisc.edu>

Yang, Richard wrote:
> Dear R-helpers;
> 
> 	Using nls() to fit a function,Rdum, defined below I stumbled on an
> error: "Error in eval(expr, envir, enclos): Object "s0" not found." 
> 
> 	The function Rdum is defined as
> 
>      Rdum <- deriv(~ h1 * (s0 + sl0*sl + sm0*sm  + sp01*sp1 + sp02*sp2 +
> sp03*sp3+sp04*sp4) *
>       ((1 - exp(-(s1 + sl1*sl + sm1*sm  + sp11*sp1 + sp12*sp2  +
> sp14*sp4)*t2)) /
>        (1 - exp(-(s1 + sl1*sl + sm1*sm  + sp11*sp1 + sp12*sp2 +
> sp14*sp4)*t1)))^
>        (s2 + sl2*sl+sm2*sm+ sp21*sp1+sp22*sp2+sp23*sp3+sp24*sp4),
>   c("s0", "s10","sm0", "sp01","sp02","sp03","sp04",
>      "s1", "sl1","sm1", "sp11", "sp12","sp14",
>      "s2","sl2","sm2", "sp21","sp22","sp23","sp24"),
>      function(s0,sl0,sm0,sp01,sp02,sp03,sp04, s1, sl1, sm1, sp11,sp12, sp14,
> 
>     s2, sl2, sm2, sp21,sp22, sp23, sp24, h1, t1,t2, sl, sm,sp1, sp2, sp3,
> sp4){})
> 
> and variables t1, t2, h1, h2, sl, sm, sp1, sp2, sp3, and sp4 in the gbht10D
> data frame are fitted to the function by
> 
>    Dum.nls <- nls(h2 ~ Rdum(s0, sl0, sm0, sp01, sp02, sp03, sp04, sl,
> sl1,sm1,sp11,sp12, sp14,
>    s2, sl2, sm2, sp21, sp22, sp23, sp24,h1, t1, t2, sl, sm, sp1, sp2, sp3,
> sp4), data=gbht10D,
>    start=c(0.8413,-0.1602,-0.0156,0.0527,0.0513,0.00314,0.0378,
>    -0.00598,-0.0125,0.00328,0.00989,0.0113,0.00583,
>    1.9255,0.9427,0.2408,-0.0643,0.0047,0.0016,-0.00519))
> 
>     The complained object "s0" is one of the 20 parameters in the function
> to be estimated from the dataset gbht10D. 
> 
>     The nls() script is relatively short. Stepping into the source code, I
> located the offending line:
> 
> Browse[1]> start
>  [1]  0.84130 -0.16020 -0.01560  0.05270  0.05130  0.00314  0.03780 -0.00598
> -0.01250
> [10]  0.00328  0.00989  0.01130  0.00583  1.92550  0.94270  0.24080 -0.06430
> 0.00470
> [19]  0.00160 -0.00519
> Browse[1]> n
> debug: pnames <- names(start)
> Browse[1]> n
> debug: varNames <- varNames[is.na(match(varNames, pnames, nomatch = NA))]
> Browse[1]> n
> debug: varIndex <- sapply(varNames, function(varName, data, respLength) {
>     length(eval(as.name(varName), data))%%respLength == 0
> }, data, length(eval(formula[[2]], data)))
> Browse[1]> varNames
>  [1] "h2"   "s0"   "sl0"  "sm0"  "sp01" "sp02" "sp03" "sp04" "sl"   "sl1"
> "sm1"  "sp11"
> [13] "sp12" "sp14" "s2"   "sl2"  "sm2"  "sp21" "sp22" "sp23" "sp24" "h1"
> "t1"   "t2"  
> [25] "sm"   "sp1"  "sp2"  "sp3"  "sp4" 
> Browse[1]> n
> Error in eval(expr, envir, enclos) : Object "s0" not found. 
> 
> The "s0" is shown in varNames. Is the error message misleading? Or did I
> miss something? 

The part of the manual page that states that start must be a named
vector?

    start: a named list or named numeric vector of starting estimates

The piece of code you are examining matches the starting values against 
the parameter names.  Because you did not name the starting values it 
cannot find a match for "s0".



From f.harrell at vanderbilt.edu  Tue Nov 30 23:16:42 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 30 Nov 2004 16:16:42 -0600
Subject: [R] [R-pkgs] New Hmisc Package Available
Message-ID: <41ACF14A.5030803@vanderbilt.edu>

An updated version of Hmisc is now available from CRAN.  The Web site 
for Hmisc is http://biostat.mc.vanderbilt.edu/s/Hmisc.  The change log 
may be found at http://biostat.mc.vanderbilt.edu/changelog/Hmisc.html. 
Changes made after 2004-11-24 should be ignored; these will be in the 
next version.

The most major change in Hmisc is that thanks to discussions with a 
highly respected, persistent, and convincing member of the R community, 
Hmisc no longer overrides [.factor to drop unused factor levels by 
default upon subsetting.  You can get the old behavior at any time by 
typing dropUnusedLevels().  A message to that effect appears when you 
attach the package.  Also Hmisc no longer overrides the interaction 
function, as the R builtin version provides the sep argument.  In the 
next version, the summary.formula function will be updated to 
automatically drop unused levels when this is appropriate.

Some other key changes are the addition of bubble plot capability in 
xYplot and addition of the capability of csv.get to correct certain date 
formatting inconsistencies.  sasxport.get now has keep and drop 
arguments to restrict attention to a subset of datasets when many 
datasets are being imported.  Other changes include bug and 
documentation fixes.

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://stat.ethz.ch/mailman/listinfo/r-packages



From tchur at optushome.com.au  Tue Nov 30 23:36:37 2004
From: tchur at optushome.com.au (Tim Churches)
Date: Wed, 01 Dec 2004 09:36:37 +1100
Subject: [R] Unable to understand strptime() behaviour
Message-ID: <41ACF5F5.9010403@optushome.com.au>

R V2.0.1 on Windows XP.

I have read the help pages on strptime() over and over, but can't
understand why strptime() is producing the following results.

   > v <- format("2002-11-31", format="%Y-%m-%d")
   > v
[1] "2002-11-31"
   > factor(v, levels=v)
[1] 2002-11-31
Levels: 2002-11-31
   > x <- strptime("2002-11-31", format="%Y-%m-%d")
   > x
[1] "2002-12-01"
   > factor(x, levels=x)
[1] <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA> <NA>
Levels: 2002-12-01 NA NA NA NA NA NA NA NA

Tim C



