From jasont at indigoindustrial.co.nz  Tue Jun  1 06:14:08 2004
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Mon, 31 May 2004 16:14:08 -1200 (NZST)
Subject: [R] problems with quantreg installation
In-Reply-To: <courier.40BA9B14.00007AC1@softhome.net>
References: <200405301004.i4UA3Phl025118@hypatia.math.ethz.ch>
	<courier.40BA9B14.00007AC1@softhome.net>
Message-ID: <57661.203.9.176.60.1085976848.squirrel@webmail.maxnet.co.nz>

> make: g77: Command not found
> make: *** [akj.o] Error 127


You need a fortran compiler.  Search the RPM lists for g77 (or just
"fortran").

Cheers

Jason



From edd at debian.org  Tue Jun  1 00:26:36 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 31 May 2004 17:26:36 -0500
Subject: [R] ffnet problem
In-Reply-To: <4087DF45000CD8E9@ims3e.cp.tin.it>
References: <20040531194121.GA990@sonny.eddelbuettel.com>
	<4087DF45000CD8E9@ims3e.cp.tin.it>
Message-ID: <20040531222636.GA2323@sonny.eddelbuettel.com>

On Mon, May 31, 2004 at 10:38:37PM +0200, v.demartino2 at virgilio.it wrote:
> 
> >> 
> >> I've just installed the contributed ffnet package wit no problem at all.
> >
> >"No problem at all" seems unlikely in light of ...
> 
> Dirk,
> 
> You're right! 
> Here you are what happened:
> 
> desktop:/tmp# R INSTALL -l /usr/local/lib/R/library ffnet/
> * Installing *source* package 'ffnet' ...
> ** libs
> make: Nothing to be done for `all'.
> ** R
> ** help
>  >>> Building/Updating help pages for package 'ffnet'
>      Formats: text html latex example
> ......................................................
> ......................................................
> 
> And  the compilation of ffnet commands kept going on as though  there had
> been no problems.... This was somewhat misleading to a newbye!!
> 
> Anyway, what steps should I take now?

Don't know, sorry. But looking at the comment in

      http://cran.r-project.org/contrib/extra/ffnet/ReadMe

it would appear that one needs additional sources to build ffnet. Also, if
the package built reliably, it would probably reside in the main part of
CRAN rather than in the contrib/extra directory.

Dirk

-- 
FEATURE:  VW Beetle license plate in California



From andy_liaw at merck.com  Tue Jun  1 01:13:04 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 31 May 2004 19:13:04 -0400
Subject: [R] ffnet problem
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7DE5@usrymx25.merck.com>

Given that those files are dated 27/06/2000, they are unlikely to work with
the current version of R.  (R has changed quite a bit since 2000!!)

Don't know why/how the code depends on NR codes, as one can find better
quality code in many instances.  It might not be too difficult to get rid of
the dependency on NR codes.

BTW, why not use nnet in the VR bundle?  (Obviously I have not looked at
ffnet at all...)

Best,
Andy

> From: Dirk Eddelbuettel
> 
> On Mon, May 31, 2004 at 10:38:37PM +0200, 
> v.demartino2 at virgilio.it wrote:
> > 
> > >> 
> > >> I've just installed the contributed ffnet package wit no 
> problem at all.
> > >
> > >"No problem at all" seems unlikely in light of ...
> > 
> > Dirk,
> > 
> > You're right! 
> > Here you are what happened:
> > 
> > desktop:/tmp# R INSTALL -l /usr/local/lib/R/library ffnet/
> > * Installing *source* package 'ffnet' ...
> > ** libs
> > make: Nothing to be done for `all'.
> > ** R
> > ** help
> >  >>> Building/Updating help pages for package 'ffnet'
> >      Formats: text html latex example
> > ......................................................
> > ......................................................
> > 
> > And  the compilation of ffnet commands kept going on as 
> though  there had
> > been no problems.... This was somewhat misleading to a newbye!!
> > 
> > Anyway, what steps should I take now?
> 
> Don't know, sorry. But looking at the comment in
> 
      http://cran.r-project.org/contrib/extra/ffnet/ReadMe

it would appear that one needs additional sources to build ffnet. Also, if
the package built reliably, it would probably reside in the main part of
CRAN rather than in the contrib/extra directory.

Dirk

-- 
FEATURE:  VW Beetle license plate in California

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Tue Jun  1 01:17:13 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 31 May 2004 19:17:13 -0400
Subject: [R] LAPACK and ScaLAPACK new functionality survey
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7DE6@usrymx25.merck.com>

Apologies for the off-topic post, and apologies for those already seen this
elsewhere...

>From NA-digest last week:

-------------------------------------------------------

From: Jack Dongarra <dongarra at cs.utk.edu>
Date: Fri, 28 May 2004 16:17:17 -0400
Subject: LAPACK and ScaLAPACK New Functionality Survey

LAPACK and ScaLAPACK new functionality survey

We plan to update the LAPACK and ScaLAPACK libraries and would like to have
feedback from users on what functionalities they think are missing and would
be needed in order to make these libraries more useful for the community. We
invite you to enter your suggestions in the form below. It would be most
useful
to have input by June 16th, although we would welcome your input at any
time. 

Both LAPACK and ScaLAPACK provide well-tested, open source, reviewed code
implementing trusted algorithms that guarantee reliability, efficiency and
accuracy. Any new functionality must adhere to these standards and should
have a significant impact in order to justify the development costs. We are
also interested in suggestions regarding user interfaces, documentation,
language interfaces, target (parallel) architectures and other issues, again
provided the impact is large enough. 

We already plan to include a variety of improved algorithms discovered over
the years by a number of researchers (e.g. faster or more accurate
eigenvalue and SVD algorithms, extra precise iterative refinement, recursive
blocking for some linear solvers, etc.). We also know of a variety of other
possible functions we could add (e.g. updating and downdating
factorizations), but are uncertain of their impact. 

Please see http://icl.cs.utk.edu/lapack-survey.html for the survey.
We would like to have your input by June 16th, 2004.

Regards,
Jack, Jim, and Sven



From Meredith.Briggs at team.telstra.com  Tue Jun  1 01:44:22 2004
From: Meredith.Briggs at team.telstra.com (Briggs, Meredith M)
Date: Tue, 1 Jun 2004 09:44:22 +1000
Subject: [R] Tree() and confidence intervals
Message-ID: <3B5823541A25D311B3B90008C7F9056410E3556E@ntmsg0092.corpmail.telstra.com.au>



	Hello

	I'm currently using Monte Carlo techniques to estimate prices (variable not static) from the following type of data:

	1,22,40,22,33,5,2000
	3,45,33,6,7,0,3000
	22,22,33,44,55,66,70000



	Each row is a record from group A and the cells in all but the last column are the volumes of 'widgets' in the record. The last column is the cost of all the widgets in the record. Any widget can have a different prices in each record but the price is assumed to be normally distributed with a starting price and deviation.

	The aim is to apply the estimated prices to eg an average record (in terms of volumes of widgets per record) from group B and compare the cost of this average record against the cost of an average record from group A.

	I've used a Monte Carlo approach to estimate confidence intervals but thought another view could be obtained by using tree() to split the records into two disparate groups and run these two groups separately through the Monte Carlo model. 

	Is this feasible?

thanks



From lamac_k at hotmail.com  Tue Jun  1 02:34:53 2004
From: lamac_k at hotmail.com (lamack lamack)
Date: Tue, 01 Jun 2004 00:34:53 +0000
Subject: [R] rcmd with a libgsl.a
Message-ID: <BAY7-F1216lGnA6PSxI0004c7a5@hotmail.com>

Dear all, I'm new to R and am trying to create a dll in order to be able to 
use the "dyn.load".
I work with some examples and its works fine.

Now, I would like to create a dll that use a lib (libgsl.a). I have linked 
its in
a main c program and its works fine  too. How can I instruct rcm ... to 
include this lib???

The .h file is in mingw\include\gsl and I put #include <gsl/gsl_cdf.h> in 
the C code. The libgsl.a is in mingw\lib.



Best regards.



From jasont at indigoindustrial.co.nz  Wed Jun  2 05:11:54 2004
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Tue, 1 Jun 2004 15:11:54 -1200 (NZST)
Subject: [R] ffnet problem
In-Reply-To: <4087DF45000CD8E9@ims3e.cp.tin.it>
References: <20040531194121.GA990@sonny.eddelbuettel.com>
	<4087DF45000CD8E9@ims3e.cp.tin.it>
Message-ID: <38215.203.9.176.60.1086059514.squirrel@webmail.maxnet.co.nz>

> Anyway, what steps should I take now?

"should" isn't quite what I'm telling you; just free advice.  :)

I use nnet from the nnet package (VR bundle), and find it very good.  And
it doesn't require any additional libraries.  If you've got a binary
installation, you've probably got it already...

library(nnet)
?nnet

I found nnet isn't as "quick and dirty" out of the box as ffnet, but gives
you much more control over the fit and diagnostics (less "black-box"-ish).

Cheers

Jason



From maechler at stat.math.ethz.ch  Tue Jun  1 09:04:49 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 1 Jun 2004 09:04:49 +0200
Subject: [R] Rmetrics New Built
In-Reply-To: <40BBA658.7010604@itp.phys.ethz.ch>
References: <40BADEEA.2000506@itp.phys.ethz.ch>
	<Pine.LNX.4.58.0405310949130.4526@illuminati.stderr.org>
	<20040531161852.GA31422@sonny.eddelbuettel.com>
	<Pine.LNX.4.58.0405311306060.3564@illuminati.stderr.org>
	<40BBA658.7010604@itp.phys.ethz.ch>
Message-ID: <16572.10897.540752.490499@gargle.gargle.HOWL>

>>>>> "Diethelm" == Diethelm Wuertz <wuertz at itp.phys.ethz.ch>
>>>>>     on Mon, 31 May 2004 21:40:40 +0000 writes:

    Diethelm> Hello Elijah, Hello Dirk
    Diethelm> First of all many thanks for your help and support.

    Diethelm> 1) Sorry, I  used non-standard kyewords and thus I added to my 
    Diethelm> doc/KEYWORDS.db database
    Diethelm> the following lines:

    Diethelm> Rmetrics: Rmetrics
    Diethelm> Rmetrics|winRmetrics: winRmetrics
    Diethelm> Rmetrics|fBasics: Basics
    Diethelm> Rmetrics|fSeries: Series
    Diethelm> Rmetrics|fExtremes: Extremes
    Diethelm> Rmetrics|fOptions: Options

    Diethelm> Then you don't get the warnings.

    Diethelm> Maybe it is not a good idea to have non-standard keywords.

yes, definitely not a good idea!
The point about them is that there's a public well defined set.

We know that \keyword{} is a bit of a misnomer (inherited from
S's .KW) and denotes rather something like "category" (which has
to be relatively broad and still relatively well defined).
Consequently we have begun to use \concept{.} for `free' keywords
or key phrases.  Note that \concept{} entries will also be found
by help.search().

Adding new keywords to the KEYWORDS.db (and KEYWORDS) is
a possibility, but not to be done lightly.
We (R-core) have added a few new keywords in the past, but not
more than one or two per year (examples from the last three
years are  'graphs', 'survey', 'databases', 'datagen', 'internal').

We don't have a specific keyword for the realm of financial
mathematics and statistics (there's "ts" for time series and
"chron" for 'date and times'). 
I don't think there should be more than one though. Some cheap 
ideas would be one of
\keyword{financial}
\keyword{finance}
\keyword{finmetrics}
What does Insightful use for (their add-on module) "S+Finmetrics"?

Best regards,
Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From ligges at statistik.uni-dortmund.de  Tue Jun  1 09:17:15 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 01 Jun 2004 09:17:15 +0200
Subject: [R] Rmetrics New Built
In-Reply-To: <40BBA658.7010604@itp.phys.ethz.ch>
References: <40BADEEA.2000506@itp.phys.ethz.ch>	<Pine.LNX.4.58.0405310949130.4526@illuminati.stderr.org>	<20040531161852.GA31422@sonny.eddelbuettel.com>	<Pine.LNX.4.58.0405311306060.3564@illuminati.stderr.org>
	<40BBA658.7010604@itp.phys.ethz.ch>
Message-ID: <40BC2D7B.9030905@statistik.uni-dortmund.de>

Diethelm Wuertz wrote:

> Hello Elijah, Hello Dirk
> 
> First of all many thanks for your help and support.
> 
> 1) Sorry, I  used non-standard kyewords and thus I added to my 
> doc/KEYWORDS.db database
> the following lines:
> 
> Rmetrics: Rmetrics
> Rmetrics|winRmetrics: winRmetrics
> Rmetrics|fBasics: Basics
> Rmetrics|fSeries: Series
> Rmetrics|fExtremes: Extremes
> Rmetrics|fOptions: Options
> 
> Then you don't get the warnings. Maybe it is not a good idea to have 
> non-standard keywords.
> [This should only produce a WARNING ...]

The name \keywords{} is a bit missleading, I think you want to specify 
\concept{} entries which are used for help.search().

Uwe Ligges


> 
> 2) fExtremes depends also on evir, sorry that I forgot this. Now I have 
> corrected the
> DESCRIPTION file as:
> 
> Depends: R (>= 1.9.0), evd, evir, ismev
> 
> 
> 3) I have not yet found the error:
> <>        Running examples in fBasics-Ex.R failed.
>         The error most likely occurred in:
>         ### * B1-ghypDistribution
> This runs fine under MS Windows ...
> [Hint: The hyperbolic Distribution uses calls to a Fortran program ? -
> does this work properly for Fortran under Linux?]
> 
> 
> 4+5) Currently I have no idea what the following means:
>             Error in .tryQuietly(....
> I still try to find it out
> 
> Thanks again Diethelm
> 
> 
> elijah wright wrote:
> 
> 
>>>>>If the Linux and Mac OSX builds are successfully done, I will submit
>>>>>the packages to the CRAN server.
>>>>>       
>>>>>
>>>>
>>>>all four of the packages successfully build on Debian unstable - no
>>>>errors, nor warnings.  this is a Good Thing.  I would guesstimate that
>>>>     
>>>>
>>>
>>>Ah, thanks, good to know. Did you try 'R CMD check' too, for good
>>>measure?
>>>   
>>>
>>
>>
>>good idea.  doing that reveals the following.  dirk, most of this is for
>>your reference as a repackager, so that you know what dependencies will
>>have to be met for useful debian packages.  :)  there are also some
>>problems that become evident with the packages...
>>
>>1) fBasics requires package "date"
>>
>>after i fixed the date package dependency, i get this error from R check:
>>
>>Running examples in fBasics-Ex.R failed.
>>The error most likely occurred in:
>>
>> 
>>
>>
>>>### * B1-ghypDistribution
>>>
>>>flush(stderr()); flush(stdout())
>>>   
>>>
>>
>>
>>
>>2) fExtremes requires packages "evd" and "ismev"
>>
>>after fixing those two dependencies, R CMD check produces the following
>>output:
>>
>>* checking S3 generic/method consistency ... WARNING
>>Error in .tryQuietly({ : Error in library(package, lib.loc = lib.loc,
>>character.only = TRUE, verbose = FALSE) :
>>       .First.lib failed
>>Execution halted
>>* checking for replacement functions with final arg not named 'value' ...
>>WARNING
>>Error in .tryQuietly({ : Error in library(package, lib.loc = lib.loc,
>>character.only = TRUE, verbose = FALSE) :
>>       .First.lib failed
>>Execution halted
>>* checking foreign function calls ... WARNING
>>Error in .tryQuietly({ : Error in library(package, lib.loc = lib.loc,
>>character.only = TRUE, verbose = FALSE) :
>>       .First.lib failed
>>Execution halted
>>* checking Rd files ... WARNING
>>Rd files with non-standard keywords:
>> 'man/C3-gpdglmFit.Rd': fExtremes
>> 'man/C1-gpdFamily.Rd': fExtremes
>> 'man/C6-rlargFit.Rd': fExtremes
>> 'man/C5-ppFit.Rd': fExtremes
>> 'man/B1-gevFamily.Rd': fExtremes
>> 'man/Z1-fExtremesTools.Rd': fExtremes
>> 'man/B4-mdaPlots.Rd': fExtremes
>> 'man/B2-gevFit.Rd': fExtremes
>> 'man/D1-exindexPlots.Rd': fExtremes
>> 'man/C4-potFit.Rd': fExtremes
>> 'man/A2-getExtremes.Rd': fExtremes
>> 'man/B3-gevglmFit.Rd': fExtremes
>> 'man/A1-evPlots.Rd': fExtremes
>> 'man/C2-gpdFit.Rd': fExtremes
>>Each '\keyword' entry should specify one of the standard keywords (as
>>listed in file 'KEYWORDS.db' in the 'doc' subdirectory of the R home
>>directory).
>>See chapter 'Writing R documentation files' in manual 'Writing R
>>Extensions'.
>>* checking for missing documentation entries ... ERROR
>>Error in .tryQuietly({ : Error in library(package, lib.loc = lib.loc,
>>character.only = TRUE, verbose = FALSE) :
>>
>>
>>
>>
>>3) fOptions documentation needs work?
>>
>>* checking Rd files ... WARNING
>>Rd files with non-standard keywords:
>> 'man/D2-MonteCarloOptions.Rd': fOptions
>> 'man/C2-HestonNandiOptions.Rd': fOptions
>> 'man/B2-MultAssetsOptions.Rd': fOptions
>> 'man/B5-BinaryOptions.Rd': fOptions
>> 'man/C1-hngarchFit.Rd': fOptions
>> 'man/A2-BasicAmericanOptions.Rd': fOptions
>> 'man/Z1-fOptionsTools.Rd': fOptions
>> 'man/B1-MultExercisesOptions.Rd': fOptions
>> 'man/D1-LowDiscrepancy.Rd': fOptions
>> 'man/B3-LookbackOptions.Rd': fOptions
>> 'man/A3-BinomialTreeOptions.Rd': fOptions
>> 'man/B6-AsianOptions.Rd': fOptions
>> 'man/B4-BarrierOptions.Rd': fOptions
>> 'man/A1-PlainVanillaOptions.Rd': fOptions
>> 'man/B7-FXTransOptions.Rd': fOptions
>>Each '\keyword' entry should specify one of the standard keywords (as
>>listed in file 'KEYWORDS.db' in the 'doc' subdirectory of the R home
>>directory).
>>
>>
>>4) fSeries:
>>
>>Packages required but not available:
>> mda polspline
>>
>>
>>fixed these dependencies on my local system - no biggie - then from R
>>CMD check got:
>>
>>* checking S3 generic/method consistency ... WARNING
>>Error in .tryQuietly({ : Error in library(package, lib.loc = lib.loc,
>>character.only = TRUE, verbose = FALSE) :
>>       .First.lib failed
>>Execution halted
>>* checking for replacement functions with final arg not named 'value' ...
>>WARNING
>>Error in .tryQuietly({ : Error in library(package, lib.loc = lib.loc,
>>character.only = TRUE, verbose = FALSE) :
>>       .First.lib failed
>>Execution halted
>>* checking foreign function calls ... WARNING
>>Error in .tryQuietly({ : Error in library(package, lib.loc = lib.loc,
>>character.only = TRUE, verbose = FALSE) :
>>       .First.lib failed
>>Execution halted
>>* checking Rd files ... WARNING
>>Rd files with non-standard keywords:
>> 'man/D1-fSeriesData.Rd': fSeries
>> 'man/Z1-fSeriesTools.Rd': fSeries
>> 'man/B2-lmTests.Rd': fSeries
>> 'man/B1-regressionModelling.Rd': fSeries
>> 'man/C3-rollingAnalysis.Rd': fSeries
>> 'man/A4-randomInnovations.Rd': fSeries
>> 'man/A3-garchModelling.Rd': fSeries
>> 'man/C2-benchmarkAnalysis.Rd': fSeries
>> 'man/A2-armaStatistics.Rd': fSeries
>> 'man/A1-armaModelling.Rd': fSeries
>> 'man/A5-tseriesTests.Rd': fSeries
>> 'man/C1-technicalAnalysis.Rd': fSeries
>>Each '\keyword' entry should specify one of the standard keywords (as
>>listed in file 'KEYWORDS.db' in the 'doc' subdirectory of the R home
>>directory).
>>See chapter 'Writing R documentation files' in manual 'Writing R
>>Extensions'.
>>* checking for missing documentation entries ... ERROR
>>Error in .tryQuietly({ : Error in library(package, lib.loc = lib.loc,
>>character.only = TRUE, verbose = FALSE) :
>>
>>
>>
>>methinks that some things may not be quite right just yet - even though
>>the builds of individual packages complete successfully.
>>
>>Diethelm, consider this an extended bug report...  :)
>>
>>
>> 
>>
>>
>>>>they should build trivially on OSX as well, but i'm at home today [in
>>>>the US, today is Memorial day - a sort of low-key holiday when people
>>>>tend to have picnics and things] and have no intention of driving to
>>>>campus...
>>>>
>>>>maybe you can sucker Dirk into packaging them for Debian and getting
>>>>them into the system there?  :)
>>>>     
>>>>
>>>
>>>That has of course been the plan all along, for both Debian and
>>>Quantian.
>>>   
>>>
>>
>>hooray!  i'm sure many people are glad to hear this.
>>
>> 
>>
>>
>>>I'm a little behind on a few other things, but knowing that these build
>>>out of the box may well move them up the priority queue :)
>>>   
>>>
>>
>>it looks like the emergent issues may be the same across the four
>>packages.  perhaps Diethelm has thoughts?
>>
>>thanks,
>>
>>elijah
>>
>> 
>>
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From a.trapletti at bluewin.ch  Tue Jun  1 09:32:47 2004
From: a.trapletti at bluewin.ch (Adrian Trapletti)
Date: Tue, 01 Jun 2004 09:32:47 +0200
Subject: [R] optim(method="SANN")
Message-ID: <40BC311F.6030605@bluewin.ch>

>
> Hello List
>
> I'm working on a combinatoric problem in which the object is to
> minimize the badness() of a vector.  I think this class of problem is 
> only
> soluble by optim() using method=SANN.
>
> The badness() of anything is >= 0, and when I've found a solution with
> zero badness, I want optim() to stop (carrying on beyond zero badness
> cannot improve the solution).  Efficiency is crucial here.
>
> The  ?optim manpage states
>
>           For '"SANN"' 'maxit' gives the total number of function
>           evaluations. There is no other stopping criterion.
>
> How best to make optim() stop as soon as it finds a zero badness
> solution?
>

method=SANN does currently not support an absolute convergence 
tolerance. However, it should be straightforward to extend it towards 
supporting other stopping criteria.

best
Adrian



From papydien at yahoo.fr  Tue Jun  1 10:47:22 2004
From: papydien at yahoo.fr (=?iso-8859-1?q?Papy=20Gaston?=)
Date: Tue, 1 Jun 2004 10:47:22 +0200 (CEST)
Subject: [R] Problem in random (lme)
Message-ID: <20040601084722.9817.qmail@web50903.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040601/63d84e7f/attachment.pl

From wuertz at itp.phys.ethz.ch  Tue Jun  1 11:04:08 2004
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Tue, 01 Jun 2004 09:04:08 +0000
Subject: [R] Rmetrics New Built
In-Reply-To: <16572.10897.540752.490499@gargle.gargle.HOWL>
References: <40BADEEA.2000506@itp.phys.ethz.ch>	<Pine.LNX.4.58.0405310949130.4526@illuminati.stderr.org>	<20040531161852.GA31422@sonny.eddelbuettel.com>	<Pine.LNX.4.58.0405311306060.3564@illuminati.stderr.org>	<40BBA658.7010604@itp.phys.ethz.ch>
	<16572.10897.540752.490499@gargle.gargle.HOWL>
Message-ID: <40BC4688.8030901@itp.phys.ethz.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040601/d04647c0/attachment.pl

From Torsten.Steuernagel at gmx.de  Tue Jun  1 11:21:14 2004
From: Torsten.Steuernagel at gmx.de (Torsten Steuernagel)
Date: Tue, 01 Jun 2004 11:21:14 +0200
Subject: [R] "privileged slots",
In-Reply-To: <rvaeb051m69kao6q6g1e14eiev9i3okcgt@4ax.com>
References: <40B671D8.20218.25D99F1@localhost>
Message-ID: <40BC66AA.27228.27CF9B@localhost>

On 28 May 2004 at 8:19, Duncan Murdoch wrote:

> I'd advise against doing this kind of optimization.  It will make your
> code harder to maintain, and while it might be faster today, if "@<-"
> is really a major time sink, it's an obvious candidate for
> optimization in R, e.g. by making it .Internal or .Primitive.  When
> that happens, your "optimized" code will likely be slower (if it even
> works at all).

Agreed. I don't recommend doing this either. I don't believe it makes 
any difference using "slot<-" instead of "@<-" in real life. Anyway, that 
"optimized" code should always work (slower or not) because "slot<-" 
is fully documented and I don't see why it should be removed or its 
behaviour should change. That wouldn't only break the kind of code 
mentioned here but also everything else that makes use of "slot<-".

- Torsten



From R.A.Sanderson at newcastle.ac.uk  Tue Jun  1 12:29:41 2004
From: R.A.Sanderson at newcastle.ac.uk (Roy Sanderson)
Date: Tue, 01 Jun 2004 10:29:41 +0000
Subject: [R] multi-model inference
Message-ID: <3.0.3.32.20040601102941.00b1ea00@popin.ncl.ac.uk>

Hello

I've been investigating using multi-model inference, based on calculating
AIC and AIC weights, using the techniques outlined in Burnham and
Anderson's (2002) book.  However I notice a couple of emails in the R-help
archive stating that there are errors in the technique.  Are these errors
associated with the particular implementation that B & A propose in their
text, or is the whole approach flawed in some way?

Many thanks
Roy



From madrid at linuxmeeting.net  Tue Jun  1 11:41:03 2004
From: madrid at linuxmeeting.net (Daniele Medri)
Date: Tue, 1 Jun 2004 11:41:03 +0200
Subject: [R] The beautifull way to plot a decision tree
Message-ID: <200406011141.03112.madrid@linuxmeeting.net>

Dearest,

I wanna plot a decision tree (from rpart/tree object) more beautifull and 
descriptive than the standard version (thinking about the tree plotted by SAS 
or SPSS Decision Tree).

Tips about? 
Are there plan to develop a plot function for that using "grid" library?

Thanks.

Bye
-- 
Daniele Medri



From jarioksa at sun3.oulu.fi  Tue Jun  1 11:44:09 2004
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: 01 Jun 2004 12:44:09 +0300
Subject: [R] "privileged slots",
In-Reply-To: <40BC66AA.27228.27CF9B@localhost>
References: <40B671D8.20218.25D99F1@localhost>
	<40BC66AA.27228.27CF9B@localhost>
Message-ID: <1086083049.8636.16.camel@biol102145.oulu.fi>

On Tue, 2004-06-01 at 12:21, Torsten Steuernagel wrote:
> On 28 May 2004 at 8:19, Duncan Murdoch wrote:
> 
> > I'd advise against doing this kind of optimization.  It will make your
> > code harder to maintain, and while it might be faster today, if "@<-"
> > is really a major time sink, it's an obvious candidate for
> > optimization in R, e.g. by making it .Internal or .Primitive.  When
> > that happens, your "optimized" code will likely be slower (if it even
> > works at all).
> 
> Agreed. I don't recommend doing this either. I don't believe it makes 
> any difference using "slot<-" instead of "@<-" in real life. Anyway, that 
> "optimized" code should always work (slower or not) because "slot<-" 
> is fully documented and I don't see why it should be removed or its 
> behaviour should change. That wouldn't only break the kind of code 
> mentioned here but also everything else that makes use of "slot<-".
> 
There are several other things that were fully documented and still were
removed. One of the latest cases was print.coefmat which was abruptly
made Defunct without warning or grace period: code written for 1.8*
didn't work in 1.9.0 and if corrected for 1.9.0 it wouldn't work in
pre-1.9.0. Anything can change in R without warning, and your code may
be broken anytime. Just be prepared.

cheers, jari oksanen 
-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From tuerk at phonetik.uni-muenchen.de  Tue Jun  1 12:02:04 2004
From: tuerk at phonetik.uni-muenchen.de (Uli Tuerk)
Date: Tue, 1 Jun 2004 12:02:04 +0200 (CEST)
Subject: [R] Importing binary data
Message-ID: <Pine.LNX.4.58.0406011153530.17916@linux28.phonetik.uni-muenchen.de>


Hi everybody!

I've a large dataset, about 2 Mio entries of the format which I would like 
to import into a frame:
<integer><integer><float><string><float><string><string>

Because to the huge data amount I've choosen a binary format instead 
of a text format when exporting from Matlab.
My import function is attached below. It works fine for only some entries 
but is deadly slow when trying to read the complete set. 

Does anybody has some pointers for me for improving the import or handling 
such large data sets? 

Thanks in advance!

Uli



read.DET.data <- function ( f ) {
	counter <- 1
	spk.v <- c()
	imp.v <- c()
	score.v <- c()
	th.v <- c()
	ses.v <- c()
	rec.v <- c()
	type.v <- c()
	fid <- file( f ,"rb")
	tempi <- readBin(fid , integer(), size=1, signed=FALSE)
	while ( length(tempi) != 0) {
		spk.v[ counter ] <- tempi
		imp.v[ counter ] <- readBin(fid, integer(), size=1, signed=FALSE)
		score.v[ counter  ] <- readBin(fid, numeric(), size=4)
		type.v[ counter ] <- readBin(fid, character())
		th.v[ counter ] <- readBin(fid, numeric(), size=4)
		ses.v[ counter ] <- readBin(fid, character())
		rec.v[ counter ] <- readBin(fid, character())
		counter <- counter + 1
		tempi <- readBin(fid, integer(), size=1, signed=FALSE)
	}
	close( fid )
	spkf <- factor ( spk.v )
	impf <- factor ( imp.v )
	
	det.f <- data.frame( spk=spkf, imp=impf, score=score.v, th=th.v, ses=ses.v, rec=rec.v, type=type.v)

	det.f
}



From Roger.Bivand at nhh.no  Tue Jun  1 12:26:43 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Tue, 1 Jun 2004 12:26:43 +0200 (CEST)
Subject: [R] "privileged slots",
In-Reply-To: <1086083049.8636.16.camel@biol102145.oulu.fi>
Message-ID: <Pine.LNX.4.44.0406011217550.17075-100000@reclus.nhh.no>

On 1 Jun 2004, Jari Oksanen wrote:

> On Tue, 2004-06-01 at 12:21, Torsten Steuernagel wrote:
> > On 28 May 2004 at 8:19, Duncan Murdoch wrote:
> > 
> > > I'd advise against doing this kind of optimization.  It will make your
> > > code harder to maintain, and while it might be faster today, if "@<-"
> > > is really a major time sink, it's an obvious candidate for
> > > optimization in R, e.g. by making it .Internal or .Primitive.  When
> > > that happens, your "optimized" code will likely be slower (if it even
> > > works at all).
> > 
> > Agreed. I don't recommend doing this either. I don't believe it makes 
> > any difference using "slot<-" instead of "@<-" in real life. Anyway, that 
> > "optimized" code should always work (slower or not) because "slot<-" 
> > is fully documented and I don't see why it should be removed or its 
> > behaviour should change. That wouldn't only break the kind of code 
> > mentioned here but also everything else that makes use of "slot<-".
> > 
> There are several other things that were fully documented and still were
> removed. One of the latest cases was print.coefmat which was abruptly
> made Defunct without warning or grace period: code written for 1.8*
> didn't work in 1.9.0 and if corrected for 1.9.0 it wouldn't work in
> pre-1.9.0. Anything can change in R without warning, and your code may
> be broken anytime. Just be prepared.

And the tools to stay prepared are there too - if your packages are on
CRAN, they get checked against patched and devel for free and the outcome
listed; if not on CRAN, you can always check them on devel locally too.  
Watching the notes on developer.r-project.org helps - the
print.coefmat change was highlighted for some time before 1.8.0 on:

http://developer.r-project.org/180update.txt

It is true, though, that writing current packages to be 
backwards-compatible to earlier R versions is extra overhead, and some 
users may not be able to change versions even if they'd like to. Using the 
appropriate methods is typically going to be more future-proof anyway (as 
a different thread noted recently in connection with namespaces).

> 
> cheers, jari oksanen 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From martin.klaffenboeck at inode.at  Tue Jun  1 12:57:44 2004
From: martin.klaffenboeck at inode.at (Martin Klaffenboeck)
Date: Tue, 1 Jun 2004 12:57:44 +0200
Subject: [R] advanced dist
Message-ID: <20040601105744.GC2511@martin.kleinerdrache.org>

Hi there.

I have now found that dist() does what I want in a specific case.

But I have (out of many fields) in my Fragebogen.data file:

Vpn	Code	Family	Test
1	X1	m	45
2	X1	t	58
3	X2	m	44
4	X2	t	43
...

When I now do:
Fbg <- read.table("Fragebogen.data", header=TRUE)
dist(Fbg['Test'])
I have the distances between everyone to everyone.

Now I want to have the mothers (m in Family) at the y axis and the  
daugthers (t) on the x axis, so I have half the size of my distance  
matrix.

Is that possible somehow?

And if possible, I would have the 'Code' at the top to know which  
mother and wich daugther are compared.

Thanks,
Martin



From mi2kelgrum at yahoo.com  Tue Jun  1 13:11:41 2004
From: mi2kelgrum at yahoo.com (Mikkel Grum)
Date: Tue, 1 Jun 2004 04:11:41 -0700 (PDT)
Subject: [R] converting text coordinates to decimal degrees
Message-ID: <20040601111141.62566.qmail@web60203.mail.yahoo.com>

I receive GPS readings in a text string such as
"0121.6723S 03643.6893E" and need the coordinates as
decimal degrees in two separate variables: "-1.361205"
and "36.728155".  How do I do this in R?

mikkel



From a.trapletti at bluewin.ch  Tue Jun  1 13:24:29 2004
From: a.trapletti at bluewin.ch (Adrian Trapletti)
Date: Tue, 01 Jun 2004 13:24:29 +0200
Subject: [R] ffnet problem
Message-ID: <40BC676D.9090505@bluewin.ch>

>
>
>Given that those files are dated 27/06/2000, they are unlikely to work with
>the current version of R.  (R has changed quite a bit since 2000!!)
>  
>
In fact, I stopped to support ffnet in 2000, and I would be very 
surprised if it would still run under the newer versions of R.

>Don't know why/how the code depends on NR codes, as one can find better
>quality code in many instances.  It might not be too difficult to get rid of
>the dependency on NR codes.
>  
>
The underlying c++ code of ffnet has been programmed around 1996 at a 
time when the capabilities of R to do multidimensional optimization 
through an API where rather limited. Hence my decision at that time to 
base the code on Numerical Recipes. But from today's perspective, it 
would be much better to call the optimizers from optim() through the API.

>BTW, why not use nnet in the VR bundle?  (Obviously I have not looked at
>ffnet at all...)
>  
>
I also suggest you use nnet.

>Best,
>Andy
>
>  
>
>>> From: Dirk Eddelbuettel
>>> 
>>> On Mon, May 31, 2004 at 10:38:37PM +0200, 
>>> v.demartino2 at virgilio.it wrote:
>>    
>>
>>>> > 
>>>      
>>>
>>>>>> > >> 
>>>>>> > >> I've just installed the contributed ffnet package wit no 
>>>>>          
>>>>>
>>> problem at all.
>>    
>>
>>>>> > >
>>>>> > >"No problem at all" seems unlikely in light of ...
>>>>        
>>>>
>>>> > 
>>>> > Dirk,
>>>> > 
>>>> > You're right! 
>>>> > Here you are what happened:
>>>> > 
>>>> > desktop:/tmp# R INSTALL -l /usr/local/lib/R/library ffnet/
>>>> > * Installing *source* package 'ffnet' ...
>>>> > ** libs
>>>> > make: Nothing to be done for `all'.
>>>> > ** R
>>>> > ** help
>>>> >  >>> Building/Updating help pages for package 'ffnet'
>>>> >      Formats: text html latex example
>>>> > ......................................................
>>>> > ......................................................
>>>> > 
>>>> > And  the compilation of ffnet commands kept going on as 
>>>      
>>>
>>> though  there had
>>    
>>
>>>> > been no problems.... This was somewhat misleading to a newbye!!
>>>> > 
>>>> > Anyway, what steps should I take now?
>>>      
>>>
>>> 
>>> Don't know, sorry. But looking at the comment in
>>> 
>>    
>>
>      http://cran.r-project.org/contrib/extra/ffnet/ReadMe
>
>it would appear that one needs additional sources to build ffnet. Also, if
>the package built reliably, it would probably reside in the main part of
>CRAN rather than in the contrib/extra directory.
>  
>
Numerical Recipes in C and apply the patches provided by ffnet (But I am 
not sure if the patches still work with newer versions of NRC).

>Dirk
>
> -- FEATURE: VW Beetle license plate in California 
> ______________________________________________ 
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help PLEASE do read 
> the posting guide! http://www.R-project.org/posting-guide.html
>
best
Adrian



From B.Rowlingson at lancaster.ac.uk  Tue Jun  1 13:29:58 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 01 Jun 2004 12:29:58 +0100
Subject: [R] converting text coordinates to decimal degrees
In-Reply-To: <20040601111141.62566.qmail@web60203.mail.yahoo.com>
References: <20040601111141.62566.qmail@web60203.mail.yahoo.com>
Message-ID: <40BC68B6.4030201@lancaster.ac.uk>

Mikkel Grum wrote:
> I receive GPS readings in a text string such as
> "0121.6723S 03643.6893E" and need the coordinates as
> decimal degrees in two separate variables: "-1.361205"
> and "36.728155".  How do I do this in R?

  you'd use some string processing functions, like strsplit, nchar, 
maybe grep and regexp, to build a function like this:

convertCoord <- function(coordString){
   bits <- strsplit(coordString," ")
   lat <- bits[[1]][1]
   lon <- bits[[1]][2]

   mdCon <- function(mdstring){
     d <- as.numeric(substr(mdstring,1,2))
     m <- as.numeric(substr(mdstring,3,99))/60
     return(d+m)
   }

   latD <- mdCon(substr(lat,1,nchar(lat)-1))
   latSign <- ifelse(substr(lat,nchar(lat),nchar(lat))=="N",1,-1)
   lonD <- mdCon(substr(lon,1,nchar(lon)-1))
   lonSign <- ifelse(substr(lon,nchar(lon),nchar(lon))=="E",1,-1)

   c(lonD*lonSign, latD*latSign)

}

  - this works for the single test case you've given us:

 > convertCoord("0121.6723S 03643.6893E")
[1] 13.728155 -1.361205

  - but lots of assumptions are coded into my function. Any deviation 
from that precise format will break it!

  Anyway, that should give you an idea.

Baz



From ligges at statistik.uni-dortmund.de  Tue Jun  1 13:49:54 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 01 Jun 2004 13:49:54 +0200
Subject: [R] Importing binary data
In-Reply-To: <Pine.LNX.4.58.0406011153530.17916@linux28.phonetik.uni-muenchen.de>
References: <Pine.LNX.4.58.0406011153530.17916@linux28.phonetik.uni-muenchen.de>
Message-ID: <40BC6D62.5080402@statistik.uni-dortmund.de>

Uli Tuerk wrote:

> Hi everybody!
> 
> I've a large dataset, about 2 Mio entries of the format which I would like 
> to import into a frame:
> <integer><integer><float><string><float><string><string>
> 
> Because to the huge data amount I've choosen a binary format instead 
> of a text format when exporting from Matlab.
> My import function is attached below. It works fine for only some entries 
> but is deadly slow when trying to read the complete set. 
> 
> Does anybody has some pointers for me for improving the import or handling 
> such large data sets? 

Suggestion:

a) Use a database!!!



And only for very strong reasons against a):

b) Rewrite your import code in C.

c) optimize the code below by initializing the objects in full length 
(e.g. imp.v <- numeric(n)) (maybe you can read it from the header or 
derive the size from the size of the file  ....)


Uwe Ligges



> Thanks in advance!
> 
> Uli
> 
> 
> 
> read.DET.data <- function ( f ) {
> 	counter <- 1
> 	spk.v <- c()
> 	imp.v <- c()
> 	score.v <- c()
> 	th.v <- c()
> 	ses.v <- c()
> 	rec.v <- c()
> 	type.v <- c()
> 	fid <- file( f ,"rb")
> 	tempi <- readBin(fid , integer(), size=1, signed=FALSE)
> 	while ( length(tempi) != 0) {
> 		spk.v[ counter ] <- tempi
> 		imp.v[ counter ] <- readBin(fid, integer(), size=1, signed=FALSE)
> 		score.v[ counter  ] <- readBin(fid, numeric(), size=4)
> 		type.v[ counter ] <- readBin(fid, character())
> 		th.v[ counter ] <- readBin(fid, numeric(), size=4)
> 		ses.v[ counter ] <- readBin(fid, character())
> 		rec.v[ counter ] <- readBin(fid, character())
> 		counter <- counter + 1
> 		tempi <- readBin(fid, integer(), size=1, signed=FALSE)
> 	}
> 	close( fid )
> 	spkf <- factor ( spk.v )
> 	impf <- factor ( imp.v )
> 	
> 	det.f <- data.frame( spk=spkf, imp=impf, score=score.v, th=th.v, ses=ses.v, rec=rec.v, type=type.v)
> 
> 	det.f
> }
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Tue Jun  1 13:55:49 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 01 Jun 2004 13:55:49 +0200
Subject: [R] The beautifull way to plot a decision tree
In-Reply-To: <200406011141.03112.madrid@linuxmeeting.net>
References: <200406011141.03112.madrid@linuxmeeting.net>
Message-ID: <40BC6EC5.9060507@statistik.uni-dortmund.de>

Daniele Medri wrote:

> Dearest,
> 
> I wanna plot a decision tree (from rpart/tree object) more beautifull and 
> descriptive than the standard version (thinking about the tree plotted by SAS 
> or SPSS Decision Tree).
> 
> Tips about? 
> Are there plan to develop a plot function for that using "grid" library?

You can configure quite a lot for rpart objects, see
?plot.rpart
?post.rpart
?text.rpart

There is the software Klimt for interactive stuff with R's trees by 
Simon Urbanek: http://stats.math.uni-augsburg.de/Klimt/intro.html
It also produces nice plots of tree/rpart objects.

Uwe Ligges


> Thanks.
> 
> Bye



From spencer.graves at pdf.com  Tue Jun  1 14:09:13 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 01 Jun 2004 05:09:13 -0700
Subject: [R] Problem in random (lme)
In-Reply-To: <20040601084722.9817.qmail@web50903.mail.yahoo.com>
References: <20040601084722.9817.qmail@web50903.mail.yahoo.com>
Message-ID: <40BC71E9.2030104@pdf.com>

      Have you seen Pinheiro and Bates (2000) Mixed-Effects Models in S 
and S-Plus (Springer)?  I read that book, worked the examples, and 
learned how to do it. It's currently advertised for EUR 184,95 from 
Amazon.fr but only $85 from Amazon.com;  I suspect you could order from 
Amazon.com, pay international shipping and still get it cheaper and 
faster than from Amazon.fr. 

      If you "have problems in random", please provide an example of the 
error message you get -- and "

PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html".  

	  hope this helps.  spencer graves

Papy Gaston wrote:

>In a lattice 11x11 with 6 repetitions, we want to compare lines to their two parents. 3 of the 6 repetitions are sprinkled and the 3 others not. There are 5 factors : hm (hydrous mode), variety, block, rep and grandrep. grandrep gathers two repetition, a sprinkled and a not (as in Split Plot !). I use lme but I have problems in random. Can someone help me ?
>Ibnou DIENG
> 
>
>
>		
>---------------------------------
>
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From spencer.graves at pdf.com  Tue Jun  1 14:19:54 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 01 Jun 2004 05:19:54 -0700
Subject: [R] multi-model inference
In-Reply-To: <3.0.3.32.20040601102941.00b1ea00@popin.ncl.ac.uk>
References: <3.0.3.32.20040601102941.00b1ea00@popin.ncl.ac.uk>
Message-ID: <40BC746A.9030309@pdf.com>

      I think it is better to be fully Bayes if you can figure out how 
to do so.  I've modified Venables and Ripley's stepAIC to use Burnham 
and Anderson's AIC.c for stepwise regression, but Ripley is negatively 
impressed with that;  he referred me to his (1996) Pattern Recognition 
and Neural Networks (Springer).  I need to modify this further to use an 
informative prior, because a limited simulation study demonstrated that 
Burnham and Anderson's "Akaike weights" showed an inappropriate 
preference against the null hypothesis when zero correlation was 
simulated.  With normally distributed residuals, a fully Bayesian 
solution is feasible, but so far as I know has not yet been programmed 
(at least for S-Plus or R).  My inadequate attempt at doing so is 
downloadable from "www.prodsyse.com", and you are free to use this as a 
starting point if you would like. 

      hope this helps.  spencer graves

Roy Sanderson wrote:

>Hello
>
>I've been investigating using multi-model inference, based on calculating
>AIC and AIC weights, using the techniques outlined in Burnham and
>Anderson's (2002) book.  However I notice a couple of emails in the R-help
>archive stating that there are errors in the technique.  Are these errors
>associated with the particular implementation that B & A propose in their
>text, or is the whole approach flawed in some way?
>
>Many thanks
>Roy
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From Torsten.Steuernagel at gmx.de  Tue Jun  1 14:21:05 2004
From: Torsten.Steuernagel at gmx.de (Torsten Steuernagel)
Date: Tue, 01 Jun 2004 14:21:05 +0200
Subject: [R] "privileged slots",
In-Reply-To: <1086083049.8636.16.camel@biol102145.oulu.fi>
References: <40BC66AA.27228.27CF9B@localhost>
Message-ID: <40BC90D1.30044.CC75C9@localhost>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040601/20db70c7/attachment.pl

From maechler at stat.math.ethz.ch  Tue Jun  1 14:49:36 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 1 Jun 2004 14:49:36 +0200
Subject: [R] "privileged slots",
In-Reply-To: <1086083049.8636.16.camel@biol102145.oulu.fi>
References: <40B671D8.20218.25D99F1@localhost>
	<40BC66AA.27228.27CF9B@localhost>
	<1086083049.8636.16.camel@biol102145.oulu.fi>
Message-ID: <16572.31584.95389.342281@gargle.gargle.HOWL>

>>>>> "Jari" == Jari Oksanen <jarioksa at sun3.oulu.fi>
>>>>>     on 01 Jun 2004 12:44:09 +0300 writes:

    Jari> On Tue, 2004-06-01 at 12:21, Torsten Steuernagel wrote:
    >> On 28 May 2004 at 8:19, Duncan Murdoch wrote:
    >> 
    >> > I'd advise against doing this kind of optimization.  It will make your
    >> > code harder to maintain, and while it might be faster today, if "@<-"
    >> > is really a major time sink, it's an obvious candidate for
    >> > optimization in R, e.g. by making it .Internal or .Primitive.  When
    >> > that happens, your "optimized" code will likely be slower (if it even
    >> > works at all).
    >> 
    >> Agreed. I don't recommend doing this either. I don't believe it makes 
    >> any difference using "slot<-" instead of "@<-" in real life. Anyway, that 
    >> "optimized" code should always work (slower or not) because "slot<-" 
    >> is fully documented and I don't see why it should be removed or its 
    >> behaviour should change. That wouldn't only break the kind of code 
    >> mentioned here but also everything else that makes use of "slot<-".
    >> 
    Jari> There are several other things that were fully
    Jari> documented and still were removed. One of the latest
    Jari> cases was print.coefmat which was abruptly made
    Jari> Defunct without warning or grace period: 
    Jari> code written for 1.8* didn't work in 1.9.0 and if
    Jari> corrected for 1.9.0 it wouldn't work in  pre-1.9.0. 

that's wrong.
In 1.8.*  every use of print.coefmat() would give a warning,
e.g.,

  > print.coefmat(cmat)
       Estimate Std.Err Z value    Pr(>z)    
  [1,]  10.2645  3.6570  2.8068  0.005004 ** 
  [2,]   8.6213  3.0335  2.8420  0.004483 ** 
  [3,]  11.8762  1.9379  6.1284 8.875e-10 ***
  ---
  Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 
  Warning message: 
  'print.coefmat' is deprecated.
  Use 'printCoefmat' instead.
  See ?Deprecated. 

which tells you what to use instead.
So there was a 6-month "grace period" (and that's the rule: one
cycle of 'deprecated' before 'defunct').

And as Roger Bivand just mentioned: You could have gotten that
warning even considerably earlier -- when 1.8.0 was R-devel,
all CRAN packages using print.coefmat() -- in code that is
executed in the package examples or 'tests --- had been listed as
giving warnings on the CRAN package check page 
{from CRAN main page  --> sidebar[Packages] has a section 
 'Daily package check results'}

    Jari> Anything can change in R without warning,
    Jari> and your code may be broken anytime. Just be prepared.

I can understand your feelings, but your statement 
is quite a bit exaggerated, see above.

Martin



From agustin at ija.csic.es  Tue Jun  1 15:22:08 2004
From: agustin at ija.csic.es (Agustin.Lobo)
Date: Tue, 1 Jun 2004 15:22:08 +0200 (MET DST)
Subject: [R] qhull in R?
Message-ID: <Pine.OSF.3.91.1040601151737.14971X-100000@paleo>


Hi,

does anyone know if there is an implementation
of qhull (http://www.qhull.org/) in R? anyone is
planning on it? 

"Qhull computes convex hulls, Delaunay triangulations, halfspace 
intersections about a point, Voronoi diagrams, furthest-site Delaunay 
triangulations, and furthest-site Voronoi diagrams. It runs in 2-d, 3-d, 
4-d, and higher dimensions. It implements the Quickhull algorithm for 
computing the convex hull. Qhull handles roundoff errors from floating 
point arithmetic. It computes volumes, surface areas, and approximations 
to the convex hull."

Thanks

Agus

PLEASE NOTE NEW E-MAIL ADDRESS
Dr. Agustin Lobo
Instituto de Ciencias de la Tierra (CSIC)
Lluis Sole Sabaris s/n
08028 Barcelona SPAIN
tel 34 93409 5410
fax 34 93411 0012
Agustin.Lobo at ija.csic.es



From sekemp at glam.ac.uk  Tue Jun  1 15:13:23 2004
From: sekemp at glam.ac.uk (Samuel Kemp (Comp))
Date: Tue, 01 Jun 2004 14:13:23 +0100
Subject: [R] S/R programming books
Message-ID: <40BC80F3.9040508@glam.ac.uk>

Hi,

I have been using R for a few months now and I am confident that the 
language has everything I will need to complete my PhD. I can create 
functions, script files and packages, but I would like to write my 
programs more efficiently (maybe using OO). Can anyone recommend a good 
book on the "art" of good R programming?

Kind Regards,

Sam.



From martin.klaffenboeck at inode.at  Tue Jun  1 15:15:17 2004
From: martin.klaffenboeck at inode.at (Martin Klaffenboeck)
Date: Tue, 1 Jun 2004 15:15:17 +0200
Subject: [R] signifikanz?
Message-ID: <20040601131517.GA8935@martin.kleinerdrache.org>

Hello,

When I use:

cor(x, y)

I get a correlations coefficient.  But how can I see if it is  
signifikant on a .95 or .99 level?

Thanks,
Martin



From rpeng at jhsph.edu  Tue Jun  1 15:18:34 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Tue, 01 Jun 2004 09:18:34 -0400
Subject: [R] S/R programming books
In-Reply-To: <40BC80F3.9040508@glam.ac.uk>
References: <40BC80F3.9040508@glam.ac.uk>
Message-ID: <40BC822A.1070202@jhsph.edu>

You might be interested in

William N. Venables and Brian D. Ripley. S Programming. Springer, 
2000. ISBN 0-387-98966-8.

There are also many books listed at http://www.r-project.org under 
"Publications".

-roger

Samuel Kemp (Comp) wrote:
> Hi,
> 
> I have been using R for a few months now and I am confident that the 
> language has everything I will need to complete my PhD. I can create 
> functions, script files and packages, but I would like to write my 
> programs more efficiently (maybe using OO). Can anyone recommend a good 
> book on the "art" of good R programming?
> 
> Kind Regards,
> 
> Sam.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From HaroldD at ccsso.org  Tue Jun  1 15:21:39 2004
From: HaroldD at ccsso.org (Harold Doran)
Date: Tue, 1 Jun 2004 09:21:39 -0400
Subject: [R] signifikanz?
Message-ID: <CFF85773D9245040A333571B7E6D651702C4941D@ccssosrv1.ccsso.org>

You need cor.test(x,y). However, I do not think you mean significant at the .95 or .99 level, do you? 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Martin
Klaffenboeck
Sent: Tuesday, June 01, 2004 9:15 AM
To: r-help
Subject: [R] signifikanz?


Hello,

When I use:

cor(x, y)

I get a correlations coefficient.  But how can I see if it is  
signifikant on a .95 or .99 level?

Thanks,
Martin

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From H.RINNER at tirol.gv.at  Tue Jun  1 15:22:38 2004
From: H.RINNER at tirol.gv.at (RINNER Heinrich)
Date: Tue, 1 Jun 2004 15:22:38 +0200
Subject: AW: [R] signifikanz?
Message-ID: <6A6B3B547E312840A98A9DD31516B321180563@mxs1.tirol.local>

?cor tells you to
>See Also: cor.test<
where you can find everything you need.

regards
Heinrich.

> -----Urspr??ngliche Nachricht-----
> Von: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] Im Auftrag von 
> Martin Klaffenboeck
> Gesendet: Dienstag, 01. Juni 2004 15:15
> An: r-help
> Betreff: [R] signifikanz?
> 
> 
> Hello,
> 
> When I use:
> 
> cor(x, y)
> 
> I get a correlations coefficient.  But how can I see if it is  
> signifikant on a .95 or .99 level?
> 
> Thanks,
> Martin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From fm3a004 at math.uni-hamburg.de  Tue Jun  1 15:22:24 2004
From: fm3a004 at math.uni-hamburg.de (Christian Hennig)
Date: Tue, 1 Jun 2004 15:22:24 +0200 (MET DST)
Subject: [R] signifikanz?
In-Reply-To: <20040601131517.GA8935@martin.kleinerdrache.org>
Message-ID: <Pine.GSO.3.95q.1040601152217.14312C-100000@sun11.math.uni-hamburg.de>

?cor.test

On Tue, 1 Jun 2004, Martin Klaffenboeck wrote:

> Hello,
> 
> When I use:
> 
> cor(x, y)
> 
> I get a correlations coefficient.  But how can I see if it is  
> signifikant on a .95 or .99 level?
> 
> Thanks,
> Martin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

***********************************************************************
Christian Hennig
Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
#######################################################################
ich empfehle www.boag-online.de



From wolski at molgen.mpg.de  Tue Jun  1 15:24:29 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Tue, 01 Jun 2004 15:24:29 +0200
Subject: [R] signifikanz?
In-Reply-To: <20040601131517.GA8935@martin.kleinerdrache.org>
References: <20040601131517.GA8935@martin.kleinerdrache.org>
Message-ID: <200406011524290446.01352F8C@mail.math.fu-berlin.de>

Hi!

Look for Fishers Z transformation
e.g:
http://www.quantlet.com/mdstat/scripts/mva/htmlbook/mvahtmlframe50.html
Theorem 3.2

sincerely
Eryk


*********** REPLY SEPARATOR  ***********

On 01.06.2004 at 15:15 Martin Klaffenboeck wrote:

>Hello,
>
>When I use:
>
>cor(x, y)
>
>I get a correlations coefficient.  But how can I see if it is  
>signifikant on a .95 or .99 level?
>
>Thanks,
>Martin
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Tue Jun  1 15:45:54 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 01 Jun 2004 06:45:54 -0700
Subject: [R] S/R programming books
In-Reply-To: <40BC822A.1070202@jhsph.edu>
References: <40BC80F3.9040508@glam.ac.uk> <40BC822A.1070202@jhsph.edu>
Message-ID: <40BC8892.1080101@pdf.com>

      Samuel: 

      If you have just started with R, then you may also wish to look at 
Venables and Ripley, Modern Applied Statistics with S, though you may 
also wish to examine the free material at www.r-project.org, as suggested. 

      Also, if you tell us more about your special interests, someone 
might be able to make more focused suggestions. 

      hope this helps.  spencer graves

Roger D. Peng wrote:

> You might be interested in
>
> William N. Venables and Brian D. Ripley. S Programming. Springer, 
> 2000. ISBN 0-387-98966-8.
>
> There are also many books listed at http://www.r-project.org under 
> "Publications".
>
> -roger
>
> Samuel Kemp (Comp) wrote:
>
>> Hi,
>>
>> I have been using R for a few months now and I am confident that the 
>> language has everything I will need to complete my PhD. I can create 
>> functions, script files and packages, but I would like to write my 
>> programs more efficiently (maybe using OO). Can anyone recommend a 
>> good book on the "art" of good R programming?
>>
>> Kind Regards,
>>
>> Sam.
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From Jesus.Frias at dit.ie  Tue Jun  1 15:48:14 2004
From: Jesus.Frias at dit.ie (Jesus Frias)
Date: Tue, 01 Jun 2004 14:48:14 +0100
Subject: [R] L term in anova.lme
Message-ID: <LGECJJCANFBOOHCMGPJEKELCDDAA.Jesus.Frias@dit.ie>

Dear R-helpers,

	Could somebody provide an example of the use of the L term in the
anova.lme() function to test linear combinations of different terms in a lme
model?. Is it possible to make more than one test in a single call to
anova.lme()?.

best regards,

Jesus

--------------------------------------------------------------
Jes??s Mar??a Fr??as Celayeta
School of Food Sci. and Env. Health.
Faculty of Tourism and Food
Dublin Institute of Technology
Cathal Brugha St., Dublin 1. Ireland
t +353 1 4024459 f +353 1 4024495
w www.dit.ie/DIT/tourismfood/science/staff/frias.html
--------------------------------------------------------------


-- 
This message has been scanned for content and 
viruses by the DIT Information Services MailScanner 
Service, and is believed to be clean.
http://www.dit.ie



From talitaperciano at hotmail.com  Tue Jun  1 16:01:30 2004
From: talitaperciano at hotmail.com (Talita Leite)
Date: Tue, 01 Jun 2004 11:01:30 -0300
Subject: [R] Parametric Curves
Message-ID: <BAY14-F9jvJ1A2fy4B10001e820@hotmail.com>

Hi everybody!

I'm trying to plot a parametric curve (three dimensions) using R but I 
didn't obtain good results. Somebody have already done something like that? 
Please help me!



Talita Perciano Costa Leite
Graduanda em Ci??ncia da Computa????o
Universidade Federal de Alagoas - UFAL
Departamento de Tecnologia da Informa????o - TCI
Constru????o de Conhecimento por Agrupamento de Dados - CoCADa



From B.Rowlingson at lancaster.ac.uk  Tue Jun  1 15:57:33 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 01 Jun 2004 14:57:33 +0100
Subject: [R] converting text coordinates to decimal degrees
In-Reply-To: <40BC68B6.4030201@lancaster.ac.uk>
References: <20040601111141.62566.qmail@web60203.mail.yahoo.com>
	<40BC68B6.4030201@lancaster.ac.uk>
Message-ID: <40BC8B4D.3000808@lancaster.ac.uk>

Barry Rowlingson wrote:

>> I receive GPS readings in a text string such as
>> "0121.6723S 03643.6893E" and need the coordinates as
>> decimal degrees in two separate variables: "-1.361205"
>> and "36.728155".  How do I do this in R?

>  - this works for the single test case you've given us:

  Whoops, it doesn't. I'd forgotten that longitude degrees could be 
three digits (0-180) but latitude is only two (0-90). Fixed:

convertCoord <- function(coordString){
   bits <- strsplit(coordString," ")
   lat <- bits[[1]][1]
   lon <- bits[[1]][2]
   mdCon <- function(mdstring,nd){
     d <- as.numeric(substr(mdstring,1,nd))
     m <- as.numeric(substr(mdstring,nd+1,99))/60
     return(d+m)
   }
   latD <- mdCon(substr(lat,1,nchar(lat)-1),2)
   latSign <- ifelse(substr(lat,nchar(lat),nchar(lat))=="N",1,-1)
   lonD <- mdCon(substr(lon,1,nchar(lon)-1),3)
   lonSign <- ifelse(substr(lon,nchar(lon),nchar(lon))=="E",1,-1)
   c(lonD*lonSign, latD*latSign)
}

  > convertCoord("0121.6723S 03643.6893E")
  [1] 36.728155 -1.361205

Note this isn't vectorised - if you feed it a vector of those coordinate 
strings, it wont do the right thing.

Baz



From wolski at molgen.mpg.de  Tue Jun  1 16:05:45 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Tue, 01 Jun 2004 16:05:45 +0200
Subject: [R] S/R programming books
In-Reply-To: <40BC80F3.9040508@glam.ac.uk>
References: <40BC80F3.9040508@glam.ac.uk>
Message-ID: <200406011605450932.01273406@mail.math.fu-berlin.de>

Hallo Samuel!

 You can find a talk by Friedrich Leisch about S4 at the UseR2004 conference side
"S4 Classes and Methods"
Its a good overview of S3 and S4.
goolge Use R2004 and look for keynotes.



If you decide to use S4 to implement your packages. (what is recomended) you would have no many choises in buying a books. Most of the S/R books are about using R and not about R programming. There is only one book/document  that describes S4 "Programming with Data" by John Chambers (the so called Green Book). The functionality which the "methods" package provides follows the language specification described in this book. So it is a "must have" so to say. 

If you do not like S4 you can take a look at Henrik Bengtsson R.oo packages  (Google R.oo). People which like Jave like it much more than S4.


Sincerely Eryk



*********** REPLY SEPARATOR  ***********

On 6/1/2004 at 2:13 PM Samuel Kemp (Comp) wrote:

>Hi,
>
>I have been using R for a few months now and I am confident that the 
>language has everything I will need to complete my PhD. I can create 
>functions, script files and packages, but I would like to write my 
>programs more efficiently (maybe using OO). Can anyone recommend a good 
>book on the "art" of good R programming?
>
>Kind Regards,
>
>Sam.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



Dipl. bio-chem. Eryk Witold Wolski    @    MPI-Moleculare Genetic   
Ihnestrasse 63-73 14195 Berlin       'v'    
tel: 0049-30-83875219               /   \    
mail: wolski at molgen.mpg.de        ---W-W----    http://www.molgen.mpg.de/~wolski



From jmc at research.bell-labs.com  Tue Jun  1 16:39:55 2004
From: jmc at research.bell-labs.com (John Chambers)
Date: Tue, 01 Jun 2004 10:39:55 -0400
Subject: [R] "privileged slots"
References: <40B5E6BF.1090100@uni-bayreuth.de>
	<16566.3676.127996.996941@gargle.gargle.HOWL>
	<40B6FC16.2000006@uni-bayreuth.de>
	<16567.2552.567272.315859@gargle.gargle.HOWL>
Message-ID: <40BC953B.B9E38192@research.bell-labs.com>

(I'm picking up this thread after it started, so may be unaware of some
contributions).

> 
>     Matthias> Martin Maechler schrieb:
>     >>>>>>> "Matthias" == Matthias Kohl
>     >>>>>>> <Matthias.Kohl at uni-bayreuth.de> on Thu, 27 May 2004
>     >>>>>>> 14:01:51 +0100 writes:
>     >>>>>>>
>     >>>>>>>
>     >>
>     Matthias> Hi all, in the help for RClassUtils I found the
>     Matthias> expression "privileged slots" in function
>     Matthias> "checkSlotAssignment" with the explanation:
>     >>
>     Matthias> /privileged slots (those that can only be set by
>     Matthias> accesor functions defined along with the class
>     Matthias> itself)/
>     >>
>     >>
>     >> RClassUtils ???
>     >>
>     >> > help.search("RClassUtils")
>     >>
>     >>
>     Matthias> your right, sorry
> 
>     Matthias> but, at least a R Site search in "Functions" gives
>     Matthias> me one match: "Utilities for Managing Class
>     Matthias> Definitions" which hast the "title":
>     Matthias> RClassUtils{methods} R Documentation
> 
>     >> No help files found with alias or concept or title
>     >> matching 'RClassUtils' using fuzzy matching.
>     >>
>     >> -----
>     >>
>     >> So I guess that's not something in a standard R document.
>     >> You should rather keep to the 'official documentation'
>     >> ...
>     >>
>     >>
>     Matthias> I thought this is a official documentation ...

Not really.  It's internal documentation, not meant to be visible.  So
it doesn't get as much attention as perhaps it should.  Better to use
the public documentation (see below).
> 
>     Matthias> I thought all slots of a (not private) class can
>     Matthias> be a accessed by a user via the @ Operator.
>     >>  I tend to agree with your thoughts...
>     >>
>     Matthias> Is there a way to make a single slot of a class
>     Matthias> (not the whole class) private, so that you can
>     Matthias> access this slot only via an accessor function
>     Matthias> (not via @)?
>     >>  I'd rather guess not.

The current situation is that slot access is just done by slot name, so
anything that can be found is allowed.

In the original design of S4 classes, there was provision for specifying
access when setClass was called.  This was never implemented and in fact
is not in the Programming with Data book, as far as I know.

The access= argument was retained in setClass for portability, but the
"official" documentation in ?setClass says:

  access: Access list for the class.  Saved in the definition, but not
          currently used.

That's the safer place to look for information.

It is possible, but not very likely, that access lists might be revived
if the underlying mechansim for slot access is changed.

John Chambers

>     >>
>     Matthias> Thanks, for your help Matthias
>     >>  Martin
> 
> _______________________________________________
> R-core list: https://www.stat.math.ethz.ch/mailman/listinfo/r-core

-- 
John M. Chambers                  jmc at bell-labs.com
Bell Labs, Lucent Technologies    office: (908)582-2681
700 Mountain Avenue, Room 2C-282  fax:    (908)582-3340
Murray Hill, NJ  07974            web: http://www.cs.bell-labs.com/~jmc



From dmb at mrc-dunn.cam.ac.uk  Tue Jun  1 17:02:18 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Tue, 1 Jun 2004 16:02:18 +0100 (BST)
Subject: [R] X/Emacs and R in Linux
Message-ID: <Pine.LNX.4.21.0406011553360.32212-100000@mail.mrc-dunn.cam.ac.uk>


Martin Maechler said...
>
>>>>>> "lanceh" == lanceh  <lanceh at ibm.net> writes:
>
>    lanceh> Sorry if this is a redundent question but I searched the
>    lanceh> archives and although I found a couple of similar questions I
>    lanceh> saw no responses.  I am running in Linux and when I try to
>
>You should have looked in the R-FAQ (see footer of this E-mail) ..
>
>    lanceh> invoke R from either XEmacs or Emacs via 'M-x R' I get an 
>    lanceh> error
>    lanceh> message which basically says that Emacs doesn't understand
>    lunchh> the
>    lanceh> command.  What do I need to do so that X/Emacs will invoke R
>    lanceh> when I type 'M-x R'?
>
>You need to install the emacs package "ESS" (= Emacs Speaks Statistics).
>(works under both Emacs & XEmacs).
>You find it at any CRAN site, under   src/other/ess/.
>Installation instructions are included.
>
>There's a mailing list for ESS,  ess-help at stat.math.ethz.ch,
>in case you have problems in installation or usage of ESS.


Yikes! whatever happened to a good old 'INSTALL' file?

People should not assume that just because you use linux you automatically
know what you are doing!

First it took me ages to find the install instructions, they were hidden
on page nine of a pdf file called ess.pdf. Then it took me ages to
understand them. Their are a lot of comments in the file 'ess-site.el' and
I don't think I understood any of them. 

Normally I run emacs by typing 'emacs', I can honestly say I don't know
much more about how it works than that!

In the end I ignored this file and I just added the "(load
'path-to-ess-site.el-file')" to my .emacs file. 

compile bit code? not me!

Then it took me a long time to get going with emacs and ess, I had to
guess the R-mode command, as it is somehow assumed that you dont need it.

Now I am getting there, but easy it wasnt. 

Sorry for all the complaints,
Dan.



From rossini at blindglobe.net  Tue Jun  1 16:51:25 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Tue, 01 Jun 2004 07:51:25 -0700
Subject: [R] X/Emacs and R in Linux
In-Reply-To: <Pine.LNX.4.21.0406011553360.32212-100000@mail.mrc-dunn.cam.ac.uk>
	(Dan Bolser's message of "Tue, 1 Jun 2004 16:02:18 +0100 (BST)")
References: <Pine.LNX.4.21.0406011553360.32212-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <85k6yrl4b6.fsf@servant.blindglobe.net>

Dan Bolser <dmb at mrc-dunn.cam.ac.uk> writes:

> Yikes! whatever happened to a good old 'INSTALL' file?

There is a nice README file which you should read...

> People should not assume that just because you use linux you automatically
> know what you are doing!

Apparently :-).

> First it took me ages to find the install instructions, they were hidden
> on page nine of a pdf file called ess.pdf. Then it took me ages to
> understand them. Their are a lot of comments in the file 'ess-site.el' and
> I don't think I understood any of them. 
>
> In the end I ignored this file and I just added the "(load
> 'path-to-ess-site.el-file')" to my .emacs file. 
>
> compile bit code? not me!

Exactly.  No reason to.  And you know that if you read the
documentation.

> Then it took me a long time to get going with emacs and ess, I had to
> guess the R-mode command, as it is somehow assumed that you dont need it.
>
> Now I am getting there, but easy it wasnt. 
>
> Sorry for all the complaints,

And you sent them to the wrong list, as well.

best,
-tony

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From martin.klaffenboeck at gmx.at  Tue Jun  1 17:44:23 2004
From: martin.klaffenboeck at gmx.at (Martin Klaffenboeck)
Date: Tue, 1 Jun 2004 17:44:23 +0200
Subject: [R] signifikanz?
In-Reply-To: <CFF85773D9245040A333571B7E6D651702C4941D@ccssosrv1.ccsso.org>
	(from HaroldD@ccsso.org on Di, Jun 01, 2004 at 15:21:39 +0200)
References: <CFF85773D9245040A333571B7E6D651702C4941D@ccssosrv1.ccsso.org>
Message-ID: <20040601154423.GC8935@martin.kleinerdrache.org>

Am 01.06.2004 15:21:39 schrieb(en) Harold Doran:
> You need cor.test(x,y). However, I do not think you mean significant
> at the .95 or .99 level, do you?

Hm.  Do I mean .05 and .01 level?

Martin

> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Martin
> Klaffenboeck
> Sent: Tuesday, June 01, 2004 9:15 AM
> To: r-help
> Subject: [R] signifikanz?
> 
> 
> Hello,
> 
> When I use:
> 
> cor(x, y)
> 
> I get a correlations coefficient.  But how can I see if it is
> signifikant on a .95 or .99 level?
> 
> Thanks,
> Martin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!  
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!  
> http://www.R-project.org/posting-guide.html
> 
> 
>



From edd at debian.org  Tue Jun  1 17:40:49 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 1 Jun 2004 10:40:49 -0500
Subject: [R] New 'R in Finance' mailing list
Message-ID: <20040601154049.GA13044@sonny.eddelbuettel.com>


Thanks again to everybody who participated in the finance sessions at the
recent useR! 2004 conference.  During the discussions, the idea of a mailing
list for R and Finance came up. Thanks to Martin, such a list has now been
created and can be accessed via the page

	https://www.stat.math.ethz.ch/mailman/listinfo/r-sig-finance

from which subscription requests can be made using the usual confirmation
system employed by the mailman software. Everybody interested in 'Finance'
(we will try not to be too picky regarding definitions) and R us cordially
invited to subscribe. Also feel free to forward this message to interested
colleagues. 

With best regards,  Dirk

-- 
FEATURE:  VW Beetle license plate in California



From samir.kc at bmg.eur.nl  Tue Jun  1 18:37:53 2004
From: samir.kc at bmg.eur.nl (samirkc)
Date: Tue, 01 Jun 2004 18:37:53 +0200
Subject: [R] read.spss
Message-ID: <40BCB0E1.8000801@bmg.eur.nl>

HI,
I tried to read an spss file, i got this message, please help me out. I 
am a very new user. Please suggest.
Samir

Error in read.spss("coughfever.sav") : Error reading system-file header.
In addition: Warning message:
coughfever.sav: File layout code has unexpected value 50331648.  Value 
should be 2, in big-endian or little-endian



From dmb at mrc-dunn.cam.ac.uk  Tue Jun  1 19:04:13 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Tue, 1 Jun 2004 18:04:13 +0100 (BST)
Subject: [R] X/Emacs and R in Linux
In-Reply-To: <85k6yrl4b6.fsf@servant.blindglobe.net>
Message-ID: <Pine.LNX.4.21.0406011800540.1545-100000@mail.mrc-dunn.cam.ac.uk>

On Tue, 1 Jun 2004, A.J. Rossini wrote:

>Dan Bolser <dmb at mrc-dunn.cam.ac.uk> writes:
>
>> Yikes! whatever happened to a good old 'INSTALL' file?
>
>There is a nice README file which you should read...
>
>> People should not assume that just because you use linux you automatically
>> know what you are doing!
>
>Apparently :-).
>
>> First it took me ages to find the install instructions, they were hidden
>> on page nine of a pdf file called ess.pdf. Then it took me ages to
>> understand them. Their are a lot of comments in the file 'ess-site.el' and
>> I don't think I understood any of them. 
>>
>> In the end I ignored this file and I just added the "(load
>> 'path-to-ess-site.el-file')" to my .emacs file. 
>>
>> compile bit code? not me!
>
>Exactly.  No reason to.  And you know that if you read the
>documentation.
>
>> Then it took me a long time to get going with emacs and ess, I had to
>> guess the R-mode command, as it is somehow assumed that you dont need it.
>>
>> Now I am getting there, but easy it wasnt. 
>>
>> Sorry for all the complaints,
>
>And you sent them to the wrong list, as well.

Ahhh, what a tipical piece of R related information... well... which list
should I post to then? Why can't I see a README? why does the
documentation suggest I need to compile bit code? why do you assume that I
*should* know what I am doing?

I want to use R with emacs but I have no experience, I didn't find the
docs usefull and I am sorry for existing.

All the best,

>
>best,
>-tony
>
>



From tobias_verbeke at skynet.be  Tue Jun  1 18:56:43 2004
From: tobias_verbeke at skynet.be (Tobias Verbeke)
Date: Tue, 1 Jun 2004 16:56:43 +0000
Subject: [R] read.spss
In-Reply-To: <40BCB0E1.8000801@bmg.eur.nl>
References: <40BCB0E1.8000801@bmg.eur.nl>
Message-ID: <20040601165643.11e417f5.tobias_verbeke@skynet.be>

On Tue, 01 Jun 2004 18:37:53 +0200
samirkc <samir.kc at bmg.eur.nl> wrote:

> HI,
> I tried to read an spss file, i got this message, please help me out. I 
> am a very new user. Please suggest.
> Samir
> 
> Error in read.spss("coughfever.sav") : Error reading system-file header.
> In addition: Warning message:
> coughfever.sav: File layout code has unexpected value 50331648.  Value 
> should be 2, in big-endian or little-endian

There has already been a thread regarding a similar
warning message.

See http://finzi.psych.upenn.edu/R/Rhelp02a/archive/21173.html
and the other messages in the thread.

HTH,
Tobias



From tobias_verbeke at skynet.be  Tue Jun  1 18:59:28 2004
From: tobias_verbeke at skynet.be (Tobias Verbeke)
Date: Tue, 1 Jun 2004 16:59:28 +0000
Subject: [R] S/R programming books
In-Reply-To: <40BC80F3.9040508@glam.ac.uk>
References: <40BC80F3.9040508@glam.ac.uk>
Message-ID: <20040601165928.1f939552.tobias_verbeke@skynet.be>

On Tue, 01 Jun 2004 14:13:23 +0100
"Samuel Kemp (Comp)" <sekemp at glam.ac.uk> wrote:

> Hi,
> 
> I have been using R for a few months now and I am confident that the 
> language has everything I will need to complete my PhD. I can create 
> functions, script files and packages, but I would like to write my 
> programs more efficiently (maybe using OO). Can anyone recommend a good 
> book on the "art" of good R programming?
> 

If you have patience (book announced for July 2004) 
and read German, "Programmieren mit R" by Uwe Ligges
(Springer Verlag, ISBN: 3-540-20727-9)
might be an other answer to your needs.

See:
http://www.springeronline.com/sgw/cda/frontpage/0,10735,5-40109-22-26682866-0,00.html

HTH,
Tobias



From rossini at blindglobe.net  Tue Jun  1 19:06:19 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Tue, 01 Jun 2004 10:06:19 -0700
Subject: [R] X/Emacs and R in Linux
In-Reply-To: <Pine.LNX.4.21.0406011800540.1545-100000@mail.mrc-dunn.cam.ac.uk>
	(Dan Bolser's message of "Tue, 1 Jun 2004 18:04:13 +0100 (BST)")
References: <Pine.LNX.4.21.0406011800540.1545-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <85llj7xl6c.fsf@servant.blindglobe.net>

Dan Bolser <dmb at mrc-dunn.cam.ac.uk> writes:

>>And you sent them to the wrong list, as well.
>
> Ahhh, what a tipical piece of R related information... well... which list
> should I post to then? Why can't I see a README? why does the
> documentation suggest I need to compile bit code? why do you assume that I
> *should* know what I am doing?

ESS is not just for R.  ESS-help is a rather useful mailing list.
Some of the people there have time.

I don't.  Not today.

> I want to use R with emacs but I have no experience, I didn't find the
> docs usefull and I am sorry for existing.

We've got lots of docs in various places -- would be great if someone
could point to all of them. 


-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From umalvarez at fata.unam.mx  Tue Jun  1 19:21:12 2004
From: umalvarez at fata.unam.mx (Ulises Mora Alvarez)
Date: Tue, 1 Jun 2004 12:21:12 -0500 (CDT)
Subject: [R] X/Emacs and R in Linux
In-Reply-To: <Pine.LNX.4.21.0406011553360.32212-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <Pine.LNX.4.44.0406011218030.1803-100000@athena.fata.unam.mx>

Hi!

Perhaps you should try with Paul Johnson's RPM's at: 

http://lark.cc.ku.edu/~pauljohn/software/favoriteEmacsFiles/

Good look.


On Tue, 1 Jun 2004, Dan Bolser wrote:

> 
> Martin Maechler said...
> >
> >>>>>> "lanceh" == lanceh  <lanceh at ibm.net> writes:
> >
> >    lanceh> Sorry if this is a redundent question but I searched the
> >    lanceh> archives and although I found a couple of similar questions I
> >    lanceh> saw no responses.  I am running in Linux and when I try to
> >
> >You should have looked in the R-FAQ (see footer of this E-mail) ..
> >
> >    lanceh> invoke R from either XEmacs or Emacs via 'M-x R' I get an 
> >    lanceh> error
> >    lanceh> message which basically says that Emacs doesn't understand
> >    lunchh> the
> >    lanceh> command.  What do I need to do so that X/Emacs will invoke R
> >    lanceh> when I type 'M-x R'?
> >
> >You need to install the emacs package "ESS" (= Emacs Speaks Statistics).
> >(works under both Emacs & XEmacs).
> >You find it at any CRAN site, under   src/other/ess/.
> >Installation instructions are included.
> >
> >There's a mailing list for ESS,  ess-help at stat.math.ethz.ch,
> >in case you have problems in installation or usage of ESS.
> 
> 
> Yikes! whatever happened to a good old 'INSTALL' file?
> 
> People should not assume that just because you use linux you automatically
> know what you are doing!
> 
> First it took me ages to find the install instructions, they were hidden
> on page nine of a pdf file called ess.pdf. Then it took me ages to
> understand them. Their are a lot of comments in the file 'ess-site.el' and
> I don't think I understood any of them. 
> 
> Normally I run emacs by typing 'emacs', I can honestly say I don't know
> much more about how it works than that!
> 
> In the end I ignored this file and I just added the "(load
> 'path-to-ess-site.el-file')" to my .emacs file. 
> 
> compile bit code? not me!
> 
> Then it took me a long time to get going with emacs and ess, I had to
> guess the R-mode command, as it is somehow assumed that you dont need it.
> 
> Now I am getting there, but easy it wasnt. 
> 
> Sorry for all the complaints,
> Dan.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Ulises M. Alvarez
LAB. DE ONDAS DE CHOQUE
FISICA APLICADA Y TECNOLOGIA AVANZADA
UNAM
umalvarez at fata.unam.mx



From baron at psych.upenn.edu  Tue Jun  1 19:24:28 2004
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Tue, 1 Jun 2004 13:24:28 -0400
Subject: [R] X/Emacs and R in Linux
In-Reply-To: <85llj7xl6c.fsf@servant.blindglobe.net>
References: <Pine.LNX.4.21.0406011800540.1545-100000@mail.mrc-dunn.cam.ac.uk>
	<85llj7xl6c.fsf@servant.blindglobe.net>
Message-ID: <20040601172428.GB17948@psych>

On 06/01/04 10:06, A.J. Rossini wrote:
>We've got lots of docs in various places -- would be great if someone
>could point to all of them.

FWIW, I think that the basic Readme is pretty clear about Unix
installation (which also works on Linux, of course, since Linux
is a form of Unix).  It comes with ESS, and it is on
http://stat.ethz.ch/ESS/ .

But it would be nice to see something in the main ESS site that
is even simpler.  Namely, if you use Xemacs rather the Emacs, ESS
is now a package within Xemacs (although they have not gotten the
latest version yet).  So, on Linux, if you install xemacs and
xemacs-sumo (all the packages in one fat file), all you need to
do is add the line
(require 'ess-site)
to your .xemacs/init.el file.  You can, of course, get RPMs for
both xemacs and xemacs-sumo.  They even come with most (all?)
distributions, so all you have to do is make sure they are
installed.

So this is REALLY EASY, and should, I think, be the standard
approach for those - like Dan Bolser I suspect - who are now
moving to Linux because they have heard that it might be a good
thing to have on their desktops, and not because they like to
fool around with things to get them to work.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron
R search page:        http://finzi.psych.upenn.edu/



From zelickr at pdx.edu  Tue Jun  1 19:55:24 2004
From: zelickr at pdx.edu (Randy Zelick)
Date: Tue, 1 Jun 2004 10:55:24 -0700 (PDT)
Subject: [R] swapping with data.frame
Message-ID: <Pine.GSO.4.44.0406011030060.2980-100000@gere.odin.pdx.edu>

Hi there,

I have some data which are convenient to enter as lists. For example:

t1<-list(fname="animal1",testname="hyla",dspkr="left",res1=39.7,res2=15.0)
t2<-list(fname="animal1",testname="bufo",dspkr="left",res1=14.4,res2=56.1)
t3<-list(fname="animal2",testname="hyla",dspkr="right",res1=22.6,res2=11.8)

I would like to generate a dataframe, but *not* the way this approach
works...

fdf<-data.frame(t1,t2,t3)

fdf

    fname testname dspkr res1 res2   fname testname dspkr res1 res2 ...
1 animal1     hyla  left 39.7   15 animal1     bufo  left 14.4 56.1 ...


Instead, what I would like is:

     fname   testname   dspkr   res1    res2

t1 animal1       hyla    left   39.7    15.0
t2 animal1       bufo    left   14.4    56.1
t3 animal2       hyla   right   22.6    11.8


or this would be fine too...

    x   fname   testname   dspkr   res1    res2

1  t1 animal1       hyla    left   39.7    15.0
2  t2 animal1       bufo    left   14.4    56.1
3  t3 animal2       hyla   right   22.6    11.8

Is there a practical (hopefully simple) way to do this?

Thanks,

=Randy=

R. Zelick				email: zelickr at pdx.edu
Department of Biology			voice: 503-725-3086
Portland State University		fax:   503-725-3888

mailing:
P.O. Box 751
Portland, OR 97207

shipping:
1719 SW 10th Ave, Room 246
Portland, OR 97201



From wolski at molgen.mpg.de  Tue Jun  1 20:03:03 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Tue, 01 Jun 2004 20:03:03 +0200
Subject: [R] swapping with data.frame
In-Reply-To: <Pine.GSO.4.44.0406011030060.2980-100000@gere.odin.pdx.edu>
References: <Pine.GSO.4.44.0406011030060.2980-100000@gere.odin.pdx.edu>
Message-ID: <200406012003030037.02006DAD@mail.math.fu-berlin.de>

Hi!
U can use.

rbind(t1,t2,t3)

Sincerely
Eryk

*********** REPLY SEPARATOR  ***********

On 6/1/2004 at 10:55 AM Randy Zelick wrote:

>Hi there,
>
>I have some data which are convenient to enter as lists. For example:
>
>t1<-list(fname="animal1",testname="hyla",dspkr="left",res1=39.7,res2=15.0)
>t2<-list(fname="animal1",testname="bufo",dspkr="left",res1=14.4,res2=56.1)
>t3<-list(fname="animal2",testname="hyla",dspkr="right",res1=22.6,res2=11.8)
>
>I would like to generate a dataframe, but *not* the way this approach
>works...
>
>fdf<-data.frame(t1,t2,t3)
>
>fdf
>
>    fname testname dspkr res1 res2   fname testname dspkr res1 res2 ...
>1 animal1     hyla  left 39.7   15 animal1     bufo  left 14.4 56.1 ...
>
>
>Instead, what I would like is:
>
>     fname   testname   dspkr   res1    res2
>
>t1 animal1       hyla    left   39.7    15.0
>t2 animal1       bufo    left   14.4    56.1
>t3 animal2       hyla   right   22.6    11.8
>
>
>or this would be fine too...
>
>    x   fname   testname   dspkr   res1    res2
>
>1  t1 animal1       hyla    left   39.7    15.0
>2  t2 animal1       bufo    left   14.4    56.1
>3  t3 animal2       hyla   right   22.6    11.8
>
>Is there a practical (hopefully simple) way to do this?
>
>Thanks,
>
>=Randy=
>
>R. Zelick				email: zelickr at pdx.edu
>Department of Biology			voice: 503-725-3086
>Portland State University		fax:   503-725-3888
>
>mailing:
>P.O. Box 751
>Portland, OR 97207
>
>shipping:
>1719 SW 10th Ave, Room 246
>Portland, OR 97201
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



Dipl. bio-chem. Eryk Witold Wolski    @    MPI-Moleculare Genetic   
Ihnestrasse 63-73 14195 Berlin       'v'    
tel: 0049-30-83875219               /   \    
mail: wolski at molgen.mpg.de        ---W-W----    http://www.molgen.mpg.de/~wolski



From spencer.graves at pdf.com  Tue Jun  1 20:36:27 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 01 Jun 2004 11:36:27 -0700
Subject: [R] Confidence Bounds on QQ Plots? 
Message-ID: <40BCCCAB.7090007@pdf.com>

      What's the current best wisdom on how to construct confidence 
bounds on something like a normal probability plot? 

      I recall having read a suggestion to Monte Carlo something like 
201 simulated lines with the same number of points, then sort the order 
statistics, and plot the 6th and 196th of these.  [I use 201 not 200 
because quantile(1:201, c(0.025, 0.975)) = 6 and 196 while 
quantile(1:200, c(0.025, 0.975)) = 5.975 and 11.025.]  I think I know 
how to do this, but before I code it, I'd like to ask two questions on 
this issue: 

      1.  Where can I find this in the literature?  I didn't find it 
where I thought it was, nor in anyplace else that seemed obvious to me, 
but I don't think I made it up and I'd like to give credit where credit 
it due. 

      2.  Are there better alternatives available, especially if the 
distribution is a compound mixture that is easily simulated but not so 
easily characterized analytically? 

      Thanks,
      spencer graves



From vograno at evafunds.com  Tue Jun  1 21:57:12 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Tue, 1 Jun 2004 12:57:12 -0700
Subject: [R] C-level try-catch
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A53884BC@phost015.EVAFUNDS.intermedia.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040601/041b11a7/attachment.pl

From ligges at statistik.uni-dortmund.de  Tue Jun  1 22:03:46 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 01 Jun 2004 22:03:46 +0200
Subject: [R] Confidence Bounds on QQ Plots?
In-Reply-To: <40BCCCAB.7090007@pdf.com>
References: <40BCCCAB.7090007@pdf.com>
Message-ID: <40BCE122.9080105@statistik.uni-dortmund.de>

Spencer Graves wrote:
>      What's the current best wisdom on how to construct confidence 
> bounds on something like a normal probability plot?
>      I recall having read a suggestion to Monte Carlo something like 201 
> simulated lines with the same number of points, then sort the order 
> statistics, and plot the 6th and 196th of these.  [I use 201 not 200 
> because quantile(1:201, c(0.025, 0.975)) = 6 and 196 while 
> quantile(1:200, c(0.025, 0.975)) = 5.975 and 11.025.]  I think I know 
> how to do this, but before I code it, I'd like to ask two questions on 
> this issue:
>      1.  Where can I find this in the literature?  I didn't find it 
> where I thought it was, nor in anyplace else that seemed obvious to me, 
> but I don't think I made it up and I'd like to give credit where credit 
> it due.
>      2.  Are there better alternatives available, especially if the 
> distribution is a compound mixture that is easily simulated but not so 
> easily characterized analytically?
>      Thanks,
>      spencer graves
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html


John Fox has qq.plot() in his "car" package for (not only) plotting 
pointwise confidence envelops into QQ-Plots.

See his books:

@BOOK{fox:1997,
  author =       "Fox,~J.",
  title=         {{Applied Regression Analysis, Linear Models, and 
Related Methods}},
  publisher =    {Sage},
  address =      "Thousand Oaks",
  year =         "1997"
}
@BOOK{fox:2002,
  author =       "Fox,~J.",
  title=         {{An R and S-PLUS Companion to Applied Regression}},
  publisher =    {Sage},
  address =      "Thousand Oaks",
  year =         "2002"
}

Uwe Ligges



From Scott.Waichler at pnl.gov  Tue Jun  1 22:05:27 2004
From: Scott.Waichler at pnl.gov (Waichler, Scott R)
Date: Tue, 01 Jun 2004 13:05:27 -0700
Subject: [R] Making a ranking algorithm more efficient
Message-ID: <7E4C06F49D6FEB49BE4B60E5FC92ED7A05F3F1@pnlmse35.pnl.gov>


I would like to make a ranking operation more efficient if possible.
The goal is to rank a set of points representing objective 
function values such that points which are "dominated" by no 
others have rank 1, those which are dominated by one other point 
have rank 2, etc.  In the example with two dimensions below, objective
functions 1 and 2 are to be minimized.  Points a-e are non-dominated,
rank 1 points.  Point f is rank 2 because point b is superior, and 
point g is rank 2 because point d is superior.  Point h is rank 3 
because points c and d are both superior.

       | a 
       |    f
       |  b
       |       h          (figure requires monospaced, plain-text
display)
Obj 1  |     c    
       |         g 
       |      d     
       |            e
       |____________________

              Obj 2
 

I need to compute the ranks of the rows of a matrix, where each row
represents a point in objective space and the columns
contain the objective function values to be minimized.  
Rank is defined as the number of points with objective function 
values less than or equal to the current point (including the current 
point).  I've written the following function with two loops:

  PARETO.RANK <- function(obj.array) {
    obj.array <- unique(obj.array)
    
    ind.row <- 1:nrow(obj.array)
    ind.col <- 1:ncol(obj.array)

    rank.vec <- rep(NA, length(ind.row)) # initialize

    # Loop thru rows (points in objective function space)
    for(i in ind.row) { 
      set <- ind.row
      for (j in ind.col) {  # Loop thru objective functions
        set <- set[ obj.array[set,j] <= obj.array[i,j] ]
      }
      rank.vec[i] <- length(set)
    }
    return(rank.vec)
  } # end PARETO.RANK3()

Can anyone think of a way to do this more efficiently, for example by
vectorizing further?  

Thanks,
Scott Waichler
scott.waichler at pnl.gov



From andy_liaw at merck.com  Tue Jun  1 22:17:42 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 1 Jun 2004 16:17:42 -0400
Subject: [R] Confidence Bounds on QQ Plots?
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7DF7@usrymx25.merck.com>

Dr. Venables used that as an example for program efficiency in his Advanced
S-PLUS programming course, which I took a few years back.  He cited Atkinson
(1985).  Unfortunately I do not have my copy of Atkinson on hand...

Best,
Andy

> From: Spencer Graves
> 
>       What's the current best wisdom on how to construct confidence 
> bounds on something like a normal probability plot? 
> 
>       I recall having read a suggestion to Monte Carlo something like 
> 201 simulated lines with the same number of points, then sort 
> the order 
> statistics, and plot the 6th and 196th of these.  [I use 201 not 200 
> because quantile(1:201, c(0.025, 0.975)) = 6 and 196 while 
> quantile(1:200, c(0.025, 0.975)) = 5.975 and 11.025.]  I think I know 
> how to do this, but before I code it, I'd like to ask two 
> questions on 
> this issue: 
> 
>       1.  Where can I find this in the literature?  I didn't find it 
> where I thought it was, nor in anyplace else that seemed 
> obvious to me, 
> but I don't think I made it up and I'd like to give credit 
> where credit 
> it due. 
> 
>       2.  Are there better alternatives available, especially if the 
> distribution is a compound mixture that is easily simulated 
> but not so 
> easily characterized analytically? 
> 
>       Thanks,
>       spencer graves
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From v_bill_pikounis at merck.com  Tue Jun  1 22:30:08 2004
From: v_bill_pikounis at merck.com (Pikounis, Bill)
Date: Tue, 1 Jun 2004 16:30:08 -0400
Subject: [R] Confidence Bounds on QQ Plots?
Message-ID: <CFBD404F5E0C9547B4E10B7BDC3DFA2F04156390@usrymx18.merck.com>

Spencer,
Venables & Ripley's S Programming (2000) book comprehensively covers
"Simulation envelopes for normal scores plots" in Section 7.3, pages 161 -
163.  The Atkinson "Plots, Transformations, and Regression" (1985) book is
cited. 

The V & R example and discussion, as usual, is very informative on both the
programming and data analysis fronts.

Hope that helps,
Bill

----------------------------------------
Bill Pikounis, Ph.D.

Biometrics Research Department
Merck Research Laboratories

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Spencer Graves
> Sent: Tuesday, June 01, 2004 2:36 PM
> To: R Help
> Subject: [R] Confidence Bounds on QQ Plots?
> 
> 
>       What's the current best wisdom on how to construct confidence 
> bounds on something like a normal probability plot? 
> 
>       I recall having read a suggestion to Monte Carlo something like 
> 201 simulated lines with the same number of points, then sort 
> the order 
> statistics, and plot the 6th and 196th of these.  [I use 201 not 200 
> because quantile(1:201, c(0.025, 0.975)) = 6 and 196 while 
> quantile(1:200, c(0.025, 0.975)) = 5.975 and 11.025.]  I think I know 
> how to do this, but before I code it, I'd like to ask two 
> questions on 
> this issue: 
> 
>       1.  Where can I find this in the literature?  I didn't find it 
> where I thought it was, nor in anyplace else that seemed 
> obvious to me, 
> but I don't think I made it up and I'd like to give credit 
> where credit 
> it due. 
> 
>       2.  Are there better alternatives available, especially if the 
> distribution is a compound mixture that is easily simulated 
> but not so 
> easily characterized analytically? 
> 
>       Thanks,
>       spencer graves
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From bates at stat.wisc.edu  Tue Jun  1 23:22:03 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 01 Jun 2004 16:22:03 -0500
Subject: [R] [R-pkgs] lme4_0.6-1 uploaded
Message-ID: <6rbrk36kjo.fsf@bates4.stat.wisc.edu>

I have uploaded release 0.6-1 of the lme4 package to the incoming area
on CRAN.  I expect it will be transferred to the archive within a
couple of days, after which Uwe's Windows build daemon should be able
to get it to build a Windows version.  The lme4 package itself is a
pure R package (i.e. it does not contain any code to be compiled) but
it depends on the Matrix_0.8-7 package which has a considerable amount
of C code in it.

This package contains the version of lme that I spoke of at useR!2004.
Using sparse matrix methods - in particular, Tim Davis' LDL package -
we are able to fit models with crossed random effects quickly and
effectively.  This package also contains an implementation of GLMM for
Generalized Linear Mixed Models using either Penalized
Quasi-Likelihood (PQL) or ML estimation using the Laplacian
approximation to the marginal likelihood.  At present method =
'Laplace' is considerably slower than method = 'PQL'.  We would
recommend using PQL for model building and determining final parameter
estimates using method = 'Laplace'.

Feedback is welcome.

-- 
Douglas Bates                            bates at stat.wisc.edu
Statistics Department                    608/262-2598
University of Wisconsin - Madison        http://www.stat.wisc.edu/~bates/

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://www.stat.math.ethz.ch/mailman/listinfo/r-packages



From dmb at mrc-dunn.cam.ac.uk  Wed Jun  2 00:02:08 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Tue, 1 Jun 2004 23:02:08 +0100 (BST)
Subject: [R] X/Emacs and R in Linux
In-Reply-To: <Pine.LNX.4.44.0406011218030.1803-100000@athena.fata.unam.mx>
Message-ID: <Pine.LNX.4.21.0406012257520.3562-100000@mail.mrc-dunn.cam.ac.uk>


Good look in deed!

RPM worked a treat... now if only I could find a man page... ;)

Thanks guys, and sorry for my mope. I love R!

Cheers,
Dan.

On Tue, 1 Jun 2004, Ulises Mora Alvarez wrote:

>Hi!
>
>Perhaps you should try with Paul Johnson's RPM's at: 
>
>http://lark.cc.ku.edu/~pauljohn/software/favoriteEmacsFiles/
>
>Good look.
>
>
>On Tue, 1 Jun 2004, Dan Bolser wrote:
>
>> 
>> Martin Maechler said...
>> >
>> >>>>>> "lanceh" == lanceh  <lanceh at ibm.net> writes:
>> >
>> >    lanceh> Sorry if this is a redundent question but I searched the
>> >    lanceh> archives and although I found a couple of similar questions I
>> >    lanceh> saw no responses.  I am running in Linux and when I try to
>> >
>> >You should have looked in the R-FAQ (see footer of this E-mail) ..
>> >
>> >    lanceh> invoke R from either XEmacs or Emacs via 'M-x R' I get an 
>> >    lanceh> error
>> >    lanceh> message which basically says that Emacs doesn't understand
>> >    lunchh> the
>> >    lanceh> command.  What do I need to do so that X/Emacs will invoke R
>> >    lanceh> when I type 'M-x R'?
>> >
>> >You need to install the emacs package "ESS" (= Emacs Speaks Statistics).
>> >(works under both Emacs & XEmacs).
>> >You find it at any CRAN site, under   src/other/ess/.
>> >Installation instructions are included.
>> >
>> >There's a mailing list for ESS,  ess-help at stat.math.ethz.ch,
>> >in case you have problems in installation or usage of ESS.
>> 
>> 
>> Yikes! whatever happened to a good old 'INSTALL' file?
>> 
>> People should not assume that just because you use linux you automatically
>> know what you are doing!
>> 
>> First it took me ages to find the install instructions, they were hidden
>> on page nine of a pdf file called ess.pdf. Then it took me ages to
>> understand them. Their are a lot of comments in the file 'ess-site.el' and
>> I don't think I understood any of them. 
>> 
>> Normally I run emacs by typing 'emacs', I can honestly say I don't know
>> much more about how it works than that!
>> 
>> In the end I ignored this file and I just added the "(load
>> 'path-to-ess-site.el-file')" to my .emacs file. 
>> 
>> compile bit code? not me!
>> 
>> Then it took me a long time to get going with emacs and ess, I had to
>> guess the R-mode command, as it is somehow assumed that you dont need it.
>> 
>> Now I am getting there, but easy it wasnt. 
>> 
>> Sorry for all the complaints,
>> Dan.
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>> 
>
>



From spencer.graves at pdf.com  Tue Jun  1 23:49:16 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 01 Jun 2004 14:49:16 -0700
Subject: [R] Confidence Bounds on QQ Plots?
In-Reply-To: <CFBD404F5E0C9547B4E10B7BDC3DFA2F04156390@usrymx18.merck.com>
References: <CFBD404F5E0C9547B4E10B7BDC3DFA2F04156390@usrymx18.merck.com>
Message-ID: <40BCF9DC.2040909@pdf.com>

Thanks to Uwe Ligges, Andy Liaw, and Bill Pikounis for 3 useful 
replies.  I had seen the description in "S Programming", but forgot 
where I had seen it.  When I couldn't find it in MASS, I got confused.  
I will also check John Fox's work. 

      Thanks again. 
      Best Wishes,
      Spencer Graves    

Pikounis, Bill wrote:

>Spencer,
>Venables & Ripley's S Programming (2000) book comprehensively covers
>"Simulation envelopes for normal scores plots" in Section 7.3, pages 161 -
>163.  The Atkinson "Plots, Transformations, and Regression" (1985) book is
>cited. 
>
>The V & R example and discussion, as usual, is very informative on both the
>programming and data analysis fronts.
>
>Hope that helps,
>Bill
>
>----------------------------------------
>Bill Pikounis, Ph.D.
>
>Biometrics Research Department
>Merck Research Laboratories
>
>  
>
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch 
>>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Spencer Graves
>>Sent: Tuesday, June 01, 2004 2:36 PM
>>To: R Help
>>Subject: [R] Confidence Bounds on QQ Plots?
>>
>>
>>      What's the current best wisdom on how to construct confidence 
>>bounds on something like a normal probability plot? 
>>
>>      I recall having read a suggestion to Monte Carlo something like 
>>201 simulated lines with the same number of points, then sort 
>>the order 
>>statistics, and plot the 6th and 196th of these.  [I use 201 not 200 
>>because quantile(1:201, c(0.025, 0.975)) = 6 and 196 while 
>>quantile(1:200, c(0.025, 0.975)) = 5.975 and 11.025.]  I think I know 
>>how to do this, but before I code it, I'd like to ask two 
>>questions on 
>>this issue: 
>>
>>      1.  Where can I find this in the literature?  I didn't find it 
>>where I thought it was, nor in anyplace else that seemed 
>>obvious to me, 
>>but I don't think I made it up and I'd like to give credit 
>>where credit 
>>it due. 
>>
>>      2.  Are there better alternatives available, especially if the 
>>distribution is a compound mixture that is easily simulated 
>>but not so 
>>easily characterized analytically? 
>>
>>      Thanks,
>>      spencer graves
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From spencer.graves at pdf.com  Wed Jun  2 00:25:54 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 01 Jun 2004 15:25:54 -0700
Subject: [R] GLMM(..., family=binomial(link="cloglog"))? 
Message-ID: <40BD0272.9040802@pdf.com>

      I'm having trouble using binomial(link="cloglog") with GLMM in 
lme4, Version: 0.5-2, Date: 2004/03/11.  The example in the Help file 
works fine, even simplified as follows: 

      fm0 <- GLMM(immun~1, data=guImmun, family=binomial, random=~1|comm)

      However, for another application, I need binomial(link="cloglog"), 
and this generates an error for me: 

 > fm0. <- GLMM(immun~1, data=guImmun, family=binomial(link="cloglog"), 
random=~1|comm)
Error in getClass(thisClass) : "family" is not a defined class
Error in GLMM(immun ~ 1, data = guImmun, family = binomial(link = 
"cloglog"),  :
        S language method selection got an error when called from 
internal dispatch for function "GLMM"

      Any suggestions? 
      Thanks,
      Spencer Graves



From deepayan at stat.wisc.edu  Wed Jun  2 01:12:21 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Tue, 1 Jun 2004 18:12:21 -0500
Subject: [R] GLMM(..., family=binomial(link="cloglog"))?
In-Reply-To: <40BD0272.9040802@pdf.com>
References: <40BD0272.9040802@pdf.com>
Message-ID: <200406011812.21394.deepayan@stat.wisc.edu>

On Tuesday 01 June 2004 17:25, Spencer Graves wrote:
>       I'm having trouble using binomial(link="cloglog") with GLMM in
> lme4, Version: 0.5-2, Date: 2004/03/11.  The example in the Help file
> works fine, even simplified as follows:
>
>       fm0 <- GLMM(immun~1, data=guImmun, family=binomial,
> random=~1|comm)
>
>       However, for another application, I need
> binomial(link="cloglog"),
>
> and this generates an error for me:
>  > fm0. <- GLMM(immun~1, data=guImmun,
>  > family=binomial(link="cloglog"),
>
> random=~1|comm)
> Error in getClass(thisClass) : "family" is not a defined class
> Error in GLMM(immun ~ 1, data = guImmun, family = binomial(link =
> "cloglog"),  :
>         S language method selection got an error when called from
> internal dispatch for function "GLMM"
>
>       Any suggestions?

This should work better in the new lme4 (0.6-x) announced earlier today 
on r-packages. There is still a bug with cases (like in your example) 
with only one fixed effect where the show and summary methods produce 
an error, but that should be fixed soon.

Deepayan



From jfox at mcmaster.ca  Wed Jun  2 01:23:27 2004
From: jfox at mcmaster.ca (John Fox)
Date: Tue, 1 Jun 2004 19:23:27 -0400
Subject: [R] Confidence Bounds on QQ Plots?
In-Reply-To: <40BCF9DC.2040909@pdf.com>
Message-ID: <20040601232326.NTGC13900.tomts36-srv.bellnexxia.net@JohnDesktop8300>

Dear Spencer et al.,

The simulated envelopes in the car package are for studentized residuals
from linear models, and the original reference is indeed to Atkinson's 1985
book. I don't see why the same approach -- really a parametric bootstrap --
shouldn't be applicable more generally, though shouldn't be necessary for
independently sampled observations. Finally, the qq.plot() function in car
calculates point-wise confidence envelopes based on the standard errors of
order statistics for an independent sample from the reference distribution.

I hope that this helps,
 John

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Spencer Graves
> Sent: Tuesday, June 01, 2004 4:49 PM
> To: Pikounis, Bill
> Cc: R Help
> Subject: Re: [R] Confidence Bounds on QQ Plots?
> 
> Thanks to Uwe Ligges, Andy Liaw, and Bill Pikounis for 3 
> useful replies.  I had seen the description in "S 
> Programming", but forgot where I had seen it.  When I 
> couldn't find it in MASS, I got confused.  
> I will also check John Fox's work. 
> 
>       Thanks again. 
>       Best Wishes,
>       Spencer Graves    
> 
> Pikounis, Bill wrote:
> 
> >Spencer,
> >Venables & Ripley's S Programming (2000) book comprehensively covers 
> >"Simulation envelopes for normal scores plots" in Section 7.3, pages 
> >161 - 163.  The Atkinson "Plots, Transformations, and Regression" 
> >(1985) book is cited.
> >
> >The V & R example and discussion, as usual, is very 
> informative on both 
> >the programming and data analysis fronts.
> >
> >Hope that helps,
> >Bill
> >
> >----------------------------------------
> >Bill Pikounis, Ph.D.
> >
> >Biometrics Research Department
> >Merck Research Laboratories
> >
> >  
> >
> >>-----Original Message-----
> >>From: r-help-bounces at stat.math.ethz.ch 
> >>[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Spencer Graves
> >>Sent: Tuesday, June 01, 2004 2:36 PM
> >>To: R Help
> >>Subject: [R] Confidence Bounds on QQ Plots?
> >>
> >>
> >>      What's the current best wisdom on how to construct confidence 
> >>bounds on something like a normal probability plot?
> >>
> >>      I recall having read a suggestion to Monte Carlo 
> something like
> >>201 simulated lines with the same number of points, then sort the 
> >>order statistics, and plot the 6th and 196th of these.  [I 
> use 201 not 
> >>200 because quantile(1:201, c(0.025, 0.975)) = 6 and 196 while 
> >>quantile(1:200, c(0.025, 0.975)) = 5.975 and 11.025.]  I 
> think I know 
> >>how to do this, but before I code it, I'd like to ask two 
> questions on 
> >>this issue:
> >>
> >>      1.  Where can I find this in the literature?  I 
> didn't find it 
> >>where I thought it was, nor in anyplace else that seemed obvious to 
> >>me, but I don't think I made it up and I'd like to give 
> credit where 
> >>credit it due.
> >>
> >>      2.  Are there better alternatives available, 
> especially if the 
> >>distribution is a compound mixture that is easily simulated 
> but not so 
> >>easily characterized analytically?
> >>
> >>      Thanks,
> >>      spencer graves
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide! 
> >>http://www.R-project.org/posting-guide.html
> >>
> >>
> >>    
> >>
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! 
> >http://www.R-project.org/posting-guide.html
> >
> >  
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From r.darnell at uq.edu.au  Wed Jun  2 01:56:08 2004
From: r.darnell at uq.edu.au (Ross Darnell)
Date: Wed, 02 Jun 2004 09:56:08 +1000
Subject: [R] WinMenu's question
Message-ID: <wu2qrfxj.fsf@uq.edu.au>

I am using the Windows menu functions below which will work on the
first pass, but if I repeat the same script I cannot get the
WinMenuAddItem to work. This is a problem if I change the menu
structure and reread the source code I am forced to quit and restart Rgui.

"try.menu" <- function(){
  OS <- .Platform$OS.type
  GUI <- .Platform$GUI
  if (!(OS == "windows" & GUI == "Rgui")) return("Sorry, you must be running R using Rgui.exe on MS Windows")
  try(winMenuDelItem('EMG/Graphics','Plot.trace'))
  try(winMenuDel('EMG/Graphics'))
  try(winMenuDel("EMG"))
  winMenuAdd("EMG")
  winMenuAdd("EMG/Graphics")
  winMenuAddItem("EMG/Graphics","Plot trace","plot.trace()")
}
"plot.trace" <- function(){
  x <- eval(parse(text=trace.dialog()))
  plot(x,type="l")
  invisible()
}


Help much appreciated

Ross Darnell
-- 
University of Queensland, Brisbane QLD 4067 AUSTRALIA
Email: <r.darnell at uq.edu.au>
Phone: +61 7 3365 6087     Fax: +61 7 3365 4754
http://www.shrs.uq.edu.au/shrs/school_staff/ross_darnell.html



From spencer.graves at pdf.com  Wed Jun  2 01:58:56 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 01 Jun 2004 16:58:56 -0700
Subject: [R] GLMM(..., family=binomial(link="cloglog"))?
In-Reply-To: <200406011812.21394.deepayan@stat.wisc.edu>
References: <40BD0272.9040802@pdf.com>
	<200406011812.21394.deepayan@stat.wisc.edu>
Message-ID: <40BD1840.7030402@pdf.com>

Hi, Deepayan: 

      Thanks for your reply.  How can I get the new release in a Windows 
2000 format, downloaded and properly installed? 

      I tried "update.packages", but the new version has not yet 
migrated within reach of the default "update.packages" function call.  I 
tried downloading "lme4 0.6-0-2.tar.gz" from 
"http://www.stat.wisc.edu/%7Ebates/", unzipping it and replacing the old 
"\\R\rw1090pat\library\lme4" with the new.  Unfortunately, when I then 
tried to start R, I got a fatal error message: 

      Error in loadNamespace(i[[1]], c(lib.loc, libPaths()), 
keep.source):  There is no package called 'Matrix'. 

      Fortunately, I was able to restore R to apparently functioning 
order by merely restoring the old version. 

      Is there something easy I can do to get "lme4 0.6-0-2.tar.gz" to 
install properly for me?  Or do I need to wait until it migrates to my 
update.packages default ("http://cran.r-project.org") or some other 
designated CRAN mirror? 

      Thanks for your excellent work in bringing this excellent software 
to its present state. 

      Best Wishes,
      spencer graves

Deepayan Sarkar wrote:

>On Tuesday 01 June 2004 17:25, Spencer Graves wrote:
>  
>
>>      I'm having trouble using binomial(link="cloglog") with GLMM in
>>lme4, Version: 0.5-2, Date: 2004/03/11.  The example in the Help file
>>works fine, even simplified as follows:
>>
>>      fm0 <- GLMM(immun~1, data=guImmun, family=binomial,
>>random=~1|comm)
>>
>>      However, for another application, I need
>>binomial(link="cloglog"),
>>
>>and this generates an error for me:
>> > fm0. <- GLMM(immun~1, data=guImmun,
>> > family=binomial(link="cloglog"),
>>
>>random=~1|comm)
>>Error in getClass(thisClass) : "family" is not a defined class
>>Error in GLMM(immun ~ 1, data = guImmun, family = binomial(link =
>>"cloglog"),  :
>>        S language method selection got an error when called from
>>internal dispatch for function "GLMM"
>>
>>      Any suggestions?
>>    
>>
>
>This should work better in the new lme4 (0.6-x) announced earlier today 
>on r-packages. There is still a bug with cases (like in your example) 
>with only one fixed effect where the show and summary methods produce 
>an error, but that should be fixed soon.
>
>Deepayan
>  
>



From eduarmasrs at yahoo.com.br  Wed Jun  2 03:28:50 2004
From: eduarmasrs at yahoo.com.br (Eduardo Dutra de Armas)
Date: Tue, 1 Jun 2004 22:28:50 -0300
Subject: [R] Manova and contrasts
Message-ID: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAAlkN94tU7pE2294PPHpOIAsKAAAAQAAAAc/fnRrrxC0yE+SuHTOfnZgEAAAAA@yahoo.com.br>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040601/dac82b07/attachment.pl

From andy_liaw at merck.com  Wed Jun  2 03:53:02 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 1 Jun 2004 21:53:02 -0400
Subject: [R] GLMM(..., family=binomial(link="cloglog"))?
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7DFB@usrymx25.merck.com>

As Doug said in his announcement, version 0.6-1 of lme4 (which is pure R
code) depends on the Matrix package, version 0.8-7.  AFAICT the Windows
binary on CRAN for Matrix is version 0.8-6.  Not sure if that will work with
the current lme4...  It's probably best to wait for the right versions of
these packages to propagate to appropriate places on CRAN...

Best,
Andy

> From: Spencer Graves
> 
> Hi, Deepayan: 
> 
>       Thanks for your reply.  How can I get the new release 
> in a Windows 
> 2000 format, downloaded and properly installed? 
> 
>       I tried "update.packages", but the new version has not yet 
> migrated within reach of the default "update.packages" 
> function call.  I 
> tried downloading "lme4 0.6-0-2.tar.gz" from 
> "http://www.stat.wisc.edu/%7Ebates/", unzipping it and 
> replacing the old 
> "\\R\rw1090pat\library\lme4" with the new.  Unfortunately, 
> when I then 
> tried to start R, I got a fatal error message: 
> 
>       Error in loadNamespace(i[[1]], c(lib.loc, libPaths()), 
> keep.source):  There is no package called 'Matrix'. 
> 
>       Fortunately, I was able to restore R to apparently functioning 
> order by merely restoring the old version. 
> 
>       Is there something easy I can do to get "lme4 
> 0.6-0-2.tar.gz" to 
> install properly for me?  Or do I need to wait until it 
> migrates to my 
> update.packages default ("http://cran.r-project.org") or some other 
> designated CRAN mirror? 
> 
>       Thanks for your excellent work in bringing this 
> excellent software 
> to its present state. 
> 
>       Best Wishes,
>       spencer graves
> 
> Deepayan Sarkar wrote:
> 
> >On Tuesday 01 June 2004 17:25, Spencer Graves wrote:
> >  
> >
> >>      I'm having trouble using binomial(link="cloglog") with GLMM in
> >>lme4, Version: 0.5-2, Date: 2004/03/11.  The example in the 
> Help file
> >>works fine, even simplified as follows:
> >>
> >>      fm0 <- GLMM(immun~1, data=guImmun, family=binomial,
> >>random=~1|comm)
> >>
> >>      However, for another application, I need
> >>binomial(link="cloglog"),
> >>
> >>and this generates an error for me:
> >> > fm0. <- GLMM(immun~1, data=guImmun,
> >> > family=binomial(link="cloglog"),
> >>
> >>random=~1|comm)
> >>Error in getClass(thisClass) : "family" is not a defined class
> >>Error in GLMM(immun ~ 1, data = guImmun, family = binomial(link =
> >>"cloglog"),  :
> >>        S language method selection got an error when called from
> >>internal dispatch for function "GLMM"
> >>
> >>      Any suggestions?
> >>    
> >>
> >
> >This should work better in the new lme4 (0.6-x) announced 
> earlier today 
> >on r-packages. There is still a bug with cases (like in your 
> example) 
> >with only one fixed effect where the show and summary 
> methods produce 
> >an error, but that should be fixed soon.
> >
> >Deepayan
> >  
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From deepayan at stat.wisc.edu  Wed Jun  2 04:48:47 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Tue, 1 Jun 2004 21:48:47 -0500
Subject: [R] Parametric Curves
In-Reply-To: <BAY14-F9jvJ1A2fy4B10001e820@hotmail.com>
References: <BAY14-F9jvJ1A2fy4B10001e820@hotmail.com>
Message-ID: <200406012148.47729.deepayan@stat.wisc.edu>

On Tuesday 01 June 2004 09:01, Talita Leite wrote:
> Hi everybody!
>
> I'm trying to plot a parametric curve (three dimensions) using R but
> I didn't obtain good results. Somebody have already done something
> like that? Please help me!

Could you give us more details ? To me, this would be having 3 numeric 
vectors (say x, y and z), and plot them joining consecutive points by 
lines. If that's what you want, see ?cloud in the lattice package. (You 
will need to use type = 'l' for lines.)

Deepayan



From andy_liaw at merck.com  Wed Jun  2 04:55:40 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 1 Jun 2004 22:55:40 -0400
Subject: [R] Parametric Curves
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7DFE@usrymx25.merck.com>



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Deepayan Sarkar
> Sent: Tuesday, June 01, 2004 10:49 PM
> To: r-help at stat.math.ethz.ch
> Cc: Talita Leite
> Subject: Re: [R] Parametric Curves
> 
> 
> On Tuesday 01 June 2004 09:01, Talita Leite wrote:
> > Hi everybody!
> >
> > I'm trying to plot a parametric curve (three dimensions) using R but
> > I didn't obtain good results. Somebody have already done something
> > like that? Please help me!
> 
> Could you give us more details ? To me, this would be having 
> 3 numeric 
> vectors (say x, y and z), and plot them joining consecutive points by 
> lines. If that's what you want, see ?cloud in the lattice 
> package. (You 
> will need to use type = 'l' for lines.)
> 
> Deepayan

Deepayan is right, of course.  Here's a simple example:

t <- seq(0, 4*pi, length=500)
x <- sin(t)
y <- cos(t)
z <- t
library(lattice)
cloud(z ~ x * y, type="l")

HTH,
Andy



From david.netherway at adelaide.edu.au  Wed Jun  2 06:24:13 2004
From: david.netherway at adelaide.edu.au (David J. Netherway)
Date: Wed, 02 Jun 2004 13:54:13 +0930
Subject: [R] loess prediction limits
Message-ID: <40BD566D.7090504@adelaide.edu.au>

Hello

I am plotting a loess curve with confidence limits as below.
How do I create the prediction limits? Is multiplying the standard
errors by sqrt(n) appropriate?
 
data <- mndata
lo <- loess(data[[variableName]] ~ Age, data, span=1.0,
       control = loess.control(surface = "direct"))
xPoints <- data.frame(age = seq(1,240,1))
lo1 <- predict(lo, xPoints, se = TRUE)
age <- xPoints$age
lines(age,lo1$fit, col=4)
# now do +/- 2 std errors
lo1p <- lo1$fit + 2*lo1$se.fit
lo1m <- lo1$fit - 2*lo1$se.fit
lines(age,lo1p, col=4)
lines(age,lo1m, col=4)

Thanks, David



From gplomp at brain.riken.jp  Wed Jun  2 07:16:22 2004
From: gplomp at brain.riken.jp (Gijs Plomp)
Date: Wed, 02 Jun 2004 14:16:22 +0900
Subject: [R] denDF in lme with random factor
Message-ID: <40BD62A6.3080807@brain.riken.jp>

Dear All,

I would like to do a repeated measures analyis of a 4x6x2 factorial 
design with subject as a random variable.

According to the lme documetation, this is the way to do it:
 > anova(exp2.lme<-lme(RTs~rot*fu*fig, random = ~1|sub, data=exp2))

Yet, this gives a table in which all denDFs are identical. This does not 
seem right.

When I specify the test as
 > anova(exp2.lme<-lme(RTs~rot*fu*fig, random = ~1|sub/rot, data=exp2))
I get the right denDF value for the factor 'rot' only, theoutput looks 
like this:

 numDF denDF   F-value p-value
(Intercept)     1  8582 224052069  <.0001
rot             3    45         3  0.0356
fu              5  8582        81  <.0001
fig             1  8582       630  <.0001
rot:fu         15  8582         0  0.9565
rot:fig         3  8582         1  0.3089
fu:fig          5  8582        32  <.0001
rot:fu:fig     15  8582         1  0.8884


Is there a way to specify that subjects are a random factor over all 
variables?
i.e. the lme equivalent of  
exp2.aov<-aov(RTs~fig*rot*fu+Error(sub/(fig+rot+fu)), data=exp2)

I am new to R and hope this question is not too much of a 'newby' one...

Sincerely,

Gijs Plomp



From papydien at yahoo.fr  Wed Jun  2 09:22:54 2004
From: papydien at yahoo.fr (=?iso-8859-1?q?Papy=20Gaston?=)
Date: Wed, 2 Jun 2004 09:22:54 +0200 (CEST)
Subject: [R] Problem in random (lme)
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7DF3@usrymx25.merck.com>
Message-ID: <20040602072254.9020.qmail@web50902.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040602/32116d65/attachment.pl

From s-plus at wiwi.uni-bielefeld.de  Wed Jun  2 10:13:49 2004
From: s-plus at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Wed, 02 Jun 2004 10:13:49 +0200
Subject: [R] advanced dist
References: <20040601105744.GC2511@martin.kleinerdrache.org>
Message-ID: <40BD8C3D.5060609@wiwi.uni-bielefeld.de>

Martin Klaffenboeck wrote:

> Hi there.
>
> I have now found that dist() does what I want in a specific case.
>
> But I have (out of many fields) in my Fragebogen.data file:
>
> Vpn    Code    Family    Test
> 1    X1    m    45
> 2    X1    t    58
> 3    X2    m    44
> 4    X2    t    43
> ...
>
> When I now do:
> Fbg <- read.table("Fragebogen.data", header=TRUE)
> dist(Fbg['Test'])
> I have the distances between everyone to everyone.
>
> Now I want to have the mothers (m in Family) at the y axis and the  
> daugthers (t) on the x axis, so I have half the size of my distance  
> matrix.
>
> Is that possible somehow?
>
> And if possible, I would have the 'Code' at the top to know which  
> mother and wich daugther are compared.
>
> Thanks,
> Martin
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html

what about:

 >  mtdist<- as.matrix(dist(Fbg['Test']))
    dimnames(mtdist) <- list( Fbg[ 'Family' ], Fbg[ 'Family' ] )
    mtdist [ Fbg['Code']=="m" ,Fbg['Code']=="t" ]

Peter Wolf



From ligges at statistik.uni-dortmund.de  Wed Jun  2 10:30:07 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 02 Jun 2004 10:30:07 +0200
Subject: [R] WinMenu's question
In-Reply-To: <wu2qrfxj.fsf@uq.edu.au>
References: <wu2qrfxj.fsf@uq.edu.au>
Message-ID: <40BD900F.7050802@statistik.uni-dortmund.de>

Ross Darnell wrote:

> I am using the Windows menu functions below which will work on the
> first pass, but if I repeat the same script I cannot get the
> WinMenuAddItem to work. This is a problem if I change the menu
> structure and reread the source code I am forced to quit and restart Rgui.
> 
> "try.menu" <- function(){
>   OS <- .Platform$OS.type
>   GUI <- .Platform$GUI
>   if (!(OS == "windows" & GUI == "Rgui")) return("Sorry, you must be running R using Rgui.exe on MS Windows")
>   try(winMenuDelItem('EMG/Graphics','Plot.trace'))
                                            ^ note the dot

>   try(winMenuDel('EMG/Graphics'))
>   try(winMenuDel("EMG"))
>   winMenuAdd("EMG")
>   winMenuAdd("EMG/Graphics")
>   winMenuAddItem("EMG/Graphics","Plot trace","plot.trace()")
                                        ^ note the blank


Uwe Ligges


> }
> "plot.trace" <- function(){
>   x <- eval(parse(text=trace.dialog()))
>   plot(x,type="l")
>   invisible()
> }
> 
> 
> Help much appreciated
> 
> Ross Darnell



From spencer.graves at pdf.com  Wed Jun  2 10:32:17 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 02 Jun 2004 01:32:17 -0700
Subject: [R] S4 classes? 
Message-ID: <40BD9091.4020304@pdf.com>

      The following example, extracted from pp. 38-42 of Chambers (1998) 
Programming with Data (Springer), works for me in S-Plus 6.2 but not R 
1.9.0pat, which returns the indicated error messages: 

 > setClass('track', representation(x="numeric", y="numeric"))
[1] "track"
 > tr1 <- new("track", x=1:3, y=4:6)
 > setMethod("plot",
+ signature(x="track", y="missing"),
+ function(x, y, ...)plot(x at x, x at y, ...)
+ )
[1] "plot"
 > plot(tr1)
Error in plot.window(xlim, ylim, log, asp, ...) :
        need finite xlim values
In addition: Warning messages:
1: no finite arguments to min; returning Inf
2: no finite arguments to max; returning -Inf
3: no finite arguments to min; returning Inf
4: no finite arguments to max; returning -Inf

      Am I missing something or is my copy of R 1.9.0pat corrupted?  
(This is under Windows 2000.) 

      Thanks Much!
      Spencer Graves



From maechler at stat.math.ethz.ch  Wed Jun  2 10:44:57 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 2 Jun 2004 10:44:57 +0200
Subject: [R] S4 classes? 
In-Reply-To: <40BD9091.4020304@pdf.com>
References: <40BD9091.4020304@pdf.com>
Message-ID: <16573.37769.800928.565169@gargle.gargle.HOWL>

>>>>> "Spencer" == Spencer Graves <spencer.graves at pdf.com>
>>>>>     on Wed, 02 Jun 2004 01:32:17 -0700 writes:

    Spencer> The following example, extracted from pp. 38-42 of Chambers (1998) 
    Spencer> Programming with Data (Springer), works for me in S-Plus 6.2 but not R 
    Spencer> 1.9.0pat, which returns the indicated error messages: 

    >> setClass('track', representation(x="numeric", y="numeric"))
    Spencer> [1] "track"
    >> tr1 <- new("track", x=1:3, y=4:6)
    >> setMethod("plot",
    Spencer> + signature(x="track", y="missing"),
    Spencer> + function(x, y, ...)plot(x at x, x at y, ...)
    Spencer> + )
    Spencer> [1] "plot"
    >> plot(tr1)
    Spencer> Error in plot.window(xlim, ylim, log, asp, ...) :
    Spencer> need finite xlim values
    Spencer> In addition: Warning messages:
    Spencer> 1: no finite arguments to min; returning Inf
    Spencer> 2: no finite arguments to max; returning -Inf
    Spencer> 3: no finite arguments to min; returning Inf
    Spencer> 4: no finite arguments to max; returning -Inf

    Spencer> Am I missing something or is my copy of R 1.9.0pat corrupted?  

    Spencer> (This is under Windows 2000.) 

It works without a problem for me, both in R-1.9.0 or R-patched.

Did you try these commands from a "freshly started"   
'R --vanilla' ?
   {you will know how to do this ("R --vanila") in Windows ?}

Martin



From dimitris.rizopoulos at med.kuleuven.ac.be  Wed Jun  2 10:52:41 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Wed, 2 Jun 2004 10:52:41 +0200
Subject: [R] S4 classes? 
References: <40BD9091.4020304@pdf.com>
Message-ID: <00f701c4487e$f7627300$ad133a86@www.domain>

Dear Spencer,

I do not encounter any problem running your code in my machine. Maybe
is something wrong with your R installation!

setClass('track', representation(x="numeric", y="numeric"))
tr1 <- new("track", x=1:3, y=4:6)
plot.track <- function(x, y, ...){
  plot(x at x, x at y, xlab="x", ylab="y")
}

setMethod("plot", signature(x="track", y="missing"), plot.track)
## or
setMethod("plot", signature(x="track", y="missing"), function(x, y,
...) plot(x at x, x at x, ...))

## both work
plot(tr1)

Best,
Dimitris




----- Original Message ----- 
From: "Spencer Graves" <spencer.graves at pdf.com>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, June 02, 2004 10:32 AM
Subject: [R] S4 classes?


>       The following example, extracted from pp. 38-42 of Chambers
(1998)
> Programming with Data (Springer), works for me in S-Plus 6.2 but not
R
> 1.9.0pat, which returns the indicated error messages:
>
>  > setClass('track', representation(x="numeric", y="numeric"))
> [1] "track"
>  > tr1 <- new("track", x=1:3, y=4:6)
>  > setMethod("plot",
> + signature(x="track", y="missing"),
> + function(x, y, ...)plot(x at x, x at y, ...)
> + )
> [1] "plot"
>  > plot(tr1)
> Error in plot.window(xlim, ylim, log, asp, ...) :
>         need finite xlim values
> In addition: Warning messages:
> 1: no finite arguments to min; returning Inf
> 2: no finite arguments to max; returning -Inf
> 3: no finite arguments to min; returning Inf
> 4: no finite arguments to max; returning -Inf
>
>       Am I missing something or is my copy of R 1.9.0pat corrupted?
> (This is under Windows 2000.)
>
>       Thanks Much!
>       Spencer Graves
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From feraudj at ensisun.imag.fr  Wed Jun  2 10:58:08 2004
From: feraudj at ensisun.imag.fr (jerome.feraud@ensimag.imag.fr)
Date: Wed, 2 Jun 2004 10:58:08 +0200 (DST)
Subject: [R] ARCH-M, EGARCH
Message-ID: <Pine.A41.4.44.0406021054100.230718-100000@ensibm.imag.fr>


Hi,

I would like to know if there are R packages in order to fit ARIMA models
with ARCH-M and EGARCH variance specifications. I know packages tseries,
stats, nlme where I found functions : arima.sim, arima, garch. But it's
not enough for me. I need to study ARCH-m and EGARCH. Thank you very much
for your help.

Best regards,

Jerome.



From s-plus at wiwi.uni-bielefeld.de  Wed Jun  2 11:09:13 2004
From: s-plus at wiwi.uni-bielefeld.de (Peter Wolf)
Date: Wed, 02 Jun 2004 11:09:13 +0200
Subject: [R] Making a ranking algorithm more efficient
References: <7E4C06F49D6FEB49BE4B60E5FC92ED7A05F3F1@pnlmse35.pnl.gov>
Message-ID: <40BD9939.60306@wiwi.uni-bielefeld.de>

let's start by defining x and y matching the properties of the points in 
the picture
<<*>>=
x<-c(1,2,4,5,9,3,7,6); y<-c(10,8,5,3,2,9,4,7)
plot(x,y,pch=letters[1:8])

@
along each dimension we have to compare the coordinates of the points:
<<*>>=
outer(x,x,"<")

@
we get a logical matrix:
output-start
Wed Jun  2 10:36:52 2004
      [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]
[1,] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
[2,] FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
[3,] FALSE FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE
[4,] FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE  TRUE
[5,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[6,] FALSE FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE
[7,] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE
[8,] FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE
output-end

results of dimension x and of dimension y have to be aggregated by "&"
<<*>>=
outer(x,x,"<")&outer(y,y,"<")

@
and we get:
output-start
Wed Jun  2 10:44:56 2004
      [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]
[1,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[2,] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE
[3,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE
[4,] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE
[5,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[6,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[7,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
[8,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
output-end

TRUE in position (i,j) indicates that x- and y-coordinates of point i 
are smaller
than those of point j. The sum of row i shows the number of
points that dominate point i. The sum of col j shows the number of points
that are dominated by point j.
 
@
so we can compute the the number of dominated points by applying 
function sum on the cols:
<<*>>=
apply(outer(x,x,"<")&outer(y,y,"<"),2,sum)

@
at last some polishing
<<*>>=
result<-apply(outer(x,x,"<")&outer(y,y,"<"),2,sum)+1
names(result)<-letters[1:8]
result

@
output-start
Wed Jun  2 10:59:03 2004
a b c d e f g h
1 1 1 1 1 2 2 3
output-end

in the case of more than 2 dimensions you have to add further 
outer-operations

Peter Wolf




Waichler, Scott R wrote:

>I would like to make a ranking operation more efficient if possible.
>The goal is to rank a set of points representing objective 
>function values such that points which are "dominated" by no 
>others have rank 1, those which are dominated by one other point 
>have rank 2, etc.  In the example with two dimensions below, objective
>functions 1 and 2 are to be minimized.  Points a-e are non-dominated,
>rank 1 points.  Point f is rank 2 because point b is superior, and 
>point g is rank 2 because point d is superior.  Point h is rank 3 
>because points c and d are both superior.
>
>       | a 
>       |    f
>       |  b
>       |       h          (figure requires monospaced, plain-text
>display)
>Obj 1  |     c    
>       |         g 
>       |      d     
>       |            e
>       |____________________
>
>              Obj 2
> 
>
>I need to compute the ranks of the rows of a matrix, where each row
>represents a point in objective space and the columns
>contain the objective function values to be minimized.  
>Rank is defined as the number of points with objective function 
>values less than or equal to the current point (including the current 
>point).  I've written the following function with two loops:
>
>  PARETO.RANK <- function(obj.array) {
>    obj.array <- unique(obj.array)
>    
>    ind.row <- 1:nrow(obj.array)
>    ind.col <- 1:ncol(obj.array)
>
>    rank.vec <- rep(NA, length(ind.row)) # initialize
>
>    # Loop thru rows (points in objective function space)
>    for(i in ind.row) { 
>      set <- ind.row
>      for (j in ind.col) {  # Loop thru objective functions
>        set <- set[ obj.array[set,j] <= obj.array[i,j] ]
>      }
>      rank.vec[i] <- length(set)
>    }
>    return(rank.vec)
>  } # end PARETO.RANK3()
>
>Can anyone think of a way to do this more efficiently, for example by
>vectorizing further?  
>
>Thanks,
>Scott Waichler
>scott.waichler at pnl.gov
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From Carsten.Colombier at efv.admin.ch  Wed Jun  2 11:17:04 2004
From: Carsten.Colombier at efv.admin.ch (Carsten.Colombier@efv.admin.ch)
Date: Wed, 2 Jun 2004 11:17:04 +0200 
Subject: [R] autokorrelationresistente Kovarianz in R und S-plus
Message-ID: <2CAE512CEB72EE448AADE3444E1FB7185B47E2@ad04mexefd3.ad.admin.ch>

Liebes r-help-Team,

ich bin gerade an meiner NDK-Abschlussarbeit und wollte anfragen, ob Sie
wissen, ob R und S-plus eine Kovarianz f??r  robuste MM-Sch??tzer zur
Verf??gung stellen, die gegen Autokorrelation resistent ist.

K??nnen Sie mir da weiterhelfen?

Vielen Dank,
Carsten Colombier


Dr. Carsten Colombier
Economist
Swiss Federal Finance Administration FFA
Group of Economic Advisers
Bundesgasse 3
CH-3003 Bern
Switzerland

email  carsten.colombier at efv.admin.ch
phone 0041-31-3226332
fax     0041-31-3230833



From Achim.Zeileis at wu-wien.ac.at  Wed Jun  2 11:50:49 2004
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Wed, 2 Jun 2004 11:50:49 +0200
Subject: [R] autokorrelationresistente Kovarianz in R und S-plus
In-Reply-To: <2CAE512CEB72EE448AADE3444E1FB7185B47E2@ad04mexefd3.ad.admin.ch>
References: <2CAE512CEB72EE448AADE3444E1FB7185B47E2@ad04mexefd3.ad.admin.ch>
Message-ID: <20040602115049.500a3462.Achim.Zeileis@wu-wien.ac.at>

Carsten,

the language on this list is English.

On Wed, 2 Jun 2004 11:17:04 +0200  Carsten.Colombier at efv.admin.ch wrote:

> Liebes r-help-Team,
> 
> ich bin gerade an meiner NDK-Abschlussarbeit und wollte anfragen, ob
> Sie wissen, ob R und S-plus eine Kovarianz f??r  robuste MM-Sch??tzer
> zur Verf??gung stellen, die gegen Autokorrelation resistent ist.

This can be done with a combination of the function rlm() for fitting
robust linear models in MASS and the functions from the package sandwich
for computing HAC (heteroskedasticity and autocorrelation consistent)
covariance matrices. One possibility would be to compute a kernel HAC
estimate with automatic bandwidth selection (Andrews, 1991, Econometric)
which is implemented in kernHAC. So you first fit the model 
  fm <- rlm(yourmodel, method = "MM")
using rlm() and then can estimate the corresponding HAC covariance
matrix of the coefficients by
  kernHAC(fm)

hth,
Z

> K??nnen Sie mir da weiterhelfen?
> 
> Vielen Dank,
> Carsten Colombier
> 
> 
> Dr. Carsten Colombier
> Economist
> Swiss Federal Finance Administration FFA
> Group of Economic Advisers
> Bundesgasse 3
> CH-3003 Bern
> Switzerland
> 
> email  carsten.colombier at efv.admin.ch
> phone 0041-31-3226332
> fax     0041-31-3230833
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From ligges at statistik.uni-dortmund.de  Wed Jun  2 12:03:03 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 02 Jun 2004 12:03:03 +0200
Subject: [R] S4 classes?
In-Reply-To: <40BD9091.4020304@pdf.com>
References: <40BD9091.4020304@pdf.com>
Message-ID: <40BDA5D7.7060908@statistik.uni-dortmund.de>

Spencer Graves wrote:

>      The following example, extracted from pp. 38-42 of Chambers (1998) 
> Programming with Data (Springer), works for me in S-Plus 6.2 but not R 
> 1.9.0pat, which returns the indicated error messages:
>  > setClass('track', representation(x="numeric", y="numeric"))
> [1] "track"
>  > tr1 <- new("track", x=1:3, y=4:6)
>  > setMethod("plot",
> + signature(x="track", y="missing"),
> + function(x, y, ...)plot(x at x, x at y, ...)
> + )
> [1] "plot"
>  > plot(tr1)
> Error in plot.window(xlim, ylim, log, asp, ...) :
>        need finite xlim values
> In addition: Warning messages:
> 1: no finite arguments to min; returning Inf
> 2: no finite arguments to max; returning -Inf
> 3: no finite arguments to min; returning Inf
> 4: no finite arguments to max; returning -Inf
> 
>      Am I missing something or is my copy of R 1.9.0pat corrupted?  
> (This is under Windows 2000.)
>      Thanks Much!
>      Spencer Graves


Works for me with R-1.9.0 Patched (2004-05-24)

Uwe



From breitling at lb-netz.de  Wed Jun  2 12:52:29 2004
From: breitling at lb-netz.de (Lutz Ph. Breitling)
Date: Wed, 02 Jun 2004 11:52:29 +0100
Subject: [R] poisson regression with robust error variance ('eyestudy')
Message-ID: <6.0.1.1.0.20040602101549.01f674e0@192.168.1.2>

Dear all,

i am trying to redo the 'eyestudy' analysis presented on the site
http://www.ats.ucla.edu/stat/stata/faq/relative_risk.htm
with R (1.9.0), with special interest in the section on "relative risk 
estimation by poisson regression with robust error variance".

so i guess rlm is the function to use. but what is its equivalent to the 
glm's argument "family" to indicate 'poisson'? or am i somehow totally 
wrong and this is not applicable here?

thx a lot-
lutz


=============================
Lutz Ph. Breitling, CMd
Unit? des Recherches M?dicale
H?pital Albert Schweitzer
B.P. 118 Lambar?n? (GABON)  
-------------- next part --------------

---




From HStevens at MUOhio.edu  Wed Jun  2 14:48:20 2004
From: HStevens at MUOhio.edu (Martin Henry H. Stevens)
Date: Wed, 2 Jun 2004 08:48:20 -0400
Subject: [R] Parametric Curves
In-Reply-To: <BAY14-F9jvJ1A2fy4B10001e820@hotmail.com>
References: <BAY14-F9jvJ1A2fy4B10001e820@hotmail.com>
Message-ID: <20B1071C-B493-11D8-ADA6-000A958F43CC@MUOhio.edu>

See also the scatterplot3d package.
Hank
On Jun 1, 2004, at 10:01 AM, Talita Leite wrote:

> Hi everybody!
>
> I'm trying to plot a parametric curve (three dimensions) using R but I 
> didn't obtain good results. Somebody have already done something like 
> that? Please help me!
>
>
>
> Talita Perciano Costa Leite
> Graduanda em Ci??ncia da Computa????o
> Universidade Federal de Alagoas - UFAL
> Departamento de Tecnologia da Informa????o - TCI
> Constru????o de Conhecimento por Agrupamento de Dados - CoCADa
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>
Dr. Martin Henry H. Stevens, Assistant Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/botany/bot/henry.html
http://www.muohio.edu/ecology/
http://www.muohio.edu/botany/



From gregory_r_warnes at groton.pfizer.com  Wed Jun  2 14:58:23 2004
From: gregory_r_warnes at groton.pfizer.com (Warnes, Gregory R)
Date: Wed, 2 Jun 2004 08:58:23 -0400 
Subject: [R] Contrasts
Message-ID: <D7A3CFD7825BD6119B880002A58F06C20C52147F@groexmb02.pfizer.com>


Try the fit.contrast() and estimable() functions from my gregmisc package,
or the contrasts() function in Frank Harrell's Design package.

-Greg

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Kimberly Ann
> Fernandes
> Sent: Monday, May 31, 2004 1:10 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Contrasts
> 
> 
> Hello,
> 
> I am trying to figure out how to conduct a t-test on a 
> specific contrast
> for my data.  I have four factors in my data and would like 
> to conduct a
> t-test on the average of the data from the first two factors 
> against the
> average of the data on the second two factor (i.e. is the 
> average of the
> first two different from the average of the second two).  Is there a
> quick way to do this?  I found the contrast function, but wasn't sure
> how to apply it.
> 
> Thank you,
> Kim
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 


LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}



From pgreen at umich.edu  Wed Jun  2 15:21:14 2004
From: pgreen at umich.edu (Paul E. Green)
Date: Wed, 2 Jun 2004 09:21:14 -0400
Subject: [R] methods for complex sample surveys
Message-ID: <001c01c448a4$7b5c00c0$f94ed38d@TRIRM326>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040602/fc7b7914/attachment.pl

From tobias.verbeke at bivv.be  Wed Jun  2 15:25:25 2004
From: tobias.verbeke at bivv.be (tobias.verbeke@bivv.be)
Date: Wed, 2 Jun 2004 15:25:25 +0200
Subject: [R] methods for complex sample surveys
In-Reply-To: <001c01c448a4$7b5c00c0$f94ed38d@TRIRM326>
Message-ID: <OFE94E2ED6.091425A7-ONC1256EA7.0049ADC7-C1256EA7.0049E732@BIVV.BE>





r-help-bounces at stat.math.ethz.ch wrote on 02/06/2004 15:21:14:

> I have learned a lot from this list. I would
> like to thank the developers and contributors who
> devote so much of their time to this project.
>
> Does anyone know if any methods have been
> developed for handling data from complex sample
> surveys that include sample weights, clusters,
> strata, and so on? I know that SUDAAN, Stata
> have some abilities. Does anything exist in R/S?

There is a survey package on CRAN, by Thomas Lumley.

HTH,

Tobias



From r-help at stat.math.ethz.ch  Wed Jun  2 14:24:54 2004
From: r-help at stat.math.ethz.ch (r-help@stat.math.ethz.ch)
Date: Wed, 2 Jun 2004 13:24:54 +0100
Subject: [R] Virus intercepted
Message-ID: <200406021224.i52COsFJ012906@free.tvtel.pt>

A message you sent to
	<psycorps at tvtel.pt>
contained Worm.SomeFool.Gen-1 and has not been delivered.



From wolski at molgen.mpg.de  Wed Jun  2 15:47:30 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Wed, 02 Jun 2004 15:47:30 +0200
Subject: [R] How to iterate through two objects of class "list" fast?
Message-ID: <200406021547300821.0670A037@mail.math.fu-berlin.de>

Hi!

I have 2 list objects
One list contains the data (list1) the second list2 contains the e.g linear model for each element of list 1.
list2 <- lapply(list1,mylm,response) I obtain list 2 by something like this.

now I have a function myfunc  which takes an element from list1 and the coresponding element from list2 does something and ....

list3[[x]] <- myfunc(list1[[x]],list2[[x]])

Until now I am runnign this in a for(x in 1:length(list1)) loop. But "for" loops are slow especially if list3 is HUGE.

What can I do?

Sincerely
Eryk



From v.demartino2 at virgilio.it  Wed Jun  2 17:51:39 2004
From: v.demartino2 at virgilio.it (Vittorio)
Date: Wed, 2 Jun 2004 16:51:39 +0100
Subject: [R] ffnet problem
In-Reply-To: <40BC676D.9090505@bluewin.ch>
References: <40BC676D.9090505@bluewin.ch>
Message-ID: <200406021651.40135.v.demartino2@virgilio.it>

On Tuesday 01 June 2004 12:24, Adrian Trapletti wrote:
> >Given that those files are dated 27/06/2000, they are unlikely to work
> > with the current version of R.  (R has changed quite a bit since 2000!!)
>
> In fact, I stopped to support ffnet in 2000, and I would be very
> surprised if it would still run under the newer versions of R.
>
> >Don't know why/how the code depends on NR codes, as one can find better
> >quality code in many instances.  It might not be too difficult to get rid
> > of the dependency on NR codes.
>
> The underlying c++ code of ffnet has been programmed around 1996 at a
> time when the capabilities of R to do multidimensional optimization
> through an API where rather limited. Hence my decision at that time to
> base the code on Numerical Recipes. But from today's perspective, it
> would be much better to call the optimizers from optim() through the API.
>
> >BTW, why not use nnet in the VR bundle?  (Obviously I have not looked at
> >ffnet at all...)
>
> I also suggest you use nnet.
>
> >Best,
> >Andy
> >

Thanks to the many who answered clarifying the point.

I was just having a go at nnet when I knew about ffnet and wanted only to have 
an alternative, one more option. You know open source and free sofware 
usually offers this opportunity!

Ciao Vittorio



From swhite at aegis-semi.com  Wed Jun  2 16:07:25 2004
From: swhite at aegis-semi.com (Steven White)
Date: Wed, 2 Jun 2004 10:07:25 -0400
Subject: [R] use of split.screen() with postscript device
Message-ID: <200406021007.25768.swhite@aegis-semi.com>

Greetings,

I am new to R and quite confused by the apparent inconsistency in it's 
behavior. My latest difficulty lies in producing eps output for use with 
latex. In particular, I was successful in obtaining a single figure by using:

postscript("ripple1.eps",onefile=FALSE)
plot(nu_set[[1]],ip[[1]])
lines(nu_set[[1]],predict(fitIp[[1]]))
dev.off()

...but then I tried to place two figures on the same output sheet by using 
split.screen() as follows:

postscript("ripple1.eps",onefile=FALSE)
split.screen(c(1,2))
screen(1)
plot(nu_set[[1]],ip[[1]])
lines(nu_set[[1]],predict(fitIp[[1]]))
screen(2)
plot(rangeIpNorm)
dev.off()

and received no output whatsoever. Furthermore, I can no longer get postscript 
output using the original commands! I've tried exiting and re-starting R, and 
am just about to reboot because I almost cannot believe what I am seeing.

Can you spot anything that I am doing wrong?

Thanks & Best Regards,
Steve



From p.dalgaard at biostat.ku.dk  Wed Jun  2 16:13:19 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 02 Jun 2004 16:13:19 +0200
Subject: [R] How to iterate through two objects of class "list" fast?
In-Reply-To: <200406021547300821.0670A037@mail.math.fu-berlin.de>
References: <200406021547300821.0670A037@mail.math.fu-berlin.de>
Message-ID: <x2ekoy120w.fsf@biostat.ku.dk>

"Wolski" <wolski at molgen.mpg.de> writes:

> list3[[x]] <- myfunc(list1[[x]],list2[[x]])
> 
> Until now I am runnign this in a for(x in 1:length(list1)) loop. But "for" loops are slow especially if list3 is HUGE.
> 
> What can I do?

Use mapply(). I wouldn't expect the for loop to be all that slow
unless you have forgotten to allocate memory for the result list up
front, though.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From tlumley at u.washington.edu  Wed Jun  2 16:36:29 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 2 Jun 2004 07:36:29 -0700 (PDT)
Subject: [R] poisson regression with robust error variance ('eyestudy')
In-Reply-To: <6.0.1.1.0.20040602101549.01f674e0@192.168.1.2>
References: <6.0.1.1.0.20040602101549.01f674e0@192.168.1.2>
Message-ID: <Pine.A41.4.58.0406020732060.77094@homer08.u.washington.edu>

On Wed, 2 Jun 2004, Lutz Ph. Breitling wrote:

> Dear all,
>
> i am trying to redo the 'eyestudy' analysis presented on the site
> http://www.ats.ucla.edu/stat/stata/faq/relative_risk.htm
> with R (1.9.0), with special interest in the section on "relative risk
> estimation by poisson regression with robust error variance".
>
> so i guess rlm is the function to use. but what is its equivalent to the
> glm's argument "family" to indicate 'poisson'? or am i somehow totally
> wrong and this is not applicable here?
>

No, no.  You want glm() and then a function to compute the robust
covariance matrix (there's robcov() in the Hmisc package), or use gee()
from the "gee" package or geese() from "geepack" with independence working
correlation.

These are not outlier-resistant estimates of the regression coefficients,
they are model-agnostic estimates of the standard errors.

Stata is unusual in providing these covariance matrix estimates for just
about every regression estimator.  I think R should consider doing
something similar, but haven't got around to it.

	-thomas


> thx a lot-
> lutz
>
>
> =============================
> Lutz Ph. Breitling, CMd
> Unit des Recherches Mdicale
> Hpital Albert Schweitzer
> B.P. 118 Lambarn (GABON)
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From tlumley at u.washington.edu  Wed Jun  2 16:37:13 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 2 Jun 2004 07:37:13 -0700 (PDT)
Subject: [R] methods for complex sample surveys
In-Reply-To: <001c01c448a4$7b5c00c0$f94ed38d@TRIRM326>
References: <001c01c448a4$7b5c00c0$f94ed38d@TRIRM326>
Message-ID: <Pine.A41.4.58.0406020736550.77094@homer08.u.washington.edu>

On Wed, 2 Jun 2004, Paul E. Green wrote:

> I have learned a lot from this list. I would
> like to thank the developers and contributors who
> devote so much of their time to this project.
>
> Does anyone know if any methods have been
> developed for handling data from complex sample
> surveys that include sample weights, clusters,
> strata, and so on? I know that SUDAAN, Stata
> have some abilities. Does anything exist in R/S?

There is quite a lot of this in the "survey" package.

	-thomas


>
> Paul E. Green
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From tlumley at u.washington.edu  Wed Jun  2 16:39:13 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 2 Jun 2004 07:39:13 -0700 (PDT)
Subject: [R] How to iterate through two objects of class "list" fast?
In-Reply-To: <x2ekoy120w.fsf@biostat.ku.dk>
References: <200406021547300821.0670A037@mail.math.fu-berlin.de>
	<x2ekoy120w.fsf@biostat.ku.dk>
Message-ID: <Pine.A41.4.58.0406020738320.77094@homer08.u.washington.edu>

On Wed, 2 Jun 2004, Peter Dalgaard wrote:

> "Wolski" <wolski at molgen.mpg.de> writes:
>
> > list3[[x]] <- myfunc(list1[[x]],list2[[x]])
> >
> > Until now I am runnign this in a for(x in 1:length(list1)) loop. But "for" loops are slow especially if list3 is HUGE.
> >
> > What can I do?
>
> Use mapply(). I wouldn't expect the for loop to be all that slow
> unless you have forgotten to allocate memory for the result list up
> front, though.
>

If you preallocate the memory the for loop is probably faster than
mapply().

	-thomas



From wolski at molgen.mpg.de  Wed Jun  2 17:20:35 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Wed, 02 Jun 2004 17:20:35 +0200
Subject: [R] How to iterate through two objects of class "list"
  fast?
In-Reply-To: <Pine.A41.4.58.0406020738320.77094@homer08.u.washington.edu>
References: <200406021547300821.0670A037@mail.math.fu-berlin.de>
	<x2ekoy120w.fsf@biostat.ku.dk>
	<Pine.A41.4.58.0406020738320.77094@homer08.u.washington.edu>
Message-ID: <200406021720350476.019990E3@mail.math.fu-berlin.de>

Hallo!

Hi I was checking the performance of the for loop and mapply with system.time()

Case a)
the result list is allocated. It is the same list which element i pass as an argument.
e.g.
list2[[x]] <- myfunc(list2[[x]],list3[[x]])
24.18 s

Case b)
the same with mapply(myfunc,list2,list3)
1.96 s

Case c)
I have adjuested my function with a helper function to work with apply
tmp<-function(xx,oby)
{
myfunc(xx,oby[[xx at info]])
}
and than apply(obx,tmp,oby)
0.36 s

And finally 

Case d)
res<-list(length(oby))
res[[x]] <- myfunc(list2[[x]],list3[[x]])
0.36 s

I am Impressed

Thanks a lot Thomas L and Peter D.
Sincerely
Eryk



*********** REPLY SEPARATOR  ***********

On 6/2/2004 at 7:39 AM Thomas Lumley wrote:

>>>On Wed, 2 Jun 2004, Peter Dalgaard wrote:
>>>
>>>> "Wolski" <wolski at molgen.mpg.de> writes:
>>>>
>>>> > list3[[x]] <- myfunc(list1[[x]],list2[[x]])
>>>> >
>>>> > Until now I am runnign this in a for(x in 1:length(list1)) loop. But
>>>"for" loops are slow especially if list3 is HUGE.
>>>> >
>>>> > What can I do?
>>>>
>>>> Use mapply(). I wouldn't expect the for loop to be all that slow
>>>> unless you have forgotten to allocate memory for the result list up
>>>> front, though.
>>>>
>>>
>>>If you preallocate the memory the for loop is probably faster than
>>>mapply().
>>>
>>>	-thomas



Dipl. bio-chem. Eryk Witold Wolski    @    MPI-Moleculare Genetic   
Ihnestrasse 63-73 14195 Berlin       'v'    
tel: 0049-30-83875219               /   \    
mail: wolski at molgen.mpg.de        ---W-W----    http://www.molgen.mpg.de/~wolski



From Yves.Rosseel at UGent.be  Wed Jun  2 17:56:00 2004
From: Yves.Rosseel at UGent.be (Yves Rosseel)
Date: Wed, 02 Jun 2004 17:56:00 +0200
Subject: [R] Manova and contrasts
Message-ID: <40BDF890.5020407@UGent.be>

Dear Eduardo,

> Hi R-users
> I'm trying to do multivariate analysis of variance of a experiment with
> 3 treatments, 2 variables and 5 replicates.
> The procedure adopted in SAS is as follow, but I'm having difficulty in
> to implement the contrasts for comparison of all treatments in R.

[See the original mail for the SAS input and output...]

What you probably want is a general function to test multivariate linear 
hypotheses of the type 'LBM=K' with B the parameter matrix. 
Unfortunately, as far as I know, no such a function is available in R or 
any package on CRAN. (For the univariate case, there are some functions 
for testing linear hypotheses in the packages car and gregmisc) It is, 
however, fairly easy to write such a function yourself. All you need to 
do is to compute the corresponding SSCP matrix for your particular set 
of contrasts, and 'borrow' some code from the 'summary.manova' function 
(only Wilks lambda is computed here):

mlh <- function(fit, L, M) {

     if(!inherits(fit, "maov"))
         stop("object must be of class \"manova\" or \"maov\"")


     if(  is.null(dim(L)) )
         L <- t(L)

     rss.qr <- qr( crossprod(  fit$residuals %*% M  ) )

     X <- as.matrix( model.matrix(fit) )
     B <- as.matrix( fit$coef )

     H <- t(M) %*% t(L%*%B)  %*%
          as.matrix(solve(L%*%solve(t(X)%*%X)%*%t(L)))  %*%
          (L%*%B)%*%M

     eig <- Re(eigen(qr.coef(rss.qr, H), symmetric = FALSE)$values)

     q <- nrow(L); df.res <- fit$df.residual

     test <- prod(1/(1 + eig))
     p <- length(eig)
     tmp1 <- df.res - 0.5 * (p - q + 1)
     tmp2 <- (p * q - 2)/4
     tmp3 <- p^2 + q^2 - 5
     tmp3 <-  if(tmp3 > 0) sqrt(((p*q)^2 - 4)/tmp3) else 1


     wilks <- test
     F     <- ((test^(-1/tmp3) - 1) * (tmp1 * tmp3 - 2 * tmp2))/p/q
     df1   <- p * q
     df2   <- tmp1 * tmp3 - 2 * tmp2
     Prob  <- pf(F, df1, df2, lower.tail = FALSE)

     out <- list(wilks=wilks, F=F, df1=df1, df2=df2, Prob=Prob)
     out
}


To test the first contrast in your example (contrast 'TESTE vs TURFE' 
TRA 1 -1  0), you can proceed as follows:

# your data (copied from your original mail)
### R
X1 =
c(4.63,4.38,4.94,4.96,4.48,6.03,5.96,6.16,6.33,6.08,4.71,4.81,4.49,4.43,
4.56)
X2 =
c(0.95,0.89,1.01,1.23,0.94,1.08,1.05,1.08,1.19,1.08,0.96,0.93,0.87,0.82,
0.91)
Trat = as.factor(c(rep("TESTE",5),rep("TURFE",5), rep("TURNA",5)))
Y = cbind(X1,X2)
fit = manova(Y ~ Trat)

# construct a 'L-matrix' (in this case only one row)
L <- rbind( c(0, -1, 0) )

# note: your contrast is '1 -1 0'
# adding a zero for the intercept gives
# L = (0 1 -1 0)
# removing 'redundant' coefficients gives
# L = (0 -1 0)
# The coefficient '1' should not be
# specified because the corresponding  parameters were fixed
# to zero, and (unlike SAS or SPSS!) these
# redundant parameters are not included in the parameter matrix

# M-matrix is 2x2 identity matrix
M <- diag(2)

# test multivariate linear hypothesis
mlh(fit, L, M)

# the output is the same as in your SAS output:
$wilks
[1] 0.03437322

$F
[1] 154.5083

$df1
[1] 2

$df2
[1] 11

$Prob
[1] 8.896343e-09



Since multivariate linear hypotheses are widely used in psychology and 
related disciplines, it would be nice if a more robust and cleaned-up 
version of the 'mlh' function would eventually be added to R...

Yves Rosseel.

-- 
Dr. Yves Rosseel -- http://allserv.ugent.be/~yrosseel/
Department of Data Analysis, Ghent University
Henri Dunantlaan 1, B-9000 Gent, Belgium



From f.harrell at vanderbilt.edu  Wed Jun  2 18:25:51 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 02 Jun 2004 11:25:51 -0500
Subject: [R] poisson regression with robust error variance ('eyestudy')
In-Reply-To: <Pine.A41.4.58.0406020732060.77094@homer08.u.washington.edu>
References: <6.0.1.1.0.20040602101549.01f674e0@192.168.1.2>
	<Pine.A41.4.58.0406020732060.77094@homer08.u.washington.edu>
Message-ID: <40BDFF8F.6090906@vanderbilt.edu>

Thomas Lumley wrote:
> On Wed, 2 Jun 2004, Lutz Ph. Breitling wrote:
> 
> 
>>Dear all,
>>
>>i am trying to redo the 'eyestudy' analysis presented on the site
>>http://www.ats.ucla.edu/stat/stata/faq/relative_risk.htm
>>with R (1.9.0), with special interest in the section on "relative risk
>>estimation by poisson regression with robust error variance".
>>
>>so i guess rlm is the function to use. but what is its equivalent to the
>>glm's argument "family" to indicate 'poisson'? or am i somehow totally
>>wrong and this is not applicable here?
>>
> 
> 
> No, no.  You want glm() and then a function to compute the robust
> covariance matrix (there's robcov() in the Hmisc package), or use gee()
> from the "gee" package or geese() from "geepack" with independence working
> correlation.

Slight correction: robcov in the Design package, can easily be used with 
Design's glmD function.  -Frank

> 
> These are not outlier-resistant estimates of the regression coefficients,
> they are model-agnostic estimates of the standard errors.
> 
> Stata is unusual in providing these covariance matrix estimates for just
> about every regression estimator.  I think R should consider doing
> something similar, but haven't got around to it.
> 
> 	-thomas
> 
> 
> 
>>thx a lot-
>>lutz
>>
>>
>>=============================
>>Lutz Ph. Breitling, CMd
> 
> 
> Thomas Lumley			Assoc. Professor, Biostatistics
> tlumley at u.washington.edu	University of Washington, Seattle
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From martin.klaffenboeck at gmx.at  Wed Jun  2 19:05:29 2004
From: martin.klaffenboeck at gmx.at (Martin Klaffenboeck)
Date: Wed, 2 Jun 2004 19:05:29 +0200
Subject: [R] advanced dist
In-Reply-To: <40BD8C3D.5060609@wiwi.uni-bielefeld.de> (from
	s-plus@wiwi.uni-bielefeld.de on Mi, Jun 02,
	2004 at 10:13:49 +0200)
References: <20040601105744.GC2511@martin.kleinerdrache.org>
	<40BD8C3D.5060609@wiwi.uni-bielefeld.de>
Message-ID: <20040602170529.GA6984@martin.kleinerdrache.org>

Am 02.06.2004 10:13:49 schrieb(en) Peter Wolf:

> what about:
> 
> >  mtdist<- as.matrix(dist(Fbg['Test']))
>    dimnames(mtdist) <- list( Fbg[ 'Family' ], Fbg[ 'Family' ] )
>    mtdist [ Fbg['Code']=="m" ,Fbg['Code']=="t" ]

Thanks, thats really what I want.  I hope I can do that by myself,  
soon.

Martin



From fzh113 at hecky.it.northwestern.edu  Wed Jun  2 20:43:43 2004
From: fzh113 at hecky.it.northwestern.edu (Fred)
Date: Wed, 2 Jun 2004 13:43:43 -0500
Subject: [R] What's the Edgeworth expansion of a random vector
Message-ID: <001001c448d1$87f7b860$a7560d18@f0z6305>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040602/a29a3be4/attachment.pl

From pem at theriver.com  Wed Jun  2 20:56:07 2004
From: pem at theriver.com (Patrick E. McKnight)
Date: Wed, 2 Jun 2004 11:56:07 -0700
Subject: [R] Request comments on missing data diagnosis code
Message-ID: <20040602115607.69fe58f8@thurstone>

Greetings,

The attached code is my preliminary attempt to create a comprehensive missing data diagnostic package.  I would greatly appreciate any constructive feedback about the code and encourage others to use it if they feel it is helpful.  One warning might be in order - I have tested the mcar test only superficially.  There might be a few other bugs but it mostly works for my initial purposes.  I intend to create a complete package with the proper documentation shortly.  

Apologies in advance if this is not the proper forum to post code but I did not see anything in the FAQ's or development pages that forbid this distribution mechanism.

-- 

Cheers,

Patrick
--------------------------------------------
Patrick E. McKnight, Ph.D.
Research Assistant Professor
Evaluation Group for Analysis of Data
University of Arizona
Department of Psychology
Tucson, AZ  85721
520-621-5463
pem at u.arizona.edu


From wolski at molgen.mpg.de  Wed Jun  2 21:01:48 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Wed, 02 Jun 2004 21:01:48 +0200
Subject: [R] How do i draw a step function using R?
In-Reply-To: <BAY1-F108JObCSRckNR0003de3c@hotmail.com>
References: <BAY1-F108JObCSRckNR0003de3c@hotmail.com>
Message-ID: <200406022101480308.026413AD@mail.math.fu-berlin.de>

Hi John!
Was you trying to use
help.search("stepfunc")


Sincerely Eryk
*********** REPLY SEPARATOR  ***********

On 6/2/2004 at 6:48 PM john miller wrote:

>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



Dipl. bio-chem. Eryk Witold Wolski    @    MPI-Moleculare Genetic   
Ihnestrasse 63-73 14195 Berlin       'v'    
tel: 0049-30-83875219               /   \    
mail: wolski at molgen.mpg.de        ---W-W----    http://www.molgen.mpg.de/~wolski



From JEFHEN at SAFECO.com  Wed Jun  2 21:19:30 2004
From: JEFHEN at SAFECO.com (HENRIKSON, JEFFREY)
Date: Wed, 2 Jun 2004 12:19:30 -0700
Subject: [R] data filtering
Message-ID: <9410EC84C0872141B27A2726613EF45D02A534F2@psmrdcex01.psm.pin.safeco.com>


I would like to know if there is a way to do the following command in
one step, primarily for speed on large data (5 million elements), and
secondarily for readablity.

mean(delta[(intersect(which(x[['class']]==0),which(delta<1)))])


Do I really have to rely on an intersect operator?  Isn't that
O(nlg(n))?  Can't I just filter in one step?  As an R newbie, I would
have guessed I could write

mean(delta[which((x[['class']]==0) && (delta<1))])

But I guess no such luck since (delta<1), etc are vectors.  Are they
really implemented as vectors?  Ie, if I take 5M data points, does it
allocate 20MB of RAM to make a test that passes most of the elements?

The only thing I can think of is to use closures to write something like
a Lisp list "filter".  Not sure on the readabilty merits, especially if
there is a direct way to do it.  If Matlab had closures I know running
them in a loop would be a bear on runtime anyway.


Jeff Henrikson



From mi2kelgrum at yahoo.com  Wed Jun  2 21:28:23 2004
From: mi2kelgrum at yahoo.com (Mikkel Grum)
Date: Wed, 2 Jun 2004 12:28:23 -0700 (PDT)
Subject: [R] factor -> numeric
Message-ID: <20040602192823.18594.qmail@web60205.mail.yahoo.com>

Hi,

I'm extracting data from a database with values for
different observation types in the same variable
(another variable deifnes the observation type).  Some
of these observation types are factors, so R naturally
classifies the entire variable as a factor.  I want to
select a subset and convert the values to numeric
values, but it isn't working; as shown below:

> rsm2<-subset(rsm,PlantStrataId==2,select=c(Value))
> rsm2$Value
  [1]                               70 30             
              15 50      
 [26]                2                                
                         
 [51]                                                
6                         
 [76]                                                 
                 30    10
[101]                                        17       
              15         
[126]   
24 Levels:  0-25 1 10 12 13 15 17 2 20 23 25 25-50 3
30 4 40 5 50 50-75 ... Present
> rsm2$Value<-as.numeric("rsm2$Value")
Warning message: 
NAs introduced by coercion 
> rsm2$Value
  [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
NA NA NA NA NA NA NA NA NA
 [26] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
NA NA NA NA NA NA NA NA NA
 [51] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
NA NA NA NA NA NA NA NA NA
 [76] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
NA NA NA NA NA NA NA NA NA
[101] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
NA NA NA NA NA NA NA NA NA
[126] NA
>

is there any way out?  Help much appreciated.

cheers,
Mikkel



From rpeng at jhsph.edu  Wed Jun  2 21:33:32 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Wed, 02 Jun 2004 15:33:32 -0400
Subject: [R] data filtering
In-Reply-To: <9410EC84C0872141B27A2726613EF45D02A534F2@psmrdcex01.psm.pin.safeco.com>
References: <9410EC84C0872141B27A2726613EF45D02A534F2@psmrdcex01.psm.pin.safeco.com>
Message-ID: <40BE2B8C.5000003@jhsph.edu>

I think you want to use a single `&' rather than a double `&&'.

-roger

HENRIKSON, JEFFREY wrote:
> I would like to know if there is a way to do the following command in
> one step, primarily for speed on large data (5 million elements), and
> secondarily for readablity.
> 
> mean(delta[(intersect(which(x[['class']]==0),which(delta<1)))])
> 
> 
> Do I really have to rely on an intersect operator?  Isn't that
> O(nlg(n))?  Can't I just filter in one step?  As an R newbie, I would
> have guessed I could write
> 
> mean(delta[which((x[['class']]==0) && (delta<1))])
> 
> But I guess no such luck since (delta<1), etc are vectors.  Are they
> really implemented as vectors?  Ie, if I take 5M data points, does it
> allocate 20MB of RAM to make a test that passes most of the elements?
> 
> The only thing I can think of is to use closures to write something like
> a Lisp list "filter".  Not sure on the readabilty merits, especially if
> there is a direct way to do it.  If Matlab had closures I know running
> them in a loop would be a bear on runtime anyway.
> 
> 
> Jeff Henrikson
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From spencer.graves at pdf.com  Wed Jun  2 21:50:33 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 02 Jun 2004 12:50:33 -0700
Subject: [R] factor -> numeric
In-Reply-To: <20040602192823.18594.qmail@web60205.mail.yahoo.com>
References: <20040602192823.18594.qmail@web60205.mail.yahoo.com>
Message-ID: <40BE2F89.9040801@pdf.com>

      Have you tried "as.numeric(as.character(rsm2$Value))"?  The 
construct "as.numeric(rsm2$Value)" returns the NUMERIC CODES for the 
different levels;  "as.character(rsm2$Value)" returns the character 
representation, which you can then convert to numeric. 

      hope this helps.  spencer graves

Mikkel Grum wrote:

>Hi,
>
>I'm extracting data from a database with values for
>different observation types in the same variable
>(another variable deifnes the observation type).  Some
>of these observation types are factors, so R naturally
>classifies the entire variable as a factor.  I want to
>select a subset and convert the values to numeric
>values, but it isn't working; as shown below:
>
>  
>
>>rsm2<-subset(rsm,PlantStrataId==2,select=c(Value))
>>rsm2$Value
>>    
>>
>  [1]                               70 30             
>              15 50      
> [26]                2                                
>                         
> [51]                                                
>6                         
> [76]                                                 
>                 30    10
>[101]                                        17       
>              15         
>[126]   
>24 Levels:  0-25 1 10 12 13 15 17 2 20 23 25 25-50 3
>30 4 40 5 50 50-75 ... Present
>  
>
>>rsm2$Value<-as.numeric("rsm2$Value")
>>    
>>
>Warning message: 
>NAs introduced by coercion 
>  
>
>>rsm2$Value
>>    
>>
>  [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
>NA NA NA NA NA NA NA NA NA
> [26] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
>NA NA NA NA NA NA NA NA NA
> [51] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
>NA NA NA NA NA NA NA NA NA
> [76] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
>NA NA NA NA NA NA NA NA NA
>[101] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
>NA NA NA NA NA NA NA NA NA
>[126] NA
>  
>
>
>is there any way out?  Help much appreciated.
>
>cheers,
>Mikkel
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From saroj at wayne.edu  Thu Jun  3 01:11:41 2004
From: saroj at wayne.edu (Saroj Mohapatra)
Date: Wed, 2 Jun 2004 16:11:41 -0700
Subject: [R] Distributed computing with R
Message-ID: <000b01c448f6$f72665e0$7fe40992@BIOINFO>

Dear all,

We have started using R for data analysis since a few months and find it
useful. We are planning to acquire a high-end dedicated system for
microarray data analysis and thinking of a distributed environment. I
would appreciate if some one could send some pointers regarding how to
choose a proper hardware configuration, software (R or other software,
esp. MATLAB), issues on setting up the cluster, etc. Has anyone here
some experience of R on a cluster? Does it provide significant benefits
as regards processing time? Is setting up the cluster more difficult
than using R on it?

Thanks.

Saroj K Mohapatra, MD
Research Associate
Tainsky Lab
Karmanos Cancer Institute
Wayne State University School of Medicine
110 E. Warren, Room 311
Detroit MI 48201
313-833-0715 x2424
saroj at wayne.edu



From upton at mitre.org  Wed Jun  2 22:26:40 2004
From: upton at mitre.org (Stephen C. Upton)
Date: Wed, 2 Jun 2004 16:26:40 -0400
Subject: [R] Distributed computing with R
In-Reply-To: <000b01c448f6$f72665e0$7fe40992@BIOINFO>
Message-ID: <026d01c448df$e9e76760$17470843@MITRE.ORG>

Soraj,

Haven't had any experience with R in a distributed computing environment,
but have used Condor (http://www.cs.wisc.edu/condor/) with several
applications. It's free, has a public license for use, is easy to use, but
is not open source. Since you can "batch up" R code, this might be a
reasonable option. 

HTH
steve

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Saroj Mohapatra
Sent: Wednesday, June 02, 2004 7:12 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Distributed computing with R


Dear all,

We have started using R for data analysis since a few months and find it
useful. We are planning to acquire a high-end dedicated system for
microarray data analysis and thinking of a distributed environment. I
would appreciate if some one could send some pointers regarding how to
choose a proper hardware configuration, software (R or other software,
esp. MATLAB), issues on setting up the cluster, etc. Has anyone here
some experience of R on a cluster? Does it provide significant benefits
as regards processing time? Is setting up the cluster more difficult
than using R on it?

Thanks.

Saroj K Mohapatra, MD
Research Associate
Tainsky Lab
Karmanos Cancer Institute
Wayne State University School of Medicine
110 E. Warren, Room 311
Detroit MI 48201
313-833-0715 x2424
saroj at wayne.edu

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From anthony at darrouzet-nardi.net  Wed Jun  2 22:26:48 2004
From: anthony at darrouzet-nardi.net (Anthony Darrouzet-Nardi)
Date: Wed, 2 Jun 2004 13:26:48 -0700
Subject: [R] Using postscript() in a script that is source()ed
Message-ID: <p0521062dbce3df17e7c5@[4.243.164.183]>

I have a script that draws 9 graphics which I would like to run by 
source()ing the file. In the script I make objects that contain each 
of the graphics (all lattice objects or functions that contain 
lattice functions). Then at the end of the script, I have a section 
which prints them to separate eps files like this:

postscript(file = "dryoutcover.eps", height = 5, width = 7,
	horizontal = FALSE, onefile = FALSE, paper = "special")
dryoutcover.plot # this is an object created by xyplot and it's one 
of the ones that doesn't print.
dev.off()

When I run the script using source(), only a couple of the graphics 
are drawn in the eps files. For the rest of them, there are blank eps 
files.

The script works properly (the 9 eps files all have graphics in them) 
when I cut and paste the contents of it into R. It also works 
properly if I use source(..., verbose = T) giving output such as:


>>>>  eval(expression_nr. 82 )
		 =================

>  postscript(file = "dryoutcover.eps", height = 5, width = 7,
     horizontal = FALSE, onefile = FALSE, paper = "special")
curr.fun: symbol postscript
  .. after 'expression(postscript(file = "dryoutcover.eps", height = 
5, width = 7, ''    horizontal = FALSE, onefile = FALSE, paper = 
"special"))'

>>>>  eval(expression_nr. 83 )
		 =================

>  dryoutcover.plot
  .. after 'expression(dryoutcover.plot)'

>>>>  eval(expression_nr. 84 )
		 =================

>  dev.off()
curr.fun: symbol dev.off
quartz
      2
  .. after 'expression(dev.off())'


How can I get the non-verbose source() of my script to work?

Anthony

R 1.9.0
Mac OS 10.3.3



From maechler at stat.math.ethz.ch  Wed Jun  2 22:30:06 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 2 Jun 2004 22:30:06 +0200
Subject: [R] swapping with data.frame
In-Reply-To: <200406012003030037.02006DAD@mail.math.fu-berlin.de>
References: <Pine.GSO.4.44.0406011030060.2980-100000@gere.odin.pdx.edu>
	<200406012003030037.02006DAD@mail.math.fu-berlin.de>
Message-ID: <16574.14542.86321.232142@gargle.gargle.HOWL>

>>>>> "Eryk" == Eryk Wolski <wolski at molgen.mpg.de>
>>>>>     on Tue, 01 Jun 2004 20:03:03 +0200 writes:

    Eryk> Hi!  U can use.

    Eryk> rbind(t1,t2,t3)

yes, but not for getting a data frame!
nor does give
    as.data.frame( rbind(t1,t2,t3) )
give the correct result {make use of  str(.)  to see this quickly}

Correct (though not "extremely simple") is

     do.call("rbind", lapply(list(t1,t2,t3), as.data.frame))


> str(do.call("rbind", lapply(list(t1,t2,t3), as.data.frame)))
`data.frame':	       3 obs. of  5 variables:
 $ fname   : Factor w/ 2 levels "animal1","animal2": 1 1 2
 $ testname: Factor w/ 2 levels "hyla","bufo": 1 2 1
 $ dspkr   : Factor w/ 2 levels "left","right": 1 1 2
 $ res1    : num  39.7 14.4 22.6
 $ res2    : num  15 56.1 11.8



    Eryk> On 6/1/2004 at 10:55 AM Randy Zelick wrote:

    >> Hi there,
    >> 
    >> I have some data which are convenient to enter as
    >> lists. For example:
    >> 
    >> t1<-list(fname="animal1",testname="hyla",dspkr="left",res1=39.7,res2=15.0)
    >> t2<-list(fname="animal1",testname="bufo",dspkr="left",res1=14.4,res2=56.1)
    >> t3<-list(fname="animal2",testname="hyla",dspkr="right",res1=22.6,res2=11.8)
    >> 
    >> I would like to generate a dataframe, but *not* the way
    >> this approach works...
    >> 
    >> fdf<-data.frame(t1,t2,t3)
    >> 
    >> fdf
    >> 
    >> fname testname dspkr res1 res2 fname testname dspkr res1
    >> res2 ...  1 animal1 hyla left 39.7 15 animal1 bufo left
    >> 14.4 56.1 ...
    >> 
    >> 
    >> Instead, what I would like is:
    >> 
    >> fname testname dspkr res1 res2
    >> 
    >> t1 animal1 hyla left 39.7 15.0 t2 animal1 bufo left 14.4
    >> 56.1 t3 animal2 hyla right 22.6 11.8
    >> 
    >> 
    >> or this would be fine too...
    >> 
    >> x fname testname dspkr res1 res2
    >> 
    >> 1 t1 animal1 hyla left 39.7 15.0 2 t2 animal1 bufo left
    >> 14.4 56.1 3 t3 animal2 hyla right 22.6 11.8
    >> 
    >> Is there a practical (hopefully simple) way to do this?
    >> 
    >> Thanks,
    >> 
    >> =Randy=



From maechler at stat.math.ethz.ch  Wed Jun  2 22:39:06 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 2 Jun 2004 22:39:06 +0200
Subject: [R] How do I prevent google search to post my questions asked
	here??
In-Reply-To: <BAY1-F68g9s0lOAOZId0003deb7@hotmail.com>
References: <BAY1-F68g9s0lOAOZId0003deb7@hotmail.com>
Message-ID: <16574.15082.681601.75508@gargle.gargle.HOWL>

>>>>> "john" == john miller <john_miller28 at hotmail.com>
>>>>>     on Wed, 02 Jun 2004 18:55:01 +0000 writes:

(everything in the subject):

    john> How do I prevent google search to post my questions
    john> asked here?? 

you don't: 
R-help is famous and celebrity can't be gotten rid off ;-)

Regards, Martin



From wolski at molgen.mpg.de  Wed Jun  2 22:41:39 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Wed, 02 Jun 2004 22:41:39 +0200
Subject: [R] swapping with data.frame
In-Reply-To: <16574.14542.86321.232142@gargle.gargle.HOWL>
References: <Pine.GSO.4.44.0406011030060.2980-100000@gere.odin.pdx.edu>
	<200406012003030037.02006DAD@mail.math.fu-berlin.de>
	<16574.14542.86321.232142@gargle.gargle.HOWL>
Message-ID: <200406022241390258.02BF7CFB@mail.math.fu-berlin.de>

Hi!

True. Randy Zelick wrote me today that it is not working. 
I asked him to send his observation to the list.

I bymyself was trying today to find a solution but its a subotimal one in comparsion with yours.

Sorry for the wrong solution

and
Thanks.


Eryk

*********** REPLY SEPARATOR  ***********

On 6/2/2004 at 10:30 PM Martin Maechler wrote:

>>>>>>>> "Eryk" == Eryk Wolski <wolski at molgen.mpg.de>
>>>>>>>>     on Tue, 01 Jun 2004 20:03:03 +0200 writes:
>>>
>>>    Eryk> Hi!  U can use.
>>>
>>>    Eryk> rbind(t1,t2,t3)
>>>
>>>yes, but not for getting a data frame!
>>>nor does give
>>>    as.data.frame( rbind(t1,t2,t3) )
>>>give the correct result {make use of  str(.)  to see this quickly}
>>>
>>>Correct (though not "extremely simple") is
>>>
>>>     do.call("rbind", lapply(list(t1,t2,t3), as.data.frame))
>>>
>>>
>>>> str(do.call("rbind", lapply(list(t1,t2,t3), as.data.frame)))
>>>`data.frame':	       3 obs. of  5 variables:
>>> $ fname   : Factor w/ 2 levels "animal1","animal2": 1 1 2
>>> $ testname: Factor w/ 2 levels "hyla","bufo": 1 2 1
>>> $ dspkr   : Factor w/ 2 levels "left","right": 1 1 2
>>> $ res1    : num  39.7 14.4 22.6
>>> $ res2    : num  15 56.1 11.8
>>>
>>>
>>>
>>>    Eryk> On 6/1/2004 at 10:55 AM Randy Zelick wrote:
>>>
>>>    >> Hi there,
>>>    >> 
>>>    >> I have some data which are convenient to enter as
>>>    >> lists. For example:
>>>    >> 
>>>    >>
>>>t1<-list(fname="animal1",testname="hyla",dspkr="left",res1=39.7,res2=15.0)
>>>    >>
>>>t2<-list(fname="animal1",testname="bufo",dspkr="left",res1=14.4,res2=56.1)
>>>    >>
>>>t3<-list(fname="animal2",testname="hyla",dspkr="right",res1=22.6,res2=11.8)
>>>    >> 
>>>    >> I would like to generate a dataframe, but *not* the way
>>>    >> this approach works...
>>>    >> 
>>>    >> fdf<-data.frame(t1,t2,t3)
>>>    >> 
>>>    >> fdf
>>>    >> 
>>>    >> fname testname dspkr res1 res2 fname testname dspkr res1
>>>    >> res2 ...  1 animal1 hyla left 39.7 15 animal1 bufo left
>>>    >> 14.4 56.1 ...
>>>    >> 
>>>    >> 
>>>    >> Instead, what I would like is:
>>>    >> 
>>>    >> fname testname dspkr res1 res2
>>>    >> 
>>>    >> t1 animal1 hyla left 39.7 15.0 t2 animal1 bufo left 14.4
>>>    >> 56.1 t3 animal2 hyla right 22.6 11.8
>>>    >> 
>>>    >> 
>>>    >> or this would be fine too...
>>>    >> 
>>>    >> x fname testname dspkr res1 res2
>>>    >> 
>>>    >> 1 t1 animal1 hyla left 39.7 15.0 2 t2 animal1 bufo left
>>>    >> 14.4 56.1 3 t3 animal2 hyla right 22.6 11.8
>>>    >> 
>>>    >> Is there a practical (hopefully simple) way to do this?
>>>    >> 
>>>    >> Thanks,
>>>    >> 
>>>    >> =Randy=
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



Dipl. bio-chem. Eryk Witold Wolski    @    MPI-Moleculare Genetic   
Ihnestrasse 63-73 14195 Berlin       'v'    
tel: 0049-30-83875219               /   \    
mail: wolski at molgen.mpg.de        ---W-W----    http://www.molgen.mpg.de/~wolski



From andy_liaw at merck.com  Wed Jun  2 20:56:06 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 2 Jun 2004 14:56:06 -0400
Subject: [R] How do i draw a step function using R?
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7E0F@usrymx25.merck.com>

I believe the posting guide suggests that you not ask the question in the
subject...

plot(..., type="s")

or

?stepfun

Andy

> From: john miller



From armin at xss.de  Wed Jun  2 23:18:26 2004
From: armin at xss.de (Armin Roehrl)
Date: Wed, 02 Jun 2004 23:18:26 +0200
Subject: [R] Distributed computing with R
In-Reply-To: <026d01c448df$e9e76760$17470843@MITRE.ORG>
References: <026d01c448df$e9e76760$17470843@MITRE.ORG>
Message-ID: <40BE4422.3090005@xss.de>

If you do some programming, you might want to look at MPI.
R-extensions for MPI exist  (RMPI).

It all depends a lot on what kind of usage you envisage of your cluster.
Open-PBS is also a good batch system. Maybe you also want to
look at Mosix, which is a modified linux system.

Depending on what your ultimate computing ressources are,
maybe also look at IBM's Globus toolkit.

Parallel programming is fun. The world is inherently parallel!
Ciao,
    -Armin.

----------------------------------------
Armin Roehrl, http://www.approximity.com
We manage risk



From rossini at blindglobe.net  Wed Jun  2 23:25:28 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Wed, 02 Jun 2004 14:25:28 -0700
Subject: [R] Distributed computing with R
In-Reply-To: <40BE4422.3090005@xss.de> (Armin Roehrl's message of "Wed, 02
	Jun 2004 23:18:26 +0200")
References: <026d01c448df$e9e76760$17470843@MITRE.ORG>
	<40BE4422.3090005@xss.de>
Message-ID: <85ekoxr6t3.fsf@servant.blindglobe.net>


Also see SNOW (which simplifies parallel programming, sits on top of
rpvm, Rmpi, or a socket-based system).

Depends on whether you want parallelism on the:

1. User-level -- the libraries such as PVM, LAM-MPI, etc will help,
                 and there are various packages which provide an API
                 to those.

2. System-level -- then Condor, Sun Grid Engine / Maui scheduler, and
                   similar queueing/batching/allocation daemons will
                   help (computational grid software is usually a
                   generalization of this which adds authentication
                   and resource allocation).

3. Kernel-level -- then OpenMOSIX, BPROC, etc will help.

They are mostly orthogonal.  Mostly... :-).

best,
-tony



Armin Roehrl <armin at xss.de> writes:

> If you do some programming, you might want to look at MPI.
> R-extensions for MPI exist  (RMPI).
>
> It all depends a lot on what kind of usage you envisage of your cluster.
> Open-PBS is also a good batch system. Maybe you also want to
> look at Mosix, which is a modified linux system.
>
> Depending on what your ultimate computing ressources are,
> maybe also look at IBM's Globus toolkit.
>
> Parallel programming is fun. The world is inherently parallel!
> Ciao,
>     -Armin.
>
> ----------------------------------------
> Armin Roehrl, http://www.approximity.com
> We manage risk
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From pem at theriver.com  Wed Jun  2 23:51:52 2004
From: pem at theriver.com (Patrick E. McKnight)
Date: Wed, 2 Jun 2004 14:51:52 -0700
Subject: [R] Missing data diagnostics and apology
Message-ID: <20040602145152.3e1ccfdb@localhost>

Greetings again,

Seems I've touched a nerve with my previous post and after a few kind folks alerted me to a better distribution method I chose to put the code here:

http://egad.psych.arizona.edu/mddiag.final.R.gz

Any tips, criticisms, or additions are welcomed.  Thanks for your patience.

-- 
Cheers, 

Patrick
_______________________________________________________________
Patrick E. McKnight, Ph.D.
EGAD - Evaluation Group for Analysis of Data
Department of Psychology
University of Arizona
Tucson, AZ  85721
520-621-5463 (W)
240-332-6367 (Fax)
pem at u.arizona.edu



From jasont at indigoindustrial.co.nz  Thu Jun  3 00:21:14 2004
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Thu, 3 Jun 2004 10:21:14 +1200
Subject: [R] zipping a new package
In-Reply-To: <BAY12-F100Jzt47kBeq0003aa29@hotmail.com>
References: <BAY12-F100Jzt47kBeq0003aa29@hotmail.com>
Message-ID: <20040602222114.GA4542@kryten.akl.indigoindustrial.co.nz>

On Sun, May 30, 2004 at 03:19:24PM -0500, Laura Holt wrote:
> Dear R People:
> 
> I have finally created a little R package.
> 
> Do I need to do anything special to create a zip file for that package, or 
> just use Winzip, please?
> 
> thanks so much
> 
> R Windows Version 1.9.0
> 

Re-read "Writing R Extensions" for a full description, particularly
"Checking and building packages".  In your case, from the directory 
just above the top-level directory of your package.

Rcmd BUILD --binary yourpackagename

This takes care of a lot of things related to the build; zipping is
only part of it.

Cheers

Jason
-- 
Indigo Industrial Controls Ltd.
http://www.indigoindustrial.co.nz
64-21-343-545
jasont at indigoindustrial.co.nz



From tlumley at u.washington.edu  Thu Jun  3 00:50:10 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 2 Jun 2004 15:50:10 -0700 (PDT)
Subject: [R] Using postscript() in a script that is source()ed
In-Reply-To: <p0521062dbce3df17e7c5@[4.243.164.183]>
References: <p0521062dbce3df17e7c5@[4.243.164.183]>
Message-ID: <Pine.A41.4.58.0406021548430.172398@homer11.u.washington.edu>

On Wed, 2 Jun 2004, Anthony Darrouzet-Nardi wrote:

> I have a script that draws 9 graphics which I would like to run by
> source()ing the file. In the script I make objects that contain each
> of the graphics (all lattice objects or functions that contain
> lattice functions). Then at the end of the script, I have a section
> which prints them to separate eps files like this:

There isn't any print() command here, so the object isn't printed.


> postscript(file = "dryoutcover.eps", height = 5, width = 7,
> 	horizontal = FALSE, onefile = FALSE, paper = "special")
> dryoutcover.plot # this is an object created by xyplot and it's one
> of the ones that doesn't print.
> dev.off()
>
> When I run the script using source(), only a couple of the graphics
> are drawn in the eps files. For the rest of them, there are blank eps
> files.
>

<SNIP>
> How can I get the non-verbose source() of my script to work?

Use print().

	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From christianlederer at t-online.de  Thu Jun  3 06:20:03 2004
From: christianlederer at t-online.de (Christian Lederer)
Date: Thu, 03 Jun 2004 06:20:03 +0200
Subject: [R] Sloppy argument checking for named arguments
Message-ID: <40BEA6F3.9090604@t-online.de>


Dear R Gurus,

i recently noticed that R does sloppy argument checking for named
arguments, if the argument contains a dot.
Example:

 > f <- function(foo.bar=0) { print(foo.bar) }
 > f(foo=1)
[1] 1

I guess, this should be considered as a bug.
Anyway, the consequence is that bugs caused by typing errors
of this kind may are *very* hard to discover in complex progams.

Christian



From rpeng at jhsph.edu  Thu Jun  3 04:56:25 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Wed, 02 Jun 2004 22:56:25 -0400
Subject: [R] Distributed computing with R
In-Reply-To: <000b01c448f6$f72665e0$7fe40992@BIOINFO>
References: <000b01c448f6$f72665e0$7fe40992@BIOINFO>
Message-ID: <40BE9359.40804@jhsph.edu>

I would suggest installing PVM or LAM-MPI and using the R 
packages `snow' and `rpvm' (or `Rmpi').  I've found the `snow' 
package very simple to use and useful for quick and dirty 
solutions.  I've used `snow' with an openMosix setup and on a 
simple cluster of workstations without any scheduler.  openMosix 
is nice because you don't have to worry about which process goes 
where but that's not to say it doesn't have its own difficulties.
Overall, my experience with parallel computing in R has been a 
little clunky but that's mostly because the problems I work on 
don't benefit much from such a setup.

-roger

Saroj Mohapatra wrote:
> Dear all,
> 
> We have started using R for data analysis since a few months and find it
> useful. We are planning to acquire a high-end dedicated system for
> microarray data analysis and thinking of a distributed environment. I
> would appreciate if some one could send some pointers regarding how to
> choose a proper hardware configuration, software (R or other software,
> esp. MATLAB), issues on setting up the cluster, etc. Has anyone here
> some experience of R on a cluster? Does it provide significant benefits
> as regards processing time? Is setting up the cluster more difficult
> than using R on it?
> 
> Thanks.
> 
> Saroj K Mohapatra, MD
> Research Associate
> Tainsky Lab
> Karmanos Cancer Institute
> Wayne State University School of Medicine
> 110 E. Warren, Room 311
> Detroit MI 48201
> 313-833-0715 x2424
> saroj at wayne.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From matthew_wiener at merck.com  Thu Jun  3 05:20:51 2004
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Wed, 2 Jun 2004 23:20:51 -0400
Subject: [R] Sloppy argument checking for named arguments
Message-ID: <45AAE6FD142DCB43A38C00A11FF5DF3EE36338@uswsmx03.merck.com>

Christian --

This is not a bug, but a feature, and the dot is not the issue here.  R uses
partial argument matching on function arguments.  

  So:

> f <- function(foobar = 0){print(foobar)}
> f
function(foobar = 0){print(foobar)}
>
> f(fo=1)
[1] 1
> f(foo=1)
[1] 1
> f(foob=1)
[1] 1
> f(fooba=1)
[1] 1
> f(foon=1)
Error in f(foon = 1) : unused argument(s) (foon ...)
>
> f(foobar2=1)
Error in f(foobar2 = 1) : unused argument(s) (foobar2 ...)

This is mentioned in Venables & Ripley, MASS 4th edition on p. 55. I can't
find it right now in the "Introduction to R" that is linked to on the main
html help index page.   However, it is discussed in the R Language
Definition you can get to from the same page.

It seems like there must be an easier path to the information that I haven't
thought of.

Hope this helps,

Matt Wiener

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of
christianlederer at t-online.de
Sent: Thursday, June 03, 2004 12:20 AM
To: r-help at stat.math.ethz.ch
Subject: [R] Sloppy argument checking for named arguments



Dear R Gurus,

i recently noticed that R does sloppy argument checking for named
arguments, if the argument contains a dot.
Example:

 > f <- function(foo.bar=0) { print(foo.bar) }
 > f(foo=1)
[1] 1

I guess, this should be considered as a bug.
Anyway, the consequence is that bugs caused by typing errors
of this kind may are *very* hard to discover in complex progams.

Christian

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From fernando.pineda at jhu.edu  Thu Jun  3 05:22:22 2004
From: fernando.pineda at jhu.edu (Fernando Pineda)
Date: Wed, 2 Jun 2004 23:22:22 -0400
Subject: [R] returning strings to R from C functions
Message-ID: <a06020411bce448569d11@[10.56.44.89]>

I'm using .C() and .External() and have no problems sending integers, 
reals or strings from R to C. Nor do I have problems sending integers 
or reals back from C to R. But I'm pulling my hair out trying to set 
a string value in a C function and then sending it back from C to to 
R. I've searched the usual sources and tried various casts, macros 
and allocation schemes, but I'm failing miserably. Could someone 
please put me out of my misery and point me to a working example?

Thanks,

-- Fernando
-- 
Fernando Pineda
Associate Professor
Dept. of Molecular Microbiology & Immunology
Johns Hopkins Bloomberg School of Public Health
Room E5146
615 N. Wolfe St., Baltimore, MD 21205-2179
fernando.pineda at jhu.edu
443-287-3673 (office)
410-955-0105 (fax)
http://www.pinedalab.jhsph.edu



From ggrothendieck at myway.com  Thu Jun  3 06:15:41 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 3 Jun 2004 04:15:41 +0000 (UTC)
Subject: [R] Sloppy argument checking for named arguments
References: <40BEA6F3.9090604@t-online.de>
Message-ID: <loom.20040603T054222-704@post.gmane.org>


It has already been pointed out that partial matching is a feature.

However, if the function uses ... then it will only match arguments 
exactly so if you want to write a function in which full matching only 
is accepted you can do it by putting in a dummy first argument of 
... and then issuing an error messaage if ... matches anything:

f <- function( ..., foo.bar = 0 )  { 
          stopifnot( length(list(...)) == 0 )
          print( foo.bar )
}

 
R> f( foo.bar = 1 )
[1] 1
R> f( foo = 1 )
Error: length(list(...)) == 0 is not TRUE


Christian Lederer <christianlederer <at> t-online.de> writes:

: 
: Dear R Gurus,
: 
: i recently noticed that R does sloppy argument checking for named
: arguments, if the argument contains a dot.
: Example:
: 
:  > f <- function(foo.bar=0) { print(foo.bar) }
:  > f(foo=1)
: [1] 1
: 
: I guess, this should be considered as a bug.
: Anyway, the consequence is that bugs caused by typing errors
: of this kind may are *very* hard to discover in complex progams.
: 
: Christian
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://www.stat.math.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
: 
:



From wolski at molgen.mpg.de  Thu Jun  3 08:32:12 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Thu, 03 Jun 2004 08:32:12 +0200
Subject: [R] Sloppy argument checking for named arguments
In-Reply-To: <loom.20040603T054222-704@post.gmane.org>
References: <40BEA6F3.9090604@t-online.de>
	<loom.20040603T054222-704@post.gmane.org>
Message-ID: <200406030832120736.04DC1E06@mail.math.fu-berlin.de>

Hi Gabor, Christian, R users!

I asked several days ago how to provide a signature in S4 for an existing function of the type which gabor proposes.
There are already some functions in the base package which are using the syntax sugested by you.

e.g:

rbind <-
function (..., deparse.level = 1) 
...

But I have not found any way to provide a signature (to overload) such functions in S4.

setMethod("rbind",signature(...="myobj"

simple does not work.


Therefore, when I am programming new functions, I am try to think about, how to do not clutter the name space. It means how to choose argument names, so that other people can use the same functions names programming they own classes. That means that the parameter names, especially the name of the first parameters should be as 
general as possible e.g 'x' (as for most graphic functions in the base package)
or 'object' as eg. for the summary function in base package.

This also means to me not to use argument names with '.' like foo.bar!

S3 does argument checking  so if you provide a function f(...) it means that all other packages that like to work with you package and want to implement a function f for they own classes have to provide to it the same arguments (e.g. f.myclass<-funtion(...)) .  But even worser with S4 ... see above.


Would be nice if someone proofs that I am false. 


Sincerely 

Eryk.





*********** REPLY SEPARATOR  ***********

On 6/3/2004 at 4:15 AM Gabor Grothendieck wrote:

>>>It has already been pointed out that partial matching is a feature.
>>>
>>>However, if the function uses ... then it will only match arguments 
>>>exactly so if you want to write a function in which full matching only 
>>>is accepted you can do it by putting in a dummy first argument of 
>>>... and then issuing an error messaage if ... matches anything:
>>>
>>>f <- function( ..., foo.bar = 0 )  { 
>>>          stopifnot( length(list(...)) == 0 )
>>>          print( foo.bar )
>>>}
>>>
>>> 
>>>R> f( foo.bar = 1 )
>>>[1] 1
>>>R> f( foo = 1 )
>>>Error: length(list(...)) == 0 is not TRUE
>>>
>>>
>>>Christian Lederer <christianlederer <at> t-online.de> writes:
>>>
>>>: 
>>>: Dear R Gurus,
>>>: 
>>>: i recently noticed that R does sloppy argument checking for named
>>>: arguments, if the argument contains a dot.
>>>: Example:
>>>: 
>>>:  > f <- function(foo.bar=0) { print(foo.bar) }
>>>:  > f(foo=1)
>>>: [1] 1
>>>: 
>>>: I guess, this should be considered as a bug.
>>>: Anyway, the consequence is that bugs caused by typing errors
>>>: of this kind may are *very* hard to discover in complex progams.
>>>: 
>>>: Christian
>>>: 
>>>: ______________________________________________
>>>: R-help <at> stat.math.ethz.ch mailing list
>>>: https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>: PLEASE do read the posting guide!
>>>http://www.R-project.org/posting-guide.html
>>>: 
>>>:
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



Dipl. bio-chem. Eryk Witold Wolski    @    MPI-Moleculare Genetic   
Ihnestrasse 63-73 14195 Berlin       'v'    
tel: 0049-30-83875219               /   \    
mail: wolski at molgen.mpg.de        ---W-W----    http://www.molgen.mpg.de/~wolski



From wolski at molgen.mpg.de  Thu Jun  3 08:57:23 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Thu, 03 Jun 2004 08:57:23 +0200
Subject: [R] returning strings to R from C functions
In-Reply-To: <a06020411bce448569d11@[10.56.44.89]>
References: <a06020411bce448569d11@[10.56.44.89]>
Message-ID: <200406030857230608.04F32BDE@mail.math.fu-berlin.de>

Hallo!

There is a package called BioSeq1 at bioconductor. It Is using the .C interface and was doing a lot of string manipulation. Should have tons of examples. 

The simplest way to do it is. 
You my alloc fist the char in R.

mychar<-paste(rep(" ",1000),collapse="")


tmp<-.C(
as.character(mychar)
nchar(mychar)
)

You can write into the char on the C side what you like. The char will be in tmp[[1]]

 
Sincerely Eryk





*********** REPLY SEPARATOR  ***********

On 6/2/2004 at 11:22 PM Fernando Pineda wrote:

>I'm using .C() and .External() and have no problems sending integers, 
>reals or strings from R to C. Nor do I have problems sending integers 
>or reals back from C to R. But I'm pulling my hair out trying to set 
>a string value in a C function and then sending it back from C to to 
>R. I've searched the usual sources and tried various casts, macros 
>and allocation schemes, but I'm failing miserably. Could someone 
>please put me out of my misery and point me to a working example?
>
>Thanks,
>
>-- Fernando
>-- 
>Fernando Pineda
>Associate Professor
>Dept. of Molecular Microbiology & Immunology
>Johns Hopkins Bloomberg School of Public Health
>Room E5146
>615 N. Wolfe St., Baltimore, MD 21205-2179
>fernando.pineda at jhu.edu
>443-287-3673 (office)
>410-955-0105 (fax)
>http://www.pinedalab.jhsph.edu
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



Dipl. bio-chem. Eryk Witold Wolski    @    MPI-Moleculare Genetic   
Ihnestrasse 63-73 14195 Berlin       'v'    
tel: 0049-30-83875219               /   \    
mail: wolski at molgen.mpg.de        ---W-W----    http://www.molgen.mpg.de/~wolski



From robert.king at newcastle.edu.au  Thu Jun  3 08:58:16 2004
From: robert.king at newcastle.edu.au (Robert King)
Date: Thu, 3 Jun 2004 16:58:16 +1000 (EST)
Subject: [R] Problem with mgcv PACKAGES file format?
Message-ID: <Pine.LNX.4.58.0406031646001.28371@tolstoy.newcastle.edu.au>

Hello All,

I'm getting this error (Version: 1.9.0-1 on a debian system)

> update.packages("mgcv")
trying URL `ftp://mirror.aarnet.edu.au/pub/cran/src/contrib/PACKAGES'
ftp data connection made, file length 169516 bytes
opened URL
.......... .......... .......... .......... ..........
.......... .......... .......... .......... ..........
.......... .......... .......... .......... ..........
.......... .....
downloaded 165Kb

Error in "colnames<-"(`*tmp*`, value = c("Package", "LibPath", pkgFlds)) :
        attempt to set colnames on object with less than two dimensions

The relevant section of the package file is:

Package: mgcv
Version: 1.0-8
Author: Simon Wood
Maintainer: Simon Wood
Title: Multiple smoothing parameter estimation and GAMs by GCV
Description: Routines for GAMs and other generalized ridge regression
             with multiple smoothing parameter selection by GCV or UBRE.
             Also GAMMs by REML or PQL. Includes a gam() function.
Priority: recommended
Depends: R (>= 1.9.0), nlme (>= 3.1-40), MASS (>= 7.1-11)
License: GPL version 2 or later
Packaged: Wed May 26 11:17:05 2004; simon

Is the error a result of something I can't see in the format of this?  I'm
guessing it must be because its occuring before the download attempts, so
R won't have seen the actual package file?


----
Robert King, Statistics, School of Mathematical & Physical Sciences,
University of Newcastle, Australia
Room V133  ph +61 2 4921 5548
Robert.King at newcastle.edu.au   http://maths.newcastle.edu.au/~rking/

"It's easy to lie with statistics.  It's even easier to lie without them."
	-- Frederick Mosteller



From maechler at stat.math.ethz.ch  Thu Jun  3 09:11:36 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 3 Jun 2004 09:11:36 +0200
Subject: [R] returning strings to R from C functions
In-Reply-To: <a06020411bce448569d11@[10.56.44.89]>
References: <a06020411bce448569d11@[10.56.44.89]>
Message-ID: <16574.53032.965713.231724@gargle.gargle.HOWL>

>>>>> "Fernando" == Fernando Pineda <fernando.pineda at jhu.edu>
>>>>>     on Wed, 2 Jun 2004 23:22:22 -0400 writes:

    Fernando> I'm using .C() and .External() and have no
    Fernando> problems sending integers, reals or strings from R
    Fernando> to C. Nor do I have problems sending integers or
    Fernando> reals back from C to R. But I'm pulling my hair
    Fernando> out trying to set a string value in a C function
    Fernando> and then sending it back from C to to R. I've
    Fernando> searched the usual sources and tried various
    Fernando> casts, macros and allocation schemes, but I'm
    Fernando> failing miserably. Could someone please put me out
    Fernando> of my misery and point me to a working example?

You did read the section entitled
     "Interface functions `.C' and `.Fortran'"
in the  "Writing R Extensions" manual ?

In particular,

>>    The following table gives the mapping between the modes of R vectors
>> and the types of arguments to a C function or FORTRAN subroutine.
>> 
>>      R storage mode     C type             FORTRAN type
>>      `logical'          `int *'            `INTEGER'
>>      `integer'          `int *'            `INTEGER'
>>      `double'           `double *'         `DOUBLE PRECISION'
>>      `complex'          `Rcomplex *'       `DOUBLE COMPLEX'
>>      `character'        `char **'          `CHARACTER*255'

??

One working example is the R function  formatC()
with the C code in <Rsource>/src/appl/strsignif.c

Martin Maechler



From robert.king at newcastle.edu.au  Thu Jun  3 09:27:48 2004
From: robert.king at newcastle.edu.au (Robert King)
Date: Thu, 3 Jun 2004 17:27:48 +1000 (EST)
Subject: [R] Re: Problem with mgcv PACKAGES file format?
In-Reply-To: <Pine.LNX.4.58.0406031646001.28371@tolstoy.newcastle.edu.au>
References: <Pine.LNX.4.58.0406031646001.28371@tolstoy.newcastle.edu.au>
Message-ID: <Pine.LNX.4.58.0406031724190.28832@tolstoy.newcastle.edu.au>

Problem solved.

This is a bad interaction of packages installed from debian with the R
update.packages() system.

Moral of the story is don't try to update.packges() for any of the
packages installed as r-cran-package.name.here debian packages.

Robert.

On Thu, 3 Jun 2004, Robert King wrote:
> Hello All,
>
> I'm getting this error (Version: 1.9.0-1 on a debian system)
>
> > update.packages("mgcv")
> trying URL `ftp://mirror.aarnet.edu.au/pub/cran/src/contrib/PACKAGES'
> ftp data connection made, file length 169516 bytes
> opened URL
> .......... .......... .......... .......... ..........
> .......... .......... .......... .......... ..........
> .......... .......... .......... .......... ..........
> .......... .....
> downloaded 165Kb
>
> Error in "colnames<-"(`*tmp*`, value = c("Package", "LibPath", pkgFlds)) :
>         attempt to set colnames on object with less than two dimensions
>
> The relevant section of the package file is:
>
> Package: mgcv
> Version: 1.0-8
> Author: Simon Wood
> Maintainer: Simon Wood
> Title: Multiple smoothing parameter estimation and GAMs by GCV
> Description: Routines for GAMs and other generalized ridge regression
>              with multiple smoothing parameter selection by GCV or UBRE.
>              Also GAMMs by REML or PQL. Includes a gam() function.
> Priority: recommended
> Depends: R (>= 1.9.0), nlme (>= 3.1-40), MASS (>= 7.1-11)
> License: GPL version 2 or later
> Packaged: Wed May 26 11:17:05 2004; simon
>
> Is the error a result of something I can't see in the format of this?  I'm
> guessing it must be because its occuring before the download attempts, so
> R won't have seen the actual package file?
>
>
> ----
> Robert King, Statistics, School of Mathematical & Physical Sciences,
> University of Newcastle, Australia
> Room V133  ph +61 2 4921 5548
> Robert.King at newcastle.edu.au   http://maths.newcastle.edu.au/~rking/
>
> "It's easy to lie with statistics.  It's even easier to lie without them."
> 	-- Frederick Mosteller
>

----
Robert King, Statistics, School of Mathematical & Physical Sciences,
University of Newcastle, Australia
Room V133  ph +61 2 4921 5548
Robert.King at newcastle.edu.au   http://maths.newcastle.edu.au/~rking/

"It's easy to lie with statistics.  It's even easier to lie without them."
	-- Frederick Mosteller



From maechler at stat.math.ethz.ch  Thu Jun  3 09:36:36 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 3 Jun 2004 09:36:36 +0200
Subject: [R] bug in installed.packages {was "Problem... mgcv PACKAGES
	file.."}
In-Reply-To: <Pine.LNX.4.58.0406031646001.28371@tolstoy.newcastle.edu.au>
References: <Pine.LNX.4.58.0406031646001.28371@tolstoy.newcastle.edu.au>
Message-ID: <16574.54532.969349.143982@gargle.gargle.HOWL>

Hi Robert,

>>>>> "Robert" == Robert King <robert.king at newcastle.edu.au>
>>>>>     on Thu, 3 Jun 2004 16:58:16 +1000 (EST) writes:

    Robert> Hello All, I'm getting this error (Version: 1.9.0-1
    Robert> on a debian system)

    >> update.packages("mgcv")
    Robert> trying URL
    Robert> `ftp://mirror.aarnet.edu.au/pub/cran/src/contrib/PACKAGES'
    Robert> ftp data connection made, file length 169516 bytes
    Robert> opened URL
    Robert> .......... .......... .......... .......... ..........
    Robert> .......... .......... .......... .......... ..........
    Robert> .......... .......... .......... .......... ..........
    Robert> .......... .....  downloaded 165Kb

    Robert> Error in "colnames<-"(`*tmp*`, value = c("Package",
    Robert> "LibPath", pkgFlds)) : attempt to set colnames on
    Robert> object with less than two dimensions

I can confirm the problem (with R 1.9.1 alpha of a few days ago)
It's not related to "mgcv" at all.

For didactical purposes I show here how to find out more 
(i.e. how to start learning to debug):

> update.packages("sfsmisc")
trying URL `http://cran.ch.r-project.org/src/contrib/PACKAGES'
Content type `text/plain; charset=ISO-8859-1' length 169516 bytes
opened URL
.......... .......... .......... .......... ..........
.......... .......... .......... .......... ..........
.......... .......... .......... .......... ..........
.......... .....
downloaded 165Kb

Error in "colnames<-"(`*tmp*`, value = c("Package", "LibPath", pkgFlds)) : 
	attempt to set colnames on object with less than two dimensions

  [same bug --- now one crucial thing to learn : ]
		    --------------------------
> traceback()
5: stop("attempt to set colnames on object with less than two dimensions")
4: "colnames<-"(`*tmp*`, value = c("Package", "LibPath", pkgFlds))
3: installed.packages(lib.loc = lib.loc)
2: old.packages(lib.loc = lib.loc, contriburl = contriburl, method = method, 
       available = available)
1: update.packages("sfsmisc")

  [ok, it's inside  installed.packages() that the problem occurs;
   hence let's look inside there : ]

> debug(installed.packages)
> update.packages("sfsmisc")
trying URL `http://cran.ch.r-project.org/src/contrib/PACKAGES'
Content type `text/plain; charset=ISO-8859-1' length 169516 bytes
opened URL
.......... .......... .......... .......... ..........
.......... .......... .......... .......... ..........
.......... .......... .......... .......... ..........
.......... .....
downloaded 165Kb

debugging in: installed.packages(lib.loc = lib.loc)
debug: {
    if (is.null(lib.loc)) 
        lib.loc <- .libPaths()
    pkgFlds <- c("Version", "Priority", "Bundle", "Depends")
    if (!is.null(priority)) {
        if (!is.character(priority)) 
            stop("`priority' must be character or NULL")
        if (any(b <- priority %in% "high")) 
            priority <- c(priority[!b], "recommended", "base")
    }
    retval <- character()
    for (lib in lib.loc) {
        pkgs <- .packages(all.available = TRUE, lib.loc = lib)
        for (p in pkgs) {
            desc <- unlist(packageDescription(p, lib = lib, fields = pkgFlds))
            if (!is.null(priority)) 
                if (is.na(pmatch(desc["Priority"], priority))) 
                  next
            retval <- rbind(retval, c(p, lib, desc))
        }
    }
    if (!is.null(retval)) 
        colnames(retval) <- c("Package", "LibPath", pkgFlds)
    retval
}
Browse[1]> 
debug: if (is.null(lib.loc)) lib.loc <- .libPaths()
Browse[1]> 
debug: pkgFlds <- c("Version", "Priority", "Bundle", "Depends")
Browse[1]> 
debug: if (!is.null(priority)) {
    if (!is.character(priority)) 
        stop("`priority' must be character or NULL")
    if (any(b <- priority %in% "high")) 
        priority <- c(priority[!b], "recommended", "base")
}
Browse[1]> 
debug: retval <- character()
Browse[1]> 
debug: for (lib in lib.loc) {
    pkgs <- .packages(all.available = TRUE, lib.loc = lib)
    for (p in pkgs) {
        desc <- unlist(packageDescription(p, lib = lib, fields = pkgFlds))
        if (!is.null(priority)) 
            if (is.na(pmatch(desc["Priority"], priority))) 
                next
        retval <- rbind(retval, c(p, lib, desc))
    }
}
Browse[1]> 
debug: lib
Browse[1]> 
debug: pkgs <- .packages(all.available = TRUE, lib.loc = lib)
Browse[1]> 
debug: for (p in pkgs) {
    desc <- unlist(packageDescription(p, lib = lib, fields = pkgFlds))
    if (!is.null(priority)) 
        if (is.na(pmatch(desc["Priority"], priority))) 
            next
    retval <- rbind(retval, c(p, lib, desc))
}
Browse[1]> 
debug: if (!is.null(retval)) colnames(retval) <- c("Package", "LibPath", 
    pkgFlds)
Browse[1]> retval
character(0)
Browse[1]> 
Error in "colnames<-"(`*tmp*`, value = c("Package", "LibPath", pkgFlds)) : 
	attempt to set colnames on object with less than two dimensions
> 

----------------

If look at the last dozen lines the bug is "obvious"
["Good Programming rule":   
       !is.null(.) is NOT the same as length(.) > 0
]

What I don't understand is that it took so long before the
problem was seen by someone...

It also seems to me that the value of lib.loc (namely "sfsmisc")
at that moment is wrong as well - which would point to a problem
in the update.packages() function...  

Well, anyway, more details belong to R-bugs / R-devel,
not here (R-help).

Martin Maechler



From maechler at stat.math.ethz.ch  Thu Jun  3 09:44:31 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 3 Jun 2004 09:44:31 +0200
Subject: [R] bug in installed.packages...
In-Reply-To: <16574.54532.969349.143982@gargle.gargle.HOWL>
References: <Pine.LNX.4.58.0406031646001.28371@tolstoy.newcastle.edu.au>
	<16574.54532.969349.143982@gargle.gargle.HOWL>
Message-ID: <16574.55007.372848.198335@gargle.gargle.HOWL>

>>>>> "MM" == Martin Maechler <maechler at stat.math.ethz.ch>
>>>>>     on Thu, 3 Jun 2004 09:36:36 +0200 writes:
 
    [things about a bug in installed.packages() ]

	  <.............>

    MM> It also seems to me that the value of lib.loc (namely
    MM> "sfsmisc") at that moment is wrong as well - which would
    MM> point to a problem in the update.packages() function...

Looking at the help page for update.packages()  [blush !! ;-)]
pretty quickly revealed the reason why the bug in 
   installed.packages()
has very rarely triggered :

Robert called update.packages("mgcv")
which is a so called "user error",
since the first argument of update.packages() {and
installed.packages() as well} is `lib.loc' and not 'pkgs'.

In other words, you can't give a package name as argument to these,
but arguably you should get a better error message when you do.

Martin



From sekemp at glam.ac.uk  Thu Jun  3 10:22:17 2004
From: sekemp at glam.ac.uk (Samuel Kemp (Comp))
Date: Thu, 03 Jun 2004 09:22:17 +0100
Subject: [R] Thanks
Message-ID: <40BEDFB9.10404@glam.ac.uk>

Hi,

Thank you all for your helpful comments on the available R/S programming 
books.

Kind regards,

Sam.



From Nicolas.Stransky at curie.fr  Thu Jun  3 12:35:57 2004
From: Nicolas.Stransky at curie.fr (Nicolas STRANSKY)
Date: Thu, 03 Jun 2004 12:35:57 +0200
Subject: [R] Problem with par("usr")
Message-ID: <40BEFF0D.3080907@curie.fr>

Hi,

I'm trying to use the "usr" argument but I see no effect of this option 
on my plots. For example:

 > par(usr=c(0,4,0,4))
 > plot(1,1)

This plots one point fine, but the coordinates of the plotting region 
are not those that I specified using par()...
I can check this with
 > par("usr")
[1] 0.568 1.432 0.568 1.432

What can I change in order to have this "usr" argument working ?

The aim of all this is to be able to superpose two plots where xlim and 
ylim arguments are not sufficient for having the same scalings...

Thanks for your help.
-- 
Nicolas STRANSKY
??quipe Oncologie Mol??culaire
Institut Curie - UMR 144 - CNRS                 Tel : +33 1 42 34 63 40
26, rue d'Ulm - 75248 Paris Cedex 5 - FRANCE    Fax : +33 1 42 34 63 49



From cdeclercq at nordnet.fr  Thu Jun  3 12:58:47 2004
From: cdeclercq at nordnet.fr (Christophe Declercq)
Date: Thu, 3 Jun 2004 12:58:47 +0200
Subject: [R] Problem with par("usr")
In-Reply-To: <40BEFF0D.3080907@curie.fr>
Message-ID: <MJELLLFFFCNHMHOOLCMBEECECHAA.cdeclercq@nordnet.fr>

Bonjour, Nicolas

par('usr') is read-only (see '?par')

That makes sense: user coordinates come from what you have plotted.

Would you show us an example of what exactly you want to do?

Christophe
--
Christophe Declercq, MD
Observatoire r??gional de la sant?? Nord-Pas-de-Calais
13, rue Faidherbe
F-59046 LILLE C??dex
Phone 33 3 20 15 49 24
Fax 33 3 20 55 92 30
E-mail c.declercq at orsnpdc.org



> -----Message d'origine-----
> De : r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]De la part de Nicolas STRANSKY
> Envoy?? : jeudi 3 juin 2004 11:36
> ?? : r-help at stat.math.ethz.ch
> Objet : [R] Problem with par("usr")
>
>
> Hi,
>
> I'm trying to use the "usr" argument but I see no effect of this option
> on my plots. For example:
>
>  > par(usr=c(0,4,0,4))
>  > plot(1,1)
>
> This plots one point fine, but the coordinates of the plotting region
> are not those that I specified using par()...
> I can check this with
>  > par("usr")
> [1] 0.568 1.432 0.568 1.432
>
> What can I change in order to have this "usr" argument working ?
>
> The aim of all this is to be able to superpose two plots where xlim and
> ylim arguments are not sufficient for having the same scalings...
>
> Thanks for your help.
> --
> Nicolas STRANSKY
> ??quipe Oncologie Mol??culaire
> Institut Curie - UMR 144 - CNRS                 Tel : +33 1 42 34 63 40
> 26, rue d'Ulm - 75248 Paris Cedex 5 - FRANCE    Fax : +33 1 42 34 63 49
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
>



From jeaneid at chass.utoronto.ca  Thu Jun  3 13:11:56 2004
From: jeaneid at chass.utoronto.ca (Jean Eid)
Date: Thu, 3 Jun 2004 07:11:56 -0400
Subject: [R] cameraa rotation graphics 
Message-ID: <Pine.SGI.4.40.0406030708420.20321693-100000@origin.chass.utoronto.ca>

Dear all,
Is there a camera rotation for 3d graphics in R. I have seen it in a
conference one time and thought it is pretty neat. the presenter was able
to rotate the 3d graph with dragging the mouse up down left right. If not
in R is there something that is open source that does this.



Thank you

Jean Eid



From k.wang at auckland.ac.nz  Thu Jun  3 13:11:08 2004
From: k.wang at auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Thu, 3 Jun 2004 23:11:08 +1200
Subject: [R] cameraa rotation graphics 
In-Reply-To: <Pine.SGI.4.40.0406030708420.20321693-100000@origin.chass.utoronto.ca>
Message-ID: <20040603111112.FSHM12806.web4-rme.xtra.co.nz@kevinlpt>

Hi,

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch

> Dear all,
> Is there a camera rotation for 3d graphics in R. I have seen it in a
> conference one time and thought it is pretty neat. the
> presenter was able
> to rotate the 3d graph with dragging the mouse up down left
> right. If not
> in R is there something that is open source that does this.

Do you mean Daniel Adler's rgl
(http://wsopuppenkiste.wiso.uni-goettingen.de/~dadler/) or the
Ggobi/Rggobi http://www.ggobi.org/?

Kevin

--------------------------------------------
Ko-Kang Kevin Wang, MSc(Hon)
SLC Stats Workshops Co-ordinator
The University of Auckland
New Zealand



From wolski at molgen.mpg.de  Thu Jun  3 13:30:58 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Thu, 03 Jun 2004 13:30:58 +0200
Subject: [R] use of split.screen() with postscript device
In-Reply-To: <200406021007.25768.swhite@aegis-semi.com>
References: <200406021007.25768.swhite@aegis-semi.com>
Message-ID: <200406031330580619.0018D499@mail.math.fu-berlin.de>

Hi!

par(mfrow=c(2,1))
splits the plot in two parts for example.
For more information about graphics read introduction to R
provided with your R installation.
if par(mfrow ...
par(mfcol is not flexible enough for you
you can examine the documentation for

?layout

If you need even more flexibility
look at the grid package of Paul Murrel

There is also an artikle in Rnews about combining the graphics function in the graphics package from P. Murrel.

You may also to consider to learn the graphic functions in the lattice package of by Deepayan Sarkar 
which build on grid.

Sincerely
Eryk




*********** REPLY SEPARATOR  ***********

On 6/2/2004 at 10:07 AM Steven White wrote:

>>>Greetings,
>>>
>>>I am new to R and quite confused by the apparent inconsistency in it's 
>>>behavior. My latest difficulty lies in producing eps output for use with 
>>>latex. In particular, I was successful in obtaining a single figure by
>>>using:
>>>
>>>postscript("ripple1.eps",onefile=FALSE)
>>>plot(nu_set[[1]],ip[[1]])
>>>lines(nu_set[[1]],predict(fitIp[[1]]))
>>>dev.off()
>>>
>>>...but then I tried to place two figures on the same output sheet by
>>>using 
>>>split.screen() as follows:
>>>
>>>postscript("ripple1.eps",onefile=FALSE)
>>>split.screen(c(1,2))
>>>screen(1)
>>>plot(nu_set[[1]],ip[[1]])
>>>lines(nu_set[[1]],predict(fitIp[[1]]))
>>>screen(2)
>>>plot(rangeIpNorm)
>>>dev.off()
>>>
>>>and received no output whatsoever. Furthermore, I can no longer get
>>>postscript 
>>>output using the original commands! I've tried exiting and re-starting
>>>R, and 
>>>am just about to reboot because I almost cannot believe what I am seeing.
>>>
>>>Can you spot anything that I am doing wrong?
>>>
>>>Thanks & Best Regards,
>>>Steve
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



Dipl. bio-chem. Eryk Witold Wolski    @    MPI-Moleculare Genetic   
Ihnestrasse 63-73 14195 Berlin       'v'    
tel: 0049-30-83875219               /   \    
mail: wolski at molgen.mpg.de        ---W-W----    http://www.molgen.mpg.de/~wolski



From Nicolas.Stransky at curie.fr  Thu Jun  3 14:04:35 2004
From: Nicolas.Stransky at curie.fr (Nicolas STRANSKY)
Date: Thu, 03 Jun 2004 14:04:35 +0200
Subject: [R] Re: Problem with par("usr")
In-Reply-To: <MJELLLFFFCNHMHOOLCMBEECECHAA.cdeclercq@nordnet.fr>
References: <MJELLLFFFCNHMHOOLCMBEECECHAA.cdeclercq@nordnet.fr>
Message-ID: <40BF13D3.4000909@curie.fr>

Christophe Declercq wrote:

> Bonjour, Nicolas

Bonjour,

> par('usr') is read-only (see '?par')

This part of the manual is a bit unclear to me because 'mfrow' doesn't 
seem to be read-only, unlike 'usr'...

> That makes sense: user coordinates come from what you have plotted.

Ok, fine.

> Would you show us an example of what exactly you want to do?

While trying to show a precise example, I had to re-launch R and this 
resolved all my problems (isn't it beautiful ? ;) ). Formerly, even if 
specifying the plot limits with 'xlim', I wasn't able to superpose two 
plots obtained with 'hist' with the same x axis (the scale wasn't the 
same, which is a bit annoying when you want to superpose plots). Now it 
works fine, but I had to close and re-launch R, I wonder why...

-- 
Nicolas STRANSKY
??quipe Oncologie Mol??culaire
Institut Curie - UMR 144 - CNRS                 Tel : +33 1 42 34 63 40
26, rue d'Ulm - 75248 Paris Cedex 5 - FRANCE    Fax : +33 1 42 34 63 49



From M.Mamin at intershop.de  Thu Jun  3 14:18:46 2004
From: M.Mamin at intershop.de (Marc Mamin)
Date: Thu, 3 Jun 2004 14:18:46 +0200
Subject: [R] catching the warnings
Message-ID: <A03188C6623C0D46A703CB5AA59907F2170676@JENMAIL01.ad.intershop.net>

Hello,

I'd like to catch the warnings in a variable in order to evaluate them, but...


> tt<-warnings()
Warning messages:
1: XML Parsing Error: test.xml:2: xmlParseStartTag: invalid element name
2: XML Parsing Error: test.xml:3: Extra content at the end of the document
> tt
NULL

is there a way to achieve this (R1.8.1)?

thanks,

Marc



From edd at debian.org  Thu Jun  3 14:30:05 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 3 Jun 2004 07:30:05 -0500
Subject: [R] Problem with mgcv PACKAGES file format?
In-Reply-To: <Pine.LNX.4.58.0406031646001.28371@tolstoy.newcastle.edu.au>
References: <Pine.LNX.4.58.0406031646001.28371@tolstoy.newcastle.edu.au>
Message-ID: <20040603123005.GA11822@sonny.eddelbuettel.com>

On Thu, Jun 03, 2004 at 04:58:16PM +1000, Robert King wrote:
> Hello All,
> 
> I'm getting this error (Version: 1.9.0-1 on a debian system)
> 
> > update.packages("mgcv")
[... snip ...]

Try

$ apt-get -t unstable install r-cran-mgcv

as mgcv is (obviously, given its 'recommend' status) one of the CRAN
packages in Debian itself.  Not sure why your built failed, though.  Ours
seem to work, witness the autobuilder logs at

	http://buildd.debian.org/build.php?pkg=mgcv

r-cran-mgcv should migrate from unstable to testing in the usual 10 day
window.

Hth, Dirk

-- 
FEATURE:  VW Beetle license plate in California



From edd at debian.org  Thu Jun  3 14:31:58 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Thu, 3 Jun 2004 07:31:58 -0500
Subject: [R] Re: Problem with mgcv PACKAGES file format?
In-Reply-To: <Pine.LNX.4.58.0406031724190.28832@tolstoy.newcastle.edu.au>
References: <Pine.LNX.4.58.0406031646001.28371@tolstoy.newcastle.edu.au>
	<Pine.LNX.4.58.0406031724190.28832@tolstoy.newcastle.edu.au>
Message-ID: <20040603123158.GB11822@sonny.eddelbuettel.com>

On Thu, Jun 03, 2004 at 05:27:48PM +1000, Robert King wrote:
> Problem solved.
> 
> This is a bad interaction of packages installed from debian with the R
> update.packages() system.
> 
> Moral of the story is don't try to update.packges() for any of the
> packages installed as r-cran-package.name.here debian packages.

Ah, yes, that is an achilles heel, and I am not sure how to handle that.
That may remain a problen.

Dirk

-- 
FEATURE:  VW Beetle license plate in California



From iperez at uniandes.edu.co  Thu Jun  3 14:35:41 2004
From: iperez at uniandes.edu.co (=?iso-8859-1?Q?Ignacio_P=E9rez?=)
Date: Thu, 3 Jun 2004 07:35:41 -0500
Subject: [R] Dr. Carmona's
Message-ID: <200406031235.i53CZloi022375@cocuy.uniandes.edu.co>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040603/c70edf92/attachment.pl

From rpeng at jhsph.edu  Thu Jun  3 14:43:05 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 03 Jun 2004 08:43:05 -0400
Subject: [R] catching the warnings
In-Reply-To: <A03188C6623C0D46A703CB5AA59907F2170676@JENMAIL01.ad.intershop.net>
References: <A03188C6623C0D46A703CB5AA59907F2170676@JENMAIL01.ad.intershop.net>
Message-ID: <40BF1CD9.9010207@jhsph.edu>

The warnings are stored in a variable `last.warning' in the workspace. 
  warnings() simply prints this variable.

-roger

Marc Mamin wrote:
> Hello,
> 
> I'd like to catch the warnings in a variable in order to evaluate them, but...
> 
> 
> 
>>tt<-warnings()
> 
> Warning messages:
> 1: XML Parsing Error: test.xml:2: xmlParseStartTag: invalid element name
> 2: XML Parsing Error: test.xml:3: Extra content at the end of the document
> 
>>tt
> 
> NULL
> 
> is there a way to achieve this (R1.8.1)?
> 
> thanks,
> 
> Marc
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From maechler at stat.math.ethz.ch  Thu Jun  3 15:01:10 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 3 Jun 2004 15:01:10 +0200
Subject: [R] labRow/labCol options in heatmap()
In-Reply-To: <40309E49.2090803@molgen.mpg.de>
References: <40309E49.2090803@molgen.mpg.de>
Message-ID: <16575.8470.573648.189268@gargle.gargle.HOWL>

{I'm vading through old "todo" e-mails...}

>>>>> "Anja" == Anja von Heydebreck <heydebre at molgen.mpg.de>
>>>>>     on Mon, 16 Feb 2004 11:41:13 +0100 writes:

    Anja> The function heatmap() allows to specify row/column labels
    Anja> via the options labRow/labCol. From the code of heatmap(),
    Anja> I understand that when no labels are specified, the row/column
    Anja> labels (or indices) of the input matrix are taken as labels and 
    Anja> re-ordered together with the rows and columns of the matrix before 
    Anja> plotting, whereas labels supplied via labRow/labCol are plotted
    Anja> in the original order.

    <..........>

    Anja> To see an example, try

    Anja> a <- matrix(c(1,1,1,2,0,2), nrow=3)
    Anja> rownames(a) <- 1:3
    Anja> x11()
    Anja> heatmap(a)
    Anja> x11()
    Anja> heatmap(a, labRow=rownames(a))

    Anja> Is this really meant to work like this? - From
    Anja> the help of heatmap(), one might expect also user-supplied
    Anja> labels to be re-ordered together with the rows and
    Anja> columns of the matrix.

Yes, indeed -- and as you suggest, the current behavior is not
what it should be.

This is being fixed in R-patched aka "R 1.9.1 alpha" (to be
1.9.1 later this month).

Thanking you for the clear report,
Martin Maechler



From sdavis2 at mail.nih.gov  Thu Jun  3 15:04:59 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 03 Jun 2004 09:04:59 -0400
Subject: [R] labRow/labCol options in heatmap()
In-Reply-To: <16575.8470.573648.189268@gargle.gargle.HOWL>
Message-ID: <BCE49A3B.8B70%sdavis2@mail.nih.gov>

Anja,

Not meant as a plug, but heatmap.2 in the gregmisc package has the correct
behavior and a few other options.

Sean

On 6/3/04 9:01 AM, "Martin Maechler" <maechler at stat.math.ethz.ch> wrote:

> {I'm vading through old "todo" e-mails...}
> 
>>>>>> "Anja" == Anja von Heydebreck <heydebre at molgen.mpg.de>
>>>>>>     on Mon, 16 Feb 2004 11:41:13 +0100 writes:
> 
>   Anja> The function heatmap() allows to specify row/column labels
>   Anja> via the options labRow/labCol. From the code of heatmap(),
>   Anja> I understand that when no labels are specified, the row/column
>   Anja> labels (or indices) of the input matrix are taken as labels and
>   Anja> re-ordered together with the rows and columns of the matrix before
>   Anja> plotting, whereas labels supplied via labRow/labCol are plotted
>   Anja> in the original order.
> 
>   <..........>
> 
>   Anja> To see an example, try
> 
>   Anja> a <- matrix(c(1,1,1,2,0,2), nrow=3)
>   Anja> rownames(a) <- 1:3
>   Anja> x11()
>   Anja> heatmap(a)
>   Anja> x11()
>   Anja> heatmap(a, labRow=rownames(a))
> 
>   Anja> Is this really meant to work like this? - From
>   Anja> the help of heatmap(), one might expect also user-supplied
>   Anja> labels to be re-ordered together with the rows and
>   Anja> columns of the matrix.
> 
> Yes, indeed -- and as you suggest, the current behavior is not
> what it should be.
> 
> This is being fixed in R-patched aka "R 1.9.1 alpha" (to be
> 1.9.1 later this month).
> 
> Thanking you for the clear report,
> Martin Maechler
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From sdavis2 at mail.nih.gov  Thu Jun  3 15:32:17 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 03 Jun 2004 09:32:17 -0400
Subject: [R] RMySQL question
Message-ID: <BCE4A0A1.8B75%sdavis2@mail.nih.gov>

In perl DBI, there is a method for preparing sql commands with wildcards and
then executing them with parameter bindings.  Is there a way to do this
within RMySQL?  I would like to be able to look up values in a table based
on a key supplied from R.

Thanks,
Sean



From wolski at molgen.mpg.de  Thu Jun  3 15:41:50 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Thu, 03 Jun 2004 15:41:50 +0200
Subject: [R] labRow/labCol options in heatmap()
In-Reply-To: <BCE49A3B.8B70%sdavis2@mail.nih.gov>
References: <BCE49A3B.8B70%sdavis2@mail.nih.gov>
Message-ID: <200406031541500845.0090A3A8@mail.math.fu-berlin.de>

Hi!

?paste 
works fine for me.

Sincerely
Eryk

*********** REPLY SEPARATOR  ***********

On 6/3/2004 at 9:04 AM Sean Davis wrote:

>>>Anja,
>>>
>>>Not meant as a plug, but heatmap.2 in the gregmisc package has the
>>>correct
>>>behavior and a few other options.
>>>
>>>Sean
>>>
>>>On 6/3/04 9:01 AM, "Martin Maechler" <maechler at stat.math.ethz.ch> wrote:
>>>
>>>> {I'm vading through old "todo" e-mails...}
>>>> 
>>>>>>>>> "Anja" == Anja von Heydebreck <heydebre at molgen.mpg.de>
>>>>>>>>>     on Mon, 16 Feb 2004 11:41:13 +0100 writes:
>>>> 
>>>>   Anja> The function heatmap() allows to specify row/column labels
>>>>   Anja> via the options labRow/labCol. From the code of heatmap(),
>>>>   Anja> I understand that when no labels are specified, the row/column
>>>>   Anja> labels (or indices) of the input matrix are taken as labels and
>>>>   Anja> re-ordered together with the rows and columns of the matrix
>>>before
>>>>   Anja> plotting, whereas labels supplied via labRow/labCol are plotted
>>>>   Anja> in the original order.
>>>> 
>>>>   <..........>
>>>> 
>>>>   Anja> To see an example, try
>>>> 
>>>>   Anja> a <- matrix(c(1,1,1,2,0,2), nrow=3)
>>>>   Anja> rownames(a) <- 1:3
>>>>   Anja> x11()
>>>>   Anja> heatmap(a)
>>>>   Anja> x11()
>>>>   Anja> heatmap(a, labRow=rownames(a))
>>>> 
>>>>   Anja> Is this really meant to work like this? - From
>>>>   Anja> the help of heatmap(), one might expect also user-supplied
>>>>   Anja> labels to be re-ordered together with the rows and
>>>>   Anja> columns of the matrix.
>>>> 
>>>> Yes, indeed -- and as you suggest, the current behavior is not
>>>> what it should be.
>>>> 
>>>> This is being fixed in R-patched aka "R 1.9.1 alpha" (to be
>>>> 1.9.1 later this month).
>>>> 
>>>> Thanking you for the clear report,
>>>> Martin Maechler
>>>> 
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide!
>>>http://www.R-project.org/posting-guide.html
>>>>
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



Dipl. bio-chem. Eryk Witold Wolski    @    MPI-Moleculare Genetic   
Ihnestrasse 63-73 14195 Berlin       'v'    
tel: 0049-30-83875219               /   \    
mail: wolski at molgen.mpg.de        ---W-W----    http://www.molgen.mpg.de/~wolski



From wolski at molgen.mpg.de  Thu Jun  3 15:46:56 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Thu, 03 Jun 2004 15:46:56 +0200
Subject: [R] RMySQL question
In-Reply-To: <BCE4A0A1.8B75%sdavis2@mail.nih.gov>
References: <BCE4A0A1.8B75%sdavis2@mail.nih.gov>
Message-ID: <200406031546560875.00954F16@mail.math.fu-berlin.de>

Hi!

?paste 
works fine for me.

Sincerely
Eryk


*********** REPLY SEPARATOR  ***********

On 6/3/2004 at 9:32 AM Sean Davis wrote:

>>>In perl DBI, there is a method for preparing sql commands with
>>>wildcards and
>>>then executing them with parameter bindings.  Is there a way to do this
>>>within RMySQL?  I would like to be able to look up values in a table
>>>based
>>>on a key supplied from R.
>>>
>>>Thanks,
>>>Sean
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



Dipl. bio-chem. Eryk Witold Wolski    @    MPI-Moleculare Genetic   
Ihnestrasse 63-73 14195 Berlin       'v'    
tel: 0049-30-83875219               /   \    
mail: wolski at molgen.mpg.de        ---W-W----    http://www.molgen.mpg.de/~wolski



From dj at research.bell-labs.com  Thu Jun  3 16:03:31 2004
From: dj at research.bell-labs.com (David James)
Date: Thu, 3 Jun 2004 10:03:31 -0400
Subject: [R] RMySQL question
In-Reply-To: <BCE4A0A1.8B75%sdavis2@mail.nih.gov>;
	from sdavis2@mail.nih.gov on Thu, Jun 03, 2004 at 09:32:17AM -0400
References: <BCE4A0A1.8B75%sdavis2@mail.nih.gov>
Message-ID: <20040603100330.A15131@jessie.research.bell-labs.com>

Prepared statements are not yet in the "production" 4.0 release
of MySQL, only in the development or alpha version 4.1.  I hope
to add prepared statements and data.frame bindings by the time
4.1 becomes the "production" release.  The API for working with
prepared statements and bindings from R will be most likely very
similar the current ROracle implementation.

--
David

Sean Davis wrote:
> In perl DBI, there is a method for preparing sql commands with wildcards and
> then executing them with parameter bindings.  Is there a way to do this
> within RMySQL?  I would like to be able to look up values in a table based
> on a key supplied from R.
> 
> Thanks,
> Sean
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From gb at stat.umu.se  Thu Jun  3 16:19:40 2004
From: gb at stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Thu, 3 Jun 2004 16:19:40 +0200
Subject: [R] dropterm and frailty
Message-ID: <20040603141939.GA19890@stat.umu.se>

Is it meaningful to use 'dropterm' (MASS) on a fitted coxph (survival) model
with a frailty term? Trying to do so gives results that do not look
meaningful (to me), so my guess is that the answer to my question is 'No'.

So, what is the recommended way of comparing nested mixed-effects models in
general? 
-- 
 G??ran Brostr??m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume?? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume??, Sweden             e-mail: gb at stat.umu.se



From amackey at pcbi.upenn.edu  Thu Jun  3 16:50:55 2004
From: amackey at pcbi.upenn.edu (Aaron J. Mackey)
Date: Thu, 3 Jun 2004 10:50:55 -0400
Subject: [R] RMySQL question
In-Reply-To: <200406031546560875.00954F16@mail.math.fu-berlin.de>
References: <BCE4A0A1.8B75%sdavis2@mail.nih.gov>
	<200406031546560875.00954F16@mail.math.fu-berlin.de>
Message-ID: <6B0E5B36-B56D-11D8-BF60-000A95A56386@pcbi.upenn.edu>


Yes, of course paste() works; the question is about getting more 
efficient query execution by preparing statements with placeholders in 
advance (as opposed to paste()-ing together a new SQL statement for 
every single statement execution, and requiring the database to 
reinterpret/recompile it every time).

The response about prepared statements only available in 4.1 was a bit 
odd to me at first, since in the Perl DBI world we've been preparing 
and binding for along time now with DBD::mysql; in that case though, 
the optimization is really one of DBI's, not MySQL's.  4.1 will bring 
that level of optimization to the database.

So to truly answer the OP's question, there is no prepare/bind facility 
in RMySQL, nor does there need to be one to get the same level of 
performance as seen in DBD::mysql (but in the future, both DBD::mysql 
and RMySQL may get even faster if they start using the MySQL API for 
prepared statements).  The additional affect of proper quoting provided 
by the DBI's binding facility is available to you via RMySQL's quote() 
function (if memory serves me right ... it's been awhile).

-Aaron

On Jun 3, 2004, at 9:46 AM, Wolski wrote:

> ?paste
> works fine for me.
>
> On 6/3/2004 at 9:32 AM Sean Davis wrote:
>
>>>> In perl DBI, there is a method for preparing sql commands with
>>>> wildcards and
>>>> then executing them with parameter bindings.  Is there a way to do 
>>>> this
>>>> within RMySQL?  I would like to be able to look up values in a table
>>>> based
>>>> on a key supplied from R.



From pgilbert at bank-banque-canada.ca  Thu Jun  3 16:49:40 2004
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Thu, 03 Jun 2004 10:49:40 -0400
Subject: [R] Distributed computing with R
In-Reply-To: <85ekoxr6t3.fsf@servant.blindglobe.net>
References: <026d01c448df$e9e76760$17470843@MITRE.ORG>	<40BE4422.3090005@xss.de>
	<85ekoxr6t3.fsf@servant.blindglobe.net>
Message-ID: <40BF3A84.2000608@bank-banque-canada.ca>

Tony

Thanks, this categorization has cleared up a few things I have found 
confusing. But should I read this to mean that SNOW would not run  on  
a  system or kernel level parallel setup?

Thanks,
Paul Gilbert

A.J. Rossini wrote:

>Also see SNOW (which simplifies parallel programming, sits on top of
>rpvm, Rmpi, or a socket-based system).
>
>Depends on whether you want parallelism on the:
>
>1. User-level -- the libraries such as PVM, LAM-MPI, etc will help,
>                 and there are various packages which provide an API
>                 to those.
>
>2. System-level -- then Condor, Sun Grid Engine / Maui scheduler, and
>                   similar queueing/batching/allocation daemons will
>                   help (computational grid software is usually a
>                   generalization of this which adds authentication
>                   and resource allocation).
>
>3. Kernel-level -- then OpenMOSIX, BPROC, etc will help.
>
>They are mostly orthogonal.  Mostly... :-).
>
>best,
>-tony
>
>
>
>Armin Roehrl <armin at xss.de> writes:
>
>  
>
>>If you do some programming, you might want to look at MPI.
>>R-extensions for MPI exist  (RMPI).
>>
>>It all depends a lot on what kind of usage you envisage of your cluster.
>>Open-PBS is also a good batch system. Maybe you also want to
>>look at Mosix, which is a modified linux system.
>>
>>Depending on what your ultimate computing ressources are,
>>maybe also look at IBM's Globus toolkit.
>>
>>Parallel programming is fun. The world is inherently parallel!
>>Ciao,
>>    -Armin.
>>
>>----------------------------------------
>>Armin Roehrl, http://www.approximity.com
>>We manage risk
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>    
>>
>
>  
>



From rxg218 at psu.edu  Thu Jun  3 17:38:14 2004
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Thu, 03 Jun 2004 11:38:14 -0400
Subject: [R] a question regarding apply
Message-ID: <1086277094.7975.3.camel@blue.chem.psu.edu>

Hi,
  I have a matrix, m, over whose rows I want to apply a function. I also
have a vector, r, whose length is equal to the rows of m.

The obvious way is:

result <- apply( m, c(1), fun )

However the function call requires the row from m and the corresponding
element of r.

So, I want to pass m[i,] and r[i] to the function.

Currently I use a loop. But can this be modified to use apply (or
related functions)?

Thanks,

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
Q: What do you get when you cross a Post Modernist with a Mafioso?
A: An offer you can't understand.



From tplate at blackmesacapital.com  Thu Jun  3 17:40:53 2004
From: tplate at blackmesacapital.com (Tony Plate)
Date: Thu, 03 Jun 2004 09:40:53 -0600
Subject: [R] "privileged slots",
In-Reply-To: <1086083049.8636.16.camel@biol102145.oulu.fi>
References: <40B671D8.20218.25D99F1@localhost>
	<40BC66AA.27228.27CF9B@localhost>
	<1086083049.8636.16.camel@biol102145.oulu.fi>
Message-ID: <6.1.0.6.2.20040603092013.042110a0@mailhost.blackmesacapital.com>

At Tuesday 03:44 AM 6/1/2004, Jari Oksanen wrote:
> > [snip]
>There are several other things that were fully documented and still were
>removed. One of the latest cases was print.coefmat which was abruptly
>made Defunct without warning or grace period: code written for 1.8*
>didn't work in 1.9.0 and if corrected for 1.9.0 it wouldn't work in
>pre-1.9.0. Anything can change in R without warning, and your code may
>be broken anytime. Just be prepared.

This is true of many software packages.  In our production environment we 
often (usually) run older versions of software, including statistical 
software, because of bugs or changed behaviors (or fears thereof) in new 
versions.  We usually run the latest versions in our test and 
non-production systems and only upgrade our production systems when two 
conditions are satisfied: (1) we need the features in the upgrade and (2) 
we are comfortable that the upgraded package will run reliably.  From what 
I can see, R is only distinguished from other software packages in these 
regards by the extreme speed with which bug fixes for the latest version 
are made available (in contrast, we're still waiting more than a year for 
fixes for bugs in some commercial software that were described as 
"critical" bugs by the vendor's support team) and the high level of respect 
accorded to users by the core developers (changes are debated and effects 
on existing software seem to be taken seriously).

One very helpful tool to deal with software updates is automated 
testing.  I highly recommend it.  R comes with a testing framework.

-- Tony Plate

>cheers, jari oksanen
>--
>Jari Oksanen <jarioksa at sun3.oulu.fi>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From tlumley at u.washington.edu  Thu Jun  3 17:43:46 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 3 Jun 2004 08:43:46 -0700 (PDT)
Subject: [R] a question regarding apply
In-Reply-To: <1086277094.7975.3.camel@blue.chem.psu.edu>
References: <1086277094.7975.3.camel@blue.chem.psu.edu>
Message-ID: <Pine.A41.4.58.0406030841100.160018@homer06.u.washington.edu>

On Thu, 3 Jun 2004, Rajarshi Guha wrote:

> Hi,
>   I have a matrix, m, over whose rows I want to apply a function. I also
> have a vector, r, whose length is equal to the rows of m.
>
> The obvious way is:
>
> result <- apply( m, c(1), fun )
>
> However the function call requires the row from m and the corresponding
> element of r.
>
> So, I want to pass m[i,] and r[i] to the function.
>
> Currently I use a loop. But can this be modified to use apply (or
> related functions)?

One way is to stick the vector onto the matrix

  apply(cbind(r,m), 1, function(x) fun(mi=x[-1],ri=x[1]))

Incidentally, there's never any point in writing c(1)
> identical(c(1),1)
[1] TRUE


	-thomas



From rpeng at jhsph.edu  Thu Jun  3 17:40:00 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 03 Jun 2004 11:40:00 -0400
Subject: [R] Distributed computing with R
In-Reply-To: <40BF3A84.2000608@bank-banque-canada.ca>
References: <026d01c448df$e9e76760$17470843@MITRE.ORG>
	<40BE4422.3090005@xss.de> <85ekoxr6t3.fsf@servant.blindglobe.net>
	<40BF3A84.2000608@bank-banque-canada.ca>
Message-ID: <40BF4650.5010202@jhsph.edu>

snow works well on an openMosix system, and is actually quite 
convenient since you don't have to worry about which process is going 
to which computer.  The kernel migrates the processes automatically 
(usually).

-roger

Paul Gilbert wrote:
> Tony
> 
> Thanks, this categorization has cleared up a few things I have found 
> confusing. But should I read this to mean that SNOW would not run  on  
> a  system or kernel level parallel setup?
> 
> Thanks,
> Paul Gilbert
> 
> A.J. Rossini wrote:
> 
>> Also see SNOW (which simplifies parallel programming, sits on top of
>> rpvm, Rmpi, or a socket-based system).
>>
>> Depends on whether you want parallelism on the:
>>
>> 1. User-level -- the libraries such as PVM, LAM-MPI, etc will help,
>>                 and there are various packages which provide an API
>>                 to those.
>>
>> 2. System-level -- then Condor, Sun Grid Engine / Maui scheduler, and
>>                   similar queueing/batching/allocation daemons will
>>                   help (computational grid software is usually a
>>                   generalization of this which adds authentication
>>                   and resource allocation).
>>
>> 3. Kernel-level -- then OpenMOSIX, BPROC, etc will help.
>>
>> They are mostly orthogonal.  Mostly... :-).
>>
>> best,
>> -tony
>>
>>
>>
>> Armin Roehrl <armin at xss.de> writes:
>>
>>  
>>
>>> If you do some programming, you might want to look at MPI.
>>> R-extensions for MPI exist  (RMPI).
>>>
>>> It all depends a lot on what kind of usage you envisage of your cluster.
>>> Open-PBS is also a good batch system. Maybe you also want to
>>> look at Mosix, which is a modified linux system.
>>>
>>> Depending on what your ultimate computing ressources are,
>>> maybe also look at IBM's Globus toolkit.
>>>
>>> Parallel programming is fun. The world is inherently parallel!
>>> Ciao,
>>>    -Armin.
>>>
>>> ----------------------------------------
>>> Armin Roehrl, http://www.approximity.com
>>> We manage risk
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>>
>>>   
>>
>>
>>  
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From csilva at ipimar.pt  Thu Jun  3 17:53:57 2004
From: csilva at ipimar.pt (Cristina Silva)
Date: Thu, 3 Jun 2004 16:53:57 +0100
Subject: [R] Confidence intervals for predicted values in nls
References: <200406031006.i53A2wwV002842@hypatia.math.ethz.ch>
Message-ID: <006201c44982$fa8a99e0$52040a0a@Csilva>

Dear all

I have tried to estimate the confidence intervals for predicted values of a
nonlinear model fitted with nls. The function predict gives the predicted
values and the lower and upper limits of the prediction, when the class of
the object is lm or glm. When the object is derived from nls, the function
predict (or predict.nls) gives only the predicted values. The se.fit and
interval aguments are just ignored.

Could anybody tell me how to estimate the confidence intervals for the
predicted values (not the model parameters), using an object of class nls?

Regards,

Cristina

------------------------------------------
Cristina Silva
IPIMAR - Departamento de Recursos Marinhos
Av. de Bras??lia, 1449-006 Lisboa
Portugal
Tel.: 351 21 3027096
Fax: 351 21 3015948
csilva at ipimar.pt



From tplate at blackmesacapital.com  Thu Jun  3 18:16:23 2004
From: tplate at blackmesacapital.com (Tony Plate)
Date: Thu, 03 Jun 2004 10:16:23 -0600
Subject: [R] Importing binary data
In-Reply-To: <Pine.LNX.4.58.0406011153530.17916@linux28.phonetik.uni-mue
	nchen.de>
References: <Pine.LNX.4.58.0406011153530.17916@linux28.phonetik.uni-muenchen.de>
Message-ID: <6.1.0.6.2.20040603101440.041b12f0@mailhost.blackmesacapital.com>

Probably the simplest way to improve the speed of your code would be to 
write the data so that all the data in a column is contiguous.  Then you'll 
be able to read each column with a single call to readBin().

hope this helps,

Tony Plate

At Tuesday 04:02 AM 6/1/2004, Uli Tuerk wrote:

>Hi everybody!
>
>I've a large dataset, about 2 Mio entries of the format which I would like
>to import into a frame:
><integer><integer><float><string><float><string><string>
>
>Because to the huge data amount I've choosen a binary format instead
>of a text format when exporting from Matlab.
>My import function is attached below. It works fine for only some entries
>but is deadly slow when trying to read the complete set.
>
>Does anybody has some pointers for me for improving the import or handling
>such large data sets?
>
>Thanks in advance!
>
>Uli
>
>
>
>read.DET.data <- function ( f ) {
>         counter <- 1
>         spk.v <- c()
>         imp.v <- c()
>         score.v <- c()
>         th.v <- c()
>         ses.v <- c()
>         rec.v <- c()
>         type.v <- c()
>         fid <- file( f ,"rb")
>         tempi <- readBin(fid , integer(), size=1, signed=FALSE)
>         while ( length(tempi) != 0) {
>                 spk.v[ counter ] <- tempi
>                 imp.v[ counter ] <- readBin(fid, integer(), size=1, 
> signed=FALSE)
>                 score.v[ counter  ] <- readBin(fid, numeric(), size=4)
>                 type.v[ counter ] <- readBin(fid, character())
>                 th.v[ counter ] <- readBin(fid, numeric(), size=4)
>                 ses.v[ counter ] <- readBin(fid, character())
>                 rec.v[ counter ] <- readBin(fid, character())
>                 counter <- counter + 1
>                 tempi <- readBin(fid, integer(), size=1, signed=FALSE)
>         }
>         close( fid )
>         spkf <- factor ( spk.v )
>         impf <- factor ( imp.v )
>
>         det.f <- data.frame( spk=spkf, imp=impf, score=score.v, th=th.v, 
> ses=ses.v, rec=rec.v, type=type.v)
>
>         det.f
>}
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Thu Jun  3 18:46:59 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 03 Jun 2004 09:46:59 -0700
Subject: [R] S4 classes?
In-Reply-To: <40BDA5D7.7060908@statistik.uni-dortmund.de>
References: <40BD9091.4020304@pdf.com>
	<40BDA5D7.7060908@statistik.uni-dortmund.de>
Message-ID: <40BF5603.2060708@pdf.com>

      Thanks to Eryk Wolski, Martin Maechler, Dimitris Rizopoulos and 
Uwe Ligges:  I uninstalled my apparently corrupted R 1.9.0pat and 
installed R 1.9.1 alpha, and at least this simple example now works. 

      Best Wishes,
      spencer graves

Uwe Ligges wrote:

> Spencer Graves wrote:
>
>>      The following example, extracted from pp. 38-42 of Chambers 
>> (1998) Programming with Data (Springer), works for me in S-Plus 6.2 
>> but not R 1.9.0pat, which returns the indicated error messages:
>>  > setClass('track', representation(x="numeric", y="numeric"))
>> [1] "track"
>>  > tr1 <- new("track", x=1:3, y=4:6)
>>  > setMethod("plot",
>> + signature(x="track", y="missing"),
>> + function(x, y, ...)plot(x at x, x at y, ...)
>> + )
>> [1] "plot"
>>  > plot(tr1)
>> Error in plot.window(xlim, ylim, log, asp, ...) :
>>        need finite xlim values
>> In addition: Warning messages:
>> 1: no finite arguments to min; returning Inf
>> 2: no finite arguments to max; returning -Inf
>> 3: no finite arguments to min; returning Inf
>> 4: no finite arguments to max; returning -Inf
>>
>>      Am I missing something or is my copy of R 1.9.0pat corrupted?  
>> (This is under Windows 2000.)
>>      Thanks Much!
>>      Spencer Graves
>
>
>
> Works for me with R-1.9.0 Patched (2004-05-24)
>
> Uwe
>



From wwsprague at ucdavis.edu  Thu Jun  3 19:19:26 2004
From: wwsprague at ucdavis.edu (wwsprague@ucdavis.edu)
Date: Thu, 03 Jun 2004 10:19:26 -0700
Subject: [R] printing tabular data nicely
Message-ID: <c9nmj3$acp$1@sea.gmane.org>

Hi R-heplers,

I would like to print various matrices, dataframes, tables, etc to 
files, preferably nicely formatted postscript for import into papers. 
Is there a way to do this?

I know ?cat, ?writeLines, ?format, ?paste.  But I am not sure of a good 
combination of these in order to get a nice looking table of information.

Any ideas?  I guess I want (almost) publication ready output, just like 
you get for "plot"...

(I *don't* want to print to the console, btw)

Thx again.
W



From sundar.dorai-raj at PDF.COM  Thu Jun  3 19:27:52 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Thu, 03 Jun 2004 12:27:52 -0500
Subject: [R] printing tabular data nicely
In-Reply-To: <c9nmj3$acp$1@sea.gmane.org>
References: <c9nmj3$acp$1@sea.gmane.org>
Message-ID: <40BF5F98.5040308@pdf.com>



wwsprague at ucdavis.edu wrote:

> Hi R-heplers,
> 
> I would like to print various matrices, dataframes, tables, etc to 
> files, preferably nicely formatted postscript for import into papers. Is 
> there a way to do this?
> 
> I know ?cat, ?writeLines, ?format, ?paste.  But I am not sure of a good 
> combination of these in order to get a nice looking table of information.
> 
> Any ideas?  I guess I want (almost) publication ready output, just like 
> you get for "plot"...
> 
> (I *don't* want to print to the console, btw)
> 
> Thx again.
> W
> 

Take a look at the xtable package. I'm not sure if you can get xtable to 
print directly to a file. If not you can use xtable in conjunction with 
?sink.

--sundar



From monica.palaseanu-lovejoy at stud.man.ac.uk  Thu Jun  3 19:39:41 2004
From: monica.palaseanu-lovejoy at stud.man.ac.uk (Monica Palaseanu-Lovejoy)
Date: Thu, 3 Jun 2004 18:39:41 +0100
Subject: [R] ecdf plots, lines, and y values
Message-ID: <E1BVwBu-0003qR-Lm@serenity.mcc.ac.uk>

Hi,

I have a question for the group, perhaps someone can help me 
figure this out.  I've already looked in the help files and they were 
no help to me.  

I have a vector of values and I am plotting an ecdf graph. 

1. How can i draw a continuous line through the ecdf points? (lines 
and type for the plot with an ecdf object does not work)

2. Supposing I have this line drawn. I can add a vertical line of a 
known x value which intersects the graph. How can I determine the 
y value of the graph point that is intersected by the abline? 

Any help would be appreciated.

    Thanks in advance,
    Monica



From hrobison at unr.nevada.edu  Thu Jun  3 20:15:25 2004
From: hrobison at unr.nevada.edu (HILLARY ROBISON)
Date: Thu, 3 Jun 2004 11:15:25 -0700 (PDT)
Subject: [R] GAM question
Message-ID: <Pine.SOL.4.58.0406031041170.17406@lehman.scsr.nevada.edu>

I am trying to use R to do a weighted GAM with PA (presence/random) as the
response variable (Y, which is a 0 or a 1) and ASPECT (values go from
0-3340), DEM (from 1500-3300), HLI (from 0-5566), PLAN (from -3 to 3),
PROF (from -3 to 3), SLOPE (from 100-500) and TRI (from 0-51) as
predictor variables (Xs).  I need to weight each observation by its WO
value (from 0.18 to 0.98).  I have specified the following models in R
(see below), but I can't figure out what the R reported errors plainly
mean. One of the errors seems to tell me my dataset is too big (it's
109,729 rows by 16 columns) - is this possible?  Given what I am trying to
accomplish (a weighted, logistic GAM with 7 variables), am I specifying my
model correctly?  I would like to attach my dataset (it's 2,064 KB
as a WinZip file), but I don't know if it'll go through to the list given
the HTML & attachment contraints of the list...  I even tried a weighted,
logistic GLM with the seven variables to see if that would work and if so,
perhaps it was a GAM problem.  I also tried a logistic, weighted GAM with
one variable to see if that would work.  My next step while I wait to
hear back from the list is to try a dummy dataset that is small to see if
a weighted, logistic GAM with seven variables will work at all or if I am
speciying the model correctly.  Would anyone be willing to have my dataset
sent so they can check it out if that would help solve the issue?  Thank you!
Hillary (hrobison at unr.nevada.edu)

> # trial, all, weighted
> topo8 <- gam(PA ~ s(SLOPE10) + s(ASPECT10) + s(GYADEMPLUS) + s(TRI) +
s(HLI) + s(PLAN10) + s(PROF10), family=binomial, data=topox, weights = w0)
Warning in eval(expr, envir, enclos) : non-integer #successes in a
binomial glm!
Error: cannot allocate vector of size 60865 Kb

> topo9 <- glm(PA ~ SLOPE10 + ASPECT10 + GYADEMPLUS + TRI + HLI + PLAN10 +
PROF10, family=binomial, data=topox, weights = w0)
Warning in eval(expr, envir, enclos) : non-integer #successes in a
binomial glm!

> # trial, weighted, slope only
> topo10 <- gam(PA ~ s(SLOPE10), family=binomial, data=topox, weights =
w0)
Warning in eval(expr, envir, enclos) : non-integer #successes in a
binomial glm!



From pgilbert at bank-banque-canada.ca  Thu Jun  3 20:35:52 2004
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Thu, 03 Jun 2004 14:35:52 -0400
Subject: [R] "privileged slots",
In-Reply-To: <6.1.0.6.2.20040603092013.042110a0@mailhost.blackmesacapital.com>
References: <40B671D8.20218.25D99F1@localhost>	<40BC66AA.27228.27CF9B@localhost>	<1086083049.8636.16.camel@biol102145.oulu.fi>
	<6.1.0.6.2.20040603092013.042110a0@mailhost.blackmesacapital.com>
Message-ID: <40BF6F88.9010802@bank-banque-canada.ca>

Tony Plate wrote:

> At Tuesday 03:44 AM 6/1/2004, Jari Oksanen wrote:
>
>> > [snip]
>>  Anything can change in R without warning, and your code may
>> be broken anytime. Just be prepared.
>
> >[snip]
> One very helpful tool to deal with software updates is automated 
> testing.  I highly recommend it.  R comes with a testing framework. 

If you package up your code, and put lots of tests in the tests 
directory, and put lots of examples in the documentation, and put in 
demos for good measure, then it is really, really,  easy to test with 
beta versions of R. This not only  helps get any R bugs that affect you 
fixed before release, but also gives you considerable warning time when 
there are pending intended changes. Not only that, but if you put the 
package on CRAN then the testing all gets done for you automatically on 
several platforms, and on alpha versions, and sometimes even when R 
design changes are been considered. There is an extremely large payback 
from contributing to CRAN.

Paul Gilbert



From rpeng at jhsph.edu  Thu Jun  3 20:04:33 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 03 Jun 2004 14:04:33 -0400
Subject: [R] printing tabular data nicely
In-Reply-To: <40BF5F98.5040308@pdf.com>
References: <c9nmj3$acp$1@sea.gmane.org> <40BF5F98.5040308@pdf.com>
Message-ID: <40BF6831.1050906@jhsph.edu>

The print method for xtable can send the output to a file.

-roger

Sundar Dorai-Raj wrote:
> 
> 
> wwsprague at ucdavis.edu wrote:
> 
>> Hi R-heplers,
>>
>> I would like to print various matrices, dataframes, tables, etc to 
>> files, preferably nicely formatted postscript for import into papers. 
>> Is there a way to do this?
>>
>> I know ?cat, ?writeLines, ?format, ?paste.  But I am not sure of a 
>> good combination of these in order to get a nice looking table of 
>> information.
>>
>> Any ideas?  I guess I want (almost) publication ready output, just 
>> like you get for "plot"...
>>
>> (I *don't* want to print to the console, btw)
>>
>> Thx again.
>> W
>>
> 
> Take a look at the xtable package. I'm not sure if you can get xtable to 
> print directly to a file. If not you can use xtable in conjunction with 
> ?sink.
> 
> --sundar
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From dmurdoch at pair.com  Thu Jun  3 21:06:14 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Thu, 03 Jun 2004 15:06:14 -0400
Subject: [R] printing tabular data nicely
In-Reply-To: <c9nmj3$acp$1@sea.gmane.org>
References: <c9nmj3$acp$1@sea.gmane.org>
Message-ID: <uitub0tfb9gl859iufpfcv81ub9tg9vka9@4ax.com>

On Thu, 03 Jun 2004 10:19:26 -0700, wwsprague at ucdavis.edu wrote :

>Hi R-heplers,
>
>I would like to print various matrices, dataframes, tables, etc to 
>files, preferably nicely formatted postscript for import into papers. 
>Is there a way to do this?
>
>I know ?cat, ?writeLines, ?format, ?paste.  But I am not sure of a good 
>combination of these in order to get a nice looking table of information.
>
>Any ideas?  I guess I want (almost) publication ready output, just like 
>you get for "plot"...
>
>(I *don't* want to print to the console, btw)

As Sundar said, xtable is probably what you want.  You may also
benefit from the fairly new "addmargins" function.

Duncan Murdoch



From ligges at statistik.uni-dortmund.de  Thu Jun  3 21:36:48 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 03 Jun 2004 21:36:48 +0200
Subject: [R] ecdf plots, lines, and y values
In-Reply-To: <E1BVwBu-0003qR-Lm@serenity.mcc.ac.uk>
References: <E1BVwBu-0003qR-Lm@serenity.mcc.ac.uk>
Message-ID: <40BF7DD0.8020107@statistik.uni-dortmund.de>

Monica Palaseanu-Lovejoy wrote:
> Hi,
> 
> I have a question for the group, perhaps someone can help me 
> figure this out.  I've already looked in the help files and they were 
> no help to me.  
> 
> I have a vector of values and I am plotting an ecdf graph. 
> 
> 1. How can i draw a continuous line through the ecdf points? (lines 
> and type for the plot with an ecdf object does not work)

Well, the help page ?plot.ecdf and ?plot.stepfun is pretty clear and 
tells us to use verticals = TRUE.
 From the help page:

   F10 <- ecdf(rnorm(10))
   plot(F10, verticals = TRUE, do.p = FALSE)


> 2. Supposing I have this line drawn. I can add a vertical line of a 
> known x value which intersects the graph. How can I determine the 
> y value of the graph point that is intersected by the abline? 

Use knots(), e.g. by extending the former example as follows:

   kF10 <- knots(F10)
   sum(kF10 <= x) / length(kF10)

Uwe Ligges



> Any help would be appreciated.
> 
>     Thanks in advance,
>     Monica
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From chrysopa at insecta.ufv.br  Thu Jun  3 22:28:55 2004
From: chrysopa at insecta.ufv.br (Ronaldo Reis Jr.)
Date: Thu, 3 Jun 2004 17:28:55 -0300
Subject: [R] [OFF] program to estimate the best fit
Message-ID: <200406031728.55841.chrysopa@insecta.ufv.br>

Hi,

exist in R or any other program for linux that estimate the best fit for data 
using severals functions? Somethink like tablecurve

Thanks
Ronaldo
-- 
	Um menino-prodigio e uma crianca cujos pais tem muita 
	imaginacao.
		-- Jean Costeau 
--
|>   // | \\   [***********************************]
|   ( ??   ?? )  [Ronaldo Reis J??nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36571-000 Vi??osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-2532                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From bdavenhall at sbcglobal.net  Thu Jun  3 22:46:40 2004
From: bdavenhall at sbcglobal.net (Brian Davenhall)
Date: Thu, 3 Jun 2004 13:46:40 -0700
Subject: [R] Simulating a landscape (matrix) in R
Message-ID: <003601c449ab$e2e1ca60$c2e5fea9@oemcomputer>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040603/23ada4bd/attachment.pl

From ripley at stats.ox.ac.uk  Thu Jun  3 23:22:14 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 3 Jun 2004 22:22:14 +0100 (BST)
Subject: [R] Confidence intervals for predicted values in nls
In-Reply-To: <006201c44982$fa8a99e0$52040a0a@Csilva>
Message-ID: <Pine.LNX.4.44.0406032216050.3493-100000@gannet.stats>

On Thu, 3 Jun 2004, Cristina Silva wrote:

> I have tried to estimate the confidence intervals for predicted values of a
> nonlinear model fitted with nls. The function predict gives the predicted
> values and the lower and upper limits of the prediction, when the class of
> the object is lm or glm. When the object is derived from nls, the function
> predict (or predict.nls) gives only the predicted values. The se.fit and
> interval aguments are just ignored.

Thre are no such arguments either to the generic function nls() nor its 
"nls" method.  Please do read the documentation!

> Could anybody tell me how to estimate the confidence intervals for the
> predicted values (not the model parameters), using an object of class nls?

First you need to understand how to do this in theory: thereafter it is a 
programming task.  Hint: to find a confidence region for the parameters is 
not at all easy, as the examples in MASS (the book) and elsewhere show, 
and there is no guarantee that the confidence region for the prediction 
will be a single interval.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From vicgrave at hotmail.com  Thu Jun  3 23:38:08 2004
From: vicgrave at hotmail.com (Victor Gravenholt)
Date: Thu, 3 Jun 2004 23:38:08 +0200
Subject: [R] Simulating a landscape (matrix) in R
References: <003601c449ab$e2e1ca60$c2e5fea9@oemcomputer>
Message-ID: <BAY19-DAV19HMwVN7mi00000b26@hotmail.com>

You could try something like this.
Simulating with a large number of grid cells is however very RAM expensive.

library(MASS)
library(spatial)
 x <- expand.grid(1:30, 1:30)
 distances <- as.matrix(dist(x, diag=T, upper=T))
 Sigma <- expcov(r=distances, d=10, se=1) 
 z <- mvrnorm(n = 1, mu=rep(0,900), Sigma) 
 z <- matrix(z,nrow=30)
 image(z)



----- Original Message ----- 
From: "Brian Davenhall" <bdavenhall at sbcglobal.net>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, June 03, 2004 10:46 PM
Subject: [R] Simulating a landscape (matrix) in R


> I'm trying to figure out how one might go about simulating a landscape
> (matrix) in R.  For example if one wanted to generate a simulated landscape
> of precipitation values for some area (say a 100 X 100 matrix) they could
> generate 10,000 numbers using a random normal distribution with a mean and
> std. dev. and randomly allocate these generated numbers to the grid cells.
> However, this is too simplistic and the resulting matrix will be very noisy
> since one cell could have a very high value and an adjacent cell could have
> a very low value.  Is it possible to generate a simulated matrix that would
> somehow incorporate a measure of spatial autocorrelation, so that grid cells
> closer to each other are similar?
> 
>  
> 
> Is this sort of thing possible in R to get a realistic surface or are other
> software packages (e.g., surface visualizations/rendering) more appropriate
> for this sort of thing?
> 
> 
> [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From RaVishwanath at coh.org  Thu Jun  3 23:49:22 2004
From: RaVishwanath at coh.org (Vishwanath, Rahul)
Date: Thu, 3 Jun 2004 14:49:22 -0700
Subject: [R] conditionals with String variables
Message-ID: <56D82F14F418964482B022FABB9322D3E81940@exchmbx5.Coh.ORG>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040603/8cebca13/attachment.pl

From ripley at stats.ox.ac.uk  Fri Jun  4 00:01:08 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 3 Jun 2004 23:01:08 +0100 (BST)
Subject: [R] prcomp help
In-Reply-To: <Pine.LNX.4.60.0405301930110.1617@l1>
Message-ID: <Pine.LNX.4.44.0406032300220.3493-100000@gannet.stats>

That's because R-devel and R-patched have a workaround for a bug in the 
latest gcc: see the NEWS file.

On Sun, 30 May 2004, Al Piszcz wrote:

> 
> I downloaded and built the development version
> of R dated 30 May. make check, and prcomp
> were successful.
> 
> 
> 
> On Sun, 30 May 2004, Uwe Ligges wrote:
> 
> > Date: Sun, 30 May 2004 22:01:58 +0200
> > From: Uwe Ligges <ligges at statistik.uni-dortmund.de>
> > To: Al Piszcz <apiszcz at solarrain.com>
> > Cc: r-help at stat.math.ethz.ch
> > Subject: Re: [R] prcomp help
> > 
> > Al Piszcz wrote:
> >> 
> >> Slackware 9.1, R 1.9.0, 2.54GHZ P4, 2GB RAM
> >> 
> >> example(prcomp) never finishes
> >> 
> >> 
> >>> example(prcomp)
> >> 
> >> 
> >> prcomp> data(USArrests)
> >> 
> >> prcomp> prcomp(USArrests)
> >> 
> >> 
> >> ====
> >> 
> >> The following test also appears to hang.
> >> 
> >>> a<-matrix(rnorm(100,mean=32,sd=31),10,10)
> >>> b<-prcomp(a)
> >
> > Works on Windows and several other OSs.
> >
> > Do you have a local copy of prcomp() (which is different from the original 
> > one)?
> >
> > What happens if you start R with --vanilla and try again?
> >
> > What happened after
> >  make check
> > during your R installation? make check should have reported an error here (or 
> > hang itself)....
> >
> > Uwe Ligges
> >
> >
> >
> >
> >> 
> >> 
> >> 
> >> ====
> >> 
> >> What is the recommended debug approach?
> >> 
> >> Thank you.
> >> 
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide! 
> >> http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From apiszcz at solarrain.com  Fri Jun  4 00:02:49 2004
From: apiszcz at solarrain.com (Al Piszcz)
Date: Thu, 3 Jun 2004 18:02:49 -0400 (EDT)
Subject: [R] prcomp help
In-Reply-To: <Pine.LNX.4.44.0406032300220.3493-100000@gannet.stats>
References: <Pine.LNX.4.44.0406032300220.3493-100000@gannet.stats>
Message-ID: <Pine.LNX.4.60.0406031801450.10548@l1>


Thanks.
This is now OBE, I downloaded the R-Patched version
from 30 May, added the VR bundle and am all set for now.



On Thu, 3 Jun 2004, Prof Brian Ripley wrote:

> Date: Thu, 3 Jun 2004 23:01:08 +0100 (BST)
> From: Prof Brian Ripley <ripley at stats.ox.ac.uk>
> To: Al Piszcz <apiszcz at solarrain.com>
> Cc: Uwe Ligges <ligges at statistik.uni-dortmund.de>, r-help at stat.math.ethz.ch
> Subject: Re: [R] prcomp help
> 
> That's because R-devel and R-patched have a workaround for a bug in the
> latest gcc: see the NEWS file.
>
> On Sun, 30 May 2004, Al Piszcz wrote:
>
>>
>> I downloaded and built the development version
>> of R dated 30 May. make check, and prcomp
>> were successful.
>>
>>
>>
>> On Sun, 30 May 2004, Uwe Ligges wrote:
>>
>>> Date: Sun, 30 May 2004 22:01:58 +0200
>>> From: Uwe Ligges <ligges at statistik.uni-dortmund.de>
>>> To: Al Piszcz <apiszcz at solarrain.com>
>>> Cc: r-help at stat.math.ethz.ch
>>> Subject: Re: [R] prcomp help
>>>
>>> Al Piszcz wrote:
>>>>
>>>> Slackware 9.1, R 1.9.0, 2.54GHZ P4, 2GB RAM
>>>>
>>>> example(prcomp) never finishes
>>>>
>>>>
>>>>> example(prcomp)
>>>>
>>>>
>>>> prcomp> data(USArrests)
>>>>
>>>> prcomp> prcomp(USArrests)
>>>>
>>>>
>>>> ====
>>>>
>>>> The following test also appears to hang.
>>>>
>>>>> a<-matrix(rnorm(100,mean=32,sd=31),10,10)
>>>>> b<-prcomp(a)
>>>
>>> Works on Windows and several other OSs.
>>>
>>> Do you have a local copy of prcomp() (which is different from the original
>>> one)?
>>>
>>> What happens if you start R with --vanilla and try again?
>>>
>>> What happened after
>>>  make check
>>> during your R installation? make check should have reported an error here (or
>>> hang itself)....
>>>
>>> Uwe Ligges
>>>
>>>
>>>
>>>
>>>>
>>>>
>>>>
>>>> ====
>>>>
>>>> What is the recommended debug approach?
>>>>
>>>> Thank you.
>>>>
>>>> ______________________________________________
>>>> R-help at stat.math.ethz.ch mailing list
>>>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>> PLEASE do read the posting guide!
>>>> http://www.R-project.org/posting-guide.html
>>>
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From Henrik.Bengtsson at matstat.lu.se  Fri Jun  4 00:56:13 2004
From: Henrik.Bengtsson at matstat.lu.se (Henrik Bengtsson)
Date: Fri, 4 Jun 2004 00:56:13 +0200
Subject: warnings and tryCatch() (Was: RE: [R] catching the warnings)
In-Reply-To: <40BF1CD9.9010207@jhsph.edu>
Message-ID: <003101c449bd$f96244b0$750040d5@hblaptop>

Seeing this question I was thinking of using tryCatch() to "catch" warnings.
Here is an example:

doWarn <- function() {
  a <<- 1
  warning("Wow!")
  a <<- 2
}

lastError <- lastWarning <- NULL
tryCatch({ 
  x <- 2 
  doWarn()
  x <- 3
  stop("Oops.")
  x <- 4
}, warning = function(warn) { 
  lastWarning <<- warn
}, error = function(err) { 
  lastError <<- err
})
stopifnot(a==1)
stopifnot(x==2)
stopifnot(inherits(lastWarning, "simpleWarning"))
stopifnot(is.null(lastError))

However, as the example shows, as soon as a warning is caught, tryCatch()
returns without evaluating the rest of the expressions. My question: is
there a simple way to continue the evaluation of expressions following the
expression that generated the condition? For instance, can I continue at "a
<<- 2" inside doWarn() by "doing something" within the warning handler? 

I have noticed withCallingHandlers():

lastError <- lastWarning <- NULL
withCallingHandlers({ 
  x <- 2 
  doWarn()
  x <- 3
  stop("Oops.")
  x <- 4
}, warning = function(warn) { 
  lastWarning <<- warn
}, error = function(err) { 
  lastError <<- err
})
stopifnot(a==2)
stopifnot(x==3)
stopifnot(inherits(lastWarning, "simpleWarning"))
stopifnot(inherits(lastError, "simpleError"))

Is the latter intended for what I am asking for? It will solve it for
warnings, but it will not allow to restart/continue after an error. Also, in
this case the error message is shown.

Cheers

Henrik Bengtsson


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Roger D. Peng
Sent: Thursday, June 03, 2004 2:43 PM
To: Marc Mamin
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] catching the warnings


The warnings are stored in a variable `last.warning' in the workspace. 
  warnings() simply prints this variable.

-roger

Marc Mamin wrote:
> Hello,
> 
> I'd like to catch the warnings in a variable in order to evaluate 
> them, but...
> 
> 
> 
>>tt<-warnings()
> 
> Warning messages:
> 1: XML Parsing Error: test.xml:2: xmlParseStartTag: invalid element 
> name
> 2: XML Parsing Error: test.xml:3: Extra content at the end of the document
> 
>>tt
> 
> NULL
> 
> is there a way to achieve this (R1.8.1)?
> 
> thanks,
> 
> Marc
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From MDavy at hortresearch.co.nz  Fri Jun  4 01:23:00 2004
From: MDavy at hortresearch.co.nz (Marcus Davy)
Date: Fri, 04 Jun 2004 11:23:00 +1200
Subject: [R] printing tabular data nicely
Message-ID: <s0c05bb1.038@hrp3.palm.cri.nz>


Hi,
If you choose to use LaTeX and xtable, you can also enhance the visual
presentation
of the output tables using the booktab LaTeX package. Have a look at:

http://www.ecs.soton.ac.uk/~srg/softwaretools/document/start/booktabs.pdf

You can change the horizontal line tags \hline to \toprule, \midrule,
and \bottomrule, at your discretion. 
There was also a recent thread about rownames quirks with xtable:

http://tolstoy.newcastle.edu.au/R/help/04/05/0678.html


marcus



>>> Duncan Murdoch <dmurdoch at pair.com> 4/06/2004 7:06:14 AM >>>
On Thu, 03 Jun 2004 10:19:26 -0700, wwsprague at ucdavis.edu wrote :

>Hi R-heplers,
>
>I would like to print various matrices, dataframes, tables, etc to 
>files, preferably nicely formatted postscript for import into papers.

>Is there a way to do this?
>
>I know ?cat, ?writeLines, ?format, ?paste.  But I am not sure of a
good 
>combination of these in order to get a nice looking table of
information.
>
>Any ideas?  I guess I want (almost) publication ready output, just
like 
>you get for "plot"...
>
>(I *don't* want to print to the console, btw)

As Sundar said, xtable is probably what you want.  You may also
benefit from the fairly new "addmargins" function.

Duncan Murdoch

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html

______________________________________________________

The contents of this e-mail are privileged and/or confidenti...{{dropped}}



From f.harrell at vanderbilt.edu  Fri Jun  4 01:42:29 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Thu, 03 Jun 2004 19:42:29 -0400
Subject: [R] printing tabular data nicely
In-Reply-To: <s0c05bb1.038@hrp3.palm.cri.nz>
References: <s0c05bb1.038@hrp3.palm.cri.nz>
Message-ID: <40BFB765.8020806@vanderbilt.edu>

Marcus Davy wrote:
> Hi,
> If you choose to use LaTeX and xtable, you can also enhance the visual
> presentation
> of the output tables using the booktab LaTeX package. Have a look at:
> 
> http://www.ecs.soton.ac.uk/~srg/softwaretools/document/start/booktabs.pdf
> 
> You can change the horizontal line tags \hline to \toprule, \midrule,
> and \bottomrule, at your discretion. 
> There was also a recent thread about rownames quirks with xtable:
> 
> http://tolstoy.newcastle.edu.au/R/help/04/05/0678.html
> 
> 
> marcus

The LaTeX ctable style, which uses booktab I believe, is slightly more 
up to date and has more options.  The latex function in the Hmisc 
package has a ctable option as well as a booktabs option.

Frank Harrell

> 
> 
> 
> 
>>>>Duncan Murdoch <dmurdoch at pair.com> 4/06/2004 7:06:14 AM >>>
> 
> On Thu, 03 Jun 2004 10:19:26 -0700, wwsprague at ucdavis.edu wrote :
> 
> 
>>Hi R-heplers,
>>
>>I would like to print various matrices, dataframes, tables, etc to 
>>files, preferably nicely formatted postscript for import into papers.
> 
> 
>>Is there a way to do this?
>>
>>I know ?cat, ?writeLines, ?format, ?paste.  But I am not sure of a
> 
> good 
> 
>>combination of these in order to get a nice looking table of
> 
> information.
> 
>>Any ideas?  I guess I want (almost) publication ready output, just
> 
> like 
> 
>>you get for "plot"...
>>
>>(I *don't* want to print to the console, btw)
> 
> 
> As Sundar said, xtable is probably what you want.  You may also
> benefit from the fairly new "addmargins" function.
> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help 
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________________
> 
> The contents of this e-mail are privileged and/or confidenti...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From andy_liaw at merck.com  Fri Jun  4 02:55:03 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 3 Jun 2004 20:55:03 -0400
Subject: [R] [OFF] program to estimate the best fit
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7E15@usrymx25.merck.com>

None that I know of, but having seen what Tablecurve could do (on DOS) more
than 11 years ago (when `data-mining' was still a dirty word), I don't think
that sort of analysis should be encouraged at all...

Andy

> From: Ronaldo Reis Jr.
> Sent: Thursday, June 03, 2004 4:29 PM
> To: R-Help
> Subject: [R] [OFF] program to estimate the best fit
> 
> 
> Hi,
> 
> exist in R or any other program for linux that estimate the 
> best fit for data 
> using severals functions? Somethink like tablecurve
> 
> Thanks
> Ronaldo
> -- 
> 	Um menino-prodigio e uma crianca cujos pais tem muita 
> 	imaginacao.
> 		-- Jean Costeau 
> --
> |>   // | \\   [***********************************]
> |   ( ??   ?? )  [Ronaldo Reis J??nior                ]
> |>      V      [UFV/DBA-Entomologia                ]
> |    /     \   [36571-000 Vi??osa - MG              ]
> |>  /(.''`.)\  [Fone: 31-3899-2532                 ]
> |  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
> |>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
> |    ( `-  )   [***********************************]
> |>>  _/   \_Powered by GNU/Debian Woody/Sarge
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From luke at stat.uiowa.edu  Fri Jun  4 03:06:39 2004
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Thu, 3 Jun 2004 20:06:39 -0500 (CDT)
Subject: warnings and tryCatch() (Was: RE: [R] catching the warnings)
In-Reply-To: <003101c449bd$f96244b0$750040d5@hblaptop>
Message-ID: <Pine.LNX.4.44.0406031942230.5919-100000@itasca2.stat.uiowa.edu>

The bits you need for collecting warnings are described on the
`warning' help page, though rather tersely.  The implementation of
suppressWarnings provides an example:
                                           
> suppressWarnings
function (expr)
{
    withCallingHandlers(expr, 
                        warning = function(w) invokeRestart("muffleWarning"))
}

This does two things:

    It uses withCallingHandlers to have the handler called from
    within the warning call rather than after a jump back to the context
    where the handler is established, as happens with tryCatch.

    Within the handler it invokes the 'muffleWarnings' restart to avoid
    continuing with the default handling of the warning (which would put
    it on the list that eventually gets printed).

If you want to write a function that computes a value and collects all
warning you could do it like this:

    withWarnings <- function(expr) {
	myWarnings <- NULL
	wHandler <- function(w) {
	    myWarnings <<- c(myWarnings, list(w))
	    invokeRestart("muffleWarning")
	}
	val <- withCallingHandlers(expr, warning = wHandler)
	list(value = val, warnings = myWarnings)
    }

This gives

    > withWarnings({warning("A");warning("B");1})
    $value
    [1] 1

    $warnings
    $warnings[[1]]
    <simpleWarning in withCallingHandlers(expr, warning = wHandler): A>

    $warnings[[2]]
    <simpleWarning in withCallingHandlers(expr, warning = wHandler): B>


On Fri, 4 Jun 2004, Henrik Bengtsson wrote:

> Seeing this question I was thinking of using tryCatch() to "catch" warnings.
> Here is an example:
> 
> doWarn <- function() {
>   a <<- 1
>   warning("Wow!")
>   a <<- 2
> }
> 
> lastError <- lastWarning <- NULL
> tryCatch({ 
>   x <- 2 
>   doWarn()
>   x <- 3
>   stop("Oops.")
>   x <- 4
> }, warning = function(warn) { 
>   lastWarning <<- warn
> }, error = function(err) { 
>   lastError <<- err
> })
> stopifnot(a==1)
> stopifnot(x==2)
> stopifnot(inherits(lastWarning, "simpleWarning"))
> stopifnot(is.null(lastError))
> 
> However, as the example shows, as soon as a warning is caught, tryCatch()
> returns without evaluating the rest of the expressions. My question: is
> there a simple way to continue the evaluation of expressions following the
> expression that generated the condition? For instance, can I continue at "a
> <<- 2" inside doWarn() by "doing something" within the warning handler? 

You can by using a calling handler rather than an exiting
one. tryCatch establishes exiting handlrs that are called after a jump
from the point where the condition is signaled to the point where the
handler is established, i.e. the tryCatch context.

> I have noticed withCallingHandlers():
> 
> lastError <- lastWarning <- NULL
> withCallingHandlers({ 
>   x <- 2 
>   doWarn()
>   x <- 3
>   stop("Oops.")
>   x <- 4
> }, warning = function(warn) { 
>   lastWarning <<- warn
> }, error = function(err) { 
>   lastError <<- err
> })
> stopifnot(a==2)
> stopifnot(x==3)
> stopifnot(inherits(lastWarning, "simpleWarning"))
> stopifnot(inherits(lastError, "simpleError"))
> 
> Is the latter intended for what I am asking for? It will solve it for
> warnings, but it will not allow to restart/continue after an error. Also, in
> this case the error message is shown.

If the calling handler returns then other available handlers, if any,
are tried and finally the default mechanism of the signaling function
is used.  For warnings the default handling involves storing the
message and then continuing; for errors signaled with stop or the
internal error function it usually involves printing a message and
jumping to top level.

Hope that helps.

luke

> Cheer
> 
> Henrik Bengtsson
> 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Roger D. Peng
> Sent: Thursday, June 03, 2004 2:43 PM
> To: Marc Mamin
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] catching the warnings
> 
> 
> The warnings are stored in a variable `last.warning' in the workspace. 
>   warnings() simply prints this variable.
> 
> -roger
> 
> Marc Mamin wrote:
> > Hello,
> > 
> > I'd like to catch the warnings in a variable in order to evaluate 
> > them, but...
> > 
> > 
> > 
> >>tt<-warnings()
> > 
> > Warning messages:
> > 1: XML Parsing Error: test.xml:2: xmlParseStartTag: invalid element 
> > name
> > 2: XML Parsing Error: test.xml:3: Extra content at the end of the document
> > 
> >>tt
> > 
> > NULL
> > 
> > is there a way to achieve this (R1.8.1)?
> > 
> > thanks,
> > 
> > Marc
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Luke Tierney
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
   Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu



From Matthias.Templ at statistik.gv.at  Fri Jun  4 09:16:39 2004
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Fri, 4 Jun 2004 09:16:39 +0200
Subject: [R] R CMD check, Windows XP, perl
Message-ID: <83536658864BC243BE3C06D7E936ABD501536929@xchg1.statistik.local>

Dear R users and developers,

I'm trying to build a package. The R CMD build works fine.

The R CMD check produces allways the following error:

  ..
  installing indices
Can`t locate File/Basename.pm in @ING (@ING contains: D:/../rw1090/share/perl d:/../rw1090/lib .) at D:/../rw1090/share/perl/build-help-windows.pl line 20. BEGIN failed--compilation aborted at D:/../rw1090/share/perl/build-help-windows.pl line 20.
Make[2]: *** [indices] Error 2
Make[1] *** [all] Error 2
Make: *** [pkg-myyfunction] Error 2
*** Installation of myyfunction failed ***

..

bulid-help-windows.pl at line 20 contains:     use File::Basename;

library(myyfunction) works, but I have no documentation for my function. The documentation in the man-directory contains \name, \alias, \title, \description and \keyword, which I have filled correctly.

I haven't found something to solve my problem in the writing R extensions manual or in the readme.package file.

Can anybody help me again please?

(I??m working with R 1.9.0 on Windows XP, Perl 5.8.3)

Thanks,
Matthias



From h0444k87 at student.hu-berlin.de  Fri Jun  4 09:59:52 2004
From: h0444k87 at student.hu-berlin.de (h0444k87@student.hu-berlin.de)
Date: Fri, 4 Jun 2004 09:59:52 MET
Subject: [R] rpart
Message-ID: <200406040759.i547xqbp011536@nsuncom.rz.hu-berlin.de>

Hello everyone,

I'm a newbie to R and to CART so I hope my questions don't seem too stupid.

1.)
My first question concerns the rpart() method. Which method does rpart use in
order to get the best split - entropy impurity, Bayes error (min. error) or Gini
index? Is there a way to make it use the entropy impurity?

The second and third question concern the output of the printcp() function.
2.)
What exactly are the cps in that sense here? I assumed them to be the treshold
complexity parameters as in Breiman et al., 1998, Section 3.3? Are they the same
as the treshold niveaus of alpha? I have read somewhere that the cps here are
the  treshold alphas divided by the root node error. Is that true?

3.)
How is rel error computed?
I am supposed to evaluate the goodness of classification of of the CART method.
Do you think rel error is a good measure for that?

I'd be very thankful if anyone could give me hand on that. This is a project for
uni and I desperately need a good mark.

Thank you very much in advance,

Mareike



From k.wang at auckland.ac.nz  Fri Jun  4 10:07:45 2004
From: k.wang at auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Fri, 4 Jun 2004 20:07:45 +1200
Subject: [R] rpart
References: <200406040759.i547xqbp011536@nsuncom.rz.hu-berlin.de>
Message-ID: <002c01c44a0b$04989260$6433d882@stat.auckland.ac.nz>

Hi,

I think most, if not all, your questions can be answered by:

1) ?rpart

2) Some search through the r-help mailing list

3) Read the chapter on tree-based models in MASS 4 (Modern Applied
Statistics with S) by Venables and Ripley

Kevin

----- Original Message ----- 
From: <h0444k87 at student.hu-berlin.de>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, June 04, 2004 9:59 PM
Subject: [R] rpart


> Hello everyone,
>
> I'm a newbie to R and to CART so I hope my questions don't seem too
stupid.
>
> 1.)
> My first question concerns the rpart() method. Which method does rpart use
in
> order to get the best split - entropy impurity, Bayes error (min. error)
or Gini
> index? Is there a way to make it use the entropy impurity?
>
> The second and third question concern the output of the printcp()
function.
> 2.)
> What exactly are the cps in that sense here? I assumed them to be the
treshold
> complexity parameters as in Breiman et al., 1998, Section 3.3? Are they
the same
> as the treshold niveaus of alpha? I have read somewhere that the cps here
are
> the  treshold alphas divided by the root node error. Is that true?
>
> 3.)
> How is rel error computed?
> I am supposed to evaluate the goodness of classification of of the CART
method.
> Do you think rel error is a good measure for that?
>
> I'd be very thankful if anyone could give me hand on that. This is a
project for
> uni and I desperately need a good mark.
>
> Thank you very much in advance,
>
> Mareike
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>



From maechler at stat.math.ethz.ch  Fri Jun  4 10:30:29 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Fri, 4 Jun 2004 10:30:29 +0200
Subject: [R] ecdf plots, lines, and y values
In-Reply-To: <40BF7DD0.8020107@statistik.uni-dortmund.de>
References: <E1BVwBu-0003qR-Lm@serenity.mcc.ac.uk>
	<40BF7DD0.8020107@statistik.uni-dortmund.de>
Message-ID: <16576.13093.410292.898656@gargle.gargle.HOWL>

>>>>> "UweL" == Uwe Ligges <ligges at statistik.uni-dortmund.de>
>>>>>     on Thu, 03 Jun 2004 21:36:48 +0200 writes:

    UweL> Monica Palaseanu-Lovejoy wrote:
    >> Hi,
    >> 
    >> I have a question for the group, perhaps someone can help me 
    >> figure this out.  I've already looked in the help files and they were 
    >> no help to me.  
    >> 
    >> I have a vector of values and I am plotting an ecdf graph. 
    >> 
    >> 1. How can i draw a continuous line through the ecdf points? (lines 
    >> and type for the plot with an ecdf object does not work)

    UweL> Well, the help page ?plot.ecdf and ?plot.stepfun is pretty clear and 
    UweL> tells us to use verticals = TRUE.
    UweL> From the help page:

    UweL> F10 <- ecdf(rnorm(10))
    UweL> plot(F10, verticals = TRUE, do.p = FALSE)

Thank you Uwe,
but maybe Monica doesn't want to really plot the ecdf, but
rather a "continuized" {polyline} version of it ?

That's a bit less obvious to do and can be achieved by noting
that the (x,y) values are the (x,y) vectors in F10's
environment and that plot() or lines() on a list uses the list's
$x and $y components, e.g.,

 F10 <- ecdf(rnorm(10))
 plot(F10, verticals = TRUE, do.p = FALSE)
 lines(as.list(environment(F10)), col = "blue")

But even more nicely:

 with(environment(F10), lines(x,y,  col = "blue"))

This last trick can also be applied for the following question:

    >> 2. Supposing I have this line drawn. I can add a vertical line of a 
    >> known x value which intersects the graph. How can I determine the 
    >> y value of the graph point that is intersected by the abline? 

## Construct a "linear interpolator" using approxfun:
polyF10 <- with(environment(F10), approxfun(x,y))

## 'test' it graphically, using curve():
plot(F10, verticals = TRUE, do.p = FALSE)
curve(polyF10(x), add = TRUE, col="red")

## now use it, e.g. for two abscissa values simultaneously

 x0 <- c(0, 0.5)
 y0 <- polyF10(x0)

 x.min <- with(environment(F10), min(x))
## or rather simply
 x.min <- par("usr")[1]

 segments(x0,    0, x0,y0, lty=3)
 segments(x.min,y0, x0,y0, lty=3)


This seems a pretty neat example of making use of the function
objects returned from ecdf() and approxfun().

Regards,  Martin Maechler



From simon at stats.gla.ac.uk  Fri Jun  4 10:47:45 2004
From: simon at stats.gla.ac.uk (Simon Wood)
Date: Fri, 4 Jun 2004 09:47:45 +0100 (BST)
Subject: [R] GAM question
In-Reply-To: <Pine.SOL.4.58.0406031041170.17406@lehman.scsr.nevada.edu>
References: <Pine.SOL.4.58.0406031041170.17406@lehman.scsr.nevada.edu>
Message-ID: <Pine.SOL.4.58.0406040939370.13861@moon.stats.gla.ac.uk>


> Warning in eval(expr, envir, enclos) : non-integer #successes in a
> binomial glm!

- one way of specifying a logistic regression model is to supply the
observed proportion of sucesses as the response variable (e.g. y) and the
binomial n as the weights. The warning is complaining that y/n is
non-integer. Depending on exactly why you are weighting, you might want to
use the quasibinomial family in place of binomial,

> Error: cannot allocate vector of size 60865 Kb

The gam fit may get a bit memory intensive given the number of data you
have. ?gam gives various approaches for dealing with large datasets, but
you might want to change the smoothing basis to one that is
computationally cheaper  than the default.

eg. replace s(x) terms by s(x,bs="cr").

Simon

On Thu, 3 Jun 2004, HILLARY  ROBISON wrote:

> I am trying to use R to do a weighted GAM with PA (presence/random) as the
> response variable (Y, which is a 0 or a 1) and ASPECT (values go from
> 0-3340), DEM (from 1500-3300), HLI (from 0-5566), PLAN (from -3 to 3),
> PROF (from -3 to 3), SLOPE (from 100-500) and TRI (from 0-51) as
> predictor variables (Xs).  I need to weight each observation by its WO
> value (from 0.18 to 0.98).  I have specified the following models in R
> (see below), but I can't figure out what the R reported errors plainly
> mean. One of the errors seems to tell me my dataset is too big (it's
> 109,729 rows by 16 columns) - is this possible?  Given what I am trying to
> accomplish (a weighted, logistic GAM with 7 variables), am I specifying my
> model correctly?  I would like to attach my dataset (it's 2,064 KB
> as a WinZip file), but I don't know if it'll go through to the list given
> the HTML & attachment contraints of the list...  I even tried a weighted,
> logistic GLM with the seven variables to see if that would work and if so,
> perhaps it was a GAM problem.  I also tried a logistic, weighted GAM with
> one variable to see if that would work.  My next step while I wait to
> hear back from the list is to try a dummy dataset that is small to see if
> a weighted, logistic GAM with seven variables will work at all or if I am
> speciying the model correctly.  Would anyone be willing to have my dataset
> sent so they can check it out if that would help solve the issue?  Thank you!
> Hillary (hrobison at unr.nevada.edu)
>
> > # trial, all, weighted
> > topo8 <- gam(PA ~ s(SLOPE10) + s(ASPECT10) + s(GYADEMPLUS) + s(TRI) +
> s(HLI) + s(PLAN10) + s(PROF10), family=binomial, data=topox, weights = w0)
> Warning in eval(expr, envir, enclos) : non-integer #successes in a
> binomial glm!
> Error: cannot allocate vector of size 60865 Kb
>
> > topo9 <- glm(PA ~ SLOPE10 + ASPECT10 + GYADEMPLUS + TRI + HLI + PLAN10 +
> PROF10, family=binomial, data=topox, weights = w0)
> Warning in eval(expr, envir, enclos) : non-integer #successes in a
> binomial glm!
>
> > # trial, weighted, slope only
> > topo10 <- gam(PA ~ s(SLOPE10), family=binomial, data=topox, weights =
> w0)
> Warning in eval(expr, envir, enclos) : non-integer #successes in a
> binomial glm!
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From e.pebesma at geog.uu.nl  Fri Jun  4 12:39:29 2004
From: e.pebesma at geog.uu.nl (Edzer J. Pebesma)
Date: Fri, 04 Jun 2004 12:39:29 +0200
Subject: [R] Simulating a landscape (matrix) in R
Message-ID: <40C05161.1070005@geog.uu.nl>

For creating larger "landscapes" (Gaussian random fields), you may
consider using package gstat, which uses the sequential Gaussian simulation
algorithm with local approximations. In the example below, only the nearest
20 neighbours are used to approximate each of the conditional distributions.

The example below which simulates & plots a 300x300 grid, took 1 minute
on a 2 GHz CPU.

Best regards,
--
Edzer

library(gstat)
g = gstat(NULL, "id0", z~1, ~x+y, dummy=TRUE,nmax=20,
        model=vgm(1,"Exp",500),beta=10)
locs=expand.grid(x=1:300,y=1:300)
out <- predict.gstat(g, locs, nsim=1)
levelplot(sim1~x+y,out,col.regions=bpy.colors())



From danbebber at forestecology.co.uk  Fri Jun  4 13:46:08 2004
From: danbebber at forestecology.co.uk (Dan Bebber)
Date: Fri, 4 Jun 2004 12:46:08 +0100
Subject: [R] Error() term in glm model formula
Message-ID: <000401c44a29$86acc280$442501a3@plants.ox.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040604/c15027d3/attachment.pl

From k.wang at auckland.ac.nz  Fri Jun  4 13:50:32 2004
From: k.wang at auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Fri, 4 Jun 2004 23:50:32 +1200
Subject: [R] How to Describe R to Finance People
In-Reply-To: <20040603111112.FSHM12806.web4-rme.xtra.co.nz@kevinlpt>
Message-ID: <20040604115038.NQWW12806.web4-rme.xtra.co.nz@kevinlpt>

Hi,

I've been doing a joint research with someone from the Property
Department here and she is about to give a presentation on the
results.  The audience will include people from Property and Finance,
and she is wondering how to describe R to these people (as I used R to
do the analyses), since she has never even heard of R before our joint
research (and has been using SPSS).  The difficult part is she has
only about 1 ~ 2 minutes to talk about R.

The following is what I have in mind, any suggestions from people in
Finance will be greatly appreciated!  (From our research together I
think it may be safe to assume the audience will know, or at least
have heard of, basic statistical terminology such as multiple linear
regression and dummy variables).

\begin{quote}
R was originally developed by Dr. Ross Ihaka and Dr. Robert Gentleman
from the Department of Statistics at the University of Auckland in
1992.  It is free and in the last decade it has evolved into one of
the most powerful statistical software, with over 150 user-contributed
add-on packages.  It is not only used by statisticians or scientists,
but also econometricians and people in finance due to its cost (FREE)
and its powerfulness.

Although it has a slightly higher learning curve than SPSS-like
program, it gets easier to use once one is familiar with it.  One of
the main advantage it has over SPSS-like software is that you do not
need to explicitly create dummy variables.  You only need to specify
your dependent variable and independent variables and R will fit it
(and create dummy variables automatically) for you.

It also has many state-of-art free resources, including manuals,
contributed tutorials and documentations, online.  A free mailing list
is also available for people to ask questions and questions are
usually answered by more experienced users around the world within a
few hours (sometimes even within minutes).
\end{quote}

As mentioned above, she was rather impressed when I mention that one
does not need to create dummy variables in R.  Therefore I am thinking
she might be interested in mentioning it in her talk.

I have never had experience of trying to introduce R to
non-Scientists, hence I would appreciate any comments!

Cheers,

Kevin

--------------------------------------------
Ko-Kang Kevin Wang, MSc(Hon)
SLC Stats Workshops Co-ordinator
The University of Auckland
New Zealand



From Carsten.Colombier at efv.admin.ch  Fri Jun  4 14:06:51 2004
From: Carsten.Colombier at efv.admin.ch (Carsten.Colombier@efv.admin.ch)
Date: Fri, 4 Jun 2004 14:06:51 +0200 
Subject: [R] Export r-output. e.g. regression results, in text file
Message-ID: <2CAE512CEB72EE448AADE3444E1FB7185B47EF@ad04mexefd3.ad.admin.ch>

Dear r-help-team,

do you know if there is, except copy and paste, a possibility to export an
r-output of commands like summary(), e.g. a regression output, in a text
file?

Best regards,
Carsten

Dr. Carsten Colombier
Economist
Swiss Federal Finance Administration FFA
Group of Economic Advisers
Bundesgasse 3
CH-3003 Bern
Switzerland

email  carsten.colombier at efv.admin.ch
phone 0041-31-3226332
fax     0041-31-3230833



From prechelt at pcpool.mi.fu-berlin.de  Fri Jun  4 14:06:26 2004
From: prechelt at pcpool.mi.fu-berlin.de (Lutz Prechelt)
Date: Fri, 4 Jun 2004 14:06:26 +0200
Subject: [R] How to Describe R to Finance People
Message-ID: <85D25331FFB7AE4C900EA467D4ADA3920459B3@circle.pcpool.mi.fu-berlin.de>

> the main advantage it has over SPSS-like software is that you do not
> need to explicitly create dummy variables.  You only need to specify
> your dependent variable and independent variables and R will fit it
> (and create dummy variables automatically) for you.

Does the audience know exactly what the creation of dummy variables
in SPSS is and means?
If not, they might consider this geek talk.

Why don't you just go for a concrete example?:
Show an SPSS operation sequence and the equivalent R
expressions side-by-side.
No need to explain much, then.

  Lutz



From ripley at stats.ox.ac.uk  Fri Jun  4 14:06:38 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 4 Jun 2004 13:06:38 +0100 (BST)
Subject: [R] R CMD check, Windows XP, perl
In-Reply-To: <83536658864BC243BE3C06D7E936ABD501536929@xchg1.statistik.local>
Message-ID: <Pine.LNX.4.44.0406041302390.24876-100000@gannet.stats>

File::Basename is part of Perl.  Your @ING looks wrong (it contains no
Perl system directories).  Do you have any Perl environmental variables
such as PERL5LIB set?

On Fri, 4 Jun 2004, TEMPL Matthias wrote:

> Dear R users and developers,
> 
> I'm trying to build a package. The R CMD build works fine.
> 
> The R CMD check produces allways the following error:
> 
>   ..
>   installing indices
> Can`t locate File/Basename.pm in @ING (@ING contains: D:/../rw1090/share/perl
 d:/../rw1090/lib .) at D:/../rw1090/share/perl/build-help-windows.pl line 20. 
BEGIN failed--compilation aborted at D:/../rw1090/share/perl/build-help-windows.pl line 20.
> Make[2]: *** [indices] Error 2
> Make[1] *** [all] Error 2
> Make: *** [pkg-myyfunction] Error 2
> *** Installation of myyfunction failed ***
> 
> ..
> 
> bulid-help-windows.pl at line 20 contains:     use File::Basename;
> 
> library(myyfunction) works, but I have no documentation for my function.
> The documentation in the man-directory contains \name, \alias, \title,
> \description and \keyword, which I have filled correctly.
> 
> I haven't found something to solve my problem in the writing R
> extensions manual or in the readme.package file.

It's a problem with your Perl installation, so try instead your Perl 
documentation.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Jun  4 14:07:27 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 4 Jun 2004 13:07:27 +0100 (BST)
Subject: [R] Export r-output. e.g. regression results, in text file
In-Reply-To: <2CAE512CEB72EE448AADE3444E1FB7185B47EF@ad04mexefd3.ad.admin.ch>
Message-ID: <Pine.LNX.4.44.0406041306570.24876-100000@gannet.stats>

On Fri, 4 Jun 2004 Carsten.Colombier at efv.admin.ch wrote:

> do you know if there is, except copy and paste, a possibility to export an
> r-output of commands like summary(), e.g. a regression output, in a text
> file?

?sink

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From B.Rowlingson at lancaster.ac.uk  Fri Jun  4 14:22:31 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 04 Jun 2004 13:22:31 +0100
Subject: [R] How to Describe R to Finance People
In-Reply-To: <20040604115038.NQWW12806.web4-rme.xtra.co.nz@kevinlpt>
References: <20040604115038.NQWW12806.web4-rme.xtra.co.nz@kevinlpt>
Message-ID: <40C06987.3000102@lancaster.ac.uk>

Ko-Kang Kevin Wang wrote:

> It is not only used by statisticians or scientists,
> but also econometricians and people in finance due to its cost (FREE)
> and its powerfulness.

  I think 'power' is preferable here to 'powerfulness'! Never use a big 
word when a diminutive one will do.

> Although it has a slightly higher learning curve than SPSS-like

  We're usually more concerned with the first derivative of the learning 
curve than its intercept. Better to say R has a steeper initial learning 
curve. Maybe plot one out in R?

> I have never had experience of trying to introduce R to
> non-Scientists, hence I would appreciate any comments!

  I'd stress how having a programming language as part of your stats 
package encourages free exploration of your data. You're not constrained 
to the dialogs and buttons and menus of your average stats package.

  You could also stress the other part of the 'free' aspect of R, in 
that you are free to use and abuse it almost at will, and you'll never 
be in a position where you can't run some version of R. Yes, I've had a 
user this morning want to recover something from an Splus .Data directory...

Enjoy spreading the word,

Barry



From mikewhite.diu at tiscali.co.uk  Fri Jun  4 14:36:05 2004
From: mikewhite.diu at tiscali.co.uk (Mike White)
Date: Fri, 4 Jun 2004 13:36:05 +0100
Subject: [R] Ward clustering problem
Message-ID: <001c01c44a30$81464c10$8158e150@FSSFQCV7BGDVED>

I have a training set of data for known classes with 5 observations of 12
variables for each class.  I want to use this information to classify new
data into classes which are known to be different to those in the training
set but each new class may contain one or more observations.  The
distribution of within class distances is expected to be similar for all
classes and this is found to be the case for the training data.  I have
tried using the maximum within class distance for the training data to set
the h variable in cutree for the clustered new data.  This appears to work
fine for "average" and "complete" clustering methods but not for the Ward
clustering method as the distance axis of the dendrogram does not directly
relate to the distances between observations.

Can anyone advise on how to optimise the h value of cutree when using the
Ward clustering method or is there a better approach to this type of
classification problem?

Thanks
Mike White



From f.harrell at vanderbilt.edu  Fri Jun  4 14:44:01 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 04 Jun 2004 08:44:01 -0400
Subject: [R] How to Describe R to Finance People
In-Reply-To: <20040604115038.NQWW12806.web4-rme.xtra.co.nz@kevinlpt>
References: <20040604115038.NQWW12806.web4-rme.xtra.co.nz@kevinlpt>
Message-ID: <40C06E91.50302@vanderbilt.edu>

Ko-Kang Kevin Wang wrote:
> Hi,
> 
> I've been doing a joint research with someone from the Property
> Department here and she is about to give a presentation on the
> results.  The audience will include people from Property and Finance,
> and she is wondering how to describe R to these people (as I used R to
> do the analyses), since she has never even heard of R before our joint
> research (and has been using SPSS).  The difficult part is she has
> only about 1 ~ 2 minutes to talk about R.
> 
> The following is what I have in mind, any suggestions from people in
> Finance will be greatly appreciated!  (From our research together I
> think it may be safe to assume the audience will know, or at least
> have heard of, basic statistical terminology such as multiple linear
> regression and dummy variables).
> 
> \begin{quote}
> R was originally developed by Dr. Ross Ihaka and Dr. Robert Gentleman
> from the Department of Statistics at the University of Auckland in
> 1992.  It is free and in the last decade it has evolved into one of
> the most powerful statistical software, with over 150 user-contributed
> add-on packages.  It is not only used by statisticians or scientists,
> but also econometricians and people in finance due to its cost (FREE)
> and its powerfulness.

You might say that many professional statisticians consider R to be the 
premier statistical software system.

> 
> Although it has a slightly higher learning curve than SPSS-like
> program, it gets easier to use once one is familiar with it.  One of
> the main advantage it has over SPSS-like software is that you do not
> need to explicitly create dummy variables.  You only need to specify
> your dependent variable and independent variables and R will fit it
> (and create dummy variables automatically) for you.

You might mention that transformations may be specified on the fly in 
general, e.g. sqrt(y) ~ x1 + log(x2) in any model.

Sometimes in a presentation (when more time is available) I spend a few 
minutes touting the advantages of command languages even for things in 
which most people think that a WYSIWYG interface is superior.  A good 
example of this is graphviz for drawing diagrams.  I cover this in 
http://biostat.mc.vanderbilt.edu/twiki/pub/Main/StatCompCourse/sCompGraph.pdf.

This relates to reproducible analysis which is also surveyed in those notes.

Frank Harrell

> 
> It also has many state-of-art free resources, including manuals,
> contributed tutorials and documentations, online.  A free mailing list
> is also available for people to ask questions and questions are
> usually answered by more experienced users around the world within a
> few hours (sometimes even within minutes).
> \end{quote}
> 
> As mentioned above, she was rather impressed when I mention that one
> does not need to create dummy variables in R.  Therefore I am thinking
> she might be interested in mentioning it in her talk.
> 
> I have never had experience of trying to introduce R to
> non-Scientists, hence I would appreciate any comments!
> 
> Cheers,
> 
> Kevin
> 
> --------------------------------------------
> Ko-Kang Kevin Wang, MSc(Hon)
> SLC Stats Workshops Co-ordinator
> The University of Auckland
> New Zealand
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From baron at psych.upenn.edu  Fri Jun  4 14:47:37 2004
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Fri, 4 Jun 2004 08:47:37 -0400
Subject: [R] How to Describe R to Finance People
In-Reply-To: <40C06987.3000102@lancaster.ac.uk>
References: <20040604115038.NQWW12806.web4-rme.xtra.co.nz@kevinlpt>
	<40C06987.3000102@lancaster.ac.uk>
Message-ID: <20040604124737.GB4399@psych>

On 06/04/04 13:22, Barry Rowlingson wrote:
>> Although it has a slightly higher learning curve than SPSS-like
>
>  We're usually more concerned with the first derivative of the learning
>curve than its intercept. Better to say R has a steeper initial learning
>curve. Maybe plot one out in R?

According to my introductory psychology text (Woodworth and
Schlossberg's "Experimental Psychology, Revised Edition" [1954],
ch. 18), a learning curve is a plot of (some measure of) amount
learned as a function of learning trials.  A steep initial slope
therefore implies fast learning.  I wish it were true, but I fear
the opposite is intended.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron
R search page:        http://finzi.psych.upenn.edu/



From u4065696 at anu.edu.au  Fri Jun  4 14:50:10 2004
From: u4065696 at anu.edu.au (Karmei Tang)
Date: Fri, 4 Jun 2004 22:50:10 +1000
Subject: [R] S+ code for stochastic volatility models
Message-ID: <000201c44a32$7c085e80$deeccb96@SNNCI7104507>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040604/8c22df25/attachment.pl

From v_bill_pikounis at merck.com  Fri Jun  4 14:59:32 2004
From: v_bill_pikounis at merck.com (Pikounis, Bill)
Date: Fri, 4 Jun 2004 08:59:32 -0400
Subject: [R] How to Describe R to Finance People
Message-ID: <CFBD404F5E0C9547B4E10B7BDC3DFA2F041563AB@usrymx18.merck.com>

Hi Kevin,
I think your \begin{quote}...\end{quote} block covers it nicely.  Some
generic remarks to consider:

* If the presenter is using slides, distill the description into 1 slide
(for the 1-2 minutes).

* Of course Edward Tufte would not approve, but I think three main bullet
points are useful, with some sub-items that could be explicit or described
verbally:

	* R is free (as in beer, wine, etc.)

	* There is a strong, growing R community 
		* Useful documentation, and 
		* Help is always a sincere, intelligent email posting away

      * (Choose an authentic example, such as what you described with the
SPSS dummy coding)

By "authentic", I mean something you feel much of the audience will relate
to.  That could be specific to data analysis and/or interpretation in
Finance, or just a more convenient/efficient way to do tedious tasks, like
good computer software should do.  The "plot" example I just now see cited
by Barry Rowlingson is a good one, I think.

The first two items derive from the principle of open source software, and
it could be pointed out that the Earth now has several examples of where the
open-source model has been indisputably successful, based on the facts.  And
such examples are not just for "geeks" anymore; note for example OpenOffice
and KDE.

Hope that helps and good luck,
Bill


----------------------------------------
Bill Pikounis, Ph.D.

Biometrics Research Department
Merck Research Laboratories
PO Box 2000, MailDrop RY33-300  
126 E. Lincoln Avenue
Rahway, New Jersey 07065-0900
USA

Phone: 732 594 3913
Fax: 732 594 1565


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of 
> Ko-Kang Kevin Wang
> Sent: Friday, June 04, 2004 7:51 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] How to Describe R to Finance People
> 
> 
> Hi,
> 
> I've been doing a joint research with someone from the Property
> Department here and she is about to give a presentation on the
> results.  The audience will include people from Property and Finance,
> and she is wondering how to describe R to these people (as I used R to
> do the analyses), since she has never even heard of R before our joint
> research (and has been using SPSS).  The difficult part is she has
> only about 1 ~ 2 minutes to talk about R.
> 
> The following is what I have in mind, any suggestions from people in
> Finance will be greatly appreciated!  (From our research together I
> think it may be safe to assume the audience will know, or at least
> have heard of, basic statistical terminology such as multiple linear
> regression and dummy variables).
> 
> \begin{quote}
> R was originally developed by Dr. Ross Ihaka and Dr. Robert Gentleman
> from the Department of Statistics at the University of Auckland in
> 1992.  It is free and in the last decade it has evolved into one of
> the most powerful statistical software, with over 150 user-contributed
> add-on packages.  It is not only used by statisticians or scientists,
> but also econometricians and people in finance due to its cost (FREE)
> and its powerfulness.
> 
> Although it has a slightly higher learning curve than SPSS-like
> program, it gets easier to use once one is familiar with it.  One of
> the main advantage it has over SPSS-like software is that you do not
> need to explicitly create dummy variables.  You only need to specify
> your dependent variable and independent variables and R will fit it
> (and create dummy variables automatically) for you.
> 
> It also has many state-of-art free resources, including manuals,
> contributed tutorials and documentations, online.  A free mailing list
> is also available for people to ask questions and questions are
> usually answered by more experienced users around the world within a
> few hours (sometimes even within minutes).
> \end{quote}
> 
> As mentioned above, she was rather impressed when I mention that one
> does not need to create dummy variables in R.  Therefore I am thinking
> she might be interested in mentioning it in her talk.
> 
> I have never had experience of trying to introduce R to
> non-Scientists, hence I would appreciate any comments!
> 
> Cheers,
> 
> Kevin
> 
> --------------------------------------------
> Ko-Kang Kevin Wang, MSc(Hon)
> SLC Stats Workshops Co-ordinator
> The University of Auckland
> New Zealand
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From pgilbert at bank-banque-canada.ca  Fri Jun  4 16:19:05 2004
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Fri, 04 Jun 2004 10:19:05 -0400
Subject: [R] How to Describe R to Finance People
In-Reply-To: <20040604115038.NQWW12806.web4-rme.xtra.co.nz@kevinlpt>
References: <20040604115038.NQWW12806.web4-rme.xtra.co.nz@kevinlpt>
Message-ID: <40C084D9.9070502@bank-banque-canada.ca>

Ko-Kang Kevin Wang wrote:

>It is not only used by statisticians or scientists,
>but also econometricians and people in finance due to its cost (FREE)
>and its powerfulness.
>
I think "(FREE)" will distract your intended audience from the real 
point. In a corporate environment, lots of people argue that free 
software actually costs more than commercial software because of 
internal support cost, etc, etc.  These arguments will all hinge 
critically on the corporate IT support abilities. For R, I have never 
seen a convincing argument that it costs more, but the real point is 
that this is irrelevant. If it costs less, that is nice.  If it costs 
more, then that is what you pay to use something that is better. If they 
need to, I think people in finance are generally willing to pay, so I 
think it is a mistake to put much emphasis on the cost. Put the emphasis 
on how good it is.

Paul Gilbert
Head Statistician/Statisticien en chef,
Department of Monetary and Financial Analysis,
     /D??partement des ??tudes mon??taires et financiers,
Bank of Canada/Banque du Canada
234 Wellington St.,
Ottawa,
Canada K1A 0G9



From edd at debian.org  Fri Jun  4 16:27:42 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 4 Jun 2004 09:27:42 -0500
Subject: [R] How to Describe R to Finance People
In-Reply-To: <20040604115038.NQWW12806.web4-rme.xtra.co.nz@kevinlpt>
References: <20040603111112.FSHM12806.web4-rme.xtra.co.nz@kevinlpt>
	<20040604115038.NQWW12806.web4-rme.xtra.co.nz@kevinlpt>
Message-ID: <20040604142742.GA28844@sonny.eddelbuettel.com>


Kevin,

This is a nice start. I'll Cc this back to the recently formed 'R in
Finance' list we formed as a result of the two Finance sessions at use useR.

One small correction would be that the number of CRAN packages is now in
excess of 350 (not 150). 

And a point you may want to add is that the S language has been used in some
geek corners of Finance for decades; the proximity of Bell Labs to Wall St
may have something to do with that, as does of course the power and elegance
of the language.

S-Plus also offers some fancypants add-on modules, and we'll try hard over
the next few months and years to catch up. A major help in that regard is
Diethelm Wuertz' most excellent Rmetrics collection at

	http://www.rmetrics.org

This is currently windows-only, but Diethelm and I are committed to getting
it into Debian (and my Quantian) so that general Unix/Linux use should fall
out as a side benefit.

Hope this helps, Dirk

On Fri, Jun 04, 2004 at 11:50:32PM +1200, Ko-Kang Kevin Wang wrote:
> Hi,
> 
> I've been doing a joint research with someone from the Property
> Department here and she is about to give a presentation on the
> results.  The audience will include people from Property and Finance,
> and she is wondering how to describe R to these people (as I used R to
> do the analyses), since she has never even heard of R before our joint
> research (and has been using SPSS).  The difficult part is she has
> only about 1 ~ 2 minutes to talk about R.
> 
> The following is what I have in mind, any suggestions from people in
> Finance will be greatly appreciated!  (From our research together I
> think it may be safe to assume the audience will know, or at least
> have heard of, basic statistical terminology such as multiple linear
> regression and dummy variables).
> 
> \begin{quote}
> R was originally developed by Dr. Ross Ihaka and Dr. Robert Gentleman
> from the Department of Statistics at the University of Auckland in
> 1992.  It is free and in the last decade it has evolved into one of
> the most powerful statistical software, with over 150 user-contributed
> add-on packages.  It is not only used by statisticians or scientists,
> but also econometricians and people in finance due to its cost (FREE)
> and its powerfulness.
> 
> Although it has a slightly higher learning curve than SPSS-like
> program, it gets easier to use once one is familiar with it.  One of
> the main advantage it has over SPSS-like software is that you do not
> need to explicitly create dummy variables.  You only need to specify
> your dependent variable and independent variables and R will fit it
> (and create dummy variables automatically) for you.
> 
> It also has many state-of-art free resources, including manuals,
> contributed tutorials and documentations, online.  A free mailing list
> is also available for people to ask questions and questions are
> usually answered by more experienced users around the world within a
> few hours (sometimes even within minutes).
> \end{quote}
> 
> As mentioned above, she was rather impressed when I mention that one
> does not need to create dummy variables in R.  Therefore I am thinking
> she might be interested in mentioning it in her talk.
> 
> I have never had experience of trying to introduce R to
> non-Scientists, hence I would appreciate any comments!
> 
> Cheers,
> 
> Kevin
> 
> --------------------------------------------
> Ko-Kang Kevin Wang, MSc(Hon)
> SLC Stats Workshops Co-ordinator
> The University of Auckland
> New Zealand
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
FEATURE:  VW Beetle license plate in California



From amurta at ipimar.pt  Fri Jun  4 17:39:47 2004
From: amurta at ipimar.pt (Alberto Murta)
Date: Fri, 4 Jun 2004 15:39:47 +0000
Subject: [R] Confidence intervals for predicted values in nls
In-Reply-To: <Pine.LNX.4.44.0406032216050.3493-100000@gannet.stats>
References: <Pine.LNX.4.44.0406032216050.3493-100000@gannet.stats>
Message-ID: <200406041539.47142.amurta@ipimar.pt>

Parece que n??o tiveste grandes ajudas. Eu se fosse a ti fazia isso com um 
bootstrap bem planeado

Alberto


On Thursday 03 June 2004 21:22, Prof Brian Ripley wrote:
> On Thu, 3 Jun 2004, Cristina Silva wrote:
> > I have tried to estimate the confidence intervals for predicted values of
> > a nonlinear model fitted with nls. The function predict gives the
> > predicted values and the lower and upper limits of the prediction, when
> > the class of the object is lm or glm. When the object is derived from
> > nls, the function predict (or predict.nls) gives only the predicted
> > values. The se.fit and interval aguments are just ignored.
>
> Thre are no such arguments either to the generic function nls() nor its
> "nls" method.  Please do read the documentation!
>
> > Could anybody tell me how to estimate the confidence intervals for the
> > predicted values (not the model parameters), using an object of class
> > nls?
>
> First you need to understand how to do this in theory: thereafter it is a
> programming task.  Hint: to find a confidence region for the parameters is
> not at all easy, as the examples in MASS (the book) and elsewhere show,
> and there is no guarantee that the confidence region for the prediction
> will be a single interval.

-- 
                                         Alberto G. Murta
Institute for Agriculture and Fisheries Research (INIAP-IPIMAR) 
Av. Brasilia, 1449-006 Lisboa, Portugal | Phone: +351 213027062
Fax:+351 213015948 | http://ipimar-iniap.ipimar.pt/pelagicos/



From tpapp at axelero.hu  Fri Jun  4 16:44:58 2004
From: tpapp at axelero.hu (Tamas Papp)
Date: Fri, 4 Jun 2004 16:44:58 +0200
Subject: [R] How to Describe R to Finance People
In-Reply-To: <20040604115038.NQWW12806.web4-rme.xtra.co.nz@kevinlpt>
References: <20040603111112.FSHM12806.web4-rme.xtra.co.nz@kevinlpt>
	<20040604115038.NQWW12806.web4-rme.xtra.co.nz@kevinlpt>
Message-ID: <20040604144458.GA955@localhost>

On Fri, Jun 04, 2004 at 11:50:32PM +1200, Ko-Kang Kevin Wang wrote:

> Although it has a slightly higher learning curve than SPSS-like
> program, it gets easier to use once one is familiar with it.  One of
> the main advantage it has over SPSS-like software is that you do not
> need to explicitly create dummy variables.  You only need to specify
> your dependent variable and independent variables and R will fit it
> (and create dummy variables automatically) for you.

I think that even if the above is an advantage when compared to SPSS,
it is more of a minor, convenient feature than one of the major
advantages of R.

Even though I majored in finance, at the moment I consider myself to
be more of a macroeconomist than a "finance person".  I wrote a lot of
my finance calculations in R (without using Rmetrics, as this was two
years ago and I did not know about Rmetrics then), including
derivatives pricing, binomial trees and term structure models.

I would emphasize the following:

1. R is free, both in the sense of "gratis" and "libre".  The latter
is more important in this context, as "finance people" can usually
afford the price of software, but IMO free software often means better
quality (this applies to R for sure).

2. The programming language is really friendly and convenient to work
with.  In finance, you often need to hack together special solutions
for problems that are not conventional (especially in term structure
models, but I think that the same applies to bi- and trinomial models
and their ilk).  As an R newbie, it took me an afternoon to implement
a basic toolkit for the former, which I could use for interesting
explorations.

3. Advanced graphing packages.  This is quite important, visualization
is often the key in finance models -- sometimes one doesn't notice
that the model is wrong until one sees a graph (eg a term structure
model with unexplained "breaks" in the interest rate curve).

4. Interface to databases, eg Oracle and MySQL.  Building certain
types of models requires access to huge amounts of data (eg credit
scoring systems).

5. Tons of statistical functions.  For example, R has all the tools
one needs to build very sophisticated credit scoring systems (note
that banks for often pay thousands of dollars for commercial versions
of software used for this purpose, I am baffled about this).

Hope this helps,

Tamas

-- 
Tam??s K. Papp
E-mail: tpapp at axelero.hu
Please try to send only (latin-2) plain text, not HTML or other garbage.



From amurta at ipimar.pt  Fri Jun  4 17:49:59 2004
From: amurta at ipimar.pt (Alberto Murta)
Date: Fri, 4 Jun 2004 15:49:59 +0000
Subject: [R] Confidence intervals for predicted values in nls
In-Reply-To: <200406041539.47142.amurta@ipimar.pt>
References: <Pine.LNX.4.44.0406032216050.3493-100000@gannet.stats>
	<200406041539.47142.amurta@ipimar.pt>
Message-ID: <200406041549.59126.amurta@ipimar.pt>

Ops! 
I apologise for posting my last message to the R list by mistake.


-- 
                                         Alberto G. Murta
Institute for Agriculture and Fisheries Research (INIAP-IPIMAR) 
Av. Brasilia, 1449-006 Lisboa, Portugal | Phone: +351 213027062
Fax:+351 213015948 | http://ipimar-iniap.ipimar.pt/pelagicos/



From danbebber at forestecology.co.uk  Fri Jun  4 17:45:18 2004
From: danbebber at forestecology.co.uk (Dan Bebber)
Date: Fri, 4 Jun 2004 16:45:18 +0100
Subject: [R] Error() term in glm model formula
Message-ID: <000001c44a4a$f0128f40$442501a3@plants.ox.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040604/79e93bf8/attachment.pl

From MSchwartz at MedAnalytics.com  Fri Jun  4 18:06:59 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 04 Jun 2004 11:06:59 -0500
Subject: [R] How to Describe R to Finance People
In-Reply-To: <40C084D9.9070502@bank-banque-canada.ca>
References: <20040604115038.NQWW12806.web4-rme.xtra.co.nz@kevinlpt>
	<40C084D9.9070502@bank-banque-canada.ca>
Message-ID: <1086365219.12223.67.camel@localhost.localdomain>

On Fri, 2004-06-04 at 09:19, Paul Gilbert wrote:
> Ko-Kang Kevin Wang wrote:
> 
> >It is not only used by statisticians or scientists,
> >but also econometricians and people in finance due to its cost (FREE)
> >and its powerfulness.
> >
> I think "(FREE)" will distract your intended audience from the real 
> point. In a corporate environment, lots of people argue that free 
> software actually costs more than commercial software because of 
> internal support cost, etc, etc.  These arguments will all hinge 
> critically on the corporate IT support abilities. For R, I have never 
> seen a convincing argument that it costs more, but the real point is 
> that this is irrelevant. If it costs less, that is nice.  If it costs 
> more, then that is what you pay to use something that is better. If they 
> need to, I think people in finance are generally willing to pay, so I 
> think it is a mistake to put much emphasis on the cost. Put the emphasis 
> on how good it is.


I agree that quality and value are important, but I think that the issue
of cost should not be discounted out of hand. Value (for both company
and client) is directly tied to cost.

Cost may be less of a concern for very large corporations to some
extent, though certainly non-trivial as we continue to see companies
finding ways to reduce their cost of operations as an important part of
the strategy to improve profitability. Typically, this is done via
reductions in personnel costs (ie. layoffs, reductions in benefits,
salary/wage cuts, etc.), but IT costs are surely a target as is noted
daily/weekly/monthly in various IT and business trade rags. IT costs are
not just those associated with the initial purchase, but with ongoing
operating costs as well.

I can speak from personal experience, as the President and Owner of a
health care consulting business who has funded this operation with my
own funds, that cost is a significant issue. I do not have shareholders
or private investors providing operating capital with the promise of
future returns on their investments. Every dollar I spend has to be
recovered via client billings or it comes out of my own pocket.

This is not just important for me, but for my clients as well.

The more I spend on "the cost of doing business", the more that I would
have to pass on to my clients to recoup those same costs. My ability to
offer clients reasonable project fees is directly correlated to my
underlying cost structure. There is a market driven threshold beyond
which I could not pass those costs on to clients and still have clients
willing to pay for services.

A product like SAS for example, which I had previously used for a number
of years working for a larger medical software company, is no longer
affordable to me as a small business owner. The last time that I
checked, the annual licensing for a single user commercial license for
Base, Stat and Graph was in the neighborhood of $5,000 U.S. **Per
Year**. That is for _one person_. Calculate those costs for a larger
staff...

Even a product such as that other "R like" commercial offering, while
less costly than SAS, still adds to overhead. I would rather allocate
the funds for that product along with my time, to supporting the R
Foundation and this community to repay the value and benefit that I
receive from it (which is nothing short of phenomenal).

The bottom line is that cost is a non-trivial issue. If a company is
willing to pay more for a functionally equivalent product, because the
training and support is (or is perceived to be) superior so be it. That
may enable managers and other decision makers to sleep better at night.

I would however challenge the level of support provided by any
commercial company to that which is provided by this community, given
the depth and breadth of expertise present and the expedience with which
communications take place here.

I use R. My company benefits from it. My clients benefit from it.

..and I sleep just fine (when I do sleep)...  :-)

Regards,

Marc Schwartz



From Tom.Vanwalleghem at geo.kuleuven.ac.be  Fri Jun  4 18:53:19 2004
From: Tom.Vanwalleghem at geo.kuleuven.ac.be (Tom Vanwalleghem)
Date: Fri, 04 Jun 2004 18:53:19 +0200
Subject: [R] use of "rcorr.cens" with binary response?
Message-ID: <40C0A8FF.346F8E88@geo.kuleuven.ac.be>

Dear R-helpers,

I recently switched from SAS to R, in order to model the occurrence of
rare events through logistic regression.
Is there a package available in R to calculate the Goodman-Kruskal
Gamma?
After searching a bit I found a function "rcorr.cens" which should do
the job, but it is not clear to me how to define the input vectors? Is
"x" a vector with the fitted probabilities and "s" a vector containing
the observed response variable? Or does anybody know an alternative?

Any help would be greatly appreciated,
Thanks,
Tom
--
Tom Vanwalleghem
Physical and Regional Geography, K.U.Leuven
Redingenstraat 16
B-3000 LEUVEN
+32(0)16/326414



From EFG at Stowers-Institute.org  Fri Jun  4 19:01:25 2004
From: EFG at Stowers-Institute.org (Glynn, Earl)
Date: Fri, 4 Jun 2004 12:01:25 -0500
Subject: [R] Plot documentation; Axis documentation
Message-ID: <CED81D34E37D5043A1211565277A51E5010624C2@exchkc02.stowers-institute.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040604/7abea713/attachment.pl

From f.harrell at vanderbilt.edu  Fri Jun  4 15:04:31 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 04 Jun 2004 09:04:31 -0400
Subject: [R] [R-pkgs] New versions of Hmisc and Design on CRAN
Message-ID: <40C0735F.8050204@vanderbilt.edu>

New versions of source packages for Hmisc and Design are available from 
CRAN for R 1.9 on Linux/Unix.   Knowing Uwe Ligges, a Windows binary is 
not far behind.  Changelogs are at 
http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/ChangelogHmisc and
http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/ChangelogDesign . 
These versions pass R CMD check for the latest R-devel so I expect they 
will work on the next production release of R.

Of special note is the Hmisc sasxport.get function's new method='csv' 
argument.  This works with a SAS macro to use SAS PROC EXPORT to export 
an entire data library, and handles variable and value labels, date, 
time, data/time variables [for dates, the new R Date class is now used 
as is also now used for sas.get, spss.get, csv.get].  We have found the 
new SAS import method in sasxport.get to be the most reliable way to 
convert datasets from SAS if you can run SAS on your system or on a 
server.  An example may be found at 
http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/SASexportHowto and 
the SAS macro is at 
http://biostat.mc.vanderbilt.edu/twiki/pub/Main/Hmisc/exportlib.sas . 
The main page for Hmisc is http://biostat.mc.vanderbilt.edu/s/Hmisc . 
sasxport.get largely supplants sas.get.

For those users who are not familiar with the Hmisc/LaTeX combination, 
have a look at 
http://biostat.mc.vanderbilt.edu/twiki/pub/Main/StatReport/summary.pdf

Thanks to Kurt Hornik for always providing such an excellent service to 
the community through his work on CRAN.
-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://www.stat.math.ethz.ch/mailman/listinfo/r-packages



From f.harrell at vanderbilt.edu  Fri Jun  4 19:02:19 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 04 Jun 2004 13:02:19 -0400
Subject: [R] use of "rcorr.cens" with binary response?
In-Reply-To: <40C0A8FF.346F8E88@geo.kuleuven.ac.be>
References: <40C0A8FF.346F8E88@geo.kuleuven.ac.be>
Message-ID: <40C0AB1B.4040409@vanderbilt.edu>

Tom Vanwalleghem wrote:
> Dear R-helpers,
> 
> I recently switched from SAS to R, in order to model the occurrence of
> rare events through logistic regression.
> Is there a package available in R to calculate the Goodman-Kruskal
> Gamma?
> After searching a bit I found a function "rcorr.cens" which should do
> the job, but it is not clear to me how to define the input vectors? Is
> "x" a vector with the fitted probabilities and "s" a vector containing
> the observed response variable? Or does anybody know an alternative?
> 
> Any help would be greatly appreciated,
> Thanks,
> Tom
> --
> Tom Vanwalleghem
> Physical and Regional Geography, K.U.Leuven
> Redingenstraat 16
> B-3000 LEUVEN
> +32(0)16/326414
> 

If dealing with a binary response, Somers' Dxy rank correlation may have 
a slight advantage over gamma.  You can get Dxy from somers2 in Hmisc 
and from that you can easily compute ROC area (Dxy = 2*(ROC - .5)). 
rcorr.cens thought will also give standard errors.  You are right about 
the inputs except that x can be probability or log odds - anything that 
ranks the same as probability.

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From MSchwartz at MedAnalytics.com  Fri Jun  4 19:28:46 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 04 Jun 2004 12:28:46 -0500
Subject: [R] Plot documentation; Axis documentation
In-Reply-To: <CED81D34E37D5043A1211565277A51E5010624C2@exchkc02.stowers-institute.org>
References: <CED81D34E37D5043A1211565277A51E5010624C2@exchkc02.stowers-institute.org>
Message-ID: <1086370126.12223.98.camel@localhost.localdomain>

On Fri, 2004-06-04 at 12:01, Glynn, Earl wrote:
> Why when I do a  "help(plot)" do I not see anything about parameters
> such as xlim or ylim?  As someone new to R, finding that xlim and ylim
> even existed wasn't all that easy. Even help.search("xlim") shows
> nothing once I know xlim exists.  
> 
> I'd like to change the default axes but "help(axis)" isn't that
> informative about changing the frequency of ticks on the axes.
> 
> Do people really refer to the x-axis as "1" and the y-axis as "2" as
> shown in help(axis)? 
> 
>    plot(1:4, rnorm(4), axes=FALSE)
>    axis(1, 1:4, LETTERS[1:4])
>    axis(2)
> 
> I hadn't a clue what the "1" and "2" meant here without reading
> additional documentation.  And where is the "LETTERS" constant defined
> and what else is defined there?
> 
> Are there no common R constants defined somewhere so the axes be defined
> symbolically?  Perhaps AXIS_X = 1, AXIS_Y = 2 would be better than just
> "1" and "2":
> 
>    plot(1:4, rnorm(4), axes=FALSE)
>    axis(AXIS_X, 1:4, LETTERS[1:4])
>    axis(AXIS_Y)
> 
> This would at least provide a clue about what is going on here.
> 
> Why is R such a graphics rich language and the documentation is so
> lacking in graphics examples?  Why can't the documentation include
> graphics too so one can study code and graphics at the same time?  How
> do I know the graphics I'm seeing is what it's supposed to look like?
> 
> I'd rather do more in R than MatLab but I find the R documentation
> somewhat lacking.  I prefer not to read the R source code to find the
> answers.
> 
> Thanks for any insight about this.
> 
> efg


Reading the posting guide, for which there is a link at the bottom of
each list e-mail, would be a good place to start. The section on
"Further Resources" provides important links.

Specifically on graphics:

1. Start by reading chapter 12 in An Introduction to R, which covers
graphics basics.

2. V&R's MASS also has an excellent chapter (4) on graphics.

3. There is also an article in R News "R Help Desk"
(http://cran.r-project.org/doc/Rnews/Rnews_2003-2.pdf) that would likely
be helpful as well.

Reviewing these resources would be crucial to assist your comprehension.
I think that you will find the documentation for R to be substantial, if
you take the time to properly research it. The posting guide will help
get you started in that endeavor.

In most cases this obviates any need to review source code, though a
critical advantage of R is the ability to do just that when you need to.

HTH,

Marc Schwartz



From DAVID.BICKEL at PIONEER.COM  Fri Jun  4 19:32:48 2004
From: DAVID.BICKEL at PIONEER.COM (Bickel, David)
Date: Fri, 4 Jun 2004 12:32:48 -0500
Subject: [R] running R command in the background
Message-ID: <5F883C17941B9F4E80E5FA8C9F1C5E0EBD5BE5@jhms08.phibred.com>

Is there a way to call an R function to work in the background within the same R session? What I have in mind is the equivalent of adding '&' at the end of a UNIX command, so that R can process an intensive command while I execute other R commands. I want to do this in the same R session, rather than with multiple calls to R, so that all my modified objects will stay in the same workspace.

I would appreciate any help with this.

Thanks,
David
______________________________
David Bickel  http://davidbickel.com
Research Scientist
Pioneer Hi-Bred International
Bioinformatics & Discovery Research
7250 NW 62nd Ave., PO Box 552
Johnston, Iowa 50131-0552
706-736-9151 Home
515-334-4739 Work
515-334-6634 Fax
david.bickel at pioneer.com, bickel at prueba.info



This communication is for use by the intended recipient and ...{{dropped}}



From tpapp at axelero.hu  Fri Jun  4 19:47:45 2004
From: tpapp at axelero.hu (Tamas Papp)
Date: Fri, 4 Jun 2004 19:47:45 +0200
Subject: [R] How to Describe R to Finance People
In-Reply-To: <1086365219.12223.67.camel@localhost.localdomain>
References: <20040604115038.NQWW12806.web4-rme.xtra.co.nz@kevinlpt>
	<40C084D9.9070502@bank-banque-canada.ca>
	<1086365219.12223.67.camel@localhost.localdomain>
Message-ID: <20040604174745.GC1113@localhost>

On Fri, Jun 04, 2004 at 11:06:59AM -0500, Marc Schwartz wrote:

> I agree that quality and value are important, but I think that the issue
> of cost should not be discounted out of hand. Value (for both company
> and client) is directly tied to cost.
>
> [...]
> 
> The bottom line is that cost is a non-trivial issue. If a company is
> willing to pay more for a functionally equivalent product, because the
> training and support is (or is perceived to be) superior so be it. That
> may enable managers and other decision makers to sleep better at night.

I agree with your points.  However, as far as I remember, the original
poster wants to give a 2-3 minute summary about the benefits of R.  I
would not open such a complicated issue (the total cost of ownership)
in such a small timeframe, but focus on the more "technical" benefits
instead.

Best,

Tamas

-- 
Tam??s K. Papp
E-mail: tpapp at axelero.hu
Please try to send only (latin-2) plain text, not HTML or other garbage.



From tpapp at axelero.hu  Fri Jun  4 19:43:56 2004
From: tpapp at axelero.hu (Tamas Papp)
Date: Fri, 4 Jun 2004 19:43:56 +0200
Subject: [R] Plot documentation; Axis documentation
In-Reply-To: <CED81D34E37D5043A1211565277A51E5010624C2@exchkc02.stowers-institute.org>
References: <CED81D34E37D5043A1211565277A51E5010624C2@exchkc02.stowers-institute.org>
Message-ID: <20040604174356.GA1113@localhost>

On Fri, Jun 04, 2004 at 12:01:25PM -0500, Glynn, Earl wrote:

> Why when I do a  "help(plot)" do I not see anything about parameters
> such as xlim or ylim?  As someone new to R, finding that xlim and ylim
> even existed wasn't all that easy. Even help.search("xlim") shows
> nothing once I know xlim exists.  

Suppose you want to change the range of x adn y values (that is what
xlim and ylim does, but you don't know that yet).  You type

?plot

and you see no parameter that is directly relevant.  However, the help
page gives you a couple of directions to go from here: you can check
?par (mentioned in the very first lines) and see a plethora of
parameters.

You search for "axis" (type /axis in a Linux terminal, or use C-S in
Emacs/ESS), and soon you will find 

     'xaxs' The style of axis interval calculation to be used for the
          x-axis.  Possible values are '"r"', '"i"', '"e"', '"s"',
          '"d"'.  The styles are generally controlled by the range of
          data or 'xlim', if given. Style '"r"' (regular) first extends

So you now know that you need something called "xlim" (and ylim,
though that is not mentioned explicitly), but find no further
information on this page.  Therefore you should follow in another
direction from ?plot: the very next one (in See also) is plot.default,
which has

    xlim: the x limits (min,max) of the plot.

    ylim: the y limits of the plot.

Lo and behold.

> I'd like to change the default axes but "help(axis)" isn't that
> informative about changing the frequency of ticks on the axes.

How would you like to change them?  This list won't be able to help
you unless you tell us.

> I hadn't a clue what the "1" and "2" meant here without reading
> additional documentation.  And where is the "LETTERS" constant defined
> and what else is defined there?

?LETTERS has the answer.

> Are there no common R constants defined somewhere so the axes be defined
> symbolically?  Perhaps AXIS_X = 1, AXIS_Y = 2 would be better than just
> "1" and "2":
> 
>    plot(1:4, rnorm(4), axes=FALSE)
>    axis(AXIS_X, 1:4, LETTERS[1:4])
>    axis(AXIS_Y)
> 
> This would at least provide a clue about what is going on here.

It would also clutter the namespace.  Feel free to put

AXIS_X <- 1
AXIS_Y <- 2

in your .Rprofile.

> Why is R such a graphics rich language and the documentation is so
> lacking in graphics examples?  Why can't the documentation include
> graphics too so one can study code and graphics at the same time?  How
> do I know the graphics I'm seeing is what it's supposed to look like?

Because the documentation is pretty much device independent, you can
read in interactively on a terminal, or make HTML of PDF output.
Graphics would complicate that.  IMHO the idea is worth thinking
about, but it might involve a lot of work with little benefits.

If you want to see examples and the resulting figures side-by-side,
get one of the good books on R (MASS would be a good one, see the
homepage).  I think that you can buy a lot of good books for the price
of Matlab.

> I'd rather do more in R than MatLab but I find the R documentation
> somewhat lacking.  I prefer not to read the R source code to find the
> answers.

I have been using R for a while, but I never HAD to read the source
code of any function to find out anything.  On the other hand, reading
the sources will teach you a lot about R, as the core functions were
written by very good programmers in R.

Best,

Tamas

-- 
Tam??s K. Papp
E-mail: tpapp at axelero.hu
Please try to send only (latin-2) plain text, not HTML or other garbage.



From ripley at stats.ox.ac.uk  Fri Jun  4 19:52:02 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 4 Jun 2004 18:52:02 +0100 (BST)
Subject: [R] running R command in the background
In-Reply-To: <5F883C17941B9F4E80E5FA8C9F1C5E0EBD5BE5@jhms08.phibred.com>
Message-ID: <Pine.LNX.4.44.0406041845030.25830-100000@gannet.stats>

This cannot presently be done, as the R internals do not multitask.  I am 
not sure what you would want to happen if the background and foreground 
commands both tried to write a new version of object `foo', BTW.

Allowing multiple execution threads and setReader() as described in the
Green Book is a design goal and has been since 2001 at least, so don't
expect it soon.  It is hard (lots of the code used, e.g. LINPACK, is not
reentrant and so needs to be semaphored) and if achieved will need
vigilance to keep working.

On Fri, 4 Jun 2004, Bickel, David wrote:

> Is there a way to call an R function to work in the background within
> the same R session? What I have in mind is the equivalent of adding '&'
> at the end of a UNIX command, so that R can process an intensive command
> while I execute other R commands. I want to do this in the same R
> session, rather than with multiple calls to R, so that all my modified
> objects will stay in the same workspace.
> 
> I would appreciate any help with this.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From MSchwartz at MedAnalytics.com  Fri Jun  4 20:16:30 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 04 Jun 2004 13:16:30 -0500
Subject: [R] How to Describe R to Finance People
In-Reply-To: <20040604174745.GC1113@localhost>
References: <20040604115038.NQWW12806.web4-rme.xtra.co.nz@kevinlpt>
	<40C084D9.9070502@bank-banque-canada.ca>
	<1086365219.12223.67.camel@localhost.localdomain>
	<20040604174745.GC1113@localhost>
Message-ID: <1086372990.12223.116.camel@localhost.localdomain>

On Fri, 2004-06-04 at 12:47, Tamas Papp wrote:
> On Fri, Jun 04, 2004 at 11:06:59AM -0500, Marc Schwartz wrote:
> 
> > I agree that quality and value are important, but I think that the issue
> > of cost should not be discounted out of hand. Value (for both company
> > and client) is directly tied to cost.
> >
> > [...]
> > 
> > The bottom line is that cost is a non-trivial issue. If a company is
> > willing to pay more for a functionally equivalent product, because the
> > training and support is (or is perceived to be) superior so be it. That
> > may enable managers and other decision makers to sleep better at night.
> 
> I agree with your points.  However, as far as I remember, the original
> poster wants to give a 2-3 minute summary about the benefits of R.  I
> would not open such a complicated issue (the total cost of ownership)
> in such a small timeframe, but focus on the more "technical" benefits
> instead.

Actually, Kevin indicated 1 to 2 minutes...  ;-)

My response was clearly more detailed than what Kevin could incorporate
into the presentation structure. It was more a response to Paul's
comments on cost not being an issue. My experience indicates otherwise.

The TCO issue is clearly a subject of much debate, especially when
fueled by widely disseminated "studies" funded (overtly or otherwise) by
a certain large software company based in the northwestern U.S....that
tends to introduce a certain 'a priori' bias.

I agree that you cannot adequately address the issue in such a short
talk. However, I think that it can be raised briefly, with supporting
comments, such as those made by Frank Harrell regarding reproducible
analyses, which also supports cost reduction via improvements in quality
and productivity. Something that point and click based tools cannot
offer effectively.

Thanks for raising the clarification Tamas.

Marc



From youngas7 at yahoo.com  Fri Jun  4 21:11:39 2004
From: youngas7 at yahoo.com (Andrew Young)
Date: Fri, 4 Jun 2004 12:11:39 -0700 (PDT)
Subject: [R] running R command in the background
In-Reply-To: <Pine.LNX.4.44.0406041845030.25830-100000@gannet.stats>
Message-ID: <20040604191139.31091.qmail@web61009.mail.yahoo.com>


You could try looking at the library "fork".  However,
this generally sounds like a bad idea and if you do
this, you're kind of asking for trouble with threading
issues, as R is not thread-safe.

I'm not sure if fork works on all platforms, since I
believe it works by running a unix shell command. But
it's worth a shot.

Best regards,
-Andrew


--- Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> This cannot presently be done, as the R internals do
> not multitask.  I am 
> not sure what you would want to happen if the
> background and foreground 
> commands both tried to write a new version of object
> `foo', BTW.
> 
> Allowing multiple execution threads and setReader()
> as described in the
> Green Book is a design goal and has been since 2001
> at least, so don't
> expect it soon.  It is hard (lots of the code used,
> e.g. LINPACK, is not
> reentrant and so needs to be semaphored) and if
> achieved will need
> vigilance to keep working.
> 
> On Fri, 4 Jun 2004, Bickel, David wrote:
> 
> > Is there a way to call an R function to work in
> the background within
> > the same R session? What I have in mind is the
> equivalent of adding '&'
> > at the end of a UNIX command, so that R can
> process an intensive command
> > while I execute other R commands. I want to do
> this in the same R
> > session, rather than with multiple calls to R, so
> that all my modified
> > objects will stay in the same workspace.
> > 
> > I would appreciate any help with this.
> 
> -- 
> Brian D. Ripley,                 
> ripley at stats.ox.ac.uk
> Professor of Applied Statistics, 
> http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865
> 272861 (self)
> 1 South Parks Road,                     +44 1865
> 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865
> 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
>
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From youngas7 at yahoo.com  Fri Jun  4 21:14:45 2004
From: youngas7 at yahoo.com (Andrew Young)
Date: Fri, 4 Jun 2004 12:14:45 -0700 (PDT)
Subject: [R] running R command in the background
In-Reply-To: <20040604191139.31091.qmail@web61009.mail.yahoo.com>
Message-ID: <20040604191445.16418.qmail@web61004.mail.yahoo.com>

By "library" I of course mean "package".

-Andrew


--- Andrew Young <youngas7 at yahoo.com> wrote:
> 
> You could try looking at the library "fork". 
> However,
> this generally sounds like a bad idea and if you do
> this, you're kind of asking for trouble with
> threading
> issues, as R is not thread-safe.
> 
> I'm not sure if fork works on all platforms, since I
> believe it works by running a unix shell command.
> But
> it's worth a shot.
> 
> Best regards,
> -Andrew
> 
> 
> --- Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> > This cannot presently be done, as the R internals
> do
> > not multitask.  I am 
> > not sure what you would want to happen if the
> > background and foreground 
> > commands both tried to write a new version of
> object
> > `foo', BTW.
> > 
> > Allowing multiple execution threads and
> setReader()
> > as described in the
> > Green Book is a design goal and has been since
> 2001
> > at least, so don't
> > expect it soon.  It is hard (lots of the code
> used,
> > e.g. LINPACK, is not
> > reentrant and so needs to be semaphored) and if
> > achieved will need
> > vigilance to keep working.
> > 
> > On Fri, 4 Jun 2004, Bickel, David wrote:
> > 
> > > Is there a way to call an R function to work in
> > the background within
> > > the same R session? What I have in mind is the
> > equivalent of adding '&'
> > > at the end of a UNIX command, so that R can
> > process an intensive command
> > > while I execute other R commands. I want to do
> > this in the same R
> > > session, rather than with multiple calls to R,
> so
> > that all my modified
> > > objects will stay in the same workspace.
> > > 
> > > I would appreciate any help with this.
> > 
> > -- 
> > Brian D. Ripley,                 
> > ripley at stats.ox.ac.uk
> > Professor of Applied Statistics, 
> > http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865
> > 272861 (self)
> > 1 South Parks Road,                     +44 1865
> > 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865
> > 272595
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> >
>
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
>
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From pgilbert at bank-banque-canada.ca  Fri Jun  4 21:26:04 2004
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Fri, 04 Jun 2004 15:26:04 -0400
Subject: [R] How to Describe R to Finance People
In-Reply-To: <1086365219.12223.67.camel@localhost.localdomain>
References: <20040604115038.NQWW12806.web4-rme.xtra.co.nz@kevinlpt>	
	<40C084D9.9070502@bank-banque-canada.ca>
	<1086365219.12223.67.camel@localhost.localdomain>
Message-ID: <40C0CCCC.4050407@bankofcanada.ca>

Marc Schwartz wrote:

>On Fri, 2004-06-04 at 09:19, Paul Gilbert wrote:
>  
>
>>Ko-Kang Kevin Wang wrote:
>>
>>    
>>
>>>It is not only used by statisticians or scientists,
>>>but also econometricians and people in finance due to its cost (FREE)
>>>and its powerfulness.
>>>
>>>      
>>>
>>I think "(FREE)" will distract your intended audience from the real 
>>point. In a corporate environment, lots of people argue that free 
>>software actually costs more than commercial software because of 
>>internal support cost, etc, etc.  These arguments will all hinge 
>>critically on the corporate IT support abilities. For R, I have never 
>>seen a convincing argument that it costs more, but the real point is 
>>that this is irrelevant. If it costs less, that is nice.  If it costs 
>>more, then that is what you pay to use something that is better. If they 
>>need to, I think people in finance are generally willing to pay, so I 
>>think it is a mistake to put much emphasis on the cost. Put the emphasis 
>>on how good it is.
>>    
>>
>
>
>I agree that quality and value are important, but I think that the issue
>of cost should not be discounted out of hand. Value (for both company
>and client) is directly tied to cost.
>  
>
[snip ...]

Marc

I agree with this, and most of what you say. Cost is important in both 
large and small companies, and also in government. The point is really 
that total cost of ownership is a very complicate thing, and you should 
not get into it without the specifics of a particular company and 
situation in mind. For example, if the end users takes responsibility 
for all the support, the cost implications will be very different from 
the situation where the IT department needs to guarantee availability. 
Even in your own company you may have a very different attitude with 
respect to your research software and your accounts receivable software. 
People in finance making real time market decisions will have a very 
different cost structure from academics in finance.

The point was really that many people are very sensitive to arguments 
about cost, and often have positions that they feel obliged to promote. 
So, as soon as you mention cost you are likely to get into a very long 
discussion that will not be fruitful unless you are prepared to talk 
about very particular situations. For this particular audience I think 
it would probably be more to the point to describe how good and reliable 
R is. A particular company may decided R is just too expensive. For 
example, some companies in finance are worried about real time decision 
making. They may have to hire 10 more IT staff to guarantee 24/7 
availaility with no more than 5 minutes outage per week whereas, with 
commercial software,  they may be able to buy guarantees. (This is not a 
statement about commercial software being more reliable, it is a 
statement about being able to buy insurance.)  But, as you say, for must 
of us R is a real bargain.

Paul



From andy_liaw at merck.com  Fri Jun  4 21:59:03 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 4 Jun 2004 15:59:03 -0400
Subject: [R] Sloppy argument checking for named arguments
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7E2A@usrymx25.merck.com>

> From: Gabor Grothendieck
> 
> It has already been pointed out that partial matching is a feature.
> 
> However, if the function uses ... then it will only match arguments 

I believe a more precise description is: `no partial (or positional)
matching for arguments that come after the ...'.

Best,
Andy


> exactly so if you want to write a function in which full 
> matching only 
> is accepted you can do it by putting in a dummy first argument of 
> ... and then issuing an error messaage if ... matches anything:
> 
> f <- function( ..., foo.bar = 0 )  { 
>           stopifnot( length(list(...)) == 0 )
>           print( foo.bar )
> }
> 
>  
> R> f( foo.bar = 1 )
> [1] 1
> R> f( foo = 1 )
> Error: length(list(...)) == 0 is not TRUE
> 
> 
> Christian Lederer <christianlederer <at> t-online.de> writes:
> 
> : 
> : Dear R Gurus,
> : 
> : i recently noticed that R does sloppy argument checking for named
> : arguments, if the argument contains a dot.
> : Example:
> : 
> :  > f <- function(foo.bar=0) { print(foo.bar) }
> :  > f(foo=1)
> : [1] 1
> : 
> : I guess, this should be considered as a bug.
> : Anyway, the consequence is that bugs caused by typing errors
> : of this kind may are *very* hard to discover in complex progams.
> : 
> : Christian
> : 
> : ______________________________________________
> : R-help <at> stat.math.ethz.ch mailing list
> : https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> : PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> : 
> :
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ripley at stats.ox.ac.uk  Fri Jun  4 22:46:49 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 4 Jun 2004 21:46:49 +0100 (BST)
Subject: [R] running R command in the background
In-Reply-To: <20040604191445.16418.qmail@web61004.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0406042142300.25992-100000@gannet.stats>

Did you try actually reading the request (let alone the informed 
repsonse you included) ...

> I want to do this in the same session, rather than with multiple calls 
to R,

whereas package fork's DESCRIPTION says

Description: These functions provides simple wrappers around the Unix
        process management API calls: fork, wait, waitpid, kill, and
        _exit.  This enables construction of programs that utilize
        multiple concurrent processes.

exactly what was *not* asked for.


On Fri, 4 Jun 2004, Andrew Young wrote:

> By "library" I of course mean "package".
> 
> -Andrew
> 
> 
> --- Andrew Young <youngas7 at yahoo.com> wrote:
> > 
> > You could try looking at the library "fork". 
> > However,
> > this generally sounds like a bad idea and if you do
> > this, you're kind of asking for trouble with
> > threading
> > issues, as R is not thread-safe.
> > 
> > I'm not sure if fork works on all platforms, since I
> > believe it works by running a unix shell command.
> > But
> > it's worth a shot.

Only for zero marks.

> > 
> > Best regards,
> > -Andrew
> > 
> > 
> > --- Prof Brian Ripley <ripley at stats.ox.ac.uk> wrote:
> > > This cannot presently be done, as the R internals
> > do
> > > not multitask.  I am 
> > > not sure what you would want to happen if the
> > > background and foreground 
> > > commands both tried to write a new version of
> > object
> > > `foo', BTW.
> > > 
> > > Allowing multiple execution threads and
> > setReader()
> > > as described in the
> > > Green Book is a design goal and has been since
> > 2001
> > > at least, so don't
> > > expect it soon.  It is hard (lots of the code
> > used,
> > > e.g. LINPACK, is not
> > > reentrant and so needs to be semaphored) and if
> > > achieved will need
> > > vigilance to keep working.
> > > 
> > > On Fri, 4 Jun 2004, Bickel, David wrote:
> > > 
> > > > Is there a way to call an R function to work in
> > > the background within
> > > > the same R session? What I have in mind is the
> > > equivalent of adding '&'
> > > > at the end of a UNIX command, so that R can
> > > process an intensive command
> > > > while I execute other R commands. I want to do
> > > this in the same R
> > > > session, rather than with multiple calls to R,
> > so
> > > that all my modified
> > > > objects will stay in the same workspace.
> > > > 
> > > > I would appreciate any help with this.
> > > 
> > > -- 
> > > Brian D. Ripley,                 
> > > ripley at stats.ox.ac.uk
> > > Professor of Applied Statistics, 
> > > http://www.stats.ox.ac.uk/~ripley/
> > > University of Oxford,             Tel:  +44 1865
> > > 272861 (self)
> > > 1 South Parks Road,                     +44 1865
> > > 272866 (PA)
> > > Oxford OX1 3TG, UK                Fax:  +44 1865
> > > 272595
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > >
> >
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> >
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jonathan_lees at unc.edu  Fri Jun  4 23:26:00 2004
From: jonathan_lees at unc.edu (Jonathan M. Lees)
Date: Fri, 04 Jun 2004 17:26:00 -0400
Subject: [R] three button mouse? or mouse modifiers?
Message-ID: <40C0E8E8.6040304@unc.edu>



Is there any way to get R to recognize a three button mouse
or a mouse click with a modifier, like control-left mouse
or meta-leftmouse
or shift ?

I need more control in my graphics screen and
I would rather not go the xgobi route.

-- 
==========================================
Prof. Jonathan M. Lees
Department of Geological Sciences
CB #3315, Mitchell Hall
University of North Carolina
Chapel Hill, NC  27599-3315
(919) 962-0695
FAX (919) 966-4519

jonathan_lees at unc.edu
http://www.unc.edu/~leesj



From kwong at bcgsc.ca  Fri Jun  4 23:53:03 2004
From: kwong at bcgsc.ca (Kim Wong)
Date: Fri, 4 Jun 2004 14:53:03 -0700
Subject: [R] hist, lines, rug
Message-ID: <D0A9CEDA10056D489283BE93B8F29DAE26DF@xchange1.phage.bcgsc.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040604/29f6c408/attachment.pl

From kwright at eskimo.com  Sat Jun  5 01:24:55 2004
From: kwright at eskimo.com (Kevin Wright)
Date: Fri, 4 Jun 2004 16:24:55 -0700 (PDT)
Subject: [R] Mozilla search engine plugin for R java search applet?
Message-ID: <200406042324.QAA07176@eskimo.com>

I'm interested in adding a toolbar search for my locally installed R 
documentation.  I've pursued doing it myself, but have gotten in over my 
head and thought to ask here if anybody else has done something like this
before I continue this path.

Some things that might be relevant:

Creating Mozilla search plugins
http://mycroft.mozdev.org/deepdocs/quickstart.html

An example of an R search engine
http://www.statslab.cam.ac.uk/~djw1005/Stats/Interests/searchtemplate.html


I'm using Firefox on Windows, but I suspect that's not relevant.

Javascript bookmarklets might also be relevant.

Kevin Wright



From MSchwartz at MedAnalytics.com  Sat Jun  5 05:30:26 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Fri, 04 Jun 2004 22:30:26 -0500
Subject: [R] How to Describe R to Finance People
In-Reply-To: <40C0CCCC.4050407@bankofcanada.ca>
References: <20040604115038.NQWW12806.web4-rme.xtra.co.nz@kevinlpt>
	<40C084D9.9070502@bank-banque-canada.ca>
	<1086365219.12223.67.camel@localhost.localdomain>
	<40C0CCCC.4050407@bankofcanada.ca>
Message-ID: <1086406226.12223.267.camel@localhost.localdomain>

On Fri, 2004-06-04 at 14:26, Paul Gilbert wrote:
> Marc Schwartz wrote:

snip

> >
> >I agree that quality and value are important, but I think that the issue
> >of cost should not be discounted out of hand. Value (for both company
> >and client) is directly tied to cost.
> >  
> >
> [snip ...]
> 
> Marc
> 
> I agree with this, and most of what you say. Cost is important in both 
> large and small companies, and also in government. The point is really 
> that total cost of ownership is a very complicate thing, and you should 
> not get into it without the specifics of a particular company and 
> situation in mind. For example, if the end users takes responsibility 
> for all the support, the cost implications will be very different from 
> the situation where the IT department needs to guarantee availability. 
> Even in your own company you may have a very different attitude with 
> respect to your research software and your accounts receivable software. 
> People in finance making real time market decisions will have a very 
> different cost structure from academics in finance.

Paul,

Sorry for the delay in my reply. A sudden request came from a client
this afternoon and I just finished the analysis.

I am in agreement with you that situational differences will bias the
focus on costs and perhaps even the ability to define them well. Within
the timeframe that Kevin and/or his associate has for this presentation,
this topic cannot be adequately covered. That does not mean that you
cannot raise it for further consideration by the audience within the
context of pointing out R's strengths.

I suspect that if you point out these issues, somebody with the right
insight will have a light bulb go on relative to the possibility of cost
savings versus their current set of tools. They will of course need to
pursue that line of thinking outside of the scope of this presentation,
which is fine.

I do use a commercial product for accounting (which is the only reason
that I still dual-boot Windows and Fedora Core 2.) My alternative is to
send all of my paperwork to my accountant to do all of my ledger
entries, which at $300 U.S. per hour is not inconsequential. So, yes, I
make a cost based decision to purchase a commercial OTS product that
enables me to do the grunt work and send an electronic file to my
accountant for review. My cost per unit of time is cheaper than my
accountant's. If I could do the same thing with an open source product,
hallelujah. It would save me even more. Unfortunately, as is oft
discussed, this is one area in which the Linux world is still lacking. I
suspect that will change in time however.

On the other hand, I use a payroll services company to handle that part
of the business. Their costs to run the payroll, pay taxes, worker's
comp and all the rest of the associated procedures are cheaper than what
I could do on my own. It is not that I could not do it technically, but
that use of my time would in the long run cost me more money than what I
pay for the service.

Each component of the process does need to be evaluated within the
context of the alternatives and appropriate risk/benefit considerations.

> The point was really that many people are very sensitive to arguments 
> about cost, and often have positions that they feel obliged to promote. 
> So, as soon as you mention cost you are likely to get into a very long 
> discussion that will not be fruitful unless you are prepared to talk 
> about very particular situations. For this particular audience I think 
> it would probably be more to the point to describe how good and reliable 
> R is. A particular company may decided R is just too expensive. For 
> example, some companies in finance are worried about real time decision 
> making. They may have to hire 10 more IT staff to guarantee 24/7 
> availaility with no more than 5 minutes outage per week whereas, with 
> commercial software,  they may be able to buy guarantees. (This is not a 
> statement about commercial software being more reliable, it is a 
> statement about being able to buy insurance.)  But, as you say, for must 
> of us R is a real bargain.

There is no doubt that folks are willing to spend more in some cases for
a piece of paper that guarantees availability and perhaps some form of
compensation for downtime, which in these situations will likely result
in lost revenue. As long as clients are willing to pay for those
features and/or companies determine that business imperatives warrant
such expenditures...

I think that your closing point relative to R's reliability is
important. This goes to the old saying "Facts are negotiable, perception
is reality." Why would R be more or less available or reliable than
another analytic tool? More often than not, I suspect that it is not the
application but the underlying infrastructure that results in reduced
availability.

The perception issue may be the biggest hurdle that the open source
world needs to (and will) overcome in the commercial marketplace.

Anyway, I think that we are more in agreement than disagreement here. I
hope that Kevin has benefited from the discourse in some fashion.

Best regards,

Marc



From m.g.walker at massey.ac.nz  Sat Jun  5 06:04:27 2004
From: m.g.walker at massey.ac.nz (Matthew Walker)
Date: Sat, 05 Jun 2004 16:04:27 +1200
Subject: [R] More than one series in a coplot
Message-ID: <40C1464B.6000400@massey.ac.nz>

Hi!

I would like to know, how do you plot more than one data series when 
using "coplot"?

I think I know the answer if "plot" is being used:
x <- 1:10
y <- 1:10
y2 <- 1:10* 1.1
plot ( y ~ x, type="n" )
points( y ~ x, type = "b", col = "red" )  # plot the points and lines 
for the first series
points( y2 ~ x, type = "b", col = "blue" )  # plot the points and lines 
for the second series

But how should it be done with "coplot"?

If I make my question more concrete:
df <- data.frame( x=x, y=y, y2=y2, v=c("a","b"))  # using the vectors above
coplot( df$y~df$x | df$v, type="b", col="red")  # gives me a coplot of y ~ x

How do I now put points and lines for y2 on the same graphs?

Thank you for your thoughts,

Matthew



From eduarmasrs at yahoo.com.br  Sat Jun  5 06:44:32 2004
From: eduarmasrs at yahoo.com.br (Eduardo Dutra de Armas)
Date: Sat, 5 Jun 2004 01:44:32 -0300
Subject: [R] Comparing treatments in Multivariate Analysis
Message-ID: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAAlkN94tU7pE2294PPHpOIAsKAAAAQAAAAGgKOJG21YE2In1ETItOBXAEAAAAA@yahoo.com.br>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040605/ba6e30b2/attachment.pl

From deepayan at stat.wisc.edu  Sat Jun  5 07:47:00 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Sat, 5 Jun 2004 00:47:00 -0500
Subject: [R] More than one series in a coplot
In-Reply-To: <40C1464B.6000400@massey.ac.nz>
References: <40C1464B.6000400@massey.ac.nz>
Message-ID: <200406050047.00963.deepayan@stat.wisc.edu>

On Friday 04 June 2004 23:04, Matthew Walker wrote:
> Hi!
>
> I would like to know, how do you plot more than one data series when
> using "coplot"?
>
> I think I know the answer if "plot" is being used:
> x <- 1:10
> y <- 1:10
> y2 <- 1:10* 1.1
> plot ( y ~ x, type="n" )
> points( y ~ x, type = "b", col = "red" )  # plot the points and lines
> for the first series
> points( y2 ~ x, type = "b", col = "blue" )  # plot the points and
> lines for the second series
>
> But how should it be done with "coplot"?
>
> If I make my question more concrete:
> df <- data.frame( x=x, y=y, y2=y2, v=c("a","b"))  # using the vectors
> above coplot( df$y~df$x | df$v, type="b", col="red")  # gives me a
> coplot of y ~ x
>
> How do I now put points and lines for y2 on the same graphs?

Don't know about coplot, but with xyplot from the lattice package (which 
one might consider to be a refined version of coplot) you could do:

xyplot(y + y2 ~ x, df, type = 'b', col = c('blue', 'red'))

Deepayan



From ripley at stats.ox.ac.uk  Sat Jun  5 09:53:18 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 5 Jun 2004 08:53:18 +0100 (BST)
Subject: [R] hist, lines, rug
In-Reply-To: <D0A9CEDA10056D489283BE93B8F29DAE26DF@xchange1.phage.bcgsc.ca>
Message-ID: <Pine.LNX.4.44.0406050851080.20581-100000@gannet.stats>

On Fri, 4 Jun 2004, Kim Wong wrote:

> I'm trying to plot a historgram with a density plot superimposed:
>  
> hist(x, seq(-1, 1, by = 0.1), prob = T, col = "blue")
> lines(density(x,  bw = 0.1))
> rug(x)
>  
> I don't want to add rug(x) but the histogram will not be plotted unless
> rug(x) is there. It does not work if the "hist" line alone is present.
> Can you help?

No, as 

x <- runif(1000, -1, 1)
hist(x, seq(-1, 1, by = 0.1), prob = T, col = "blue")
lines(density(x,  bw = 0.1))

works for me.  Please give us a fully reproducible example (what is x? 
what version of R? what graphics device? ...).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sorenh at agrsci.dk  Sat Jun  5 11:05:10 2004
From: sorenh at agrsci.dk (=?iso-8859-1?Q?S=F8ren_H=F8jsgaard?=)
Date: Sat, 5 Jun 2004 11:05:10 +0200
Subject: [R] 'invalid HOMEDRIVE'
Message-ID: <000001c44adc$46d03fd0$f2f1d7c3@djf.agrsci.dk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040605/9b5f3dc4/attachment.pl

From ripley at stats.ox.ac.uk  Sat Jun  5 11:16:17 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 5 Jun 2004 10:16:17 +0100 (BST)
Subject: [R] 'invalid HOMEDRIVE'
In-Reply-To: <000001c44adc$46d03fd0$f2f1d7c3@djf.agrsci.dk>
Message-ID: <Pine.LNX.4.44.0406051012200.11933-100000@gannet.stats>

Do look in the archives when this has come up tens of times.  The usual
cause is a bug in a Windows XP critical update: see

http://www.murdoch-sutherland.com/HOMEPATH.html

Adding HOME=C:/ (or some other sensible value) to the end of the target in
the shortcut used to launch R will circumvent this.  (There are more
details in the rw-FAQ on how to set environment variables.)


On Sat, 5 Jun 2004, S??ren H??jsgaard wrote:

> Dear all,
> One of my students have installed R1.9.0 on windows, and gets the fatal error
>     'invalid HOMEDRIVE'
> Can anyone help her/me out on that one?
> Thanks in advance 
> S??ren H??jsgaard

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ccleland at optonline.net  Sat Jun  5 11:18:01 2004
From: ccleland at optonline.net (Chuck Cleland)
Date: Sat, 05 Jun 2004 05:18:01 -0400
Subject: [R] 'invalid HOMEDRIVE'
In-Reply-To: <000001c44adc$46d03fd0$f2f1d7c3@djf.agrsci.dk>
References: <000001c44adc$46d03fd0$f2f1d7c3@djf.agrsci.dk>
Message-ID: <40C18FC9.3010806@optonline.net>

   This was discussed on the list toward the end of April 2004. 
You might have a look at the following:

http://finzi.psych.upenn.edu/R/Rhelp02a/archive/26983.html

   as well as other posts leading up to that.

   Also, your student might install the patched version:

http://mirrors.sunsite.dk/cran/bin/windows/base/rw1091alpha.exe

hope this helps,

Chuck Cleland

S??ren H??jsgaard wrote:
> One of my students have installed R1.9.0 on windows, and gets the fatal error
>     'invalid HOMEDRIVE'
> Can anyone help her/me out on that one?

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From breitling at lb-netz.de  Sat Jun  5 13:59:27 2004
From: breitling at lb-netz.de (Lutz Ph. Breitling)
Date: Sat, 05 Jun 2004 12:59:27 +0100
Subject: [R] poisson regression with robust error variance
  ('eyestudy')
In-Reply-To: <40BDFF8F.6090906@vanderbilt.edu>
References: <6.0.1.1.0.20040602101549.01f674e0@192.168.1.2>
	<Pine.A41.4.58.0406020732060.77094@homer08.u.washington.edu>
	<40BDFF8F.6090906@vanderbilt.edu>
Message-ID: <6.0.1.1.0.20040605125109.01c28148@192.168.1.2>

Thank you very much for your comments!

however, i still do not get it right.
robcov() accepts fit objects like lrm or ols objects as arguments,
but obviously not the glmD objects (or at least not as simple as that).
below some code to demonstrate.
what am i still doing wrong?

thx for your efforts-
lutz


id<-1:500
outcome<-sample(c(0,1), 500, replace=T, prob=c(.6, .4))
exposed<-sample(c(0,1), 500, replace=T, prob=c(.5, .5))
my.data<-data.frame(id=id, ou=outcome, ex=exposed)

model1<-glmD(ou~ex, my.data, family=poisson(link="log"))
robcov(model1)

Error in match.arg(type) : ARG should be one of deviance, pearson, working, 
response, partial




At 17:25 02.06.2004, Frank E Harrell Jr wrote:

>Thomas Lumley wrote:
>>On Wed, 2 Jun 2004, Lutz Ph. Breitling wrote:
>>
>>>Dear all,
>>>
>>>i am trying to redo the 'eyestudy' analysis presented on the site
>>>http://www.ats.ucla.edu/stat/stata/faq/relative_risk.htm
>>>with R (1.9.0), with special interest in the section on "relative risk
>>>estimation by poisson regression with robust error variance".
>>>
>>>so i guess rlm is the function to use. but what is its equivalent to the
>>>glm's argument "family" to indicate 'poisson'? or am i somehow totally
>>>wrong and this is not applicable here?
>>
>>No, no.  You want glm() and then a function to compute the robust
>>covariance matrix (there's robcov() in the Hmisc package), or use gee()
>>from the "gee" package or geese() from "geepack" with independence working
>>correlation.
>
>Slight correction: robcov in the Design package, can easily be used with 
>Design's glmD function.  -Frank
>
>>These are not outlier-resistant estimates of the regression coefficients,
>>they are model-agnostic estimates of the standard errors.
>>Stata is unusual in providing these covariance matrix estimates for just
>>about every regression estimator.  I think R should consider doing
>>something similar, but haven't got around to it.
>>         -thomas
>>
>>
>>>thx a lot-
>>>lutz
>>>
>>>
>>>=============================
>>>Lutz Ph. Breitling, CMd
>>
>>Thomas Lumley                   Assoc. Professor, Biostatistics
>>tlumley at u.washington.edu        University of Washington, Seattle
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>--
>Frank E Harrell Jr   Professor and Chair           School of Medicine
>                      Department of Biostatistics   Vanderbilt University
>
>
>
>
>---
>Incoming mail is certified Virus Free.
>Checked by AVG anti-virus system (http://www.grisoft.com).
>Version: 6.0.690 / Virus Database: 451 - Release Date: 22.05.2004

=============================
Lutz Ph. Breitling
Unit? des Recherches M?dicale
H?pital Albert Schweitzer
B.P. 118 Lambar?n? (GABON) 
-------------- next part --------------

---




From rossini at blindglobe.net  Sat Jun  5 13:53:11 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Sat, 05 Jun 2004 04:53:11 -0700
Subject: [R] Distributed computing with R
In-Reply-To: <40BF3A84.2000608@bank-banque-canada.ca> (Paul Gilbert's
	message of "Thu, 03 Jun 2004 10:49:40 -0400")
References: <026d01c448df$e9e76760$17470843@MITRE.ORG>
	<40BE4422.3090005@xss.de> <85ekoxr6t3.fsf@servant.blindglobe.net>
	<40BF3A84.2000608@bank-banque-canada.ca>
Message-ID: <85llj2z0ew.fsf@servant.blindglobe.net>


No -- the point is that they are mostly orthogonal solutions, not
mutually exclusive.  "Mostly" implies that sometimes, 1 + 1 = 0.5
(i.e. negative interactions can happen if you do not think through
what each is doing for scheduling/job transfer/migration).

For example, you can use SGE, OpenMOSIX, and SNOW-on-PVM (or other
message passing library) all together.

SGE and OpenMOSIX might not be too happy, since they are trying to do
the same thing at different levels, but it would "work" (perhaps
inefficiently).

best,
-tony


Paul Gilbert <pgilbert at bank-banque-canada.ca> writes:

> Tony
>
> Thanks, this categorization has cleared up a few things I have found
> confusing. But should I read this to mean that SNOW would not run  on
> a  system or kernel level parallel setup?
>
> Thanks,
> Paul Gilbert
>
> A.J. Rossini wrote:
>
>>Also see SNOW (which simplifies parallel programming, sits on top of
>>rpvm, Rmpi, or a socket-based system).
>>
>>Depends on whether you want parallelism on the:
>>
>>1. User-level -- the libraries such as PVM, LAM-MPI, etc will help,
>>                 and there are various packages which provide an API
>>                 to those.
>>
>>2. System-level -- then Condor, Sun Grid Engine / Maui scheduler, and
>>                   similar queueing/batching/allocation daemons will
>>                   help (computational grid software is usually a
>>                   generalization of this which adds authentication
>>                   and resource allocation).
>>
>>3. Kernel-level -- then OpenMOSIX, BPROC, etc will help.
>>
>>They are mostly orthogonal.  Mostly... :-).
>>
>>best,
>>-tony
>>
>>
>>
>>Armin Roehrl <armin at xss.de> writes:
>>
>>
>>>If you do some programming, you might want to look at MPI.
>>>R-extensions for MPI exist  (RMPI).
>>>
>>>It all depends a lot on what kind of usage you envisage of your cluster.
>>>Open-PBS is also a good batch system. Maybe you also want to
>>>look at Mosix, which is a modified linux system.
>>>
>>>Depending on what your ultimate computing ressources are,
>>>maybe also look at IBM's Globus toolkit.
>>>
>>>Parallel programming is fun. The world is inherently parallel!
>>>Ciao,
>>>    -Armin.
>>>
>>>----------------------------------------
>>>Armin Roehrl, http://www.approximity.com
>>>We manage risk
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>>
>>>
>>
>>
>
>

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From filippo.piro at tiscali.it  Sat Jun  5 14:41:27 2004
From: filippo.piro at tiscali.it (Filippo Piro)
Date: Sat, 5 Jun 2004 14:41:27 +0200
Subject: [R] saving metafiles in R1090
Message-ID: <005101c44afa$6bd64360$48425452@LocalHost>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040605/7d818785/attachment.pl

From pburns at pburns.seanet.com  Sat Jun  5 14:35:18 2004
From: pburns at pburns.seanet.com (Patrick Burns)
Date: Sat, 05 Jun 2004 13:35:18 +0100
Subject: [R] How to Describe R to Finance People
References: <20040604115038.NQWW12806.web4-rme.xtra.co.nz@kevinlpt>
Message-ID: <40C1BE06.3070006@pburns.seanet.com>

For general points, you can look at "An Introduction to the S Language"
in the tutorials section of the Burns Statistics website.

For your specific audience, it sounds like property is of special 
consideration
so you could come up with an example of how R can easily
analyse spatial relationships (graphically or analytically) that would be
virtually impossible without it.

My viewpoint on the issue of cost is that it is the free as in "libre" 
that is
the most important part in finance.  Once R breaks through the gate
once, it is free to spread.  Often the bureaucratic cost of buying something
is a much larger impediment than the dollar cost.


Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Ko-Kang Kevin Wang wrote:

>Hi,
>
>I've been doing a joint research with someone from the Property
>Department here and she is about to give a presentation on the
>results.  The audience will include people from Property and Finance,
>and she is wondering how to describe R to these people (as I used R to
>do the analyses), since she has never even heard of R before our joint
>research (and has been using SPSS).  The difficult part is she has
>only about 1 ~ 2 minutes to talk about R.
>
>The following is what I have in mind, any suggestions from people in
>Finance will be greatly appreciated!  (From our research together I
>think it may be safe to assume the audience will know, or at least
>have heard of, basic statistical terminology such as multiple linear
>regression and dummy variables).
>
>\begin{quote}
>R was originally developed by Dr. Ross Ihaka and Dr. Robert Gentleman
>from the Department of Statistics at the University of Auckland in
>1992.  It is free and in the last decade it has evolved into one of
>the most powerful statistical software, with over 150 user-contributed
>add-on packages.  It is not only used by statisticians or scientists,
>but also econometricians and people in finance due to its cost (FREE)
>and its powerfulness.
>
>Although it has a slightly higher learning curve than SPSS-like
>program, it gets easier to use once one is familiar with it.  One of
>the main advantage it has over SPSS-like software is that you do not
>need to explicitly create dummy variables.  You only need to specify
>your dependent variable and independent variables and R will fit it
>(and create dummy variables automatically) for you.
>
>It also has many state-of-art free resources, including manuals,
>contributed tutorials and documentations, online.  A free mailing list
>is also available for people to ask questions and questions are
>usually answered by more experienced users around the world within a
>few hours (sometimes even within minutes).
>\end{quote}
>
>As mentioned above, she was rather impressed when I mention that one
>does not need to create dummy variables in R.  Therefore I am thinking
>she might be interested in mentioning it in her talk.
>
>I have never had experience of trying to introduce R to
>non-Scientists, hence I would appreciate any comments!
>
>Cheers,
>
>Kevin
>
>--------------------------------------------
>Ko-Kang Kevin Wang, MSc(Hon)
>SLC Stats Workshops Co-ordinator
>The University of Auckland
>New Zealand
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>



From f.harrell at vanderbilt.edu  Sat Jun  5 14:46:12 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sat, 05 Jun 2004 08:46:12 -0400
Subject: [R] poisson regression with robust error variance  ('eyestudy')
In-Reply-To: <6.0.1.1.0.20040605125109.01c28148@192.168.1.2>
References: <6.0.1.1.0.20040602101549.01f674e0@192.168.1.2>
	<Pine.A41.4.58.0406020732060.77094@homer08.u.washington.edu>
	<40BDFF8F.6090906@vanderbilt.edu>
	<6.0.1.1.0.20040605125109.01c28148@192.168.1.2>
Message-ID: <40C1C094.4050708@vanderbilt.edu>

Lutz Ph. Breitling wrote:
> Thank you very much for your comments!
> 
> however, i still do not get it right.
> robcov() accepts fit objects like lrm or ols objects as arguments,
> but obviously not the glmD objects (or at least not as simple as that).
> below some code to demonstrate.
> what am i still doing wrong?
> 
> thx for your efforts-
> lutz
> 
> 
> id<-1:500
> outcome<-sample(c(0,1), 500, replace=T, prob=c(.6, .4))
> exposed<-sample(c(0,1), 500, replace=T, prob=c(.5, .5))
> my.data<-data.frame(id=id, ou=outcome, ex=exposed)
> 
> model1<-glmD(ou~ex, my.data, family=poisson(link="log"))
> robcov(model1)
> 
> Error in match.arg(type) : ARG should be one of deviance, pearson, 
> working, response, partial

Sorry I didn't think of that sooner.  robcov needs the residuals method 
for the fitter to allow a type="score" or type="hscore" (for Efron's 
method) argument.  Until someone adds score residuals to residuals.glm 
robcov will not work for you.  residuals.lrm and residuals.coxph are 
examples where score residuals are computed.  You can get robust 
variance-covariance estimates with the bootstrap using bootcov for glmD 
fits.  Oddly in your example I am finding that the bootstrap variances 
are lower than the information-matrix-based ones.

Frank Harrell

> 
> At 17:25 02.06.2004, Frank E Harrell Jr wrote:
> 
>> Thomas Lumley wrote:
>>
>>> On Wed, 2 Jun 2004, Lutz Ph. Breitling wrote:
>>>
>>>> Dear all,
>>>>
>>>> i am trying to redo the 'eyestudy' analysis presented on the site
>>>> http://www.ats.ucla.edu/stat/stata/faq/relative_risk.htm
>>>> with R (1.9.0), with special interest in the section on "relative risk
>>>> estimation by poisson regression with robust error variance".
>>>>
>>>> so i guess rlm is the function to use. but what is its equivalent to 
>>>> the
>>>> glm's argument "family" to indicate 'poisson'? or am i somehow totally
>>>> wrong and this is not applicable here?
>>>
>>>
>>> No, no.  You want glm() and then a function to compute the robust
>>> covariance matrix (there's robcov() in the Hmisc package), or use gee()
>>> from the "gee" package or geese() from "geepack" with independence 
>>> working
>>> correlation.
>>
>>
>> Slight correction: robcov in the Design package, can easily be used 
>> with Design's glmD function.  -Frank
>>
>>> These are not outlier-resistant estimates of the regression 
>>> coefficients,
>>> they are model-agnostic estimates of the standard errors.
>>> Stata is unusual in providing these covariance matrix estimates for just
>>> about every regression estimator.  I think R should consider doing
>>> something similar, but haven't got around to it.
>>>         -thomas
>>>
>>>
>>>> thx a lot-
>>>> lutz
>>>>
>>>>
>>>> =============================
>>>> Lutz Ph. Breitling, CMd
>>>
>>>
>>> Thomas Lumley                   Assoc. Professor, Biostatistics
>>> tlumley at u.washington.edu        University of Washington, Seattle
> 
> =============================
> Lutz Ph. Breitling
> Unit?? des Recherches M??dicale
> H??pital Albert Schweitzer
> B.P. 118 Lambar??n?? (GABON)
> 
> 

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From f.harrell at vanderbilt.edu  Sat Jun  5 15:01:15 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sat, 05 Jun 2004 09:01:15 -0400
Subject: [R] How to Describe R to Finance People
In-Reply-To: <40C1BE06.3070006@pburns.seanet.com>
References: <20040604115038.NQWW12806.web4-rme.xtra.co.nz@kevinlpt>
	<40C1BE06.3070006@pburns.seanet.com>
Message-ID: <40C1C41B.7020504@vanderbilt.edu>

One other important point in "selling" R is that its use results in 
analysts doing a better job.  I find that many SAS users for example 
routinely assume that all covariable effects are linear because it is 
messy to do otherwise in SAS.  S has a natural modeling language for 
flexible nonlinear effects (primarily because predictors can be 
automatically-generated matrices containing basis functions) in addition 
to computing dummy variables "secretly" and computing product terms for 
interactions on the fly.

I prefer open source software for many reasons, but a major reason that 
R is superior for statistical analysis and graphics today is its 
functionality and extendibility.  This leads to better analyses.

Frank Harrell



From andy_liaw at merck.com  Sat Jun  5 15:16:25 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 5 Jun 2004 09:16:25 -0400
Subject: [R] saving metafiles in R1090
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7E2F@usrymx25.merck.com>

> From: Filippo Piro
> 
> I find that the GUI command 'save ... as enhanced metafile' 
> gives most of the times only a void file (1kb) in the Windows 
> version 1.9.0
> please, tell me what I have missed

... the fact that this has been reported (on this list, no less) and fixed
in the patched version, to be R-1.9.1.  Please search the archive for this
list, as the posting guide suggests.  (You've read that, haven't you?)

Andy

> Filippo Piro
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ivo.welch at yale.edu  Sat Jun  5 16:11:18 2004
From: ivo.welch at yale.edu (ivo welch)
Date: Sat, 05 Jun 2004 10:11:18 -0400
Subject: [R] Re: How to Describe R to Finance People
In-Reply-To: <200406051002.i55A2QlY012543@hypatia.math.ethz.ch>
References: <200406051002.i55A2QlY012543@hypatia.math.ethz.ch>
Message-ID: <40C1D486.2020506@yale.edu>


well, it depends on who you call finance people.  i am a finance 
professor, and i use R for my own work these days.  two of my colleagues 
are using S on occasion, S being "close enough" IMHO.

how about students?  I am also writing an introductory finance text 
book, which is currently freely available from my website 
(http://welch.som.yale.edu/book/).  all the figures will eventually be 
done in R (at the moment, some are still in gnuplot), the book will say 
so, and i will provide the code for it.  the statistical analysis is 
done in R, but much of it is not shown (it is just an intro text book).  
hopefully, this will get more finance students asking "what is this 
program?  how can i use it? etc."

but R has also huge drawbacks.  most importantly, there is no good 
*current* textbook for an intro R user.  that is, not for the fancy 
statistical techniques, but lots about data manipulation, plots, linear 
regression, heteroskedasticity and related (white-like) corrections, 
programming, "cookbook" (ala perl cookbook---more about the simple 
stuff:  how to delete or insert a row, how to delete or insert a column, 
typical problems, especially when doing IO).  so, honestly, i cannot 
recommend R to my finance students right now.   this mailing 
list---wonderful as it is [though sometimes "grumpy"]---cannot be a 
substitute for such an intro R textbook.  i cannot ask 300 students to 
use it as their support hotline.  i am afraid that if R becomes more 
successful, this mailing list will be overwhelmed.  the 10-30 people in 
the know who donate their time to help here just cannot do it.   we 
definitely do need this R textbook.  and, though I love the first parts 
of Ripley&Venables, they want it to be a "stats book in R", not a book 
about R.  (witness brian ripley's annoyed reaction everytime i tried to 
suggest elaborations on the first part, or them writing another book.)

one more big problem:  the name "R".  I cannot easily specify to do a 
comprehensive google search on subject matter "insert and R".  A single 
letter like R just does not connect well with google.  this is of course 
steeped in too much history, but a name change would help---calling it 
some random 6-letter combination.

regards,

/ivo welch



From baron at psych.upenn.edu  Sat Jun  5 16:54:10 2004
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Sat, 5 Jun 2004 10:54:10 -0400
Subject: [R] Re: How to Describe R to Finance People
In-Reply-To: <40C1D486.2020506@yale.edu>
References: <200406051002.i55A2QlY012543@hypatia.math.ethz.ch>
	<40C1D486.2020506@yale.edu>
Message-ID: <20040605145410.GA1521@psych>

On 06/05/04 10:11, ivo welch wrote:
>but R has also huge drawbacks.  most importantly, there is no good
>*current* textbook for an intro R user.  that is, not for the fancy
>statistical techniques, but lots about data manipulation, plots, linear
>regression, heteroskedasticity and related (white-like) corrections,
>programming, "cookbook" (ala perl cookbook---more about the simple
>stuff:  how to delete or insert a row, how to delete or insert a column,
>typical problems, especially when doing IO).  so, honestly, i cannot
>recommend R to my finance students right now.

I recommend to students that they read the Introduction that
comes with R for this purpose, then the "Notes..." that I wrote
with Yuelin Li.  See the "R references" section of my R page
(below).  Unfortunately, these notes are not meant to be a
complete text, and they don't even cover all the points you just
mentioned (although they probably should).  Yuelin, if I may
speak for him, has expressed some interest in expanding our Notes
into an actual textbook, eventually, so your comments may
encourage him.  Still, I think you might find these notes helpful
for students.  (And there are other similar sets of introductory
documents in the R contributed documents section:
http://cran.r-project.org/other-docs.html.)  I also recommend a
reference card (see my page, which links to another one in
addition to the one in the R contributed docs page), which is a
good substitute for a GUI.  And I provide templates for the kinds
of analyses that students will do, so that the students can
modify the templates without understanding everything in them.
That said, I'm not talking about a class of 300.  I end up doing
a lot of help by email.  It would be overwhelming with a large
class (or the students would be too shy to ask for help, in which
case _they_ would be overwhelmed).

>one more big problem:  the name "R".  I cannot easily specify to do a
>comprehensive google search on subject matter "insert and R".  A single
>letter like R just does not connect well with google.  this is of course
>steeped in too much history, but a name change would help---calling it
>some random 6-letter combination.

Google is pretty smart.  If you enter "R" and nothing else, you
get to the right place.  But you might also find my R page
helpful.  My students use that too.  I think that the name R is
here to stay.  It has a certain coolness about it.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron
R search page:        http://finzi.psych.upenn.edu/



From edd at debian.org  Sat Jun  5 16:57:46 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sat, 5 Jun 2004 09:57:46 -0500
Subject: [R] Re: How to Describe R to Finance People
In-Reply-To: <40C1D486.2020506@yale.edu>
References: <200406051002.i55A2QlY012543@hypatia.math.ethz.ch>
	<40C1D486.2020506@yale.edu>
Message-ID: <20040605145746.GA10718@sonny.eddelbuettel.com>

On Sat, Jun 05, 2004 at 10:11:18AM -0400, ivo welch wrote:
> but R has also huge drawbacks.  most importantly, there is no good 
> *current* textbook for an intro R user.  that is, not for the fancy 
> statistical techniques, but lots about data manipulation, plots, linear 
> regression, heteroskedasticity and related (white-like) corrections, 

I disagree. Peter's book does, as do several of the free pdf files on the R
website.

Having come here from financial/computational econometrics, I'd agree that
the terminology is different. There may not be a command 'ols' in the intro
texts, and you may not find a reference to HAC estimator that is called
Newey-West.

But that doesn't mean it doesn't exist. Asking Google to look for 'r-help
Newey' as in 
	http://www.google.com/search?q=r-help%20Newey
yields 50 hits (some are doubles, though).	

> programming, "cookbook" (ala perl cookbook---more about the simple 
> stuff:  how to delete or insert a row, how to delete or insert a column, 
> typical problems, especially when doing IO).  so, honestly, i cannot

It's all in the main intro book. S being an established language, there are
also many published (i.e. dead tree format) and unpublished texts. E.g. you
could do worse than starting your students off on Pat Burns' 'Guide to the
unwilling S user'. There are several S books to pick from.

That said, I do know that several people agree with your assessment that an
additional R book for Finance would find a market.  I'd expect there to be a
several of those books within a few years. Maybe you'll even write one?

> one more big problem:  the name "R".  I cannot easily specify to do a 
> comprehensive google search on subject matter "insert and R".  A single 

See above: substitute r-help or site:r-project.org for R and you're done.
Last but not least, there _dedicated_ R search engines linked to from the
main site. Did you try those?

Beste Gruesse,  Dirk

PS I'll CC this to r-sig-finance, we can discuss more there.

-- 
FEATURE:  VW Beetle license plate in California



From ggrothendieck at myway.com  Sat Jun  5 17:28:04 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 5 Jun 2004 15:28:04 +0000 (UTC)
Subject: [R] three button mouse? or mouse modifiers?
References: <40C0E8E8.6040304@unc.edu>
Message-ID: <loom.20040605T172620-132@post.gmane.org>

Jonathan M. Lees <jonathan_lees <at> unc.edu> writes:
> Is there any way to get R to recognize a three button mouse
> or a mouse click with a modifier, like control-left mouse
> or meta-leftmouse
> or shift ?

The tcltk package supports three button mice.  See this thread

   http://maths.newcastle.edu.au/~rking/R/help/03b/7168.html



From gustavo at estatcamp.com.br  Sat Jun  5 19:46:42 2004
From: gustavo at estatcamp.com.br (Gustavo Pinheiro)
Date: Sat,  5 Jun 2004 14:46:42 -0300
Subject: [R] Building R as shared library (dll) in Windows
Message-ID: <20040605174642.14875.qmail@hm101.locaweb.com.br>

Hello,

I've read that to compile R as a shared library in Linux you have to issue the following command before make:
./configure --enable-R-shlib

How is that done in Windows? There is no 'configure' in src/gnuwin32 and if it has to go somewhere in Mkrules, I could not figure out exactly where.

Help please.



From ripley at stats.ox.ac.uk  Sat Jun  5 20:07:52 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 5 Jun 2004 19:07:52 +0100 (BST)
Subject: [R] Building R as shared library (dll) in Windows
In-Reply-To: <20040605174642.14875.qmail@hm101.locaweb.com.br>
Message-ID: <Pine.LNX.4.44.0406051903230.9779-100000@gannet.stats>

On Sat, 5 Jun 2004, Gustavo Pinheiro wrote:

> I've read that to compile R as a shared library in Linux you have to issue the following command before make:
> ./configure --enable-R-shlib
> 
> How is that done in Windows? There is no 'configure' in src/gnuwin32 and if it has to go somewhere in Mkrules, I could not figure out exactly where.
> 
> Help please.

There are no `shared libraries' under Windows (and in fact
--enable-R-shlib builds a dynamic library rather than a shared library on
systems which care about the distinction).  However, it is the default to
build R as R.dll with import library libR.a.  You could try reading the
documentation, especially that on (D)COM interfaces to R, which make this
process easier.  However you can link your own code against R.dll, and
there are examples in src/gnuwin/front-ends (and a README file there).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bxc at steno.dk  Sat Jun  5 21:51:14 2004
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Sat, 5 Jun 2004 21:51:14 +0200
Subject: [R] coef and vcoc for polr inconsistent??
Message-ID: <0ABD88905D18E347874E0FB71C0B29E901D8E215@exdkba022.novo.dk>

Is the following an inconsistency, programming glitch or a feature? 
One would expect that vcov(obj) was the variance-covariance of
coef(obj), 
but apparently this is not the case for polr objects:

> x <- rnorm( 100 )
> y <- rnorm( 100 )
> ff <- factor( sample( 1:4, 100, replace=T ) )
> pm <- polr( ff ~ x + y )
> coef( pm )
         x          y 
0.21219010 0.03558506 
> vcov( pm )

Re-fitting to get Hessian

               x            y          1|2          2|3          3|4
x    0.033462790 -0.002414745 -0.006183451 -0.004164503 -0.001585546
y   -0.002414745  0.032953020  0.003821232  0.003645342  0.003756031
1|2 -0.006183451  0.003821232  0.060134608  0.029651508  0.018105699
2|3 -0.004164503  0.003645342  0.029651508  0.041604224  0.025465751
3|4 -0.001585546  0.003756031  0.018105699  0.025465751  0.051576286

But of course you CAN get wat you want:

> summary( pm )$coef

Re-fitting to get Hessian

          Value Std. Error    t value
x    0.21219010  0.1829284  1.1599628
y    0.03558506  0.1815297  0.1960289
1|2 -1.29481930  0.2452236 -5.2801580
2|3 -0.22054056  0.2039711 -1.0812342
3|4  0.98703785  0.2271041  4.3461907

> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    1              
minor    9.0            
year     2004           
month    04             
day      12             
language R       


----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc



From dmurdoch at pair.com  Sat Jun  5 22:41:41 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Sat, 05 Jun 2004 16:41:41 -0400
Subject: [R] Re: How to Describe R to Finance People
In-Reply-To: <40C1D486.2020506@yale.edu>
References: <200406051002.i55A2QlY012543@hypatia.math.ethz.ch>
	<40C1D486.2020506@yale.edu>
Message-ID: <qnb4c09uvsceb8vpllidhuuku5a58pngil@4ax.com>

On Sat, 05 Jun 2004 10:11:18 -0400, ivo welch <ivo.welch at yale.edu>
wrote:


>one more big problem:  the name "R".  I cannot easily specify to do a 
>comprehensive google search on subject matter "insert and R".  A single 
>letter like R just does not connect well with google. 

I just did a search for the string "insert and R", and it was indeed
useless.  However, "R and insert" came up with 4 relevant hits in the
first 10 and the usual warning about using "and"; when I leave it out
and type "R insert" I got up to 6 relevant hits out of 10.

So I think the name R is not so bad, but Google is a little more
subtle in its searches than I would have guessed.

Duncan Murdoch



From jfox at mcmaster.ca  Sun Jun  6 01:15:07 2004
From: jfox at mcmaster.ca (John Fox)
Date: Sat, 5 Jun 2004 19:15:07 -0400
Subject: [R] coef and vcoc for polr inconsistent??
In-Reply-To: <0ABD88905D18E347874E0FB71C0B29E901D8E215@exdkba022.novo.dk>
Message-ID: <20040605231507.OBAR26419.tomts10-srv.bellnexxia.net@JohnDesktop8300>

Dear Bendix,

vcov() is indeed giving you the covariances among the coefficients and
inter-level cutpoints, as you can see by taking the square-roots of the
diagonal entries of the covariance matrix that it reports and comparing
these with the values in the table provided by summary() -- or was your
point that coef() doesn't report the cutpoints?

I hope that this helps,
 John 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of BXC 
> (Bendix Carstensen)
> Sent: Saturday, June 05, 2004 2:51 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] coef and vcoc for polr inconsistent??
> 
> Is the following an inconsistency, programming glitch or a feature? 
> One would expect that vcov(obj) was the variance-covariance 
> of coef(obj), but apparently this is not the case for polr objects:
> 
> > x <- rnorm( 100 )
> > y <- rnorm( 100 )
> > ff <- factor( sample( 1:4, 100, replace=T ) ) pm <- polr( 
> ff ~ x + y ) 
> > coef( pm )
>          x          y 
> 0.21219010 0.03558506 
> > vcov( pm )
> 
> Re-fitting to get Hessian
> 
>                x            y          1|2          2|3          3|4
> x    0.033462790 -0.002414745 -0.006183451 -0.004164503 -0.001585546
> y   -0.002414745  0.032953020  0.003821232  0.003645342  0.003756031
> 1|2 -0.006183451  0.003821232  0.060134608  0.029651508  0.018105699
> 2|3 -0.004164503  0.003645342  0.029651508  0.041604224  0.025465751
> 3|4 -0.001585546  0.003756031  0.018105699  0.025465751  0.051576286
> 
> But of course you CAN get wat you want:
> 
> > summary( pm )$coef
> 
> Re-fitting to get Hessian
> 
>           Value Std. Error    t value
> x    0.21219010  0.1829284  1.1599628
> y    0.03558506  0.1815297  0.1960289
> 1|2 -1.29481930  0.2452236 -5.2801580
> 2|3 -0.22054056  0.2039711 -1.0812342
> 3|4  0.98703785  0.2271041  4.3461907
> 
> > version
>          _              
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    1              
> minor    9.0            
> year     2004           
> month    04             
> day      12             
> language R       
>



From gblevins at mn.rr.com  Sun Jun  6 06:16:26 2004
From: gblevins at mn.rr.com (Greg Blevins)
Date: Sat, 5 Jun 2004 23:16:26 -0500
Subject: [R] Request help writing a function
Message-ID: <001d01c44b7d$09087180$aaca5e18@glblpyirxqz5lp>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040605/f7e743cc/attachment.pl

From ggrothendieck at myway.com  Sun Jun  6 06:51:55 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 6 Jun 2004 04:51:55 +0000 (UTC)
Subject: [R] Request help writing a function
References: <001d01c44b7d$09087180$aaca5e18@glblpyirxqz5lp>
Message-ID: <loom.20040606T064601-325@post.gmane.org>


I think you need to give an example of input and output to clarify your
question but in case this helps, if x is a numeric vector then
mean(x==2, na.rm=T) gives the proportion of 2s among the non-NAs in x.

Greg Blevins <gblevins <at> mn.rr.com> writes:

: 
: I have been wrestling with this function for quite a while, and am not 
making headway.
: 
: 1) I want to apply a function to the following columns of a dataframe:
: 
: myfunction. <- apply(ph5028[,c(83:107)],2,function(x) ...
: 
: 2) Within each of the above columns there is a single numeric code, 1, 2 or 
3 or an NA.
: 
: 3) My goal is to determine the percent of time each person used a 2 code.  
So if a person across these columns had
: say 8 numerical entries and if 4 of these were the number 2, the answer for 
that person would be 50%.
: 
: Best regards,
: 
: Greg Blevins
: The Market Solutions Group



From gblevins at mn.rr.com  Sun Jun  6 07:43:59 2004
From: gblevins at mn.rr.com (Greg Blevins)
Date: Sun, 6 Jun 2004 00:43:59 -0500
Subject: [R] Request help writing a function
References: <001d01c44b7d$09087180$aaca5e18@glblpyirxqz5lp>
	<loom.20040606T064601-325@post.gmane.org>
Message-ID: <00f101c44b89$45deab40$aaca5e18@glblpyirxqz5lp>

Gabor,
Here is some further clarification using a toy example.
dataframe layout

                v1               v2                   v3                v4

Person1    1                NA                 3                  2

Person2     2                 2                    2                  2

Person3     1                NA                  2                  2

I am looking for the proportion of time each person used a 2 code across the
variables v1 to v4. For example, using the above data, the answer would look
as follows:

Person1     33%

Person2     100%

Person3     66%

Thanks again,

Greg Blevins

----- Original Message ----- 
From: "Gabor Grothendieck" <ggrothendieck at myway.com>
To: <r-help at stat.math.ethz.ch>
Sent: Saturday, June 05, 2004 11:51 PM
Subject: Re: [R] Request help writing a function


>
> I think you need to give an example of input and output to clarify your
> question but in case this helps, if x is a numeric vector then
> mean(x==2, na.rm=T) gives the proportion of 2s among the non-NAs in x.
>
> Greg Blevins <gblevins <at> mn.rr.com> writes:
>
> :
> : I have been wrestling with this function for quite a while, and am not
> making headway.
> :
> : 1) I want to apply a function to the following columns of a dataframe:
> :
> : myfunction. <- apply(ph5028[,c(83:107)],2,function(x) ...
> :
> : 2) Within each of the above columns there is a single numeric code, 1, 2
or
> 3 or an NA.
> :
> : 3) My goal is to determine the percent of time each person used a 2
code.
> So if a person across these columns had
> : say 8 numerical entries and if 4 of these were the number 2, the answer
for
> that person would be 50%.
> :
> : Best regards,
> :
> : Greg Blevins
> : The Market Solutions Group
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>



From nielssteenkrogh at hotmail.com  Sun Jun  6 10:50:27 2004
From: nielssteenkrogh at hotmail.com (Niels Steen Krogh)
Date: Sun, 06 Jun 2004 10:50:27 +0200
Subject: [R] Datacube or hypercube or olap-functionality in R
Message-ID: <BAY7-F37vUbLkiChiwV0000f111@hotmail.com>

Dear R-users.

I'm going to develop olap-functionality for easy data-description (and 
simple outputs like sum, median and avg) for large datasets (20 MB-1 GB).

We use R (in combination with python, LaTex, MySQL, Access and HDF5) for our 
statistical analysis and reporting and - if possible - would like to use 
R-api on the serverside of a webbased olap-solution. (connecting through rpy 
and RSOAP).
Seaching with "hypercube", "datacube" or "olap" in the R-archives didn't 
give usable hints.
Searching sourceforge gave hits for Mondrian (JAVA) and Bee (PERL) but none 
of them use R for the backend.

Do you know of any prior work or writings on this subject using R?

R-platform:
Linux og Windows
Latest version

/Niels









Cand. Polit.
Niels Steen Krogh
Solsortvej 44
2000 F.

Tlf: 3888 8613

ZiteLab / EmpoweR youR data with R, Zope and SOAP

_________________________________________________________________
F?? mere ud af din Hotmail



From ccleland at optonline.net  Sun Jun  6 11:01:44 2004
From: ccleland at optonline.net (Chuck Cleland)
Date: Sun, 06 Jun 2004 05:01:44 -0400
Subject: [R] Request help writing a function
In-Reply-To: <00f101c44b89$45deab40$aaca5e18@glblpyirxqz5lp>
References: <001d01c44b7d$09087180$aaca5e18@glblpyirxqz5lp>
	<loom.20040606T064601-325@post.gmane.org>
	<00f101c44b89$45deab40$aaca5e18@glblpyirxqz5lp>
Message-ID: <40C2DD78.9090506@optonline.net>

Greg:
   I think you want to apply the function Gabor suggested to the 
rows of your subsetted dataframe:

mydata <- as.data.frame(matrix(c
              (1, NA, 3, 2,
               2,  2, 2, 2,
               1, NA, 2, 2), byrow = TRUE, ncol = 4))

apply(mydata, 1, function(x){mean(x == 2, na.rm = TRUE)})

         1         2         3
0.3333333 1.0000000 0.6666667

   If you want percentages rather than proportions, just convert 
within the function.

apply(mydata, 1, function(x){mean(x == 2, na.rm = TRUE)*100})

hope this helps,

Chuck Cleland

Greg Blevins wrote:
> Here is some further clarification using a toy example.
> dataframe layout
> 
>                 v1               v2                   v3                v4
> 
> Person1    1                NA                 3                  2
> 
> Person2     2                 2                    2                  2
> 
> Person3     1                NA                  2                  2
> 
> I am looking for the proportion of time each person used a 2 code across the
> variables v1 to v4. For example, using the above data, the answer would look
> as follows:
> 
> Person1     33%
> 
> Person2     100%
> 
> Person3     66%

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From kan_liu1 at yahoo.com  Sun Jun  6 11:47:23 2004
From: kan_liu1 at yahoo.com (kan Liu)
Date: Sun, 6 Jun 2004 02:47:23 -0700 (PDT)
Subject: [R] Average R-squared of model1 to model n
Message-ID: <20040606094723.11945.qmail@web60608.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040606/1b288840/attachment.pl

From alex.bach at irta.es  Sun Jun  6 17:38:50 2004
From: alex.bach at irta.es (Alex Bach)
Date: Sun, 6 Jun 2004 17:38:50 +0200
Subject: [R] Repeated measures
Message-ID: <9BDA40D4-B7CF-11D8-89A1-000393D55A8C@irta.es>

Dear R-gurus,

I am pretty much new on R.
I am trying to to do a repeated analysis of a linear mixed model with 
R, and I consistently fail...

The problem is: Cow is the random factor, treatment is the fixed 
factor. The dependent variable is milk yield, which is measured several 
times (repeatedly over time), thus there is another variable which is 
time (i.e. week).

The model would be something like this: milk yield = cow + treatment + 
time + treatment*time

With time as a repeated measure.

Would some one be kind enough to guide on how could I set up the 
statement in R?
I imagine I have to use LME but I have not been smart enough to figure 
out how to do it by just reading its manual. Also, with SAS there is a 
nice contrast, called SLICE that allows you to test when in time there 
is difference between treatments. I do not know if there is something 
like this is R.

Thank you very much!

PD. For SAS users, what I am using in SAS to perform this analysis 
(with an Autoregressive covariance structure) the program would read 
like this:

proc MIXED covtest;
CLASS cow treat time;
MODEL yield=  treat time treat*time;
REPEATED time/SUB=cow(treat) TYPE=ARH(1) R RCORR;
RANDOM cow;
LSMEANS treat time treat*time/SLICE = time;
RUN;


Thank you very much,

Sincerely,

Alex



From alex.bach at irta.es  Sun Jun  6 17:52:12 2004
From: alex.bach at irta.es (Alex Bach)
Date: Sun, 6 Jun 2004 17:52:12 +0200
Subject: [R] Repeated measures
Message-ID: <7986C619-B7D1-11D8-B147-000393D55A8C@irta.es>

Dear R-gurus,

I am pretty much new on R.
I am trying to to do a repeated analysis of a linear mixed model with 
R, and I consistently fail...

The problem is: Cow is the random factor, treatment is the fixed 
factor. The dependent variable is milk yield, which is measured several 
times (repeatedly over time), thus there is another variable which is 
time (i.e. week).

The model would be something like this: milk yield = cow + treatment + 
time + treatment*time

With time as a repeated measure.

Would some one be kind enough to guide on how could I set up the 
statement in R?
I imagine I have to use LME but I have not been smart enough to figure 
out how to do it by just reading its manual. Also, with SAS there is a 
nice contrast, called SLICE that allows you to test when in time there 
is difference between treatments. I do not know if there is something 
like this is R.

Thank you very much!

PD. For SAS users, what I am using in SAS to perform this analysis 
(with an Autoregressive covariance structure) the program would read 
like this:

proc MIXED covtest;
CLASS cow treat time;
MODEL yield=  treat time treat*time;
REPEATED time/SUB=cow(treat) TYPE=ARH(1) R RCORR;
RANDOM cow;
LSMEANS treat time treat*time/SLICE = time;
RUN;


Thank you very much,

Sincerely,

Alex



From spencer.graves at pdf.com  Sun Jun  6 18:21:49 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 06 Jun 2004 09:21:49 -0700
Subject: [R] Repeated measures
In-Reply-To: <7986C619-B7D1-11D8-B147-000393D55A8C@irta.es>
References: <7986C619-B7D1-11D8-B147-000393D55A8C@irta.es>
Message-ID: <40C3449D.4020406@pdf.com>

      1.  You didn't say which "manual" you were reading on "lme", if 
you have not consulted Pinheiro and Bates (2000) Mixed-Effects Models in 
S and S-Plus (Springer), I suggest you do so.  The issues are discussed 
in greater depth with many examples.  I found this book well worth the 
time and money I invested in it. 

      2.  Have you considered the following: 

      lme(milk yield = cow + treatment + time + treatment*time, random = 
~time|cow ... )

      This will NOT have the ARCH(1) R RCORR error structure;  "lme" can 
handle certain types of autocorrelated error structures, but I don't 
remember the details at the moment.  Pinheiro and Bates discuss some 
capabilities of this nature. 

      hope this helps. 
      spencer graves

Alex Bach wrote:

> Dear R-gurus,
>
> I am pretty much new on R.
> I am trying to to do a repeated analysis of a linear mixed model with 
> R, and I consistently fail...
>
> The problem is: Cow is the random factor, treatment is the fixed 
> factor. The dependent variable is milk yield, which is measured 
> several times (repeatedly over time), thus there is another variable 
> which is time (i.e. week).
>
> The model would be something like this: milk yield = cow + treatment + 
> time + treatment*time
>
> With time as a repeated measure.
>
> Would some one be kind enough to guide on how could I set up the 
> statement in R?
> I imagine I have to use LME but I have not been smart enough to figure 
> out how to do it by just reading its manual. Also, with SAS there is a 
> nice contrast, called SLICE that allows you to test when in time there 
> is difference between treatments. I do not know if there is something 
> like this is R.
>
> Thank you very much!
>
> PD. For SAS users, what I am using in SAS to perform this analysis 
> (with an Autoregressive covariance structure) the program would read 
> like this:
>
> proc MIXED covtest;
> CLASS cow treat time;
> MODEL yield=  treat time treat*time;
> REPEATED time/SUB=cow(treat) TYPE=ARH(1) R RCORR;
> RANDOM cow;
> LSMEANS treat time treat*time/SLICE = time;
> RUN;
>
>
> Thank you very much,
>
> Sincerely,
>
> Alex
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Sun Jun  6 20:01:37 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 6 Jun 2004 18:01:37 +0000 (UTC)
Subject: [R] Average R-squared of model1 to model n
References: <20040606094723.11945.qmail@web60608.mail.yahoo.com>
Message-ID: <loom.20040606T195146-794@post.gmane.org>


Suppose m=2, Y1=Y and Y2= -Y.  Then (b) is zero so (a) must be
greater or equal to (b).  Thus (b) is not necessarily greater 
than (a).


kan Liu <kan_liu1 <at> yahoo.com> writes:

: 
: Hi,
: 
: We got a question about interpretating R-suqared.
: 
: The actual outputs for a test dataset is X=(x1,x2, ..., xn).
: model 1 predicted the outputs as Y1=(y11,y12,..., y1n)
: model n predicted the outputs as Y2=(y21,y22,..., y2n)
: 
: ... 
: model m predicted the outputs as Ym=(ym1,ym2,..., ymn)
: 
: Now we have two ways to calculate R squared to evaluate the average 
performance of committee model.
: 
: (a) Calculate R squared between (X, Y1), (X, Y2), ..., (X,Ym), and then 
averaging the R squared
: (b) Calculate average Y=(Y1+Y2, + ... Ym)/m, and then calculate the R 
squared between (X, Y). 
: 
: We found it seemed that R squared calculated in (b) is 'always' higher than 
that in (a).
: 
: Does this result depends on the test dataset or this happened by chance?Can 
you advise me any reference for
: this issue? 
: 
: Many thanks in advance!
: 
: Kan
: 
: 
: 		
: ---------------------------------
: 
: 	[[alternative HTML version deleted]]
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://www.stat.math.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
: 
:



From ckjmaner at carolina.rr.com  Sun Jun  6 20:13:59 2004
From: ckjmaner at carolina.rr.com (Charles and Kimberly Maner)
Date: Sun, 6 Jun 2004 14:13:59 -0400
Subject: [R] Printing Lattice Graphs from Windows
In-Reply-To: <000401c44a29$86acc280$442501a3@plants.ox.ac.uk>
Message-ID: <200406061814.i56IE7Lr012812@ms-smtp-03-eri0.southeast.rr.com>


Hello.  I have researched this topic and have found no answers.  I am
running R 1.9.0 and am trying to print a lattice graph, (e.g., xyplot(1~1)),
using mouse right click -> print.  It produces a blank page.  Also, I right
clich, copy the metafile and paste into a MS Office document, (e.g., .ppt,
.doc) and, same thing, a blank.  I have updated to the latest lattice
package and still no printing.  Any help/advice?


Thanks,
Charles



From andy_liaw at merck.com  Sun Jun  6 20:25:12 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sun, 6 Jun 2004 14:25:12 -0400
Subject: [R] Request help writing a function
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7E32@usrymx25.merck.com>

Perhaps simpler:

prop2 <- rowMeans(ph5028[, 83:107] == 2, na.rm=TRUE)

HTH,
Andy

> From: Chuck Cleland
> 
> Greg:
>    I think you want to apply the function Gabor suggested to the 
> rows of your subsetted dataframe:
> 
> mydata <- as.data.frame(matrix(c
>               (1, NA, 3, 2,
>                2,  2, 2, 2,
>                1, NA, 2, 2), byrow = TRUE, ncol = 4))
> 
> apply(mydata, 1, function(x){mean(x == 2, na.rm = TRUE)})
> 
>          1         2         3
> 0.3333333 1.0000000 0.6666667
> 
>    If you want percentages rather than proportions, just convert 
> within the function.
> 
> apply(mydata, 1, function(x){mean(x == 2, na.rm = TRUE)*100})
> 
> hope this helps,
> 
> Chuck Cleland
> 
> Greg Blevins wrote:
> > Here is some further clarification using a toy example.
> > dataframe layout
> > 
> >                 v1               v2                   v3    
>             v4
> > 
> > Person1    1                NA                 3                  2
> > 
> > Person2     2                 2                    2        
>           2
> > 
> > Person3     1                NA                  2          
>         2
> > 
> > I am looking for the proportion of time each person used a 
> 2 code across the
> > variables v1 to v4. For example, using the above data, the 
> answer would look
> > as follows:
> > 
> > Person1     33%
> > 
> > Person2     100%
> > 
> > Person3     66%
> 
> -- 
> Chuck Cleland, Ph.D.
> NDRI, Inc.
> 71 West 23rd Street, 8th floor
> New York, NY 10010
> tel: (212) 845-4495 (Tu, Th)
> tel: (732) 452-1424 (M, W, F)
> fax: (917) 438-0894
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From umalvarez at fata.unam.mx  Sun Jun  6 21:18:02 2004
From: umalvarez at fata.unam.mx (Ulises Mora Alvarez)
Date: Sun, 6 Jun 2004 14:18:02 -0500 (CDT)
Subject: [R] Printing Lattice Graphs from Windows
In-Reply-To: <200406061814.i56IE7Lr012812@ms-smtp-03-eri0.southeast.rr.com>
Message-ID: <Pine.LNX.4.44.0406061414350.28765-100000@athena.fata.unam.mx>

Hi!

Take a look at the help pages of 'dev.print', 'dev.copy' and 
'dev.copy2eps'. These functions are very simple to use and, depending on 
your system, give you a lot of choises.

Regards.


On Sun, 6 Jun 2004, Charles and Kimberly Maner wrote:

> 
> Hello.  I have researched this topic and have found no answers.  I am
> running R 1.9.0 and am trying to print a lattice graph, (e.g., xyplot(1~1)),
> using mouse right click -> print.  It produces a blank page.  Also, I right
> clich, copy the metafile and paste into a MS Office document, (e.g., .ppt,
> .doc) and, same thing, a blank.  I have updated to the latest lattice
> package and still no printing.  Any help/advice?
> 
> 
> Thanks,
> Charles
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Ulises M. Alvarez
LAB. DE ONDAS DE CHOQUE
FISICA APLICADA Y TECNOLOGIA AVANZADA
UNAM
umalvarez at fata.unam.mx



From spencer.graves at pdf.com  Sun Jun  6 21:26:45 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 06 Jun 2004 12:26:45 -0700
Subject: [R] Printing Lattice Graphs from Windows
In-Reply-To: <Pine.LNX.4.44.0406061414350.28765-100000@athena.fata.unam.mx>
References: <Pine.LNX.4.44.0406061414350.28765-100000@athena.fata.unam.mx>
Message-ID: <40C36FF5.80106@pdf.com>

      Using R 1.9.1 alpha via XEmacs with ESS under Windows 2000, I did 
the following: 

      library(lattice)
      xyplot(1~1)

      I got the plot in a separate window, from which I did File -> Copy 
to the clipboard -> as a metafile.  I then changed to MS Word, and it 
pasted fine. 

      When I started R 1.9.1 alpha directly (without XEmacs) and tried 
the same thing, I got a standard lattice plot.  However, when I tried 
File -> Copy to the clipboard -> as a metafile, I got a blank plot when 
copying into MS Word.  When I copied as a bitmap, it came through fine.  
However, I suspect that is unacceptable. 

      Conclusion:   ESS with Emacs or XEmacs could provide you an 
alternative to 'dev.copy', etc., suggested by Ulises Mora Alverez. 

      hope this helps.  spencer graves    

Ulises Mora Alvarez wrote:

>Hi!
>
>Take a look at the help pages of 'dev.print', 'dev.copy' and 
>'dev.copy2eps'. These functions are very simple to use and, depending on 
>your system, give you a lot of choises.
>
>Regards.
>
>
>On Sun, 6 Jun 2004, Charles and Kimberly Maner wrote:
>
>  
>
>>Hello.  I have researched this topic and have found no answers.  I am
>>running R 1.9.0 and am trying to print a lattice graph, (e.g., xyplot(1~1)),
>>using mouse right click -> print.  It produces a blank page.  Also, I right
>>clich, copy the metafile and paste into a MS Office document, (e.g., .ppt,
>>.doc) and, same thing, a blank.  I have updated to the latest lattice
>>package and still no printing.  Any help/advice?
>>
>>
>>Thanks,
>>Charles
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>    
>>
>
>  
>



From edd at debian.org  Sun Jun  6 22:00:52 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 6 Jun 2004 15:00:52 -0500
Subject: [R] Printing Lattice Graphs from Windows
In-Reply-To: <40C36FF5.80106@pdf.com>
References: <Pine.LNX.4.44.0406061414350.28765-100000@athena.fata.unam.mx>
	<40C36FF5.80106@pdf.com>
Message-ID: <20040606200052.GA26334@sonny.eddelbuettel.com>

On Sun, Jun 06, 2004 at 12:26:45PM -0700, Spencer Graves wrote:
>      Using R 1.9.1 alpha via XEmacs with ESS under Windows 2000, I did 
> the following: 
> 
>      library(lattice)
>      xyplot(1~1)
> 
>      I got the plot in a separate window, from which I did File -> Copy 
> to the clipboard -> as a metafile.  I then changed to MS Word, and it 
> pasted fine. 

Ok.
 
>      When I started R 1.9.1 alpha directly (without XEmacs) and tried 
> the same thing, I got a standard lattice plot.  However, when I tried 
> File -> Copy to the clipboard -> as a metafile, I got a blank plot when 
> copying into MS Word.  When I copied as a bitmap, it came through fine.  
> However, I suspect that is unacceptable. 

Not ok. 

But isn't it a bug in the windows version of R if the Rgui.exe cannot copy
to the clipboard where Rterm.exe can?

I may be missing some subtle detail, though... 

Dirk

-- 
FEATURE:  VW Beetle license plate in California



From dmurdoch at pair.com  Sun Jun  6 22:24:36 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Sun, 06 Jun 2004 16:24:36 -0400
Subject: [R] Printing Lattice Graphs from Windows
In-Reply-To: <20040606200052.GA26334@sonny.eddelbuettel.com>
References: <Pine.LNX.4.44.0406061414350.28765-100000@athena.fata.unam.mx>
	<40C36FF5.80106@pdf.com>
	<20040606200052.GA26334@sonny.eddelbuettel.com>
Message-ID: <rgu6c0h7lde1hrbf3ualghr8itipoqn7ba@4ax.com>

On Sun, 6 Jun 2004 15:00:52 -0500, Dirk Eddelbuettel <edd at debian.org>
wrote:

>
>But isn't it a bug in the windows version of R if the Rgui.exe cannot copy
>to the clipboard where Rterm.exe can?

Yes, definitely.  Can someone distill this down to code that uses only
the base packages (e.g. just grid)?  Lattice is a recommended package,
so I'd refer you to its maintainer, but it's probably something in a
base package that's going wrong.

Once you've done that, please submit it as a bug report with
reproducible code.  I've never looked at the Windows metafile device
driver, so it's unlikely I'll fix this by 1.9.1, but it is something
that should be fixed.

Duncan Murdoch



From spencer.graves at pdf.com  Sun Jun  6 22:32:00 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 06 Jun 2004 13:32:00 -0700
Subject: [R] Printing Lattice Graphs from Windows
In-Reply-To: <20040606200052.GA26334@sonny.eddelbuettel.com>
References: <Pine.LNX.4.44.0406061414350.28765-100000@athena.fata.unam.mx>
	<40C36FF5.80106@pdf.com>
	<20040606200052.GA26334@sonny.eddelbuettel.com>
Message-ID: <40C37F40.9070005@pdf.com>

      I agree:  It sounds like a  bug, as you said, Irk, in that 
Rgui.exe cannot copy a metafile to the clipboard, at least under MS 
Windows 2000, 5.00.2195, Service Pack 3, even though Rterm.exe can.  
Therefore, I'm including "r-bugs at biostat.ku.dk" in the list of addresses 
to this email. 

      Spencer Graves

Irk Eddelbuettel wrote:

>On Sun, Jun 06, 2004 at 12:26:45PM -0700, Spencer Graves wrote:
>  
>
>>     Using R 1.9.1 alpha via XEmacs with ESS under Windows 2000, I did 
>>the following: 
>>
>>     library(lattice)
>>     xyplot(1~1)
>>
>>     I got the plot in a separate window, from which I did File -> Copy 
>>to the clipboard -> as a metafile.  I then changed to MS Word, and it 
>>pasted fine. 
>>    
>>
>
>Ok.
> 
>  
>
>>     When I started R 1.9.1 alpha directly (without XEmacs) and tried 
>>the same thing, I got a standard lattice plot.  However, when I tried 
>>File -> Copy to the clipboard -> as a metafile, I got a blank plot when 
>>copying into MS Word.  When I copied as a bitmap, it came through fine.  
>>However, I suspect that is unacceptable. 
>>    
>>
>
>Not ok. 
>
>But isn't it a bug in the windows version of R if the Rgui.exe cannot copy
>to the clipboard where Rterm.exe can?
>
>I may be missing some subtle detail, though... 
>
>Irk
>
>  
>



From ripley at stats.ox.ac.uk  Sun Jun  6 22:36:30 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 6 Jun 2004 21:36:30 +0100 (BST)
Subject: [R] Printing Lattice Graphs from Windows
In-Reply-To: <rgu6c0h7lde1hrbf3ualghr8itipoqn7ba@4ax.com>
Message-ID: <Pine.LNX.4.44.0406062127450.31642-100000@gannet.stats>

On Sun, 6 Jun 2004, Duncan Murdoch wrote:

> On Sun, 6 Jun 2004 15:00:52 -0500, Dirk Eddelbuettel <edd at debian.org>
> wrote:
> 
> >
> >But isn't it a bug in the windows version of R if the Rgui.exe cannot copy
> >to the clipboard where Rterm.exe can?
> 
> Yes, definitely.  Can someone distill this down to code that uses only
> the base packages (e.g. just grid)?  Lattice is a recommended package,
> so I'd refer you to its maintainer, but it's probably something in a
> base package that's going wrong.
> 
> Once you've done that, please submit it as a bug report with
> reproducible code.  I've never looked at the Windows metafile device
> driver, so it's unlikely I'll fix this by 1.9.1, but it is something
> that should be fixed.

It is not just lattice (try the second example in ?Grid) and it is not 
just metafile (try saving to PDF) and it is not wholly reproducible.  But 
most of the time (but not quite all the time)

library(grid)
grid.show.viewport(viewport(x=0.6, y=0.6,
                            w=unit(1, "inches"), h=unit(1,"inches")))
# copy to metafile or PDF from menu

fails.   This is the same from RGui or Rterm, and I've had one copy 
succeed and then a second copy fail.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.murrell at auckland.ac.nz  Sun Jun  6 22:50:52 2004
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Mon, 07 Jun 2004 08:50:52 +1200
Subject: [R] Printing Lattice Graphs from Windows
References: <Pine.LNX.4.44.0406062127450.31642-100000@gannet.stats>
Message-ID: <40C383AC.7010909@stat.auckland.ac.nz>

Hi

Sorry I haven't had a chance to look at this in the last week or so.  I 
think the problem may be in the core graphics engine (when there is grid 
output, but no "traditional" output).  Don't understand yet why this 
only seems to have kicked in with 1.9 (if anyone has ever seen it with 
1.8 or earlier please let me know).

Anyway I am now hunting this ...

Paul


Prof Brian Ripley wrote:
> On Sun, 6 Jun 2004, Duncan Murdoch wrote:
> 
> 
>>On Sun, 6 Jun 2004 15:00:52 -0500, Dirk Eddelbuettel <edd at debian.org>
>>wrote:
>>
>>
>>>But isn't it a bug in the windows version of R if the Rgui.exe cannot copy
>>>to the clipboard where Rterm.exe can?
>>
>>Yes, definitely.  Can someone distill this down to code that uses only
>>the base packages (e.g. just grid)?  Lattice is a recommended package,
>>so I'd refer you to its maintainer, but it's probably something in a
>>base package that's going wrong.
>>
>>Once you've done that, please submit it as a bug report with
>>reproducible code.  I've never looked at the Windows metafile device
>>driver, so it's unlikely I'll fix this by 1.9.1, but it is something
>>that should be fixed.
> 
> 
> It is not just lattice (try the second example in ?Grid) and it is not 
> just metafile (try saving to PDF) and it is not wholly reproducible.  But 
> most of the time (but not quite all the time)
> 
> library(grid)
> grid.show.viewport(viewport(x=0.6, y=0.6,
>                             w=unit(1, "inches"), h=unit(1,"inches")))
> # copy to metafile or PDF from menu
> 
> fails.   This is the same from RGui or Rterm, and I've had one copy 
> succeed and then a second copy fail.
> 
> 


-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From spencer.graves at pdf.com  Sun Jun  6 23:02:12 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 06 Jun 2004 14:02:12 -0700
Subject: [R] Printing Lattice Graphs from Windows
In-Reply-To: <40C37F40.9070005@pdf.com>
References: <Pine.LNX.4.44.0406061414350.28765-100000@athena.fata.unam.mx>	<40C36FF5.80106@pdf.com>	<20040606200052.GA26334@sonny.eddelbuettel.com>
	<40C37F40.9070005@pdf.com>
Message-ID: <40C38654.4010402@pdf.com>

Hi, Duncan: 

      I just did "plot(1:2)" in Rgui.exe, and that copied fine as a 
metafile into MS Word via the clipboard. 

      Then I exited and restarted Rterm.exe under ESS and tried it 
again.  This time, I got a blank image copied into MS Word.  However, 
after I modified the Lattice defaults via, 
"trellis.par.set('background', list('white')); 
trellis.par.set('plot.symbol',  list(cex=0.8, col=1, font=1, pch=1))", I 
was able to copy the results of "xyplot(1~1)" as a metafile into Word.  
Unfortunately, when I tried modifying the Lattice defaults the same way 
under Rgui.exe. I again got a blank image copied into Word. 

      hope this helps.  spencer graves

#######################
Duncan Murdoch wrote:

On Sun, 6 Jun 2004 15:00:52 -0500, Dirk Eddelbuettel <edd at debian.org>
wrote:


>But isn't it a bug in the windows version of R if the Rgui.exe cannot copy
>to the clipboard where Rterm.exe can?
>  
>

Yes, definitely.  Can someone distill this down to code that uses only
the base packages (e.g. just grid)?  Lattice is a recommended package,
so I'd refer you to its maintainer, but it's probably something in a
base package that's going wrong.

Once you've done that, please submit it as a bug report with
reproducible code.  I've never looked at the Windows metafile device
driver, so it's unlikely I'll fix this by 1.9.1, but it is something
that should be fixed.

Duncan Murdoch

Spencer Graves wrote:

>      I agree:  It sounds like a  bug, as you said, Irk, in that 
> Rgui.exe cannot copy a metafile to the clipboard, at least under MS 
> Windows 2000, 5.00.2195, Service Pack 3, even though Rterm.exe can.  
> Therefore, I'm including "r-bugs at biostat.ku.dk" in the list of 
> addresses to this email.
>      Spencer Graves
>
> Irk Eddelbuettel wrote:
>
>> On Sun, Jun 06, 2004 at 12:26:45PM -0700, Spencer Graves wrote:
>>  
>>
>>>     Using R 1.9.1 alpha via XEmacs with ESS under Windows 2000, I 
>>> did the following:
>>>     library(lattice)
>>>     xyplot(1~1)
>>>
>>>     I got the plot in a separate window, from which I did File -> 
>>> Copy to the clipboard -> as a metafile.  I then changed to MS Word, 
>>> and it pasted fine.   
>>
>>
>> Ok.
>>
>>  
>>
>>>     When I started R 1.9.1 alpha directly (without XEmacs) and tried 
>>> the same thing, I got a standard lattice plot.  However, when I 
>>> tried File -> Copy to the clipboard -> as a metafile, I got a blank 
>>> plot when copying into MS Word.  When I copied as a bitmap, it came 
>>> through fine.  However, I suspect that is unacceptable.   
>>
>>
>> Not ok.
>> But isn't it a bug in the windows version of R if the Rgui.exe cannot 
>> copy
>> to the clipboard where Rterm.exe can?
>>
>> I may be missing some subtle detail, though...
>> Irk
>>
>>  
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From edd at debian.org  Mon Jun  7 00:06:42 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 6 Jun 2004 17:06:42 -0500
Subject: [R] Printing Lattice Graphs from Windows
In-Reply-To: <40C37F40.9070005@pdf.com>
References: <Pine.LNX.4.44.0406061414350.28765-100000@athena.fata.unam.mx>
	<40C36FF5.80106@pdf.com>
	<20040606200052.GA26334@sonny.eddelbuettel.com>
	<40C37F40.9070005@pdf.com>
Message-ID: <20040606220642.GA27858@sonny.eddelbuettel.com>

On Sun, Jun 06, 2004 at 01:32:00PM -0700, Spencer Graves wrote:
>      I agree:  It sounds like a  bug, as you said, Irk, in that 

That brilliant :)  Many people, in Germany as well as abroad, managed to
chop Eddelbuettel quite well. Turning Dirk into Irk is rather unsurpassed.

Dirk, with a big grin

-- 
FEATURE:  VW Beetle license plate in California



From yuelin at mail.med.upenn.edu  Mon Jun  7 00:27:53 2004
From: yuelin at mail.med.upenn.edu (Yuelin Li)
Date: Sun, 6 Jun 2004 18:27:53 -0400
Subject: [R] strata() in clogit()
Message-ID: <20040606222753.GI3413@pandora.outcomes.chop.edu>

How can I get the log odds associated with the levels in strata()
within a clogit() model?  I'm running R-1.9.0 on a Linux platform.

I am using clogit() to run a Rasch model in Item Response Theory in
psychometrics.  Symbolically, the model is:

  logit(p_{j,k}) = \log({\Pr(p_{j,k}) \over \Pr(1-p_{j,k})}) =
   \theta_j - \alpha_k,

That is, the log odds of answering an test item correctly is a function
of the item difficulty (\theta_j) and the ability of individual
students (\alpha_k).  The following R syntax gives me the \theta
coefficients associated with items i5 to i17, but summary() does not
automatically summarize the log odds associated with the strata "id".
I suppose the log odds associated with strata(id) gives me \alpha_k.
But I can't seem to get them with summary().  Help is greatly
appreciated.

Yuelin Li.

---- my R syntax is -----------

      > exam2.clog<- clogit(resp ~ i5+i6+i7+i8+i9+i10+i11+i12+i13+i14+
				   i15+i16+i17+ strata(id), data=exam1.1)
      > summary(exam2.clog)
      Call:
      clogit(resp ~ i5 + i6 + i7 + i8 + i9 + i10 + i11 + i12 + i13 +
	  i14 + i15 + i16 + i17 + strata(id), data = exam1.1)

	n= 476

	   coef exp(coef) se(coef)     z       p
      i5  0.514      1.67    1.025 0.501 6.2e-01
      i6  0.923      2.52    0.991 0.931 3.5e-01
      i7  0.514      1.67    1.025 0.501 6.2e-01
      i8  1.875      6.52    0.955 1.964 5.0e-02

      [snip]

          exp(coef) exp(-coef) lower .95 upper .95
      i5       1.67   0.597954     0.224      12.5
      i6       2.52   0.397477     0.361      17.6

      Rsquare= 0.525   (max possible= 0.659 )
      Likelihood ratio test= 355  on 13 df,   p=0
      Wald test            = 108  on 13 df,   p=0
      Score (logrank) test = 277  on 13 df,   p=0



From p.dalgaard at biostat.ku.dk  Mon Jun  7 00:23:28 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 07 Jun 2004 00:23:28 +0200
Subject: [R] Printing Lattice Graphs from Windows
In-Reply-To: <20040606220642.GA27858@sonny.eddelbuettel.com>
References: <Pine.LNX.4.44.0406061414350.28765-100000@athena.fata.unam.mx>
	<40C36FF5.80106@pdf.com>
	<20040606200052.GA26334@sonny.eddelbuettel.com>
	<40C37F40.9070005@pdf.com>
	<20040606220642.GA27858@sonny.eddelbuettel.com>
Message-ID: <x2ise4milb.fsf@biostat.ku.dk>

Dirk Eddelbuettel <edd at debian.org> writes:

> On Sun, Jun 06, 2004 at 01:32:00PM -0700, Spencer Graves wrote:
> >      I agree:  It sounds like a  bug, as you said, Irk, in that 
> 
> That brilliant :)  Many people, in Germany as well as abroad, managed to
> chop Eddelbuettel quite well. Turning Dirk into Irk is rather unsurpassed.
> 
> Dirk, with a big grin

Not to say "smirk", Dirk. (Did Spencer's quirk irk Dirk?) ;-)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From HaroldD at ccsso.org  Mon Jun  7 00:59:08 2004
From: HaroldD at ccsso.org (Harold Doran)
Date: Sun, 6 Jun 2004 18:59:08 -0400
Subject: [R] Repeated measures
Message-ID: <CFF85773D9245040A333571B7E6D651702C5FD6B@ccssosrv1.ccsso.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040606/d6d7630e/attachment.pl

From spencer.graves at pdf.com  Mon Jun  7 01:06:35 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 06 Jun 2004 16:06:35 -0700
Subject: [R] Printing Lattice Graphs from Windows
In-Reply-To: <x2ise4milb.fsf@biostat.ku.dk>
References: <Pine.LNX.4.44.0406061414350.28765-100000@athena.fata.unam.mx>	<40C36FF5.80106@pdf.com>	<20040606200052.GA26334@sonny.eddelbuettel.com>	<40C37F40.9070005@pdf.com>	<20040606220642.GA27858@sonny.eddelbuettel.com>
	<x2ise4milb.fsf@biostat.ku.dk>
Message-ID: <40C3A37B.90806@pdf.com>

Entschuldigung!  My English spellchecker failed me again.  spencer graves

Peter Dalgaard wrote:

>Dirk Eddelbuettel <edd at debian.org> writes:
>
>  
>
>>On Sun, Jun 06, 2004 at 01:32:00PM -0700, Spencer Graves wrote:
>>    
>>
>>>     I agree:  It sounds like a  bug, as you said, Irk, in that 
>>>      
>>>
>>That brilliant :)  Many people, in Germany as well as abroad, managed to
>>chop Eddelbuettel quite well. Turning Dirk into Irk is rather unsurpassed.
>>
>>Dirk, with a big grin
>>    
>>
>
>Not to say "smirk", Dirk. (Did Spencer's quirk irk Dirk?) ;-)
>
>  
>



From ckjmaner at carolina.rr.com  Mon Jun  7 01:10:46 2004
From: ckjmaner at carolina.rr.com (Charles and Kimberly Maner)
Date: Sun, 6 Jun 2004 19:10:46 -0400
Subject: [R] Printing Lattice Graphs from Windows
In-Reply-To: <40C38654.4010402@pdf.com>
Message-ID: <200406062311.i56NAufP019368@ms-smtp-01-eri0.southeast.rr.com>


Hi folks.  It looks like it's stirred some discussion ultimately
resulting/concluding that this phenomena is a possible bug either in the
lattice package or in R 1.9.0/1.9.1 itself.  So, I'll stay tuned, so to
speak, for either an update in the lattice package or R as it seems that
that's where the bug may lie.  FYI, the resulting plot, (e.g., plot(1~1)),
worked as it should either printing or as a copy and paste as a metafile
which causes me to believe it's in the lattice package.

I appreciate you guys looking into this.  (As you may/may not, for some us
corporate folk, data mining is (1) pulling data, (2) producing a graphical
representation of a story/hypothesis from the data and (3) pasting it into
MS Word and/or MS PowerPoint for presentation quickly.  So, to much of you
all's potential satisfaction, a number of us use R instead of S-Plus.)


Thanks,
Charles

-----Original Message-----
From: Spencer Graves [mailto:spencer.graves at pdf.com] 
Sent: Sunday, June 06, 2004 5:02 PM
To: Spencer Graves
Cc: Dirk Eddelbuettel; r-bugs at biostat.ku.dk; Ulises Mora Alvarez;
r-help at stat.math.ethz.ch; Charles and Kimberly Maner
Subject: Re: [R] Printing Lattice Graphs from Windows

Hi, Duncan: 

      I just did "plot(1:2)" in Rgui.exe, and that copied fine as a metafile
into MS Word via the clipboard. 

      Then I exited and restarted Rterm.exe under ESS and tried it again.
This time, I got a blank image copied into MS Word.  However, after I
modified the Lattice defaults via, "trellis.par.set('background',
list('white')); trellis.par.set('plot.symbol',  list(cex=0.8, col=1, font=1,
pch=1))", I was able to copy the results of "xyplot(1~1)" as a metafile into
Word.  
Unfortunately, when I tried modifying the Lattice defaults the same way
under Rgui.exe. I again got a blank image copied into Word. 

      hope this helps.  spencer graves

#######################
Duncan Murdoch wrote:

On Sun, 6 Jun 2004 15:00:52 -0500, Dirk Eddelbuettel <edd at debian.org>
wrote:


>But isn't it a bug in the windows version of R if the Rgui.exe cannot 
>copy to the clipboard where Rterm.exe can?
>  
>

Yes, definitely.  Can someone distill this down to code that uses only the
base packages (e.g. just grid)?  Lattice is a recommended package, so I'd
refer you to its maintainer, but it's probably something in a base package
that's going wrong.

Once you've done that, please submit it as a bug report with reproducible
code.  I've never looked at the Windows metafile device driver, so it's
unlikely I'll fix this by 1.9.1, but it is something that should be fixed.

Duncan Murdoch

Spencer Graves wrote:

>      I agree:  It sounds like a  bug, as you said, Irk, in that 
> Rgui.exe cannot copy a metafile to the clipboard, at least under MS 
> Windows 2000, 5.00.2195, Service Pack 3, even though Rterm.exe can.
> Therefore, I'm including "r-bugs at biostat.ku.dk" in the list of 
> addresses to this email.
>      Spencer Graves
>
> Irk Eddelbuettel wrote:
>
>> On Sun, Jun 06, 2004 at 12:26:45PM -0700, Spencer Graves wrote:
>>  
>>
>>>     Using R 1.9.1 alpha via XEmacs with ESS under Windows 2000, I 
>>> did the following:
>>>     library(lattice)
>>>     xyplot(1~1)
>>>
>>>     I got the plot in a separate window, from which I did File -> 
>>> Copy to the clipboard -> as a metafile.  I then changed to MS Word,
>>> and it pasted fine.   
>>
>>
>> Ok.
>>
>>  
>>
>>>     When I started R 1.9.1 alpha directly (without XEmacs) and tried 
>>> the same thing, I got a standard lattice plot.  However, when I 
>>> tried File -> Copy to the clipboard -> as a metafile, I got a blank 
>>> plot when copying into MS Word.  When I copied as a bitmap, it came
>>> through fine.  However, I suspect that is unacceptable.   
>>
>>
>> Not ok.
>> But isn't it a bug in the windows version of R if the Rgui.exe cannot 
>> copy to the clipboard where Rterm.exe can?
>>
>> I may be missing some subtle detail, though...
>> Irk
>>
>>  
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From deepayan at stat.wisc.edu  Mon Jun  7 01:56:41 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Sun, 6 Jun 2004 18:56:41 -0500
Subject: [R] Printing Lattice Graphs from Windows
In-Reply-To: <200406062311.i56NAufP019368@ms-smtp-01-eri0.southeast.rr.com>
References: <200406062311.i56NAufP019368@ms-smtp-01-eri0.southeast.rr.com>
Message-ID: <200406061856.41705.deepayan@stat.wisc.edu>

On Sunday 06 June 2004 18:10, Charles and Kimberly Maner wrote:
> Hi folks.  It looks like it's stirred some discussion ultimately
> resulting/concluding that this phenomena is a possible bug either in
> the lattice package or in R 1.9.0/1.9.1 itself.  So, I'll stay tuned,
> so to speak, for either an update in the lattice package or R as it
> seems that that's where the bug may lie.  FYI, the resulting plot,
> (e.g., plot(1~1)), worked as it should either printing or as a copy
> and paste as a metafile which causes me to believe it's in the
> lattice package.
>
> I appreciate you guys looking into this.  (As you may/may not, for
> some us corporate folk, data mining is (1) pulling data, (2)
> producing a graphical representation of a story/hypothesis from the
> data and (3) pasting it into MS Word and/or MS PowerPoint for
> presentation quickly.  So, to much of you all's potential
> satisfaction, a number of us use R instead of S-Plus.)

I can't check myself, but I believe a possible workaround is to use the 
respective devices directly (e.g. pdf() or win.metafile()) instead of 
copying from the GUI menu (at least I haven't heard anyone say that 
this does not work). 

Deepayan



From spencer.graves at pdf.com  Mon Jun  7 02:43:42 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 06 Jun 2004 17:43:42 -0700
Subject: [R] Printing Lattice Graphs from Windows
In-Reply-To: <200406061856.41705.deepayan@stat.wisc.edu>
References: <200406062311.i56NAufP019368@ms-smtp-01-eri0.southeast.rr.com>
	<200406061856.41705.deepayan@stat.wisc.edu>
Message-ID: <40C3BA3E.1000504@pdf.com>

Hi, Deepayan: 

      Following your suggestion, I tried the following in Rgui.exe: 

 > xyplot(1~1)
 > win.metafile()
 > graphics.off()

      I then switched to MS Word, pasted, and got the same blank plot as 
before.  (I actually tried other things as well, but came to this after 
reading the documentation.) 

      ???
      spencer graves

Deepayan Sarkar wrote:

>On Sunday 06 June 2004 18:10, Charles and Kimberly Maner wrote:
>  
>
>>Hi folks.  It looks like it's stirred some discussion ultimately
>>resulting/concluding that this phenomena is a possible bug either in
>>the lattice package or in R 1.9.0/1.9.1 itself.  So, I'll stay tuned,
>>so to speak, for either an update in the lattice package or R as it
>>seems that that's where the bug may lie.  FYI, the resulting plot,
>>(e.g., plot(1~1)), worked as it should either printing or as a copy
>>and paste as a metafile which causes me to believe it's in the
>>lattice package.
>>
>>I appreciate you guys looking into this.  (As you may/may not, for
>>some us corporate folk, data mining is (1) pulling data, (2)
>>producing a graphical representation of a story/hypothesis from the
>>data and (3) pasting it into MS Word and/or MS PowerPoint for
>>presentation quickly.  So, to much of you all's potential
>>satisfaction, a number of us use R instead of S-Plus.)
>>    
>>
>
>I can't check myself, but I believe a possible workaround is to use the 
>respective devices directly (e.g. pdf() or win.metafile()) instead of 
>copying from the GUI menu (at least I haven't heard anyone say that 
>this does not work). 
>
>Deepayan
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From ckjmaner at carolina.rr.com  Mon Jun  7 02:54:01 2004
From: ckjmaner at carolina.rr.com (Charles and Kimberly Maner)
Date: Sun, 6 Jun 2004 20:54:01 -0400
Subject: [R] Printing Lattice Graphs from Windows
In-Reply-To: <40C3BA3E.1000504@pdf.com>
Message-ID: <200406070054.i570sBNs022738@ms-smtp-02-eri0.southeast.rr.com>


Hi.  Another curious observation.  When executing the following:

> trellis.device(bg="transparent")
> xyplot(1~1)

I can then copy and paste in MS Word fine.  But, printing directly
(right-click, print) still will not work--prints a blank sheet.


--Charles

-----Original Message-----
From: Spencer Graves [mailto:spencer.graves at pdf.com] 
Sent: Sunday, June 06, 2004 8:44 PM
To: Deepayan Sarkar
Cc: r-help at stat.math.ethz.ch; Charles and Kimberly Maner
Subject: Re: [R] Printing Lattice Graphs from Windows

Hi, Deepayan: 

      Following your suggestion, I tried the following in Rgui.exe: 

 > xyplot(1~1)
 > win.metafile()
 > graphics.off()

      I then switched to MS Word, pasted, and got the same blank plot as
before.  (I actually tried other things as well, but came to this after
reading the documentation.) 

      ???
      spencer graves

Deepayan Sarkar wrote:

>On Sunday 06 June 2004 18:10, Charles and Kimberly Maner wrote:
>  
>
>>Hi folks.  It looks like it's stirred some discussion ultimately 
>>resulting/concluding that this phenomena is a possible bug either in 
>>the lattice package or in R 1.9.0/1.9.1 itself.  So, I'll stay tuned, 
>>so to speak, for either an update in the lattice package or R as it 
>>seems that that's where the bug may lie.  FYI, the resulting plot, 
>>(e.g., plot(1~1)), worked as it should either printing or as a copy 
>>and paste as a metafile which causes me to believe it's in the lattice 
>>package.
>>
>>I appreciate you guys looking into this.  (As you may/may not, for 
>>some us corporate folk, data mining is (1) pulling data, (2) producing 
>>a graphical representation of a story/hypothesis from the data and (3) 
>>pasting it into MS Word and/or MS PowerPoint for presentation quickly.  
>>So, to much of you all's potential satisfaction, a number of us use R 
>>instead of S-Plus.)
>>    
>>
>
>I can't check myself, but I believe a possible workaround is to use the 
>respective devices directly (e.g. pdf() or win.metafile()) instead of 
>copying from the GUI menu (at least I haven't heard anyone say that 
>this does not work).
>
>Deepayan
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! 
>http://www.R-project.org/posting-guide.html
>  
>



From spencer.graves at pdf.com  Mon Jun  7 03:40:29 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 06 Jun 2004 18:40:29 -0700
Subject: [R] contour lines on levelplot? 
Message-ID: <40C3C78D.1010001@pdf.com>

      With "image" and "contour", one can get both colors and lines to 
enhance the image of a contour plot.  What's the best way to do this 
with Lattice graphics?  The following is one ugly hack, producing the 
desired result after much trial and error (R 1.9.1 alpha under Windows 
2000): 

# setup
DF <- expand.grid(x=1:3, y=1:3)
DF$z <- (DF$x+DF$y)

# Traditional "base" graphics: 
image(x=1:3, y=1:3, z=array(DF$z, dim=c(3,3)))
contour(x=1:3, y=1:3, z=array(DF$z, dim=c(3,3)), add=T)

# Lattice hack: 
lvlplt <- levelplot(z~x*y, DF)
cont <- contourplot(z~x*y, DF)
print(lvlplt, more=T)
print(cont, position=c(0, 0, 0.9055, 1))

##################
      By comparison, the same ugly hack in S-Plus required 0.952 instead 
of 0.955.  Is this the best we can currently get with a modest effort? 

      Thanks,
      spencer graves



From dmurdoch at pair.com  Mon Jun  7 03:47:22 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Sun, 06 Jun 2004 21:47:22 -0400
Subject: [R] Printing Lattice Graphs from Windows
In-Reply-To: <40C3BA3E.1000504@pdf.com>
References: <200406062311.i56NAufP019368@ms-smtp-01-eri0.southeast.rr.com>
	<200406061856.41705.deepayan@stat.wisc.edu>
	<40C3BA3E.1000504@pdf.com>
Message-ID: <jsh7c059ooia32cgrsr54p4fdesq4cd3l3@4ax.com>

On Sun, 06 Jun 2004 17:43:42 -0700, Spencer Graves
<spencer.graves at pdf.com> wrote:

>Hi, Deepayan: 
>
>      Following your suggestion, I tried the following in Rgui.exe: 
>
> > xyplot(1~1)
> > win.metafile()
> > graphics.off()
>
>      I then switched to MS Word, pasted, and got the same blank plot as 
>before.  (I actually tried other things as well, but came to this after 
>reading the documentation.) 

You should open the device *first*.  Then you plot to it, then close
it and the graphics are saved.

That is,

> library(lattice)
> win.metafile()
> xyplot(1 ~ 1)
> dev.off()

This worked for me in a single attempt; I haven't tested it
thoroughly.

Generally speaking, you get better graphics results by plotting to the
final graphics device rather than plotting to the screen and copying
somewhere, because the graphics system makes some choices based on the
target device capabilities, and those choices might not be appropriate
on a different device.  I notice lines are often too thin when printed
from a plot on screen, for example.

Duncan Murdoch



From spencer.graves at pdf.com  Mon Jun  7 04:00:27 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 06 Jun 2004 19:00:27 -0700
Subject: [R] Printing Lattice Graphs from Windows
In-Reply-To: <jsh7c059ooia32cgrsr54p4fdesq4cd3l3@4ax.com>
References: <200406062311.i56NAufP019368@ms-smtp-01-eri0.southeast.rr.com>
	<200406061856.41705.deepayan@stat.wisc.edu>
	<40C3BA3E.1000504@pdf.com>
	<jsh7c059ooia32cgrsr54p4fdesq4cd3l3@4ax.com>
Message-ID: <40C3CC3B.5020406@pdf.com>

Hi, Duncan: 

      Thanks.  It worked for me, complete with the gray background and 
pastel blue dot.  The following also worked to eliminate the (to me 
ugly) gray background, etc.: 

win.metafile()
trellis.par.set('background', list('white'))
trellis.par.set('plot.symbol',
  list(cex=0.8, col="blue", font=1, pch=1))
xyplot(1 ~ 1)
dev.off()

      Thanks again.  spencer graves

Duncan Murdoch wrote:

>On Sun, 06 Jun 2004 17:43:42 -0700, Spencer Graves
><spencer.graves at pdf.com> wrote:
>
>  
>
>>Hi, Deepayan: 
>>
>>     Following your suggestion, I tried the following in Rgui.exe: 
>>
>>    
>>
>>>xyplot(1~1)
>>>win.metafile()
>>>graphics.off()
>>>      
>>>
>>     I then switched to MS Word, pasted, and got the same blank plot as 
>>before.  (I actually tried other things as well, but came to this after 
>>reading the documentation.) 
>>    
>>
>
>You should open the device *first*.  Then you plot to it, then close
>it and the graphics are saved.
>
>That is,
>
>  
>
>>library(lattice)
>>win.metafile()
>>xyplot(1 ~ 1)
>>dev.off()
>>    
>>
>
>This worked for me in a single attempt; I haven't tested it
>thoroughly.
>
>Generally speaking, you get better graphics results by plotting to the
>final graphics device rather than plotting to the screen and copying
>somewhere, because the graphics system makes some choices based on the
>target device capabilities, and those choices might not be appropriate
>on a different device.  I notice lines are often too thin when printed
>from a plot on screen, for example.
>
>Duncan Murdoch
>  
>



From ckjmaner at carolina.rr.com  Mon Jun  7 05:11:00 2004
From: ckjmaner at carolina.rr.com (Charles and Kimberly Maner)
Date: Sun, 6 Jun 2004 23:11:00 -0400
Subject: [R] Printing Lattice Graphs from Windows
In-Reply-To: <40C3CC3B.5020406@pdf.com>
Message-ID: <200406070311.i573BBLr025183@ms-smtp-03-eri0.southeast.rr.com>


Hi.  This did not work for me per my R output/session below:

> library(lattice)
> win.metafile()
> trellis.par.set('background', list('white'))
> trellis.par.set('plot.symbol',list(cex=0.8, col="blue", font=1, pch=1))
> xyplot(1 ~ 1)
Error in unit(rep(1 * xaxis.cex[1], length(strbar)), "strheight", strbar) : 
        x and units must have length > 0
> dev.off()
null device 
          1 
>  

When I pasted the result into MS Word, I got the same paste--blank.  And,
yes, another default bg color such as "white" or "transparent" would be
great as I am definitely not a fan of the standard "gray" either.  I
researched how to default it to something else, but it seems to have to be
done manually when a lattice/trellis graph is fired up.  Strange, though, as
the standard/base graph/plotting does default to a "white" background.


--Charles

-----Original Message-----
From: Spencer Graves [mailto:spencer.graves at pdf.com] 
Sent: Sunday, June 06, 2004 10:00 PM
To: Duncan Murdoch
Cc: Deepayan Sarkar; r-help at stat.math.ethz.ch; Charles and Kimberly Maner
Subject: Re: [R] Printing Lattice Graphs from Windows

Hi, Duncan: 

      Thanks.  It worked for me, complete with the gray background and
pastel blue dot.  The following also worked to eliminate the (to me
ugly) gray background, etc.: 

win.metafile()
trellis.par.set('background', list('white')) trellis.par.set('plot.symbol',
  list(cex=0.8, col="blue", font=1, pch=1))
xyplot(1 ~ 1)
dev.off()

      Thanks again.  spencer graves

Duncan Murdoch wrote:

>On Sun, 06 Jun 2004 17:43:42 -0700, Spencer Graves 
><spencer.graves at pdf.com> wrote:
>
>  
>
>>Hi, Deepayan: 
>>
>>     Following your suggestion, I tried the following in Rgui.exe: 
>>
>>    
>>
>>>xyplot(1~1)
>>>win.metafile()
>>>graphics.off()
>>>      
>>>
>>     I then switched to MS Word, pasted, and got the same blank plot 
>>as before.  (I actually tried other things as well, but came to this 
>>after reading the documentation.)
>>    
>>
>
>You should open the device *first*.  Then you plot to it, then close it 
>and the graphics are saved.
>
>That is,
>
>  
>
>>library(lattice)
>>win.metafile()
>>xyplot(1 ~ 1)
>>dev.off()
>>    
>>
>
>This worked for me in a single attempt; I haven't tested it thoroughly.
>
>Generally speaking, you get better graphics results by plotting to the 
>final graphics device rather than plotting to the screen and copying 
>somewhere, because the graphics system makes some choices based on the 
>target device capabilities, and those choices might not be appropriate 
>on a different device.  I notice lines are often too thin when printed 
>from a plot on screen, for example.
>
>Duncan Murdoch
>  
>



From kuroki at oak.dti.ne.jp  Mon Jun  7 05:14:26 2004
From: kuroki at oak.dti.ne.jp (Chihiro Kuroki)
Date: Mon, 07 Jun 2004 12:14:26 +0900
Subject: [R] pmvt problem in multcomp
In-Reply-To: <Pine.LNX.4.51.0405271205330.1100@artemis.imbe.med.uni-erlangen.de>
References: <87vfirzwhq.wl@oak.dti.ne.jp>
	<Pine.LNX.4.51.0405231138160.19910@artemis.imbe.med.uni-erlangen.de>
	<87oeobimv3.wl@oak.dti.ne.jp>
	<Pine.LNX.4.51.0405271205330.1100@artemis.imbe.med.uni-erlangen.de>
Message-ID: <87llj0vz3h.wl@oak.dti.ne.jp>

I found the following function in http://aoki2.si.gunma-u.ac.jp/R/dunnett.html.

require(mvtnorm)
dunnett <- function(dat, observe, group)
{
    printf <- function(fmt, ...) cat(sprintf(fmt, ...))

    get.rho <- function(ni)
    {
        k <- length(ni)
        rho <- outer(ni, ni, function(x, y) { sqrt(x/(x+ni[1])*y/(y+ni[1])) })
        diag(rho) <- 0
        sum(rho[-1,-1])/(k-2)/(k-1)
    }

    pdunnett <- function(a,df,x,r)
    {
        corr <- diag(a-1)
        corr[lower.tri(corr)] <- r
        1-pmvt(lower=-x, upper=x, delta=rep(0, a-1), df=df, corr=corr, abseps=0.0001)
    }

    x <- dat[,observe]
    ni <- table(g <- dat[,group])
    gv <- as.numeric(names(ni))
    a <- length(ni)
    n <- sum(ni)
    mi <- vi <- rep(0, a)
    for (i in 1:a) {
        mi[i] <- mean(xi <- x[g==gv[i]])
        vi[i] <- var(xi)
    }
    Vw <- sum(vi*(ni-1))/(n-a)
    rho <- get.rho(ni)
    printf("rho=%5.3f\n", rho)
    for (i in 2:a) {
        ti <- abs(mi[i]-mi[1])/sqrt(Vw*(1/ni[i]+1/ni[1]))
        pi <- pdunnett(a, n-a, ti, rho)
        printf("group:%i t=%.3f p=%.3f\n", i, ti, pi[1])
    }
}

> > > summary(simtest(y4 ~ f2, data=dat2, type="Dunnett"))
> >
> > 	 Simultaneous tests: Dunnett contrasts
> >
> > Call:
> > simtest.formula(formula = y4 ~ f2, data = dat2, type = "Dunnett")
> >
> > 	 Dunnett contrasts for factor f2
> >
> > Contrast matrix:
> >           f21 f22 f23 f24 f25
> > f22-f21 0  -1   1   0   0   0
> > f23-f21 0  -1   0   1   0   0
> > f24-f21 0  -1   0   0   1   0
> > f25-f21 0  -1   0   0   0   1
> >
> >
> > Absolute Error Tolerance:  0.001
> >
> > Coefficients:
> >         Estimate t value Std.Err. p raw p Bonf p adj
> > f25-f21    5.167  -4.644    1.022 0.000  0.000 0.000
> > f23-f21    2.875  -2.813    1.022 0.008  0.024 0.022
> > f24-f21    2.625  -2.569    1.022 0.015  0.029 0.028 --- (A)
> > f22-f21    2.125  -2.079    1.113 0.045  0.045 0.045
> > ---------------------------------

dunnett(dat2,1,2)
rho=0.426
group:5 t=4.644 p=0.000
group:3 t=2.813 p=0.028
group:4 t=2.569 p=0.051 --- (B)
group:2 t=2.079 p=0.145
(sorted in order of p values.)

p values are different although t values are equal.

> > I got the following inequality from the appended chart of a
> > book.
> >
> 
> hm, without knowing what
> 
> > 2.558 < d(5, 35, 0.4263464, 0.05) < 2.598
> 
> means it is hard to tell what the problem is. Could you please explain it
> further?

The alternative hypothesis is "two sided".

When significant level is equal to 0.05 , number of groups=5,
df of error=35 and rho=0.426, I think that absolute t-value
should be between 2.558 and 2.598.

So, (B) is easy to understand for me than (A).
In (A), I cannot believe easily that the "p adj" value is
smaller than "p Bonf".
And I want to know why the above results(A,B) are different.

Best regards,
-- 
kuroki
GnuPG fingerprint = 90FD FE79 905F 26F9 29C4  096F 8AA2 2C42 5130 1469



From kuroki at oak.dti.ne.jp  Mon Jun  7 06:38:24 2004
From: kuroki at oak.dti.ne.jp (Chihiro Kuroki)
Date: Mon, 07 Jun 2004 13:38:24 +0900
Subject: [R] pmvt problem in multcomp
In-Reply-To: <87llj0vz3h.wl@oak.dti.ne.jp>
References: <87vfirzwhq.wl@oak.dti.ne.jp>
	<Pine.LNX.4.51.0405231138160.19910@artemis.imbe.med.uni-erlangen.de>
	<87oeobimv3.wl@oak.dti.ne.jp>
	<Pine.LNX.4.51.0405271205330.1100@artemis.imbe.med.uni-erlangen.de>
	<87llj0vz3h.wl@oak.dti.ne.jp>
Message-ID: <87k6yknfsv.wl@oak.dti.ne.jp>

At Mon, 07 Jun 2004 12:14:26 +0900,
myself-oak wrote:
> In (A), I cannot believe easily that the "p adj" value is
> smaller than "p Bonf".

I'm sorry to have made a mistake in the above-mentioned text.
The text is only to be proper.

I just wanted to say ... "I think the difference between "p
adj" and "p Bonf" is too small."
-- 
kuroki
GnuPG fingerprint = 90FD FE79 905F 26F9 29C4  096F 8AA2 2C42 5130 1469



From kkthird at yahoo.com  Mon Jun  7 07:08:33 2004
From: kkthird at yahoo.com (KKThird@Yahoo.Com)
Date: Sun, 6 Jun 2004 22:08:33 -0700 (PDT)
Subject: [R] MCLUST Covariance Parameterization.
Message-ID: <20040607050833.38599.qmail@web51003.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040606/ca6ce978/attachment.pl

From deepayan at stat.wisc.edu  Mon Jun  7 08:38:14 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Mon, 7 Jun 2004 01:38:14 -0500
Subject: [R] Printing Lattice Graphs from Windows
In-Reply-To: <200406070311.i573BBLr025183@ms-smtp-03-eri0.southeast.rr.com>
References: <200406070311.i573BBLr025183@ms-smtp-03-eri0.southeast.rr.com>
Message-ID: <200406070138.14701.deepayan@stat.wisc.edu>

On Sunday 06 June 2004 22:11, Charles and Kimberly Maner wrote:
> Hi.  This did not work for me per my R output/session below:
> > library(lattice)
> > win.metafile()
> > trellis.par.set('background', list('white'))

This doesn't make sense; the components need to be named. Should be 

trellis.par.set('background', list(col = 'white'))

> > trellis.par.set('plot.symbol',list(cex=0.8, col="blue", font=1,
> > pch=1)) 
> > xyplot(1 ~ 1) 
>
> Error in unit(rep(1 * xaxis.cex[1], length(strbar)), "strheight",
> strbar) : x and units must have length > 0

Looks like a completely unrelated bug, which I'll fix. Can be worked 
around by starting the device with 

trellis.device(win.metafile)

> > dev.off()
>
> null device
>           1
>
>
> When I pasted the result into MS Word, I got the same paste--blank. 

Probably due to the error above and not the bug we have been discussing.

> And, yes, another default bg color such as "white" or "transparent"
> would be great as I am definitely not a fan of the standard "gray"
> either.  I researched how to default it to something else, but it
> seems to have to be done manually when a lattice/trellis graph is
> fired up.  Strange, though, as the standard/base graph/plotting does
> default to a "white" background.

Well, the idea is for screen devices to have grey backgrounds and 
'print' devices white. Currently, only pdf, postscript and xfig are 
considered print devices. If the general consensus is that win.metafile 
should also be put into that category, I could probably make the 
necessary changes easily enough.

That said, defaults are just that, and you can change them. In 
particular, calling 

lset(col.whitebg())

before the xyplot call should give a fairly robust scheme with a white 
(or rather 'transparent') background.

Deepayan



From deepayan at stat.wisc.edu  Mon Jun  7 08:52:19 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Mon, 7 Jun 2004 01:52:19 -0500
Subject: [R] contour lines on levelplot?
In-Reply-To: <40C3C78D.1010001@pdf.com>
References: <40C3C78D.1010001@pdf.com>
Message-ID: <200406070152.19328.deepayan@stat.wisc.edu>

On Sunday 06 June 2004 20:40, Spencer Graves wrote:
>       With "image" and "contour", one can get both colors and lines
> to enhance the image of a contour plot.  What's the best way to do
> this with Lattice graphics?  The following is one ugly hack,
> producing the desired result after much trial and error (R 1.9.1
> alpha under Windows 2000):
>
> # setup
> DF <- expand.grid(x=1:3, y=1:3)
> DF$z <- (DF$x+DF$y)
>
> # Traditional "base" graphics:
> image(x=1:3, y=1:3, z=array(DF$z, dim=c(3,3)))
> contour(x=1:3, y=1:3, z=array(DF$z, dim=c(3,3)), add=T)
>
> # Lattice hack:
> lvlplt <- levelplot(z~x*y, DF)
> cont <- contourplot(z~x*y, DF)
> print(lvlplt, more=T)
> print(cont, position=c(0, 0, 0.9055, 1))
>
> ##################
>       By comparison, the same ugly hack in S-Plus required 0.952
> instead of 0.955.  Is this the best we can currently get with a
> modest effort?

There's certainly much easier ways to do this, namely 

contourplot(z~x*y, DF, region = TRUE)

or 

levelplot(z~x*y, DF, contour = TRUE)

the only restriction being that you would need to use the same 'at' 
vector. If you want them to differ, you need to write a small panel 
function of your own that makes use of panel.levelplot().

Deepayan



From wuertz at itp.phys.ethz.ch  Mon Jun  7 09:24:14 2004
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Mon, 07 Jun 2004 07:24:14 +0000
Subject: [R] Rmetrics - new Built 190.10054
Message-ID: <40C4181E.8010605@itp.phys.ethz.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040607/3c75b8f7/attachment.pl

From maj at stats.waikato.ac.nz  Mon Jun  7 09:29:08 2004
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Mon, 07 Jun 2004 19:29:08 +1200
Subject: [R] MCLUST Covariance Parameterization.
In-Reply-To: <20040607050833.38599.qmail@web51003.mail.yahoo.com>
References: <20040607050833.38599.qmail@web51003.mail.yahoo.com>
Message-ID: <40C41944.80401@stats.waikato.ac.nz>

I'm not going to directly answer your question but it seems to me that 
you want to fit a completely unconstrained Gaussian mixture model. This 
may not be the best thing to do as without constraints on the Sigma_k 
the model may have more parameters than it is reasonable to try to 
estimate with the available data.

A preliminary fit of a mixture model, even if it does not have quite the 
  covariance structure that you want gives a clustering of the data that 
you can use to explore the correlation structure of the components 
empirically: then you can revise the covariance structure and re-fit.

I think that this sort of approach is likely to be more effective than 
fitting the fully unstructured model directly.

Murray Jorgensen

KKThird at Yahoo.Com wrote:

> Hello all (especially MCLUS users). 
> 
> I'm trying to make use of the MCLUST package by C. Fraley and A. Raftery. My problem is trying to figure out how the (model) identifier (e.g, EII, VII, VVI, etc.) relates to the covariance matrix. The parameterization of the covariance matrix makes use of the method of decomposition in Banfield and Rraftery (1993) and Fraley and Raftery (2002) where 
> 
> Sigma_k = lambda_k*D_k*A_k*D_k^' 
> 
> where Sigma_k is the covariance matrix for the kth (k=1,...,G), lambda_k is the kth groups constant of proportionality, D_k is the orthogonal matrix of eigenvectors for the kth group, and A_k is a diagonal matrix whose elements are proportional to the eigenvalues. The parameterization of the covariance matrix Sigma_k depends on the distribution (whether spherical, diagonal, or ellipsoidal), volume (equal or variable), shape (equal or variable), and orientation (coordinate axes, equal, or variable). The distribution, volume, shape and orientation are a function of  lambda_k, D_k, and A_k. Thus, depending on whether or not these values are constant across class defines Sigma_k. 
> 
> What I'm trying to figure out is how the distribution, volume, shape, and orientation relate to Sigma_k. As far as the parameterization of Sigma_k, what do "distribution," "volume," "shape," and "orientation" even mean. Does a table exist of how these values relate to the Sigma_k? I know a table exists in the MCLUST software manual on the MCLUST website, but this table doesn't relate the values of distribution, volume, shape, and orientation to Sigma_k directly, only to how Sigma_k would be parameterized (this isn?t helpful unless you know what distribution, volume, shape and orientation  mean in terms of the within class covariance matrix) So, just what do the distribution, volume, shape, and orientation mean in the context of Sigma_k?
> 
>  
> What do the distribution, volume, shape, and orientation mean for a Sigma_k=sigma^2*I where I is a p by p covariance matrix, sigma^2 is the constant variance and Sigma_1=Sigma_2=....=Sigma_G. What about when a Sigma_k=sigma^2_k*I, or when Sigma_1=Sigma_2=....=Sigma_G in situations where each element of the (constant across class) covariance matrix is different?
> 
> I would say I have a pretty good understanding of finite mixture modeling, but nothing I've read (expect the works cited in the 2002 JASA paper) talks about parameterizing the Sigma_k matrix in such a way. It would be nice to specify a structure directly for Sigma_k (as most books talk about). Any help on this issue would be greatly appreciated. 
> 
> Thanks, 
> Ken.
> 
> __________________________________________________
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862



From Lorenz.Gygax at fat.admin.ch  Mon Jun  7 09:36:51 2004
From: Lorenz.Gygax at fat.admin.ch (Lorenz.Gygax@fat.admin.ch)
Date: Mon, 7 Jun 2004 09:36:51 +0200 
Subject: [R] dfs in lme
Message-ID: <BF74FADD4B44554CA7E53D0B5242CD6A018DAFDA@evd-s7014.evd.admin.ch>

Dear R-mixed-effects-modelers,

I could not answer this questions with the book by Pinheiro & Bates and did
not find anything appropriate in the archives, either ...

We are preparing a short lecture on degrees of freedom and would like to
show lme's as an example as we often need to work with these. I have a
problem in understanding how many dfs are needed if random terms are used
for explanatory variables in addition to the intercept (if I have understood
correctly that ist the same as saying that interactions between random and
fixed effects are considered). I tried the following code:

library ('nlme')
options (contrasts= c ('contr.treatment', 'contr.poly'))

# create fake data
data.df <- data.frame (gruppe= rep (1:4, rep (20, 4)))

# create response variable
data.df$zv <- rnorm (80, 2)

# create potential explanatory variables
data.df$explan <- rnorm (80, 2)
data.df$treat <-  as.factor (sample (1:3, 80, T))
data.df$treat1 <- as.factor (sample (1:4, 80, T))
data.df$treat2 <- as.factor (sample (1:5, 80, T))
data.df$treat3 <- as.factor (sample (1:6, 80, T))

# with each of the explanatory variables
withoutInt <- lme (zv ~ explan, data= data.df, random= ~1 | gruppe)
withInt <- lme (zv ~ explan, data= data.df, random= ~ explan | gruppe)
anova (withoutInt)
anova (withInt)
anova (withoutInt, withInt)


There are two main things that I wonder about:

(1) the two anova() commands on the single models have the same residual
degrees of freedom even though the model withInt has an additional
explanatory variable. Why are the residual dfs not reduced?

(2) In the model comparison, it becomes visible that the model with 'explan'
in the random effect does indeed use more dfs. But I cannot figure out where
that number of dfs comes from. I thought that basically for each of the
levels in the grouping variable additional parameters are estimated? Thus, I
would expect somethind like df(interaction)= df(explanatory
variable)*df(random effect), but what I find is:

explanatory variable	delta-dfs of the model comparison
                        (= dfs of the interaction of the explanatory
                           variable with the random effect 'gruppe',
                           which has 4 levels, 3 dfs)
continuous (1 df)		 2
3 levels (2 dfs)		 5
4 levels (3 dfs)		 9
5 levels (4 dfs)		14
6 levels (5 dfs)		20

Can anyone point me in the right direction on where and how to answer these
questions?

Many thanks and regards, Lorenz
- 
Lorenz Gygax
Tel: +41 (0)52 368 33 84 / lorenz.gygax at fat.admin.ch      

Center for proper housing of ruminants and pigs
Swiss Veterinary Office
agroscope FAT T??nikon, CH-8356 Ettenhausen / Switzerland
Fax : +41 (0)52 365 11 90 / Tel: +41 (0)52 368 31 31



From fm3a004 at math.uni-hamburg.de  Mon Jun  7 11:06:25 2004
From: fm3a004 at math.uni-hamburg.de (Christian Hennig)
Date: Mon, 7 Jun 2004 11:06:25 +0200 (MET DST)
Subject: [R] MCLUST Covariance Parameterization.
In-Reply-To: <20040607050833.38599.qmail@web51003.mail.yahoo.com>
Message-ID: <Pine.GSO.3.95q.1040607104603.17662A-100000@sun11.math.uni-hamburg.de>

Dear Ken,

in principle you have all relevant informations already in your mail.
As far as I know, the parameterization of Fraley and Raftery is the most 
intuitive one. I don't know for which kind of application you need 
direct parameterization,
but in my experience the parameters volume, shape and orientation are 
more interesting in most applications than the direct values of Sigma_k.

However, not all possible structures seem to be implemented. Your examples
are not, I suspect:

> What do the distribution, volume, shape, and orientation mean for a Sigma_k=sigma^2*I where I is a p by p covariance matrix, sigma^2 is the constant variance and Sigma_1=Sigma_2=....=Sigma_G. 

This would be VEE. If you assume det(Sigma_1)=1 (which is necessary for your
parameterization to be identified), then sigma^2 is lambda, i.e., 
the volume parameter, and Sigma_1 would be the remaining matrix product.
However, VEE is not implemented. You may mail to Chris Fraley and ask why...
You see that the problem is not the parameterization, but the fact that 
VEE is missing in mclust.

(It is somewhat confusing the you use I for the covariance matrix, because
emclust uses this letter for a covariance matrix, which is the identity 
matrix.)
 

> What about when a Sigma_k=sigma^2_k*I, or when Sigma_1=Sigma_2=....=Sigma_G in situations where each element of the (constant across class) covariance matrix is different?

I do not really understand this. Do you want to assume that the elements of
Sigma_1 should be pairwise different? Why do you need such an assumption?
That's not a very favourable choice for estimation, I think,  and it would 
be estimated by VEE as well (which would yield such a solution with
probability 1), if it would be implemented.

Best,
Christian

***********************************************************************
Christian Hennig
Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
#######################################################################
ich empfehle www.boag-online.de



From kan_liu1 at yahoo.com  Mon Jun  7 11:16:40 2004
From: kan_liu1 at yahoo.com (kan Liu)
Date: Mon, 7 Jun 2004 02:16:40 -0700 (PDT)
Subject: [R] Average R-squared of model1 to model n
Message-ID: <20040607091640.63819.qmail@web60603.mail.yahoo.com>

Hi,
 
We got a question about interpretating R-suqared.
 
The actual outputs for a test dataset is X=(x1,x2,
..., xn).
model 1 predicted the outputs as Y1=(y11,y12,..., y1n)
model n predicted the outputs as Y2=(y21,y22,..., y2n)
 
...
model m predicted the outputs as Ym=(ym1,ym2,..., ymn)
 
Now we have two ways to calculate R squared to
evaluate the average performance of committee model.
 
(a) Calculate R squared between (X, Y1), (X, Y2), ...,
(X,Ym), and then averaging the R squared
(b) Calculate average Y=(Y1+Y2, + ... Ym)/m, and then
calculate the R squared between (X, Y).
 
We found it seemed that R squared calculated in (b) is
'always' higher than that in (a).
 
Does this result depends on the test dataset or this
happened by chance?Can you advise me any reference for
this issue? 

Many thanks in advance!

Kan



From jlozano at apoy.upm.edu.ph  Mon Jun  7 12:06:02 2004
From: jlozano at apoy.upm.edu.ph (Jingky Lozano)
Date: Mon,  7 Jun 2004 18:06:02 +0800
Subject: [R] Censboot Warning and Error Messages
Message-ID: <1086602762.40c43e0a04fbf@mail.upm.edu.ph>

Good day R help list!!!

I've been trying to do Bootstrap in R on Censored data.  I encountered
WARNING/ERROR messages which I could not find explanation.
I've been searching on the literature for two days now and still can't find
answers.  I hope there's anyone out there who can help me 
with these two questions: 

1. If the "Loglik converged before variable..." message appears (please see
printout below) while doing ordinary bootstrap,
does it mean that I cannot trust the result of the bootstrap statistics?  Is
there a valid way to resolve it like increase the sample size?

2. In doing conditional bootstrap with survival data, how can one handle a data
with two largest survival time observations (ties)
but one is censored while the other is not.  For example, if the censoring time
is 48 months and a patient died exactly at that time, he will
have the same survival time as another patient who was also observed to lived 48
months long but classified as censored because he was still 
alive at the set censoring time.  Doing the recommended algorithm in R gives an
"error in sample length..." message.

I have pasted the actual outputs below for your reference.  Thanks in advanced.

regards,
Jei   





**********************************
#1

> library(survival)
> library(boot)
> filename <- type2a  # this file has 100 records
> 
> r <- 50  
> 
> data.cox <- coxph(Surv(time,cens==1)~x1+x2+x3a+x3b+x3c, data=filename)
> data.surv <- survfit(data.cox)
> data.cens <- survfit(Surv(time-0.001*(cens==1),cens!=1),data=filename)
> 
> beta.fun <- function(filename) { 
+ cox <- coef(coxph(Surv(filename$time,filename$cens==1) ~
filename$x1+filename$x2+filename$x3a+filename$x3b+filename$x3c))
+ }
> 
> 
> ## Calculate cox's regression coefficients' standard error and bias using the
Ordinary Bootstrap
> 
> set.seed(1234)#set the random start
> cox.ord <-censboot(data=filename,statistic=beta.fun,R=r,F.surv=data.surv,
G.surv=data.cens,cox=data.cox,sim="ordinary")
Warning messages: 
1: Loglik converged before variable  3,4,5 ; beta may be infinite.  in:
fitter(X, Y, strats, offset, init, control, weights = weights,  
2: Loglik converged before variable  3,4,5 ; beta may be infinite.  in:
fitter(X, Y, strats, offset, init, control, weights = weights,  
3: Loglik converged before variable  3,4,5 ; beta may be infinite.  in:
fitter(X, Y, strats, offset, init, control, weights = weights,  
4: Loglik converged before variable  3,4,5 ; beta may be infinite.  in:
fitter(X, Y, strats, offset, init, control, weights = weights,  
> 
> cox.ord

CASE RESAMPLING BOOTSTRAP FOR CENSORED DATA


Call:
censboot(data = filename, statistic = beta.fun, R = r, F.surv = data.surv, 
    G.surv = data.cens, sim = "ordinary", cox = data.cox)


Bootstrap Statistics :
       original        bias    std. error
t1*  0.01967371 -0.0007196731  0.01281441
t2* -0.93237995 -0.0126333032  0.41266809
t3*  0.70063667  1.2257364224  4.74488522
t4*  0.96192181  1.3134875081  4.77301974
t5*  2.90407255  1.4075296648  4.68596872
> 
*********************************
#2
> sample
   time cens group
1     9    1     1
2    13    1     1
3    13    0     1
4    18    1     1
5    23    1     1
6    28    0     1
7    31    1     1
8    34    1     1
9    45    0     1
10   48    1     1
11   48    0     1
12    5    1     2
13    5    1     2
14    8    1     2
15    8    1     2
16   12    1     2
17   16    0     2
18   23    1     2
19   27    1     2
20   30    1     2
21   33    1     2
22   43    1     2
23   45    1     2


> filename <- sample
> 
> r <- 50  
> 
> ## Calculate median survival time; standard error and bias using the Ordinary
Bootstrap 
> 
> data.fun <- function(data) {
+ surv <- survfit(Surv(data$time, data$cens))
+ md   <- min(surv$time[surv$surv<0.5])
+ }
> set.seed(1234)#set the random start
> msurv.ord <- censboot(data=filename, statistic=data.fun,R=r)
> msurv.ord#median survival time using ordinary bootstrap

CASE RESAMPLING BOOTSTRAP FOR CENSORED DATA


Call:
censboot(data = filename, statistic = data.fun, R = r)


Bootstrap Statistics :
    original  bias    std. error
t1*       27    0.52    5.466932
> 
> 
> ## Calculate median survival time; standard error and bias using the
Conditional Bootstrap
> 
> data.fun <- function(data) {
+ surv <- survfit(Surv(data$time, data$cens))
+ md   <- min(surv$time[surv$surv<0.5])
+ }
> data.fail <- survfit(Surv(time, cens), data=filename)
> data.cens <- survfit(Surv(time-0.001*cens,1-cens), data=filename)
> set.seed(1234)#set the random start
> msurv.cond <-censboot(data=filename,statistic=data.fun,R=r,
F.surv=data.fail,G.surv=data.cens,sim="cond")
Error in sample(length(x), size, replace, prob) : 
        invalid first argument
> msurv.cond



----------------------------------------------------------------

University of the Philippines Manila (http://mail.upm.edu.ph)



From Nicolas.Stransky at curie.fr  Mon Jun  7 12:18:19 2004
From: Nicolas.Stransky at curie.fr (Nicolas STRANSKY)
Date: Mon, 07 Jun 2004 12:18:19 +0200
Subject: [R] Aggregate rows to see the number of occurences
Message-ID: <40C440EB.60204@curie.fr>

Hi,

I have a set of data like the following:
       [,1]  [,2]
[1,]   10    2
[2,]    7    0
[3,]    1    0
[4,]    1    0
[5,]   15    0
[6,]   17    4
[7,]    4    0
[8,]   19    8
[9,]   10    2
[10,]  19    5

I'd like to aggregate it in order to obtain the frequency (the number of 
occurences) for each couple of values (e.g.: (10,2) appears twice, (7,0) 
appears once). Something cool would be to have this value in a third 
column...
I've been looking at aggregate() but either I couldn't get the right 
parameters, or this is not the right tool to use...

Thank's for any help !

-- 
Nicolas STRANSKY
??quipe Oncologie Mol??culaire
Institut Curie - UMR 144 - CNRS                 Tel : +33 1 42 34 63 40
26, rue d'Ulm - 75248 Paris Cedex 5 - FRANCE    Fax : +33 1 42 34 63 49



From nicolas.pelay at rd.francetelecom.com  Mon Jun  7 12:57:48 2004
From: nicolas.pelay at rd.francetelecom.com (zze-PELAY Nicolas FTRD/DMR/BEL)
Date: Mon, 7 Jun 2004 12:57:48 +0200
Subject: [R] question
Message-ID: <D4EF10CCEB2CE742BEEA9838C590B0E801FD2C5A@ftrdmel2.rd.francetelecom.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040607/1120cf59/attachment.pl

From petr.pikal at precheza.cz  Mon Jun  7 13:05:02 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 07 Jun 2004 13:05:02 +0200
Subject: [R] Aggregate rows to see the number of occurences
In-Reply-To: <40C440EB.60204@curie.fr>
Message-ID: <40C467FE.31368.13D27F4@localhost>

Hallo

On 7 Jun 2004 at 12:18, Nicolas STRANSKY wrote:

> Hi,
> 
> I have a set of data like the following:
>        [,1]  [,2]
> [1,]   10    2
> [2,]    7    0
> [3,]    1    0
> [4,]    1    0
> [5,]   15    0
> [6,]   17    4
> [7,]    4    0
> [8,]   19    8
> [9,]   10    2
> [10,]  19    5

Maybe it can be done more elegantly but table and match can 
probably do what you want.

> tab
   V2 V3
1  10  2
2   7  0
3   1  0
4   1  0
5  15  0
6  17  4
7   4  0
8  19  8
9  10  2
10 19  5

> counts<-table(tab[,1])
> count.no<-as.numeric(names(counts))
> selection<-match(tab[,1],count.no)

> cbind(tab,no=as.numeric(counts[selection]))
   V2 V3 no
1  10  2  2
2   7  0  1
3   1  0  2
4   1  0  2
5  15  0  1
6  17  4  1
7   4  0  1
8  19  8  2
9  10  2  2
10 19  5  2
>

Is this what you want?

Cheers
Petr

> 
> I'd like to aggregate it in order to obtain the frequency (the number
> of occurences) for each couple of values (e.g.: (10,2) appears twice,
> (7,0) appears once). Something cool would be to have this value in a
> third column... I've been looking at aggregate() but either I couldn't
> get the right parameters, or this is not the right tool to use...
> 
> Thank's for any help !
> 
> -- 
> Nicolas STRANSKY
> ??quipe Oncologie Mol??culaire
> Institut Curie - UMR 144 - CNRS                 Tel : +33 1 42 34 63
> 40 26, rue d'Ulm - 75248 Paris Cedex 5 - FRANCE    Fax : +33 1 42 34
> 63 49
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From petr.pikal at precheza.cz  Mon Jun  7 13:12:07 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 07 Jun 2004 13:12:07 +0200
Subject: [R] question
In-Reply-To: <D4EF10CCEB2CE742BEEA9838C590B0E801FD2C5A@ftrdmel2.rd.francetelecom.fr>
Message-ID: <40C469A7.6186.143A768@localhost>

Hi

On 7 Jun 2004 at 12:57, zze-PELAY Nicolas FTRD/DMR/BE 
wrote:

> Hello,
> 
> I'd like to know if I can modify a constant value with a function.
> 
> Example :
> 
> a=4
> 
> f1<-function()
> {a=3}

Simply use <<- in the function. But I am not sure if this is 
recommendable way of items manipulating in R.

> f1<-function() a<<-3
> f1()
> a<-4
> a
[1] 4
> f1()
> a
[1] 3

Cheers
Petr

BTW using informative subject is preferable to plain "question" 
statement. :-)

> 
> Of course, after the function f1() is executed , the value of a is
> always 4. I'd like the value returned is 3, like it is possible in C
> by doing *a=3
> 
> 
> Thank you
> 
> nicolas
> 
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From petr.pikal at precheza.cz  Mon Jun  7 13:27:22 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 07 Jun 2004 13:27:22 +0200
Subject: [R] Aggregate rows to see the number of occurences
In-Reply-To: <40C467FE.31368.13D27F4@localhost>
References: <40C440EB.60204@curie.fr>
Message-ID: <40C46D3A.361.1519D23@localhost>

Oops, you wanted to count number of pairs, this modification 
should work

int<-interaction(tab[,1],tab[,2])
counts<-table(int)
count.no<-names(counts)
selection<-match(int,count.no)
cbind(tab,no=as.numeric(counts[selection]))

Cheers
Petr





On 7 Jun 2004 at 13:05, Petr Pikal wrote:

> Hallo
> 
> On 7 Jun 2004 at 12:18, Nicolas STRANSKY wrote:
> 
> > Hi,
> > 
> > I have a set of data like the following:
> >        [,1]  [,2]
> > [1,]   10    2
> > [2,]    7    0
> > [3,]    1    0
> > [4,]    1    0
> > [5,]   15    0
> > [6,]   17    4
> > [7,]    4    0
> > [8,]   19    8
> > [9,]   10    2
> > [10,]  19    5
> 
> Maybe it can be done more elegantly but table and match can 
> probably do what you want.
> 
> > tab
>    V2 V3
> 1  10  2
> 2   7  0
> 3   1  0
> 4   1  0
> 5  15  0
> 6  17  4
> 7   4  0
> 8  19  8
> 9  10  2
> 10 19  5
> 
> > counts<-table(tab[,1])
> > count.no<-as.numeric(names(counts))
> > selection<-match(tab[,1],count.no)
> 
> > cbind(tab,no=as.numeric(counts[selection]))
>    V2 V3 no
> 1  10  2  2
> 2   7  0  1
> 3   1  0  2
> 4   1  0  2
> 5  15  0  1
> 6  17  4  1
> 7   4  0  1
> 8  19  8  2
> 9  10  2  2
> 10 19  5  2
> >
> 
> Is this what you want?
> 
> Cheers
> Petr
> 
> > 
> > I'd like to aggregate it in order to obtain the frequency (the
> > number of occurences) for each couple of values (e.g.: (10,2)
> > appears twice, (7,0) appears once). Something cool would be to have
> > this value in a third column... I've been looking at aggregate() but
> > either I couldn't get the right parameters, or this is not the right
> > tool to use...
> > 
> > Thank's for any help !
> > 
> > -- 
> > Nicolas STRANSKY
> > ??quipe Oncologie Mol??culaire
> > Institut Curie - UMR 144 - CNRS                 Tel : +33 1 42 34 63
> > 40 26, rue d'Ulm - 75248 Paris Cedex 5 - FRANCE    Fax : +33 1 42 34
> > 63 49
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 
> Petr Pikal
> petr.pikal at precheza.cz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From laura at env.leeds.ac.uk  Mon Jun  7 13:32:20 2004
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Mon, 7 Jun 2004 12:32:20 +0100 (BST)
Subject: [R] filled.contour - color palette so z=0 ONLY is blue
Message-ID: <Pine.LNX.4.44.0406071228270.12968-100000@env-pc-phd13>

I am trying to create a topographic map of an island - the filled.contour
function works fine except i am experiencing difficulty trying to
represent the sea properly. Basically I want the default colour blue for
any instance where z=0, if I simply use the default topo.color I get
shades of blue for z values up to 220 metres. Is there a way in which I
can specify terrain.color for all values of z>0 but blue for z=0 - or is
there another way of acheiving this simply?

Thanks,
Laura



From j.van_den_hoff at fz-rossendorf.de  Mon Jun  7 13:36:01 2004
From: j.van_den_hoff at fz-rossendorf.de (joerg van den hoff)
Date: Mon, 07 Jun 2004 13:36:01 +0200
Subject: [R] Confidence intervals for predicted values in nls
In-Reply-To: <006201c44982$fa8a99e0$52040a0a@Csilva>
References: <200406031006.i53A2wwV002842@hypatia.math.ethz.ch>
	<006201c44982$fa8a99e0$52040a0a@Csilva>
Message-ID: <40C45321.3050408@fz-rossendorf.de>

Cristina Silva wrote:

>Dear all
>
>I have tried to estimate the confidence intervals for predicted values of a
>nonlinear model fitted with nls. The function predict gives the predicted
>values and the lower and upper limits of the prediction, when the class of
>the object is lm or glm. When the object is derived from nls, the function
>predict (or predict.nls) gives only the predicted values. The se.fit and
>interval aguments are just ignored.
>
>Could anybody tell me how to estimate the confidence intervals for the
>predicted values (not the model parameters), using an object of class nls?
>
>Regards,
>
>Cristina
>
>------------------------------------------
>Cristina Silva
>IPIMAR - Departamento de Recursos Marinhos
>Av. de Bras??lia, 1449-006 Lisboa
>Portugal
>Tel.: 351 21 3027096
>Fax: 351 21 3015948
>csilva at ipimar.pt
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>
maybe this example helps:

==============cut here===============
#define a model formula (a and b are the parameters, "f" is "x"):
frml <- k1 ~ f*(1-a*exp(-b/f))

#simulate some data:
a0 <- .6
b0 <- 1.2
f  <- seq(0.01,4,length=20)
k1true<- f*(1-a0*exp(-b0/f))
#include some noise
amp <- .1
k1 <- rnorm(k1true,k1true,amp*k1true)

#fit:
fifu <- deriv(frml,c("a","b"),function(a,b,x){})
rr<-nls(k1~fifu(a,b,f),start=list(a=.5,b=1))

#the derivatives and variance/covariance matrix:
#(derivs could be extracted from fifu, too)
dk1.da <- D(frml[[3]],'a')
dk1.db <- D(frml[[3]],'b')
covar <- vcov(rr)

#gaussian error propagation:
a <- coef(rr)['a']
b <- coef(rr)['b']
vark1 <-
       eval(dk1.da)^2*covar[1,1]+
       eval(dk1.db)^2*covar[2,2]+
       2*eval(dk1.da)*eval(dk1.db)*covar[1,2]

errk1 <- sqrt(vark1)
lower.bound <- fitted(rr)-2*errk1      #two sigma !
upper.bound <- fitted(rr)+2*errk1      #dito

plot(f,k1,pch=1)
ff <- outer(c(1,1),f)
kk <- outer(c(1,1),k1)*c(1-amp,1+amp)
matlines(ff,kk,lty=3,col=1)

matlines(f,cbind(k1true,fitted(rr),lower.bound,upper.bound),col=c(1,2,3,3),lty=c(1,1,2,2))
xylim <- par('usr')
xpos <- .1*(xylim[2]-xylim[1])+xylim[1]
ypos <- xylim[4] - .1*(xylim[4]-xylim[3])
legend(xpos,ypos,
      c(     
         'data',
         'true',
         'fit', 
         'confidence'
       ),     
       pch=c(1,-1,-1,-1),
       lty=c(0,1,1,2),
       col=c(1,1,2,3)
)
==============cut here===============
if you put this in a file and source it a few times from within R you'll 
get an impression how often the fit deviates from the 'true' curve more 
than expected from
the shown confidence limits.

I believe this approach is 'nearly' valid as long as gaussian error 
probagation is valid (i.e. only to first order in covar and therefore 
for not too large std. errors, am I right?).
to my simple physicist's mind this should suffice to get 'reasonable' 
(probably, in strict sense, not completely correct?) confidence 
intervals for the fit/the prediction.
If somebody objects, please let me know!


joerg



From pallier at lscp.ehess.fr  Mon Jun  7 13:30:44 2004
From: pallier at lscp.ehess.fr (Christophe Pallier)
Date: Mon, 07 Jun 2004 13:30:44 +0200
Subject: [R] Aggregate rows to see the number of occurences
In-Reply-To: <40C440EB.60204@curie.fr>
References: <40C440EB.60204@curie.fr>
Message-ID: <40C451E4.3050007@lscp.ehess.fr>


 > I have a set of data like the following:

>       [,1]  [,2]
> [1,]   10    2 ...
> [10,]  19    5
>
> I'd like to aggregate it in order to obtain the frequency (the number 
> of occurences) for each couple of values (e.g.: (10,2) appears twice, 
> (7,0) appears once). Something cool would be to have this value in a 
> third column...
> I've been looking at aggregate() but either I couldn't get the right 
> parameters, or this is not the right tool to use...
>
You can use:

 x=paste(a[,1],a[,2],sep=",")
 table(x)

then, if you need to have the count for each line from the original table:
 
  table(x)[x]

Or you could indeed use the 'aggregate' function:

  aggregate(a[,1],list(a[,1],a[,2]),length)

This yields one line per unique value (but that may be what you want...)

Christophe Pallier
www.pallier.org



From p.dalgaard at biostat.ku.dk  Mon Jun  7 13:48:33 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 07 Jun 2004 13:48:33 +0200
Subject: [R] filled.contour - color palette so z=0 ONLY is blue
In-Reply-To: <Pine.LNX.4.44.0406071228270.12968-100000@env-pc-phd13>
References: <Pine.LNX.4.44.0406071228270.12968-100000@env-pc-phd13>
Message-ID: <x27jujk2r2.fsf@biostat.ku.dk>

Laura Quinn <laura at env.leeds.ac.uk> writes:

> I am trying to create a topographic map of an island - the filled.contour
> function works fine except i am experiencing difficulty trying to
> represent the sea properly. Basically I want the default colour blue for
> any instance where z=0, if I simply use the default topo.color I get
> shades of blue for z values up to 220 metres. Is there a way in which I
> can specify terrain.color for all values of z>0 but blue for z=0 - or is
> there another way of acheiving this simply?

 my.colors <- function(n)c("blue",terrain.colors(n-1))
 data(volcano) 
 filled.contour(volcano, color = my.colors, asp = 1)

You'll probably need to diddle the levels=  setting too.
-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Matthias.Templ at statistik.gv.at  Mon Jun  7 14:11:42 2004
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Mon, 7 Jun 2004 14:11:42 +0200
Subject: [R] question
Message-ID: <83536658864BC243BE3C06D7E936ABD50153692E@xchg1.statistik.local>

> Hello,
> 
> I'd like to know if I can modify a constant value with a function.
> 
> Example :
> 
> a=4
> 
> f1<-function()
> {a=3}
> 
> Of course, after the function f1() is executed , the value of 
> a is always 4. I'd like the value returned is 3, like it is 
> possible in C by doing *a=3
> 
Use return:

f1 <- function()
{a=3; return(a)}


Matthias



From andy_liaw at merck.com  Mon Jun  7 14:35:14 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 7 Jun 2004 08:35:14 -0400
Subject: [R] Average R-squared of model1 to model n
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7E35@usrymx25.merck.com>

The Y1, Y2, etc. that Kan mentioned are predicted values of a test set data
from models that supposedly were fitted to the same (or similar) data.  It's
hard for me to imagine the outcome would be as `severe' as Y1 = -Y2.

That said, I do not think that the R-squared (or q-squared as some call it)
of the aggregate model is necessarily larger or equal to the average
R-squared of the component models.  It obviously depends on how the
component models are generated.  As a hypothetical example (because I
haven't acutally tried it, just speculating):  Suppose the data are
generated from a step function, the sort that would be perfect for
regression trees.  If one grows several well-pruned trees, I'd guess that
the average R-squared of the individual trees has a chance of being larger
than the R-squared of the averaged model.

Best,
Andy

> From: Gabor Grothendieck
> 
> Suppose m=2, Y1=Y and Y2= -Y.  Then (b) is zero so (a) must be
> greater or equal to (b).  Thus (b) is not necessarily greater 
> than (a).
> 
> 
> kan Liu <kan_liu1 <at> yahoo.com> writes:
> 
> : 
> : Hi,
> : 
> : We got a question about interpretating R-suqared.
> : 
> : The actual outputs for a test dataset is X=(x1,x2, ..., xn).
> : model 1 predicted the outputs as Y1=(y11,y12,..., y1n)
> : model n predicted the outputs as Y2=(y21,y22,..., y2n)
> : 
> : ... 
> : model m predicted the outputs as Ym=(ym1,ym2,..., ymn)
> : 
> : Now we have two ways to calculate R squared to evaluate the average 
> performance of committee model.
> : 
> : (a) Calculate R squared between (X, Y1), (X, Y2), ..., 
> (X,Ym), and then 
> averaging the R squared
> : (b) Calculate average Y=(Y1+Y2, + ... Ym)/m, and then 
> calculate the R 
> squared between (X, Y). 
> : 
> : We found it seemed that R squared calculated in (b) is 
> 'always' higher than 
> that in (a).
> : 
> : Does this result depends on the test dataset or this 
> happened by chance?Can 
> you advise me any reference for
> : this issue? 
> : 
> : Many thanks in advance!
> : 
> : Kan
> : 
> : 
> : 		
> : ---------------------------------
> : 
> : 	[[alternative HTML version deleted]]
> : 
> : ______________________________________________
> : R-help <at> stat.math.ethz.ch mailing list
> : https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> : PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> : 
> :
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From statho3 at web.de  Mon Jun  7 14:43:48 2004
From: statho3 at web.de (Thomas Stabla)
Date: Mon, 7 Jun 2004 14:43:48 +0200 (CEST)
Subject: [R] Lazy Evaluation?
Message-ID: <Pine.LNX.4.58.0406071409590.1687@spock.vulcan>

Hello,

I've stumbled upon following problem, when trying to overload the methods
for group Math for an S4-class which contains functions as slots.


  setClass("NumFunction", representation = list(fun = "function"))

  NumFunction <- function(f) new("NumFunction", fun = f)

  square <- function(x) x^2
  NF <- NumFunction(square)

  setMethod("Math",
            "NumFunction",
            function(x){
                nfun  <- function(n) callGeneric(x at fun(n))
                tmp <- function(n) nfun(n)
                NumFunction(tmp)
            })

  sinNF <- sin(NF)
  sinNF at fun(sqrt(pi/2))

# works as expected, returns 1

# now a slightly different version of setMethod("Math", "NumFunction",
# ...), which dispenses the "unnecessary" wrapper function tmp()

  setMethod("Math",
            "NumFunction",
            function(x){
                nfun  <- function(n) callGeneric(x at fun(n))
                return(NumFunction(nfun))
            })

   sinNF <- sin(NF)
   sinNF at fun(sqrt(pi/2))

# produces an error, namely:
# Error in typeof(fdef) : evaluation nested too deeply: infinite
# recursion / options(expression=)?


Replacing the generating function NumFunction() with corresponding
new(..) calls doesn't change the outcome.

When I call the newly defined functions nfun resp. tmp from within the
function body of setMethod(), e.g.

  setMethod("Math",
            "NumFunction",
            function(x){
                nfun  <- function(n) callGeneric(x at fun(n))
                cat(nfun(1))
                NumFunction(nfun)
            })

by tmp(1) resp. nfun(1), both versions "cat" correct output when called by
sin(NF).

If I don't return NumFunction(tmp) resp. NumFunction(nfun) but tmp resp.
nfun, both versions work just fine, i.e. sin(NF)(sqrt(pi/2)) returns 1.

Would someone explain me this behavior? (R Version 1.9.0)

Best regards,
Thomas Stabla

Sourcecode can be found at:
http://www.uni-bayreuth.de/departments/math/org/mathe7/DISTR/NumFun.R



From Nicolas.Stransky at curie.fr  Mon Jun  7 14:44:32 2004
From: Nicolas.Stransky at curie.fr (Nicolas STRANSKY)
Date: Mon, 07 Jun 2004 14:44:32 +0200
Subject: [R] Aggregate rows to see the number of occurences
In-Reply-To: <40C46D3A.361.1519D23@localhost>
References: <40C440EB.60204@curie.fr> <40C46D3A.361.1519D23@localhost>
Message-ID: <40C46330.7040202@curie.fr>

Petr Pikal wrote:

> Oops, you wanted to count number of pairs, this modification 
> should work
> 
> int<-interaction(tab[,1],tab[,2])
> counts<-table(int)
> count.no<-names(counts)
> selection<-match(int,count.no)
> cbind(tab,no=as.numeric(counts[selection]))

Yes, thank's for all your answers. This is a way to do what I wanted to.

I have to also say that there is a "one-liner" to do it :) (hint by P. 
Hup??).

 > x <- cbind(c(10,7,1,1,15,17,4,19,10,19,10), c(2,0,0,0,0,4,0,8,2,5,2))
 > aggregate(x, list(a=x[,1],b=x[,2]),NROW)

-- 
Nicolas STRANSKY
??quipe Oncologie Mol??culaire
Institut Curie - UMR 144 - CNRS                 Tel : +33 1 42 34 63 40
26, rue d'Ulm - 75248 Paris Cedex 5 - FRANCE    Fax : +33 1 42 34 63 49



From sway at tanox.com  Mon Jun  7 14:49:09 2004
From: sway at tanox.com (Shawn Way)
Date: Mon, 7 Jun 2004 07:49:09 -0500
Subject: [R] Xtable giving an interesting problem
Message-ID: <2DBF8A8E1A1AEE4AB3618AC4D6BF3088071FEE@houston.tanox.net>

I'm using the current version of xtable for 1.9.0 and I have an
interesting error:

Error in match.names(clabs, names(xi)) : names don't match previous
names:
	 F value, Pr(>F)
In addition: Warning message: 
longer object length
	is not a multiple of shorter object length in: clabs == nmi 

This is produced in the following manner:

>data.trans <- data.frame(ref=rep(c(3.995,20.003),2),
                              actual=c(.1,100.3,.1,100.3),
                              level=gl(2,1))
 
>parameters <- data.frame(trans.range=c(0,100), trans.output=c(4,20),
                          ref.error=c(.0074))
>a <- lm(trans.range~trans.output,data=parameters)
>pred <- data.frame(trans.output=data.trans$actual)
>data.trans$cor <- predict(a,pred)
>fit.trans <- lm(cor~ref,data.trans)
>fit.trans.aov <- aov(cor~ref+Error(level),data.trans)
>fit.trans

Call:
lm(formula = cor ~ ref, data = data.trans)

Coefficients:
(Intercept)          ref  
    -180.66        39.12  

> fit.trans.aov

Call:
aov(formula = cor ~ ref + Error(level), data = data.trans)

Grand Mean: 288.75 

Stratum 1: level

Terms:
                     ref
Sum of Squares  392189.1
Deg. of Freedom        1

Estimated effects are balanced

Stratum 2: Within

Terms:
                   Residuals
Sum of Squares  1.074245e-27
Deg. of Freedom            2

Residual standard error: 2.317591e-14 
> summary(fit.trans.aov)

Error: level
    Df Sum Sq Mean Sq
ref  1 392189  392189

Error: Within
          Df     Sum Sq    Mean Sq F value Pr(>F)
Residuals  2 1.0742e-27 5.3712e-28               
> library(xtable)
> xtable(fit.trans.aov)
Error in match.names(clabs, names(xi)) : names don't match previous
names:
	 F value, Pr(>F)
In addition: Warning message: 
longer object length
	is not a multiple of shorter object length in: clabs == nmi 


Any ideas?


"Everything should made as simple as possible, but not simpler." 
-Albert Einstein


Shawn Way, PE
Tanox, Inc.
10301 Stella Link
Houston, TX 77025
Engineering Manager
Sway[at]tanox.com



Note: Any use, dissemination, forwarding, printing or copying of this
e-mail without consent of Tanox, Inc. is not authorized.  Further, this
communication may contain confidential information intended only for the
person to whom it is addressed, and any use, dissemination, forwarding,
printing or copying of such confidential information without the express
consent of Tanox or in violation of any agreements to which the
recipient is subject is prohibited.  If you have received this e-mail in
error, please immediately notify the sender and delete the original and
all copies.  Any views or opinions expressed may be solely those of the
author and do not necessarily represent the views or opinions of Tanox,
Inc.



From ripley at stats.ox.ac.uk  Mon Jun  7 15:30:05 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 7 Jun 2004 14:30:05 +0100 (BST)
Subject: [R] Xtable giving an interesting problem
In-Reply-To: <2DBF8A8E1A1AEE4AB3618AC4D6BF3088071FEE@houston.tanox.net>
Message-ID: <Pine.LNX.4.44.0406071419320.1149-100000@gannet.stats>

Did you try dump.frames/debugger?  Or even traceback()?

> debugger()
Message:  Error in match.names(clabs, names(xi)) : names don't match 
previous names:
         F value, Pr(>F)
Available environments had calls:
1: xtable(fit.trans.aov)
2: xtable.aovlist(fit.trans.aov)
3: xtable.summary.aovlist(summary(x), caption = caption, label = label, 
align = ali
4: rbind(result, xtable.anova(x[[i]][[1]], caption = caption, label = 
label, align
5: rbind(...)
6: match.names(clabs, names(xi))
7: stop(paste("names don't match previous names:\n\t", paste(nmi[nii == 0], collaps

and if you look in xtable.summary.aovlist you will see it is trying to 
rbind a dataframe with 5 columns to one with three columns.  That's not
surprising given the print output for summary(fit.trans.aov) and is a 
design error in the xtable package that should be reported to the 
maintainer.

However, is this model really `interesting'?  I cannot see what you hoped 
to learn by fitting it.


On Mon, 7 Jun 2004, Shawn Way wrote:

> I'm using the current version of xtable for 1.9.0 and I have an
> interesting error:
> 
> Error in match.names(clabs, names(xi)) : names don't match previous
> names:
> 	 F value, Pr(>F)
> In addition: Warning message: 
> longer object length
> 	is not a multiple of shorter object length in: clabs == nmi 
> 
> This is produced in the following manner:
> 
> >data.trans <- data.frame(ref=rep(c(3.995,20.003),2),
>                               actual=c(.1,100.3,.1,100.3),
>                               level=gl(2,1))
>  
> >parameters <- data.frame(trans.range=c(0,100), trans.output=c(4,20),
>                           ref.error=c(.0074))
> >a <- lm(trans.range~trans.output,data=parameters)
> >pred <- data.frame(trans.output=data.trans$actual)
> >data.trans$cor <- predict(a,pred)
> >fit.trans <- lm(cor~ref,data.trans)
> >fit.trans.aov <- aov(cor~ref+Error(level),data.trans)
> >fit.trans
> 
> Call:
> lm(formula = cor ~ ref, data = data.trans)
> 
> Coefficients:
> (Intercept)          ref  
>     -180.66        39.12  
> 
> > fit.trans.aov
> 
> Call:
> aov(formula = cor ~ ref + Error(level), data = data.trans)
> 
> Grand Mean: 288.75 
> 
> Stratum 1: level
> 
> Terms:
>                      ref
> Sum of Squares  392189.1
> Deg. of Freedom        1
> 
> Estimated effects are balanced
> 
> Stratum 2: Within
> 
> Terms:
>                    Residuals
> Sum of Squares  1.074245e-27
> Deg. of Freedom            2
> 
> Residual standard error: 2.317591e-14 
> > summary(fit.trans.aov)
> 
> Error: level
>     Df Sum Sq Mean Sq
> ref  1 392189  392189
> 
> Error: Within
>           Df     Sum Sq    Mean Sq F value Pr(>F)
> Residuals  2 1.0742e-27 5.3712e-28               
> > library(xtable)
> > xtable(fit.trans.aov)
> Error in match.names(clabs, names(xi)) : names don't match previous
> names:
> 	 F value, Pr(>F)
> In addition: Warning message: 
> longer object length
> 	is not a multiple of shorter object length in: clabs == nmi 
> 
> 
> Any ideas?
> 
> 
> "Everything should made as simple as possible, but not simpler." 
> -Albert Einstein
> 
> 
> Shawn Way, PE
> Tanox, Inc.
> 10301 Stella Link
> Houston, TX 77025
> Engineering Manager
> Sway[at]tanox.com
> 
> 
> 
> Note: Any use, dissemination, forwarding, printing or copying of this
> e-mail without consent of Tanox, Inc. is not authorized.  Further, this
> communication may contain confidential information intended only for the
> person to whom it is addressed, and any use, dissemination, forwarding,
> printing or copying of such confidential information without the express
> consent of Tanox or in violation of any agreements to which the
> recipient is subject is prohibited.  If you have received this e-mail in
> error, please immediately notify the sender and delete the original and
> all copies.  Any views or opinions expressed may be solely those of the
> author and do not necessarily represent the views or opinions of Tanox,
> Inc.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Mon Jun  7 15:34:57 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 7 Jun 2004 15:34:57 +0200
Subject: [R] question
In-Reply-To: <40C469A7.6186.143A768@localhost>
References: <D4EF10CCEB2CE742BEEA9838C590B0E801FD2C5A@ftrdmel2.rd.francetelecom.fr>
	<40C469A7.6186.143A768@localhost>
Message-ID: <16580.28417.571718.544664@gargle.gargle.HOWL>

>>>>> "Petr" == Petr Pikal <petr.pikal at precheza.cz>
>>>>>     on Mon, 07 Jun 2004 13:12:07 +0200 writes:

    Petr> Hi On 7 Jun 2004 at 12:57, zze-PELAY Nicolas
    Petr> FTRD/DMR/BE wrote:

    >> Hello,
    >> 
    >> I'd like to know if I can modify a constant value with a
    >> function.
    >> 
    >> Example :
    >> 
    >> a=4
    >> 
    >> f1<-function() {a=3}

    Petr> Simply use <<- in the function.
yes.

    Petr> But I am not sure if this is recommendable way of
    Petr> items manipulating in R. 

Thank you, Petr.  Indeed, I am even sure that it *is* not recommended:
Very much recommended is to write a function such that 

 1) it only uses its arguments (and no global variables) as input
 2) provides all its results as return() value, often a list().

There are exceptions to these rules, e.g. for plot() 
functions, "2)" will often not be true [and for print.*() methods,
the unwritten R standard asks for print.*(x, ....) to return(invisible(x))]
"1)" {and sometimes 2) as well} will be violated for UI (not only GUI)
functions. 

When working very large objects, it can make sense to
violate both "1)" and "2)".. -- but that's really for more
advanced programming.

Martin Maechler



From kkthird at yahoo.com  Mon Jun  7 16:01:53 2004
From: kkthird at yahoo.com (KKThird@Yahoo.Com)
Date: Mon, 7 Jun 2004 07:01:53 -0700 (PDT)
Subject: [R] MCLUST Covariance Parameterization.
In-Reply-To: <Pine.GSO.3.95q.1040607104603.17662A-100000@sun11.math.uni-hamburg.de>
Message-ID: <20040607140153.45842.qmail@web51010.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040607/4670d7ad/attachment.pl

From nicolas.pelay at rd.francetelecom.com  Mon Jun  7 16:08:58 2004
From: nicolas.pelay at rd.francetelecom.com (zze-PELAY Nicolas FTRD/DMR/BEL)
Date: Mon, 7 Jun 2004 16:08:58 +0200
Subject: [R] Precision for a former question*
Message-ID: <D4EF10CCEB2CE742BEEA9838C590B0E801FD3062@ftrdmel2.rd.francetelecom.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040607/12991795/attachment.pl

From fm3a004 at math.uni-hamburg.de  Mon Jun  7 16:14:49 2004
From: fm3a004 at math.uni-hamburg.de (Christian Hennig)
Date: Mon, 7 Jun 2004 16:14:49 +0200 (MET DST)
Subject: [R] MCLUST Covariance Parameterization.
In-Reply-To: <20040607140153.45842.qmail@web51010.mail.yahoo.com>
Message-ID: <Pine.GSO.3.95q.1040607160822.17662K-100000@sun11.math.uni-hamburg.de>

Hi Ken,

it seems that you want equal covariance matrices, which means equal, but
free volume, orientation and shape. That's "EEE", and it *is*
implemented. 

I still do not really understand your "translation" problem. All
information is in the formula which appeared in your first posting:

Sigma_k = lambda_k*D_k*A_k*D_k^'

That is, if you have lambda (volume>0), D (orientation, orthogonal) and A
(shape, diagonal>0), you can compute Sigma and if you have Sigma, you can
compute lambda, A, and D by spectral decomposition. 

Hope this helps,
Christian

On Mon, 7 Jun 2004, KKThird at Yahoo.Com wrote:

> Hi Christian and thanks for your message. 
>  
> >From reading "standard" mixture model books, I don't recall any of them talking directly about shape, orientation, and volume. So, in the finite mixture context I'm not exactly sure what these terms even mean as they relate to the covariance matrix. Maybe I'm missing something but it seems that the Fraley and Raftery framework is different than any mixture approach I've read about. 
>  
> I have a three dimensional model. Thus, the covariance matrix will be 3 by 3 for each of the G classes. I want the covariance matrix to be unrestricted (i.e., each of the 6 elements free to be estimated), yet for each of the G classes to share a common covariance matrix. That is, Sigma_1=Sigma_2=...=Sigma_G, where the elements are themselves unrestricted. 
>  
> My problem is translating the structure of the covariance matrix to the Fraley and Raftery framework. Reading their work I kept thanking they would have a table that translated the structure of the covariance matrix to their method of parameterization. You mention that it is the most intuitive framework, and I would agree if what was interested was the volume, shape, orientation, and distribution. My impression though is that people think in terms of the covariance structure. Where are these terms even defined? 
>  
> Anyway, thanks for your help. Any insight would be greatly appreciated. 
> Ken
> 
> 
> Christian Hennig <fm3a004 at math.uni-hamburg.de> wrote:
> Dear Ken,
> 
> in principle you have all relevant informations already in your mail.
> As far as I know, the parameterization of Fraley and Raftery is the most 
> intuitive one. I don't know for which kind of application you need 
> direct parameterization,
> but in my experience the parameters volume, shape and orientation are 
> more interesting in most applications than the direct values of Sigma_k.
> 
> However, not all possible structures seem to be implemented. Your examples
> are not, I suspect:
> 
> > What do the distribution, volume, shape, and orientation mean for a Sigma_k=sigma^2*I where I is a p by p covariance matrix, sigma^2 is the constant variance and Sigma_1=Sigma_2=....=Sigma_G. 
> 
> This would be VEE. If you assume det(Sigma_1)=1 (which is necessary for your
> parameterization to be identified), then sigma^2 is lambda, i.e., 
> the volume parameter, and Sigma_1 would be the remaining matrix product.
> However, VEE is not implemented. You may mail to Chris Fraley and ask why...
> You see that the problem is not the parameterization, but the fact that 
> VEE is missing in mclust.
> 
> (It is somewhat confusing the you use I for the covariance matrix, because
> emclust uses this letter for a covariance matrix, which is the identity 
> matrix.)
> 
> 
> > What about when a Sigma_k=sigma^2_k*I, or when Sigma_1=Sigma_2=....=Sigma_G in situations where each element of the (constant across class) covariance matrix is different?
> 
> I do not really understand this. Do you want to assume that the elements of
> Sigma_1 should be pairwise different? Why do you need such an assumption?
> That's not a very favourable choice for estimation, I think, and it would 
> be estimated by VEE as well (which would yield such a solution with
> probability 1), if it would be implemented.
> 
> Best,
> Christian
> 
> ***********************************************************************
> Christian Hennig
> Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
> hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
> #######################################################################
> ich empfehle www.boag-online.de
> 
> 
> __________________________________________________
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

***********************************************************************
Christian Hennig
Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
#######################################################################
ich empfehle www.boag-online.de



From spencer.graves at pdf.com  Mon Jun  7 17:05:17 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 07 Jun 2004 08:05:17 -0700
Subject: [R] contour lines on levelplot?
In-Reply-To: <200406070152.19328.deepayan@stat.wisc.edu>
References: <40C3C78D.1010001@pdf.com>
	<200406070152.19328.deepayan@stat.wisc.edu>
Message-ID: <40C4842D.2050009@pdf.com>

Thanks very much.  spencer graves

Deepayan Sarkar wrote:

>On Sunday 06 June 2004 20:40, Spencer Graves wrote:
>  
>
>>      With "image" and "contour", one can get both colors and lines
>>to enhance the image of a contour plot.  What's the best way to do
>>this with Lattice graphics?  The following is one ugly hack,
>>producing the desired result after much trial and error (R 1.9.1
>>alpha under Windows 2000):
>>
>># setup
>>DF <- expand.grid(x=1:3, y=1:3)
>>DF$z <- (DF$x+DF$y)
>>
>># Traditional "base" graphics:
>>image(x=1:3, y=1:3, z=array(DF$z, dim=c(3,3)))
>>contour(x=1:3, y=1:3, z=array(DF$z, dim=c(3,3)), add=T)
>>
>># Lattice hack:
>>lvlplt <- levelplot(z~x*y, DF)
>>cont <- contourplot(z~x*y, DF)
>>print(lvlplt, more=T)
>>print(cont, position=c(0, 0, 0.9055, 1))
>>
>>##################
>>      By comparison, the same ugly hack in S-Plus required 0.952
>>instead of 0.955.  Is this the best we can currently get with a
>>modest effort?
>>    
>>
>
>There's certainly much easier ways to do this, namely 
>
>contourplot(z~x*y, DF, region = TRUE)
>
>or 
>
>levelplot(z~x*y, DF, contour = TRUE)
>
>the only restriction being that you would need to use the same 'at' 
>vector. If you want them to differ, you need to write a small panel 
>function of your own that makes use of panel.levelplot().
>
>Deepayan
>  
>



From sdhyok at email.unc.edu  Mon Jun  7 17:16:10 2004
From: sdhyok at email.unc.edu (Shin, Daehyok)
Date: Mon, 7 Jun 2004 11:16:10 -0400
Subject: [R] Vectors of years, months, and days to dates?
Message-ID: <OAEOKPIGCLDDHAEMCAKIAEBDCOAA.sdhyok@email.unc.edu>

The interface for dates in R is a little confusing to me.
I want to create a vector of Date objects from vectors of years, months, and
days.
One solution I found is:

years <- c(1991, 1992)
months <- c(1, 10)
days <- c(1, 2)

dates <- as.Date(ISOdate(years, months, days))

But, in this solution the ISOdate function converts the vectors into
characters,
which can cause serious performance and memory loss
when the vectors of years, months, and days are huge.
I am quite sure there is much better solution for it. What is it?

Thanks.

Daehyok Shin



From Matthias.Schmidt at forst.bwl.de  Mon Jun  7 17:40:46 2004
From: Matthias.Schmidt at forst.bwl.de (Schmidt.Matthias (FORST))
Date: Mon, 7 Jun 2004 17:40:46 +0200 
Subject: [R] models for compositional data by J. Aitchison
Message-ID: <855D381F618DE84FA58C035BA803FCE6B9A735@fvafr-se1.forst.bwl.de>

Hi,
is there any R software for model building with data of the compositional
data type:
The statistical analysis of compositional data" by J. Aitchison.

thanks

***********************************************************
Matthias Schmidt
Forstliche Versuchs- und Forschungsanstalt Baden-W??rttemberg (FVA) 
Abteilung Biometrie und Informatik
Wonnhaldestr. 4
79100 Freiburg i. Br
Tel.: + 49 (0)761 / 4018 -187
Fax: + 49 (0)761 / 4018 - 333



From ripley at stats.ox.ac.uk  Mon Jun  7 17:44:09 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 7 Jun 2004 16:44:09 +0100 (BST)
Subject: [R] Vectors of years, months, and days to dates?
In-Reply-To: <OAEOKPIGCLDDHAEMCAKIAEBDCOAA.sdhyok@email.unc.edu>
Message-ID: <Pine.LNX.4.44.0406071626300.13644-100000@gannet.stats>

On Mon, 7 Jun 2004, Shin, Daehyok wrote:

> The interface for dates in R is a little confusing to me.
> I want to create a vector of Date objects from vectors of years, months, and
> days.
> One solution I found is:
> 
> years <- c(1991, 1992)
> months <- c(1, 10)
> days <- c(1, 2)
> 
> dates <- as.Date(ISOdate(years, months, days))
> 
> But, in this solution the ISOdate function converts the vectors into
> characters,
> which can cause serious performance and memory loss
> when the vectors of years, months, and days are huge.

Really?  You have measured the loss?  A million causes no problem for 
example, and what are you going to do with a million dates that is 
instantaneous and worthwhile?  And a million dates are hardly going to be 
unique so you only need to convert the unique values.

> I am quite sure there is much better solution for it. What is it?

Write your own C code, or make a POSIXlt object directly from the numbers 
and convert that.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sdhyok at email.unc.edu  Mon Jun  7 18:01:56 2004
From: sdhyok at email.unc.edu (Daehyok Shin)
Date: Mon, 07 Jun 2004 12:01:56 -0400
Subject: [R] Vectors of years, months, and days to dates?
In-Reply-To: <Pine.LNX.4.44.0406071626300.13644-100000@gannet.stats>
References: <Pine.LNX.4.44.0406071626300.13644-100000@gannet.stats>
Message-ID: <155082767.1086609715@davsta30.depts.unc.edu>

How can I create POSIXlt directly from the numbers?
I failed to find the solution from help documents.

Daehyok

--On Monday, June 07, 2004 4:44 PM +0100 Prof Brian Ripley 
<ripley at stats.ox.ac.uk> wrote:

> On Mon, 7 Jun 2004, Shin, Daehyok wrote:
>
>> The interface for dates in R is a little confusing to me.
>> I want to create a vector of Date objects from vectors of years, months,
>> and days.
>> One solution I found is:
>>
>> years <- c(1991, 1992)
>> months <- c(1, 10)
>> days <- c(1, 2)
>>
>> dates <- as.Date(ISOdate(years, months, days))
>>
>> But, in this solution the ISOdate function converts the vectors into
>> characters,
>> which can cause serious performance and memory loss
>> when the vectors of years, months, and days are huge.
>
> Really?  You have measured the loss?  A million causes no problem for
> example, and what are you going to do with a million dates that is
> instantaneous and worthwhile?  And a million dates are hardly going to be
> unique so you only need to convert the unique values.
>
>> I am quite sure there is much better solution for it. What is it?
>
> Write your own C code, or make a POSIXlt object directly from the numbers
> and convert that.
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From Roger.Bivand at nhh.no  Mon Jun  7 18:19:42 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 7 Jun 2004 18:19:42 +0200 (CEST)
Subject: [R] Vectors of years, months, and days to dates?
In-Reply-To: <155082767.1086609715@davsta30.depts.unc.edu>
Message-ID: <Pine.LNX.4.44.0406071815580.311-100000@reclus.nhh.no>

On Mon, 7 Jun 2004, Daehyok Shin wrote:

> How can I create POSIXlt directly from the numbers?
> I failed to find the solution from help documents.

?POSIXlt

?as.POSIXlt:

> res <- as.POSIXlt(paste(years, months, days, sep="-"))
> str(res)
`POSIXlt', format: chr [1:2] "1991-01-01" "1992-10-02"
> res$year 
[1] 91 92

> 
> Daehyok
> 
> --On Monday, June 07, 2004 4:44 PM +0100 Prof Brian Ripley 
> <ripley at stats.ox.ac.uk> wrote:
> 
> > On Mon, 7 Jun 2004, Shin, Daehyok wrote:
> >
> >> The interface for dates in R is a little confusing to me.
> >> I want to create a vector of Date objects from vectors of years, months,
> >> and days.
> >> One solution I found is:
> >>
> >> years <- c(1991, 1992)
> >> months <- c(1, 10)
> >> days <- c(1, 2)
> >>
> >> dates <- as.Date(ISOdate(years, months, days))
> >>
> >> But, in this solution the ISOdate function converts the vectors into
> >> characters,
> >> which can cause serious performance and memory loss
> >> when the vectors of years, months, and days are huge.
> >
> > Really?  You have measured the loss?  A million causes no problem for
> > example, and what are you going to do with a million dates that is
> > instantaneous and worthwhile?  And a million dates are hardly going to be
> > unique so you only need to convert the unique values.
> >
> >> I am quite sure there is much better solution for it. What is it?
> >
> > Write your own C code, or make a POSIXlt object directly from the numbers
> > and convert that.
> >
> > --
> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865 272861 (self)
> > 1 South Parks Road,                     +44 1865 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From dmurdoch at pair.com  Mon Jun  7 18:22:19 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Mon, 07 Jun 2004 12:22:19 -0400
Subject: [R] Precision for a former question*
In-Reply-To: <D4EF10CCEB2CE742BEEA9838C590B0E801FD3062@ftrdmel2.rd.francetelecom.fr>
References: <D4EF10CCEB2CE742BEEA9838C590B0E801FD3062@ftrdmel2.rd.francetelecom.fr>
Message-ID: <0d59c0llququo0q8qpnehv1p33772g0k67@4ax.com>

On Mon, 7 Jun 2004 16:08:58 +0200, "zze-PELAY Nicolas FTRD/DMR/BEL"
<nicolas.pelay at rd.francetelecom.com> wrote :

>I got many answers to the question I asked below , and I thank you all.
>Several of you told me to use ->> but told also that "it is not a
>recommendable way of items manipulating in R".
>I don't really understand what it exactly means :
>	1) does it mean it's not a very good way of programming , a
>dangerous way of programming because the variable a can be modified ? ,
>etc
>Or 	2) does it mean that R is not done to manipulate the operation
>->> in a function in a safe way , that  there can be some mistakes, that
>the memory allocation is not forseen for this case?

I think it's dangerous in the first sense.  It's best to try to
contain the effects of functions as much as possible, avoiding side
effects like a change to a variable outside the function.

Something else that hasn't been mentioned is that <<- is different in
R and S-PLUS, so it may come to haunt you if you ever try to port your
code there.

Duncan Murdoch



From sdhyok at email.unc.edu  Mon Jun  7 18:29:54 2004
From: sdhyok at email.unc.edu (Shin, Daehyok)
Date: Mon, 7 Jun 2004 12:29:54 -0400
Subject: [R] Vectors of years, months, and days to dates?
In-Reply-To: <Pine.LNX.4.44.0406071815580.311-100000@reclus.nhh.no>
Message-ID: <OAEOKPIGCLDDHAEMCAKIOEBECOAA.sdhyok@email.unc.edu>

> > res <- as.POSIXlt(paste(years, months, days, sep="-"))

This command still convert numbers to a character vector, right?

Daehyok Shin (Peter)
Terrestrial Hydrological Ecosystem Modellers
Geography Department
University of North Carolina-Chapel Hill
sdhyok at email.unc.edu

"We can do no great things, 
only small things with great love."
                         - Mother Teresa

> -----Original Message-----
> From: Roger Bivand [mailto:Roger.Bivand at nhh.no]
> Sent: Monday, June 07, 2004 PM 12:20
> To: Daehyok Shin
> Cc: R, Help
> Subject: Re: [R] Vectors of years, months, and days to dates?
> 
> 
> On Mon, 7 Jun 2004, Daehyok Shin wrote:
> 
> > How can I create POSIXlt directly from the numbers?
> > I failed to find the solution from help documents.
> 
> ?POSIXlt
> 
> ?as.POSIXlt:
> 
> > res <- as.POSIXlt(paste(years, months, days, sep="-"))
> > str(res)
> `POSIXlt', format: chr [1:2] "1991-01-01" "1992-10-02"
> > res$year 
> [1] 91 92
> 
> > 
> > Daehyok
> > 
> > --On Monday, June 07, 2004 4:44 PM +0100 Prof Brian Ripley 
> > <ripley at stats.ox.ac.uk> wrote:
> > 
> > > On Mon, 7 Jun 2004, Shin, Daehyok wrote:
> > >
> > >> The interface for dates in R is a little confusing to me.
> > >> I want to create a vector of Date objects from vectors of 
> years, months,
> > >> and days.
> > >> One solution I found is:
> > >>
> > >> years <- c(1991, 1992)
> > >> months <- c(1, 10)
> > >> days <- c(1, 2)
> > >>
> > >> dates <- as.Date(ISOdate(years, months, days))
> > >>
> > >> But, in this solution the ISOdate function converts the vectors into
> > >> characters,
> > >> which can cause serious performance and memory loss
> > >> when the vectors of years, months, and days are huge.
> > >
> > > Really?  You have measured the loss?  A million causes no problem for
> > > example, and what are you going to do with a million dates that is
> > > instantaneous and worthwhile?  And a million dates are hardly 
> going to be
> > > unique so you only need to convert the unique values.
> > >
> > >> I am quite sure there is much better solution for it. What is it?
> > >
> > > Write your own C code, or make a POSIXlt object directly from 
> the numbers
> > > and convert that.
> > >
> > > --
> > > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > > University of Oxford,             Tel:  +44 1865 272861 (self)
> > > 1 South Parks Road,                     +44 1865 272866 (PA)
> > > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> > >
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> > 
> 
> -- 
> Roger Bivand
> Economic Geography Section, Department of Economics, Norwegian School of
> Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
> Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
> e-mail: Roger.Bivand at nhh.no
> 
> 
>



From sdhyok at email.unc.edu  Mon Jun  7 18:32:23 2004
From: sdhyok at email.unc.edu (Shin, Daehyok)
Date: Mon, 7 Jun 2004 12:32:23 -0400
Subject: [R] Vectors of years, months, and days to dates?
In-Reply-To: <OF8304BB0D.5B36A247-ON85256EAC.00592330@nd.convergys.com>
Message-ID: <OAEOKPIGCLDDHAEMCAKICEBFCOAA.sdhyok@email.unc.edu>

Thanks, but, what I want is to convert three vectors of years, months, and
days directly into POSIXlt or Date objects
without creating character vectors.

Daehyok Shin (Peter)
Terrestrial Hydrological Ecosystem Modellers
Geography Department
University of North Carolina-Chapel Hill
sdhyok at email.unc.edu

"We can do no great things,
only small things with great love."
                         - Mother Teresa

> -----Original Message-----
> From: james.holtman at convergys.com [mailto:james.holtman at convergys.com]
> Sent: Monday, June 07, 2004 PM 12:15
> To: sdhyok at email.unc.edu
> Subject: Re: [R] Vectors of years, months, and days to dates?
>
>
>
>
>
>
> If you 'unclass' dates you will get a numeric vector that just
> has the days
> since the epoch.  Is this what you want?
>
> > years <- c(1991, 1992)
> > months <- c(1, 10)
> > days <- c(1, 2)
> >
> > dates <- as.Date(ISOdate(years, months, days))
> > dates
> [1] "1991-01-01" "1992-10-02"
> > str(dates)
> Class 'Date'  num [1:2] 7670 8310
> > unclass(dates)
> [1] 7670 8310
> >
> __________________________________________________________
> James Holtman        "What is the problem you are trying to solve?"
> Executive Technical Consultant  --  Office of Technology, Convergys
> james.holtman at convergys.com
> +1 (513) 723-2929
>
>
>
>
>                       "Shin, Daehyok"
>
>                       <sdhyok at email.unc.edu        To:       "R,
> Help" <r-help at stat.math.ethz.ch>
>                       >                            cc:
>
>                       Sent by:                     Subject:  [R]
> Vectors of years, months, and days to dates?
>                       r-help-bounces at stat.m
>
>                       ath.ethz.ch
>
>
>
>
>
>                       06/07/2004 11:16
>
>                       Please respond to
>
>                       sdhyok
>
>
>
>
>
>
>
>
>
> The interface for dates in R is a little confusing to me.
> I want to create a vector of Date objects from vectors of years, months,
> and
> days.
> One solution I found is:
>
> years <- c(1991, 1992)
> months <- c(1, 10)
> days <- c(1, 2)
>
> dates <- as.Date(ISOdate(years, months, days))
>
> But, in this solution the ISOdate function converts the vectors into
> characters,
> which can cause serious performance and memory loss
> when the vectors of years, months, and days are huge.
> I am quite sure there is much better solution for it. What is it?
>
> Thanks.
>
> Daehyok Shin
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>
>
>
>



From p.dalgaard at biostat.ku.dk  Mon Jun  7 18:35:26 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 07 Jun 2004 18:35:26 +0200
Subject: [R] Vectors of years, months, and days to dates?
In-Reply-To: <Pine.LNX.4.44.0406071626300.13644-100000@gannet.stats>
References: <Pine.LNX.4.44.0406071626300.13644-100000@gannet.stats>
Message-ID: <x265a3fhrl.fsf@biostat.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> On Mon, 7 Jun 2004, Shin, Daehyok wrote:
> 
> > The interface for dates in R is a little confusing to me.
> > I want to create a vector of Date objects from vectors of years, months, and
> > days.
> > One solution I found is:
> > 
> > years <- c(1991, 1992)
> > months <- c(1, 10)
> > days <- c(1, 2)
> > 
> > dates <- as.Date(ISOdate(years, months, days))
> > 
> > But, in this solution the ISOdate function converts the vectors into
> > characters,
> > which can cause serious performance and memory loss
> > when the vectors of years, months, and days are huge.
> 
> Really?  You have measured the loss?  A million causes no problem for 
> example, and what are you going to do with a million dates that is 
> instantaneous and worthwhile?  And a million dates are hardly going to be 
> unique so you only need to convert the unique values.

However, considering what goes on in ISOdatetime, one might as well do

  as.Date(paste(years, months, days, sep="-"))


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From s2tpyork at mail1.vcu.edu  Mon Jun  7 18:55:37 2004
From: s2tpyork at mail1.vcu.edu (Tim York)
Date: Mon, 07 Jun 2004 12:55:37 -0400
Subject: [R] error message
Message-ID: <40C49E09.50909@titan.vcu.edu>

I received the below error using UNIX R.  Could someone instruct me on 
helpful batch options to avoid this please? 

"ERROR: cannot allocate vector of size 320000kb"



From Roger.Bivand at nhh.no  Mon Jun  7 19:02:22 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 7 Jun 2004 19:02:22 +0200 (CEST)
Subject: [R] Vectors of years, months, and days to dates?
In-Reply-To: <OAEOKPIGCLDDHAEMCAKIOEBECOAA.sdhyok@email.unc.edu>
Message-ID: <Pine.LNX.4.44.0406071847040.311-100000@reclus.nhh.no>

On Mon, 7 Jun 2004, Shin, Daehyok wrote:

> > > res <- as.POSIXlt(paste(years, months, days, sep="-"))
> 
> This command still convert numbers to a character vector, right?

Yes, as Prof. Ripley said, the overhead of conversion to character and
using carefully crafted R functions is much less, for reasonable numbers
of dates, than inserting all the appropriate values into the POSIXlt class
object, since you need, in addition to years (since 1900), checked
month-days (31 February?), and months, week-days, year days, daylight
savings time flag, and seconds, minutes and hours. 

I would also be interested to know whether you can demonstrate
(system.time()) that anything reliable is faster than as.POSIXlt() for
fewer than millions of dates (and would you trust it on 29 February even
if it was faster?). Using base classes and functions is in general much
more robust than rolling your own, simply because many more people use
them - they have much more data run through them, and the time you might
save trying to avoid integer to character conversion will/should be eaten
up by debugging.

> 
> Daehyok Shin (Peter)
> Terrestrial Hydrological Ecosystem Modellers
> Geography Department
> University of North Carolina-Chapel Hill
> sdhyok at email.unc.edu
> 
> "We can do no great things, 
> only small things with great love."
>                          - Mother Teresa
> 
> > -----Original Message-----
> > From: Roger Bivand [mailto:Roger.Bivand at nhh.no]
> > Sent: Monday, June 07, 2004 PM 12:20
> > To: Daehyok Shin
> > Cc: R, Help
> > Subject: Re: [R] Vectors of years, months, and days to dates?
> > 
> > 
> > On Mon, 7 Jun 2004, Daehyok Shin wrote:
> > 
> > > How can I create POSIXlt directly from the numbers?
> > > I failed to find the solution from help documents.
> > 
> > ?POSIXlt
> > 
> > ?as.POSIXlt:
> > 
> > > res <- as.POSIXlt(paste(years, months, days, sep="-"))
> > > str(res)
> > `POSIXlt', format: chr [1:2] "1991-01-01" "1992-10-02"
> > > res$year 
> > [1] 91 92
> > 
> > > 
> > > Daehyok
> > > 
> > > --On Monday, June 07, 2004 4:44 PM +0100 Prof Brian Ripley 
> > > <ripley at stats.ox.ac.uk> wrote:
> > > 
> > > > On Mon, 7 Jun 2004, Shin, Daehyok wrote:
> > > >
> > > >> The interface for dates in R is a little confusing to me.
> > > >> I want to create a vector of Date objects from vectors of 
> > years, months,
> > > >> and days.
> > > >> One solution I found is:
> > > >>
> > > >> years <- c(1991, 1992)
> > > >> months <- c(1, 10)
> > > >> days <- c(1, 2)
> > > >>
> > > >> dates <- as.Date(ISOdate(years, months, days))
> > > >>
> > > >> But, in this solution the ISOdate function converts the vectors into
> > > >> characters,
> > > >> which can cause serious performance and memory loss
> > > >> when the vectors of years, months, and days are huge.
> > > >
> > > > Really?  You have measured the loss?  A million causes no problem for
> > > > example, and what are you going to do with a million dates that is
> > > > instantaneous and worthwhile?  And a million dates are hardly 
> > going to be
> > > > unique so you only need to convert the unique values.
> > > >
> > > >> I am quite sure there is much better solution for it. What is it?
> > > >
> > > > Write your own C code, or make a POSIXlt object directly from 
> > the numbers
> > > > and convert that.
> > > >
> > > > --
> > > > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > > > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > > > University of Oxford,             Tel:  +44 1865 272861 (self)
> > > > 1 South Parks Road,                     +44 1865 272866 (PA)
> > > > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> > > >
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > > 
> > 
> > -- 
> > Roger Bivand
> > Economic Geography Section, Department of Economics, Norwegian School of
> > Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
> > Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
> > e-mail: Roger.Bivand at nhh.no
> > 
> > 
> > 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From p.dalgaard at biostat.ku.dk  Mon Jun  7 18:56:57 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 07 Jun 2004 18:56:57 +0200
Subject: [R] Vectors of years, months, and days to dates?
In-Reply-To: <OAEOKPIGCLDDHAEMCAKICEBFCOAA.sdhyok@email.unc.edu>
References: <OAEOKPIGCLDDHAEMCAKICEBFCOAA.sdhyok@email.unc.edu>
Message-ID: <x21xkrfgrq.fsf@biostat.ku.dk>

"Shin, Daehyok" <sdhyok at email.unc.edu> writes:

> Thanks, but, what I want is to convert three vectors of years, months, and
> days directly into POSIXlt or Date objects
> without creating character vectors.

Well, if you insist:

 x <- as.POSIXlt(structure(rep(0,2),class="POSIXct"))
 x$year <- years-1900
 x$mon <- months-1
 x$mday <- days
 as.Date(as.POSIXct(x))

This is definitely not kosher programming, and I doubt that it is the
least bit faster than the other solutions, but there's no character
vectors in there...

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ggrothendieck at myway.com  Mon Jun  7 19:05:28 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 7 Jun 2004 17:05:28 +0000 (UTC)
Subject: [R] Vectors of years, months, and days to dates?
References: <OF8304BB0D.5B36A247-ON85256EAC.00592330@nd.convergys.com>
	<OAEOKPIGCLDDHAEMCAKICEBFCOAA.sdhyok@email.unc.edu>
Message-ID: <loom.20040607T185904-774@post.gmane.org>

Shin, Daehyok <sdhyok <at> email.unc.edu> writes:

> 
> Thanks, but, what I want is to convert three vectors of years, months, and
> days directly into POSIXlt or Date objects
> without creating character vectors.

I would probably just convert it to character using paste and from
that to Date as others have already suggested but if it is really
important to you for some reason not to use character variables
as intermediates try this:

require(chron)
z <- structure(julian(months, days, years), class="Date")

Note that when you print it out it may look like its character but
its not.    Try 

class(z)
is.character(z)
str(z)
dput(z)

to convince yourself.



From andy_liaw at merck.com  Mon Jun  7 19:09:27 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 7 Jun 2004 13:09:27 -0400
Subject: [R] error message
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7E3D@usrymx25.merck.com>

The message says at some point in the calculation, R tries to get about
312MB of memory and was not able to.  Maybe try increasing the amount of
virtual (or physical) memory in your system?

Andy

> From: Tim York
> 
> I received the below error using UNIX R.  Could someone 
> instruct me on 
> helpful batch options to avoid this please? 
> 
> "ERROR: cannot allocate vector of size 320000kb"
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From andy_liaw at merck.com  Mon Jun  7 19:24:38 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 7 Jun 2004 13:24:38 -0400
Subject: [R] (low level) profiling of code
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7E3E@usrymx25.merck.com>

Dear R-help,

Can some one tell me how to profile compiled code dynamically loaded into R?
Here's what I tried on our dual Opteron running SUSE Linux Enterprise Server
8 (GCC 3.3):

Start with R-patched dated 2004-06-07: 

(I also had MAIN_CFLAGS="-pg" in config.site.)

R is now configured for x86_64-unknown-linux-gnu

  Source directory:          .
  Installation directory:    /usr/local

  C compiler:                gcc  -O2 -g -pg -march=k8 -msse2 -m64
  C++ compiler:              g++  -O2 -g -pg -march=k8 -msse2 -m64
  Fortran compiler:          g77  -O2 -g -pg -march=k8 -msse2 -m64

  Interfaces supported:      X11, tcltk
  External libraries:        readline
  Additional capabilities:   PNG, JPEG
  Options enabled:           R profiling

  Recommended packages:      no

I then tried running 

  /path/to/R-patched/bin/R CMD BATCH -q -slave myscript.R

where inside myscript.R is call to R functions that calls .C().  However,
this does not produce the gmon.out file.  Can anyone tell me what I'm
missing?  Any help much appreciated!

Best,
Andy

Andy Liaw, PhD
Biometrics Research      PO Box 2000, RY33-300     
Merck Research Labs           Rahway, NJ 07065
andy_liaw (at) merck.com        732-594-0820



From dli at ctg.queensu.ca  Mon Jun  7 19:44:49 2004
From: dli at ctg.queensu.ca (don)
Date: Mon, 07 Jun 2004 13:44:49 -0400
Subject: [R] ask for help
Message-ID: <40C4A991.3020304@ctg.queensu.ca>

Dear colleagues,
I am a beginner of using R. I am involved in some microarray data 
analyses and found that R is the most common software in this area. I 
tried to install the free software today. It seemed to be successful in 
installing, but when I trigger the short-cut, I got a error massage 
saying 'Fetal error: invalid HOMEDRIVE'. I have no idea where the 
homedrive was defined (or implied) and how to solve the problem. I 
wonder if someone could help me to start the first step of this fancy trip.
Many thanks!

Don Li
National Cancer Institute of Canada - Clinical Trials Group



From ripley at stats.ox.ac.uk  Mon Jun  7 19:46:45 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 7 Jun 2004 18:46:45 +0100 (BST)
Subject: [R] (low level) profiling of code
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7E3E@usrymx25.merck.com>
Message-ID: <Pine.LNX.4.44.0406071841170.13771-100000@gannet.stats>

You normally need to switch R profiling off, as that uses the same
interrupts as low-level profiling.

I would expect you to get gmon.out output from the the main R executable 
on any run, so the first question must be `does your OS support -pg?'

I don't think dynamically loaded code is relevant (provided it is compiled
with -pg).  I am pretty sure I managed to profile some examples from the
cluster package on RH8.0 Linux the other day.

On Mon, 7 Jun 2004, Liaw, Andy wrote:

> Dear R-help,
> 
> Can some one tell me how to profile compiled code dynamically loaded into R?
> Here's what I tried on our dual Opteron running SUSE Linux Enterprise Server
> 8 (GCC 3.3):
> 
> Start with R-patched dated 2004-06-07: 
> 
> (I also had MAIN_CFLAGS="-pg" in config.site.)
> 
> R is now configured for x86_64-unknown-linux-gnu
> 
>   Source directory:          .
>   Installation directory:    /usr/local
> 
>   C compiler:                gcc  -O2 -g -pg -march=k8 -msse2 -m64
>   C++ compiler:              g++  -O2 -g -pg -march=k8 -msse2 -m64
>   Fortran compiler:          g77  -O2 -g -pg -march=k8 -msse2 -m64
> 
>   Interfaces supported:      X11, tcltk
>   External libraries:        readline
>   Additional capabilities:   PNG, JPEG
>   Options enabled:           R profiling
> 
>   Recommended packages:      no
> 
> I then tried running 
> 
>   /path/to/R-patched/bin/R CMD BATCH -q -slave myscript.R
> 
> where inside myscript.R is call to R functions that calls .C().  However,
> this does not produce the gmon.out file.  Can anyone tell me what I'm
> missing?  Any help much appreciated!


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From wolski at molgen.mpg.de  Mon Jun  7 19:53:19 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Mon, 07 Jun 2004 19:53:19 +0200
Subject: [R] ask for help
In-Reply-To: <40C4A991.3020304@ctg.queensu.ca>
References: <40C4A991.3020304@ctg.queensu.ca>
Message-ID: <200406071953190743.017766D6@mail.math.fu-berlin.de>

Hi Don!

Welcome new R user.

Just use one of the mailing list search interfaces which you can find browsing the www.r-project.org page.
After you find the search interface just paste you error message in the search form.
I ensure you you will find the a lot of e-mails wich will help you to solve your problem.

Sincerely

Eryk

*********** REPLY SEPARATOR  ***********

On 6/7/2004 at 1:44 PM don wrote:

>>>Dear colleagues,
>>>I am a beginner of using R. I am involved in some microarray data 
>>>analyses and found that R is the most common software in this area. I 
>>>tried to install the free software today. It seemed to be successful in 
>>>installing, but when I trigger the short-cut, I got a error massage 
>>>saying 'Fetal error: invalid HOMEDRIVE'. I have no idea where the 
>>>homedrive was defined (or implied) and how to solve the problem. I 
>>>wonder if someone could help me to start the first step of this fancy
>>>trip.
>>>Many thanks!
>>>
>>>Don Li
>>>National Cancer Institute of Canada - Clinical Trials Group
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



Dipl. bio-chem. Eryk Witold Wolski    @    MPI-Moleculare Genetic   
Ihnestrasse 63-73 14195 Berlin       'v'    
tel: 0049-30-83875219               /   \    
mail: wolski at molgen.mpg.de        ---W-W----    http://www.molgen.mpg.de/~wolski



From ripley at stats.ox.ac.uk  Mon Jun  7 19:49:34 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 7 Jun 2004 18:49:34 +0100 (BST)
Subject: [R] ask for help
In-Reply-To: <40C4A991.3020304@ctg.queensu.ca>
Message-ID: <Pine.LNX.4.44.0406071847110.13771-100000@gannet.stats>

The problem is probably a bug in your OS.  You haven't even told us what
that is, but I bet you have Windows XP with critical update KB835732
installed.  If so, see

http://www.murdoch-sutherland.com/HOMEPATH.html

and the archives of this list.

On Mon, 7 Jun 2004, don wrote:

> I am a beginner of using R. I am involved in some microarray data 
> analyses and found that R is the most common software in this area. I 
> tried to install the free software today. It seemed to be successful in 
> installing, but when I trigger the short-cut, I got a error massage 
> saying 'Fetal error: invalid HOMEDRIVE'. I have no idea where the 
> homedrive was defined (or implied) and how to solve the problem. I 
> wonder if someone could help me to start the first step of this fancy trip.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rolf at math.unb.ca  Mon Jun  7 20:18:23 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Mon, 7 Jun 2004 15:18:23 -0300 (ADT)
Subject: [R] ask for help
Message-ID: <200406071818.i57IINHr008825@erdos.math.unb.ca>


Don Li wrote:

> I am a beginner of using R. I am involved in some microarray data 
> analyses and found that R is the most common software in this area. I 
> tried to install the free software today. It seemed to be successful in 
> installing, but when I trigger the short-cut, I got a error massage 
> saying 'Fetal error: .....

	Shouldn't that be ``Foetal error''? :-)

				cheers,

					Rolf Turner
					rolf at math.unb.ca



From sdhyok at email.unc.edu  Mon Jun  7 20:20:48 2004
From: sdhyok at email.unc.edu (Shin, Daehyok)
Date: Mon, 7 Jun 2004 14:20:48 -0400
Subject: [R] Vectors of years, months, and days to dates?
In-Reply-To: <Pine.LNX.4.44.0406071847040.311-100000@reclus.nhh.no>
Message-ID: <OAEOKPIGCLDDHAEMCAKIKEBICOAA.sdhyok@email.unc.edu>

Is what I asked such an exceptional case?
A lot of data I am dealing with (usually hydrologic data) are recorded with
three columns of years, months, and days.
In my opinion, the direct conversion from numeric vectors of years, months
and days into some internal representation of date
is widely supported in most matrix oriented languages (ex. datenum() in
MATLAB).
Am I really asking an odd operation for date conversion?

For performance, you are right. The loss of performance is negligible when
users call it directly.
But, what happens when it is called in an iterative loop?
How can we assume some functions are only called directly by users in an
interactive shell?

Please consider positively this kind of simple interface for as.Date.
I am quite sure this operation will be used widely once implemented.

as.Date(c(years, months, days))

Is there no one supporting my idea?

Daehyok Shin (Peter)
Terrestrial Hydrological Ecosystem Modellers
Geography Department
University of North Carolina-Chapel Hill
sdhyok at email.unc.edu

"We can do no great things,
only small things with great love."
                         - Mother Teresa

> -----Original Message-----
> From: Roger Bivand [mailto:Roger.Bivand at nhh.no]
> Sent: Monday, June 07, 2004 PM 1:02
> To: Shin, Daehyok
> Cc: R, Help
> Subject: RE: [R] Vectors of years, months, and days to dates?
>
>
> On Mon, 7 Jun 2004, Shin, Daehyok wrote:
>
> > > > res <- as.POSIXlt(paste(years, months, days, sep="-"))
> >
> > This command still convert numbers to a character vector, right?
>
> Yes, as Prof. Ripley said, the overhead of conversion to character and
> using carefully crafted R functions is much less, for reasonable numbers
> of dates, than inserting all the appropriate values into the POSIXlt class
> object, since you need, in addition to years (since 1900), checked
> month-days (31 February?), and months, week-days, year days, daylight
> savings time flag, and seconds, minutes and hours.
>
> I would also be interested to know whether you can demonstrate
> (system.time()) that anything reliable is faster than as.POSIXlt() for
> fewer than millions of dates (and would you trust it on 29 February even
> if it was faster?). Using base classes and functions is in general much
> more robust than rolling your own, simply because many more people use
> them - they have much more data run through them, and the time you might
> save trying to avoid integer to character conversion will/should be eaten
> up by debugging.
>
> >
> > Daehyok Shin (Peter)
> > Terrestrial Hydrological Ecosystem Modellers
> > Geography Department
> > University of North Carolina-Chapel Hill
> > sdhyok at email.unc.edu
> >
> > "We can do no great things,
> > only small things with great love."
> >                          - Mother Teresa
> >
> > > -----Original Message-----
> > > From: Roger Bivand [mailto:Roger.Bivand at nhh.no]
> > > Sent: Monday, June 07, 2004 PM 12:20
> > > To: Daehyok Shin
> > > Cc: R, Help
> > > Subject: Re: [R] Vectors of years, months, and days to dates?
> > >
> > >
> > > On Mon, 7 Jun 2004, Daehyok Shin wrote:
> > >
> > > > How can I create POSIXlt directly from the numbers?
> > > > I failed to find the solution from help documents.
> > >
> > > ?POSIXlt
> > >
> > > ?as.POSIXlt:
> > >
> > > > res <- as.POSIXlt(paste(years, months, days, sep="-"))
> > > > str(res)
> > > `POSIXlt', format: chr [1:2] "1991-01-01" "1992-10-02"
> > > > res$year
> > > [1] 91 92
> > >
> > > >
> > > > Daehyok
> > > >
> > > > --On Monday, June 07, 2004 4:44 PM +0100 Prof Brian Ripley
> > > > <ripley at stats.ox.ac.uk> wrote:
> > > >
> > > > > On Mon, 7 Jun 2004, Shin, Daehyok wrote:
> > > > >
> > > > >> The interface for dates in R is a little confusing to me.
> > > > >> I want to create a vector of Date objects from vectors of
> > > years, months,
> > > > >> and days.
> > > > >> One solution I found is:
> > > > >>
> > > > >> years <- c(1991, 1992)
> > > > >> months <- c(1, 10)
> > > > >> days <- c(1, 2)
> > > > >>
> > > > >> dates <- as.Date(ISOdate(years, months, days))
> > > > >>
> > > > >> But, in this solution the ISOdate function converts the
> vectors into
> > > > >> characters,
> > > > >> which can cause serious performance and memory loss
> > > > >> when the vectors of years, months, and days are huge.
> > > > >
> > > > > Really?  You have measured the loss?  A million causes no
> problem for
> > > > > example, and what are you going to do with a million dates that is
> > > > > instantaneous and worthwhile?  And a million dates are hardly
> > > going to be
> > > > > unique so you only need to convert the unique values.
> > > > >
> > > > >> I am quite sure there is much better solution for it. What is it?
> > > > >
> > > > > Write your own C code, or make a POSIXlt object directly from
> > > the numbers
> > > > > and convert that.
> > > > >
> > > > > --
> > > > > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > > > > Professor of Applied Statistics,
> http://www.stats.ox.ac.uk/~ripley/
> > > > > University of Oxford,             Tel:  +44 1865 272861 (self)
> > > > > 1 South Parks Road,                     +44 1865 272866 (PA)
> > > > > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> > > > >
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > > >
> > >
> > > --
> > > Roger Bivand
> > > Economic Geography Section, Department of Economics,
> Norwegian School of
> > > Economics and Business Administration, Breiviksveien 40,
> N-5045 Bergen,
> > > Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
> > > e-mail: Roger.Bivand at nhh.no
> > >
> > >
> > >
> >
>
> --
> Roger Bivand
> Economic Geography Section, Department of Economics, Norwegian School of
> Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
> Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
> e-mail: Roger.Bivand at nhh.no
>
>
>



From rolf at math.unb.ca  Mon Jun  7 20:33:12 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Mon, 7 Jun 2004 15:33:12 -0300 (ADT)
Subject: [R] Vectors of years, months, and days to dates?
Message-ID: <200406071833.i57IXCDv009438@erdos.math.unb.ca>

Daehyok Shin (Peter) wrote:

> Is what I asked such an exceptional case?

	Apparently.

> A lot of data I am dealing with (usually hydrologic data) are
> recorded with three columns of years, months, and days.  In my
> opinion, the direct conversion from numeric vectors of years, months
> and days into some internal representation of date is widely
> supported in most matrix oriented languages (ex. datenum() in
> MATLAB).  Am I really asking an odd operation for date conversion?

	Apparently.  You still haven't given any indication of what's
	wrong with the way R does things, which is perfectly
	transparent and user friendly.  If other packages do things
	in a slightly different way, so what?

> For performance, you are right. The loss of performance is negligible
> when users call it directly.  But, what happens when it is called in
> an iterative loop?  How can we assume some functions are only called
> directly by users in an interactive shell?

	Have you ***any*** evidence that R's procedure degrades
	performance, under any circumstances?  (Apparently not.) In
	that case why are you going on and on about it?

> Please consider positively this kind of simple interface for as.Date.
> [as.Date(c(years, months, days))]

	The standard response to this kind of request is ``R is a
	cooperative endeavour.  Feel free to contribute.''

> I am quite sure this operation will be used widely once implemented.

	Have you ***any*** evidence for this assertion?

> Is there no one supporting my idea?

	Apparently not.

					cheers,

						Rolf Turner
						rolf at math.unb.ca



From spencer.graves at pdf.com  Mon Jun  7 20:48:27 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 07 Jun 2004 11:48:27 -0700
Subject: [R] GLMM(..., family=binomial(link="cloglog"))?
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7DFB@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF7DFB@usrymx25.merck.com>
Message-ID: <40C4B87B.7060301@pdf.com>

      Thanks, Andy, Doug, Deepayan.  I now have lme4 0.6-1 2004/05/31 
installed for R 1.9.1 alpha under Windows 2000.  When I tried the 
example below, GLMM ran, but the print method reported an error: 

Generalized Linear Mixed Model

Fixed: immun ~ 1
Data: guImmun
 log-likelihood:  -1440.052
Random effects:
 Groups Name        Variance Std.Dev.
 comm   (Intercept) 0.31686  0.5629 

Estimated scale (compare to 1)  0.9717356
Error in cm[, 1] : incorrect number of dimensions

      The following is a possibly simpler example that returns the same 
error message: 

 > simDF <- data.frame(yield=(1:4)/5, gp=factor(rep(1:2,2)), nobs=5)
 > simFit <- GLMM(yield~1, family=binomial, data=simDF, random=~1|gp, 
weights=nobs)
Iteration 1 Termination Criterion: 2.206532e-08
Iteration 2 Termination Criterion: 8.659736e-16
 > simFit
Generalized Linear Mixed Model

Fixed: yield ~ 1
Data: simDF
 log-likelihood:  -2.128803
Random effects:
 Groups Name        Variance   Std.Dev. 
 gp     (Intercept) 2.2065e-09 4.6974e-05

Estimated scale (compare to 1)  1
Error in cm[, 1] : incorrect number of dimensions
 >
      Any suggestions on what I should try next?  So far, I've only 
gotten this with an intercept-only model such as yield~1.  When I added 
a term from the help file example making it, "immun ~ kid2p", this error 
disappeared. 

      Thanks,
      Spencer Graves

Liaw, Andy wrote:

>As Doug said in his announcement, version 0.6-1 of lme4 (which is pure R
>code) depends on the Matrix package, version 0.8-7.  AFAICT the Windows
>binary on CRAN for Matrix is version 0.8-6.  Not sure if that will work with
>the current lme4...  It's probably best to wait for the right versions of
>these packages to propagate to appropriate places on CRAN...
>
>Best,
>Andy
>
>  
>
>>From: Spencer Graves
>>
>>Hi, Deepayan: 
>>
>>      Thanks for your reply.  How can I get the new release 
>>in a Windows 
>>2000 format, downloaded and properly installed? 
>>
>>      I tried "update.packages", but the new version has not yet 
>>migrated within reach of the default "update.packages" 
>>function call.  I 
>>tried downloading "lme4 0.6-0-2.tar.gz" from 
>>"http://www.stat.wisc.edu/%7Ebates/", unzipping it and 
>>replacing the old 
>>"\\R\rw1090pat\library\lme4" with the new.  Unfortunately, 
>>when I then 
>>tried to start R, I got a fatal error message: 
>>
>>      Error in loadNamespace(i[[1]], c(lib.loc, libPaths()), 
>>keep.source):  There is no package called 'Matrix'. 
>>
>>      Fortunately, I was able to restore R to apparently functioning 
>>order by merely restoring the old version. 
>>
>>      Is there something easy I can do to get "lme4 
>>0.6-0-2.tar.gz" to 
>>install properly for me?  Or do I need to wait until it 
>>migrates to my 
>>update.packages default ("http://cran.r-project.org") or some other 
>>designated CRAN mirror? 
>>
>>      Thanks for your excellent work in bringing this 
>>excellent software 
>>to its present state. 
>>
>>      Best Wishes,
>>      spencer graves
>>
>>Deepayan Sarkar wrote:
>>
>>    
>>
>>>On Tuesday 01 June 2004 17:25, Spencer Graves wrote:
>>> 
>>>
>>>      
>>>
>>>>     I'm having trouble using binomial(link="cloglog") with GLMM in
>>>>lme4, Version: 0.5-2, Date: 2004/03/11.  The example in the 
>>>>        
>>>>
>>Help file
>>    
>>
>>>>works fine, even simplified as follows:
>>>>
>>>>     fm0 <- GLMM(immun~1, data=guImmun, family=binomial,
>>>>random=~1|comm)
>>>>
>>>>     However, for another application, I need
>>>>binomial(link="cloglog"),
>>>>
>>>>and this generates an error for me:
>>>>        
>>>>
>>>>>fm0. <- GLMM(immun~1, data=guImmun,
>>>>>family=binomial(link="cloglog"),
>>>>>          
>>>>>
>>>>random=~1|comm)
>>>>Error in getClass(thisClass) : "family" is not a defined class
>>>>Error in GLMM(immun ~ 1, data = guImmun, family = binomial(link =
>>>>"cloglog"),  :
>>>>       S language method selection got an error when called from
>>>>internal dispatch for function "GLMM"
>>>>
>>>>     Any suggestions?
>>>>   
>>>>
>>>>        
>>>>
>>>This should work better in the new lme4 (0.6-x) announced 
>>>      
>>>
>>earlier today 
>>    
>>
>>>on r-packages. There is still a bug with cases (like in your 
>>>      
>>>
>>example) 
>>    
>>
>>>with only one fixed effect where the show and summary 
>>>      
>>>
>>methods produce 
>>    
>>
>>>an error, but that should be fixed soon.
>>>
>>>Deepayan
>>> 
>>>
>>>      
>>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From Roger.Bivand at nhh.no  Mon Jun  7 21:00:13 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 7 Jun 2004 21:00:13 +0200 (CEST)
Subject: [R] Vectors of years, months, and days to dates?
In-Reply-To: <OAEOKPIGCLDDHAEMCAKIKEBICOAA.sdhyok@email.unc.edu>
Message-ID: <Pine.LNX.4.44.0406072029300.311-100000@reclus.nhh.no>

On Mon, 7 Jun 2004, Shin, Daehyok wrote:

> Is what I asked such an exceptional case?
> A lot of data I am dealing with (usually hydrologic data) are recorded with
> three columns of years, months, and days.
> In my opinion, the direct conversion from numeric vectors of years, months
> and days into some internal representation of date
> is widely supported in most matrix oriented languages (ex. datenum() in
> MATLAB).
> Am I really asking an odd operation for date conversion?
> 
> For performance, you are right. The loss of performance is negligible when
> users call it directly.
> But, what happens when it is called in an iterative loop?
> How can we assume some functions are only called directly by users in an
> interactive shell?

Please read the code inside the functions we have been discussing. You 
will see that there is plenty of error checking, which is needed. In 
addition, paste() is vectorised. Specifically:

> years <- sample(1900:2000, 100000, replace=TRUE)
> months <- sample(1:12, 100000, replace=TRUE)
> days <- sample(1:28, 100000, replace=TRUE)
> system.time(x <- as.Date(as.POSIXlt(paste(years, months, days, sep="-"))))
[1] 5.91 0.02 5.98 0.00 0.00
> f <- function(y, m, d) { # taken from Peter Dalgaard's reply
+ x <- as.POSIXlt(structure(rep(0,length(y)),class="POSIXct"))
+ x$year <- y-1900
+ x$mon <- m-1
+ x$mday <- d
+ as.Date(as.POSIXct(x))
+ }
> system.time(y <- f(years, months, days))
[1] 2.76 0.01 2.78 0.00 0.00
> identical(x, y)
[1] TRUE

So for 100000 dates, the integer insertion is only about twice as fast, 
but it took much longer than the difference to find that out.

> 
> Please consider positively this kind of simple interface for as.Date.
> I am quite sure this operation will be used widely once implemented.
> 
> as.Date(c(years, months, days))

I have a feeling that won't do what you want, really. Looking at 
as.POSIXlt, and as.Date, you could create a variant recognising an integer 
matrix with your special class as having years in column 1, etc., but I 
think that your implementation and support costs will outbalance the 
saving you think you are making. 

The most enjoyable fortunes package has a fitting opinion:

> fortune("Fox")

I think that it's generally a good idea not to resist the most natural way of
programming in R.
   -- John Fox
      R-help (March 2004)
> 
> Is there no one supporting my idea?

Well, you are, so that's a start - contribute an "as.Date.MatrixOfDates" 
method to dispatch on a "MatrixOfDates" class, and you may find others?

> 
> Daehyok Shin (Peter)
> Terrestrial Hydrological Ecosystem Modellers
> Geography Department
> University of North Carolina-Chapel Hill
> sdhyok at email.unc.edu
> 
> "We can do no great things,
> only small things with great love."
>                          - Mother Teresa
> 
> > -----Original Message-----
> > From: Roger Bivand [mailto:Roger.Bivand at nhh.no]
> > Sent: Monday, June 07, 2004 PM 1:02
> > To: Shin, Daehyok
> > Cc: R, Help
> > Subject: RE: [R] Vectors of years, months, and days to dates?
> >
> >
> > On Mon, 7 Jun 2004, Shin, Daehyok wrote:
> >
> > > > > res <- as.POSIXlt(paste(years, months, days, sep="-"))
> > >
> > > This command still convert numbers to a character vector, right?
> >
> > Yes, as Prof. Ripley said, the overhead of conversion to character and
> > using carefully crafted R functions is much less, for reasonable numbers
> > of dates, than inserting all the appropriate values into the POSIXlt class
> > object, since you need, in addition to years (since 1900), checked
> > month-days (31 February?), and months, week-days, year days, daylight
> > savings time flag, and seconds, minutes and hours.
> >
> > I would also be interested to know whether you can demonstrate
> > (system.time()) that anything reliable is faster than as.POSIXlt() for
> > fewer than millions of dates (and would you trust it on 29 February even
> > if it was faster?). Using base classes and functions is in general much
> > more robust than rolling your own, simply because many more people use
> > them - they have much more data run through them, and the time you might
> > save trying to avoid integer to character conversion will/should be eaten
> > up by debugging.
> >
> > >
> > > Daehyok Shin (Peter)
> > > Terrestrial Hydrological Ecosystem Modellers
> > > Geography Department
> > > University of North Carolina-Chapel Hill
> > > sdhyok at email.unc.edu
> > >
> > > "We can do no great things,
> > > only small things with great love."
> > >                          - Mother Teresa
> > >
> > > > -----Original Message-----
> > > > From: Roger Bivand [mailto:Roger.Bivand at nhh.no]
> > > > Sent: Monday, June 07, 2004 PM 12:20
> > > > To: Daehyok Shin
> > > > Cc: R, Help
> > > > Subject: Re: [R] Vectors of years, months, and days to dates?
> > > >
> > > >
> > > > On Mon, 7 Jun 2004, Daehyok Shin wrote:
> > > >
> > > > > How can I create POSIXlt directly from the numbers?
> > > > > I failed to find the solution from help documents.
> > > >
> > > > ?POSIXlt
> > > >
> > > > ?as.POSIXlt:
> > > >
> > > > > res <- as.POSIXlt(paste(years, months, days, sep="-"))
> > > > > str(res)
> > > > `POSIXlt', format: chr [1:2] "1991-01-01" "1992-10-02"
> > > > > res$year
> > > > [1] 91 92
> > > >
> > > > >
> > > > > Daehyok
> > > > >
> > > > > --On Monday, June 07, 2004 4:44 PM +0100 Prof Brian Ripley
> > > > > <ripley at stats.ox.ac.uk> wrote:
> > > > >
> > > > > > On Mon, 7 Jun 2004, Shin, Daehyok wrote:
> > > > > >
> > > > > >> The interface for dates in R is a little confusing to me.
> > > > > >> I want to create a vector of Date objects from vectors of
> > > > years, months,
> > > > > >> and days.
> > > > > >> One solution I found is:
> > > > > >>
> > > > > >> years <- c(1991, 1992)
> > > > > >> months <- c(1, 10)
> > > > > >> days <- c(1, 2)
> > > > > >>
> > > > > >> dates <- as.Date(ISOdate(years, months, days))
> > > > > >>
> > > > > >> But, in this solution the ISOdate function converts the
> > vectors into
> > > > > >> characters,
> > > > > >> which can cause serious performance and memory loss
> > > > > >> when the vectors of years, months, and days are huge.
> > > > > >
> > > > > > Really?  You have measured the loss?  A million causes no
> > problem for
> > > > > > example, and what are you going to do with a million dates that is
> > > > > > instantaneous and worthwhile?  And a million dates are hardly
> > > > going to be
> > > > > > unique so you only need to convert the unique values.
> > > > > >
> > > > > >> I am quite sure there is much better solution for it. What is it?
> > > > > >
> > > > > > Write your own C code, or make a POSIXlt object directly from
> > > > the numbers
> > > > > > and convert that.
> > > > > >
> > > > > > --
> > > > > > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > > > > > Professor of Applied Statistics,
> > http://www.stats.ox.ac.uk/~ripley/
> > > > > > University of Oxford,             Tel:  +44 1865 272861 (self)
> > > > > > 1 South Parks Road,                     +44 1865 272866 (PA)
> > > > > > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> > > > > >
> > > > >
> > > > > ______________________________________________
> > > > > R-help at stat.math.ethz.ch mailing list
> > > > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > > > > PLEASE do read the posting guide!
> > > > http://www.R-project.org/posting-guide.html
> > > > >
> > > >
> > > > --
> > > > Roger Bivand
> > > > Economic Geography Section, Department of Economics,
> > Norwegian School of
> > > > Economics and Business Administration, Breiviksveien 40,
> > N-5045 Bergen,
> > > > Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
> > > > e-mail: Roger.Bivand at nhh.no
> > > >
> > > >
> > > >
> > >
> >
> > --
> > Roger Bivand
> > Economic Geography Section, Department of Economics, Norwegian School of
> > Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
> > Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
> > e-mail: Roger.Bivand at nhh.no
> >
> >
> >
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From bates at stat.wisc.edu  Mon Jun  7 21:28:39 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 07 Jun 2004 14:28:39 -0500
Subject: [R] GLMM(..., family=binomial(link="cloglog"))?
In-Reply-To: <40C4B87B.7060301@pdf.com>
References: <3A822319EB35174CA3714066D590DCD504AF7DFB@usrymx25.merck.com>
	<40C4B87B.7060301@pdf.com>
Message-ID: <6r8yez88wo.fsf@bates4.stat.wisc.edu>

Spencer Graves <spencer.graves at pdf.com> writes:

>       Thanks, Andy, Doug, Deepayan.  I now have lme4 0.6-1 2004/05/31
> installed for R 1.9.1 alpha under Windows 2000.  When I tried the
> example below, GLMM ran, but the print method reported an error:
> Generalized Linear Mixed Model

As I write this I am uploading lme4_0.6-2 to the incoming area at
CRAN.  This version fixes that bug (I neglected to add drop=FALSE in a
subsetting operation in the show method for the summary.ssclme class.)

You can see the details of the fix at
  http://bates4.stat.wisc.edu:2180/cgi-bin/viewcvs.cgi/trunk/lme4/R/ssclme.R
or

/home/bates/src/Rlibs/lme4 $ svn diff -r 209 R/ssclme.R
Index: R/ssclme.R
===================================================================
--- R/ssclme.R	(revision 209)
+++ R/ssclme.R	(working copy)
@@ -205,7 +205,7 @@
                       cm = cbind(cm, stat, pval)
                       colnames(cm) = c(nms, "t value", "Pr(>|t|)")
                   } else {
-                      cm = cm[, 1:2]
+                      cm = cm[, 1:2, drop = FALSE]
                       stat = cm[,1]/cm[,2]
                       pval = 2*pnorm(abs(stat), lower = FALSE)
                       nms = colnames(cm)



From spencer.graves at pdf.com  Mon Jun  7 21:44:27 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 07 Jun 2004 12:44:27 -0700
Subject: [R] GLMM(..., family=binomial(link="cloglog"))?
In-Reply-To: <6r8yez88wo.fsf@bates4.stat.wisc.edu>
References: <3A822319EB35174CA3714066D590DCD504AF7DFB@usrymx25.merck.com>	<40C4B87B.7060301@pdf.com>
	<6r8yez88wo.fsf@bates4.stat.wisc.edu>
Message-ID: <40C4C59B.6070801@pdf.com>

Hi, Doug: 

      Thanks.  I ran 'tst <- getMethod("show", "summary.ssclme")', then 
edited tst as you indicated and ran 'setMethod("show", "summary.ssclme", 
tst)', and it fixed the problem. 

      Best Wishes,
      spencer graves 

Douglas Bates wrote:

>Spencer Graves <spencer.graves at pdf.com> writes:
>
>  
>
>>      Thanks, Andy, Doug, Deepayan.  I now have lme4 0.6-1 2004/05/31
>>installed for R 1.9.1 alpha under Windows 2000.  When I tried the
>>example below, GLMM ran, but the print method reported an error:
>>Generalized Linear Mixed Model
>>    
>>
>
>As I write this I am uploading lme4_0.6-2 to the incoming area at
>CRAN.  This version fixes that bug (I neglected to add drop=FALSE in a
>subsetting operation in the show method for the summary.ssclme class.)
>
>You can see the details of the fix at
>  http://bates4.stat.wisc.edu:2180/cgi-bin/viewcvs.cgi/trunk/lme4/R/ssclme.R
>or
>
>/home/bates/src/Rlibs/lme4 $ svn diff -r 209 R/ssclme.R
>Index: R/ssclme.R
>===================================================================
>--- R/ssclme.R	(revision 209)
>+++ R/ssclme.R	(working copy)
>@@ -205,7 +205,7 @@
>                       cm = cbind(cm, stat, pval)
>                       colnames(cm) = c(nms, "t value", "Pr(>|t|)")
>                   } else {
>-                      cm = cm[, 1:2]
>+                      cm = cm[, 1:2, drop = FALSE]
>                       stat = cm[,1]/cm[,2]
>                       pval = 2*pnorm(abs(stat), lower = FALSE)
>                       nms = colnames(cm)
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From gavin.simpson at ucl.ac.uk  Mon Jun  7 22:08:14 2004
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Mon, 07 Jun 2004 21:08:14 +0100
Subject: [R] error during make of R-patched on Fedora core 2
Message-ID: <40C4CB2E.1080200@ucl.ac.uk>

Dear list,

I've just upgraded to Fedora Core 2 and seeing as there wasn't an rpm 
for this OS on CRAN yet I thought it was about time I had a go at 
compiling R myself. Having run into the X11 problem I switched to trying 
to install R-patched. I followed the instructions in the R Installation 
& Admin manual to download the sources of the Recommended packages and 
place the files in R_HOME/src/library/Recommended. ./configure worked 
fine so I progressed to make, which has hit upon this error when the 
process arrived at the Recommended packages:

make[2]: Leaving directory `/home/gavin/tmp/R-patched/src/library'
make[2]: Entering directory `/home/gavin/tmp/R-patched/src/library'
building/updating vignettes for package 'grid' ...
make[2]: Leaving directory `/home/gavin/tmp/R-patched/src/library'
make[2]: Entering directory `/home/gavin/tmp/R-patched/src/library'
make[2]: Leaving directory `/home/gavin/tmp/R-patched/src/library'
make[1]: Leaving directory `/home/gavin/tmp/R-patched/src/library'
make[1]: Entering directory 
`/home/gavin/tmp/R-patched/src/library/Recommended'
make[2]: Entering directory 
`/home/gavin/tmp/R-patched/src/library/Recommended'
make[2]: Leaving directory 
`/home/gavin/tmp/R-patched/src/library/Recommended'
make[2]: Entering directory 
`/home/gavin/tmp/R-patched/src/library/Recommended'
make[2]: *** No rule to make target `survival.ts', needed by 
`stamp-recommended'.  Stop.
make[2]: Leaving directory 
`/home/gavin/tmp/R-patched/src/library/Recommended'
make[1]: *** [recommended-packages] Error 2
make[1]: Leaving directory 
`/home/gavin/tmp/R-patched/src/library/Recommended'
make: *** [stamp-recommended] Error 2

Being a relative newbie to Linux I have no-idea how to continue to solve 
this issue :-(

The only difference I can see between the /src/library/Recommended 
directories of R-1.9.0 and R-patched is that in R-1.9.0 it contains 
links to each of the tar.gz (excluding the version info) as well as the 
tar.gz themselves for each of the packages. Is this in some way related 
to my problem?

If anyone can help me solve this issue I'd be most grateful.

Thanks in advance,

Gavin
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpson at ucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From Roger.Bivand at nhh.no  Mon Jun  7 22:17:20 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Mon, 7 Jun 2004 22:17:20 +0200 (CEST)
Subject: [R] error during make of R-patched on Fedora core 2
In-Reply-To: <40C4CB2E.1080200@ucl.ac.uk>
Message-ID: <Pine.LNX.4.44.0406072214580.459-100000@reclus.nhh.no>

This feels like:

"After downloading the R sources you should also download the recommended 
packages by entering the R source tree and running

    tools/rsync-recommended

from a shell command line."

from the R Sources page on CRAN - could you try that (I guess rsync is 
installed on most Linux standard distributions)?

On Mon, 7 Jun 2004, Gavin Simpson wrote:

> Dear list,
> 
> I've just upgraded to Fedora Core 2 and seeing as there wasn't an rpm 
> for this OS on CRAN yet I thought it was about time I had a go at 
> compiling R myself. Having run into the X11 problem I switched to trying 
> to install R-patched. I followed the instructions in the R Installation 
> & Admin manual to download the sources of the Recommended packages and 
> place the files in R_HOME/src/library/Recommended. ./configure worked 
> fine so I progressed to make, which has hit upon this error when the 
> process arrived at the Recommended packages:
> 
> make[2]: Leaving directory `/home/gavin/tmp/R-patched/src/library'
> make[2]: Entering directory `/home/gavin/tmp/R-patched/src/library'
> building/updating vignettes for package 'grid' ...
> make[2]: Leaving directory `/home/gavin/tmp/R-patched/src/library'
> make[2]: Entering directory `/home/gavin/tmp/R-patched/src/library'
> make[2]: Leaving directory `/home/gavin/tmp/R-patched/src/library'
> make[1]: Leaving directory `/home/gavin/tmp/R-patched/src/library'
> make[1]: Entering directory 
> `/home/gavin/tmp/R-patched/src/library/Recommended'
> make[2]: Entering directory 
> `/home/gavin/tmp/R-patched/src/library/Recommended'
> make[2]: Leaving directory 
> `/home/gavin/tmp/R-patched/src/library/Recommended'
> make[2]: Entering directory 
> `/home/gavin/tmp/R-patched/src/library/Recommended'
> make[2]: *** No rule to make target `survival.ts', needed by 
> `stamp-recommended'.  Stop.
> make[2]: Leaving directory 
> `/home/gavin/tmp/R-patched/src/library/Recommended'
> make[1]: *** [recommended-packages] Error 2
> make[1]: Leaving directory 
> `/home/gavin/tmp/R-patched/src/library/Recommended'
> make: *** [stamp-recommended] Error 2
> 
> Being a relative newbie to Linux I have no-idea how to continue to solve 
> this issue :-(
> 
> The only difference I can see between the /src/library/Recommended 
> directories of R-1.9.0 and R-patched is that in R-1.9.0 it contains 
> links to each of the tar.gz (excluding the version info) as well as the 
> tar.gz themselves for each of the packages. Is this in some way related 
> to my problem?
> 
> If anyone can help me solve this issue I'd be most grateful.
> 
> Thanks in advance,
> 
> Gavin
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From bxc at steno.dk  Mon Jun  7 22:30:01 2004
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Mon, 7 Jun 2004 22:30:01 +0200
Subject: [R] latex function in Hmisc
Message-ID: <0ABD88905D18E347874E0FB71C0B29E901D8E282@exdkba022.novo.dk>

Is there a way to prevent latex.default() from starting LaTeX and JUST
create the file requested?
I generate a number of LaTeX tables for inclusion in a document running
R in batch, and I don't want a lot of calls to LaTeX. 
I cannot find any arguments for that task in the documentation.
It seems a little clumsy to have to direct the output from a function
with a "file" argument using sink() and 'file=""'.

Bendix Carstensen
----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc



From MSchwartz at MedAnalytics.com  Mon Jun  7 22:34:46 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon, 07 Jun 2004 15:34:46 -0500
Subject: [R] error during make of R-patched on Fedora core 2
In-Reply-To: <40C4CB2E.1080200@ucl.ac.uk>
References: <40C4CB2E.1080200@ucl.ac.uk>
Message-ID: <1086640486.2235.241.camel@localhost.localdomain>

On Mon, 2004-06-07 at 15:08, Gavin Simpson wrote:
> Dear list,
> 
> I've just upgraded to Fedora Core 2 and seeing as there wasn't an rpm 
> for this OS on CRAN yet I thought it was about time I had a go at 
> compiling R myself. Having run into the X11 problem I switched to trying 
> to install R-patched. I followed the instructions in the R Installation 
> & Admin manual to download the sources of the Recommended packages and 
> place the files in R_HOME/src/library/Recommended. ./configure worked 
> fine so I progressed to make, which has hit upon this error when the 
> process arrived at the Recommended packages:
> 
> make[2]: Leaving directory `/home/gavin/tmp/R-patched/src/library'
> make[2]: Entering directory `/home/gavin/tmp/R-patched/src/library'
> building/updating vignettes for package 'grid' ...
> make[2]: Leaving directory `/home/gavin/tmp/R-patched/src/library'
> make[2]: Entering directory `/home/gavin/tmp/R-patched/src/library'
> make[2]: Leaving directory `/home/gavin/tmp/R-patched/src/library'
> make[1]: Leaving directory `/home/gavin/tmp/R-patched/src/library'
> make[1]: Entering directory 
> `/home/gavin/tmp/R-patched/src/library/Recommended'
> make[2]: Entering directory 
> `/home/gavin/tmp/R-patched/src/library/Recommended'
> make[2]: Leaving directory 
> `/home/gavin/tmp/R-patched/src/library/Recommended'
> make[2]: Entering directory 
> `/home/gavin/tmp/R-patched/src/library/Recommended'
> make[2]: *** No rule to make target `survival.ts', needed by 
> `stamp-recommended'.  Stop.
> make[2]: Leaving directory 
> `/home/gavin/tmp/R-patched/src/library/Recommended'
> make[1]: *** [recommended-packages] Error 2
> make[1]: Leaving directory 
> `/home/gavin/tmp/R-patched/src/library/Recommended'
> make: *** [stamp-recommended] Error 2
> 
> Being a relative newbie to Linux I have no-idea how to continue to solve 
> this issue :-(
> 
> The only difference I can see between the /src/library/Recommended 
> directories of R-1.9.0 and R-patched is that in R-1.9.0 it contains 
> links to each of the tar.gz (excluding the version info) as well as the 
> tar.gz themselves for each of the packages. Is this in some way related 
> to my problem?
> 
> If anyone can help me solve this issue I'd be most grateful.
> 
> Thanks in advance,
> 
> Gavin


You might want to try the following commands using rsync as an
alternative to downloading the tarball:

rsync -rCv rsync.r-project.org::r-patched R-patched
./R-patched/tools/rsync-recommended
cd R-patched
./configure
make

I actually have the above in a script file that I can just run quickly,
when I want to update the code.

I am now running FC2, so if you have any problems, drop me a line.

Best regards,

Marc Schwartz



From gavin.simpson at ucl.ac.uk  Mon Jun  7 22:51:45 2004
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Mon, 07 Jun 2004 21:51:45 +0100
Subject: [R] error during make of R-patched on Fedora core 2
In-Reply-To: <Pine.LNX.4.44.0406072214580.459-100000@reclus.nhh.no>
References: <Pine.LNX.4.44.0406072214580.459-100000@reclus.nhh.no>
Message-ID: <40C4D561.6030300@ucl.ac.uk>

Roger Bivand wrote:
> This feels like:
> 
> "After downloading the R sources you should also download the recommended 
> packages by entering the R source tree and running
> 
>     tools/rsync-recommended
> 
> from a shell command line."
> 
> from the R Sources page on CRAN - could you try that (I guess rsync is 
> installed on most Linux standard distributions)?
> 
> On Mon, 7 Jun 2004, Gavin Simpson wrote:
> 

<snip>

Thanks Roger and Marc, for suggesting I use ./tools/rsync-recommended 
from within the R-patched directory.

This seems to have done the trick as make completed without errors this 
time round. The Recommended directory also contained the links to the 
actual tar.gz files after doing the rsync command, so I guess this was 
the problem (or at least related to it.) I'm off home now with the 
laptop to see if I can finish make check-all and make install R.

I have re-read the section describing the installation process for 
R-patched or R-devel in the R Installation and Administration manual 
(from R.1.9.0) just in case I missed something. Section 1.2 of this 
manual indicates that one can proceed *either* by downloading R-patched 
and then the Recommended packages from CRAN and placing the tar.gz files 
in R_HOME/src/library/Recommended, or by using rsync to download 
R-patched, and then to get the Recommended packages. The two are quite 
separately documented in the manual, and do seem to be in disagreement 
with the R-sources page on the CRAN website, which doesn't mention the 
manual download method (for Recommended) at all.

Is there something wrong with the current Recommended files on CRAN, or 
is the section in the R Installation & Admin manual out-of-date or in 
error, or am I missing something vital here? This isn't a complaint: I'm 
just pointing this out in case this is something that needs updating in 
the documentation.

All the best,

Gavin
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpson at ucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From bates at stat.wisc.edu  Mon Jun  7 22:59:24 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 07 Jun 2004 15:59:24 -0500
Subject: [R] error during make of R-patched on Fedora core 2
In-Reply-To: <40C4CB2E.1080200@ucl.ac.uk>
References: <40C4CB2E.1080200@ucl.ac.uk>
Message-ID: <6rwu2j6q4z.fsf@bates4.stat.wisc.edu>

Gavin Simpson <gavin.simpson at ucl.ac.uk> writes:

> Dear list,
> 
> I've just upgraded to Fedora Core 2 and seeing as there wasn't an rpm
> for this OS on CRAN yet I thought it was about time I had a go at
> compiling R myself. Having run into the X11 problem I switched to
> trying to install R-patched. I followed the instructions in the R
> Installation & Admin manual to download the sources of the Recommended
> packages and place the files in
> R_HOME/src/library/Recommended. ./configure worked fine so I
> progressed to make, which has hit upon this error when the process
> arrived at the Recommended packages:
> 
> 
> make[2]: Leaving directory `/home/gavin/tmp/R-patched/src/library'
> make[2]: Entering directory `/home/gavin/tmp/R-patched/src/library'
> building/updating vignettes for package 'grid' ...
> make[2]: Leaving directory `/home/gavin/tmp/R-patched/src/library'
> make[2]: Entering directory `/home/gavin/tmp/R-patched/src/library'
> make[2]: Leaving directory `/home/gavin/tmp/R-patched/src/library'
> make[1]: Leaving directory `/home/gavin/tmp/R-patched/src/library'
> make[1]: Entering directory
> `/home/gavin/tmp/R-patched/src/library/Recommended'
> 
> make[2]: Entering directory
> `/home/gavin/tmp/R-patched/src/library/Recommended'
> 
> make[2]: Leaving directory
> `/home/gavin/tmp/R-patched/src/library/Recommended'
> 
> make[2]: Entering directory
> `/home/gavin/tmp/R-patched/src/library/Recommended'
> 
> make[2]: *** No rule to make target `survival.ts', needed by
> `stamp-recommended'.  Stop.
> 
> make[2]: Leaving directory
> `/home/gavin/tmp/R-patched/src/library/Recommended'
> 
> make[1]: *** [recommended-packages] Error 2
> make[1]: Leaving directory
> `/home/gavin/tmp/R-patched/src/library/Recommended'
> 
> make: *** [stamp-recommended] Error 2
> 
> Being a relative newbie to Linux I have no-idea how to continue to
> solve this issue :-(
> 
> 
> The only difference I can see between the /src/library/Recommended
> directories of R-1.9.0 and R-patched is that in R-1.9.0 it contains
> links to each of the tar.gz (excluding the version info) as well as
> the tar.gz themselves for each of the packages. Is this in some way
> related to my problem?
> 
> 
> If anyone can help me solve this issue I'd be most grateful.
> 
> Thanks in advance,

The easiest approach is to run
 ./tools/rsync-recommended 
from the top-level source directory for R.  That uses rsync to
downloads the sources for the recommended packages then creates the
necessary links.  (At least it works for R-devel and I believe it
works for R-patched.  There was a time when the links were not getting
corrected properly.)



From rwang at math.ucalgary.ca  Mon Jun  7 23:47:56 2004
From: rwang at math.ucalgary.ca (Rui)
Date: Mon, 7 Jun 2004 15:47:56 -0600
Subject: [R] Load a dll
Message-ID: <000e01c44cd9$18141500$f63d9f88@math.ucalgary.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040607/19ff09a2/attachment.pl

From MSchwartz at MedAnalytics.com  Tue Jun  8 00:01:21 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Mon, 07 Jun 2004 17:01:21 -0500
Subject: [R] error during make of R-patched on Fedora core 2
In-Reply-To: <40C4D561.6030300@ucl.ac.uk>
References: <Pine.LNX.4.44.0406072214580.459-100000@reclus.nhh.no>
	<40C4D561.6030300@ucl.ac.uk>
Message-ID: <1086645681.2235.274.camel@localhost.localdomain>

On Mon, 2004-06-07 at 15:51, Gavin Simpson wrote:

snip

> Thanks Roger and Marc, for suggesting I use ./tools/rsync-recommended 
> from within the R-patched directory.
> 
> This seems to have done the trick as make completed without errors this 
> time round. The Recommended directory also contained the links to the 
> actual tar.gz files after doing the rsync command, so I guess this was 
> the problem (or at least related to it.) I'm off home now with the 
> laptop to see if I can finish make check-all and make install R.
> 
> I have re-read the section describing the installation process for 
> R-patched or R-devel in the R Installation and Administration manual 
> (from R.1.9.0) just in case I missed something. Section 1.2 of this 
> manual indicates that one can proceed *either* by downloading R-patched 
> and then the Recommended packages from CRAN and placing the tar.gz files 
> in R_HOME/src/library/Recommended, or by using rsync to download 
> R-patched, and then to get the Recommended packages. The two are quite 
> separately documented in the manual, and do seem to be in disagreement 
> with the R-sources page on the CRAN website, which doesn't mention the 
> manual download method (for Recommended) at all.
> 
> Is there something wrong with the current Recommended files on CRAN, or 
> is the section in the R Installation & Admin manual out-of-date or in 
> error, or am I missing something vital here? This isn't a complaint: I'm 
> just pointing this out in case this is something that needs updating in 
> the documentation.
> 
> All the best,
> 
> Gavin

Perhaps I am being dense, but in reviewing the two documents (R Admin
and the CRAN sources page), I think that the only thing lacking is a
description on the CRAN page of the manual download option for the Rec
packages.

You would need to go here now for 1.9.1 Alpha/Beta which is where the
current r-patched is:

http://www.cran.mirrors.pair.com/src/contrib/1.9.1/Recommended/

The standard links on CRAN are for the current 'released' version, which
is still 1.9.0 for the moment.

Procedurally, I think that the rsync approach is substantially easier
(one step instead of multiple downloads) and certainly less error prone.
Also the ./tools/rsync-recommended script is set up to pick up the
proper package versions, which also helps to avoid conflicts.

HTH,

Marc



From vs017185 at mnsi.net  Tue Jun  8 00:10:40 2004
From: vs017185 at mnsi.net (russell)
Date: Mon, 07 Jun 2004 18:10:40 -0400
Subject: [R] msm capabilities
Message-ID: <40C4E7E0.8060709@mnsi.net>

Hello,

I'm wondering if anyone has used the msm package to compute the steady 
state probabilities for a Markov model?

thanks,

Russell



From Han-Lin.Lai at noaa.gov  Tue Jun  8 00:26:28 2004
From: Han-Lin.Lai at noaa.gov (Han-Lin Lai)
Date: Mon, 07 Jun 2004 15:26:28 -0700
Subject: [R] betareg
Message-ID: <13dcee1380ae.1380ae13dcee@mercury.akctr.noaa.gov>

Hi,
I try beta regression model using "betareg".  I have two questions:

(i)  The example data pratergrouped is not exist.
> pratergrouped
 [1] V11 V1  V2  V3  V4  V5  V6  V7  V8  V9  V10
<0 rows> (or 0-length row.names)

(ii) In the description of betareg, it said variable, say y, is 
restricted in (0,1).  Does this retriction be [0,1] (Kieschnick and 
McCullough,  Statistical modeling 2003, 193-213)?

Thanks for helps in advance.

Cheers!
Han



From spencer.graves at pdf.com  Tue Jun  8 01:56:10 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 07 Jun 2004 16:56:10 -0700
Subject: [R] GLMM(..., family=binomial(link="cloglog"))?
In-Reply-To: <6r8yez88wo.fsf@bates4.stat.wisc.edu>
References: <3A822319EB35174CA3714066D590DCD504AF7DFB@usrymx25.merck.com>	<40C4B87B.7060301@pdf.com>
	<6r8yez88wo.fsf@bates4.stat.wisc.edu>
Message-ID: <40C5009A.90705@pdf.com>

	  Another GLMM/glmm problem:  I simulate rbinom(N, 100, pz), where 
logit(pz) = rnorm(N).  I'd like to estimate the mean and standard 
deviation of logit(pz).  I've tried GLMM{lme4}, glmmPQL{MASS}, and 
glmm{Jim Lindsey's repeated}.  In several replicates of this for N = 10, 
100, 500, etc., my glmm call produced estimates of the standard 
deviation of the random effect in the range of 0.6 to 0.8 (never as high 
as the 1 simulated).  Meanwhile, my calls to GLMM produced estimates 
between 1e-12 and 1e-9, while the glmmPQL results tended to be closer to 
0.001, though it gave one number as high as 0.7.  (I'm running R 1.9.1 
alpha, lme4 0.6-1 under Windows 2000)

	  Am I doing something wrong, or do these results suggest bugs in the 
software or deficiencies in the theory or ... ?

	  Consider the following:

 > set.seed(1); N <- 10
 > z <- rnorm(N)
 > pz <- inv.logit(z)
 > DF <- data.frame(z=z, pz=pz, y=rbinom(N, 100, pz)/100, n=100, 
smpl=factor(1:N))
 > GLMM(y~1, family=binomial, data=DF, random=~1|smpl, weights=n)
Generalized Linear Mixed Model

Fixed: y ~ 1
Data: DF
  log-likelihood:  -55.8861
Random effects:
  Groups Name        Variance   Std.Dev.
  smpl   (Intercept) 1.7500e-12 1.3229e-06

Estimated scale (compare to 1)  3.280753

Fixed effects:
             Estimate Std. Error z value Pr(>|z|)
(Intercept) 0.148271   0.063419  2.3379  0.01939

Number of Observations: 10
Number of Groups: 10

 > library(repeated) # Jim Lindsey's repeated package
 > glmm(y~1, family=binomial, data=DF, weights=n, nest=smpl)

  Warning messages:
1: non-integer #successes in a binomial glm! in: eval(expr, envir, 
enclos) ...

Coefficients:
(Intercept)           sd
      0.1971       0.6909

Degrees of Freedom: 9 Total (i.e. Null);  8 Residual
Null Deviance:      111.8
Residual Deviance: 32.03        AIC: 1305
Normal mixing variance: 0.4773187

 > glmmPQL(y~1, family=binomial, data=DF, random=~1|smpl, weights=n)

Linear mixed-effects model fit by maximum likelihood
   Data: DF
   Log-likelihood: -10.78933
   Fixed: y ~ 1
(Intercept)
   0.1622299

Random effects:
  Formula: ~1 | smpl
         (Intercept)  Residual
StdDev:   0.7023349 0.5413203

Variance function:
  Structure: fixed weights
  Formula: ~invwt
Number of Observations: 10
Number of Groups: 10

	  Suggestions?

	  So far, the best tool I've got for this problem is a normal 
probability plot of a transform of the binomial responses with Monte 
Carlo confidence bands, as suggested by Venables and Ripley, S 
Programming and Atkinson (1985).  However, I ultimately need to estimate 
these numbers.

	  Thanks,
	  spencer graves



From andy_liaw at merck.com  Tue Jun  8 02:40:05 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 7 Jun 2004 20:40:05 -0400
Subject: [R] (low level) profiling of code
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7E49@usrymx25.merck.com>

Someone get out the whip...

Apologies for wasting everyone's time.  I missed setting MAIN_LDFLAGS="-pg"
in consig.site. 

Best,
Andy

> From: Prof Brian Ripley 
> 
> You normally need to switch R profiling off, as that uses the same
> interrupts as low-level profiling.
> 
> I would expect you to get gmon.out output from the the main R 
> executable 
> on any run, so the first question must be `does your OS support -pg?'
> 
> I don't think dynamically loaded code is relevant (provided 
> it is compiled
> with -pg).  I am pretty sure I managed to profile some 
> examples from the
> cluster package on RH8.0 Linux the other day.
> 
> On Mon, 7 Jun 2004, Liaw, Andy wrote:
> 
> > Dear R-help,
> > 
> > Can some one tell me how to profile compiled code 
> dynamically loaded into R?
> > Here's what I tried on our dual Opteron running SUSE Linux 
> Enterprise Server
> > 8 (GCC 3.3):
> > 
> > Start with R-patched dated 2004-06-07: 
> > 
> > (I also had MAIN_CFLAGS="-pg" in config.site.)
> > 
> > R is now configured for x86_64-unknown-linux-gnu
> > 
> >   Source directory:          .
> >   Installation directory:    /usr/local
> > 
> >   C compiler:                gcc  -O2 -g -pg -march=k8 -msse2 -m64
> >   C++ compiler:              g++  -O2 -g -pg -march=k8 -msse2 -m64
> >   Fortran compiler:          g77  -O2 -g -pg -march=k8 -msse2 -m64
> > 
> >   Interfaces supported:      X11, tcltk
> >   External libraries:        readline
> >   Additional capabilities:   PNG, JPEG
> >   Options enabled:           R profiling
> > 
> >   Recommended packages:      no
> > 
> > I then tried running 
> > 
> >   /path/to/R-patched/bin/R CMD BATCH -q -slave myscript.R
> > 
> > where inside myscript.R is call to R functions that calls 
> .C().  However,
> > this does not produce the gmon.out file.  Can anyone tell 
> me what I'm
> > missing?  Any help much appreciated!
> 
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> 
>



From lauraholt_983 at hotmail.com  Tue Jun  8 03:02:58 2004
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Mon, 07 Jun 2004 20:02:58 -0500
Subject: [R] AR models
Message-ID: <BAY12-F92gyojEoiJHD0004ec18@hotmail.com>

Dear R People:

Is it possible to fit an AR model such as:

y_t = fee_1 y_t-1 + fee_2 y_t-9 + a_t,
please?

I know that we can fit an AR(9) model, but I was wondering if we could do a
partial as described.

Thank you for your help.

Sincerely,
Laura Holt
mailto: lauraholt_983 at hotmail.com

_________________________________________________________________




From ok at cs.otago.ac.nz  Tue Jun  8 03:58:37 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Tue, 8 Jun 2004 13:58:37 +1200 (NZST)
Subject: [R] How to Describe R to Finance People
Message-ID: <200406080158.i581wb6d303940@atlas.otago.ac.nz>

Tamas Papp <tpapp at axelero.hu> wrote:
	I would emphasize the following:
...	
	2. The programming language is really friendly and convenient to work
	with.  In finance, you often need to hack together special solutions
	for problems that are not conventional (especially in term structure
	models, but I think that the same applies to bi- and trinomial models
	and their ilk).  As an R newbie, it took me an afternoon to implement
	a basic toolkit for the former, which I could use for interesting
	explorations.

There are three perspectives on programming languages like the S/R family:
(1) The programming language perspective.
    I am sorry to tell you that the only excuse for R is S.
    R is *weird*.  It combines error-prone C-like syntax with data structures
    that are APL-like but not sufficiently* APL-like to have behaviour that
    is easy to reason about.  The scope rules (certainly the scope rules for
    S) were obviously designed by someone who had a fanatical hatred of
    compilers and wanted to ensure that the language could never be usefully
    compiled.  Thanks to 'with' the R scope rules are little better.  The
    fact that (object)$name returns NULL instead of reporting an error when
    the object doesn't _have_ a $name property means that errors can be
    delayed to the point where debugging is harder than it needs to be.
    The existence of two largely unrelated object-oriented extensions
    further complicates the language.

    APL, Xerox's Interactive Data-analysis Language (an extension of
    Interlisp), and Lisp-Stat are *far* cleaner as programming languages.

(2) The Excel perspective.
    Many people are using Excel as their statistics package.
    (One of my students persists in using it for making his graphs.
    I've talked another lecturer and his student into using R although
    it's too late for a paper they've published.  Dark backgrounds for
    printed graphs don't work well.  What persuaded them was a number
    of relevant things available out-of-the-box in R.)
    In Excel, there are at least three "languages":
    - direct manipulation
    - the formula language
    - Visual Basic for Applications

    What R offers is a *single* language that's used uniformly,
    with a rich data model.

(3) The statistics package perspective.
    I've used Minitab, GLIM, SPSS, Matlab, and Octave (and BASIS, if
    anyone remembers that).  I've owned and read manuals for GENSTAT.
    Recently I had occasion to look at more SAS code than I ever wish to.
    (No, I didn't understand it.)  Several of these packages share a
    common design feature:  there is a language of built-in commands,
    perhaps a data transformation language, and a separate macro language.
    The macro language is typically very clumsy.

    What R offers is a *single* language that's used uniformly,
    with a rich data model.  Analysis components you get off CRAN are
    accessed using the *same* syntax as 'built-in' analyses.

    R also offers something important for people who _do_ have the money
    to buy commercial packages, and who might be reluctant to trust open
    source:  a very high degree of compatibility with an established
    commercial product.  R could almost be seen as an indefinite lifetime
    'evaluation' version of S-Plus.

The fact that I can use 'contributed' code, or even my own code, exactly
the same way as 'built-in' stuff, even including access to on-line
documentation and examples, actually *improves* the learning curve for R
compared with many packages.  At least it did for me.

In the words of an old Listerine commercial:  'I hate it.  Twice a day.'



From hjjin at pearl.cs.pusan.ac.kr  Tue Jun  8 04:23:42 2004
From: hjjin at pearl.cs.pusan.ac.kr (=?ks_c_5601-1987?B?wfjI8cGk?=)
Date: Tue, 8 Jun 2004 11:23:42 +0900
Subject: [R] [Q] raw -> gpr in aroma package
Message-ID: <200406080222.i582MJt190037@jade.cs.pusan.ac.kr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040608/2c7b49cf/attachment.pl

From gblevins at mn.rr.com  Tue Jun  8 04:28:16 2004
From: gblevins at mn.rr.com (Greg Blevins)
Date: Mon, 7 Jun 2004 21:28:16 -0500
Subject: [R] Recoding a multiple response question into a series of 1,
	0 variables 
Message-ID: <0a6601c44d00$434942f0$aaca5e18@glblpyirxqz5lp>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040607/fef0a2e4/attachment.pl

From baron at psych.upenn.edu  Tue Jun  8 04:45:28 2004
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Mon, 7 Jun 2004 22:45:28 -0400
Subject: [R] Recoding a multiple response question into a series of 1,
	0 variables
In-Reply-To: <0a6601c44d00$434942f0$aaca5e18@glblpyirxqz5lp>
References: <0a6601c44d00$434942f0$aaca5e18@glblpyirxqz5lp>
Message-ID: <20040608024528.GA2333@psych>

On 06/07/04 21:28, Greg Blevins wrote:
>Hello R folks.
>
>1) The question that generated the data, which I call Qx:
>Which of the following 5 items have you performed in the past month?  (multipe
>response)
>
>2) How the data is coded in my current dataframe:
>The first item that a person selected is coded under a field called Qxfirst; the
>second selected under Qxsecond, etc.  For the first Person, the NAs mean that that
>person only selected two of the five items.
>
>Hypothetical data is shown
>
>                    Qxfirst    Qxsecond    Qxthird    Qxfourth    Qxfifth
>Person1        4            2                NA            NA            NA
>Person2        1            3                4               5               NA
>Person3        3            2                NA            NA            NA
>
>3) How I want the data to be be coded:
>
>I want each field to be one of the five items and I want each field to contain a 1 or
>0 code--1 if they mentioned the item, 0 otherwise.
>
>Given the above data, the new fields would look as follows:
>
>                    Item1    Item2        Item3            Item4        Item5
>Person1        0            1               0                1               0
>Person2        1            0               1                1               1
>Person3        0            1               1                0               0

Here is an idea:
X <- c(4,5,NA,NA,NA) # one row
Y <- rep(NA,5) # an empty row
Y[X] <- 1

Y is now
NA NA NA 1 1
which is what you want.

So you need to do this on each row and then convert the NAs to
0s.  So first create an empty data frame, the same size as your
original one X, like my Y.  Callit Y.  Then a loop?  (I can't
think of a better way just now, like with mapply.)

for (i in [whatever]) Y[i][X[i]] <- 1

(Not tested.)  Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron
R search page:        http://finzi.psych.upenn.edu/



From baron at psych.upenn.edu  Tue Jun  8 04:47:26 2004
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Mon, 7 Jun 2004 22:47:26 -0400
Subject: [R] Recoding a multiple response question into a series of 1,
	0 variables
In-Reply-To: <20040608024528.GA2333@psych>
References: <0a6601c44d00$434942f0$aaca5e18@glblpyirxqz5lp>
	<20040608024528.GA2333@psych>
Message-ID: <20040608024726.GA4769@psych>

On 06/07/04 22:45, Jonathan Baron should have written:
>for (i in [whatever]) Y[i,][X[i,]] <- 1



From ok at cs.otago.ac.nz  Tue Jun  8 04:57:23 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Tue, 8 Jun 2004 14:57:23 +1200 (NZST)
Subject: [R] Vectors of years, months, and days to dates?
Message-ID: <200406080257.i582vNoh293238@atlas.otago.ac.nz>

Rolf Turner <rolf at math.unb.ca> attacked:
		Have you ***any*** evidence that R's procedure degrades
		performance, under any circumstances?  (Apparently not.) In
		that case why are you going on and on about it?
	
I did the following:

    library(chron)
    ymd.to.POSIXlt <-
        function (y, m, d) as.POSIXlt(chron(julian(y=y, x=m, d=d)))
    n <- 100000
    y <- sample(1970:2004, n, replace=TRUE)
    m <- sample(1:12,      n, replace=TRUE)
    d <- sample(1:28,      n, replace=TRUE)
    system.time(ymd.to.POSIXlt(y, m, d))
    [1]  8.78  0.10 31.76  0.00  0.00
    system.time(as.POSIXlt(paste(y,m,d, sep="-")))
    [1] 14.64  0.13 53.30  0.00  0.00

This was on a 500MHz SunBlade100, a slow machine by today's standards,
using R 1.9.0.  There is all the evidence one could reasonably ask for
that going through paste() instead of providing y, m, d as numeric
vectors *does* degrade performance under at least some circumstances.

The cost is less than a factor of 2, but not negligible either.

		The standard response to this kind of request is ``R is a
		cooperative endeavour.  Feel free to contribute.''
	
Well, ymd.to.POSIXlt is my contribution.

	> Is there no one supporting my idea?
	
		Apparently not.
	
Wrong:  the idea *is* already supported (all except for one trivial
line of code) by the chron package.



From p.murrell at auckland.ac.nz  Tue Jun  8 05:05:05 2004
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Tue, 08 Jun 2004 15:05:05 +1200
Subject: [R] More than one series in a coplot
References: <40C1464B.6000400@massey.ac.nz>
Message-ID: <40C52CE1.205@stat.auckland.ac.nz>

Hi


Matthew Walker wrote:
> Hi!
> 
> I would like to know, how do you plot more than one data series when 
> using "coplot"?
> 
> I think I know the answer if "plot" is being used:
> x <- 1:10
> y <- 1:10
> y2 <- 1:10* 1.1
> plot ( y ~ x, type="n" )
> points( y ~ x, type = "b", col = "red" )  # plot the points and lines 
> for the first series
> points( y2 ~ x, type = "b", col = "blue" )  # plot the points and lines 
> for the second series
> 
> But how should it be done with "coplot"?
> 
> If I make my question more concrete:
> df <- data.frame( x=x, y=y, y2=y2, v=c("a","b"))  # using the vectors above
> coplot( df$y~df$x | df$v, type="b", col="red")  # gives me a coplot of y 
> ~ x
> 
> How do I now put points and lines for y2 on the same graphs?


Use the panel argument, as below ...

x <- 1:10
y <- 1:10
y2 <- 1:10* 1.1
df <- data.frame( x=x, y=y, y2=y2, v=c("a","b"))  # using the vectors above
coplot( df$y~df$x | df$v, type="b", col="red",
        subscripts=TRUE,
        panel=function(x, y, subscripts, ...) {
          points(x, y, ...)
          points(x, y2[subscripts], ...)})

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From mmr at tci.ufal.br  Tue Jun  8 05:38:19 2004
From: mmr at tci.ufal.br (=?iso-8859-1?b?TeFyY2lv?= de Medeiros Ribeiro)
Date: Tue,  8 Jun 2004 00:38:19 -0300
Subject: [R] Differential Equations
Message-ID: <1086665899.40c534abee53d@webmail.tci.ufal.br>

Hello!

I would like to know if R can solve Differential Equations...
I don't think so because, in my point, I see R like a Statistical System, not a 
Math System. Am I wrong?

Thank you very much.

M??rcio de Medeiros Ribeiro
Graduando em Ci??ncia da Computa????o
Departamento de Tecnologia da Informa????o - TCI
Universidade Federal de Alagoas - UFAL
Macei?? - Alagoas - Brasil
Projeto CoCADa



From sdhyok at email.unc.edu  Tue Jun  8 06:52:26 2004
From: sdhyok at email.unc.edu (Shin, Daehyok)
Date: Tue, 8 Jun 2004 00:52:26 -0400
Subject: [R] Vectors of years, months, and days to dates?
In-Reply-To: <200406080249.i582nZpf129946@atlas.otago.ac.nz>
Message-ID: <OAEOKPIGCLDDHAEMCAKIGEBMCOAA.sdhyok@email.unc.edu>

Thanks, Richard.
This is what I was looking for.
As you said, the chron package seems so useful that I will use it a lot.
I don't think the performance gain (53:31) is not what we can neglect.
Still, I am wondering why this kind of intuitive interface is not supported
through the base package.

Daehyok Shin (Peter)

> -----Original Message-----
> From: Richard A. O'Keefe [mailto:ok at cs.otago.ac.nz]
> Sent: Monday, June 07, 2004 PM 10:50
> To: sdhyok at email.unc.edu
> Subject: RE: [R] Vectors of years, months, and days to dates?
>
>
> 	as.Date(c(years, months, days))
>
> That would put all the years, followed by all the months,
> followed by all the days, in one vector.
>
> Try the chron package, where you can use
>     julian(y=years, x=months, d=days)
> 	# vectors => vector of day numbers (I really mean 'x', not 'm')
>     chron(julian(...))
> 	# vector of day numbers => vector of chron objects
>     as.POSIXlt(chron(...))
> 	# vector of chron objects => vector of POSIXlt objects
>
> So:
>     ymd.to.POSIXlt <-
>         function (y, m, d) as.POSIXlt(chron(julian(y=y, x=m, d=d)))
>
> Now,
> > n <- 100000
> > y <- sample(1970:2004, n, replace=TRUE)
> > m <- sample(1:12,      n, replace=TRUE)
> > d <- sample(1:28,      n, replace=TRUE)
> > system.time(ymd.to.POSIXlt(y, m, d))
> [1]  8.78  0.10 31.76  0.00  0.00    # user cpu, system cpu, elapsed
> > system.time(as.POSIXlt(paste(y,m,d, sep="-")))
> [1] 14.64  0.13 53.30  0.00  0.00
>
> This was on a 500MHz SunBlade100, a slow machine by today's standards.
> There *is* a time advantage for the approach you prefer, but it's less
> than a factor of 2.
>
> 	Is there no one supporting my idea?
>
> It _is_ supported, via the chron() package.  Just add ymd.to.POSIXlt
> to your .Rprofile
>
> It may even be that chron objects do everything you need.
>



From p.connolly at hortresearch.co.nz  Tue Jun  8 06:55:24 2004
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Tue, 8 Jun 2004 16:55:24 +1200
Subject: [R] Vectors of years, months, and days to dates?
In-Reply-To: <200406080257.i582vNoh293238@atlas.otago.ac.nz>; from
	ok@cs.otago.ac.nz on Tue, Jun 08, 2004 at 02:57:23PM +1200
References: <200406080257.i582vNoh293238@atlas.otago.ac.nz>
Message-ID: <20040608165524.S2137@hortresearch.co.nz>

On Tue, 08-Jun-2004 at 02:57PM +1200, Richard A. O'Keefe wrote:

|> Rolf Turner <rolf at math.unb.ca> attacked:
|> 		Have you ***any*** evidence that R's procedure degrades
|> 		performance, under any circumstances?  (Apparently not.) In
|> 		that case why are you going on and on about it?
|> 	
|> I did the following:
|> 
|>     library(chron)
|>     ymd.to.POSIXlt <-
|>         function (y, m, d) as.POSIXlt(chron(julian(y=y, x=m, d=d)))
|>     n <- 100000
|>     y <- sample(1970:2004, n, replace=TRUE)
|>     m <- sample(1:12,      n, replace=TRUE)
|>     d <- sample(1:28,      n, replace=TRUE)
|>     system.time(ymd.to.POSIXlt(y, m, d))
|>     [1]  8.78  0.10 31.76  0.00  0.00
|>     system.time(as.POSIXlt(paste(y,m,d, sep="-")))
|>     [1] 14.64  0.13 53.30  0.00  0.00
|> 
|> This was on a 500MHz SunBlade100, a slow machine by today's standards,
|> using R 1.9.0.  There is all the evidence one could reasonably ask for
|> that going through paste() instead of providing y, m, d as numeric
|> vectors *does* degrade performance under at least some circumstances.

Ain't that just the darnest thing?

I tried on a somewhat faster machine and got this:


>     system.time(ymd.to.POSIXlt(y, m, d))
[1] 2.06 0.22 2.29 0.00 0.00
>     system.time(as.POSIXlt(paste(y,m,d, sep="-")))
[1] 2.82 0.04 2.86 0.00 0.00

Negligible difference, I'd have thought, but I tried a few more times


>     system.time(ymd.to.POSIXlt(y, m, d))
[1] 1.12 0.11 1.23 0.00 0.00
>     system.time(as.POSIXlt(paste(y,m,d, sep="-")))
[1] 2.69 0.04 2.73 0.00 0.00
>     system.time(ymd.to.POSIXlt(y, m, d))
[1] 0.98 0.02 1.00 0.00 0.00
>     system.time(as.POSIXlt(paste(y,m,d, sep="-")))
[1] 2.82 0.00 2.82 0.00 0.00
>     system.time(ymd.to.POSIXlt(y, m, d))
[1] 0.98 0.04 1.02 0.00 0.00
>     system.time(as.POSIXlt(paste(y,m,d, sep="-")))
[1] 2.65 0.03 2.68 0.00 0.00
> 


ymd.to.POSIXlt benefits from repeated, but the as.POSIXlt method
doesn't.  I even tried it again with a new R process and got the same
sort of thing?

What makes the difference?

-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From wolski at molgen.mpg.de  Tue Jun  8 08:50:56 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Tue, 08 Jun 2004 08:50:56 +0200
Subject: [R] Differential Equations
In-Reply-To: <1086665899.40c534abee53d@webmail.tci.ufal.br>
References: <1086665899.40c534abee53d@webmail.tci.ufal.br>
Message-ID: <200406080850560166.043F44BD@mail.math.fu-berlin.de>

Hi!

   "Suspicion Breeds Confidence!"

           --Brazil


Take a look at the packages listed on cran.r-project.org

or on the r prompt (if all R-packages installed type)

help.search("differential equations")

Help files with alias or concept or title matching 'differential
equations' using fuzzy matching:

IndomethODE(nlmeODE)    Pharmacokinetic modelling of Indomethacin
                        using differential equations
nlmeODE(nlmeODE)        Non-linear mixed-effects modelling in nlme
                        using differential equations
lsoda(odesolve)         Solve System of ODE (ordinary differential
                        equation)s.
rk4(odesolve)           Solve System of ODE (ordinary differential
                        equation)s by classical Runge-Kutta 4th order
                        integration.



Type 'help(FOO, package = PKG)' to inspect entry 'FOO(PKG) TITLE'.




Sincerely
Eryk

*********** REPLY SEPARATOR  ***********

On 6/8/2004 at 12:38 AM M??rcio de Medeiros Ribeiro wrote:

>>>Hello!
>>>
>>>I would like to know if R can solve Differential Equations...
>>>I don't think so because, in my point, I see R like a Statistical
>>>System, not a 
>>>Math System. Am I wrong?
>>>
>>>Thank you very much.
>>>
>>>M??rcio de Medeiros Ribeiro
>>>Graduando em Ci??ncia da Computa????o
>>>Departamento de Tecnologia da Informa????o - TCI
>>>Universidade Federal de Alagoas - UFAL
>>>Macei?? - Alagoas - Brasil
>>>Projeto CoCADa
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



Dipl. bio-chem. Eryk Witold Wolski    @    MPI-Moleculare Genetic   
Ihnestrasse 63-73 14195 Berlin       'v'    
tel: 0049-30-83875219               /   \    
mail: wolski at molgen.mpg.de        ---W-W----    http://www.molgen.mpg.de/~wolski



From ripley at stats.ox.ac.uk  Tue Jun  8 08:52:35 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 8 Jun 2004 07:52:35 +0100 (BST)
Subject: [R] AR models
In-Reply-To: <BAY12-F92gyojEoiJHD0004ec18@hotmail.com>
Message-ID: <Pine.LNX.4.44.0406080752010.20143-100000@gannet.stats>

On Mon, 7 Jun 2004, Laura Holt wrote:

> Dear R People:
> 
> Is it possible to fit an AR model such as:
> 
> y_t = fee_1 y_t-1 + fee_2 y_t-9 + a_t,
> please?
> 
> I know that we can fit an AR(9) model, but I was wondering if we could do a
> partial as described.

Yes.  See ?arima, and especially the 'fixed' argument.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Tue Jun  8 09:01:03 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 08 Jun 2004 09:01:03 +0200
Subject: [R] Load a dll
In-Reply-To: <000e01c44cd9$18141500$f63d9f88@math.ucalgary.ca>
References: <000e01c44cd9$18141500$f63d9f88@math.ucalgary.ca>
Message-ID: <40C5642F.6070408@statistik.uni-dortmund.de>

Rui wrote:
> Hi folks,
>  
> I have a question about how to load a dll. 
> First, I use the command 
> 
>>dyn.load("lassofu.dll");
> 
> then, I got the message below
> "NULL
> Warning message: 
> DLL attempted to change FPU control word from 9001f to 90003";

See ?dyn.load.
This message should make you a little bit worried about your dll.


> After I tried to use this dll in one of s functions, I got the message
> below
> "Error in .Fortran("lasso", as.double(x), as.double(y), as.double(b),
> as.integer(n),  : 
>         Fortran function name not in load table".
> BTW, my system is Windows98 + R1.9.0.
> Could anyone help me to solve this question? Thanks

Did you follow the instructions in the manual "Writing R Extensions" and 
file readme.packages?

Uwe Ligges


> Rui Wang
>  
> Phone: (403)220-4501
> Email: rwang at math.ucalgary.ca
> Department of Mathematics and Statistics
> University of Calgary
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From unung at enciety.com  Tue Jun  8 09:18:27 2004
From: unung at enciety.com (Unung Istopo Hartanto)
Date: Tue, 08 Jun 2004 14:18:27 +0700
Subject: [R] How do I sort data containts character
Message-ID: <1086679106.3320.27.camel@IT05>

Sorry, it's simple question:

example :
> data.test
  label 	value
1   one        21.35746
2   two        22.07592
3 three        20.74098

>

I would like the return :

label 	value
3 three        20.74098
1   one        21.35746
2   two        22.07592

Anyone can help it?

Thanks,

regards

Unung Istopo



From dimitris.rizopoulos at med.kuleuven.ac.be  Tue Jun  8 09:19:38 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Tue, 8 Jun 2004 09:19:38 +0200
Subject: [R] How do I sort data containts character
References: <1086679106.3320.27.camel@IT05>
Message-ID: <006d01c44d28$f6a36330$ad133a86@www.domain>

try to use:

data.test[order(data.test[,2]),]


I hope this helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Doctoral Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "Unung Istopo Hartanto" <unung at enciety.com>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, June 08, 2004 9:18 AM
Subject: [R] How do I sort data containts character


> Sorry, it's simple question:
>
> example :
> > data.test
>   label value
> 1   one        21.35746
> 2   two        22.07592
> 3 three        20.74098
>
> >
>
> I would like the return :
>
> label value
> 3 three        20.74098
> 1   one        21.35746
> 2   two        22.07592
>
> Anyone can help it?
>
> Thanks,
>
> regards
>
> Unung Istopo
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Tue Jun  8 09:36:52 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 08 Jun 2004 09:36:52 +0200
Subject: [R] GLMM(..., family=binomial(link="cloglog"))?
In-Reply-To: <40C5009A.90705@pdf.com>
References: <3A822319EB35174CA3714066D590DCD504AF7DFB@usrymx25.merck.com>
	<40C4B87B.7060301@pdf.com> <6r8yez88wo.fsf@bates4.stat.wisc.edu>
	<40C5009A.90705@pdf.com>
Message-ID: <x27jui8prf.fsf@biostat.ku.dk>

Spencer Graves <spencer.graves at pdf.com> writes:

> Data: DF
>   log-likelihood:  -55.8861
> Random effects:
>   Groups Name        Variance   Std.Dev.
>   smpl   (Intercept) 1.7500e-12 1.3229e-06
> 
> Estimated scale (compare to 1)  3.280753
> 
> Fixed effects:
>              Estimate Std. Error z value Pr(>|z|)
> (Intercept) 0.148271   0.063419  2.3379  0.01939
> 
> Number of Observations: 10
> Number of Groups: 10

The only information you have about sigma is in the overdispersion
of y, so you probably cannot both have a scale parameter and a random
effect of sample in there. Doug did say something about fixing the
scale in GLMM, but I forget whether he had implemented it or planned
to...

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From phgrosjean at sciviews.org  Tue Jun  8 09:24:20 2004
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Tue, 8 Jun 2004 09:24:20 +0200
Subject: [R] Recoding a multiple response question into a series of 1,
	0 variables
In-Reply-To: <20040608024528.GA2333@psych>
Message-ID: <MABBLJDICACNFOLGIHJOGECKEIAA.phgrosjean@sciviews.org>

Hello,
Here is a slightly more sophisticate and fully vectorized, answer.

RecodeChoices <- function(mat) {
	# Make sure mat is a matrix (in case it is a data.frame)
	mat <- as.matrix(mat)

	# Get dimensions of the matrix
	Dim <- dim(mat)
	Nr <- Dim[1]
	Nc <- Dim[2]

	# Flatten it into a vector, but by row (need to transpose first!)
	mat <- t(mat)
	dim(mat) <- NULL

	# Offset is a vector of offsets to make locations unique in vector mat
	# (a solution to avoid loops, see Jonathan Baron's answer)
	Offset <- sort(rep(0:(Nr - 1) * Nc, Nc))

	# Initialize a vector of results of the same size with 0's
	res <- rep(0, Nr * Nc)

	# Now replace locations pointed by (mat + Offset) by 1 in res
	res[mat + Offset] <- 1

	# Transform res into a matrix of same size of mat, by row
	res <- matrix(res, nrow = Nr, byrow = TRUE)

	# Return the result
	return(res)
}

# Now your example:
A <- matrix(c(4,  2, NA, NA, NA,
              1,  3,  4,  5, NA,
              3,  2, NA, NA, NA), nrow = 3, byrow = TRUE)
A
RecodeChoices(A)

Depending on the use you make of this, it is perhaps preferable to recode it
as a boolean (as.numeric() would give you the c(1, 0) as above easily). To
do this, just replace:
res <- rep(0, Nr * Nc)   by   res <- rep(FALSE, Nr * Nc)
and:
res[mat + Offset] <- TRUE

You may also consider to make it factors... and to finalize this function,
you should add code to collect row and column names from mat and apply them
to res, and perhaps transforn res into a data.frame if mat was a data.frame
itself.

Best,

Philippe Grosjean

.......................................................<?}))><....
 ) ) ) ) )
( ( ( ( (   Prof. Philippe Grosjean
\  ___   )
 \/ECO\ (   Numerical Ecology of Aquatic Systems
 /\___/  )  Mons-Hainaut University, Pentagone
/ ___  /(   8, Av. du Champ de Mars, 7000 Mons, Belgium
 /NUM\/  )
 \___/\ (   phone: + 32.65.37.34.97, fax: + 32.65.37.33.12
       \ )  email: Philippe.Grosjean at umh.ac.be
 ) ) ) ) )  SciViews project coordinator (http://www.sciviews.org)
( ( ( ( (
...................................................................

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Jonathan Baron
Sent: Tuesday, 08 June, 2004 04:45
To: Greg Blevins
Cc: R-Help
Subject: Re: [R] Recoding a multiple response question into a series of
1,0 variables


On 06/07/04 21:28, Greg Blevins wrote:
>Hello R folks.
>
>1) The question that generated the data, which I call Qx:
>Which of the following 5 items have you performed in the past month?
(multipe
>response)
>
>2) How the data is coded in my current dataframe:
>The first item that a person selected is coded under a field called
Qxfirst; the
>second selected under Qxsecond, etc.  For the first Person, the NAs mean
that that
>person only selected two of the five items.
>
>Hypothetical data is shown
>
>                    Qxfirst    Qxsecond    Qxthird    Qxfourth    Qxfifth
>Person1        4            2                NA            NA            NA
>Person2        1            3                4               5
NA
>Person3        3            2                NA            NA            NA
>
>3) How I want the data to be be coded:
>
>I want each field to be one of the five items and I want each field to
contain a 1 or
>0 code--1 if they mentioned the item, 0 otherwise.
>
>Given the above data, the new fields would look as follows:
>
>                    Item1    Item2        Item3            Item4
Item5
>Person1        0            1               0                1
0
>Person2        1            0               1                1
1
>Person3        0            1               1                0
0

Here is an idea:
X <- c(4,5,NA,NA,NA) # one row
Y <- rep(NA,5) # an empty row
Y[X] <- 1

Y is now
NA NA NA 1 1
which is what you want.

So you need to do this on each row and then convert the NAs to
0s.  So first create an empty data frame, the same size as your
original one X, like my Y.  Callit Y.  Then a loop?  (I can't
think of a better way just now, like with mapply.)

for (i in [whatever]) Y[i][X[i]] <- 1

(Not tested.)  Jon
--
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron
R search page:        http://finzi.psych.upenn.edu/

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Tue Jun  8 09:58:47 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 8 Jun 2004 08:58:47 +0100 (BST)
Subject: [R] Censboot Warning and Error Messages
In-Reply-To: <1086602762.40c43e0a04fbf@mail.upm.edu.ph>
Message-ID: <Pine.LNX.4.44.0406080856400.14999-100000@gannet.stats>

On Mon, 7 Jun 2004, Jingky Lozano wrote:

> Good day R help list!!!
> 
> I've been trying to do Bootstrap in R on Censored data.  I encountered
> WARNING/ERROR messages which I could not find explanation.
> I've been searching on the literature for two days now and still can't find
> answers.  I hope there's anyone out there who can help me 
> with these two questions: 
> 
> 1. If the "Loglik converged before variable..." message appears (please see
> printout below) while doing ordinary bootstrap,
> does it mean that I cannot trust the result of the bootstrap statistics?  Is
> there a valid way to resolve it like increase the sample size?

This is a message from coxph and nothing at all to do with censboot.

> 2. In doing conditional bootstrap with survival data, how can one handle a data
> with two largest survival time observations (ties)
> but one is censored while the other is not.  For example, if the censoring time
> is 48 months and a patient died exactly at that time, he will
> have the same survival time as another patient who was also observed to lived 48
> months long but classified as censored because he was still 
> alive at the set censoring time.  Doing the recommended algorithm in R gives an
> "error in sample length..." message.

Recommended by whom?  You could break the ties (death first) and avoid the 
problem.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From PJARES at clinic.ub.es  Tue Jun  8 10:25:38 2004
From: PJARES at clinic.ub.es (PJARES@clinic.ub.es)
Date: Tue, 8 Jun 2004 10:25:38 +0200
Subject: [R] (no subject)
Message-ID: <35FDBC81CCC83B498CB939CAA187B50F13CA0C@IDIBXMAIL1.csc.es>

Hi!!
I am a new user of R (just trying to analysis microarrays with some packages from the bioconductor project).
I would like to import a text-delimeted file containing 20 columns and 22200 rows.

I have tried 
read.table;
scan(file="")
 <matrix(scan("file", n=20*200,20,200, byrow=TRUE)); 
Doesn't matter what I try I got the next message:
error in file (file "r"): unable to open connection
in addition warning message
cannot open file "MAS5orig" (this is the name of the file I am trying to import in R)

Any advice? which comman I should you use to import a text file (Excell, text tab-delimeted file) into R.

Thank you very much for your help.

Best wishes,

Pedro

Pedro Jares, Ph.D.
Genomics Unit, IDIBAPS
Barcelona University
C/ Villarroel 170, 
08036 .Barcelona, Spain
Telf. 93 2275400,
Ext 2184 o 2129
Fax 93 2275717



From k.wang at auckland.ac.nz  Tue Jun  8 10:32:56 2004
From: k.wang at auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Tue, 8 Jun 2004 20:32:56 +1200
Subject: [R] Reading in Files (was: no subject)
References: <35FDBC81CCC83B498CB939CAA187B50F13CA0C@IDIBXMAIL1.csc.es>
Message-ID: <00ae01c44d33$33020de0$7433d882@stat.auckland.ac.nz>

Please use a more appropriate subject!

----- Original Message ----- 
From: <PJARES at clinic.ub.es>
To: <R-help at stat.math.ethz.ch>
Sent: Tuesday, June 08, 2004 8:25 PM
Subject: [R] (no subject)


> Hi!!
> I am a new user of R (just trying to analysis microarrays with some
packages from the bioconductor project).
> I would like to import a text-delimeted file containing 20 columns and
22200 rows.
>
> I have tried
> read.table;
> scan(file="")
>  <matrix(scan("file", n=20*200,20,200, byrow=TRUE));
> Doesn't matter what I try I got the next message:
> error in file (file "r"): unable to open connection
> in addition warning message
> cannot open file "MAS5orig" (this is the name of the file I am trying to
import in R)

It's a syntax error, I think.  Why do you have a "<" before matrix()
command?

Also, which operating system are you running?  Are you sure your working
directory is correct?

Cheers,

Kevin



From wolski at molgen.mpg.de  Tue Jun  8 10:43:12 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Tue, 08 Jun 2004 10:43:12 +0200
Subject: [R] (no subject)
In-Reply-To: <35FDBC81CCC83B498CB939CAA187B50F13CA0C@IDIBXMAIL1.csc.es>
References: <35FDBC81CCC83B498CB939CAA187B50F13CA0C@IDIBXMAIL1.csc.es>
Message-ID: <200406081043120090.04A60AA4@mail.math.fu-berlin.de>

Hi!

The message means that R does not find the file. 

Under windows (if you are using windows) you must escape the backslash with a backslash \.
or give us an example how you provide the path to the file.


Or use:

file.choose()

or.

library(tkWidgets)
fileBrowser()


both functions will return the proper path to the file.

Sincerely 

Eryk




*********** REPLY SEPARATOR  ***********

On 6/8/2004 at 10:25 AM PJARES at clinic.ub.es wrote:

>>>Hi!!
>>>I am a new user of R (just trying to analysis microarrays with some
>>>packages from the bioconductor project).
>>>I would like to import a text-delimeted file containing 20 columns and
>>>22200 rows.
>>>
>>>I have tried 
>>>read.table;
>>>scan(file="")
>>> <matrix(scan("file", n=20*200,20,200, byrow=TRUE)); 
>>>Doesn't matter what I try I got the next message:
>>>error in file (file "r"): unable to open connection
>>>in addition warning message
>>>cannot open file "MAS5orig" (this is the name of the file I am trying to
>>>import in R)
>>>
>>>Any advice? which comman I should you use to import a text file (Excell,
>>>text tab-delimeted file) into R.
>>>
>>>Thank you very much for your help.
>>>
>>>Best wishes,
>>>
>>>Pedro
>>>
>>>Pedro Jares, Ph.D.
>>>Genomics Unit, IDIBAPS
>>>Barcelona University
>>>C/ Villarroel 170, 
>>>08036 .Barcelona, Spain
>>>Telf. 93 2275400,
>>>Ext 2184 o 2129
>>>Fax 93 2275717
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



Dipl. bio-chem. Eryk Witold Wolski    @    MPI-Moleculare Genetic   
Ihnestrasse 63-73 14195 Berlin       'v'    
tel: 0049-30-83875219               /   \    
mail: wolski at molgen.mpg.de        ---W-W----    http://www.molgen.mpg.de/~wolski



From hb at maths.lth.se  Tue Jun  8 10:47:49 2004
From: hb at maths.lth.se (Henrik Bengtsson)
Date: Tue, 8 Jun 2004 10:47:49 +0200
Subject: [R] [Q] raw -> gpr in aroma package
In-Reply-To: <200406080222.i582MJt190037@jade.cs.pusan.ac.kr>
Message-ID: <000001c44d35$48950e30$070040d5@hblaptop>

Hi. 

First, I think you should address questions about aroma directly to me (the
author) instead of the r-help list since it is not a "core" R package.

To answer you question, you can not make a GenePixData object out of a
RawData object because the latter contains much less information than a
minimum GPR file would require. However, you can given an existing
GenePixData object overwrite/modify or add fields to it. But, before
explaining how you can do it you should be aware that modifying an existing
GPR structure will make some of the data inconsistent and this may (or may
not) confuse the GenePix software. Adding fields may as well confuse
GenePix.

Easiest is if you modify an existing field:

gpr[["F532 Median"]] <- raw2$G
gpr[["F635 Median"]] <- raw2$R

To find existing fields do ll(gpr).

To add new fields you also have to updated the internal/private .fieldNames
field, which is not recommended:
gpr$R <- raw2$R
gpr$G <- raw2$G
gpr$.fieldNames <- c(gpr$.fieldNames, c("R","G"))

You write a GenePixData object to file by

write(gpr, "result.gpr")

Aroma will update the GenePix header so it follows the correct GPR file
format. Some people have reported that GenePix requires the header elements
to come in a pre-defined order. Currently aroma does not take care of this
and it might be that you will experience problems. As a last resort you can
always append fields manually in, say, Excel. If you do so, do not forget to
update the ATF header from, say
ATF	 1.0 
24	43
to
ATF	 1.0 
24	45
where "43" and "45" is the number of fields/columns.

But again, I'm not convinced that you should edit GPR files yourself.

Cheers

Henrik Bengtsson
http://www.braju.com/R/

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of ????????????
Sent: Tuesday, June 08, 2004 4:24 AM
To: r-help at stat.math.ethz.ch
Cc: 'junmyungjae'
Subject: [R] [Q] raw -> gpr in aroma package


Hi.
 
Is it possible to make gpr from raw?
 
library(aroma)
#read gpr file
gpr <- GenePixData$read("gpr123.gpr", path=aroma$dataPath)
# gpr -> raw
raw <- as.RawData(gpr)
# raw -> ma
ma <- getSignal(raw, bgSubtract=FALSE)
ma.norm <- clone(ma)
#normalization
normalizeWithinSlide(ma.norm, "s")
#ma -> raw
raw2 <- as.RawData(ma)
 
I want to make gpr data from raw2 and then I want to write new gpr(
write(gpr,"result.gpr")). Is it possible? Can anyone help me with this?
 
Thanks,
 
Hee-Jeong Jin.

	[[alternative HTML version deleted]]

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From gavin.simpson at ucl.ac.uk  Tue Jun  8 11:24:18 2004
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Tue, 08 Jun 2004 10:24:18 +0100
Subject: [R] error during make of R-patched on Fedora core 2
In-Reply-To: <1086645681.2235.274.camel@localhost.localdomain>
References: <Pine.LNX.4.44.0406072214580.459-100000@reclus.nhh.no>
	<40C4D561.6030300@ucl.ac.uk>
	<1086645681.2235.274.camel@localhost.localdomain>
Message-ID: <40C585C2.4090008@ucl.ac.uk>

Marc Schwartz wrote:
> On Mon, 2004-06-07 at 15:51, Gavin Simpson wrote:
> 
> snip
> 
> 
>>Thanks Roger and Marc, for suggesting I use ./tools/rsync-recommended 
>>from within the R-patched directory.
>>
>>This seems to have done the trick as make completed without errors this 
>>time round. The Recommended directory also contained the links to the 
>>actual tar.gz files after doing the rsync command, so I guess this was 
>>the problem (or at least related to it.) I'm off home now with the 
>>laptop to see if I can finish make check-all and make install R.
>>
>>I have re-read the section describing the installation process for 
>>R-patched or R-devel in the R Installation and Administration manual 
>>(from R.1.9.0) just in case I missed something. Section 1.2 of this 
>>manual indicates that one can proceed *either* by downloading R-patched 
>>and then the Recommended packages from CRAN and placing the tar.gz files 
>>in R_HOME/src/library/Recommended, or by using rsync to download 
>>R-patched, and then to get the Recommended packages. The two are quite 
>>separately documented in the manual, and do seem to be in disagreement 
>>with the R-sources page on the CRAN website, which doesn't mention the 
>>manual download method (for Recommended) at all.
>>
>>Is there something wrong with the current Recommended files on CRAN, or 
>>is the section in the R Installation & Admin manual out-of-date or in 
>>error, or am I missing something vital here? This isn't a complaint: I'm 
>>just pointing this out in case this is something that needs updating in 
>>the documentation.
>>
>>All the best,
>>
>>Gavin
> 
> 
> Perhaps I am being dense, but in reviewing the two documents (R Admin
> and the CRAN sources page), I think that the only thing lacking is a
> description on the CRAN page of the manual download option for the Rec
> packages.
> 
> You would need to go here now for 1.9.1 Alpha/Beta which is where the
> current r-patched is:
> 
> http://www.cran.mirrors.pair.com/src/contrib/1.9.1/Recommended/
> 
> The standard links on CRAN are for the current 'released' version, which
> is still 1.9.0 for the moment.

Yes, but having downloaded the contents of that directory (as VERSION 
indicated that R-patched was 1.9.1 alpha), the links to the source files 
for the Recommended packages or not present (obviously). And make 
doesn't seem to work without these links. The rsync approach places the 
package sources *and* the links in the correct directory.

So the instructions in the Admin manual are lacking a statement that you 
need to create links to each of the package sources in the following 
form name-of-package.tgz which links to name-of-package_version.tar.gz. 
As it stands, the instructions in the Installation & Admin manual are 
not sufficient to get the manual download method to work.

> Procedurally, I think that the rsync approach is substantially easier
> (one step instead of multiple downloads) and certainly less error prone.
> Also the ./tools/rsync-recommended script is set up to pick up the
> proper package versions, which also helps to avoid conflicts.

I agree - being a bit of a Linux newbie, I hadn't used rsync before. 
Seeing how easy it was to use this method of getting the required 
sources I will be using this method in future.

> HTH,
> 
> Marc

Cheers

Gavin
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpson at ucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From eobudho at uonbi.ac.ke  Tue Jun  8 17:16:25 2004
From: eobudho at uonbi.ac.ke (eobudho@uonbi.ac.ke)
Date: Tue, 8 Jun 2004 12:16:25 -0300 (EAT)
Subject: [R] Downloading of packages
Message-ID: <1239.192.168.1.22.1086686185.squirrel@mail.uonbi.ac.ke>

Hi,
I have been trying to download BradleyTerry and brlr software but in
vain. Can you help me please. I already have R installed in my computer.
Thank you in advance.

Elias Obudho.




-----------------------------------------
University of Nairobi Mail Services
   "You can't afford to stay offline"
http://mail.uonbi.ac.ke/



From nicolas.pelay at rd.francetelecom.com  Tue Jun  8 11:44:55 2004
From: nicolas.pelay at rd.francetelecom.com (zze-PELAY Nicolas FTRD/DMR/BEL)
Date: Tue, 8 Jun 2004 11:44:55 +0200
Subject: [R] Little question
Message-ID: <D4EF10CCEB2CE742BEEA9838C590B0E80200F776@ftrdmel2.rd.francetelecom.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040608/ead58e02/attachment.pl

From chris.jackson at imperial.ac.uk  Tue Jun  8 11:55:39 2004
From: chris.jackson at imperial.ac.uk (Chris Jackson)
Date: Tue, 08 Jun 2004 10:55:39 +0100
Subject: [R] msm capabilities
In-Reply-To: <40C4E7E0.8060709@mnsi.net>
References: <40C4E7E0.8060709@mnsi.net>
Message-ID: <40C58D1B.4010908@imperial.ac.uk>

russell wrote:

> Hello,
>
> I'm wondering if anyone has used the msm package to compute the steady 
> state probabilities for a Markov model? 


There's no built-in function in msm to do this, but this would be a
useful feature.  For discrete time Markov chains this is a matter of
finding the eigenvector of the transition probability matrix.  But msm
is really for fitting continuous-time Markov models.  In the continuous
case, assuming a steady state p exists,   you'd need to solve the two
equations

p.Q = 0
p.1 = 1

for example, using something like

 > n <- nrow(Q)
 > qr.solve(rbind(t(Q), rep(1, n)),  c(rep(0,n), 1))

This is also the limit as t -> Inf of P(t) = Exp(tQ).

Chris (author of msm)

-- 
Christopher Jackson <chris.jackson at imperial.ac.uk>, Research Associate,
Department of Epidemiology and Public Health, Imperial College
School of Medicine, Norfolk Place, London W2 1PG, tel. 020 759 43371



From jasont at indigoindustrial.co.nz  Tue Jun  8 11:56:09 2004
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Tue, 08 Jun 2004 21:56:09 +1200
Subject: [R] Little question
In-Reply-To: <D4EF10CCEB2CE742BEEA9838C590B0E80200F776@ftrdmel2.rd.francetelecom.fr>
References: <D4EF10CCEB2CE742BEEA9838C590B0E80200F776@ftrdmel2.rd.francetelecom.fr>
Message-ID: <1086688569.5441.3.camel@linux.site>

On Tue, 2004-06-08 at 21:44, zze-PELAY Nicolas FTRD/DMR/BEL wrote:
> Here I got a function :
> 
> f<-function(x){
> res<-list()
> res$bool=(x>=0)
> res$tot=x
> return(res)
> }
> 
> I want that a=res$bool and b=res$tot
> I know that a possible solution is :
> 
Like....

foo <- function(x,...){
	list(a=(x>=0),b=x)
}

Cheers

Jason



From ligges at statistik.uni-dortmund.de  Tue Jun  8 13:03:24 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 08 Jun 2004 13:03:24 +0200
Subject: [R] Downloading of packages
In-Reply-To: <1239.192.168.1.22.1086686185.squirrel@mail.uonbi.ac.ke>
References: <1239.192.168.1.22.1086686185.squirrel@mail.uonbi.ac.ke>
Message-ID: <40C59CFC.8030409@statistik.uni-dortmund.de>

eobudho at uonbi.ac.ke wrote:

> Hi,
> I have been trying to download BradleyTerry and brlr software but in

These are packages.

> vain. Can you help me please. I already have R installed in my computer.
> Thank you in advance.

Version of R?
Operating system and its version?
What was the error message after trying
  install.packages(c("BradleyTerry", "brlr"))
?

Uwe Ligges



> Elias Obudho.
> 
> 
> 
> 
> -----------------------------------------
> University of Nairobi Mail Services
>    "You can't afford to stay offline"
> http://mail.uonbi.ac.ke/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ozric at web.de  Tue Jun  8 13:09:25 2004
From: ozric at web.de (Christian Schulz)
Date: Tue, 8 Jun 2004 13:09:25 +0200
Subject: [R] level assignment
Message-ID: <200406081309.25257.ozric@web.de>

Hi,

i would like recocde some numeric variables in one
step, but hanging unexpected in a level asignment problem?

for(i in 2:length(msegmente))
{   msegmente[,i] <- as.factor(msegmente[,i])
}
 
Problem is that not every level is in every variable, so the
asignment is necessary!?

levels(LT.200301) <-   c(1=AK,3=GC,10=OC, 
29=AM,32=IA,38=ACH,52=ZBA,53=A9L,59=EHK)
Error: syntax error
> levels(LT.200301) <-   list(c(1=AK,3=GC,10=OC, 
29=AM,32=IA,38=ACH,52=ZBA,53=A9L,59=EHK))
Error: syntax error
> levels(LT.200301) <-   c(1="AK",3="GC",10="OC", 
29="AM",32="IA",38="ACH",52="ZBA",53="A9L",59="EHK")
Error: syntax error

Many thanks for any hint/help
Christian



From ripley at stats.ox.ac.uk  Tue Jun  8 13:23:19 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 8 Jun 2004 12:23:19 +0100 (BST)
Subject: [R] error during make of R-patched on Fedora core 2
In-Reply-To: <40C585C2.4090008@ucl.ac.uk>
Message-ID: <Pine.LNX.4.44.0406081217120.15138-100000@gannet.stats>

On Tue, 8 Jun 2004, Gavin Simpson wrote:

> Marc Schwartz wrote:
> > On Mon, 2004-06-07 at 15:51, Gavin Simpson wrote:
> > 
> > snip
> > 
> > 
> >>Thanks Roger and Marc, for suggesting I use ./tools/rsync-recommended 
> >>from within the R-patched directory.
> >>
> >>This seems to have done the trick as make completed without errors this 
> >>time round. The Recommended directory also contained the links to the 
> >>actual tar.gz files after doing the rsync command, so I guess this was 
> >>the problem (or at least related to it.) I'm off home now with the 
> >>laptop to see if I can finish make check-all and make install R.
> >>
> >>I have re-read the section describing the installation process for 
> >>R-patched or R-devel in the R Installation and Administration manual 
> >>(from R.1.9.0) just in case I missed something. Section 1.2 of this 
> >>manual indicates that one can proceed *either* by downloading R-patched 
> >>and then the Recommended packages from CRAN and placing the tar.gz files 
> >>in R_HOME/src/library/Recommended, or by using rsync to download 
> >>R-patched, and then to get the Recommended packages. The two are quite 
> >>separately documented in the manual, and do seem to be in disagreement 
> >>with the R-sources page on the CRAN website, which doesn't mention the 
> >>manual download method (for Recommended) at all.
> >>
> >>Is there something wrong with the current Recommended files on CRAN, or 
> >>is the section in the R Installation & Admin manual out-of-date or in 
> >>error, or am I missing something vital here? This isn't a complaint: I'm 
> >>just pointing this out in case this is something that needs updating in 
> >>the documentation.
> >>
> >>All the best,
> >>
> >>Gavin
> > 
> > 
> > Perhaps I am being dense, but in reviewing the two documents (R Admin
> > and the CRAN sources page), I think that the only thing lacking is a
> > description on the CRAN page of the manual download option for the Rec
> > packages.
> > 
> > You would need to go here now for 1.9.1 Alpha/Beta which is where the
> > current r-patched is:
> > 
> > http://www.cran.mirrors.pair.com/src/contrib/1.9.1/Recommended/
> > 
> > The standard links on CRAN are for the current 'released' version, which
> > is still 1.9.0 for the moment.
> 
> Yes, but having downloaded the contents of that directory (as VERSION 
> indicated that R-patched was 1.9.1 alpha), the links to the source files 
> for the Recommended packages or not present (obviously). And make 
> doesn't seem to work without these links. The rsync approach places the 
> package sources *and* the links in the correct directory.
> 
> So the instructions in the Admin manual are lacking a statement that you 
> need to create links to each of the package sources in the following 
> form name-of-package.tgz which links to name-of-package_version.tar.gz. 
> As it stands, the instructions in the Installation & Admin manual are 
> not sufficient to get the manual download method to work.

You need to run tools/link-recommended.  I've added that to R-admin.

> > Procedurally, I think that the rsync approach is substantially easier
> > (one step instead of multiple downloads) and certainly less error prone.
> > Also the ./tools/rsync-recommended script is set up to pick up the
> > proper package versions, which also helps to avoid conflicts.
> 
> I agree - being a bit of a Linux newbie, I hadn't used rsync before. 
> Seeing how easy it was to use this method of getting the required 
> sources I will be using this method in future.

rsync is great, *provided* you have permission to use the ports it uses.  
Users with http proxies often do not, hence the description of the manual 
method.  During alpha/beta periods, we do make a complete tarball 
available, and I wonder if we should not be doing so with 
R-patched/R-devel at all times.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Achim.Zeileis at wu-wien.ac.at  Tue Jun  8 13:37:45 2004
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 8 Jun 2004 13:37:45 +0200
Subject: [R] level assignment
In-Reply-To: <200406081309.25257.ozric@web.de>
References: <200406081309.25257.ozric@web.de>
Message-ID: <20040608133745.50baf0ef.Achim.Zeileis@wu-wien.ac.at>

On Tue, 8 Jun 2004 13:09:25 +0200 Christian Schulz wrote:

> Hi,
> 
> i would like recocde some numeric variables in one
> step, but hanging unexpected in a level asignment problem?
> 
> for(i in 2:length(msegmente))
> {   msegmente[,i] <- as.factor(msegmente[,i])
> }
>  
> Problem is that not every level is in every variable, so the
> asignment is necessary!?
> 
> levels(LT.200301) <-   c(1=AK,3=GC,10=OC, 
> 29=AM,32=IA,38=ACH,52=ZBA,53=A9L,59=EHK)
> Error: syntax error
> > levels(LT.200301) <-   list(c(1=AK,3=GC,10=OC, 
> 29=AM,32=IA,38=ACH,52=ZBA,53=A9L,59=EHK))
> Error: syntax error
> > levels(LT.200301) <-   c(1="AK",3="GC",10="OC", 
> 29="AM",32="IA",38="ACH",52="ZBA",53="A9L",59="EHK")
> Error: syntax error

I'm not sure what exactly you are trying to do, but does replacing

  as.factor(msegmente[,i])

by

  factor(msegmente[,i],
         levels = c(1, 3, 10, 29, 32, 38, 52, 53, 59),
         labels = c("AK","GC","OC","AM","IA","ACH","ZBA","A9L","EHK"))

yield the desired result?

hth,
Z


> Many thanks for any hint/help
> Christian
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From kan_liu1 at yahoo.com  Tue Jun  8 13:41:48 2004
From: kan_liu1 at yahoo.com (kan Liu)
Date: Tue, 8 Jun 2004 04:41:48 -0700 (PDT)
Subject: [R] Average R-squared of model1 to model n
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7E35@usrymx25.merck.com>
Message-ID: <20040608114148.78106.qmail@web60603.mail.yahoo.com>

Hi,

Thanks for your message. I tried to prove that the
R-squared of the averaged model is always greater than
or equals to the average R-squared of individual
models (supposed m=2), Please see the attached r2.pdf.
I hope this can be generalized to general case (m >
2).

Any comment would be very appreciated!


Kan

Cambridge University, UK

--- "Liaw, Andy" <andy_liaw at merck.com> wrote:
> The Y1, Y2, etc. that Kan mentioned are predicted
> values of a test set data
> from models that supposedly were fitted to the same
> (or similar) data.  It's
> hard for me to imagine the outcome would be as
> `severe' as Y1 = -Y2.
> 
> That said, I do not think that the R-squared (or
> q-squared as some call it)
> of the aggregate model is necessarily larger or
> equal to the average
> R-squared of the component models.  It obviously
> depends on how the
> component models are generated.  As a hypothetical
> example (because I
> haven't acutally tried it, just speculating): 
> Suppose the data are
> generated from a step function, the sort that would
> be perfect for
> regression trees.  If one grows several well-pruned
> trees, I'd guess that
> the average R-squared of the individual trees has a
> chance of being larger
> than the R-squared of the averaged model.
> 
> Best,
> Andy
> 
> > From: Gabor Grothendieck
> > 
> > Suppose m=2, Y1=Y and Y2= -Y.  Then (b) is zero so
> (a) must be
> > greater or equal to (b).  Thus (b) is not
> necessarily greater 
> > than (a).
> > 
> > 
> > kan Liu <kan_liu1 <at> yahoo.com> writes:
> > 
> > : 
> > : Hi,
> > : 
> > : We got a question about interpretating
> R-suqared.
> > : 
> > : The actual outputs for a test dataset is
> X=(x1,x2, ..., xn).
> > : model 1 predicted the outputs as
> Y1=(y11,y12,..., y1n)
> > : model n predicted the outputs as
> Y2=(y21,y22,..., y2n)
> > : 
> > : ... 
> > : model m predicted the outputs as
> Ym=(ym1,ym2,..., ymn)
> > : 
> > : Now we have two ways to calculate R squared to
> evaluate the average 
> > performance of committee model.
> > : 
> > : (a) Calculate R squared between (X, Y1), (X,
> Y2), ..., 
> > (X,Ym), and then 
> > averaging the R squared
> > : (b) Calculate average Y=(Y1+Y2, + ... Ym)/m, and
> then 
> > calculate the R 
> > squared between (X, Y). 
> > : 
> > : We found it seemed that R squared calculated in
> (b) is 
> > 'always' higher than 
> > that in (a).
> > : 
> > : Does this result depends on the test dataset or
> this 
> > happened by chance?Can 
> > you advise me any reference for
> > : this issue? 
> > : 
> > : Many thanks in advance!
> > : 
> > : Kan
> > : 
> > : 
> > : 		
> > : ---------------------------------
> > : 
> > : 	[[alternative HTML version deleted]]
> > : 
> > : ______________________________________________
> > : R-help <at> stat.math.ethz.ch mailing list
> > :
>
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > : PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > : 
> > :
> > 
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> >
>
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> > 
> 
> 
>
------------------------------------------------------------------------------
> Notice:  This e-mail message, together with any
> attachments, contains information of Merck & Co.,
> Inc. (One Merck Drive, Whitehouse Station, New
> Jersey, USA 08889), and/or its affiliates (which may
> be known outside the United States as Merck Frosst,
> Merck Sharp & Dohme or MSD and in Japan, as Banyu)
> that may be confidential, proprietary copyrighted
> and/or legally privileged. It is intended solely for
> the use of the individual or entity named on this
> message.  If you are not the intended recipient, and
> have received this message in error, please notify
> us immediately by reply e-mail and then delete it
> from your system.
> ------------------------------------------------------------------------------


	
		
__________________________________


-------------- next part --------------
A non-text attachment was scrubbed...
Name: r2.pdf
Type: application/pdf
Size: 31911 bytes
Desc: r2.pdf
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20040608/5e70bb49/r2.pdf

From dvumani at hotmail.com  Tue Jun  8 14:22:37 2004
From: dvumani at hotmail.com (Vumani Dlamini)
Date: Tue, 08 Jun 2004 12:22:37 +0000
Subject: [R] counting number of objects in a list
Message-ID: <BAY16-F52QUo77sGJaC000087cf@hotmail.com>

Dear R-users;

I am interested in getting the number of objects in a list, and have thus 
far been unsuccessful. Can you please help.

Thank you.


Vumani



From ligges at statistik.uni-dortmund.de  Tue Jun  8 14:28:21 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 08 Jun 2004 14:28:21 +0200
Subject: [R] counting number of objects in a list
In-Reply-To: <BAY16-F52QUo77sGJaC000087cf@hotmail.com>
References: <BAY16-F52QUo77sGJaC000087cf@hotmail.com>
Message-ID: <40C5B0E5.908@statistik.uni-dortmund.de>

Vumani Dlamini wrote:

> Dear R-users;
> 
> I am interested in getting the number of objects in a list, and have 
> thus far been unsuccessful. Can you please help.
> 
> Thank you.
> 
> 
> Vumani
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html

?length

Uwe Ligges



From andy_liaw at merck.com  Tue Jun  8 14:26:57 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 8 Jun 2004 08:26:57 -0400
Subject: [R] counting number of objects in a list
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7E4D@usrymx25.merck.com>

Lists in the S language are like vectors, so length(mylist) would tell you
how many components there are in mylist.  Beware, though, that lists can
well be nested...

Andy

> From: Vumani Dlamini
> 
> Dear R-users;
> 
> I am interested in getting the number of objects in a list, 
> and have thus 
> far been unsuccessful. Can you please help.
> 
> Thank you.
> 
> Vumani



From ozric at web.de  Tue Jun  8 14:29:24 2004
From: ozric at web.de (Christian Schulz)
Date: Tue, 8 Jun 2004 14:29:24 +0200
Subject: [R] level assignment
In-Reply-To: <20040608133745.50baf0ef.Achim.Zeileis@wu-wien.ac.at>
References: <200406081309.25257.ozric@web.de>
	<20040608133745.50baf0ef.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <200406081429.24067.ozric@web.de>

many thanks!

christian


> I'm not sure what exactly you are trying to do, but does replacing
>
>   as.factor(msegmente[,i])
>
> by
>
>   factor(msegmente[,i],
>          levels = c(1, 3, 10, 29, 32, 38, 52, 53, 59),
>          labels = c("AK","GC","OC","AM","IA","ACH","ZBA","A9L","EHK"))
>
> yield the desired result?
>
> hth,
> Z
>
> > Many thanks for any hint/help
> > Christian
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html



From dimitris.rizopoulos at med.kuleuven.ac.be  Tue Jun  8 14:29:16 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Tue, 8 Jun 2004 14:29:16 +0200
Subject: [R] counting number of objects in a list
References: <BAY16-F52QUo77sGJaC000087cf@hotmail.com>
Message-ID: <003b01c44d54$3770d6b0$ad133a86@www.domain>

you mean something like:

x <- list(a=1:10, b=matrix(rnorm(5*6), 5, 6), c=5)
length(x)

Best,
Dimitris

----
Dimitris Rizopoulos
Doctoral Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "Vumani Dlamini" <dvumani at hotmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, June 08, 2004 2:22 PM
Subject: [R] counting number of objects in a list


> Dear R-users;
>
> I am interested in getting the number of objects in a list, and have
thus
> far been unsuccessful. Can you please help.
>
> Thank you.
>
>
> Vumani
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From dmurdoch at pair.com  Tue Jun  8 14:52:27 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Tue, 08 Jun 2004 08:52:27 -0400
Subject: [R] Printing Lattice Graphs from Windows
In-Reply-To: <200406061814.i56IE7Lr012812@ms-smtp-03-eri0.southeast.rr.com>
References: <000401c44a29$86acc280$442501a3@plants.ox.ac.uk>
	<200406061814.i56IE7Lr012812@ms-smtp-03-eri0.southeast.rr.com>
Message-ID: <e5dbc0pfprrmjpa6kqmv03qj16fqdb97nn@4ax.com>

On Sun, 6 Jun 2004 14:13:59 -0400, "Charles and Kimberly Maner"
<ckjmaner at carolina.rr.com> wrote :

>Hello.  I have researched this topic and have found no answers.  I am
>running R 1.9.0 and am trying to print a lattice graph, (e.g., xyplot(1~1)),
>using mouse right click -> print.  It produces a blank page.  Also, I right
>clich, copy the metafile and paste into a MS Office document, (e.g., .ppt,
>.doc) and, same thing, a blank.  I have updated to the latest lattice
>package and still no printing.  Any help/advice?

This bug appears to be fixed in the current alpha build (Version 1.9.1
alpha (2004-06-08)).  This will be available on CRAN by tomorrow
morning, and on the mirrors soon afterwards.

You can download the windows binary from
<http://cran.us.r-project.org/bin/windows/base/rpatched.html>.  Wait
until it gives today or later as the build date, or you'll get an old
one.

Duncan Murdoch



From M.Mamin at intershop.de  Tue Jun  8 15:38:10 2004
From: M.Mamin at intershop.de (Marc Mamin)
Date: Tue, 8 Jun 2004 15:38:10 +0200
Subject: [R] Computting statistics on a matrix with 2 factor column
Message-ID: <A03188C6623C0D46A703CB5AA59907F217067D@JENMAIL01.ad.intershop.net>

Hello,

I suppose this is a basic question but couldn't find a solution.:

I have a large matrix with let say 3 columns:

V1	V2	V3
a	x	2
a	x	4
a	y	8
b	z	16

and I want to compute some statistics based on 
the levels resulting form the combination of the two first columns

e.g.:

SUM->

V1	V2	V3
a	x	6
a	y	8
b	z	16


Thanks for your hints .

Marc



From ccleland at optonline.net  Tue Jun  8 15:44:02 2004
From: ccleland at optonline.net (Chuck Cleland)
Date: Tue, 08 Jun 2004 09:44:02 -0400
Subject: [R] Computting statistics on a matrix with 2 factor column
In-Reply-To: <A03188C6623C0D46A703CB5AA59907F217067D@JENMAIL01.ad.intershop.net>
References: <A03188C6623C0D46A703CB5AA59907F217067D@JENMAIL01.ad.intershop.net>
Message-ID: <40C5C2A2.9090209@optonline.net>

?tapply
?aggregate

   You probably have a data frame, not a matrix.

Marc Mamin wrote:
> I suppose this is a basic question but couldn't find a solution.:
> 
> I have a large matrix with let say 3 columns:
> 
> V1	V2	V3
> a	x	2
> a	x	4
> a	y	8
> b	z	16
> 
> and I want to compute some statistics based on 
> the levels resulting form the combination of the two first columns
> 
> e.g.:
> 
> SUM->
> 
> V1	V2	V3
> a	x	6
> a	y	8
> b	z	16

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From gavin.simpson at ucl.ac.uk  Tue Jun  8 16:00:12 2004
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Tue, 08 Jun 2004 15:00:12 +0100
Subject: [R] Computting statistics on a matrix with 2 factor column
In-Reply-To: <A03188C6623C0D46A703CB5AA59907F217067D@JENMAIL01.ad.intershop.net>
References: <A03188C6623C0D46A703CB5AA59907F217067D@JENMAIL01.ad.intershop.net>
Message-ID: <40C5C66C.5080607@ucl.ac.uk>

Marc Mamin wrote:
> Hello,
> 
> I suppose this is a basic question but couldn't find a solution.:
> 
> I have a large matrix with let say 3 columns:
> 
> V1	V2	V3
> a	x	2
> a	x	4
> a	y	8
> b	z	16
> 
> and I want to compute some statistics based on 
> the levels resulting form the combination of the two first columns
> 
> e.g.:
> 
> SUM->
> 
> V1	V2	V3
> a	x	6
> a	y	8
> b	z	16
> 
> 
> Thanks for your hints .
> 
> Marc
> 

?tapply and ?aggregate are two ways, with aggregate giving you something 
that more closely resembles what you asked for:

 > a <- factor(c("a","a","a","b"))
 > b <- factor(c("x","x","y","x"))
 > c <- c(2,4,8,16)
 > abc <- data.frame(a, b, c)
 > abc
   a b  c
1 a x  2
2 a x  4
3 a y  8
4 b x 16
 > tapply(abc$c, list(abc$a, abc$b), sum)
    x  y
a  6  8
b 16 NA
 > aggregate(abc$c, list(abc$a, abc$b), sum)
   Group.1 Group.2  x
1       a       x  6
2       b       x 16
3       a       y  8

HTH

Gavin

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpson at ucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From MSchwartz at MedAnalytics.com  Tue Jun  8 16:14:23 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Tue, 08 Jun 2004 09:14:23 -0500
Subject: [R] error during make of R-patched on Fedora core 2
In-Reply-To: <Pine.LNX.4.44.0406081217120.15138-100000@gannet.stats>
References: <Pine.LNX.4.44.0406081217120.15138-100000@gannet.stats>
Message-ID: <1086704063.2235.304.camel@localhost.localdomain>

On Tue, 2004-06-08 at 06:23, Prof Brian Ripley wrote:
> On Tue, 8 Jun 2004, Gavin Simpson wrote:
> 
> > Marc Schwartz wrote:
> > > On Mon, 2004-06-07 at 15:51, Gavin Simpson wrote:
> > > 
> > > snip

snip

> > > 
> > > 
> > > Perhaps I am being dense, but in reviewing the two documents (R Admin
> > > and the CRAN sources page), I think that the only thing lacking is a
> > > description on the CRAN page of the manual download option for the Rec
> > > packages.

snip

> > 
> > Yes, but having downloaded the contents of that directory (as VERSION 
> > indicated that R-patched was 1.9.1 alpha), the links to the source files 
> > for the Recommended packages or not present (obviously). And make 
> > doesn't seem to work without these links. The rsync approach places the 
> > package sources *and* the links in the correct directory.

Yep. I was being dense. Missed the symlink part of the process. My
error.

I also missed the venus transit this morning due to clouds...  :-(

> > So the instructions in the Admin manual are lacking a statement that you 
> > need to create links to each of the package sources in the following 
> > form name-of-package.tgz which links to name-of-package_version.tar.gz. 
> > As it stands, the instructions in the Installation & Admin manual are 
> > not sufficient to get the manual download method to work.
> 
> You need to run tools/link-recommended.  I've added that to R-admin.

Should Fritz also add that to the CRAN 'R Sources' page so that both
locations are in synch procedurally?

> > > Procedurally, I think that the rsync approach is substantially easier
> > > (one step instead of multiple downloads) and certainly less error prone.
> > > Also the ./tools/rsync-recommended script is set up to pick up the
> > > proper package versions, which also helps to avoid conflicts.
> > 
> > I agree - being a bit of a Linux newbie, I hadn't used rsync before. 
> > Seeing how easy it was to use this method of getting the required 
> > sources I will be using this method in future.
> 
> rsync is great, *provided* you have permission to use the ports it uses.  
> Users with http proxies often do not, hence the description of the manual 
> method.  During alpha/beta periods, we do make a complete tarball 
> available, and I wonder if we should not be doing so with 
> R-patched/R-devel at all times.

Good point on rsync. Perhaps another option to consider/suggest (though
it might complicate things) is to use wget. Since wget supports proxy
servers, etc. and can use http, it might be an alternative for folks.

The wget command syntax (assuming that your working dir is the main R
source dir) would be:

wget -r -l1 --no-parent -A*.gz -nd -P src/library/Recommended
http://www.cran.mirrors.pair.com/src/contrib/1.9.1/Recommended

The above _should_ be one one line, but of course will wrap here. There
should be a space " " between the two lines. The above will copy the tar
files (-A*.gz) from the server (-r -l1 --no-parent) to the appropriate
'Recommended' directory (-P), without recreating the source server's
tree (-nd).

One could refer the reader to 'man wget' or
http://www.gnu.org/software/wget/wget.html for further information on
how to use wget behind proxies and related issues.

You would then of course run the ./tools/link-recommended script to
create the symlinks, followed by ./configure and make.

HTH,

Marc



From bates at stat.wisc.edu  Tue Jun  8 16:59:58 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 08 Jun 2004 09:59:58 -0500
Subject: [R] GLMM(..., family=binomial(link="cloglog"))?
In-Reply-To: <40C5009A.90705@pdf.com>
References: <3A822319EB35174CA3714066D590DCD504AF7DFB@usrymx25.merck.com>
	<40C4B87B.7060301@pdf.com> <6r8yez88wo.fsf@bates4.stat.wisc.edu>
	<40C5009A.90705@pdf.com>
Message-ID: <6racze9jtd.fsf@bates4.stat.wisc.edu>

Spencer Graves <spencer.graves at pdf.com> writes:

> 	  Another GLMM/glmm problem:  I simulate rbinom(N, 100, pz),
> 	  where logit(pz) = rnorm(N).  I'd like to estimate the mean
> 	  and standard deviation of logit(pz).  I've tried GLMM{lme4},
> 	  glmmPQL{MASS}, and glmm{Jim Lindsey's repeated}.  In several
> 	  replicates of this for N = 10, 100, 500, etc., my glmm call
> 	  produced estimates of the standard deviation of the random
> 	  effect in the range of 0.6 to 0.8 (never as high as the 1
> 	  simulated).  Meanwhile, my calls to GLMM produced estimates
> 	  between 1e-12 and 1e-9, while the glmmPQL results tended to
> 	  be closer to 0.001, though it gave one number as high as
> 	  0.7.  (I'm running R 1.9.1 alpha, lme4 0.6-1 under Windows
> 	  2000)
> 
> 
> 	  Am I doing something wrong, or do these results suggest bugs
> 	  in the software or deficiencies in the theory or ... ?
> 
> 
> 	  Consider the following:
> 
>  > set.seed(1); N <- 10
>  > z <- rnorm(N)
>  > pz <- inv.logit(z)
>  > DF <- data.frame(z=z, pz=pz, y=rbinom(N, 100, pz)/100, n=100,
>  > smpl=factor(1:N))
> 
>  > GLMM(y~1, family=binomial, data=DF, random=~1|smpl, weights=n)
> Generalized Linear Mixed Model

Check the observed proportions in the data and see if they apparently
vary enough to be able to expect to estimate a random effect.

It is entirely possible to have the MLE of a variance component be
zero.

Another thing to do it to check the convergence.  Use

GLMM(y ~ 1, family = binomial, random = ~1|smpl, weigths = n, 
     control = list(EMv=TRUE, msV=TRUE))

or 

GLMM(y ~ 1, family = binomial, random = ~1|smpl, weigths = n, 
     control = list(EMv=TRUE, msV=TRUE, opt = 'optim'))

You will see that both optimizers push the precision of the random
effects to very large values (i.e. the variance going to zero) in the
second of the penalized least squares steps.

I think that this is a legitimate optimum for the approximate
problem.  It may be an indication that the approximate problem is not
the best one to use.  As George Box would tell us,

 You have a big approximation and a small approximation.  The big
 approximation is that your approximation to the problem you want to
 solve.  The small approximation is involved in getting the solution
 to the approximate problem.

For this case, even if I turn off the PQL iterations and go directly
to the Laplacian approximation I still get a near-zero estimate of the
variance component.  You can see the gory details with

GLMM(y ~ 1, family = binomial, random = ~1|smpl, weigths = n,
     control = list(EMv=TRUE, msV=TRUE, glmmMaxIter = 1), method = 'Laplace')

I am beginning to suspect that for these data the MLE of the variance
component is zero.

> 	  So far, the best tool I've got for this problem is a normal
> 	  probability plot of a transform of the binomial responses
> 	  with Monte Carlo confidence bands, as suggested by Venables
> 	  and Ripley, S Programming and Atkinson (1985).  However, I
> 	  ultimately need to estimate these numbers.

I think the most reliable method of fitting this particular form of a
GLMM is using adaptive Gauss-Hermite quadrature to evaluate the
marginal likelihood and optimizing that directly.  In this model the
marginal likelihood is a function of two parameters.  If you have
access to SAS I would try to fit these data with PROC NLMIXED and see
what that does.  You may also be able to use Goran Brostrom's package
for R on this model.  As a third option you could set up evaluation of
the marginal likelihood using either the Laplacian approximation to
the integral or your own version of adaptive Gauss-Hermite and look at
the contours of the marginal log-likelihood.



From bernd.weiss at uni-koeln.de  Tue Jun  8 17:06:05 2004
From: bernd.weiss at uni-koeln.de (Bernd Weiss)
Date: Tue, 08 Jun 2004 17:06:05 +0200
Subject: [R] Problems compiling Rd-Files
Message-ID: <40C5F1FD.4157.721022@localhost>

Dear all,

there has been a thread on missing Rd.sty, when converting Rd-files 
to DVI-files, see 
<http://maths.newcastle.edu.au/~rking/R/help/03b/8179.html>. 

This problem could be solved by editing Rd2div.sh manually, but then 
I got the following error message:

---[error]-------------------------------------------------------
 LaTeX Error: Command \middle already defined.
               Or name \end... illegal, see p.192 of the manual.

See the LaTeX manual or LaTeX Companion for explanation.
Type  H <return>  for immediate help.
 ...                                              
                                                  
l.45 \newlength{\middle}
                        
! Missing $ inserted.
<inserted text> 
.
.
.
for the full message see <http://www.metaanalyse.de/rd-error.txt>
---[error]-------------------------------------------------------

Again, there has been a thread on this topic at 
<http://tolstoy.newcastle.edu.au/R/help/04/03/1707.html>. 

Has anybody encountered the same problem? 

I am using R1.9 on a Windows 2000 system and a fully updated MikTeX. 

Any help will be appreciated!

Bernd



From spencer.graves at pdf.com  Tue Jun  8 17:15:41 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 08 Jun 2004 08:15:41 -0700
Subject: [R] GLMM(..., family=binomial(link="cloglog"))?
In-Reply-To: <x27jui8prf.fsf@biostat.ku.dk>
References: <3A822319EB35174CA3714066D590DCD504AF7DFB@usrymx25.merck.com>	<40C4B87B.7060301@pdf.com>
	<6r8yez88wo.fsf@bates4.stat.wisc.edu>	<40C5009A.90705@pdf.com>
	<x27jui8prf.fsf@biostat.ku.dk>
Message-ID: <40C5D81D.5030603@pdf.com>

Hi, Peter: 

      Thanks.  The help page on GLMM in lme4 0.6-1 2004/05/31 mentions 
GLMM(formula, family, data, random, ...) with additional arguments 
subset, method, na.action, control, and "model, x logicals".  I may try 
reading the source code. 

      On the other hand, my need is sufficiently specialized that I may 
just program the log(likelihood) for that specific model.  Then I can 
make contour and perspective plots of the log(likelihood) surface to 
examine thereby the adequacy of Wald's approximation for different 
parameterizations, as well as feed it to "optim" for estimation.  [My 
application requires binomial(link="cloglog") not "logit";  I used 
"logit" in the simulation below, because it is more commonly known and 
understood.] 

      Best Wishes,
      Spencer Graves

Peter Dalgaard wrote:

>Spencer Graves <spencer.graves at pdf.com> writes:
>
>  
>
>>Data: DF
>>  log-likelihood:  -55.8861
>>Random effects:
>>  Groups Name        Variance   Std.Dev.
>>  smpl   (Intercept) 1.7500e-12 1.3229e-06
>>
>>Estimated scale (compare to 1)  3.280753
>>
>>Fixed effects:
>>             Estimate Std. Error z value Pr(>|z|)
>>(Intercept) 0.148271   0.063419  2.3379  0.01939
>>
>>Number of Observations: 10
>>Number of Groups: 10
>>    
>>
>
>The only information you have about sigma is in the overdispersion
>of y, so you probably cannot both have a scale parameter and a random
>effect of sample in there. Doug did say something about fixing the
>scale in GLMM, but I forget whether he had implemented it or planned
>to...
>
>  
>



From spencer.graves at pdf.com  Tue Jun  8 17:32:24 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 08 Jun 2004 08:32:24 -0700
Subject: [R] GLMM(..., family=binomial(link="cloglog"))?
In-Reply-To: <6racze9jtd.fsf@bates4.stat.wisc.edu>
References: <3A822319EB35174CA3714066D590DCD504AF7DFB@usrymx25.merck.com>	<40C4B87B.7060301@pdf.com>
	<6r8yez88wo.fsf@bates4.stat.wisc.edu>	<40C5009A.90705@pdf.com>
	<6racze9jtd.fsf@bates4.stat.wisc.edu>
Message-ID: <40C5DC08.70806@pdf.com>

Hi, Doug: 

      Thanks.  I'll try the things you suggests.  The observed 
proportions ranged from roughly 0.2 to 0.8 in 100 binomial random 
samples where sigma is at most 0.05.  Jim Lindsey's "glmm" does 
Gauss-Hermite quadrature, but I don't know if it bothers with the 
adaptive step.  With it, I've seen estimates of the variance component 
ranging from 0.4 to 0.7 or so.  Since I simulated normal 0 standard 
deviation of 1, the algorithm was clearly underestimating what was 
simulated.  My next step, I think, is to program adaptive Gauss-Hermite 
quadrature for something closer to my real problem (as you just 
suggested), and see what I get. 

      You mentioned the little vs. big approximations:  My real 
application involves something close to a binomial response driven by 
Poisson defects, where the Poisson defect rate is not constant.  I've 
shown that it can make a difference whether the defect rate is lognormal 
or gamma, so that is another complication and another reason to write my 
own log(likelihood).  I've thought about writing my own function to do 
adaptive Gauss-Hermite quadrature, as you suggested, but decided to 
check more carefully the available tools before I jumped into my own 
software development effort. 

      Thanks again.
      Spencer Graves    

Douglas Bates wrote:

>Spencer Graves <spencer.graves at pdf.com> writes:
>
>  
>
>>	  Another GLMM/glmm problem:  I simulate rbinom(N, 100, pz),
>>	  where logit(pz) = rnorm(N).  I'd like to estimate the mean
>>	  and standard deviation of logit(pz).  I've tried GLMM{lme4},
>>	  glmmPQL{MASS}, and glmm{Jim Lindsey's repeated}.  In several
>>	  replicates of this for N = 10, 100, 500, etc., my glmm call
>>	  produced estimates of the standard deviation of the random
>>	  effect in the range of 0.6 to 0.8 (never as high as the 1
>>	  simulated).  Meanwhile, my calls to GLMM produced estimates
>>	  between 1e-12 and 1e-9, while the glmmPQL results tended to
>>	  be closer to 0.001, though it gave one number as high as
>>	  0.7.  (I'm running R 1.9.1 alpha, lme4 0.6-1 under Windows
>>	  2000)
>>
>>
>>	  Am I doing something wrong, or do these results suggest bugs
>>	  in the software or deficiencies in the theory or ... ?
>>
>>
>>	  Consider the following:
>>
>> > set.seed(1); N <- 10
>> > z <- rnorm(N)
>> > pz <- inv.logit(z)
>> > DF <- data.frame(z=z, pz=pz, y=rbinom(N, 100, pz)/100, n=100,
>> > smpl=factor(1:N))
>>
>> > GLMM(y~1, family=binomial, data=DF, random=~1|smpl, weights=n)
>>Generalized Linear Mixed Model
>>    
>>
>
>Check the observed proportions in the data and see if they apparently
>vary enough to be able to expect to estimate a random effect.
>
>It is entirely possible to have the MLE of a variance component be
>zero.
>
>Another thing to do it to check the convergence.  Use
>
>GLMM(y ~ 1, family = binomial, random = ~1|smpl, weigths = n, 
>     control = list(EMv=TRUE, msV=TRUE))
>
>or 
>
>GLMM(y ~ 1, family = binomial, random = ~1|smpl, weigths = n, 
>     control = list(EMv=TRUE, msV=TRUE, opt = 'optim'))
>
>You will see that both optimizers push the precision of the random
>effects to very large values (i.e. the variance going to zero) in the
>second of the penalized least squares steps.
>
>I think that this is a legitimate optimum for the approximate
>problem.  It may be an indication that the approximate problem is not
>the best one to use.  As George Box would tell us,
>
> You have a big approximation and a small approximation.  The big
> approximation is that your approximation to the problem you want to
> solve.  The small approximation is involved in getting the solution
> to the approximate problem.
>
>For this case, even if I turn off the PQL iterations and go directly
>to the Laplacian approximation I still get a near-zero estimate of the
>variance component.  You can see the gory details with
>
>GLMM(y ~ 1, family = binomial, random = ~1|smpl, weigths = n,
>     control = list(EMv=TRUE, msV=TRUE, glmmMaxIter = 1), method = 'Laplace')
>
>I am beginning to suspect that for these data the MLE of the variance
>component is zero.
>
>  
>
>>	  So far, the best tool I've got for this problem is a normal
>>	  probability plot of a transform of the binomial responses
>>	  with Monte Carlo confidence bands, as suggested by Venables
>>	  and Ripley, S Programming and Atkinson (1985).  However, I
>>	  ultimately need to estimate these numbers.
>>    
>>
>
>I think the most reliable method of fitting this particular form of a
>GLMM is using adaptive Gauss-Hermite quadrature to evaluate the
>marginal likelihood and optimizing that directly.  In this model the
>marginal likelihood is a function of two parameters.  If you have
>access to SAS I would try to fit these data with PROC NLMIXED and see
>what that does.  You may also be able to use Goran Brostrom's package
>for R on this model.  As a third option you could set up evaluation of
>the marginal likelihood using either the Laplacian approximation to
>the integral or your own version of adaptive Gauss-Hermite and look at
>the contours of the marginal log-likelihood.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From rolf at math.unb.ca  Tue Jun  8 17:46:35 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Tue, 8 Jun 2004 12:46:35 -0300 (ADT)
Subject: [R] George Box quote.
Message-ID: <200406081546.i58FkZH9014831@erdos.math.unb.ca>


Doug Bates wrote:

> As George Box would tell us,
> 
>  You have a big approximation and a small approximation.  The big
>  approximation is your approximation to the problem you want to
>  solve.  The small approximation is involved in getting the solution
>  to the approximate problem.

I asked Prof. Bates if he had a source or citation for that quote.
(I love it, and would like to make use of it.)  He said no.  Can
anyone out there give me a reference to it?

Thanks.
				cheers,

					Rolf Turner
					rolf at math.unb.ca



From junko_yano_ at hotmail.com  Tue Jun  8 18:07:21 2004
From: junko_yano_ at hotmail.com (=?iso-2022-jp?B?GyRCTHBMbhsoQiAbJEI9ZztSGyhC?=)
Date: Wed, 09 Jun 2004 01:07:21 +0900
Subject: [R] SJAVA error
Message-ID: <BAY8-F121uLfL4q1wsS00023456@hotmail.com>

Hi

I'm trying to use SJava and I have troubles. 
I try to run examples from "Calling R from Java"
but,I have an error that 
"fatal error: enable to open the base package"

I heard  SJAVA bug,
so,could you  send me your compiled SJava package with the modified 
REmbed.c because 
in Windows i'm not able to recompile!!!

--example
package org.omegahat.R.Java;

public class REvalSample {
	public static void main(String[] args) {
		String[] rargs = { "--slave", "--vanilla" };

		System.out.println("Java??R???????????");

		ROmegahatInterpreter interp =
			new ROmegahatInterpreter(
				ROmegahatInterpreter.fixArgs(rargs),
				false);
		REvaluator e = new REvaluator();

		Object val = e.eval("x <- sin(seq(0, 2*pi, length=30))");
		val = e.eval("x * 2.0");

		if (val != null) {
			double[] objects = (double[]) val;
			for (int i = 0; i < objects.length; i++) {
				System.err.println("(" + i + ") " + objects[i]);
			}
		}
	}
}
---------

Thank you 

------------
Junko Yano
E-mail : junko_yano_ at hotmail.com



From richard.kittler at amd.com  Tue Jun  8 18:19:51 2004
From: richard.kittler at amd.com (richard.kittler@amd.com)
Date: Tue, 8 Jun 2004 09:19:51 -0700
Subject: [R] Is there an R-version of rayplot
Message-ID: <858788618A93D111B45900805F85267A0BCB2D4E@caexmta3.amd.com>

I need to make plots similar to those produced by the s-plus rayplot function but can't seem to find it in R.  These 'vector maps' plot a ray or vector at each specified location. Is there something similar in R ? 

--Rich

Richard Kittler 
AMD TDG
408-749-4099



From manuellopezibanez at yahoo.es  Tue Jun  8 17:41:13 2004
From: manuellopezibanez at yahoo.es (=?ISO-8859-1?Q?Manuel_L=F3pez-Ib=E1=F1ez?=)
Date: Tue, 08 Jun 2004 17:41:13 +0200
Subject: [R] interaction plot with intervals based on TukeyHSD
Message-ID: <40C5DE19.9030706@yahoo.es>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040608/87a66216/attachment.pl

From daniela.marconi at libero.it  Tue Jun  8 18:38:04 2004
From: daniela.marconi at libero.it (daniela.marconi@libero.it)
Date: Tue,  8 Jun 2004 18:38:04 +0200
Subject: [R] clustalw
Message-ID: <HZ00VG$6990D78B874A6FEAFF1ED097D14491E3@libero.it>

Hi, 
I'm using the function clustalw in packages dna, but every time i have a segmentation fault!
In your opinion What is the problem?Memory?
Please help me!!!!
Daniela



From rossini at blindglobe.net  Tue Jun  8 18:43:21 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Tue, 08 Jun 2004 09:43:21 -0700
Subject: [R] clustalw
In-Reply-To: <HZ00VG$6990D78B874A6FEAFF1ED097D14491E3@libero.it> (daniela's
	message of "Tue,  8 Jun 2004 18:38:04 +0200")
References: <HZ00VG$6990D78B874A6FEAFF1ED097D14491E3@libero.it>
Message-ID: <85k6yi2e6u.fsf@servant.blindglobe.net>


Might be a compilation problem, or change of system libraries.


"daniela\.marconi\@libero\.it" <daniela.marconi at libero.it> writes:

> Hi, 
> I'm using the function clustalw in packages dna, but every time i have a segmentation fault!
> In your opinion What is the problem?Memory?
> Please help me!!!!
> Daniela
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From wolski at molgen.mpg.de  Tue Jun  8 18:46:14 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Tue, 08 Jun 2004 18:46:14 +0200
Subject: [R] clustalw
In-Reply-To: <HZ00VG$6990D78B874A6FEAFF1ED097D14491E3@libero.it>
References: <HZ00VG$6990D78B874A6FEAFF1ED097D14491E3@libero.it>
Message-ID: <200406081846140287.06603DF7@mail.math.fu-berlin.de>

Hi Daniela!

>From where you got this package?
I even havent found it at cran.
It is not a bioconductor package either.
and
search.help("clustalW")

In any case it is better to contact the package provider directly.

Sincerely Eryk


*********** REPLY SEPARATOR  ***********

On 6/8/2004 at 6:38 PM daniela.marconi at libero.it wrote:

>>>Hi, 
>>>I'm using the function clustalw in packages dna, but every time i have a
>>>segmentation fault!
>>>In your opinion What is the problem?Memory?
>>>Please help me!!!!
>>>Daniela
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



Dipl. bio-chem. Eryk Witold Wolski    @    MPI-Moleculare Genetic   
Ihnestrasse 63-73 14195 Berlin       'v'    
tel: 0049-30-83875219               /   \    
mail: wolski at molgen.mpg.de        ---W-W----    http://www.molgen.mpg.de/~wolski



From tplate at blackmesacapital.com  Tue Jun  8 18:49:09 2004
From: tplate at blackmesacapital.com (Tony Plate)
Date: Tue, 08 Jun 2004 10:49:09 -0600
Subject: [R] How to Describe R to Finance People
In-Reply-To: <200406080158.i581wb6d303940@atlas.otago.ac.nz>
References: <200406080158.i581wb6d303940@atlas.otago.ac.nz>
Message-ID: <6.1.0.6.2.20040608104105.043c2bc8@mailhost.blackmesacapital.com>

At Monday 07:58 PM 6/7/2004, Richard A. O'Keefe wrote:
>[snip]
>There are three perspectives on programming languages like the S/R family:
>(1) The programming language perspective.
>     I am sorry to tell you that the only excuse for R is S.
>     R is *weird*.  It combines error-prone C-like syntax with data structures
>     that are APL-like but not sufficiently* APL-like to have behaviour that
>     is easy to reason about.  The scope rules (certainly the scope rules for
>     S) were obviously designed by someone who had a fanatical hatred of
>     compilers and wanted to ensure that the language could never be usefully
>     compiled.

What in particular about the scope rules for S makes it tough for 
compilers?  The scope for ordinary variables seems pretty straightforward 
-- either local or in one of several global locations.  (Or are you 
referring to the feature of the get() function that it can access variables 
in any frame?)


>   Thanks to 'with' the R scope rules are little better.  The
>     fact that (object)$name returns NULL instead of reporting an error when
>     the object doesn't _have_ a $name property means that errors can be
>     delayed to the point where debugging is harder than it needs to be.

Yup, that's why I proposed (and provided an implementation) of an 
alternative "$$" operator that did report an error when object$$name didn't 
have a "name" component (and also didn't allow abbreviation), but there was 
no interest shown in incorporating this into R.

-- Tony Plate



From rdiaz at cnio.es  Tue Jun  8 18:48:53 2004
From: rdiaz at cnio.es (Ramon Diaz-Uriarte)
Date: Tue, 8 Jun 2004 18:48:53 +0200
Subject: [R] bootstrap: stratified resampling
Message-ID: <200406081848.53986.rdiaz@cnio.es>

Dear All,

I was writing a small wrapper to bootstrap a classification algorithm, but if 
we generate the indices in the "usual way" as:

bootindex <- sample(index, N, replace = TRUE)

there is a non-zero probability that all the samples belong to only 
one class, thus leading to problems in the fitting (or that some classes will 
end up with only one sample, which will be a problem for quadratic 
discriminant analysis).

It thought this situation should be frequent enough to be mentioned in the 
literature, but I have found almost no mention in the references I have 
available, except for Hirst (see below). If I've reread correctly, this issue 
is not mentioned in Efron & Tibshirani (1997; the .632+ paper), or in Efron 
and Gong (the TAS "leisure look" paper), or the Efron & Tibshirani 1993 
bootstrap book, or Chernick's "Bootstrap methods" book. I've only seen some 
side mentions in Ripley's Pattern recognition (when talking about stratified 
cross-validation), and Davison & Hinkley's bootstrap book when, on p. 304, 
they refer to some subsets having singular design matrices, and thus 
requiring stratification on covars. McLachlan (in his discriminant analysis 
book), on p. 347, differentiates between mixture sampling and separate 
sampling, but I can find a mention of what do when, under mixture sampling, we 
end up with all samples in only one group.

Only Hirst (1996, Technometrics, 38 (4): 389--399) says that each bootstrap 
sample should include at least one observation for each group, and at least 
enough different observations from each group to allow estimation of the 
covariance matrix (he is referring to discriminant analysis), and thus he 
uses essentially stratified bootstrap samples.

Interestingly, the "boot" function (boot library) says "For nonparametric 
multi-sample problems stratified resampling is used.". As well, the 
predab.resample (Design library) says  "group: a grouping variable used to 
stratify the sample upon bootstrapping. This allows one to handle k-sample 
problems, (...)".

That the authors of boot and Design are using stratified resampling indicates 
to me that this might be the obvious, unproblematic way to go, but I 
understood that stratified resampling was OK only when that was sampling 
scheme that generated the data.  

What am I missing?

Thanks,

R.


-- 
Ram??n D??az-Uriarte
Bioinformatics Unit
Centro Nacional de Investigaciones Oncol??gicas (CNIO)
(Spanish National Cancer Center)
Melchor Fern??ndez Almagro, 3
28029 Madrid (Spain)
Fax: +-34-91-224-6972
Phone: +-34-91-224-6900

http://bioinfo.cnio.es/~rdiaz
PGP KeyID: 0xE89B3462
(http://bioinfo.cnio.es/~rdiaz/0xE89B3462.asc)



From hodgess at gator.uhd.edu  Tue Jun  8 19:11:04 2004
From: hodgess at gator.uhd.edu (Erin Hodgess)
Date: Tue, 8 Jun 2004 12:11:04 -0500
Subject: [R] off topic publication question
Message-ID: <200406081711.i58HB4q23066@gator.dt.uh.edu>

Dear R People:

Please excuse the off topic question, but I
know that I'll get a good answer here.


If a single author is writing a journal article,
should she use "We performed a test"
or "I performed a test",
please?

I had learned to use "we" without regard to the number
of authors.  Is that true, please?

Thanks for the off topic help.

Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu



From rossini at blindglobe.net  Tue Jun  8 19:13:06 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Tue, 08 Jun 2004 10:13:06 -0700
Subject: [R] clustalw
In-Reply-To: <200406081846140287.06603DF7@mail.math.fu-berlin.de>
	(wolski@molgen.mpg.de's
	message of "Tue, 08 Jun 2004 18:46:14 +0200")
References: <HZ00VG$6990D78B874A6FEAFF1ED097D14491E3@libero.it>
	<200406081846140287.06603DF7@mail.math.fu-berlin.de>
Message-ID: <851xkq2ct9.fsf@servant.blindglobe.net>


It's on Jim Lindsey's WWW site.  It's a nice package.  We've got an
extension that is somewhat incomplete that incorporates BLAST results
(from NCBI, though could config for local use) in a package called
BioSeq1 on Bioconductor's dev site -- I don't think that the CVS
viewer is available, but I could provide a tarball for builds (might
need a bit of hacking in its current state) if there is interest.

Email me privately if so.

best,
-tony


"Wolski" <wolski at molgen.mpg.de> writes:

> Hi Daniela!
>
>>From where you got this package?
> I even havent found it at cran.
> It is not a bioconductor package either.
> and
> search.help("clustalW")
>
> In any case it is better to contact the package provider directly.
>
> Sincerely Eryk
>
>
> *********** REPLY SEPARATOR  ***********
>
> On 6/8/2004 at 6:38 PM daniela.marconi at libero.it wrote:
>
>>>>Hi, 
>>>>I'm using the function clustalw in packages dna, but every time i have a
>>>>segmentation fault!
>>>>In your opinion What is the problem?Memory?
>>>>Please help me!!!!
>>>>Daniela
>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>
> Dipl. bio-chem. Eryk Witold Wolski    @    MPI-Moleculare Genetic   
> Ihnestrasse 63-73 14195 Berlin       'v'    
> tel: 0049-30-83875219               /   \    
> mail: wolski at molgen.mpg.de        ---W-W----    http://www.molgen.mpg.de/~wolski
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From j.van_den_hoff at fz-rossendorf.de  Tue Jun  8 19:13:21 2004
From: j.van_den_hoff at fz-rossendorf.de (joerg van den hoff)
Date: Tue, 08 Jun 2004 19:13:21 +0200
Subject: [R] scoping rules
Message-ID: <40C5F3B1.2060507@fz-rossendorf.de>

is there a good way to get the following fragment to work when calling 
it as "wrapper(1)" ?

#======== cut here==========
wrapper <- function (choose=0)
{
   x <- seq(0,2*pi,len=100)
   y <- sin(1.5*x);
   y <- rnorm(y,y,.1*max(y))

   if (choose==0) {
      rm(fifu,pos=1)
      fifu <- function(w,x) {sin(w*x)}
   }
   else
      assign('fifu',function(w,x) {sin(w*x)},.GlobalEnv)

   res <- nls(y ~ fifu(w,x),start=list(w=1))
   res
}
#======== cut here==========

I understand, the problem is that  the scoping rules are such that "nls" 
does not resolve 'fifu' in the parent environment, but rather in the 
GlobalEnv. (this is different for the data, which *are* taken from the 
parent environment of the nls-call).

The solution to "assign" 'fifu' directly into the GlobalEnv (which 
happens when calling 'wrapper(1)") does obviously work but leads to the 
undesirable effect of accumulating objects in the workspace which are 
not needed there (and might overwrite existing ones).

so: is there a way to enforce that "nls" takes the model definition from 
the parent environment together with the data?

joerg



From mhassan at scitegic.com  Tue Jun  8 19:21:14 2004
From: mhassan at scitegic.com (Moises Hassan)
Date: Tue, 8 Jun 2004 10:21:14 -0700
Subject: [R] binary data
Message-ID: <830D8D4719112B418ABBC3A0EBA9581272EA1D@webmail.scitegic.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040608/7fd2af38/attachment.pl

From ps-list at masny.dk  Tue Jun  8 19:23:26 2004
From: ps-list at masny.dk (Peter Sebastian Masny)
Date: Tue, 8 Jun 2004 10:23:26 -0700
Subject: [R] Comparing two pairs of non-normal datasets in R?
Message-ID: <200406081023.27157.ps-list@masny.dk>

Hi all,

I'm using R to analyze some research and I'm not sure which test would be 
appropriate for my data.  I was hoping someone here might be able to help.

Short version:
Evaluate null hypothesis that change A1->A2 is similar to change C1->C2, for 
continuous, non-normal datasets.


Long version:

I have two populations A and C.  I take a measurement on samples of these 
populations before and after a process.  So basically I have:
A1 - sample of A before process
A2 -  sample of A after process
C1 - sample of C (control) before process
C2 - sample of C (control) after process

The data is continuous and I have about 100 measurements in each dataset.  
Also, the data is not normally distributed (more like a Poisson).

By Wilcoxon Rank Sum, A1 is significantly different than A2 and C1 is 
different than C2.

Here is the problem:
C1 is only slightly different than C2 (Wilcoxon, p<.02), while A1 is more 
noticeably different than A2 (p<1E-22).  What I would like to do is assume 
that the changes seen in C are typical, and evaluate the changes in A 
relative to the changes in C (i.e. are the changes greater?).

Any thoughts?



Thanks,
Peter Masny



From p.dalgaard at biostat.ku.dk  Tue Jun  8 19:40:19 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 08 Jun 2004 19:40:19 +0200
Subject: [R] error during make of R-patched on Fedora core 2
In-Reply-To: <1086704063.2235.304.camel@localhost.localdomain>
References: <Pine.LNX.4.44.0406081217120.15138-100000@gannet.stats>
	<1086704063.2235.304.camel@localhost.localdomain>
Message-ID: <x2oenuc5j0.fsf@biostat.ku.dk>

Marc Schwartz <MSchwartz at medanalytics.com> writes:

> wget -r -l1 --no-parent -A*.gz -nd -P src/library/Recommended
> http://www.cran.mirrors.pair.com/src/contrib/1.9.1/Recommended
> 
> The above _should_ be one one line, but of course will wrap here. There
> should be a space " " between the two lines.

Kids these days... Make that

wget -r -l1 --no-parent -A*.gz -nd -P src/library/Recommended \
 http://www.cran.mirrors.pair.com/src/contrib/1.9.1/Recommended

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From MSchwartz at MedAnalytics.com  Tue Jun  8 19:54:21 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Tue, 08 Jun 2004 12:54:21 -0500
Subject: [R] error during make of R-patched on Fedora core 2
In-Reply-To: <x2oenuc5j0.fsf@biostat.ku.dk>
References: <Pine.LNX.4.44.0406081217120.15138-100000@gannet.stats>
	<1086704063.2235.304.camel@localhost.localdomain>
	<x2oenuc5j0.fsf@biostat.ku.dk>
Message-ID: <1086717261.2235.373.camel@localhost.localdomain>

On Tue, 2004-06-08 at 12:40, Peter Dalgaard wrote:
> Marc Schwartz <MSchwartz at medanalytics.com> writes:
> 
> > wget -r -l1 --no-parent -A*.gz -nd -P src/library/Recommended
> > http://www.cran.mirrors.pair.com/src/contrib/1.9.1/Recommended
> > 
> > The above _should_ be one one line, but of course will wrap here. There
> > should be a space " " between the two lines.
> 
> Kids these days... Make that
> 
> wget -r -l1 --no-parent -A*.gz -nd -P src/library/Recommended \
>  http://www.cran.mirrors.pair.com/src/contrib/1.9.1/Recommended

LOL

Thanks Dad  ;-)

Marc



From laura at env.leeds.ac.uk  Tue Jun  8 19:56:34 2004
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Tue, 8 Jun 2004 18:56:34 +0100 (BST)
Subject: [R] overlaying text() onto image() or filled.contour()
Message-ID: <Pine.LNX.4.44.0406081850540.17972-100000@env-pc-phd13>

I am trying to add some markers onto a contour map image I have created,
by using the text() function when I have already produced the map using
either image() or filled.contour(). For some reason the points appear to
be shifted considerably to the right of where they should be appearing,
despite me using exactly the same co-ordinate systems for both. This
offset is also dependent on the aspect ratio I use.

The map I am looking at is around 3.5x larger in height than width and I
need to maximise this in an X window. If I simply use asp=1 the map is
pretty unreadable, is there a way I can drastically reduce the margin
sizes perhaps? The par() commands I at ve tried haven;t made an appreciable
difference - and i'M still very puzzled as to why my data markers are
appearing in the wrong place...any suggestions?

Thanks,
Laura



From spencer.graves at pdf.com  Tue Jun  8 20:36:41 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 08 Jun 2004 11:36:41 -0700
Subject: [R] Comparing two pairs of non-normal datasets in R?
In-Reply-To: <200406081023.27157.ps-list@masny.dk>
References: <200406081023.27157.ps-list@masny.dk>
Message-ID: <40C60739.2040905@pdf.com>

      Have you considered "qqplot(A1, A2)" and "qqplot(C1, C2)"?  If A2, 
A2, C1, C2 are "more like Poisson", I might try "qqplot(sqrt(A1), 
sqrt(A2))", etc.:  Without the "sqrt", the image might be excessively 
distorted by largest values, at least in my experience. 

      hope this helps.  spencer graves

Peter Sebastian Masny wrote:

>Hi all,
>
>I'm using R to analyze some research and I'm not sure which test would be 
>appropriate for my data.  I was hoping someone here might be able to help.
>
>Short version:
>Evaluate null hypothesis that change A1->A2 is similar to change C1->C2, for 
>continuous, non-normal datasets.
>
>
>Long version:
>
>I have two populations A and C.  I take a measurement on samples of these 
>populations before and after a process.  So basically I have:
>A1 - sample of A before process
>A2 -  sample of A after process
>C1 - sample of C (control) before process
>C2 - sample of C (control) after process
>
>The data is continuous and I have about 100 measurements in each dataset.  
>Also, the data is not normally distributed (more like a Poisson).
>
>By Wilcoxon Rank Sum, A1 is significantly different than A2 and C1 is 
>different than C2.
>
>Here is the problem:
>C1 is only slightly different than C2 (Wilcoxon, p<.02), while A1 is more 
>noticeably different than A2 (p<1E-22).  What I would like to do is assume 
>that the changes seen in C are typical, and evaluate the changes in A 
>relative to the changes in C (i.e. are the changes greater?).
>
>Any thoughts?
>
>
>
>Thanks,
>Peter Masny
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From jmc at research.bell-labs.com  Tue Jun  8 20:52:28 2004
From: jmc at research.bell-labs.com (John Chambers)
Date: Tue, 08 Jun 2004 14:52:28 -0400
Subject: [R] Lazy Evaluation?
References: <Pine.LNX.4.58.0406071409590.1687@spock.vulcan>
Message-ID: <40C60AEC.F4EAC337@research.bell-labs.com>

No, not lazy evaluation.  The explanation has to do with environments
and how callGeneric works.

It's an interesting (if obscure) example.

Here's the essence of it, embedded in some comments on debugging and on
style.  (This will be a  fairly long discussion, I'm afraid.)

A useful starting point in debugging is often to look at the objects
involved.  In this case there are two versions of a method that
generates an object containing a function.

In the first version, which works, the object generated in the example
is:

> sinNF
An object of class "NumFunction"
Slot "fun":
function(n) nfun(n)
<environment: 0x2f5c268>

In the second, which gets into an infinite loop, it is:

> sinNF
An object of class "NumFunction"
Slot "fun":
function(n) callGeneric(x at fun(n))
<environment: 0x3ada884>

The first version is totally inscrutable to the user who looks at the
object.

But a little thinking will suggest why the second one fails.  The
`callGeneric' function is meant to be used inside a method definition. 
But sinNF at fun isn't a method; it's not obvious what callGeneric should
do but it probably won't be good.

(It's arguagle that it should fail right away because the function being
used is not a generic.  But even legitimate uses of callGeneric can
accidentally get into infinite loops by in effect ending up with the
same method on the same data.)

We can verify the infinite loop by using the trace() function to catch
calls to callGeneric.

> trace(callGeneric, recover)

Then:

> sinNF at fun(sqrt(pi/2))
Tracing callGeneric(x at fun(n)) on entry 

Enter a frame number, or 0 to exit   
1:sinNF at fun(sqrt(pi/2)) 
2:callGeneric(x at fun(n)) 
Selection: 0
Tracing callGeneric(x at fun(n)) on entry 

Enter a frame number, or 0 to exit   
1:sinNF at fun(sqrt(pi/2)) 
2:callGeneric(x at fun(n)) 
3:eval(call, sys.frame(sys.parent())) 
4:eval(expr, envir, enclos) 
5:sinNF at fun(x at fun(n)) 
6:callGeneric(x at fun(n)) 

and so on.

But why does the first version work?  Because that nfun function is
defined inside another function where a local definition of the name of
the generic is stored in its environment.  So the call from nfun to
callGeneric does find the generic (sin in this case).  To figure out
that this happens however, would require a lot of analysis.

This is definitely NOT the sort of arcane knowledge users are expected
to apply.

There was a discussion on this mailing list recently of the recommended
style that could be called "functional".  The definition of a function
should make sense on its own and, in particular, should avoid depending
on external objects, particularly external objects that may be changed. 
The methods in the example are very far from this style.

Both versions of sinNF are essentially incomprehensible on their own,
and the one that works more so.  Not meant as a criticism, it was
remarkable that you arrived at a version that did work.

But if possible one would like to get to the same effect with an object
that makes sense.  In this example, it's possible to use some of the
same information to construct an object that says what it does.  I think
you want to compose the function currently in the object with the
function you're calling.  By making the computation a method for the
Math group generic, you get all the math functions to work this way in
one step (clever, though rather unintuitive for the user).

To produce a clearer (and more efficient) version, get the .Generic
evaluated when the object is created; e.g., 

setMethod("Math", 
          "NumFunction",
          function(x){
            f <- function(n){g <- gg; f(g(n))}
            body(f) <- substitute(
                          {g <- gg; f(g(n))},
                           list(f = as.name(.Generic), gg = x at fun))
            NumFunction(f)
          })

Then the object sinNF makes sense to the user:

> sinNF
An object of class "NumFunction"
Slot "fun":
function (n) 
{
    g <- function (x) 
    x^2
    sin(g(n))
}
<environment: 0x384c8e8>

> sinNF at fun(sqrt(pi/2))
[1] 1

Thomas Stabla wrote:
> 
> Hello,
> 
> I've stumbled upon following problem, when trying to overload the methods
> for group Math for an S4-class which contains functions as slots.
> 
>   setClass("NumFunction", representation = list(fun = "function"))
> 
>   NumFunction <- function(f) new("NumFunction", fun = f)
> 
>   square <- function(x) x^2
>   NF <- NumFunction(square)
> 
>   setMethod("Math",
>             "NumFunction",
>             function(x){
>                 nfun  <- function(n) callGeneric(x at fun(n))
>                 tmp <- function(n) nfun(n)
>                 NumFunction(tmp)
>             })
> 
>   sinNF <- sin(NF)
>   sinNF at fun(sqrt(pi/2))
> 
> # works as expected, returns 1
> 
> # now a slightly different version of setMethod("Math", "NumFunction",
> # ...), which dispenses the "unnecessary" wrapper function tmp()
> 
>   setMethod("Math",
>             "NumFunction",
>             function(x){
>                 nfun  <- function(n) callGeneric(x at fun(n))
>                 return(NumFunction(nfun))
>             })
> 
>    sinNF <- sin(NF)
>    sinNF at fun(sqrt(pi/2))
> 
> # produces an error, namely:
> # Error in typeof(fdef) : evaluation nested too deeply: infinite
> # recursion / options(expression=)?
> 
> Replacing the generating function NumFunction() with corresponding
> new(..) calls doesn't change the outcome.
> 
> When I call the newly defined functions nfun resp. tmp from within the
> function body of setMethod(), e.g.
> 
>   setMethod("Math",
>             "NumFunction",
>             function(x){
>                 nfun  <- function(n) callGeneric(x at fun(n))
>                 cat(nfun(1))
>                 NumFunction(nfun)
>             })
> 
> by tmp(1) resp. nfun(1), both versions "cat" correct output when called by
> sin(NF).
> 
> If I don't return NumFunction(tmp) resp. NumFunction(nfun) but tmp resp.
> nfun, both versions work just fine, i.e. sin(NF)(sqrt(pi/2)) returns 1.
> 
> Would someone explain me this behavior? (R Version 1.9.0)
> 
> Best regards,
> Thomas Stabla
> 
> Sourcecode can be found at:
> http://www.uni-bayreuth.de/departments/math/org/mathe7/DISTR/NumFun.R
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

-- 
John M. Chambers                  jmc at bell-labs.com
Bell Labs, Lucent Technologies    office: (908)582-2681
700 Mountain Avenue, Room 2C-282  fax:    (908)582-3340
Murray Hill, NJ  07974            web: http://www.cs.bell-labs.com/~jmc



From ps-list at masny.dk  Tue Jun  8 21:14:28 2004
From: ps-list at masny.dk (Peter Sebastian Masny)
Date: Tue, 8 Jun 2004 12:14:28 -0700
Subject: [R] Comparing two pairs of non-normal datasets in R?
In-Reply-To: <200406081743.i58HhlTd020394@erdos.math.unb.ca>
References: <200406081743.i58HhlTd020394@erdos.math.unb.ca>
Message-ID: <200406081214.28716.ps-list@masny.dk>

On Tuesday 08 June 2004 10:43 am, you wrote:
> If I understand you correctly, you have two set of ***paired***
> data, one set from the A population, and one from the C population.
>
> Form the pairwise differences:
>
> 	A.diff <- A1 - A2
> 	C.diff <- C1 - C2

Alas, they are not paired.  A1 and A2 are samples from the same population, 
but of different members.  Also, the number of measurements is different for 
each dataset.

> Boxplots and histograms of A.diff and C.diff will tell you
> (much more than a test ever would) what's ***really*** going on.

The boxplots I have clearly show the difference, but I need a p value to go 
with it.

Here are the boxplots if that helps:
http://www.ps.masny.dk/guests/misc/A1.png
http://www.ps.masny.dk/guests/misc/A2.png
http://www.ps.masny.dk/guests/misc/C1.png
http://www.ps.masny.dk/guests/misc/C2.png

> P.S. BTW --- you say that your data are continuous, but that their
> distributions are ``more like a Poisson''.  The Poisson distribution
> is DISCRETE!!!

Hence the "like".  The data is indeed continuous, but a distribution graph 
increases towards one extreme...

Visually, the results are convincing, but I really need a test of 
significance.



Thank you very much for the help,

Peter



From jferrer at ivic.ve  Tue Jun  8 21:44:54 2004
From: jferrer at ivic.ve (jferrer@ivic.ve)
Date: Tue, 8 Jun 2004 15:44:54 -0400 (VET)
Subject: [R] vardiag Package and nlregb
Message-ID: <Pine.LNX.4.44.0406081523590.5147-100000@jotaerre.ivic.ve>

Hi everyone,

I'm interested in the analysis of spatial data, and I'm trying out several
R-packages.

Today I was attempting to use the package vardiag (version 0.1):

>> library(vardiag)
>> rs4.vo <- varobj(rs4[,2:4],trace=2)
>[1] 1
>Error: couldn't find function "nlregb"

so far I know "nlregb" is a S-plus function for optimization, so this can
not work in R, other?

How could one create or import an appropiate "variogram object" to use the
other functions in this package, for example if I estimate the empirical
variogram using geoR?

thanks,

JR Ferrer-Paris

PS: Currently I'm using R 1.9.0 on Linux.


Dipl.-Biol. J.R. Ferrer Paris ~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Laboratorio de Biolog??a de Organismos - Centro de Ecolog??a
   Instituto Venezolano de Investigaciones Cient??ficas
             Apartado 21827 - Caracas 1020A
           REPUBLICA BOLIVARIANA DE VENEZUELA
     Tel:00-58-212-5041452 --- Fax: 00-58-212-5041088
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ jferrer at ivic.ve



From vograno at evafunds.com  Tue Jun  8 21:23:58 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Tue, 8 Jun 2004 12:23:58 -0700
Subject: [R] fast mkChar
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A5568739@phost015.EVAFUNDS.intermedia.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040608/90047992/attachment.pl

From spencer.graves at pdf.com  Tue Jun  8 21:34:36 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 08 Jun 2004 12:34:36 -0700
Subject: [R] Comparing two pairs of non-normal datasets in R?
In-Reply-To: <200406081214.28716.ps-list@masny.dk>
References: <200406081743.i58HhlTd020394@erdos.math.unb.ca>
	<200406081214.28716.ps-list@masny.dk>
Message-ID: <40C614CC.9000604@pdf.com>

      It looks like "ks.test" might help, but it seems to me you need to 
make parsimonious models of the change in distributions, so you estimate 
a few parameters for the distributions of each of your 4 data sets with 
standard errors for all the parameters you estimate.  Then you can test 
if the change in the estimated parameters in going from A1 -> A2 exceeds 
the change going from C1 -> C2 using z scores or t tests of the 
differences in the parameter estimates -- or chi-squares / F's, etc., if 
you want to do multiple dimensions all at once. 

      hope this helps.  spencer graves

Peter Sebastian Masny wrote:

>On Tuesday 08 June 2004 10:43 am, you wrote:
>  
>
>>If I understand you correctly, you have two set of ***paired***
>>data, one set from the A population, and one from the C population.
>>
>>Form the pairwise differences:
>>
>>	A.diff <- A1 - A2
>>	C.diff <- C1 - C2
>>    
>>
>
>Alas, they are not paired.  A1 and A2 are samples from the same population, 
>but of different members.  Also, the number of measurements is different for 
>each dataset.
>
>  
>
>>Boxplots and histograms of A.diff and C.diff will tell you
>>(much more than a test ever would) what's ***really*** going on.
>>    
>>
>
>The boxplots I have clearly show the difference, but I need a p value to go 
>with it.
>
>Here are the boxplots if that helps:
>http://www.ps.masny.dk/guests/misc/A1.png
>http://www.ps.masny.dk/guests/misc/A2.png
>http://www.ps.masny.dk/guests/misc/C1.png
>http://www.ps.masny.dk/guests/misc/C2.png
>
>  
>
>>P.S. BTW --- you say that your data are continuous, but that their
>>distributions are ``more like a Poisson''.  The Poisson distribution
>>is DISCRETE!!!
>>    
>>
>
>Hence the "like".  The data is indeed continuous, but a distribution graph 
>increases towards one extreme...
>
>Visually, the results are convincing, but I really need a test of 
>significance.
>
>
>
>Thank you very much for the help,
>
>Peter
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From pcampbell at econ.bbk.ac.uk  Tue Jun  8 21:56:22 2004
From: pcampbell at econ.bbk.ac.uk (Phineas Campbell)
Date: Tue, 8 Jun 2004 20:56:22 +0100
Subject: [R] off topic publication question
In-Reply-To: <200406081711.i58HB4q23066@gator.dt.uh.edu>
Message-ID: <NGECIFANPOJAGABBAEAPGEBADBAA.pcampbell@econ.bbk.ac.uk>

I had assumed that the use of we in articles was either due to formality,
like the distinction between tu and vous in French.  The English monarch
never refer to themselves in the singular.  Or we as in both the author and
the reader.  However a sample of size 4 of articles to hand suggests that
the use of we for single author papers is not universal.

HTH
Phineas Campbell
http://www.phineas.pwp.blueyonder.co.uk/


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Erin Hodgess
Sent: Tuesday, June 08, 2004 6:11 PM
To: r-help at stat.math.ethz.ch
Subject: [R] off topic publication question


Dear R People:

Please excuse the off topic question, but I
know that I'll get a good answer here.


If a single author is writing a journal article,
should she use "We performed a test"
or "I performed a test",
please?

I had learned to use "we" without regard to the number
of authors.  Is that true, please?

Thanks for the off topic help.

Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From jcmartinez at banxico.org.mx  Tue Jun  8 22:10:28 2004
From: jcmartinez at banxico.org.mx (=?iso-8859-1?Q?Mart=EDnez_Ovando_Juan_Carlos?=)
Date: Tue, 8 Jun 2004 15:10:28 -0500
Subject: [R] a doubt
Message-ID: <ED7E0E44EAADFB46A6ABA12A647C1306122D170D@CORREOINT.banxico.org.mx>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040608/cba3e370/attachment.pl

From p.murrell at auckland.ac.nz  Tue Jun  8 22:11:52 2004
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Wed, 09 Jun 2004 08:11:52 +1200
Subject: [R] overlaying text() onto image() or filled.contour()
References: <Pine.LNX.4.44.0406081850540.17972-100000@env-pc-phd13>
Message-ID: <40C61D88.5030507@stat.auckland.ac.nz>

Hi


Laura Quinn wrote:
> I am trying to add some markers onto a contour map image I have created,
> by using the text() function when I have already produced the map using
> either image() or filled.contour(). For some reason the points appear to
> be shifted considerably to the right of where they should be appearing,
> despite me using exactly the same co-ordinate systems for both. This
> offset is also dependent on the aspect ratio I use.


See example three in help(filled.contour) - you can use the plot.axes 
argument.


> The map I am looking at is around 3.5x larger in height than width and I
> need to maximise this in an X window. If I simply use asp=1 the map is
> pretty unreadable, is there a way I can drastically reduce the margin
> sizes perhaps? The par() commands I at ve tried haven;t made an appreciable
> difference - and i'M still very puzzled as to why my data markers are
> appearing in the wrong place...any suggestions?


par(mar=whatever) has some effect - although par("mar")[4] does get 
overridden to make a space between the plot and the legend.

Paul
-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From ripley at stats.ox.ac.uk  Tue Jun  8 22:20:13 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 8 Jun 2004 21:20:13 +0100 (BST)
Subject: [R] a doubt
In-Reply-To: <ED7E0E44EAADFB46A6ABA12A647C1306122D170D@CORREOINT.banxico.org.mx>
Message-ID: <Pine.LNX.4.44.0406082119010.1277-100000@gannet.stats>

?system
?shell (under Windows, which we must guess you are using if `DOS' means 
MS-DOS).

Please use an informative subject line.

On Tue, 8 Jun 2004, Mart??nez Ovando Juan Carlos wrote:

> I'm working with different models that where implemented in DOS system
> each one, but for doing comparisons between them I require to put some
> of the results in an unified data form for all the models. I wonder if
> some of you have the knowledge of one function in R that enable me the
> invocation of DOS execution files inside the R application to allow me
> doing an R interface with such programs, i.e. some function like the
> existed 'dos()' function in Matlab, which execute '.exe' files from DOS,
> after have created the corresponding specification file in plain format.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Tue Jun  8 22:23:24 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 08 Jun 2004 22:23:24 +0200
Subject: [R] fast mkChar
In-Reply-To: <C698D707214E6F4AB39AB7096C3DE5A5568739@phost015.EVAFUNDS.intermedia.net>
References: <C698D707214E6F4AB39AB7096C3DE5A5568739@phost015.EVAFUNDS.intermedia.net>
Message-ID: <x2k6yhdcjn.fsf@biostat.ku.dk>

"Vadim Ogranovich" <vograno at evafunds.com> writes:

> Hi,
>  
> To speed up reading of large (few million lines) CSV files I am writing
> custom read functions (in C). By timing various approaches I figured out
> that one of the bottlenecks in reading character fields is the mkChar()
> function which on each call incurs a lot of garbage-collection-related
> overhead.
>  
> I wonder if there is a "vectorized" version of mkChar, say mkChar2(char
> **, int length) that converts an array of C strings to a string vector,
> which somehow amortizes the gc overhead over the entire array?
>  
> If no such function exists, I'd appreciate any hint as to how to write
> it.

The real issue here is that character vectors are implemented as
generic vectors of little R objects (CHARSXP type) that each hold one
string. Allocating all those objects is probably what does you in.

The reason behind the implementation is probably that doing it that
way allows the mechanics of the garbage collector to be applied
directly (CHARSXPs are just vectors of bytes), but it is obviously
wasteful in terms of total allocation. If you can think up something
better, please say so (but remember that the memory management issues
are nontrivial).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From dmurdoch at pair.com  Tue Jun  8 22:32:36 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Tue, 08 Jun 2004 16:32:36 -0400
Subject: [R] fast mkChar
In-Reply-To: <C698D707214E6F4AB39AB7096C3DE5A5568739@phost015.EVAFUNDS.intermedia.net>
References: <C698D707214E6F4AB39AB7096C3DE5A5568739@phost015.EVAFUNDS.intermedia.net>
Message-ID: <9t7cc0dg3228vsdthhqsich9gdbjfuphcs@4ax.com>

On Tue, 8 Jun 2004 12:23:58 -0700, "Vadim Ogranovich"
<vograno at evafunds.com> wrote :

>Hi,
> 
>To speed up reading of large (few million lines) CSV files I am writing
>custom read functions (in C). By timing various approaches I figured out
>that one of the bottlenecks in reading character fields is the mkChar()
>function which on each call incurs a lot of garbage-collection-related
>overhead.
> 
>I wonder if there is a "vectorized" version of mkChar, say mkChar2(char
>**, int length) that converts an array of C strings to a string vector,
>which somehow amortizes the gc overhead over the entire array?
> 
>If no such function exists, I'd appreciate any hint as to how to write
>it.

It's not easy.  Internally R strings always have a header at the
front, so you need to allocate memory and move C strings to get R to
understand them.  

Duncan Murdoch



From jcmartinez at banxico.org.mx  Tue Jun  8 22:35:47 2004
From: jcmartinez at banxico.org.mx (=?iso-8859-1?Q?Mart=EDnez_Ovando_Juan_Carlos?=)
Date: Tue, 8 Jun 2004 15:35:47 -0500
Subject: [R] (no subject)
Message-ID: <ED7E0E44EAADFB46A6ABA12A647C1306122D18B6@CORREOINT.banxico.org.mx>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040608/ce8841fa/attachment.pl

From jcmartinez at banxico.org.mx  Tue Jun  8 22:36:49 2004
From: jcmartinez at banxico.org.mx (=?iso-8859-1?Q?Mart=EDnez_Ovando_Juan_Carlos?=)
Date: Tue, 8 Jun 2004 15:36:49 -0500
Subject: [R] a doubt
Message-ID: <ED7E0E44EAADFB46A6ABA12A647C1306122D18D6@CORREOINT.banxico.org.mx>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040608/1d99151d/attachment.pl

From jgentry at jimmy.harvard.edu  Tue Jun  8 22:38:38 2004
From: jgentry at jimmy.harvard.edu (Jeff Gentry)
Date: Tue, 8 Jun 2004 16:38:38 -0400 (EDT)
Subject: [R] (no subject)
In-Reply-To: <ED7E0E44EAADFB46A6ABA12A647C1306122D18B6@CORREOINT.banxico.org.mx>
Message-ID: <Pine.SOL.4.20.0406081638120.18153-100000@santiam.dfci.harvard.edu>

> implemented in MS-DOS to. The question for you is if you know about
> the existence of an R function that allows me to run -in Windows-
> executable files in MS-DOS from the R command window.

Does system() do what you're looking for?



From epsobolik at wrightsoft.com  Tue Jun  8 23:23:49 2004
From: epsobolik at wrightsoft.com (Philip Sobolik)
Date: Tue, 08 Jun 2004 17:23:49 -0400
Subject: [R] data.frame size limit
Message-ID: <6.1.0.6.0.20040608172142.027a3c00@moltok.wrightsoft.com>

Is there a limit to the number of columns that a data.frame can have?  For 
example, can I read.csv() a file that has 1000 columns and 10,000 rows, 
will it break or is it limited by available memory.

...................................................................................
Philip Sobolik                  781-862-8719 x111
Wrightsoft Corporation          781-861-2058 fax
394 Lowell Street, Suite 12     epsobolik at wrightsoft.com
Lexington, MA   02420           www.wrightsoft.com



From spencer.graves at pdf.com  Tue Jun  8 23:33:22 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 08 Jun 2004 14:33:22 -0700
Subject: [R] a doubt
In-Reply-To: <ED7E0E44EAADFB46A6ABA12A647C1306122D18D6@CORREOINT.banxico.org.mx>
References: <ED7E0E44EAADFB46A6ABA12A647C1306122D18D6@CORREOINT.banxico.org.mx>
Message-ID: <40C630A2.1050708@pdf.com>

      All of R is available in source from "www.r-project.org".  This 
includes software for ARIMA, etc. 

PLEASE do read the posting guide! "http://www.R-project.org/posting-guide.html";  it describes several things you can do that will get you closer to what you want and will help you formulate a question that might be easier for people on this list to understand and respond to.  

	  Espero que esto le ayuda.  Buena Suerte.  
	  spencer graves

Mart??nez Ovando Juan Carlos wrote:

>Hello again,
>
> 
>
>In a previous message I request your help, but I don't have been clear in my problem. Specifically, I'm trying to create an interface in R for the X-12-ARIMA and TRAMO SEATS, for the versions that run in MS-DOS. This problem awake in me the interest for make interfaces to comparing some Bayesian models for classification that where implemented in MS-DOS to. The question for you is if you know about the existence of an R function that allows me to run -in Windows- executable files in MS-DOS from the R command window. 
>
> 
>
>The function 'dos' in Matlab that I have mention in the previous message allows to run '.exe' programs for a specific directory.  
>
> 
>
>Many thanks.      
>
> 
>
>Saludos,
>
> 
>
>            Juan Carlos
>
> 
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From vograno at evafunds.com  Tue Jun  8 23:56:45 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Tue, 8 Jun 2004 14:56:45 -0700
Subject: [R] fast mkChar
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A5568755@phost015.EVAFUNDS.intermedia.net>

I am no expert in memory management in R so it's hard for me to tell
what is and what is not doable. From reading the code of allocVector()
in memory.c I think that the critical part is to vectorize
CLASS_GET_FREE_NODE and use the vectorized version along the lines of
the code fragment below (taken from memory.c).

	if (node_class < NUM_SMALL_NODE_CLASSES) {
	    CLASS_GET_FREE_NODE(node_class, s); 

If this is possible than the rest is just a matter of code refactoring.

By vectorizing I mean writing a macro CLASS_GET_FREE_NODE2(node_class,
s, n) which in one go allocates n little objects of class node_class and
"inscribes" them into the elements of vector s, which is assumed to be
long enough to hold these objects.

If this is doable than the only missing piece would be a new function
setChar(CHARSXP rstr, const char * cstr) which copies 'cstr' into 'rstr'
and (re)allocates the heap memory if necessary. Here the setChar() macro
is safe since s[i]-s are all brand new and thus are not shared with any
other object.



> -----Original Message-----
> From: Peter Dalgaard [mailto:p.dalgaard at biostat.ku.dk] 
> Sent: Tuesday, June 08, 2004 1:23 PM
> To: Vadim Ogranovich
> Cc: R-Help
> Subject: Re: [R] fast mkChar
> 
> "Vadim Ogranovich" <vograno at evafunds.com> writes:
> 
> > Hi,
> >  
> > To speed up reading of large (few million lines) CSV files I am 
> > writing custom read functions (in C). By timing various 
> approaches I 
> > figured out that one of the bottlenecks in reading 
> character fields is 
> > the mkChar() function which on each call incurs a lot of 
> > garbage-collection-related overhead.
> >  
> > I wonder if there is a "vectorized" version of mkChar, say 
> > mkChar2(char **, int length) that converts an array of C 
> strings to a 
> > string vector, which somehow amortizes the gc overhead over 
> the entire array?
> >  
> > If no such function exists, I'd appreciate any hint as to 
> how to write 
> > it.
> 
> The real issue here is that character vectors are implemented 
> as generic vectors of little R objects (CHARSXP type) that 
> each hold one string. Allocating all those objects is 
> probably what does you in.
> 
> The reason behind the implementation is probably that doing 
> it that way allows the mechanics of the garbage collector to 
> be applied directly (CHARSXPs are just vectors of bytes), but 
> it is obviously wasteful in terms of total allocation. If you 
> can think up something better, please say so (but remember 
> that the memory management issues are nontrivial).
> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: 
> (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: 
> (+45) 35327907
> 
>



From ivo.welch at yale.edu  Wed Jun  9 00:34:04 2004
From: ivo.welch at yale.edu (ivo welch)
Date: Tue, 08 Jun 2004 18:34:04 -0400
Subject: [R] fighting with ps.options and xlim/ylim
Message-ID: <40C63EDC.9080802@yale.edu>


sorry to impose again. 

At the default point size, R seems very good in selecting nice xlim/ylim 
parameters, and leaving a little bit of space at the edges of its 
xlim/ylim.  alas, I now need to create ps graphics that can only occupy 
a quarter of a page, so I need to blow up the text for readability.  
Easy, I thought: make ps.options(pointsize=24).  Alas, this turns out to 
be trickier than I thought.

In plot, autogenerated xlim and ylim should now probably be scaled a 
little, though more importantly, there needs to be more space at the 
edge (e.g., if ylim=c(1,2), R seems to really draw axes from about 0.9 
to 1.1).  if I do not increase this space, my axis label names overflow, 
as do some text() annotations that are inside xlim/ylim, but have 
pos=1.  (e.g., text(1,1,"something", pos=1) in the example; at standard 
point size, this fits nicely; just no longer.)

how to create smaller figures than full page is probably not an 
infrequent need.  has anyone written a "smallpostscript-figures" 
package, which sets this and perhaps other parameters?

if not, how do I tell R that

     * the usual space it leaves at the xlim/ylim needs to be bigger now?

    * that I would like it to be more generous/less generous in its 
autogeneration of good ylim/xlim default coordinates

help appreciated.

regards,

/iaw



From p.dalgaard at biostat.ku.dk  Wed Jun  9 00:34:36 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 09 Jun 2004 00:34:36 +0200
Subject: [R] fast mkChar
In-Reply-To: <C698D707214E6F4AB39AB7096C3DE5A5568755@phost015.EVAFUNDS.intermedia.net>
References: <C698D707214E6F4AB39AB7096C3DE5A5568755@phost015.EVAFUNDS.intermedia.net>
Message-ID: <x23c551xxf.fsf@biostat.ku.dk>

"Vadim Ogranovich" <vograno at evafunds.com> writes:

> I am no expert in memory management in R so it's hard for me to tell
> what is and what is not doable. From reading the code of allocVector()
> in memory.c I think that the critical part is to vectorize
> CLASS_GET_FREE_NODE and use the vectorized version along the lines of
> the code fragment below (taken from memory.c).
> 
> 	if (node_class < NUM_SMALL_NODE_CLASSES) {
> 	    CLASS_GET_FREE_NODE(node_class, s); 
> 
> If this is possible than the rest is just a matter of code refactoring.
> 
> By vectorizing I mean writing a macro CLASS_GET_FREE_NODE2(node_class,
> s, n) which in one go allocates n little objects of class node_class and
> "inscribes" them into the elements of vector s, which is assumed to be
> long enough to hold these objects.
> 
> If this is doable than the only missing piece would be a new function
> setChar(CHARSXP rstr, const char * cstr) which copies 'cstr' into 'rstr'
> and (re)allocates the heap memory if necessary. Here the setChar() macro
> is safe since s[i]-s are all brand new and thus are not shared with any
> other object.

I had a similar idea initially, but I don't think it can fly: First,
allocating n objects at once is not likely to be much faster than
allocating them one-by-one, especially when you consider the
implications of having to deal with near-out-of-memory conditions.
Second, you have to know the string lengths when allocating, since the
structure of a vector object (CHARSXP) is a header immediately
followed by the data.

A more interesting line to pursue is that - depending on what it
really is that you need - you might be able to create a different kind
of object that could "walk and quack" like a character vector, but is
stored differently internally. E.g. you could set up a representation
that is just a block of pointers, pointing to strings that are being
maintained in malloc-style.

Have a look at External pointers and finalization.


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From jasont at indigoindustrial.co.nz  Wed Jun  9 00:54:29 2004
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Wed, 09 Jun 2004 10:54:29 +1200
Subject: [R] off topic publication question
In-Reply-To: <200406081711.i58HB4q23066@gator.dt.uh.edu>
References: <200406081711.i58HB4q23066@gator.dt.uh.edu>
Message-ID: <1086735268.5175.36.camel@linux.site>

On Wed, 2004-06-09 at 05:11, Erin Hodgess wrote:
> If a single author is writing a journal article,
> should she use "We performed a test"
> or "I performed a test",
> please?


Does the particular journal specify a style manual or style sheet?  The
Chicago Manual of Style \cite{Chicago2003}, p160, says "'We' is
sometimes used by an individual who is speaking for a group... called
the editorial 'we'.  Some writers also use 'we' to make their prose
appear less personal and to draw in the reader or listener."

Although journals may vary in their requirements, you are presumably
speaking for a group (the University of Houston Downtown), and "we" is
probably safer than "I".  Probably.

It's important to note that the Chicago Manual does *not* claim to be
authoritative, merely helpful.
 
Cheers

Jason

@Book{Chicago2003,
  editor =	 {University of Chicago Press Staff},
  title = 	 {The Chicago Manual of Style},
  publisher = 	 {University of Chicago Press},
  year = 	 2003,
  address =	 {Chicago},
  edition =	 {Fifteenth}
}



From p.connolly at hortresearch.co.nz  Wed Jun  9 01:23:49 2004
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Wed, 9 Jun 2004 11:23:49 +1200
Subject: [R] off topic publication question
In-Reply-To: <NGECIFANPOJAGABBAEAPGEBADBAA.pcampbell@econ.bbk.ac.uk>; from
	pcampbell@econ.bbk.ac.uk on Tue, Jun 08, 2004 at 08:56:22PM +0100
References: <200406081711.i58HB4q23066@gator.dt.uh.edu>
	<NGECIFANPOJAGABBAEAPGEBADBAA.pcampbell@econ.bbk.ac.uk>
Message-ID: <20040609112349.U2137@hortresearch.co.nz>

On Tue, 08-Jun-2004 at 08:56PM +0100, Phineas Campbell wrote:

|> I had assumed that the use of we in articles was either due to
|> formality, like the distinction between tu and vous in French.  The
|> English monarch never refer to themselves in the singular.  

Hence the use of the term "The Royal We" when referring to a
purportedly group decision that was really made by one individual who
is attempting to hide that fact.  In the case of an author, the
attempt could be to appear more modest.

We are not all prima donnas.  :-)


-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From MSchwartz at MedAnalytics.com  Wed Jun  9 02:22:45 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Tue, 08 Jun 2004 19:22:45 -0500
Subject: [R] fighting with ps.options and xlim/ylim
In-Reply-To: <40C63EDC.9080802@yale.edu>
References: <40C63EDC.9080802@yale.edu>
Message-ID: <1086740565.29898.55.camel@localhost.localdomain>

On Tue, 2004-06-08 at 17:34, ivo welch wrote:
> sorry to impose again. 
> 
> At the default point size, R seems very good in selecting nice xlim/ylim 
> parameters, and leaving a little bit of space at the edges of its 
> xlim/ylim.  alas, I now need to create ps graphics that can only occupy 
> a quarter of a page, so I need to blow up the text for readability.  
> Easy, I thought: make ps.options(pointsize=24).  Alas, this turns out to 
> be trickier than I thought.
> 
> In plot, autogenerated xlim and ylim should now probably be scaled a 
> little, though more importantly, there needs to be more space at the 
> edge (e.g., if ylim=c(1,2), R seems to really draw axes from about 0.9 
> to 1.1).  if I do not increase this space, my axis label names overflow, 
> as do some text() annotations that are inside xlim/ylim, but have 
> pos=1.  (e.g., text(1,1,"something", pos=1) in the example; at standard 
> point size, this fits nicely; just no longer.)
> 
> how to create smaller figures than full page is probably not an 
> infrequent need.  has anyone written a "smallpostscript-figures" 
> package, which sets this and perhaps other parameters?
> 
> if not, how do I tell R that
> 
>      * the usual space it leaves at the xlim/ylim needs to be bigger now?
> 
>     * that I would like it to be more generous/less generous in its 
> autogeneration of good ylim/xlim default coordinates
> 
> help appreciated.
> 
> regards,
> 
> /iaw

Ivo,

The default parameters for plots, specifically par("xaxs") and
par("yaxs") are set in such a fashion as to extend the axis ranges by 4%
(relative to the actual data ranges) in each direction. The default
setting being "r".

Try for example:

> plot(c(0, 1), c(0, 1))
> par("usr")
[1] -0.04  1.04 -0.04  1.04

par("usr") shows the current actual ranges of the plot region.

Now try:

> plot(c(0, 1), c(0, 1), xaxs = "i", yaxs = "i")
> par("usr")
[1] 0 1 0 1

The use of "i" sets the xlim and ylim values exactly to the range of the
x and y points given.

Conceptually, if you need to extend the xlim and ylim values to
something other than the exact ranges or the ranges extended by 4%, you
can use something like:

plot(x, y, xlim = range(x) * adj.x, ylim = range(y) * adj.y)

where adj.x and adj.y are adjustment factors for the axis range limits.

Example values might be:

adj.x <- c(.9, 1.1)
adj.y <- c(.8, 1.2)

which would extend the x axis by 10% and the y axis by 20%.

The exact implementation may be dependent upon the rest of your code and
other factors, but the concept remains the same.

The question I would have for you is what are you doing with the
graphics. Are you creating a 2 x 2 matrix of graphics, each one being a
quarter page, or are you simply scaling a single plot so that it would
fit to a quarter page?

If the former, you can use something like par(mfrow = c(2, 2)), which
would set up a 2 x 2 matrix within a single plotting region in the
device. If you need the overall area to be square, you could also set
par(pty = "s"). See ?par for more information and some of the examples
at the bottom of the help page.

If the latter, simply create an EPS file [see the Details section of
?postscript for the proper arguments to postscript()] that can then be
included in, for example, a LaTeX file or a word processing document
file. In this case, you can scale the size of the overall plot within
the document page, without having to worry about how large the original
plot is. That is one of the advantages of using a vector based plot
rather than a bitmap plot. They can be scaled as needed. That being
said, you might just need to play around with certain parameters like
the plot margins [ie. par("mar")], to ensure that there is enough room
for your labels and other text components. Without seeing your actual
plot, it is hard to give specific recommendations.

If you are indeed using LaTeX, you might want to review
www.ctan.org/tex-archive/info/epslatex.pdf, which provides some guidance
on how to include graphics in LaTeX files and related issues.

HTH,

Marc Schwartz



From ivo.welch at yale.edu  Wed Jun  9 03:18:34 2004
From: ivo.welch at yale.edu (ivo welch)
Date: Tue, 08 Jun 2004 21:18:34 -0400
Subject: [R] fighting with ps.options and xlim/ylim
In-Reply-To: <1086740565.29898.55.camel@localhost.localdomain>
References: <40C63EDC.9080802@yale.edu>
	<1086740565.29898.55.camel@localhost.localdomain>
Message-ID: <40C6656A.2090401@yale.edu>


thank you, marc.  I will play around with these parameters tomorrow at 
my real computer.  yes, the idea is to just create an .eps and .pdf 
file, which is then \includegraphics[0.25\textwidth]{} in pdflatex.  I 
need to tweak with the parameter ps.options(pointsize) because 
otherwise, I end up with 5pt fonts---which is not readable.  And once I 
do this, I need different R parameter defaults on the axes.  With the 
advice I have gotten, I think I am all set now.  However, I am a little 
bit surprised that noone has written a package around this task---there 
must be many people that have to produce quarter-page (or half-page) 
graphics, and probably everyone is tweaking plot parameters a bit 
differently.  It would be nice to build in some of this intelligence 
into plot parameters, themselves.   of course, R is a free volunteer 
effort, and I am grateful for all the stuff that has been done already.

/iaw



From ivo.welch at yale.edu  Wed Jun  9 03:22:32 2004
From: ivo.welch at yale.edu (ivo welch)
Date: Tue, 08 Jun 2004 21:22:32 -0400
Subject: [R] more obvious contribution mechanism?
Message-ID: <40C66657.7010408@yale.edu>


can we put a "how to donate money to R" on the R webpage?  perhaps with 
a paypal button?

even better, because I would like to donate some funds from my research 
budget, could the R-project possibly sell some trinkets for a high price 
for support?  it is difficult to explain to a non-profit org (like yale) 
why i want to donate its money to another non-profit org.  they would be 
happy to pay $1,000 for S, of course, but not to donate $100 for R.  
buying a blank CD for $100 is much easier.

regards,

/iaw



From dmurdoch at pair.com  Wed Jun  9 04:02:45 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Tue, 08 Jun 2004 22:02:45 -0400
Subject: [R] fighting with ps.options and xlim/ylim
In-Reply-To: <40C6656A.2090401@yale.edu>
References: <40C63EDC.9080802@yale.edu>
	<1086740565.29898.55.camel@localhost.localdomain>
	<40C6656A.2090401@yale.edu>
Message-ID: <lorcc0dgmpajr608478bb73m89qn326uac@4ax.com>

On Tue, 08 Jun 2004 21:18:34 -0400, ivo welch <ivo.welch at yale.edu>
wrote:
> And once I 
>do this, I need different R parameter defaults on the axes.  With the 
>advice I have gotten, I think I am all set now.  However, I am a little 
>bit surprised that noone has written a package around this task---there 
>must be many people that have to produce quarter-page (or half-page) 
>graphics, and probably everyone is tweaking plot parameters a bit 
>differently. 

My general strategy for this is to change the width and height used in
the pdf() or postscript() device call, then just trust the defaults
chosen by R.  For inclusion in a paper, I generally specify sizes
about twice as big as I really want, and get text size similar to the
printed text.  So in your case, assuming a page is around 6 inches
wide, I'd use something like

pdf(width=3, height=3, ...)

and then get LaTeX to shrink it to half the size.

Duncan Murdoch



From MSchwartz at MedAnalytics.com  Wed Jun  9 04:14:53 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Tue, 08 Jun 2004 21:14:53 -0500
Subject: [R] fighting with ps.options and xlim/ylim
In-Reply-To: <40C6656A.2090401@yale.edu>
References: <40C63EDC.9080802@yale.edu>
	<1086740565.29898.55.camel@localhost.localdomain>
	<40C6656A.2090401@yale.edu>
Message-ID: <1086747293.29898.66.camel@localhost.localdomain>

On Tue, 2004-06-08 at 20:18, ivo welch wrote:
> thank you, marc.  I will play around with these parameters tomorrow at 
> my real computer.  yes, the idea is to just create an .eps and .pdf 
> file, which is then \includegraphics[0.25\textwidth]{} in pdflatex.  I 
> need to tweak with the parameter ps.options(pointsize) because 
> otherwise, I end up with 5pt fonts---which is not readable.  And once I 
> do this, I need different R parameter defaults on the axes.  With the 
> advice I have gotten, I think I am all set now.  However, I am a little 
> bit surprised that noone has written a package around this task---there 
> must be many people that have to produce quarter-page (or half-page) 
> graphics, and probably everyone is tweaking plot parameters a bit 
> differently.  It would be nice to build in some of this intelligence 
> into plot parameters, themselves.   of course, R is a free volunteer 
> effort, and I am grateful for all the stuff that has been done already.
> 
> /iaw


You might want to try to set the 'height' and 'width' arguments for
postscript() to something larger than the defaults. For example, use 6 x
6 (if square) and then use your code above to scale the plot down to
size. That might help with your font size and spacing problem, rather
than adjusting the point size.

I don't have a 'rule of thumb', but experience suggests that downsizing
a plot that is too big is better than upsizing one that is too small,
especially for a partial page.

I have done some other things using the 'seminar' LaTeX package for
landscape orientation slides and there I generally use the exact size
for the EPS files. But that is generally the only time that I do that.

YMMV,

Marc



From MSchwartz at MedAnalytics.com  Wed Jun  9 04:26:24 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Tue, 08 Jun 2004 21:26:24 -0500
Subject: [R] fighting with ps.options and xlim/ylim
In-Reply-To: <lorcc0dgmpajr608478bb73m89qn326uac@4ax.com>
References: <40C63EDC.9080802@yale.edu>
	<1086740565.29898.55.camel@localhost.localdomain>
	<40C6656A.2090401@yale.edu>
	<lorcc0dgmpajr608478bb73m89qn326uac@4ax.com>
Message-ID: <1086747984.29898.77.camel@localhost.localdomain>

On Tue, 2004-06-08 at 21:02, Duncan Murdoch wrote:
> On Tue, 08 Jun 2004 21:18:34 -0400, ivo welch <ivo.welch at yale.edu>
> wrote:
> > And once I 
> >do this, I need different R parameter defaults on the axes.  With the 
> >advice I have gotten, I think I am all set now.  However, I am a little 
> >bit surprised that noone has written a package around this task---there 
> >must be many people that have to produce quarter-page (or half-page) 
> >graphics, and probably everyone is tweaking plot parameters a bit 
> >differently. 
> 
> My general strategy for this is to change the width and height used in
> the pdf() or postscript() device call, then just trust the defaults
> chosen by R.  For inclusion in a paper, I generally specify sizes
> about twice as big as I really want, and get text size similar to the
> printed text.  So in your case, assuming a page is around 6 inches
> wide, I'd use something like
> 
> pdf(width=3, height=3, ...)
> 
> and then get LaTeX to shrink it to half the size.
> 
> Duncan Murdoch


I just got Duncan's msg, so I think that we are thinking along the same
lines here.

I agree with Duncan's suggestion relative to trying a 2x scaling factor
and would see how that goes with your particular plot. Then adjust if
need be as you develop some intuition.

Marc



From jasont at indigoindustrial.co.nz  Wed Jun  9 05:32:43 2004
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Wed, 09 Jun 2004 15:32:43 +1200
Subject: [R] more obvious contribution mechanism?
In-Reply-To: <40C66657.7010408@yale.edu>
References: <40C66657.7010408@yale.edu>
Message-ID: <1086751962.5414.6.camel@linux.site>

On Wed, 2004-06-09 at 13:22, ivo welch wrote:
> can we put a "how to donate money to R" on the R webpage?  perhaps with 
> a paypal button?
> 

Does the R Foundation link meet this need?

http://www.r-project.org/foundation/main.html

Cheers

Jason



From ok at cs.otago.ac.nz  Wed Jun  9 07:55:10 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Wed, 9 Jun 2004 17:55:10 +1200 (NZST)
Subject: [R] How to Describe R to Finance People
Message-ID: <200406090555.i595tAQT331559@atlas.otago.ac.nz>

I wrote:
	> The scope rules (certainly the scope rules for S) were obviously
	> designed by someone who had a fanatical hatred of compilers and
	> wanted to ensure that the language could never be usefully
	> compiled.
Drat!  I forgot the semi-smiley!

Tony Plate <tplate at blackmesacapital.com> wrote:
	What in particular about the scope rules for S makes it tough
	for compilers?  The scope for ordinary variables seems pretty
	straightforward -- either local or in one of several global
	locations.

One of *several* global locations.
attach() is the big one.
I spent a couple of months trying to design a compiler that would
respect all the statements about variable access in "The (New) S
Programming Language" book.

	(Or are you referring to the feature of the get()
	function that it can access variables in any frame?)
	
That's part of it too.  Worse still is that you can create and
delete variables dynamically in any frame.  (Based on my reading of
the blue book, not on experiments with an S system.)  And on one
fairly natural reading of the blue book,

    f <- function (x) {
	z <- y "this is a global reference"
	y <- z + 1
	g()
	y/z "this is a local reference"
    }

whether a reference inside a function is local or global would be 
a dynamic* property even if the function g() couldn't zap the local
definition of y.
	
R has its own problems, like the fact than with(thingy, expression)
introduces local variables and you don't know WHICH local variables
until you look at the run-time value of thingy.  Actual transcript:

    > z <- 12
    > f <- function (d) with(d, z)
    > f(list())
    [1] 12
    > f(list(z=27))
    [1] 27

Is that reference to z a reference to the global z or to a local copy
of d$z?  We don't know until we find out whether d *has* a $z property.
How are you supposed to compile a language where you don't know which
variable references are global and which are local?

(Answer: it can be done, but it ain't pretty!)

In this trivial example, it's obvious that the intent is for z to come
from d, but (a) it isn't an error if it doesn't and (b) realistic examples
have more variables, some of which are often visible without 'with'.



From ripley at stats.ox.ac.uk  Wed Jun  9 08:25:11 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 9 Jun 2004 07:25:11 +0100 (BST)
Subject: [R] data.frame size limit
In-Reply-To: <6.1.0.6.0.20040608172142.027a3c00@moltok.wrightsoft.com>
Message-ID: <Pine.LNX.4.44.0406090720290.6505-100000@gannet.stats>

On Tue, 8 Jun 2004, Philip Sobolik wrote:

> Is there a limit to the number of columns that a data.frame can have? 

Yes.  A data frame is a list, and a list is limited to 2^31-1 items.

>  For 
> example, can I read.csv() a file that has 1000 columns and 10,000 rows, 
> will it break or is it limited by available memory.

It will not `break', but it may fail with an error message, depending how 
much the `available memory' is.  R will run in 16Mb RAM but this problem 
will not.

Both ?read.table and the R Data Import/Export Manual give hints on how to 
read large tables efficiently, and it may well be necessary to follow 
them.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tmulholl at bigpond.net.au  Wed Jun  9 08:58:14 2004
From: tmulholl at bigpond.net.au (Tom Mulholland)
Date: Wed, 9 Jun 2004 14:58:14 +0800
Subject: [R] Help with a Lattice plot that fails with an empty unique
	combination
Message-ID: <000201c44def$238619d0$2202a8c0@ACER>

While using Lattice I received the following error.

Error in if (xx != 0) xx/10 else z/10 : argument is of length zero
In addition: Warning messages:
1: is.na() applied to non-(list or vector) in: is.na(x)
2: is.na() applied to non-(list or vector) in: is.na(x)
3: no finite arguments to min; returning Inf
4: no finite arguments to max; returning -Inf
5: NaNs produced in: log(x, base)
Can anyone point me in the right direction.

I have included a reproducible example from part of the data. The problem
appears to be the lack of data in column POH which I can see when I cross
tab the data

xtabs(IWD ~ about + SOA,data=test1) (from the dummy example below)

            SOA
about        ARON CRDA DCJH DJKT DEGF DOPC FACO FRNE HHW MEPR NGA OTHE OTRE
JHG POH
  non-I NO G   0    1    0    0  443    0    2  172  486  96   73 233  217
408   0
  I NO G       1    0    0    0  251    0    0   15  545  29    0   8  698
207   0
  non-I ERT    0  435  153   66  152   17   15    7   85 233   32  15  147
216   0
  I ERT       12    1    0    0  227    0    0    2  234  98    0   5  211
17   0
            SOA
about        PKJI SPOC YTGB SUCH
  non-I NO G 527    1  945  160
  I NO G     555    0  135   45
  non-I ERT  556    0  805    1
  I ERT      592    0  205  224
>

I normally subset data and drop unused factors at that stage. But my
investigations during the creation of this sample, especially when I was
creatiing the dummy dataset and found that i needed to isolate the problem
before I could create one that would duplicate the problem, indicate that it
was not just one NA doing the damage. I replaced all the NAs in the IWD with
1s and it still did not work, because the relevvent SOA varaibles were also
NA.

Aha, I thought, time to reread the lattice documentation to see what I have
missed (it will probably be in there and I've missed it, but that's another
story.) I noted the subset option but I can't see how one would filter out
the specific variations that need to be dropped.

Here's my sample to replicate the problem.

test1 <- structure(list(IWD = c(5, 3, 15, 2, 40, 1,
20, 40, 45, 50, 35, 33, 33, 3, 50, 14, 139, 139, 13, 3, 1, 1,
127, 1, 14, 1, 4, 3, 3, 3, 3, 3, 21, 51, 35, 33, 11, 11, 11,
6, 1, 7, 6, 8, 1, 1, 1, 4, 14, 6, 3, 34, 25, 3, 34, 1, 10, 14,
7, 35, 4, 13, 7, 45, 13, 7, 45, 13, 7, 45, 13, 7, 45, 13, 7,
7, 45, 13, 3, 13, 106, 4, 9, 1, 7, 40, 20, 24, 12, 2, 7, 1, 7,
6, 30, 18, NA, 2, 2, 4, 17, 4, 48, 4, 4, 1, 29, 17, 29, 2, 4,
6, 12, 3, 2, 17, 23, 65, 65, 6, 2, 23, 5, 2, 1, 4, 4, 4, 64,
24, 4, 47, 1, 44, 17, 24, 25, 57, 9, 27, 19, 7, 1, 1, 1, 4, 23,
8, 10, 10, 19, 21, 19, 21, 17, 9, 25, 12, 65, 10, 2, 2, 12, 1,
1, 19, 19, 19, 84, 6, 4, 84, 1, 7, 3, 16, 2, 84, 4, 1, 1, 1,
28, 1, 65, 10, 10, 10, 9, 8, 32, 32, 1, 15, 9, 4, 17, 3, 3, 6,
1, 12, 12, 16, 18, 17, 1, 61, 3, 3, 3, 3, 7, 1, 2, 13, NA, 49,
155, 31, 1, 43, 3, 10, 32, 65, 17, 287, 7, 55, 12, 2, 1, 12,
12, 12, 13, 43, 43, 8, 34, 14, 4, 14, 4, 32, 36, 12, 21, 13,
6, 6, 6, 6, 6, 16, 25, 25, 3, 4, 1, 1, 65, 54, 54, 72, 25, 36,
3, 26, 26, 3, 13, 111, 26, 13, NA, NA, 3, 1, 12, 1, 1, 22, 1,
25, 64, 72, 72, 1, 22, 2, 12, 3, 13, 139, 7, 1, 16, 115, 11,
54, 11, 49, 4, 1, 1, 1, 7, 11, 3, 2, 157, 4, 4, 9, 3, 15, 15,
9, 4, 19, 53, 53, 60, 221, 107, 107, 53, 3, 7, 173, NA, NA, 31,
38, 38, 74, 14, NA, NA, 92, 53, 14, 107, 107, 15, 38, 14, 53,
74, 47, 47, 7, 3, 49, 42, 74, 33, 3, 8, 4, 31, 26, 26, 26, 33,
2, 5, 5, 8, 29, 4, 94, 94, 3, 75, 1, NA, 131, 5, 11, 75, 12,
107, 74, 9, NA, NA, NA, NA, 22, 20, 1, 3, 2, 237, 237, 1, 1,
36, 36, 26, 45, 9, 12, 82, 72, 62, 198, 198, 1, 1, 8, 72, 72,
2, 2, 107, 3, 2, 2, 72, 104, 11, 197, 197, 142, 2, 198, 3, 20,
107, 107, 50, 72, 20, 72, NA, 82, 8, 118, 9, 3, 72, 24, 72, 50,
NA, 13, 107, 16, 11, 4, 8, 49, 237, 21, 201, 201, 201, 201, 176,
176, 2), SOA = structure(as.integer(c(16, 9,
16, 16, 10, 14, 10, 10, 14, 13, 14, 16, 16, 18, 13, 18, 18, 18,
18, 18, 16, 5, 5, 1, 14, 2, 12, 18, 18, 18, 16, 16, 11, 16, 13,
16, 8, 8, 8, 16, 18, 5, 14, 14, 5, 5, 5, 16, 16, 12, 19, 16,
14, 13, 14, 19, 13, 12, 16, 14, 13, 18, 5, 18, 18, 5, 18, 18,
5, 18, 18, 5, 18, 18, 5, 5, 18, 18, 16, 14, 16, 16, 9, 2, 8,
14, 10, 14, 1, 16, 16, 18, 10, 5, 16, 5, NA, 5, 14, 13, 16, 13,
14, 14, 18, 18, 8, 6, 8, 18, 16, 5, 13, 8, 8, 16, 16, 13, 13,
5, 16, 16, 16, 13, 16, 10, 10, 10, 12, 10, 13, 18, 14, 16, 16,
14, 14, 16, 5, 14, 18, 7, 7, 7, 12, 9, 5, 16, 14, 14, 16, 14,
16, 14, 16, 10, 12, 12, 13, 9, 14, 14, 18, 13, 13, 16, 16, 16,
5, 16, 16, 5, 16, 16, 16, 16, 5, 5, 10, 14, 14, 18, 18, 9, 13,
14, 14, 14, 13, 14, 5, 5, 14, 14, 13, 18, 9, 16, 16, 16, 5, 16,
16, 16, 16, 16, 16, 16, 13, 13, 13, 13, 16, 11, 16, 16, NA, 5,
10, 13, 13, 16, 14, 16, 18, 18, 16, 9, 18, 5, 8, 14, 13, 18,
18, 18, 16, 16, 16, 16, 5, 16, 16, 18, 16, 16, 5, 18, 18, 16,
16, 16, 16, 16, 16, 13, 5, 5, 2, 5, 19, 5, 18, 16, 16, 11, 5,
5, 13, 13, 13, 16, 16, 16, 13, 16, NA, NA, 10, 16, 18, 16, 16,
13, 16, 5, 8, 3, 3, 16, 13, 16, 18, 16, 16, 13, 13, 8, 16, 13,
18, 16, 18, 18, 2, 5, 5, 5, 18, 16, 8, 10, 19, 9, 9, 9, 16, 19,
19, 18, 14, 16, 16, 16, 9, 19, 2, 2, 16, 14, 13, 14, NA, NA,
9, 14, 14, 9, 5, NA, NA, 9, 16, 12, 2, 2, 19, 14, 12, 16, 9,
12, 12, 16, 13, 16, 10, 9, 4, 16, 7, 13, 9, 16, 16, 16, 4, 18,
9, 8, 12, 10, 13, 18, 18, 9, 18, 17, NA, 14, 12, 16, 18, 18,
8, 13, 18, NA, NA, NA, NA, 16, 13, 18, 3, 19, 9, 9, 9, 18, 9,
9, 18, 18, 18, 16, 18, 13, 5, 5, 5, 8, 8, 16, 9, 9, 5, 5, 8,
18, 8, 8, 9, 5, 16, 15, 15, 16, 5, 5, 3, 13, 8, 8, 9, 13, 13,
9, NA, 18, 16, 5, 18, 3, 9, 16, 13, 9, NA, 18, 8, 14, 11, 9,
16, 10, 9, 9, 8, 8, 8, 8, 18, 18, 8)), .Label = c("ARON", "CRDA",
"DCJH", "DJKT", "DEGF", "DOPC", "FACO", "FRNE", "HHW", "MEPR",
"NGA", "OTHE", "OTRE", "JHG", "POH", "PKJI", "SPOC", "YTGB",
"SUCH"), class = "factor"), about = structure(as.integer(c(3,
4, 1, 3, 1, 2, 3, 1, 2, 4, 3, 1, 1, 1, 4, 3, 1, 1, 3, 4, 3, 3,
4, 2, 1, 4, 1, 4, 4, 4, 4, 4, 3, 4, 2, 1, 1, 1, 1, 1, 1, 3, 1,
1, 3, 3, 3, 1, 2, 1, 1, 3, 2, 4, 1, 4, 1, 3, 3, 3, 4, 3, 1, 3,
3, 1, 3, 3, 1, 3, 3, 1, 3, 3, 1, 3, 3, 3, 3, 1, 1, 1, 4, 1, 3,
1, 3, 3, 4, 1, 3, 3, 4, 2, 3, 3, NA, 3, 3, 3, 3, 3, 1, 1, 1,
1, 1, 3, 1, 1, 3, 2, 1, 1, 4, 2, 4, 2, 2, 2, 3, 4, 1, 2, 1, 1,
1, 1, 1, 2, 1, 3, 1, 2, 3, 3, 1, 4, 3, 3, 2, 3, 1, 1, 3, 3, 3,
2, 2, 2, 1, 3, 1, 3, 3, 3, 1, 1, 2, 1, 1, 1, 1, 4, 4, 1, 1, 1,
1, 1, 1, 2, 1, 1, 1, 3, 3, 2, 1, 1, 1, 3, 1, 1, 2, 3, 3, 1, 1,
1, 1, 1, 4, 1, 2, 1, 1, 1, 1, 1, 1, 3, 3, 3, 2, 3, 1, 4, 1, 1,
1, 1, 3, 1, 3, 4, NA, 1, 3, 2, 4, 4, 3, 2, 2, 1, 1, 1, 3, 2,
2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 1, 3, 3, 1, 3, 4, 1, 4, 1, 4, 3,
3, 3, 3, 3, 4, 4, 4, 3, 2, 3, 3, 1, 3, 3, 1, 4, 1, 4, 4, 4, 3,
4, 1, 4, 4, NA, NA, 2, 1, 4, 4, 3, 1, 2, 4, 1, 3, 3, 3, 1, 3,
4, 3, 4, 3, 2, 1, 1, 1, 1, 3, 1, 1, 3, 3, 3, 3, 1, 4, 2, 2, 1,
1, 1, 3, 4, 2, 2, 2, 3, 2, 2, 2, 2, 4, 3, 3, 4, 1, 1, 1, NA,
NA, 1, 2, 2, 4, 3, NA, NA, 1, 2, 1, 3, 3, 2, 2, 1, 2, 4, 1, 1,
4, 4, 4, 4, 4, 3, 3, 3, 2, 1, 3, 3, 3, 3, 3, 1, 1, 2, 3, 1, 1,
1, 1, 2, 1, NA, NA, 4, 3, 4, 3, NA, 2, 3, NA, NA, NA, NA, 2,
2, 4, 3, 4, NA, NA, 1, 1, 3, 3, 3, 4, 3, 3, 1, 2, 3, NA, NA,
1, 1, 2, 2, 2, 2, 2, NA, 1, 1, 1, 2, 1, 1, NA, NA, 2, 2, NA,
3, 2, NA, NA, 2, 2, 2, 2, NA, 1, 2, NA, 3, 3, 2, 2, 2, 2, NA,
1, NA, 4, 3, 2, 2, 4, NA, 2, NA, NA, NA, NA, 3, 3, 1)), .Label = c("non-I NO
G",
"I NO G", "non-I ERT", "I ERT"
), class = "factor")), .Names = c("IWD",
"SOA", "about"))

xtabs(IWD ~ about + SOA,data=test1)

print(stripplot(IWD ~ about | SOA,data=test1,
       ylab="Days",
       sub="For days less than 365 days",
       between=list(x=0.3,y=0.3), jitter=T,
       main = paste("Covering District",j),
       scales = list( x = list(rot = 90))))


Ciao, Tom



Tom Mulholland
Tom Mulholland Associates

Ph: 9371 1302
Mob: 0417 962139

ABN: 74 018 074 092


---



From muteau at ensam.inra.fr  Wed Jun  9 09:12:11 2004
From: muteau at ensam.inra.fr (Vincent MUTEAUD)
Date: Wed, 09 Jun 2004 09:12:11 +0200
Subject: [R] SJAVA error
In-Reply-To: <BAY8-F121uLfL4q1wsS00023456@hotmail.com>
Message-ID: <5.0.2.1.2.20040609090813.00ada508@ensam.inra.fr>

Hi,
I have the same error than you two weeks ago. As I have not found how to 
do, I give up my example.
Yunko, I am interesting if you found how to proceed.
Thanks

Vincent




A 01:07 09/06/2004 +0900, =?iso-2022-jp?B?GyRCTHBMbhsoQiAbJEI9ZztSGyhC?= a 
??crit :
>Hi
>
>I'm trying to use SJava and I have troubles. I try to run examples from 
>"Calling R from Java"
>but,I have an error that "fatal error: enable to open the base package"
>
>I heard  SJAVA bug,
>so,could you  send me your compiled SJava package with the modified 
>REmbed.c because in Windows i'm not able to recompile!!!
>
>--example
>package org.omegahat.R.Java;
>
>public class REvalSample {
>         public static void main(String[] args) {
>                 String[] rargs = { "--slave", "--vanilla" };
>
> 
>System.out.println("Java$B$+$i(BR$B$r%3!<%k$9$k%W%m%0%i%`(B");
>
>                 ROmegahatInterpreter interp =
>                         new ROmegahatInterpreter(
>                                 ROmegahatInterpreter.fixArgs(rargs),
>                                 false);
>                 REvaluator e = new REvaluator();
>
>                 Object val = e.eval("x <- sin(seq(0, 2*pi, length=30))");
>                 val = e.eval("x * 2.0");
>
>                 if (val != null) {
>                         double[] objects = (double[]) val;
>                         for (int i = 0; i < objects.length; i++) {
>                                 System.err.println("(" + i + ") " + 
> objects[i]);
>                         }
>                 }
>         }
>}
>---------
>
>Thank you
>------------
>Junko Yano
>E-mail : junko_yano_ at hotmail.com
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Wed Jun  9 09:26:23 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 9 Jun 2004 08:26:23 +0100 (BST)
Subject: [R] SJAVA error
In-Reply-To: <5.0.2.1.2.20040609090813.00ada508@ensam.inra.fr>
Message-ID: <Pine.LNX.4.44.0406090820210.17932-100000@gannet.stats>

SJava is not part of R but rather of Omegahat, so this is the wrong list.
But as a hint, I think you have not set R_HOME, or not set it correctly.

But please consult the R posting guide for hints as to what information 
might be needed to help either of you, as neither or you have provided 
basic information like the OS, version of R, version of SJava ....

On Wed, 9 Jun 2004, Vincent MUTEAUD wrote:

> Hi,
> I have the same error than you two weeks ago. As I have not found how to 
> do, I give up my example.
> Yunko, I am interesting if you found how to proceed.
> Thanks
> 
> Vincent
> 
> A 01:07 09/06/2004 +0900, =?iso-2022-jp?B?GyRCTHBMbhsoQiAbJEI9ZztSGyhC?= a 
> ??crit :
> >Hi
> >
> >I'm trying to use SJava and I have troubles. I try to run examples from 
> >"Calling R from Java"
> >but,I have an error that "fatal error: enable to open the base package"
> >
> >I heard  SJAVA bug,
> >so,could you  send me your compiled SJava package with the modified 
> >REmbed.c because in Windows i'm not able to recompile!!!

EVERYONE should be able to compile under Windows as all the tools required
are free.

> >--example
> >package org.omegahat.R.Java;
> >
> >public class REvalSample {
> >         public static void main(String[] args) {
> >                 String[] rargs = { "--slave", "--vanilla" };
> >
> > 
> >System.out.println("Java$B$+$i(BR$B$r%3!<%k$9$k%W%m%0%i%`(B");
> >
> >                 ROmegahatInterpreter interp =
> >                         new ROmegahatInterpreter(
> >                                 ROmegahatInterpreter.fixArgs(rargs),
> >                                 false);
> >                 REvaluator e = new REvaluator();
> >
> >                 Object val = e.eval("x <- sin(seq(0, 2*pi, length=30))");
> >                 val = e.eval("x * 2.0");
> >
> >                 if (val != null) {
> >                         double[] objects = (double[]) val;
> >                         for (int i = 0; i < objects.length; i++) {
> >                                 System.err.println("(" + i + ") " + 
> > objects[i]);
> >                         }
> >                 }
> >         }
> >}

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Wed Jun  9 09:55:15 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Wed, 9 Jun 2004 09:55:15 +0200
Subject: [R] Is there an R-version of rayplot
In-Reply-To: <858788618A93D111B45900805F85267A0BCB2D4E@caexmta3.amd.com>
References: <858788618A93D111B45900805F85267A0BCB2D4E@caexmta3.amd.com>
Message-ID: <16582.49763.734981.516992@gargle.gargle.HOWL>

>>>>> "richard" == richard kittler <richard.kittler at amd.com>
>>>>>     on Tue, 8 Jun 2004 09:19:51 -0700 writes:

    richard> I need to make plots similar to those produced by
    richard> the s-plus rayplot function but can't seem to find
    richard> it in R.  These 'vector maps' plot a ray or vector
    richard> at each specified location. Is there something
    richard> similar in R ?  --Rich

Do 

   ?stars
   ?symbols

help further?
Also, maybe,
	par(ask = TRUE)
	example(stars)

Regards,
Martin Maechler



From bianca.vieru at free.fr  Wed Jun  9 10:15:29 2004
From: bianca.vieru at free.fr (Bianca)
Date: Wed, 9 Jun 2004 10:15:29 +0200
Subject: [R] nominal data
Message-ID: <004501c44df9$edb787c0$759daf81@limsi.fr>

Hi,

I am a new user of R. I have 2 series of nominal data (2 series of answers
for the same question) and I want to calculate the correlation between these
2 series. I've tried to use the correlation function (corr, ...) but all are
for numeric data... Does anyone know what function should I use for the
nominal data?

Thanks a lot,
Bianca



From david.netherway at adelaide.edu.au  Wed Jun  9 10:20:17 2004
From: david.netherway at adelaide.edu.au (David J. Netherway)
Date: Wed, 09 Jun 2004 17:50:17 +0930
Subject: [R] Specifying xlevels in effects library
Message-ID: <40C6C841.1080200@adelaide.edu.au>

library(effects)
mod <- lm(Measurement ~ Age + Sex, data=d)
e <-effect("Sex",mod)
 
The effect is evaluated at the mean age.
 
 > e
Sex effect
Sex
       F        M  
43.33083 44.48531  
 >
 > e$model.matrix
   (Intercept)      Age SexM
1            1 130.5859    0
23           1 130.5859    1
 
To evaluate the effect at Age=120 I tried:
e <-effect("Sex",mod,xlevels=list(Age=c(120)))
but the effect was still evaluated at 130.5859.
 
Is this an incorrect usage of xlevels?

Thanks, David



From unung at enciety.com  Wed Jun  9 10:39:01 2004
From: unung at enciety.com (Unung Istopo Hartanto)
Date: Wed, 09 Jun 2004 15:39:01 +0700
Subject: [R] nominal data
In-Reply-To: <004501c44df9$edb787c0$759daf81@limsi.fr>
References: <004501c44df9$edb787c0$759daf81@limsi.fr>
Message-ID: <1086770341.4983.9.camel@IT05>

Hi,

try ?cor

but. I doubt it's your problem, and I think there's an another way to
make correlation with nominal data, not used ?cor. But, i've been
explore it yet.

ex. :
cbind(c("one","one","two","three","two","three"),c("one","three","three","three","two","two")) -> test

> test
     [,1]    [,2]
[1,] "one"   "one"
[2,] "one"   "three"
[3,] "two"   "three"
[4,] "three" "three"
[5,] "two"   "two"
[6,] "three" "two"

> data.frame(test)
     X1    X2
1   one   one
2   one three
3   two three
4 three three
5   two   two
6 three   two

> data.frame(test) -> data

> cor(data$X1,data$X2)
[1] 0.5940885

I hope this helps,

Unung

On Wed, 2004-06-09 at 15:15, Bianca wrote:
> Hi,
> 
> I am a new user of R. I have 2 series of nominal data (2 series of answers
> for the same question) and I want to calculate the correlation between these
> 2 series. I've tried to use the correlation function (corr, ...) but all are
> for numeric data... Does anyone know what function should I use for the
> nominal data?
> 
> Thanks a lot,
> Bianca
>



From muteau at ensam.inra.fr  Wed Jun  9 10:48:15 2004
From: muteau at ensam.inra.fr (Vincent MUTEAUD)
Date: Wed, 09 Jun 2004 10:48:15 +0200
Subject: [R] SJAVA error
In-Reply-To: <Pine.LNX.4.44.0406090820210.17932-100000@gannet.stats>
References: <5.0.2.1.2.20040609090813.00ada508@ensam.inra.fr>
Message-ID: <5.0.2.1.2.20040609104631.00ad7020@ensam.inra.fr>

Thanks
I am using windows NT, R 1.9.0 and SJava 0.65 modified. I think my R_HOME 
is set correctly

A 08:26 09/06/2004 +0100, Prof Brian Ripley a ??crit :
>SJava is not part of R but rather of Omegahat, so this is the wrong list.
>But as a hint, I think you have not set R_HOME, or not set it correctly.
>
>But please consult the R posting guide for hints as to what information
>might be needed to help either of you, as neither or you have provided
>basic information like the OS, version of R, version of SJava ....
>
>On Wed, 9 Jun 2004, Vincent MUTEAUD wrote:
>
> > Hi,
> > I have the same error than you two weeks ago. As I have not found how to
> > do, I give up my example.
> > Yunko, I am interesting if you found how to proceed.
> > Thanks
> >
> > Vincent
> >
> > A 01:07 09/06/2004 +0900, =?iso-2022-jp?B?GyRCTHBMbhsoQiAbJEI9ZztSGyhC?= a
> > ??crit :
> > >Hi
> > >
> > >I'm trying to use SJava and I have troubles. I try to run examples from
> > >"Calling R from Java"
> > >but,I have an error that "fatal error: enable to open the base package"
> > >
> > >I heard  SJAVA bug,
> > >so,could you  send me your compiled SJava package with the modified
> > >REmbed.c because in Windows i'm not able to recompile!!!
>
>EVERYONE should be able to compile under Windows as all the tools required
>are free.
>
> > >--example
> > >package org.omegahat.R.Java;
> > >
> > >public class REvalSample {
> > >         public static void main(String[] args) {
> > >                 String[] rargs = { "--slave", "--vanilla" };
> > >
> > >
> > >System.out.println("Java$B$+$i(BR$B$r%3!<%k$9$k%W%m%0%i%`(B");
> > >
> > >                 ROmegahatInterpreter interp =
> > >                         new ROmegahatInterpreter(
> > >                                 ROmegahatInterpreter.fixArgs(rargs),
> > >                                 false);
> > >                 REvaluator e = new REvaluator();
> > >
> > >                 Object val = e.eval("x <- sin(seq(0, 2*pi, length=30))");
> > >                 val = e.eval("x * 2.0");
> > >
> > >                 if (val != null) {
> > >                         double[] objects = (double[]) val;
> > >                         for (int i = 0; i < objects.length; i++) {
> > >                                 System.err.println("(" + i + ") " +
> > > objects[i]);
> > >                         }
> > >                 }
> > >         }
> > >}
>
>--
>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>University of Oxford,             Tel:  +44 1865 272861 (self)
>1 South Parks Road,                     +44 1865 272866 (PA)
>Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From s0129600 at sms.ed.ac.uk  Wed Jun  9 11:05:46 2004
From: s0129600 at sms.ed.ac.uk (O Tosas Auguet)
Date: Wed,  9 Jun 2004 10:05:46 +0100
Subject: [R] GlmmPQL
Message-ID: <1086771946.40c6d2ea63217@sms.ed.ac.uk>


Dear all,


I have two questions concerning model simplification in GlmmPQL, for for random
and fixed effects:

1. Fixed effects: I don't know if I can simply specify anova(model) and trust
the table that comes up with the p value for each variable  in the fixed
effects formula. I have read that the only way to test for fixed effects is to
do approximate wald tests based on the standard errors of the models where I am
subsequently withdrawing one variable from the fixed effect formula at a time.
What does "aproximate" wald test mean? What is the best option?

2. Random effects: If AIC is not meaningful in GlmmPQL, how do I test for the
significance of the random effects?

3. I way to see if 1 single level of random effects is helpful in terms of
analysing the data, would be to comapre the GlmmPQL model with a glm models
without random effects, but again: what do I compare if AIC is not meaninful?
and if there is something I can compare, could I test for the significance of
that difference?

Could someone bring light to this?

Thanks,

Olga Tosas



From catch_utsav at yahoo.com  Wed Jun  9 11:06:59 2004
From: catch_utsav at yahoo.com (Utsav Boobna)
Date: Wed, 9 Jun 2004 02:06:59 -0700 (PDT)
Subject: [R] "attach" in R corr. to Spus one
Message-ID: <20040609090659.27700.qmail@web14806.mail.yahoo.com>

Hi,
    I am a newbie to R, just trying to switch from
Splus. I am wondering to know if there is any such
command in R as "attach" in Splus which can be used to
attach the full directory.

Also, any idea/referrence about, how to load c
functions in R would be highly appreciated.

Thanks,
Utsav



From wolski at molgen.mpg.de  Wed Jun  9 11:11:56 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Wed, 09 Jun 2004 11:11:56 +0200
Subject: [R] SJAVA error
References: <5.0.2.1.2.20040609090813.00ada508@ensam.inra.fr>
	<200406091018150510.09B577B9@mail.math.fu-berlin.de>
Message-ID: <200406091111560091.09E69C1E@mail.math.fu-berlin.de>

Hi!

For Windows and mac os X use of rJava may help.
http://stats.math.uni-augsburg.de/iPlots/
>From there also as it seems a working older version of SJave can be downloaded.

Sincerely
Eryk



*********** REPLY SEPARATOR  ***********

On 6/9/2004 at 9:12 AM Vincent MUTEAUD wrote:

>>>Hi,
>>>I have the same error than you two weeks ago. As I have not found how to 
>>>do, I give up my example.
>>>Yunko, I am interesting if you found how to proceed.
>>>Thanks
>>>
>>>Vincent
>>>
>>>
>>>
>>>
>>>A 01:07 09/06/2004 +0900, =?iso-2022-jp?B?GyRCTHBMbhsoQiAbJEI9ZztSGyhC?=
>>>a 
>>>??crit :
>>>>Hi
>>>>
>>>>I'm trying to use SJava and I have troubles. I try to run examples from 
>>>>"Calling R from Java"
>>>>but,I have an error that "fatal error: enable to open the base package"
>>>>
>>>>I heard  SJAVA bug,
>>>>so,could you  send me your compiled SJava package with the modified 
>>>>REmbed.c because in Windows i'm not able to recompile!!!
>>>>
>>>>--example
>>>>package org.omegahat.R.Java;
>>>>
>>>>public class REvalSample {
>>>>         public static void main(String[] args) {
>>>>                 String[] rargs = { "--slave", "--vanilla" };
>>>>
>>>> 
>>>>System.out.println("Java$B$+$i(BR$B$r%3!<%k$9$k%W%m%0%i%`(B");
>>>>
>>>>                 ROmegahatInterpreter interp =
>>>>                         new ROmegahatInterpreter(
>>>>                                 ROmegahatInterpreter.fixArgs(rargs),
>>>>                                 false);
>>>>                 REvaluator e = new REvaluator();
>>>>
>>>>                 Object val = e.eval("x <- sin(seq(0, 2*pi,
>>>length=30))");
>>>>                 val = e.eval("x * 2.0");
>>>>
>>>>                 if (val != null) {
>>>>                         double[] objects = (double[]) val;
>>>>                         for (int i = 0; i < objects.length; i++) {
>>>>                                 System.err.println("(" + i + ") " + 
>>>> objects[i]);
>>>>                         }
>>>>                 }
>>>>         }
>>>>}
>>>>---------
>>>>
>>>>Thank you
>>>>------------
>>>>Junko Yano
>>>>E-mail : junko_yano_ at hotmail.com
>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide!
>>>http://www.R-project.org/posting-guide.html
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



Dipl. bio-chem. Eryk Witold Wolski    @    MPI-Moleculare Genetic   
Ihnestrasse 63-73 14195 Berlin       'v'    
tel: 0049-30-83875219               /   \    
mail: wolski at molgen.mpg.de        ---W-W----    http://www.molgen.mpg.de/~wolski



From ripley at stats.ox.ac.uk  Wed Jun  9 11:13:43 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 9 Jun 2004 10:13:43 +0100 (BST)
Subject: [R] SJAVA error
In-Reply-To: <5.0.2.1.2.20040609104631.00ad7020@ensam.inra.fr>
Message-ID: <Pine.LNX.4.44.0406091010040.18828-100000@gannet.stats>

Please do read what I said. This is *the wrong list*, so please use teh 
correct one.

On Wed, 9 Jun 2004, Vincent MUTEAUD wrote:

> Thanks
> I am using windows NT, R 1.9.0 and SJava 0.65 modified. I think my R_HOME 
> is set correctly

It cannot be or the base package would be found.  The relevant code in R 
is

    fp = R_OpenLibraryFile("base");
    if (fp == NULL) {
	R_Suicide("unable to open the base package\n");
    }

and

FILE *R_OpenLibraryFile(char *file)
{
    char buf[256];
    FILE *fp;

    snprintf(buf, 256, "%s/library/base/R/%s", R_Home, file);
    fp = R_fopen(buf, "r");
    return fp;
}

> A 08:26 09/06/2004 +0100, Prof Brian Ripley a ??crit :
> >SJava is not part of R but rather of Omegahat, so this is the wrong list.
> >But as a hint, I think you have not set R_HOME, or not set it correctly.
> >
> >But please consult the R posting guide for hints as to what information
> >might be needed to help either of you, as neither or you have provided
> >basic information like the OS, version of R, version of SJava ....
> >
> >On Wed, 9 Jun 2004, Vincent MUTEAUD wrote:
> >
> > > Hi,
> > > I have the same error than you two weeks ago. As I have not found how to
> > > do, I give up my example.
> > > Yunko, I am interesting if you found how to proceed.
> > > Thanks
> > >
> > > Vincent
> > >
> > > A 01:07 09/06/2004 +0900, =?iso-2022-jp?B?GyRCTHBMbhsoQiAbJEI9ZztSGyhC?= a
> > > ??crit :
> > > >Hi
> > > >
> > > >I'm trying to use SJava and I have troubles. I try to run examples from
> > > >"Calling R from Java"
> > > >but,I have an error that "fatal error: enable to open the base package"
> > > >
> > > >I heard  SJAVA bug,
> > > >so,could you  send me your compiled SJava package with the modified
> > > >REmbed.c because in Windows i'm not able to recompile!!!
> >
> >EVERYONE should be able to compile under Windows as all the tools required
> >are free.
> >
> > > >--example
> > > >package org.omegahat.R.Java;
> > > >
> > > >public class REvalSample {
> > > >         public static void main(String[] args) {
> > > >                 String[] rargs = { "--slave", "--vanilla" };
> > > >
> > > >
> > > >System.out.println("Java$B$+$i(BR$B$r%3!<%k$9$k%W%m%0%i%`(B");
> > > >
> > > >                 ROmegahatInterpreter interp =
> > > >                         new ROmegahatInterpreter(
> > > >                                 ROmegahatInterpreter.fixArgs(rargs),
> > > >                                 false);
> > > >                 REvaluator e = new REvaluator();
> > > >
> > > >                 Object val = e.eval("x <- sin(seq(0, 2*pi, length=30))");
> > > >                 val = e.eval("x * 2.0");
> > > >
> > > >                 if (val != null) {
> > > >                         double[] objects = (double[]) val;
> > > >                         for (int i = 0; i < objects.length; i++) {
> > > >                                 System.err.println("(" + i + ") " +
> > > > objects[i]);
> > > >                         }
> > > >                 }
> > > >         }
> > > >}
> >
> >--
> >Brian D. Ripley,                  ripley at stats.ox.ac.uk
> >Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> >University of Oxford,             Tel:  +44 1865 272861 (self)
> >1 South Parks Road,                     +44 1865 272866 (PA)
> >Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Jun  9 11:20:43 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 9 Jun 2004 10:20:43 +0100 (BST)
Subject: [R] GlmmPQL
In-Reply-To: <1086771946.40c6d2ea63217@sms.ed.ac.uk>
Message-ID: <Pine.LNX.4.44.0406091015090.18828-100000@gannet.stats>

I think you need to read the references given in ?glmmPQL and its
reference to understand what PQL actually does.  If you don't know the
theory behind a statistical method, you should try to understand it before
trying to use it.

The same applies to AIC.  Even if AIC were computable, do you know its 
theoretical limitations (and it does not apply to testing if random 
effects have zero variance, for example)?

On Wed, 9 Jun 2004, O Tosas Auguet wrote:

> 
> Dear all,
> 
> 
> I have two questions concerning model simplification in GlmmPQL, for for
> random and fixed effects:
> 
> 1. Fixed effects: I don't know if I can simply specify anova(model) and
> trust the table that comes up with the p value for each variable in the
> fixed effects formula. I have read that the only way to test for fixed
> effects is to do approximate wald tests based on the standard errors of
> the models where I am subsequently withdrawing one variable from the
> fixed effect formula at a time. What does "aproximate" wald test mean?
> What is the best option?
> 
> 2. Random effects: If AIC is not meaningful in GlmmPQL, how do I test
> for the significance of the random effects?
> 
> 3. I way to see if 1 single level of random effects is helpful in terms
> of analysing the data, would be to comapre the GlmmPQL model with a glm
> models without random effects, but again: what do I compare if AIC is
> not meaninful? and if there is something I can compare, could I test for
> the significance of that difference?
> 
> Could someone bring light to this?

The references will.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Ulrich.Halekoh at agrsci.dk  Wed Jun  9 11:43:26 2004
From: Ulrich.Halekoh at agrsci.dk (Ulrich Halekoh)
Date: Wed, 9 Jun 2004 11:43:26 +0200
Subject: [R] inconsistency on p-value calculation of anova for quasi
	binomial 
Message-ID: <EA09C4B2B0F16E44B8F3311629493C0DF21A4C@DJFPOST01.djf.agrsci.dk>

Hej,

providing the dispersion parameter estimate to 
the anova function
for a quasibinomial fit results in two different
ways to calculate the p-value for the same statistic.


In the following example I test for the interaction effect.

In the versions  (a1 and a2) the p-value is based on the
F_1_17 distribution,
in the version (a3) it is calculated via the normal.

I think  anova should behave (by default) in all versions either
 in the one or the other 
way.


#example based on the orobanche data set provoded with the dispmod-package
library(dispmod)
data(orobanche)
orobanche$y<-with(orobanche,cbind(germinated,seeds-germinated))
g1<-glm(y~host+variety,family=quasibinomial,data=orobanche)
g2<-glm(y~host+variety+host:variety,family=quasibinomial,data=orobanche)

a1<-anova(g2,g1,test='F',dispersion=summary(g2)$dispersion)
a2<-anova(g2,test='F')

a3<-anova(g2,test='F',dispersion=summary(g2)$dispersion)



ulrich

R 1.9.0
Windows 2000


==============================================================
Ulrich Halekoh,  PhD                         Phone: +45 8999 1825
Biometry Research Unit                       Fax:   +45 8999 1300
Danish Institute of Agricultural Sciences    E-mail: ulrich.halekoh at agrsci.dk
Research Centre Foulum, DK-8830 Tjele, Denmark



From david.netherway at adelaide.edu.au  Wed Jun  9 12:22:09 2004
From: david.netherway at adelaide.edu.au (David J. Netherway)
Date: Wed, 09 Jun 2004 19:52:09 +0930
Subject: [R] Getting Pr from Summary(lm)
Message-ID: <40C6E4D1.1040103@adelaide.edu.au>

Hello,

I am trying to get the P values from the output of a summary for lm.

lm <- lm(y ~ age + sex)
s <- summary(lm)

I thought that I might be able to get them using a combination of scan, 
grep and sub.
But I got stuck on the first step - being able to process "s" as a text 
string.
I could perhaps write it to file than scan it back but there is probably 
an easier
way to do the whole thing.

Help would be welcome, David



From petr.pikal at precheza.cz  Wed Jun  9 12:24:33 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 09 Jun 2004 12:24:33 +0200
Subject: [R] "attach" in R corr. to Spus one
In-Reply-To: <20040609090659.27700.qmail@web14806.mail.yahoo.com>
Message-ID: <40C70181.17998.670D1A@localhost>

Hallo

On 9 Jun 2004 at 2:06, Utsav Boobna wrote:

> Hi,
>     I am a newbie to R, just trying to switch from
> Splus. I am wondering to know if there is any such
> command in R as "attach" in Splus which can be used to
> attach the full directory.

I would recommend you to spent some time going through docummentation and 
especially FAQs. 

getwd and setwd for setting the working directory

but in R objects are stored in one file .Rdata.

OTOH in S+ objects are (were - I did not use it since quite a lot time) stored as 
separate files.

For attaching objects see

?attach

Cheers
Petr

> 
> Also, any idea/referrence about, how to load c
> functions in R would be highly appreciated.
> 
> Thanks,
> Utsav
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From p.dalgaard at biostat.ku.dk  Wed Jun  9 12:19:24 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 09 Jun 2004 12:19:24 +0200
Subject: [R] Getting Pr from Summary(lm)
In-Reply-To: <40C6E4D1.1040103@adelaide.edu.au>
References: <40C6E4D1.1040103@adelaide.edu.au>
Message-ID: <x2n03djaoj.fsf@biostat.ku.dk>

"David J. Netherway" <david.netherway at adelaide.edu.au> writes:

> Hello,
> 
> I am trying to get the P values from the output of a summary for lm.
> 
> lm <- lm(y ~ age + sex)
> s <- summary(lm)
> 
> I thought that I might be able to get them using a combination of
> scan, grep and sub.
> But I got stuck on the first step - being able to process "s" as a
> text string.
> I could perhaps write it to file than scan it back but there is
> probably an easier
> way to do the whole thing.
> 
> Help would be welcome, David

Summaries are (usually) R objects which can be manipulated
programmatically. They should be confused with what their print
methods print. Consider

example(summary.lm)
coef(sld90)[,4,drop=F]


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Wed Jun  9 12:26:36 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 9 Jun 2004 11:26:36 +0100 (BST)
Subject: [R] Getting Pr from Summary(lm)
In-Reply-To: <40C6E4D1.1040103@adelaide.edu.au>
Message-ID: <Pine.LNX.4.44.0406091123520.30809-100000@gannet.stats>

coef(summary(lmfit))[, 4]

See ?summary.lm, and note that summary() methods in general produce an 
object which the print method then prints.

On Wed, 9 Jun 2004, David J. Netherway wrote:

> Hello,
> 
> I am trying to get the P values from the output of a summary for lm.
> 
> lm <- lm(y ~ age + sex)

Not a good choice of object name!

> s <- summary(lm)
> 
> I thought that I might be able to get them using a combination of scan, 
> grep and sub.
> But I got stuck on the first step - being able to process "s" as a text 
> string.
> I could perhaps write it to file than scan it back but there is probably 
> an easier
> way to do the whole thing.

You could sink to a textConnection if you really wanted the printed 
output.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Torsten.Hothorn at rzmail.uni-erlangen.de  Wed Jun  9 12:27:28 2004
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Wed, 9 Jun 2004 12:27:28 +0200 (CEST)
Subject: [R] Getting Pr from Summary(lm)
In-Reply-To: <40C6E4D1.1040103@adelaide.edu.au>
References: <40C6E4D1.1040103@adelaide.edu.au>
Message-ID: <Pine.LNX.4.51.0406091226360.9141@artemis.imbe.med.uni-erlangen.de>


> Hello,
>
> I am trying to get the P values from the output of a summary for lm.
>
> lm <- lm(y ~ age + sex)
> s <- summary(lm)

s$coefficients

gives you a matrix with the P-values in the fourth column

Torsten

>
> I thought that I might be able to get them using a combination of scan,
> grep and sub.
> But I got stuck on the first step - being able to process "s" as a text
> string.
> I could perhaps write it to file than scan it back but there is probably
> an easier
> way to do the whole thing.
>
> Help would be welcome, David
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From baron at psych.upenn.edu  Wed Jun  9 12:33:10 2004
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Wed, 9 Jun 2004 06:33:10 -0400
Subject: [R] nominal data
In-Reply-To: <004501c44df9$edb787c0$759daf81@limsi.fr>
References: <004501c44df9$edb787c0$759daf81@limsi.fr>
Message-ID: <20040609103310.GA29773@psych>

On 06/09/04 10:15, Bianca wrote:
>Hi,
>
>I am a new user of R. I have 2 series of nominal data (2 series of answers
>for the same question) and I want to calculate the correlation between these
>2 series. I've tried to use the correlation function (corr, ...) but all are
>for numeric data... Does anyone know what function should I use for the
>nominal data?

In part this is a statistics question, and the answer depends
heavily on why you want to know.

But one way to do it is to use Cohen's kappa.  This is in the psy
contributed packages, and described in our "Notes on R for
psychology ..." (linked from my R search page - look under
"inter-rater agreement").

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron
R search page:        http://finzi.psych.upenn.edu/



From avril.coghlan at ucd.ie  Wed Jun  9 12:34:46 2004
From: avril.coghlan at ucd.ie (Avril Coghlan)
Date: Wed, 09 Jun 2004 11:34:46 +0100
Subject: [R] testing effects of quantitative predictors on a
 categorical	response variable
Message-ID: <1086777286.10088.16.camel@bioinf14>

Hello,

   I have a small statistics question, and
as I'm quite new to statistics and R, I'm not
sure if I'm doing things correctly. 

   I am looking at two quantitative
variables (x,y) that are correlated. 
When I divide the data set according to a categorical
variable z, then x and y are more poorly correlated
when z = A than when z = B (see attached figure).
In fact x and y are two (correlated) predictor
variables and z is a categorical response variable that
x and y affect.

   I would like to use R to make some statistical
test to show that you seem to get z = A when
the value of x is much less than y, while you
tend to get z = B when x is approximately the same as y. 
Can anybody tell me what I should be doing?
I tried a logistic regression:
> glm1 <- glm(z ~ y + x,family=binomial(),trace=T)
which gives Pr(>|z|) < 0.01 for both x and y, but
I'm not sure if this is valid to do, since x and y are correlated?

As well this test does not show that it is for values of
x << y that we tend to get z = A, and that for
values of x approx = y, that we tend to get z = B. I'm
not sure how to show this?
  
I'll be very grateful if anyone can help.

Avril
-------------- next part --------------
A non-text attachment was scrubbed...
Name: avril.ps
Type: application/postscript
Size: 8001 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20040609/ea3dee5c/avril.ps

From jferrer at ivic.ve  Wed Jun  9 13:01:35 2004
From: jferrer at ivic.ve (jferrer@ivic.ve)
Date: Wed, 9 Jun 2004 07:01:35 -0400 (VET)
Subject: [R] Getting Pr from Summary(lm)
In-Reply-To: <40C6E4D1.1040103@adelaide.edu.au>
Message-ID: <Pine.LNX.4.44.0406090651392.5147-100000@jotaerre.ivic.ve>

Hi David,

To see what is stored in an object, use names().
In your case you could try this:

> lm <- lm(y ~ age + sex)
> s <- summary(lm)
>  names(s)
 [1] "call"          "terms"         "residuals"     "coefficients"
 [5] "aliased"       "sigma"         "df"            "r.squared"
 [9] "adj.r.squared" "fstatistic"    "cov.unscaled"
> s$coefficients
> s$coefficients[,4]

cheers,

JR

En respuesta a / Antwort zu / Reply to:

~~Hello,
~~
~~I am trying to get the P values from the output of a summary for lm.
~~
~~lm <- lm(y ~ age + sex)
~~s <- summary(lm)
~~
~~I thought that I might be able to get them using a combination of scan,
~~grep and sub.
~~But I got stuck on the first step - being able to process "s" as a text
~~string.
~~I could perhaps write it to file than scan it back but there is probably
~~an easier
~~way to do the whole thing.
~~
~~Help would be welcome, David
~~
~~______________________________________________
~~R-help at stat.math.ethz.ch mailing list
~~https://www.stat.math.ethz.ch/mailman/listinfo/r-help
~~PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
~~

Dipl.-Biol. J.R. Ferrer Paris ~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Laboratorio de Biolog??a de Organismos - Centro de Ecolog??a
   Instituto Venezolano de Investigaciones Cient??ficas
             Apartado 21827 - Caracas 1020A
           REPUBLICA BOLIVARIANA DE VENEZUELA
     Tel:00-58-212-5041452 --- Fax: 00-58-212-5041088
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ jferrer at ivic.ve



From jferrer at ivic.ve  Wed Jun  9 13:18:49 2004
From: jferrer at ivic.ve (jferrer@ivic.ve)
Date: Wed, 9 Jun 2004 07:18:49 -0400 (VET)
Subject: [R] testing effects of quantitative predictors on a categorical
	response variable
In-Reply-To: <1086777286.10088.16.camel@bioinf14>
Message-ID: <Pine.LNX.4.44.0406090707220.5147-100000@jotaerre.ivic.ve>

Hi Avril,

I'm not sure what you want to show. Do you want to know the effects of
each variable? or just predict when you get z=A and when z=B?

In the latter case, I think that, if x and y are in the same units, you
could simply try

w <- y-x
glm1 <- glm(z ~ w,family=binomial(),trace=T)

Hope it helps,

JR


En respuesta a / Antwort zu / Reply to:

~~Hello,
~~
~~   I have a small statistics question, and
~~as I'm quite new to statistics and R, I'm not
~~sure if I'm doing things correctly.
~~
~~   I am looking at two quantitative
~~variables (x,y) that are correlated.
~~When I divide the data set according to a categorical
~~variable z, then x and y are more poorly correlated
~~when z = A than when z = B (see attached figure).
~~In fact x and y are two (correlated) predictor
~~variables and z is a categorical response variable that
~~x and y affect.
~~
~~   I would like to use R to make some statistical
~~test to show that you seem to get z = A when
~~the value of x is much less than y, while you
~~tend to get z = B when x is approximately the same as y.
~~Can anybody tell me what I should be doing?
~~I tried a logistic regression:
~~> glm1 <- glm(z ~ y + x,family=binomial(),trace=T)
~~which gives Pr(>|z|) < 0.01 for both x and y, but
~~I'm not sure if this is valid to do, since x and y are correlated?
~~
~~As well this test does not show that it is for values of
~~x << y that we tend to get z = A, and that for
~~values of x approx = y, that we tend to get z = B. I'm
~~not sure how to show this?
~~
~~I'll be very grateful if anyone can help.
~~
~~Avril
~~

Dipl.-Biol. J.R. Ferrer Paris ~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Laboratorio de Biolog??a de Organismos - Centro de Ecolog??a
   Instituto Venezolano de Investigaciones Cient??ficas
             Apartado 21827 - Caracas 1020A
           REPUBLICA BOLIVARIANA DE VENEZUELA
     Tel:00-58-212-5041452 --- Fax: 00-58-212-5041088
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ jferrer at ivic.ve



From breitling at lb-netz.de  Wed Jun  9 13:13:50 2004
From: breitling at lb-netz.de (Lutz Ph. Breitling)
Date: Wed, 09 Jun 2004 12:13:50 +0100
Subject: [R] poisson regression with robust error variance ('eyestudy')
In-Reply-To: <40C1C094.4050708@vanderbilt.edu>
References: <6.0.1.1.0.20040602101549.01f674e0@192.168.1.2>
	<Pine.A41.4.58.0406020732060.77094@homer08.u.washington.edu>
	<40BDFF8F.6090906@vanderbilt.edu>
	<6.0.1.1.0.20040605125109.01c28148@192.168.1.2>
	<40C1C094.4050708@vanderbilt.edu>
Message-ID: <6.0.1.1.0.20040609120328.01c94c38@192.168.1.2>

okay, so now the bootcov works fine.

aren't the lower bootstrap variances just what Karla is talking about when 
she writes on the website describing the eyestudy that i was trying to redo 
in the first place:
"Using a Poisson model without robust error variances will result in a 
confidence interval that is too wide."
(Karla Lindquist, Senior Statistician in the Division of Geriatrics at UCSF)

but one more question: so i cannot get SANDWICH estimates of the standard 
error for a [R] glm or glmD?

many thanks for all your efforts!
lutz


At 13:46 05.06.2004, Frank E Harrell Jr wrote:

>Sorry I didn't think of that sooner.  robcov needs the residuals method 
>for the fitter to allow a type="score" or type="hscore" (for Efron's 
>method) argument.  Until someone adds score residuals to residuals.glm 
>robcov will not work for you.  residuals.lrm and residuals.coxph are 
>examples where score residuals are computed.  You can get robust 
>variance-covariance estimates with the bootstrap using bootcov for glmD 
>fits.  Oddly in your example I am finding that the bootstrap variances are 
>lower than the information-matrix-based ones.
>
>Frank Harrell

=============================
Lutz Ph. Breitling
Unit? des Recherches M?dicale
H?pital Albert Schweitzer
B.P. 118 Lambar?n? (GABON) 
-------------- next part --------------

---




From j.van_den_hoff at fz-rossendorf.de  Wed Jun  9 13:09:35 2004
From: j.van_den_hoff at fz-rossendorf.de (joerg van den hoff)
Date: Wed, 09 Jun 2004 13:09:35 +0200
Subject: [R] Is there an R-version of rayplot
In-Reply-To: <16582.49763.734981.516992@gargle.gargle.HOWL>
References: <858788618A93D111B45900805F85267A0BCB2D4E@caexmta3.amd.com>
	<16582.49763.734981.516992@gargle.gargle.HOWL>
Message-ID: <40C6EFEF.20706@fz-rossendorf.de>

maybe this q&d try helps?


#=================cut herer=============

vectorplot <- function (field) {

   #input is a (N x 4 array) of N vectors:
   #   field[,1:2] - x/y position  of vectors
   #   field[,3:4] - x/y componnent of vectors
   # plotted are the 2-D vectors attached to  the specified positions

   if (is.null(dim(field))||dim(field)[2] != 4) stop("N x 4 array expected")

   loc <- field[,1:2]
   val <- field[,3:4]

   alpha <- rbind(loc[,1],loc[,1]+val[,1])
   omega <- rbind(loc[,2],loc[,2]+val[,2])

   matplot(alpha,omega,type='l',lty=1,col='black') #the vector lines
   points(loc) #the start points
   points(loc+val,pch=20) #the end points
}

#example:

loc=matrix(rnorm(100),50,2)
field <- cbind(loc,loc)
vectorplot(field)

#=================cut herer=============

there are no nice arrow heads, of course, but one could construct some...
for now, vectors start with open circles and end with filled circles.

joerg



From JonesW at kssg.com  Wed Jun  9 13:27:47 2004
From: JonesW at kssg.com (Wayne Jones)
Date: Wed, 9 Jun 2004 12:27:47 +0100 
Subject: [R]  market-basket analysis in R
Message-ID: <6B5A9304046AD411BD0200508BDFB6CB02955F45@gimli.middleearth.kssg.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040609/bbfbe4b6/attachment.pl

From schnitzlerj at lyon.who.int  Wed Jun  9 13:53:32 2004
From: schnitzlerj at lyon.who.int (Johannes SCHNITZLER)
Date: Wed, 9 Jun 2004 13:53:32 +0200
Subject: [R] Greek fonts
Message-ID: <AD1C47A0C9C12C46987E2D31EE1E63B40B0D3D@lps001.lyon.who.int>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040609/bf65bbaf/attachment.pl

From H.RINNER at tirol.gv.at  Wed Jun  9 13:57:45 2004
From: H.RINNER at tirol.gv.at (RINNER Heinrich)
Date: Wed, 9 Jun 2004 13:57:45 +0200
Subject: [R]  market-basket analysis in R
Message-ID: <6A6B3B547E312840A98A9DD31516B321180571@mxs1.tirol.local>

Hi,
I have asked a similar question quite a while ago, maybe you find the replys given in that thread useful:
http://finzi.psych.upenn.edu/R/Rhelp02/archive/3977.html.

Anyway, this is two years old now, so maybe there's some news in the meantime.

Regards
Heinrich.

> -----Urspr??ngliche Nachricht-----
> Von: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] Im Auftrag von Wayne Jones
> Gesendet: Mittwoch, 09. Juni 2004 13:28
> An: r-help at stat.math.ethz.ch
> Betreff: [R] market-basket analysis in R
> 
> 
> 
> 
> Hi there fellow R-users, 
> 
> Does anyone know if there exists a package for associated 
> rules data mining
> (market basket analysis) in R. 
> 
> I have tried searching CRAN but with no luck. 
> 
> Regards
> 
> Wayne
> 
> 
> KSS Ltd
> Seventh Floor  St James's Buildings  79 Oxford Street  
> Manchester  M1 6SS  England
> Company Registration Number 2800886
> Tel: +44 (0) 161 228 0040	Fax: +44 (0) 161 236 6305
> mailto:kssg at kssg.com		http://www.kssg.com
> 
> 
> The information in this Internet email is confidential and 
> m...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From f.harrell at vanderbilt.edu  Wed Jun  9 08:00:20 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 09 Jun 2004 08:00:20 +0200
Subject: [R] "attach" in R corr. to Spus one
In-Reply-To: <40C70181.17998.670D1A@localhost>
References: <40C70181.17998.670D1A@localhost>
Message-ID: <40C6A774.2010203@vanderbilt.edu>

Petr Pikal wrote:
> Hallo
> 
> On 9 Jun 2004 at 2:06, Utsav Boobna wrote:
> 
> 
>>Hi,
>>    I am a newbie to R, just trying to switch from
>>Splus. I am wondering to know if there is any such
>>command in R as "attach" in Splus which can be used to
>>attach the full directory.
> 
> 
> I would recommend you to spent some time going through docummentation and 
> especially FAQs. 
> 
> getwd and setwd for setting the working directory
> 
> but in R objects are stored in one file .Rdata.

I prefer to control which objects are stored, and to use compression, by 
using save( ) and load( ), rather than using the .Rdata mechanism. 
save( ) can store one object or an entire series of objects in one .rda 
file.

Frank

> 
> OTOH in S+ objects are (were - I did not use it since quite a lot time) stored as 
> separate files.
> 
> For attaching objects see
> 
> ?attach
> 
> Cheers
> Petr
> 
> 
>>Also, any idea/referrence about, how to load c
>>functions in R would be highly appreciated.
>>
>>Thanks,
>>Utsav
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>>http://www.R-project.org/posting-guide.html
> 
> 
> Petr Pikal
> petr.pikal at precheza.cz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From andy_liaw at merck.com  Wed Jun  9 14:18:09 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 9 Jun 2004 08:18:09 -0400
Subject: [R] more obvious contribution mechanism?
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7E64@usrymx25.merck.com>

There's no paypal button, but it isn't that much harder:  Download the pdf
form, fill it out and fax it back.  How hard can that be?

Andy

> From: Jason Turner
> 
> On Wed, 2004-06-09 at 13:22, ivo welch wrote:
> > can we put a "how to donate money to R" on the R webpage?  
> perhaps with 
> > a paypal button?
> > 
> 
> Does the R Foundation link meet this need?
> 
http://www.r-project.org/foundation/main.html

Cheers

Jason



From flom at ndri.org  Wed Jun  9 14:25:02 2004
From: flom at ndri.org (Peter Flom)
Date: Wed, 09 Jun 2004 08:25:02 -0400
Subject: [R] more obvious contribution mechanism?
Message-ID: <s0c6c97b.088@MAIL.NDRI.ORG>

It certainly isn't hard to do it this way, but a PayPal button is even
easier.  I think adding one would bring in extra money.  

Just my opinion, of course

Peter

Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)



>>> "Liaw, Andy" <andy_liaw at merck.com> 6/9/2004 8:18:09 AM >>>
There's no paypal button, but it isn't that much harder:  Download the
pdf
form, fill it out and fax it back.  How hard can that be?

Andy

> From: Jason Turner
> 
> On Wed, 2004-06-09 at 13:22, ivo welch wrote:
> > can we put a "how to donate money to R" on the R webpage?  
> perhaps with 
> > a paypal button?
> > 
> 
> Does the R Foundation link meet this need?
> 
http://www.r-project.org/foundation/main.html 

Cheers

Jason

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From OritG at Amdocs.com  Wed Jun  9 14:28:07 2004
From: OritG at Amdocs.com (Orit Harel (Gdalyahu))
Date: Wed, 9 Jun 2004 15:28:07 +0300
Subject: [R] e1071, R1.9.0, Solaris 2.9, should I be worried?
Message-ID: <58EE6177EA63BF42A0C3E95974AEC4E103613A01@tlvmail2.corp.amdocs.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040609/901bdd0f/attachment.pl

From ozric at web.de  Wed Jun  9 14:52:23 2004
From: ozric at web.de (Christian Schulz)
Date: Wed, 9 Jun 2004 14:52:23 +0200
Subject: [R]  market-basket analysis in R
In-Reply-To: <6B5A9304046AD411BD0200508BDFB6CB02955F45@gimli.middleearth.kssg.com>
References: <6B5A9304046AD411BD0200508BDFB6CB02955F45@gimli.middleearth.kssg.com>
Message-ID: <200406091452.23675.ozric@web.de>

I don't know any package!?
 
 but you could try:

http://fuzzy.cs.uni-magdeburg.de/~borgelt/software.html
http://www.cs.umb.edu/~laur/ARtool/
http://www.cs.waikato.ac.nz/ml/weka/

regards,
Christian


Am Mittwoch, 9. Juni 2004 13:27 schrieb Wayne Jones:
> Hi there fellow R-users,
>
> Does anyone know if there exists a package for associated rules data mining
> (market basket analysis) in R.
>
> I have tried searching CRAN but with no luck.
>
> Regards
>
> Wayne
>
>
> KSS Ltd
> Seventh Floor  St James's Buildings  79 Oxford Street  Manchester  M1 6SS 
> England Company Registration Number 2800886
> Tel: +44 (0) 161 228 0040	Fax: +44 (0) 161 236 6305
> mailto:kssg at kssg.com		http://www.kssg.com
>
>
> The information in this Internet email is confidential and m...{{dropped}}
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From ivo.welch at yale.edu  Wed Jun  9 14:47:41 2004
From: ivo.welch at yale.edu (ivo welch)
Date: Wed, 09 Jun 2004 08:47:41 -0400
Subject: [R] Re: more obvious contribution mechanism
Message-ID: <40C706ED.80703@yale.edu>


hi:  thanks everyone for pointing me to the contribution page..

[a] it was not obvious to me how to find this page.  if you like 
contributions, please make it a bit more obvious.  I would think a 
button on the home page would be a good idea.

[b] any kind of trinket sale (a CD?) for money would be useful for 
research budget based donations.  now I will try to find out if my 
research budget allows me to send donations.  (how about buying some air 
for $100?)  grrrr...

[c] for individuals, a paypal button to donate $x would be a very good idea.

[d] VISA works well internationally---but non-US check based items do 
not for our accounting.  especially if the R-project does not have a US 
bank account, paypal would be useful.

Andy: yes, the paypal-like scam is another phishing one.  the same 
applies to most other orgs these days (ebay, amazon, etc.)

regards,

/ivo



From andy_liaw at merck.com  Wed Jun  9 14:51:36 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 9 Jun 2004 08:51:36 -0400
Subject: [R] "attach" in R corr. to Spus one
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7E67@usrymx25.merck.com>

> From: Frank E Harrell Jr
> 
> Petr Pikal wrote:
> > Hallo
> > 
> > On 9 Jun 2004 at 2:06, Utsav Boobna wrote:
> > 
> > 
> >>Hi,
> >>    I am a newbie to R, just trying to switch from
> >>Splus. I am wondering to know if there is any such
> >>command in R as "attach" in Splus which can be used to
> >>attach the full directory.
> > 
> > 
> > I would recommend you to spent some time going through 
> docummentation and 
> > especially FAQs. 
> > 
> > getwd and setwd for setting the working directory
> > 
> > but in R objects are stored in one file .Rdata.
> 
> I prefer to control which objects are stored, and to use 
> compression, by 
> using save( ) and load( ), rather than using the .Rdata mechanism. 
> save( ) can store one object or an entire series of objects 
> in one .rda 
> file.

I mostly do the same.  For fairly large objects or objects that I do not
want to accidentally alter during a session, I use attach() instead of
load().  One convenience of attach() is that since the objects are not
loaded into .GlobalEnv, I would have a less cluttered .RData even if I do
save the workspace when I q().

Cheers,
Andy

 
> Frank
> 
> > 
> > OTOH in S+ objects are (were - I did not use it since quite 
> a lot time) stored as 
> > separate files.
> > 
> > For attaching objects see
> > 
> > ?attach
> > 
> > Cheers
> > Petr
> > 
> > 
> >>Also, any idea/referrence about, how to load c
> >>functions in R would be highly appreciated.
> >>
> >>Thanks,
> >>Utsav
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide!
> >>http://www.R-project.org/posting-guide.html
> > 
> > 
> > Petr Pikal
> > petr.pikal at precheza.cz
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> > 
> 
> 
> -- 
> Frank E 
> Harrell Jr   Professor and Chair           School of Medicine
>                       Department of Biostatistics   
> Vanderbilt University
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From andy_liaw at merck.com  Wed Jun  9 15:00:51 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 9 Jun 2004 09:00:51 -0400
Subject: [R] "attach" in R corr. to Spus one
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7E68@usrymx25.merck.com>

Now that I acutally read the original question:

If there are .RData files in other directories that you want to attach(),
you can certainly do that.  One difference between R and S-plus is that in
S-plus you can attach() a directory at position 1, effectively making that
the place that new objects are stored.  In R, attach()ing at position 1 is
not allowed.  I must say that I like the way R works better, as it does a
better job at preventing me from keeping garbage around.

Best,
Andy

> From: Liaw, Andy
> 
> > From: Frank E Harrell Jr
> > 
> > Petr Pikal wrote:
> > > Hallo
> > > 
> > > On 9 Jun 2004 at 2:06, Utsav Boobna wrote:
> > > 
> > > 
> > >>Hi,
> > >>    I am a newbie to R, just trying to switch from
> > >>Splus. I am wondering to know if there is any such
> > >>command in R as "attach" in Splus which can be used to
> > >>attach the full directory.
> > > 
> > > 
> > > I would recommend you to spent some time going through 
> > docummentation and 
> > > especially FAQs. 
> > > 
> > > getwd and setwd for setting the working directory
> > > 
> > > but in R objects are stored in one file .Rdata.
> > 
> > I prefer to control which objects are stored, and to use 
> > compression, by 
> > using save( ) and load( ), rather than using the .Rdata mechanism. 
> > save( ) can store one object or an entire series of objects 
> > in one .rda 
> > file.
> 
> I mostly do the same.  For fairly large objects or objects 
> that I do not
> want to accidentally alter during a session, I use attach() instead of
> load().  One convenience of attach() is that since the objects are not
> loaded into .GlobalEnv, I would have a less cluttered .RData 
> even if I do
> save the workspace when I q().
> 
> Cheers,
> Andy
> 
>  
> > Frank
> > 
> > > 
> > > OTOH in S+ objects are (were - I did not use it since quite 
> > a lot time) stored as 
> > > separate files.
> > > 
> > > For attaching objects see
> > > 
> > > ?attach
> > > 
> > > Cheers
> > > Petr
> > > 
> > > 
> > >>Also, any idea/referrence about, how to load c
> > >>functions in R would be highly appreciated.
> > >>
> > >>Thanks,
> > >>Utsav
> > > 
> > > Petr Pikal
> > > petr.pikal at precheza.cz
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > > 
> > 
> > 
> > -- 
> > Frank E 
> > Harrell Jr   Professor and Chair           School of Medicine
> >                       Department of Biostatistics   
> > Vanderbilt University



From gb at stat.umu.se  Wed Jun  9 15:38:30 2004
From: gb at stat.umu.se (=?iso-8859-1?Q?G=F6ran_Brostr=F6m?=)
Date: Wed, 9 Jun 2004 15:38:30 +0200
Subject: [R] GLMM(..., family=binomial(link="cloglog"))?
In-Reply-To: <40C5DC08.70806@pdf.com>
References: <3A822319EB35174CA3714066D590DCD504AF7DFB@usrymx25.merck.com>
	<40C4B87B.7060301@pdf.com>
	<6r8yez88wo.fsf@bates4.stat.wisc.edu> <40C5009A.90705@pdf.com>
	<6racze9jtd.fsf@bates4.stat.wisc.edu> <40C5DC08.70806@pdf.com>
Message-ID: <20040609133830.GA24700@stat.umu.se>

On Tue, Jun 08, 2004 at 08:32:24AM -0700, Spencer Graves wrote:
> Hi, Doug: 
> 
>      Thanks.  I'll try the things you suggests.  The observed 
> proportions ranged from roughly 0.2 to 0.8 in 100 binomial random 
> samples where sigma is at most 0.05.  Jim Lindsey's "glmm" does 
> Gauss-Hermite quadrature, but I don't know if it bothers with the 
> adaptive step.  With it, I've seen estimates of the variance component 
> ranging from 0.4 to 0.7 or so.  Since I simulated normal 0 standard 
> deviation of 1, the algorithm was clearly underestimating what was 
> simulated.  My next step, I think, is to program adaptive Gauss-Hermite 
> quadrature for something closer to my real problem (as you just 
> suggested), and see what I get. 
[...]

> Douglas Bates wrote:
> 
> >Spencer Graves <spencer.graves at pdf.com> writes:
> >
[...]
> >>	  Consider the following:
> >>
> >>> set.seed(1); N <- 10
> >>> z <- rnorm(N)
> >>> pz <- inv.logit(z)
> >>> DF <- data.frame(z=z, pz=pz, y=rbinom(N, 100, pz)/100, n=100,
> >>> smpl=factor(1:N))
> >>
> >>> GLMM(y~1, family=binomial, data=DF, random=~1|smpl, weights=n)

I think the "weights=n" is the problem, i.e., you summarize Bernoulli
data to Bin(100, p) data, and that gives a completely different estimate of
the variance of the random effect. (This looks as an error in lme4 to me,
or am I missing something? Doug?) Really, the two ways of representing data
should give equivalent analyses, but it doesn't. The same phenomenon
appears in 'glm', i.e. without random effects, but only for the residual
sum of squares, df, and AIC. 

The following is a small function calling lme4 twice, first as
before, and then with data 'wrapped up' into Bernoulli data. I also run
'glmmML' in the latter case (since glmmML only allows the Bernoulli
representation):

-------------------------------------------------------------------   
sim <- function(N = 10, grpSize = 10, std = 1){
    require(glmmML)
    require(lme4)
    
    set.seed(1)
    z <- rnorm(N, mean = 0, sd = std)
#    pz <- inv.logit(z); is identical to (in 'stats')
    pz <- plogis(z)
    Y <- rbinom(N, grpSize, pz)

    ## 'Summary' data frame:
    
    DF <- data.frame(z = z, pz = pz,
                     y = Y / grpSize,
                     n = grpSize,
                     smpl = factor(1:N))

    fit1 <- GLMM(y~1, family = binomial, data = DF,
                 random = ~1|smpl, weights = n)
    ##fit1 <- glm(y~1, family = binomial, data = DF, weights = n)


    ## 'Individual' data frame:

    n <- N * grpSize
    z <- rep(z, rep(grpSize, N))
    pz <- rep(pz, rep(grpSize, N))

    y <- numeric(n)
    for (i in 1:N) y[((i - 1) * grpSize + 1):((i-1)*grpSize + Y[i])] <- 1 
    smpl <- as.factor(rep(1:N, rep(grpSize, N)))
    
    bigDF <- data.frame(z = z, pz = pz, y = y,
                        smpl = smpl)
    fit2 <- GLMM(y~1, family=binomial, data=bigDF, random=~1|smpl)
    ##fit2 <- glm(y~1, family=binomial, data=bigDF)

    fitML <- glmmML(y ~ 1, family = binomial,
                    data = bigDF,
                    cluster = bigDF$smpl)
    
    return(list(fit1 = fit1,
                fit2 = fit2,
                fitML = fitML
                )
           )
}
----------------------------------------------------------------

Output:

$fit1
Generalized Linear Mixed Model

Family: binomial family with logit link
Fixed: y ~ 1 
Data: DF 
 log-likelihood:  -11.59919 
Random effects:
 Groups Name        Variance   Std.Dev.  
 smpl   (Intercept) 3.0384e-10 1.7431e-05

Estimated scale (compare to 1)  1.391217 

Fixed effects:
            Estimate Std. Error z value Pr(>|z|)
(Intercept)  0.48955    0.20602  2.3762  0.01749

Number of Observations: 10
Number of Groups: 10 

$fit2
Generalized Linear Mixed Model

Family: binomial family with logit link
Fixed: y ~ 1 
Data: bigDF 
 log-likelihood:  -64.8084 
Random effects:
 Groups Name        Variance Std.Dev.
 smpl   (Intercept) 0.58353  0.7639  

Estimated scale (compare to 1)  0.9563351 

Fixed effects:
            Estimate Std. Error z value Pr(>|z|)
(Intercept)  0.52811    0.32286  1.6357   0.1019

Number of Observations: 100
Number of Groups: 10 

$fitML

Call:  glmmML(formula = y ~ 1, data = bigDF, 
              cluster = bigDF$smpl, family = binomial) 


              coef se(coef)     z Pr(>|z|)
(Intercept) 0.5587   0.3313 1.686   0.0917

Standard deviation in mixing distribution:  0.764 
Std. Error:                                 0.3655 

Residual deviance: 129.5 on 98 degrees of freedom       AIC: 133.5 
---------------
Note the big difference in estimated random effect 'sd' in the two lme4
runs! Note further how close to each other the corresponding estimates for
the second run of lme4 and of glmmML are.

[...]

G??ran
-- 
 G??ran Brostr??m                    tel: +46 90 786 5223
 Department of Statistics          fax: +46 90 786 6614
 Ume?? University                   http://www.stat.umu.se/egna/gb/
 SE-90187 Ume??, Sweden             e-mail: gb at stat.umu.se



From andy_liaw at merck.com  Wed Jun  9 15:40:49 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 9 Jun 2004 09:40:49 -0400
Subject: [R] Specifying xlevels in effects library
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7E69@usrymx25.merck.com>

Prof. Fox will be able to give the definitive answer, but from my reading of
?effect, xlevels refers to the values of the factor whose effect you're
interested in, not the ones being `marginalized'.  I believe you need to
play with the `typical' argument.

HTH,
Andy

> From: David J. Netherway
> 
> library(effects)
> mod <- lm(Measurement ~ Age + Sex, data=d)
> e <-effect("Sex",mod)
>  
> The effect is evaluated at the mean age.
>  
>  > e
> Sex effect
> Sex
>        F        M  
> 43.33083 44.48531  
>  >
>  > e$model.matrix
>    (Intercept)      Age SexM
> 1            1 130.5859    0
> 23           1 130.5859    1
>  
> To evaluate the effect at Age=120 I tried:
> e <-effect("Sex",mod,xlevels=list(Age=c(120)))
> but the effect was still evaluated at 130.5859.
>  
> Is this an incorrect usage of xlevels?
> 
> Thanks, David
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From junko_yano_ at hotmail.com  Wed Jun  9 15:43:51 2004
From: junko_yano_ at hotmail.com (=?iso-2022-jp?B?GyRCTHBMbhsoQiAbJEI9ZztSGyhC?=)
Date: Wed, 09 Jun 2004 22:43:51 +0900
Subject: [R] SJAVA error
Message-ID: <BAY8-F808DzHx63RNpg00040024@hotmail.com>

I am using windows xp, R 1.9.0 and SJava 0.65 modified. 
perhaps I set correctly.
 
could you  send me your compiled SJava package with the 
modified REmbed.c because in Windows i'm not able to recompile!!!
could you provide me your solution.

---
CLASSPATH
     C:\Program Files\R\rw1090\library\SJava;
     C:\Program Files\R\rw1090\library\SJava\org\omegahat\Jars\antlr.jar;
     C:\Program 
Files\R\rw1090\library\SJava\org\omegahat\Jars\Environment.jar;
     C:\Program Files\R\rw1090\library\SJava\org\omegahat\Jars\jas.jar;
     C:\Program Files\R\rw1090\library\SJava\org\omegahat\Jars\jhall.jar;
     C:\Program 
Files\R\rw1090\library\SJava\org\omegahat\Jars\ROmegahatExamples.jar
 
 JAVA_HOME
     C:\j2sdk1.4.1_03
 path
     C:\j2sdk1.4.1_03\jre\bin\client;
     C:\Program Files\R\rw1090\library\SJava\libs;
     C:\Program Files\R\rw1090\bin
 
 R_HOME
     C:\Program Files\R\rw1090
 
 SJAVA
     C:\Program Files\R\rw1090\library\SJava

thank you
------------
Junko Yano
E-mail : junko_yano_ at hotmail.com





>From: Vincent MUTEAUD <muteau at ensam.inra.fr>
>To: Prof Brian Ripley <ripley at stats.ox.ac.uk>
>CC: ?? ?? <junko_yano_ at hotmail.com>,        <r-help at stat.math.ethz.ch>
>Subject: Re: [R] SJAVA error
>Date: Wed, 09 Jun 2004 10:48:15 +0200
>
>Thanks
>I am using windows NT, R 1.9.0 and SJava 0.65 modified. I think my 
>R_HOME is set correctly
>
>A 08:26 09/06/2004 +0100, Prof Brian Ripley a ?rit :
>>SJava is not part of R but rather of Omegahat, so this is the wrong 
>>list.
>>But as a hint, I think you have not set R_HOME, or not set it 
>>correctly.
>>
>>But please consult the R posting guide for hints as to what 
>>information
>>might be needed to help either of you, as neither or you have 
>>provided
>>basic information like the OS, version of R, version of SJava ....
>>
>>On Wed, 9 Jun 2004, Vincent MUTEAUD wrote:
>>
>> > Hi,
>> > I have the same error than you two weeks ago. As I have not 
>>found how to
>> > do, I give up my example.
>> > Yunko, I am interesting if you found how to proceed.
>> > Thanks
>> >
>> > Vincent
>> >
>> > A 01:07 09/06/2004 +0900, 
>>=?iso-2022-jp?B?GyRCTHBMbhsoQiAbJEI9ZztSGyhC?= a
>> > ?rit :
>> > >Hi
>> > >
>> > >I'm trying to use SJava and I have troubles. I try to run 
>>examples from
>> > >"Calling R from Java"
>> > >but,I have an error that "fatal error: enable to open the base 
>>package"
>> > >
>> > >I heard  SJAVA bug,
>> > >so,could you  send me your compiled SJava package with the 
>>modified
>> > >REmbed.c because in Windows i'm not able to recompile!!!
>>
>>EVERYONE should be able to compile under Windows as all the tools 
>>required
>>are free.
>>
>> > >--example
>> > >package org.omegahat.R.Java;
>> > >
>> > >public class REvalSample {
>> > >         public static void main(String[] args) {
>> > >                 String[] rargs = { "--slave", "--vanilla" };
>> > >
>> > >
>> > >System.out.println("Java$B$+$i(BR$B$r%3!<%k$9$k%W%m%0%i%`(B");
>> > >
>> > >                 ROmegahatInterpreter interp =
>> > >                         new ROmegahatInterpreter(
>> > >                                 
>>ROmegahatInterpreter.fixArgs(rargs),
>> > >                                 false);
>> > >                 REvaluator e = new REvaluator();
>> > >
>> > >                 Object val = e.eval("x <- sin(seq(0, 2*pi, 
>>length=30))");
>> > >                 val = e.eval("x * 2.0");
>> > >
>> > >                 if (val != null) {
>> > >                         double[] objects = (double[]) val;
>> > >                         for (int i = 0; i < objects.length; 
>>i++) {
>> > >                                 System.err.println("(" + i + 
>>") " +
>> > > objects[i]);
>> > >                         }
>> > >                 }
>> > >         }
>> > >}
>>
>>--
>>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>>Professor of Applied Statistics,  
>>http://www.stats.ox.ac.uk/~ripley/
>>University of Oxford,             Tel:  +44 1865 272861 (self)
>>1 South Parks Road,                     +44 1865 272866 (PA)
>>Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From andy_liaw at merck.com  Wed Jun  9 15:56:14 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 9 Jun 2004 09:56:14 -0400
Subject: [R] Specifying xlevels in effects library
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7E6A@usrymx25.merck.com>

Here's an example:

> library(effects)
> age <- round(rnorm(100, mean=30, sd=5))
> sex <- factor(sample(c("M","F"), 100, replace=TRUE))
> y <- rnorm(100)
> fit <- lm(y ~ age + sex)
> eff <- effect("sex", fit, typical=function(...) 25)
> eff

 sex effect
sex
          F           M 
-0.23387685  0.07063834 

Now we can check if that looks right:

> predict(fit, newdata=data.frame(age=c(25, 25), sex=factor(c("M","F"))))
          1           2 
 0.07063834 -0.23387685 

HTH,
Andy



> From: Liaw, Andy
> 
> Prof. Fox will be able to give the definitive answer, but 
> from my reading of
> ?effect, xlevels refers to the values of the factor whose 
> effect you're
> interested in, not the ones being `marginalized'.  I believe 
> you need to
> play with the `typical' argument.
> 
> HTH,
> Andy
> 
> > From: David J. Netherway
> > 
> > library(effects)
> > mod <- lm(Measurement ~ Age + Sex, data=d)
> > e <-effect("Sex",mod)
> >  
> > The effect is evaluated at the mean age.
> >  
> >  > e
> > Sex effect
> > Sex
> >        F        M  
> > 43.33083 44.48531  
> >  >
> >  > e$model.matrix
> >    (Intercept)      Age SexM
> > 1            1 130.5859    0
> > 23           1 130.5859    1
> >  
> > To evaluate the effect at Age=120 I tried:
> > e <-effect("Sex",mod,xlevels=list(Age=c(120)))
> > but the effect was still evaluated at 130.5859.
> >  
> > Is this an incorrect usage of xlevels?
> > 
> > Thanks, David
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 
> --------------------------------------------------------------
> ----------------
> Notice:  This e-mail message, together with any attachments, 
> contains information of Merck & Co., Inc. (One Merck Drive, 
> Whitehouse Station, New Jersey, USA 08889), and/or its 
> affiliates (which may be known outside the United States as 
> Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as 
> Banyu) that may be confidential, proprietary copyrighted 
> and/or legally privileged. It is intended solely for the use 
> of the individual or entity named on this message.  If you 
> are not the intended recipient, and have received this 
> message in error, please notify us immediately by reply 
> e-mail and then delete it from your system.
> --------------------------------------------------------------
> ----------------
>



From nuin at terra.com.br  Wed Jun  9 15:52:27 2004
From: nuin at terra.com.br (Paulo Nuin)
Date: 09 Jun 2004 09:52:27 -0400
Subject: [R] fast mkChar
In-Reply-To: <C698D707214E6F4AB39AB7096C3DE5A5568755@phost015.EVAFUNDS.intermedia.net>
References: <C698D707214E6F4AB39AB7096C3DE5A5568755@phost015.EVAFUNDS.intermedia.net>
Message-ID: <1086789150.3534.4.camel@info5>

Hello everyone

This is my first message to the list and I believe the question I am
including is a simple one.

I have a matrix where I need to calculate ANOVA for the rows as the
columns represent a different treatment. I would like to know if there
is a command or a series of commans that I can enter to do that. 

At the moment I have a external script that extracts each row from the
matrix, transforms it in a column, another factor columns is add and the
text file is thrown to Rterm --vanilla.

Any help is appreciated.

Thanks a lot

Paulo Nuin



From petr.pikal at precheza.cz  Wed Jun  9 16:01:21 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 09 Jun 2004 16:01:21 +0200
Subject: [R] "attach" in R corr. to Spus one
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7E68@usrymx25.merck.com>
Message-ID: <40C73451.12242.A6B3D7@localhost>

Hallo Andy,

Well, my original respond was to Utsav's question if it is possible to attach a 
directory in R as in S+.

I personally prefer to start R session from a directory where I have all data and 
results from certain project (txt, doc, xls, png, pdf....). Therefore I use only 
.RData and I usually store histories of my work in separate files, so as basically all 
my data and my knowledge of attaching different directories in one session is not 
at all comprehensive. 

But if I am not mistaken you cannot attach a directory without some suitable file 
(.RData or saved file) in it but you can specify different working directory by 
setwd() and list files in it by list.files(). I presume that after setwd() I can 
attach(".RData") to get access to objects from it but ls() still gives me list of 
objects stored in an .RData file in the directory from which R was started.

OTOH saving an object by save(object, file="myfile") saves the object to 
directory specified by setwd().

Cheers
Petr

On 9 Jun 2004 at 9:00, Liaw, Andy wrote:

> Now that I acutally read the original question:
> 
> If there are .RData files in other directories that you want to
> attach(), you can certainly do that.  One difference between R and
> S-plus is that in S-plus you can attach() a directory at position 1,
> effectively making that the place that new objects are stored.  In R,
> attach()ing at position 1 is not allowed.  I must say that I like the
> way R works better, as it does a better job at preventing me from
> keeping garbage around.
> 
> Best,
> Andy
> 
> > From: Liaw, Andy
> > 
> > > From: Frank E Harrell Jr
> > > 
> > > Petr Pikal wrote:
> > > > Hallo
> > > > 
> > > > On 9 Jun 2004 at 2:06, Utsav Boobna wrote:
> > > > 
> > > > 
> > > >>Hi,
> > > >>    I am a newbie to R, just trying to switch from
> > > >>Splus. I am wondering to know if there is any such
> > > >>command in R as "attach" in Splus which can be used to
> > > >>attach the full directory.
> > > > 
> > > > 
> > > > I would recommend you to spent some time going through 
> > > docummentation and 
> > > > especially FAQs. 
> > > > 
> > > > getwd and setwd for setting the working directory
> > > > 
> > > > but in R objects are stored in one file .Rdata.
> > > 
> > > I prefer to control which objects are stored, and to use 
> > > compression, by 
> > > using save( ) and load( ), rather than using the .Rdata mechanism.
> > > save( ) can store one object or an entire series of objects in one
> > > .rda file.
> > 
> > I mostly do the same.  For fairly large objects or objects 
> > that I do not
> > want to accidentally alter during a session, I use attach() instead
> > of load().  One convenience of attach() is that since the objects
> > are not loaded into .GlobalEnv, I would have a less cluttered .RData
> > even if I do save the workspace when I q().
> > 
> > Cheers,
> > Andy
> > 
> >  
> > > Frank
> > > 
> > > > 
> > > > OTOH in S+ objects are (were - I did not use it since quite 
> > > a lot time) stored as 
> > > > separate files.
> > > > 
> > > > For attaching objects see
> > > > 
> > > > ?attach
> > > > 
> > > > Cheers
> > > > Petr
> > > > 
> > > > 
> > > >>Also, any idea/referrence about, how to load c
> > > >>functions in R would be highly appreciated.
> > > >>
> > > >>Thanks,
> > > >>Utsav
> > > > 
> > > > Petr Pikal
> > > > petr.pikal at precheza.cz
> > > > 
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide! 
> > > http://www.R-project.org/posting-guide.html
> > > > 
> > > 
> > > 
> > > -- 
> > > Frank E 
> > > Harrell Jr   Professor and Chair           School of Medicine
> > >                       Department of Biostatistics   
> > > Vanderbilt University
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From Giovanna.Jonalasinio at uniroma1.it  Wed Jun  9 16:02:49 2004
From: Giovanna.Jonalasinio at uniroma1.it (Giovanna.Jonalasinio@uniroma1.it)
Date: Wed, 9 Jun 2004 16:02:49 +0200
Subject: [R] Giovanna Jonalasinio
 =?iso-8859-1?q?=E8_fuori_ufficio-_away_from_?=
 =?iso-8859-1?q?the_office?=
Message-ID: <OF49A63CA8.A406B5CE-ONC1256EAE.004D29B2-C1256EAE.004D29B2@Uniroma1.it>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040609/0e886ed6/attachment.pl

From ivo.welch at yale.edu  Wed Jun  9 16:13:26 2004
From: ivo.welch at yale.edu (ivo welch)
Date: Wed, 09 Jun 2004 10:13:26 -0400
Subject: [R] Re: fighting with ps.options and xlim/ylim
Message-ID: <40C71B06.3020609@yale.edu>


Thanks again for all the messages.

Is the 4% in par('usr') hardcoded?  if so, may I suggest making this a 
user-changeable parameter for x and y axis?

I looked at psfrag, and it seems like a great package.  alas, I have 
switched to pdflatex, and pdffrag does not exist.  :-(

I also discovered that there is a pdf device now.  neat.  now I need to 
experiment with it.  of course, for curiosity, I looked at google to 
determine when it was added, but "R changes pdf" gives me everything 
except R related entries... :-(.

regards,

/ivo



From andy_liaw at merck.com  Wed Jun  9 16:24:19 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 9 Jun 2004 10:24:19 -0400
Subject: [R] "attach" in R corr. to Spus one
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7E6E@usrymx25.merck.com>

Hi Petr,

I don't think I've said anything that is inconsistent with what you said,
have I?

The strategy that you mentioned is the one that is recommended, and I use it
myself.  However, life is not always that simple.  There are times when the
task at hand involves data/code/whatever that live in a different directory
(perhaps most likely either the parent- or sub-directory of pwd).

.RData is just a .rda file, and thus can be attach()ed, and that's what I
said in my sentence.  E.g., I can attach("subdirectory/.RData"), then use
ls(2) to see what's in there.  (ls() does have a few optional arguments.)
Regarding save(), you can save to anywhere by using full path in the file
name (otherwise it wouldn't be terribly useful).

Best,
Andy

> From: Petr Pikal
> 
> Hallo Andy,
> 
> Well, my original respond was to Utsav's question if it is 
> possible to attach a 
> directory in R as in S+.
> 
> I personally prefer to start R session from a directory where 
> I have all data and 
> results from certain project (txt, doc, xls, png, pdf....). 
> Therefore I use only 
> .RData and I usually store histories of my work in separate 
> files, so as basically all 
> my data and my knowledge of attaching different directories 
> in one session is not 
> at all comprehensive. 
> 
> But if I am not mistaken you cannot attach a directory 
> without some suitable file 
> (.RData or saved file) in it but you can specify different 
> working directory by 
> setwd() and list files in it by list.files(). I presume that 
> after setwd() I can 
> attach(".RData") to get access to objects from it but ls() 
> still gives me list of 
> objects stored in an .RData file in the directory from which 
> R was started.
> 
> OTOH saving an object by save(object, file="myfile") saves 
> the object to 
> directory specified by setwd().
> 
> Cheers
> Petr
> 
> On 9 Jun 2004 at 9:00, Liaw, Andy wrote:
> 
> > Now that I acutally read the original question:
> > 
> > If there are .RData files in other directories that you want to
> > attach(), you can certainly do that.  One difference between R and
> > S-plus is that in S-plus you can attach() a directory at position 1,
> > effectively making that the place that new objects are 
> stored.  In R,
> > attach()ing at position 1 is not allowed.  I must say that 
> I like the
> > way R works better, as it does a better job at preventing me from
> > keeping garbage around.
> > 
> > Best,
> > Andy
> > 
> > > From: Liaw, Andy
> > > 
> > > > From: Frank E Harrell Jr
> > > > 
> > > > Petr Pikal wrote:
> > > > > Hallo
> > > > > 
> > > > > On 9 Jun 2004 at 2:06, Utsav Boobna wrote:
> > > > > 
> > > > > 
> > > > >>Hi,
> > > > >>    I am a newbie to R, just trying to switch from
> > > > >>Splus. I am wondering to know if there is any such
> > > > >>command in R as "attach" in Splus which can be used to
> > > > >>attach the full directory.
> > > > > 
> > > > > 
> > > > > I would recommend you to spent some time going through 
> > > > docummentation and 
> > > > > especially FAQs. 
> > > > > 
> > > > > getwd and setwd for setting the working directory
> > > > > 
> > > > > but in R objects are stored in one file .Rdata.
> > > > 
> > > > I prefer to control which objects are stored, and to use 
> > > > compression, by 
> > > > using save( ) and load( ), rather than using the .Rdata 
> mechanism.
> > > > save( ) can store one object or an entire series of 
> objects in one
> > > > .rda file.
> > > 
> > > I mostly do the same.  For fairly large objects or objects 
> > > that I do not
> > > want to accidentally alter during a session, I use 
> attach() instead
> > > of load().  One convenience of attach() is that since the objects
> > > are not loaded into .GlobalEnv, I would have a less 
> cluttered .RData
> > > even if I do save the workspace when I q().
> > > 
> > > Cheers,
> > > Andy
> > > 
> > >  
> > > > Frank
> > > > 
> > > > > 
> > > > > OTOH in S+ objects are (were - I did not use it since quite 
> > > > a lot time) stored as 
> > > > > separate files.
> > > > > 
> > > > > For attaching objects see
> > > > > 
> > > > > ?attach
> > > > > 
> > > > > Cheers
> > > > > Petr
> > > > > 
> > > > > 
> > > > >>Also, any idea/referrence about, how to load c
> > > > >>functions in R would be highly appreciated.
> > > > >>
> > > > >>Thanks,
> > > > >>Utsav
> > > > > 
> > > > > Petr Pikal
> > > > > petr.pikal at precheza.cz
> > > > > 
> > > > > ______________________________________________
> > > > > R-help at stat.math.ethz.ch mailing list
> > > > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > > > > PLEASE do read the posting guide! 
> > > > http://www.R-project.org/posting-guide.html
> > > > > 
> > > > 
> > > > 
> > > > -- 
> > > > Frank E 
> > > > Harrell Jr   Professor and Chair           School of Medicine
> > > >                       Department of Biostatistics   
> > > > Vanderbilt University
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 
> Petr Pikal
> petr.pikal at precheza.cz
> 
> 
> 
>



From ligges at statistik.uni-dortmund.de  Wed Jun  9 16:26:41 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 09 Jun 2004 16:26:41 +0200
Subject: [R] SJAVA error
In-Reply-To: <BAY8-F808DzHx63RNpg00040024@hotmail.com>
References: <BAY8-F808DzHx63RNpg00040024@hotmail.com>
Message-ID: <40C71E21.1040901@statistik.uni-dortmund.de>

?? ?? wrote:

> I am using windows xp, R 1.9.0 and SJava 0.65 modified. perhaps I set 
> correctly.
> 
> could you  send me your compiled SJava package with the modified 
> REmbed.c because in Windows i'm not able to recompile!!!
> could you provide me your solution.

You can compile yourself! Please read readme.packages which tells you
how to set up our system in order to compile packages.

Uwe Ligges


> ---
> CLASSPATH
>     C:\Program Files\R\rw1090\library\SJava;
>     C:\Program Files\R\rw1090\library\SJava\org\omegahat\Jars\antlr.jar;
>     C:\Program 
> Files\R\rw1090\library\SJava\org\omegahat\Jars\Environment.jar;
>     C:\Program Files\R\rw1090\library\SJava\org\omegahat\Jars\jas.jar;
>     C:\Program Files\R\rw1090\library\SJava\org\omegahat\Jars\jhall.jar;
>     C:\Program 
> Files\R\rw1090\library\SJava\org\omegahat\Jars\ROmegahatExamples.jar
> 
> JAVA_HOME
>     C:\j2sdk1.4.1_03
> path
>     C:\j2sdk1.4.1_03\jre\bin\client;
>     C:\Program Files\R\rw1090\library\SJava\libs;
>     C:\Program Files\R\rw1090\bin
> 
> R_HOME
>     C:\Program Files\R\rw1090
> 
> SJAVA
>     C:\Program Files\R\rw1090\library\SJava
> 
> thank you
> ------------
> Junko Yano
> E-mail : junko_yano_ at hotmail.com
> 
> 
> 
> 
> 
>> From: Vincent MUTEAUD <muteau at ensam.inra.fr>
>> To: Prof Brian Ripley <ripley at stats.ox.ac.uk>
>> CC: ?? ?? <junko_yano_ at hotmail.com>,        
>> <r-help at stat.math.ethz.ch>
>> Subject: Re: [R] SJAVA error
>> Date: Wed, 09 Jun 2004 10:48:15 +0200
>>
>> Thanks
>> I am using windows NT, R 1.9.0 and SJava 0.65 modified. I think my 
>> R_HOME is set correctly
>>
>> A 08:26 09/06/2004 +0100, Prof Brian Ripley a ?rit :
>>
>>> SJava is not part of R but rather of Omegahat, so this is the wrong 
>>> list.
>>> But as a hint, I think you have not set R_HOME, or not set it correctly.
>>>
>>> But please consult the R posting guide for hints as to what information
>>> might be needed to help either of you, as neither or you have provided
>>> basic information like the OS, version of R, version of SJava ....
>>>
>>> On Wed, 9 Jun 2004, Vincent MUTEAUD wrote:
>>>
>>> > Hi,
>>> > I have the same error than you two weeks ago. As I have not found 
>>> how to
>>> > do, I give up my example.
>>> > Yunko, I am interesting if you found how to proceed.
>>> > Thanks
>>> >
>>> > Vincent
>>> >
>>> > A 01:07 09/06/2004 +0900, 
>>> =?iso-2022-jp?B?GyRCTHBMbhsoQiAbJEI9ZztSGyhC?= a
>>> > ?rit :
>>> > >Hi
>>> > >
>>> > >I'm trying to use SJava and I have troubles. I try to run examples 
>>> from
>>> > >"Calling R from Java"
>>> > >but,I have an error that "fatal error: enable to open the base 
>>> package"
>>> > >
>>> > >I heard  SJAVA bug,
>>> > >so,could you  send me your compiled SJava package with the modified
>>> > >REmbed.c because in Windows i'm not able to recompile!!!
>>>
>>> EVERYONE should be able to compile under Windows as all the tools 
>>> required
>>> are free.
>>>
>>> > >--example
>>> > >package org.omegahat.R.Java;
>>> > >
>>> > >public class REvalSample {
>>> > >         public static void main(String[] args) {
>>> > >                 String[] rargs = { "--slave", "--vanilla" };
>>> > >
>>> > >
>>> > >System.out.println("Java$B$+$i(BR$B$r%3!<%k$9$k%W%m%0%i%`(B");
>>> > >
>>> > >                 ROmegahatInterpreter interp =
>>> > >                         new ROmegahatInterpreter(
>>> > >                                 ROmegahatInterpreter.fixArgs(rargs),
>>> > >                                 false);
>>> > >                 REvaluator e = new REvaluator();
>>> > >
>>> > >                 Object val = e.eval("x <- sin(seq(0, 2*pi, 
>>> length=30))");
>>> > >                 val = e.eval("x * 2.0");
>>> > >
>>> > >                 if (val != null) {
>>> > >                         double[] objects = (double[]) val;
>>> > >                         for (int i = 0; i < objects.length; i++) {
>>> > >                                 System.err.println("(" + i + ") " +
>>> > > objects[i]);
>>> > >                         }
>>> > >                 }
>>> > >         }
>>> > >}
>>>
>>> -- 
>>> Brian D. Ripley,                  ripley at stats.ox.ac.uk
>>> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>>> University of Oxford,             Tel:  +44 1865 272861 (self)
>>> 1 South Parks Road,                     +44 1865 272866 (PA)
>>> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>>
>>
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ligges at statistik.uni-dortmund.de  Wed Jun  9 16:30:11 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 09 Jun 2004 16:30:11 +0200
Subject: [R] Re: fighting with ps.options and xlim/ylim
In-Reply-To: <40C71B06.3020609@yale.edu>
References: <40C71B06.3020609@yale.edu>
Message-ID: <40C71EF3.4000903@statistik.uni-dortmund.de>

ivo welch wrote:

> 
> Thanks again for all the messages.
> 
> Is the 4% in par('usr') hardcoded?  if so, may I suggest making this a 
> user-changeable parameter for x and y axis?

See ?par and its argumets xaxp, yaxp which can be set to "i".


> I looked at psfrag, and it seems like a great package.  alas, I have 
> switched to pdflatex, and pdffrag does not exist.  :-(
> 
> I also discovered that there is a pdf device now.  neat.  

Since R-1.3.0, as the News file tells us.

Uwe Ligges



> now I need to 
> experiment with it.  of course, for curiosity, I looked at google to 
> determine when it was added, but "R changes pdf" gives me everything 
> except R related entries... :-(.
> 
> regards,
> 
> /ivo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From flom at ndri.org  Wed Jun  9 16:32:04 2004
From: flom at ndri.org (Peter Flom)
Date: Wed, 09 Jun 2004 10:32:04 -0400
Subject: [R] Dot chart question
Message-ID: <s0c6e744.088@MAIL.NDRI.ORG>

Running R 1.8.1 on  a Windows machine

In dotchart, I would like to shrink the labels on the tick marks (that
is, the numbers) without shrinking anything else.  I could not find this
in either the Rhelp archives or in ?dotchart, which recmmends cex to
avoid 'label overlap', but cex shrinks all the characters in the plot.

Is there a way to do this?

Thanks

Peter

Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)



From ccleland at optonline.net  Wed Jun  9 16:39:04 2004
From: ccleland at optonline.net (Chuck Cleland)
Date: Wed, 09 Jun 2004 10:39:04 -0400
Subject: [R] Dot chart question
In-Reply-To: <s0c6e744.088@MAIL.NDRI.ORG>
References: <s0c6e744.088@MAIL.NDRI.ORG>
Message-ID: <40C72108.8070004@optonline.net>

   See cex.axis and other cex.* arguments to par() in ?par.  For example:

 > data(VADeaths)
 >      dotchart(VADeaths, main = "Death Rates in Virginia - 1940")
 > par(cex.axis = .6)
 >      dotchart(VADeaths, main = "Death Rates in Virginia - 1940")

hope this helps,

Chuck Cleland

Peter Flom wrote:
> Running R 1.8.1 on  a Windows machine
> 
> In dotchart, I would like to shrink the labels on the tick marks (that
> is, the numbers) without shrinking anything else.  I could not find this
> in either the Rhelp archives or in ?dotchart, which recmmends cex to
> avoid 'label overlap', but cex shrinks all the characters in the plot.
> 
> Is there a way to do this?

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From spencer.graves at pdf.com  Wed Jun  9 16:40:24 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 09 Jun 2004 07:40:24 -0700
Subject: [R] GLMM(..., family=binomial(link="cloglog"))?
In-Reply-To: <20040609133830.GA24700@stat.umu.se>
References: <3A822319EB35174CA3714066D590DCD504AF7DFB@usrymx25.merck.com>	<40C4B87B.7060301@pdf.com>	<6r8yez88wo.fsf@bates4.stat.wisc.edu>
	<40C5009A.90705@pdf.com>	<6racze9jtd.fsf@bates4.stat.wisc.edu>
	<40C5DC08.70806@pdf.com> <20040609133830.GA24700@stat.umu.se>
Message-ID: <40C72158.3010402@pdf.com>

Hi, Go"ran:

Thanks for the analysis. Unfortunately, it still leaves me with 2
problems. First, I'm dealing with extremely small defect rates involving
thousands and millions of Bernoulli trials, so creating bigDF would
require computers with much more memory and processing speed available.

Second, there still seems to be substantial bias in the estimates. Do
you have any thoughts about the origins of this bias and what I can do
about it? Maybe I should try your bigDF GLMM call with method="Laplace".
Since Lindsey's "glmm" claims to be doing Gauss-Hermite quadrature, I
wonder if adaptive Gauss-Hermite will fix this bias problem. I guess
I'll just have to try it and find out.

Thanks again.
Spencer Graves

Go"ran Brostro"m wrote:

>On Tue, Jun 08, 2004 at 08:32:24AM -0700, Spencer Graves wrote:
>  
>
>>Hi, Doug: 
>>
>>     Thanks.  I'll try the things you suggests.  The observed 
>>proportions ranged from roughly 0.2 to 0.8 in 100 binomial random 
>>samples where sigma is at most 0.05.  Jim Lindsey's "glmm" does 
>>Gauss-Hermite quadrature, but I don't know if it bothers with the 
>>adaptive step.  With it, I've seen estimates of the variance component 
>>ranging from 0.4 to 0.7 or so.  Since I simulated normal 0 standard 
>>deviation of 1, the algorithm was clearly underestimating what was 
>>simulated.  My next step, I think, is to program adaptive Gauss-Hermite 
>>quadrature for something closer to my real problem (as you just 
>>suggested), and see what I get. 
>>    
>>
>[...]
>
>  
>
>>Douglas Bates wrote:
>>
>>    
>>
>>>Spencer Graves <spencer.graves at pdf.com> writes:
>>>
>>>      
>>>
>[...]
>  
>
>>>>	  Consider the following:
>>>>
>>>>        
>>>>
>>>>>set.seed(1); N <- 10
>>>>>z <- rnorm(N)
>>>>>pz <- inv.logit(z)
>>>>>DF <- data.frame(z=z, pz=pz, y=rbinom(N, 100, pz)/100, n=100,
>>>>>smpl=factor(1:N))
>>>>>          
>>>>>
>>>>>GLMM(y~1, family=binomial, data=DF, random=~1|smpl, weights=n)
>>>>>          
>>>>>
>
>I think the "weights=n" is the problem, i.e., you summarize Bernoulli
>data to Bin(100, p) data, and that gives a completely different estimate of
>the variance of the random effect. (This looks as an error in lme4 to me,
>or am I missing something? Doug?) Really, the two ways of representing data
>should give equivalent analyses, but it doesn't. The same phenomenon
>appears in 'glm', i.e. without random effects, but only for the residual
>sum of squares, df, and AIC. 
>
>The following is a small function calling lme4 twice, first as
>before, and then with data 'wrapped up' into Bernoulli data. I also run
>'glmmML' in the latter case (since glmmML only allows the Bernoulli
>representation):
>
>-------------------------------------------------------------------   
>sim <- function(N = 10, grpSize = 10, std = 1){
>    require(glmmML)
>    require(lme4)
>    
>    set.seed(1)
>    z <- rnorm(N, mean = 0, sd = std)
>#    pz <- inv.logit(z); is identical to (in 'stats')
>    pz <- plogis(z)
>    Y <- rbinom(N, grpSize, pz)
>
>    ## 'Summary' data frame:
>    
>    DF <- data.frame(z = z, pz = pz,
>                     y = Y / grpSize,
>                     n = grpSize,
>                     smpl = factor(1:N))
>
>    fit1 <- GLMM(y~1, family = binomial, data = DF,
>                 random = ~1|smpl, weights = n)
>    ##fit1 <- glm(y~1, family = binomial, data = DF, weights = n)
>
>
>    ## 'Individual' data frame:
>
>    n <- N * grpSize
>    z <- rep(z, rep(grpSize, N))
>    pz <- rep(pz, rep(grpSize, N))
>
>    y <- numeric(n)
>    for (i in 1:N) y[((i - 1) * grpSize + 1):((i-1)*grpSize + Y[i])] <- 1 
>    smpl <- as.factor(rep(1:N, rep(grpSize, N)))
>    
>    bigDF <- data.frame(z = z, pz = pz, y = y,
>                        smpl = smpl)
>    fit2 <- GLMM(y~1, family=binomial, data=bigDF, random=~1|smpl)
>    ##fit2 <- glm(y~1, family=binomial, data=bigDF)
>
>    fitML <- glmmML(y ~ 1, family = binomial,
>                    data = bigDF,
>                    cluster = bigDF$smpl)
>    
>    return(list(fit1 = fit1,
>                fit2 = fit2,
>                fitML = fitML
>                )
>           )
>}
>----------------------------------------------------------------
>
>Output:
>
>$fit1
>Generalized Linear Mixed Model
>
>Family: binomial family with logit link
>Fixed: y ~ 1 
>Data: DF 
> log-likelihood:  -11.59919 
>Random effects:
> Groups Name        Variance   Std.Dev.  
> smpl   (Intercept) 3.0384e-10 1.7431e-05
>
>Estimated scale (compare to 1)  1.391217 
>
>Fixed effects:
>            Estimate Std. Error z value Pr(>|z|)
>(Intercept)  0.48955    0.20602  2.3762  0.01749
>
>Number of Observations: 10
>Number of Groups: 10 
>
>$fit2
>Generalized Linear Mixed Model
>
>Family: binomial family with logit link
>Fixed: y ~ 1 
>Data: bigDF 
> log-likelihood:  -64.8084 
>Random effects:
> Groups Name        Variance Std.Dev.
> smpl   (Intercept) 0.58353  0.7639  
>
>Estimated scale (compare to 1)  0.9563351 
>
>Fixed effects:
>            Estimate Std. Error z value Pr(>|z|)
>(Intercept)  0.52811    0.32286  1.6357   0.1019
>
>Number of Observations: 100
>Number of Groups: 10 
>
>$fitML
>
>Call:  glmmML(formula = y ~ 1, data = bigDF, 
>              cluster = bigDF$smpl, family = binomial) 
>
>
>              coef se(coef)     z Pr(>|z|)
>(Intercept) 0.5587   0.3313 1.686   0.0917
>
>Standard deviation in mixing distribution:  0.764 
>Std. Error:                                 0.3655 
>
>Residual deviance: 129.5 on 98 degrees of freedom       AIC: 133.5 
>---------------
>Note the big difference in estimated random effect 'sd' in the two lme4
>runs! Note further how close to each other the corresponding estimates for
>the second run of lme4 and of glmmML are.
>
>[...]
>
>Go"ran
>  
>



From pgilbert at bank-banque-canada.ca  Wed Jun  9 16:41:58 2004
From: pgilbert at bank-banque-canada.ca (Paul Gilbert)
Date: Wed, 09 Jun 2004 10:41:58 -0400
Subject: [R] X-12-ARIMA
In-Reply-To: <ED7E0E44EAADFB46A6ABA12A647C1306122D18D6@CORREOINT.banxico.org.mx>
References: <ED7E0E44EAADFB46A6ABA12A647C1306122D18D6@CORREOINT.banxico.org.mx>
Message-ID: <40C721B6.9050605@bankofcanada.ca>

(subject changed from "[R] a doubt" )

The fortran source code for X-12-ARIMA seems to be available at 
<http://www.census.gov/srd/www/x12a/x12down_unix.html>. Incorporating 
this into a standard R package would be a much better approach than 
trying to hack an OS specific version with system calls to a .exe file. 
I expect there will be a fair amount of work in either case, and I don't 
have time to look at it now. If anyone wants to do the real work then I 
would be interested in using it (but using calls to a .exe would not be 
very useful to me). The first thing to check would be the license, as I 
have not looked at it. The next is probably the question of whether the 
fortran is nicely organized in subroutines without IO, so R can call 
those and avoid fortran IO problems.

(Perhaps there are R users at the U.S. Census Bureau that would be 
willing to help?)

Paul Gilbert

Mart??nez Ovando Juan Carlos wrote:

>Hello again,
>
>In a previous message I request your help, but I don't have been clear in my problem. Specifically, I'm trying to create an interface in R for the X-12-ARIMA and TRAMO SEATS, for the versions that run in MS-DOS. This problem awake in me the interest for make interfaces to comparing some Bayesian models for classification that where implemented in MS-DOS to. The question for you is if you know about the existence of an R function that allows me to run -in Windows- executable files in MS-DOS from the R command window. 
>
>
>The function 'dos' in Matlab that I have mention in the previous message allows to run '.exe' programs for a specific directory.  
>
>
>Many thanks.      
>
>Saludos,
>
>            Juan Carlos
>
>  
>



From flom at ndri.org  Wed Jun  9 16:42:35 2004
From: flom at ndri.org (Peter Flom)
Date: Wed, 09 Jun 2004 10:42:35 -0400
Subject: [R] Dot chart question
Message-ID: <s0c6e9ba.054@MAIL.NDRI.ORG>

Thanks, this worked.  I had tried

fx.dotchart2(freqrelig[order(freqrelig)],
 label = levels(religfact)[order(freqrelig)],
 main = 'Religions among young adults in Bushwick \n (log scale)',
 xlab = 'Frequency', log = 'x', cex.axis = .7)

which did nothing.......

Peter

>>> Chuck Cleland <ccleland at optonline.net> 6/9/2004 10:39:04 AM >>>
   See cex.axis and other cex.* arguments to par() in ?par.  For
example:

 > data(VADeaths)
 >      dotchart(VADeaths, main = "Death Rates in Virginia - 1940")
 > par(cex.axis = .6)
 >      dotchart(VADeaths, main = "Death Rates in Virginia - 1940")

hope this helps,

Chuck Cleland

Peter Flom wrote:
> Running R 1.8.1 on  a Windows machine
> 
> In dotchart, I would like to shrink the labels on the tick marks
(that
> is, the numbers) without shrinking anything else.  I could not find
this
> in either the Rhelp archives or in ?dotchart, which recmmends cex to
> avoid 'label overlap', but cex shrinks all the characters in the
plot.
> 
> Is there a way to do this?

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From nuin at terra.com.br  Wed Jun  9 16:36:58 2004
From: nuin at terra.com.br (Paulo Nuin)
Date: 09 Jun 2004 10:36:58 -0400
Subject: [R] Anova question
Message-ID: <1086791820.2476.6.camel@info5>

Hello everyone

This is my first message to the list and I believe the question I am
including is a simple one.

I have a matrix where I need to calculate ANOVA for the rows as the
columns represent a different treatment. I would like to know if there
is a command or a series of commans that I can enter to do that. 

At the moment I have a external script that extracts each row from the
matrix, transforms it in a column, another factor columns is add and the
text file is thrown to Rterm --vanilla.

Any help is appreciated.

Thanks a lot

Paulo Nuin



From Xavier.Abulker at fimat.com  Wed Jun  9 16:51:41 2004
From: Xavier.Abulker at fimat.com (Xavier.Abulker@fimat.com)
Date: Wed, 9 Jun 2004 16:51:41 +0200
Subject: [R] robust correlation in R
Message-ID: <OFDD75FAD7.A91C955B-ONC1256EAE.0050DBD4@fimat-france.fr>





Dear R user group,

I'm looking for a robust mesure of correlation in R.
I found a very interesting article by Dr Rich Herrington on
http://www.unt.edu/benchmarks/archives/2001/december01/rss.htm and I'd like
to implement
exaclty this method but my problem is that everything is here developped in
R language and is very slow when the correlation matrix is important.
I'm wondering if someone has maybe developped the same (or equivalent)
method using  functions in C like it is currently developped in r for the
covariance and variance.
Thank you for your help.

Xavier


*************************************************************************
Ce message et toutes les pieces jointes (ci-apres le "messag...{{dropped}}



From dmurdoch at pair.com  Wed Jun  9 16:52:19 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Wed, 09 Jun 2004 10:52:19 -0400
Subject: Reshaping matrix (was: [R] fast mkChar)
In-Reply-To: <1086789150.3534.4.camel@info5>
References: <C698D707214E6F4AB39AB7096C3DE5A5568755@phost015.EVAFUNDS.intermedia.net>
	<1086789150.3534.4.camel@info5>
Message-ID: <sj8ec054oalc6cobvqi7g4l39jlpofn5k9@4ax.com>

On 09 Jun 2004 09:52:27 -0400, Paulo Nuin <nuin at terra.com.br> wrote :

>Hello everyone
>
>This is my first message to the list and I believe the question I am
>including is a simple one.
>
>I have a matrix where I need to calculate ANOVA for the rows as the
>columns represent a different treatment. I would like to know if there
>is a command or a series of commans that I can enter to do that. 
>
>At the moment I have a external script that extracts each row from the
>matrix, transforms it in a column, another factor columns is add and the
>text file is thrown to Rterm --vanilla.

Welcome to the list.  One suggestion:  you should use a subject line
that matches the topic of your question, or your question is likely to
get lost.

Most of the modelling functions in R assume that data is in a vector.
You can convert a matrix to a vector using the as.numeric() function.
To get the treatments associated with each one, you could apply
as.numeric() to the col() function. You should also convert this to a
factor, for the anova.

For example,

> original <- matrix(1:12, 3, 4)
> vector <- as.numeric(original)
> treatments <- as.factor(as.numeric(col(original)))
> vector
 [1]  1  2  3  4  5  6  7  8  9 10 11 12
> treatments
 [1] 1 1 1 2 2 2 3 3 3 4 4 4
Levels: 1 2 3 4
> anova(lm(vector ~ treatments))
Analysis of Variance Table

Response: vector
           Df Sum Sq Mean Sq F value    Pr(>F)    
treatments  3    135      45      45 2.356e-05 ***
Residuals   8      8       1                      
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1 

You can read the help pages on each of these functions to see what
options are available.

Duncan Murdoch



From Xavier.Abulker at fimat.com  Wed Jun  9 16:59:41 2004
From: Xavier.Abulker at fimat.com (Xavier.Abulker@fimat.com)
Date: Wed, 9 Jun 2004 16:59:41 +0200
Subject: [R] robust correlation in R
Message-ID: <OFCA70BCF6.8C810F30-ONC1256EAE.00525719@fimat-france.fr>






Dear R user group,

I'm looking for a robust mesure of correlation in R.
I found a very interesting article by Dr Rich Herrington on
http://www.unt.edu/benchmarks/archives/2001/december01/rss.htm and I'd like
to implement
exaclty this method but my problem is that everything is here developped in
R language and is very slow when the correlation matrix is important.
I'm wondering if someone has maybe developped the same (or equivalent)
method using  functions in C like it is currently developped in r for the
covariance and variance.
Thank you for your help.

Xavier



*************************************************************************
Ce message et toutes les pieces jointes (ci-apres le "messag...{{dropped}}



From 0034058 at fudan.edu.cn  Wed Jun  9 16:59:43 2004
From: 0034058 at fudan.edu.cn (vinkwai wong)
Date: Wed, 09 Jun 2004 22:59:43 +0800
Subject: [R] ask for data manipulation!
Message-ID: <0HZ100JDXQPCF0@mail.fudan.edu.cn>

i have such a data:
V1   V2   V3  V4
a    b    c   e
a    f    d   NA
b    d    e   f
h    d    NA  NA
a    d    f   e

(V1,V2,V3,V4 can be any one of a, b, c, d, e, f, g, h. that is to say ,it has 8 possible values.)

and i want to change it the the following form:
a    b    c    d    e    f    g    h
8    7    6    2.5   5   2.5  2.5  2.5    #2.5is the mean of (4+3+2+1)
8    3    3     6    3    7   3    3      #3 is the mean of (5+4+3+2+1)
2.5  8    2.5   7    6    5   2.5   2.5    #2.5is the mean of (4+3+2+1)
3.5 3.5   3.5   7    3.5  3.5  3.5   8               #3.5is the mean of (6+54+3+2+1)
8   2.5   2.5   7    5    6   2.5    2.5


i want to know if there is some easy way to achieve my goal using R?



From edd at debian.org  Wed Jun  9 17:08:33 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 9 Jun 2004 10:08:33 -0500
Subject: [R] X-12-ARIMA
In-Reply-To: <40C721B6.9050605@bankofcanada.ca>
References: <ED7E0E44EAADFB46A6ABA12A647C1306122D18D6@CORREOINT.banxico.org.mx>
	<40C721B6.9050605@bankofcanada.ca>
Message-ID: <20040609150833.GA6441@sonny.eddelbuettel.com>

On Wed, Jun 09, 2004 at 10:41:58AM -0400, Paul Gilbert wrote:
> (subject changed from "[R] a doubt" )
> 
> The fortran source code for X-12-ARIMA seems to be available at 
> <http://www.census.gov/srd/www/x12a/x12down_unix.html>. Incorporating 
> this into a standard R package would be a much better approach than 
> trying to hack an OS specific version with system calls to a .exe file. 
> I expect there will be a fair amount of work in either case, and I don't 
> have time to look at it now. If anyone wants to do the real work then I 
> would be interested in using it (but using calls to a .exe would not be 
> very useful to me). The first thing to check would be the license, as I 
> have not looked at it. The next is probably the question of whether the 
> fortran is nicely organized in subroutines without IO, so R can call 
> those and avoid fortran IO problems.

Couldn't agree more.

GNU Gretl, a sleek intro- to intermediate econometrics program, actually has
interfaces to both x12a and tramo-seats, the latter being more of a problem
as it is not available in source from the Bank of Spain. Gretl works on (at
least) Windows, OS X and Linux.  I maintain the Debian package too.

Would be nice if someone looking for a decent code project took Census' x12a
sources and brought the dreadful text-file based interface forward into this
century by linking it to R.

Dirk


> 
> (Perhaps there are R users at the U.S. Census Bureau that would be 
> willing to help?)
> 
> Paul Gilbert
> 
> Mart?nez Ovando Juan Carlos wrote:
> 
> >Hello again,
> >
> >In a previous message I request your help, but I don't have been clear in 
> >my problem. Specifically, I'm trying to create an interface in R for the 
> >X-12-ARIMA and TRAMO SEATS, for the versions that run in MS-DOS. This 
> >problem awake in me the interest for make interfaces to comparing some 
> >Bayesian models for classification that where implemented in MS-DOS to. 
> >The question for you is if you know about the existence of an R function 
> >that allows me to run -in Windows- executable files in MS-DOS from the R 
> >command window. 
> >
> >The function 'dos' in Matlab that I have mention in the previous message 
> >allows to run '.exe' programs for a specific directory.  
> >
> >Many thanks.      
> >
> >Saludos,
> >
> >           Juan Carlos
> >
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 

-- 
FEATURE:  VW Beetle license plate in California



From cepl at surfbest.net  Wed Jun  9 17:09:42 2004
From: cepl at surfbest.net (Matej Cepl)
Date: Wed, 9 Jun 2004 11:09:42 -0400
Subject: Wrong question [Wasn't: Re: [R] fast mkChar]
In-Reply-To: <1086789150.3534.4.camel@info5>
References: <C698D707214E6F4AB39AB7096C3DE5A5568755@phost015.EVAFUNDS.intermedia.net>
	<1086789150.3534.4.camel@info5>
Message-ID: <200406091109.43353.cepl@surfbest.net>

On Wednesday 09 of June 2004 09:52, you wrote:
> This is my first message to the list and I believe the question
> I am including is a simple one.

http://www.r-project.org/posting-guide.html

-- 
Matej Cepl, http://www.ceplovi.cz/matej
GPG Finger: 89EF 4BC6 288A BF43 1BAB  25C3 E09F EF25 D964 84AC
138 Highland Ave. #10, Somerville, Ma 02143, (617) 623-1488
 
Of course I'm respectable. I'm old. Politicians, ugly buildings,
and whores all get respectable if they last long enough.
      --John Huston in "Chinatown."



From Wanzare at HCJP.com  Wed Jun  9 17:13:10 2004
From: Wanzare at HCJP.com (Manoj - Hachibushu Capital)
Date: Thu, 10 Jun 2004 00:13:10 +0900
Subject: [R] Multiple regression
Message-ID: <1CBA12F2D414914989C723D196B287DC05562B@jp-svr-ex1.hcjp.com>

Hi,
	I am trying to do multiple regression on a set of data using backward stepwise regression....however backward stepwise regression is critised for overfitting data. To actually observe the bias and to come up with a better method to use..Could you all stats experts kindly give me pointers to any alternative procedure (or references) to use over backward stepwise regression from your experience? 

TIA.

Manoj



From richard.kittler at amd.com  Wed Jun  9 17:14:48 2004
From: richard.kittler at amd.com (richard.kittler@amd.com)
Date: Wed, 9 Jun 2004 08:14:48 -0700
Subject: [R] Is there an R-version of rayplot
Message-ID: <858788618A93D111B45900805F85267A0BCB2D5C@caexmta3.amd.com>

Thanks for this example.  I have since found the 'arrows' function which is not as comprehensive as rayplot but seems to work well for basic vector field plots, e.g. 

 y <- rep(1:5,rep(5,5))  
 x <- rep(1:5,5)         

 x1 <- x-sqrt(x*x + y*y)/25
 y1 <- y-2*x/125

 plot(x,y,,main='Sample Vector Plot',type='n')
 arrows(x,y,x1,y1,length=0.1) 


--Rich

Richard Kittler 
AMD TDG
408-749-4099

-----Original Message-----
From: joerg van den hoff [mailto:j.van_den_hoff at fz-rossendorf.de] 
Sent: Wednesday, June 09, 2004 4:10 AM
Cc: Kittler, Richard; r-help at stat.math.ethz.ch
Subject: Re: [R] Is there an R-version of rayplot


maybe this q&d try helps?


#=================cut herer=============

vectorplot <- function (field) {

   #input is a (N x 4 array) of N vectors:
   #   field[,1:2] - x/y position  of vectors
   #   field[,3:4] - x/y componnent of vectors
   # plotted are the 2-D vectors attached to  the specified positions

   if (is.null(dim(field))||dim(field)[2] != 4) stop("N x 4 array expected")

   loc <- field[,1:2]
   val <- field[,3:4]

   alpha <- rbind(loc[,1],loc[,1]+val[,1])
   omega <- rbind(loc[,2],loc[,2]+val[,2])

   matplot(alpha,omega,type='l',lty=1,col='black') #the vector lines
   points(loc) #the start points
   points(loc+val,pch=20) #the end points
}

#example:

loc=matrix(rnorm(100),50,2)
field <- cbind(loc,loc)
vectorplot(field)

#=================cut herer=============

there are no nice arrow heads, of course, but one could construct some... for now, vectors start with open circles and end with filled circles.

joerg



From dimitris.rizopoulos at med.kuleuven.ac.be  Wed Jun  9 17:18:36 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Wed, 9 Jun 2004 17:18:36 +0200
Subject: [R] Multiple regression
References: <1CBA12F2D414914989C723D196B287DC05562B@jp-svr-ex1.hcjp.com>
Message-ID: <003901c44e35$0a8db9b0$ad133a86@www.domain>

Have a look at Prof. Harrell's book (Chapters 4 and 5)

@Book{harrell:01,
  author    = {F. E. Harrell, Jr.},
  title     = {Regression Modeling Strategies: With Applications to
                Linear Models, Logistic Regression and Survival
Analysis},
  year      = {2001},
  address   = {New York},
  publisher = {Springer-Verlag}
}

Best,
Dimitris

----
Dimitris Rizopoulos
Doctoral Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Manoj - Hachibushu Capital" <Wanzare at hcjp.com>
To: <r-help at stat.math.ethz.ch>
Sent: Wednesday, June 09, 2004 5:13 PM
Subject: [R] Multiple regression


> Hi,
> I am trying to do multiple regression on a set of data using
backward stepwise regression....however backward stepwise regression
is critised for overfitting data. To actually observe the bias and to
come up with a better method to use..Could you all stats experts
kindly give me pointers to any alternative procedure (or references)
to use over backward stepwise regression from your experience?
>
> TIA.
>
> Manoj
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ozric at web.de  Wed Jun  9 17:26:39 2004
From: ozric at web.de (Christian Schulz)
Date: Wed, 9 Jun 2004 17:26:39 +0200
Subject: [R]  market-basket analysis in R
In-Reply-To: <6B5A9304046AD411BD0200508BDFB6CB02955F46@gimli.middleearth.kssg.com>
References: <6B5A9304046AD411BD0200508BDFB6CB02955F46@gimli.middleearth.kssg.com>
Message-ID: <200406091726.39439.ozric@web.de>

Hi,

when i want start more than one year ago
with a fuzzy-association rule algorithm to
learn programming in r  i have to work in business
and it's too bad that here  until now nobody need a tool 
for market basket analysis  in  r  :-).

regards,Christian


Am Mittwoch, 9. Juni 2004 15:38 schrieb Wayne Jones:
> Hi Christian,
>
> Thanks for that. The links will be very useful.
>
> Im surprised that no package exists in R though!!
>
> Regards
>
> Wayne
>
>
> -----Original Message-----
> From: Christian Schulz [mailto:ozric at web.de]
> Sent: 09 June 2004 13:52
> To: r-help at stat.math.ethz.ch
> Cc: Wayne Jones
> Subject: Re: [R] market-basket analysis in R
>
>
> I don't know any package!?
>
>  but you could try:
>
> http://fuzzy.cs.uni-magdeburg.de/~borgelt/software.html
> http://www.cs.umb.edu/~laur/ARtool/
> http://www.cs.waikato.ac.nz/ml/weka/
>
> regards,
> Christian
>
> Am Mittwoch, 9. Juni 2004 13:27 schrieb Wayne Jones:
> > Hi there fellow R-users,
> >
> > Does anyone know if there exists a package for associated rules data
>
> mining
>
> > (market basket analysis) in R.
> >
> > I have tried searching CRAN but with no luck.
> >
> > Regards
> >
> > Wayne
> >
> >
> > KSS Ltd
> > Seventh Floor  St James's Buildings  79 Oxford Street  Manchester  M1 6SS
> > England Company Registration Number 2800886
> > Tel: +44 (0) 161 228 0040	Fax: +44 (0) 161 236 6305
> > mailto:kssg at kssg.com		http://www.kssg.com
> >
> >
> > The information in this Internet email is confidential and
> > m...{{dropped}}
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html



From f.harrell at vanderbilt.edu  Wed Jun  9 17:42:49 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Wed, 09 Jun 2004 10:42:49 -0500
Subject: [R] Dot chart question
In-Reply-To: <s0c6e744.088@MAIL.NDRI.ORG>
References: <s0c6e744.088@MAIL.NDRI.ORG>
Message-ID: <40C72FF9.30105@vanderbilt.edu>

Peter Flom wrote:
> Running R 1.8.1 on  a Windows machine
> 
> In dotchart, I would like to shrink the labels on the tick marks (that
> is, the numbers) without shrinking anything else.  I could not find this
> in either the Rhelp archives or in ?dotchart, which recmmends cex to
> avoid 'label overlap', but cex shrinks all the characters in the plot.
> 
> Is there a way to do this?
> 
> Thanks
> 
> Peter
> 
> Peter L. Flom, PhD
> Assistant Director, Statistics and Data Analysis Core
> Center for Drug Use and HIV Research
> National Development and Research Institutes
> 71 W. 23rd St
> www.peterflom.com
> New York, NY 10010
> (212) 845-4485 (voice)
> (917) 438-0894 (fax)

There are probably many ways, but one is to use dotchart2 in Hmisc, I think.

Frank

> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From rwang at math.ucalgary.ca  Wed Jun  9 17:56:12 2004
From: rwang at math.ucalgary.ca (Rui)
Date: Wed, 9 Jun 2004 09:56:12 -0600
Subject: [R] About dll from c++ routine
Message-ID: <000001c44e3a$49e48800$f63d9f88@math.ucalgary.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040609/f6f22568/attachment.pl

From prechelt at pcpool.mi.fu-berlin.de  Wed Jun  9 17:58:26 2004
From: prechelt at pcpool.mi.fu-berlin.de (Lutz Prechelt)
Date: Wed, 9 Jun 2004 17:58:26 +0200
Subject: [R] Comparing two pairs of non-normal datasets in R?
Message-ID: <85D25331FFB7AE4C900EA467D4ADA3920459CA@circle.pcpool.mi.fu-berlin.de>

> Here are the boxplots if that helps:
> http://www.ps.masny.dk/guests/misc/A1.png
> http://www.ps.masny.dk/guests/misc/A2.png
> http://www.ps.masny.dk/guests/misc/C1.png
> http://www.ps.masny.dk/guests/misc/C2.png

Here is how I would do it:
It looks like your distributions can be characterized by
just a single parameter.

Then your question is:
   Is the change in the parameter value from A1 to A2
   larger than that from C1 to C2?
(Larger meaning what?: Difference? Ratio?)

So I suggest
- you estimate your four parameters
- compute your two differences or ratios
- compute the difference of those
- and repeat all this for many resamples (bootstrapping)

Then you get a bootstrap distribution of the 
value that you hope is significantly non-zero.
>From that distribution you can read your p-value.
(Preferably based on a BCa confidence interval)

library(boot)

Does that make sense?

  Lutz



From MSchwartz at MedAnalytics.com  Wed Jun  9 18:06:00 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Wed, 09 Jun 2004 11:06:00 -0500
Subject: [R] Re: fighting with ps.options and xlim/ylim
In-Reply-To: <40C71EF3.4000903@statistik.uni-dortmund.de>
References: <40C71B06.3020609@yale.edu>
	<40C71EF3.4000903@statistik.uni-dortmund.de>
Message-ID: <1086797160.29898.115.camel@localhost.localdomain>

On Wed, 2004-06-09 at 09:30, Uwe Ligges wrote:
> ivo welch wrote:
> 
> > 
> > Thanks again for all the messages.
> > 
> > Is the 4% in par('usr') hardcoded?  if so, may I suggest making this a 
> > user-changeable parameter for x and y axis?
> 
> See ?par and its argumets xaxp, yaxp which can be set to "i".

Quick correction. That should be xaxs and yaxs. See my initial reply.

xaxp and yaxp are for the positions of the tick marks.

> > I looked at psfrag, and it seems like a great package.  alas, I have 
> > switched to pdflatex, and pdffrag does not exist.  :-(

One option to point out, is that if the functionality in psfrag is
important to you, you can use 'ps2pdf' to convert a ps file to a pdf
file. ps2pdf filters the ps file through ghostscript to create the pdf
file.

It means a three step process (latex, dvips and ps2pdf), but it can
provide additional functionality that pdflatex does not support, such as
the use of \special as in the package 'pstricks'. pdf does not have any
programming language functionality as does postscript, so there are some
tradeoffs and likely why there is no pdffrag.

Food for thought.

> > I also discovered that there is a pdf device now.  neat.  
> 
> Since R-1.3.0, as the News file tells us.
> 
> Uwe Ligges

HTH,

Marc



From ramasamy at cancer.org.uk  Wed Jun  9 18:07:49 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: 09 Jun 2004 17:07:49 +0100
Subject: [R] Anova question
In-Reply-To: <1086791820.2476.6.camel@info5>
References: <1086791820.2476.6.camel@info5>
Message-ID: <1086797268.5496.27.camel@vpn202001.lif.icnet.uk>

On Wed, 2004-06-09 at 15:36, Paulo Nuin wrote:
> Hello everyone
> 
> This is my first message to the list and I believe the question I am
> including is a simple one.
> 
> I have a matrix where I need to calculate ANOVA for the rows as the
> columns represent a different treatment. I would like to know if there
> is a command or a series of commans that I can enter to do that. 

let us say x represents the measurements. Further suppose that the first
10 columns represent Treatment A and the next 10 Treatment B.

x <- rnorm(20)                         # simulated value for response
g <- as.factor( rep(c("A", "B"), each=10) ) # treatments

model <- lm(x ~ g)
summary(model)
anova(model)

For more information, see help(lm) and help(aov). The books cited in
these references are pretty good and worth looking into. You might also
find the introductory book on R by Peter Daalgard or online
documentations helpful.

> At the moment I have a external script that extracts each row from the
> matrix, transforms it in a column, another factor columns is add and the
> text file is thrown to Rterm --vanilla.

You do not have to do this manually. You can use the apply function to
apply the ANOVA each row. Example

g <- as.factor( rep(c("A", "B"), each=10) ) # treatments
apply( my.matrix, 1, function(x) {
 model <- lm(x ~ g)
 print(summary(model))
})

If you have hundreds of rows, then you way just store specific output
(test statistics, p-value) instead of printing it. Example coef(model),
summary(model)$coefficients.



From ripley at stats.ox.ac.uk  Wed Jun  9 18:13:50 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 9 Jun 2004 17:13:50 +0100 (BST)
Subject: [R] About dll from c++ routine
In-Reply-To: <000001c44e3a$49e48800$f63d9f88@math.ucalgary.ca>
Message-ID: <Pine.LNX.4.44.0406091709280.2973-100000@gannet.stats>

On Wed, 9 Jun 2004, Rui wrote:

> Hi folks,
>  
> My system is Windows98 + R1.9.0. 
> The path for my system is c:\perl\bin; c:\mingw\bin; c:\rtools;
> c:\windows; c:\windows\command; c:\rw1090\bin.
>  
> I created three files followed the examples in "Writing R extensions" in
> the directory c:\temp:
> // X.hh
> class X {
> public: X (); ~X ();
> };
> class Y {
> public: Y (); ~Y ();
> };
>  
> // X.cc
> #include <iostream>
> #include "X.hh"
> static Y y;
> X::X() { std::cout << "constructor X" << std::endl; }
> X::~X() { std::cout << "destructor X" << std::endl; }
> Y::Y() { std::cout << "constructor Y" << std::endl; }
> Y::~Y() { std::cout << "destructor Y" << std::endl; }
>  
>  
> // X_main.cc:
> #include "X.hh"
> extern "C" {
> void X_main () {
> X x;
> }
> } // extern "C"



> Then I followed the instructions in "R for windows FAQ" using the
> command:
> C:\temp>RCMD SHLIB X.CC X_MAIN.CC
>  
> However, I got the following message: 
> In file included from x_main.cc:2:
> x.hh:7:3: Warning: no newline at end of the file
> making convolve.d from convolve.cc
> g++   -IC: /R/RW1090/src/include -Wall -o2  -c  x.cc  -o  x.o
> In file included from x.cc:3:
> x.hh:7:3: Warning: no newline at end of the file
> make: *** No rule to make target 'x_main.o?, needed by 'X.a?. Stop
>  
> I guess I failed to get the dll file finally. 
>  
> Any suggestion would be greatly appreciated.

Add newlines at the end of your files as warned.

It would help to name your files consistently, e.g. x_main.cc and not 
X_main.cc and X_MAIN.CC.

If you follow the instructions carefully, this does work.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From HStevens at MUOhio.edu  Wed Jun  9 18:45:04 2004
From: HStevens at MUOhio.edu (Martin Henry H. Stevens)
Date: Wed, 9 Jun 2004 12:45:04 -0400
Subject: [R] lsoda with arbitrary zero thresholds
Message-ID: <5BA2C97E-BA34-11D8-A219-000A958F43CC@MUOhio.edu>

using R 2.0.0
I am trying to do some population modeling with lsoda, where I set 
arbitrary zero population sizes when values get close to zero, but am 
having no luck.
As an example of what I have tried, I use code below from the help page 
on lsoda in which I include my modification bordered by ###

parms <- c(k1=0.04, k2=1e4, k3=3e7)
my.atol <- c(1e-6,  1e-10,  1e-6)
times <- seq(0,)

lsexamp <- function(t, y, p)
   { ### The next line is where I try to insert the threshold
ifelse(y < 0.4,  0, y)
###### all else is unchanged
     yd1 <- -p["k1"] * y[1] + p["k2"] * y[2]*y[3]
     yd3 <- p["k3"] * y[2]^2
     list(c(yd1,-yd1-yd3,yd3),c(massbalance=sum(y)))
   }
out <- lsoda(c(.5,0,.5),times,lsexamp, parms, rtol=1e-4, atol= my.atol) 
# Initial values differ from help page
matplot(out[,1],out[,2:5], type="l")
out[dim(out)[1],] # The intent of my could was to cause population 1 to 
fall to zero as soon as it reached < 0.4

Any thoughts would be appreciated. Thanks!
Hank Stevens


Dr. Martin Henry H. Stevens, Assistant Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/botany/bot/henry.html
http://www.muohio.edu/ecology/
http://www.muohio.edu/botany/
"E Pluribus Unum"



From jfox at mcmaster.ca  Wed Jun  9 18:47:39 2004
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 09 Jun 2004 12:47:39 -0400
Subject: [R] Specifying xlevels in effects library
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7E69@usrymx25.merck.com>
Message-ID: <web-46087207@cgpsrv2.cis.mcmaster.ca>

Dear David and Andy,

First, my apologies for somehow missing the original message.

Andy has it right: xlevels refers to the predictors in the term
corresponding to the effect to be computed. His solution is clever -- I
wouldn't have thought of it -- but it could get you into trouble in a
more complicated model.

Another possibility is to specify an effect that includes both
predictors (i.e., the Sex*Age effect, which is higher-order to the
terms actually in the model), and to set Age to the desired value in
xlevels. At present, effect() wants more than one value for each
variable in the xlevels list. I'll look at changing that, but at
present you could do something like

effect("Sex*Age", mod, xlevels=(Age=c(120,120)))

This should get you what you want, albeit redundantly.

I hope that this helps,
 John


On Wed, 9 Jun 2004 09:40:49 -0400
 "Liaw, Andy" <andy_liaw at merck.com> wrote:
> Prof. Fox will be able to give the definitive answer, but from my
> reading of
> ?effect, xlevels refers to the values of the factor whose effect
> you're
> interested in, not the ones being `marginalized'.  I believe you need
> to
> play with the `typical' argument.
> 
> HTH,
> Andy
> 
> > From: David J. Netherway
> > 
> > library(effects)
> > mod <- lm(Measurement ~ Age + Sex, data=d)
> > e <-effect("Sex",mod)
> >  
> > The effect is evaluated at the mean age.
> >  
> >  > e
> > Sex effect
> > Sex
> >        F        M  
> > 43.33083 44.48531  
> >  >
> >  > e$model.matrix
> >    (Intercept)      Age SexM
> > 1            1 130.5859    0
> > 23           1 130.5859    1
> >  
> > To evaluate the effect at Age=120 I tried:
> > e <-effect("Sex",mod,xlevels=list(Age=c(120)))
> > but the effect was still evaluated at 130.5859.
> >  
> > Is this an incorrect usage of xlevels?
> > 
> > Thanks, David
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html


--------------------------------
John Fox
Department of Sociology
McMaster University
Hamilton, Ontario, Canada
http://socserv.mcmaster.ca/jfox/



From picard at lanl.gov  Wed Jun  9 18:58:18 2004
From: picard at lanl.gov (Rick Picard)
Date: Wed, 09 Jun 2004 10:58:18 -0600
Subject: [R] question related to S-Plus
Message-ID: <40C741AA.5040304@lanl.gov>

Dear r-help,
      Having used S-Plus for many years, it has
been suggested to me that I could benefit from
learning R.  A question to this end, though.
Would all of my existing S-Plus arrays, functions,
and so on have to be re-created from scratch in R,
or is there a way to copy them into the .Rdata
directory?  The answer to this question has major
implications for the extent to which R is attractive
in the near term, and any enlightenment would be
much appreciated.

Thanks,
Rick Picard



From HStevens at MUOhio.edu  Wed Jun  9 19:43:42 2004
From: HStevens at MUOhio.edu (Martin Henry H. Stevens)
Date: Wed, 9 Jun 2004 13:43:42 -0400
Subject: [R] lsoda with arbitrary zero thresholds (with psuedo-solution)
In-Reply-To: <5BA2C97E-BA34-11D8-A219-000A958F43CC@MUOhio.edu>
References: <5BA2C97E-BA34-11D8-A219-000A958F43CC@MUOhio.edu>
Message-ID: <8CAAC445-BA3C-11D8-A219-000A958F43CC@MUOhio.edu>

I have  a new and less distressing, but potentially more interesting, 
problem.
I realized the major flaw my old "solution" and now have a solution 
that kind of works but is rather inelegant and I think may be 
problematic in difficult systems.
Borrowing from the lsoda example again I once again highlight the code 
that I have changed to put in place arbitrary thresholds:

parms <- c(k1=0.04, k2=1e4, k3=3e7)
my.atol <- c(1e-6,  1e-10,  1e-6)
times <- seq(0,1000)
lsexamp <- function(t, y, p)
   {
     if(y[1] < .4) yd1 <- -y[1] ### These if, else statements are new
     else yd1 <- -p["k1"] * y[1] + p["k2"] * y[2]*y[3]
     if(y[3] < .4) yd3 <- -y[3]   ### These if,else statements are new
     else yd3 <- p["k3"] * y[2]^2
     list(c(yd1,-yd1-yd3,yd3),c(massbalance=sum(y)))
   }
out <- lsoda(c(.5,0,.5),times,lsexamp, parms, rtol=1e-4, atol= my.atol, 
hmax=.1)
matplot(out[,1],out[,2:5], type="l")
out[dim(out)[1],] # The intent of my could was to cause population 1 to 
fall to zero as soon as it reached < 0.4. However, the populations 1 
and 2 reach approximations of 0 (4e-281 and 5e-11).

So, I have two questions:
Can I set thresholds in a more elegant and simpler way?
Are the approximate zero values close enough?

Thank you kindly, as ever.
Sincerely,
Hank


On Jun 9, 2004, at 12:45 PM, Martin Henry H. Stevens wrote:

> using R 2.0.0
> I am trying to do some population modeling with lsoda, where I set 
> arbitrary zero population sizes when values get close to zero, but am 
> having no luck.
> As an example of what I have tried, I use code below from the help 
> page on lsoda in which I include my modification bordered by ###
>
> parms <- c(k1=0.04, k2=1e4, k3=3e7)
> my.atol <- c(1e-6,  1e-10,  1e-6)
> times <- seq(0,)
>
> lsexamp <- function(t, y, p)
>   { ### The next line is where I try to insert the threshold
> ifelse(y < 0.4,  0, y)
> ###### all else is unchanged
>     yd1 <- -p["k1"] * y[1] + p["k2"] * y[2]*y[3]
>     yd3 <- p["k3"] * y[2]^2
>     list(c(yd1,-yd1-yd3,yd3),c(massbalance=sum(y)))
>   }
> out <- lsoda(c(.5,0,.5),times,lsexamp, parms, rtol=1e-4, atol= 
> my.atol) # Initial values differ from help page
> matplot(out[,1],out[,2:5], type="l")
> out[dim(out)[1],] # The intent of my could was to cause population 1 
> to fall to zero as soon as it reached < 0.4
>
> Any thoughts would be appreciated. Thanks!
> Hank Stevens
>
>
> Dr. Martin Henry H. Stevens, Assistant Professor
> 338 Pearson Hall
> Botany Department
> Miami University
> Oxford, OH 45056
>
> Office: (513) 529-4206
> Lab: (513) 529-4262
> FAX: (513) 529-4243
> http://www.cas.muohio.edu/botany/bot/henry.html
> http://www.muohio.edu/ecology/
> http://www.muohio.edu/botany/
> "E Pluribus Unum"
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>
Dr. Martin Henry H. Stevens, Assistant Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/botany/bot/henry.html
http://www.muohio.edu/ecology/
http://www.muohio.edu/botany/
"E Pluribus Unum"



From gregory_r_warnes at groton.pfizer.com  Wed Jun  9 20:11:41 2004
From: gregory_r_warnes at groton.pfizer.com (Warnes, Gregory R)
Date: Wed, 9 Jun 2004 14:11:41 -0400 
Subject: [R] [R-pkgs] gregmisc 1.11.2 including read.xls()
Message-ID: <D7A3CFD7825BD6119B880002A58F06C20C52150A@groexmb02.pfizer.com>


Release 1.11.2 of the gregmisc() package of functions is now available on
CRAN at http://cran.r-project.org/src/contrib/Descriptions/gregmisc.html for
both Unix and Windows systems.

The most notable enhancement provided by this release is:

- read.xls(), a function to read Microsoft Excel files by
  translating them to csv files via the xls2csv.pl script has
  been added.  I've also provided Unix and MS-Windows scripts 
  in <R_LIBRARY_DIR>/gregmisc/bin which allow you to call 
  xls2csv directly.  The code uses perl libraries which are
  included in the package, so perl must be in the execution path.

Other enhancements include:

- Improvements to CrossTable() by Marc Schwartz <MSchwartz at MedAnalytics.com>

- Improvements to ooplot() by Lodewijk Bonebakker <bonebakker at comcast.net>

- plotCI() and plotmeans() now have improved argument handling.

- The running() function now has an additional parameter `simplify'
  which controls whether the returned values are simplified into a
  vector/matrix or left as a list.

- A makefile that will download and attempt to install all available
  packages from CRAN and Bioconductor is now provided in
  $PACKAGE$/gregmisc/tools/

- space() can now space points along the 'y' dimension.

- Fix an error in the permutations code for repeats.allow=T and r>2.
  Both the bug report and fix are from Elizabeth Purdom
  <epurdom at stanford.edu>.

- Various fixes for compatibility with R 1.9.X.

See the NEWS and ChangeLog files in the .tar.gz file for additional details.

-Greg

Gregory R. Warnes
Manager, Non-Clinical Statistics
Pfizer Global Research and Development



LEGAL NOTICE\ Unless expressly stated otherwise, this messag...{{dropped}}

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://www.stat.math.ethz.ch/mailman/listinfo/r-packages



From brahm at alum.mit.edu  Wed Jun  9 20:29:34 2004
From: brahm at alum.mit.edu (David Brahm)
Date: Wed, 9 Jun 2004 14:29:34 -0400
Subject: [R] Re: R equivalent of Splus rowVars function
References: <3A822319EB35174CA3714066D590DCD504AF7E4B@usrymx25.merck.com>
Message-ID: <16583.22286.988964.819482@arbres1a.fmr.com>


Mark Leeds <mleeds at mlp.com> wrote (to S-News):
> does anyone know the R equivalent of the SPlus rowVars function ?

Andy Liaw <andy_liaw at merck.com> replied:
> More seriously, I seem to recall David Brahms at one time had created an R
> package with these dimensional summary statistics, using C code.  (And I
> pointed him to the `two-pass' algorithm for variance.)

Here are the functions that didn't make it into R's base package, which should
be very similar to the S-Plus functions of the same names.  The "twopass"
argument determines whether Andy's two-pass algorithm (Chan Golub & LeVegue) is
used (it's slower but more accurate).  In real life I set the "twopass" default
to FALSE, because in finance noise is always bigger than signal.

I am cc'ing to R-help, as this is really an R question.
-- 
                              -- David Brahm (brahm at alum.mit.edu)


colVars <- function(x, na.rm=FALSE, dims=1, unbiased=TRUE, SumSquares=FALSE,
                    twopass=TRUE) {
  if (SumSquares) return(colSums(x^2, na.rm, dims))
  N <- colSums(!is.na(x), FALSE, dims)
  Nm1 <- if (unbiased) N-1 else N
  if (twopass) {x <- if (dims==length(dim(x))) x - mean(x, na.rm=na.rm) else
                     sweep(x, (dims+1):length(dim(x)), colMeans(x,na.rm,dims))}
  (colSums(x^2, na.rm, dims) - colSums(x, na.rm, dims)^2/N) / Nm1
}

rowVars <- function(x, na.rm=FALSE, dims=1, unbiased=TRUE, SumSquares=FALSE,
                    twopass=TRUE) {
  if (SumSquares) return(rowSums(x^2, na.rm, dims))
  N <- rowSums(!is.na(x), FALSE, dims)
  Nm1 <- if (unbiased) N-1 else N
  if (twopass) {x <- if (dims==0) x - mean(x, na.rm=na.rm) else
                     sweep(x, 1:dims, rowMeans(x,na.rm,dims))}
  (rowSums(x^2, na.rm, dims) - rowSums(x, na.rm, dims)^2/N) / Nm1
}

colStdevs <- function(x, ...) sqrt(colVars(x, ...))

rowStdevs <- function(x, ...) sqrt(rowVars(x, ...))



From zhuw at mail.smu.edu  Wed Jun  9 15:49:17 2004
From: zhuw at mail.smu.edu (Zhu Wang)
Date: Wed, 09 Jun 2004 13:49:17 +0000
Subject: [R] Building package on Windows: No rule to make target '-llapack'
In-Reply-To: <200406091003.i59A3c06014323@hypatia.math.ethz.ch>
References: <200406091003.i59A3c06014323@hypatia.math.ethz.ch>
Message-ID: <1086788957.2450.12.camel@zwang.stat.smu.edu>

Dear all,

I have a problem to build a package on Windows XP while there is no problem on Linux. The Makefile is something like:
###########
LIBNAME=cts

PKG_LIBS = $(LAPACK_LIBS) $(BLAS_LIBS) $(FLIBS)

OBJS=file1.o ... file20.o -llapack -lblas

$(LIBNAME)$(SHLIB_EXT): $(OBJS)
        $(SHLIB_LD) $(SHLIB_LDFLAGS) -o $@ $(OBJS) $(FLIBS)

clean:
        @rm -f *.o *.$(SHLIB_EXT)

realclean: clean
#############

To build the package on Windows XP, I have followed the instructions to install tools/software required and it seems the 
'make' worked fine, except for the error message:

make[3]: No rule to make target 'llapack', needed by 'cts.a'. stop.

Now I think maybe two problems: one is that maybe I do not have Lapack and Blas installed
on Windows XP and second is that I do not set up a correct file, something like 'configure'.
Maybe there are more problems. I have read some files in \src\gnuwin32, but I did not find
what I needed.

Thanks for any advice.

Zhu Wang

Statistical Science Department
Southern Methodist University



From rossini at blindglobe.net  Wed Jun  9 20:55:15 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Wed, 09 Jun 2004 11:55:15 -0700
Subject: [R] Building package on Windows: No rule to make target '-llapack'
In-Reply-To: <1086788957.2450.12.camel@zwang.stat.smu.edu> (Zhu Wang's
	message of "Wed, 09 Jun 2004 13:49:17 +0000")
References: <200406091003.i59A3c06014323@hypatia.math.ethz.ch>
	<1086788957.2450.12.camel@zwang.stat.smu.edu>
Message-ID: <854qpkwoh8.fsf@servant.blindglobe.net>

Zhu Wang <zhuw at mail.smu.edu> writes:

> Now I think maybe two problems: one is that maybe I do not have Lapack and Blas installed

I think this definitely needs to be solved.

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From vograno at evafunds.com  Wed Jun  9 20:59:10 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Wed, 9 Jun 2004 11:59:10 -0700
Subject: [R] fast mkChar
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A5568790@phost015.EVAFUNDS.intermedia.net>

Thank you for the lead, Peter. It may be useful for other packages I
write.

As to the strings, I think I have to take what is already there. I agree
that strings would be better managed in malloc-style fashion (probably
with reference counter) and not by gc(). However I don't want to have a
system with two different string classes, such close relatives seldom
coexist peacefully.

BTW, the slowness of mkChar explains why R is so slow when it needs to
compute names for long vectors.

Thank you for an interesting discussion,
Vadim 

> -----Original Message-----
> From: Peter Dalgaard [mailto:p.dalgaard at biostat.ku.dk] 
> Sent: Tuesday, June 08, 2004 3:35 PM
> To: Vadim Ogranovich
> Cc: R-Help
> Subject: Re: [R] fast mkChar
> 
> "Vadim Ogranovich" <vograno at evafunds.com> writes:
> 
> > I am no expert in memory management in R so it's hard for 
> me to tell 
> > what is and what is not doable. From reading the code of 
> allocVector() 
> > in memory.c I think that the critical part is to vectorize 
> > CLASS_GET_FREE_NODE and use the vectorized version along 
> the lines of 
> > the code fragment below (taken from memory.c).
> > 
> > 	if (node_class < NUM_SMALL_NODE_CLASSES) {
> > 	    CLASS_GET_FREE_NODE(node_class, s);
> > 
> > If this is possible than the rest is just a matter of code 
> refactoring.
> > 
> > By vectorizing I mean writing a macro 
> CLASS_GET_FREE_NODE2(node_class, 
> > s, n) which in one go allocates n little objects of class 
> node_class 
> > and "inscribes" them into the elements of vector s, which 
> is assumed 
> > to be long enough to hold these objects.
> > 
> > If this is doable than the only missing piece would be a 
> new function 
> > setChar(CHARSXP rstr, const char * cstr) which copies 
> 'cstr' into 'rstr'
> > and (re)allocates the heap memory if necessary. Here the setChar() 
> > macro is safe since s[i]-s are all brand new and thus are 
> not shared 
> > with any other object.
> 
> I had a similar idea initially, but I don't think it can fly: 
> First, allocating n objects at once is not likely to be much 
> faster than allocating them one-by-one, especially when you 
> consider the implications of having to deal with 
> near-out-of-memory conditions.
> Second, you have to know the string lengths when allocating, 
> since the structure of a vector object (CHARSXP) is a header 
> immediately followed by the data.
> 
> A more interesting line to pursue is that - depending on what 
> it really is that you need - you might be able to create a 
> different kind of object that could "walk and quack" like a 
> character vector, but is stored differently internally. E.g. 
> you could set up a representation that is just a block of 
> pointers, pointing to strings that are being maintained in 
> malloc-style.
> 
> Have a look at External pointers and finalization.
> 
> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: 
> (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: 
> (+45) 35327907
> 
>



From ryszard.czerminski at pharma.novartis.com  Wed Jun  9 21:24:08 2004
From: ryszard.czerminski at pharma.novartis.com (ryszard.czerminski@pharma.novartis.com)
Date: Wed, 9 Jun 2004 15:24:08 -0400
Subject: [R] how to initialize random seed properly ?
Message-ID: <OFA819C8AE.C1425078-ON85256EAE.006A0801-85256EAE.006AC76D@EU.novartis.net>

I want to start R processes on multiple processors from single shell 
script
and I want all of them to have different random seeds.
One way of doing this is

        sleep 2 # (with 'sleep 1' I am often getting the same number)
               ...
        set.seed(unclass(Sys.time()))

Is there a simpler way without a need to sleep between invoking
different R processes ?

Ryszard



From ivo.welch at yale.edu  Wed Jun  9 21:29:59 2004
From: ivo.welch at yale.edu (ivo welch)
Date: Wed, 09 Jun 2004 15:29:59 -0400
Subject: [R] direct data frame entry
Message-ID: <40C76537.6090700@yale.edu>


hi:  I searched the last 2 hours for a way to enter a data frame 
directly in my program.  (I know how to read from a file.)  that is, I 
would like to say something like

    d <- this.is.a.data.frame(   c("obs1name", 0.2, 0.3),
                                          c("obs2name", 0.4, 1.0),
                                          c("obs3name", 0.6, 2.0) , 
varnames=c("name", "val1", "val2")  );

everything I have tried sofar (usually, building with rbind and then 
names(d)) has come out with factors for the numbers, which is obviously 
not what I want.   this must be a pretty elementary request, so it 
should probably be an example under data.frame (or read.table).  of 
course, it is probably somewhere---just I have do not remember it and 
could not find it after 2 hours of searching.  I also tried the r-help 
archives---at the very least, I hope we will get the answer there for 
future lookups.

regards, /iaw



From andy_liaw at merck.com  Wed Jun  9 21:40:02 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 9 Jun 2004 15:40:02 -0400
Subject: [R] direct data frame entry
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7E7C@usrymx25.merck.com>

?data.frame says:

Usage:

     data.frame(..., row.names = NULL, check.rows = FALSE, check.names =
TRUE)

Arguments:

     ...: these arguments are of either the form 'value' or
          'tag=value'.  Component names are created based on the tag
          (if present) or the deparsed argument itself.

which means you need to do the `transpose' of what you did: give
data.frame() columns, rather than rows.  E.g.,

dat <- data.frame(x=factor(c("A", "B", "A", "C"), y=1:4,
rownames=LETTERS[1:4])

It's hard to build a data frame by row, because one needs to check and make
sure data in each column are consistent, that data in a factor column have
the right levels, etc.

Andy

> From: ivo welch
> 
> hi:  I searched the last 2 hours for a way to enter a data frame 
> directly in my program.  (I know how to read from a file.)  
> that is, I 
> would like to say something like
> 
>     d <- this.is.a.data.frame(   c("obs1name", 0.2, 0.3),
>                                           c("obs2name", 0.4, 1.0),
>                                           c("obs3name", 0.6, 2.0) , 
> varnames=c("name", "val1", "val2")  );
> 
> everything I have tried sofar (usually, building with rbind and then 
> names(d)) has come out with factors for the numbers, which is 
> obviously 
> not what I want.   this must be a pretty elementary request, so it 
> should probably be an example under data.frame (or read.table).  of 
> course, it is probably somewhere---just I have do not remember it and 
> could not find it after 2 hours of searching.  I also tried 
> the r-help 
> archives---at the very least, I hope we will get the answer there for 
> future lookups.
> 
> regards, /iaw
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From tplate at blackmesacapital.com  Wed Jun  9 21:42:22 2004
From: tplate at blackmesacapital.com (Tony Plate)
Date: Wed, 09 Jun 2004 13:42:22 -0600
Subject: [R] direct data frame entry
In-Reply-To: <40C76537.6090700@yale.edu>
References: <40C76537.6090700@yale.edu>
Message-ID: <6.1.0.6.2.20040609133839.04445488@mailhost.blackmesacapital.com>

easy to do it by column:

 > d <- 
data.frame(name=c("obs1name","obs2name","obs3name"),val1=c(0.2,0.4,0.6),val2=c(0.3,1.0,2.0),row.names=c("r1","r2","r3"))
 > d
        name val1 val2
r1 obs1name  0.2  0.3
r2 obs2name  0.4  1.0
r3 obs3name  0.6  2.0
 >

(when you do it by row, you get the numbers as factors because 
c("obs1name", 0.2, 0.3) etc. are character vectors)

At Wednesday 01:29 PM 6/9/2004, ivo welch wrote:

>hi:  I searched the last 2 hours for a way to enter a data frame directly 
>in my program.  (I know how to read from a file.)  that is, I would like 
>to say something like
>
>    d <- this.is.a.data.frame(   c("obs1name", 0.2, 0.3),
>                                          c("obs2name", 0.4, 1.0),
>                                          c("obs3name", 0.6, 2.0) , 
> varnames=c("name", "val1", "val2")  );
>
>everything I have tried sofar (usually, building with rbind and then 
>names(d)) has come out with factors for the numbers, which is obviously 
>not what I want.   this must be a pretty elementary request, so it should 
>probably be an example under data.frame (or read.table).  of course, it is 
>probably somewhere---just I have do not remember it and could not find it 
>after 2 hours of searching.  I also tried the r-help archives---at the 
>very least, I hope we will get the answer there for future lookups.
>
>regards, /iaw
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From rossini at blindglobe.net  Wed Jun  9 21:48:31 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Wed, 09 Jun 2004 12:48:31 -0700
Subject: [R] how to initialize random seed properly ?
In-Reply-To: <OFA819C8AE.C1425078-ON85256EAE.006A0801-85256EAE.006AC76D@EU.novartis.net>
	(ryszard czerminski's message of "Wed, 9 Jun 2004 15:24:08 -0400")
References: <OFA819C8AE.C1425078-ON85256EAE.006A0801-85256EAE.006AC76D@EU.novartis.net>
Message-ID: <85ise0v7g0.fsf@servant.blindglobe.net>


For reproducibility, you probably want to specify the starting seeds.
I'd worry about using systime.   (for selecting the seeds, you might
consider random draws from a uniform).

best,
-tony

ryszard.czerminski at pharma.novartis.com writes:

> I want to start R processes on multiple processors from single shell 
> script
> and I want all of them to have different random seeds.
> One way of doing this is
>
>         sleep 2 # (with 'sleep 1' I am often getting the same number)
>                ...
>         set.seed(unclass(Sys.time()))
>
> Is there a simpler way without a need to sleep between invoking
> different R processes ?
>
> Ryszard
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From martin.klaffenboeck at inode.at  Wed Jun  9 22:00:50 2004
From: martin.klaffenboeck at inode.at (Martin Klaffenboeck)
Date: Wed, 9 Jun 2004 22:00:50 +0200
Subject: [R] moving data and output?
Message-ID: <20040609200050.GA14912@martin.kleinerdrache.org>

Hello,

I have a few questions now:

1.  How can I move data the following way:

I have 2 variables:

one	two
1	5  ^
3	4  |
1	3  |
4	4  |

Now I want to move the two one arround (sorry I don't know how to say  
that in english).  That means:  I want to move the first item at the  
end of my column and move the second at the first place, the third at  
the second, and so on.  You can see it at the arrow next to the 'two'  
column.  The colum named 'one' should be as it is.

2. How can I make outputs of the grafics (plot, hist, ...) into a file?

3. Can I make latex output of the grafics (any tool, like texdraw or  
pictex ...)?

4. I know about sink().  But can I format the output for LaTeX, like a  
\begin{tabular} ... \end{tabular} for a dist() matrix or similar?

5. Acording to the other questions, where can I find answers for this  
questions, if I'm not the first one who is asking?

Thanks for help,
Martin



From ivo.welch at yale.edu  Wed Jun  9 22:07:03 2004
From: ivo.welch at yale.edu (ivo welch)
Date: Wed, 09 Jun 2004 16:07:03 -0400
Subject: [R] direct data frame entry
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7E7C@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF7E7C@usrymx25.merck.com>
Message-ID: <40C76DE7.7040205@yale.edu>


thank you, chaps.  ok, so this is not as straightforward as I had 
thought.  perhaps the read.table() function should have the ability to 
read inline (terminated, e.g., by two newlines, or a usersettable 
string), rather than just from a file.  this would be a nice feature.

regards,  /iaw



From ps-list at masny.dk  Wed Jun  9 22:42:30 2004
From: ps-list at masny.dk (Peter Sebastian Masny)
Date: Wed, 9 Jun 2004 13:42:30 -0700
Subject: [R] Comparing two pairs of non-normal datasets in R?
In-Reply-To: <85D25331FFB7AE4C900EA467D4ADA3920459CA@circle.pcpool.mi.fu-berlin.de>
References: <85D25331FFB7AE4C900EA467D4ADA3920459CA@circle.pcpool.mi.fu-berlin.de>
Message-ID: <200406091342.30460.ps-list@masny.dk>

On Wednesday 09 June 2004 08:58 am, Lutz Prechelt wrote:
...
> Does that make sense?
>
>   Lutz


Thanks, everyone, very much for your replies.  I think I'm getting a little 
out of my league with quantile regession estimates and bootstrap tests of 
distribution parameters.  It makes sense, but I'm going to have to invoke 
some local help to make sure I get it right.

Thanks again,

Peter



From wolski at molgen.mpg.de  Wed Jun  9 23:20:10 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Wed, 09 Jun 2004 23:20:10 +0200
Subject: [R] direct data frame entry
In-Reply-To: <40C76537.6090700@yale.edu>
References: <40C76537.6090700@yale.edu>
Message-ID: <200406092320100495.000CDDFE@mail.math.fu-berlin.de>

Hi Ivo!


https://www.stat.math.ethz.ch/pipermail/r-help/2004-June/050601.html



Sincerely
Eryk

*********** REPLY SEPARATOR  ***********

On 6/9/2004 at 3:29 PM ivo welch wrote:

>>>hi:  I searched the last 2 hours for a way to enter a data frame 
>>>directly in my program.  (I know how to read from a file.)  that is, I 
>>>would like to say something like
>>>
>>>    d <- this.is.a.data.frame(   c("obs1name", 0.2, 0.3),
>>>                                          c("obs2name", 0.4, 1.0),
>>>                                          c("obs3name", 0.6, 2.0) , 
>>>varnames=c("name", "val1", "val2")  );
>>>
>>>everything I have tried sofar (usually, building with rbind and then 
>>>names(d)) has come out with factors for the numbers, which is obviously 
>>>not what I want.   this must be a pretty elementary request, so it 
>>>should probably be an example under data.frame (or read.table).  of 
>>>course, it is probably somewhere---just I have do not remember it and 
>>>could not find it after 2 hours of searching.  I also tried the r-help 
>>>archives---at the very least, I hope we will get the answer there for 
>>>future lookups.
>>>
>>>regards, /iaw
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



Dipl. bio-chem. Eryk Witold Wolski    @    MPI-Moleculare Genetic   
Ihnestrasse 63-73 14195 Berlin       'v'    
tel: 0049-30-83875219               /   \    
mail: wolski at molgen.mpg.de        ---W-W----    http://www.molgen.mpg.de/~wolski



From grenyer at virginia.edu  Wed Jun  9 23:45:41 2004
From: grenyer at virginia.edu (Rich Grenyer)
Date: Wed, 9 Jun 2004 17:45:41 -0400
Subject: [R] Two-dimensional Kolmogorov-Smirnov test
Message-ID: <5A739EDE-BA5E-11D8-8DCC-000A95F0D0D2@virginia.edu>

Hi - is the 2-D Kolmogorov-Smirnov test of Fasano & Francheschini 
(1987) implemented in any of the R-packages at present? The algorithm 
and code exist in the Numerical Recipes series (e.g. Press et al. 1992) 
but I thought I should check before playing with .C() and dyn.load() 
for the first time...

Thanks in advance,

Rich

Fasano G and Franceschini A 1987 A multi-dimensional version of the 
Kolmorogov-Smirnov test Mon. Not. R. Astron. Soc. 225 155-70
Press W H, Teukolsky S A, Vettering W T and Flannery B P 1992 Numerical 
Recipes in FORTRAN: The Art of Scientific Computing (Cambridge: 
Cambridge University Press) p 640ff


--------------------
Rich Grenyer, Ph.D.
Biology Department - University of Virginia
Gilmer Hall
Charlottesville, Virginia
VA 22904
United States of America

tel: (+1) 434 982 5629
fax: (+1) 434 982 5626
http://faculty.virginia.edu/gittleman/rich



From ggrothendieck at myway.com  Thu Jun 10 00:09:07 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 9 Jun 2004 22:09:07 +0000 (UTC)
Subject: [R] direct data frame entry
References: <3A822319EB35174CA3714066D590DCD504AF7E7C@usrymx25.merck.com>
	<40C76DE7.7040205@yale.edu>
Message-ID: <loom.20040610T000757-55@post.gmane.org>

ivo welch <ivo.welch <at> yale.edu> writes:

> thank you, chaps.  ok, so this is not as straightforward as I had 
> thought.  perhaps the read.table() function should have the ability to 
> read inline (terminated, e.g., by two newlines, or a usersettable 
> string), rather than just from a file.  this would be a nice feature.

Have a look at the my.stdin function at:

  https://stat.ethz.ch/pipermail/r-help/2003-June/033622.html

It is intended to be used in a sourced file as shown in the
example there.



From deepayan at stat.wisc.edu  Thu Jun 10 01:04:38 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Wed, 9 Jun 2004 18:04:38 -0500
Subject: [R] Help with a Lattice plot that fails with an empty unique
	combination
In-Reply-To: <000201c44def$238619d0$2202a8c0@ACER>
References: <000201c44def$238619d0$2202a8c0@ACER>
Message-ID: <200406091804.38752.deepayan@stat.wisc.edu>

On Wednesday 09 June 2004 01:58, Tom Mulholland wrote:
> While using Lattice I received the following error.
>
> Error in if (xx != 0) xx/10 else z/10 : argument is of length zero
> In addition: Warning messages:
> 1: is.na() applied to non-(list or vector) in: is.na(x)
> 2: is.na() applied to non-(list or vector) in: is.na(x)
> 3: no finite arguments to min; returning Inf
> 4: no finite arguments to max; returning -Inf
> 5: NaNs produced in: log(x, base)
> Can anyone point me in the right direction.

A traceback() shows that this is happening due to jitter() being called 
with a length-0 numeric. I have added a check in panel.stripplot. Until 
the next release, you can work around it by:


assignInNamespace("panel.stripplot",     
    function(x, y, jitter.data = FALSE, factor = 0.5,
             horizontal = TRUE, groups = NULL, ...)
{
    if (length(x) < 1) return()
    x <- as.numeric(x)
    y <- as.numeric(y)
    y.jitter  <-
        if (horizontal && jitter.data) 
            jitter(y, factor = factor) else y
    x.jitter  <-
        if (!horizontal && jitter.data) 
            jitter(x, factor = factor) else x
    if (is.null(groups)) panel.xyplot(x = x.jitter, 
        y = y.jitter, ...) else 
    panel.superpose(x = x.jitter, y = y.jitter, 
        groups = groups, ...)
}, "lattice")


Deepayan



From p.murrell at auckland.ac.nz  Thu Jun 10 01:23:20 2004
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Thu, 10 Jun 2004 11:23:20 +1200
Subject: [R] bar plot patterns
References: <40BB1093.23325.1FD412@localhost>
	<003501c44718$85741120$ac90148e@Toshi>
Message-ID: <40C79BE8.8090306@stat.auckland.ac.nz>

Hi


Osman wrote:
 > Thank you for your answer. I have about 7 stacks it is not very
 > appealing to have just stripes with changing angles. I was wondering
 > if there is a way to have varying patters in black and white.


R graphics is inherently vector-based;  there is no native support for 
bitmaps so bitmap fill patterns are not really an option.  (There is the 
pixmap package for drawing bitmaps as a rectangle per pixel)

Of course, it is possible to do vector fill patterns, but again there is 
little convenient support for this sort of thing currently.  The code 
below shows one way that this sort of thing could be done.  It might be 
a useful starting point if your barplot is a one-off.

Paul

### R code ###

library(grid)

gridLocns <- function(sep=unit(0.25, "inches"),
                       startx=0.5*runif(1)*sep,
                       starty=0.5*runif(1)*vshift) {
   # hshift and vshift between rows of dots
   # Only really makes sense if sep is "absolute"
   if (round(convertWidth(sep, "inches", valueOnly=TRUE) -
             convertHeight(sep, "inches", valueOnly=TRUE), 5) > 0)
     stop("sep must be absolute")
   hshift <- sep*cos(pi*60/180)
   vshift <- sep*sin(pi*60/180)
   # How many sep steps to take in each direction in
   # order to guarantee to fill current viewport?
   nsepx <- 1/convertWidth(sep, "npc", valueOnly=TRUE) + 2
   nsepy <- 1/convertHeight(hshift, "npc", valueOnly=TRUE) + 2
   list(x=startx + rep(1:nsepx - 2, nsepy)*sep +
        rep(rep(0:1, each=nsepx), nsepy/2)*hshift,
        y=starty + rep(1:nsepy - 2, each=nsepx)*vshift)
}

dotFill <- function(size=unit(2, "mm"),
                     sep=unit(0.25, "inches")) {
   locns <- gridLocns(sep)
   grid.circle(locns$x, locns$y, 0.5*size, gp=gpar(fill="black"))
}

charFill <- function(char="+",
                      sep=unit(0.25, "inches")) {
   locns <- gridLocns(sep)
   grid.text(char, locns$x, locns$y, gp=gpar(fontsize=20))
}

N <- 6
counts <- runif(N)
fill <- c(dotFill, charFill)
pushViewport(plotViewport(c(5,4,4,2)))
pushViewport(viewport(xscale=c(0, N+1), yscale=c(0, max(counts)*1.1)))
# Draw a filled bar for each count
for (i in 1:N) {
   pushViewport(viewport(x=i, y=0, height=counts[i], width=0.5,
                         default.units="native",
                         just=c("centre", "bottom"),
                         clip=TRUE))
   fill[[i %% length(fill) + 1]]()
   grid.rect()
   popViewport()
}
grid.xaxis(at=1:N)
grid.yaxis()
grid.rect()
popViewport(2)

### R code ###


 > Osman ----- Original Message ----- From: Petr Pikal To: Osman Cc:
 > r-help at stat.math.ethz.ch Sent: Monday, May 31, 2004 5:01 AM Subject:
 > Re: [R] bar plot patterns
 >
 >
 >
 >
 >
 >
 > On 29 May 2004 at 16:54, Osman wrote:
 >
 >
 >> Dear R users,
 >>
 >> Is there a package or function that can produce multiple fill
 >> patterns to be used instead of colors in barplots or pie charts?
 >> Shades of grey are difficult to differentiate if more than 3 to 5..
 >>
 >
 >
 > Hi
 >
 >
 > This is what help page says about colours and shading lines
 >
 >
 > density: a vector giving the density of shading lines, in lines per
 > inch, for the bars or bar components. The default value of 'NULL'
 > means that no shading lines are drawn. Non-positive values of
 > 'density' also inhibit the drawing of shading lines.
 >
 >
 > angle: the slope of shading lines, given as an angle in degrees
 > (counter-clockwise), for the bars or bar components.
 >
 >
 > col: a vector of colors for the bars or bar components.
 >
 >
 >
 >
 >
 >
 > Do you want something else than
 >
 >
 > barplot(1:3, col=c(2,3,3),density=c(2,4,6), angle=c(30,60,90))
 >
 >
 > Cheers Petr
 >
 >
 >>
 >> Osman [[alternative HTML version deleted]]
 >>
 >> ______________________________________________
 >> R-help at stat.math.ethz.ch mailing list
 >> https://www.stat.math.ethz.ch/mailman/listinfo/r-help PLEASE do
 >> read the posting guide! http://www.R-project.org/posting-guide.html
 >>
 >
 >
 >
 >
 > Petr Pikal petr.pikal at precheza.cz [[alternative HTML version
 > deleted]]
 >
 > ______________________________________________
 > R-help at stat.math.ethz.ch mailing list
 > https://www.stat.math.ethz.ch/mailman/listinfo/r-help PLEASE do read
 > the posting guide! http://www.R-project.org/posting-guide.html


-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From david.netherway at adelaide.edu.au  Thu Jun 10 03:27:15 2004
From: david.netherway at adelaide.edu.au (David J. Netherway)
Date: Thu, 10 Jun 2004 10:57:15 +0930
Subject: [R] Specifying xlevels in effects library
In-Reply-To: <web-46087207@cgpsrv2.cis.mcmaster.ca>
References: <web-46087207@cgpsrv2.cis.mcmaster.ca>
Message-ID: <40C7B8F3.8040406@adelaide.edu.au>

Andy and John,

I looked at typical when xlevels did not work but when I saw that it was 
a function I went no further. Setting the function to a constant was a 
good idea.
John's method seems to require that I change the model:

 > eff <-effect("sex*age",mod,xlevel=(Age=c(120,120)))
Error in all(!(factors[, term1] & (!factors[, term2]))) : 
        subscript out of bounds
In addition: Warning message: 
sex:age does not appear in the model in: effect("sex*age", mod, xlevel = 
(age = c(120, 120))) 

Andy's method works as suggested for this simple case.

Thanks for your time.

Cheers, David



From david.netherway at adelaide.edu.au  Thu Jun 10 03:49:05 2004
From: david.netherway at adelaide.edu.au (David J. Netherway)
Date: Thu, 10 Jun 2004 11:19:05 +0930
Subject: [R] Getting Pr from Summary(lm)
In-Reply-To: <Pine.LNX.4.44.0406090651392.5147-100000@jotaerre.ivic.ve>
References: <Pine.LNX.4.44.0406090651392.5147-100000@jotaerre.ivic.ve>
Message-ID: <40C7BE11.5010309@adelaide.edu.au>

Thanks for all the relies.

I recently discovered "names" and applied it to "lm" objects but did not 
think to apply it to the "summary" object.

Cheers, David



From jfox at mcmaster.ca  Thu Jun 10 03:52:21 2004
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 9 Jun 2004 21:52:21 -0400
Subject: [R] Specifying xlevels in effects library
In-Reply-To: <40C7B8F3.8040406@adelaide.edu.au>
Message-ID: <20040610015219.MBNW9492.tomts16-srv.bellnexxia.net@JohnDesktop8300>

Dear David,

You don't have to change the model, but you do have to name the variable age
(or Age) consistently, and give a list as the xlevels argument. Hence, if
the variable names are really lower-case:   eff <- effect("sex*age", mod,
xlevels=list(age=c(120,120))).

Regards,
 John

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of David 
> J. Netherway
> Sent: Wednesday, June 09, 2004 8:27 PM
> To: R-help at stat.math.ethz.ch
> Subject: Re: [R] Specifying xlevels in effects library
> 
> Andy and John,
> 
> I looked at typical when xlevels did not work but when I saw 
> that it was a function I went no further. Setting the 
> function to a constant was a good idea.
> John's method seems to require that I change the model:
> 
>  > eff <-effect("sex*age",mod,xlevel=(Age=c(120,120)))
> Error in all(!(factors[, term1] & (!factors[, term2]))) : 
>         subscript out of bounds
> In addition: Warning message: 
> sex:age does not appear in the model in: effect("sex*age", 
> mod, xlevel = (age = c(120, 120))) 
> 
> Andy's method works as suggested for this simple case.
> 
> Thanks for your time.
> 
> Cheers, David
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From jfox at mcmaster.ca  Thu Jun 10 03:54:18 2004
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 9 Jun 2004 21:54:18 -0400
Subject: [R] Specifying xlevels in effects library [2]
In-Reply-To: <40C7B8F3.8040406@adelaide.edu.au>
Message-ID: <20040610015415.XLBG28143.tomts25-srv.bellnexxia.net@JohnDesktop8300>

Dear David,

An addendum: Looking at my original posting, I see that failing to specify a
list as the xlevels argument was my error.

Sorry,
 John

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of David 
> J. Netherway
> Sent: Wednesday, June 09, 2004 8:27 PM
> To: R-help at stat.math.ethz.ch
> Subject: Re: [R] Specifying xlevels in effects library
> 
> Andy and John,
> 
> I looked at typical when xlevels did not work but when I saw 
> that it was a function I went no further. Setting the 
> function to a constant was a good idea.
> John's method seems to require that I change the model:
> 
>  > eff <-effect("sex*age",mod,xlevel=(Age=c(120,120)))
> Error in all(!(factors[, term1] & (!factors[, term2]))) : 
>         subscript out of bounds
> In addition: Warning message: 
> sex:age does not appear in the model in: effect("sex*age", 
> mod, xlevel = (age = c(120, 120))) 
> 
> Andy's method works as suggested for this simple case.
> 
> Thanks for your time.
> 
> Cheers, David
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From alistair at statsresearch.co.nz  Thu Jun 10 04:37:50 2004
From: alistair at statsresearch.co.nz (Alistair Gray)
Date: Thu, 10 Jun 2004 14:37:50 +1200
Subject: [R] X-12-ARIMA
Message-ID: <40C7C97E.1030705@statsresearch.co.nz>

Dear All,
I've used the X-12-ARIMA or its earlier versions from S+ and R under both Unix 
and Windows platforms for many years using the klugey approach of calling an 
executable using in R the system function.  I've found this serviceable  for the 
following reasons.

1) Paul Gilbert's hunch is correct that many of the subroutines have extensive 
IO calls (especially the X-11 engine) and so it is not straightfoward to call 
using .Fortran.

2) X-12-ARIMA has in the Unix final version 0.2.10 which I use 15 different 
commands with their own parameters covering about six pages of description so 
it's not clear to me having a long R function parameter list is a great advantage.

My experience with people who have ported X-12-ARIMA into other econometrics 
software is that they port a limited range of the commands and options. (I'd be 
interested to know from Dirk Eddelbuettel if this is the case for GNU-Gretl.)

3) X-12_ARIMA through its specification files (particularly the metafiles) is 
set up to handle multiple runs on different time series with different parameter 
settings. I'm not sure I'd want to re-invent it. Secondly I find it an OK way to 
keep track of what I've done.

However, I'd be a grateful user of a less klugey approach.

Below is a clearer explanation of my klugey approach and functions which I've 
used successfully. Feel free to use them.

# create data file for the series to be read by  x12 fortran pgm
# blp is a object of class ts written to file blp.dat in the x12 datevalue
# format

writex12in(blp,"blp")

# create the specification file using your favourite editor say blp.spc to be
# read by x12 fortran pgm
# a vanilla specification file might have the following

# series{
#   title="building consents"
#   start=1973.01
#   span=(1973.01, 2000.12)	
#   period=12
#   file="blp.dat"
#   format="datevalue"
# }

# x11{
#   mode=mult
#   sigmalim=(1.8 2.8)
#   seasonalma=x11default
#   trendma=13
#   appendfcst=no
#   save=(b1 c17 d10 d11 d12 d13)
#   savelog=(m1 m2 m3 m4 m5 m6 m7 m8 m9 m10 m11 q q2 msr icr fb1 fd8 msf ids)
# }


# execute the  x12 fortran pgm. executable x12a stored in /home/fred/x12a

system("/home/fred/x12a/x12a blp")


# read  x12 fortran pgm output tables back into R

blp.x12 <- readx12out("blp", adtype="M", calendar=F, tblnames=NULL)


# basic versions of writex12in  and readx12out

writex12in <- function(tso,file){
   write.table(cbind(time(tso) %/% 1, cycle(tso), tso),
               file=paste(file,".dat",sep=""),
               sep=" ", quote=F, row.names=F, col.names=F)
}


readx12out <- function(file, adtype = "M", calendar = F, tblnames = NULL) {
	notbls <- 6 + (calendar != F) + length(tblnames)
	comp <- vector("list", notbls)
	if(calendar == F) {
		names(comp) <- c("original", "seasonal", "adjusted", "trend",
                                  "irregular", "weights", tblnames)
		tblnames <- c("b1", "d10", "d11", "d12", "d13", "c17", tblnames)
	}
	else {
		names(comp) <- c("original", "seasonal", "adjusted", "trend",
                                  "irregular", "calendar", "weights", tblnames)
		if(calendar == "C")
			tblnames <- c("b1", "d10", "d11", "d12", "d13", "xca", "c17",
				tblnames)
		else tblnames <- c("b1", "d10", "d11", "d12", "d13", "c16", "c17",
				tblnames)
	}
	for(i in seq(along = comp)) {
		series <- read.table(paste(file, ".", tblnames[i], sep = ""),
			as.is = T, skip = 2)
                 times <- series[,1]
                 begin <- c(times[1]%/%100,times[1]%%100)
                 freq <- max(times%%100)
		comp[[i]] <- ts(data = series[,2], start = begin, frequency = freq)
	}
# for the log transform the adjusted series is the log of the original
# also x12 transforms trend, seasonal, etc back into original scale
# i.e. making a multiplicative decomposition in which case 1.0 needs to be
# subtracted from seasonal, irregular and calendar to be consistent with
# multiplicative. Also x12 makes correction for bias to trend in original scale
# so best to treat as multiplicative decomposition rather than decomposition in
# transformed scale
	if(adtype == "L") {
		comp$transformed <- comp$original
		comp$seasonal <- comp$seasonal - 1
		comp$irregular <- comp$irregular - 1
		if(calendar != F)
			comp$calendar <- comp$calendar - 1
		comp$power <- "log additive presented as multiplicative. 1.0 has been 
subtracted from seasonal, irregular & calendar (if present)"
	}
	else if(adtype == "M") {
		comp$transformed <- comp$original
		comp$seasonal <- comp$seasonal - 1
		comp$irregular <- comp$irregular - 1
		if(calendar != F)
			comp$calendar <- comp$calendar - 1
		comp$power <- "multiplicative. 1.0 has been subtracted from seasonal, 
irregular & calendar (if present)"
	}
	else if(adtype == "A") {
		comp$original <- rtso
		comp$power <- "additive"
	}
	return(comp)
}




-- 
Alistair Gray                       Email:  alistair at statsresearch.co.nz
Statistics Research Associates Ltd  Web:    www.statsresearch.co.nz
PO Box 12 649, Thorndon, Wellington Phone:  +64 +4 972 6531
NEW ZEALAND                         Mobile: +64 +21 610 569



From edd at debian.org  Thu Jun 10 04:42:53 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 9 Jun 2004 21:42:53 -0500
Subject: [R] X-12-ARIMA
In-Reply-To: <40C7C97E.1030705@statsresearch.co.nz>
References: <40C7C97E.1030705@statsresearch.co.nz>
Message-ID: <20040610024253.GA15693@sonny.eddelbuettel.com>


Alistair,

That look like a very good way of getting things done. I had at one point in
the past thought about undertaking a similar approach of dealing with the
input/output files to x12arima from R. 

However, my job priorities changed and I didn't have the need for x12arima
any more.  For the same reason, I never really dug into what gretl does --
you'd need to see for yourself: http://gretl.sourceforge.net/

Regards, Dirk

-- 
FEATURE:  VW Beetle license plate in California



From jasont at indigoindustrial.co.nz  Fri Jun 11 05:28:15 2004
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Thu, 10 Jun 2004 15:28:15 -1200 (NZST)
Subject: [R] moving data and output?
In-Reply-To: <20040609200050.GA14912@martin.kleinerdrache.org>
References: <20040609200050.GA14912@martin.kleinerdrache.org>
Message-ID: <51568.203.9.176.60.1086838095.squirrel@webmail.maxnet.co.nz>

> Hello,
>
> I have a few questions now:

Yes.  Since the mail is archived to help other people, in future please
send a small mail for each question, with a descriptive subject line for
each.  This makes it easier for everyone.

> 1.  How can I move data the following way:
>
> I have 2 variables:
>
> one	two
> 1	5  ^
> 3	4  |
> 1	3  |
> 4	4  |
>
> Now I want to move the two one arround (sorry I don't know how to say
> that in english).  That means:  I want to move the first item at the
> end of my column and move the second at the first place, the third at
> the second, and so on.  You can see it at the arrow next to the 'two'
> column.  The colum named 'one' should be as it is.

By using indexing.  If this is a matrix called mymatrix:

mymatrix[,2] <- mymatrix[c(4,1:3),2]

If it's a data frame called mydf:

mydf$two <- mydf$two[c(4,1:3)]



> 2. How can I make outputs of the grafics (plot, hist, ...) into a file?

See the help pages and examples...

?postscript
?Devices
?pdf

> 3. Can I make latex output of the grafics (any tool, like texdraw or
> pictex ...)?

I use either eps or pdf for inclusion into LaTeX.  You also have the fig
file format, so you can use xfig to edit before inclusion.

> 4. I know about sink().  But can I format the output for LaTeX, like a
> \begin{tabular} ... \end{tabular} for a dist() matrix or similar?

Two ways:
1) Install the Hmisc package, and see ?latex
2) Install the xtable package, and see ?xtable.

> 5. Acording to the other questions, where can I find answers for this
> questions, if I'm not the first one who is asking?

1) help.search("some keyword")
2) go to http://cran.r-project.org, and follow the "Search" link to search
the mail archives.

Cheers

Jason



From ripley at stats.ox.ac.uk  Thu Jun 10 08:40:38 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 10 Jun 2004 07:40:38 +0100 (BST)
Subject: [R] Building package on Windows: No rule to make target '-llapack'
In-Reply-To: <1086788957.2450.12.camel@zwang.stat.smu.edu>
Message-ID: <Pine.LNX.4.44.0406100729270.3886-100000@gannet.stats>

On Wed, 9 Jun 2004, Zhu Wang wrote:

> I have a problem to build a package on Windows XP while there is no
> problem on Linux. The Makefile is something like:

There would be a problem on Linux, if that Makefile were used.  I suspect
it is not used.

> ###########
> LIBNAME=cts
> 
> PKG_LIBS = $(LAPACK_LIBS) $(BLAS_LIBS) $(FLIBS)

(You should not need $(FLIBS) here.)

> OBJS=file1.o ... file20.o -llapack -lblas

The last two are not objects, and -llapack refers to something like 
liblapack.a.  I don't see how this can work anywhere.  On the other hand, 
you have lapack and blas selected in PKG_LIBS, but you are not using that 
macro.

> $(LIBNAME)$(SHLIB_EXT): $(OBJS)
>         $(SHLIB_LD) $(SHLIB_LDFLAGS) -o $@ $(OBJS) $(FLIBS)
> 
> clean:
>         @rm -f *.o *.$(SHLIB_EXT)
> 
> realclean: clean
> #############
> 
> To build the package on Windows XP, I have followed the instructions to
> install tools/software required and it seems the 'make' worked fine,
> except for the error message:
> 
> make[3]: No rule to make target 'llapack', needed by 'cts.a'. stop.
> 
> Now I think maybe two problems: one is that maybe I do not have Lapack and Blas installed
> on Windows XP and second is that I do not set up a correct file, something like 'configure'.
> Maybe there are more problems. I have read some files in \src\gnuwin32, but I did not find
> what I needed.

What are you actually trying to do?  It is not normal for a package to
have a Makefile in its src directory, which is what I guess (but only
guess) you have presented.  All that is normally needed is a Makevars
file.  Take a look at e.g. that in mclust.

Is this your package or someone else's (in which case why are you not
asking the author)?  Either you or that author needs to read `Writing R 
Extensions' and look at some of the many examples in CRAN packages.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From wuertz at itp.phys.ethz.ch  Thu Jun 10 08:43:04 2004
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Thu, 10 Jun 2004 06:43:04 +0000
Subject: [R] Re: R equivalent of Splus rowVars function
In-Reply-To: <16583.22286.988964.819482@arbres1a.fmr.com>
References: <3A822319EB35174CA3714066D590DCD504AF7E4B@usrymx25.merck.com>
	<16583.22286.988964.819482@arbres1a.fmr.com>
Message-ID: <40C802F8.6030600@itp.phys.ethz.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040610/53afb3c2/attachment.pl

From etptupaf at bs.ehu.es  Thu Jun 10 08:58:47 2004
From: etptupaf at bs.ehu.es (F. Tusell)
Date: Thu, 10 Jun 2004 08:58:47 +0200
Subject: [R] Failure to compile on Itanium
Message-ID: <40C806A7.7060502@bs.ehu.es>

I am trying to compile R on an Itanium machine running Red Hat 7.2 and 
gcc version 2.96.
The build fails with the following symptoms:

g77  -fPIC  -g -O2 -c cmplx.f -o cmplx.lo
g77  -fPIC  -g -O2 -c cmplxblas.f -o cmplxblas.lo
gcc -shared -L/usr/local/lib  -o libRlapack.so dlapack0.lo dlapack1.lo
dlapack2.lo dlapack3.lo cmplx.lo  cmplxblas.lo  -L/usr/local/lib
-L/usr/lib/gcc-lib/ia64-redhat-linux/2.96
-L/usr/lib/gcc-lib/ia64-redhat-linux/2.96/../../.. -lg2c -lm
/usr/bin/ld: open.o: @gprel relocation against dynamic symbol f__buflen
/usr/bin/ld: open.o: @gprel relocation against dynamic symbol f__buflen
/usr/bin/ld: open.o: @gprel relocation against dynamic symbol f__buflen
/usr/bin/ld: open.o: @gprel relocation against dynamic symbol f__buflen
/usr/bin/ld: open.o: @gprel relocation against dynamic symbol f__buflen
/usr/bin/ld: open.o: @gprel relocation against dynamic symbol f__buflen
collect2: ld returned 1 exit status
make[4]: *** [libRlapack.so] Error 1
make[4]: Leaving directory `/home/etptupaf/R-1.9.0/src/modules/lapack'
make[3]: *** [R] Error 2
make[3]: Leaving directory `/home/etptupaf/R-1.9.0/src/modules/lapack'
make[2]: *** [R] Error 1
make[2]: Leaving directory `/home/etptupaf/R-1.9.0/src/modules'
make[1]: *** [R] Error 1
make[1]: Leaving directory `/home/etptupaf/R-1.9.0/src'
make: *** [R] Error 1

I searched and found in

  _http://tolstoy.newcastle.edu.au/R/devel/04a/0679.html_

an answer from BD Ripley to a query with exactly the same problem, 
suggesting that a newer
version of gcc be used.

This solution is out of reach for me, as the system managers are not 
willing to undertake
any changes. They suggest instead that I use the "recommended" compilers,
ecc and ecf. I have tried to do so with no success so far: the  
configure script fails to find
convenient values for the different flags to be set. I have set some 
manually (in config.site),
to no avail so far. Has anyone compiled R with ecc and ecf? Or with 
gcc-2.96? I would be
grateful if he/she could share his/her config.site settings. Best, ft.

-- 
Fernando TUSELL                                e-mail:
Departamento de Econometr??a y Estad??stica           etptupaf at bs.ehu.es 
Facultad de CC.EE. y Empresariales             Tel:   (+34)94.601.3733
Avenida Lendakari Aguirre, 83                  Fax:   (+34)94.601.3754
E-48015 BILBAO  (Spain)                        Secr:  (+34)94.601.3740



From xinan at molgen.mpg.de  Thu Jun 10 09:06:18 2004
From: xinan at molgen.mpg.de (Xinan Yang)
Date: Thu, 10 Jun 2004 09:06:18 +0200
Subject: [R] question about hierclust {multiv}
Message-ID: <40C8086A.4050607@molgen.mpg.de>

my major is bioinformatics, and i'm trying to cluster ( agglomerate
the closest pari of observations ) in R.


i have already got my own similarities metric, but do not know how to
clust it based on similarities instead of dissimilarities.


since the help document of hierclust mentions the parameter "sim",
which seems good to me, but it doesn't appear in the code of
hierclust() function again? and no sample about it.  so could anybody
please help me as author?

thanks in advance

xinan yang
xinan at molgen.mpg.de



From maechler at stat.math.ethz.ch  Thu Jun 10 09:09:12 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 10 Jun 2004 09:09:12 +0200
Subject: [R] Help with a Lattice plot that fails with an empty unique
	combination
In-Reply-To: <200406091804.38752.deepayan@stat.wisc.edu>
References: <000201c44def$238619d0$2202a8c0@ACER>
	<200406091804.38752.deepayan@stat.wisc.edu>
Message-ID: <16584.2328.622974.808165@gargle.gargle.HOWL>

>>>>> "Deepayan" == Deepayan Sarkar <deepayan at stat.wisc.edu>
>>>>>     on Wed, 9 Jun 2004 18:04:38 -0500 writes:

    Deepayan> On Wednesday 09 June 2004 01:58, Tom Mulholland wrote:
    >> While using Lattice I received the following error.
    >> 
    >> Error in if (xx != 0) xx/10 else z/10 : argument is of length zero
    >> In addition: Warning messages:
    >> 1: is.na() applied to non-(list or vector) in: is.na(x)
    >> 2: is.na() applied to non-(list or vector) in: is.na(x)
    >> 3: no finite arguments to min; returning Inf
    >> 4: no finite arguments to max; returning -Inf
    >> 5: NaNs produced in: log(x, base)
    >> Can anyone point me in the right direction.

    Deepayan> A traceback() shows that this is happening due to
    Deepayan> jitter() being called with a length-0 numeric.

clearly a bug in jitter()  -- which has been fixed 2 minutes ago
for R 1.9.1 alpha {and R-devel}:
Now "jitter(x)" has "if(length(x) == 0) return(x)".

    Deepayan> I have added a check in panel.stripplot. Until the
    Deepayan> next release, you can work around it by:

    <................>


Martin Maechler



From ripley at stats.ox.ac.uk  Thu Jun 10 09:11:45 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 10 Jun 2004 08:11:45 +0100 (BST)
Subject: [R] Failure to compile on Itanium
In-Reply-To: <40C806A7.7060502@bs.ehu.es>
Message-ID: <Pine.LNX.4.44.0406100804260.3886-100000@gannet.stats>

On Thu, 10 Jun 2004, F. Tusell wrote:

> I am trying to compile R on an Itanium machine running Red Hat 7.2 and 
> gcc version 2.96.

> The build fails with the following symptoms:
> 
> g77  -fPIC  -g -O2 -c cmplx.f -o cmplx.lo
> g77  -fPIC  -g -O2 -c cmplxblas.f -o cmplxblas.lo
> gcc -shared -L/usr/local/lib  -o libRlapack.so dlapack0.lo dlapack1.lo
> dlapack2.lo dlapack3.lo cmplx.lo  cmplxblas.lo  -L/usr/local/lib
> -L/usr/lib/gcc-lib/ia64-redhat-linux/2.96
> -L/usr/lib/gcc-lib/ia64-redhat-linux/2.96/../../.. -lg2c -lm
> /usr/bin/ld: open.o: @gprel relocation against dynamic symbol f__buflen
> /usr/bin/ld: open.o: @gprel relocation against dynamic symbol f__buflen
> /usr/bin/ld: open.o: @gprel relocation against dynamic symbol f__buflen
> /usr/bin/ld: open.o: @gprel relocation against dynamic symbol f__buflen
> /usr/bin/ld: open.o: @gprel relocation against dynamic symbol f__buflen
> /usr/bin/ld: open.o: @gprel relocation against dynamic symbol f__buflen
> collect2: ld returned 1 exit status
> make[4]: *** [libRlapack.so] Error 1
> make[4]: Leaving directory `/home/etptupaf/R-1.9.0/src/modules/lapack'
> make[3]: *** [R] Error 2
> make[3]: Leaving directory `/home/etptupaf/R-1.9.0/src/modules/lapack'
> make[2]: *** [R] Error 1
> make[2]: Leaving directory `/home/etptupaf/R-1.9.0/src/modules'
> make[1]: *** [R] Error 1
> make[1]: Leaving directory `/home/etptupaf/R-1.9.0/src'
> make: *** [R] Error 1
> 
> I searched and found in
> 
>   _http://tolstoy.newcastle.edu.au/R/devel/04a/0679.html_
> 
> an answer from BD Ripley to a query with exactly the same problem, 

That's not who that page says the author was.

> suggesting that a newer version of gcc be used.

It's essential to allow compiled Fortran code to be included in a dynamic 
library on your platform, as was explained on that page.

> This solution is out of reach for me, as the system managers are not 
> willing to undertake
> any changes. 

What is the problem with your compiling gcc-3.4.0 and using that?  It is
no more complex that compiling R.

> They suggest instead that I use the "recommended" compilers, ecc and
> ecf. I have tried to do so with no success so far: the configure script
> fails to find convenient values for the different flags to be set. I
> have set some manually (in config.site), to no avail so far. Has anyone
> compiled R with ecc and ecf? Or with gcc-2.96?  I would be grateful if
> he/she could share his/her config.site settings. Best, ft.

The alternative is to try to compile a version of R which is as out of 
date as your OS and gcc compiler.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Thu Jun 10 09:11:48 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 10 Jun 2004 09:11:48 +0200
Subject: [R] Getting Pr from Summary(lm)
In-Reply-To: <40C7BE11.5010309@adelaide.edu.au>
References: <Pine.LNX.4.44.0406090651392.5147-100000@jotaerre.ivic.ve>
	<40C7BE11.5010309@adelaide.edu.au>
Message-ID: <16584.2484.689158.804790@gargle.gargle.HOWL>

>>>>> "David" == David J Netherway <david.netherway at adelaide.edu.au>
>>>>>     on Thu, 10 Jun 2004 11:19:05 +0930 writes:

    David> Thanks for all the relies.

    David> I recently discovered "names" and applied it to "lm"
    David> objects but did not think to apply it to the
    David> "summary" object.

ok, and now you should probably discover  'str()' as well.
Martin



From wolski at molgen.mpg.de  Thu Jun 10 09:13:31 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Thu, 10 Jun 2004 09:13:31 +0200
Subject: [R] question about hierclust {multiv}
In-Reply-To: <40C8086A.4050607@molgen.mpg.de>
References: <40C8086A.4050607@molgen.mpg.de>
Message-ID: <200406100913310377.0062F94A@mail.math.fu-berlin.de>

Hi!

I would convert the simmilarities into dissimilarities by myself.
Its quite easy.
For example to make a dissimilarity from correlation which is a simmilarity measures you can:
dcorr = 1 - cor

and dcorr will be a dissimilarity.
More general. Make the smallest value the largest and the largest the smallest. And ensure that the transformation is monotonical.


Sincerely
Eryk

*********** REPLY SEPARATOR  ***********

On 6/10/2004 at 9:06 AM Xinan Yang wrote:

>>>my major is bioinformatics, and i'm trying to cluster ( agglomerate
>>>the closest pari of observations ) in R.
>>>
>>>
>>>i have already got my own similarities metric, but do not know how to
>>>clust it based on similarities instead of dissimilarities.
>>>
>>>
>>>since the help document of hierclust mentions the parameter "sim",
>>>which seems good to me, but it doesn't appear in the code of
>>>hierclust() function again? and no sample about it.  so could anybody
>>>please help me as author?
>>>
>>>thanks in advance
>>>
>>>xinan yang
>>>xinan at molgen.mpg.de
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



Dipl. bio-chem. Eryk Witold Wolski    @    MPI-Moleculare Genetic   
Ihnestrasse 63-73 14195 Berlin       'v'    
tel: 0049-30-83875219               /   \    
mail: wolski at molgen.mpg.de        ---W-W----    http://www.molgen.mpg.de/~wolski



From j.van_den_hoff at fz-rossendorf.de  Thu Jun 10 09:36:21 2004
From: j.van_den_hoff at fz-rossendorf.de (joerg van den hoff)
Date: Thu, 10 Jun 2004 09:36:21 +0200
Subject: [R] nls and R scoping rules
Message-ID: <40C80F75.2060503@fz-rossendorf.de>

I apologize for posting this in essence the second time (no light at the 
end of the tunnel yet..):

is there a way to enforce that "nls" takes both, the data *and* the 
model definition from the parent environment? the following fragment 
shows the problem.

#======== cut here==========
wrapper <- function (choose=0)
{
   x <- seq(0,2*pi,len=100)
   y <- sin(1.5*x);
   y <- rnorm(y,y,.1*max(y))

   if (choose==0) {
      rm(fifu,pos=1)
      fifu <- function(w,x) {sin(w*x)}
   }
   else
      assign('fifu',function(w,x) {sin(w*x)},.GlobalEnv)

   res <- nls(y ~ fifu(w,x),start=list(w=1))
   res
}
#======== cut here==========

if called as "wrapper(1)" this runs fine because the fitting function 
"fifu" is assigned in the GlobalEnv.
if called as "wrapper(0)", "fifu" is defined only locally and "nls" 
(actually, "nlsModel", I think) does not know what I'm talking about.

I understand, the problem is that  the scoping rules are such that "nls"
does not resolve 'fifu' in the parent environment, but rather in the
GlobalEnv. (this is different for the data, which *are* taken from the
parent environment of the nls-call).

I tried some variants of using "eval" but without starting to modify 
"nls" itself there seems no way (up to now, anyway).

The solution to "assign" 'fifu' directly into the GlobalEnv does work, 
of course, but leads to the undesirable effect of accumulating objects 
in the workspace which are not needed there (and might overwrite 
existing ones).

in response to my first post, I got the hint that for "lm" the situation 
is different: it handles the above situation as desired (i.e. accepts 
local model definition). in so far one might even argue that this 
behaviour of "lm" and "nls" leads to an inconsistent behaviour of R in 
quite similar situations (e.g. replacing at some point a linar model by 
a nonlinear model in some large project is not achieved by simply 
replacing "lm" by "nls" somewhere deep down in the source code).

regards,
joerg

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! 
http://www.R-project.org/posting-guide.html



From maechler at stat.math.ethz.ch  Thu Jun 10 09:40:30 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 10 Jun 2004 09:40:30 +0200
Subject: [R] question about hierclust {multiv}
In-Reply-To: <40C8086A.4050607@molgen.mpg.de>
References: <40C8086A.4050607@molgen.mpg.de>
Message-ID: <16584.4206.357523.445724@gargle.gargle.HOWL>

As I've already said on the R-devel list,

 MM> why on earth are you using hierclust() from the ORPHANED package
 MM> 'multiv',  when there's  hclust() in the core 'stats' package
 MM> and 'agnes' in the recommended 'cluster' package ?

{and your question is not about hierclust but about dissimilarities and has
 already been answered}.

Regards,
Martin Maechler



From ripley at stats.ox.ac.uk  Thu Jun 10 10:55:11 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 10 Jun 2004 09:55:11 +0100 (BST)
Subject: [R] nls and R scoping rules
In-Reply-To: <40C80F75.2060503@fz-rossendorf.de>
Message-ID: <Pine.LNX.4.44.0406100942430.6105-100000@gannet.stats>

On Thu, 10 Jun 2004, joerg van den hoff wrote:

> I apologize for posting this in essence the second time (no light at the 
> end of the tunnel yet..):
> 
> is there a way to enforce that "nls" takes both, the data *and* the 
> model definition from the parent environment? the following fragment 
> shows the problem.
> 
> #======== cut here==========
> wrapper <- function (choose=0)
> {
>    x <- seq(0,2*pi,len=100)
>    y <- sin(1.5*x);
>    y <- rnorm(y,y,.1*max(y))
> 
>    if (choose==0) {
>       rm(fifu,pos=1)
>       fifu <- function(w,x) {sin(w*x)}
>    }
>    else
>       assign('fifu',function(w,x) {sin(w*x)},.GlobalEnv)
> 
>    res <- nls(y ~ fifu(w,x),start=list(w=1))
>    res
> }
> #======== cut here==========
> 
> if called as "wrapper(1)" this runs fine because the fitting function 
> "fifu" is assigned in the GlobalEnv.
> if called as "wrapper(0)", "fifu" is defined only locally and "nls" 
> (actually, "nlsModel", I think) does not know what I'm talking about.
> 
> I understand, the problem is that  the scoping rules are such that "nls"
> does not resolve 'fifu' in the parent environment, but rather in the
> GlobalEnv. (this is different for the data, which *are* taken from the
> parent environment of the nls-call).

I assume by `data' you mean not the `data' argument to nls but the 
variables referred to in your formula.

> I tried some variants of using "eval" but without starting to modify 
> "nls" itself there seems no way (up to now, anyway).
> 
> The solution to "assign" 'fifu' directly into the GlobalEnv does work, 
> of course, but leads to the undesirable effect of accumulating objects 
> in the workspace which are not needed there (and might overwrite 
> existing ones).

S has frame 1 for this purpose.  The nearest equivalent I know in R is to 
attach an environment in position 2 and assign objects like 'fifu' there, 
which avoids your `undesirable effect'.

> in response to my first post, I got the hint that for "lm" the situation 
> is different: it handles the above situation as desired (i.e. accepts 
> local model definition). in so far one might even argue that this 
> behaviour of "lm" and "nls" leads to an inconsistent behaviour of R in 
> quite similar situations (e.g. replacing at some point a linar model by 
> a nonlinear model in some large project is not achieved by simply 
> replacing "lm" by "nls" somewhere deep down in the source code).

I think that is simply wrong.  It is a _function_ in your non-linear model
definition that is not being found, not the variables (which you call
`data' above).  You do currently need to ensure that the functions in your
formulae are in scope when called from nlsModel.

Around R 1.2.x the notion was introduced that variables should be looked 
for in the environment of a formula.  Functions using model.frame got 
converted to do that, but nls did not.  I guess that the best way forward 
is to ensure that nls (and nlsModel) does search the environment of the 
formula for functions.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From prechelt at pcpool.mi.fu-berlin.de  Thu Jun 10 11:35:02 2004
From: prechelt at pcpool.mi.fu-berlin.de (Lutz Prechelt)
Date: Thu, 10 Jun 2004 11:35:02 +0200
Subject: [R] how to initialize random seed properly ?
Message-ID: <85D25331FFB7AE4C900EA467D4ADA3920459CC@circle.pcpool.mi.fu-berlin.de>


> I want to start R processes on multiple processors from single shell 
> script
> and I want all of them to have different random seeds.

The usual way would be using the process ID for the initialization.
Or the shell script could just hand out consecutive numbers 1, 2, 3, ...

  Lutz



From steffi.vf at bluewin.ch  Thu Jun 10 12:00:23 2004
From: steffi.vf at bluewin.ch (Stefanie von Felten)
Date: Thu, 10 Jun 2004 10:00:23 -0000
Subject: [R] Manova and specifying the model
Message-ID: <000a01c44ed1$8a787ce0$d4c7ca3e@DJGN730J>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040610/e908b639/attachment.pl

From ripley at stats.ox.ac.uk  Thu Jun 10 12:05:51 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 10 Jun 2004 11:05:51 +0100 (BST)
Subject: [R] nls and R scoping rules
In-Reply-To: <Pine.LNX.4.44.0406100942430.6105-100000@gannet.stats>
Message-ID: <Pine.LNX.4.44.0406101049040.6234-100000@gannet.stats>

On Thu, 10 Jun 2004, Prof Brian Ripley wrote:

> Around R 1.2.x the notion was introduced that variables should be looked 
> for in the environment of a formula.  Functions using model.frame got 
> converted to do that, but nls did not.  I guess that the best way forward 
> is to ensure that nls (and nlsModel) does search the environment of the 
> formula for functions.

It transpires that is rather easy to achieve.  At the top of nlsModel and 
nlsModel.plinear use

    env <- new.env(parent=environment(form))

instead of

    env <- new.env()

This then automatically searches for objects used in the formula

Notes

1) nls is in a namespace, so you need to fix the copy in the namespace or 
fix the source code and rebuild.

2) This will add to the baggage needed when you save() a nls fit in a 
workspace.  I think that is inevitable as we cannot just identify the 
funtions used in the formula (as they might call other functions in the 
local environment), and it is necessary to capture the objects needed to 
evaluate the formula for the predict() method to be reliable.

We could call all.names() to get the names of functions used directly in 
the formula, but that seems not good enough (previous para).

Question to the cognescenti: is the price in 2) too great for this to be 
done for 1.9.1?  I will put the change in R-devel for now.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From k.wang at auckland.ac.nz  Thu Jun 10 12:17:22 2004
From: k.wang at auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Thu, 10 Jun 2004 22:17:22 +1200
Subject: [R] Table to Data Frame
Message-ID: <008001c44ed4$1ecd2290$6633d882@stat.auckland.ac.nz>

[Forwarding on behalf of a colleague]

She's got a list with several tables:
> tab <- list()
> for(i in 1:6) {
+ tab[[i]] <- table(freq[i])
+ }
> tab
[[1]]

   0 0.17  0.3  0.5    1  2.5    3    4
 196    2    5    1    5    2    5    2

[[2]]

   0 0.17  0.3    1  2.5    3    4
 199    1    3    6    2    6    1

[[3]]

  0 0.5
217   1

[[4]]

  0 2.5
216   2

[[5]]

   0 0.17  0.3  0.5    1  2.5    3    4
 207    1    1    1    1    2    4    1

[[6]]

   0 0.17    3
 216    1    1


And would like to convert to a data frame, like:

    0   0.17  0.3  0.5  1
196        2     5     1  5
199        1     3     0  6
217        0     0     1   0
[snip]


Basically down the columns she'd like to have the counts.  But because each
table in the list has got different number of columns, I've been unable to
convert them into a data frame for her.

Any help would be greatly appreciated!

Cheers,

Kevin



From petr.pikal at precheza.cz  Thu Jun 10 12:17:56 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 10 Jun 2004 12:17:56 +0200
Subject: [R] Manova and specifying the model
In-Reply-To: <000a01c44ed1$8a787ce0$d4c7ca3e@DJGN730J>
Message-ID: <40C85174.24115.F44B1F@localhost>

Hallo

On 23 May 2004 at 13:44, Stefanie von Felten wrote:

> Hi,
> 
> I would like to conduct a MANOVA. I know that there 's the manova()
> funciton and the summary.manova() function to get the appropriate
> summary of test statistics.
> 
> I just don't manage to specify my model in the manova() call. How to
> specify a model with multiple responses and one explanatory factor?
> 
> If I type:
> pcor.manova<-manova(isol+hcom+habarea+inclin+windprot+shrubcov+herbh+b
> aregr+flowcov~pcor, data=pcor.df)
> 
> I always get error messages like:
> Error in manova(isol + hcom + habarea + inclin + windprot + shrubcov +
>  : 
>         need multiple response

Well, I looked in the help page to manova and it transferred me to a 
summary.manova help page with example which seems to organize multiple 
respodse to a matrix. So I presume

Y<- cbind(isol,hcom,habarea,inclin,windprot,shrubcov,herbh,baregr,flowcov)

and

manova(Y~pcor, data=pcor.df)

should work as expected, hopefully.

Cheers
Petr

> 
> Can someone help me?
> 
> Thanks
> Steffi
> 
> 
> 
> ----------------------------------------
> Stefanie von Felten
> K??ppelistrasse 24
> 4600 Olten
> Tel: 062/296 13 14
> e-mail: steffi.vf at bluewin.ch
>  [[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From j.van_den_hoff at fz-rossendorf.de  Thu Jun 10 12:37:50 2004
From: j.van_den_hoff at fz-rossendorf.de (joerg van den hoff)
Date: Thu, 10 Jun 2004 12:37:50 +0200
Subject: [R] nls and R scoping rules
In-Reply-To: <Pine.LNX.4.44.0406101049040.6234-100000@gannet.stats>
References: <Pine.LNX.4.44.0406101049040.6234-100000@gannet.stats>
Message-ID: <40C839FE.1000407@fz-rossendorf.de>

Prof Brian Ripley wrote:

>On Thu, 10 Jun 2004, Prof Brian Ripley wrote:
>
>  
>
>>Around R 1.2.x the notion was introduced that variables should be looked 
>>for in the environment of a formula.  Functions using model.frame got 
>>converted to do that, but nls did not.  I guess that the best way forward 
>>is to ensure that nls (and nlsModel) does search the environment of the 
>>formula for functions.
>>    
>>
>
>It transpires that is rather easy to achieve.  At the top of nlsModel and 
>nlsModel.plinear use
>
>    env <- new.env(parent=environment(form))
>
>instead of
>
>    env <- new.env()
>
>This then automatically searches for objects used in the formula
>
>Notes
>
>1) nls is in a namespace, so you need to fix the copy in the namespace or 
>fix the source code and rebuild.
>
>2) This will add to the baggage needed when you save() a nls fit in a 
>workspace.  I think that is inevitable as we cannot just identify the 
>funtions used in the formula (as they might call other functions in the 
>local environment), and it is necessary to capture the objects needed to 
>evaluate the formula for the predict() method to be reliable.
>
>We could call all.names() to get the names of functions used directly in 
>the formula, but that seems not good enough (previous para).
>
>Question to the cognescenti: is the price in 2) too great for this to be 
>done for 1.9.1?  I will put the change in R-devel for now.
>
>  
>
thank's a lot for the two responses. that  seems exactly what helps me out.

remaining question: how can I edit (I mean from within R, not by messing 
around with the source code directly) an invisible function object such 
as "nlsModel.plinear"?
help.search('invisible'), help('function'), help('edit') at least did 
not tell me.

joerg



From ripley at stats.ox.ac.uk  Thu Jun 10 12:46:07 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 10 Jun 2004 11:46:07 +0100 (BST)
Subject: [R] nls and R scoping rules
In-Reply-To: <40C839FE.1000407@fz-rossendorf.de>
Message-ID: <Pine.LNX.4.44.0406101140570.6320-100000@gannet.stats>

On Thu, 10 Jun 2004, joerg van den hoff wrote:

> Prof Brian Ripley wrote:
> 
> >On Thu, 10 Jun 2004, Prof Brian Ripley wrote:
> >
> >  
> >
> >>Around R 1.2.x the notion was introduced that variables should be looked 
> >>for in the environment of a formula.  Functions using model.frame got 
> >>converted to do that, but nls did not.  I guess that the best way forward 
> >>is to ensure that nls (and nlsModel) does search the environment of the 
> >>formula for functions.
> >>    
> >>
> >
> >It transpires that is rather easy to achieve.  At the top of nlsModel and 
> >nlsModel.plinear use
> >
> >    env <- new.env(parent=environment(form))
> >
> >instead of
> >
> >    env <- new.env()
> >
> >This then automatically searches for objects used in the formula
> >
> >Notes
> >
> >1) nls is in a namespace, so you need to fix the copy in the namespace or 
> >fix the source code and rebuild.
> >
> >2) This will add to the baggage needed when you save() a nls fit in a 
> >workspace.  I think that is inevitable as we cannot just identify the 
> >funtions used in the formula (as they might call other functions in the 
> >local environment), and it is necessary to capture the objects needed to 
> >evaluate the formula for the predict() method to be reliable.
> >
> >We could call all.names() to get the names of functions used directly in 
> >the formula, but that seems not good enough (previous para).
> >
> >Question to the cognescenti: is the price in 2) too great for this to be 
> >done for 1.9.1?  I will put the change in R-devel for now.
> >
> >  
> >
> thank's a lot for the two responses. that  seems exactly what helps me out.
> 
> remaining question: how can I edit (I mean from within R, not by messing 
> around with the source code directly) an invisible function object such 
> as "nlsModel.plinear"?
> help.search('invisible'), help('function'), help('edit') at least did 
> not tell me.

It depends why they are hidden, but for functions not exported from a
namespace see ?fixInNamespace (hence my comment about namespaces).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Thu Jun 10 12:50:16 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 10 Jun 2004 12:50:16 +0200
Subject: [R] Table to Data Frame
In-Reply-To: <008001c44ed4$1ecd2290$6633d882@stat.auckland.ac.nz>
References: <008001c44ed4$1ecd2290$6633d882@stat.auckland.ac.nz>
Message-ID: <40C83CE8.4000802@statistik.uni-dortmund.de>

Ko-Kang Kevin Wang wrote:

> [Forwarding on behalf of a colleague]
> 
> She's got a list with several tables:
> 
>>tab <- list()
>>for(i in 1:6) {


Most easy solution:
Make freq[i] a factor with all the levels that may appear. Then all 
tables have same dimension, e.g.:

tab <- lapply(freq, function(x) table(
     factor(x, levels = c(0, 0.17, 0.3, 0.5, 1, 2.5, 3, 4))))

Simplify yourself - I don't know much about "freq" ...

Uwe

> + tab[[i]] <- table(freq[i])
> + }
> 
>>tab
> 
> [[1]]
> 
>    0 0.17  0.3  0.5    1  2.5    3    4
>  196    2    5    1    5    2    5    2
> 
> [[2]]
> 
>    0 0.17  0.3    1  2.5    3    4
>  199    1    3    6    2    6    1
> 
> [[3]]
> 
>   0 0.5
> 217   1
> 
> [[4]]
> 
>   0 2.5
> 216   2
> 
> [[5]]
> 
>    0 0.17  0.3  0.5    1  2.5    3    4
>  207    1    1    1    1    2    4    1
> 
> [[6]]
> 
>    0 0.17    3
>  216    1    1
> 
> 
> And would like to convert to a data frame, like:
> 
>     0   0.17  0.3  0.5  1
> 196        2     5     1  5
> 199        1     3     0  6
> 217        0     0     1   0
> [snip]
> 
> 
> Basically down the columns she'd like to have the counts.  But because each
> table in the list has got different number of columns, I've been unable to
> convert them into a data frame for her.
> 
> Any help would be greatly appreciated!
> 
> Cheers,
> 
> Kevin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From wolski at molgen.mpg.de  Thu Jun 10 13:04:58 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Thu, 10 Jun 2004 13:04:58 +0200
Subject: [R] Table to Data Frame
In-Reply-To: <008001c44ed4$1ecd2290$6633d882@stat.auckland.ac.nz>
References: <008001c44ed4$1ecd2290$6633d882@stat.auckland.ac.nz>
Message-ID: <200406101304580367.0136DB3B@mail.math.fu-berlin.de>

Hi!


First as Uwe Ligges explained and finally.

do.call("rbind",tab)


Sincerely
Eryk


*********** REPLY SEPARATOR  ***********

On 6/10/2004 at 10:17 PM Ko-Kang Kevin Wang wrote:

>>>[Forwarding on behalf of a colleague]
>>>
>>>She's got a list with several tables:
>>>> tab <- list()
>>>> for(i in 1:6) {
>>>+ tab[[i]] <- table(freq[i])
>>>+ }
>>>> tab
>>>[[1]]
>>>
>>>   0 0.17  0.3  0.5    1  2.5    3    4
>>> 196    2    5    1    5    2    5    2
>>>
>>>[[2]]
>>>
>>>   0 0.17  0.3    1  2.5    3    4
>>> 199    1    3    6    2    6    1
>>>
>>>[[3]]
>>>
>>>  0 0.5
>>>217   1
>>>
>>>[[4]]
>>>
>>>  0 2.5
>>>216   2
>>>
>>>[[5]]
>>>
>>>   0 0.17  0.3  0.5    1  2.5    3    4
>>> 207    1    1    1    1    2    4    1
>>>
>>>[[6]]
>>>
>>>   0 0.17    3
>>> 216    1    1
>>>
>>>
>>>And would like to convert to a data frame, like:
>>>
>>>    0   0.17  0.3  0.5  1
>>>196        2     5     1  5
>>>199        1     3     0  6
>>>217        0     0     1   0
>>>[snip]
>>>
>>>
>>>Basically down the columns she'd like to have the counts.  But because
>>>each
>>>table in the list has got different number of columns, I've been unable
>>>to
>>>convert them into a data frame for her.
>>>
>>>Any help would be greatly appreciated!
>>>
>>>Cheers,
>>>
>>>Kevin
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



Dipl. bio-chem. Eryk Witold Wolski    @    MPI-Moleculare Genetic   
Ihnestrasse 63-73 14195 Berlin       'v'    
tel: 0049-30-83875219               /   \    
mail: wolski at molgen.mpg.de        ---W-W----    http://www.molgen.mpg.de/~wolski



From JonesW at kssg.com  Thu Jun 10 12:52:20 2004
From: JonesW at kssg.com (Wayne Jones)
Date: Thu, 10 Jun 2004 11:52:20 +0100
Subject: [R] Clustering Categorial and Continuous Variables
Message-ID: <6B5A9304046AD411BD0200508BDFB6CB02955F48@gimli.middleearth.kssg.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040610/2039e905/attachment.pl

From ramasamy at cancer.org.uk  Thu Jun 10 13:11:47 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: 10 Jun 2004 12:11:47 +0100
Subject: [R] Table to Data Frame
In-Reply-To: <40C83CE8.4000802@statistik.uni-dortmund.de>
References: <008001c44ed4$1ecd2290$6633d882@stat.auckland.ac.nz>
	<40C83CE8.4000802@statistik.uni-dortmund.de>
Message-ID: <1086865906.4411.4.camel@vpn202001.lif.icnet.uk>

# Generate data
universe <- c(0, 0.17, 0.5, 1, 2.5, 3, 4)
prob     <- c(94, 1, 1, 1, 1, 1, 1)/1000
tab <- list(NULL)
for(i in 1:6){  tab[[i]] <- table(sample(universe, 218, prob=prob,
rep=TRUE)) }

# Re-table 
all <- unique(unlist(sapply(tab, function(x) names(x))))
retab <- t(sapply( tab, function(x) table( factor( rep( rownames(x), x)
, levels=all ))))

The key is to have the exactly the same names (stored in object all) in
all table and sapply will simplify it.

Uwe suggestion is probably best if you can regenerate the data.

Regards, Adai.


On Thu, 2004-06-10 at 11:50, Uwe Ligges wrote:
> Ko-Kang Kevin Wang wrote:
> 
> > [Forwarding on behalf of a colleague]
> > 
> > She's got a list with several tables:
> > 
> >>tab <- list()
> >>for(i in 1:6) {
> 
> 
> Most easy solution:
> Make freq[i] a factor with all the levels that may appear. Then all 
> tables have same dimension, e.g.:
> 
> tab <- lapply(freq, function(x) table(
>      factor(x, levels = c(0, 0.17, 0.3, 0.5, 1, 2.5, 3, 4))))
> 
> Simplify yourself - I don't know much about "freq" ...
> 
> Uwe
> 
> > + tab[[i]] <- table(freq[i])
> > + }
> > 
> >>tab
> > 
> > [[1]]
> > 
> >    0 0.17  0.3  0.5    1  2.5    3    4
> >  196    2    5    1    5    2    5    2
> > 
> > [[2]]
> > 
> >    0 0.17  0.3    1  2.5    3    4
> >  199    1    3    6    2    6    1
> > 
> > [[3]]
> > 
> >   0 0.5
> > 217   1
> > 
> > [[4]]
> > 
> >   0 2.5
> > 216   2
> > 
> > [[5]]
> > 
> >    0 0.17  0.3  0.5    1  2.5    3    4
> >  207    1    1    1    1    2    4    1
> > 
> > [[6]]
> > 
> >    0 0.17    3
> >  216    1    1
> > 
> > 
> > And would like to convert to a data frame, like:
> > 
> >     0   0.17  0.3  0.5  1
> > 196        2     5     1  5
> > 199        1     3     0  6
> > 217        0     0     1   0
> > [snip]
> > 
> > 
> > Basically down the columns she'd like to have the counts.  But because each
> > table in the list has got different number of columns, I've been unable to
> > convert them into a data frame for her.
> > 
> > Any help would be greatly appreciated!
> > 
> > Cheers,
> > 
> > Kevin
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From wolski at molgen.mpg.de  Thu Jun 10 13:26:36 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Thu, 10 Jun 2004 13:26:36 +0200
Subject: [R] Clustering Categorial and Continuous Variables
In-Reply-To: <6B5A9304046AD411BD0200508BDFB6CB02955F48@gimli.middleearth.kssg.com>
References: <6B5A9304046AD411BD0200508BDFB6CB02955F48@gimli.middleearth.kssg.com>
Message-ID: <200406101326360764.014AAB18@mail.math.fu-berlin.de>

Hi!

You need a apropriate dissimilarity measure.
look for daisy in package cluster
help("daisy",package="cluster")


x: numeric matrix or data frame.  Dissimilarities will be
          computed between the rows of 'x'.  Columns of mode 'numeric'
          (i.e. all columns when 'x' is a matrix) will be recognized as
          interval scaled variables, columns of class 'factor' will be
          recognized as nominal variables, and columns of class
          'ordered' will be recognized as ordinal variables.  Other
          variable types should be specified with the 'type' argument. 
          Missing values ('NA's) are allowed. 
...

Fore example Gower 1971 proposed a coefficient for variables of different type(?) categorial continous binary.


sincerely 
Eryk

*********** REPLY SEPARATOR  ***********

On 6/10/2004 at 11:52 AM Wayne Jones wrote:

>>>Hi there fellow R users, 
>>>
>>>R has many different clustering packages (e.g. mclust,cluster,e1071).
>>>
>>>However, can anyone recommend a method to deal with data sets that
>>>contain
>>>categorial and continuous variables?
>>>
>>>Regards
>>>
>>>Wayne
>>>
>>>
>>>
>>>KSS Ltd
>>>Seventh Floor  St James's Buildings  79 Oxford Street  Manchester  M1
>>>6SS  England
>>>Company Registration Number 2800886
>>>Tel: +44 (0) 161 228 0040	Fax: +44 (0) 161 236 6305
>>>mailto:kssg at kssg.com		http://www.kssg.com
>>>
>>>
>>>The information in this Internet email is confidential and
>>>m...{{dropped}}
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



Dipl. bio-chem. Eryk Witold Wolski    @    MPI-Moleculare Genetic   
Ihnestrasse 63-73 14195 Berlin       'v'    
tel: 0049-30-83875219               /   \    
mail: wolski at molgen.mpg.de        ---W-W----    http://www.molgen.mpg.de/~wolski



From steve.roberts at man.ac.uk  Thu Jun 10 13:28:08 2004
From: steve.roberts at man.ac.uk (Steve Roberts)
Date: Thu, 10 Jun 2004 12:28:08 +0100
Subject: [R] Help with plotmath
Message-ID: <40C853D7.3056.F47687@localhost>

There must be a simple answer. I want to plot an expression, 
where the expression is held in a string variable. The "obvious" 
solution along the lines of

ex<-"x^2"
plot( c(0,1), c(0,1), main=as.expression(ex) )

gives me the a title x^2 - ie doesn't treat it like an expression. I 
suspect I don't understand expressions properly. For the real 
problem I do need to have my expression string in a variable as it is 
data-dependent. 

Can someone tell me the magic words?

Steve
  Dr Steve Roberts 
  steve.roberts at man.ac.uk

Senior Lecturer in Medical Statistics,
CMMCH NHS Trust and University of Manchester Biostatistics Group,
0161 275 5192 / 0161 276 5785



From ripley at stats.ox.ac.uk  Thu Jun 10 13:39:47 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 10 Jun 2004 12:39:47 +0100 (BST)
Subject: [R] Help with plotmath
In-Reply-To: <40C853D7.3056.F47687@localhost>
Message-ID: <Pine.LNX.4.44.0406101238260.6134-100000@gannet.stats>

You *parse* strings to form expressions.

plot( c(0,1), c(0,1), main=parse(text=ex) )


On Thu, 10 Jun 2004, Steve Roberts wrote:

> There must be a simple answer. I want to plot an expression, 
> where the expression is held in a string variable. The "obvious" 
> solution along the lines of
> 
> ex<-"x^2"
> plot( c(0,1), c(0,1), main=as.expression(ex) )
> 
> gives me the a title x^2 - ie doesn't treat it like an expression. I 
> suspect I don't understand expressions properly. For the real 
> problem I do need to have my expression string in a variable as it is 
> data-dependent. 

You might also want to explore substitute.

> Can someone tell me the magic words?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From martin.klaffenboeck at gmx.at  Thu Jun 10 13:43:24 2004
From: martin.klaffenboeck at gmx.at (Martin Klaffenboeck)
Date: Thu, 10 Jun 2004 13:43:24 +0200
Subject: [R] moving data and output?
In-Reply-To: <s0c81ab3.010@hrp3.palm.cri.nz> (from PAlspach@hortresearch.co.nz
	on Mi, Jun 09, 2004 at 22:23:51 +0200)
References: <s0c81ab3.010@hrp3.palm.cri.nz>
Message-ID: <20040610114324.GA17217@martin.kleinerdrache.org>

Am 09.06.2004 22:23:51 schrieb(en) Peter Alspach:

> >2. How can I make outputs of the grafics (plot, hist, ...) into a
> file?
> 
> what platform (Windows or Unix)?

Unix.  (Gentoo Linux in special)

Martin



From petr.pikal at precheza.cz  Thu Jun 10 13:44:21 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Thu, 10 Jun 2004 13:44:21 +0200
Subject: [R] Help with plotmath
In-Reply-To: <40C853D7.3056.F47687@localhost>
Message-ID: <40C865B5.6607.1436946@localhost>

Hi

On 10 Jun 2004 at 12:28, Steve Roberts wrote:

> There must be a simple answer. I want to plot an expression, 
> where the expression is held in a string variable. The "obvious"
> solution along the lines of
> 
> ex<-"x^2"
> plot( c(0,1), c(0,1), main=as.expression(ex) )

Plotmath example led me to

ex<-expression(x^2)
plot( c(0,1), c(0,1), main=(ex) )

hope it is what you want.

Cheers
Petr


> 
> gives me the a title x^2 - ie doesn't treat it like an expression. I
> suspect I don't understand expressions properly. For the real problem
> I do need to have my expression string in a variable as it is
> data-dependent. 
> 
> Can someone tell me the magic words?
> 
> Steve
>   Dr Steve Roberts 
>   steve.roberts at man.ac.uk
> 
> Senior Lecturer in Medical Statistics,
> CMMCH NHS Trust and University of Manchester Biostatistics Group, 0161
> 275 5192 / 0161 276 5785
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From v_bill_pikounis at merck.com  Thu Jun 10 13:50:53 2004
From: v_bill_pikounis at merck.com (Pikounis, Bill)
Date: Thu, 10 Jun 2004 07:50:53 -0400
Subject: [R] question related to S-Plus
Message-ID: <CFBD404F5E0C9547B4E10B7BDC3DFA2F041563E4@usrymx18.merck.com>

Hi Rick,

> learning R.  A question to this end, though.
> Would all of my existing S-Plus arrays, functions,
> and so on have to be re-created from scratch in R,
> or is there a way to copy them into the .RData
> directory?  The answer to this question has major

I am not sure if you have had this answered already.  You definitely cannot
just copy the binary objects into a .RData workspace.  On the S-PLUS side, I
would try 
?data.dump (and particularly, its argument oldStyle if you have S3/S-PLUS
2000 or earlier objects), and/or ?dput to get your S-PLUS objects exported
to ASCII format.  

Then from the R side, see ?source and ?dget to import things into R objects.
I do not recall off-hand what the best combinations and argument settings
are for the most common situations. But I recall the reliability being
pretty high in my practice if not perfect, which is more than I could ask
for.  

I'd also like to suggest that if you do not already, you might want to make
a habit to use and keep files of source code with objects that you create,
especially functions and calls to create data.frames, etc.... As wonderful
as R is, I find S-PLUS is still needed a lot in my work, so when you need to
work with both, having a file of the source commands is most convenient
(actually, I personally find it absolutely essential).

Hope that helps.

Bill

---------------------------------------
Bill Pikounis, Ph.D.

Biometrics Research Department
Merck Research Laboratories
PO Box 2000, MailDrop RY33-300  
126 E. Lincoln Avenue
Rahway, New Jersey 07065-0900
USA

Phone: 732 594 3913
Fax: 732 594 1565


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Rick Picard
> Sent: Wednesday, June 09, 2004 12:58 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] question related to S-Plus
> 
> 
> Dear r-help,
>       Having used S-Plus for many years, it has
> been suggested to me that I could benefit from
> learning R.  A question to this end, though.
> Would all of my existing S-Plus arrays, functions,
> and so on have to be re-created from scratch in R,
> or is there a way to copy them into the .Rdata
> directory?  The answer to this question has major
> implications for the extent to which R is attractive
> in the near term, and any enlightenment would be
> much appreciated.
> 
> Thanks,
> Rick Picard
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Mathieu.Vuilleumier at unine.ch  Thu Jun 10 14:05:27 2004
From: Mathieu.Vuilleumier at unine.ch (VUILLEUMIER Mathieu)
Date: Thu, 10 Jun 2004 14:05:27 +0200
Subject: [R] ordered probit or logit / recursive regression
Message-ID: <BD1D7341BE3930408509F95C86A451CBC95051@mail1.UNINE.CH>

I make a study in health econometrics and have a categorical dependent variable (take value 1-5). I would like to fit an ordered probit or ordered logit but i didn't find a command or package who make that. Does anyone know if it's exists ?

other question : i would like to make a least square recursive regression. Is there a special command in R ?

Thanks for your answers.

Best regards,

**************************************************************
Mathieu Vuilleumier - collaborateur scientifique
Institut de recherches ??conomique et r??gionale (IRER)
Universit?? de Neuch??tel
Pierre-??-Mazel 7, CH-2000 Neuch??tel
Tel : 032/ 718 14 66
E-mail : mathieu.vuilleumier at unine.ch



From ripley at stats.ox.ac.uk  Thu Jun 10 14:10:21 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 10 Jun 2004 13:10:21 +0100 (BST)
Subject: [R] question related to S-Plus
In-Reply-To: <CFBD404F5E0C9547B4E10B7BDC3DFA2F041563E4@usrymx18.merck.com>
Message-ID: <Pine.LNX.4.44.0406101305260.6302-100000@gannet.stats>

On Thu, 10 Jun 2004, Pikounis, Bill wrote:

> Hi Rick,
> 
> > learning R.  A question to this end, though.
> > Would all of my existing S-Plus arrays, functions,
> > and so on have to be re-created from scratch in R,
> > or is there a way to copy them into the .RData
> > directory?  The answer to this question has major
> 
> I am not sure if you have had this answered already.  You definitely cannot
> just copy the binary objects into a .RData workspace.  On the S-PLUS side, I
> would try 
> ?data.dump (and particularly, its argument oldStyle if you have S3/S-PLUS
> 2000 or earlier objects), and/or ?dput to get your S-PLUS objects exported
> to ASCII format.  
> 
> Then from the R side, see ?source and ?dget to import things into R objects.
> I do not recall off-hand what the best combinations and argument settings
> are for the most common situations. But I recall the reliability being
> pretty high in my practice if not perfect, which is more than I could ask
> for.  

To clarify that a bit.

For functions, use dump() in S-PLUS and source() in R.

For data objects such as arrays and data frames use data.dump(oldStyle=T) 
(assuming S-PLUS 5.x or 6.x, otherwise just data.dump) in S-PLUS and
data.restore in package foreign to read in.  (That will help keep 
precision, use integers as appropriate etc.)

Also, R does have an `R Data Import/Export Manual' which elaborates on 
such issues.  It is possible to read S-PLUS 4.x (or earlier) objects using 
package foreign, but there is less error checking.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Jun 10 14:13:07 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 10 Jun 2004 13:13:07 +0100 (BST)
Subject: [R] ordered probit or logit / recursive regression
In-Reply-To: <BD1D7341BE3930408509F95C86A451CBC95051@mail1.UNINE.CH>
Message-ID: <Pine.LNX.4.44.0406101312020.6302-100000@gannet.stats>

On Thu, 10 Jun 2004, VUILLEUMIER Mathieu wrote:

> I make a study in health econometrics and have a categorical dependent
> variable (take value 1-5). I would like to fit an ordered probit or
> ordered logit but i didn't find a command or package who make that. Does
> anyone know if it's exists ?

Does polr in package MASS or lrm in package Design do what you want?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From luke at stat.uiowa.edu  Thu Jun 10 14:39:11 2004
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Thu, 10 Jun 2004 07:39:11 -0500 (CDT)
Subject: [R] nls and R scoping rules
In-Reply-To: <Pine.LNX.4.44.0406101049040.6234-100000@gannet.stats>
Message-ID: <Pine.LNX.4.44.0406100735090.9551-100000@itasca2.stat.uiowa.edu>

On Thu, 10 Jun 2004, Prof Brian Ripley wrote:

> On Thu, 10 Jun 2004, Prof Brian Ripley wrote:
> 
> > Around R 1.2.x the notion was introduced that variables should be looked 
> > for in the environment of a formula.  Functions using model.frame got 
> > converted to do that, but nls did not.  I guess that the best way forward 
> > is to ensure that nls (and nlsModel) does search the environment of the 
> > formula for functions.
> 
> It transpires that is rather easy to achieve.  At the top of nlsModel and 
> nlsModel.plinear use
> 
>     env <- new.env(parent=environment(form))
> 
> instead of
> 
>     env <- new.env()
> 
> This then automatically searches for objects used in the formula
> 
> Notes
> 
> 1) nls is in a namespace, so you need to fix the copy in the namespace or 
> fix the source code and rebuild.
> 
> 2) This will add to the baggage needed when you save() a nls fit in a 
> workspace.  I think that is inevitable as we cannot just identify the 
> funtions used in the formula (as they might call other functions in the 
> local environment), and it is necessary to capture the objects needed to 
> evaluate the formula for the predict() method to be reliable.
> 
> We could call all.names() to get the names of functions used directly in 
> the formula, but that seems not good enough (previous para).
> 
> Question to the cognescenti: is the price in 2) too great for this to be 
> done for 1.9.1?  I will put the change in R-devel for now.

I don't think so.  The fit objects already contain the formula (via
the environment of the functions in the structure).  Since environment
sharing is preserved within a serialization, this means the change
would only alter the parent of the env environment, not add anything
extra.

Best,

luke


-- 
Luke Tierney
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
   Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu



From ryszard.czerminski at pharma.novartis.com  Thu Jun 10 15:25:31 2004
From: ryszard.czerminski at pharma.novartis.com (ryszard.czerminski@pharma.novartis.com)
Date: Thu, 10 Jun 2004 09:25:31 -0400
Subject: [R] how to initialize random seed properly ?
Message-ID: <OF20028691.B7307868-ON85256EAF.00499899-85256EAF.0049F207@EU.novartis.net>

Here is my best solution so far using $RANDOM in bash
or maybe somebody has "pure R"  solution ?

#!/bin/bash
for i in 1 2; do
p=tmp$i
cat > $p.R << EOF
set.seed($RANDOM)
cat('rnorm(3) =', rnorm(3), '\n')
EOF
nice R CMD BATCH --no-save --no-restore $p.R $p.log &
done
sleep 3
grep rnorm tmp[12].log

Best regards,
Ryszard





Ryszard Czerminski/PH/Novartis at PH
Sent by: r-help-bounces at stat.math.ethz.ch
06/09/2004 03:24 PM

 
        To:     r-help at stat.math.ethz.ch
        cc: 
        Subject:        [R] how to initialize random seed properly ?


I want to start R processes on multiple processors from single shell 
script
and I want all of them to have different random seeds.
One way of doing this is

        sleep 2 # (with 'sleep 1' I am often getting the same number)
               ...
        set.seed(unclass(Sys.time()))

Is there a simpler way without a need to sleep between invoking
different R processes ?

Ryszard

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From steve.roberts at man.ac.uk  Thu Jun 10 15:29:07 2004
From: steve.roberts at man.ac.uk (Steve Roberts)
Date: Thu, 10 Jun 2004 14:29:07 +0100
Subject: [R] Help with plotmath
In-Reply-To: <40C865B5.6607.1436946@localhost>
References: <40C853D7.3056.F47687@localhost>
Message-ID: <40C87032.18465.1633920@localhost>

No that isn't it I'm afraid - my expression is pasted together from 
various bits and pieces and is at some stage a string variable. I 
needed parse as Brian pointed out....

Thanks anyway,

Steve.

From:           	"Petr Pikal" <petr.pikal at precheza.cz>
To:             	"Steve Roberts" <steve.roberts at man.ac.uk>
Date sent:      	Thu, 10 Jun 2004 13:44:21 +0200
Subject:        	Re: [R] Help with plotmath
Copies to:      	r-help at stat.math.ethz.ch
Priority:       	normal

> Hi
> 
> On 10 Jun 2004 at 12:28, Steve Roberts wrote:
> 
> > There must be a simple answer. I want to plot an expression, 
> > where the expression is held in a string variable. The "obvious"
> > solution along the lines of
> > 
> > ex<-"x^2"
> > plot( c(0,1), c(0,1), main=as.expression(ex) )
> 
> Plotmath example led me to
> 
> ex<-expression(x^2)
> plot( c(0,1), c(0,1), main=(ex) )
> 
> hope it is what you want.
> 
> Cheers
> Petr
> 
> 
> > 
> > gives me the a title x^2 - ie doesn't treat it like an expression. I
> > suspect I don't understand expressions properly. For the real problem
> > I do need to have my expression string in a variable as it is
> > data-dependent. 
> > 
> > Can someone tell me the magic words?
> > 
> > Steve
> >   Dr Steve Roberts 
> >   steve.roberts at man.ac.uk
> > 
> > Senior Lecturer in Medical Statistics,
> > CMMCH NHS Trust and University of Manchester Biostatistics Group, 0161
> > 275 5192 / 0161 276 5785
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 
> Petr Pikal
> petr.pikal at precheza.cz
> 
> 


  Dr Steve Roberts 
  steve.roberts at man.ac.uk

Senior Lecturer in Medical Statistics,
CMMCH NHS Trust and University of Manchester Biostatistics Group,
0161 275 5192 / 0161 276 5785



From Setzer.Woodrow at epamail.epa.gov  Thu Jun 10 15:30:25 2004
From: Setzer.Woodrow at epamail.epa.gov (Setzer.Woodrow@epamail.epa.gov)
Date: Thu, 10 Jun 2004 09:30:25 -0400
Subject: [R] lsoda with arbitrary zero thresholds (with psuedo-solution)
Message-ID: <OF3C0295D2.AD1945F9-ON85256EAF.00484666-85256EAF.004A316A@epamail.epa.gov>





Dear Hank,
Last question first:  really, only you can say for sure if 4e-281 and
5e-11 are small enough; it depends on the units you measure your state
variables in.  However, this strategy cannot get the state variables to
exactly 0.  Obviously, you could get closer to 0.0 faster by setting the
derivatives even larger in absolute value.  You may run into problems
with the solver when the derivatives are discontinuous functions of the
state variables.

There is a simple and elegant solution in theory, but not (yet)
available in odesolve.  soda has a variant called lsodar that returns
whenever a function of the state variables satisfies a given set of
conditions (in your case, you could tell lsodar to return whenever any
state variable drops below 0.4).  Once the call to lsodar returns, you'd
then reset all the state variables that were < 0.4 to 0, and restart the
integrator at that point.  I've been meaning to add lsodar to the
odesolve package for some time, but I never seem to have the week or so
of time I'd need to do it.  You can simulate the action of lsodar by
breaking your simulation in short sections, and doing a search to
identify time intervals where a state variable drops below its critical
value (that is, suppose you note that at t1 y[1] > 0.4, and at t3, y[1]
< 0.4]; then search the time interval between t1 and t3 for the value
where abs(y[1] - 0.4) < eps, say t2 ).  Stop the current integration at
t2, change the state variables, and restart.  For any given problem,
you'd probably have to experiment with time reporting intervals (the
intervals between points in the 'times' vector) so as not to miss any
important events.

Woody

On Jun 9, 2004, at 1:43 PM, Martin Henry H. Stevens wrote:

> I have  a new and less distressing, but potentially more interesting,
> problem.
> I realized the major flaw my old "solution" and now have a solution
> that kind of works but is rather inelegant and I think may be
> problematic in difficult systems.
> Borrowing from the lsoda example again I once again highlight the code

> that I have changed to put in place arbitrary thresholds:

> parms <- c(k1=0.04, k2=1e4, k3=3e7)
> my.atol <- c(1e-6,  1e-10,  1e-6)
> times <- seq(0,1000)
> lsexamp <- function(t, y, p)
>    {
>      if(y[1] < .4) yd1 <- -y[1] ### These if, else statements are new
>      else yd1 <- -p["k1"] * y[1] + p["k2"] * y[2]*y[3]
>      if(y[3] < .4) yd3 <- -y[3]   ### These if,else statements are new
>      else yd3 <- p["k3"] * y[2]^2
>      list(c(yd1,-yd1-yd3,yd3),c(massbalance=sum(y)))
>    }
> out <- lsoda(c(.5,0,.5),times,lsexamp, parms, rtol=1e-4, atol=
my.atol,
> hmax=.1)
> matplot(out[,1],out[,2:5], type="l")
> out[dim(out)[1],] # The intent of my could was to cause population 1
to
> fall to zero as soon as it reached < 0.4. However, the populations 1
> and 2 reach approximations of 0 (4e-281 and 5e-11).

> So, I have two questions:
> Can I set thresholds in a more elegant and simpler way?
> Are the approximate zero values close enough?

> Thank you kindly, as ever.
> Sincerely,
> Hank


> On Jun 9, 2004, at 12:45 PM, Martin Henry H. Stevens wrote:

>> using R 2.0.0
>> I am trying to do some population modeling with lsoda, where I set
>> arbitrary zero population sizes when values get close to zero, but am

>> having no luck.
>> As an example of what I have tried, I use code below from the help
>> page on lsoda in which I include my modification bordered by ###
>>
>> parms <- c(k1=0.04, k2=1e4, k3=3e7)
>> my.atol <- c(1e-6,  1e-10,  1e-6)
>> times <- seq(0,)
>>
>> lsexamp <- function(t, y, p)
>>   { ### The next line is where I try to insert the threshold
>> ifelse(y < 0.4,  0, y)
>> ###### all else is unchanged
>>     yd1 <- -p["k1"] * y[1] + p["k2"] * y[2]*y[3]
>>     yd3 <- p["k3"] * y[2]^2
>>     list(c(yd1,-yd1-yd3,yd3),c(massbalance=sum(y)))
>>   }
>> out <- lsoda(c(.5,0,.5),times,lsexamp, parms, rtol=1e-4, atol=
>> my.atol) # Initial values differ from help page
>> matplot(out[,1],out[,2:5], type="l")
>> out[dim(out)[1],] # The intent of my could was to cause population 1
>> to fall to zero as soon as it reached < 0.4
>>
>> Any thoughts would be appreciated. Thanks!
>> Hank Stevens
>>
>>
>> Dr. Martin Henry H. Stevens, Assistant Professor
>> 338 Pearson Hall
>> Botany Department
>> Miami University
>> Oxford, OH 45056
>>
>> Office: (513) 529-4206
>> Lab: (513) 529-4262
>> FAX: (513) 529-4243
>> http://www.cas.muohio.edu/botany/bot/henry.html
>> http://www.muohio.edu/ecology/
>> http://www.muohio.edu/botany/
>> "E Pluribus Unum"

R. Woodrow Setzer, Jr.                        Phone: (919) 541-0128
Experimental Toxicology Division             Fax:  (919) 541-4284
Pharmacokinetics Branch
NHEERL B143-01; US EPA; RTP, NC 27711



From itayf at fhcrc.org  Thu Jun 10 15:49:12 2004
From: itayf at fhcrc.org (Itay Furman)
Date: Thu, 10 Jun 2004 06:49:12 -0700 (PDT)
Subject: [R] how to initialize random seed properly ?
In-Reply-To: <OF20028691.B7307868-ON85256EAF.00499899-85256EAF.0049F207@EU.novartis.net>
Message-ID: <Pine.LNX.4.44.0406100644580.5009-100000@cezanne.fhcrc.org>


perldoc -f srand
has a quick and good advice on initializing a random sequence.
I second the recommendation to prepare the list of random numbers 
in advance for sake of reproducibility.

	Itay
--------------------------------------------------------------
itayf at fhcrc.org		Fred Hutchinson Cancer Research Center



From rossini at blindglobe.net  Thu Jun 10 15:56:42 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Thu, 10 Jun 2004 06:56:42 -0700
Subject: [R] how to initialize random seed properly ?
In-Reply-To: <OF20028691.B7307868-ON85256EAF.00499899-85256EAF.0049F207@EU.novartis.net>
	(ryszard
	czerminski's message of "Thu, 10 Jun 2004 09:25:31 -0400")
References: <OF20028691.B7307868-ON85256EAF.00499899-85256EAF.0049F207@EU.novartis.net>
Message-ID: <854qpj8qjp.fsf@servant.blindglobe.net>

ryszard.czerminski at pharma.novartis.com writes:

> Here is my best solution so far using $RANDOM in bash
> or maybe somebody has "pure R"  solution ?

Sure -- use the rsprng or rlecuyer packages from CRAN, which support
(independent?) streams of random numbers from a common set of seeds.

best,
-tony

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From jyanosky at hsph.harvard.edu  Thu Jun 10 16:04:27 2004
From: jyanosky at hsph.harvard.edu (Jeff Yanosky)
Date: Thu, 10 Jun 2004 10:04:27 -0400
Subject: [R] Response to questions raised in Mar 17 reply
Message-ID: <40CE9695@www.webmail.hsph.harvard.edu>

Hi, 
I would like to repsond to a few questions raised in a reply to a question 
posted by another user on March 17, 2004.  The entire message is copied at the 
end of this email.

The relevant questions and statements are as follows:

What did you not understand about help(memory.size)? 
This is also in the rw-FAQ: what in that did you not understand? 
...
Yes, so try a machine with 2Gb RAM.

I have received the "Error: cannot allocate vector of size xxx Kb" many times 
and have scoured the R help files (?memory.limit), the R help archive pages, 
and the R-W FAQs.  The following issues are what I specifically do not 
understand about the help information available to address memory issues:

1) In ?memory.limit, it is stated that memory.size(max=TRUE) will return "the 
maximum amount of memory obtained from the operating system".  However, my 
system returns only 1.57 GB of maximum memory, even though under System 
Properties in Windows XP the operating system reports 2.5 GB on the system.

2)In ?memory.limit, it is stated that memory.limit(size=NA) will "report the 
memory size", but is this the available memory, the total, or the amount in 
use?  If it is the total, how is it different from memory.size(max=TRUE)?  On 
my system the result of the three relevant memory functions are as follows:

> MemSizeinGB=(memory.limit(size=NA))/1E9
> MaxMemGB=(memory.size(max=T))/1E9
> MemInUseGB=(memory.size(max=F))/1E9
> MemSizeinGB
[1] 4.246733
> MaxMemGB
[1] 1.567990
> MemInUseGB
[1] 0.7674916

3)I have used the --max-mem-size option in the icon properties, shortcut tab, 
target field after the pathname of the executable, and this appears to have 
increased the result of memory.limit(size=NA).  However, even with the 
memory.limit set to 4.24 GB, I receive the "Error: cannot allocate vector of 
135154 Kb" when trying to fit a GAM to some temperature data.  How was the "2 
GB" quantity determined in the reply below?  Is there a rule of thumb for 
determining how much memory will be needed for a specific vector size?

Thanks very much, 
Jeff Yanosky
Harvard School of Public Health



On Wed, 17 Mar 2004, Matt Loveland wrote:


> I'm having trouble with glmmPQL.


I think you are having trouble with memory limits, actually. As the 
author of glmmPQL, I don't appreciate my code being blamed for something 
else.


> I'm fitting a 2 level random intercept model, with 90,000 cases and about 
330 groups. I'm unable to get any results on the full data set. I can get it 
to work if I sample down to about 30,000 cases. But for models with N's much 
larger than that I get the following warning message:
>
> 
m3=glmmPQL(prepfood~iage+iemployed+iwhite+ieduclevl+imarried+servcomm+leadgrup
+leadsty4, family=binomial, random=~1|congrega1,data=data)
> Error: cannot allocate vector of size 4135 Kb
> In addition: Warning message:
> Reached total allocation of 253Mb: see help(memory.size)
>
> I've tried increasing my virtual memory size, and also defragmenting my
> hard drive. It hasn't helped. I've seen other people asking similar
> questions on the archive, but it seems that this problem should have
> gone away after earlier versions of R, is that right?


Do read the page it asks you too. You are on Windows, and you need to use 
the --max-mem-size flag when starting R to increase the memory available 
to R. However, if you do swapping may make your machine nigh unusable.


What did you not understand about help(memory.size)? 
This is also in the rw-FAQ: what in that did you not understand?


> Is this a data problem, am I fitting a bad model, or is it a memory size
> problem. I'm hoping the last one, and any help is appreciated.


Yes, so try a machine with 2Gb RAM.



-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From richard.kittler at amd.com  Thu Jun 10 16:21:42 2004
From: richard.kittler at amd.com (richard.kittler@amd.com)
Date: Thu, 10 Jun 2004 07:21:42 -0700
Subject: [R] Lattice::qqmath -- groups option question
Message-ID: <858788618A93D111B45900805F85267A0BCB2D69@caexmta3.amd.com>

Does the 'groups' option on qqmath just color the points differently in the main distribution or does it actually overlay separate quantile plots for each subset? I would like to be able to do the latter.

--Rich

Richard Kittler 
AMD TDG
408-749-4099



From deepayan at stat.wisc.edu  Thu Jun 10 16:54:36 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 10 Jun 2004 09:54:36 -0500
Subject: [R] Lattice::qqmath -- groups option question
In-Reply-To: <858788618A93D111B45900805F85267A0BCB2D69@caexmta3.amd.com>
References: <858788618A93D111B45900805F85267A0BCB2D69@caexmta3.amd.com>
Message-ID: <200406100954.36716.deepayan@stat.wisc.edu>

On Thursday 10 June 2004 09:21, richard.kittler at amd.com wrote:
> Does the 'groups' option on qqmath just color the points differently
> in the main distribution or does it actually overlay separate
> quantile plots for each subset? I would like to be able to do the
> latter.

The 'groups' option doesn't really do anything in any of the high level 
lattice functions, it just sends the groups variable (and associated 
subscripts) to the panel function. It is the panel function that needs 
to know how to deal with that information. 

As things stand now, panel.qqmath doesn't know what to do with it. More 
importantly, qqmath is currently structured in a way that would make 
this very difficult to implement.

I don't think there's a good reason for this (other than consistency 
with S-PLUS) and I would prefer to allow the possibility of grouped Q-Q 
plots. But this would need a fair bit of restructuring and will 
certainly not happen before R 2.0.0.

Deepayan



From HBaize at buttecounty.net  Thu Jun 10 16:55:40 2004
From: HBaize at buttecounty.net (Baize, Harold)
Date: Thu, 10 Jun 2004 07:55:40 -0700
Subject: [R] Informal discussion group about R
Message-ID: <9C6522E7D1039748892843B70C845484AEBC48@mail2.bci.buttecounty.net>


I've started a "tribe" for discussing R and 
sharing scripts. Tribe.net is one of the popular 
on-line social communities, like "Friendster". 
Visit and see if it is a forum that you find useful. 
To join the "tribe" you will need to register with 
Tribe.net.

I hope it will be of help to newbies, although I'm 
new to R myself. 

Here is the url:

 http://r-statisticalenvironment.tribe.net


Harold



From zhuw at mail.smu.edu  Thu Jun 10 12:06:54 2004
From: zhuw at mail.smu.edu (Zhu Wang)
Date: Thu, 10 Jun 2004 10:06:54 +0000
Subject: [R] Building package on Windows: No rule to make target '-llapack'
In-Reply-To: <Pine.LNX.4.44.0406100729270.3886-100000@gannet.stats>
References: <Pine.LNX.4.44.0406100729270.3886-100000@gannet.stats>
Message-ID: <1086862014.2807.17.camel@zwang.stat.smu.edu>

On Thu, 2004-06-10 at 06:40, Prof Brian Ripley wrote:
> On Wed, 9 Jun 2004, Zhu Wang wrote:
> 
> > I have a problem to build a package on Windows XP while there is no
> > problem on Linux. The Makefile is something like:
> 
> There would be a problem on Linux, if that Makefile were used.  I suspect
> it is not used.

Believe or not, it works.

> > ###########
> > LIBNAME=cts
> > 
> > PKG_LIBS = $(LAPACK_LIBS) $(BLAS_LIBS) $(FLIBS)
> 
> (You should not need $(FLIBS) here.)
> 
> > OBJS=file1.o ... file20.o -llapack -lblas
> 
> The last two are not objects, and -llapack refers to something like 
> liblapack.a.  I don't see how this can work anywhere.  On the other hand, 
> you have lapack and blas selected in PKG_LIBS, but you are not using that 
> macro.

I did this because I did not find good examples about how to link Lapack
and Blas using R. So I tried this and it works.

> > $(LIBNAME)$(SHLIB_EXT): $(OBJS)
> >         $(SHLIB_LD) $(SHLIB_LDFLAGS) -o $@ $(OBJS) $(FLIBS)
> > 
> > clean:
> >         @rm -f *.o *.$(SHLIB_EXT)
> > 
> > realclean: clean
> > #############
> > 
> > To build the package on Windows XP, I have followed the instructions to
> > install tools/software required and it seems the 'make' worked fine,
> > except for the error message:
> > 
> > make[3]: No rule to make target 'llapack', needed by 'cts.a'. stop.
> > 
> > Now I think maybe two problems: one is that maybe I do not have Lapack and Blas installed
> > on Windows XP and second is that I do not set up a correct file, something like 'configure'.
> > Maybe there are more problems. I have read some files in \src\gnuwin32, but I did not find
> > what I needed.
> 
> What are you actually trying to do?  It is not normal for a package to
> have a Makefile in its src directory, which is what I guess (but only
> guess) you have presented.  All that is normally needed is a Makevars
> file.  Take a look at e.g. that in mclust.

I did find an R package which uses Makefile in its src directory, and I
just happened to be familiar with. But it was on the author's website
and not on CRAN. Maybe that's a bad example. I will take a look at that
in mclust.

> Is this your package or someone else's (in which case why are you not
> asking the author)?  Either you or that author needs to read `Writing R 
> Extensions' and look at some of the many examples in CRAN packages.

This is mine. I agree with you. Certainly there are more stuff I need to
know about 'Writing R Extensions'. The package is on my website, see
below. Using the following:

#R CMD check cts
#R CMD INSTALL cts

I have successfully installed the package. Thanks for your advice. On
the other hand, it might be a successful story how R is robust!

-- 
Zhu Wang

Statistical Science Department
Southern Methodist University
Dallas, TX 75275-0332
Phone:(214)768-2453  Fax:(214)768-4035
zhuw at mail.smu.edu
http://people.smu.edu/zhuw



From rwang at math.ucalgary.ca  Thu Jun 10 17:27:42 2004
From: rwang at math.ucalgary.ca (Rui)
Date: Thu, 10 Jun 2004 09:27:42 -0600
Subject: [R] Everything was ok, but I failed to get .o file in g77
Message-ID: <000001c44eff$79234c80$f63d9f88@math.ucalgary.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040610/5ce1377f/attachment.pl

From richard.kittler at amd.com  Thu Jun 10 17:38:29 2004
From: richard.kittler at amd.com (richard.kittler@amd.com)
Date: Thu, 10 Jun 2004 08:38:29 -0700
Subject: [R] Lattice::qqmath -- groups option question
Message-ID: <858788618A93D111B45900805F85267A0BCB2D6C@caexmta3.amd.com>

Thanks for the info. Given that I rule out qqmath would the best method be to make repeated calls to qqnorm without plotting and then overlay the results or is there a more elegant method? 

--Rich

Richard Kittler 
AMD TDG
408-749-4099

-----Original Message-----
From: Deepayan Sarkar [mailto:deepayan at stat.wisc.edu] 
Sent: Thursday, June 10, 2004 7:55 AM
To: r-help at stat.math.ethz.ch
Cc: Kittler, Richard
Subject: Re: [R] Lattice::qqmath -- groups option question


On Thursday 10 June 2004 09:21, richard.kittler at amd.com wrote:
> Does the 'groups' option on qqmath just color the points differently 
> in the main distribution or does it actually overlay separate quantile 
> plots for each subset? I would like to be able to do the latter.

The 'groups' option doesn't really do anything in any of the high level 
lattice functions, it just sends the groups variable (and associated 
subscripts) to the panel function. It is the panel function that needs 
to know how to deal with that information. 

As things stand now, panel.qqmath doesn't know what to do with it. More 
importantly, qqmath is currently structured in a way that would make 
this very difficult to implement.

I don't think there's a good reason for this (other than consistency 
with S-PLUS) and I would prefer to allow the possibility of grouped Q-Q 
plots. But this would need a fair bit of restructuring and will 
certainly not happen before R 2.0.0.

Deepayan



From ripley at stats.ox.ac.uk  Thu Jun 10 17:41:47 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 10 Jun 2004 16:41:47 +0100 (BST)
Subject: [R] Everything was ok, but I failed to get .o file in g77
In-Reply-To: <000001c44eff$79234c80$f63d9f88@math.ucalgary.ca>
Message-ID: <Pine.LNX.4.44.0406101635180.15188-100000@gannet.stats>

I am afraid your message is in an unreadable character set and none was 
specified by your mailer.  So I don't really know what you used, but I 
have ^V where - should be, and so on.

What you would need is

g77 -O2 -c all.f
     ^ note
You appear to have asked for a file '2' here via -o2.

g77 -shared -o all.dll all.o -lg2c
             ^ note          ^^^^^ note

It is much less error-prone to use

R CMD SHLIB all.f   under Unix-alikes

Rcmd SHLIB all.f    under Windows

On Thu, 10 Jun 2004, Rui wrote:

> Hi folks,
>  
> I tried to compile a FORTRAN routine using the command "g77 -o2 -c
> all.f", it seems everything is fine, there is no error message at all,
> but finally I can not get the file all.o. Therefore, I can not move on
> to the next step using "g77 -shared -c all.dll all.o" to get the file
> all.dll. The following is what happened on my screen:
> > g77 -o2 -c all.f
> >
> Could anyone help me? Thanks in advance.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From christopher.knight at plant-sciences.oxford.ac.uk  Thu Jun 10 17:46:56 2004
From: christopher.knight at plant-sciences.oxford.ac.uk (Chris Knight)
Date: 10 Jun 2004 16:46:56 +0100
Subject: [R] odesolve: lsoda vs rk4
Message-ID: <1086882415.1328.168.camel@dops7118.plants.ox.ac.uk>

I'm trying to use odesolve for integrating various series of coupled 1st
order differential equations (derived from a system of enzymatic
catalysis and copied below, apologies for the excessively long set of
parameters).

The thing that confuses me is that, whilst I can run the function rk4:

out <- rk4(y=y,times=times,func=func, parms=parms)

and the results look not unreasonable:

out<-as.data.frame(out)
par(mfrow=c(4,1))
for (i in 2:(dim(out)[2]))plot(out[,1],out[,i], pch=".", xlab="time",
ylab=names(out)[i])

If I try doing the same thing with lsoda:

out <- lsoda(y=y,times=times,func=func, parms=parms, rtol=1e-1, atol=
1e-1)

I run into problems with a series of 'Excessive precision requested'
warnings with no output beyond the first time point.

Fiddling with rtol and atol doesn't seem to do very much.

What is likely to be causing this (I'm guessing the wide range of the
absolute values of the parameters can't be helping), is there anything I
can sensibly do about it and, failing that, can I reasonably take the
rk4 results as being meaningful?

Any help much appreciated,
Thanks in advance,

Chris


func <- function(t, y, p)
{
Ad <- p["p2"]*(p["p1"]*y["A"]*y["D"])/(p["p2"]+p["p3"]) +
p["p6"]*(p["p4"]*y["B"]*p["p10"])/(p["p5"]+p["p6"]) -
p["p1"]*y["A"]*y["C"]
Bd <- p["p3"]*(p["p1"]*y["A"]*y["D"])/(p["p2"]+p["p3"]) +
p["p5"]*(p["p4"]*y["B"]*p["p10"])/(p["p5"]+p["p6"]) -
p["p4"]*y["B"]*p["p10"] 
Cd <- (p["p1"]+p["p7"])*y["A"]*y["D"] -
p["p1"]*y["A"]*y["C"]-p["p9"]*y["C"]
Dd <-p["p9"]*y["C"] - p["p7"]*y["A"]*y["D"]
list(c(Ad, Bd, Cd, Dd))
}

parms<-c(p1=4.8e5, p2=1.25, p3=1.3, p4=1e6, p5=1, p6=1.25, p7=1e6,
p8=16, p9=0.35, p10=0.235e-6)
y<-c(A=2.5e-6,B=2.5e-6, C=1.7e-6, D=0.57e-6)
times <- c(0.05 * (0:999))
-- 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Dr. Christopher Knight                               Tel:+44 1865 275111
Dept. Plant Sciences                                     +44 1865 275790
South Parks Road
Oxford     OX1 3RB                                   Fax:+44 1865 275074
` ?? . , ,><(((??> 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



From rwang at math.ucalgary.ca  Thu Jun 10 17:48:38 2004
From: rwang at math.ucalgary.ca (Rui)
Date: Thu, 10 Jun 2004 09:48:38 -0600
Subject: [R] Questions about Preserving registers
Message-ID: <000501c44f02$6588a780$f63d9f88@math.ucalgary.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040610/44d89a2f/attachment.pl

From ligges at statistik.uni-dortmund.de  Thu Jun 10 17:59:58 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 10 Jun 2004 17:59:58 +0200
Subject: [R] Questions about Preserving registers
In-Reply-To: <000501c44f02$6588a780$f63d9f88@math.ucalgary.ca>
References: <000501c44f02$6588a780$f63d9f88@math.ucalgary.ca>
Message-ID: <40C8857E.8010204@statistik.uni-dortmund.de>

Rui wrote:

> Hi folks,
>  
> I tried to use Mirosoft Fortran Powerstation 4.0 to create a dll file.

And what is the reason not to use the recommended gcc?

Uwe Ligges


> However, when I used the command dyn.load("test.dll"), I got the
> following message:
>  
> NULL
> Warning message: 
> DLL attempted to change FPU control word from 9001f to 90003
>  
> I read the instruction on Duncan Murdoch?s website about preserving
> registers, but I still don?t understand it. For example,
>  
> 1. On first entry to the DLL, set the control word to the standard R
> value, then save that as the Delphi default. Put this code in the
> library module:
> ## My question: Is that mean put the following code in my Fortran
> routine without any change based on the message I got? 
>  
> procedure Rwin_fpset; cdecl; external 'R.dll';
>  
> function Get8087CW:word;
> begin
>   asm
>     fstcw result
>   end;
> end;
>  
> begin
>   Rwin_fpset();
>   Set8087CW(Get8087CW);
> end.
>  
> 2. After any operation that may have changed the status word, put this
> call before returning to R:
> ##  My question: where do I put the line below?
>  
>   Set8087CW(Default8087CW);
>  
>  
> 3. The following function may be useful in debugging; it returns false
> when the control word has been unexpectedly changed:
>  
> function Okay8087CW:boolean;
> begin
>   result := ((Default8087CW xor Get8087CW) and not $E060) = 0;
> end;
>  
> Thanks.
>  
> Rui
>  
> Phone: (403)220-4501
> Email: rwang at math.ucalgary.ca
> Department of Mathematics and Statistics
> University of Calgary
>  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From rishi at iqmail.net  Thu Jun 10 18:10:59 2004
From: rishi at iqmail.net (Rishi Ganti)
Date: Thu, 10 Jun 2004 09:10:59 -0700
Subject: [R] displaying a table in full vs. typing variable name
Message-ID: <98181bd75139453c93cb8200151baf40.rishi@iqmail.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040610/3d19fce6/attachment.pl

From tlumley at u.washington.edu  Thu Jun 10 18:12:05 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 10 Jun 2004 09:12:05 -0700 (PDT)
Subject: [R] Questions about Preserving registers
In-Reply-To: <000501c44f02$6588a780$f63d9f88@math.ucalgary.ca>
References: <000501c44f02$6588a780$f63d9f88@math.ucalgary.ca>
Message-ID: <Pine.A41.4.58.0406100910240.114950@homer09.u.washington.edu>

On Thu, 10 Jun 2004, Rui wrote:

> Hi folks,
>
> I tried to use Mirosoft Fortran Powerstation 4.0 to create a dll file.
> However, when I used the command dyn.load("test.dll"), I got the
> following message:
>
> NULL
> Warning message:
> DLL attempted to change FPU control word from 9001f to 90003
>
> I read the instruction on Duncan Murdochs website about preserving
> registers, but I still dont understand it. For example,

This code is for Delphi.  You would need to see how to set the FPU control
work in Powerstation Fortran, which should be described in its manual.

The warning is harmless, so you could just ignore it.  The warning means
that R has done the change for you.

	-thomas



>
> 1. On first entry to the DLL, set the control word to the standard R
> value, then save that as the Delphi default. Put this code in the
> library module:
> ## My question: Is that mean put the following code in my Fortran
> routine without any change based on the message I got?
>
> procedure Rwin_fpset; cdecl; external 'R.dll';
>
> function Get8087CW:word;
> begin
>   asm
>     fstcw result
>   end;
> end;
>
> begin
>   Rwin_fpset();
>   Set8087CW(Get8087CW);
> end.
>
> 2. After any operation that may have changed the status word, put this
> call before returning to R:
> ##  My question: where do I put the line below?
>
>   Set8087CW(Default8087CW);
>
>
> 3. The following function may be useful in debugging; it returns false
> when the control word has been unexpectedly changed:
>
> function Okay8087CW:boolean;
> begin
>   result := ((Default8087CW xor Get8087CW) and not $E060) = 0;
> end;
>
> Thanks.
>
> Rui
>
> Phone: (403)220-4501
> Email: rwang at math.ucalgary.ca
> Department of Mathematics and Statistics
> University of Calgary
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From andy_liaw at merck.com  Thu Jun 10 18:20:03 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 10 Jun 2004 12:20:03 -0400
Subject: [R] displaying a table in full vs. typing variable name
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7E87@usrymx25.merck.com>

Can you show how you know the output has only two columns?

My suspicion is that you are only seeing the `bottom' of the output, as in:

> x <- as.data.frame(matrix(rnorm(45), 5, 9))
> x
          V1         V2         V3          V4         V5           V6
V7
1 -1.3241632 -0.9702919 -0.5070715 -0.01663753 -0.4867538 -0.557795429
0.03280373
2 -0.5320836 -0.1909115 -1.7211625  0.96653354 -0.9725586 -0.615651853
-0.94440277
3  1.0103191 -0.2025051 -0.3503634  0.79479308  0.1371226  0.001793848
0.20177179
4 -0.4493096 -0.4028332  0.5452380 -1.10063483 -0.5791613 -0.349573429
-0.15464095
5  1.7484486  0.1401273  1.7100899  0.31960320  1.2972502 -0.039784320
1.24036281
          V8          V9
1 -0.7514973  0.07072891
2 -1.0099401 -0.02827683
3 -1.3194879  0.19499958
4  1.7867851 -0.29690245
5 -0.1055813 -0.33510362

Why not use write.table() or even write(), instead of sink()?

Andy

> From: Rishi Ganti
> 
> I have a data frame called totaldata that is 10,000 rows by 
> about 9 columns.  The
> data frame contains many zero entries which are important.  
> If I type  >totaldata
>  it only prints out the first two columns. I expect (and 
> want) it to print out all
> 9 columns. (I am actually "sinking" this to a text file, so 
> imagine my surprise
> when the text file has only a few columns).  I know totaldata 
> contains all the data as
> summary(totaldata) lists what I expect (9 variables).  Thanks 
> for your help.  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ligges at statistik.uni-dortmund.de  Thu Jun 10 18:26:04 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 10 Jun 2004 18:26:04 +0200
Subject: [R] displaying a table in full vs. typing variable name
In-Reply-To: <98181bd75139453c93cb8200151baf40.rishi@iqmail.net>
References: <98181bd75139453c93cb8200151baf40.rishi@iqmail.net>
Message-ID: <40C88B9C.60204@statistik.uni-dortmund.de>

Rishi Ganti wrote:

> I have a data frame called totaldata that is 10,000 rows by about 9 columns.

If "about 9" equals 2, the behaviour reported below is expected.


 > The
> data frame contains many zero entries which are important.  If I type  >totaldata
>  it only prints out the first two columns. I expect (and want) it to print out all
> 9 columns. 

Are you sure that R has not wrapped the data frame by printing the first 
few columns at first, and ended by printing the last 2 columns?
This way the data fit into your window. You can set the width using 
options(), and get all columns at once.


> (I am actually "sinking" this to a text file, so imagine my surprise
> when the text file has only a few columns).  


Please consider to use write.table()!

Uwe Ligges


 > I know totaldata contains all the data as
> summary(totaldata) lists what I expect (9 variables).  Thanks for your help.  
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From MSchwartz at MedAnalytics.com  Thu Jun 10 18:30:26 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 10 Jun 2004 11:30:26 -0500
Subject: [R] displaying a table in full vs. typing variable name
In-Reply-To: <40C88B9C.60204@statistik.uni-dortmund.de>
References: <98181bd75139453c93cb8200151baf40.rishi@iqmail.net>
	<40C88B9C.60204@statistik.uni-dortmund.de>
Message-ID: <1086885026.29898.347.camel@localhost.localdomain>

On Thu, 2004-06-10 at 11:26, Uwe Ligges wrote:
> Rishi Ganti wrote:
> 
> > I have a data frame called totaldata that is 10,000 rows by about 9 columns.
> 
> If "about 9" equals 2, the behaviour reported below is expected.


That is, of course, for sufficiently large values of "about"...

;-)

Marc



From deepayan at stat.wisc.edu  Thu Jun 10 18:32:10 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 10 Jun 2004 11:32:10 -0500
Subject: [R] Lattice::qqmath -- groups option question
In-Reply-To: <858788618A93D111B45900805F85267A0BCB2D6C@caexmta3.amd.com>
References: <858788618A93D111B45900805F85267A0BCB2D6C@caexmta3.amd.com>
Message-ID: <200406101132.10897.deepayan@stat.wisc.edu>

On Thursday 10 June 2004 10:38, richard.kittler at amd.com wrote:
> Thanks for the info. Given that I rule out qqmath would the best
> method be to make repeated calls to qqnorm without plotting and then
> overlay the results or is there a more elegant method?

I don't think there's any good way currently. 

You could use densityplot (the reason to prefer that over xyplot is that 
the formula is similar, and the panel function would only be supplied 
an 'x' vector and no 'y' vector), with a custom prepanel and 
panel.groups functions that draw the Q-Q plot given the 'x' values. I 
don't have time to work out the details right now, but let me know if 
you have trouble getting it to work.

Deepayan



From p.dalgaard at biostat.ku.dk  Thu Jun 10 18:26:19 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 10 Jun 2004 18:26:19 +0200
Subject: [R] odesolve: lsoda vs rk4
In-Reply-To: <1086882415.1328.168.camel@dops7118.plants.ox.ac.uk>
References: <1086882415.1328.168.camel@dops7118.plants.ox.ac.uk>
Message-ID: <x28yev751w.fsf@biostat.ku.dk>

Chris Knight <christopher.knight at plant-sciences.oxford.ac.uk> writes:

> I'm trying to use odesolve for integrating various series of coupled 1st
> order differential equations (derived from a system of enzymatic
> catalysis and copied below, apologies for the excessively long set of
> parameters).
> 
> The thing that confuses me is that, whilst I can run the function rk4:
.....
> I run into problems with a series of 'Excessive precision requested'
> warnings with no output beyond the first time point.
> 
> Fiddling with rtol and atol doesn't seem to do very much.
.....
> parms<-c(p1=4.8e5, p2=1.25, p3=1.3, p4=1e6, p5=1, p6=1.25, p7=1e6,
> p8=16, p9=0.35, p10=0.235e-6)
> y<-c(A=2.5e-6,B=2.5e-6, C=1.7e-6, D=0.57e-6)
> times <- c(0.05 * (0:999))

I'm not going to dig deeply into your problem formulation, but my
hunch is that you need to rescale your problem so that you get rid of
the very small and large numbers in the parameters and starting
values. Extreme values there tend to confuse convergence criteria.
(The difference between the two solvers is probably exactly that lsoda
has internal loops and convergence criteria, whereas rk4 just chugs
along in fixed time steps.)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From dmurdoch at pair.com  Thu Jun 10 20:38:28 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Thu, 10 Jun 2004 14:38:28 -0400
Subject: [R] Questions about Preserving registers
In-Reply-To: <Pine.A41.4.58.0406100910240.114950@homer09.u.washington.edu>
References: <000501c44f02$6588a780$f63d9f88@math.ucalgary.ca>
	<Pine.A41.4.58.0406100910240.114950@homer09.u.washington.edu>
Message-ID: <lhahc0pouc4ir465ds1ohrv1vphq2sv57g@4ax.com>

On Thu, 10 Jun 2004 09:12:05 -0700 (PDT), Thomas Lumley
<tlumley at u.washington.edu> wrote :

>> DLL attempted to change FPU control word from 9001f to 90003
>>
>> I read the instruction on Duncan Murdoch?s website about preserving
>> registers, but I still don?t understand it. For example,
>
>This code is for Delphi.  You would need to see how to set the FPU control
>work in Powerstation Fortran, which should be described in its manual.
>
>The warning is harmless, so you could just ignore it.  The warning means
>that R has done the change for you.

I think it's better to try to fix it.  R only checks once (on
loading); if Fortran makes the change again, then R code might go
wrong later, and if Fortran depends on the original value, then the
Fortran code might not work properly.

Duncan Murdoch



From rpeng at jhsph.edu  Thu Jun 10 23:38:00 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Thu, 10 Jun 2004 17:38:00 -0400
Subject: [R] off topic publication question
In-Reply-To: <200406081711.i58HB4q23066@gator.dt.uh.edu>
References: <200406081711.i58HB4q23066@gator.dt.uh.edu>
Message-ID: <40C8D4B8.4080803@jhsph.edu>

I always thought that the use of "we" was a reference to the 
author(s) and the reader.

-roger

Erin Hodgess wrote:

> Dear R People:
> 
> Please excuse the off topic question, but I
> know that I'll get a good answer here.
> 
> 
> If a single author is writing a journal article,
> should she use "We performed a test"
> or "I performed a test",
> please?
> 
> I had learned to use "we" without regard to the number
> of authors.  Is that true, please?
> 
> Thanks for the off topic help.
> 
> Sincerely,
> Erin Hodgess
> Associate Professor
> Department of Computer and Mathematical Sciences
> University of Houston - Downtown
> mailto: hodgess at gator.uhd.edu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From k.wang at auckland.ac.nz  Thu Jun 10 23:56:19 2004
From: k.wang at auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Fri, 11 Jun 2004 09:56:19 +1200
Subject: [R] How to Describe R to Finance People
References: <20040604115038.NQWW12806.web4-rme.xtra.co.nz@kevinlpt>
	<40C084D9.9070502@bank-banque-canada.ca>
	<1086365219.12223.67.camel@localhost.localdomain>
	<40C0CCCC.4050407@bankofcanada.ca>
	<1086406226.12223.267.camel@localhost.localdomain>
	<003001c44aae$3f182b50$7433d882@stat.auckland.ac.nz>
	<1086407421.12223.283.camel@localhost.localdomain>
	<40C48042.9050300@bankofcanada.ca>
	<1086633323.2235.217.camel@localhost.localdomain>
Message-ID: <001e01c44f35$c2de3520$6633d882@stat.auckland.ac.nz>

Thank you very much to those who contributed to this rather interesting
discussion/debate.  I was very surprised (and almost overwhelmed) by the
volume of replies based on this topic!

I have prepared some slides and put the draft version on
www.stat.auckland.ac.nz/~kwan022/tmp/RFin.ppt.  Feel free to download a copy
if you are interested in what I am planning to use -- of course, any more
suggestions are welcome but remember, it will only be a 1 ~ 2 minute talk on
R *_*

Cheers,

Kevin



From bates at stat.wisc.edu  Fri Jun 11 00:12:54 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 10 Jun 2004 17:12:54 -0500
Subject: [R] GLMM(..., family=binomial(link="cloglog"))?
In-Reply-To: <20040609133830.GA24700@stat.umu.se>
References: <3A822319EB35174CA3714066D590DCD504AF7DFB@usrymx25.merck.com>
	<40C4B87B.7060301@pdf.com> <6r8yez88wo.fsf@bates4.stat.wisc.edu>
	<40C5009A.90705@pdf.com> <6racze9jtd.fsf@bates4.stat.wisc.edu>
	<40C5DC08.70806@pdf.com> <20040609133830.GA24700@stat.umu.se>
Message-ID: <6ry8mvhxjt.fsf@bates4.stat.wisc.edu>

G??ran Brostr??m <gb at stat.umu.se> writes:

> I think the "weights=n" is the problem, i.e., you summarize Bernoulli
> data to Bin(100, p) data, and that gives a completely different estimate of
> the variance of the random effect. (This looks as an error in lme4 to me,
> or am I missing something? Doug?) Really, the two ways of representing data
> should give equivalent analyses, but it doesn't. The same phenomenon
> appears in 'glm', i.e. without random effects, but only for the residual
> sum of squares, df, and AIC. 

It is indeed an error in lme4.  Thanks for picking that up, Goran.
Fixing it is now on the "To Do" list.



From rxg218 at psu.edu  Fri Jun 11 00:38:17 2004
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Thu, 10 Jun 2004 18:38:17 -0400
Subject: [R] a scope problem
Message-ID: <1086907097.8888.8.camel@blue.chem.psu.edu>

Hi,
 I have some code that looks like:

    dftc <- df[sets$tcset,]
    pt <- numeric(nrow(dftc))
    sub <- 1:nrow(dftc)
    for (i in 1:nrow(dftc)) {
        n <- nnet( fmla, data=dftc, weights=wts, subset=sub[-i], size=4,
decay=0.01)
        pt[i] <- predict( n, dftc[ i, ], type='class' )
    }

However running this give me the error:

Error in eval(expr, envir, enclos) : Object "i" not found

I have noted this problem in some other instances. For example if I
define a function

f <- function( dat, sets ) {
  # use sets
}

I sometimes get an error similar to that above. 

Does anybody know why this would happen?

(R 1.9.0 on Fedora Core 2)
-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
All laws are simulations of reality.
-- John C. Lilly



From Henrik.Bengtsson at matstat.lu.se  Fri Jun 11 01:03:51 2004
From: Henrik.Bengtsson at matstat.lu.se (Henrik Bengtsson)
Date: Fri, 11 Jun 2004 01:03:51 +0200
Subject: [R] tryCatch() and preventing interrupts in 'finally'
Message-ID: <000d01c44f3f$331c6790$0701a8c0@hblaptop>

With tryCatch() it is possible to catch interrupts with tryCatch(). Then you
can use a 'finally' statement to clean up, release resources etc. However,
how can I "protect" against additional interrupts? This is a concern when
the hold down Ctrl+C and generates a sequence of interrupts. Example:

tryCatch({
  cat("Press Ctrl+C...\n");
  Sys.sleep(5);
}, interrupt = function(interrupt) {
  # ...and it will get caught
  cat("Caugh an interrupt:\n");
  print(interrupt);
}, finally = {
  cat("Trying to finalize. Press Ctrl+C...\n");
  Sys.sleep(5);
  cat("...and this line will not be evaluated.\n");
})

Additional tryCatch() inside 'finally' will have the same problem and so.
One idea I had was to wrap up the 'finally' statement in a
withCallingHandlers(, interrupt=...), but it seems that
withCallingHandlers() does not catch interrupts. For example, I tried to
replace 'tryCatch' with 'withCallingHandlers' in the above code.

So, what I basically need is a way to turn off interrupts during the
evaluation of 'finally' or a way for it to restart at the last interrupted
point. Is this possible?

Thanks (again)!

Henrik Bengtsson

Dept. of Mathematical Statistics @ Centre for Mathematical Sciences 
Lund Institute of Technology/Lund University, Sweden (+2h UTC)
+46 46 2229611 (off), +46 708 909208 (cell), +46 46 2224623 (fax)
h b @ m a t h s . l t h . s e, http://www.maths.lth.se/~hb/



From tplate at blackmesacapital.com  Fri Jun 11 02:40:10 2004
From: tplate at blackmesacapital.com (Tony Plate)
Date: Thu, 10 Jun 2004 18:40:10 -0600
Subject: [R] a scope problem
In-Reply-To: <1086907097.8888.8.camel@blue.chem.psu.edu>
References: <1086907097.8888.8.camel@blue.chem.psu.edu>
Message-ID: <6.1.0.6.2.20040610183807.044ceac8@mailhost.blackmesacapital.com>

This looks like it probably is a scope problem with non-standard evaluation 
rules for the argument subset= of nnet.

Instead of subset=sub[-i], try data=dftc[-i,]  (I've not tested this since 
I don't have the data objects you used.)

hope this helps,

Tony Plate

At Thursday 04:38 PM 6/10/2004, you wrote:
>Hi,
>  I have some code that looks like:
>
>     dftc <- df[sets$tcset,]
>     pt <- numeric(nrow(dftc))
>     sub <- 1:nrow(dftc)
>     for (i in 1:nrow(dftc)) {
>         n <- nnet( fmla, data=dftc, weights=wts, subset=sub[-i], size=4,
>decay=0.01)
>         pt[i] <- predict( n, dftc[ i, ], type='class' )
>     }
>
>However running this give me the error:
>
>Error in eval(expr, envir, enclos) : Object "i" not found
>
>I have noted this problem in some other instances. For example if I
>define a function
>
>f <- function( dat, sets ) {
>   # use sets
>}
>
>I sometimes get an error similar to that above.
>
>Does anybody know why this would happen?
>
>(R 1.9.0 on Fedora Core 2)
>-------------------------------------------------------------------
>Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
>GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
>-------------------------------------------------------------------
>All laws are simulations of reality.
>-- John C. Lilly
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From tiago17 at socrates.Berkeley.EDU  Fri Jun 11 02:49:24 2004
From: tiago17 at socrates.Berkeley.EDU (Tiago R Magalhaes)
Date: Thu, 10 Jun 2004 17:49:24 -0700 (PDT)
Subject: [R] running R UNIX in a mac computer
Message-ID: <Pine.SOL.4.56.0406101737450.22513@socrates.Berkeley.EDU>

Hi to you all

My question is:

there is a package written in UNIX for which there is no Mac version.

I would like to know if it's possible to install the R UNIX version on the
MacOSX and run that UNIX package on my Mac (through this UNIX R Vresion on
a Mac)

I have seen a porfile for r version 1.8.1 on darwin:
http://r.darwinports.com/
is that it?

aother question related to that
if it's possible to use UNIX R in Mac, does anyone know how fast or how
slow that is?

thanks for any help!



From umalvarez at fata.unam.mx  Fri Jun 11 03:10:30 2004
From: umalvarez at fata.unam.mx (Ulises Mora Alvarez)
Date: Thu, 10 Jun 2004 20:10:30 -0500 (CDT)
Subject: [R] running R UNIX in a mac computer
In-Reply-To: <Pine.SOL.4.56.0406101737450.22513@socrates.Berkeley.EDU>
Message-ID: <Pine.LNX.4.44.0406102006330.14227-100000@athena.fata.unam.mx>

Hi:

Please tell us: which Mac OS are you using?

If you are using any of the 10.x, then you should download R-1.9.0 at:
http://cran.us.r-project.org/bin/macosx

If you have anything older (MacOS 9 or lower), then you should visit:
http://cran.us.r-project.org/bin/macos

Regards...

On Thu, 10 Jun 2004, Tiago R Magalhaes wrote:

> Hi to you all
> 
> My question is:
> 
> there is a package written in UNIX for which there is no Mac version.
> 
> I would like to know if it's possible to install the R UNIX version on the
> MacOSX and run that UNIX package on my Mac (through this UNIX R Vresion on
> a Mac)
> 
> I have seen a porfile for r version 1.8.1 on darwin:
> http://r.darwinports.com/
> is that it?
> 
> aother question related to that
> if it's possible to use UNIX R in Mac, does anyone know how fast or how
> slow that is?
> 
> thanks for any help!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Ulises M. Alvarez
LAB. DE ONDAS DE CHOQUE
FISICA APLICADA Y TECNOLOGIA AVANZADA
UNAM
umalvarez at fata.unam.mx



From luke at stat.uiowa.edu  Fri Jun 11 03:52:28 2004
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Thu, 10 Jun 2004 20:52:28 -0500 (CDT)
Subject: [R] tryCatch() and preventing interrupts in 'finally'
In-Reply-To: <000d01c44f3f$331c6790$0701a8c0@hblaptop>
Message-ID: <Pine.LNX.4.44.0406102038120.12506-100000@itasca2.stat.uiowa.edu>

More complete support for interrupt management is still on the to do
list, unfortunately.  onexit actions have similar issues (and have for
a long time).  What I was hoping to do is provide a user level
mechanism for turning off interrupts around a piece of code and then
optionally enabling them within the code.  It may make sense to adopt
that behavior for tryCatch as the detault, and maybe also for
functions with onexit actions.  There is a risk of deadlock if the
cleanup code goes into an infinite loop, but this may be better than
the current behavior of allowing interrutps in cleanup code.  There
are some nasty little issues involved in getting this right, which is
why it hasn't happened yet.

Best,

luke

On Fri, 11 Jun 2004, Henrik Bengtsson wrote:

> With tryCatch() it is possible to catch interrupts with tryCatch(). Then you
> can use a 'finally' statement to clean up, release resources etc. However,
> how can I "protect" against additional interrupts? This is a concern when
> the hold down Ctrl+C and generates a sequence of interrupts. Example:
> 
> tryCatch({
>   cat("Press Ctrl+C...\n");
>   Sys.sleep(5);
> }, interrupt = function(interrupt) {
>   # ...and it will get caught
>   cat("Caugh an interrupt:\n");
>   print(interrupt);
> }, finally = {
>   cat("Trying to finalize. Press Ctrl+C...\n");
>   Sys.sleep(5);
>   cat("...and this line will not be evaluated.\n");
> })
> 
> Additional tryCatch() inside 'finally' will have the same problem and so.
> One idea I had was to wrap up the 'finally' statement in a
> withCallingHandlers(, interrupt=...), but it seems that
> withCallingHandlers() does not catch interrupts. For example, I tried to
> replace 'tryCatch' with 'withCallingHandlers' in the above code.
> 
> So, what I basically need is a way to turn off interrupts during the
> evaluation of 'finally' or a way for it to restart at the last interrupted
> point. Is this possible?
> 
> Thanks (again)!
> 
> Henrik Bengtsson
> 
> Dept. of Mathematical Statistics @ Centre for Mathematical Sciences 
> Lund Institute of Technology/Lund University, Sweden (+2h UTC)
> +46 46 2229611 (off), +46 708 909208 (cell), +46 46 2224623 (fax)
> h b @ m a t h s . l t h . s e, http://www.maths.lth.se/~hb/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Luke Tierney
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
   Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu



From tiago17 at socrates.Berkeley.EDU  Fri Jun 11 04:32:25 2004
From: tiago17 at socrates.Berkeley.EDU (Tiago R Magalhaes)
Date: Thu, 10 Jun 2004 19:32:25 -0700 (PDT)
Subject: [R] running R UNIX in a mac computer
In-Reply-To: <Pine.LNX.4.44.0406102006330.14227-100000@athena.fata.unam.mx>
References: <Pine.LNX.4.44.0406102006330.14227-100000@athena.fata.unam.mx>
Message-ID: <Pine.SOL.4.56.0406101926450.15681@socrates.Berkeley.EDU>


Hola ulisses

thank you very much for replying to my posting

I am using the last Mac Version:
OS X 10.3 Panther

I use R in the Mac and it works fine
The thing is that I want to use a package that was made in UNIX and I
wanted to know how easy it would be to run the UNIX version of R in the
Mac so that I can use this package for UNIX...

shouldn't you be watching soccer? some of my UNAM friends are nuts for a
couple of days already!


On Thu, 10 Jun 2004, Ulises Mora Alvarez wrote:

> Hi:
>
> Please tell us: which Mac OS are you using?
>
> If you are using any of the 10.x, then you should download R-1.9.0 at:
> http://cran.us.r-project.org/bin/macosx
>
> If you have anything older (MacOS 9 or lower), then you should visit:
> http://cran.us.r-project.org/bin/macos
>
> Regards...
>
> On Thu, 10 Jun 2004, Tiago R Magalhaes wrote:
>
> > Hi to you all
> >
> > My question is:
> >
> > there is a package written in UNIX for which there is no Mac version.
> >
> > I would like to know if it's possible to install the R UNIX version on the
> > MacOSX and run that UNIX package on my Mac (through this UNIX R Vresion on
> > a Mac)
> >
> > I have seen a porfile for r version 1.8.1 on darwin:
> > http://r.darwinports.com/
> > is that it?
> >
> > aother question related to that
> > if it's possible to use UNIX R in Mac, does anyone know how fast or how
> > slow that is?
> >
> > thanks for any help!
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>
> --
> Ulises M. Alvarez
> LAB. DE ONDAS DE CHOQUE
> FISICA APLICADA Y TECNOLOGIA AVANZADA
> UNAM
> umalvarez at fata.unam.mx
>
>
>



From umalvarez at fata.unam.mx  Fri Jun 11 06:29:51 2004
From: umalvarez at fata.unam.mx (Ulises Mora Alvarez)
Date: Thu, 10 Jun 2004 23:29:51 -0500 (CDT)
Subject: [R] running R UNIX in a mac computer
In-Reply-To: <Pine.SOL.4.56.0406101926450.15681@socrates.Berkeley.EDU>
Message-ID: <Pine.LNX.4.44.0406102308220.14596-100000@athena.fata.unam.mx>

Hola!

If you are already using Panther with  R.dmg, then is pretty straight 
forward to use a "Unix" package (panther it's a kind of Unix). If the 
package is already on CRAN, all you have to do is to search the package 
(either from the command line or from  the menus) and then install. If 
what you are looking for isn't not on CRAN, but it's available as a *.tgz 
or tar.gz or as a collection of files, you can use "Install from local 
files" in the "Packages" menu.  

Good look.


PS. Yes, I should be watching soccer (I'm getting my degree at the UNAM), 
but I decided to sacrifice the first half... After all, the final match 
will be on Sunday and the meeting with my committee is close, very 
close... Best regards.


On Thu, 10 Jun 2004, Tiago R Magalhaes wrote:

> 
> Hola ulisses
> 
> thank you very much for replying to my posting
> 
> I am using the last Mac Version:
> OS X 10.3 Panther
> 
> I use R in the Mac and it works fine
> The thing is that I want to use a package that was made in UNIX and I
> wanted to know how easy it would be to run the UNIX version of R in the
> Mac so that I can use this package for UNIX...
> 
> shouldn't you be watching soccer? some of my UNAM friends are nuts for a
> couple of days already!
> 
> 
> On Thu, 10 Jun 2004, Ulises Mora Alvarez wrote:
> 
> > Hi:
> >
> > Please tell us: which Mac OS are you using?
> >
> > If you are using any of the 10.x, then you should download R-1.9.0 at:
> > http://cran.us.r-project.org/bin/macosx
> >
> > If you have anything older (MacOS 9 or lower), then you should visit:
> > http://cran.us.r-project.org/bin/macos
> >
> > Regards...
> >
> > On Thu, 10 Jun 2004, Tiago R Magalhaes wrote:
> >
> > > Hi to you all
> > >
> > > My question is:
> > >
> > > there is a package written in UNIX for which there is no Mac version.
> > >
> > > I would like to know if it's possible to install the R UNIX version on the
> > > MacOSX and run that UNIX package on my Mac (through this UNIX R Vresion on
> > > a Mac)
> > >
> > > I have seen a porfile for r version 1.8.1 on darwin:
> > > http://r.darwinports.com/
> > > is that it?
> > >
> > > aother question related to that
> > > if it's possible to use UNIX R in Mac, does anyone know how fast or how
> > > slow that is?
> > >
> > > thanks for any help!
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> > >
> >
> > --
> > Ulises M. Alvarez
> > LAB. DE ONDAS DE CHOQUE
> > FISICA APLICADA Y TECNOLOGIA AVANZADA
> > UNAM
> > umalvarez at fata.unam.mx
> >
> >
> >
> 

-- 
Ulises M. Alvarez
LAB. DE ONDAS DE CHOQUE
FISICA APLICADA Y TECNOLOGIA AVANZADA
UNAM
umalvarez at fata.unam.mx



From wolfram at fischer-zim.ch  Fri Jun 11 09:17:39 2004
From: wolfram at fischer-zim.ch (Wolfram Fischer)
Date: Fri, 11 Jun 2004 09:17:39 +0200
Subject: [R] lattice: cumsum and xyplot
Message-ID: <20040611071739.GA2771@s1x.local>

I want to display cumulative summary functions with lattice.

First I tried to get cumulated data:
    library(lattice)
    data(barley)

    d.cum <- with( barley, by( yield, INDICES=list(site=site,year=year), FUN=cumsum ) )

I got a list of vectors.
I tried to get a dataframe which I could use in xyplot.
But neither of the following functions led to the goal:

    d.cum.df1 <- as.data.frame.list( d.cum )
    d.cum.df2 <- as.data.frame.array( d.cum )


Then I tried to solve my problem within the panel function.
But now I had to set a value for ylim.

    test.xyplot <- function( data=barley, yr=1931, ymax=600, type='l', ... ){
        print( xyplot( data=data, subset=year==yr
            , type=type
            , panel=function( x, y, ... ){
                panel.xyplot( x, cumsum(y), ... )
            }
            , ylim=c( 0, ymax )
            , yield ~ variety | site
            , scales=list( x=list( alternating=1, rot=90 ) )
            , ...
            ))
    }

What could I do to get a dataframe containing the cumulative values
of ``yield'' which I could use to get the cumulative summary plots?

Thanks - Wolfram



From wolski at molgen.mpg.de  Fri Jun 11 09:33:58 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Fri, 11 Jun 2004 09:33:58 +0200
Subject: [R] lattice: cumsum and xyplot
In-Reply-To: <20040611071739.GA2771@s1x.local>
References: <20040611071739.GA2771@s1x.local>
Message-ID: <200406110933580777.059BF67B@mail.math.fu-berlin.de>

Hi!
To get a data.frame

as.data.frame(do.call("rbind",d.cum))

Sincerely 
Eryk

*********** REPLY SEPARATOR  ***********

On 6/11/2004 at 9:17 AM Wolfram Fischer wrote:

>>>I want to display cumulative summary functions with lattice.
>>>
>>>First I tried to get cumulated data:
>>>    library(lattice)
>>>    data(barley)
>>>
>>>    d.cum <- with( barley, by( yield, INDICES=list(site=site,year=year),
>>>FUN=cumsum ) )
>>>
>>>I got a list of vectors.
>>>I tried to get a dataframe which I could use in xyplot.
>>>But neither of the following functions led to the goal:
>>>
>>>    d.cum.df1 <- as.data.frame.list( d.cum )
>>>    d.cum.df2 <- as.data.frame.array( d.cum )
>>>
>>>
>>>Then I tried to solve my problem within the panel function.
>>>But now I had to set a value for ylim.
>>>
>>>    test.xyplot <- function( data=barley, yr=1931, ymax=600, type='l',
>>>... ){
>>>        print( xyplot( data=data, subset=year==yr
>>>            , type=type
>>>            , panel=function( x, y, ... ){
>>>                panel.xyplot( x, cumsum(y), ... )
>>>            }
>>>            , ylim=c( 0, ymax )
>>>            , yield ~ variety | site
>>>            , scales=list( x=list( alternating=1, rot=90 ) )
>>>            , ...
>>>            ))
>>>    }
>>>
>>>What could I do to get a dataframe containing the cumulative values
>>>of ``yield'' which I could use to get the cumulative summary plots?
>>>
>>>Thanks - Wolfram
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



Dipl. bio-chem. Eryk Witold Wolski    @    MPI-Moleculare Genetic   
Ihnestrasse 63-73 14195 Berlin       'v'    
tel: 0049-30-83875219               /   \    
mail: wolski at molgen.mpg.de        ---W-W----    http://www.molgen.mpg.de/~wolski



From rksh at soc.soton.ac.uk  Fri Jun 11 10:09:19 2004
From: rksh at soc.soton.ac.uk (Robin Hankin)
Date: Fri, 11 Jun 2004 09:09:19 +0100
Subject: [R] rownames of single row matrices
Message-ID: <a06002005bcef17fef7d5@[139.166.242.29]>

Hi

I want to extract rows of a matrix, and preserve rownames even if only
one row is selected.  Toy example:

R> a <- matrix(1:9,3,3)
R> rownames(a) <- letters[1:3]
R> colnames(a) <- LETTERS[1:3]
R> a
   A B C
a 1 4 7
b 2 5 8
c 3 6 9


Extract the first two rows:
R> wanted <- 1:2
R> a[wanted,]
   A B C
a 1 4 7
b 2 5 8

rownames come through fine.  Now extract just
one row:

R> a[1,]
A B C
1 4 7


rowname is lost!  (also note that this isn't a 1-by-n matrix as 
needed.  This object
is a vector). How do I get the rowname back?
My best effort is:

extract <- function(a,wanted){
   if(length(wanted)>1) {
      return(a[wanted,])
   } else {
      out <- t(as.matrix(a[wanted,]))
      rownames(out) <- rownames(a)[wanted]
      return(out)
   }
}

[note the transpose and as.matrix()].  There must be a better way!
Anyone got any better ideas?

What is the R rationale for treating a[1,] so differently from a[1:2,]  ?


-- 
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
SO14 3ZH
tel +44(0)23-8059-7743
initialDOTsurname at soc.soton.ac.uk (edit in obvious way; spam precaution)



From dimitris.rizopoulos at med.kuleuven.ac.be  Fri Jun 11 10:14:43 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Fri, 11 Jun 2004 10:14:43 +0200
Subject: [R] rownames of single row matrices
References: <a06002005bcef17fef7d5@[139.166.242.29]>
Message-ID: <004401c44f8c$2736b910$ad133a86@www.domain>

Dear Robin,

I think you just need:

a[1,,drop=F]

Best,
Dimitris

----
Dimitris Rizopoulos
Doctoral Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "Robin Hankin" <rksh at soc.soton.ac.uk>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, June 11, 2004 10:09 AM
Subject: [R] rownames of single row matrices


> Hi
>
> I want to extract rows of a matrix, and preserve rownames even if
only
> one row is selected.  Toy example:
>
> R> a <- matrix(1:9,3,3)
> R> rownames(a) <- letters[1:3]
> R> colnames(a) <- LETTERS[1:3]
> R> a
>    A B C
> a 1 4 7
> b 2 5 8
> c 3 6 9
>
>
> Extract the first two rows:
> R> wanted <- 1:2
> R> a[wanted,]
>    A B C
> a 1 4 7
> b 2 5 8
>
> rownames come through fine.  Now extract just
> one row:
>
> R> a[1,]
> A B C
> 1 4 7
>
>
> rowname is lost!  (also note that this isn't a 1-by-n matrix as
> needed.  This object
> is a vector). How do I get the rowname back?
> My best effort is:
>
> extract <- function(a,wanted){
>    if(length(wanted)>1) {
>       return(a[wanted,])
>    } else {
>       out <- t(as.matrix(a[wanted,]))
>       rownames(out) <- rownames(a)[wanted]
>       return(out)
>    }
> }
>
> [note the transpose and as.matrix()].  There must be a better way!
> Anyone got any better ideas?
>
> What is the R rationale for treating a[1,] so differently from
a[1:2,]  ?
>
>
> -- 
> Robin Hankin
> Uncertainty Analyst
> Southampton Oceanography Centre
> SO14 3ZH
> tel +44(0)23-8059-7743
> initialDOTsurname at soc.soton.ac.uk (edit in obvious way; spam
precaution)
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From lecoutre at stat.ucl.ac.be  Fri Jun 11 10:15:28 2004
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Fri, 11 Jun 2004 10:15:28 +0200
Subject: [R] rownames of single row matrices
In-Reply-To: <a06002005bcef17fef7d5@[139.166.242.29]>
References: <a06002005bcef17fef7d5@[139.166.242.29]>
Message-ID: <6.0.1.1.2.20040611101334.021dcec0@stat4ux.stat.ucl.ac.be>



Hi Robin

Have a look at:

 > help("[")

The fact that dimensions are lost when extracting is a feature of the language.

What you need is the "drop" option.

 > a[1,,drop=FALSE]
   A B C
a 1 4 7

Eric

At 10:09 11/06/2004, Robin Hankin wrote:
>Hi
>
>I want to extract rows of a matrix, and preserve rownames even if only
>one row is selected.  Toy example:
>
>R> a <- matrix(1:9,3,3)
>R> rownames(a) <- letters[1:3]
>R> colnames(a) <- LETTERS[1:3]
>R> a
>   A B C
>a 1 4 7
>b 2 5 8
>c 3 6 9
>
>
>Extract the first two rows:
>R> wanted <- 1:2
>R> a[wanted,]
>   A B C
>a 1 4 7
>b 2 5 8
>
>rownames come through fine.  Now extract just
>one row:
>
>R> a[1,]
>A B C
>1 4 7
>
>
>rowname is lost!  (also note that this isn't a 1-by-n matrix as 
>needed.  This object
>is a vector). How do I get the rowname back?
>My best effort is:
>
>extract <- function(a,wanted){
>   if(length(wanted)>1) {
>      return(a[wanted,])
>   } else {
>      out <- t(as.matrix(a[wanted,]))
>      rownames(out) <- rownames(a)[wanted]
>      return(out)
>   }
>}
>
>[note the transpose and as.matrix()].  There must be a better way!
>Anyone got any better ideas?
>
>What is the R rationale for treating a[1,] so differently from a[1:2,]  ?
>
>
>--
>Robin Hankin
>Uncertainty Analyst
>Southampton Oceanography Centre
>SO14 3ZH
>tel +44(0)23-8059-7743
>initialDOTsurname at soc.soton.ac.uk (edit in obvious way; spam precaution)
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From jasont at indigoindustrial.co.nz  Fri Jun 11 10:18:14 2004
From: jasont at indigoindustrial.co.nz (Jason Turner)
Date: Fri, 11 Jun 2004 20:18:14 +1200
Subject: [R] rownames of single row matrices
In-Reply-To: <a06002005bcef17fef7d5@[139.166.242.29]>
References: <a06002005bcef17fef7d5@[139.166.242.29]>
Message-ID: <1086941893.5416.8.camel@linux.site>

On Fri, 2004-06-11 at 20:09, Robin Hankin wrote:
> Hi
> 
> I want to extract rows of a matrix, and preserve rownames even if only
> one row is selected.  

The infamous drop=TRUE default.  help("[") goes into this.

Your toy example, two ways:

> a <- matrix(1:9,3,3)
> rownames(a) <- letters[1:3]
> colnames(a) <- letters[1:3]
> a[1,,drop=FALSE]
  a b c
a 1 4 7
> a[1,,drop=TRUE]
a b c 
1 4 7 

Does that help?

Cheers

Jason



From david.lennartsson at saida-med.com  Fri Jun 11 10:27:51 2004
From: david.lennartsson at saida-med.com (David Lennartsson)
Date: Fri, 11 Jun 2004 10:27:51 +0200
Subject: [R] Questions about Preserving registers
In-Reply-To: <lhahc0pouc4ir465ds1ohrv1vphq2sv57g@4ax.com>
References: <000501c44f02$6588a780$f63d9f88@math.ucalgary.ca>	<Pine.A41.4.58.0406100910240.114950@homer09.u.washington.edu>
	<lhahc0pouc4ir465ds1ohrv1vphq2sv57g@4ax.com>
Message-ID: <40C96D07.2060605@saida-med.com>

Duncan Murdoch wrote:

>On Thu, 10 Jun 2004 09:12:05 -0700 (PDT), Thomas Lumley
><tlumley at u.washington.edu> wrote :
>
>  
>
>>>DLL attempted to change FPU control word from 9001f to 90003
>>>
>>>I read the instruction on Duncan Murdochs website about preserving
>>>registers, but I still dont understand it. For example,
>>>      
>>>
>>This code is for Delphi.  You would need to see how to set the FPU control
>>work in Powerstation Fortran, which should be described in its manual.
>>
>>The warning is harmless, so you could just ignore it.  The warning means
>>that R has done the change for you.
>>    
>>
>
>I think it's better to try to fix it.  R only checks once (on
>loading); if Fortran makes the change again, then R code might go
>wrong later, and if Fortran depends on the original value, then the
>Fortran code might not work properly.
>
>Duncan Murdoch
>  
>
Hi,

yes, I have seen this happen when loadng the precompiled yacas dll into 
R (package to be released). I have not
bothered to dig deeper and I have no idea of how to get and set the FPU 
control word in the wrapper. Is there
a compiler macro or does it have to be done with assembler?

David



From Matthias.Kohl at uni-bayreuth.de  Fri Jun 11 11:30:52 2004
From: Matthias.Kohl at uni-bayreuth.de (Matthias Kohl)
Date: Fri, 11 Jun 2004 10:30:52 +0100
Subject: [R] setValidity() changes "Extends"?
Message-ID: <40C97BCC.3050902@uni-bayreuth.de>

Hi,

I'm using Version 1.9.0  (2004-04-12)  on Windows NT/98/2000 and found 
the following difference between using setClass(..., valdity = ), 
respectively using setValidity() afterwards:

setValidity() changes the "Extends"-part of a derived class, is this 
intended or a bug or am I missing something?

##########################################
## Example code
##########################################
setClass("Class1", representation("name" = "character"))

if(!isGeneric("name")) setGeneric("name", function(object) 
standardGeneric("name"))
setMethod("name", "Class1", function(object) object at name)

setClass("Class2", representation("Class1"))
setClass("Class3", representation("Class2"))

getClass("Class3") # as I expected

#Slots:
#              
#Name:       name
#Class: character
#
#Extends:
#Class "Class2", directly
#Class "Class1", by class "Class2"


validClass3 <- function(object){TRUE}
setValidity("Class3", validClass3)

#Slots:
#             
#Name:       name
#Class: character
#
#Extends: "Class2"         # has been changed???

getClass("Class3")  # why does setValidity change "Extends"?
                              # am I missing something?
                              # This doesn't happen if I use
                              # setClass(..., validity = )
                              # It, of course, also works if I 
explicitly use
                              # setClass("Class3", contains = 
c("Class2", "Class1")

C32 <- new("Class3")
name(C32) # generates an error?!

#Error in name(C32) : No direct or inherited method for function "name" 
for this call

is(C32, "Class1") # however
#TRUE



From rksh at soc.soton.ac.uk  Fri Jun 11 10:39:13 2004
From: rksh at soc.soton.ac.uk (Robin Hankin)
Date: Fri, 11 Jun 2004 09:39:13 +0100
Subject: [R] rownames of single row matrices
In-Reply-To: <a06002005bcef17fef7d5@[139.166.242.29]>
References: <a06002005bcef17fef7d5@[139.166.242.29]>
Message-ID: <a06002008bcef1d252cfa@[139.166.242.29]>

Go list!

The examples on  help("[") were all I needed.  The examples don't explicitly
say that rownames are preserved; perhaps this is obvious.

  It'd be nice if matrix "m" on the help page had rownames.


best wishes

rksh

-- 
Robin Hankin
Uncertainty Analyst
Southampton Oceanography Centre
SO14 3ZH
tel +44(0)23-8059-7743
initialDOTsurname at soc.soton.ac.uk (edit in obvious way; spam precaution)



From jarioksa at sun3.oulu.fi  Fri Jun 11 10:45:05 2004
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: 11 Jun 2004 11:45:05 +0300
Subject: [R] running R UNIX in a mac computer
In-Reply-To: <Pine.SOL.4.56.0406101737450.22513@socrates.Berkeley.EDU>
References: <Pine.SOL.4.56.0406101737450.22513@socrates.Berkeley.EDU>
Message-ID: <1086943505.7797.27.camel@biol102145.oulu.fi>

On Fri, 2004-06-11 at 03:49, Tiago R Magalhaes wrote:
> Hi to you all
> 
> My question is:
> 
> there is a package written in UNIX for which there is no Mac version.
> 
> I would like to know if it's possible to install the R UNIX version on the
> MacOSX and run that UNIX package on my Mac (through this UNIX R Vresion on
> a Mac)
> 
> I have seen a porfile for r version 1.8.1 on darwin:
> http://r.darwinports.com/
> is that it?
> 
> aother question related to that
> if it's possible to use UNIX R in Mac, does anyone know how fast or how
> slow that is?

Tiago,

If it is a CRAN package *without* MacOS version, there obviously is a
reason for this handicap, and you cannot run the package. If it is a
stray package, its developer probably just doesn't have opportunity or
will to build a Mac binary, but you can build it yourself if you're
lucky. Check the FAQ and ReadMe files with your R/Aqua version to see
what you need. With little trouble you can easily use source packages
with your Mac R. Many tools are already installed in your OS (perl at
least). If the package has only R files, you may be able to install a
source package directly. If it has C source code, you should first
install MacOS X Developer tools (XCode): it comes with your OS
installation CD/DVD, but it is not installed by default. If the package
has Fortran source code, you got to find external Fortran compiler:
MacOS X ships with C compiler, but without Fortran compiler. See the Mac
R FAQ for the best alternatives to find the compiler (this FAQ is
installed with your R).

Installing a Darwin R orobably won't help you. It needs and uses exactly
the same tools to build the packages as R/Aqua. If you can't install a
source package in R/Aqua, you cannot install it in R/Darwin, and vice
versa. The toolset is the decisive part, not the R shell. I assume that
both versions of R are just as fast (or slow). R/Aqua uses highly
optimized BLAS for numeric functions, and if R/Darwin uses the same
library, it is just as fast. If it doesn't use optimized BLAS, it will
be clearly slower. 

I have installed Linux in Mac, but I found out that R was clearly (20%)
slower in Linux than in MacOS in the very same piece of hardware. The
main reason seemed to be that Linux R didn't have optimized BLAS because
the largest differences were in functions calling svd and qr (I used
YellowDog Linux) -- the Linux version took 150%(!) longer to run the
same svd-heavy test code. Another reason seemed to be that the Fortran
compiler produces much slower code in Linux than in MacOS X (difference
about 20%).

cheers, jari oksanen
-- 
Jari Oksanen <jarioksa at sun3.oulu.fi>



From wolfram at fischer-zim.ch  Fri Jun 11 10:52:25 2004
From: wolfram at fischer-zim.ch (Wolfram Fischer)
Date: Fri, 11 Jun 2004 10:52:25 +0200
Subject: [R] lattice: cumsum and xyplot
In-Reply-To: <200406110933580777.059BF67B@mail.math.fu-berlin.de>
References: <20040611071739.GA2771@s1x.local>
	<200406110933580777.059BF67B@mail.math.fu-berlin.de>
Message-ID: <20040611085225.GA3162@s1x.local>

--- In reply to: ---
>Date:    11.06.04 09:33 (+0200)
>From:    Wolski <wolski at molgen.mpg.de>
>Subject: Re: [R] lattice: cumsum and xyplot
>
> Hi!
> To get a data.frame
> 
> as.data.frame(do.call("rbind",d.cum))

Thanks for this interesting hint.
To get the lost names of factors again,
I tried now the following solution:

data(barley)
d.cum <-
	with( barley, by( yield, INDICES=list(site=site,year=year), FUN=function(x)x ) )

test.bylist2dataframe <- function( bylist, col.names=NULL ){
	veqq <- as.data.frame( do.call( 'rbind', d.cum ) )
	if( ! is.null( col.names ) ) colnames( veqq ) <- col.names
	viqt.vars <- names( dimnames(bylist) )
	if( length( viqt.vars ) == 2 ){
		veqq[,viqt.vars[1]] <- as.factor(
			rep( dimnames( bylist )[[viqt.vars[1]]], times=dim(bylist)[2] ) )
		veqq[,viqt.vars[2]] <- as.factor(
			rep( dimnames( bylist )[[viqt.vars[2]]], each=dim(bylist)[1] ) )
	}
	veqq
}

d.cum.dfr <- test.bylist2dataframe( d.cum, col.names=unique( as.character( barley$variety ) ) )

But now I have the following problems:
- ``d.cum.dfr'' is not yet normalised.
- My solution works only for two ``INDICES''.

So, what to do? - Wolfram

> *********** REPLY SEPARATOR  ***********
> 
> On 6/11/2004 at 9:17 AM Wolfram Fischer wrote:
> 
> >>>I want to display cumulative summary functions with lattice.
> >>>
> >>>First I tried to get cumulated data:
> >>>    library(lattice)
> >>>    data(barley)
> >>>
> >>>    d.cum <- with( barley, by( yield, INDICES=list(site=site,year=year),
> >>>FUN=cumsum ) )
> >>>
> >>>I got a list of vectors.
> >>>I tried to get a dataframe which I could use in xyplot.
> >>>But neither of the following functions led to the goal:
> >>>
> >>>    d.cum.df1 <- as.data.frame.list( d.cum )
> >>>    d.cum.df2 <- as.data.frame.array( d.cum )
> >>>
> >>>
> >>>Then I tried to solve my problem within the panel function.
> >>>But now I had to set a value for ylim.
> >>>
> >>>    test.xyplot <- function( data=barley, yr=1931, ymax=600, type='l',
> >>>... ){
> >>>        print( xyplot( data=data, subset=year==yr
> >>>            , type=type
> >>>            , panel=function( x, y, ... ){
> >>>                panel.xyplot( x, cumsum(y), ... )
> >>>            }
> >>>            , ylim=c( 0, ymax )
> >>>            , yield ~ variety | site
> >>>            , scales=list( x=list( alternating=1, rot=90 ) )
> >>>            , ...
> >>>            ))
> >>>    }
> >>>
> >>>What could I do to get a dataframe containing the cumulative values
> >>>of ``yield'' which I could use to get the cumulative summary plots?
> >>>
> >>>Thanks - Wolfram



From Nathan.Weisz at uni-konstanz.de  Fri Jun 11 11:26:23 2004
From: Nathan.Weisz at uni-konstanz.de (Nathan Weisz)
Date: Fri, 11 Jun 2004 11:26:23 +0200
Subject: [R] comparing regression slopes
Message-ID: <67A18C43-BB89-11D8-98A3-000393597A3C@uni-konstanz.de>

Dear List,

I used rlm to calculate two regression models for two data sets (rlm 
due to two outlying values in one of the data sets). Now I want to 
compare the two regression slopes. I came across some R-code of Spencer 
Graves in reply to a similar problem:
http://www.mail-archive.com/r-help at stat.math.ethz.ch/msg06666.html

The code was:

 > df1 <- data.frame(x=1:10, y=1:10+rnorm(10)) #3observations in 
original code
 > df2 <- data.frame(x=1:10, y=1:10+rnorm(10))
 >
 > fit1 <- lm(y~x, df1)
 > s1 <- summary(fit1)$coefficients
 > fit2 <- lm(y~x, df2)
 > s2 <- summary(fit2)$coefficients
 >
 > db <- (s2[2,1]-s1[2,1])
 > sd <- sqrt(s2[2,2]^2+s1[2,2]^2)
 > df <- (fit1$df.residual+fit2$df.residual)
 > td <- db/sd
 > 2*pt(-abs(td), df)
[1] 0.9510506

However when I use rlm instead of lm I get NA for df.residual.
 > fit1 <- rlm(y~x, df1)
 > fit1$df.residual
[1] NA

Does this mean that I can not apply the code for values gained by rlm? 
In the example above I continued by taking  the df from:
 > summary(fit1)$df[2]
[1] 8

Is this o.k.?

Help greatly appreciated.

Best,
Nathan Weisz



From ripley at stats.ox.ac.uk  Fri Jun 11 11:45:56 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 11 Jun 2004 10:45:56 +0100 (BST)
Subject: [R] comparing regression slopes
In-Reply-To: <67A18C43-BB89-11D8-98A3-000393597A3C@uni-konstanz.de>
Message-ID: <Pine.LNX.4.44.0406111043190.29781-100000@gannet.stats>

It's not OK.  Treat the results from rlm as having infinite df since the
theory is asymptotic (use pnorm not pt), and don't expect anything useful
in samples as small as 10 cases (30+, preferably 100+ would be OK).

On Fri, 11 Jun 2004, Nathan Weisz wrote:

> Dear List,
> 
> I used rlm to calculate two regression models for two data sets (rlm 
> due to two outlying values in one of the data sets). Now I want to 
> compare the two regression slopes. I came across some R-code of Spencer 
> Graves in reply to a similar problem:
> http://www.mail-archive.com/r-help at stat.math.ethz.ch/msg06666.html
> 
> The code was:
> 
>  > df1 <- data.frame(x=1:10, y=1:10+rnorm(10)) #3observations in 
> original code
>  > df2 <- data.frame(x=1:10, y=1:10+rnorm(10))
>  >
>  > fit1 <- lm(y~x, df1)
>  > s1 <- summary(fit1)$coefficients
>  > fit2 <- lm(y~x, df2)
>  > s2 <- summary(fit2)$coefficients
>  >
>  > db <- (s2[2,1]-s1[2,1])
>  > sd <- sqrt(s2[2,2]^2+s1[2,2]^2)
>  > df <- (fit1$df.residual+fit2$df.residual)
>  > td <- db/sd
>  > 2*pt(-abs(td), df)
> [1] 0.9510506
> 
> However when I use rlm instead of lm I get NA for df.residual.
>  > fit1 <- rlm(y~x, df1)
>  > fit1$df.residual
> [1] NA
> 
> Does this mean that I can not apply the code for values gained by rlm? 
> In the example above I continued by taking  the df from:
>  > summary(fit1)$df[2]
> [1] 8
> 
> Is this o.k.?
> 
> Help greatly appreciated.
> 
> Best,
> Nathan Weisz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From spencer.graves at pdf.com  Fri Jun 11 12:14:48 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 11 Jun 2004 03:14:48 -0700
Subject: [R] comparing regression slopes
In-Reply-To: <Pine.LNX.4.44.0406111043190.29781-100000@gannet.stats>
References: <Pine.LNX.4.44.0406111043190.29781-100000@gannet.stats>
Message-ID: <40C98618.5020703@pdf.com>

      Fortunately, you don't have to accept "no" for an answer:  You can 
dream up something that you think is sensible, simulate scenarios you 
want to detect plus scenarios with no difference, and find out what your 
procedure produces, the distributions of your sample statistics, etc., 
e.g., as described by Venables and Ripley (2000) S Programming 
(Springer) to obtain confidence intervals for normal probability plots.  
R has many functions for pseudo-random number generation as well as 
packages for bootstrapping.  Only a few hours ago, I checked a 
theoretical computation using pseudo-random numbers. 

      hope this helps.  spencer graves

Prof Brian Ripley wrote:

>It's not OK.  Treat the results from rlm as having infinite df since the
>theory is asymptotic (use pnorm not pt), and don't expect anything useful
>in samples as small as 10 cases (30+, preferably 100+ would be OK).
>
>On Fri, 11 Jun 2004, Nathan Weisz wrote:
>
>  
>
>>Dear List,
>>
>>I used rlm to calculate two regression models for two data sets (rlm 
>>due to two outlying values in one of the data sets). Now I want to 
>>compare the two regression slopes. I came across some R-code of Spencer 
>>Graves in reply to a similar problem:
>>http://www.mail-archive.com/r-help at stat.math.ethz.ch/msg06666.html
>>
>>The code was:
>>
>> > df1 <- data.frame(x=1:10, y=1:10+rnorm(10)) #3observations in 
>>original code
>> > df2 <- data.frame(x=1:10, y=1:10+rnorm(10))
>> >
>> > fit1 <- lm(y~x, df1)
>> > s1 <- summary(fit1)$coefficients
>> > fit2 <- lm(y~x, df2)
>> > s2 <- summary(fit2)$coefficients
>> >
>> > db <- (s2[2,1]-s1[2,1])
>> > sd <- sqrt(s2[2,2]^2+s1[2,2]^2)
>> > df <- (fit1$df.residual+fit2$df.residual)
>> > td <- db/sd
>> > 2*pt(-abs(td), df)
>>[1] 0.9510506
>>
>>However when I use rlm instead of lm I get NA for df.residual.
>> > fit1 <- rlm(y~x, df1)
>> > fit1$df.residual
>>[1] NA
>>
>>Does this mean that I can not apply the code for values gained by rlm? 
>>In the example above I continued by taking  the df from:
>> > summary(fit1)$df[2]
>>[1] 8
>>
>>Is this o.k.?
>>
>>Help greatly appreciated.
>>
>>Best,
>>Nathan Weisz
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>
>>    
>>
>
>  
>



From monica.palaseanu-lovejoy at stud.man.ac.uk  Fri Jun 11 12:19:50 2004
From: monica.palaseanu-lovejoy at stud.man.ac.uk (Monica Palaseanu-Lovejoy)
Date: Fri, 11 Jun 2004 11:19:50 +0100
Subject: [R] Continuous-lag Markov chains?
Message-ID: <E1BYj8h-000ITp-I3@serenity.mcc.ac.uk>

Hi again,

I am wondering if there is any R package which does continuous-
lag Markov chains modelling? I discovered MCMCpack but i am not 
sure it does continuous-lags.

Thanks for your answers,

Monica
Manchester University



From hans.kestler at medizin.uni-ulm.de  Fri Jun 11 12:20:54 2004
From: hans.kestler at medizin.uni-ulm.de (Hans Kestler)
Date: Fri, 11 Jun 2004 12:20:54 +0200
Subject: [R] set user-agent for download
Message-ID: <40C98786.9E27815C@medizin.uni-ulm.de>

Hi,

does anybody know if it is possible to set a user-agent for download
using a proxy?

Thank you.

Hans Kestler

From yukangtu at hotmail.com  Fri Jun 11 12:51:16 2004
From: yukangtu at hotmail.com (Tu Yu-Kang)
Date: Fri, 11 Jun 2004 10:51:16 +0000
Subject: [R] how to install yags in R
Message-ID: <BAY12-F61dllUMiOUy600058e8b@hotmail.com>

Dear R users,

I search the R archives and noted that the same problem has been posted but 
without solution.

I know there IS instructions by the author of yags, but I just couldn't 
figured out.

I know gee and geepack can also perform generalized estimating equation, 
but the reason why I need yags is I want to perform a Small Sample 
Adjustments by Michael P. Fay and Barry I. Graubard in Biometrics, 2001, 
57: 1198-1206.

I'm working on some dental data (I'm a dentist) with repeat measurements on 
relatively small sample sizes (around 20 to 50 teeth), and GEE seems to be 
quite ineffient (i.e. less powerful) compared to other methods.

I wonder it is really due to the sample size is too small, and GEE should 
not be used?

Many thanks in advance.

best regards,

Yu-Kang





From ligges at statistik.uni-dortmund.de  Fri Jun 11 13:01:54 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 11 Jun 2004 13:01:54 +0200
Subject: [R] set user-agent for download
In-Reply-To: <40C98786.9E27815C@medizin.uni-ulm.de>
References: <40C98786.9E27815C@medizin.uni-ulm.de>
Message-ID: <40C99122.20104@statistik.uni-dortmund.de>

Hans Kestler wrote:
> Hi,
> 
> does anybody know if it is possible to set a user-agent for download
> using a proxy?

See ?download.file for details.

Uwe Ligges



> Thank you.
> 
> Hans Kestler
> 
> 
> ------------------------------------------------------------------------
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From P.Lemmens at nici.kun.nl  Fri Jun 11 13:17:12 2004
From: P.Lemmens at nici.kun.nl (Paul Lemmens)
Date: Fri, 11 Jun 2004 13:17:12 +0200
Subject: [R] Informal discussion group about R
In-Reply-To: <9C6522E7D1039748892843B70C845484AEBC48@mail2.bci.buttecounty.net>
References: <9C6522E7D1039748892843B70C845484AEBC48@mail2.bci.buttecounty.ne t>
Message-ID: <6149E495295ED49DFC51668A@lemmens.socsci.kun.nl>

Hoi Harold,

--On donderdag 10 juni 2004 7:55 -0700 "Baize, Harold" 
<HBaize at buttecounty.net> wrote:

>
> I've started a "tribe" for discussing R and
> sharing scripts. Tribe.net is one of the popular
>
> I hope it will be of help to newbies, although I'm
> new to R myself.
>
> Here is the url:
>
>  http://r-statisticalenvironment.tribe.net
>
Recently I discovered that there's also a community on Orkut called 
R-project <http://www.orkut.com/Community.aspx?cmm=11845>, with similar 
entry level discussions on problem solving with/in R. The same if holds: 
registration with Orkut obligatory. Perhaps these communities can/will 
function as the low-entry level help that was discussed around a year ago.

kind regards, Paul


-- 
Paul Lemmens
NICI, University of Nijmegen              ASCII Ribbon Campaign /"\
Montessorilaan 3 (B.01.05)                    Against HTML Mail \ /
NL-6525 HR Nijmegen                                              X
The Netherlands                                                 / \
Phonenumber    +31-24-3612648
Fax            +31-24-3616066



From chris.jackson at imperial.ac.uk  Fri Jun 11 13:21:19 2004
From: chris.jackson at imperial.ac.uk (Chris Jackson)
Date: Fri, 11 Jun 2004 12:21:19 +0100
Subject: [R] Continuous-lag Markov chains?
In-Reply-To: <E1BYj8h-000ITp-I3@serenity.mcc.ac.uk>
References: <E1BYj8h-000ITp-I3@serenity.mcc.ac.uk>
Message-ID: <40C995AF.4000701@imperial.ac.uk>

Monica Palaseanu-Lovejoy wrote:

>Hi again,
>
>I am wondering if there is any R package which does continuous-
>lag Markov chains modelling? I discovered MCMCpack but i am not 
>sure it does continuous-lags.
>
>  
>
MCMCpack fits Bayesian models using Markov Chain Monte Carlo
simulation, which is a very different thing to fitting Markov chain models
to data! 

You might be interested in my msm package on CRAN, which fits continuous
time Markov models to either fully-observed trajectories or 
interval-censored
data.

Chris

-- 
Christopher Jackson <chris.jackson at imperial.ac.uk>, Research Associate,
Department of Epidemiology and Public Health, Imperial College
School of Medicine, Norfolk Place, London W2 1PG, tel. 020 759 43371



From Emmanuel.Engelhart at hte-company.de  Fri Jun 11 14:31:17 2004
From: Emmanuel.Engelhart at hte-company.de (Emmanuel Engelhart)
Date: Fri, 11 Jun 2004 13:31:17 +0100
Subject: [R] [RSPerl] Loading problem
Message-ID: <E843F0337279D611A0B500306E00C185482E7E@hte-exchange.intra.hte-company.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040611/9b08325c/attachment.pl

From gavin.simpson at ucl.ac.uk  Fri Jun 11 14:07:18 2004
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Fri, 11 Jun 2004 13:07:18 +0100
Subject: [R] Sweave and multiple graphs
Message-ID: <40C9A076.2090209@ucl.ac.uk>

Dear list,

I am using Sweave to build a small report. I want to produce a series of 
figures, each figure containing a number of plots and then have them 
included in the Sweave file.

An example would be to :

postscript(file = "ANCbwplot%03d.eps", onefile = FALSE, other options...)
oldpar <- par(mfrow = c(2,2))
....
do lots of plots to produce a number of eps files
....
par(oldpar)
dev.off()

The example in the Sweave FAQ shows how to do something similar for 
cases where you know how many figures there are, but I do not know how 
many figures will be produced so want to produce a more generic solution.

I thought of doing the above code in Sweave, and because I named the 
plots in a unique way, I now want to read all the files in the current 
directory that match "ANCbwplot001.eps" or "ANCbwplot002.eps" or 
"ANCbwplot003.eps" an so on. If I have this as a vector in R, then I can 
loop over this vector and do something like:

<<results=tex,echo=FALSE>>=
file.vec <- all files in directory that match name
for(i in seq(along=file.vec))
{
   cat("\\includegraphics{", file.vec[i], "}\n\n", sep"")
}
@

in Sweave.

I'm not sure about getting a list of file names from the current working 
directory that match a given string that I can then loop over and print 
out using cat as shown above. If anyone has any suggestions as to how to 
go about doing this that they are willing to share I would be most grateful.

Thanks in advance,

Gavin

-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpson at ucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From tobias.verbeke at bivv.be  Fri Jun 11 14:22:09 2004
From: tobias.verbeke at bivv.be (tobias.verbeke@bivv.be)
Date: Fri, 11 Jun 2004 14:22:09 +0200
Subject: [R] Sweave and multiple graphs
In-Reply-To: <40C9A076.2090209@ucl.ac.uk>
Message-ID: <OFF10D8A6D.64A668E5-ONC1256EB0.00438813-C1256EB0.00440598@BIVV.BE>





r-help-bounces at stat.math.ethz.ch wrote on 11/06/2004 14:07:18:

> Dear list,
>
> I am using Sweave to build a small report. I want to produce a series of
> figures, each figure containing a number of plots and then have them
> included in the Sweave file.
>
> An example would be to :
>
> postscript(file = "ANCbwplot%03d.eps", onefile = FALSE, other options...)
> oldpar <- par(mfrow = c(2,2))
> ....
> do lots of plots to produce a number of eps files
> ....
> par(oldpar)
> dev.off()
>
> The example in the Sweave FAQ shows how to do something similar for
> cases where you know how many figures there are, but I do not know how
> many figures will be produced so want to produce a more generic solution.
>
> I thought of doing the above code in Sweave, and because I named the
> plots in a unique way, I now want to read all the files in the current
> directory that match "ANCbwplot001.eps" or "ANCbwplot002.eps" or
> "ANCbwplot003.eps" an so on. If I have this as a vector in R, then I can
> loop over this vector and do something like:
>
> <<results=tex,echo=FALSE>>=
> file.vec <- all files in directory that match name
> for(i in seq(along=file.vec))
> {
>    cat("\\includegraphics{", file.vec[i], "}\n\n", sep"")
> }
> @
>
> in Sweave.
>
> I'm not sure about getting a list of file names from the current working
> directory that match a given string that I can then loop over and print
> out using cat as shown above. If anyone has any suggestions as to how to
> go about doing this that they are willing to share I would be most
grateful.
>
> Thanks in advance,
r-help-bounces at stat.math.ethz.ch wrote on 11/06/2004 14:07:18:

> Dear list,
>

[ generation of lots of graphics files to include in
  Sweave document]

>
> I'm not sure about getting a list of file names from the current working
> directory that match a given string that I can then loop over and print
> out using cat as shown above. If anyone has any suggestions as to how to
> go about doing this that they are willing to share I would be most
grateful.
>
> Thanks in advance,

Use list.files(). It has a path argument (to specify the directory)
and a pattern argument to put the regular expression.

mygraphs <- list.files(path="./mygraphs", pattern="^ANCbwplot.*\\.eps")

See ?list.files and maybe ?regex

HTH,
Tobias



From manuellopezibanez at yahoo.es  Fri Jun 11 14:36:12 2004
From: manuellopezibanez at yahoo.es (=?ISO-8859-1?Q?Manuel_L=F3pez-Ib=E1=F1ez?=)
Date: Fri, 11 Jun 2004 14:36:12 +0200
Subject: [R] interaction plot with error bars based on TukeyHSD intervals
Message-ID: <40C9A73C.5080502@yahoo.es>

Hello,

maybe the previous message was not very clear. Thus, I will try to 
explain the problem in a better way.

I would like to have an interaction plot with error bars based on 
TukeyHSD intervals.

In an experiment with two factors "A" and "B", each of them with two 
levels "A={a1,a2}" and "B={b1,b2}", I would like to do a plot like the 
following:


<attached image prueba.png>



I calculate the corresponding interval with:

tk <- TukeyHSD(aov(X$response ~ (factor(X$A) +  factor(X$B))2, data=X) , 
conf.level=0.95)

HSDfactor <- 
max(abs(tk$"factor(X$A):factor(X$B)"[,2]-tk$"factor(X$A):factor(X$B)"[,3]))


Finally, I modified interaction.plot() to add error bars with:

......................................................................
++ ylim <- c(min(cells)-(HSDfactor*0.5), max(cells)+(HSDfactor*0.5))
                                                                                                                                              


  matplot(xvals, cells, ..., type = type,  xlim = xlim, ylim = ylim,
          xlab = xlab, ylab = ylab, axes = axes, xaxt = "n",
          col = col, lty = lty, pch = pch)
                                                                                                                                              


++  ly <- cells[,1]+(HSDfactor*0.5)
++   uy <- cells[,1]-(HSDfactor*0.5)
                                                                                                                                              


++  errbar(xvals,cells[,1],ly,uy,add=TRUE, lty=3, cap=0, lwd=2)
                                                                                                                                             

++  ly <- cells[,2]+(HSDfactor*0.5)
++   uy <- cells[,2]-(HSDfactor*0.5)
                                                                                                                                             

++  errbar(xvals,cells[,2],ly,uy,add=TRUE, lty=3, cap=0, lwd=2)

 if(legend) {
        yrng <- diff(ylim)
        yleg <- ylim[2] - 0.1 * yrng

.............................................................................


However, the resulting intervals are much bigger than they should be, 
thus I think I did something wrong.

I have searched on Google, CRAN and the R mail archive but I only found 
related problems [1, 2], but not any answer...

Andrew Robinson [1] shows that there is a problem when using TukeyHSD 
for interaction terms. However, nobody suggested any work-around.

In another different thread, Martin Henry H. Stevens [2] suggests a 
work-around for the problem, but he thinks that the results obtained are 
"slightly inaccurate". As well, there is not feedback to solve the problem.


Any idea?

Thank you very much.

Manuel.

[1] http://finzi.psych.upenn.edu/R/Rhelp02/archive/12926.html
[2] http://finzi.psych.upenn.edu/R/Rhelp02/archive/32849.html



-------------- next part --------------
A non-text attachment was scrubbed...
Name: prueba.png
Type: image/png
Size: 18151 bytes
Desc: not available
Url : https://stat.ethz.ch/pipermail/r-help/attachments/20040611/5a021a43/prueba.png

From dieter.menne at menne-biomed.de  Fri Jun 11 14:56:50 2004
From: dieter.menne at menne-biomed.de (Dieter Menne)
Date: Fri, 11 Jun 2004 14:56:50 +0200
Subject: [R] Modifying Code in .rda based packages (e.g. lme4)
Message-ID: <INEGIMHGODBGKFPOJBBMOEJJCBAA.dieter.menne@menne-biomed.de>

Dear List,

assume I want to make a minor local change in a package that is supplied as
.rda. For example, I want to get rid of the non-verbose-protected
"Iteration" message in GLMM/lme4.

Probably I have to load / change / save the package, but could someone help
me to get the syntax right?

Dieter Menne



From ripley at stats.ox.ac.uk  Fri Jun 11 15:04:19 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 11 Jun 2004 14:04:19 +0100 (BST)
Subject: [R] Modifying Code in .rda based packages (e.g. lme4)
In-Reply-To: <INEGIMHGODBGKFPOJBBMOEJJCBAA.dieter.menne@menne-biomed.de>
Message-ID: <Pine.LNX.4.44.0406111359150.7940-100000@gannet.stats>

No standard R package is supplied as a .rda, including not lme4.  You
must be looking at a binary installation, and you would do best to
reinstall from the sources.  You could use

R --vanilla
load("..../all.rda")
fix(GLMM)
save(ls(all=T), file="..../all.rda", compress = TRUE)
q()

but we would not recommend it.  Indeed, we do not recommend your altering
functions in other people's packages.  Why not just make a copy of GLMM
with another name and alter that?


On Fri, 11 Jun 2004, Dieter Menne wrote:

> assume I want to make a minor local change in a package that is supplied as
> .rda. For example, I want to get rid of the non-verbose-protected
> "Iteration" message in GLMM/lme4.
> 
> Probably I have to load / change / save the package, but could someone help
> me to get the syntax right?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rpeng at jhsph.edu  Fri Jun 11 15:04:26 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Fri, 11 Jun 2004 09:04:26 -0400
Subject: [R] Modifying Code in .rda based packages (e.g. lme4)
In-Reply-To: <INEGIMHGODBGKFPOJBBMOEJJCBAA.dieter.menne@menne-biomed.de>
References: <INEGIMHGODBGKFPOJBBMOEJJCBAA.dieter.menne@menne-biomed.de>
Message-ID: <40C9ADDA.7030702@jhsph.edu>

You should probably download the source for the package from CRAN, 
modify the source, then reinstall.

-roger

Dieter Menne wrote:
> Dear List,
> 
> assume I want to make a minor local change in a package that is supplied as
> .rda. For example, I want to get rid of the non-verbose-protected
> "Iteration" message in GLMM/lme4.
> 
> Probably I have to load / change / save the package, but could someone help
> me to get the syntax right?
> 
> Dieter Menne
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From iwhite at staffmail.ed.ac.uk  Fri Jun 11 15:06:36 2004
From: iwhite at staffmail.ed.ac.uk (I M S White)
Date: Fri, 11 Jun 2004 14:06:36 +0100 (BST)
Subject: [R] own family for glm()
Message-ID: <Pine.GSO.4.58.0406111402470.6866@holyrood.ed.ac.uk>

Is it possible in R to create a customized family to be used with glm()? I
see no mention of this possibility in the documentation. (S-plus has
something called make.family).

======================================
I.White
ICAPB, University of Edinburgh
Ashworth Laboratories, West Mains Road
Edinburgh EH9 3JT
Fax: 0131 650 6564  Tel: 0131 650 5490
E-mail: iwhite at staffmail.ed.ac.uk



From sekemp at glam.ac.uk  Fri Jun 11 15:18:42 2004
From: sekemp at glam.ac.uk (Kemp S E (Comp))
Date: Fri, 11 Jun 2004 14:18:42 +0100
Subject: [R] dll file missing?
Message-ID: <EF1C49A3F569D41186C900508B6DDC99110F8585@ems3.glam.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040611/a2bea86b/attachment.pl

From ripley at stats.ox.ac.uk  Fri Jun 11 15:23:09 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 11 Jun 2004 14:23:09 +0100 (BST)
Subject: [R] own family for glm()
In-Reply-To: <Pine.GSO.4.58.0406111402470.6866@holyrood.ed.ac.uk>
Message-ID: <Pine.LNX.4.44.0406111419260.8139-100000@gannet.stats>

On Fri, 11 Jun 2004, I M S White wrote:

> Is it possible in R to create a customized family to be used with glm()? I
> see no mention of this possibility in the documentation. (S-plus has
> something called make.family).

It is possible.  Package MASS has two examples, the simpler of which is 
neg.bin and the other is negative.binomial.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Jun 11 15:27:31 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 11 Jun 2004 14:27:31 +0100 (BST)
Subject: [R] dll file missing?
In-Reply-To: <EF1C49A3F569D41186C900508B6DDC99110F8585@ems3.glam.ac.uk>
Message-ID: <Pine.LNX.4.44.0406111423380.8139-100000@gannet.stats>

On Fri, 11 Jun 2004, Kemp S E (Comp) wrote:

> Hi,
>  
> I am trying to "do" a dyn.load(), but I get the following error...
>  
> > dyn.load("fileGT.dll")
> Error in dyn.load(x, as.logical(local), as.logical(now)) : 
>         unable to load shared library "C:/R_Files/fileGT.dll":
>   LoadLibrary failure:  The specified module could not be found.
>  
> It states it can't find the dll but it is in that directory. 

Actually, it doesn't say that.  It says a specified module could not be
found, without specifying which.  Windows could be more helpful in its
error messages (which version of Windows is this?).

> I have checked
> the file properties and the read and execute checkboxes are ticked.
>  
> My C++ code uses a 3rd party library. I have linked it using the g++
> commands and it compiles fine. Could this be the source of my problems?????

It could well be.  Any DLLs that fileGT.dll depends on must be in the same
directory or in your PATH.  Those DLLs are modules specified by
fileGT.dll, hence the confusing error message.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From oehler at phys.ethz.ch  Fri Jun 11 15:52:32 2004
From: oehler at phys.ethz.ch (Oscar Oehler)
Date: Fri, 11 Jun 2004 15:52:32 +0200
Subject: [R] Einlesen von Daten unter R
Message-ID: <40C9B920.9040800@phys.ethz.ch>

Sehr geehrte Damen und Herren,

An der Fachhochschule Winterthur wurde f??r mich im Rahmen einer 
Diplomarbeit (Prof. Ruckstuhl) ein R-Programm zur Auswertung von 
IR-Spektren (line-shape-Analyse zur quantitativen Analyse von 
Gasmischungen) entwickelt. Die Daten werden einer Excel-Tabelle 
entnommen. Bisher werden die spektroskopischen Daten ??ber den Inhalt von 
64 Kan??len ??ber die RS232-Schnittstelle seriell mittels eines 
Labview-Programmes in eine Excel-Tabelle eingelesen.

Die Verwendung der beiden Programme Labview und R  ist unpraktisch. Es 
stellt sich daher die Frage, ob serielle Daten auch ??ber R eingelesen 
werden k??nnen oder ob das R-Programm in ein Labview-Programm eingebaut 
werden kann.

Ich w??re sehr froh, wenn Sie mir in dieser Sache behilflich sein 
k??nnten. Ebenso m??chte ich gerne N??heres ??ber R erfahren. Zur Verf??gung 
steht mir bisher das bekannte B??chlein ??ber R und eine Beschreibung 
"Message Boxes in R TclTk".

Mit freundlichen Gr??ssen


Dr. Oscar Oehler
Phys.Dept. ETHZ
HPF D12
8093 Z??rich
Tel 3 21 65



From bates at stat.wisc.edu  Fri Jun 11 15:53:15 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: 11 Jun 2004 08:53:15 -0500
Subject: [R] Modifying Code in .rda based packages (e.g. lme4)
In-Reply-To: <INEGIMHGODBGKFPOJBBMOEJJCBAA.dieter.menne@menne-biomed.de>
References: <INEGIMHGODBGKFPOJBBMOEJJCBAA.dieter.menne@menne-biomed.de>
Message-ID: <6r659yi4l0.fsf@bates4.stat.wisc.edu>

"Dieter Menne" <dieter.menne at menne-biomed.de> writes:

> assume I want to make a minor local change in a package that is supplied as
> .rda. For example, I want to get rid of the non-verbose-protected
> "Iteration" message in GLMM/lme4.
> 
> Probably I have to load / change / save the package, but could someone help
> me to get the syntax right?

Brian Ripley already replied on how to change the code.  

I just wanted to say that we will add verbose protection to the
Iteration messages in GLMM/lme4.



From klaus.juenemann at epigenomics.com  Fri Jun 11 15:42:30 2004
From: klaus.juenemann at epigenomics.com (Klaus Juenemann)
Date: Fri, 11 Jun 2004 15:42:30 +0200
Subject: [R] [R-pkgs] New package: RUnit
Message-ID: <40C9B6C6.1020604@epigenomics.com>

We would like to announce the availability on CRAN of a new package: RUnit

It contains a unit testing framework strongly inspired by Javas popular 
JUint package. In addition it contains some functionality to investigate 
the degree to which some function is covered by a test suite.

The main aims of the package are
- to support a development style where test cases are written and 
constantly executed parallel to implementing the actual functinality.
- to deliver the results of a test run in a format as structured and 
helpful as possible.

Besides the usual function documentation the doc subdirectory contains a 
pdf file with additional information.


Questions, comments and suggestions are greatly appreciated.

Happy testing

Matthias Burger
Thomas Koenig
Klaus Juenemann




-- 
Klaus Juenemann

Epigenomics AG          www.epigenomics.com           Kastanienallee 24
+493024345393                                              10435 Berlin

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://www.stat.math.ethz.ch/mailman/listinfo/r-packages



From myint.tin-tin-htar at jvr.ap-hop-paris.fr  Fri Jun 11 16:51:07 2004
From: myint.tin-tin-htar at jvr.ap-hop-paris.fr (myint.tin-tin-htar@jvr.ap-hop-paris.fr)
Date: Fri, 11 Jun 2004 16:51:07 +0200
Subject: [R] ROC for threshold value, biometrics
Message-ID: <40C9C6DB.79485F98@jvr.ap-hop-paris.fr>

Hello,

I am just a beginner of R 1.9.0.
I try to construct a predictive score for the development of liver
cancer in cirrhotic patients. So dependant variable is binanry (cancer
yes or no). Independant variables are biological data. The aim is to
find out a cut-off value which differentiate (theoratically)  from
normal to pathological state for each biological data.

How can I step in procedue to get a cut-off value (threshold) for each
variable? I think I should try by ROC. But I'm not sure. If so, someone
can lead me?
If not, someone can advice me how ?

Any advice will be cordially aprreciated.

Tin Tin Htar Myint
Research assistant
Liver unit
Jean Verdier Hospital, Bondy
France



From sekemp at glam.ac.uk  Fri Jun 11 17:21:29 2004
From: sekemp at glam.ac.uk (Kemp S E (Comp))
Date: Fri, 11 Jun 2004 16:21:29 +0100
Subject: [R] Rcmd SHLIB error
Message-ID: <EF1C49A3F569D41186C900508B6DDC99110F858B@ems3.glam.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040611/9bb12ae5/attachment.pl

From lanzi at elet.polimi.it  Fri Jun 11 17:52:17 2004
From: lanzi at elet.polimi.it (Pier Luca Lanzi)
Date: Fri, 11 Jun 2004 17:52:17 +0200
Subject: [R] *** NEWBIE QUESTION *** QUANTILE FUNCTIONS
Message-ID: <1086969137.4233.18.camel@lanzi.elet.polimi.it>

Dear all, 

sorry this will sound as naive as it can be.

I need to know whether there is a closed analytical form (even
approximated) to the quantile function for the Binomial?

and for the Poisson?

if not, what is the best citation to use when stating this?

Thank you, 

Pier Luca



From gavin.simpson at ucl.ac.uk  Fri Jun 11 17:46:50 2004
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Fri, 11 Jun 2004 16:46:50 +0100
Subject: [R] Sweave and multiple graphs
In-Reply-To: <OFF10D8A6D.64A668E5-ONC1256EB0.00438813-C1256EB0.00440598@BIVV.BE>
References: <OFF10D8A6D.64A668E5-ONC1256EB0.00438813-C1256EB0.00440598@BIVV.BE>
Message-ID: <40C9D3EA.6020708@ucl.ac.uk>

tobias.verbeke at bivv.be wrote:
> 
> Use list.files(). It has a path argument (to specify the directory)
> and a pattern argument to put the regular expression.
> 
> mygraphs <- list.files(path="./mygraphs", pattern="^ANCbwplot.*\\.eps")
> 
> See ?list.files and maybe ?regex
> 
> HTH,
> Tobias

Hi Tobias.

Thanks for the reply. I just found list.files() but was struggling with 
specifying the pattern argument. I have this working very well now for 
my application like so in a Sweave file:

<<echo=FALSE,results=tex>>=
postscript(file = "ANCbwplot%03d.eps", onefile = FALSE,
            paper = "special", width = 4, height = 6,
            horizontal = FALSE)
oldpar <- par(mfrow = c(4,3))
for (i in seq(along = g1865.w.res99))
   {
     multplot(g1865.w.res99[[i]], m.title = names(g1865.w.res99[i]))
   }
par(oldpar)
invisible(dev.off())
graphs <- list.files(pattern = "^ANCbwplot.*\\.eps")
for (i in seq(along = graphs))
   {
     cat("\\includegraphics{", graphs[i], "}\n\n", sep = "")
   }
@

multplot() is just a helper function that extracts the relevant 
information from the list g1865.w.res99 and plots a boxplot

Many thanks for your help,

Gavin
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpson at ucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From sundar.dorai-raj at PDF.COM  Fri Jun 11 18:00:51 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Fri, 11 Jun 2004 11:00:51 -0500
Subject: [R] *** NEWBIE QUESTION *** QUANTILE FUNCTIONS
In-Reply-To: <1086969137.4233.18.camel@lanzi.elet.polimi.it>
References: <1086969137.4233.18.camel@lanzi.elet.polimi.it>
Message-ID: <40C9D733.4030902@pdf.com>



Pier Luca Lanzi wrote:

> Dear all, 
> 
> sorry this will sound as naive as it can be.
> 
> I need to know whether there is a closed analytical form (even
> approximated) to the quantile function for the Binomial?
> 
> and for the Poisson?
> 
> if not, what is the best citation to use when stating this?
> 
> Thank you, 
> 
> Pier Luca
> 

Hi Pier,

?qbinom, ?qpois

--sundar



From wegmann_mailinglist at gmx.net  Fri Jun 11 18:09:23 2004
From: wegmann_mailinglist at gmx.net (Martin Wegmann)
Date: Fri, 11 Jun 2004 18:09:23 +0200
Subject: [R] Einlesen von Daten unter R
In-Reply-To: <40C9B920.9040800@phys.ethz.ch>
References: <40C9B920.9040800@phys.ethz.ch>
Message-ID: <200406111809.24859.wegmann_mailinglist@gmx.net>

hello, 

sorry, I cannot help you but if you post it again in English (because it is an 
English speaking mailing list) your are likely to receive a helpful reply. 
Cheers Martin


On Friday 11 June 2004 15:52, Oscar Oehler wrote:
> Sehr geehrte Damen und Herren,
>
> An der Fachhochschule Winterthur wurde f??r mich im Rahmen einer
> Diplomarbeit (Prof. Ruckstuhl) ein R-Programm zur Auswertung von
> IR-Spektren (line-shape-Analyse zur quantitativen Analyse von
> Gasmischungen) entwickelt. Die Daten werden einer Excel-Tabelle
> entnommen. Bisher werden die spektroskopischen Daten ??ber den Inhalt von
> 64 Kan??len ??ber die RS232-Schnittstelle seriell mittels eines
> Labview-Programmes in eine Excel-Tabelle eingelesen.
>
> Die Verwendung der beiden Programme Labview und R  ist unpraktisch. Es
> stellt sich daher die Frage, ob serielle Daten auch ??ber R eingelesen
> werden k??nnen oder ob das R-Programm in ein Labview-Programm eingebaut
> werden kann.
>
> Ich w??re sehr froh, wenn Sie mir in dieser Sache behilflich sein
> k??nnten. Ebenso m??chte ich gerne N??heres ??ber R erfahren. Zur Verf??gung
> steht mir bisher das bekannte B??chlein ??ber R und eine Beschreibung
> "Message Boxes in R TclTk".
>
> Mit freundlichen Gr??ssen
>
>
> Dr. Oscar Oehler
> Phys.Dept. ETHZ
> HPF D12
> 8093 Z??rich
> Tel 3 21 65
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From flom at ndri.org  Fri Jun 11 18:10:15 2004
From: flom at ndri.org (Peter Flom)
Date: Fri, 11 Jun 2004 12:10:15 -0400
Subject: [R] par specification inside formula vs. outside function call
Message-ID: <s0c9a134.001@MAIL.NDRI.ORG>

The other day I asked a question about changing the size of axis labels
in a dotchart.  Chuck Cleland provided the answer, which was to specify

par(cex.axis = .8)
outside the call to dotchart

I had tried this inside the formula, and it did nothing (nor did it
produce an error).  

I was wondering when it is necessary to specify things outside versus.
inside a call to a function

Thanks

Peter

Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)



From ggrothendieck at myway.com  Fri Jun 11 18:38:02 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 11 Jun 2004 16:38:02 +0000 (UTC)
Subject: [R] Einlesen von Daten unter R
References: <40C9B920.9040800@phys.ethz.ch>
Message-ID: <loom.20040611T183206-26@post.gmane.org>


My German is not very good but I assume your question is how to interface
Labview and R.  You might want to have a look at the two R COM interfaces
that are available:

   http://cran.r-project.org/other-software.html

   http://mailman.csd.univie.ac.at/mailman/listinfo/rcom-l

   http://www.omegahat.org


Oscar Oehler <oehler <at> phys.ethz.ch> writes:

: 
: Sehr geehrte Damen und Herren,
: 
: An der Fachhochschule Winterthur wurde fr mich im Rahmen einer 
: Diplomarbeit (Prof. Ruckstuhl) ein R-Programm zur Auswertung von 
: IR-Spektren (line-shape-Analyse zur quantitativen Analyse von 
: Gasmischungen) entwickelt. Die Daten werden einer Excel-Tabelle 
: entnommen. Bisher werden die spektroskopischen Daten ber den Inhalt von 
: 64 Kanlen ber die RS232-Schnittstelle seriell mittels eines 
: Labview-Programmes in eine Excel-Tabelle eingelesen.
: 
: Die Verwendung der beiden Programme Labview und R  ist unpraktisch. Es 
: stellt sich daher die Frage, ob serielle Daten auch ber R eingelesen 
: werden knnen oder ob das R-Programm in ein Labview-Programm eingebaut 
: werden kann.
: 
: Ich wre sehr froh, wenn Sie mir in dieser Sache behilflich sein 
: knnten. Ebenso mchte ich gerne Nheres ber R erfahren. Zur Verfgung 
: steht mir bisher das bekannte Bchlein ber R und eine Beschreibung 
: "Message Boxes in R TclTk".
: 
: Mit freundlichen Grssen
: 
: 
: Dr. Oscar Oehler
: Phys.Dept. ETHZ
: HPF D12
: 8093 Zrich
: Tel 3 21 65



From JGPorzak at LoyaltyMatrix.com  Fri Jun 11 19:11:50 2004
From: JGPorzak at LoyaltyMatrix.com (Jim Porzak)
Date: Fri, 11 Jun 2004 10:11:50 -0700
Subject: [R] My useR! slides: Doing Customer Intelligence with R
Message-ID: <5.1.1.6.0.20040611093537.00ae6b68@pop3.norton.antivirus>

FYI for useR! attendees & others.

The slides for my talk "Doing Customer Intelligence with R" are up on our R 
weblog: R.LoyaltyMatrix.com

Also note any books ordered through the links on the blog will benefit the 
R Foundation - see blog for details.

I will be using the blog as an informal log of our adventures using R for 
data mining, business intelligence and, in particular, customer 
intelligence. Comments on my blog posts are encouraged. Guest authors are 
welcome - contact me directly.

Once again, many thanks to the useR! organizers and the local team in 
Vienna for the great meeting!

Jim Porzak
Director of Analytics
Loyalty Matrix, Inc.
R.LoyaltyMatrix.com
www.LoyaltyMatrix.com



From lauraholt_983 at hotmail.com  Fri Jun 11 19:51:25 2004
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Fri, 11 Jun 2004 12:51:25 -0500
Subject: [R] question about Rcmd SHLIB
Message-ID: <BAY12-F123KlKHMO9qz00059fd8@hotmail.com>

Dear R People:

I'm trying to use the Rcmd SHLIB to produce a dll.

Rcmd SHLIB -o test2.dll test2.f
make[1]: *** [libR.a] Error 255
make: *** [libR] Error 2

Where do I go to find out about the "make" errors, please?
I suspect that I might be missing something.  I have the tools for creating 
new packages, but
maybe I left out something.

This is for R version 1.9.0 on Windows.

Thanks in advance,
Sincerely,
Laura
mailto: lauraholt_983 at hotmail.com



From andy_liaw at merck.com  Fri Jun 11 20:01:12 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 11 Jun 2004 14:01:12 -0400
Subject: [R] par specification inside formula vs. outside function
 cal l
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7E99@usrymx25.merck.com>

> From: Peter Flom
> 
> The other day I asked a question about changing the size of 
> axis labels
> in a dotchart.  Chuck Cleland provided the answer, which was 
> to specify
> 
> par(cex.axis = .8)
> outside the call to dotchart
> 
> I had tried this inside the formula, and it did nothing (nor did it
> produce an error).  
> 
> I was wondering when it is necessary to specify things outside versus.
> inside a call to a function

My guess is that the fourth line from the bottom of dotchart():

    axis(1)
 
which does not contain "...", so any graphical parameters you set through
arguments to dotchart() would not have any effect on the x-axis.  You can
create a copy of dotchart (say dotchart2) that has

    axis(1, ...)

instead, and I believe that will work.

Andy

 
> Thanks
> 
> Peter
> 
> Peter L. Flom, PhD
> Assistant Director, Statistics and Data Analysis Core
> Center for Drug Use and HIV Research
> National Development and Research Institutes
> 71 W. 23rd St
> www.peterflom.com
> New York, NY 10010
> (212) 845-4485 (voice)
> (917) 438-0894 (fax)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ripley at stats.ox.ac.uk  Fri Jun 11 20:02:32 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 11 Jun 2004 19:02:32 +0100 (BST)
Subject: [R] question about Rcmd SHLIB
In-Reply-To: <BAY12-F123KlKHMO9qz00059fd8@hotmail.com>
Message-ID: <Pine.LNX.4.44.0406111901500.30189-100000@gannet.stats>

See readme.packages.

It would help to run

make libR.a

first.

On Fri, 11 Jun 2004, Laura Holt wrote:

> Dear R People:
> 
> I'm trying to use the Rcmd SHLIB to produce a dll.
> 
> Rcmd SHLIB -o test2.dll test2.f
> make[1]: *** [libR.a] Error 255
> make: *** [libR] Error 2
> 
> Where do I go to find out about the "make" errors, please?
> I suspect that I might be missing something.  I have the tools for creating 
> new packages, but
> maybe I left out something.
> 
> This is for R version 1.9.0 on Windows.
> 
> Thanks in advance,
> Sincerely,
> Laura
> mailto: lauraholt_983 at hotmail.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From christoph.lehmann at gmx.ch  Fri Jun 11 20:12:52 2004
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Fri, 11 Jun 2004 20:12:52 +0200
Subject: [R] lme newbie question
Message-ID: <1086977572.4436.11.camel@christophl>

Hi
I try to implement a simple 2-factorial repeated-measure anova in the
lme framework and would be grateful for a short feedback

-my dependent var is a reaction-time (rt), 
-as dependent var I have 
   -the age-group (0/1) the subject belongs to (so this is a
    between-subject factor), and 
   -two WITHIN experimental conditions, one (angle) having 5, the other
    3 (hands) factor-levels; means each subjects performs on 3 * 5 = 15
    different task diffiulties

Am I right in this lme implementation, when I want to investigate the
influence of the age.group, and the two conditions on the rt:

	my.lme <- lme(rt ~ age.group + angles * hands, data = my.data, random =
~ 1 |subject)

then I think I would have to compare the model above with a more
elaborated one, including more interactions:

	my.lme2 <- lme(rt ~ age.group * angles * hands, data = my.data, random
= ~ 1 |subject)

and comparing them by performing a likelhood-ratio test, yes?

I think, if I would like to generalize the influence of the experimental
conditions on the rt I should define angles and hands as a random
effect, yes? 

?

thanks for a short feedback. It seems, repeated-measures anova's aren't
a trivial topic in R :)

Cheers!

Christoph
-- 
Christoph Lehmann <christoph.lehmann at gmx.ch>



From p.dalgaard at biostat.ku.dk  Fri Jun 11 20:20:35 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 11 Jun 2004 20:20:35 +0200
Subject: [R] question about Rcmd SHLIB
In-Reply-To: <BAY12-F123KlKHMO9qz00059fd8@hotmail.com>
References: <BAY12-F123KlKHMO9qz00059fd8@hotmail.com>
Message-ID: <x2isdyard8.fsf@biostat.ku.dk>

"Laura Holt" <lauraholt_983 at hotmail.com> writes:

> Dear R People:
> 
> I'm trying to use the Rcmd SHLIB to produce a dll.
> 
> Rcmd SHLIB -o test2.dll test2.f
> make[1]: *** [libR.a] Error 255
> make: *** [libR] Error 2
> 
> Where do I go to find out about the "make" errors, please?
> I suspect that I might be missing something.  I have the tools for
> creating new packages, but
> maybe I left out something.
> 
> This is for R version 1.9.0 on Windows.

I've seen this before... I seem to recall that it is a PATH issue.
Looks like make went looking for libR.a  and couldn't find it.
 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From patrick.drechsler at gmx.net  Fri Jun 11 20:38:22 2004
From: patrick.drechsler at gmx.net (Patrick Drechsler)
Date: Fri, 11 Jun 2004 20:38:22 +0200
Subject: [R] [StatDataML] compile error
Message-ID: <m3pt86szxd.fsf@pdrechsler.fqdn.th-h.de>

Hi,

sorry if this is OT here. I'm trying to compile StatDataML[1] to
import Matlab data into R. The R part works fine but compiling
the Matlab part exits with this error message (following the
INSTALL instructions: autoconf -> ./configure -> make):

,----[ patrick at trurl:~/src/statdatml/StatDataML/MatOct> make ]
| make[1]: Entering directory `/home/patrick/src/statdatml/StatDataML/MatOct/matlab'
[...]
| mex readsdml.c    -Dmatlab
| readsdml.c:26:27: libxml/parser.h: Datei oder Verzeichnis nicht gefunden
[...]
`----

libxml2 is installed:

,----[ patrick at trurl:~/src/statdatml/StatDataML/MatOct> locate libxml/parser.h ]
| /usr/include/libxml2/libxml/parser.h
`----

Can somebody give me a pointer if there's anything else I need
to install to get StatDataML running? Do I need to configure xml
somehow?

I'm running current Matlab (6.5SP1) and R 1.9.1 alpha on a linux
box (SuSE 8.2Prof).


Footnotes: 
[1] http://www.omegahat.org/StatDataML/

-- 
"What happens if a big asteroid hits Earth ? Judging from
 realistic simulations involving a sledge hammer and a common
 laboratory frog, we can assume it will be pretty bad."
                                                -- Dave Barry



From loesljrg at accucom.net  Fri Jun 11 21:01:27 2004
From: loesljrg at accucom.net (JRG)
Date: Fri, 11 Jun 2004 15:01:27 -0400
Subject: [R] lme newbie question
In-Reply-To: <1086977572.4436.11.camel@christophl>
Message-ID: <200406111901.i5BJ1DDf012325@hypatia.math.ethz.ch>

On 11 Jun 04, at 20:12, Christoph Lehmann wrote:

> Hi
> I try to implement a simple 2-factorial repeated-measure anova in the
> lme framework and would be grateful for a short feedback
> 
> -my dependent var is a reaction-time (rt), 
> -as dependent var I have 
>    -the age-group (0/1) the subject belongs to (so this is a
>     between-subject factor), and 
>    -two WITHIN experimental conditions, one (angle) having 5, the other
>     3 (hands) factor-levels; means each subjects performs on 3 * 5 = 15
>     different task diffiulties
> 
> Am I right in this lme implementation, when I want to investigate the
> influence of the age.group, and the two conditions on the rt:
> 
> 	my.lme <- lme(rt ~ age.group + angles * hands, data = my.data, random =
> ~ 1 |subject)
> 
> then I think I would have to compare the model above with a more
> elaborated one, including more interactions:
> 
> 	my.lme2 <- lme(rt ~ age.group * angles * hands, data = my.data, random
> = ~ 1 |subject)
> 
> and comparing them by performing a likelhood-ratio test, yes?
> 
> I think, if I would like to generalize the influence of the experimental
> conditions on the rt I should define angles and hands as a random
> effect, yes? 
> 

Perhaps I've missed something here, but wouldn't your ability to generalize about the experimental conditions depend, in part, 
on how their levels were selected?  Were the angles randomly sampled?  Were the hands randomly sampled (not sure what 
that would mean)?  If not, how does defining these conditions to be random effects in a model enable valid generalization?

---JRG



John R. Gleason

Syracuse University
430 Huntington Hall                      Voice:   315-443-3107
Syracuse, NY 13244-2340  USA             FAX:     315-443-4085

PGP public key at keyservers



From hkang at urban.csuohio.edu  Fri Jun 11 21:22:27 2004
From: hkang at urban.csuohio.edu (hkang)
Date: Fri, 11 Jun 2004 15:22:27 -0400
Subject: [R] space-time k function problem
Message-ID: <000801c44fe9$6f2f6d90$e7eb9489@Hojesony>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040611/1652d0e9/attachment.pl

From Roger.Bivand at nhh.no  Fri Jun 11 21:35:55 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 11 Jun 2004 21:35:55 +0200 (CEST)
Subject: [R] space-time k function problem
In-Reply-To: <000801c44fe9$6f2f6d90$e7eb9489@Hojesony>
Message-ID: <Pine.LNX.4.44.0406112134050.4534-100000@reclus.nhh.no>

On Fri, 11 Jun 2004, hkang wrote:

> I am intersted in time-space clustering of local industry.
> 
> I made the point file, polygons and time table then run 'stkhat' function in Splancs,
> 
> but it generates only time k function. I noticed that it has only one ks value. ...
> 
> 
> 
> Can anybody help me?. 
> 

I think you need to coerce manufacpt to be a two-column matrix, now it is 
a data.frame, which is a different object structure.

as.matrix(manufacpt) should help, look at str(manufacpt) to see what is 
inside.

> thanks in advance.
> 
> 
> 
> 
> 
> countiespt <- read.table('d:/dissertation/dailynew/jun10/countiespt_ok.txt', sep=",")
> 
>  polymap(countiespt)
> 
>  
> 
>  manufacpt <- read.table('d:/dissertation/dailynew/jun10/manufacpt.txt', sep=",")
> 
>  names(manufacpt)<-c("x", "y")
> 
>  
> 
>  
> 
>  manufac23<-read.table('d:/dissertation/dailynew/jun10/manufac23.txt', sep=",")
> 
>  names(manufac23)<-c("x", "y", "t")
> 
> 
> 
> manufac23<-read.table('d:/dissertation/dailynew/jun10/manufac23.txt', sep=",")
> names(manufac23)<-c("x", "y", "t")
> bur2 <- stkhat(manufacpt, manufac23$t, countiespt, c(34060, 37712),  seq(1,60,60), seq(91,1005,60))
> oldpar <- par(mfrow=c(2,1))
> plot(bur2$s, bur1$ks, type="l", xlab="distance", ylab="Estimated K",
>   main="spatial K function")
> plot(bur2$t, bur2$kt, type="l", xlab="time", ylab="Estimated K",
>   main="temporal K function")
> par(oldpar)
> 
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From rxg218 at psu.edu  Fri Jun 11 21:40:20 2004
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Fri, 11 Jun 2004 15:40:20 -0400
Subject: [R] probabilistic neural networks
Message-ID: <1086982820.12262.36.camel@blue.chem.psu.edu>

Hi,
  I'm working on a classification problem and one of the methods I'd
like to use are neural networks. I've been using nnet to build a
classification network. However I would like to have the probabilities
associated with the prediction. 

Are there any implementations of probabilistic neural networks available
in R?

thanks,

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
Artificial intelligence has the same relation to intelligence as
artificial flowers have to flowers.
-- David Parnas



From ripley at stats.ox.ac.uk  Fri Jun 11 22:02:43 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 11 Jun 2004 21:02:43 +0100 (BST)
Subject: [R] probabilistic neural networks
In-Reply-To: <1086982820.12262.36.camel@blue.chem.psu.edu>
Message-ID: <Pine.LNX.4.44.0406112101300.30326-100000@gannet.stats>

That's not what a `probabilistic neural network' is.

However, nnet already does what you ask: do read the references as the 
posting guide asks.

On Fri, 11 Jun 2004, Rajarshi Guha wrote:

>   I'm working on a classification problem and one of the methods I'd
> like to use are neural networks. I've been using nnet to build a
> classification network. However I would like to have the probabilities
> associated with the prediction. 
> 
> Are there any implementations of probabilistic neural networks available
> in R?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From juderyan at tcindex.com  Fri Jun 11 22:20:46 2004
From: juderyan at tcindex.com (Jude Ryan)
Date: Fri, 11 Jun 2004 16:20:46 -0400
Subject: [R] Error when I try to build / plot a tree using rpart()
Message-ID: <40CA141E.10905@tcindex.com>

Hi,

I am using the rpart package to build a classification tree. I did 
manage to build a tree with data on a previous project. However, when 
attampting to build a tree on a project I am working on, I seem to be 
getting the error shown below:

 > nhg3.rp <- rpart(profitresp ~., nhg3, method="class")
 > plot(nhg3.rp, branch=0.4, uniform=T); text(nhg3.rp, digits=3)
Error in yval[, 1] : incorrect number of dimensions

The distribution of my binary dependent variable is:
 > table(nhg$profitresp)

   0    1
3703 4360

I am using 105 potential predictor variables. I am trying to come up 
with a decision rule to identify profitable responders from 
non-responders to a mailing.

Some other details are:
 > summary(nhg3.rp)
Call:
rpart(formula = profitresp ~ ., data = nhg3, method = "class")
  n= 8063

           CP nsplit rel error
1 0.009451796      0         1
Error in yval[, 1] : incorrect number of dimensions

 > print(nhg3.rp)
n= 8063

node), split, n, loss, yval, (yprob)
      * denotes terminal node

1) root 8063 3703 1 (0.4592583 0.5407417) *

 > printcp(nhg3.rp)

Classification tree:
rpart(formula = profitresp ~ ., data = nhg3, method = "class")

Variables actually used in tree construction:
character(0)

Root node error: 3703/8063 = 0.45926

n= 8063

         CP nsplit rel error
1 0.0094518      0         1

Any help is appreciated.

Thanks much,

Jude Ryan



From f.harrell at vanderbilt.edu  Fri Jun 11 17:06:10 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 11 Jun 2004 17:06:10 +0200
Subject: [R] ROC for threshold value, biometrics
In-Reply-To: <40C9C6DB.79485F98@jvr.ap-hop-paris.fr>
References: <40C9C6DB.79485F98@jvr.ap-hop-paris.fr>
Message-ID: <40C9CA62.6070903@vanderbilt.edu>

myint.tin-tin-htar at jvr.ap-hop-paris.fr wrote:
> Hello,
> 
> I am just a beginner of R 1.9.0.
> I try to construct a predictive score for the development of liver
> cancer in cirrhotic patients. So dependant variable is binanry (cancer
> yes or no). Independant variables are biological data. The aim is to
> find out a cut-off value which differentiate (theoratically)  from
> normal to pathological state for each biological data.

A binary endpoing is not a good choice for this problem, as the time to 
diagnosis is very important.  Unless you only have a biopsy at a single 
fixed time (e.g., 5 years post study entry) it would be good to consider 
for example a Cox proportional hazards model.  And there are many 
reasons for not using a cutoff, as detailed in my book Regression 
Modeling Strategies.


> 
> How can I step in procedue to get a cut-off value (threshold) for each
> variable? I think I should try by ROC. But I'm not sure. If so, someone
> can lead me?

Besides not recommending the use of cutoffs on an overall predicted 
value, I think that using cutoffs based on separate analyses of 
predictors is even worse.

Frank

> If not, someone can advice me how ?
> 
> Any advice will be cordially aprreciated.
> 
> Tin Tin Htar Myint
> Research assistant
> Liver unit
> Jean Verdier Hospital, Bondy
> France
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From vograno at evafunds.com  Sat Jun 12 01:35:54 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Fri, 11 Jun 2004 16:35:54 -0700
Subject: [R] memory allocation and interrupts
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A55687F8@phost015.EVAFUNDS.intermedia.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040611/03d646dd/attachment.pl

From alanc at umit.maine.edu  Sat Jun 12 02:13:13 2004
From: alanc at umit.maine.edu (Alan Cobo-Lewis)
Date: Fri, 11 Jun 2004 20:13:13 -0400
Subject: [R] Re: R-help Digest, Vol 16, Issue 11
In-Reply-To: <200406111001.i5BA1mli018213@hypatia.math.ethz.ch>
References: <200406111001.i5BA1mli018213@hypatia.math.ethz.ch>
Message-ID: <fc.004c4d1918a7a57d3b9aca00a322ba34.18a7a705@umit.maine.edu>

r-help at stat.math.ethz.ch writes:
>I make a study in health econometrics and have a categorical dependent variable (take value 1-5). I would like to fit an ordered probit or ordered logit but i didn't find a command or package who make that. Does anyone know if it's exists ?

Try polr() from the MASS package (part of the standard install, also available from r-project.org)
Or try vglm() from Thomas Yee's VGAM package (in beta, available from Yee's web page, which you can find by googling Yee and VGAM)


--
Alan B. Cobo-Lewis, Ph.D.		(207) 581-3840 tel
Department of Psychology		(207) 581-6128 fax
University of Maine
Orono, ME 04469-5742     		alanc at maine.edu

http://www.umaine.edu/visualperception



From andy_liaw at merck.com  Sat Jun 12 03:25:31 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Fri, 11 Jun 2004 21:25:31 -0400
Subject: [R] Error when I try to build / plot a tree using rpart()
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7EA0@usrymx25.merck.com>

You didn't get a tree.  The output of print() tells you that you only have
the root node.  You may need to adjust some of the parameters with
rpart(..., control=rpart.control(...)).  See ?rpart.control.

HTH,
Andy

> From: Jude Ryan
> 
> Hi,
> 
> I am using the rpart package to build a classification tree. I did 
> manage to build a tree with data on a previous project. However, when 
> attampting to build a tree on a project I am working on, I seem to be 
> getting the error shown below:
> 
>  > nhg3.rp <- rpart(profitresp ~., nhg3, method="class")
>  > plot(nhg3.rp, branch=0.4, uniform=T); text(nhg3.rp, digits=3)
> Error in yval[, 1] : incorrect number of dimensions
> 
> The distribution of my binary dependent variable is:
>  > table(nhg$profitresp)
> 
>    0    1
> 3703 4360
> 
> I am using 105 potential predictor variables. I am trying to come up 
> with a decision rule to identify profitable responders from 
> non-responders to a mailing.
> 
> Some other details are:
>  > summary(nhg3.rp)
> Call:
> rpart(formula = profitresp ~ ., data = nhg3, method = "class")
>   n= 8063
> 
>            CP nsplit rel error
> 1 0.009451796      0         1
> Error in yval[, 1] : incorrect number of dimensions
> 
>  > print(nhg3.rp)
> n= 8063
> 
> node), split, n, loss, yval, (yprob)
>       * denotes terminal node
> 
> 1) root 8063 3703 1 (0.4592583 0.5407417) *
> 
>  > printcp(nhg3.rp)
> 
> Classification tree:
> rpart(formula = profitresp ~ ., data = nhg3, method = "class")
> 
> Variables actually used in tree construction:
> character(0)
> 
> Root node error: 3703/8063 = 0.45926
> 
> n= 8063
> 
>          CP nsplit rel error
> 1 0.0094518      0         1
> 
> Any help is appreciated.
> 
> Thanks much,
> 
> Jude Ryan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ajayshah at mayin.org  Sat Jun 12 08:28:08 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Sat, 12 Jun 2004 11:58:08 +0530
Subject: [R] ordered probit or logit / recursive regression
Message-ID: <BD1D7341BE3930408509F95C86A451CBC95051@mail1.UNINE.CH>

> I make a study in health econometrics and have a categorical
> dependent variable (take value 1-5). I would like to fit an ordered
> probit or ordered logit but i didn't find a command or package who
> make that. Does anyone know if it's exists ?

R is very fancy. You won't get mundane things like ordered probit off
the shelf. (I will be very happy if someone will show how to use glm()
to do a vanilla probit!) But you will get markov chain monte carlo for
the ordered probit model! :-) It's cool.

> help.search("ordered probit")
MCMCoprobit(mcmcpack-0.4-8)
                        Markov chain Monte Carlo for Ordered Probit
                        Regression
> library(MCMCpack)
##
## Markov chain Monte Carlo Package (MCMCpack)
## Copyright (C) 2003 Andrew D. Martin and Kevin M. Quinn
##
Loading required package: coda 
Loading required package: MASS 
> ?MCMCoprobit

The example in this function suggests:

        x1 <- rnorm(100); x2 <- rnorm(100);
        z <- 1.0 + x1*0.1 - x2*0.5 + rnorm(100);
        y <- z; y[z < 0] <- 0; y[z >= 0 & z < 1] <- 1;
        y[z >= 1 & z < 1.5] <- 2; y[z >= 1.5] <- 3;
        posterior <- MCMCoprobit(y ~ x1 + x2, tune=0.3, mcmc=20000)
        plot(posterior)
        summary(posterior)

In our standard econometrics notation for the ordered probit model, we
use beta for the coefficients and tau for the vector of cutoffs, I
think they are using beta = (1,.1,-.5), T=100, y* = beta'x + u,
sigma(u) = 1, tau = (0, 1, 1.5). (They use 'z' for our y*).

When I run this, I get:

> summary(posterior)

Iterations = 1:19996
Thinning interval = 5 
Number of chains = 1 
Sample size per chain = 4000 

1. Empirical mean and standard deviation for each variable,
   plus standard error of the mean:

                Mean     SD Naive SE Time-series SE
(Intercept)  1.10010 0.1580 0.002497       0.003518
x1           0.09309 0.1238 0.001958       0.001953
x2          -0.53336 0.1194 0.001887       0.001943
gamma2       1.08807 0.1565 0.002474       0.004187
gamma3       1.54971 0.1818 0.002875       0.004952

2. Quantiles for each variable:

               2.5%      25%      50%     75%   97.5%
(Intercept)  0.8037  0.99397  1.09560  1.2053  1.4253
x1          -0.1503  0.01075  0.09285  0.1784  0.3393
x2          -0.7671 -0.61606 -0.53358 -0.4534 -0.2977
gamma2       0.7989  0.98142  1.08194  1.1901  1.4077
gamma3       1.2020  1.42604  1.54146  1.6618  1.9291


-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From ripley at stats.ox.ac.uk  Sat Jun 12 09:12:08 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 12 Jun 2004 08:12:08 +0100 (BST)
Subject: [R] memory allocation and interrupts
In-Reply-To: <C698D707214E6F4AB39AB7096C3DE5A55687F8@phost015.EVAFUNDS.intermedia.net>
Message-ID: <Pine.LNX.4.44.0406120759340.31108-100000@gannet.stats>

On Fri, 11 Jun 2004, Vadim Ogranovich wrote:

> A recent discussion on the list about tryCatch and signals made me think
> about memory allocation and signals in C extension modules. What happens
> to the memory allocated by R_alloc and Calloc if the user pressed Ctr-C
> during the call? R-ext doesn't seem to discuss this. I'd guess that
> R_alloc is interrupt-safe while Calloc is not, but I am not sure. In any
> case a paragraph in R-ext on signals would be helpful.

Easy: such code is not interruptible by Ctrl-C (sic).  And that *is* in
R_exts.* (sic), even with an entry `interrupts' in its index!


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Sat Jun 12 10:47:17 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 12 Jun 2004 10:47:17 +0200
Subject: [R] memory allocation and interrupts
In-Reply-To: <Pine.LNX.4.44.0406120759340.31108-100000@gannet.stats>
References: <Pine.LNX.4.44.0406120759340.31108-100000@gannet.stats>
Message-ID: <x27judnox6.fsf@biostat.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> On Fri, 11 Jun 2004, Vadim Ogranovich wrote:
> 
> > A recent discussion on the list about tryCatch and signals made me think
> > about memory allocation and signals in C extension modules. What happens
> > to the memory allocated by R_alloc and Calloc if the user pressed Ctr-C
> > during the call? R-ext doesn't seem to discuss this. I'd guess that
> > R_alloc is interrupt-safe while Calloc is not, but I am not sure. In any
> > case a paragraph in R-ext on signals would be helpful.
> 
> Easy: such code is not interruptible by Ctrl-C (sic).  And that *is* in
> R_exts.* (sic), even with an entry `interrupts' in its index!

The programmer might reenable interrupts though. This is not something
that we have a precedence (nor a policy) for, but it has crossed my
mind a couple of times. 

In some cases it should be possible to wrap time-consuming C code in a
setjmp/longjmp construct. It's not possible to do it generally because
all hell breaks loose if the C code calls back into R, but not all C
code does that. 

Of course it is critically important that the code resets the
interrupt handling to a sane state when it is done, so it would be
nice if we could abstract a reasonably safe construction into a
RUN_INTERRUPTIBLE() macro.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From Torsten.Steuernagel at gmx.de  Sat Jun 12 12:12:59 2004
From: Torsten.Steuernagel at gmx.de (Torsten Steuernagel)
Date: Sat, 12 Jun 2004 12:12:59 +0200
Subject: [R] Einlesen von Daten unter R
In-Reply-To: <40C9B920.9040800@phys.ethz.ch>
Message-ID: <40CAF34B.19312.41E632@localhost>

Hello Oscar,

I reply in English because that's the language used on this list.

> Die Verwendung der beiden Programme Labview und R  ist unpraktisch. Es
> stellt sich daher die Frage, ob serielle Daten auch ??ber R eingelesen
> werden k??nnen oder ob das R-Programm in ein Labview-Programm eingebaut
> werden kann.

I don't know Labview so I can't say anything about the possibility to integrate R into 
Labview or vice versa. I did a quick search on CRAN but haven't found anything related 
to Labview. If you simply want to avoid Excel, you might check if Labview supports other 
output formats than Excel. Most applications are able to write data to plain text files 
which R handles easily.

I don't think that there is a special serial communications library for R but since the 
serial devices are treated as special files on Win32 (I assume you're using Windows, on 
Unix it's similar), you can use R's I/O functions to access your COM ports. I suggest you 
read the help for connections (? connections) to get an overview of R's I/O connections 
as well as the help for functions like cat, write, scan, readBin, writeBin etc. To get an 
idea, the following code will dial my internet provider (T-Online) using my Notebook's 
modem connected to COM1:

handle <- file("com1", open="r+b")
cat("ATD0191011\r\n", file=handle)
close(handle) 

If that approach will work for you depends on the requirements of the device you want 
to control. R, as far as I know, doesn't provide functions that let you configure your 
serial port (i.e. baut rate, timeouts, handshakings and so on). You might try to configure 
those from a Windows command line (aka "DOS box") using the MODE command. 
Otherwise, you need to write some C functions that interface with the Windows API to 
set up the device control block accordingly. Furthermore, you should also read the 
comments on blocking vs. non-blocking I/O modes in the help for connections.
 
> Ich w??re sehr froh, wenn Sie mir in dieser Sache behilflich sein
> k??nnten. Ebenso m??chte ich gerne N??heres ??ber R erfahren. Zur
> Verf??gung steht mir bisher das bekannte B??chlein ??ber R und eine
> Beschreibung "Message Boxes in R TclTk".

You should check the "Publications" section on the R homepage (http://www.r-
project.org/doc/bib/R-publications.html) for a list of R and S related publications. As a 
start I recommend reading the manuals that come with R, especially "An Introduction to 
R".

BFN,

Torsten



From dmurdoch at pair.com  Sat Jun 12 14:28:35 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Sat, 12 Jun 2004 08:28:35 -0400
Subject: [R] Questions about Preserving registers
In-Reply-To: <40C96D07.2060605@saida-med.com>
References: <000501c44f02$6588a780$f63d9f88@math.ucalgary.ca>	<Pine.A41.4.58.0406100910240.114950@homer09.u.washington.edu>
	<lhahc0pouc4ir465ds1ohrv1vphq2sv57g@4ax.com>
	<40C96D07.2060605@saida-med.com>
Message-ID: <5vslc05dac91mem8qhose700ld11s2rnu7@4ax.com>

On Fri, 11 Jun 2004 10:27:51 +0200, David Lennartsson
<david.lennartsson at saida-med.com> wrote:
> I have not
>bothered to dig deeper and I have no idea of how to get and set the FPU 
>control word in the wrapper. Is there
>a compiler macro or does it have to be done with assembler?

Which compiler are you using?  The minGW gcc has a function
_controlfp() to get and set the control word.  Here's the code that R
uses to check for changes during a DLL load and fix them.

Here is the function R uses to set things up in the first place:

void Rwin_fpset()
{
    _fpreset();
    _controlfp(_MCW_EM, _MCW_EM);
    _controlfp(_PC_64, _MCW_PC);
}

And here is the code R  uses to preserve the control word across a DLL
load:

rcw = _controlfp(0,0) & ~_MCW_IC;  /* Infinity control is ignored */
_clearfp();
tdlh = LoadLibrary(path);
dllcw = _controlfp(0,0) & ~_MCW_IC;
if (dllcw != rcw) {
    _controlfp(rcw, _MCW_EM | _MCW_IC | _MCW_RC | _MCW_PC);
    ...

Duncan Murdoch



From patrick.drechsler at gmx.net  Sat Jun 12 16:08:27 2004
From: patrick.drechsler at gmx.net (Patrick Drechsler)
Date: Sat, 12 Jun 2004 16:08:27 +0200
Subject: [R] sweave and psfrag
Message-ID: <m3vfhwkgx0.fsf@pdrechsler.fqdn.th-h.de>

Hi,

can somebody give me an example on how to automatically replace
all labels in Sweave[1] R plots with corresponding PSFrag[2]
labels?

Thankful for any pointers,

Patrick

Footnotes: 
[1] <URL:http://www.ci.tuwien.ac.at/~leisch/Sweave/>

[2] <URL:http://www.ctan.org/tex-archive/help/Catalogue/entries/psfrag.html?action=/tools/cataloguesearch&catstring=psfrag>

-- 
"Military intelligence is a contradiction in terms." (Groucho Marx)



From ctsolomon at wisc.edu  Sat Jun 12 16:39:34 2004
From: ctsolomon at wisc.edu (Chris Solomon)
Date: Sat, 12 Jun 2004 09:39:34 -0500
Subject: [R] invalid HOMEDRIVE
Message-ID: <000001c4508b$154842f0$66ae96c6@limnology.wisc.edu>

Hi all-

Some recent change...perhaps a windows update, or perhaps a change in
the network on which I use my computer....has made it impossible for me
to start R. When I try to start the program, I get a message that says
"Fatal error: invalid HOMEDRIVE". Any ideas on how to fix this?

Thanks much
Chris



*******
Chris Solomon
Center for Limnology
Univ. of Wisconsin
(715) 356-9494



From ripley at stats.ox.ac.uk  Sat Jun 12 17:05:56 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 12 Jun 2004 16:05:56 +0100 (BST)
Subject: [R] invalid HOMEDRIVE
In-Reply-To: <000001c4508b$154842f0$66ae96c6@limnology.wisc.edu>
Message-ID: <Pine.LNX.4.44.0406121603500.4074-100000@gannet.stats>

It is (a not very recent) Windows XP update.  A simple search of the
archives of this list will find you a dozen or more answers to this
question.  The simplest is to add HOME=P:\Solomon (for some suitable path
that actually exists)  to the end of the target of the shortcut you used 
to start R.

On Sat, 12 Jun 2004, Chris Solomon wrote:

> Some recent change...perhaps a windows update, or perhaps a change in
> the network on which I use my computer....has made it impossible for me
> to start R. When I try to start the program, I get a message that says
> "Fatal error: invalid HOMEDRIVE". Any ideas on how to fix this?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Sat Jun 12 17:09:50 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 12 Jun 2004 17:09:50 +0200
Subject: [R] invalid HOMEDRIVE
In-Reply-To: <000001c4508b$154842f0$66ae96c6@limnology.wisc.edu>
References: <000001c4508b$154842f0$66ae96c6@limnology.wisc.edu>
Message-ID: <40CB1CBE.5090704@statistik.uni-dortmund.de>

Chris Solomon wrote:
> Hi all-
> 
> Some recent change...perhaps a windows update, or perhaps a change in
> the network on which I use my computer....has made it impossible for me
> to start R. When I try to start the program, I get a message that says
> "Fatal error: invalid HOMEDRIVE". Any ideas on how to fix this?
> 
> Thanks much
> Chris
> 

Chris,

we have seen this question roughly 83264861846 times on R-help now (this 
might be a very subjective estimation - in fact, I haven't counted).
Please search the mail archives before asking questions.

The solution is to use R-patched, available at
YourCRANmirror/bin/windows/base/rpatched.html

It's CHANGES file contains the following lines:

   Workaround for bug in Microsoft critical update KB835732:
   if HOMEPATH environment variable is not set properly, R
   would not start (PR#6802).

Uwe Ligges



> 
> *******
> Chris Solomon
> Center for Limnology
> Univ. of Wisconsin
> (715) 356-9494
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From tlumley at u.washington.edu  Sat Jun 12 17:53:58 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Sat, 12 Jun 2004 08:53:58 -0700 (PDT)
Subject: [R] ordered probit or logit / recursive regression
In-Reply-To: <BD1D7341BE3930408509F95C86A451CBC95051@mail1.UNINE.CH>
References: <BD1D7341BE3930408509F95C86A451CBC95051@mail1.UNINE.CH>
Message-ID: <Pine.A41.4.58.0406120853050.81062@homer03.u.washington.edu>

On Sat, 12 Jun 2004, Ajay Shah wrote:

>
> R is very fancy. You won't get mundane things like ordered probit off
> the shelf. (I will be very happy if someone will show how to use glm()
> to do a vanilla probit!)


glm(y~x+z, family=binomial(probit))


Be happy,

	-thomas



From ripley at stats.ox.ac.uk  Sat Jun 12 21:11:20 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 12 Jun 2004 20:11:20 +0100 (BST)
Subject: [R] ordered probit or logit / recursive regression
In-Reply-To: <Pine.A41.4.58.0406120853050.81062@homer03.u.washington.edu>
Message-ID: <Pine.LNX.4.44.0406122007190.4353-100000@gannet.stats>

On Sat, 12 Jun 2004, Thomas Lumley wrote:

> On Sat, 12 Jun 2004, Ajay Shah wrote:
> 
> > R is very fancy. You won't get mundane things like ordered probit off
> > the shelf. 

But you do (and if you have tried to write them you will discover they are
not `mundane' to get right).  Two people have already answered for the
ordered logit, and if you look in the list archives you will find (well,
you should find) a modification of polr to popr (which will appear in the
next release of the VR bundle).

> (I will be very happy if someone will show how to use glm()
> > to do a vanilla probit!)
> 

Quoth Thomas Lumley:

> glm(y~x+z, family=binomial(probit))
> 
> 
> Be happy,

Be even happier that ordered probit also is available.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mdwillett at ucdavis.edu  Sat Jun 12 23:08:11 2004
From: mdwillett at ucdavis.edu (Martin Willett)
Date: Sat, 12 Jun 2004 14:08:11 -0700 (PDT)
Subject: [R] lda
Message-ID: <200406122108.i5CL8Be3002512@tremex.ucdavis.edu>


I am trying to write the following code in R.  The code works in S+ and i 
am trying to do the program in R.

x=discrim(admit~gpa+gmat,prior=c("uniform"),data=data.mm)

i wrote the following in R:

x=lda(admit~gpa+gmat,data=data.mm)

i could not figure out how to write prior=c("uniform") in R.  I would get 
an error every time.  I think that it has something to do 
with "uniform".  Do you know what i use instead of "uniform" for R?  I am 
trying to do a uniform distribution.
Thank you.



From mt at michaelltaylor.com  Sat Jun 12 23:15:11 2004
From: mt at michaelltaylor.com (Michaell Taylor)
Date: Sat, 12 Jun 2004 21:15:11 +0000
Subject: [R] optimize linear function
Message-ID: <200406122115.11823.mt@michaelltaylor.com>



I am attempting to optimize a regression model's parameters to meet a specific 
target for the sum of positive errors over sum of the dependent variable 
(minErr below).  

I see two courses of action , 1) estimate a linear model then iteratively 
reduce the regressors to achieve the desired positive error threshold 
(naturally the regressors and predicted values are biased - but this is 
acceptable).  Were the problem only a single independent variable problem, 
this approach would be fairly trivial.  But the two variable model proves 
more troublesome from an efficiency point of view (efficiency being the mean 
negative error for a given minErr).

The second method, which seems more efficient, is to optimize the linear 
regressors until the criteria is met.  There are several optimizer packages 
out there, but none seem particularly well suited to this sort of task.  

Any suggestions out there?

===========  Sample Problem = first approach  ================

# if we say... 
B1 <- .8
B2 <- .2

# then construct some data..
x1 <- rnorm(100,mean=100,sd=20)
x2 <- rnorm(100,mean=10,sd=2)
y <- B1*x1+B2*x2+rnorm(100,mean=5,sd=10)
# normally of course B1 and B2 are unknown to begin with

# linear model  (presumably yielding B1=.8 and B2=.2)
m<-lm(y ~ x1+ x2)
# test on summation of positive errors.
e <- resid(m)
minErr <- (sum(ifelse(e<0,0,e))/sum(y))-.03
while (minErr>.03){
	Make some adjustment to B1 and B2
	minErr <- (sum(ifelse(e<0,0,e))/sum(y))-.03
	}

======== Second Approach ===========

construct a function to minimize minErr with beginning values from 'm'.


Any suggestions for approaches greatly appreciated.



From elw at stderr.org  Sat Jun 12 23:17:30 2004
From: elw at stderr.org (elijah wright)
Date: Sat, 12 Jun 2004 16:17:30 -0500 (CDT)
Subject: [R] lda
In-Reply-To: <200406122108.i5CL8Be3002512@tremex.ucdavis.edu>
References: <200406122108.i5CL8Be3002512@tremex.ucdavis.edu>
Message-ID: <Pine.LNX.4.58.0406121616490.14237@illuminati.stderr.org>


> i could not figure out how to write prior=c("uniform") in R.  I would get
> an error every time.  I think that it has something to do
> with "uniform".  Do you know what i use instead of "uniform" for R?  I am
> trying to do a uniform distribution.

try ?runif  (random uniform distribution)



From ripley at stats.ox.ac.uk  Sat Jun 12 23:28:59 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 12 Jun 2004 22:28:59 +0100 (BST)
Subject: [R] lda
In-Reply-To: <200406122108.i5CL8Be3002512@tremex.ucdavis.edu>
Message-ID: <Pine.LNX.4.44.0406122225330.4995-100000@gannet.stats>

There is a help page for lda: please read it for yourself (as the posting
guide requests you too).  lda in R works the same way in R as it works in
S-PLUS: in both it is support software for a book, and the posting guide
also asks you to read that book.


On Sat, 12 Jun 2004, Martin Willett wrote:

> 
> I am trying to write the following code in R.  The code works in S+ and i 
> am trying to do the program in R.
> 
> x=discrim(admit~gpa+gmat,prior=c("uniform"),data=data.mm)
> 
> i wrote the following in R:
> 
> x=lda(admit~gpa+gmat,data=data.mm)
> 
> i could not figure out how to write prior=c("uniform") in R.  I would get 
> an error every time.  I think that it has something to do 
> with "uniform".  Do you know what i use instead of "uniform" for R?  I am 
> trying to do a uniform distribution.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From andy_liaw at merck.com  Sat Jun 12 23:29:37 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 12 Jun 2004 17:29:37 -0400
Subject: [R] lda
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7EA3@usrymx25.merck.com>

Seems rather straightforward to me.  The prior="uniform" in discrim() says
to use equal prior for each group.  You can do the same by explicitly
specifying the priors; e.g.,

x <- lda(admit ~ gpa + gmat, data=data.mm, 
         prior=1/nlevels(data.mm$admit))

HTH,
Andy

> From: Martin Willett
> 
> I am trying to write the following code in R.  The code works 
> in S+ and i 
> am trying to do the program in R.
> 
> x=discrim(admit~gpa+gmat,prior=c("uniform"),data=data.mm)
> 
> i wrote the following in R:
> 
> x=lda(admit~gpa+gmat,data=data.mm)
> 
> i could not figure out how to write prior=c("uniform") in R.  
> I would get 
> an error every time.  I think that it has something to do 
> with "uniform".  Do you know what i use instead of "uniform" 
> for R?  I am 
> trying to do a uniform distribution.
> Thank you.



From ripley at stats.ox.ac.uk  Sat Jun 12 23:35:07 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 12 Jun 2004 22:35:07 +0100 (BST)
Subject: [R] optimize linear function
In-Reply-To: <200406122115.11823.mt@michaelltaylor.com>
Message-ID: <Pine.LNX.4.44.0406122231010.4995-100000@gannet.stats>

This is just a linear programming problem.  So the packages which do 
linear programming are `particularly well suited to this sort of task'
and theory tells you a lot about the solution.

Resarching how L_1 regression is done (that is that sum of the absolute 
values of the errors) should give you a lot of help.  This exact 
problem is a special case of one formulation of support vector machines.

On Sat, 12 Jun 2004, Michaell Taylor wrote:

> I am attempting to optimize a regression model's parameters to meet a specific 
> target for the sum of positive errors over sum of the dependent variable 
> (minErr below).  

The sum of the dependent variable is a constant so can be ignored.

> I see two courses of action , 1) estimate a linear model then iteratively 
> reduce the regressors to achieve the desired positive error threshold 
> (naturally the regressors and predicted values are biased - but this is 
> acceptable).  Were the problem only a single independent variable problem, 
> this approach would be fairly trivial.  But the two variable model proves 
> more troublesome from an efficiency point of view (efficiency being the mean 
> negative error for a given minErr).
> 
> The second method, which seems more efficient, is to optimize the linear 
> regressors until the criteria is met.  There are several optimizer packages 
> out there, but none seem particularly well suited to this sort of task.  
> 
> Any suggestions out there?
> 
> ===========  Sample Problem = first approach  ================
> 
> # if we say... 
> B1 <- .8
> B2 <- .2
> 
> # then construct some data..
> x1 <- rnorm(100,mean=100,sd=20)
> x2 <- rnorm(100,mean=10,sd=2)
> y <- B1*x1+B2*x2+rnorm(100,mean=5,sd=10)
> # normally of course B1 and B2 are unknown to begin with
> 
> # linear model  (presumably yielding B1=.8 and B2=.2)
> m<-lm(y ~ x1+ x2)
> # test on summation of positive errors.
> e <- resid(m)
> minErr <- (sum(ifelse(e<0,0,e))/sum(y))-.03
> while (minErr>.03){
> 	Make some adjustment to B1 and B2
> 	minErr <- (sum(ifelse(e<0,0,e))/sum(y))-.03
> 	}
> 
> ======== Second Approach ===========
> 
> construct a function to minimize minErr with beginning values from 'm'.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From devshruti at hotmail.com  Fri Jun 11 11:26:00 2004
From: devshruti at hotmail.com (Devshruti Pahuja)
Date: Fri, 11 Jun 2004 02:26:00 -0700
Subject: [R] Regression query
Message-ID: <BAY9-DAV20HldBIBx3500004f20@hotmail.com>

Hi

I have a set of data with both quantitative and categorical predictors.
After scaling of response variable, i looked for multicollinearity (VIF
values)
among the predictors and removed the predictors who were hinding some of the
other significant
predictors. I'm curious to know whether the predictors (who are not
significant)
while doing simple 'lm' will be involved in interactions. How do i take into
account
interactions of those predictors whom i removed just on the basis of
multicollinearity ?

I'll appreciate if someone can throw some light on this matter and how to
use R to detect
the interactions effectively .

Thanks

Regards
Dev

------Final 'lm model'--------------------
> logmodelfull_minus_run_hr_walk_batting <- lm(log(salary) ~ hit+rbi + walk
+ obp + strike.out+free.agent.eligible+free.agent.1991+arbitr.elgible.)
> summary(logmodelfull_minus_run_hr_walk_batting)

Call:
lm(formula = log(salary) ~ hit + rbi + walk + obp + strike.out +
    free.agent.eligible + free.agent.1991 + arbitr.elgible.)

Residuals:
     Min       1Q   Median       3Q      Max
-2.41786 -0.28911 -0.02814  0.31890  1.49007

Coefficients:
                      Estimate Std. Error t value Pr(>|t|)
(Intercept)           5.340782   0.251218  21.260  < 2e-16 ***
hit                   0.004479   0.001158   3.867 0.000133 ***
rbi                   0.011102   0.002195   5.059 7.05e-07 ***
walk                  0.005421   0.002206   2.457 0.014533 *
obp                  -1.385584   0.824105  -1.681 0.093653 .
strike.out           -0.005399   0.001438  -3.755 0.000205 ***
free.agent.eligible1  1.611521   0.080657  19.980  < 2e-16 ***
free.agent.19911     -0.301243   0.103481  -2.911 0.003848 **
arbitr.elgible.1      1.293059   0.086696  14.915  < 2e-16 ***
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1

Residual standard error: 0.5351 on 328 degrees of freedom
Multiple R-Squared: 0.7981,     Adjusted R-squared: 0.7932
F-statistic: 162.1 on 8 and 328 DF,  p-value: < 2.2e-16

----------------------------------------------------------------------------
----------------------------------------------------


--------------with
interactions----------------------------------------------------------------
---------------------------

>
> summary(baseball.lgmodel_with_interactions_ALL_arbid)

Call:
lm(formula = log(salary) ~ hit + rbi + strike.out + free.agent.eligible +
    free.agent.1991 + arbitr.elgible. + hit * free.agent.1991 +
    hit * arbitr.elgible. + hit * rbi + rbi * free.agent.eligible +
    rbi * arbitr.elgible. + rbi * arbitr.1991 + hit * strike.out +
    strike.out * free.agent.eligible + strike.out * arbitr.elgible. +
    strike.out * run + strike.out * hr + hit * free.agent.eligible +
    free.agent.eligible * run + hit * free.agent.1991 + strike.out *
    free.agent.1991 + free.agent.1991 * batting + free.agent.1991 *
    obp + arbitr.elgible. * run + batting * double + obp * run +
    obp * hr + walk * stolen.base + hit * arbitr.1991 + free.agent.eligible
*
    double + arbitr.elgible. * double + strike.out * triple +
    triple * batting + triple * walk + triple * walk + hit *
    hr + rbi * hr + free.agent.eligible * hr + free.agent.1991 *
    hr + arbitr.elgible. * hr + hr * arbitr.1991 + hit * walk +
    free.agent.eligible * walk + walk * rbi + rbi * stolen.base +
    strike.out * stolen.base + stolen.base * batting + stolen.base *
    walk + stolen.base * rbi + stolen.base * walk + arbitr.elgible. *
    error)

Residuals:
     Min       1Q   Median       3Q      Max
-2.29352 -0.28287 -0.03748  0.29790  1.31590

Coefficients:
                                  Estimate Std. Error t value Pr(>|t|)
(Intercept)                      5.217e+00  3.467e-01  15.048  < 2e-16 ***
hit                              6.927e-03  6.226e-03   1.112 0.266889
rbi                              1.908e-02  1.150e-02   1.658 0.098350 .
strike.out                      -5.692e-03  4.586e-03  -1.241 0.215517
free.agent.eligible1             1.287e+00  2.259e-01   5.699 3.05e-08 ***
free.agent.19911                 3.828e-01  6.575e-01   0.582 0.560914
arbitr.elgible.1                 1.038e+00  2.195e-01   4.726 3.63e-06 ***
arbitr.19911                    -1.024e+00  4.392e-01  -2.331 0.020443 *
run                              4.932e-02  2.905e-02   1.698 0.090682 .
hr                              -1.093e-01  7.208e-02  -1.516 0.130543
batting                         -1.814e-01  2.558e+00  -0.071 0.943522
obp                             -1.375e+00  2.253e+00  -0.610 0.542099
double                          -5.259e-02  4.489e-02  -1.172 0.242349
walk                             1.395e-02  9.757e-03   1.430 0.153808
stolen.base                     -1.685e-02  4.299e-02  -0.392 0.695372
triple                          -1.367e-01  1.600e-01  -0.854 0.393807
error                           -4.097e-03  6.879e-03  -0.595 0.552007
hit:free.agent.19911             8.248e-04  4.611e-03   0.179 0.858174
hit:arbitr.elgible.1             4.873e-03  6.448e-03   0.756 0.450395
hit:rbi                         -1.382e-04  7.709e-05  -1.792 0.074184 .
rbi:free.agent.eligible1         5.352e-03  9.555e-03   0.560 0.575855
rbi:arbitr.elgible.1            -3.384e-03  1.136e-02  -0.298 0.766072
rbi:arbitr.19911                 3.596e-02  2.179e-02   1.650 0.100046
hit:strike.out                   5.480e-06  5.446e-05   0.101 0.919917
strike.out:free.agent.eligible1 -2.570e-03  4.282e-03  -0.600 0.548890
strike.out:arbitr.elgible.1     -9.703e-04  5.234e-03  -0.185 0.853068
strike.out:run                   1.685e-04  1.246e-04   1.352 0.177345
strike.out:hr                   -3.088e-04  2.277e-04  -1.356 0.176229
hit:free.agent.eligible1        -1.359e-03  6.224e-03  -0.218 0.827363
free.agent.eligible1:run         1.248e-02  9.109e-03   1.370 0.171917
strike.out:free.agent.19911     -1.851e-02  5.974e-03  -3.099 0.002140 **
free.agent.19911:batting         7.076e-01  6.200e+00   0.114 0.909215
free.agent.19911:obp            -1.421e+00  3.952e+00  -0.360 0.719394
arbitr.elgible.1:run            -8.541e-03  8.773e-03  -0.974 0.331100
batting:double                   2.346e-01  1.609e-01   1.458 0.145884
run:obp                         -1.825e-01  7.492e-02  -2.436 0.015462 *
hr:obp                           3.687e-01  2.116e-01   1.742 0.082608 .
walk:stolen.base                -6.789e-05  1.557e-04  -0.436 0.663083
hit:arbitr.19911                -5.835e-03  7.084e-03  -0.824 0.410808
free.agent.eligible1:double     -1.151e-02  1.663e-02  -0.692 0.489334
arbitr.elgible.1:double          2.169e-03  1.938e-02   0.112 0.910985
strike.out:triple               -8.106e-04  6.023e-04  -1.346 0.179475
batting:triple                   5.179e-01  5.599e-01   0.925 0.355841
walk:triple                      8.755e-04  9.262e-04   0.945 0.345349
hit:hr                          -3.320e-04  2.626e-04  -1.264 0.207180
rbi:hr                           4.748e-04  3.015e-04   1.575 0.116414
free.agent.eligible1:hr          1.840e-02  2.313e-02   0.796 0.426972
free.agent.19911:hr              7.216e-02  1.889e-02   3.819 0.000165 ***
arbitr.elgible.1:hr              4.111e-02  2.803e-02   1.467 0.143564
arbitr.19911:hr                 -2.368e-02  4.647e-02  -0.510 0.610723
hit:walk                         3.173e-05  7.826e-05   0.405 0.685442
free.agent.eligible1:walk       -5.423e-03  4.984e-03  -1.088 0.277472
rbi:walk                        -7.569e-05  1.313e-04  -0.577 0.564598
rbi:stolen.base                  3.980e-05  1.605e-04   0.248 0.804409
strike.out:stolen.base          -2.611e-04  1.615e-04  -1.617 0.107004
batting:stolen.base              1.552e-01  1.434e-01   1.082 0.280020
arbitr.elgible.1:error           3.930e-03  1.390e-02   0.283 0.777495
---
Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1

Residual standard error: 0.4925 on 280 degrees of freedom
Multiple R-Squared: 0.854,      Adjusted R-squared: 0.8248
F-statistic: 29.24 on 56 and 280 DF,  p-value: < 2.2e-16



From devshruti at hotmail.com  Fri Jun 11 11:35:16 2004
From: devshruti at hotmail.com (Devshruti Pahuja)
Date: Fri, 11 Jun 2004 02:35:16 -0700
Subject: [R] Regression query
Message-ID: <BAY9-DAV29yKNJJz4Oa00004d0c@hotmail.com>

Hi

I have a set of data with both quantitative and categorical predictors.
After scaling of response variable, i looked for multicollinearity (VIF
values) among the predictors and removed the predictors who were hinding
some of the
other significant predictors. I'm curious to know whether the predictors
(who are not significant) while doing simple 'lm' will be involved in
interactions. How do i take into
account  interactions of those predictors whom i removed just on the basis
of  multicollinearity ?

 I'll appreciate if someone can throw some light on this matter and how to
use R to detect the interactions effectively .

Thanks

 Regards
 Dev

> ------Final 'lm model'--------------------
> > logmodelfull_minus_run_hr_walk_batting <- lm(log(salary) ~ hit+rbi +
walk
> + obp + strike.out+free.agent.eligible+free.agent.1991+arbitr.elgible.)
> > summary(logmodelfull_minus_run_hr_walk_batting)
>
> Call:
> lm(formula = log(salary) ~ hit + rbi + walk + obp + strike.out +
>     free.agent.eligible + free.agent.1991 + arbitr.elgible.)
>
> Residuals:
>      Min       1Q   Median       3Q      Max
> -2.41786 -0.28911 -0.02814  0.31890  1.49007
>
> Coefficients:
>                       Estimate Std. Error t value Pr(>|t|)
> (Intercept)           5.340782   0.251218  21.260  < 2e-16 ***
> hit                   0.004479   0.001158   3.867 0.000133 ***
> rbi                   0.011102   0.002195   5.059 7.05e-07 ***
> walk                  0.005421   0.002206   2.457 0.014533 *
> obp                  -1.385584   0.824105  -1.681 0.093653 .
> strike.out           -0.005399   0.001438  -3.755 0.000205 ***
> free.agent.eligible1  1.611521   0.080657  19.980  < 2e-16 ***
> free.agent.19911     -0.301243   0.103481  -2.911 0.003848 **
> arbitr.elgible.1      1.293059   0.086696  14.915  < 2e-16 ***
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
>
> Residual standard error: 0.5351 on 328 degrees of freedom
> Multiple R-Squared: 0.7981,     Adjusted R-squared: 0.7932
> F-statistic: 162.1 on 8 and 328 DF,  p-value: < 2.2e-16
>
> --------------------------------------------------------------------------
--
> ----------------------------------------------------
>
>
> --------------with
>
interactions----------------------------------------------------------------
> ---------------------------
>
> >
> > summary(baseball.lgmodel_with_interactions_ALL_arbid)
>
> Call:
> lm(formula = log(salary) ~ hit + rbi + strike.out + free.agent.eligible +
>     free.agent.1991 + arbitr.elgible. + hit * free.agent.1991 +
>     hit * arbitr.elgible. + hit * rbi + rbi * free.agent.eligible +
>     rbi * arbitr.elgible. + rbi * arbitr.1991 + hit * strike.out +
>     strike.out * free.agent.eligible + strike.out * arbitr.elgible. +
>     strike.out * run + strike.out * hr + hit * free.agent.eligible +
>     free.agent.eligible * run + hit * free.agent.1991 + strike.out *
>     free.agent.1991 + free.agent.1991 * batting + free.agent.1991 *
>     obp + arbitr.elgible. * run + batting * double + obp * run +
>     obp * hr + walk * stolen.base + hit * arbitr.1991 +
free.agent.eligible
> *
>     double + arbitr.elgible. * double + strike.out * triple +
>     triple * batting + triple * walk + triple * walk + hit *
>     hr + rbi * hr + free.agent.eligible * hr + free.agent.1991 *
>     hr + arbitr.elgible. * hr + hr * arbitr.1991 + hit * walk +
>     free.agent.eligible * walk + walk * rbi + rbi * stolen.base +
>     strike.out * stolen.base + stolen.base * batting + stolen.base *
>     walk + stolen.base * rbi + stolen.base * walk + arbitr.elgible. *
>     error)
>
> Residuals:
>      Min       1Q   Median       3Q      Max
> -2.29352 -0.28287 -0.03748  0.29790  1.31590
>
> Coefficients:
>                                   Estimate Std. Error t value Pr(>|t|)
> (Intercept)                      5.217e+00  3.467e-01  15.048  < 2e-16 ***
> hit                              6.927e-03  6.226e-03   1.112 0.266889
> rbi                              1.908e-02  1.150e-02   1.658 0.098350 .
> strike.out                      -5.692e-03  4.586e-03  -1.241 0.215517
> free.agent.eligible1             1.287e+00  2.259e-01   5.699 3.05e-08 ***
> free.agent.19911                 3.828e-01  6.575e-01   0.582 0.560914
> arbitr.elgible.1                 1.038e+00  2.195e-01   4.726 3.63e-06 ***
> arbitr.19911                    -1.024e+00  4.392e-01  -2.331 0.020443 *
> run                              4.932e-02  2.905e-02   1.698 0.090682 .
> hr                              -1.093e-01  7.208e-02  -1.516 0.130543
> batting                         -1.814e-01  2.558e+00  -0.071 0.943522
> obp                             -1.375e+00  2.253e+00  -0.610 0.542099
> double                          -5.259e-02  4.489e-02  -1.172 0.242349
> walk                             1.395e-02  9.757e-03   1.430 0.153808
> stolen.base                     -1.685e-02  4.299e-02  -0.392 0.695372
> triple                          -1.367e-01  1.600e-01  -0.854 0.393807
> error                           -4.097e-03  6.879e-03  -0.595 0.552007
> hit:free.agent.19911             8.248e-04  4.611e-03   0.179 0.858174
> hit:arbitr.elgible.1             4.873e-03  6.448e-03   0.756 0.450395
> hit:rbi                         -1.382e-04  7.709e-05  -1.792 0.074184 .
> rbi:free.agent.eligible1         5.352e-03  9.555e-03   0.560 0.575855
> rbi:arbitr.elgible.1            -3.384e-03  1.136e-02  -0.298 0.766072
> rbi:arbitr.19911                 3.596e-02  2.179e-02   1.650 0.100046
> hit:strike.out                   5.480e-06  5.446e-05   0.101 0.919917
> strike.out:free.agent.eligible1 -2.570e-03  4.282e-03  -0.600 0.548890
> strike.out:arbitr.elgible.1     -9.703e-04  5.234e-03  -0.185 0.853068
> strike.out:run                   1.685e-04  1.246e-04   1.352 0.177345
> strike.out:hr                   -3.088e-04  2.277e-04  -1.356 0.176229
> hit:free.agent.eligible1        -1.359e-03  6.224e-03  -0.218 0.827363
> free.agent.eligible1:run         1.248e-02  9.109e-03   1.370 0.171917
> strike.out:free.agent.19911     -1.851e-02  5.974e-03  -3.099 0.002140 **
> free.agent.19911:batting         7.076e-01  6.200e+00   0.114 0.909215
> free.agent.19911:obp            -1.421e+00  3.952e+00  -0.360 0.719394
> arbitr.elgible.1:run            -8.541e-03  8.773e-03  -0.974 0.331100
> batting:double                   2.346e-01  1.609e-01   1.458 0.145884
> run:obp                         -1.825e-01  7.492e-02  -2.436 0.015462 *
> hr:obp                           3.687e-01  2.116e-01   1.742 0.082608 .
> walk:stolen.base                -6.789e-05  1.557e-04  -0.436 0.663083
> hit:arbitr.19911                -5.835e-03  7.084e-03  -0.824 0.410808
> free.agent.eligible1:double     -1.151e-02  1.663e-02  -0.692 0.489334
> arbitr.elgible.1:double          2.169e-03  1.938e-02   0.112 0.910985
> strike.out:triple               -8.106e-04  6.023e-04  -1.346 0.179475
> batting:triple                   5.179e-01  5.599e-01   0.925 0.355841
> walk:triple                      8.755e-04  9.262e-04   0.945 0.345349
> hit:hr                          -3.320e-04  2.626e-04  -1.264 0.207180
> rbi:hr                           4.748e-04  3.015e-04   1.575 0.116414
> free.agent.eligible1:hr          1.840e-02  2.313e-02   0.796 0.426972
> free.agent.19911:hr              7.216e-02  1.889e-02   3.819 0.000165 ***
> arbitr.elgible.1:hr              4.111e-02  2.803e-02   1.467 0.143564
> arbitr.19911:hr                 -2.368e-02  4.647e-02  -0.510 0.610723
> hit:walk                         3.173e-05  7.826e-05   0.405 0.685442
> free.agent.eligible1:walk       -5.423e-03  4.984e-03  -1.088 0.277472
> rbi:walk                        -7.569e-05  1.313e-04  -0.577 0.564598
> rbi:stolen.base                  3.980e-05  1.605e-04   0.248 0.804409
> strike.out:stolen.base          -2.611e-04  1.615e-04  -1.617 0.107004
> batting:stolen.base              1.552e-01  1.434e-01   1.082 0.280020
> arbitr.elgible.1:error           3.930e-03  1.390e-02   0.283 0.777495
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
>
> Residual standard error: 0.4925 on 280 degrees of freedom
> Multiple R-Squared: 0.854,      Adjusted R-squared: 0.8248
> F-statistic: 29.24 on 56 and 280 DF,  p-value: < 2.2e-16
>



From devshruti at hotmail.com  Fri Jun 11 19:41:00 2004
From: devshruti at hotmail.com (Devshruti Pahuja)
Date: Fri, 11 Jun 2004 10:41:00 -0700
Subject: [R] Regression query : steps for model building
Message-ID: <BAY9-DAV86R3SVwGyjH00006c2b@hotmail.com>

Hi

I have a set of data with both quantitative and categorical predictors.
After scaling of response variable, i looked for multicollinearity (VIF
values) among the predictors and removed the predictors who were hinding
some of the
other significant predictors. I'm curious to know whether the predictors
(who are not significant) while doing simple 'lm' will be involved in
interactions. How do i take into
account  interactions of those predictors whom i removed just on the basis
of  multicollinearity ?

 I'll appreciate if someone can throw some light on this matter and how to
use R to detect the interactions effectively .

Thanks

 Regards
 Dev

> ------Final 'lm model'--------------------
> > logmodelfull_minus_run_hr_walk_batting <- lm(log(salary) ~ hit+rbi +
walk
> + obp + strike.out+free.agent.eligible+free.agent.1991+arbitr.elgible.)
> > summary(logmodelfull_minus_run_hr_walk_batting)
>
> Call:
> lm(formula = log(salary) ~ hit + rbi + walk + obp + strike.out +
>     free.agent.eligible + free.agent.1991 + arbitr.elgible.)
>
> Residuals:
>      Min       1Q   Median       3Q      Max
> -2.41786 -0.28911 -0.02814  0.31890  1.49007
>
> Coefficients:
>                       Estimate Std. Error t value Pr(>|t|)
> (Intercept)           5.340782   0.251218  21.260  < 2e-16 ***
> hit                   0.004479   0.001158   3.867 0.000133 ***
> rbi                   0.011102   0.002195   5.059 7.05e-07 ***
> walk                  0.005421   0.002206   2.457 0.014533 *
> obp                  -1.385584   0.824105  -1.681 0.093653 .
> strike.out           -0.005399   0.001438  -3.755 0.000205 ***
> free.agent.eligible1  1.611521   0.080657  19.980  < 2e-16 ***
> free.agent.19911     -0.301243   0.103481  -2.911 0.003848 **
> arbitr.elgible.1      1.293059   0.086696  14.915  < 2e-16 ***
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
>
> Residual standard error: 0.5351 on 328 degrees of freedom
> Multiple R-Squared: 0.7981,     Adjusted R-squared: 0.7932
> F-statistic: 162.1 on 8 and 328 DF,  p-value: < 2.2e-16
>
> --------------------------------------------------------------------------
--
> ----------------------------------------------------
>
>
> --------------with
>
interactions----------------------------------------------------------------
> ---------------------------
>
> >
> > summary(baseball.lgmodel_with_interactions_ALL_arbid)
>
> Call:
> lm(formula = log(salary) ~ hit + rbi + strike.out + free.agent.eligible +
>     free.agent.1991 + arbitr.elgible. + hit * free.agent.1991 +
>     hit * arbitr.elgible. + hit * rbi + rbi * free.agent.eligible +
>     rbi * arbitr.elgible. + rbi * arbitr.1991 + hit * strike.out +
>     strike.out * free.agent.eligible + strike.out * arbitr.elgible. +
>     strike.out * run + strike.out * hr + hit * free.agent.eligible +
>     free.agent.eligible * run + hit * free.agent.1991 + strike.out *
>     free.agent.1991 + free.agent.1991 * batting + free.agent.1991 *
>     obp + arbitr.elgible. * run + batting * double + obp * run +
>     obp * hr + walk * stolen.base + hit * arbitr.1991 +
free.agent.eligible
> *
>     double + arbitr.elgible. * double + strike.out * triple +
>     triple * batting + triple * walk + triple * walk + hit *
>     hr + rbi * hr + free.agent.eligible * hr + free.agent.1991 *
>     hr + arbitr.elgible. * hr + hr * arbitr.1991 + hit * walk +
>     free.agent.eligible * walk + walk * rbi + rbi * stolen.base +
>     strike.out * stolen.base + stolen.base * batting + stolen.base *
>     walk + stolen.base * rbi + stolen.base * walk + arbitr.elgible. *
>     error)
>
> Residuals:
>      Min       1Q   Median       3Q      Max
> -2.29352 -0.28287 -0.03748  0.29790  1.31590
>
> Coefficients:
>                                   Estimate Std. Error t value Pr(>|t|)
> (Intercept)                      5.217e+00  3.467e-01  15.048  < 2e-16 ***
> hit                              6.927e-03  6.226e-03   1.112 0.266889
> rbi                              1.908e-02  1.150e-02   1.658 0.098350 .
> strike.out                      -5.692e-03  4.586e-03  -1.241 0.215517
> free.agent.eligible1             1.287e+00  2.259e-01   5.699 3.05e-08 ***
> free.agent.19911                 3.828e-01  6.575e-01   0.582 0.560914
> arbitr.elgible.1                 1.038e+00  2.195e-01   4.726 3.63e-06 ***
> arbitr.19911                    -1.024e+00  4.392e-01  -2.331 0.020443 *
> run                              4.932e-02  2.905e-02   1.698 0.090682 .
> hr                              -1.093e-01  7.208e-02  -1.516 0.130543
> batting                         -1.814e-01  2.558e+00  -0.071 0.943522
> obp                             -1.375e+00  2.253e+00  -0.610 0.542099
> double                          -5.259e-02  4.489e-02  -1.172 0.242349
> walk                             1.395e-02  9.757e-03   1.430 0.153808
> stolen.base                     -1.685e-02  4.299e-02  -0.392 0.695372
> triple                          -1.367e-01  1.600e-01  -0.854 0.393807
> error                           -4.097e-03  6.879e-03  -0.595 0.552007
> hit:free.agent.19911             8.248e-04  4.611e-03   0.179 0.858174
> hit:arbitr.elgible.1             4.873e-03  6.448e-03   0.756 0.450395
> hit:rbi                         -1.382e-04  7.709e-05  -1.792 0.074184 .
> rbi:free.agent.eligible1         5.352e-03  9.555e-03   0.560 0.575855
> rbi:arbitr.elgible.1            -3.384e-03  1.136e-02  -0.298 0.766072
> rbi:arbitr.19911                 3.596e-02  2.179e-02   1.650 0.100046
> hit:strike.out                   5.480e-06  5.446e-05   0.101 0.919917
> strike.out:free.agent.eligible1 -2.570e-03  4.282e-03  -0.600 0.548890
> strike.out:arbitr.elgible.1     -9.703e-04  5.234e-03  -0.185 0.853068
> strike.out:run                   1.685e-04  1.246e-04   1.352 0.177345
> strike.out:hr                   -3.088e-04  2.277e-04  -1.356 0.176229
> hit:free.agent.eligible1        -1.359e-03  6.224e-03  -0.218 0.827363
> free.agent.eligible1:run         1.248e-02  9.109e-03   1.370 0.171917
> strike.out:free.agent.19911     -1.851e-02  5.974e-03  -3.099 0.002140 **
> free.agent.19911:batting         7.076e-01  6.200e+00   0.114 0.909215
> free.agent.19911:obp            -1.421e+00  3.952e+00  -0.360 0.719394
> arbitr.elgible.1:run            -8.541e-03  8.773e-03  -0.974 0.331100
> batting:double                   2.346e-01  1.609e-01   1.458 0.145884
> run:obp                         -1.825e-01  7.492e-02  -2.436 0.015462 *
> hr:obp                           3.687e-01  2.116e-01   1.742 0.082608 .
> walk:stolen.base                -6.789e-05  1.557e-04  -0.436 0.663083
> hit:arbitr.19911                -5.835e-03  7.084e-03  -0.824 0.410808
> free.agent.eligible1:double     -1.151e-02  1.663e-02  -0.692 0.489334
> arbitr.elgible.1:double          2.169e-03  1.938e-02   0.112 0.910985
> strike.out:triple               -8.106e-04  6.023e-04  -1.346 0.179475
> batting:triple                   5.179e-01  5.599e-01   0.925 0.355841
> walk:triple                      8.755e-04  9.262e-04   0.945 0.345349
> hit:hr                          -3.320e-04  2.626e-04  -1.264 0.207180
> rbi:hr                           4.748e-04  3.015e-04   1.575 0.116414
> free.agent.eligible1:hr          1.840e-02  2.313e-02   0.796 0.426972
> free.agent.19911:hr              7.216e-02  1.889e-02   3.819 0.000165 ***
> arbitr.elgible.1:hr              4.111e-02  2.803e-02   1.467 0.143564
> arbitr.19911:hr                 -2.368e-02  4.647e-02  -0.510 0.610723
> hit:walk                         3.173e-05  7.826e-05   0.405 0.685442
> free.agent.eligible1:walk       -5.423e-03  4.984e-03  -1.088 0.277472
> rbi:walk                        -7.569e-05  1.313e-04  -0.577 0.564598
> rbi:stolen.base                  3.980e-05  1.605e-04   0.248 0.804409
> strike.out:stolen.base          -2.611e-04  1.615e-04  -1.617 0.107004
> batting:stolen.base              1.552e-01  1.434e-01   1.082 0.280020
> arbitr.elgible.1:error           3.930e-03  1.390e-02   0.283 0.777495
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
>
> Residual standard error: 0.4925 on 280 degrees of freedom
> Multiple R-Squared: 0.854,      Adjusted R-squared: 0.8248
> F-statistic: 29.24 on 56 and 280 DF,  p-value: < 2.2e-16
>



From devshruti at hotmail.com  Fri Jun 11 12:08:16 2004
From: devshruti at hotmail.com (Devshruti Pahuja)
Date: Fri, 11 Jun 2004 03:08:16 -0700
Subject: [R] Regression query
Message-ID: <BAY9-DAV107p8bVPZu4000050a1@hotmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040611/865cd5e2/attachment.pl

From andy_liaw at merck.com  Sun Jun 13 03:37:24 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 12 Jun 2004 21:37:24 -0400
Subject: [R] Regression query : steps for model building
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7EA4@usrymx25.merck.com>

Without going through the copious output, I'll offer the following:

1. What to do will depend a lot on what you want to do with the model you
get in the end.  If you are going to make statistical inference based on the
model, you'll need to be a lot more careful on how you get to that model.

2. If you have some variables that are highly correlated
(multicollinearity), you won't have much of a chance in finding interaction
among them.  Exact collinearity is the same as confounding, which means you
can't even tell the main effects apart, let alone interaction.

I'd suggest you read Prof. Harrell's "Regression Modelling Strategies",
published by Springer.

Best,
Andy

> From: Devshruti Pahuja
> 
> Hi
> 
> I have a set of data with both quantitative and categorical 
> predictors.
> After scaling of response variable, i looked for 
> multicollinearity (VIF
> values) among the predictors and removed the predictors who 
> were hinding
> some of the
> other significant predictors. I'm curious to know whether the 
> predictors
> (who are not significant) while doing simple 'lm' will be involved in
> interactions. How do i take into
> account  interactions of those predictors whom i removed just 
> on the basis
> of  multicollinearity ?
> 
>  I'll appreciate if someone can throw some light on this 
> matter and how to
> use R to detect the interactions effectively .
> 
> Thanks
> 
>  Regards
>  Dev
> 
> > ------Final 'lm model'--------------------
> > > logmodelfull_minus_run_hr_walk_batting <- lm(log(salary) 
> ~ hit+rbi +
> walk
> > + obp + 
> strike.out+free.agent.eligible+free.agent.1991+arbitr.elgible.)
> > > summary(logmodelfull_minus_run_hr_walk_batting)
> >
> > Call:
> > lm(formula = log(salary) ~ hit + rbi + walk + obp + strike.out +
> >     free.agent.eligible + free.agent.1991 + arbitr.elgible.)
> >
> > Residuals:
> >      Min       1Q   Median       3Q      Max
> > -2.41786 -0.28911 -0.02814  0.31890  1.49007
> >
> > Coefficients:
> >                       Estimate Std. Error t value Pr(>|t|)
> > (Intercept)           5.340782   0.251218  21.260  < 2e-16 ***
> > hit                   0.004479   0.001158   3.867 0.000133 ***
> > rbi                   0.011102   0.002195   5.059 7.05e-07 ***
> > walk                  0.005421   0.002206   2.457 0.014533 *
> > obp                  -1.385584   0.824105  -1.681 0.093653 .
> > strike.out           -0.005399   0.001438  -3.755 0.000205 ***
> > free.agent.eligible1  1.611521   0.080657  19.980  < 2e-16 ***
> > free.agent.19911     -0.301243   0.103481  -2.911 0.003848 **
> > arbitr.elgible.1      1.293059   0.086696  14.915  < 2e-16 ***
> > ---
> > Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
> >
> > Residual standard error: 0.5351 on 328 degrees of freedom
> > Multiple R-Squared: 0.7981,     Adjusted R-squared: 0.7932
> > F-statistic: 162.1 on 8 and 328 DF,  p-value: < 2.2e-16
> >
> > 
> --------------------------------------------------------------
> ------------
> --
> > ----------------------------------------------------
> >
> >
> > --------------with
> >
> interactions--------------------------------------------------
> --------------
> > ---------------------------
> >
> > >
> > > summary(baseball.lgmodel_with_interactions_ALL_arbid)
> >
> > Call:
> > lm(formula = log(salary) ~ hit + rbi + strike.out + 
> free.agent.eligible +
> >     free.agent.1991 + arbitr.elgible. + hit * free.agent.1991 +
> >     hit * arbitr.elgible. + hit * rbi + rbi * free.agent.eligible +
> >     rbi * arbitr.elgible. + rbi * arbitr.1991 + hit * strike.out +
> >     strike.out * free.agent.eligible + strike.out * 
> arbitr.elgible. +
> >     strike.out * run + strike.out * hr + hit * free.agent.eligible +
> >     free.agent.eligible * run + hit * free.agent.1991 + strike.out *
> >     free.agent.1991 + free.agent.1991 * batting + free.agent.1991 *
> >     obp + arbitr.elgible. * run + batting * double + obp * run +
> >     obp * hr + walk * stolen.base + hit * arbitr.1991 +
> free.agent.eligible
> > *
> >     double + arbitr.elgible. * double + strike.out * triple +
> >     triple * batting + triple * walk + triple * walk + hit *
> >     hr + rbi * hr + free.agent.eligible * hr + free.agent.1991 *
> >     hr + arbitr.elgible. * hr + hr * arbitr.1991 + hit * walk +
> >     free.agent.eligible * walk + walk * rbi + rbi * stolen.base +
> >     strike.out * stolen.base + stolen.base * batting + stolen.base *
> >     walk + stolen.base * rbi + stolen.base * walk + 
> arbitr.elgible. *
> >     error)
> >
> > Residuals:
> >      Min       1Q   Median       3Q      Max
> > -2.29352 -0.28287 -0.03748  0.29790  1.31590
> >
> > Coefficients:
> >                                   Estimate Std. Error t 
> value Pr(>|t|)
> > (Intercept)                      5.217e+00  3.467e-01  
> 15.048  < 2e-16 ***
> > hit                              6.927e-03  6.226e-03   
> 1.112 0.266889
> > rbi                              1.908e-02  1.150e-02   
> 1.658 0.098350 .
> > strike.out                      -5.692e-03  4.586e-03  
> -1.241 0.215517
> > free.agent.eligible1             1.287e+00  2.259e-01   
> 5.699 3.05e-08 ***
> > free.agent.19911                 3.828e-01  6.575e-01   
> 0.582 0.560914
> > arbitr.elgible.1                 1.038e+00  2.195e-01   
> 4.726 3.63e-06 ***
> > arbitr.19911                    -1.024e+00  4.392e-01  
> -2.331 0.020443 *
> > run                              4.932e-02  2.905e-02   
> 1.698 0.090682 .
> > hr                              -1.093e-01  7.208e-02  
> -1.516 0.130543
> > batting                         -1.814e-01  2.558e+00  
> -0.071 0.943522
> > obp                             -1.375e+00  2.253e+00  
> -0.610 0.542099
> > double                          -5.259e-02  4.489e-02  
> -1.172 0.242349
> > walk                             1.395e-02  9.757e-03   
> 1.430 0.153808
> > stolen.base                     -1.685e-02  4.299e-02  
> -0.392 0.695372
> > triple                          -1.367e-01  1.600e-01  
> -0.854 0.393807
> > error                           -4.097e-03  6.879e-03  
> -0.595 0.552007
> > hit:free.agent.19911             8.248e-04  4.611e-03   
> 0.179 0.858174
> > hit:arbitr.elgible.1             4.873e-03  6.448e-03   
> 0.756 0.450395
> > hit:rbi                         -1.382e-04  7.709e-05  
> -1.792 0.074184 .
> > rbi:free.agent.eligible1         5.352e-03  9.555e-03   
> 0.560 0.575855
> > rbi:arbitr.elgible.1            -3.384e-03  1.136e-02  
> -0.298 0.766072
> > rbi:arbitr.19911                 3.596e-02  2.179e-02   
> 1.650 0.100046
> > hit:strike.out                   5.480e-06  5.446e-05   
> 0.101 0.919917
> > strike.out:free.agent.eligible1 -2.570e-03  4.282e-03  
> -0.600 0.548890
> > strike.out:arbitr.elgible.1     -9.703e-04  5.234e-03  
> -0.185 0.853068
> > strike.out:run                   1.685e-04  1.246e-04   
> 1.352 0.177345
> > strike.out:hr                   -3.088e-04  2.277e-04  
> -1.356 0.176229
> > hit:free.agent.eligible1        -1.359e-03  6.224e-03  
> -0.218 0.827363
> > free.agent.eligible1:run         1.248e-02  9.109e-03   
> 1.370 0.171917
> > strike.out:free.agent.19911     -1.851e-02  5.974e-03  
> -3.099 0.002140 **
> > free.agent.19911:batting         7.076e-01  6.200e+00   
> 0.114 0.909215
> > free.agent.19911:obp            -1.421e+00  3.952e+00  
> -0.360 0.719394
> > arbitr.elgible.1:run            -8.541e-03  8.773e-03  
> -0.974 0.331100
> > batting:double                   2.346e-01  1.609e-01   
> 1.458 0.145884
> > run:obp                         -1.825e-01  7.492e-02  
> -2.436 0.015462 *
> > hr:obp                           3.687e-01  2.116e-01   
> 1.742 0.082608 .
> > walk:stolen.base                -6.789e-05  1.557e-04  
> -0.436 0.663083
> > hit:arbitr.19911                -5.835e-03  7.084e-03  
> -0.824 0.410808
> > free.agent.eligible1:double     -1.151e-02  1.663e-02  
> -0.692 0.489334
> > arbitr.elgible.1:double          2.169e-03  1.938e-02   
> 0.112 0.910985
> > strike.out:triple               -8.106e-04  6.023e-04  
> -1.346 0.179475
> > batting:triple                   5.179e-01  5.599e-01   
> 0.925 0.355841
> > walk:triple                      8.755e-04  9.262e-04   
> 0.945 0.345349
> > hit:hr                          -3.320e-04  2.626e-04  
> -1.264 0.207180
> > rbi:hr                           4.748e-04  3.015e-04   
> 1.575 0.116414
> > free.agent.eligible1:hr          1.840e-02  2.313e-02   
> 0.796 0.426972
> > free.agent.19911:hr              7.216e-02  1.889e-02   
> 3.819 0.000165 ***
> > arbitr.elgible.1:hr              4.111e-02  2.803e-02   
> 1.467 0.143564
> > arbitr.19911:hr                 -2.368e-02  4.647e-02  
> -0.510 0.610723
> > hit:walk                         3.173e-05  7.826e-05   
> 0.405 0.685442
> > free.agent.eligible1:walk       -5.423e-03  4.984e-03  
> -1.088 0.277472
> > rbi:walk                        -7.569e-05  1.313e-04  
> -0.577 0.564598
> > rbi:stolen.base                  3.980e-05  1.605e-04   
> 0.248 0.804409
> > strike.out:stolen.base          -2.611e-04  1.615e-04  
> -1.617 0.107004
> > batting:stolen.base              1.552e-01  1.434e-01   
> 1.082 0.280020
> > arbitr.elgible.1:error           3.930e-03  1.390e-02   
> 0.283 0.777495
> > ---
> > Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
> >
> > Residual standard error: 0.4925 on 280 degrees of freedom
> > Multiple R-Squared: 0.854,      Adjusted R-squared: 0.8248
> > F-statistic: 29.24 on 56 and 280 DF,  p-value: < 2.2e-16
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From reybasura at hotmail.com  Mon Jun  7 15:16:23 2004
From: reybasura at hotmail.com (Edoardo Stacul)
Date: Mon, 07 Jun 2004 13:16:23 +0000
Subject: [R] (no subject)
Message-ID: <BAY22-F14BjbUf9Sp2W00060e2c@hotmail.com>

Hello, I've begun to use R only recently.
I've a problem: I can't open framework already saved about PCA. In 
particular I can't edit all the information because I've to export the 
graphics in order to publish them. How can I do to have a list of the 
matrix, statistical elaboration and graphics? Which function shall I use?
thanks in advance,
Edoardo



From rafan at infor.org  Sun Jun 13 06:23:49 2004
From: rafan at infor.org (Rong-En Fan)
Date: Sun, 13 Jun 2004 12:23:49 +0800
Subject: [R] e1071, R1.9.0, Solaris 2.9, should I be worried?
In-Reply-To: <200405250438.i4P4cicw034303@atlas.otago.ac.nz>
References: <200405250438.i4P4cicw034303@atlas.otago.ac.nz>
Message-ID: <20040613042349.GA72010@muse.csie.ntu.edu.tw>

On Tue, May 25, 2004 at 04:38:44PM +1200, Richard A. O'Keefe wrote:
> In R 1.9.0 running under Solaris 2.9 on a SunBlade 100,
> with "Sun WorkShop 6 update 2 C++ 5.3 2001/05/15" as the
> C++ compiler, I just did

> 
> How worried should I be?
> I guess the "Warning: x hides Solver::x" warnings related to a deliberate
> style choice, but what about the "String literal converted to char*" ones?

as the message says, things like

info("Exceeds max_iter in multiclass_prob\n");

and the info() accepts char * as parameter



From ajayshah at mayin.org  Sun Jun 13 08:24:49 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Sun, 13 Jun 2004 11:54:49 +0530
Subject: [R] ordered probit or logit / recursive regression
In-Reply-To: <Pine.LNX.4.44.0406122007190.4353-100000@gannet.stats>
References: <Pine.A41.4.58.0406120853050.81062@homer03.u.washington.edu>
	<Pine.LNX.4.44.0406122007190.4353-100000@gannet.stats>
Message-ID: <20040613062449.GX9596@igidr.ac.in>

On Sat, Jun 12, 2004 at 08:11:20PM +0100, Prof Brian Ripley wrote:
> On Sat, 12 Jun 2004, Thomas Lumley wrote:
> 
> > On Sat, 12 Jun 2004, Ajay Shah wrote:
> > 
> > > R is very fancy. You won't get mundane things like ordered probit off
> > > the shelf. 
> 
> But you do (and if you have tried to write them you will discover they are
> not `mundane' to get right).  Two people have already answered for the
> ordered logit, and if you look in the list archives you will find (well,
> you should find) a modification of polr to popr (which will appear in the
> next release of the VR bundle).

I was kidding. I love R. I have been waiting for R for one-third of my
life. I admire R and the R community. I am steadily switching all my
work to R, and my goal is to go to 100% R.

I know the ordered probit model. The ordered probit model is a dear
friend of mine. I have written the ordered probit model in fortran and
in C. I'm quite aware of the difficulties in involved in writing
it:
   We need to estimate tau, the vector of cutoffs, but we can't search
     in R^n since the cutoffs might fall out of sorted order, so we
     use terms like log(tau[2]-tau[1]) as the free parameters in doing
     an unrestricted search.
  Then comes either the delta method or simulations to recover the
     distribution of the untransformed parameters from the MVN
     distribution of the transformed parameters used in estimation.
  And, there are very interesting logic puzzles in identification.

Some years ago, I used to think that clarity on identification and
estimation of the ordered probit model was one of the coolest things
in my brain.

I was trying to be funny and say that "R is so cool; you don't have a
vanilla ordered probit, but you have markov chain monte carlo
inference for an ordered probit". Now _that_ is very fancy (atleast,
in my book). I've lusted after markov chain monte carlo many times,
but never quite done it. Is there a child's guide to MCMC on the net
that I can consume?

> > (I will be very happy if someone will show how to use glm()
> > > to do a vanilla probit!)
> > 
> Quoth Thomas Lumley:
> > glm(y~x+z, family=binomial(probit))
> > Be happy,
> Be even happier that ordered probit also is available.

:-) Thanks! (I haven't seen popr yet).

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From wuertz at itp.phys.ethz.ch  Sun Jun 13 09:32:35 2004
From: wuertz at itp.phys.ethz.ch (Diethelm Wuertz)
Date: Sun, 13 Jun 2004 07:32:35 +0000
Subject: [R] Rmetrics - New Built 190.10055
Message-ID: <40CC0313.2090009@itp.phys.ethz.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040613/8e7b3556/attachment.pl

From tore.wentzel-larsen at helse-bergen.no  Sun Jun 13 14:43:07 2004
From: tore.wentzel-larsen at helse-bergen.no (Tore Wentzel-Larsen)
Date: Sun, 13 Jun 2004 14:43:07 +0200
Subject: [R] Abbreviated citation of R?
Message-ID: <7132663E78AE3E45AD4471CBC2738EF51FD66D@EC3.ihelse.net>

Dear R users,

This is more a suggestion than a question.

Short version: A possibility for a short, 'inline', way to cite R in an article, 
comparable to e. g. 'SPSS Inc., Chicago, USA' or 'Stata Corporation, College 
Station, Texas, USA', without a corresponding item in the list of references,
might be an advantage in some cases.

Long version: Increasingly, it seems, journals are setting strict limits to the
number of references in scientific papers. These requirements by editors wanting 
to maximize the number of published articles in a limited space may be understandable,
but they make it difficult for authors to pay due credit to the papers and other 
recources on which the article in question is based. A solution that is used to
some extent for e. g. software and  measurement instruments is a shortened,
'inline' citation (see examples above) without a corresponding item in the
list of references at the end of the article. The possibility for a standardized
and approved way to make such a reference to R would be helpful for authors facing 
this kind of requirements from journal editors.

I have read the posting guide and R FAQ, typed citation() in R and tried 
Google and R Site Search, but did not find any discussion on this matter.

Sincerely,
Tore Wentzel-Larsen

------------------------------------------
Tore Wentzel-Larsen
statistician
Centre for Clinical research
Armauer Hansen house
Haukeland University Hospital
N-5021 Bergen
tlf   +47 55 97 55 39 (a)
faks  +47 55 97 60 88 (a)
email tore.wentzel-larsen at helse-bergen.no



From rkoenker at uiuc.edu  Sun Jun 13 07:19:12 2004
From: rkoenker at uiuc.edu (roger koenker)
Date: Sun, 13 Jun 2004 00:19:12 -0500
Subject: [R] optimize linear function
In-Reply-To: <Pine.LNX.4.44.0406122231010.4995-100000@gannet.stats>
References: <Pine.LNX.4.44.0406122231010.4995-100000@gannet.stats>
Message-ID: <34EAF26A-BCF9-11D8-928C-000393A361A2@uiuc.edu>


On Jun 12, 2004, at 4:35 PM, Prof Brian Ripley wrote:

> This is just a linear programming problem.  So the packages which do
> linear programming are `particularly well suited to this sort of task'
> and theory tells you a lot about the solution.
>
Indeed.  With the package  quantreg, for example, you could do:

	rq(y ~ x1 + x2, tau = eps)

for any small eps.    Note that in the example sum(y) and the additive
factor .03 aren't really germane.  If the problem is large you might 
want to
add method = "fn" to the call to use interior point optimization rather 
than
simplex (exterior point) methods).

url:	www.econ.uiuc.edu/~roger        	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820

>
> On Sat, 12 Jun 2004, Michaell Taylor wrote:
>
>> I am attempting to optimize a regression model's parameters to meet a 
>> specific
>> target for the sum of positive errors over sum of the dependent 
>> variable
>> (minErr below).
>
>>
>> ===========  Sample Problem = first approach  ================
>> # linear model  (presumably yielding B1=.8 and B2=.2)
[....]
>> m<-lm(y ~ x1+ x2)
>> # test on summation of positive errors.
>> e <- resid(m)
>> minErr <- (sum(ifelse(e<0,0,e))/sum(y))-.03
>>



From flom at ndri.org  Sun Jun 13 17:03:12 2004
From: flom at ndri.org (Peter Flom)
Date: Sun, 13 Jun 2004 11:03:12 -0400
Subject: [R] Regression query
Message-ID: <s0cc3489.097@MAIL.NDRI.ORG>

If variables are colinear, then looking at interactions among them
doesn't make much sense.  High collinearity means that one variable is
nearly a linear combination of others.  IOW, that variable is not adding
much information.  So, if you look at the interaction, you are ALMOST
looking at a quadratic (e.g., if the collinearity involves only 2
variables, then one is very similar to the other, so X1*X2 is almost
X1*X1).  The output will be confusing, to say the least. 

Worse, when you include collinear variables, the resulting equation is
highly sensitive to small (sometimes very small) changes in the data. 
Belsley gives an example where changes in the third decimal place result
in totally different equations.

For details see Belsley's book titled something like "collinearity and
weak data in regression" (sorry, the book and my files are at the
office, but this should let you find it

HTH

Peter L. Flom, PhD
Assistant Director, Statistics and Data Analysis Core
Center for Drug Use and HIV Research
National Development and Research Institutes
71 W. 23rd St
www.peterflom.com
New York, NY 10010
(212) 845-4485 (voice)
(917) 438-0894 (fax)


>>> "Devshruti Pahuja" <devshruti at hotmail.com> 06/11/04 5:35 AM >>>
Hi

I have a set of data with both quantitative and categorical predictors.
After scaling of response variable, i looked for multicollinearity (VIF
values) among the predictors and removed the predictors who were hinding
some of the
other significant predictors. I'm curious to know whether the predictors
(who are not significant) while doing simple 'lm' will be involved in
interactions. How do i take into
account  interactions of those predictors whom i removed just on the
basis
of  multicollinearity ?

 I'll appreciate if someone can throw some light on this matter and how
to
use R to detect the interactions effectively .

Thanks

 Regards
 Dev

> ------Final 'lm model'--------------------
> > logmodelfull_minus_run_hr_walk_batting <- lm(log(salary) ~ hit+rbi +
walk
> + obp +
strike.out+free.agent.eligible+free.agent.1991+arbitr.elgible.)
> > summary(logmodelfull_minus_run_hr_walk_batting)
>
> Call:
> lm(formula = log(salary) ~ hit + rbi + walk + obp + strike.out +
>     free.agent.eligible + free.agent.1991 + arbitr.elgible.)
>
> Residuals:
>      Min       1Q   Median       3Q      Max
> -2.41786 -0.28911 -0.02814  0.31890  1.49007
>
> Coefficients:
>                       Estimate Std. Error t value Pr(>|t|)
> (Intercept)           5.340782   0.251218  21.260  < 2e-16 ***
> hit                   0.004479   0.001158   3.867 0.000133 ***
> rbi                   0.011102   0.002195   5.059 7.05e-07 ***
> walk                  0.005421   0.002206   2.457 0.014533 *
> obp                  -1.385584   0.824105  -1.681 0.093653 .
> strike.out           -0.005399   0.001438  -3.755 0.000205 ***
> free.agent.eligible1  1.611521   0.080657  19.980  < 2e-16 ***
> free.agent.19911     -0.301243   0.103481  -2.911 0.003848 **
> arbitr.elgible.1      1.293059   0.086696  14.915  < 2e-16 ***
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
>
> Residual standard error: 0.5351 on 328 degrees of freedom
> Multiple R-Squared: 0.7981,     Adjusted R-squared: 0.7932
> F-statistic: 162.1 on 8 and 328 DF,  p-value: < 2.2e-16
>
>
--------------------------------------------------------------------------
--
> ----------------------------------------------------
>
>
> --------------with
>
interactions----------------------------------------------------------------
> ---------------------------
>
> >
> > summary(baseball.lgmodel_with_interactions_ALL_arbid)
>
> Call:
> lm(formula = log(salary) ~ hit + rbi + strike.out +
free.agent.eligible +
>     free.agent.1991 + arbitr.elgible. + hit * free.agent.1991 +
>     hit * arbitr.elgible. + hit * rbi + rbi * free.agent.eligible +
>     rbi * arbitr.elgible. + rbi * arbitr.1991 + hit * strike.out +
>     strike.out * free.agent.eligible + strike.out * arbitr.elgible. +
>     strike.out * run + strike.out * hr + hit * free.agent.eligible +
>     free.agent.eligible * run + hit * free.agent.1991 + strike.out *
>     free.agent.1991 + free.agent.1991 * batting + free.agent.1991 *
>     obp + arbitr.elgible. * run + batting * double + obp * run +
>     obp * hr + walk * stolen.base + hit * arbitr.1991 +
free.agent.eligible
> *
>     double + arbitr.elgible. * double + strike.out * triple +
>     triple * batting + triple * walk + triple * walk + hit *
>     hr + rbi * hr + free.agent.eligible * hr + free.agent.1991 *
>     hr + arbitr.elgible. * hr + hr * arbitr.1991 + hit * walk +
>     free.agent.eligible * walk + walk * rbi + rbi * stolen.base +
>     strike.out * stolen.base + stolen.base * batting + stolen.base *
>     walk + stolen.base * rbi + stolen.base * walk + arbitr.elgible. *
>     error)
>
> Residuals:
>      Min       1Q   Median       3Q      Max
> -2.29352 -0.28287 -0.03748  0.29790  1.31590
>
> Coefficients:
>                                   Estimate Std. Error t value Pr(>|t|)
> (Intercept)                      5.217e+00  3.467e-01  15.048  < 2e-16
***
> hit                              6.927e-03  6.226e-03   1.112 0.266889
> rbi                              1.908e-02  1.150e-02   1.658 0.098350
.
> strike.out                      -5.692e-03  4.586e-03  -1.241 0.215517
> free.agent.eligible1             1.287e+00  2.259e-01   5.699 3.05e-08
***
> free.agent.19911                 3.828e-01  6.575e-01   0.582 0.560914
> arbitr.elgible.1                 1.038e+00  2.195e-01   4.726 3.63e-06
***
> arbitr.19911                    -1.024e+00  4.392e-01  -2.331 0.020443
*
> run                              4.932e-02  2.905e-02   1.698 0.090682
.
> hr                              -1.093e-01  7.208e-02  -1.516 0.130543
> batting                         -1.814e-01  2.558e+00  -0.071 0.943522
> obp                             -1.375e+00  2.253e+00  -0.610 0.542099
> double                          -5.259e-02  4.489e-02  -1.172 0.242349
> walk                             1.395e-02  9.757e-03   1.430 0.153808
> stolen.base                     -1.685e-02  4.299e-02  -0.392 0.695372
> triple                          -1.367e-01  1.600e-01  -0.854 0.393807
> error                           -4.097e-03  6.879e-03  -0.595 0.552007
> hit:free.agent.19911             8.248e-04  4.611e-03   0.179 0.858174
> hit:arbitr.elgible.1             4.873e-03  6.448e-03   0.756 0.450395
> hit:rbi                         -1.382e-04  7.709e-05  -1.792 0.074184
.
> rbi:free.agent.eligible1         5.352e-03  9.555e-03   0.560 0.575855
> rbi:arbitr.elgible.1            -3.384e-03  1.136e-02  -0.298 0.766072
> rbi:arbitr.19911                 3.596e-02  2.179e-02   1.650 0.100046
> hit:strike.out                   5.480e-06  5.446e-05   0.101 0.919917
> strike.out:free.agent.eligible1 -2.570e-03  4.282e-03  -0.600 0.548890
> strike.out:arbitr.elgible.1     -9.703e-04  5.234e-03  -0.185 0.853068
> strike.out:run                   1.685e-04  1.246e-04   1.352 0.177345
> strike.out:hr                   -3.088e-04  2.277e-04  -1.356 0.176229
> hit:free.agent.eligible1        -1.359e-03  6.224e-03  -0.218 0.827363
> free.agent.eligible1:run         1.248e-02  9.109e-03   1.370 0.171917
> strike.out:free.agent.19911     -1.851e-02  5.974e-03  -3.099 0.002140
**
> free.agent.19911:batting         7.076e-01  6.200e+00   0.114 0.909215
> free.agent.19911:obp            -1.421e+00  3.952e+00  -0.360 0.719394
> arbitr.elgible.1:run            -8.541e-03  8.773e-03  -0.974 0.331100
> batting:double                   2.346e-01  1.609e-01   1.458 0.145884
> run:obp                         -1.825e-01  7.492e-02  -2.436 0.015462
*
> hr:obp                           3.687e-01  2.116e-01   1.742 0.082608
.
> walk:stolen.base                -6.789e-05  1.557e-04  -0.436 0.663083
> hit:arbitr.19911                -5.835e-03  7.084e-03  -0.824 0.410808
> free.agent.eligible1:double     -1.151e-02  1.663e-02  -0.692 0.489334
> arbitr.elgible.1:double          2.169e-03  1.938e-02   0.112 0.910985
> strike.out:triple               -8.106e-04  6.023e-04  -1.346 0.179475
> batting:triple                   5.179e-01  5.599e-01   0.925 0.355841
> walk:triple                      8.755e-04  9.262e-04   0.945 0.345349
> hit:hr                          -3.320e-04  2.626e-04  -1.264 0.207180
> rbi:hr                           4.748e-04  3.015e-04   1.575 0.116414
> free.agent.eligible1:hr          1.840e-02  2.313e-02   0.796 0.426972
> free.agent.19911:hr              7.216e-02  1.889e-02   3.819 0.000165
***
> arbitr.elgible.1:hr              4.111e-02  2.803e-02   1.467 0.143564
> arbitr.19911:hr                 -2.368e-02  4.647e-02  -0.510 0.610723
> hit:walk                         3.173e-05  7.826e-05   0.405 0.685442
> free.agent.eligible1:walk       -5.423e-03  4.984e-03  -1.088 0.277472
> rbi:walk                        -7.569e-05  1.313e-04  -0.577 0.564598
> rbi:stolen.base                  3.980e-05  1.605e-04   0.248 0.804409
> strike.out:stolen.base          -2.611e-04  1.615e-04  -1.617 0.107004
> batting:stolen.base              1.552e-01  1.434e-01   1.082 0.280020
> arbitr.elgible.1:error           3.930e-03  1.390e-02   0.283 0.777495
> ---
> Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
>
> Residual standard error: 0.4925 on 280 degrees of freedom
> Multiple R-Squared: 0.854,      Adjusted R-squared: 0.8248
> F-statistic: 29.24 on 56 and 280 DF,  p-value: < 2.2e-16
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From shadowrunner52 at hotmail.com  Sun Jun 13 17:46:19 2004
From: shadowrunner52 at hotmail.com (A Friend)
Date: Sun, 13 Jun 2004 11:46:19 -0400
Subject: [R] SJava Help
Message-ID: <BAY16-F60ymQ1WE9ql200019750@hotmail.com>

Environment:
R - 1.9.0
SJava - 0.67
Windows 2000

I can get SJava to work just fine in R but I am having trouble getting R to 
work in Java.  I have created a test class that looks like this:

import org.omegahat.R.Java.*;

public class Test {

  public static void main(String[] args) {

    ROmegahatInterpreter interp = new 
ROmegahatInterpreter(ROmegahatInterpreter.fixArgs(args), flase);

  }
}

The class compiles fine but when I run it I get the following output:

Loading RInterpeter library
R_HOME: R_HOME=C:/Program Files/R/rw1090
RVersion: R_VERSION=1.6.1

Then I get a windows popup error saying:

Fatal error: unable to open the base package

Why does the R version display as 1.6.1?  I traced through the code as well 
as I could and the problem seems to occur when ROmegahatInterpreter calls 
initR(args).  I think this is where it is making a call to R.dll.  By 
default args[0]="JavaR".  Any idea why?

I've checked the message board and haven't found a solution.  Has anyone 
been able to get R to work in Java?  Any help would be appreciated.  Working 
examples would be great.



From ckjmaner at carolina.rr.com  Sun Jun 13 17:52:17 2004
From: ckjmaner at carolina.rr.com (Charles and Kimberly Maner)
Date: Sun, 13 Jun 2004 11:52:17 -0400
Subject: [R] RE: [R-sig-finance] Rmetrics - New Built 190.10055
In-Reply-To: <40CC0313.2090009@itp.phys.ethz.ch>
Message-ID: <200406131552.i5DFqRNs003041@ms-smtp-02-eri0.southeast.rr.com>


Hello and thank you.  I look forward to using the new packages under both
Win32 as well as FreeBSD.

Quick question.  Any chance this package collection could be added, say, as
a new category under the package directory at http://cran.us.r-project.org/?
Perhaps it's not appropriate and/or not the scope as this area is under a
non-traditional category.  (Although, I think that's one of the aims of this
forum is to introduce R to an additional audience.)  Anyway, I thought I'd
ask/propose the idea.

Thanks,
Charles

-----Original Message-----
From: Diethelm Wuertz [mailto:wuertz at itp.phys.ethz.ch] 
Sent: Sunday, June 13, 2004 3:33 AM
To: r-help at stat.math.ethz.ch; r-sig-finance at stat.math.ethz.ch
Subject: [R-sig-finance] Rmetrics - New Built 190.10055

*June 13, 2004
Rmetrics - new Built 190.10055

Rmetrics is an environment and a collection of functions for teaching
financial engineering and computational finance

*The new built should now run out of the box under Windows, Linux, and Mac
OSX. In addition new functionality has been added, and some fixes has been
done. New functions and example files have been added. Please inspect the
FAQ and CHANGES <CHANGES.html> files. Goto: www.rmetrics.org .

Suggestions are welcome!

Diethelm Wuertz
www.rmetrics.org

	[[alternative HTML version deleted]]



From patrick.drechsler at gmx.net  Sun Jun 13 20:12:17 2004
From: patrick.drechsler at gmx.net (Patrick Drechsler)
Date: Sun, 13 Jun 2004 20:12:17 +0200
Subject: [R] [StatDataML] compile error
References: <m3pt86szxd.fsf@pdrechsler.fqdn.th-h.de>
Message-ID: <m3n037wcn2.fsf@pdrechsler.fqdn.th-h.de>


Patrick Drechsler wrote on 11 Jun 2004 19:38:22 MET:

[...compile error concerning lxml2...]

JFTR:

Thanks to the help of David Meyer I was able to solve this
problem. One has to add the following lines to ~/.bashrc:

export CFLAGS="-I/usr/include/libxml2"
export CPPFLAGS="-I/usr/include/libxml2"
export LDFLAGS="-L/usr/lib -lxml2 -lz -lm"


Andy Liaw alternatively gave me a pointer to the package
`R.matlab' <URL:http://www.braju.com/R/> which can also import
data from Matlab.

Thanks to both.

Patrick
-- 
"It's called a shovel... I've seen gardeners use them. 
You stick the sharp end in the ground. Then it gets a bit technical."
Terry Pratchett, Reaper Man



From edd at debian.org  Sun Jun 13 20:36:00 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 13 Jun 2004 13:36:00 -0500
Subject: [R] Re: [R-sig-finance] Rmetrics - New Built 190.10055
In-Reply-To: <200406131552.i5DFqRNs003041@ms-smtp-02-eri0.southeast.rr.com>
References: <40CC0313.2090009@itp.phys.ethz.ch>
	<200406131552.i5DFqRNs003041@ms-smtp-02-eri0.southeast.rr.com>
Message-ID: <20040613183600.GA332@sonny.eddelbuettel.com>


On Sun, Jun 13, 2004 at 11:52:17AM -0400, Charles and Kimberly Maner wrote:
> 
> Hello and thank you.  I look forward to using the new packages under both
> Win32 as well as FreeBSD.
> 
> Quick question.  Any chance this package collection could be added, say, as
> a new category under the package directory at http://cran.us.r-project.org/?

It's been planned all along to get this to CRAN once 'R CMD check' passed,
and no dependencies remain on non-Open/Free Source code.

Dirk

-- 
FEATURE:  VW Beetle license plate in California



From andy_liaw at merck.com  Sun Jun 13 21:44:20 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sun, 13 Jun 2004 15:44:20 -0400
Subject: [R] Regression query
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7EA5@usrymx25.merck.com>

> From: Peter Flom
> 
> If variables are colinear, then looking at interactions among them
> doesn't make much sense.  High collinearity means that one variable is
> nearly a linear combination of others.  IOW, that variable is 
> not adding
> much information.  So, if you look at the interaction, you are ALMOST
> looking at a quadratic (e.g., if the collinearity involves only 2
> variables, then one is very similar to the other, so X1*X2 is almost
> X1*X1).  The output will be confusing, to say the least. 
> 
> Worse, when you include collinear variables, the resulting equation is
> highly sensitive to small (sometimes very small) changes in the data. 
> Belsley gives an example where changes in the third decimal 
> place result
> in totally different equations.
> 
> For details see Belsley's book titled something like "collinearity and
> weak data in regression" (sorry, the book and my files are at the
> office, but this should let you find it

I guess you're referring to: "Conditioning Diagnostics: Collinearity and
Weak Data in Regression" (Wiley, 1992, rather pricey...).

Hocking has a plot that shows the effect of collinearity in a paper from the
early '80s (the "picket fence").  The plot is used on the cover of his
latest linear model book, also published by Wiley, now in 2nd edition.  

[An exercise for R newbies:  Try reproducing that plot in R, probably using
the Scaterplot3D package.]

Best,
Andy

 
> HTH
> 
> Peter L. Flom, PhD
> Assistant Director, Statistics and Data Analysis Core
> Center for Drug Use and HIV Research
> National Development and Research Institutes
> 71 W. 23rd St
> www.peterflom.com
> New York, NY 10010
> (212) 845-4485 (voice)
> (917) 438-0894 (fax)
> 
> 
> >>> "Devshruti Pahuja" <devshruti at hotmail.com> 06/11/04 5:35 AM >>>
> Hi
> 
> I have a set of data with both quantitative and categorical 
> predictors.
> After scaling of response variable, i looked for 
> multicollinearity (VIF
> values) among the predictors and removed the predictors who 
> were hinding
> some of the
> other significant predictors. I'm curious to know whether the 
> predictors
> (who are not significant) while doing simple 'lm' will be involved in
> interactions. How do i take into
> account  interactions of those predictors whom i removed just on the
> basis
> of  multicollinearity ?
> 
>  I'll appreciate if someone can throw some light on this 
> matter and how
> to
> use R to detect the interactions effectively .
> 
> Thanks
> 
>  Regards
>  Dev
> 
> > ------Final 'lm model'--------------------
> > > logmodelfull_minus_run_hr_walk_batting <- lm(log(salary) 
> ~ hit+rbi +
> walk
> > + obp +
> strike.out+free.agent.eligible+free.agent.1991+arbitr.elgible.)
> > > summary(logmodelfull_minus_run_hr_walk_batting)
> >
> > Call:
> > lm(formula = log(salary) ~ hit + rbi + walk + obp + strike.out +
> >     free.agent.eligible + free.agent.1991 + arbitr.elgible.)
> >
> > Residuals:
> >      Min       1Q   Median       3Q      Max
> > -2.41786 -0.28911 -0.02814  0.31890  1.49007
> >
> > Coefficients:
> >                       Estimate Std. Error t value Pr(>|t|)
> > (Intercept)           5.340782   0.251218  21.260  < 2e-16 ***
> > hit                   0.004479   0.001158   3.867 0.000133 ***
> > rbi                   0.011102   0.002195   5.059 7.05e-07 ***
> > walk                  0.005421   0.002206   2.457 0.014533 *
> > obp                  -1.385584   0.824105  -1.681 0.093653 .
> > strike.out           -0.005399   0.001438  -3.755 0.000205 ***
> > free.agent.eligible1  1.611521   0.080657  19.980  < 2e-16 ***
> > free.agent.19911     -0.301243   0.103481  -2.911 0.003848 **
> > arbitr.elgible.1      1.293059   0.086696  14.915  < 2e-16 ***
> > ---
> > Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
> >
> > Residual standard error: 0.5351 on 328 degrees of freedom
> > Multiple R-Squared: 0.7981,     Adjusted R-squared: 0.7932
> > F-statistic: 162.1 on 8 and 328 DF,  p-value: < 2.2e-16
> >
> >
> --------------------------------------------------------------
> ------------
> --
> > ----------------------------------------------------
> >
> >
> > --------------with
> >
> interactions--------------------------------------------------
> --------------
> > ---------------------------
> >
> > >
> > > summary(baseball.lgmodel_with_interactions_ALL_arbid)
> >
> > Call:
> > lm(formula = log(salary) ~ hit + rbi + strike.out +
> free.agent.eligible +
> >     free.agent.1991 + arbitr.elgible. + hit * free.agent.1991 +
> >     hit * arbitr.elgible. + hit * rbi + rbi * free.agent.eligible +
> >     rbi * arbitr.elgible. + rbi * arbitr.1991 + hit * strike.out +
> >     strike.out * free.agent.eligible + strike.out * 
> arbitr.elgible. +
> >     strike.out * run + strike.out * hr + hit * free.agent.eligible +
> >     free.agent.eligible * run + hit * free.agent.1991 + strike.out *
> >     free.agent.1991 + free.agent.1991 * batting + free.agent.1991 *
> >     obp + arbitr.elgible. * run + batting * double + obp * run +
> >     obp * hr + walk * stolen.base + hit * arbitr.1991 +
> free.agent.eligible
> > *
> >     double + arbitr.elgible. * double + strike.out * triple +
> >     triple * batting + triple * walk + triple * walk + hit *
> >     hr + rbi * hr + free.agent.eligible * hr + free.agent.1991 *
> >     hr + arbitr.elgible. * hr + hr * arbitr.1991 + hit * walk +
> >     free.agent.eligible * walk + walk * rbi + rbi * stolen.base +
> >     strike.out * stolen.base + stolen.base * batting + stolen.base *
> >     walk + stolen.base * rbi + stolen.base * walk + 
> arbitr.elgible. *
> >     error)
> >
> > Residuals:
> >      Min       1Q   Median       3Q      Max
> > -2.29352 -0.28287 -0.03748  0.29790  1.31590
> >
> > Coefficients:
> >                                   Estimate Std. Error t 
> value Pr(>|t|)
> > (Intercept)                      5.217e+00  3.467e-01  
> 15.048  < 2e-16
> ***
> > hit                              6.927e-03  6.226e-03   
> 1.112 0.266889
> > rbi                              1.908e-02  1.150e-02   
> 1.658 0.098350
> .
> > strike.out                      -5.692e-03  4.586e-03  
> -1.241 0.215517
> > free.agent.eligible1             1.287e+00  2.259e-01   
> 5.699 3.05e-08
> ***
> > free.agent.19911                 3.828e-01  6.575e-01   
> 0.582 0.560914
> > arbitr.elgible.1                 1.038e+00  2.195e-01   
> 4.726 3.63e-06
> ***
> > arbitr.19911                    -1.024e+00  4.392e-01  
> -2.331 0.020443
> *
> > run                              4.932e-02  2.905e-02   
> 1.698 0.090682
> .
> > hr                              -1.093e-01  7.208e-02  
> -1.516 0.130543
> > batting                         -1.814e-01  2.558e+00  
> -0.071 0.943522
> > obp                             -1.375e+00  2.253e+00  
> -0.610 0.542099
> > double                          -5.259e-02  4.489e-02  
> -1.172 0.242349
> > walk                             1.395e-02  9.757e-03   
> 1.430 0.153808
> > stolen.base                     -1.685e-02  4.299e-02  
> -0.392 0.695372
> > triple                          -1.367e-01  1.600e-01  
> -0.854 0.393807
> > error                           -4.097e-03  6.879e-03  
> -0.595 0.552007
> > hit:free.agent.19911             8.248e-04  4.611e-03   
> 0.179 0.858174
> > hit:arbitr.elgible.1             4.873e-03  6.448e-03   
> 0.756 0.450395
> > hit:rbi                         -1.382e-04  7.709e-05  
> -1.792 0.074184
> .
> > rbi:free.agent.eligible1         5.352e-03  9.555e-03   
> 0.560 0.575855
> > rbi:arbitr.elgible.1            -3.384e-03  1.136e-02  
> -0.298 0.766072
> > rbi:arbitr.19911                 3.596e-02  2.179e-02   
> 1.650 0.100046
> > hit:strike.out                   5.480e-06  5.446e-05   
> 0.101 0.919917
> > strike.out:free.agent.eligible1 -2.570e-03  4.282e-03  
> -0.600 0.548890
> > strike.out:arbitr.elgible.1     -9.703e-04  5.234e-03  
> -0.185 0.853068
> > strike.out:run                   1.685e-04  1.246e-04   
> 1.352 0.177345
> > strike.out:hr                   -3.088e-04  2.277e-04  
> -1.356 0.176229
> > hit:free.agent.eligible1        -1.359e-03  6.224e-03  
> -0.218 0.827363
> > free.agent.eligible1:run         1.248e-02  9.109e-03   
> 1.370 0.171917
> > strike.out:free.agent.19911     -1.851e-02  5.974e-03  
> -3.099 0.002140
> **
> > free.agent.19911:batting         7.076e-01  6.200e+00   
> 0.114 0.909215
> > free.agent.19911:obp            -1.421e+00  3.952e+00  
> -0.360 0.719394
> > arbitr.elgible.1:run            -8.541e-03  8.773e-03  
> -0.974 0.331100
> > batting:double                   2.346e-01  1.609e-01   
> 1.458 0.145884
> > run:obp                         -1.825e-01  7.492e-02  
> -2.436 0.015462
> *
> > hr:obp                           3.687e-01  2.116e-01   
> 1.742 0.082608
> .
> > walk:stolen.base                -6.789e-05  1.557e-04  
> -0.436 0.663083
> > hit:arbitr.19911                -5.835e-03  7.084e-03  
> -0.824 0.410808
> > free.agent.eligible1:double     -1.151e-02  1.663e-02  
> -0.692 0.489334
> > arbitr.elgible.1:double          2.169e-03  1.938e-02   
> 0.112 0.910985
> > strike.out:triple               -8.106e-04  6.023e-04  
> -1.346 0.179475
> > batting:triple                   5.179e-01  5.599e-01   
> 0.925 0.355841
> > walk:triple                      8.755e-04  9.262e-04   
> 0.945 0.345349
> > hit:hr                          -3.320e-04  2.626e-04  
> -1.264 0.207180
> > rbi:hr                           4.748e-04  3.015e-04   
> 1.575 0.116414
> > free.agent.eligible1:hr          1.840e-02  2.313e-02   
> 0.796 0.426972
> > free.agent.19911:hr              7.216e-02  1.889e-02   
> 3.819 0.000165
> ***
> > arbitr.elgible.1:hr              4.111e-02  2.803e-02   
> 1.467 0.143564
> > arbitr.19911:hr                 -2.368e-02  4.647e-02  
> -0.510 0.610723
> > hit:walk                         3.173e-05  7.826e-05   
> 0.405 0.685442
> > free.agent.eligible1:walk       -5.423e-03  4.984e-03  
> -1.088 0.277472
> > rbi:walk                        -7.569e-05  1.313e-04  
> -0.577 0.564598
> > rbi:stolen.base                  3.980e-05  1.605e-04   
> 0.248 0.804409
> > strike.out:stolen.base          -2.611e-04  1.615e-04  
> -1.617 0.107004
> > batting:stolen.base              1.552e-01  1.434e-01   
> 1.082 0.280020
> > arbitr.elgible.1:error           3.930e-03  1.390e-02   
> 0.283 0.777495
> > ---
> > Signif. codes:  0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
> >
> > Residual standard error: 0.4925 on 280 degrees of freedom
> > Multiple R-Squared: 0.854,      Adjusted R-squared: 0.8248
> > F-statistic: 29.24 on 56 and 280 DF,  p-value: < 2.2e-16
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From tlumley at u.washington.edu  Mon Jun 14 00:07:35 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Sun, 13 Jun 2004 15:07:35 -0700 (PDT)
Subject: [R] e1071, R1.9.0, Solaris 2.9, should I be worried?
In-Reply-To: <20040613042349.GA72010@muse.csie.ntu.edu.tw>
References: <200405250438.i4P4cicw034303@atlas.otago.ac.nz>
	<20040613042349.GA72010@muse.csie.ntu.edu.tw>
Message-ID: <Pine.A41.4.58.0406131453460.108054@homer11.u.washington.edu>

On Sun, 13 Jun 2004, Rong-En Fan wrote:

> On Tue, May 25, 2004 at 04:38:44PM +1200, Richard A. O'Keefe wrote:
> > In R 1.9.0 running under Solaris 2.9 on a SunBlade 100,
> > with "Sun WorkShop 6 update 2 C++ 5.3 2001/05/15" as the
> > C++ compiler, I just did
>
> >
> > How worried should I be?
> > I guess the "Warning: x hides Solver::x" warnings related to a deliberate
> > style choice, but what about the "String literal converted to char*" ones?
>
> as the message says, things like
>
> info("Exceeds max_iter in multiclass_prob\n");
>
> and the info() accepts char * as parameter
>

The warning is because this line of code is valid only as long as info()
does not modify its argument.  Assigning a string literal to a "char *"
rather than the more logical "const char *" is valid C even without a cast
(partly for backwards compatibility reasons), but most compilers have an
option to give warnings (in gcc it's --Wwriteable-strings) because of the
risks.

 If info() in fact does not modify its argument then there is no problem.

	-thomas



From arinbasu at softhome.net  Mon Jun 14 01:39:05 2004
From: arinbasu at softhome.net (arinbasu@softhome.net)
Date: Sun, 13 Jun 2004 17:39:05 -0600
Subject: [R] MCMC tutorials
In-Reply-To: <200406131004.i5DA3SBm008484@hypatia.math.ethz.ch> 
References: <200406131004.i5DA3SBm008484@hypatia.math.ethz.ch>
Message-ID: <courier.40CCE599.00005ADE@softhome.net>

Hi Ajay: 

A casual search of google using terms like "MCMC", "Markov Chains", "Monte 
Carlo", and "tutorial" pulled up over 4000 hits. A few links that may 
interest you, I thought, like: 

http://csep1.phy.ornl.gov/CSEP/MC/MC.html
http://www.stat.fi/isi99/proceedings/arkisto/varasto/gree0167.pdf
http://www.maths.soton.ac.uk/staff/Sahu/utrecht/
...
and so on. 

This one's a good start IMHO: 

http://www.statslab.cam.ac.uk/~mcmc/ 

Also, check out the following site since you love R: 

http://www.bus.ualberta.ca/telrod/SToBugs.htm 

HTH, 

 -Arin Basu
Kolkata, India 

> 
> Message: 22
> Date: Sun, 13 Jun 2004 11:54:49 +0530
> From: Ajay Shah <ajayshah at mayin.org>
... I've lusted after markov chain monte carlo many times,
> but never quite done it. Is there a child's guide to MCMC on the net
> that I can consume? 
> 

> Ajay Shah                                                   Consultant
> ajayshah at mayin.org                      Department of Economic Affairs
> http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi 
> 
>



From p.connolly at hortresearch.co.nz  Mon Jun 14 02:33:35 2004
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Mon, 14 Jun 2004 12:33:35 +1200
Subject: [R] Quirks with system.time and simulations
Message-ID: <20040614123335.D2137@hortresearch.co.nz>

I tried the code that Richard O'Keefe posted last week, to wit:

library(chron)
    ymd.to.POSIXlt <-
        function (y, m, d) as.POSIXlt(chron(julian(y=y, x=m, d=d)))
    n <- 100000
    y <- sample(1970:2004, n, replace=TRUE)
    m <- sample(1:12,      n, replace=TRUE)
    d <- sample(1:28,      n, replace=TRUE)
    system.time(ymd.to.POSIXlt(y, m, d))
    [1]  8.78  0.10 31.76  0.00  0.00
    system.time(as.POSIXlt(paste(y,m,d, sep="-")))
    [1] 14.64  0.13 53.30  0.00  0.00


On a somewhat newer machine, I got

$ R --vanilla

R : Copyright 2004, The R Foundation for Statistical Computing
Version 1.9.0  (2004-04-12), ISBN 3-900051-00-3

[...]


> library(chron)
>     ymd.to.POSIXlt <-
+         function (y, m, d) as.POSIXlt(chron(julian(y=y, x=m, d=d)))
>     n <- 100000
>     y <- sample(1970:2004, n, replace=TRUE)
>     m <- sample(1:12,      n, replace=TRUE)
>     d <- sample(1:28,      n, replace=TRUE)
> 
> system.time(ymd.to.POSIXlt(y, m, d))
[1] 1.67 0.24 2.01 0.00 0.00
> system.time(as.POSIXlt(paste(y,m,d, sep="-")))
[1] 3.06 0.02 3.08 0.00 0.00
> 

But then I tried a few more times...

> system.time(ymd.to.POSIXlt(y, m, d))
[1] 1.09 0.04 1.13 0.00 0.00
> system.time(ymd.to.POSIXlt(y, m, d))
[1] 1.11 0.09 1.20 0.00 0.00
>

The second time is a lot faster, but subsequent ones don't "improve further".
'
But with the "standard" function,

> system.time(as.POSIXlt(paste(y,m,d, sep="-")))
[1] 2.64 0.02 2.66 0.00 0.00
> system.time(as.POSIXlt(paste(y,m,d, sep="-")))
[1] 2.82 0.03 2.85 0.00 0.00
>
... it does improve slightly but rather a lot less.


THEN

If I compare the two methods in the reverse order,


$ R --vanilla

R : Copyright 2004, The R Foundation for Statistical Computing
Version 1.9.0  (2004-04-12), ISBN 3-900051-00-3

[....]


> library(chron)
>     ymd.to.POSIXlt <-
+         function (y, m, d) as.POSIXlt(chron(julian(y=y, x=m, d=d)))
>     n <- 100000
>     y <- sample(1970:2004, n, replace=TRUE)
>     m <- sample(1:12,      n, replace=TRUE)
>     d <- sample(1:28,      n, replace=TRUE)
> system.time(as.POSIXlt(paste(y,m,d, sep="-")))
[1] 3.66 0.02 3.76 0.00 0.00
> system.time(ymd.to.POSIXlt(y, m, d))
[1] 1.65 0.05 1.70 0.00 0.00
> 
> 
> system.time(as.POSIXlt(paste(y,m,d, sep="-")))
[1] 2.59 0.02 2.61 0.00 0.00
> system.time(as.POSIXlt(paste(y,m,d, sep="-")))
[1] 2.73 0.00 2.74 0.00 0.00
> 
> system.time(ymd.to.POSIXlt(y, m, d))
[1] 1.29 0.01 1.30 0.00 0.00
> system.time(ymd.to.POSIXlt(y, m, d))
[1] 0.94 0.00 0.94 0.00 0.00
> system.time(ymd.to.POSIXlt(y, m, d))
[1] 1.06 0.01 1.07 0.00 0.00
> 


It seems as though the first simulation makes it "easier" for
subsequent simulations of the same type AND also for simulations of a
somewhat different type also.  The degree to which it "helps" varies
according to just what is being run (no surprise there).  What I can't
figure out is what is happening that makes it quicker for second and
subsequent runs.

I even tried doing a gc() and setting seeds before each run to make a
more direct comparison, but it made no difference other than being
slightly less variable.  I have seen a similar phenomenon in other
types of simulations.

In the case of this code, it makes no difference whether n is 100 or
10000000.  Would that be attibutable to lazy evaluation?


> version
         _                
platform i686-pc-linux-gnu
arch     i686             
os       linux-gnu        
system   i686, linux-gnu  
status                    
major    1                
minor    9.0              
year     2004             
month    04               
day      12               
language R         


It's not exactly a problem, but it could have a bearing on comparing
processing times which is something that happens from time to time.
In the comparison that gave rise to the code above, the order would
have made a substantial difference to the perceived effectiveness of
Richard's code.


-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From rpeng at jhsph.edu  Mon Jun 14 02:50:45 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Sun, 13 Jun 2004 20:50:45 -0400
Subject: [R] Quirks with system.time and simulations
In-Reply-To: <20040614123335.D2137@hortresearch.co.nz>
References: <20040614123335.D2137@hortresearch.co.nz>
Message-ID: <40CCF665.1020204@jhsph.edu>

I think the first time is potentially much slower because of a 
garbage collection.  R-devel has a flag `gcFirst' for 
system.time() which (I think) forces a garbage collection before 
timing.

-roger

Patrick Connolly wrote:
> I tried the code that Richard O'Keefe posted last week, to wit:
> 
> library(chron)
>     ymd.to.POSIXlt <-
>         function (y, m, d) as.POSIXlt(chron(julian(y=y, x=m, d=d)))
>     n <- 100000
>     y <- sample(1970:2004, n, replace=TRUE)
>     m <- sample(1:12,      n, replace=TRUE)
>     d <- sample(1:28,      n, replace=TRUE)
>     system.time(ymd.to.POSIXlt(y, m, d))
>     [1]  8.78  0.10 31.76  0.00  0.00
>     system.time(as.POSIXlt(paste(y,m,d, sep="-")))
>     [1] 14.64  0.13 53.30  0.00  0.00
> 
> 
> On a somewhat newer machine, I got
> 
> $ R --vanilla
> 
> R : Copyright 2004, The R Foundation for Statistical Computing
> Version 1.9.0  (2004-04-12), ISBN 3-900051-00-3
> 
> [...]
> 
> 
> 
>>library(chron)
>>    ymd.to.POSIXlt <-
> 
> +         function (y, m, d) as.POSIXlt(chron(julian(y=y, x=m, d=d)))
> 
>>    n <- 100000
>>    y <- sample(1970:2004, n, replace=TRUE)
>>    m <- sample(1:12,      n, replace=TRUE)
>>    d <- sample(1:28,      n, replace=TRUE)
>>
>>system.time(ymd.to.POSIXlt(y, m, d))
> 
> [1] 1.67 0.24 2.01 0.00 0.00
> 
>>system.time(as.POSIXlt(paste(y,m,d, sep="-")))
> 
> [1] 3.06 0.02 3.08 0.00 0.00
> 
> 
> But then I tried a few more times...
> 
> 
>>system.time(ymd.to.POSIXlt(y, m, d))
> 
> [1] 1.09 0.04 1.13 0.00 0.00
> 
>>system.time(ymd.to.POSIXlt(y, m, d))
> 
> [1] 1.11 0.09 1.20 0.00 0.00
> 
> 
> The second time is a lot faster, but subsequent ones don't "improve further".
> '
> But with the "standard" function,
> 
> 
>>system.time(as.POSIXlt(paste(y,m,d, sep="-")))
> 
> [1] 2.64 0.02 2.66 0.00 0.00
> 
>>system.time(as.POSIXlt(paste(y,m,d, sep="-")))
> 
> [1] 2.82 0.03 2.85 0.00 0.00
> 
> ... it does improve slightly but rather a lot less.
> 
> 
> THEN
> 
> If I compare the two methods in the reverse order,
> 
> 
> $ R --vanilla
> 
> R : Copyright 2004, The R Foundation for Statistical Computing
> Version 1.9.0  (2004-04-12), ISBN 3-900051-00-3
> 
> [....]
> 
> 
> 
>>library(chron)
>>    ymd.to.POSIXlt <-
> 
> +         function (y, m, d) as.POSIXlt(chron(julian(y=y, x=m, d=d)))
> 
>>    n <- 100000
>>    y <- sample(1970:2004, n, replace=TRUE)
>>    m <- sample(1:12,      n, replace=TRUE)
>>    d <- sample(1:28,      n, replace=TRUE)
>>system.time(as.POSIXlt(paste(y,m,d, sep="-")))
> 
> [1] 3.66 0.02 3.76 0.00 0.00
> 
>>system.time(ymd.to.POSIXlt(y, m, d))
> 
> [1] 1.65 0.05 1.70 0.00 0.00
> 
>>
>>system.time(as.POSIXlt(paste(y,m,d, sep="-")))
> 
> [1] 2.59 0.02 2.61 0.00 0.00
> 
>>system.time(as.POSIXlt(paste(y,m,d, sep="-")))
> 
> [1] 2.73 0.00 2.74 0.00 0.00
> 
>>system.time(ymd.to.POSIXlt(y, m, d))
> 
> [1] 1.29 0.01 1.30 0.00 0.00
> 
>>system.time(ymd.to.POSIXlt(y, m, d))
> 
> [1] 0.94 0.00 0.94 0.00 0.00
> 
>>system.time(ymd.to.POSIXlt(y, m, d))
> 
> [1] 1.06 0.01 1.07 0.00 0.00
> 
> 
> 
> It seems as though the first simulation makes it "easier" for
> subsequent simulations of the same type AND also for simulations of a
> somewhat different type also.  The degree to which it "helps" varies
> according to just what is being run (no surprise there).  What I can't
> figure out is what is happening that makes it quicker for second and
> subsequent runs.
> 
> I even tried doing a gc() and setting seeds before each run to make a
> more direct comparison, but it made no difference other than being
> slightly less variable.  I have seen a similar phenomenon in other
> types of simulations.
> 
> In the case of this code, it makes no difference whether n is 100 or
> 10000000.  Would that be attibutable to lazy evaluation?
> 
> 
> 
>>version
> 
>          _                
> platform i686-pc-linux-gnu
> arch     i686             
> os       linux-gnu        
> system   i686, linux-gnu  
> status                    
> major    1                
> minor    9.0              
> year     2004             
> month    04               
> day      12               
> language R         
> 
> 
> It's not exactly a problem, but it could have a bearing on comparing
> processing times which is something that happens from time to time.
> In the comparison that gave rise to the code above, the order would
> have made a substantial difference to the perceived effectiveness of
> Richard's code.
> 
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From andy_liaw at merck.com  Mon Jun 14 03:24:28 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sun, 13 Jun 2004 21:24:28 -0400
Subject: [R] Quirks with system.time and simulations
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7EA6@usrymx25.merck.com>

I wonder if there's also effect of cpu cache...

Andy

> From: Roger D. Peng
> 
> I think the first time is potentially much slower because of a 
> garbage collection.  R-devel has a flag `gcFirst' for 
> system.time() which (I think) forces a garbage collection before 
> timing.
> 
> -roger
> 
> Patrick Connolly wrote:
> > I tried the code that Richard O'Keefe posted last week, to wit:
> > 
> > library(chron)
> >     ymd.to.POSIXlt <-
> >         function (y, m, d) as.POSIXlt(chron(julian(y=y, x=m, d=d)))
> >     n <- 100000
> >     y <- sample(1970:2004, n, replace=TRUE)
> >     m <- sample(1:12,      n, replace=TRUE)
> >     d <- sample(1:28,      n, replace=TRUE)
> >     system.time(ymd.to.POSIXlt(y, m, d))
> >     [1]  8.78  0.10 31.76  0.00  0.00
> >     system.time(as.POSIXlt(paste(y,m,d, sep="-")))
> >     [1] 14.64  0.13 53.30  0.00  0.00
> > 
> > 
> > On a somewhat newer machine, I got
> > 
> > $ R --vanilla
> > 
> > R : Copyright 2004, The R Foundation for Statistical Computing
> > Version 1.9.0  (2004-04-12), ISBN 3-900051-00-3
> > 
> > [...]
> > 
> > 
> > 
> >>library(chron)
> >>    ymd.to.POSIXlt <-
> > 
> > +         function (y, m, d) as.POSIXlt(chron(julian(y=y, 
> x=m, d=d)))
> > 
> >>    n <- 100000
> >>    y <- sample(1970:2004, n, replace=TRUE)
> >>    m <- sample(1:12,      n, replace=TRUE)
> >>    d <- sample(1:28,      n, replace=TRUE)
> >>
> >>system.time(ymd.to.POSIXlt(y, m, d))
> > 
> > [1] 1.67 0.24 2.01 0.00 0.00
> > 
> >>system.time(as.POSIXlt(paste(y,m,d, sep="-")))
> > 
> > [1] 3.06 0.02 3.08 0.00 0.00
> > 
> > 
> > But then I tried a few more times...
> > 
> > 
> >>system.time(ymd.to.POSIXlt(y, m, d))
> > 
> > [1] 1.09 0.04 1.13 0.00 0.00
> > 
> >>system.time(ymd.to.POSIXlt(y, m, d))
> > 
> > [1] 1.11 0.09 1.20 0.00 0.00
> > 
> > 
> > The second time is a lot faster, but subsequent ones don't 
> "improve further".
> > '
> > But with the "standard" function,
> > 
> > 
> >>system.time(as.POSIXlt(paste(y,m,d, sep="-")))
> > 
> > [1] 2.64 0.02 2.66 0.00 0.00
> > 
> >>system.time(as.POSIXlt(paste(y,m,d, sep="-")))
> > 
> > [1] 2.82 0.03 2.85 0.00 0.00
> > 
> > ... it does improve slightly but rather a lot less.
> > 
> > 
> > THEN
> > 
> > If I compare the two methods in the reverse order,
> > 
> > 
> > $ R --vanilla
> > 
> > R : Copyright 2004, The R Foundation for Statistical Computing
> > Version 1.9.0  (2004-04-12), ISBN 3-900051-00-3
> > 
> > [....]
> > 
> > 
> > 
> >>library(chron)
> >>    ymd.to.POSIXlt <-
> > 
> > +         function (y, m, d) as.POSIXlt(chron(julian(y=y, 
> x=m, d=d)))
> > 
> >>    n <- 100000
> >>    y <- sample(1970:2004, n, replace=TRUE)
> >>    m <- sample(1:12,      n, replace=TRUE)
> >>    d <- sample(1:28,      n, replace=TRUE)
> >>system.time(as.POSIXlt(paste(y,m,d, sep="-")))
> > 
> > [1] 3.66 0.02 3.76 0.00 0.00
> > 
> >>system.time(ymd.to.POSIXlt(y, m, d))
> > 
> > [1] 1.65 0.05 1.70 0.00 0.00
> > 
> >>
> >>system.time(as.POSIXlt(paste(y,m,d, sep="-")))
> > 
> > [1] 2.59 0.02 2.61 0.00 0.00
> > 
> >>system.time(as.POSIXlt(paste(y,m,d, sep="-")))
> > 
> > [1] 2.73 0.00 2.74 0.00 0.00
> > 
> >>system.time(ymd.to.POSIXlt(y, m, d))
> > 
> > [1] 1.29 0.01 1.30 0.00 0.00
> > 
> >>system.time(ymd.to.POSIXlt(y, m, d))
> > 
> > [1] 0.94 0.00 0.94 0.00 0.00
> > 
> >>system.time(ymd.to.POSIXlt(y, m, d))
> > 
> > [1] 1.06 0.01 1.07 0.00 0.00
> > 
> > 
> > 
> > It seems as though the first simulation makes it "easier" for
> > subsequent simulations of the same type AND also for 
> simulations of a
> > somewhat different type also.  The degree to which it "helps" varies
> > according to just what is being run (no surprise there).  
> What I can't
> > figure out is what is happening that makes it quicker for second and
> > subsequent runs.
> > 
> > I even tried doing a gc() and setting seeds before each run 
> to make a
> > more direct comparison, but it made no difference other than being
> > slightly less variable.  I have seen a similar phenomenon in other
> > types of simulations.
> > 
> > In the case of this code, it makes no difference whether n is 100 or
> > 10000000.  Would that be attibutable to lazy evaluation?
> > 
> > 
> > 
> >>version
> > 
> >          _                
> > platform i686-pc-linux-gnu
> > arch     i686             
> > os       linux-gnu        
> > system   i686, linux-gnu  
> > status                    
> > major    1                
> > minor    9.0              
> > year     2004             
> > month    04               
> > day      12               
> > language R         
> > 
> > 
> > It's not exactly a problem, but it could have a bearing on comparing
> > processing times which is something that happens from time to time.
> > In the comparison that gave rise to the code above, the order would
> > have made a substantial difference to the perceived effectiveness of
> > Richard's code.
> > 
> > 
> 
> -- 
> Roger D. Peng
> http://www.biostat.jhsph.edu/~rpeng/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ggrothendieck at myway.com  Mon Jun 14 03:54:32 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 13 Jun 2004 21:54:32 -0400 (EDT)
Subject: [R] terminology for frames and environments
Message-ID: <20040614015432.42B0E12CF8@mprdmxin.myway.com>



In ?exists it says:

inherits: should the enclosing frames of the environment be searched?

I believe what it is saying is that if inherits is TRUE and it fails
to find the variable it will look in the parent environment and
the parent of the parent, etc. (as opposed to looking in the calling
frame next and the caller of the caller, etc.)

Now I thought that standard terminology in R was:

1. enclosing environment or parent environment if one wanted to
refer to the parent relative to the parent/child hierarchy 
of environments

or

2. calling frame or parent frame if one
wanted to refer to a parent relative to the environments
corresponding to the functions in the stack 
of currently outstanding function calls

but here we seem to be referring to a parent frame as a #1. 

Could someone please clarify what standard terminology is?

Thanks.



From p.connolly at hortresearch.co.nz  Mon Jun 14 03:57:34 2004
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Mon, 14 Jun 2004 13:57:34 +1200
Subject: [R] Quirks with system.time and simulations
In-Reply-To: <40CCF665.1020204@jhsph.edu>; from rpeng@jhsph.edu on Sun, Jun
	13, 2004 at 08:50:45PM -0400
References: <20040614123335.D2137@hortresearch.co.nz>
	<40CCF665.1020204@jhsph.edu>
Message-ID: <20040614135734.E2137@hortresearch.co.nz>

On Sun, 13-Jun-2004 at 08:50PM -0400, Roger D. Peng wrote:

|> I think the first time is potentially much slower because of a 
|> garbage collection.  

Even if I do gc() before each one, there's no difference, so I kind of
doubt that.

|> R-devel has a flag `gcFirst' for system.time() which (I think)
|> forces a garbage collection before timing.

Would that be any different from explicitly doing gc()?


I tried something similar in Splus 3.4.  Of course, the same code
doesn't work, but a large paste job took exactly the same time (to 4
decimal places) irrespective of how many times the job had been done
before.  Evidently, things are very different there.

Best

-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From kkthird at yahoo.com  Mon Jun 14 04:29:14 2004
From: kkthird at yahoo.com (KKThird@Yahoo.Com)
Date: Sun, 13 Jun 2004 19:29:14 -0700 (PDT)
Subject: [R] A Few MCLUST Questions
Message-ID: <20040614022914.57835.qmail@web52504.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040613/3c285bd3/attachment.pl

From ggrothendieck at myway.com  Mon Jun 14 04:41:00 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 14 Jun 2004 02:41:00 +0000 (UTC)
Subject: [R] Quirks with system.time and simulations
References: <20040614123335.D2137@hortresearch.co.nz>
Message-ID: <loom.20040614T041711-575@post.gmane.org>

I don't know the answer but I tried running each of the following a few
times:

gc(); system.time(for(i in 1:15)as.POSIXlt(paste(y,m,d, sep="-")))
gc(); system.time(for(i in 1:15)ymd.to.POSIXlt(y, m, d))

and noticed that the Vcells gc trigger and Mb used varied all over
the place.  Does that suggest anything?

Patrick Connolly <p.connolly <at> hortresearch.co.nz> writes:

: 
: I tried the code that Richard O'Keefe posted last week, to wit:
: 
: library(chron)
:     ymd.to.POSIXlt <-
:         function (y, m, d) as.POSIXlt(chron(julian(y=y, x=m, d=d)))
:     n <- 100000
:     y <- sample(1970:2004, n, replace=TRUE)
:     m <- sample(1:12,      n, replace=TRUE)
:     d <- sample(1:28,      n, replace=TRUE)
:     system.time(ymd.to.POSIXlt(y, m, d))
:     [1]  8.78  0.10 31.76  0.00  0.00
:     system.time(as.POSIXlt(paste(y,m,d, sep="-")))
:     [1] 14.64  0.13 53.30  0.00  0.00
: 
: 
: On a somewhat newer machine, I got
: 
: $ R --vanilla
: 
: R : Copyright 2004, The R Foundation for Statistical Computing
: Version 1.9.0  (2004-04-12), ISBN 3-900051-00-3
: 
: [...]
: 
: > library(chron)
: >     ymd.to.POSIXlt <-
: +         function (y, m, d) as.POSIXlt(chron(julian(y=y, x=m, d=d)))
: >     n <- 100000
: >     y <- sample(1970:2004, n, replace=TRUE)
: >     m <- sample(1:12,      n, replace=TRUE)
: >     d <- sample(1:28,      n, replace=TRUE)
: > 
: > system.time(ymd.to.POSIXlt(y, m, d))
: [1] 1.67 0.24 2.01 0.00 0.00
: > system.time(as.POSIXlt(paste(y,m,d, sep="-")))
: [1] 3.06 0.02 3.08 0.00 0.00
: > 
: 
: But then I tried a few more times...
: 
: > system.time(ymd.to.POSIXlt(y, m, d))
: [1] 1.09 0.04 1.13 0.00 0.00
: > system.time(ymd.to.POSIXlt(y, m, d))
: [1] 1.11 0.09 1.20 0.00 0.00
: >
: 
: The second time is a lot faster, but subsequent ones don't "improve further".
: '
: But with the "standard" function,
: 
: > system.time(as.POSIXlt(paste(y,m,d, sep="-")))
: [1] 2.64 0.02 2.66 0.00 0.00
: > system.time(as.POSIXlt(paste(y,m,d, sep="-")))
: [1] 2.82 0.03 2.85 0.00 0.00
: >
: ... it does improve slightly but rather a lot less.
: 
: THEN
: 
: If I compare the two methods in the reverse order,
: 
: $ R --vanilla
: 
: R : Copyright 2004, The R Foundation for Statistical Computing
: Version 1.9.0  (2004-04-12), ISBN 3-900051-00-3
: 
: [....]
: 
: > library(chron)
: >     ymd.to.POSIXlt <-
: +         function (y, m, d) as.POSIXlt(chron(julian(y=y, x=m, d=d)))
: >     n <- 100000
: >     y <- sample(1970:2004, n, replace=TRUE)
: >     m <- sample(1:12,      n, replace=TRUE)
: >     d <- sample(1:28,      n, replace=TRUE)
: > system.time(as.POSIXlt(paste(y,m,d, sep="-")))
: [1] 3.66 0.02 3.76 0.00 0.00
: > system.time(ymd.to.POSIXlt(y, m, d))
: [1] 1.65 0.05 1.70 0.00 0.00
: > 
: > 
: > system.time(as.POSIXlt(paste(y,m,d, sep="-")))
: [1] 2.59 0.02 2.61 0.00 0.00
: > system.time(as.POSIXlt(paste(y,m,d, sep="-")))
: [1] 2.73 0.00 2.74 0.00 0.00
: > 
: > system.time(ymd.to.POSIXlt(y, m, d))
: [1] 1.29 0.01 1.30 0.00 0.00
: > system.time(ymd.to.POSIXlt(y, m, d))
: [1] 0.94 0.00 0.94 0.00 0.00
: > system.time(ymd.to.POSIXlt(y, m, d))
: [1] 1.06 0.01 1.07 0.00 0.00
: > 
: 
: It seems as though the first simulation makes it "easier" for
: subsequent simulations of the same type AND also for simulations of a
: somewhat different type also.  The degree to which it "helps" varies
: according to just what is being run (no surprise there).  What I can't
: figure out is what is happening that makes it quicker for second and
: subsequent runs.
: 
: I even tried doing a gc() and setting seeds before each run to make a
: more direct comparison, but it made no difference other than being
: slightly less variable.  I have seen a similar phenomenon in other
: types of simulations.
: 
: In the case of this code, it makes no difference whether n is 100 or
: 10000000.  Would that be attibutable to lazy evaluation?
: 
: > version
:          _                
: platform i686-pc-linux-gnu
: arch     i686             
: os       linux-gnu        
: system   i686, linux-gnu  
: status                    
: major    1                
: minor    9.0              
: year     2004             
: month    04               
: day      12               
: language R         
: 
: It's not exactly a problem, but it could have a bearing on comparing
: processing times which is something that happens from time to time.
: In the comparison that gave rise to the code above, the order would
: have made a substantial difference to the perceived effectiveness of
: Richard's code.
:



From maj at stats.waikato.ac.nz  Mon Jun 14 04:49:15 2004
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Mon, 14 Jun 2004 14:49:15 +1200
Subject: [R] A Few MCLUST Questions
In-Reply-To: <20040614022914.57835.qmail@web52504.mail.yahoo.com>
References: <20040614022914.57835.qmail@web52504.mail.yahoo.com>
Message-ID: <40CD122B.9050904@stats.waikato.ac.nz>

I can answer for MCLUST specifically, but in general mixture modelling 
terms it is easier to think of a reasonable initial clustering of the 
data from which the M step will quickly produce initial parameter 
estimates, than to pick a large number of initial parameters values out 
of the air. (Perhaps you may use a random grouping to start things off 
if nothing else comes to mind.) Usually if you try to do this you will 
pick parameters that make some data values very improbable leading to 
numerical difficulties in the M-step.

On the other hand you may have a good set of parameter values from a 
previously-fitted data set and you have a new, but similar set of data, 
perhaps from a different time-period or location. Then it will make 
sense to start off from the parameter values that you have.

Don't worry about the software - it should be just as easy for it to 
begin at either the E- or the M- step - it is you own intentions and 
convenience that matter.

Murray Jorgensen


KKThird at Yahoo.Com wrote:
> Hello everyone. I have a few MCLUST questions and I was hoping someone could help me out. If you?re an MCLUST user, they will likely be pretty easy to answer. Thanks in advance for any help.
> 
> Ken 
> 
>  
> 
>    What are the pros/cons of starting a finite mixture model at the "m" step versus the "e" step (where "m" is the maximization step and "e" is the expectation step of the EM algorithm)? In particular, are there any reasons for using em(modelName=XXX) versus me(modelName=XXX). Other than MCLUST, I?ve not seen a finite mixture model "program" give such an option. Would it make sense to fit both models and take the one with the largest log likelihood?
> 
>  
> 
>    Rather than the hc() function performing cluster analysis for all of G possible clusters, can it be set to only perform a specified number (e.g., set so G=2 only). Although a minimum number of clusters can be specified, there doesn?t seem to be any way to limit the number of clusters. I want to do a simulation for a fixed number of components, and thus I would like to avoid the unnecessary computations. 
> 
>  
> 
>    Is there any difference between hc(modelName=VVV) and hcVVV or hc(modelName=EEE) and hcEEE, etc.? Likewise, are there any differences between mstep(modelName=VVV) and mstepVVV or mstep(modelName=EEE) and mstepEEE, etc. If not, why do the same functions have different names?
> 
> 
> 		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862



From rpeng at jhsph.edu  Mon Jun 14 04:53:03 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Sun, 13 Jun 2004 22:53:03 -0400
Subject: [R] terminology for frames and environments
In-Reply-To: <20040614015432.42B0E12CF8@mprdmxin.myway.com>
References: <20040614015432.42B0E12CF8@mprdmxin.myway.com>
Message-ID: <40CD130F.50407@jhsph.edu>

Your understanding agrees with mine.  I always figured that the parent 
environment is determined by the lexical scope while the parent frame is 
determined by the function call stack.

-roger

Gabor Grothendieck wrote:

> 
> In ?exists it says:
> 
> inherits: should the enclosing frames of the environment be searched?
> 
> I believe what it is saying is that if inherits is TRUE and it fails
> to find the variable it will look in the parent environment and
> the parent of the parent, etc. (as opposed to looking in the calling
> frame next and the caller of the caller, etc.)
> 
> Now I thought that standard terminology in R was:
> 
> 1. enclosing environment or parent environment if one wanted to
> refer to the parent relative to the parent/child hierarchy 
> of environments
> 
> or
> 
> 2. calling frame or parent frame if one
> wanted to refer to a parent relative to the environments
> corresponding to the functions in the stack 
> of currently outstanding function calls
> 
> but here we seem to be referring to a parent frame as a #1. 
> 
> Could someone please clarify what standard terminology is?
> 
> Thanks.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From simpsa02 at maths.uwa.edu.au  Mon Jun 14 06:36:58 2004
From: simpsa02 at maths.uwa.edu.au (Alan Simpson)
Date: Mon, 14 Jun 2004 12:36:58 +0800
Subject: [R] forecasting from fracdiff objects
Message-ID: <40CD2B6A.2010803@maths.uwa.edu.au>

Does anybody know if it is possible to forcast or predict from a 
fracdiff object?

Any help would be much obliged...

Cheers,
Alan



From loarie at stanford.edu  Mon Jun 14 07:41:03 2004
From: loarie at stanford.edu (Scott Robbins Loarie)
Date: Sun, 13 Jun 2004 22:41:03 -0700 (PDT)
Subject: [R] multiv{pca} question
Message-ID: <Pine.LNX.4.44.0406132234500.893-100000@cardinal2.Stanford.EDU>

Dear R users,

I am having trouble using the pca function in the multiv library.
I am trying to generate the values found in $rproj by using $evecs to
calculate linear combinations of my input data.  However, I have been not
been able to correctly calculate the $rproj values.

Using the following standard sample data, why does iris%*%pcprim$evecs[,]
not equal pcprim$rproj?

data(iris)
iris<-as.matrix(iris[,1:4])
pcprim<-pca(iris)

Many thanks,

Scott



From nikko at hailmail.net  Mon Jun 14 08:04:01 2004
From: nikko at hailmail.net (Nicholas Lewin-Koh)
Date: Mon, 14 Jun 2004 06:04:01 UT
Subject: [R] RSPerl: getting test.pl to run
Message-ID: <1087193041.15217.198344402@webmail.messagingengine.com>

Hi,
I am trying to install and get RSPerl running so that we can use it
locally in
some of our scripts. I have followed the directions in the documentation
and fixed some of the problems I found in the archives like the
bootstrap problem.
I am using R 1.9.0 and RSPerl 0.5-7
Here is the problem:

> cd $R_HOME/library/RSPerl/examples/
> setenv LD_LIBRARY_PATH $R_HOME/bin:$R_HOME/library/RSPerl/libs/
> perl -I../share/blib/arch -I../share/blib/lib -I../scripts/ test.pl
1..1
ok 1
perl: relocation error: ../share/blib/arch/auto/R/R.so: undefined
symbol: tryEval

Since tryEval is one of the functions in RSPerl.so, I assume it is not
being linked properly
has anyone run into this or know how to fix it?

Thanks 
Nicholas



From ripley at stats.ox.ac.uk  Mon Jun 14 08:25:15 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 14 Jun 2004 07:25:15 +0100 (BST)
Subject: [R] multiv{pca} question
In-Reply-To: <Pine.LNX.4.44.0406132234500.893-100000@cardinal2.Stanford.EDU>
Message-ID: <Pine.LNX.4.44.0406140723300.21931-100000@gannet.stats>

Why are you using the OPRHANED multiv library for a facility covered by 
princomp() and prcomp() in base R?  The list might be able to help you 
with the latter, but no one is supporting multiv (hence its status).

On Sun, 13 Jun 2004, Scott Robbins Loarie wrote:

> I am having trouble using the pca function in the multiv library.
> I am trying to generate the values found in $rproj by using $evecs to
> calculate linear combinations of my input data.  However, I have been not
> been able to correctly calculate the $rproj values.
> 
> Using the following standard sample data, why does iris%*%pcprim$evecs[,]
> not equal pcprim$rproj?
> 
> data(iris)
> iris<-as.matrix(iris[,1:4])
> pcprim<-pca(iris)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From wviechtb at cyrus.psych.uiuc.edu  Mon Jun 14 08:33:27 2004
From: wviechtb at cyrus.psych.uiuc.edu (Wolfgang Viechtbauer)
Date: Mon, 14 Jun 2004 01:33:27 -0500 (CDT)
Subject: [R] Remove space for title in plot
Message-ID: <Pine.SOL.4.58.0406140131080.9087@stat.psych.uiuc.edu>

Dear R Users,

When plotting, R by default leaves some "white space" at the top of the
graph for the title. How can I get rid or reduce the size of the amount
of space decicated for the title? Thanks!

-- 
Wolfgang Viechtbauer



From ripley at stats.ox.ac.uk  Mon Jun 14 08:37:22 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 14 Jun 2004 07:37:22 +0100 (BST)
Subject: [R] Remove space for title in plot
In-Reply-To: <Pine.SOL.4.58.0406140131080.9087@stat.psych.uiuc.edu>
Message-ID: <Pine.LNX.4.44.0406140734380.21931-100000@gannet.stats>

Use par(mar=) (or mai=).  This is described in detail in `An Introduction 
to R'.  Something like

> par(mar=c(5,4,2,2)+0.1)
                ^ reduced from 4

On Mon, 14 Jun 2004, Wolfgang Viechtbauer wrote:

> When plotting, R by default leaves some "white space" at the top of the
> graph for the title. How can I get rid or reduce the size of the amount
> of space decicated for the title? Thanks!

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From marwan.khawaja at aub.edu.lb  Mon Jun 14 15:49:37 2004
From: marwan.khawaja at aub.edu.lb (Marwan Khawaja)
Date: Mon, 14 Jun 2004 09:49:37 -0400
Subject: [R] ordered probit or logit / recursive regression
In-Reply-To: <20040613062449.GX9596@igidr.ac.in>
Message-ID: <CLECJBOEBGOMOKJHJNDACEODEBAA.marwan.khawaja@aub.edu.lb>


> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Ajay Shah
> Sent: Sunday, June 13, 2004 2:25 AM
> To: Prof Brian Ripley
> Cc: VUILLEUMIER Mathieu; r-help at stat.math.ethz.ch
> Subject: Re: [R] ordered probit or logit / recursive regression
>
>
> I was trying to be funny and say that "R is so cool; you don't have a
> vanilla ordered probit, but you have markov chain monte carlo
> inference for an ordered probit". Now _that_ is very fancy (atleast,
> in my book). I've lusted after markov chain monte carlo many times,
> but never quite done it. Is there a child's guide to MCMC on the net
> that I can consume?
>

Check out Gillian Raab's notes (http://www.maths.napier.ac.uk/staff/graab.htm).
The WinBugs wibsite http://www.mrc-bsu.cam.ac.uk/bugs might also be useful --
examples, manuals and source materials.

Best, Marwan

-------------------------------------------------------------------
Marwan Khawaja         http://staff.aub.edu.lb/~mk36/
-------------------------------------------------------------------



> > > (I will be very happy if someone will show how to use glm()
> > > > to do a vanilla probit!)
> > >
> > Quoth Thomas Lumley:
> > > glm(y~x+z, family=binomial(probit))
> > > Be happy,
> > Be even happier that ordered probit also is available.
>
> :-) Thanks! (I haven't seen popr yet).
>
> --
> Ajay Shah                                                   Consultant
> ajayshah at mayin.org                      Department of Economic Affairs
> http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From Lorenz.Gygax at fat.admin.ch  Mon Jun 14 08:52:24 2004
From: Lorenz.Gygax at fat.admin.ch (Lorenz.Gygax@fat.admin.ch)
Date: Mon, 14 Jun 2004 08:52:24 +0200
Subject: [R] lme newbie question
Message-ID: <BF74FADD4B44554CA7E53D0B5242CD6A018DB022@evd-s7014.evd.admin.ch>

Hi Christoph,

> Am I right in this lme implementation, when I want to investigate the
> influence of the age.group, and the two conditions on the rt:
> 
> 	my.lme <- lme(rt ~ age.group + angles * hands, data = 
> my.data, random = ~ 1 |subject)
> 
> then I think I would have to compare the model above with a more
> elaborated one, including more interactions:
> 
> 	my.lme2 <- lme(rt ~ age.group * angles * hands, data = 
> my.data, random = ~ 1 |subject)
> 
> and comparing them by performing a likelhood-ratio test, yes?

If you compare these two models, you test whether the interactions of
age.group with angles and hands and the three-way interaction all together
make for a significant improvement of the model. Is that what you want? Also
note: if you do this, you need to use the method ML and not the default
REML. Or you start with the second model, use anova (my.lme2) and reduce the
model stepwise.

You can also ask whether there is a subject to subject variability in
variables other than the intercept (i.e. interactions between your random
and the fixed variables) and e.g. try things like random = ~ age.group +
angles * hands | subject

> I think, if I would like to generalize the influence of the experimental
> conditions on the rt I should define angles and hands as a random effect,
> yes? 

I do not see exactly what you aim at, here. Possibly, the second part of my
answer above is an answer to this as well?

> thanks for a short feedback. It seems, repeated-measures 
> anova's aren't a trivial topic in R :)

They never are. But, after having read most of Pinheiro and Bates' book
'Mixed effects modelling in S and S-PLUS' (Springer), it seems easier to me
than ever, because they use a consistent, integrated and concise approach.

Regards, Lorenz
- 
Lorenz Gygax, Dr. sc. nat.
Tel: +41 (0)52 368 33 84 / lorenz.gygax at fat.admin.ch      

Center for proper housing of ruminants and pigs
Swiss Veterinary Office
agroscope FAT T??nikon, CH-8356 Ettenhausen / Switzerland
Fax : +41 (0)52 365 11 90 / Tel: +41 (0)52 368 31 31



From jeroen.clarysse at easynet.be  Mon Jun 14 09:25:41 2004
From: jeroen.clarysse at easynet.be (jeroen clarysse)
Date: Mon, 14 Jun 2004 09:25:41 +0200
Subject: [R] complete newbie Q
Message-ID: <006e01c451e0$cc8f3d00$b566210a@p102pw181>

Hi all

I'm a programmer at the psychology dept, and last week, I was asked to write
an application to analyze some result data from CO2 measurement experiments.
I don't want to reinvent the wheel, so before I start custom coding in C,
I'd though to look around a bit and bumped into R on freshmeat.

basically, it is a 2-column data sheet, with timings on col1 and CO2 value
on col2. These value have a pretty nice oscillating nature, with some
occasional false spikes.

the analysis simply means extracting the 'ceilings' of the curves = the
start-end times of the top of each oscilation.

my Q to the mailinglist is now : can such analysis be done in R ? Or is R
not the appropriate package for this kind of stuff ?


thanks in advance !



From loarie at stanford.edu  Mon Jun 14 09:35:24 2004
From: loarie at stanford.edu (Scott Robbins Loarie)
Date: Mon, 14 Jun 2004 00:35:24 -0700 (PDT)
Subject: [R] trouble with prcomp()
In-Reply-To: <Pine.LNX.4.44.0406140723300.21931-100000@gannet.stats>
Message-ID: <Pine.LNX.4.44.0406140019070.2931-100000@cardinal2.Stanford.EDU>

Thanks very much for showing me these functions,

In prcomp(), the documentation says that Value: x is "the data
multiplied by the 'rotation' matix".  Using sample data:

data(iris)
iris<-as.matrix(iris[,1:4])
pcprim<-prcomp(iris, scale=TRUE)

Why does iris%*%pcprim$rotation (the data multiplied by the rotation
matrix) not equal pcprim$x?

Thanks very much,

Scott


On Mon, 14 Jun 2004, Prof Brian Ripley wrote:

> Why are you using the OPRHANED multiv library for a facility covered by
> princomp() and prcomp() in base R?  The list might be able to help you
> with the latter, but no one is supporting multiv (hence its status).
>
> On Sun, 13 Jun 2004, Scott Robbins Loarie wrote:
>
> > I am having trouble using the pca function in the multiv library.
> > I am trying to generate the values found in $rproj by using $evecs to
> > calculate linear combinations of my input data.  However, I have been not
> > been able to correctly calculate the $rproj values.
> >
> > Using the following standard sample data, why does iris%*%pcprim$evecs[,]
> > not equal pcprim$rproj?
> >
> > data(iris)
> > iris<-as.matrix(iris[,1:4])
> > pcprim<-pca(iris)
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From p.dalgaard at biostat.ku.dk  Mon Jun 14 09:42:29 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 Jun 2004 09:42:29 +0200
Subject: [R] terminology for frames and environments
In-Reply-To: <20040614015432.42B0E12CF8@mprdmxin.myway.com>
References: <20040614015432.42B0E12CF8@mprdmxin.myway.com>
Message-ID: <x24qped1qy.fsf@biostat.ku.dk>

"Gabor Grothendieck" <ggrothendieck at myway.com> writes:

> In ?exists it says:
> 
> inherits: should the enclosing frames of the environment be searched?
> 
> I believe what it is saying is that if inherits is TRUE and it fails
> to find the variable it will look in the parent environment and
> the parent of the parent, etc. (as opposed to looking in the calling
> frame next and the caller of the caller, etc.)
> 
> Now I thought that standard terminology in R was:
> 
> 1. enclosing environment or parent environment if one wanted to
> refer to the parent relative to the parent/child hierarchy 
> of environments
> 
> or
> 
> 2. calling frame or parent frame if one
> wanted to refer to a parent relative to the environments
> corresponding to the functions in the stack 
> of currently outstanding function calls
> 
> but here we seem to be referring to a parent frame as a #1. 
> 
> Could someone please clarify what standard terminology is?

Not sure there is one... 

We have been approaching consensus on a couple of occasions, but
(obviously) not been too good at enforcing it. I think the consensus
is that a "frame" is a set of variable bindings (implemented as a
hashed list), an environment is a frame plus an enclosing environment,
i.e. a linked list of frames, terminated by NULL. It is occasionally
necessary to refer to the individual frames as opposed to the whole
list, which is exactly the point of the inherits argument.

Notice that exists() talks about "enclosing" which is only ever used
in sense #1 above. "parent" is used in both senses (which is a bit
unfortunate -- not quite sure whether we have decided to get rid of
parent.env() eventually).

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From dimitris.rizopoulos at med.kuleuven.ac.be  Mon Jun 14 09:56:17 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Mon, 14 Jun 2004 09:56:17 +0200
Subject: [R] trouble with prcomp()
References: <Pine.LNX.4.44.0406140019070.2931-100000@cardinal2.Stanford.EDU>
Message-ID: <006c01c451e5$13863660$ad133a86@www.domain>

It is not equal because you have asked from `prcomp' to standardize
your data matrix,

try the following,

pcprim$x==scale(iris)%*%pcprim$rotation

Note also that the argument in `prcomp' is "scale." and not "scale"

I hope this helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Doctoral Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "Scott Robbins Loarie" <loarie at stanford.edu>
To: "Prof Brian Ripley" <ripley at stats.ox.ac.uk>
Cc: <r-help at stat.math.ethz.ch>
Sent: Monday, June 14, 2004 9:35 AM
Subject: [R] trouble with prcomp()


> Thanks very much for showing me these functions,
>
> In prcomp(), the documentation says that Value: x is "the data
> multiplied by the 'rotation' matix".  Using sample data:
>
> data(iris)
> iris<-as.matrix(iris[,1:4])
> pcprim<-prcomp(iris, scale=TRUE)
>
> Why does iris%*%pcprim$rotation (the data multiplied by the rotation
> matrix) not equal pcprim$x?
>
> Thanks very much,
>
> Scott
>
>
> On Mon, 14 Jun 2004, Prof Brian Ripley wrote:
>
> > Why are you using the OPRHANED multiv library for a facility
covered by
> > princomp() and prcomp() in base R?  The list might be able to help
you
> > with the latter, but no one is supporting multiv (hence its
status).
> >
> > On Sun, 13 Jun 2004, Scott Robbins Loarie wrote:
> >
> > > I am having trouble using the pca function in the multiv
library.
> > > I am trying to generate the values found in $rproj by using
$evecs to
> > > calculate linear combinations of my input data.  However, I have
been not
> > > been able to correctly calculate the $rproj values.
> > >
> > > Using the following standard sample data, why does
iris%*%pcprim$evecs[,]
> > > not equal pcprim$rproj?
> > >
> > > data(iris)
> > > iris<-as.matrix(iris[,1:4])
> > > pcprim<-pca(iris)
> >
> > --
> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > Professor of Applied Statistics,
http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865 272861 (self)
> > 1 South Parks Road,                     +44 1865 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Mon Jun 14 10:03:56 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 14 Jun 2004 09:03:56 +0100 (BST)
Subject: [R] Re: trouble with prcomp()
In-Reply-To: <Pine.LNX.4.44.0406140019070.2931-100000@cardinal2.Stanford.EDU>
Message-ID: <Pine.LNX.4.44.0406140901010.14513-100000@gannet.stats>

Centring and scaling.  The code is very easy to real, and contains

    x <- as.matrix(x)
    x <- scale(x, center = center, scale = scale.)

Note that the help page does say

     The calculation is done by a singular value decomposition of the
     (centered and scaled) data matrix ....


On Mon, 14 Jun 2004, Scott Robbins Loarie wrote:

> Thanks very much for showing me these functions,
> 
> In prcomp(), the documentation says that Value: x is "the data
> multiplied by the 'rotation' matix".  Using sample data:
> 
> data(iris)
> iris<-as.matrix(iris[,1:4])
> pcprim<-prcomp(iris, scale=TRUE)
> 
> Why does iris%*%pcprim$rotation (the data multiplied by the rotation
> matrix) not equal pcprim$x?
> 
> Thanks very much,
> 
> Scott
> 
> 
> On Mon, 14 Jun 2004, Prof Brian Ripley wrote:
> 
> > Why are you using the OPRHANED multiv library for a facility covered by
> > princomp() and prcomp() in base R?  The list might be able to help you
> > with the latter, but no one is supporting multiv (hence its status).
> >
> > On Sun, 13 Jun 2004, Scott Robbins Loarie wrote:
> >
> > > I am having trouble using the pca function in the multiv library.
> > > I am trying to generate the values found in $rproj by using $evecs to
> > > calculate linear combinations of my input data.  However, I have been not
> > > been able to correctly calculate the $rproj values.
> > >
> > > Using the following standard sample data, why does iris%*%pcprim$evecs[,]
> > > not equal pcprim$rproj?
> > >
> > > data(iris)
> > > iris<-as.matrix(iris[,1:4])
> > > pcprim<-pca(iris)
> >
> > --
> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865 272861 (self)
> > 1 South Parks Road,                     +44 1865 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> >
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From loarie at stanford.edu  Mon Jun 14 10:07:05 2004
From: loarie at stanford.edu (Scott Robbins Loarie)
Date: Mon, 14 Jun 2004 01:07:05 -0700 (PDT)
Subject: [R] Re: trouble with prcomp()
In-Reply-To: <Pine.LNX.4.44.0406140901010.14513-100000@gannet.stats>
Message-ID: <Pine.LNX.4.44.0406140106380.3470-100000@cardinal2.Stanford.EDU>

Brilliant!

Thanks so much for the help!

-Scott

On Mon, 14 Jun 2004, Prof Brian Ripley wrote:

> Centring and scaling.  The code is very easy to real, and contains
>
>     x <- as.matrix(x)
>     x <- scale(x, center = center, scale = scale.)
>
> Note that the help page does say
>
>      The calculation is done by a singular value decomposition of the
>      (centered and scaled) data matrix ....
>
>
> On Mon, 14 Jun 2004, Scott Robbins Loarie wrote:
>
> > Thanks very much for showing me these functions,
> >
> > In prcomp(), the documentation says that Value: x is "the data
> > multiplied by the 'rotation' matix".  Using sample data:
> >
> > data(iris)
> > iris<-as.matrix(iris[,1:4])
> > pcprim<-prcomp(iris, scale=TRUE)
> >
> > Why does iris%*%pcprim$rotation (the data multiplied by the rotation
> > matrix) not equal pcprim$x?
> >
> > Thanks very much,
> >
> > Scott
> >
> >
> > On Mon, 14 Jun 2004, Prof Brian Ripley wrote:
> >
> > > Why are you using the OPRHANED multiv library for a facility covered by
> > > princomp() and prcomp() in base R?  The list might be able to help you
> > > with the latter, but no one is supporting multiv (hence its status).
> > >
> > > On Sun, 13 Jun 2004, Scott Robbins Loarie wrote:
> > >
> > > > I am having trouble using the pca function in the multiv library.
> > > > I am trying to generate the values found in $rproj by using $evecs to
> > > > calculate linear combinations of my input data.  However, I have been not
> > > > been able to correctly calculate the $rproj values.
> > > >
> > > > Using the following standard sample data, why does iris%*%pcprim$evecs[,]
> > > > not equal pcprim$rproj?
> > > >
> > > > data(iris)
> > > > iris<-as.matrix(iris[,1:4])
> > > > pcprim<-pca(iris)
> > >
> > > --
> > > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > > University of Oxford,             Tel:  +44 1865 272861 (self)
> > > 1 South Parks Road,                     +44 1865 272866 (PA)
> > > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> > >
> >
> >
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>



From petr.pikal at precheza.cz  Mon Jun 14 10:14:53 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 14 Jun 2004 10:14:53 +0200
Subject: [R] (no subject)
In-Reply-To: <BAY22-F14BjbUf9Sp2W00060e2c@hotmail.com>
Message-ID: <40CD7A9D.19963.8C8331@localhost>

Hi

On 7 Jun 2004 at 13:16, Edoardo Stacul wrote:

> Hello, I've begun to use R only recently.
> I've a problem: I can't open framework already saved about PCA. In
> particular I can't edit all the information because I've to export the
> graphics in order to publish them. How can I do to have a list of the
> matrix, statistical elaboration and graphics? Which function shall I
> use? thanks in advance, Edoardo

Maybe the best starting point will be to read some documentation to R or some 
introductory text.

For making some presentation results you could consult R2HTML package.

Cheers
Petr


> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From csilva at ipimar.pt  Mon Jun 14 10:42:19 2004
From: csilva at ipimar.pt (Cristina Silva)
Date: Mon, 14 Jun 2004 09:42:19 +0100
Subject: [R] Confidence intervals for predicted values in nls
References: <200406031006.i53A2wwV002842@hypatia.math.ethz.ch>	<006201c44982$fa8a99e0$52040a0a@Csilva>
	<40C458CC.3040705@fz-rossendorf.de>
Message-ID: <003001c451eb$8184e070$0100a8c0@ferrari>

Dear Joerg,

Thank you for your mail. It was very helpful.

Regards,

Cristina



From luca at stat.unipg.it  Mon Jun 14 11:42:13 2004
From: luca at stat.unipg.it (Luca Scrucca)
Date: Mon, 14 Jun 2004 11:42:13 +0200 (MEST)
Subject: [R] ordering points as vertex of a polygon
Message-ID: <Pine.SOL.4.50.0406141138001.14108-100000@pearson.stat.unipg.it>

Dear R-users,

Suppose I have the following x-y coordinates which give the boundaries of
a polygon:
> x <- c(5,4,5,9,6,6,4,7,10,7,10,4,10)
> y <- c(6,3,2,6,3,7,5,4,4,7, 5,4, 6)

I would like to plot the following graph:
> plot(x,y)
> ord <- c(7,12,2,3,5,8,9,11,13,4,10,6,1)
> polygon(x[ord],y[ord])

How I can obtain the above ordering (in the example an anti-clockwise
ordering) such that I can use polygon() to connect the points?

I searched previous messages but I did not find any relevant to this
problem.

Thanks,

Luca

+-----------------------------------------------------------------------+
| Dr. Luca Scrucca                                                      |
| Dipartimento di Scienze Statistiche      tel. +39 - 075 - 5855278     |
| Universit degli Studi di Perugia        fax. +39 - 075 - 5855950     |
| Via Pascoli - C.P. 1315 Succ. 1                                       |
| 06100 PERUGIA  (ITALY)                                                |
|                                                  (o_   (o_   (o_      |
| E-mail:   luca at stat.unipg.it                    //\   //\   //\       |
| Web page: http://www.stat.unipg.it/luca         V_/_  V_/_  V_/_      |
+-----------------------------------------------------------------------+



From petr.pikal at precheza.cz  Mon Jun 14 12:06:25 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 14 Jun 2004 12:06:25 +0200
Subject: [R] complete newbie Q
In-Reply-To: <006e01c451e0$cc8f3d00$b566210a@p102pw181>
Message-ID: <40CD94C1.30768.F2DD37@localhost>



From baron at psych.upenn.edu  Mon Jun 14 12:19:01 2004
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Mon, 14 Jun 2004 06:19:01 -0400
Subject: [R] complete newbie Q
In-Reply-To: <006e01c451e0$cc8f3d00$b566210a@p102pw181>
References: <006e01c451e0$cc8f3d00$b566210a@p102pw181>
Message-ID: <20040614101901.GA3228@psych>

On 06/14/04 09:25, jeroen clarysse wrote:
>Hi all
>
>I'm a programmer at the psychology dept, and last week, I was asked to write
>an application to analyze some result data from CO2 measurement experiments.
>I don't want to reinvent the wheel, so before I start custom coding in C,
>I'd though to look around a bit and bumped into R on freshmeat.
>
>basically, it is a 2-column data sheet, with timings on col1 and CO2 value
>on col2. These value have a pretty nice oscillating nature, with some
>occasional false spikes.
>
>the analysis simply means extracting the 'ceilings' of the curves = the
>start-end times of the top of each oscilation.
>
>my Q to the mailinglist is now : can such analysis be done in R ? Or is R
>not the appropriate package for this kind of stuff ?

R is certainly the appropriate package - especially for someone
with programming experience - but this is not a simple problem no
matter what you use.  The problem is to eliminate the noise, the
"false spikes."  I cannot give you the solution, and I notice
that nobody else has replied either.  But I can tell you that I
dealt with a very similar problem (helping the Mozilla Foundation
measure the speed of page loading, which turned out to have a
periodicity) using various time-series functions, such as acf, as
well as trimmed means to get rid of outliers.  But the situation
was a bit different, as there were several observations at each
time point, so I could apply the trimmed mean to that time point.
Unfortunately, I don't have time to get more involved in your
problem, but this may get you started.  I think what you might
have to do is iterate between fitting the model and eliminating
outliers from the residuals, but maybe some statistician on this
list will have a better idea.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron



From jeroen.clarysse at easynet.be  Mon Jun 14 12:19:51 2004
From: jeroen.clarysse at easynet.be (jeroen clarysse)
Date: Mon, 14 Jun 2004 12:19:51 +0200
Subject: [R] complete newbie Q
References: <40CD94C1.30768.F2DD37@localhost>
Message-ID: <000c01c451f9$21164270$b566210a@p102pw181>

thanks for your extensive info, Petr !

I guess you are right that i should not try to put to much time in R when a
simple C application is faster.

but there are 2 reasons why I would still try it :

- i'm not a statistics or data-analysis expert. I do not know sufficient
math to work our fitting algorithms in C. So i would still spend a lot of
time researching the underlying math. I hope that R will solve this for me
- I don't want my collegues (who are not programmers) to start asking me
every little thing. I hope that, by showing them this analysis in R, they
will learn some of it themselves and leave me alone :-)

I will look into your sample as soon as i find the time !

thanks again !

jeroen



From ripley at stats.ox.ac.uk  Mon Jun 14 12:31:15 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 14 Jun 2004 11:31:15 +0100 (BST)
Subject: [R] ordering points as vertex of a polygon
In-Reply-To: <Pine.SOL.4.50.0406141138001.14108-100000@pearson.stat.unipg.it>
Message-ID: <Pine.LNX.4.44.0406141059140.14629-100000@gannet.stats>

On Mon, 14 Jun 2004, Luca Scrucca wrote:

> Dear R-users,
> 
> Suppose I have the following x-y coordinates which give the boundaries of
> a polygon:
> > x <- c(5,4,5,9,6,6,4,7,10,7,10,4,10)
> > y <- c(6,3,2,6,3,7,5,4,4,7, 5,4, 6)
> 
> I would like to plot the following graph:
> > plot(x,y)
> > ord <- c(7,12,2,3,5,8,9,11,13,4,10,6,1)
> > polygon(x[ord],y[ord])
> 
> How I can obtain the above ordering (in the example an anti-clockwise
> ordering) such that I can use polygon() to connect the points?

What exactly is the ordering?  The polygon is not convex, so the ordering 
depends on where you measure angles from.  Generally though you could try
something like.

xy <- cbind(x,y)
angM <- xy - rep(c(7,5), each=length(x))
angs <- apply(angM, 1, function(x) atan2(x[2], x[1]))
ord <- sort.list(angs)

which seems to replicate your solution.

> I searched previous messages but I did not find any relevant to this
> problem.

It is not really an R question.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Ted.Harding at nessie.mcc.ac.uk  Mon Jun 14 12:31:55 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 14 Jun 2004 11:31:55 +0100 (BST)
Subject: [R] ordering points as vertex of a polygon
In-Reply-To: <Pine.SOL.4.50.0406141138001.14108-100000@pearson.stat.unipg.it>
Message-ID: <XFMail.040614113155.Ted.Harding@nessie.mcc.ac.uk>

On 14-Jun-04 Luca Scrucca wrote:
> Dear R-users,
> 
> Suppose I have the following x-y coordinates which give the
> boundaries of a polygon:
>> x <- c(5,4,5,9,6,6,4,7,10,7,10,4,10)
>> y <- c(6,3,2,6,3,7,5,4,4,7, 5,4, 6)
> 
> I would like to plot the following graph:
>> plot(x,y)
>> ord <- c(7,12,2,3,5,8,9,11,13,4,10,6,1)
>> polygon(x[ord],y[ord])
> 
> How I can obtain the above ordering (in the example an anti-clockwise
> ordering) such that I can use polygon() to connect the points?
> 
> I searched previous messages but I did not find any relevant to this
> problem.

I don't think your problem is well specified! It looks as though you
want to draw a polygon, such that the edges linking successive
vertices do not intersect (and. as it happens, "anticlockwise").

However, your vertices above do not give a unique solution if the
problem is stated in this way. For instance (just looking at the plot)
there are the following (and others):

   1  7 12  2  3  5  8  9 11 13  4 10  6  1
(the most "natural" order, perhaps)

   1  7 12  8  2  3  5  9 11  4 13 10  6  1
   1  7 12  2  8  5  3  9 11 13  4 10  6  1

So what more do you need to state, to make the solution unique?

Best wishes,
Ted.

--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 14-Jun-04                                       Time: 11:31:55
------------------------------ XFMail ------------------------------



From Wanzare at HCJP.com  Mon Jun 14 12:59:41 2004
From: Wanzare at HCJP.com (Manoj - Hachibushu Capital)
Date: Mon, 14 Jun 2004 19:59:41 +0900
Subject: [R] CVnn2  + nnet question
Message-ID: <1CBA12F2D414914989C723D196B287DC05562F@jp-svr-ex1.hcjp.com>

Hi,
	I am trying to determine the number of units in the hidden layer
and the decay rate using the CVnn2 script found in MASS directory
(reference: pg 348,MASS-4). 

	The model that I am using is in the form of Y ~ X1 + X2 + X3...
+ X11 and the underlying data is time-series in nature.

	I found the MASS and nnet package extremely useful (many thanks
to the contributors). 

	However I am getting an error while using the CVnn2
function...it says 
Fold 1 
Size = 0, decay = 0, inner fold 1 Error in nnet.default(x,y,w,....): No
weights  to fit.

	Obliviously I am doing something wrong but am not able to figure
it out. Do I have pass any weights?  I am bit confused since the
documentation of nnet suggests says "Wts: initial parameter vector. If
missing chosen at Random". Has anybody faced the same error? I am using
the latest R version on Linux box. 

	
TIA

Manoj



From ripley at stats.ox.ac.uk  Mon Jun 14 13:25:52 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 14 Jun 2004 12:25:52 +0100 (BST)
Subject: [R] CVnn2  + nnet question
In-Reply-To: <1CBA12F2D414914989C723D196B287DC05562F@jp-svr-ex1.hcjp.com>
Message-ID: <Pine.LNX.4.44.0406141220010.26804-100000@gannet.stats>

You are trying to fit a neural network with no connections.  That makes no 
sense unless skip=T (as on the page you are quoting).

On Mon, 14 Jun 2004, Manoj - Hachibushu Capital wrote:

> Hi,
> 	I am trying to determine the number of units in the hidden layer
> and the decay rate using the CVnn2 script found in MASS directory
> (reference: pg 348,MASS-4). 
> 
> 	The model that I am using is in the form of Y ~ X1 + X2 + X3...
> + X11 and the underlying data is time-series in nature.
> 
> 	I found the MASS and nnet package extremely useful (many thanks
> to the contributors). 

`contributors'?  Do see the DESCRIPTION file: it is hardly anonymous work.

> 	However I am getting an error while using the CVnn2
> function...it says 

You have not shown us the call you used, but the default arguments are
size = rep(6, 2), lambda = c(0.001, 0.01) so you have used non-default 
args without telling us what.

> Fold 1 
> Size = 0, decay = 0, inner fold 1 Error in nnet.default(x,y,w,....): No
> weights  to fit.
> 
> 	Obliviously I am doing something wrong but am not able to figure
> it out. 

Please do re-read the basic description of neural nets and nnet().

> Do I have pass any weights?  I am bit confused since the
> documentation of nnet suggests says "Wts: initial parameter vector. If
> missing chosen at Random". Has anybody faced the same error? I am using
> the latest R version on Linux box. 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From john.maindonald at anu.edu.au  Mon Jun 14 13:30:25 2004
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Mon, 14 Jun 2004 21:30:25 +1000
Subject: [R] terminology for frames and environments
Message-ID: <3AC14080-BDF6-11D8-ACC2-000A95CDA0F2@anu.edu.au>

> From: Peter Dalgaard <p.dalgaard at biostat.ku.dk>
> Date: 14 June 2004 5:42:29 PM
>
> "Gabor Grothendieck" <ggrothendieck at myway.com> writes:

>> .......................
>> Could someone please clarify what standard terminology is?
>
> Not sure there is one...
>
> We have been approaching consensus on a couple of occasions, but
> (obviously) not been too good at enforcing it. I think the consensus
> is that a "frame" is a set of variable bindings (implemented as a
> hashed list), an environment is a frame plus an enclosing environment,
> i.e. a linked list of frames, terminated by NULL. It is occasionally
> necessary to refer to the individual frames as opposed to the whole
> list, which is exactly the point of the inherits argument.
>
> Notice that exists() talks about "enclosing" which is only ever used
> in sense #1 above. "parent" is used in both senses (which is a bit
> unfortunate -- not quite sure whether we have decided to get rid of
> parent.env() eventually).
>
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907

I have found it helpful, in trying to explain (to myself and others) 
what happens, to say that there is both a lexical stack and a call 
stack.  Is that a legitimate use of terminology?

John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Bioinformation Science, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.



From msadam at mrc.gov.mv  Mon Jun 14 13:46:49 2004
From: msadam at mrc.gov.mv (M. Shiham Adam)
Date: Mon, 14 Jun 2004 16:46:49 +0500
Subject: [R] Piecharts in a graph
Message-ID: <000001c45205$47dc71c0$1ec801ca@ShihamLaMER>

Dear R-Community.

I am trying show pie charts on a graph. To be precise, I have series of
catch calues by species by geographic (5 deg by 5 deg) regions. I want
to draw circles (actually pie charts showing the proportion of the catch
in each area) so that I have a graph of catch by species by geographic
area!

I know there is the <  symbols(dat$lon,dat$lat,circles = dat$sp1,
add=TRUE) > but I need pie charts instead of the simple circle!

Here is some data

> dat
    lon lat sp1 sp2 sp3
1  20  50   2   6  10
2  20  55 370  20  23
3  20  60 380  40  23
4  20  65  60 100  87
5  25  50   0   0  98
 
Any help would be much appreciated

Shiham Adam



From Wanzare at HCJP.com  Mon Jun 14 13:52:24 2004
From: Wanzare at HCJP.com (Manoj - Hachibushu Capital)
Date: Mon, 14 Jun 2004 20:52:24 +0900
Subject: [R] CVnn2  + nnet question
Message-ID: <1CBA12F2D414914989C723D196B287DC055630@jp-svr-ex1.hcjp.com>

Thanks for the prompt reply. I don't think I am using any non default
arguments but still...here is the exact syntax for two different case
(skip = T and default) and the corresponding error message that I
receive.


Default Case ("skip = F")

Syntax	: nn.test	<-
CVnn2(Y~X1+X2+X3+X4+X5+X6+X7+X8+X9+X10+X11,all.data.norm[1:train.set,],m
axit=500,nreps=10)

Error message: 
	Fold 1
		Size = 0 decay = 0
		Inner fold 1Error in nnet.default(x,y,w.....)  : No
weights to fit.



Case where Skip = T:

Syntax:	nn.test <-
CVnn2(Y~X1+X2+X3+X4+X5+X6+X7+X8+X9+X10+X11,all.data.norm[1:train.set,],m
axit=500,nreps=10,skip=T)

Error message:
	Fold 1 
	Size=0 decay = 0
	Inner fold 1Error in switch(type,raw=z,class= { : inappropriate
fit for class


Manoj 	

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: Monday, June 14, 2004 8:26 PM
To: Manoj - Hachibushu Capital
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] CVnn2 + nnet question

You are trying to fit a neural network with no connections.  That makes
no 
sense unless skip=T (as on the page you are quoting).

On Mon, 14 Jun 2004, Manoj - Hachibushu Capital wrote:

> Hi,
> 	I am trying to determine the number of units in the hidden layer
> and the decay rate using the CVnn2 script found in MASS directory
> (reference: pg 348,MASS-4). 
> 
> 	The model that I am using is in the form of Y ~ X1 + X2 + X3...
> + X11 and the underlying data is time-series in nature.
> 
> 	I found the MASS and nnet package extremely useful (many thanks
> to the contributors). 

`contributors'?  Do see the DESCRIPTION file: it is hardly anonymous
work.

> 	However I am getting an error while using the CVnn2
> function...it says 

You have not shown us the call you used, but the default arguments are
size = rep(6, 2), lambda = c(0.001, 0.01) so you have used non-default 
args without telling us what.

> Fold 1 
> Size = 0, decay = 0, inner fold 1 Error in nnet.default(x,y,w,....):
No
> weights  to fit.
> 
> 	Obliviously I am doing something wrong but am not able to figure
> it out. 

Please do re-read the basic description of neural nets and nnet().

> Do I have pass any weights?  I am bit confused since the
> documentation of nnet suggests says "Wts: initial parameter vector. If
> missing chosen at Random". Has anybody faced the same error? I am
using
> the latest R version on Linux box. 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Jun 14 14:04:47 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 14 Jun 2004 13:04:47 +0100 (BST)
Subject: [R] CVnn2  + nnet question
In-Reply-To: <1CBA12F2D414914989C723D196B287DC055630@jp-svr-ex1.hcjp.com>
Message-ID: <Pine.LNX.4.44.0406141257070.988-100000@gannet.stats>

I have no idea what you are actually doing. When I run the code in the
book the output begins

fold 1
  size = 6 decay = 0.001
  inner fold 1 2 3 4 5

and those are the default values.  You *have* altered something, and it is
not reasonable to expect R-help readers to debug your code (especially not
for a commercial user).


On Mon, 14 Jun 2004, Manoj - Hachibushu Capital wrote:

> Thanks for the prompt reply. I don't think I am using any non default
> arguments but still...here is the exact syntax for two different case
> (skip = T and default) and the corresponding error message that I
> receive.
> 
> 
> Default Case ("skip = F")
> 
> Syntax	: nn.test	<-
> CVnn2(Y~X1+X2+X3+X4+X5+X6+X7+X8+X9+X10+X11,all.data.norm[1:train.set,],m
> axit=500,nreps=10)
> 
> Error message: 
> 	Fold 1
> 		Size = 0 decay = 0
> 		Inner fold 1Error in nnet.default(x,y,w.....)  : No
> weights to fit.
> 
> 
> 
> Case where Skip = T:
> 
> Syntax:	nn.test <-
> CVnn2(Y~X1+X2+X3+X4+X5+X6+X7+X8+X9+X10+X11,all.data.norm[1:train.set,],m
> axit=500,nreps=10,skip=T)
> 
> Error message:
> 	Fold 1 
> 	Size=0 decay = 0
> 	Inner fold 1Error in switch(type,raw=z,class= { : inappropriate
> fit for class
> 
> 
> Manoj 	
> 
> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
> Sent: Monday, June 14, 2004 8:26 PM
> To: Manoj - Hachibushu Capital
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] CVnn2 + nnet question
> 
> You are trying to fit a neural network with no connections.  That makes
> no 
> sense unless skip=T (as on the page you are quoting).
> 
> On Mon, 14 Jun 2004, Manoj - Hachibushu Capital wrote:
> 
> > Hi,
> > 	I am trying to determine the number of units in the hidden layer
> > and the decay rate using the CVnn2 script found in MASS directory
> > (reference: pg 348,MASS-4). 
> > 
> > 	The model that I am using is in the form of Y ~ X1 + X2 + X3...
> > + X11 and the underlying data is time-series in nature.
> > 
> > 	I found the MASS and nnet package extremely useful (many thanks
> > to the contributors). 
> 
> `contributors'?  Do see the DESCRIPTION file: it is hardly anonymous
> work.
> 
> > 	However I am getting an error while using the CVnn2
> > function...it says 
> 
> You have not shown us the call you used, but the default arguments are
> size = rep(6, 2), lambda = c(0.001, 0.01) so you have used non-default 
> args without telling us what.
> 
> > Fold 1 
> > Size = 0, decay = 0, inner fold 1 Error in nnet.default(x,y,w,....):
> No
> > weights  to fit.
> > 
> > 	Obliviously I am doing something wrong but am not able to figure
> > it out. 
> 
> Please do re-read the basic description of neural nets and nnet().
> 
> > Do I have pass any weights?  I am bit confused since the
> > documentation of nnet suggests says "Wts: initial parameter vector. If
> > missing chosen at Random". Has anybody faced the same error? I am
> using
> > the latest R version on Linux box. 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Mon Jun 14 13:59:06 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 Jun 2004 13:59:06 +0200
Subject: [R] terminology for frames and environments
In-Reply-To: <3AC14080-BDF6-11D8-ACC2-000A95CDA0F2@anu.edu.au>
References: <3AC14080-BDF6-11D8-ACC2-000A95CDA0F2@anu.edu.au>
Message-ID: <x2zn76qrjp.fsf@biostat.ku.dk>

John Maindonald <john.maindonald at anu.edu.au> writes:


> I have found it helpful, in trying to explain (to myself and others)
> what happens, to say that there is both a lexical stack and a call
> stack.  Is that a legitimate use of terminology?

Slightly inaccurate I'd say. Both are actually trees, since multiple
calls can have the same parent (due to eval() and lazy evaluation) and
multiple environments can share the same enclosing environment. 

Since the trees are only connected by arrows pointing towards the
root, they just *look like* a stack of frames when viewed from one of
the branches. The only true stack structure is the context stack,
which holds the information on where to return from the current call.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ligges at statistik.uni-dortmund.de  Mon Jun 14 14:12:23 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 14 Jun 2004 14:12:23 +0200
Subject: [R] Piecharts in a graph
In-Reply-To: <000001c45205$47dc71c0$1ec801ca@ShihamLaMER>
References: <000001c45205$47dc71c0$1ec801ca@ShihamLaMER>
Message-ID: <40CD9627.1090804@statistik.uni-dortmund.de>

M. Shiham Adam wrote:

> Dear R-Community.
> 
> I am trying show pie charts on a graph. To be precise, I have series of
> catch calues by species by geographic (5 deg by 5 deg) regions. I want
> to draw circles (actually pie charts showing the proportion of the catch
> in each area) so that I have a graph of catch by species by geographic
> area!
> 
> I know there is the <  symbols(dat$lon,dat$lat,circles = dat$sp1,
> add=TRUE) > but I need pie charts instead of the simple circle!
> 
> Here is some data

Examples can be found, e.g., in

@Article{Rnews:Murrell:2003,
   author       = {Paul Murrell},
   title	       = {Integrating grid Graphics Output with Base Graphics
                   Output },
   journal      = {R News},
   year	       = 2003,
   volume       = 3,
   number       = 2,
   pages	       = {7--12},
   month	       = {October},
   url	       = {http://CRAN.R-project.org/doc/Rnews/}
}

and Paul's talk at the useR! (Slides available at 
http://www.ci.tuwien.ac.at/Conferences/useR-2004/Keynotes/Murrell.pdf).

Uwe Ligges




> 
>>dat
> 
>     lon lat sp1 sp2 sp3
> 1  20  50   2   6  10
> 2  20  55 370  20  23
> 3  20  60 380  40  23
> 4  20  65  60 100  87
> 5  25  50   0   0  98
>  
> Any help would be much appreciated
> 
> Shiham Adam
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From laura at env.leeds.ac.uk  Mon Jun 14 14:08:01 2004
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Mon, 14 Jun 2004 13:08:01 +0100 (BST)
Subject: [R] adjusting color palette
Message-ID: <Pine.LNX.4.44.0406141301000.7929-100000@env-pc-phd13>

Is there a way to increase the "sensitivity" of the color palette in order
to more clearly represent certain sections of data? For example I am
wanting to clearly differentiate between height data for a rolling
landscape but because of the extremes of the dataset (sea and mountain
tops), the bulk of the landscape is shaded in closely approximating green
- i have attempted to do this by using a larger color palette but this
doesn't make things any clearer.
Thanks
Laura



From ligges at statistik.uni-dortmund.de  Mon Jun 14 14:27:24 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 14 Jun 2004 14:27:24 +0200
Subject: [R] adjusting color palette
In-Reply-To: <Pine.LNX.4.44.0406141301000.7929-100000@env-pc-phd13>
References: <Pine.LNX.4.44.0406141301000.7929-100000@env-pc-phd13>
Message-ID: <40CD99AC.6070106@statistik.uni-dortmund.de>

Laura Quinn wrote:

> Is there a way to increase the "sensitivity" of the color palette in order
> to more clearly represent certain sections of data? For example I am
> wanting to clearly differentiate between height data for a rolling
> landscape but because of the extremes of the dataset (sea and mountain
> tops), the bulk of the landscape is shaded in closely approximating green
> - i have attempted to do this by using a larger color palette but this
> doesn't make things any clearer.
> Thanks
> Laura

See ?hsv.
If you need more sophisticated stuff, you might want to try out the 
package "RColorBrewer".

Uwe Ligges



From ggrothendieck at myway.com  Mon Jun 14 14:30:08 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 14 Jun 2004 12:30:08 +0000 (UTC)
Subject: [R] terminology for frames and environments
References: <3AC14080-BDF6-11D8-ACC2-000A95CDA0F2@anu.edu.au>
Message-ID: <loom.20040614T135514-784@post.gmane.org>

John Maindonald <john.maindonald <at> anu.edu.au> writes:

: 
: > From: Peter Dalgaard <p.dalgaard <at> biostat.ku.dk>
: > Date: 14 June 2004 5:42:29 PM
: >
: > "Gabor Grothendieck" <ggrothendieck <at> myway.com> writes:
: 
: >> .......................
: >> Could someone please clarify what standard terminology is?
: >
: > Not sure there is one...
: >
: > We have been approaching consensus on a couple of occasions, but
: > (obviously) not been too good at enforcing it. I think the consensus
: > is that a "frame" is a set of variable bindings (implemented as a
: > hashed list), an environment is a frame plus an enclosing environment,
: > i.e. a linked list of frames, terminated by NULL. It is occasionally
: > necessary to refer to the individual frames as opposed to the whole
: > list, which is exactly the point of the inherits argument.
: >
: > Notice that exists() talks about "enclosing" which is only ever used
: > in sense #1 above. "parent" is used in both senses (which is a bit
: > unfortunate -- not quite sure whether we have decided to get rid of
: > parent.env() eventually).
: >

: I have found it helpful, in trying to explain (to myself and others) 
: what happens, to say that there is both a lexical stack and a call 
: stack.  Is that a legitimate use of terminology?

I would not use the term "stack" for the lexical one since, in that
case, the graph is not linear and not even connected although for
any particular environment (in the sense of an object with class
"environment") that environment and its lexical ancestors form a 
linear structure.

Regarding Peter's comment, I would prefer to keep referring to an
environment as an object of class "environment" namely what 
new.env creates, parent.env changes, is.environment
queries, etc. so that R does not need a massive change.

In that case I guess:

- frame and environment are synonyms 
- enclosing environment is an environment together with its lexical ancestors
- the parent without further qualification is the lexical parent 
- the caller or call parent is the environment one higher up on call stack   
- an ordered set of environments can be used to refer to either the
  call stack, an environment and its ancestors or other ordered set of
  environments



From B.Rowlingson at lancaster.ac.uk  Mon Jun 14 14:33:12 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 14 Jun 2004 13:33:12 +0100
Subject: [R] adjusting color palette
In-Reply-To: <Pine.LNX.4.44.0406141301000.7929-100000@env-pc-phd13>
References: <Pine.LNX.4.44.0406141301000.7929-100000@env-pc-phd13>
Message-ID: <40CD9B08.4070307@lancaster.ac.uk>

Laura Quinn wrote:
> Is there a way to increase the "sensitivity" of the color palette in order
> to more clearly represent certain sections of data? For example I am
> wanting to clearly differentiate between height data for a rolling
> landscape but because of the extremes of the dataset (sea and mountain
> tops), the bulk of the landscape is shaded in closely approximating green
> - i have attempted to do this by using a larger color palette but this
> doesn't make things any clearer.

  The colour is selected linearly by the value you are drawing. Hence 
two solutions present themselves:

  one, non-linearly scale your data. for your application I think 
squaring x-mean(x) might stretch out the tails.

  two, constructing a non-linearly varying palette. you can do this by 
mucking about with the red, green, and blue values from the palette.

  the functions 'col2rgb' and 'rgb' are useful.

  Baz



From j.van_den_hoff at fz-rossendorf.de  Mon Jun 14 14:38:55 2004
From: j.van_den_hoff at fz-rossendorf.de (joerg van den hoff)
Date: Mon, 14 Jun 2004 14:38:55 +0200
Subject: [R] Modifying Code in .rda based packages (e.g. lme4)
In-Reply-To: <Pine.LNX.4.44.0406111359150.7940-100000@gannet.stats>
References: <Pine.LNX.4.44.0406111359150.7940-100000@gannet.stats>
Message-ID: <40CD9C5F.6060101@fz-rossendorf.de>

Prof Brian Ripley wrote:

>No standard R package is supplied as a .rda, including not lme4.  You
>must be looking at a binary installation, and you would do best to
>reinstall from the sources.  You could use
>
>R --vanilla
>load("..../all.rda")
>fix(GLMM)
>save(ls(all=T), file="..../all.rda", compress = TRUE)
>q()
>
>but we would not recommend it.  Indeed, we do not recommend your altering
>functions in other people's packages.  Why not just make a copy of GLMM
>with another name and alter that?
>
>
>On Fri, 11 Jun 2004, Dieter Menne wrote:
>
>  
>
>>assume I want to make a minor local change in a package that is supplied as
>>.rda. For example, I want to get rid of the non-verbose-protected
>>"Iteration" message in GLMM/lme4.
>>
>>Probably I have to load / change / save the package, but could someone help
>>me to get the syntax right?
>>    
>>
>
>  
>
I think, saving need to be done with

save(list=ls(all=T), file="..../all.rda", compress = TRUE)

otherwise R  complains about

        Object "ls(all = T)" not found

(the '...' argument comes first in the 'save' argument list)



From p.dalgaard at biostat.ku.dk  Mon Jun 14 14:33:09 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 Jun 2004 14:33:09 +0200
Subject: [R] terminology for frames and environments
In-Reply-To: <loom.20040614T135514-784@post.gmane.org>
References: <3AC14080-BDF6-11D8-ACC2-000A95CDA0F2@anu.edu.au>
	<loom.20040614T135514-784@post.gmane.org>
Message-ID: <x2vfhuqpyy.fsf@biostat.ku.dk>

Gabor Grothendieck <ggrothendieck at myway.com> writes:

> : > We have been approaching consensus on a couple of occasions, but
> : > (obviously) not been too good at enforcing it. I think the consensus
> : > is that a "frame" is a set of variable bindings (implemented as a
> : > hashed list), an environment is a frame plus an enclosing environment,
> : > i.e. a linked list of frames, terminated by NULL. It is occasionally
> : > necessary to refer to the individual frames as opposed to the whole
> : > list, which is exactly the point of the inherits argument.

....

> Regarding Peter's comment, I would prefer to keep referring to an
> environment as an object of class "environment" namely what 
> new.env creates, parent.env changes, is.environment
> queries, etc. so that R does not need a massive change.
> 
> In that case I guess:
> 
> - frame and environment are synonyms 

No. Please read what I wrote again.

> - enclosing environment is an environment together with its lexical ancestors

No. It *is* the lexical ancestor(s).

> - the parent without further qualification is the lexical parent 

No. (In particular, sys.parent() is not)

> - the caller or call parent is the environment one higher up on call stack   

...which is a tree! and the parent caller is not necessarily the one
one step above in the *context* stack. (Go play with sys.status()
until you understand this.)

> - an ordered set of environments can be used to refer to either the
>   call stack, an environment and its ancestors or other ordered set of
>   environments

Er, what do you mean by that...???

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From tlumley at u.washington.edu  Mon Jun 14 16:08:18 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 14 Jun 2004 07:08:18 -0700 (PDT)
Subject: [R] Quirks with system.time and simulations
In-Reply-To: <20040614123335.D2137@hortresearch.co.nz>
References: <20040614123335.D2137@hortresearch.co.nz>
Message-ID: <Pine.A41.4.58.0406140701390.273254@homer10.u.washington.edu>

On Mon, 14 Jun 2004, Patrick Connolly wrote:
>
> It seems as though the first simulation makes it "easier" for
> subsequent simulations of the same type AND also for simulations of a
> somewhat different type also.  The degree to which it "helps" varies
> according to just what is being run (no surprise there).  What I can't
> figure out is what is happening that makes it quicker for second and
> subsequent runs.
>

Luke Tierney would be the person most likely to have a definitive answer,
but my guess is that it is because of the generational garbage collector.
When this was added the speed of R improved about 20%, and the main reason
is that most garbage collections involve only recently allocated memory.
One effect is that memory blocks tend to get reused for the same objects
in later iterations of the simulation, which is more efficient.  For the
second simulation the gains are smaller.

Possibly a more accurate benchmark would be something like

Rprof("timing.prof")
replicate(LOTS, {oneway(); otherway()})
Rprof(NULL)
summaryRprof("timing.prof")

interleaving the two methods.


	-thomas



From macq at llnl.gov  Mon Jun 14 16:14:46 2004
From: macq at llnl.gov (Don MacQueen)
Date: Mon, 14 Jun 2004 07:14:46 -0700
Subject: [R] adjusting color palette
In-Reply-To: <Pine.LNX.4.44.0406141301000.7929-100000@env-pc-phd13>
References: <Pine.LNX.4.44.0406141301000.7929-100000@env-pc-phd13>
Message-ID: <p06002003bcf3628403db@[128.115.153.6]>

See also

   ?pallete

with which you can make your pallete be anything you want. The 
examples given there will point you to several pre-defined palletes. 
Perhaps one of them will do a better job for you than whatever you're 
trying now.

      rainbow(n, s = 1, v = 1, start = 0, end = max(1,n - 1)/n, gamma = 1)
      heat.colors(n)
      terrain.colors(n)
      topo.colors(n)
      cm.colors(n)

-Don

At 1:08 PM +0100 6/14/04, Laura Quinn wrote:
>Is there a way to increase the "sensitivity" of the color palette in order
>to more clearly represent certain sections of data? For example I am
>wanting to clearly differentiate between height data for a rolling
>landscape but because of the extremes of the dataset (sea and mountain
>tops), the bulk of the landscape is shaded in closely approximating green
>- i have attempted to do this by using a larger color palette but this
>doesn't make things any clearer.
>Thanks
>Laura
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From dmurdoch at pair.com  Mon Jun 14 16:18:09 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Mon, 14 Jun 2004 10:18:09 -0400
Subject: [R] adjusting color palette
In-Reply-To: <p06002003bcf3628403db@[128.115.153.6]>
References: <Pine.LNX.4.44.0406141301000.7929-100000@env-pc-phd13>
	<p06002003bcf3628403db@[128.115.153.6]>
Message-ID: <vrcrc05nsqsd6ra0c9kpnvqciod5v70d4a@4ax.com>

On Mon, 14 Jun 2004 07:14:46 -0700, Don MacQueen <macq at llnl.gov> wrote
:

>See also
>
>   ?pallete

Typo:  it should be ?palette.

Duncan Murdoch



From tlumley at u.washington.edu  Mon Jun 14 16:30:30 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 14 Jun 2004 07:30:30 -0700 (PDT)
Subject: [R] terminology for frames and environments
In-Reply-To: <3AC14080-BDF6-11D8-ACC2-000A95CDA0F2@anu.edu.au>
References: <3AC14080-BDF6-11D8-ACC2-000A95CDA0F2@anu.edu.au>
Message-ID: <Pine.A41.4.58.0406140710350.273254@homer10.u.washington.edu>

On Mon, 14 Jun 2004, John Maindonald wrote:
>
> I have found it helpful, in trying to explain (to myself and others)
> what happens, to say that there is both a lexical stack and a call
> stack.  Is that a legitimate use of terminology?
>


I think this is helpful, despite being inaccurate.  If you also recognise
that there are multiple "lexical stacks" then it is an even better
approximation.


Regarding "parent", the problem is that in S the term "parent frame" was
used, quite reasonably, for the calling frame, resulting in the function
sys.parent(). Backwards (?sideways) compatibility requires that
sys.parent() exist in R and return a position in the calling stack, and
this led to parent.frame() as a shortcut for sys.frame(sys.parent()).
Unfortunately in R, "parent" would more logically refer to the lexical
parent, which is the rationale for parent.env().

That's why the situation is a mess.

I think the standard, given these contradictions, is
  "enclosing environment" for the lexical parent
  "parent environment" for the thing returned by parent.frame()
and that's what the FAQ uses in discussing scoping.

The distinction between "environment" and "frame" is important. The frame
is what you find things in with get(, inherits=FALSE) and the environment
uses get(, environment=TRUE).

In almost all sitations you can just think of a calling stack and a
lexical stack, and ignore the frame/environment distinction.  You do need
to be aware that this is an abuse of notation, because it does sometimes
matter.

	-thomas



From K.E.Vorloou at durham.ac.uk  Mon Jun 14 16:30:17 2004
From: K.E.Vorloou at durham.ac.uk (Costas Vorlow)
Date: Mon, 14 Jun 2004 15:30:17 +0100
Subject: [R] Matrix_0.8-8 and norm() function
Message-ID: <40CDB679.1000806@durham.ac.uk>

Hello and apologies if this is a stupid question (just rejoined the list):

Has anything changed with the norm() function with the latest update of 
the Matrix library?

My old code seems not to be working properly even for very simple tests 
of the norm function. I get too many

"No direct or inherited method for function "norm" for this call"

messages. I am using windows,  R 1.9.0 and the Rblas.dll linked against 
version 3.4.1 of the ATLAS library.

Thanks for your time,
Costas

-- 

============================================================================
This e-mail contains information intended for the addressee only.  It may be confidential and may be the subject of legal and/or professional Privilege. Any dissemination, distribution, copyright or use of this communication without prior permission of the addressee is strictly prohibited. 
---------------------------------------------------------------------------
   Dr. Costas Vorlow                  | Tel: +44 (0)191 33 45727
   Durham Business School	      | Fax: +44 (0)191 33 45201
   Room (126), University of Durham,  | email: K.E.Vorloou at durham.ac.uk
   Mill Hill Lane,                    | or : costas at vorlow.org
   Durham DH1 3LB, UK.                | http://www.vorlow.org
----------------------------------------------------------------------------
  Fingerprint: B010 577A 9EC3 9185 08AE  8F22 1A48 B4E7 9FA6 C31A


   "How empty is theory in presence of fact!"  (Mark Twain, 1889)



From bates at wisc.edu  Mon Jun 14 11:38:00 2004
From: bates at wisc.edu (Douglas Bates)
Date: Mon, 14 Jun 2004 04:38:00 -0500
Subject: [R] Matrix_0.8-8 and norm() function
In-Reply-To: <40CDB679.1000806@durham.ac.uk>
References: <40CDB679.1000806@durham.ac.uk>
Message-ID: <40CD71F8.2010001@wisc.edu>

Costas Vorlow wrote:
> Hello and apologies if this is a stupid question (just rejoined the list):
> 
> Has anything changed with the norm() function with the latest update of 
> the Matrix library?
> 
> My old code seems not to be working properly even for very simple tests 
> of the norm function. I get too many
> 
> "No direct or inherited method for function "norm" for this call"
> 
> messages. I am using windows,  R 1.9.0 and the Rblas.dll linked against 
> version 3.4.1 of the ATLAS library.
> 
> Thanks for your time,
> Costas

Well, a lot has changed in the Matrix package.

Could you send me, perhaps off-list, a sample of your use of norm?



From wolski at molgen.mpg.de  Mon Jun 14 16:38:48 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Mon, 14 Jun 2004 16:38:48 +0200
Subject: [R] .Traceback
Message-ID: <200406141638480083.23CFB886@mail.math.fu-berlin.de>

Hi!

Is there a way to write the information stored in the .Traceback variable to a file?

When I try it with dump()
".Traceback" <-
list("dump(.Traceback, file = \"Mytraceback.R\")") 

Or there are other ways to write this information on the disk if an error occurs?

Eryk



From tlumley at u.washington.edu  Mon Jun 14 16:39:36 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 14 Jun 2004 07:39:36 -0700 (PDT)
Subject: [R] adjusting color palette
In-Reply-To: <Pine.LNX.4.44.0406141301000.7929-100000@env-pc-phd13>
References: <Pine.LNX.4.44.0406141301000.7929-100000@env-pc-phd13>
Message-ID: <Pine.A41.4.58.0406140734080.273254@homer10.u.washington.edu>

On Mon, 14 Jun 2004, Laura Quinn wrote:

> Is there a way to increase the "sensitivity" of the color palette in order
> to more clearly represent certain sections of data? For example I am
> wanting to clearly differentiate between height data for a rolling
> landscape but because of the extremes of the dataset (sea and mountain
> tops), the bulk of the landscape is shaded in closely approximating green
> - i have attempted to do this by using a larger color palette but this
> doesn't make things any clearer.

It sounds as though you are using topo.colors() to generate the palette.
Perhaps the simplest approach is to generate a large palette and then
subsample from it.

Eg   topo.colors(20)[c(1:5,7,9,11,13,15:20)]
will produce more widely spaced colors in the middle of the palette.


	-thomas



From p.dalgaard at biostat.ku.dk  Mon Jun 14 16:45:03 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 Jun 2004 16:45:03 +0200
Subject: [R] .Traceback
In-Reply-To: <200406141638480083.23CFB886@mail.math.fu-berlin.de>
References: <200406141638480083.23CFB886@mail.math.fu-berlin.de>
Message-ID: <x2n036qjv4.fsf@biostat.ku.dk>

"Wolski" <wolski at molgen.mpg.de> writes:

> Hi!
> 
> Is there a way to write the information stored in the .Traceback variable to a file?
> 
> When I try it with dump()
> ".Traceback" <-
> list("dump(.Traceback, file = \"Mytraceback.R\")") 
> 
> Or there are other ways to write this information on the disk if an error occurs?

You can assign it to a variable of a different name and save that. I
suspect that is the only way. E.g.,

  f <- function() foo()
  lm(y~x,data=f()) # lazy evaluation makes this interesting 
  x <- .Traceback
  dput(x) # or dump("x"), or save(), or....

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From dmurdoch at pair.com  Mon Jun 14 16:53:41 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Mon, 14 Jun 2004 10:53:41 -0400
Subject: [R] .Traceback
In-Reply-To: <200406141638480083.23CFB886@mail.math.fu-berlin.de>
References: <200406141638480083.23CFB886@mail.math.fu-berlin.de>
Message-ID: <6ierc053o72lhhej53duqgjo55vv3sev6s@4ax.com>

On Mon, 14 Jun 2004 16:38:48 +0200, "Wolski" <wolski at molgen.mpg.de>
wrote :

>Hi!
>
>Is there a way to write the information stored in the .Traceback variable to a file?
>
>When I try it with dump()
>".Traceback" <-
>list("dump(.Traceback, file = \"Mytraceback.R\")") 
>
>Or there are other ways to write this information on the disk if an error occurs?

dump() is usually used when you want to recreate the object.  Here,
you probably just want to print it, so something like

sink('error.txt')
traceback()
sink()

might do a better job.  But if you really want the .Traceback list, I
think you just need to assign it to a new variable before dumping,
e.g.

saveTrace <- .Traceback
dump('saveTrace', 'whereever.R')

I imagine the problem you're having is from some special case code to
handle the .Traceback variable in a dump, but I haven't checked the
source to see.

Duncan Murdoch



From wolski at molgen.mpg.de  Mon Jun 14 16:59:11 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Mon, 14 Jun 2004 16:59:11 +0200
Subject: [R] .Traceback
In-Reply-To: <6ierc053o72lhhej53duqgjo55vv3sev6s@4ax.com>
References: <200406141638480083.23CFB886@mail.math.fu-berlin.de>
	<6ierc053o72lhhej53duqgjo55vv3sev6s@4ax.com>
Message-ID: <200406141659110443.23E26355@mail.math.fu-berlin.de>

Thanks a lot!

of course the assignment

myvar<-.Traceback

is all what I need. 

Eryk



*********** REPLY SEPARATOR  ***********

On 14.06.2004 at 10:53 Duncan Murdoch wrote:

>On Mon, 14 Jun 2004 16:38:48 +0200, "Wolski" <wolski at molgen.mpg.de>
>wrote :
>
>>Hi!
>>
>>Is there a way to write the information stored in the .Traceback variable
>to a file?
>>
>>When I try it with dump()
>>".Traceback" <-
>>list("dump(.Traceback, file = \"Mytraceback.R\")") 
>>
>>Or there are other ways to write this information on the disk if an error
>occurs?
>
>dump() is usually used when you want to recreate the object.  Here,
>you probably just want to print it, so something like
>
>sink('error.txt')
>traceback()
>sink()
>
>might do a better job.  But if you really want the .Traceback list, I
>think you just need to assign it to a new variable before dumping,
>e.g.
>
>saveTrace <- .Traceback
>dump('saveTrace', 'whereever.R')
>
>I imagine the problem you're having is from some special case code to
>handle the .Traceback variable in a dump, but I haven't checked the
>source to see.
>
>Duncan Murdoch
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From daniela.marconi at libero.it  Mon Jun 14 17:39:20 2004
From: daniela.marconi at libero.it (daniela.marconi@libero.it)
Date: Mon, 14 Jun 2004 17:39:20 +0200
Subject: [R] Clustalw again
Message-ID: <HZB25K$F2927B531068C9A67E47C8B6751830AF@libero.it>

Hi,
I try to have again some tips about clustalw function  (in dna package).
I try to use this function on a windos platform, but my pc doesn`t have enough memory(512 Mb) for allocation, analizing a file with more or less 37250 sequences , so i try on a linux platform and every time i have a segmentation fault!!
What is the problem?Do you think that also on linux platform memory is the problem (1Gb)?
Thanks agin



From fm3a004 at math.uni-hamburg.de  Mon Jun 14 18:27:29 2004
From: fm3a004 at math.uni-hamburg.de (Christian Hennig)
Date: Mon, 14 Jun 2004 18:27:29 +0200 (MEST)
Subject: [R] A Few MCLUST Questions
In-Reply-To: <20040614022914.57835.qmail@web52504.mail.yahoo.com>
Message-ID: <Pine.GSO.3.95q.1040614182254.10844A-100000@sun35.math.uni-hamburg.de>

Hi Ken,

1) me starting with a partition converges toward the same result as
em starting with the parameters associated with the partition. So there is
no point in doing both and see which one is better. 

2) hc is an agglomerative hierarchical method. This means, it starts with
n clusters and reduces the number of clusters by 1 in every step. That is,
if you want to compute the solution for G=2 clusters, you *have to*
compute n, n-1, n-2,..., G+1 clusters first. By definition, it's not
possible to calculate 2, but not more clusters.

Christian

On Sun, 13 Jun 2004, KKThird at Yahoo.Com wrote:

> 
> Hello everyone. I have a few MCLUST questions and I was hoping someone could help me out. If you?re an MCLUST user, they will likely be pretty easy to answer. Thanks in advance for any help.
> 
> Ken 
> 
>  
> 
>    What are the pros/cons of starting a finite mixture model at the "m" step versus the "e" step (where "m" is the maximization step and "e" is the expectation step of the EM algorithm)? In particular, are there any reasons for using em(modelName=XXX) versus me(modelName=XXX). Other than MCLUST, I?ve not seen a finite mixture model "program" give such an option. Would it make sense to fit both models and take the one with the largest log likelihood?
> 
>  
> 
>    Rather than the hc() function performing cluster analysis for all of G possible clusters, can it be set to only perform a specified number (e.g., set so G=2 only). Although a minimum number of clusters can be specified, there doesn?t seem to be any way to limit the number of clusters. I want to do a simulation for a fixed number of components, and thus I would like to avoid the unnecessary computations. 
> 
>  
> 
>    Is there any difference between hc(modelName=VVV) and hcVVV or hc(modelName=EEE) and hcEEE, etc.? Likewise, are there any differences between mstep(modelName=VVV) and mstepVVV or mstep(modelName=EEE) and mstepEEE, etc. If not, why do the same functions have different names?
> 
> 
> 		
> ---------------------------------
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

***********************************************************************
Christian Hennig
Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
#######################################################################
ich empfehle www.boag-online.de



From maechler at stat.math.ethz.ch  Mon Jun 14 18:59:20 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 14 Jun 2004 18:59:20 +0200
Subject: [R] forecasting from fracdiff objects
In-Reply-To: <40CD2B6A.2010803@maths.uwa.edu.au>
References: <40CD2B6A.2010803@maths.uwa.edu.au>
Message-ID: <16589.55656.323831.395214@gargle.gargle.HOWL>

>>>>> "Alan" == Alan Simpson <simpsa02 at maths.uwa.edu.au>
>>>>>     on Mon, 14 Jun 2004 12:36:58 +0800 writes:

    Alan> Does anybody know if it is possible to forcast or predict from a 
    Alan> fracdiff object?

yes, it is possible.... but not (yet?) with a builtin function,
but rather by writing code to do it --- using the fracdiff()
result *and* the data from which you want to make h-ahead
predictions (which I assume is what you want).

At the moment, there's not even  residuals() or fitted()
methods available for fracdiff objects.
At least those are really just an exercise to implement
(which might not be true for the h-ahead predictions:
 There you'd probably need a version of the Durbin-Lewinson or the
 innovations algorithm).

User contributions are very welcome... ;-)

Martin Maechler <maechler at stat.math.ethz.ch>	http://stat.ethz.ch/~maechler/
Seminar fuer Statistik, ETH-Zentrum  LEO C16	Leonhardstr. 27
ETH (Federal Inst. Technology)	8092 Zurich	SWITZERLAND
phone: x-41-1-632-3408		fax: ...-1228			<><



From lists at svenhartenstein.de  Mon Jun 14 19:11:48 2004
From: lists at svenhartenstein.de (Sven Hartenstein)
Date: Mon, 14 Jun 2004 19:11:48 +0200
Subject: [R] extracting p-value of aov F-statistic
Message-ID: <87fz8y12uj.fsf@svenhartenstein.de>

Hi, 

I would like to extract the p-value of the F-statistic of a aov-object's
summary. 

Getting the p-value is so easy with t-tests (t.test(g1, y = g2,
var.equal = FALSE)$p.value), but I couldn't find anything like that for
ANOVAs.

Any help appreciated, 

Sven



From 0034058 at fudan.edu.cn  Mon Jun 14 19:40:49 2004
From: 0034058 at fudan.edu.cn (=?GB2312?B?u8bI2bnz?=)
Date: Tue, 15 Jun 2004 01:40:49 +0800
Subject: [R] error with barplot command?
Message-ID: <0HZB0095L7HRPS@mail.fudan.edu.cn>

when i use barplot ,it seems there is sth wrong with it.
my command are:
> beer = scan()
1: 3 4 1 1 3 4 3 3 1 3 2 1 2 1 2 3 2 3 1 1 1 1 4 3 1
26:
Read 25 items
> barplot(table(beer))

but it does NOT produce what i want.

> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    1              
minor    9.0            
year     2004           
month    04             
day      12             
language R    

btw:i get what i want in R1.8.1pat



From wolski at molgen.mpg.de  Mon Jun 14 19:46:17 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Mon, 14 Jun 2004 19:46:17 +0200
Subject: [R] Clustalw again
In-Reply-To: <HZB25K$F2927B531068C9A67E47C8B6751830AF@libero.it>
References: <HZB25K$F2927B531068C9A67E47C8B6751830AF@libero.it>
Message-ID: <200406141946170633.247B69B5@mail.math.fu-berlin.de>

Hi Daniela!

I understand it right? You want to compute an multiple alignment of 37000 sequences at once?
There are no multiple alignment programs in the world that can compute a multiple alignment of 37000 sequences to my knowledge.
Sincerely Eryk.
Ps.

What you can try is to cluster the sequences according their pairwise sequence alignment and compute multiple alignments for similar sequences.
But you will need a fast algorithm like BLAST algorithm and this on to  on is not implemented in R.

*********** REPLY SEPARATOR  ***********

On 14.06.2004 at 17:39 daniela.marconi at libero.it wrote:

>Hi,
>I try to have again some tips about clustalw function  (in dna package).
>I try to use this function on a windos platform, but my pc doesn`t have
>enough memory(512 Mb) for allocation, analizing a file with more or less
>37250 sequences , so i try on a linux platform and every time i have a
>segmentation fault!!
>What is the problem?Do you think that also on linux platform memory is the
>problem (1Gb)?
>Thanks agin
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From feldesmanm at pdx.edu  Mon Jun 14 19:47:20 2004
From: feldesmanm at pdx.edu (Marc R. Feldesman)
Date: Mon, 14 Jun 2004 10:47:20 -0700
Subject: [R] extracting p-value of aov F-statistic
In-Reply-To: <87fz8y12uj.fsf@svenhartenstein.de>
References: <87fz8y12uj.fsf@svenhartenstein.de>
Message-ID: <6.0.3.0.2.20040614104541.020a13b0@pop4.attglobal.net>

At 10:11 AM 6/14/2004, Sven Hartenstein wrote:
 >Hi,
 >
 >I would like to extract the p-value of the F-statistic of a aov-object's
 >summary.
 >
 >Getting the p-value is so easy with t-tests (t.test(g1, y = g2,
 >var.equal = FALSE)$p.value), but I couldn't find anything like that for
 >ANOVAs.
 >
 >Any help appreciated,
 >


Maybe something like:

 >?summary.aov

will give you a clue



From enrique.bengoechea at credit-suisse.com  Mon Jun 14 20:02:16 2004
From: enrique.bengoechea at credit-suisse.com (Enrique Bengoechea)
Date: Mon, 14 Jun 2004 20:02:16 +0200
Subject: [R] Clear Console
Message-ID: <OFA2B3CF50.A0392673-ONC1256EB3.0062845A@csintra.net>

Hi,

Could someone please point to me which function clears the R console under Windows? (exactly what the R Gui Windows menu "Edit > Clear Console" does).

Seems simple but I haven't been succesful with the help system (help.search for "console", "clear console", "screen"...) nor on the list archives.

Thanks in advance!!

Enrique



From dmurdoch at pair.com  Mon Jun 14 19:57:59 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Mon, 14 Jun 2004 13:57:59 -0400
Subject: [R] error with barplot command?
In-Reply-To: <0HZB0095L7HRPS@mail.fudan.edu.cn>
References: <0HZB0095L7HRPS@mail.fudan.edu.cn>
Message-ID: <lfprc0hfasanks6uhn1auu06o7kkmiku75@4ax.com>

On Tue, 15 Jun 2004 01:40:49 +0800, ???????????? <0034058 at fudan.edu.cn>
wrote :

>when i use barplot ,it seems there is sth wrong with it.
>my command are:
>> beer = scan()
>1: 3 4 1 1 3 4 3 3 1 3 2 1 2 1 2 3 2 3 1 1 1 1 4 3 1
>26:
>Read 25 items
>> barplot(table(beer))
>
>but it does NOT produce what i want.

Please try 1.9.1 beta.  This should be fixed now...

The beta builds for Windows are available on CRAN in
/bin/windows/base, where they are called "patch releases".  Right now
the build that's online is 5 days old, but today's build should be
online tomorrow.  Source code (at least) is available for other
platforms from the main page.

Duncan Murdoch



From gblevins at mn.rr.com  Mon Jun 14 20:16:47 2004
From: gblevins at mn.rr.com (Greg Blevins)
Date: Mon, 14 Jun 2004 13:16:47 -0500
Subject: [R] error running sammon
Message-ID: <012401c4523b$c1f89b60$aaca5e18@glblpyirxqz5lp>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040614/486c4580/attachment.pl

From DAVID.BICKEL at PIONEER.COM  Mon Jun 14 20:31:07 2004
From: DAVID.BICKEL at PIONEER.COM (Bickel, David)
Date: Mon, 14 Jun 2004 13:31:07 -0500
Subject: [R] interrupt in Linux
Message-ID: <5F883C17941B9F4E80E5FA8C9F1C5E0EBD5C26@jhms08.phibred.com>

Does anyone know how to interrupt R in RedHat? Neither control-c nor Esc is working. What I don't want to do is close the window or kill the entire R process.

Thanks,
David


This communication is for use by the intended recipient and ...{{dropped}}



From richard.kittler at amd.com  Mon Jun 14 20:37:08 2004
From: richard.kittler at amd.com (richard.kittler@amd.com)
Date: Mon, 14 Jun 2004 11:37:08 -0700
Subject: [R] Coercing a dataframe column to datetime
Message-ID: <858788618A93D111B45900805F85267A0BCB2D8B@caexmta3.amd.com>

I am trying to coerce a data frame column from character to datetime using strptime but keep getting an error because the length of the coerced object is always 9.  What am I doing wrong here:   

.................................................................
> ds <- cbind(1:2, c("02/27/92 23:03:20", "02/27/92 22:29:56")); ds 
     [,1] [,2]               
[1,] "1"  "02/27/92 23:03:20"
[2,] "2"  "02/27/92 22:29:56"
>  
> q <- strptime(ds[,2], "%m/%d/%y %H:%M:%S"); q
[1] "1992-02-27 23:03:20" "1992-02-27 22:29:56"
> 
> ds[,2] <- q
Error in "[<-"(`*tmp*`, , 2, value = q) : number of items to replace is not a multiple of replacement length
> 
> length(q)
[1] 9

.................................................................

--Rich

Richard Kittler 
AMD TDG
408-749-4099



From armin at xss.de  Mon Jun 14 20:43:18 2004
From: armin at xss.de (Armin Roehrl)
Date: Mon, 14 Jun 2004 20:43:18 +0200
Subject: [R] interrupt in Linux
In-Reply-To: <5F883C17941B9F4E80E5FA8C9F1C5E0EBD5C26@jhms08.phibred.com>
References: <5F883C17941B9F4E80E5FA8C9F1C5E0EBD5C26@jhms08.phibred.com>
Message-ID: <40CDF1C6.4010309@xss.de>


>Does anyone know how to interrupt R in RedHat? Neither control-c nor Esc is working. What I don't want to do is close the window or kill the entire R process.
>
>Thanks,
>David
>
>  
>
Try Ctrl-Z

and then to relunch it with %R from the shell-prompt.


Ciao,
        -A.

----------------------------------------
Armin Roehrl, http://www.approximity.com
We manage risk

Blogs:
http://blog.approximity.com
http://agile.approximity.com



From vograno at evafunds.com  Mon Jun 14 20:55:21 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Mon, 14 Jun 2004 11:55:21 -0700
Subject: [R] mkChar can be interrupted
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A5568842@phost015.EVAFUNDS.intermedia.net>

Hi,
 
As was discussed earlier in another thread and as documented in R-exts
.Call() should not be interruptible by Ctrl-C. However the following
code, which spends most of its time inside mkChar, turned out to be
interruptible on RH-7.3 R-1.8.1 gcc-2.96:
 
 
#include <Rinternals.h>
#include <R.h>

SEXP foo0(const SEXP nSexp) {
  int i, n;
  SEXP resSexp;

  if (!isInteger(nSexp))
    error("wrong arg type\n");

  n = asInteger(nSexp);
  resSexp = PROTECT(allocVector(STRSXP, n));
    
  Rprintf("!!!time to interrup!!!\n");
  for (i=0; i<n; ++i) {
    SET_STRING_ELT(resSexp, i, mkChar("foo"));
  }

  Rprintf("end mkChar\n");
  UNPROTECT(1);
    
  return R_NilValue;
}



# invoke 'foo0' and give it an argument large enough to let you type
Ctrl-C
# double the argument if you see "end mkChar" and do it again :-)
> x <- .Call("foo0", as.integer(1e7))
!!!time to interrup!!!

> 
> version
         _                
platform i686-pc-linux-gnu
arch     i686             
os       linux-gnu        
system   i686, linux-gnu  
status                    
major    1                
minor    8.1              
year     2003             
month    11               
day      21               
language R                


Thanks,
Vadim



From ripley at stats.ox.ac.uk  Mon Jun 14 21:04:52 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 14 Jun 2004 20:04:52 +0100 (BST)
Subject: [R] Coercing a dataframe column to datetime
In-Reply-To: <858788618A93D111B45900805F85267A0BCB2D8B@caexmta3.amd.com>
Message-ID: <Pine.LNX.4.44.0406142004190.1362-100000@gannet.stats>

You have forgotten as.POSIXct is needed too.

On Mon, 14 Jun 2004 richard.kittler at amd.com wrote:

> I am trying to coerce a data frame column from character to datetime using strptime but keep getting an error because the length of the coerced object is always 9.  What am I doing wrong here:   
> 
> .................................................................
> > ds <- cbind(1:2, c("02/27/92 23:03:20", "02/27/92 22:29:56")); ds 
>      [,1] [,2]               
> [1,] "1"  "02/27/92 23:03:20"
> [2,] "2"  "02/27/92 22:29:56"
> >  
> > q <- strptime(ds[,2], "%m/%d/%y %H:%M:%S"); q
> [1] "1992-02-27 23:03:20" "1992-02-27 22:29:56"
> > 
> > ds[,2] <- q
> Error in "[<-"(`*tmp*`, , 2, value = q) : number of items to replace is not a multiple of replacement length
> > 
> > length(q)
> [1] 9
> 
> .................................................................
> 
> --Rich
> 
> Richard Kittler 
> AMD TDG
> 408-749-4099
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Mon Jun 14 20:58:40 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 14 Jun 2004 19:58:40 +0100 (BST)
Subject: [R] error running sammon
In-Reply-To: <012401c4523b$c1f89b60$aaca5e18@glblpyirxqz5lp>
Message-ID: <Pine.LNX.4.44.0406141957090.1362-100000@gannet.stats>

Which package is the sammon() you are using in?  (There such functions in 
TWO packages.)

On Mon, 14 Jun 2004, Greg Blevins wrote:

> Hello,
> 
> I am inputing a 17 x 17 symetric matrix to sammon.  The matrix is a
> co-occurance matrix with no missing data.  If this is at all relevant,
> running hclust on this matrix works.
> 
> > samx <- sammon(q23axproduct)
> 
> I receive the following error:
> 
> Error in sammon(q23axproduct) : initial configuration must be complete
> 
> In addition: Warning messages: 
> 
> 1: some of the first 2 eigenvalues are < 0 in: cmdscale(d, k) 
> 
> 2: NaNs produced in: sqrt(ev) 
> 
> > 
> 
> Any suggestions of what is happening and how to rectify this?
> 
> Thanks
> 
> Gregory L. Blevins 
> 
> The Market Solutions Group
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From kbartz at loyaltymatrix.com  Mon Jun 14 21:08:33 2004
From: kbartz at loyaltymatrix.com (Kevin Bartz)
Date: Mon, 14 Jun 2004 12:08:33 -0700
Subject: [R] Readline on R-1.9.1a
Message-ID: <20040614191315.39BDB467C5@omta18.mta.everyone.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040614/10b10c15/attachment.pl

From lists at svenhartenstein.de  Mon Jun 14 21:02:07 2004
From: lists at svenhartenstein.de (Sven Hartenstein)
Date: Mon, 14 Jun 2004 21:02:07 +0200
Subject: [R] extracting p-value of aov F-statistic
In-Reply-To: <6.0.3.0.2.20040614104541.020a13b0@pop4.attglobal.net> (Marc R.
	Feldesman's message of "Mon, 14 Jun 2004 10:47:20 -0700")
References: <87fz8y12uj.fsf@svenhartenstein.de>
	<6.0.3.0.2.20040614104541.020a13b0@pop4.attglobal.net>
Message-ID: <87acz62cb4.fsf@svenhartenstein.de>


> > I would like to extract the p-value of the F-statistic of a aov-object's
> > summary.
> >
> > Getting the p-value is so easy with t-tests (t.test(g1, y = g2,
> > var.equal = FALSE)$p.value), but I couldn't find anything like that for
> > ANOVAs.

> Maybe something like:
>  >?summary.aov
> will give you a clue

Unfortunately not. I already checked summary, aov and lm help pages.

Sven



From itayf at fhcrc.org  Mon Jun 14 21:17:30 2004
From: itayf at fhcrc.org (Itay Furman)
Date: Mon, 14 Jun 2004 12:17:30 -0700 (PDT)
Subject: [R] How to 'stamp' a plot with meta-data?
Message-ID: <Pine.LNX.4.44.0406141148470.24733-100000@cezanne.fhcrc.org>


Dear R users,

Sometimes, for tracking purposes, I am interested to add to a 
plot some metadata such as
* the date it was produced
* filename that stores the plot
* perhaps data sources, author, etc

Ideally, I would like to be able to do this for any kind of plot, 
plot(), barplot(), hist(), etc.; and, to be able to produce 
plots with or without the metadata by a simple toggle mechanism.

Something like:

# 'Clean' plot
some.plot.func(<args>)	# plot() or barplot() or ...

# Now with metadata
options(metadata=TRUE)	# Or some other toggle mechanism

some.plot.func(<args>)	# Same plot as above, but with wider 
			# bottom margins that show the 
			# metadata.

So far I looked in "Introduction to R" and tried to find hints 
using help.search() without success.

Is it possilbe to do?
Any suggestions or pointers as to how to do it are appreciated.
Even partial solution in which the plot is set up to accomodate 
metadata in the margins, but the data needs to be added manually 
after plot()ting will be great.

	Thanks in advance,
	Itay

--------------------------------------------------------------
itayf at fhcrc.org		Fred Hutchinson Cancer Research Center



From parilov at cs.nyu.edu  Mon Jun 14 21:31:55 2004
From: parilov at cs.nyu.edu (Evgueni Parilov)
Date: Mon, 14 Jun 2004 15:31:55 -0400
Subject: [R] load function to R GUI
Message-ID: <40CDFD2B.6050909@cs.nyu.edu>

Hi all!
I looked through the manual and FAQ, and did not find any information
on how to load functions from files (with .R extension) to run them in
R GUI under Windows. The only way I know is to create and edit a
function inside GUI. But what if I want to edit it in Emacs (do not
want to use ESS) and then load into GUI?
Any suggestions...

Evgueni



From k.wang at auckland.ac.nz  Mon Jun 14 21:34:59 2004
From: k.wang at auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Tue, 15 Jun 2004 07:34:59 +1200
Subject: [R] load function to R GUI
In-Reply-To: <40CDFD2B.6050909@cs.nyu.edu>
Message-ID: <20040614193519.YCZM2764.mta1-rme.xtra.co.nz@kevinlpt>

Hi,
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch

> Hi all!
> I looked through the manual and FAQ, and did not find any
information
> on how to load functions from files (with .R extension) to run them
in
> R GUI under Windows. The only way I know is to create and edit a
> function inside GUI. But what if I want to edit it in Emacs (do not
> want to use ESS) and then load into GUI?
> Any suggestions...

Do you mean ?source?

i.e. save your function in, say, foo.R then use the source() function
to get it in.

HTH

Kevin



From f.harrell at vanderbilt.edu  Mon Jun 14 15:35:54 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Mon, 14 Jun 2004 15:35:54 +0200
Subject: [R] How to 'stamp' a plot with meta-data?
In-Reply-To: <Pine.LNX.4.44.0406141148470.24733-100000@cezanne.fhcrc.org>
References: <Pine.LNX.4.44.0406141148470.24733-100000@cezanne.fhcrc.org>
Message-ID: <40CDA9BA.70301@vanderbilt.edu>

Itay Furman wrote:
> Dear R users,
> 
> Sometimes, for tracking purposes, I am interested to add to a 
> plot some metadata such as
> * the date it was produced
> * filename that stores the plot
> * perhaps data sources, author, etc
> 
> Ideally, I would like to be able to do this for any kind of plot, 
> plot(), barplot(), hist(), etc.; and, to be able to produce 
> plots with or without the metadata by a simple toggle mechanism.
> 
> Something like:
> 
> # 'Clean' plot
> some.plot.func(<args>)	# plot() or barplot() or ...
> 
> # Now with metadata
> options(metadata=TRUE)	# Or some other toggle mechanism
> 
> some.plot.func(<args>)	# Same plot as above, but with wider 
> 			# bottom margins that show the 
> 			# metadata.
> 
> So far I looked in "Introduction to R" and tried to find hints 
> using help.search() without success.
> 
> Is it possilbe to do?
> Any suggestions or pointers as to how to do it are appreciated.
> Even partial solution in which the plot is set up to accomodate 
> metadata in the margins, but the data needs to be added manually 
> after plot()ting will be great.
> 
> 	Thanks in advance,
> 	Itay
> 
> --------------------------------------------------------------
> itayf at fhcrc.org		Fred Hutchinson Cancer Research Center

Take a look at the pstamp function in the Hmisc package.

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From parilov at cs.nyu.edu  Mon Jun 14 21:44:50 2004
From: parilov at cs.nyu.edu (Evgueni Parilov)
Date: Mon, 14 Jun 2004 15:44:50 -0400
Subject: [R] load function to R GUI
In-Reply-To: <20040614193519.YCZM2764.mta1-rme.xtra.co.nz@kevinlpt>
References: <20040614193519.YCZM2764.mta1-rme.xtra.co.nz@kevinlpt>
Message-ID: <40CE0032.70306@cs.nyu.edu>

Thanks!
That was exactly what I wanted.
Evgueni


Ko-Kang Kevin Wang wrote:

>Hi,
>  
>
>>-----Original Message-----
>>From: r-help-bounces at stat.math.ethz.ch
>>    
>>
>
>  
>
>>Hi all!
>>I looked through the manual and FAQ, and did not find any
>>    
>>
>information
>  
>
>>on how to load functions from files (with .R extension) to run them
>>    
>>
>in
>  
>
>>R GUI under Windows. The only way I know is to create and edit a
>>function inside GUI. But what if I want to edit it in Emacs (do not
>>want to use ESS) and then load into GUI?
>>Any suggestions...
>>    
>>
>
>Do you mean ?source?
>
>i.e. save your function in, say, foo.R then use the source() function
>to get it in.
>
>HTH
>
>Kevin
>
>
>
>
>  
>



From rpeng at jhsph.edu  Mon Jun 14 21:53:02 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Mon, 14 Jun 2004 15:53:02 -0400
Subject: [R] Readline on R-1.9.1a
In-Reply-To: <20040614191315.39BDB467C5@omta18.mta.everyone.net>
References: <20040614191315.39BDB467C5@omta18.mta.everyone.net>
Message-ID: <40CE021E.9070409@jhsph.edu>

If you've got the required rpm's installed (and they install in the 
usual locations), you should not need the --with-readline flag.  Try 
running configure with out it.

-roger

Kevin Bartz wrote:
> Hello! I'm trying to install R-1.9.1a with readline on Suse Linux. As
> recommended in other posts, I've installed readline and readline-devel:
> 
>  
> 
> kevin at redtail:~/R-1.9.1> rpm -qa | grep readline
> 
> readline-devel-32bit-9.0-0
> 
> readline-32bit-9.0-0
> 
> readline-4.3-207
> 
> readline-devel-4.3-207
> 
>  
> 
> Then I run configure with readline:
> 
>  
> 
> ./configure --with-readline
> 
>  
> 
> The relevant lines are:
> 
>  
> 
> kevin at redtail:~/R-1.9.1> ./configure --with-readline | grep readline
> 
> checking for rl_callback_read_char in -lreadline... no
> 
> checking readline/history.h usability... yes
> 
> checking readline/history.h presence... yes
> 
> checking for readline/history.h... yes
> 
> checking readline/readline.h usability... yes
> 
> checking readline/readline.h presence... yes
> 
> checking for readline/readline.h... yes
> 
>  
> 
> In config.log all readline tests pass except this one:
> 
>  
> 
> configure:21277: checking for rl_callback_read_char in -lreadline
> 
> configure:21307: gcc -o conftest -g -O2 -I/usr/local/include
> -L/usr/local/lib conftest.c \
> 
> -lreadline  -ldl -lm  >&5
> 
> /usr/local/lib/libreadline.so: undefined reference to `tgetnum'
> 
> /usr/local/lib/libreadline.so: undefined reference to `tgoto'
> 
> /usr/local/lib/libreadline.so: undefined reference to `tgetflag'
> 
> /usr/local/lib/libreadline.so: undefined reference to `BC'
> 
> /usr/local/lib/libreadline.so: undefined reference to `tputs'
> 
> /usr/local/lib/libreadline.so: undefined reference to `PC'
> 
> /usr/local/lib/libreadline.so: undefined reference to `tgetent'
> 
> /usr/local/lib/libreadline.so: undefined reference to `UP'
> 
> /usr/local/lib/libreadline.so: undefined reference to `tgetstr'
> 
> collect2: ld returned 1 exit status
> 
> configure:21313: $? = 1
> 
> configure: failed program was:
> 
> | /* confdefs.h.  */
> 
> |
> 
> | #define PACKAGE_NAME "R"
> 
> | #define PACKAGE_TARNAME "R"
> 
> | #define PACKAGE_VERSION "1.9.1"
> 
> | #define PACKAGE_STRING "R 1.9.1"
> 
> | #define PACKAGE_BUGREPORT "r-bugs at R-project.org"
> 
> | #define PACKAGE "R"
> 
> | #define VERSION "1.9.1"
> 
> | #define R_PLATFORM "x86_64-unknown-linux-gnu"
> 
> | #define R_CPU "x86_64"
> 
> | #define R_VENDOR "unknown"
> 
> | #define R_OS "linux-gnu"
> 
> | #define Unix 1
> 
> | #ifdef __cplusplus
> 
> | extern "C" void std::exit (int) throw (); using std::exit;
> 
> | #endif
> 
> | #define STDC_HEADERS 1
> 
> | #define HAVE_SYS_TYPES_H 1
> 
> | #define HAVE_SYS_STAT_H 1
> 
> | #define HAVE_STDLIB_H 1
> 
> | #define HAVE_STRING_H 1
> 
> | #define HAVE_MEMORY_H 1
> 
> | #define HAVE_STRINGS_H 1
> 
> | #define HAVE_INTTYPES_H 1
> 
> | #define HAVE_STDINT_H 1
> 
> | #define HAVE_UNISTD_H 1
> 
> | #define HAVE_DLFCN_H 1
> 
> | #define HAVE_LIBM 1
> 
> | #define HAVE_LIBDL 1
> 
> | /* end confdefs.h.  */
> 
> |
> 
> | /* Override any gcc2 internal prototype to avoid an error.  */
> 
> | #ifdef __cplusplus
> 
> | extern "C"
> 
> | #endif
> 
> | /* We use char because int might match the return type of a gcc2
> 
> |    builtin and then its argument prototype would still apply.  */
> 
> | char rl_callback_read_char ();
> 
> | int
> 
> | main ()
> 
> | {
> 
> | rl_callback_read_char ();
> 
> |   ;
> 
> |   return 0;
> 
> | }
> 
> configure:21338: result: no
> 
>  
> 
> Any ideas? Thanks for any help you can provide.
> 
>  
> 
> Kevin Bartz
> 
>   
> 
> 
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng



From p.dalgaard at biostat.ku.dk  Mon Jun 14 21:46:21 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 Jun 2004 21:46:21 +0200
Subject: [R] Readline on R-1.9.1a
In-Reply-To: <20040614191315.39BDB467C5@omta18.mta.everyone.net>
References: <20040614191315.39BDB467C5@omta18.mta.everyone.net>
Message-ID: <x2y8mp2a9e.fsf@biostat.ku.dk>

"Kevin Bartz" <kbartz at loyaltymatrix.com> writes:

> Hello! I'm trying to install R-1.9.1a with readline on Suse Linux. As
> recommended in other posts, I've installed readline and readline-devel:
...
> kevin at redtail:~/R-1.9.1> rpm -qa | grep readline
> 
> readline-devel-32bit-9.0-0
> 
> readline-32bit-9.0-0
> 
> readline-4.3-207
> 
> readline-devel-4.3-207

...
> configure:21307: gcc -o conftest -g -O2 -I/usr/local/include
> -L/usr/local/lib conftest.c \
> 
> -lreadline  -ldl -lm  >&5
> 
> /usr/local/lib/libreadline.so: undefined reference to `tgetnum'
> 
> /usr/local/lib/libreadline.so: undefined reference to `tgoto'
> 
> /usr/local/lib/libreadline.so: undefined reference to `tgetflag'
> 
> /usr/local/lib/libreadline.so: undefined reference to `BC'
> 
> /usr/local/lib/libreadline.so: undefined reference to `tputs'
> 
> /usr/local/lib/libreadline.so: undefined reference to `PC'
> 
> /usr/local/lib/libreadline.so: undefined reference to `tgetent'
> 
> /usr/local/lib/libreadline.so: undefined reference to `UP'
> 
> /usr/local/lib/libreadline.so: undefined reference to `tgetstr'
...

> Any ideas? Thanks for any help you can provide.

Missing ncurses or ncurses-devel, I believe.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From andy_liaw at merck.com  Mon Jun 14 21:58:12 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 14 Jun 2004 15:58:12 -0400
Subject: [R] Readline on R-1.9.1a
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7EB5@usrymx25.merck.com>

Given this is turning out to be an FAQ, wouldn't it make sense to have these
rpm's as depends for the R rpm?  (I always compile from source, so don't
know what the R rpm actually depends on.)

Best,
Andy

> From: Peter Dalgaard
> 
> "Kevin Bartz" <kbartz at loyaltymatrix.com> writes:
> 
> > Hello! I'm trying to install R-1.9.1a with readline on Suse 
> Linux. As
> > recommended in other posts, I've installed readline and 
> readline-devel:
> ...
> > kevin at redtail:~/R-1.9.1> rpm -qa | grep readline
> > 
> > readline-devel-32bit-9.0-0
> > 
> > readline-32bit-9.0-0
> > 
> > readline-4.3-207
> > 
> > readline-devel-4.3-207
> 
> ...
> > configure:21307: gcc -o conftest -g -O2 -I/usr/local/include
> > -L/usr/local/lib conftest.c \
> > 
> > -lreadline  -ldl -lm  >&5
> > 
> > /usr/local/lib/libreadline.so: undefined reference to `tgetnum'
> > 
> > /usr/local/lib/libreadline.so: undefined reference to `tgoto'
> > 
> > /usr/local/lib/libreadline.so: undefined reference to `tgetflag'
> > 
> > /usr/local/lib/libreadline.so: undefined reference to `BC'
> > 
> > /usr/local/lib/libreadline.so: undefined reference to `tputs'
> > 
> > /usr/local/lib/libreadline.so: undefined reference to `PC'
> > 
> > /usr/local/lib/libreadline.so: undefined reference to `tgetent'
> > 
> > /usr/local/lib/libreadline.so: undefined reference to `UP'
> > 
> > /usr/local/lib/libreadline.so: undefined reference to `tgetstr'
> ...
> 
> > Any ideas? Thanks for any help you can provide.
> 
> Missing ncurses or ncurses-devel, I believe.
> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: 
> (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: 
> (+45) 35327907
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ccleland at optonline.net  Mon Jun 14 21:59:25 2004
From: ccleland at optonline.net (Chuck Cleland)
Date: Mon, 14 Jun 2004 15:59:25 -0400
Subject: [R] extracting p-value of aov F-statistic
In-Reply-To: <87acz62cb4.fsf@svenhartenstein.de>
References: <87fz8y12uj.fsf@svenhartenstein.de>
	<6.0.3.0.2.20040614104541.020a13b0@pop4.attglobal.net>
	<87acz62cb4.fsf@svenhartenstein.de>
Message-ID: <40CE039D.4070706@optonline.net>

Sven:
   A suggestion from Peter Dalgaard in the archives:

http://tolstoy.newcastle.edu.au/R/help/01a/2097.html

data(warpbreaks)

LZ.aov <- summary(aov(breaks ~ wool + tension, data = warpbreaks))

as.matrix(LZ.aov[[1]][,5])
             [,1]
[1,] 0.073613669
[2,] 0.001377778
[3,]          NA

hope this helps,

Chuck Cleland

Sven Hartenstein wrote:
>>>I would like to extract the p-value of the F-statistic of a aov-object's
>>>summary.
>>>
>>>Getting the p-value is so easy with t-tests (t.test(g1, y = g2,
>>>var.equal = FALSE)$p.value), but I couldn't find anything like that for
>>>ANOVAs.
> 
> 
>>Maybe something like:
>> >?summary.aov
>>will give you a clue
> 
> 
> Unfortunately not. I already checked summary, aov and lm help pages.

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From rpeng at jhsph.edu  Mon Jun 14 22:01:02 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Mon, 14 Jun 2004 16:01:02 -0400
Subject: [R] Readline on R-1.9.1a
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7EB5@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF7EB5@usrymx25.merck.com>
Message-ID: <40CE03FE.1060903@jhsph.edu>

People who compile from source still need to install the necessary rpms.

-roger

Liaw, Andy wrote:
> Given this is turning out to be an FAQ, wouldn't it make sense to have these
> rpm's as depends for the R rpm?  (I always compile from source, so don't
> know what the R rpm actually depends on.)
> 
> Best,
> Andy
> 
> 
>>From: Peter Dalgaard
>>
>>"Kevin Bartz" <kbartz at loyaltymatrix.com> writes:
>>
>>
>>>Hello! I'm trying to install R-1.9.1a with readline on Suse 
>>
>>Linux. As
>>
>>>recommended in other posts, I've installed readline and 
>>
>>readline-devel:
>>...
>>
>>>kevin at redtail:~/R-1.9.1> rpm -qa | grep readline
>>>
>>>readline-devel-32bit-9.0-0
>>>
>>>readline-32bit-9.0-0
>>>
>>>readline-4.3-207
>>>
>>>readline-devel-4.3-207
>>
>>...
>>
>>>configure:21307: gcc -o conftest -g -O2 -I/usr/local/include
>>>-L/usr/local/lib conftest.c \
>>>
>>>-lreadline  -ldl -lm  >&5
>>>
>>>/usr/local/lib/libreadline.so: undefined reference to `tgetnum'
>>>
>>>/usr/local/lib/libreadline.so: undefined reference to `tgoto'
>>>
>>>/usr/local/lib/libreadline.so: undefined reference to `tgetflag'
>>>
>>>/usr/local/lib/libreadline.so: undefined reference to `BC'
>>>
>>>/usr/local/lib/libreadline.so: undefined reference to `tputs'
>>>
>>>/usr/local/lib/libreadline.so: undefined reference to `PC'
>>>
>>>/usr/local/lib/libreadline.so: undefined reference to `tgetent'
>>>
>>>/usr/local/lib/libreadline.so: undefined reference to `UP'
>>>
>>>/usr/local/lib/libreadline.so: undefined reference to `tgetstr'
>>
>>...
>>
>>
>>>Any ideas? Thanks for any help you can provide.
>>
>>Missing ncurses or ncurses-devel, I believe.
>>
>>-- 
>>   O__  ---- Peter Dalgaard             Blegdamsvej 3  
>>  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>> (*) \(*) -- University of Copenhagen   Denmark      Ph: 
>>(+45) 35327918
>>~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: 
>>(+45) 35327907
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng



From rvaradha at jhsph.edu  Mon Jun 14 22:01:58 2004
From: rvaradha at jhsph.edu (Ravi Varadhan)
Date: Mon, 14 Jun 2004 16:01:58 -0400
Subject: [R] Quadruple precision in R
Message-ID: <E619BDBD99B4F74D9DCA32F43BE926714C7349@XCH-VN02.sph.ad.jhsph.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040614/842cff07/attachment.pl

From edd at debian.org  Mon Jun 14 22:07:54 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Mon, 14 Jun 2004 15:07:54 -0500
Subject: [R] Readline on R-1.9.1a
In-Reply-To: <40CE03FE.1060903@jhsph.edu>
References: <3A822319EB35174CA3714066D590DCD504AF7EB5@usrymx25.merck.com>
	<40CE03FE.1060903@jhsph.edu>
Message-ID: <20040614200754.GA16197@sonny.eddelbuettel.com>

On Mon, Jun 14, 2004 at 04:01:02PM -0400, Roger D. Peng wrote:
> People who compile from source still need to install the necessary rpms.

Which is why another Linux distribution uses the concept of Build-Depends :)

This can be complicated if you strive to include just about everything that
there is:

edd at chibud:~/src/debian/R/R-1.9.1/debian> grep Build-Depends control
Build-Depends: refblas3-dev | atlas3-base-dev, lapack3-dev |
atlas3-base-dev, libgnome-dev, libzvt-dev, libgtkxmhtml-dev, tcl8.4-dev,
tk8.4-dev, libglade-gnome0-dev, bison, g77 [!m68k], f2c [m68k], groff-base,
libncurses5-dev, libreadline4-dev, debhelper (>= 3.0.0), texi2html, texinfo
(>= 4.1-2), libbz2-dev, libpcre3-dev, libpaperg-dev, tetex-bin, tetex-extra,
xpdf-reader, libpaper-utils, zlib1g-dev, libpng12-dev, libjpeg62-dev,
libpcre3-dev


Dirk

-- 
FEATURE:  VW Beetle license plate in California



From andy_liaw at merck.com  Mon Jun 14 22:08:11 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 14 Jun 2004 16:08:11 -0400
Subject: [R] Readline on R-1.9.1a
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7EB7@usrymx25.merck.com>

> From: Roger D. Peng 
> 
> People who compile from source still need to install the 
> necessary rpms.

Sure, but apparently one can install the R rpm without those, and that's the
real problem.

Andy

 
> -roger
> 
> Liaw, Andy wrote:
> > Given this is turning out to be an FAQ, wouldn't it make 
> sense to have these
> > rpm's as depends for the R rpm?  (I always compile from 
> source, so don't
> > know what the R rpm actually depends on.)
> > 
> > Best,
> > Andy



From bates at wisc.edu  Mon Jun 14 22:21:09 2004
From: bates at wisc.edu (Douglas Bates)
Date: Mon, 14 Jun 2004 15:21:09 -0500
Subject: [R] extracting p-value of aov F-statistic
In-Reply-To: <87fz8y12uj.fsf@svenhartenstein.de>
References: <87fz8y12uj.fsf@svenhartenstein.de>
Message-ID: <40CE08B5.1080209@wisc.edu>

Sven Hartenstein wrote:
> Hi, 
> 
> I would like to extract the p-value of the F-statistic of a aov-object's
> summary. 
> 
> Getting the p-value is so easy with t-tests (t.test(g1, y = g2,
> var.equal = FALSE)$p.value), but I couldn't find anything like that for
> ANOVAs.
> 

Look at

str(summary(aov.object))

For example

example(aov)
str(summary(npk.aov))



From rossini at blindglobe.net  Mon Jun 14 22:23:23 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Mon, 14 Jun 2004 13:23:23 -0700
Subject: [R] Readline on R-1.9.1a
In-Reply-To: <20040614200754.GA16197@sonny.eddelbuettel.com> (Dirk
	Eddelbuettel's message of "Mon, 14 Jun 2004 15:07:54 -0500")
References: <3A822319EB35174CA3714066D590DCD504AF7EB5@usrymx25.merck.com>
	<40CE03FE.1060903@jhsph.edu>
	<20040614200754.GA16197@sonny.eddelbuettel.com>
Message-ID: <851xkhnb2c.fsf@servant.blindglobe.net>

Dirk Eddelbuettel <edd at debian.org> writes:

> On Mon, Jun 14, 2004 at 04:01:02PM -0400, Roger D. Peng wrote:
>> People who compile from source still need to install the necessary rpms.
>
> Which is why another Linux distribution uses the concept of Build-Depends :)

Which is another reason why I use that other Linux distribution...


-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From p.dalgaard at biostat.ku.dk  Mon Jun 14 22:20:06 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 Jun 2004 22:20:06 +0200
Subject: [R] Readline on R-1.9.1a
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7EB7@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF7EB7@usrymx25.merck.com>
Message-ID: <x2u0xd28p5.fsf@biostat.ku.dk>

"Liaw, Andy" <andy_liaw at merck.com> writes:

> > From: Roger D. Peng 
> > 
> > People who compile from source still need to install the 
> > necessary rpms.
> 
> Sure, but apparently one can install the R rpm without those, and that's the
> real problem.

(Wasn't Kevin's though. But there's really no way of helping people
who build from source, beyond what they get from configure. Well, you
could refer them to read the spec files...)

You can actually *build* the R rpm on SuSE without a lot of stuff...
Detlef has put absolutely no dependency information in the spec file.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From luke at stat.uiowa.edu  Mon Jun 14 22:29:34 2004
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Mon, 14 Jun 2004 15:29:34 -0500 (CDT)
Subject: [R] mkChar can be interrupted
In-Reply-To: <C698D707214E6F4AB39AB7096C3DE5A5568842@phost015.EVAFUNDS.intermedia.net>
Message-ID: <Pine.LNX.4.44.0406141511410.21001-100000@itasca.stat.uiowa.edu>

Not sure why you think this suggest mkChar can be interrupted.

If you want to figure out how interrupt handling works on unix, run
under gdb and single step from the signal to the next point where
R_CheckUserInterrupt is called.  You should find that the signal
handler sets a flag and that flag is checked at various safe points by
calls to this function.  I don't believe there are any such safe
points in mkChar, but there are several potential ones within your
example.

As mentioned in a reply in another thread, interrupt handling is one
aspect of R internals that is still evolving.  Among other things, we
will need to make changes as we improve support for other event loops.
[In applications with graphical interfaces signals are not the right
way to deal with user interruption (in particular on operating systems
that don't support proper signals)].

Best,

luke

On Mon, 14 Jun 2004, Vadim Ogranovich wrote:

> Hi,
>  
> As was discussed earlier in another thread and as documented in R-exts
> .Call() should not be interruptible by Ctrl-C. However the following
> code, which spends most of its time inside mkChar, turned out to be
> interruptible on RH-7.3 R-1.8.1 gcc-2.96:
>  
>  
> #include <Rinternals.h>
> #include <R.h>
> 
> SEXP foo0(const SEXP nSexp) {
>   int i, n;
>   SEXP resSexp;
> 
>   if (!isInteger(nSexp))
>     error("wrong arg type\n");
> 
>   n = asInteger(nSexp);
>   resSexp = PROTECT(allocVector(STRSXP, n));
>     
>   Rprintf("!!!time to interrup!!!\n");
>   for (i=0; i<n; ++i) {
>     SET_STRING_ELT(resSexp, i, mkChar("foo"));
>   }
> 
>   Rprintf("end mkChar\n");
>   UNPROTECT(1);
>     
>   return R_NilValue;
> }
> 
> 
> 
> # invoke 'foo0' and give it an argument large enough to let you type
> Ctrl-C
> # double the argument if you see "end mkChar" and do it again :-)
> > x <- .Call("foo0", as.integer(1e7))
> !!!time to interrup!!!
> 
> > 
> > version
>          _                
> platform i686-pc-linux-gnu
> arch     i686             
> os       linux-gnu        
> system   i686, linux-gnu  
> status                    
> major    1                
> minor    8.1              
> year     2003             
> month    11               
> day      21               
> language R                
> 
> 
> Thanks,
> Vadim
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Luke Tierney
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
   Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu



From ggrothendieck at myway.com  Mon Jun 14 22:34:15 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 14 Jun 2004 20:34:15 +0000 (UTC)
Subject: [R] terminology for frames and environments
References: <3AC14080-BDF6-11D8-ACC2-000A95CDA0F2@anu.edu.au>
	<Pine.A41.4.58.0406140710350.273254@homer10.u.washington.edu>
Message-ID: <loom.20040614T223039-960@post.gmane.org>

Thomas Lumley <tlumley <at> u.washington.edu> writes:

> The distinction between "environment" and "frame" is important. The frame
> is what you find things in with get(, inherits=FALSE) and the environment
> uses get(, environment=TRUE).

The thing I find odd about this one is that if we have:

e <- new.env()
e$x <- 1
f <- new.env(parent=e)
f$x  # gives an error

then I would have expected x to be returned since f is an environment
and x is in that environment.  On the other hand, if an environment
is defined to be the same as a frame (and there is some other word for 
an environment and its ancestors) then the above notation makes sense.



From DAVID.BICKEL at PIONEER.COM  Mon Jun 14 22:38:03 2004
From: DAVID.BICKEL at PIONEER.COM (Bickel, David)
Date: Mon, 14 Jun 2004 15:38:03 -0500
Subject: [R] interrupt in Linux
Message-ID: <5F883C17941B9F4E80E5FA8C9F1C5E0EBD5C28@jhms08.phibred.com>

Patrick,

control-c seems to interrupt loops, but not the display of enormous objects, as when I typed the name of a several-hundred megabyte list. Is there a way to interrupt that?

control-D does not interrupt R, but kills the entire R process.

Thanks,
David

-----Original Message-----
From: Patrick Burns [mailto:pburns at pburns.seanet.com]
Sent: Monday, June 14, 2004 3:02 PM
To: Bickel, David
Subject: Re: [R] interrupt in Linux


Are you sure that R won't interrupt?  Perhaps you are in some C
code that doesn't pay attention to the interrupt.

I have SuSe and control-C works on all of the versions of R that
I've tried.


Patrick Burns

Burns Statistics
patrick at burns-stat.com
+44 (0)20 8525 0696
http://www.burns-stat.com
(home of S Poetry and "A Guide for the Unwilling S User")

Bickel, David wrote:

>Does anyone know how to interrupt R in RedHat? Neither control-c nor Esc is working. What I don't want to do is close the window or kill the entire R process.
>
>Thanks,
>David
>
>
>This communication is for use by the intended recipient and ...{{dropped}}
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>
>  
>





This communication is for use by the intended recipient and ...{{dropped}}



From rpeng at jhsph.edu  Mon Jun 14 22:41:22 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Mon, 14 Jun 2004 16:41:22 -0400
Subject: [R] interrupt in Linux
In-Reply-To: <5F883C17941B9F4E80E5FA8C9F1C5E0EBD5C28@jhms08.phibred.com>
References: <5F883C17941B9F4E80E5FA8C9F1C5E0EBD5C28@jhms08.phibred.com>
Message-ID: <40CE0D72.9090201@jhsph.edu>

What version of R are you using?  This was  a problem in 1.8.0 but I 
think was fixed in 1.8.1.

-roger

Bickel, David wrote:
> Patrick,
> 
> control-c seems to interrupt loops, but not the display of enormous objects, as when I typed the name of a several-hundred megabyte list. Is there a way to interrupt that?
> 
> control-D does not interrupt R, but kills the entire R process.
> 
> Thanks,
> David
> 
> -----Original Message-----
> From: Patrick Burns [mailto:pburns at pburns.seanet.com]
> Sent: Monday, June 14, 2004 3:02 PM
> To: Bickel, David
> Subject: Re: [R] interrupt in Linux
> 
> 
> Are you sure that R won't interrupt?  Perhaps you are in some C
> code that doesn't pay attention to the interrupt.
> 
> I have SuSe and control-C works on all of the versions of R that
> I've tried.
> 
> 
> Patrick Burns
> 
> Burns Statistics
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
> 
> Bickel, David wrote:
> 
> 
>>Does anyone know how to interrupt R in RedHat? Neither control-c nor Esc is working. What I don't want to do is close the window or kill the entire R process.
>>
>>Thanks,
>>David
>>
>>
>>This communication is for use by the intended recipient and ...{{dropped}}
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>
>> 
>>
> 
> 
> 
> 
> 
> 
> This communication is for use by the intended recipient and ...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng



From DAVID.BICKEL at PIONEER.COM  Mon Jun 14 22:45:00 2004
From: DAVID.BICKEL at PIONEER.COM (Bickel, David)
Date: Mon, 14 Jun 2004 15:45:00 -0500
Subject: [R] interrupt in Linux
Message-ID: <5F883C17941B9F4E80E5FA8C9F1C5E0EBD5C29@jhms08.phibred.com>

> version     
         _                
platform i686-pc-linux-gnu
arch     i686             
os       linux-gnu        
system   i686, linux-gnu  
status                    
major    1                
minor    9.0              
year     2004             
month    04               
day      12               
language R

-----Original Message-----
From: Roger D. Peng [mailto:rpeng at jhsph.edu]
Sent: Monday, June 14, 2004 3:41 PM
To: Bickel, David
Cc: Patrick Burns; r-help at stat.math.ethz.ch
Subject: Re: [R] interrupt in Linux


What version of R are you using?  This was  a problem in 1.8.0 but I 
think was fixed in 1.8.1.

-roger

Bickel, David wrote:
> Patrick,
> 
> control-c seems to interrupt loops, but not the display of enormous objects, as when I typed the name of a several-hundred megabyte list. Is there a way to interrupt that?
> 
> control-D does not interrupt R, but kills the entire R process.
> 
> Thanks,
> David
> 
> -----Original Message-----
> From: Patrick Burns [mailto:pburns at pburns.seanet.com]
> Sent: Monday, June 14, 2004 3:02 PM
> To: Bickel, David
> Subject: Re: [R] interrupt in Linux
> 
> 
> Are you sure that R won't interrupt?  Perhaps you are in some C
> code that doesn't pay attention to the interrupt.
> 
> I have SuSe and control-C works on all of the versions of R that
> I've tried.
> 
> 
> Patrick Burns
> 
> Burns Statistics
> patrick at burns-stat.com
> +44 (0)20 8525 0696
> http://www.burns-stat.com
> (home of S Poetry and "A Guide for the Unwilling S User")
> 
> Bickel, David wrote:
> 
> 
>>Does anyone know how to interrupt R in RedHat? Neither control-c nor Esc is working. What I don't want to do is close the window or kill the entire R process.
>>
>>Thanks,
>>David
>>
>>
>>This communication is for use by the intended recipient and ...{{dropped}}
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>
>> 
>>
> 
> 
> 
> 
> 
> 
> This communication is for use by the intended recipient and ...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng

_____________________________
David Bickel  http://davidbickel.com
Research Scientist
Pioneer Hi-Bred International
Bioinformatics & Discovery Research
7250 NW 62nd Ave., PO Box 552
Johnston, Iowa 50131-0552
515-270-0220 Home
515-334-4739 Work
515-334-6634 Fax
david.bickel at pioneer.com, bickel at prueba.info


This communication is for use by the intended recipient and ...{{dropped}}



From tlumley at u.washington.edu  Mon Jun 14 22:49:16 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 14 Jun 2004 13:49:16 -0700 (PDT)
Subject: [R] terminology for frames and environments
In-Reply-To: <loom.20040614T223039-960@post.gmane.org>
References: <3AC14080-BDF6-11D8-ACC2-000A95CDA0F2@anu.edu.au>
	<Pine.A41.4.58.0406140710350.273254@homer10.u.washington.edu>
	<loom.20040614T223039-960@post.gmane.org>
Message-ID: <Pine.A41.4.58.0406141341590.153096@homer12.u.washington.edu>

On Mon, 14 Jun 2004, Gabor Grothendieck wrote:

> Thomas Lumley <tlumley <at> u.washington.edu> writes:
>
> > The distinction between "environment" and "frame" is important. The frame
> > is what you find things in with get(, inherits=FALSE) and the environment
> > uses get(, environment=TRUE).
>
> The thing I find odd about this one is that if we have:
>
> e <- new.env()
> e$x <- 1
> f <- new.env(parent=e)
> f$x  # gives an error
>
> then I would have expected x to be returned since f is an environment
> and x is in that environment.  On the other hand, if an environment
> is defined to be the same as a frame (and there is some other word for
> an environment and its ancestors) then the above notation makes sense.
>

Maybe. On the other hand, $ *is* documented to look only in the first
frame of the environment.  It only doesn't make sense if you think of an
environment as a flat vector.  If you think of the nested structure then
it does make sense and is analogous to what happens with more complicated
lists and expressions.

If you had
   e<-quote(b*x)
   f<- substitute(a+tmp,list(tmp=e))
x would not appear as an element of f, but would appear as an element of
an element (yes, I do realise the tree is upside-down compared to an tree
of environments).

	-thomas



From scott.waichler at pnl.gov  Mon Jun 14 22:58:24 2004
From: scott.waichler at pnl.gov (Scott Waichler)
Date: Mon, 14 Jun 2004 13:58:24 -0700
Subject: [R] polygons around clusters of identically valued nodes in
	levelplot()
Message-ID: <200406142058.i5EKwOA32058@snow.pnl.gov>


I'm looking for a way to plot lines on top of a levelplot(),
where the lines are borders between cells of different values.
The clines() function provides contours suitable for continuous
data.  I am dealing with discrete values, spatial clusters of nodes 
where each cluster has an integer value, and I want to plot the 
borderlines between these areas.
So, in a levelplot having nodes with values 0, 1, or 2, I would
like to plot lines between nodes having the values 0 and 1, 0 and 2 and 0 and 2.
The contouring concept gives two lines for adjacent nodes having values
of 0 and 1.

Thanks,
Scott Waichler
Pacific Northwest National Laboratory
Richland, Washington, USA
scott.waichler at pnl.gov



From partha_bagchi at hgsi.com  Mon Jun 14 23:05:10 2004
From: partha_bagchi at hgsi.com (partha_bagchi@hgsi.com)
Date: Mon, 14 Jun 2004 17:05:10 -0400
Subject: [R] How to 'stamp' a plot with meta-data?
Message-ID: <OF9C2E23E5.34EB084E-ON85256EB3.0073AE3E-85256EB3.0073D501@hgsi.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040614/11ab0e6d/attachment.pl

From Ted.Harding at nessie.mcc.ac.uk  Mon Jun 14 22:21:40 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 14 Jun 2004 21:21:40 +0100 (BST)
Subject: [R] interrupt in Linux
In-Reply-To: <5F883C17941B9F4E80E5FA8C9F1C5E0EBD5C26@jhms08.phibred.com>
Message-ID: <XFMail.040614212140.Ted.Harding@nessie.mcc.ac.uk>

On 14-Jun-04 Bickel, David wrote:
> Does anyone know how to interrupt R in RedHat? Neither control-c nor
> Esc is working. What I don't want to do is close the window or kill the
> entire R process.
> 
> Thanks,
> David

Ctrl-C should interrupt what R is doing and return you to the R command
prompt. ESC (at the R command prompt) will probably be seen by 'readline'
as the lead-in to an escape sequence, with possibly unpredictable
results.

For example,

> for(i in (1:10^5)){rnorm(10)}

takes about 20 seconds on my old laptop. However, if I start it again,
and press Ctrl-C after a couple os seconds, then it stops almost
immediately.

Red Hat 9.1, R version 1.8.0

But I think R may need to unwind things that it has set up before
returning to the prompt when you press Ctrl-C, so if what you were
doing has made a lot of heavy stuff, then it might take a while
before you get the prompt back.

If Ctrl-C really won't work at all, then you may have an stty
problem.

In a normal xterm window (shell prompt) enter

  stty -a

which should return a lot of stuff beginning like:

> stty -a
speed 38400 baud; rows 56; columns 80; line = 0;
intr = ^C; quit = ^\; erase = ^?; kill = ^U; eof = ^D;
eol = <undef>; ... ...

and you need to see the "intr = ^C". If not, then you don't have
Ctrl-C set as interrupt. In which case, consult "man stty"!

Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 14-Jun-04                                       Time: 21:21:40
------------------------------ XFMail ------------------------------



From zguan at usc.edu  Mon Jun 14 23:16:39 2004
From: zguan at usc.edu (Zhixin Guan)
Date: Mon, 14 Jun 2004 14:16:39 -0700
Subject: [R] olesolve: stepsize
Message-ID: <200406142116.i5ELGpbg006862@hypatia.math.ethz.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040614/c4e78e51/attachment.pl

From dsmall at wharton.upenn.edu  Mon Jun 14 23:19:16 2004
From: dsmall at wharton.upenn.edu (Small, Dylan)
Date: Mon, 14 Jun 2004 17:19:16 -0400
Subject: [R] glmmML package
Message-ID: <0DE40D17E4B6D34F9CB8CB444DECF63302C95109@COURIER.wharton.upenn.edu>

I'm trying to use the glmmML package on a Windows machine.   When I try to install the package, I get the message:

> {pkg <- select.list(sort(.packages(all.available = TRUE)))
+ if(nchar(pkg)) library(pkg, character.only=TRUE)}
Error in dyn.load(x, as.logical(local), as.logical(now)) : 
        unable to load shared library "C:/PROGRA~1/R/rw1051/library/glmmML/libs/glmmML.dll":
  LoadLibrary failure:  The specified procedure could not be found.
In addition: Warning message: 
package glmmML was built under R version 1.9.0 
Error in library(pkg, character.only = TRUE) : 
        .First.lib failed 

Does anybody know how to correct this problem?  Thanks.

Dylan Small



From kbartz at loyaltymatrix.com  Mon Jun 14 23:15:24 2004
From: kbartz at loyaltymatrix.com (Kevin Bartz)
Date: Mon, 14 Jun 2004 14:15:24 -0700
Subject: [R] Readline on R-1.9.1a
In-Reply-To: <x2u0xd28p5.fsf@biostat.ku.dk>
Message-ID: <20040614212007.B5FB64123A@omta12.mta.everyone.net>

Hi Peter! Thanks so much for your help. Installing ncurses and ncurses-devel
did the trick for me, and now readline's working fine in R. Now how did you
manage to notice that?

Kevin

-----Original Message-----
From: Peter Dalgaard [mailto:p.dalgaard at biostat.ku.dk] 
Sent: Monday, June 14, 2004 1:20 PM
To: Liaw, Andy
Cc: 'Roger D. Peng'; 'Peter Dalgaard'; Kevin Bartz; r-help at stat.math.ethz.ch
Subject: Re: [R] Readline on R-1.9.1a

"Liaw, Andy" <andy_liaw at merck.com> writes:

> > From: Roger D. Peng 
> > 
> > People who compile from source still need to install the 
> > necessary rpms.
> 
> Sure, but apparently one can install the R rpm without those, and that's
the
> real problem.

(Wasn't Kevin's though. But there's really no way of helping people
who build from source, beyond what they get from configure. Well, you
could refer them to read the spec files...)

You can actually *build* the R rpm on SuSE without a lot of stuff...
Detlef has put absolutely no dependency information in the spec file.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ggrothendieck at myway.com  Mon Jun 14 23:33:19 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 14 Jun 2004 21:33:19 +0000 (UTC)
Subject: [R] terminology for frames and environments
References: <3AC14080-BDF6-11D8-ACC2-000A95CDA0F2@anu.edu.au>
	<Pine.A41.4.58.0406140710350.273254@homer10.u.washington.edu>
	<loom.20040614T223039-960@post.gmane.org>
	<Pine.A41.4.58.0406141341590.153096@homer12.u.washington.edu>
Message-ID: <loom.20040614T232747-526@post.gmane.org>

Thomas Lumley <tlumley <at> u.washington.edu> writes:

: If you think of the nested structure then
: it does make sense and is analogous to what happens with more complicated
: lists and expressions.
: 
: If you had
:    e<-quote(b*x)
:    f<- substitute(a+tmp,list(tmp=e))
: x would not appear as an element of f, but would appear as an element of
: an element (yes, I do realise the tree is upside-down compared to an tree
: of environments). 

To me this is not inconsistent with a list being analogous to a frame.
In both cases a deep search does not take place.    Thus a frame is a
shallow structure and an environment defined as a frame and its ancestors
is a deep structure (with respect to its parent) so I would expect that 
the deep structure would search deeply.



From Ted.Harding at nessie.mcc.ac.uk  Mon Jun 14 23:43:27 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Mon, 14 Jun 2004 22:43:27 +0100 (BST)
Subject: [R] error with barplot command?
In-Reply-To: <lfprc0hfasanks6uhn1auu06o7kkmiku75@4ax.com>
Message-ID: <XFMail.040614224327.Ted.Harding@nessie.mcc.ac.uk>


On 14-Jun-04 Duncan Murdoch wrote:
> On Tue, 15 Jun 2004 01:40:49 +0800, ???????????? <0034058 at fudan.edu.cn>
> wrote :
> 
>>when i use barplot ,it seems there is sth wrong with it.
>>my command are:
>>> beer = scan()
>>1: 3 4 1 1 3 4 3 3 1 3 2 1 2 1 2 3 2 3 1 1 1 1 4 3 1
>>26:
>>Read 25 items
>>> barplot(table(beer))
>>
>>but it does NOT produce what i want.
> 
> Please try 1.9.1 beta.  This should be fixed now...

Hmmm.
R
major    1
minor    8.0
year     2003

table(beer)
beer
 1  2  3  4 
10  4  8  3

and barplot(table(beer)) looks fine! Does this mean that barplot()
got un-fixed between 1.8.0 and 1.9.0?

Ted.




--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 14-Jun-04                                       Time: 22:43:27
------------------------------ XFMail ------------------------------



From p.dalgaard at biostat.ku.dk  Mon Jun 14 23:53:02 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 14 Jun 2004 23:53:02 +0200
Subject: [R] terminology for frames and environments
In-Reply-To: <loom.20040614T223039-960@post.gmane.org>
References: <3AC14080-BDF6-11D8-ACC2-000A95CDA0F2@anu.edu.au>
	<Pine.A41.4.58.0406140710350.273254@homer10.u.washington.edu>
	<loom.20040614T223039-960@post.gmane.org>
Message-ID: <x2pt8124e9.fsf@biostat.ku.dk>

Gabor Grothendieck <ggrothendieck at myway.com> writes:

> Thomas Lumley <tlumley <at> u.washington.edu> writes:
> 
> > The distinction between "environment" and "frame" is important. The frame
> > is what you find things in with get(, inherits=FALSE) and the environment
> > uses get(, environment=TRUE).
> 
> The thing I find odd about this one is that if we have:
> 
> e <- new.env()
> e$x <- 1
> f <- new.env(parent=e)
> f$x  # gives an error

(Actually not. I get NULL)


> then I would have expected x to be returned since f is an environment
> and x is in that environment.  On the other hand, if an environment
> is defined to be the same as a frame (and there is some other word for 
> an environment and its ancestors) then the above notation makes sense.

Why?  f$x is documented to be basically equivalent to
get("x",f,inherits=FALSE). In contrast,

> evalq(x,f)
[1] 1

works fine.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From macq at llnl.gov  Tue Jun 15 00:10:30 2004
From: macq at llnl.gov (Don MacQueen)
Date: Mon, 14 Jun 2004 15:10:30 -0700
Subject: [R] How to 'stamp' a plot with meta-data?
In-Reply-To: <Pine.LNX.4.44.0406141148470.24733-100000@cezanne.fhcrc.org>
References: <Pine.LNX.4.44.0406141148470.24733-100000@cezanne.fhcrc.org>
Message-ID: <p0600200bbcf3d1880924@[128.115.153.6]>

I use this little function for adding a datestamp manually.

  runstamp <-  function (adj = 1, cex = 0.75, line = 4)
        mtext(format(Sys.time()), side = 1, adj = adj, cex = cex, line = line)

Since the default lower margin is 5.1, line = 4 works pretty well. 
You can, of course, put any other information you want in there. But 
to have it all automated looks to me like a major task. I wouldn't 
get my hopes up.

At 12:17 PM -0700 6/14/04, Itay Furman wrote:
>Dear R users,
>
>Sometimes, for tracking purposes, I am interested to add to a
>plot some metadata such as
>* the date it was produced
>* filename that stores the plot
>* perhaps data sources, author, etc
>
>Ideally, I would like to be able to do this for any kind of plot,
>plot(), barplot(), hist(), etc.; and, to be able to produce
>plots with or without the metadata by a simple toggle mechanism.
>
>Something like:
>
># 'Clean' plot
>some.plot.func(<args>)	# plot() or barplot() or ...
>
># Now with metadata
>options(metadata=TRUE)	# Or some other toggle mechanism
>
>some.plot.func(<args>)	# Same plot as above, but with wider
>			# bottom margins that show the
>			# metadata.
>
>So far I looked in "Introduction to R" and tried to find hints
>using help.search() without success.
>
>Is it possilbe to do?
>Any suggestions or pointers as to how to do it are appreciated.
>Even partial solution in which the plot is set up to accomodate
>metadata in the margins, but the data needs to be added manually
>after plot()ting will be great.
>
>	Thanks in advance,
>	Itay
>
>--------------------------------------------------------------
>itayf at fhcrc.org		Fred Hutchinson Cancer Research Center
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From p.dalgaard at biostat.ku.dk  Tue Jun 15 00:07:28 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Jun 2004 00:07:28 +0200
Subject: [R] olesolve: stepsize
In-Reply-To: <200406142116.i5ELGpbg006862@hypatia.math.ethz.ch>
References: <200406142116.i5ELGpbg006862@hypatia.math.ethz.ch>
Message-ID: <x2llip23q7.fsf@biostat.ku.dk>

"Zhixin Guan" <zguan at usc.edu> writes:

> Hi, 
> 
> I am doing a project on the simulation of glucose metabolism based on a
> pharmacokinetic modeling in which we have 4 differential equations. I did
> this in R by using the odesolve package. It works very well, but I have two
> questions:
> 
>  
> 
> Here is the odemodel function
> 
> _________________________________________________
> 
> Ogtt.Odemodel <- function(t, y, p) { 
.....
> }

> First is that The computing time for each individual simulation is about 6
> sec if I set up time from 0 - 360 min and calculate on each minute.  In
> order to reduce computing time, I want to calculate for every two minutes,
> but I don't know how to do it.  I try to make t to be t <- seq(0, 360, by =
> 2), however, the computing time is still about 6 sec instead of 3.  I also
> found that there is a hmin parameter in odesolve. But again, it doesn't
> work.

The thing you're not telling us is what solver you are using! lsoda(),
I suppose. In that case, the times you specify are just read-off
times, whereas the actual integration proceeds in potentially much
smaller time step. If you want the solver to take larger steps, you
should probably loosen the tolerances and maybe also increase hmin. 
  
> 
> The second question is that I found that some time, the last two or three
> points of calculation is NA.   

This might be due to using approxfun with values beyond the end of
your sx data.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Tue Jun 15 00:11:32 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Jun 2004 00:11:32 +0200
Subject: [R] error with barplot command?
In-Reply-To: <XFMail.040614224327.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.040614224327.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <x2hdtd23jf.fsf@biostat.ku.dk>

(Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:

> > 
> > Please try 1.9.1 beta.  This should be fixed now...
> 
> Hmmm.
> R
> major    1
> minor    8.0
> year     2003
> 
> table(beer)
> beer
>  1  2  3  4 
> 10  4  8  3
> 
> and barplot(table(beer)) looks fine! Does this mean that barplot()
> got un-fixed between 1.8.0 and 1.9.0?

Yes. (Actually, fixed incorrectly, the 1.9.1 version should be finer
than 1.8.1's)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From vograno at evafunds.com  Tue Jun 15 00:21:31 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Mon, 14 Jun 2004 15:21:31 -0700
Subject: [R] mkChar can be interrupted
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A5568850@phost015.EVAFUNDS.intermedia.net>

 

> -----Original Message-----
> From: Luke Tierney [mailto:luke at stat.uiowa.edu] 
> Sent: Monday, June 14, 2004 1:30 PM
> To: Vadim Ogranovich
> Cc: R-Help
> Subject: Re: [R] mkChar can be interrupted
> 
> Not sure why you think this suggest mkChar can be interrupted.
> 
> If you want to figure out how interrupt handling works on 
> unix, run under gdb and single step from the signal to the 
> next point where R_CheckUserInterrupt is called.  You should 
> find that the signal handler sets a flag and that flag is 
> checked at various safe points by calls to this function.  I 
> don't believe there are any such safe points in mkChar, but 
> there are several potential ones within your example.

Apart from mkChar I am only calling SET_STRING_ELT. Is this what you
mean?

To make sure, I am not trying to enable or handle interrupts. On the
contrary, I want them to be disabled for the duration of .Call, which is
what I thought R was supposed to do for me. I am surprised it didn't.

As to why I singled out mkChar, well, strictly speaking it is
'SET_STRING_ELT(resSexp, i, mkChar("foo"))' where the interrupt somehow
goes through. But SET_STRING_ELT is much faster than mkChar so I guessed
that it must be mkChar.
Anyway, be it SET_STRING_ELT or mkChar, the interrupt should have been
blocked.


As an additional check I tried to interrupt right before and right after
  for (i=0; i<n; ++i) {
    SET_STRING_ELT(resSexp, i, mkChar("foo"));
  }
but the interrupt was rightfully blocked.


I understand that .Call blocks interrupts, but it seems that very
primitive functions like SET_STRING_ELT, mkChar can decide to handle
them. This is surprising and I think my conjecture is wrong, but how
else to explain that I was able to interrupt 'foo0'?

Thanks,
Vadim

> 
> As mentioned in a reply in another thread, interrupt handling 
> is one aspect of R internals that is still evolving.  Among 
> other things, we will need to make changes as we improve 
> support for other event loops.
> [In applications with graphical interfaces signals are not 
> the right way to deal with user interruption (in particular 
> on operating systems that don't support proper signals)].
> 
> Best,
> 
> luke
> 
> On Mon, 14 Jun 2004, Vadim Ogranovich wrote:
> 
> > Hi,
> >  
> > As was discussed earlier in another thread and as 
> documented in R-exts
> > .Call() should not be interruptible by Ctrl-C. However the 
> following 
> > code, which spends most of its time inside mkChar, turned out to be 
> > interruptible on RH-7.3 R-1.8.1 gcc-2.96:
> >  
> >  
> > #include <Rinternals.h>
> > #include <R.h>
> > 
> > SEXP foo0(const SEXP nSexp) {
> >   int i, n;
> >   SEXP resSexp;
> > 
> >   if (!isInteger(nSexp))
> >     error("wrong arg type\n");
> > 
> >   n = asInteger(nSexp);
> >   resSexp = PROTECT(allocVector(STRSXP, n));
> >     
> >   Rprintf("!!!time to interrup!!!\n");
> >   for (i=0; i<n; ++i) {
> >     SET_STRING_ELT(resSexp, i, mkChar("foo"));
> >   }
> > 
> >   Rprintf("end mkChar\n");
> >   UNPROTECT(1);
> >     
> >   return R_NilValue;
> > }
> > 
> > 
> > 
> > # invoke 'foo0' and give it an argument large enough to let 
> you type 
> > Ctrl-C # double the argument if you see "end mkChar" and do 
> it again 
> > :-)
> > > x <- .Call("foo0", as.integer(1e7))
> > !!!time to interrup!!!
> > 
> > > 
> > > version
> >          _                
> > platform i686-pc-linux-gnu
> > arch     i686             
> > os       linux-gnu        
> > system   i686, linux-gnu  
> > status                    
> > major    1                
> > minor    8.1              
> > year     2003             
> > month    11               
> > day      21               
> > language R                
> > 
> > 
> > Thanks,
> > Vadim
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> 
> --
> Luke Tierney
> University of Iowa                  Phone:             319-335-3386
> Department of Statistics and        Fax:               319-335-3017
>    Actuarial Science
> 241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
> Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu
> 
> 
>



From tlumley at u.washington.edu  Tue Jun 15 00:49:01 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Mon, 14 Jun 2004 15:49:01 -0700 (PDT)
Subject: [R] glmmML package
In-Reply-To: <0DE40D17E4B6D34F9CB8CB444DECF63302C95109@COURIER.wharton.upenn.edu>
References: <0DE40D17E4B6D34F9CB8CB444DECF63302C95109@COURIER.wharton.upenn.edu>
Message-ID: <Pine.A41.4.58.0406141548000.153096@homer12.u.washington.edu>

On Mon, 14 Jun 2004, Small, Dylan wrote:

> I'm trying to use the glmmML package on a Windows machine.   When I try to install the package, I get the message:
>
> > {pkg <- select.list(sort(.packages(all.available = TRUE)))
> + if(nchar(pkg)) library(pkg, character.only=TRUE)}
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>         unable to load shared library "C:/PROGRA~1/R/rw1051/library/glmmML/libs/glmmML.dll":
>   LoadLibrary failure:  The specified procedure could not be found.
> In addition: Warning message:
> package glmmML was built under R version 1.9.0
> Error in library(pkg, character.only = TRUE) :
>         .First.lib failed
>
> Does anybody know how to correct this problem?  Thanks.

Upgrading your version of R would be a good start -- it looks as though
you are still using 1.5.1.  This might fix the problem, and if it doesn't,
it will at least make it easier for people to duplicate it and help you.

	-thomas



From luke at stat.uiowa.edu  Tue Jun 15 01:03:18 2004
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Mon, 14 Jun 2004 18:03:18 -0500 (CDT)
Subject: [R] mkChar can be interrupted
In-Reply-To: <C698D707214E6F4AB39AB7096C3DE5A5568850@phost015.EVAFUNDS.intermedia.net>
Message-ID: <Pine.LNX.4.44.0406141740040.9466-100000@itasca2.stat.uiowa.edu>

On Mon, 14 Jun 2004, Vadim Ogranovich wrote:

> > -----Original Message-----
> > From: Luke Tierney [mailto:luke at stat.uiowa.edu] 
> > Sent: Monday, June 14, 2004 1:30 PM
> > To: Vadim Ogranovich
> > Cc: R-Help
> > Subject: Re: [R] mkChar can be interrupted
> > 
> > Not sure why you think this suggest mkChar can be interrupted.
> > 
> > If you want to figure out how interrupt handling works on 
> > unix, run under gdb and single step from the signal to the 
> > next point where R_CheckUserInterrupt is called.  You should 
> > find that the signal handler sets a flag and that flag is 
> > checked at various safe points by calls to this function.  I 
> > don't believe there are any such safe points in mkChar, but 
> > there are several potential ones within your example.
> 
> Apart from mkChar I am only calling SET_STRING_ELT. Is this what you
> mean?

You are printing, you have an assignment expression, all of those
contain points where an interrupt could be checked for.

> To make sure, I am not trying to enable or handle interrupts. On the
> contrary, I want them to be disabled for the duration of .Call, which is
> what I thought R was supposed to do for me. I am surprised it didn't.
> 
> As to why I singled out mkChar, well, strictly speaking it is
> 'SET_STRING_ELT(resSexp, i, mkChar("foo"))' where the interrupt somehow
> goes through. But SET_STRING_ELT is much faster than mkChar so I guessed
> that it must be mkChar.
> Anyway, be it SET_STRING_ELT or mkChar, the interrupt should have been
> blocked.

Not sure why you are guessing since you can find out for sure using gdb.

Looking at the current definition of the END_SUSPEND_INTERRUPTS macro
it seems that on systems where user interrupt uses signals a pending
interrupt will be handled at the end of a GC by calling onintr. So on
unix systems mkChar would contain a safe point.  This may be where the
interrupt is handled in your case.  If you want to you can confirm
this by setting a gdb breakpoint in onintr (or perhaps Rf_onintr).
(We should probably add a way of checking of user interrupts here on
non-unix systems too, though care is needed on performance.)

> As an additional check I tried to interrupt right before and right after
>   for (i=0; i<n; ++i) {
>     SET_STRING_ELT(resSexp, i, mkChar("foo"));
>   }
> but the interrupt was rightfully blocked.
> 
> 
> I understand that .Call blocks interrupts, but it seems that very
> primitive functions like SET_STRING_ELT, mkChar can decide to handle
> them. This is surprising and I think my conjecture is wrong, but how
> else to explain that I was able to interrupt 'foo0'?

At this point it looks like nothing other than GC and some graphics
code actually blocks interrupts.  Whether any ot these are still
necessary given that we no longer longjmp oout of the signal handler
is not clear. Interrputs are checked for at places where it is safe to
do a non-local exit, which on unix systems includes at the end of a
GC.  Any C code that uses R allocation has to be robust to non-local
exits out of allocation calls since out of memory situations will also
cause a non-local exit.  Similarly any C code that calls into the R
API has to be robust to non-local exits if the internal code can
generate an error, and for this purpose a user interrupt is analogous
to any other error.

Best,

luke

> > As mentioned in a reply in another thread, interrupt handling 
> > is one aspect of R internals that is still evolving.  Among 
> > other things, we will need to make changes as we improve 
> > support for other event loops.
> > [In applications with graphical interfaces signals are not 
> > the right way to deal with user interruption (in particular 
> > on operating systems that don't support proper signals)].
> > 
> > Best,
> > 
> > luke
> > 
> > On Mon, 14 Jun 2004, Vadim Ogranovich wrote:
> > 
> > > Hi,
> > >  
> > > As was discussed earlier in another thread and as 
> > documented in R-exts
> > > .Call() should not be interruptible by Ctrl-C. However the 
> > following 
> > > code, which spends most of its time inside mkChar, turned out to be 
> > > interruptible on RH-7.3 R-1.8.1 gcc-2.96:
> > >  
> > >  
> > > #include <Rinternals.h>
> > > #include <R.h>
> > > 
> > > SEXP foo0(const SEXP nSexp) {
> > >   int i, n;
> > >   SEXP resSexp;
> > > 
> > >   if (!isInteger(nSexp))
> > >     error("wrong arg type\n");
> > > 
> > >   n = asInteger(nSexp);
> > >   resSexp = PROTECT(allocVector(STRSXP, n));
> > >     
> > >   Rprintf("!!!time to interrup!!!\n");
> > >   for (i=0; i<n; ++i) {
> > >     SET_STRING_ELT(resSexp, i, mkChar("foo"));
> > >   }
> > > 
> > >   Rprintf("end mkChar\n");
> > >   UNPROTECT(1);
> > >     
> > >   return R_NilValue;
> > > }
> > > 
> > > 
> > > 
> > > # invoke 'foo0' and give it an argument large enough to let 
> > you type 
> > > Ctrl-C # double the argument if you see "end mkChar" and do 
> > it again 
> > > :-)
> > > > x <- .Call("foo0", as.integer(1e7))
> > > !!!time to interrup!!!
> > > 
> > > > 
> > > > version
> > >          _                
> > > platform i686-pc-linux-gnu
> > > arch     i686             
> > > os       linux-gnu        
> > > system   i686, linux-gnu  
> > > status                    
> > > major    1                
> > > minor    8.1              
> > > year     2003             
> > > month    11               
> > > day      21               
> > > language R                
> > > 
> > > 
> > > Thanks,
> > > Vadim
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! 
> > > http://www.R-project.org/posting-guide.html
> > > 
> > 
> > --
> > Luke Tierney
> > University of Iowa                  Phone:             319-335-3386
> > Department of Statistics and        Fax:               319-335-3017
> >    Actuarial Science
> > 241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
> > Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu
> > 
> > 
> > 
> 
> 

-- 
Luke Tierney
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
   Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu



From goyal_ncsu at yahoo.com  Mon Jun 14 22:23:40 2004
From: goyal_ncsu at yahoo.com (Lovely Goyal)
Date: Mon, 14 Jun 2004 13:23:40 -0700 (PDT)
Subject: [R] Question about nlm function
Message-ID: <20040614202340.17943.qmail@web60207.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040614/9333593e/attachment.pl

From vograno at evafunds.com  Tue Jun 15 01:34:06 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Mon, 14 Jun 2004 16:34:06 -0700
Subject: [R] mkChar can be interrupted
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A5568853@phost015.EVAFUNDS.intermedia.net>

I am confused. Here is an excerpt from R-exts:

"As from R 1.8.0 no port of R can be interrupted whilst running long
computations in
compiled code,..."

Doesn't it imply that the primitive functions like allocVector, mkChar,
etc., which are likely to occur in any compiled code called via .Call,
are not supposed to handle interrupts in any way?

Thanks,
Vadim


> From: Luke Tierney [mailto:luke at stat.uiowa.edu] 
> 
> On Mon, 14 Jun 2004, Vadim Ogranovich wrote:
> 
> > > From: Luke Tierney [mailto:luke at stat.uiowa.edu]
...
> > > 
> > > Not sure why you think this suggest mkChar can be interrupted.
> > > 
...
> > > by calls to this function.  I don't believe there are any 
> such safe 
> > > points in mkChar, but there are several potential ones 
> within your 
> > > example.
> > 
> > Apart from mkChar I am only calling SET_STRING_ELT. Is this 
> what you 
> > mean?
> 
> You are printing, you have an assignment expression, all of 
> those contain points where an interrupt could be checked for.

These are not relevant since Ctrl-C is pressed when the code is inside
  for (i=0; i<n; ++i) {
    SET_STRING_ELT(resSexp, i, mkChar("foo"));
  }

Just look at the way I deliver the signal.



From goyal_ncsu at yahoo.com  Mon Jun 14 22:51:02 2004
From: goyal_ncsu at yahoo.com (Lovely Goyal)
Date: Mon, 14 Jun 2004 13:51:02 -0700 (PDT)
Subject: [R] question about nlm function
Message-ID: <20040614205102.26625.qmail@web60207.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040614/53696052/attachment.pl

From shadowrunner52 at hotmail.com  Tue Jun 15 02:22:08 2004
From: shadowrunner52 at hotmail.com (A Friend)
Date: Mon, 14 Jun 2004 20:22:08 -0400
Subject: [R] SJava
Message-ID: <BAY16-F109Z3Gktnr670002be01@hotmail.com>

Does anyone have any experience with SJava especially on Windows?



From luke at stat.uiowa.edu  Tue Jun 15 02:42:58 2004
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Mon, 14 Jun 2004 19:42:58 -0500 (CDT)
Subject: [R] mkChar can be interrupted
In-Reply-To: <C698D707214E6F4AB39AB7096C3DE5A5568853@phost015.EVAFUNDS.intermedia.net>
Message-ID: <Pine.LNX.4.44.0406141922540.9466-100000@itasca2.stat.uiowa.edu>

On Mon, 14 Jun 2004, Vadim Ogranovich wrote:

> I am confused. Here is an excerpt from R-exts:
> 
> "As from R 1.8.0 no port of R can be interrupted whilst running long
> computations in
> compiled code,..."
> 
> Doesn't it imply that the primitive functions like allocVector, mkChar,
> etc., which are likely to occur in any compiled code called via .Call,
> are not supposed to handle interrupts in any way?

No it does not.  Read the full context.  It says that if you wite a
piece of C code that may run a long time and you want to guarantee
that users will be able to interrupt your code then you should insure
that R_CheckUserInterrupt is called periodically.  If your code
already periodically calls other R code that checks for interrupts
then you may not need to do this yourself, but in general you do.

Prior to 1.8.0 on Unix-like systems the asynchronous signal handler
for SIGINT would longjmp to the nearest top level or browser context,
which meant that on these sytems any code was interruptible at any
point unless it was explicitly protected by a construct that suspended
interrupts.  Allowing interrupts at any point meant that inopportune
interrupts could and did crash R, which is why this was changed.

Unless there is explicit documentation to the contrary you should
assume that every function in the R API might allocate and might cause
a non-local exit (i.e. a longjmp) when an exception is raised (and an
interrupt is one of, but only one of, the exceptions that might
occur).

luke

> Thanks,
> Vadim
> 
> 
> > From: Luke Tierney [mailto:luke at stat.uiowa.edu] 
> > 
> > On Mon, 14 Jun 2004, Vadim Ogranovich wrote:
> > 
> > > > From: Luke Tierney [mailto:luke at stat.uiowa.edu]
> ...
> > > > 
> > > > Not sure why you think this suggest mkChar can be interrupted.
> > > > 
> ...
> > > > by calls to this function.  I don't believe there are any 
> > such safe 
> > > > points in mkChar, but there are several potential ones 
> > within your 
> > > > example.
> > > 
> > > Apart from mkChar I am only calling SET_STRING_ELT. Is this 
> > what you 
> > > mean?
> > 
> > You are printing, you have an assignment expression, all of 
> > those contain points where an interrupt could be checked for.
> 
> These are not relevant since Ctrl-C is pressed when the code is inside
>   for (i=0; i<n; ++i) {
>     SET_STRING_ELT(resSexp, i, mkChar("foo"));
>   }
> 
> Just look at the way I deliver the signal.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Luke Tierney
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
   Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu



From ggrothendieck at myway.com  Tue Jun 15 03:26:40 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 14 Jun 2004 21:26:40 -0400 (EDT)
Subject: [R] terminology for frames and environments
Message-ID: <20040615012640.397103980@mprdmxin.myway.com>


Peter Dalgaard <p.dalgaard at biostat.ku.dk>
> 
>  
> Gabor Grothendieck <ggrothendieck at myway.com> writes:
> 
> > Thomas Lumley <tlumley <at> u.washington.edu> writes:
> > 
> > > The distinction between "environment" and "frame" is important. The frame
> > > is what you find things in with get(, inherits=FALSE) and the environment
> > > uses get(, environment=TRUE).
> > 
> > The thing I find odd about this one is that if we have:
> > 
> > e <- new.env()
> > e$x <- 1
> > f <- new.env(parent=e)
> > f$x # gives an error
> 
> (Actually not. I get NULL)

Sure. 

> 
> 
> > then I would have expected x to be returned since f is an environment
> > and x is in that environment. On the other hand, if an environment
> > is defined to be the same as a frame (and there is some other word for 
> > an environment and its ancestors) then the above notation makes sense.
> 
> Why? f$x is documented to be basically equivalent to
> get("x",f,inherits=FALSE). In contrast,
> 
> > evalq(x,f)
> [1] 1
> 
> works fine.

Well this certainly is more environment-like if one defines environment
as a frame plus ancestors but it does not address the issue that 
if f is an environment in that sense then $ "should" pick out its
components.  One can define + to mean subtraction but that's not natural.

I agree that it would have been nice if environment had been used to
mean a frame plus its ancestors and class(f) was "frame", not "environment"
but, of course, that's not how it turned out.

I think its better to maintain backward compatiblity as far as possible and
make the terms suit rather than use environment in a way which is
inconsistent with the way R works.

In fact, it might have been nice if there were both a frame class and an
environment class so one could have either behavior.  With this new
setup R might have worked like this:

	# create parent environment 
        # (in the sense of frame+ancestors) & populate
	Pe <- new.env(): Pe$x <- 1

	# create child environment
	Ce <- new.env(parent=Pe)

	# create corresponding frames
	Pf <- as.frame(Pe); Cf <- as.frame(Ce)

	# x exists in child environment
	exists(Ce$x) # TRUE

	# but not in child frame
	exists(Cf$x) # FALSE

Unfortunately this can't be done without breaking backward compatibility
although it would still be possible to come up with a new word for
environment plus ancestors (call it the envlist class) using environment 
as a synonym for frame.  In that case this example would become:

	# create parent using envlist class & populate
	Pel <- new.envlist(): Pela$x <- 1

	# create child environment+ancestors class
	Cel <- new.envlist(parent=Pel)

	# create corresponding environments (in the sense of frames)
	Pe <- as.env(Pel); Ce <- as.env(Cel)

	# x exists in child environment
	exists(Cel$x) # TRUE

	# but not in child frame
	exists(Ce$x) # FALSE

and this one is upwardly compatible with the current R.



From c.wilcox at uq.edu.au  Tue Jun 15 03:28:54 2004
From: c.wilcox at uq.edu.au (Chris Wilcox)
Date: Tue, 15 Jun 2004 11:28:54 +1000
Subject: [R] mixed models question
Message-ID: <5D1AE822-BE6B-11D8-9DFA-000A95B0D258@uq.edu.au>

I am trying to fit the following linear model to logged per capita 
fecundity data (ie number of babies per female) for a mouse:

RsNRlS <- glm(formula = ln.fecundity ~ summer.rainfall + N + 
lagged.rainfall + season, ....)

I am using this relationship in a simulation model, and the current 
statistical model I have fit is unsatisfactory.  The problem is I get a 
global estimate of variance (MSE), but I think it varies across subsets 
of the data.  Specifically, seasons when there is lots of reproduction 
(e.g. fall) tend to have high variance, while seasons with little 
reproduction (e.g. summer) have small amounts of variance.  I am 
looking for a method for estimating the coefficients in my linear 
model, and estimating a separate error for subsets of the data (ie for 
each of the 4 seasons).  The end goal is to take this linear model back 
into my simulation model to make predictions about fecundity, but with 
separate variance terms for subsets of the data.

I thought of several solutions, but some seem pretty ad hoc, while I 
think others would suffer from loss of a lot of power due to small 
sample size.  The solutions I thought of were:
1)	Fit the linear model to all data, but then estimate variances by 
season directly from the residuals.  Seems ad hoc given the assumption 
of common variance in the initial fitting.
2)	Use regression weights by season to reweight the data points/fitted 
values.  Then in the simulation model inflate/deflate the estimate of 
MSE by these weights - seems ad hoc too, as translating weights (which 
would presumably basically downgrade the importance of outliers) into 
something meaningful for prediction using the regression equation seems 
arbitrary.
3)	Fit a separate model for each season, either assume the global model 
(but maybe get poor fits due to #parameter v. #data points) or try 
reducing models and choosing the best for each season using AIC.  The 
problem with this is that I think that some parameters have effects 
common to all seasons - eg lagged rainfall represents the effect of 
rainfall drowning babies in their burrows prior to the age at which 
they can be captured.  This is pretty clearly a strong effect, just 
based on graphing the data, so I am uncertain about loosing power by 
splitting it by season.
4)	Fit a mixed model to all the data, treating each season as a random 
factor with levels 1 or 0, and estimating the variance for each.  Again 
this seems somewhat ad hoc, although based on my reading in elementary 
texts it would allow me to estimate the variance component for each of 
the seasons.

Based on talking to a colleague in my department some of the modern 
methods for fitting mixed models, in which the variance for each of the 
levels of a factor in a random effect can be estimated.  That seems the 
best option, as it explicitly does what I want.  However, it is well 
over my head in terms of either the R code or the statistical 
background.  Can anyone suggest a good approach, or a published 
example, or some useful reading on how one might approach this problem?



Chris Wilcox
The Ecology Centre
Department of Zoology and Entomology
Brisbane, 4072  QLD
Australia
tel  61-7-3365-1686
fax 61-7-3365-1655
website www.uq.edu.au/~uqcwilco



From vograno at evafunds.com  Tue Jun 15 03:31:57 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Mon, 14 Jun 2004 18:31:57 -0700
Subject: [R] mkChar can be interrupted
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A556885A@phost015.EVAFUNDS.intermedia.net>

This is disappointing. How on Earth can mkChar know when it is safe or
not to make a long jump? For example if I just opened a file how am I
supposed to close it after the long jump? I am not even talking about
C++ where long jumps are simply devastating... (and this is the language
I am coding in :-( )


Ok. A practical question: is it possible to somehow block
R_CheckUserInterrupt? I am ready to put up with out-of-memory errors,
but Ctrl-C is too common to be ignored.

And I think it makes relevant again the question I asked in another
related thread: how is memory allocated by Calloc() and R_alloc() stand
up against long jumps?

Thanks,
Vadim 



> -----Original Message-----
> From: Luke Tierney [mailto:luke at stat.uiowa.edu] 
> Sent: Monday, June 14, 2004 5:43 PM
> To: Vadim Ogranovich
> Cc: R-Help
> Subject: RE: [R] mkChar can be interrupted
> 
> On Mon, 14 Jun 2004, Vadim Ogranovich wrote:
> 
> > I am confused. Here is an excerpt from R-exts:
> > 
> > "As from R 1.8.0 no port of R can be interrupted whilst 
> running long 
> > computations in compiled code,..."
> > 
> > Doesn't it imply that the primitive functions like allocVector, 
> > mkChar, etc., which are likely to occur in any compiled code called 
> > via .Call, are not supposed to handle interrupts in any way?
> 
> No it does not.  Read the full context.  It says that if you 
> wite a piece of C code that may run a long time and you want 
> to guarantee that users will be able to interrupt your code 
> then you should insure that R_CheckUserInterrupt is called 
> periodically.  If your code already periodically calls other 
> R code that checks for interrupts then you may not need to do 
> this yourself, but in general you do.
> 
> Prior to 1.8.0 on Unix-like systems the asynchronous signal 
> handler for SIGINT would longjmp to the nearest top level or 
> browser context, which meant that on these sytems any code 
> was interruptible at any point unless it was explicitly 
> protected by a construct that suspended interrupts.  Allowing 
> interrupts at any point meant that inopportune interrupts 
> could and did crash R, which is why this was changed.
> 
> Unless there is explicit documentation to the contrary you 
> should assume that every function in the R API might allocate 
> and might cause a non-local exit (i.e. a longjmp) when an 
> exception is raised (and an interrupt is one of, but only one 
> of, the exceptions that might occur).
> 
> luke
> 
> > Thanks,
> > Vadim
> > 
> > 
> > > From: Luke Tierney [mailto:luke at stat.uiowa.edu]
> > > 
> > > On Mon, 14 Jun 2004, Vadim Ogranovich wrote:
> > > 
> > > > > From: Luke Tierney [mailto:luke at stat.uiowa.edu]
> > ...
> > > > > 
> > > > > Not sure why you think this suggest mkChar can be interrupted.
> > > > > 
> > ...
> > > > > by calls to this function.  I don't believe there are any
> > > such safe
> > > > > points in mkChar, but there are several potential ones
> > > within your
> > > > > example.
> > > > 
> > > > Apart from mkChar I am only calling SET_STRING_ELT. Is this
> > > what you
> > > > mean?
> > > 
> > > You are printing, you have an assignment expression, all of those 
> > > contain points where an interrupt could be checked for.
> > 
> > These are not relevant since Ctrl-C is pressed when the 
> code is inside
> >   for (i=0; i<n; ++i) {
> >     SET_STRING_ELT(resSexp, i, mkChar("foo"));
> >   }
> > 
> > Just look at the way I deliver the signal.
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> 
> --
> Luke Tierney
> University of Iowa                  Phone:             319-335-3386
> Department of Statistics and        Fax:               319-335-3017
>    Actuarial Science
> 241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
> Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu
> 
> 
>



From ggrothendieck at myway.com  Tue Jun 15 03:45:03 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 14 Jun 2004 21:45:03 -0400 (EDT)
Subject: [R] terminology for frames and environments
Message-ID: <20040615014503.B446A12D1F@mprdmxin.myway.com>


There was a typo in the previous version of this message.
Pela should have been Pel.  Here it is corrected.

Peter Dalgaard <p.dalgaard at biostat.ku.dk>
> 
> 
> Gabor Grothendieck <ggrothendieck at myway.com> writes:
> 
> > Thomas Lumley <tlumley <at> u.washington.edu> writes:
> > 
> > > The distinction between "environment" and "frame" is important. The frame
> > > is what you find things in with get(, inherits=FALSE) and the environment
> > > uses get(, environment=TRUE).
> > 
> > The thing I find odd about this one is that if we have:
> > 
> > e <- new.env()
> > e$x <- 1
> > f <- new.env(parent=e)
> > f$x # gives an error
> 
> (Actually not. I get NULL)

Sure. 

> 
> 
> > then I would have expected x to be returned since f is an environment
> > and x is in that environment. On the other hand, if an environment
> > is defined to be the same as a frame (and there is some other word for 
> > an environment and its ancestors) then the above notation makes sense.
> 
> Why? f$x is documented to be basically equivalent to
> get("x",f,inherits=FALSE). In contrast,
> 
> > evalq(x,f)
> [1] 1
> 
> works fine.

Well this certainly is more environment-like if one defines environment
as a frame plus ancestors but it does not address the issue that 
if f is an environment in that sense then $ "should" pick out its
components. One can define + to mean subtraction but that's not natural.

I agree that it would have been nice if environment had been used to
mean a frame plus its ancestors and class(f) was "frame", not "environment"
but, of course, that's not how it turned out.

I think its better to maintain backward compatiblity as far as possible and
make the terms suit rather than use environment in a way which is
inconsistent with the way R works.

In fact, it might have been nice if there were both a frame class and an
environment class so one could have either behavior. With this new
setup R might have worked like this:

     # create parent environment 
# (in the sense of frame+ancestors) & populate
     Pe <- new.env(): Pe$x <- 1

     # create child environment
     Ce <- new.env(parent=Pe)

     # create corresponding frames
     Pf <- as.frame(Pe); Cf <- as.frame(Ce)

     # x exists in child environment
     exists(Ce$x) # TRUE

     # but not in child frame
     exists(Cf$x) # FALSE

Unfortunately this can't be done without breaking backward compatibility
although it would still be possible to come up with a new word for
environment plus ancestors (call it the envlist class) using environment 
as a synonym for frame. In that case this example would become:

     # create parent using envlist class & populate
     Pel <- new.envlist(): Pel$x <- 1

     # create child environment+ancestors class
     Cel <- new.envlist(parent=Pel)

     # create corresponding environments (in the sense of frames)
     Pe <- as.env(Pel); Ce <- as.env(Cel)

     # x exists in child environment
     exists(Cel$x) # TRUE

     # but not in child frame
     exists(Ce$x) # FALSE

and this one is upwardly compatible with the current R.



From luke at stat.uiowa.edu  Tue Jun 15 04:45:58 2004
From: luke at stat.uiowa.edu (Luke Tierney)
Date: Mon, 14 Jun 2004 21:45:58 -0500 (CDT)
Subject: [R] mkChar can be interrupted
In-Reply-To: <C698D707214E6F4AB39AB7096C3DE5A556885A@phost015.EVAFUNDS.intermedia.net>
Message-ID: <Pine.LNX.4.44.0406142100400.9466-100000@itasca2.stat.uiowa.edu>

On Mon, 14 Jun 2004, Vadim Ogranovich wrote:

> This is disappointing. How on Earth can mkChar know when it is safe or
> not to make a long jump? For example if I just opened a file how am I
> supposed to close it after the long jump? I am not even talking about
> C++ where long jumps are simply devastating... (and this is the language
> I am coding in :-( )
>
> Ok. A practical question: is it possible to somehow block
> R_CheckUserInterrupt? I am ready to put up with out-of-memory errors,
> but Ctrl-C is too common to be ignored.

Interrupts are not the issue.  The issue is making sure that cleanup
actions occur even if there is a non-local exit.  A solution that
addresses that issue will work for any non-local exit, whether it
comes from an interrupt or an exception.  So you don't have to put up
with anything if you approach this the right way,

Currently there is no user accessible C level try/finally mechanism
for insuring that cleanup code is executed during a non-local exit.
We should make such a mechanicm available; maybe one will make it into
the next major release.

For now you have two choices:

    You can create an R level object and attach a finalizer to the object
    that will arrange for the GC to close the file at some point in the
    future if a non-local exit occurs.  Search developer.r-project.org for
    finalization and weak references for some info on this.

    One other option is to use the R_ToplevelExec function.  This has some
    drawbacks since it effectively makes invisible all other error
    handlers, but it is an option.  It is also not officially documented
    and subject to change.

> And I think it makes relevant again the question I asked in another
> related thread: how is memory allocated by Calloc() and R_alloc() stand
> up against long jumps?

R_alloc is stack-based; the stack is unwound on a non-local exit, so
this is released on regular exits and non-local ones.  It uses R
allocation, so it could itself cause a non-local exit.

Calloc is like calloc but will never return NULL.  If the allocation
fails, then an error is signaled, which will result in a non-local
exit.  If the allocation succeeds, you are responsable for calling
Free.

luke

> > -----Original Message-----
> > From: Luke Tierney [mailto:luke at stat.uiowa.edu] 
> > Sent: Monday, June 14, 2004 5:43 PM
> > To: Vadim Ogranovich
> > Cc: R-Help
> > Subject: RE: [R] mkChar can be interrupted
> > 
> > On Mon, 14 Jun 2004, Vadim Ogranovich wrote:
> > 
> > > I am confused. Here is an excerpt from R-exts:
> > > 
> > > "As from R 1.8.0 no port of R can be interrupted whilst 
> > running long 
> > > computations in compiled code,..."
> > > 
> > > Doesn't it imply that the primitive functions like allocVector, 
> > > mkChar, etc., which are likely to occur in any compiled code called 
> > > via .Call, are not supposed to handle interrupts in any way?
> > 
> > No it does not.  Read the full context.  It says that if you 
> > wite a piece of C code that may run a long time and you want 
> > to guarantee that users will be able to interrupt your code 
> > then you should insure that R_CheckUserInterrupt is called 
> > periodically.  If your code already periodically calls other 
> > R code that checks for interrupts then you may not need to do 
> > this yourself, but in general you do.
> > 
> > Prior to 1.8.0 on Unix-like systems the asynchronous signal 
> > handler for SIGINT would longjmp to the nearest top level or 
> > browser context, which meant that on these sytems any code 
> > was interruptible at any point unless it was explicitly 
> > protected by a construct that suspended interrupts.  Allowing 
> > interrupts at any point meant that inopportune interrupts 
> > could and did crash R, which is why this was changed.
> > 
> > Unless there is explicit documentation to the contrary you 
> > should assume that every function in the R API might allocate 
> > and might cause a non-local exit (i.e. a longjmp) when an 
> > exception is raised (and an interrupt is one of, but only one 
> > of, the exceptions that might occur).
> > 
> > luke
> > 
> > > Thanks,
> > > Vadim
> > > 
> > > 
> > > > From: Luke Tierney [mailto:luke at stat.uiowa.edu]
> > > > 
> > > > On Mon, 14 Jun 2004, Vadim Ogranovich wrote:
> > > > 
> > > > > > From: Luke Tierney [mailto:luke at stat.uiowa.edu]
> > > ...
> > > > > > 
> > > > > > Not sure why you think this suggest mkChar can be interrupted.
> > > > > > 
> > > ...
> > > > > > by calls to this function.  I don't believe there are any
> > > > such safe
> > > > > > points in mkChar, but there are several potential ones
> > > > within your
> > > > > > example.
> > > > > 
> > > > > Apart from mkChar I am only calling SET_STRING_ELT. Is this
> > > > what you
> > > > > mean?
> > > > 
> > > > You are printing, you have an assignment expression, all of those 
> > > > contain points where an interrupt could be checked for.
> > > 
> > > These are not relevant since Ctrl-C is pressed when the 
> > code is inside
> > >   for (i=0; i<n; ++i) {
> > >     SET_STRING_ELT(resSexp, i, mkChar("foo"));
> > >   }
> > > 
> > > Just look at the way I deliver the signal.
> > > 
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide! 
> > > http://www.R-project.org/posting-guide.html
> > > 
> > 
> > --
> > Luke Tierney
> > University of Iowa                  Phone:             319-335-3386
> > Department of Statistics and        Fax:               319-335-3017
> >    Actuarial Science
> > 241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
> > Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu
> > 
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Luke Tierney
University of Iowa                  Phone:             319-335-3386
Department of Statistics and        Fax:               319-335-3017
   Actuarial Science
241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu



From vograno at evafunds.com  Tue Jun 15 05:56:58 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Mon, 14 Jun 2004 20:56:58 -0700
Subject: [R] mkChar can be interrupted
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A556885E@phost015.EVAFUNDS.intermedia.net>

Thank you. This gives hope, I am looking forward to the next major
release.

May I make a wish that the future try/finally mechanism will inlcude
support for C++ exceptions? For example by allowing the programmer to
set an error handler that raises a C++ exception. It should be easy in
error(), but might be a problem in interrupt handlers (I am not an
expert though).

Thanks,
Vadim

> -----Original Message-----
> From: Luke Tierney [mailto:luke at stat.uiowa.edu] 
> Sent: Monday, June 14, 2004 7:46 PM
> To: Vadim Ogranovich
> Cc: R-Help
> Subject: RE: [R] mkChar can be interrupted
> 
> On Mon, 14 Jun 2004, Vadim Ogranovich wrote:
> 
> > This is disappointing. How on Earth can mkChar know when it 
> is safe or 
> > not to make a long jump? For example if I just opened a 
> file how am I 
> > supposed to close it after the long jump? I am not even 
> talking about
> > C++ where long jumps are simply devastating... (and this is the 
> > C++ language
> > I am coding in :-( )
> >
> > Ok. A practical question: is it possible to somehow block 
> > R_CheckUserInterrupt? I am ready to put up with 
> out-of-memory errors, 
> > but Ctrl-C is too common to be ignored.
> 
> Interrupts are not the issue.  The issue is making sure that 
> cleanup actions occur even if there is a non-local exit.  A 
> solution that addresses that issue will work for any 
> non-local exit, whether it comes from an interrupt or an 
> exception.  So you don't have to put up with anything if you 
> approach this the right way,
> 
> Currently there is no user accessible C level try/finally 
> mechanism for insuring that cleanup code is executed during a 
> non-local exit.
> We should make such a mechanicm available; maybe one will 
> make it into the next major release.
> 
> For now you have two choices:
> 
>     You can create an R level object and attach a finalizer 
> to the object
>     that will arrange for the GC to close the file at some 
> point in the
>     future if a non-local exit occurs.  Search 
> developer.r-project.org for
>     finalization and weak references for some info on this.
> 
>     One other option is to use the R_ToplevelExec function.  
> This has some
>     drawbacks since it effectively makes invisible all other error
>     handlers, but it is an option.  It is also not officially 
> documented
>     and subject to change.
> 
> > And I think it makes relevant again the question I asked in another 
> > related thread: how is memory allocated by Calloc() and R_alloc() 
> > stand up against long jumps?
> 
> R_alloc is stack-based; the stack is unwound on a non-local 
> exit, so this is released on regular exits and non-local 
> ones.  It uses R allocation, so it could itself cause a 
> non-local exit.
> 
> Calloc is like calloc but will never return NULL.  If the 
> allocation fails, then an error is signaled, which will 
> result in a non-local exit.  If the allocation succeeds, you 
> are responsable for calling Free.
> 
> luke
> 
> > > -----Original Message-----
> > > From: Luke Tierney [mailto:luke at stat.uiowa.edu]
> > > Sent: Monday, June 14, 2004 5:43 PM
> > > To: Vadim Ogranovich
> > > Cc: R-Help
> > > Subject: RE: [R] mkChar can be interrupted
> > > 
> > > On Mon, 14 Jun 2004, Vadim Ogranovich wrote:
> > > 
> > > > I am confused. Here is an excerpt from R-exts:
> > > > 
> > > > "As from R 1.8.0 no port of R can be interrupted whilst
> > > running long
> > > > computations in compiled code,..."
> > > > 
> > > > Doesn't it imply that the primitive functions like allocVector, 
> > > > mkChar, etc., which are likely to occur in any compiled code 
> > > > called via .Call, are not supposed to handle interrupts 
> in any way?
> > > 
> > > No it does not.  Read the full context.  It says that if 
> you wite a 
> > > piece of C code that may run a long time and you want to 
> guarantee 
> > > that users will be able to interrupt your code then you should 
> > > insure that R_CheckUserInterrupt is called periodically.  If your 
> > > code already periodically calls other R code that checks for 
> > > interrupts then you may not need to do this yourself, but 
> in general 
> > > you do.
> > > 
> > > Prior to 1.8.0 on Unix-like systems the asynchronous 
> signal handler 
> > > for SIGINT would longjmp to the nearest top level or browser 
> > > context, which meant that on these sytems any code was 
> interruptible 
> > > at any point unless it was explicitly protected by a 
> construct that 
> > > suspended interrupts.  Allowing interrupts at any point 
> meant that 
> > > inopportune interrupts could and did crash R, which is 
> why this was 
> > > changed.
> > > 
> > > Unless there is explicit documentation to the contrary you should 
> > > assume that every function in the R API might allocate and might 
> > > cause a non-local exit (i.e. a longjmp) when an exception 
> is raised 
> > > (and an interrupt is one of, but only one of, the exceptions that 
> > > might occur).
> > > 
> > > luke
> > > 
> > > > Thanks,
> > > > Vadim
> > > > 
> > > > 
> > > > > From: Luke Tierney [mailto:luke at stat.uiowa.edu]
> > > > > 
> > > > > On Mon, 14 Jun 2004, Vadim Ogranovich wrote:
> > > > > 
> > > > > > > From: Luke Tierney [mailto:luke at stat.uiowa.edu]
> > > > ...
> > > > > > > 
> > > > > > > Not sure why you think this suggest mkChar can be 
> interrupted.
> > > > > > > 
> > > > ...
> > > > > > > by calls to this function.  I don't believe there are any
> > > > > such safe
> > > > > > > points in mkChar, but there are several potential ones
> > > > > within your
> > > > > > > example.
> > > > > > 
> > > > > > Apart from mkChar I am only calling SET_STRING_ELT. Is this
> > > > > what you
> > > > > > mean?
> > > > > 
> > > > > You are printing, you have an assignment expression, all of 
> > > > > those contain points where an interrupt could be checked for.
> > > > 
> > > > These are not relevant since Ctrl-C is pressed when the
> > > code is inside
> > > >   for (i=0; i<n; ++i) {
> > > >     SET_STRING_ELT(resSexp, i, mkChar("foo"));
> > > >   }
> > > > 
> > > > Just look at the way I deliver the signal.
> > > > 
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list 
> > > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide! 
> > > > http://www.R-project.org/posting-guide.html
> > > > 
> > > 
> > > --
> > > Luke Tierney
> > > University of Iowa                  Phone:             
> 319-335-3386
> > > Department of Statistics and        Fax:               
> 319-335-3017
> > >    Actuarial Science
> > > 241 Schaeffer Hall                  email:      
> luke at stat.uiowa.edu
> > > Iowa City, IA 52242                 WWW:  
> http://www.stat.uiowa.edu
> > > 
> > > 
> > >
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> 
> --
> Luke Tierney
> University of Iowa                  Phone:             319-335-3386
> Department of Statistics and        Fax:               319-335-3017
>    Actuarial Science
> 241 Schaeffer Hall                  email:      luke at stat.uiowa.edu
> Iowa City, IA 52242                 WWW:  http://www.stat.uiowa.edu
> 
> 
>



From nortonsm at verizon.net  Tue Jun 15 06:09:03 2004
From: nortonsm at verizon.net (Scott Norton)
Date: Tue, 15 Jun 2004 00:09:03 -0400
Subject: [R] Parsing results from boot
Message-ID: <005a01c4528e$7ee902d0$6401a8c0@scott>

This probably has a super easy answer...but I claim newbie status! (I did
search help lists but this question is hard to isolate keyword-wise)

Basically, I'm trying to figure out how to parse the results from executing
boot().  I'm mainly interested in assigning the standard error estimate to a
scalar variable.

For example:
--------------------------------------------------------------
> b<-c(100,100,120,130,1000,1200,1100,1150,125)
> b.boot <- boot(b, function(b,i) median(b[i]), R=1000)
> b.boot

ORDINARY NONPARAMETRIC BOOTSTRAP


Call:
boot(data = b, statistic = function(b, i) median(b[i]), R = 1000)


Bootstrap Statistics :
    original  bias    std. error
t1*      130  354.75    450.9763
> 
------------------------------------------------------------------
I'm interested in the value for std.error (i.e. 450.9763).  

Now executing the folling:
------------------------------------------------------------------
> summary(b.boot)
          Length Class  Mode     
t0           1   -none- numeric  
t         1000   -none- numeric  
R            1   -none- numeric  
data         9   -none- numeric  
seed       626   -none- numeric  
statistic    1   -none- function 
sim          1   -none- character
call         4   -none- call     
stype        1   -none- character
strata       9   -none- numeric  
weights      9   -none- numeric  
> 
-------------------------------------------------------------------
seems to imply that it's not actually assigned to an output variable...but
it is shown when I just type "b.boot" 
Where is the std.error being assigned? and how to I access it?

Thanks in advance...

-Scott



From wang at galton.uchicago.edu  Tue Jun 15 08:42:20 2004
From: wang at galton.uchicago.edu (Yong Wang)
Date: Tue, 15 Jun 2004 01:42:20 -0500 (CDT)
Subject: [R] any linear programming routine in R
In-Reply-To: <200406141002.i5EA1vP2003373@hypatia.math.ethz.ch>
References: <200406141002.i5EA1vP2003373@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.58.0406150138250.23309@aitken.uchicago.edu>

Dear all
is there any linear programming routine available for R?
if not, can you suggest any alternatives? not need to be very powerful, I 
get only a samll problem to resolve.
many thanks
yong



From ripley at stats.ox.ac.uk  Tue Jun 15 09:10:35 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 15 Jun 2004 08:10:35 +0100 (BST)
Subject: [R] load function to R GUI
In-Reply-To: <40CE0032.70306@cs.nyu.edu>
Message-ID: <Pine.LNX.4.44.0406150809020.2203-100000@gannet.stats>

The first menu item on the `File' menu is called `Source R code' and calls
source().  Did you look through the menus?  If not, it would be worth
familiarizing yourself with them.

On Mon, 14 Jun 2004, Evgueni Parilov wrote:

> Thanks!
> That was exactly what I wanted.
> Evgueni
> 
> 
> Ko-Kang Kevin Wang wrote:
> 
> >Hi,
> >  
> >
> >>-----Original Message-----
> >>From: r-help-bounces at stat.math.ethz.ch
> >>    
> >>
> >
> >  
> >
> >>Hi all!
> >>I looked through the manual and FAQ, and did not find any
> >>    
> >>
> >information
> >  
> >
> >>on how to load functions from files (with .R extension) to run them
> >>    
> >>
> >in
> >  
> >
> >>R GUI under Windows. The only way I know is to create and edit a
> >>function inside GUI. But what if I want to edit it in Emacs (do not
> >>want to use ESS) and then load into GUI?
> >>Any suggestions...
> >>    
> >>
> >
> >Do you mean ?source?
> >
> >i.e. save your function in, say, foo.R then use the source() function
> >to get it in.
> >
> >HTH
> >
> >Kevin
> >
> >
> >
> >
> >  
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Tue Jun 15 09:13:15 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 15 Jun 2004 09:13:15 +0200
Subject: [R] Clear Console
In-Reply-To: <OFA2B3CF50.A0392673-ONC1256EB3.0062845A@csintra.net>
References: <OFA2B3CF50.A0392673-ONC1256EB3.0062845A@csintra.net>
Message-ID: <40CEA18B.8030607@statistik.uni-dortmund.de>

Enrique Bengoechea wrote:

> Hi,
> 
> Could someone please point to me which function clears the R console under Windows? (exactly what the R Gui Windows menu "Edit > Clear Console" does).
> 
> Seems simple but I haven't been succesful with the help system (help.search for "console", "clear console", "screen"...) nor on the list archives.
> 
> Thanks in advance!!

Looking into the sources tells you that there is no R function to do so.
Either use the menu or press Ctrl+L.

Uwe Ligges


> Enrique
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From phall at alum.mit.edu  Tue Jun 15 09:14:39 2004
From: phall at alum.mit.edu (Pamela Hall)
Date: Tue, 15 Jun 2004 03:14:39 -0400
Subject: [R] installing my own package - problems with INDEX
Message-ID: <p06110408bcf44463b005@[192.168.2.5]>

Hi there;

I am a neophyte to R though I have been messing around with programming in other languages and environments for some years (my dog's name is punchcard to give you some idea of how many years).  I have been trying to make a package and install it, to no avail.  The functions I have written all work as expected (by me, that is).  But I cannot get the html help pages to work.  

The failure appears to be in creating the INDEX file.  No matter how carefully I check all of my *.Rd pages, the error message that I have "unbalanced braces"  keeps showing up.  I cannot find any error in matching braces.  I am using Alpha as the editor, and it identifies mismatched braces - there are none in the *.Rd file.

Another aspect of this problem is that the INDEX file only contains one line, that of the first file.  However, the html dir has an index file (00Index.html) that contains all of the functions I have documented, but they are all linked to the same identical function web page - the first one.

Please give me a hint.  There are no mismatched braces as far as I can tell.  Even when I try to check a single function and its Rd file, I get the same error message.  And there are no other error messages.  The package can be attached and the functions work, but the help pages do not.

I must be doing something really simple, but very wrong.  What are the 5 most common first mistakes some one makes in Rd files?

thanks
-ph



From ripley at stats.ox.ac.uk  Tue Jun 15 09:27:52 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 15 Jun 2004 08:27:52 +0100 (BST)
Subject: [R] any linear programming routine in R
In-Reply-To: <Pine.LNX.4.58.0406150138250.23309@aitken.uchicago.edu>
Message-ID: <Pine.LNX.4.44.0406150819580.2203-100000@gannet.stats>

On Tue, 15 Jun 2004, Yong Wang wrote:

> is there any linear programming routine available for R?

Yes.  help.search shows

print.simplex(boot)     Print Solution to Linear Programming Problem
simplex(boot)           Simplex Method for Linear Programming Problems
simplex.object(boot)    Linear Programming Solution Objects
solveLP(linprog)        solve Linear Programming / Optimization
                        problems
lp.object(lpSolve)      LP (linear programming) object


You would have found linprog and lpsolve by looking in the R FAQ's list of 
packages.

> if not, can you suggest any alternatives? not need to be very powerful, I 
> get only a samll problem to resolve.

You asked on S-news too. `boot' is also available for S-PLUS.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Jun 15 09:51:12 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 15 Jun 2004 08:51:12 +0100 (BST)
Subject: [R] Quadruple precision in R
In-Reply-To: <E619BDBD99B4F74D9DCA32F43BE926714C7349@XCH-VN02.sph.ad.jhsph.edu>
Message-ID: <Pine.LNX.4.44.0406150837540.2448-100000@gannet.stats>

On Mon, 14 Jun 2004, Ravi Varadhan wrote:

>  Is it possible to perform computations in quadruple precision (more
> generally, with more digits in the floating-point arithmetic than that
> allowed by double precision) in R?

You are making the assumption that R uses double precision.  Whereas R 
uses C type `double', some compilers may allow you to allocate that to an 
extended precision type.  However, the most commonly used platforms and 
compilers for R have no hardware or software support for `quadruple 
precision', and it seems to have gone out of fashion.

Why do you ask?  The data in the calculations is unlikely to be anything
like as accurate as standard double precision, and well-written algorithms
usually manage to give answers to within a hundred or so times that
precision.  Unless, that is, the problem is ill conditioned, in which case
the exact answer will be sensitive to minute inaccuracies in the data (as
popularized by Lorenz's butterfly effect).

The only times I have ever needed extended precision were

- to handle exact calculations on very large integers and
- on such ill-conditioned problems.

One example was to explore the fine structure of pseudo-random number 
generators.  These days I would use an arbitrary-precision integer 
computation package (and computer algebra packages often have such 
facilities).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Tue Jun 15 09:51:41 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 15 Jun 2004 09:51:41 +0200
Subject: [R] Parsing results from boot
In-Reply-To: <005a01c4528e$7ee902d0$6401a8c0@scott>
References: <005a01c4528e$7ee902d0$6401a8c0@scott>
Message-ID: <16590.43661.251409.559774@gargle.gargle.HOWL>

>>>>> "Scott" == Scott Norton <nortonsm at verizon.net>
>>>>>     on Tue, 15 Jun 2004 00:09:03 -0400 writes:

    Scott> This probably has a super easy answer...but I claim
    Scott> newbie status! (I did search help lists but this
    Scott> question is hard to isolate keyword-wise)

    Scott> Basically, I'm trying to figure out how to parse the
    Scott> results from executing boot().  I'm mainly interested
    Scott> in assigning the standard error estimate to a scalar
    Scott> variable.

    Scott> For example:
    Scott> --------------------------------------------------------------
    >> b<-c(100,100,120,130,1000,1200,1100,1150,125)
    >> b.boot <- boot(b, function(b,i) median(b[i]), R=1000)
    >> b.boot

    Scott> ORDINARY NONPARAMETRIC BOOTSTRAP


    Scott> Call:
    Scott> boot(data = b, statistic = function(b, i) median(b[i]), R = 1000)


    Scott> Bootstrap Statistics :
    Scott> original  bias    std. error
    Scott> t1*      130  354.75    450.9763
    >> 
    Scott> ------------------------------------------------------------------
    Scott> I'm interested in the value for std.error (i.e. 450.9763).  

    Scott> Now executing the folling:
    Scott> ------------------------------------------------------------------
    >> summary(b.boot)
    Scott>           Length Class  Mode     
    Scott> t0           1   -none- numeric  
    Scott> t         1000   -none- numeric  
    Scott> R            1   -none- numeric  
    Scott> data         9   -none- numeric  
    Scott> seed       626   -none- numeric  
    Scott> statistic    1   -none- function 
    Scott> sim          1   -none- character
    Scott> call         4   -none- call     
    Scott> stype        1   -none- character
    Scott> strata       9   -none- numeric  
    Scott> weights      9   -none- numeric  
    Scott> -------------------------------------------------------------------

which shows that summary.default() is called because there's no
summary() method for 'boot' objects [comment see below].

    Scott> seems to imply that it's not actually assigned to an
    Scott> output variable...but it is shown when I just type "b.boot"
    Scott>  Where is the std.error being assigned?

Logically, it must be in the print() method for 'boot' objects..
	 
    Scott>   and how to I access it?

Typing
     getS3method("print", class = "boot")
or   getAnywhere("print.class")

gives a humongous function listing, and somewhere in there

        t0 <- boot.out$t0[index]
        if (is.null(boot.out$call$weights)) {
            op <- cbind(t0, apply(t, 2, mean, na.rm = TRUE) - 
                t0, sqrt(apply(t, 2, function(t.st) var(t.st[!is.na(t.st)]))))
            dimnames(op) <- list(rn, c("original", " bias  ", 
                " std. error"))
        }

(which is not sufficient to understand the code).
So you see that this is really computed inside the print method.

If boot was programmed as we do other typical ``classes''.
The print method would print much less,
the summary method would compute all the interested quantities
and return an object of class "summary.boot" and there would be
a simple  print.summary.boot() method.

I assume Brian Ripley (as maintainer of 'boot') would accept
(well-written) user contributions to implement these, instead of
the current print.boot() mess^H^H^H^Himplementation.

Regards,
Martin Maechler



From jinss at hkusua.hku.hk  Tue Jun 15 10:06:04 2004
From: jinss at hkusua.hku.hk (Jin Shusong)
Date: Tue, 15 Jun 2004 16:06:04 +0800
Subject: [R] symbolic iteration
Message-ID: <20040615080604.GA24225@hkusua.hku.hk>

Dear all,
  I have 122 vectors named from L1 to L122.  Now I hope to
take log to each of the series, say 
L1 <- log(L1)
...
L122<-log(L122)

Can anyone show me a iterative way to make the job simple.
I mean the way something like
for(i in 1:122){
  ...
}
or other similar methods.
Many thanks.


Jin



From wolski at molgen.mpg.de  Tue Jun 15 10:11:48 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Tue, 15 Jun 2004 10:11:48 +0200
Subject: [R] installing my own package - problems with INDEX
In-Reply-To: <p06110408bcf44463b005@[192.168.2.5]>
References: <p06110408bcf44463b005@[192.168.2.5]>
Message-ID: <200406151011480471.00155965@mail.math.fu-berlin.de>

Hi!

If I have similar problem.

I first run R CMD check. Then I change to the directory mypackage.Rcheck where R CMD check generated a file mypackage-manual.tex.
Then I try to compile the file using latex by myself. In addition I am using a latex error aware editor (texniccenter on windows).
When I have the error pinned down in tex file, its usually quite easy to find the corresponding Rd file and then the error in the Rd file.

Sincerely Eryk.


*********** REPLY SEPARATOR  ***********

On 6/15/2004 at 3:14 AM Pamela Hall wrote:

>>>Hi there;
>>>
>>>I am a neophyte to R though I have been messing around with programming
>>>in other languages and environments for some years (my dog's name is
>>>punchcard to give you some idea of how many years).  I have been trying
>>>to make a package and install it, to no avail.  The functions I have
>>>written all work as expected (by me, that is).  But I cannot get the
>>>html help pages to work.  
>>>
>>>The failure appears to be in creating the INDEX file.  No matter how
>>>carefully I check all of my *.Rd pages, the error message that I have
>>>"unbalanced braces"  keeps showing up.  I cannot find any error in
>>>matching braces.  I am using Alpha as the editor, and it identifies
>>>mismatched braces - there are none in the *.Rd file.
>>>
>>>Another aspect of this problem is that the INDEX file only contains one
>>>line, that of the first file.  However, the html dir has an index file
>>>(00Index.html) that contains all of the functions I have documented, but
>>>they are all linked to the same identical function web page - the first
>>>one.
>>>
>>>Please give me a hint.  There are no mismatched braces as far as I can
>>>tell.  Even when I try to check a single function and its Rd file, I get
>>>the same error message.  And there are no other error messages.  The
>>>package can be attached and the functions work, but the help pages do
>>>not.
>>>
>>>I must be doing something really simple, but very wrong.  What are the 5
>>>most common first mistakes some one makes in Rd files?
>>>
>>>thanks
>>>-ph
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



Dipl. bio-chem. Eryk Witold Wolski    @    MPI-Moleculare Genetic   
Ihnestrasse 63-73 14195 Berlin       'v'    
tel: 0049-30-83875219               /   \    
mail: wolski at molgen.mpg.de        ---W-W----    http://www.molgen.mpg.de/~wolski



From ripley at stats.ox.ac.uk  Tue Jun 15 10:25:54 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 15 Jun 2004 09:25:54 +0100 (BST)
Subject: [R] Parsing results from boot
In-Reply-To: <16590.43661.251409.559774@gargle.gargle.HOWL>
Message-ID: <Pine.LNX.4.44.0406150923370.2508-100000@gannet.stats>

On Tue, 15 Jun 2004, Martin Maechler wrote:

> If boot was programmed as we do other typical ``classes''.
> The print method would print much less,
> the summary method would compute all the interested quantities
> and return an object of class "summary.boot" and there would be
> a simple  print.summary.boot() method.
> 
> I assume Brian Ripley (as maintainer of 'boot') would accept
> (well-written) user contributions to implement these, instead of
> the current print.boot() mess^H^H^H^Himplementation.

Only if Angelo Canty (the author) was happy too.  If anyone is thinking of 
doing this, please discuss it with Angelo first.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Jun 15 10:33:55 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 15 Jun 2004 09:33:55 +0100 (BST)
Subject: [R] symbolic iteration
In-Reply-To: <20040615080604.GA24225@hkusua.hku.hk>
Message-ID: <Pine.LNX.4.44.0406150928160.2508-100000@gannet.stats>

I don't see why this needs to be iterative: the computations could be done
in parallel.  But via a for loop  you could use

for(i in 1:122) {
    nm <- paste("L", i, sep="")
    assign(nm, log(get(nm))
}

You would do better, I think, to have 122 similar data items in a list.

datalist <- lapply(1:122, function(x) get(paste("L", x, sep="")))

Then you could just do

logdatalist <- lapply(datalist, log)

and so on.

On Tue, 15 Jun 2004, Jin Shusong wrote:

>   I have 122 vectors named from L1 to L122.  Now I hope to
> take log to each of the series, say 
> L1 <- log(L1)
> ...
> L122<-log(L122)
> 
> Can anyone show me a iterative way to make the job simple.
> I mean the way something like
> for(i in 1:122){
>   ...
> }
> or other similar methods.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Ted.Harding at nessie.mcc.ac.uk  Tue Jun 15 10:26:06 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 15 Jun 2004 09:26:06 +0100 (BST)
Subject: [R] "Glueing" factors together
Message-ID: <XFMail.040615092606.Ted.Harding@nessie.mcc.ac.uk>

Hi folks,

Suppose I have a series of cases each with categorical
factors A, B.

What is the best way to "glue" A and B together into a single
factor? For example, given

A0 B1 ...
A1 B1 ...
A0 B2 ...
A1 B0 ...
A0 B0 ...
A1 B2 ...

then I'd like to end up with a single factor with levels

  A0B0, A0B1, A0B2, A1B0, A1B1, A1B2

according to all the combinations which actually occur in the
data (e.g. if (A1,B2) did not occur in the data, then A1B2 would
not be a level in the resulting "glued" factor).

I can see clumsy and tedious ways involving manipulating strings,
paste(), etc.; but I'm sure it must be possible more elegantly.

With thanks, and best wishes to all,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 15-Jun-04                                       Time: 09:26:06
------------------------------ XFMail ------------------------------



From dimitris.rizopoulos at med.kuleuven.ac.be  Tue Jun 15 10:34:49 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Tue, 15 Jun 2004 10:34:49 +0200
Subject: [R] symbolic iteration
References: <20040615080604.GA24225@hkusua.hku.hk>
Message-ID: <00ae01c452b3$a00d8240$ad133a86@www.domain>

Dear Jin,

you could try something like,

L1 <- runif(10)
L2 <- runif(20)
L3 <- runif(30)
L4 <- runif(40)
res <- vector(mode="list", length=4)
ss <- paste("log(L", 1:4, ")", sep="")
for(i in 1:4) res[[i]] <- eval(parse(text=ss[i]))
res

I hope this helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Doctoral Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Jin Shusong" <jinss at hkusua.hku.hk>
To: "R Help" <r-help at stat.math.ethz.ch>
Sent: Tuesday, June 15, 2004 10:06 AM
Subject: [R] symbolic iteration


> Dear all,
>   I have 122 vectors named from L1 to L122.  Now I hope to
> take log to each of the series, say
> L1 <- log(L1)
> ...
> L122<-log(L122)
>
> Can anyone show me a iterative way to make the job simple.
> I mean the way something like
> for(i in 1:122){
>   ...
> }
> or other similar methods.
> Many thanks.
>
>
> Jin
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Tue Jun 15 10:28:15 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Jun 2004 10:28:15 +0200
Subject: [R] Parsing results from boot
In-Reply-To: <Pine.LNX.4.44.0406150923370.2508-100000@gannet.stats>
References: <Pine.LNX.4.44.0406150923370.2508-100000@gannet.stats>
Message-ID: <x2isdtusww.fsf@biostat.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> On Tue, 15 Jun 2004, Martin Maechler wrote:
> 
> > If boot was programmed as we do other typical ``classes''.
> > The print method would print much less,
> > the summary method would compute all the interested quantities
> > and return an object of class "summary.boot" and there would be
> > a simple  print.summary.boot() method.
> > 
> > I assume Brian Ripley (as maintainer of 'boot') would accept
> > (well-written) user contributions to implement these, instead of
> > the current print.boot() mess^H^H^H^Himplementation.
> 
> Only if Angelo Canty (the author) was happy too.  If anyone is thinking of 
> doing this, please discuss it with Angelo first.

If someone is feeling up to the necessary brow-beating, he/she might
want to apply it to Terry Therneau and summary.coxph as well. Of
course it is somewhat discouraging that neither Brian nor Thomas seem
to have been successful in getting the point across....

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Tue Jun 15 10:35:29 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Jun 2004 10:35:29 +0200
Subject: [R] "Glueing" factors together
In-Reply-To: <XFMail.040615092606.Ted.Harding@nessie.mcc.ac.uk>
References: <XFMail.040615092606.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <x2ekohusku.fsf@biostat.ku.dk>

(Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:

...
> then I'd like to end up with a single factor with levels
> 
>   A0B0, A0B1, A0B2, A1B0, A1B1, A1B2
> 
> according to all the combinations which actually occur in the
> data (e.g. if (A1,B2) did not occur in the data, then A1B2 would
> not be a level in the resulting "glued" factor).
> 
> I can see clumsy and tedious ways involving manipulating strings,
> paste(), etc.; but I'm sure it must be possible more elegantly.

You mean like interaction()?

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From catch_utsav at yahoo.com  Tue Jun 15 10:43:21 2004
From: catch_utsav at yahoo.com (Utsav Boobna)
Date: Tue, 15 Jun 2004 01:43:21 -0700 (PDT)
Subject: [R] loading multiple C files to R
Message-ID: <20040615084321.31137.qmail@web14823.mail.yahoo.com>

Hi,
   I am willing to load multiple C files to R, which
are inter-dependent (functions used in one may be
defined in other). What I was trying is to first
compile all of them separately (using R CMD SHLIB ...)
and then load them one by one (using dyn.load("...")
), but  it doesnt work (which seems to be obvious).
During loading, it was unable to recognize the
functions declared in already loaded file.

Please help.
Is there any documentation available on this topic,
other than "Writing R extensions"?

Thanks,
Utsav



From ripley at stats.ox.ac.uk  Tue Jun 15 10:45:51 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 15 Jun 2004 09:45:51 +0100 (BST)
Subject: [R] SJava
In-Reply-To: <BAY16-F109Z3Gktnr670002be01@hotmail.com>
Message-ID: <Pine.LNX.4.44.0406150934230.2508-100000@gannet.stats>

On Mon, 14 Jun 2004, A Friend wrote:

> Does anyone have any experience with SJava especially on Windows?

Yes, people do have experience, as searching the list archives will show 
you.  The experiences reported tend not to be good ones (selection bias, 
of course).

The problem tends to be that SJava was last updated in July 2002 and 
has not been adapted for changes in R since.  I have a version for R 
1.7.1 on http://www.stats.ox.ac.uk/pub/RWin.  Simon Urbanek has been 
working on it more recently, and has a version available from 
http://stats.math.uni-augsburg.de/iPlots/.

Further, SJava is part of the Omegahat project, not R, and looking at its 
lists (if currently up: it generated a configuration error for me) would 
yield more information.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dimitris.rizopoulos at med.kuleuven.ac.be  Tue Jun 15 10:48:27 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Tue, 15 Jun 2004 10:48:27 +0200
Subject: [R] "Glueing" factors together
References: <XFMail.040615092606.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <00d601c452b5$873a9c60$ad133a86@www.domain>

Dear Ted,

you could try something like,

dat <- data.frame(V1=sample(LETTERS[1:3], 10, rep=T),
V2=sample(letters[1:3], 10, rep=T))
mat <- apply(as.matrix(dat), 1, function(x) paste(x, collapse=""))
dat$glue <- factor(mat)
dat

I hope this helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Doctoral Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Ted Harding" <Ted.Harding at nessie.mcc.ac.uk>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, June 15, 2004 10:26 AM
Subject: [R] "Glueing" factors together


> Hi folks,
>
> Suppose I have a series of cases each with categorical
> factors A, B.
>
> What is the best way to "glue" A and B together into a single
> factor? For example, given
>
> A0 B1 ...
> A1 B1 ...
> A0 B2 ...
> A1 B0 ...
> A0 B0 ...
> A1 B2 ...
>
> then I'd like to end up with a single factor with levels
>
>   A0B0, A0B1, A0B2, A1B0, A1B1, A1B2
>
> according to all the combinations which actually occur in the
> data (e.g. if (A1,B2) did not occur in the data, then A1B2 would
> not be a level in the resulting "glued" factor).
>
> I can see clumsy and tedious ways involving manipulating strings,
> paste(), etc.; but I'm sure it must be possible more elegantly.
>
> With thanks, and best wishes to all,
> Ted.
>
>
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 167 1972
> Date: 15-Jun-04                                       Time: 09:26:06
> ------------------------------ XFMail ------------------------------
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Tue Jun 15 10:53:46 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 15 Jun 2004 09:53:46 +0100 (BST)
Subject: [R] loading multiple C files to R
In-Reply-To: <20040615084321.31137.qmail@web14823.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0406150951190.2508-100000@gannet.stats>

On Tue, 15 Jun 2004, Utsav Boobna wrote:

> Hi,
>    I am willing to load multiple C files to R, which
> are inter-dependent (functions used in one may be
> defined in other). What I was trying is to first
> compile all of them separately (using R CMD SHLIB ...)
> and then load them one by one (using dyn.load("...")
> ), but  it doesnt work (which seems to be obvious).
> During loading, it was unable to recognize the
> functions declared in already loaded file.
> 
> Please help.
> Is there any documentation available on this topic,
> other than "Writing R extensions"?

Which is sufficient, as it tells you

  This accepts as arguments a list of files which
  must be object files (with extension @file{.o}) or C, C++, or FORTRAN
  sources (with extensions @file{.c}, @file{.cc} or @file{.cpp} or
  @file{.C}, and @file{.f}, respectively).  See @kbd{R CMD SHLIB --help},
  or the on-line help for @code{SHLIB}, for usage information. 

Did you not notice `a list of files'?  You should be calling R CDM SHLIB 
only once.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Ted.Harding at nessie.mcc.ac.uk  Tue Jun 15 10:54:18 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Tue, 15 Jun 2004 09:54:18 +0100 (BST)
Subject: [R] "Glueing" factors together
In-Reply-To: <x2ekohusku.fsf@biostat.ku.dk>
Message-ID: <XFMail.040615095418.Ted.Harding@nessie.mcc.ac.uk>

On 15-Jun-04 Peter Dalgaard wrote:
> (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk> writes:
> 
> ...
>> then I'd like to end up with a single factor with levels
>> 
>>   A0B0, A0B1, A0B2, A1B0, A1B1, A1B2
>> 
>> according to all the combinations which actually occur in the
>> data (e.g. if (A1,B2) did not occur in the data, then A1B2 would
>> not be a level in the resulting "glued" factor).
>> 
>> I can see clumsy and tedious ways involving manipulating strings,
>> paste(), etc.; but I'm sure it must be possible more elegantly.
> 
> You mean like interaction()?

Thanks! Yes, interaction(A,B,drop=TRUE) does it.
Best wishes,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 15-Jun-04                                       Time: 09:54:18
------------------------------ XFMail ------------------------------



From jinss at hkusua.hku.hk  Tue Jun 15 11:26:26 2004
From: jinss at hkusua.hku.hk (Jin Shusong)
Date: Tue, 15 Jun 2004 17:26:26 +0800
Subject: [R] loading multiple C files to R
In-Reply-To: <20040615084321.31137.qmail@web14823.mail.yahoo.com>
References: <20040615084321.31137.qmail@web14823.mail.yahoo.com>
Message-ID: <20040615092626.GA20077@S77.hku.hk>

On Tue, Jun 15, 2004 at 01:43:21AM -0700, Utsav Boobna wrote:
> Hi,
>    I am willing to load multiple C files to R, which
> are inter-dependent (functions used in one may be
> defined in other). What I was trying is to first
> compile all of them separately (using R CMD SHLIB ...)
> and then load them one by one (using dyn.load("...")
> ), but  it doesnt work (which seems to be obvious).
> During loading, it was unable to recognize the
> functions declared in already loaded file.
> 
> Please help.
> Is there any documentation available on this topic,
> other than "Writing R extensions"?
> 
> Thanks,
> Utsav
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Dear Utsav,

  I think that you missed an option in dyn.load().
You can load your program by
dyn.load("your.program",local=FALSE)

  Good luck.

  Jin



From etptupaf at bs.ehu.es  Tue Jun 15 11:35:13 2004
From: etptupaf at bs.ehu.es (F. Tusell)
Date: Tue, 15 Jun 2004 11:35:13 +0200
Subject: [R] "Glueing" factors together
Message-ID: <40CEC2D1.8060103@bs.ehu.es>

Hi, Ted:

Probably not the most elegant way, but not too tedious either.

 >a  <- as.factor(c("A0","A1","A2"))
 >b  <- as.factor(c("B0","B1","B2"))
 > as.vector(outer(a,b,FUN=paste,sep=""))
[1] "A0B0" "A1B0" "A2B0" "A0B1" "A1B1" "A2B1" "A0B2" "A1B2" "A2B2"

Best, ft.

-- 
Fernando TUSELL                                e-mail:
Departamento de Econometr??a y Estad??stica           etptupaf at bs.ehu.es 
Facultad de CC.EE. y Empresariales             Tel:   (+34)94.601.3733
Avenida Lendakari Aguirre, 83                  Fax:   (+34)94.601.3754
E-48015 BILBAO  (Spain)                        Secr:  (+34)94.601.3740



From etptupaf at bs.ehu.es  Tue Jun 15 11:38:30 2004
From: etptupaf at bs.ehu.es (F. Tusell)
Date: Tue, 15 Jun 2004 11:38:30 +0200
Subject: [R] "Glueing" factors together
Message-ID: <40CEC396.4030408@bs.ehu.es>

Apologies, disregard my former message!  I overlloked the requirement 
that only combinations
present in the data must be taken. ft.

-- 
Fernando TUSELL                                e-mail:
Departamento de Econometr??a y Estad??stica           etptupaf at bs.ehu.es 
Facultad de CC.EE. y Empresariales             Tel:   (+34)94.601.3733
Avenida Lendakari Aguirre, 83                  Fax:   (+34)94.601.3754
E-48015 BILBAO  (Spain)                        Secr:  (+34)94.601.3740



From ligges at statistik.uni-dortmund.de  Tue Jun 15 11:40:09 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 15 Jun 2004 11:40:09 +0200
Subject: [R] installing my own package - problems with INDEX
In-Reply-To: <p06110408bcf44463b005@[192.168.2.5]>
References: <p06110408bcf44463b005@[192.168.2.5]>
Message-ID: <40CEC3F9.6020008@statistik.uni-dortmund.de>

Pamela Hall wrote:

> Hi there;
> 
> I am a neophyte to R though I have been messing around with programming in other languages and environments for some years (my dog's name is punchcard to give you some idea of how many years).  I have been trying to make a package and install it, to no avail.  The functions I have written all work as expected (by me, that is).  But I cannot get the html help pages to work.  
> 
> The failure appears to be in creating the INDEX file.  No matter how carefully I check all of my *.Rd pages, the error message that I have "unbalanced braces"  keeps showing up.  I cannot find any error in matching braces.  I am using Alpha as the editor, and it identifies mismatched braces - there are none in the *.Rd file.

Have you checked really carefully for "Unbalanced braces"? Many editors 
Please do so an look whether opening and closing of barces happens in 
the correct order. Also, watch out for escaped braces and stuff like 
that. Some appropriate editor might help, but don't rely on it (since 
it's not the parser itself).

Uwe Ligges


> Another aspect of this problem is that the INDEX file only contains one line, that of the first file.  However, the html dir has an index file (00Index.html) that contains all of the functions I have documented, but they are all linked to the same identical function web page - the first one.
> 
> Please give me a hint.  There are no mismatched braces as far as I can tell.  Even when I try to check a single function and its Rd file, I get the same error message.  And there are no other error messages.  The package can be attached and the functions work, but the help pages do not.
> 
> I must be doing something really simple, but very wrong.  What are the 5 most common first mistakes some one makes in Rd files?
> 
> thanks
> -ph
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From baron at psych.upenn.edu  Tue Jun 15 13:00:21 2004
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Tue, 15 Jun 2004 07:00:21 -0400
Subject: [R] import SYSTAT .syd file?
Message-ID: <20040615110021.GA14690@psych>

Does anyone know how to read a SYSTAT .syd file on Linux?
(Splus 6 does it, but it is easier to find a Windows box
with Systat than to download their demo.  I'm wondering
if there is a better way than either of these options.)

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron
R search page:        http://finzi.psych.upenn.edu/



From petr.pikal at precheza.cz  Tue Jun 15 13:11:45 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 15 Jun 2004 13:11:45 +0200
Subject: [R] slope estimations of teeth like data
Message-ID: <40CEF591.12795.1227387@localhost>

Dear all

Suppose I have teeth like data similar like

x <- 1:200
y <- 0.03*x[1:100]+rnorm(100, mean=.001, sd=.03)
z <- 3-rep(seq(1,100,10),each=10)*.03+rnorm(100,mean=.001, sd=.03)
plot(x,c(y,z))

and I want to have a gradient estimations for some values from increasing part of 
data

like

y.agg <- aggregate(diff(c(y,z)), list(rep(seq(1,200,10),each=10)[1:199]), mean)

y.agg[1:10,]  ##is OK, I want that
y.agg[11:20,] ##is not OK, I do not want that

actual data are similar but more irregular and have subsequent gradual increases 
and decreases, more like

set.seed(1)
yy<-NULL
for( i in 1:10) yy <- c(yy,c(y,z)[1:floor(runif(1)*200)])
length(yy)
[1] 1098

plot(1:1098,yy)

Is there anybody who has some experience with such data, mainly how to extract 
only increasing portions or to filter values of "yy" such as only aggregated slopes 
from increasing parts are computed and other parts are set to NA. Sometimes 
actual data have so long parts of steady or even slightly increasing values at 
decreasing part that aggregated values are slightly positive although they are 
actually from decreasing portion of data.

Thank you
Petr Pikal
petr.pikal at precheza.cz



From p.dalgaard at biostat.ku.dk  Tue Jun 15 13:33:29 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Jun 2004 13:33:29 +0200
Subject: [R] import SYSTAT .syd file?
In-Reply-To: <20040615110021.GA14690@psych>
References: <20040615110021.GA14690@psych>
Message-ID: <x24qpdjbsm.fsf@biostat.ku.dk>

Jonathan Baron <baron at psych.upenn.edu> writes:

> Does anyone know how to read a SYSTAT .syd file on Linux?
> (Splus 6 does it, but it is easier to find a Windows box
> with Systat than to download their demo.  I'm wondering
> if there is a better way than either of these options.)
> 
> Jon
> -- 
> Jonathan Baron, Professor of Psychology, University of Pennsylvania
> Home page:            http://www.sas.upenn.edu/~baron
> R search page:        http://finzi.psych.upenn.edu/

Google turned up this thing:

http://www.salford-systems.com/lfr.f

(which of course we can't use in R because of the copyright. Someone might
be able to abstract a format specification from it though...)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From benjamin.esterni at wanadoo.fr  Tue Jun 15 13:47:34 2004
From: benjamin.esterni at wanadoo.fr (Benjamin Esterni)
Date: Tue, 15 Jun 2004 13:47:34 +0200 (CEST)
Subject: [R] (sans objet)
Message-ID: <28652698.1087300054921.JavaMail.www@wwinf0501>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040615/a01a3a10/attachment.pl

From vito.muggeo at giustizia.it  Tue Jun 15 13:52:34 2004
From: vito.muggeo at giustizia.it (Vito Muggeo)
Date: Tue, 15 Jun 2004 13:52:34 +0200
Subject: R: [R] slope estimations of teeth like data
References: <40CEF591.12795.1227387@localhost>
Message-ID: <02b701c452cf$421b2680$5c13070a@PROCGEN>

Dear Petr,
Probably I don't understand exactly what you are looking for.

However your "plot(x,c(y,z))" suggests a broken-line model for the response
"c(y,x)" versus the variables x. Therefore you could estimate a segmented
model to obtain (different) slope (and breakpoint) estimates. See the
package segmented.

best,
vito



----- Original Message -----
From: Petr Pikal <petr.pikal at precheza.cz>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, June 15, 2004 1:11 PM
Subject: [R] slope estimations of teeth like data


> Dear all
>
> Suppose I have teeth like data similar like
>
> x <- 1:200
> y <- 0.03*x[1:100]+rnorm(100, mean=.001, sd=.03)
> z <- 3-rep(seq(1,100,10),each=10)*.03+rnorm(100,mean=.001, sd=.03)
> plot(x,c(y,z))
>
> and I want to have a gradient estimations for some values from increasing
part of
> data
>
> like
>
> y.agg <- aggregate(diff(c(y,z)), list(rep(seq(1,200,10),each=10)[1:199]),
mean)
>
> y.agg[1:10,]  ##is OK, I want that
> y.agg[11:20,] ##is not OK, I do not want that
>
> actual data are similar but more irregular and have subsequent gradual
increases
> and decreases, more like
>
> set.seed(1)
> yy<-NULL
> for( i in 1:10) yy <- c(yy,c(y,z)[1:floor(runif(1)*200)])
> length(yy)
> [1] 1098
>
> plot(1:1098,yy)
>
> Is there anybody who has some experience with such data, mainly how to
extract
> only increasing portions or to filter values of "yy" such as only
aggregated slopes
> from increasing parts are computed and other parts are set to NA.
Sometimes
> actual data have so long parts of steady or even slightly increasing
values at
> decreasing part that aggregated values are slightly positive although they
are
> actually from decreasing portion of data.
>
> Thank you
> Petr Pikal
> petr.pikal at precheza.cz
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From david_foreman at doctors.org.uk  Tue Jun 15 11:52:22 2004
From: david_foreman at doctors.org.uk (david_foreman@doctors.org.uk)
Date: Tue, 15 Jun 2004 11:52:22 (GMT)
Subject: [R] fit.mult.impute and quantile regression
Message-ID: <1087300342_22393@drn10msi01>

I have a largish dataset (1025) with around .15 of the data missing at random overall, but more like .25 in the dependent variable.  I am interested in modelling the data using quantile regression, but do not know how to do this with multiply imputed data (which is what the dataset seems to need).  The original plan was to use qr (or whatever) from the quantreg package as the 'fitter' argument in Design's fit.mult.impute, but it is not clear whether this would work, especially as fit.mult.impute seems only to work with the default settings of its 'fitter' arguments, which rather defeats the purpose of quantile regression.  Help!!  


_______________________________________________________________________
Most doctors use http://www.Doctors.net.uk e-mail.
Move to a free professional address with spam and virus protection.



From knoblauch at lyon.inserm.fr  Tue Jun 15 14:01:38 2004
From: knoblauch at lyon.inserm.fr (Ken Knoblauch)
Date: Tue, 15 Jun 2004 14:01:38 +0200
Subject: [R] AIC in glm.nb and glm(...family=negative.binomial(.))
Message-ID: <1087300898.40cee522cb3cc@webmail.lyon.inserm.fr>

Can anyone explain to me why the AIC values are so different when
using glm.nb and glm with a negative.binomial family, from the MASS
library?  I'm using R 1.8.1 with Mac 0S 10.3.4.

>library(MASS)
> dfr <- data.frame(c=rnbinom(100,size=2,mu=rep(c(10,20,100,1000),rep(25,4))),
+                   f=factor(rep(seq(1,4),rep(25,4))))
> AIC(nb1 <- glm.nb(c~f, data=dfr))
[1] 1047
> AIC(glm(c~f, family=negative.binomial(nb1$theta), data=dfr))
[1] -431804

Actually, the difference is already apparent with the function logLik,
but I still would like to understand the difference in what is
calculated in the two instances.

Thank you, in advance.


____________________
Ken Knoblauch
Inserm U 371
Cerveau et Vision
18 avenue du Doyen Lepine
69675 Bron cedex
France
tel: +33 (0)4 72 91 34 77
fax: +33 (0)4 72 91 34 61
portable: 06 84 10 64 10



From rkoenker at uiuc.edu  Tue Jun 15 14:26:55 2004
From: rkoenker at uiuc.edu (roger koenker)
Date: Tue, 15 Jun 2004 07:26:55 -0500
Subject: [R] fit.mult.impute and quantile regression
In-Reply-To: <1087300342_22393@drn10msi01>
References: <1087300342_22393@drn10msi01>
Message-ID: <49D4A871-BEC7-11D8-807B-000A95A7E3AA@uiuc.edu>

Having not tried this, it is dangerous to speculate, but it appears to 
me that there
would be no problem passing rq arguments (crucially, only tau, the 
specification
of the quantile of interest) to fit.mult.impute, since the call to the 
"fitter" procedure
includes a ... argument.  The real question would seem to be:  are the 
assumptions
underlying the imputation procedure consistent with the rq fitting, 
that is are they
assuming something stronger than that the tauth conditional quantile 
function of
y is linear in x?   There seem to be quite a variety of options for the 
imputation
in transcan, maybe Frank could advise on this?


url:	www.econ.uiuc.edu/~roger        	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820

On Jun 15, 2004, at 11:52 AM, <david_foreman at doctors.org.uk> wrote:

> I have a largish dataset (1025) with around .15 of the data missing at 
> random overall, but more like .25 in the dependent variable.  I am 
> interested in modelling the data using quantile regression, but do not 
> know how to do this with multiply imputed data (which is what the 
> dataset seems to need).  The original plan was to use qr (or whatever) 
> from the quantreg package as the 'fitter' argument in Design's 
> fit.mult.impute, but it is not clear whether this would work, 
> especially as fit.mult.impute seems only to work with the default 
> settings of its 'fitter' arguments, which rather defeats the purpose 
> of quantile regression.  Help!!
>
>
> _______________________________________________________________________
> Most doctors use http://www.Doctors.net.uk e-mail.
> Move to a free professional address with spam and virus protection.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From kaloytyn at cc.jyu.fi  Tue Jun 15 14:27:49 2004
From: kaloytyn at cc.jyu.fi (Katja Loytynoja)
Date: Tue, 15 Jun 2004 15:27:49 +0300 (EEST)
Subject: [R] factor analysis package
Message-ID: <Pine.LNX.4.44.0406151523050.6139-100000@itu.st.jyu.fi>


Hello everyone, is there a package/packages for factor analysis, 
particularly PCA? 

thanks,
Katja 


Katja L??ytynoja 
Taitoniekantie 9 A 218
40 740 Jyv??skyl?? 
Finland
tel.+35814 608058
cell.+35850 336 0174 
kaloytyn at jyu.fi



From p.dalgaard at biostat.ku.dk  Tue Jun 15 14:30:42 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 15 Jun 2004 14:30:42 +0200
Subject: [R] factor analysis package
In-Reply-To: <Pine.LNX.4.44.0406151523050.6139-100000@itu.st.jyu.fi>
References: <Pine.LNX.4.44.0406151523050.6139-100000@itu.st.jyu.fi>
Message-ID: <x2isdthukt.fsf@biostat.ku.dk>

Katja Loytynoja <kaloytyn at cc.jyu.fi> writes:

> Hello everyone, is there a package/packages for factor analysis, 
> particularly PCA? 

help.search("factor analysis")
help.search("principal components")

(Whether PCA qualifies as "factor analysis" is debatable, though...)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Tue Jun 15 14:41:21 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 15 Jun 2004 13:41:21 +0100 (BST)
Subject: [R] factor analysis package
In-Reply-To: <Pine.LNX.4.44.0406151523050.6139-100000@itu.st.jyu.fi>
Message-ID: <Pine.LNX.4.44.0406151338330.5383-100000@gannet.stats>

R comes with support for factor analysis and PCA (*not* the same thing)
in package stats which is normally loaded.

Try

help.search("factor analysis")
help.search("principal components")

On Tue, 15 Jun 2004, Katja Loytynoja wrote:

> Hello everyone, is there a package/packages for factor analysis, 
> particularly PCA? 

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

PLEASE DO.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From phall at alum.mit.edu  Tue Jun 15 14:41:27 2004
From: phall at alum.mit.edu (Pamela Hall)
Date: Tue, 15 Jun 2004 08:41:27 -0400
Subject: [R] installing my own package - problems with INDEX
In-Reply-To: <40CEC3F9.6020008@statistik.uni-dortmund.de>
References: <p06110408bcf44463b005@[192.168.2.5]>
	<40CEC3F9.6020008@statistik.uni-dortmund.de>
Message-ID: <p06110409bcf49ba3d757@[192.168.2.5]>


>Have you checked really carefully for "Unbalanced braces"? Many editors Please do so an look whether opening and closing of barces happens in the correct order. Also, watch out for escaped braces and stuff like that. Some appropriate editor might help, but don't rely on it (since it's not the parser itself).
>
>Uwe Ligges

Yes. Alpha is a good editor, designed for programming and has a tec mode (and an R/S mode, along with many many others).  It also colourizes everything so its easy to see what you're doing.  I made the skeleton of the Rd file with the prompt() so I believe I have the correct structure.  I have even tried to build a package of 1 function and 1 Rd file.  With 1 Rd file I can count the braces!  I know the file is correct.

At 10:11 +0200 6/15/04, Wolski wrote:
>Hi!
>
>If I have similar problem.
>
>I first run R CMD check. Then I change to the directory mypackage.Rcheck where R CMD check generated a file mypackage-manual.tex.
>Then I try to compile the file using latex by myself. In addition I am using a latex error aware editor (texniccenter on windows).
>When I have the error pinned down in tex file, its usually quite easy to find the corresponding Rd file and then the error in the Rd file.
>
>Sincerely Eryk.


I am not creating latex versions of the help pages.  This package is for a small group of people (about 40) and only works on a specific set of non-public data, so I'm not trying to make this package work for everyone.  It seems that my installation of latex is a bit skiddly-whumpus, but I haven't the time to mess with it now as I have a deadline for the R package completion.  HTML files are sufficient.  So, I have used the option that doesn't make latex files.

however, the editor I use, alpha, has a tex mode and I've been using that since it keeps track of matching braces, indentation and colours text so I know what part of the code it is.

The problem really isn't one of braces - as I said, I have tried to build it with a single function and a single Rd file,and it still gives me the same error.  If I build it with 2 functions and 2 Rd file, then the same error shows up AND only the first function is indexed and the help page for the second function points to the help page for the first function.

I must be doing something else wrong.  Its not the braces.

This is what I have done that may not be right?

1.  I put all of the Rd files into 1 file with a \eof separating them.  This is how it appears in already built packages, however, I haven't seen any reference to that in the R manual.   But if I don't do that, R CMD check completely fails.

2.  What about the functions themselves?  I have tried them as separate files or concatenated into one file.  The functions always appear to have been sourced and are available in R, but the help pages fail either way.

3.  \keyword.  Very few of my functions really fit into any of the keyword.db options.  I note that there also appears to be keywords for packages that can be added (eg spatial).  Can I make my own keyword and add it to the keyword.db?

-ph



From andy_liaw at merck.com  Tue Jun 15 14:46:19 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 15 Jun 2004 08:46:19 -0400
Subject: [R] installing my own package - problems with INDEX
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7EBF@usrymx25.merck.com>

Maybe you can show us one of the Rd files that doesn't work, so some one on
this list can see what the problem might be?

Cheers,
Andy

> From: Pamela Hall
> 
> >Have you checked really carefully for "Unbalanced braces"? 
> Many editors Please do so an look whether opening and closing 
> of barces happens in the correct order. Also, watch out for 
> escaped braces and stuff like that. Some appropriate editor 
> might help, but don't rely on it (since it's not the parser itself).
> >
> >Uwe Ligges
> 
> Yes. Alpha is a good editor, designed for programming and has 
> a tec mode (and an R/S mode, along with many many others).  
> It also colourizes everything so its easy to see what you're 
> doing.  I made the skeleton of the Rd file with the prompt() 
> so I believe I have the correct structure.  I have even tried 
> to build a package of 1 function and 1 Rd file.  With 1 Rd 
> file I can count the braces!  I know the file is correct.
> 
> At 10:11 +0200 6/15/04, Wolski wrote:
> >Hi!
> >
> >If I have similar problem.
> >
> >I first run R CMD check. Then I change to the directory 
> mypackage.Rcheck where R CMD check generated a file 
> mypackage-manual.tex.
> >Then I try to compile the file using latex by myself. In 
> addition I am using a latex error aware editor (texniccenter 
> on windows).
> >When I have the error pinned down in tex file, its usually 
> quite easy to find the corresponding Rd file and then the 
> error in the Rd file.
> >
> >Sincerely Eryk.
> 
> 
> I am not creating latex versions of the help pages.  This 
> package is for a small group of people (about 40) and only 
> works on a specific set of non-public data, so I'm not trying 
> to make this package work for everyone.  It seems that my 
> installation of latex is a bit skiddly-whumpus, but I haven't 
> the time to mess with it now as I have a deadline for the R 
> package completion.  HTML files are sufficient.  So, I have 
> used the option that doesn't make latex files.
> 
> however, the editor I use, alpha, has a tex mode and I've 
> been using that since it keeps track of matching braces, 
> indentation and colours text so I know what part of the code it is.
> 
> The problem really isn't one of braces - as I said, I have 
> tried to build it with a single function and a single Rd 
> file,and it still gives me the same error.  If I build it 
> with 2 functions and 2 Rd file, then the same error shows up 
> AND only the first function is indexed and the help page for 
> the second function points to the help page for the first function.
> 
> I must be doing something else wrong.  Its not the braces.
> 
> This is what I have done that may not be right?
> 
> 1.  I put all of the Rd files into 1 file with a \eof 
> separating them.  This is how it appears in already built 
> packages, however, I haven't seen any reference to that in 
> the R manual.   But if I don't do that, R CMD check completely fails.
> 
> 2.  What about the functions themselves?  I have tried them 
> as separate files or concatenated into one file.  The 
> functions always appear to have been sourced and are 
> available in R, but the help pages fail either way.
> 
> 3.  \keyword.  Very few of my functions really fit into any 
> of the keyword.db options.  I note that there also appears to 
> be keywords for packages that can be added (eg spatial).  Can 
> I make my own keyword and add it to the keyword.db?
> 
> -ph
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ripley at stats.ox.ac.uk  Tue Jun 15 14:50:16 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 15 Jun 2004 13:50:16 +0100 (BST)
Subject: [R] (sans objet)
In-Reply-To: <28652698.1087300054921.JavaMail.www@wwinf0501>
Message-ID: <Pine.LNX.4.44.0406151344320.5383-100000@gannet.stats>

On Tue, 15 Jun 2004, Benjamin Esterni wrote:

> I have a problem with the dr function: "dimension reduction".

It seems that you are using it inappropriately.

> I give you my example, and i'll be pleased to read your comments.
> 
> #let be X a matrix 50*100:
> 
> library(dr);

You should not be terminating lines with ;.  This is R, not C.

> X<- matrix(rnorm(50*100,5,1),50,100);
> 
> #and let be Y a vector response:
> Y<- sample(0:1,50,replace=T);
> 
> #I choose (for the exp??rience, but in reality i don't have it) a few variables #which are censed to explain Y:
> 
> index<- sample(1:100,10);
> X[Y==1,index]<-10*X[Y==1,index];
> 
> #so now I want to proceed to a logistic regression, but I don't know the vector #"index". So I have to reduce the dimension of X, and that's why I use the function #"dr" (dr package).
> 
> model<- dr(Y~X,family="binomial",method="phdy");
> 
> edr<- dr.direction(model);
> 
> #And now my problem: I hope that edr is a matrix constructed with linear
> #combinaison of X, prinipally the "index" vectors of X. But in reality
> it's not the #situation:

Your variables have no predictive power at all.  Look at

pairs(cbind(Y, edr))

> library(nnet);
> fit<-multinom(Y~.,data=data.frame(edr));

Take a look at this.  The fit is useless, because your variables are.

> pred<-predict(fit,data.frame(edr));
> table(Y,pred)
> 0  21
> 0  19

Given that Y has 50 values and that table labels dims, how on earth did 
you get that?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Tue Jun 15 15:13:16 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 15 Jun 2004 15:13:16 +0200
Subject: [R] installing my own package - problems with INDEX
In-Reply-To: <p06110409bcf49ba3d757@[192.168.2.5]>
References: <p06110408bcf44463b005@[192.168.2.5]>
	<40CEC3F9.6020008@statistik.uni-dortmund.de>
	<p06110409bcf49ba3d757@[192.168.2.5]>
Message-ID: <16590.62956.29121.864345@gargle.gargle.HOWL>

>>>>> "Pamela" == Pamela Hall <phall at alum.mit.edu>
>>>>>     on Tue, 15 Jun 2004 08:41:27 -0400 writes:

 <...............>

    Pamela> I must be doing something else wrong.  Its not the braces.

    Pamela> This is what I have done that may not be right?

    Pamela> 1.  I put all of the Rd files into 1 file with a \eof separating them.  This is how it appears in already built packages, however, I haven't seen any reference to that in the R manual.   But if I don't do that, R CMD check completely fails.

That's definitely not the recommended way.
But it might not be the problem either


    Pamela> 2.  What about the functions themselves?  I have tried them as separate files or concatenated into one file.  The functions always appear to have been sourced and are available in R, but the help pages fail either way.

as you're guessing yourself, this should be a completely
orthogonal matter.

    Pamela> 3.  \keyword.  Very few of my functions really fit into any of the keyword.db options.  I note that there also appears to be keywords for packages that can be added (eg spatial).  Can I make my own keyword and add it to the keyword.db?

No, don't.

This is becoming a FAQ..  (we had this topic recently with Rmetrics),

The \keyword{}s are not ``key words'' in the usual sense, but
rather  __category__ entries. Categories of applied statistics /
data analysis.  To choose one, you might find it easier to read
KEYWORDS rather than KEYWORDS.db

For ``real'' key words, you can nowadays use \concept{ .. }
the content of which is also searched by help.search() [when the
package is installed].

Again, please tell us what you are doing and we may help you to
choose appropriate keywords (or --much less probably-- you
convince us that some new keywords are needed).

Regards,
Martin Maechler



From petr.pikal at precheza.cz  Tue Jun 15 15:14:40 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 15 Jun 2004 15:14:40 +0200
Subject: R: [R] slope estimations of teeth like data
In-Reply-To: <02b701c452cf$421b2680$5c13070a@PROCGEN>
Message-ID: <40CF1260.21479.192FE04@localhost>



From petr.pikal at precheza.cz  Tue Jun 15 15:21:46 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Tue, 15 Jun 2004 15:21:46 +0200
Subject: R: [R] slope estimations of teeth like data
Message-ID: <40CF140A.3486.1997C2F@localhost>

On 15 Jun 2004 at 13:52, Vito Muggeo wrote: 

> Dear Petr, 
> Probably I don't understand exactly what you are looking for. 
>  
> However your "plot(x,c(y,z))" suggests a broken-line model for the 
> response "c(y,x)" versus the variables x. Therefore you could estimate 
> a segmented model to obtain (different) slope (and breakpoint) 
> estimates. See the package segmented. 

Thank you Vito, but it is not what I want. plot(x,c(y,z)) shows only one "spike"  
and I have many such spikes in actual data. 

My actual data look like those 

set.seed(1) 
y <- 0.03*x[1:100]+rnorm(100, mean=.001, sd=.03) 
z <- 3-rep(seq(1,100,10),each=10)*.03+rnorm(100,mean=.001, sd=.03) 
yy <- NULL 
for( i in 1:10) yy <- c(yy,c(y,z)[1:floor(runif(1)*200)]) 
y.l <- length(yy) 
plot(1:y.l, yy) 

x axis is actually a time and y is a weight of gradually filled conteiner, which is  
irregularly emptied. I want to do an hourly and/or daily averages of increases in  
weight (it can by done by aggregate)  

myfac <- gl(y.l/12,12,length=1271) #hopefully length is ok 

y.agg <- aggregate(diff(yy), list(myfac), mean) 
## there will be list(hod=cut(time.axis,"hour")) construction actually 

0.03 can be expected average result and some aggregated values ar OK but some  
are wrong as they include values from emptying time. 

*** This*** is probably what I need, I need to set some logical vector which will  
be TRUE when there was a filling time and FALSE during other times. And I  
need to specify it according a data I have available. 

Best what I was able to do was to consider filling time as a time when let say 

diff(yy) >= 0 

was between prespecified limits, but you know how it is with real life and  
prespecified limits.  

Or I can plot my data against time, manually find out regions which are correct  
and make a aggregation only with correct data. But there are 24*60*3 values  
each day so I prefer not to do it manually.  

Or finally I can throw away any hourly average which is not in set limits, but I  
prefer to throw away as little data as possible. 

I hope I was able to clarify the issue a bit. 

Thank you 
Best regards 
Petr 


>  
> best, 
> vito 
>  
>  
>  
> ----- Original Message ----- 
> From: Petr Pikal <petr.pikal at precheza.cz> 
> To: <r-help at stat.math.ethz.ch> 
> Sent: Tuesday, June 15, 2004 1:11 PM 
> Subject: [R] slope estimations of teeth like data 
>  
>  
> > Dear all 
> > 
> > Suppose I have teeth like data similar like 
> > 
> > x <- 1:200 
> > y <- 0.03*x[1:100]+rnorm(100, mean=.001, sd=.03) 
> > z <- 3-rep(seq(1,100,10),each=10)*.03+rnorm(100,mean=.001, sd=.03) 
> > plot(x,c(y,z)) 
> > 
> > and I want to have a gradient estimations for some values from 
> > increasing 
> part of 
> > data 
> > 
> > like 
> > 
> > y.agg <- aggregate(diff(c(y,z)), 
> > list(rep(seq(1,200,10),each=10)[1:199]), 
> mean) 
> > 
> > y.agg[1:10,]  ##is OK, I want that 
> > y.agg[11:20,] ##is not OK, I do not want that 
> > 
> > actual data are similar but more irregular and have subsequent 
> > gradual 
> increases 
> > and decreases, more like 
> > 
> > set.seed(1) 
> > yy<-NULL 
> > for( i in 1:10) yy <- c(yy,c(y,z)[1:floor(runif(1)*200)]) 
> > length(yy) 
> > [1] 1098 
> > 
> > plot(1:1098,yy) 
> > 
> > Is there anybody who has some experience with such data, mainly how 
> > to 
> extract 
> > only increasing portions or to filter values of "yy" such as only 
> aggregated slopes 
> > from increasing parts are computed and other parts are set to NA. 
> Sometimes 
> > actual data have so long parts of steady or even slightly increasing 
> values at 
> > decreasing part that aggregated values are slightly positive 
> > although they 
> are 
> > actually from decreasing portion of data. 
> > 
> > Thank you 
> > Petr Pikal 
> > petr.pikal at precheza.cz 
> > 
> > ______________________________________________ 
> > R-help at stat.math.ethz.ch mailing list 
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help 
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html 

Petr Pikal
petr.pikal at precheza.cz



From Achim.Zeileis at wu-wien.ac.at  Tue Jun 15 15:24:43 2004
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 15 Jun 2004 15:24:43 +0200
Subject: [R] slope estimations of teeth like data
In-Reply-To: <40CEF591.12795.1227387@localhost>
References: <40CEF591.12795.1227387@localhost>
Message-ID: <20040615152443.67b956c7.Achim.Zeileis@wu-wien.ac.at>

On Tue, 15 Jun 2004 13:11:45 +0200 Petr Pikal wrote:

> Dear all
> 
> Suppose I have teeth like data similar like
> 
> x <- 1:200
> y <- 0.03*x[1:100]+rnorm(100, mean=.001, sd=.03)
> z <- 3-rep(seq(1,100,10),each=10)*.03+rnorm(100,mean=.001, sd=.03)
> plot(x,c(y,z))
> 
> and I want to have a gradient estimations for some values from
> increasing part of data
> 
> like
> 
> y.agg <- aggregate(diff(c(y,z)),
> list(rep(seq(1,200,10),each=10)[1:199]), mean)
> 
> y.agg[1:10,]  ##is OK, I want that
> y.agg[11:20,] ##is not OK, I do not want that
> 
> actual data are similar but more irregular and have subsequent gradual
> increases and decreases, more like
> 
> set.seed(1)
> yy<-NULL
> for( i in 1:10) yy <- c(yy,c(y,z)[1:floor(runif(1)*200)])
> length(yy)
> [1] 1098
> 
> plot(1:1098,yy)

Just like Vito, I am also not really sure what you are looking for. But
some breakpoint-estimating procedure might be of help. Vito already
mentioned segmented(), furthermore there is breakpoints() in
strucchange.

But to estimate all breakpoints in yy (as defined above) is challenging.
breakpoints() requires some time for computations, e.g.

R> library(strucchange)
R> xx <- 1:1098
R> bp <- breakpoints(yy ~ xx, h = 0.05) ## this will take some time
R> plot(xx, yy)
R> lines(xx, fitted(bp), col = 4)

The algorithm in segmented is (often considerably) quicker but requires
start values for the breakpoints.

R> library(segmented)
R> sg <- segmented(lm(yy ~ xx), xx, seq(200, 900, length = 12),
                   tol = 1e-3) ## decrease tolerance
R> plot(xx, yy)
R> lines(xx, fitted(sg), col = 4)

If this is what you are looking for you can also contact me and Vito, I
guess, to see how the function calls can be improved to suit your needs.

hth,
Z

> Is there anybody who has some experience with such data, mainly how to
> extract only increasing portions or to filter values of "yy" such as
> only aggregated slopes from increasing parts are computed and other
> parts are set to NA. Sometimes actual data have so long parts of
> steady or even slightly increasing values at decreasing part that
> aggregated values are slightly positive although they are actually
> from decreasing portion of data.
> 
> Thank you
> Petr Pikal
> petr.pikal at precheza.cz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From dmurdoch at pair.com  Tue Jun 15 14:40:38 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Tue, 15 Jun 2004 08:40:38 -0400
Subject: [R] error with barplot command?
In-Reply-To: <XFMail.040614224327.Ted.Harding@nessie.mcc.ac.uk>
References: <lfprc0hfasanks6uhn1auu06o7kkmiku75@4ax.com>
	<XFMail.040614224327.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <k2rtc09cm11r6uudrvdpdd1g19g29hbu40@4ax.com>

On Mon, 14 Jun 2004 22:43:27 +0100 (BST), (Ted Harding)
<Ted.Harding at nessie.mcc.ac.uk> wrote :

>On 14-Jun-04 Duncan Murdoch wrote:

>> Please try 1.9.1 beta.  This should be fixed now...
 ...
> Does this mean that barplot()
>got un-fixed between 1.8.0 and 1.9.0?

Yes, and this is why we *really strongly* encourage people to try out
the patch, alpha and beta versions.  Sometimes (as in this case)
changes have unintended side effects; it's much better to catch them
before a release than after.

If you're on Windows, binary builds of the patch, alpha and beta
releases are available on the CRAN mirrors (e.g.
cran.uk.r-project.org) in the /bin/windows/base directory.  Follow the
link to "the r-patched release".  For other platforms you can download
the source and recompile it.

Duncan Murdoch



From knussear at biodiversity.unr.edu  Tue Jun 15 15:55:48 2004
From: knussear at biodiversity.unr.edu (knussear)
Date: Tue, 15 Jun 2004 06:55:48 -0700
Subject: [R] Manova question.
Message-ID: <B49DEDFD-BED3-11D8-9708-000A959A19D8@biodiversity.unr.edu>

Hi list,

I'm attempting to re-create a Repeated Measures Compositional Analysis 
as
  described in the work by Aebischer et. al. (Ecology. 1993. 74(5): 
1313-1325).

In this paper they describe transitions of data into a log ratio 
difference matrix, from which they obtain two matrices using a monova 
routine.

I am able to produce the second of the two matrices, but I'm having 
trouble with the first.

the difference matrix going in is given here.

Animal	Scrub	Bl wood	Con wood 	Grass
1	0.970	-2.380	-5.154	-9.408
2	1.217	-0.173	-4.955	-5.521
3	1.178	-0.248	-4.089	0.338
4	0.520	0.466	-4.801	-1.946
5	8.445	9.319	10.753	8.171
6	8.654	9.327	10.732	8.152
7	8.429	9.350	10.818	8.141
8	9.120	9.565	3.813	8.127
9	9.227	9.882	3.813	7.779
10	9.423	8.086	3.813	8.539
11	9.626	9.392	3.813	8.135
12	9.234	8.302	3.813	8.537
13	8.672	8.908	9.832	8.416


And the first of the matrices is given here, and is "matrix of 
mean-corrected sums of squares and cross products calculated from the 
difference matrix."


	Scrub	Bl wood	Con wood 	Grass
Scrub	179.52	214.59	244.58	273.75
Bl wood	214.59	268.44	314.35	343.86
Con wood 	244.58	314.35	471.09	400.22
Grass	273.75	343.86	400.22	477.78


 From manova on the data set I can get the diagonal of the matrix, but 
not the others.

manova(y ~ NULL)

Terms:
                 Residuals
Scrub            179.5273
Bl.wood          268.4347
Con.wood         471.0845
Grass            477.8014
Deg. of Freedom        12


Could anyone offer a suggestion ?

Thanks


Ken



From ripley at stats.ox.ac.uk  Tue Jun 15 16:46:19 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 15 Jun 2004 15:46:19 +0100 (BST)
Subject: [R] AIC in glm.nb and glm(...family=negative.binomial(.))
In-Reply-To: <1087300898.40cee522cb3cc@webmail.lyon.inserm.fr>
Message-ID: <Pine.LNX.4.44.0406151514190.5694-100000@gannet.stats>

You have the code, so take a look for youself.  There's an error in the 
aic formula in negative.binomial (and neg.bin)

On Tue, 15 Jun 2004, Ken Knoblauch wrote:

> Can anyone explain to me why the AIC values are so different when
> using glm.nb and glm with a negative.binomial family, from the MASS
> library?  I'm using R 1.8.1 with Mac 0S 10.3.4.
> 
> >library(MASS)
> > dfr <- data.frame(c=rnbinom(100,size=2,mu=rep(c(10,20,100,1000),rep(25,4))),
> +                   f=factor(rep(seq(1,4),rep(25,4))))
> > AIC(nb1 <- glm.nb(c~f, data=dfr))
> [1] 1047
> > AIC(glm(c~f, family=negative.binomial(nb1$theta), data=dfr))
> [1] -431804
> 
> Actually, the difference is already apparent with the function logLik,
> but I still would like to understand the difference in what is
> calculated in the two instances.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From wolski at molgen.mpg.de  Tue Jun 15 17:46:19 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Tue, 15 Jun 2004 17:46:19 +0200
Subject: [R] Clustering of sparse graphs.
Message-ID: <200406151746190283.29342F53@mail.math.fu-berlin.de>

Hi!

I have graph with 10000  or more (up 1000k) nodes and only few edges (can be weighted or not). It is a really sparse matrix.
I can generate this graph using R. Which structure I can, should, use to store the edges? (mSparse?)
Having this datastructures which clustering algorithm I can use in R? 

Sincerely Eryk



From Setzer.Woodrow at epamail.epa.gov  Tue Jun 15 17:46:50 2004
From: Setzer.Woodrow at epamail.epa.gov (Setzer.Woodrow@epamail.epa.gov)
Date: Tue, 15 Jun 2004 11:46:50 -0400
Subject: [R] odesolve: lsoda vs rk4
Message-ID: <OFCA46EC0C.BCCA9CE7-ON85256EB4.0055BA92-85256EB4.0056AE84@epamail.epa.gov>





lsoda doesn't pass along the names attribute of the state vector, y.  If
you want to use the names of the state vector in your code, you need to
reassign it inside your ode function.  I should either fix this in the
code for lsoda, or at least document it!  For example, I can run the
following modification of your code:

func2 <- function(t, y, p)
{
  names(y) <- c("A","B","C","D")  ### Here put the names attribute back
  Ad <- p["p2"]*(p["p1"]*y["A"]*y["D"])/(p["p2"]+p["p3"]) +
    p["p6"]*(p["p4"]*y["B"]*p["p10"])/(p["p5"]+p["p6"]) -
      p["p1"]*y["A"]*y["C"]
  Bd <- p["p3"]*(p["p1"]*y["A"]*y["D"])/(p["p2"]+p["p3"]) +
    p["p5"]*(p["p4"]*y["B"]*p["p10"])/(p["p5"]+p["p6"]) -
      p["p4"]*y["B"]*p["p10"]
  Cd <- (p["p1"]+p["p7"])*y["A"]*y["D"] -
    p["p1"]*y["A"]*y["C"]-p["p9"]*y["C"]
  Dd <-p["p9"]*y["C"] - p["p7"]*y["A"]*y["D"]
  list(c(Ad, Bd, Cd, Dd))
}

out2 <- lsoda(y, times, func2, parms)

> out2[c(1:10,1000),]
       time            A            B            C             D
 [1,]  0.00 2.500000e-06 2.500000e-06 1.700000e-06  5.700000e-07
 [2,]  0.05 2.432561e-06 2.500379e-06 1.671689e-06  5.312515e-07
 [3,]  0.10 2.368078e-06 2.499286e-06 1.639363e-06  4.980014e-07
 [4,]  0.15 2.306621e-06 2.496841e-06 1.603709e-06  4.697523e-07
 [5,]  0.20 2.248381e-06 2.493370e-06 1.566611e-06  4.451401e-07
 [6,]  0.25 2.193360e-06 2.488923e-06 1.528312e-06  4.239702e-07
 [7,]  0.30 2.141477e-06 2.483726e-06 1.489881e-06  4.053217e-07
 [8,]  0.35 2.092710e-06 2.477833e-06 1.451555e-06  3.889875e-07
 [9,]  0.40 2.046809e-06 2.471378e-06 1.413729e-06  3.744582e-07
[10,]  0.45 2.003632e-06 2.464438e-06 1.376627e-06  3.614428e-07
[11,] 49.95 2.675890e-06 5.563057e-08 1.595362e-09 -7.457989e-11

In the event the results of lsoda and rk4 differ, I would trust the
lsoda result over that of rk4 (see the warnings in the help file for
rk4).

On June 10, 2004, Chris Knight wrote:

[snip ...]
I'm trying to use odesolve for integrating various series of coupled 1st
order differential equations (derived from a system of enzymatic
catalysis and copied below, apologies for the excessively long set of
parameters).

The thing that confuses me is that, whilst I can run the function rk4:

out <- rk4(y=y,times=times,func=func, parms=parms)

and the results look not unreasonable:

out<-as.data.frame(out)
par(mfrow=c(4,1))
for (i in 2:(dim(out)[2]))plot(out[,1],out[,i], pch=".", xlab="time",
ylab=names(out)[i])

If I try doing the same thing with lsoda:

out <- lsoda(y=y,times=times,func=func, parms=parms, rtol=1e-1, atol=
1e-1)

I run into problems with a series of 'Excessive precision requested'
warnings with no output beyond the first time point.

Fiddling with rtol and atol doesn't seem to do very much.

What is likely to be causing this (I'm guessing the wide range of the
absolute values of the parameters can't be helping), is there anything I
can sensibly do about it and, failing that, can I reasonably take the
rk4 results as being meaningful?

Any help much appreciated,
Thanks in advance,

Chris

[code deleted...]

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Dr. Christopher Knight                               Tel:+44 1865 275111
Dept. Plant Sciences                                     +44 1865 275790
South Parks Road
Oxford     OX1 3RB                                   Fax:+44 1865 275074
` ?? . , ,><(((??>
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~




R. Woodrow Setzer, Jr.                        Phone: (919) 541-0128
Experimental Toxicology Division             Fax:  (919) 541-4284
Pharmacokinetics Branch
NHEERL B143-01; US EPA; RTP, NC 27711



From jschum at bgc-jena.mpg.de  Tue Jun 15 17:52:15 2004
From: jschum at bgc-jena.mpg.de (Jens Schumacher)
Date: Tue, 15 Jun 2004 17:52:15 +0200
Subject: [R] Manova question.
In-Reply-To: <B49DEDFD-BED3-11D8-9708-000A959A19D8@biodiversity.unr.edu>
References: <B49DEDFD-BED3-11D8-9708-000A959A19D8@biodiversity.unr.edu>
Message-ID: <40CF1B2F.2010409@bgc-jena.mpg.de>

knussear wrote:

> Hi list,
>
> I'm attempting to re-create a Repeated Measures Compositional Analysis as
>  described in the work by Aebischer et. al. (Ecology. 1993. 74(5): 
> 1313-1325).
>
> In this paper they describe transitions of data into a log ratio 
> difference matrix, from which they obtain two matrices using a monova 
> routine.
>
> I am able to produce the second of the two matrices, but I'm having 
> trouble with the first.
>
> the difference matrix going in is given here.
>
> Animal    Scrub    Bl wood    Con wood     Grass
> 1    0.970    -2.380    -5.154    -9.408
> 2    1.217    -0.173    -4.955    -5.521
> 3    1.178    -0.248    -4.089    0.338
> 4    0.520    0.466    -4.801    -1.946
> 5    8.445    9.319    10.753    8.171
> 6    8.654    9.327    10.732    8.152
> 7    8.429    9.350    10.818    8.141
> 8    9.120    9.565    3.813    8.127
> 9    9.227    9.882    3.813    7.779
> 10    9.423    8.086    3.813    8.539
> 11    9.626    9.392    3.813    8.135
> 12    9.234    8.302    3.813    8.537
> 13    8.672    8.908    9.832    8.416
>
>
> And the first of the matrices is given here, and is "matrix of 
> mean-corrected sums of squares and cross products calculated from the 
> difference matrix."
>
>
>     Scrub    Bl wood    Con wood     Grass
> Scrub    179.52    214.59    244.58    273.75
> Bl wood    214.59    268.44    314.35    343.86
> Con wood     244.58    314.35    471.09    400.22
> Grass    273.75    343.86    400.22    477.78
>
>
> From manova on the data set I can get the diagonal of the matrix, but 
> not the others.
>
> manova(y ~ NULL)
>
> Terms:
>                 Residuals
> Scrub            179.5273
> Bl.wood          268.4347
> Con.wood         471.0845
> Grass            477.8014
> Deg. of Freedom        12
>
>
> Could anyone offer a suggestion ?
>
> Thanks
>
>
> Ken
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>
Let   data.matrix    be the above difference matrix. You obtain the "raw 
sums of squares and cross-products matrix" by

R2 <- t(data.matrix) %*% data.matrix

or even
R2 <- crossprod(data.matrix, data.matrix)

To obtain the the second matrix you have to repeat the same thing with 
matrix of mean-corrected values.

data.centered <- data.matrix - matrix(colMeans(data.matrix), 
ncol=ncol(data.matrix),nrow=nrow(data.matrix), byrow=T)

R1 <- t(data.centered) %*% data.centered

HTH

Jens Schumacher



From parilov at cs.nyu.edu  Tue Jun 15 18:50:04 2004
From: parilov at cs.nyu.edu (Evgueni Parilov)
Date: Tue, 15 Jun 2004 12:50:04 -0400
Subject: [R] load function to R GUI
In-Reply-To: <Pine.LNX.4.44.0406150809020.2203-100000@gannet.stats>
References: <Pine.LNX.4.44.0406150809020.2203-100000@gannet.stats>
Message-ID: <40CF28BC.1070605@cs.nyu.edu>

Yep, shame on me. But on the other hand, this is a good example of a 
vague menu
item. I think that instead of "Source R code...", it should be "Load 
source R code..."
unless it can do more than just loading a source from R file.

Evgueni

Prof Brian Ripley wrote:

>The first menu item on the `File' menu is called `Source R code' and calls
>source().  Did you look through the menus?  If not, it would be worth
>familiarizing yourself with them.
>
>On Mon, 14 Jun 2004, Evgueni Parilov wrote:
>
>  
>
>>Thanks!
>>That was exactly what I wanted.
>>Evgueni
>>
>>
>>Ko-Kang Kevin Wang wrote:
>>
>>    
>>
>>>Hi,
>>> 
>>>
>>>      
>>>
>>>>-----Original Message-----
>>>>From: r-help-bounces at stat.math.ethz.ch
>>>>   
>>>>
>>>>        
>>>>
>>> 
>>>
>>>      
>>>
>>>>Hi all!
>>>>I looked through the manual and FAQ, and did not find any
>>>>   
>>>>
>>>>        
>>>>
>>>information
>>> 
>>>
>>>      
>>>
>>>>on how to load functions from files (with .R extension) to run them
>>>>   
>>>>
>>>>        
>>>>
>>>in
>>> 
>>>
>>>      
>>>
>>>>R GUI under Windows. The only way I know is to create and edit a
>>>>function inside GUI. But what if I want to edit it in Emacs (do not
>>>>want to use ESS) and then load into GUI?
>>>>Any suggestions...
>>>>   
>>>>
>>>>        
>>>>
>>>Do you mean ?source?
>>>
>>>i.e. save your function in, say, foo.R then use the source() function
>>>to get it in.
>>>
>>>HTH
>>>
>>>Kevin
>>>
>>>
>>>
>>>
>>> 
>>>
>>>      
>>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>
>>    
>>
>
>  
>



From f.harrell at vanderbilt.edu  Tue Jun 15 12:53:03 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Tue, 15 Jun 2004 12:53:03 +0200
Subject: [R] fit.mult.impute and quantile regression
In-Reply-To: <49D4A871-BEC7-11D8-807B-000A95A7E3AA@uiuc.edu>
References: <1087300342_22393@drn10msi01>
	<49D4A871-BEC7-11D8-807B-000A95A7E3AA@uiuc.edu>
Message-ID: <40CED50F.7010801@vanderbilt.edu>

roger koenker wrote:
> Having not tried this, it is dangerous to speculate, but it appears to 
> me that there
> would be no problem passing rq arguments (crucially, only tau, the 
> specification
> of the quantile of interest) to fit.mult.impute, since the call to the 
> "fitter" procedure
> includes a ... argument.  The real question would seem to be:  are the 
> assumptions
> underlying the imputation procedure consistent with the rq fitting, that 
> is are they
> assuming something stronger than that the tauth conditional quantile 
> function of
> y is linear in x?   There seem to be quite a variety of options for the 
> imputation
> in transcan, maybe Frank could advise on this?

You are right Roger, fit.mult.impute is generic.  It just assumes that 
impute.transcan can find the imputations to insert, to complete the 
dataframe.  Note that transcan is no longer the function to use for 
multiple imputations.  Use aregImpute.  It is flexible as long as 
additivity holds.

Frank
> 
> 
> url:    www.econ.uiuc.edu/~roger            Roger Koenker
> email    rkoenker at uiuc.edu            Department of Economics
> vox:     217-333-4558                University of Illinois
> fax:       217-244-6678                Champaign, IL 61820
> 
> On Jun 15, 2004, at 11:52 AM, <david_foreman at doctors.org.uk> wrote:
> 
>> I have a largish dataset (1025) with around .15 of the data missing at 
>> random overall, but more like .25 in the dependent variable.  I am 
>> interested in modelling the data using quantile regression, but do not 
>> know how to do this with multiply imputed data (which is what the 
>> dataset seems to need).  The original plan was to use qr (or whatever) 
>> from the quantreg package as the 'fitter' argument in Design's 
>> fit.mult.impute, but it is not clear whether this would work, 
>> especially as fit.mult.impute seems only to work with the default 
>> settings of its 'fitter' arguments, which rather defeats the purpose 
>> of quantile regression.  Help!!
>>

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From ligges at statistik.uni-dortmund.de  Tue Jun 15 19:00:38 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 15 Jun 2004 19:00:38 +0200
Subject: [R] load function to R GUI
In-Reply-To: <40CF28BC.1070605@cs.nyu.edu>
References: <Pine.LNX.4.44.0406150809020.2203-100000@gannet.stats>
	<40CF28BC.1070605@cs.nyu.edu>
Message-ID: <40CF2B36.1090109@statistik.uni-dortmund.de>

Evgueni Parilov wrote:

> Yep, shame on me. But on the other hand, this is a good example of a 
> vague menu
> item. I think that instead of "Source R code...", it should be "Load 
> source R code..."
> unless it can do more than just loading a source from R file.

No. source() does not only "load", but also parse()s the code!

Also note that load() is completely different from source(). So why do 
you want to confuse the users?

Uwe Ligges


> Evgueni
> 
> Prof Brian Ripley wrote:
> 
>> The first menu item on the `File' menu is called `Source R code' and 
>> calls
>> source().  Did you look through the menus?  If not, it would be worth
>> familiarizing yourself with them.
>>
>> On Mon, 14 Jun 2004, Evgueni Parilov wrote:
>>
>>  
>>
>>> Thanks!
>>> That was exactly what I wanted.
>>> Evgueni
>>>
>>>
>>> Ko-Kang Kevin Wang wrote:
>>>
>>>   
>>>
>>>> Hi,
>>>>
>>>>
>>>>     
>>>>
>>>>> -----Original Message-----
>>>>> From: r-help-bounces at stat.math.ethz.ch
>>>>>  
>>>>>       
>>>>
>>>>
>>>>
>>>>     
>>>>
>>>>> Hi all!
>>>>> I looked through the manual and FAQ, and did not find any
>>>>>  
>>>>>       
>>>>
>>>> information
>>>>
>>>>
>>>>     
>>>>
>>>>> on how to load functions from files (with .R extension) to run them
>>>>>  
>>>>>       
>>>>
>>>> in
>>>>
>>>>
>>>>     
>>>>
>>>>> R GUI under Windows. The only way I know is to create and edit a
>>>>> function inside GUI. But what if I want to edit it in Emacs (do not
>>>>> want to use ESS) and then load into GUI?
>>>>> Any suggestions...
>>>>>  
>>>>>       
>>>>
>>>> Do you mean ?source?
>>>>
>>>> i.e. save your function in, say, foo.R then use the source() function
>>>> to get it in.
>>>>
>>>> HTH
>>>>
>>>> Kevin



From dmurdoch at pair.com  Tue Jun 15 19:05:36 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Tue, 15 Jun 2004 13:05:36 -0400
Subject: [R] load function to R GUI
In-Reply-To: <40CF28BC.1070605@cs.nyu.edu>
References: <Pine.LNX.4.44.0406150809020.2203-100000@gannet.stats>
	<40CF28BC.1070605@cs.nyu.edu>
Message-ID: <buauc01cvme4mtr62im7mf4a75obepfl8l@4ax.com>

On Tue, 15 Jun 2004 12:50:04 -0400, Evgueni Parilov
<parilov at cs.nyu.edu> wrote :

>Yep, shame on me. But on the other hand, this is a good example of a 
>vague menu
>item. I think that instead of "Source R code...", it should be "Load 
>source R code..."
>unless it can do more than just loading a source from R file.

The name will make more sense as you learn more about R:  it refers to
the source() function, which reads and executes a file full of R code.
The load() function doesn't work on source code, it works on saved
binary images, so your suggestion isn't really clearer.

Duncan Murdoch



From sdavis2 at mail.nih.gov  Tue Jun 15 18:59:09 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Tue, 15 Jun 2004 12:59:09 -0400
Subject: [R] Clustering of sparse graphs.
In-Reply-To: <200406151746190283.29342F53@mail.math.fu-berlin.de>
Message-ID: <BCF4A31D.9475%sdavis2@mail.nih.gov>

Did you look at SparseM?  You may want to look at Rgraphviz and at some of
the ontology tools in bioconductor for examples of graph-theoretic
approaches to analysis as well as the graph package which includes data
structures and generics for all kinds of neat stuff.  There are probably
other packages that allow graph manipulation, as well.

Sean

On 6/15/04 11:46 AM, "Wolski" <wolski at molgen.mpg.de> wrote:

> Hi!
> 
> I have graph with 10000  or more (up 1000k) nodes and only few edges (can be
> weighted or not). It is a really sparse matrix.
> I can generate this graph using R. Which structure I can, should, use to store
> the edges? (mSparse?)
> Having this datastructures which clustering algorithm I can use in R?
> 
> Sincerely Eryk
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From parilov at cs.nyu.edu  Tue Jun 15 19:25:08 2004
From: parilov at cs.nyu.edu (Evgueni Parilov)
Date: Tue, 15 Jun 2004 13:25:08 -0400
Subject: [R] load function to R GUI
In-Reply-To: <40CF2B36.1090109@statistik.uni-dortmund.de>
References: <Pine.LNX.4.44.0406150809020.2203-100000@gannet.stats>
	<40CF28BC.1070605@cs.nyu.edu>
	<40CF2B36.1090109@statistik.uni-dortmund.de>
Message-ID: <40CF30F4.2050504@cs.nyu.edu>

Thanks Uwe!
Now I understand how it works and why it was called "Source R code" 
rather than
"Load source R code".  I did not really intend to confuse anybody. I 
just wanted
to have full information on a subject.
I agree that most of the menu items are self-explained. But some of them 
may not!
Especially, if you are novice for R, like in my case.

In future, I will try to check for answer 10 times before posting it to 
this group.
Thanks again,
Evgueni




Uwe Ligges wrote:

> Evgueni Parilov wrote:
>
>> Yep, shame on me. But on the other hand, this is a good example of a 
>> vague menu
>> item. I think that instead of "Source R code...", it should be "Load 
>> source R code..."
>> unless it can do more than just loading a source from R file.
>
>
> No. source() does not only "load", but also parse()s the code!
>
> Also note that load() is completely different from source(). So why do 
> you want to confuse the users?
>
> Uwe Ligges
>
>
>> Evgueni
>>
>> Prof Brian Ripley wrote:
>>
>>> The first menu item on the `File' menu is called `Source R code' and 
>>> calls
>>> source().  Did you look through the menus?  If not, it would be worth
>>> familiarizing yourself with them.
>>>
>>> On Mon, 14 Jun 2004, Evgueni Parilov wrote:
>>>
>>>  
>>>
>>>> Thanks!
>>>> That was exactly what I wanted.
>>>> Evgueni
>>>>
>>>>
>>>> Ko-Kang Kevin Wang wrote:
>>>>
>>>>  
>>>>
>>>>> Hi,
>>>>>
>>>>>
>>>>>    
>>>>>
>>>>>> -----Original Message-----
>>>>>> From: r-help-bounces at stat.math.ethz.ch
>>>>>>  
>>>>>>       
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>    
>>>>>
>>>>>> Hi all!
>>>>>> I looked through the manual and FAQ, and did not find any
>>>>>>  
>>>>>>       
>>>>>
>>>>>
>>>>> information
>>>>>
>>>>>
>>>>>    
>>>>>
>>>>>> on how to load functions from files (with .R extension) to run them
>>>>>>  
>>>>>>       
>>>>>
>>>>>
>>>>> in
>>>>>
>>>>>
>>>>>    
>>>>>
>>>>>> R GUI under Windows. The only way I know is to create and edit a
>>>>>> function inside GUI. But what if I want to edit it in Emacs (do not
>>>>>> want to use ESS) and then load into GUI?
>>>>>> Any suggestions...
>>>>>>  
>>>>>>       
>>>>>
>>>>>
>>>>> Do you mean ?source?
>>>>>
>>>>> i.e. save your function in, say, foo.R then use the source() function
>>>>> to get it in.
>>>>>
>>>>> HTH
>>>>>
>>>>> Kevin
>>>>
>
>
>



From bates at wisc.edu  Tue Jun 15 19:11:25 2004
From: bates at wisc.edu (Douglas Bates)
Date: Tue, 15 Jun 2004 12:11:25 -0500
Subject: [R] Manova question.
In-Reply-To: <40CF1B2F.2010409@bgc-jena.mpg.de>
References: <B49DEDFD-BED3-11D8-9708-000A959A19D8@biodiversity.unr.edu>
	<40CF1B2F.2010409@bgc-jena.mpg.de>
Message-ID: <40CF2DBD.3090503@wisc.edu>

Jens Schumacher wrote:
> knussear wrote:
> 
>> Hi list,
>>
>> I'm attempting to re-create a Repeated Measures Compositional Analysis as
>>  described in the work by Aebischer et. al. (Ecology. 1993. 74(5): 
>> 1313-1325).
>>
>> In this paper they describe transitions of data into a log ratio 
>> difference matrix, from which they obtain two matrices using a monova 
>> routine.
>>
>> I am able to produce the second of the two matrices, but I'm having 
>> trouble with the first.
>>
>> the difference matrix going in is given here.
>>
>> Animal    Scrub    Bl wood    Con wood     Grass
>> 1    0.970    -2.380    -5.154    -9.408
>> 2    1.217    -0.173    -4.955    -5.521
>> 3    1.178    -0.248    -4.089    0.338
>> 4    0.520    0.466    -4.801    -1.946
>> 5    8.445    9.319    10.753    8.171
>> 6    8.654    9.327    10.732    8.152
>> 7    8.429    9.350    10.818    8.141
>> 8    9.120    9.565    3.813    8.127
>> 9    9.227    9.882    3.813    7.779
>> 10    9.423    8.086    3.813    8.539
>> 11    9.626    9.392    3.813    8.135
>> 12    9.234    8.302    3.813    8.537
>> 13    8.672    8.908    9.832    8.416
>>
>>
>> And the first of the matrices is given here, and is "matrix of 
>> mean-corrected sums of squares and cross products calculated from the 
>> difference matrix."
>>
>>
>>     Scrub    Bl wood    Con wood     Grass
>> Scrub    179.52    214.59    244.58    273.75
>> Bl wood    214.59    268.44    314.35    343.86
>> Con wood     244.58    314.35    471.09    400.22
>> Grass    273.75    343.86    400.22    477.78
>>
>>
>> From manova on the data set I can get the diagonal of the matrix, but 
>> not the others.
>>
>> manova(y ~ NULL)
>>
>> Terms:
>>                 Residuals
>> Scrub            179.5273
>> Bl.wood          268.4347
>> Con.wood         471.0845
>> Grass            477.8014
>> Deg. of Freedom        12
>>
>>
>> Could anyone offer a suggestion ?
>>
>> Thanks
>>
>>
>> Ken
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>>
> Let   data.matrix    be the above difference matrix. You obtain the "raw 
> sums of squares and cross-products matrix" by
> 
> R2 <- t(data.matrix) %*% data.matrix
> 
> or even
> R2 <- crossprod(data.matrix, data.matrix)

or, the preferred form,

R2 <- crossprod(data.matrix)



From edarmas at carpa.ciagri.usp.br  Tue Jun 15 19:45:14 2004
From: edarmas at carpa.ciagri.usp.br (Eduardo Dutra de Armas)
Date: Tue, 15 Jun 2004 14:45:14 -0300
Subject: [R] How avoid coercion to character
Message-ID: <200406151745.i5FHjHum019967@hypatia.math.ethz.ch>

Hi R-users!
I'd like to know how to avoid automatic coercion of numeric variables
passed to array() to character, when there is one categorical variable
between others. I have tried I() function, but it is just functional
in data.frame.

Best Regards!

Eduardo Dutra de Armas



From phall at alum.mit.edu  Tue Jun 15 19:17:52 2004
From: phall at alum.mit.edu (Pamela Hall)
Date: Tue, 15 Jun 2004 13:17:52 -0400
Subject: [R] installing my own package - problems with INDEX
Message-ID: <p0611040bbcf4d732c544@[192.168.2.5]>

YEAH YEAH YEAH!!  I got a bunch of functions to install with their help pages and even help.search works!!!  The INDEX file is just fine.  YEAH YEAH YEAH!

This is what I have learned:

1.  DO NOT concatenate the *.Rd files. This happens in the installation process.  This is probably completely evident to y'all, but since the only working examples one has to look at are already installed packages which have concatenated Rd files...its a wee bit confusing to novices.  But NOW, I get it.

2.  When one does not concatenate *.Rd files,  (and adds a few print lines to R CMD check when messing around to try and figure out what R CMD check is doing...) then the error messages make much more sense and I was able to find errors.  And as you suspected there were many more than just a missing bracket.

so, now I can mess around with the quality of the package since at least it now EXISTS!
...time for a different subject header.

Thank y'all very much. 
-ph



From phall at alum.mit.edu  Tue Jun 15 19:37:42 2004
From: phall at alum.mit.edu (Pamela Hall)
Date: Tue, 15 Jun 2004 13:37:42 -0400
Subject: [R] Keywords and Concepts - CTFS package
Message-ID: <p0611040dbcf4d9083374@[192.168.2.5]>

The package I am writing is for the Center for Tropical Forest Science, CTFS.  This "center" is a collaboration of 15+ institutions world wide that are investigating properties of tropical forest dynamics, species diversity, species distributions.  The investigation is composed of the same sampling design of the forest: a large 50 hectare plot (usually) in which every tree >= 10 mm in diameter has been tagged, mapped and identified.  Reenumerations occur very 5 years (mostly).  Other information such as topography, canopy structure, seedling traps, etc, etc. are collected to different degrees at different sites.  Some sites have more than one plot, some have 2, 25 ha plots, some have 52 hectare, species identification can easily take 10 years and counting, some sites have 1200 species, etc. etc.  It is a very large project with 18 pantropical sites, over 3 million trees, 5000+ species with data ranging from first census to 7th census.

The people doing the field work and the analysis vary tremendously in their backgrounds and expertise.  There are many collaborators who have done little or none of the field work, some haven't even been to the sites.  The types of analyses that are done are wide ranging, but clearly cannot be crammed into a "standard" stats package.  The powers that be at CTFS decided to adopt R for analysis and away we went.

Four years have gone by and we have an odd collection of functions, few documented even inside the code (programmers hate to document), odd collection of manuals (mostly written by me) of varying complexity and integrity.  It became obvious to me this year that we needed to use more of R capacities and quit reinventing the packaging of functions and manuals for running them.  So I have taken on the responsibility of being the clearing house for all CTFS functions, checking them for usefulness, function and generality (which is not always necessary) and writing up help files.  And now, thanks to you guys, I have managed to start the process of packaging it all together so we can all work from the same function resource base!

Now, how to fit our functions into \keywords{} and use \concepts{}.

1.  Many functions are CTFS data structure specific: 

mort.spp.habitat() which computes the mortality for a "population" of individuals that belong to a single species and occupy a "habitat" defined in previous analyses and mapped to locations in the plot.  

2.  Some functions that do most of the "work" are more generic:
mort.calc() which calculates the annual mortality rate of a population and provides confidence limits to the rate through other functions.

3.  Some are very generic and just make it simpler to interact with other CTFS programs that could probably be made more generic, but that hasn't happened yet or may take too long to run (we do a lot of randomization and generation of distributions for assigning probabilities for statistical results):
split.data() which takes a dataset of census information on trees that is a dataframe and makes a new dataset that is a list of dataframes, 1 dataframe for each species - just restructures the data for ease of use in other programs and for more rapid access of, in this case, species based information.

I believe I understand the \concepts{} section of Rd files... 

Knowing the audience for whom I am writing this package, I have provided a number of values for each function as concepts so that help.search() will dig up related and useful functions.  How the CTFS functions relate to each other is a very audience dependent phenomena.  I'll try out my ideas on the CTFS users and let them tell me whether they took a long time to find what they needed or not.

I now understand that \keywords{} are not in the usual sense and I have  viewed KEYWORDS.

For function #3 above, I would say this is a type of data manipulation and is used, within CTFS as a file utility, so is the keyword:

\keyword{Basics:manip, utilities}

For function #2 above, mortality rate is just a piece of arithmetic, the CI assigned as stats, but this isn't a survival analysis, its just defined computation suitable for the uses of the CTFS crowd .  so what keyword is appropriate?

Function #1 is a form of data manipulation too, but so are all of our R programs.

Now, I'm confused.  I agree that there is no reason to create a new keyword since the CTFS stuff is so specific, but should I just call nearly everything we write "misc"?

-ph



From phall at alum.mit.edu  Tue Jun 15 19:45:08 2004
From: phall at alum.mit.edu (Pamela Hall)
Date: Tue, 15 Jun 2004 13:45:08 -0400
Subject: [R] To run or not to run examples, CTFS package
Message-ID: <p0611040ebcf4e46fdf8d@[192.168.2.5]>

Hi again;

I have placed many examples in the CTFS Rd pages because my audience really really likes lots of examples in order to understand the options available in a functions.  At this time I have set them all to \dontrun{}.  

however, this isn't because the example command line with its function and options won't work, but rather that CTFS specific datasets must exist to have the function output mean anything (random number generation is not useful for understanding the function output).  The CTFS datasets are quite large...20-40 MB and for some functions many other datasets need to be available.   The functions can also take quite a bit of time to run in some cases, performing many simulations.

So, my questions are:

1.  Does the R CMD check command actually run all of the examples if \dontrun{} isn't in the Rd file?  And won't this take a lot of time with real datasets?

2.  Should I consider making short versions of the datasets for the use of \example{} ?  This is trivial in some cases, but not in others as the function only makes sense on the entire dataset.  however, in those cases it is, so far, with some of the shorter datasets.

3.  Or can I just leave everything as \dontrun{} and if my audience want to actually run a function, then can cut and paste the line out of the man page or just type it in with their versions of the correct datasets attached?

-ph



From knussear at biodiversity.unr.edu  Tue Jun 15 19:47:56 2004
From: knussear at biodiversity.unr.edu (knussear)
Date: Tue, 15 Jun 2004 10:47:56 -0700
Subject: [R] Manova question.
In-Reply-To: <40CF2DBD.3090503@wisc.edu>
References: <B49DEDFD-BED3-11D8-9708-000A959A19D8@biodiversity.unr.edu>
	<40CF1B2F.2010409@bgc-jena.mpg.de> <40CF2DBD.3090503@wisc.edu>
Message-ID: <221C4AAB-BEF4-11D8-8508-000A959A19D8@biodiversity.unr.edu>


On Jun 15, 2004, at 10:11 AM, Douglas Bates wrote:

> Jens Schumacher wrote:
>> knussear wrote:
>>> Hi list,
>>>
>>> I'm attempting to re-create a Repeated Measures Compositional 
>>> Analysis as
>>>  described in the work by Aebischer et. al. (Ecology. 1993. 74(5): 
>>> 1313-1325).
>>>
>>> In this paper they describe transitions of data into a log ratio 
>>> difference matrix, from which they obtain two matrices using a 
>>> monova routine.
>>>
>>> I am able to produce the second of the two matrices, but I'm having 
>>> trouble with the first.
>>>
>>> the difference matrix going in is given here.
>>>
>>> Animal    Scrub    Bl wood    Con wood     Grass
>>> 1    0.970    -2.380    -5.154    -9.408
>>> 2    1.217    -0.173    -4.955    -5.521
>>> 3    1.178    -0.248    -4.089    0.338
>>> 4    0.520    0.466    -4.801    -1.946
>>> 5    8.445    9.319    10.753    8.171
>>> 6    8.654    9.327    10.732    8.152
>>> 7    8.429    9.350    10.818    8.141
>>> 8    9.120    9.565    3.813    8.127
>>> 9    9.227    9.882    3.813    7.779
>>> 10    9.423    8.086    3.813    8.539
>>> 11    9.626    9.392    3.813    8.135
>>> 12    9.234    8.302    3.813    8.537
>>> 13    8.672    8.908    9.832    8.416
>>>
>>>
>>> And the first of the matrices is given here, and is "matrix of 
>>> mean-corrected sums of squares and cross products calculated from 
>>> the difference matrix."
>>>
>>>
>>>     Scrub    Bl wood    Con wood     Grass
>>> Scrub    179.52    214.59    244.58    273.75
>>> Bl wood    214.59    268.44    314.35    343.86
>>> Con wood     244.58    314.35    471.09    400.22
>>> Grass    273.75    343.86    400.22    477.78
>>>
>>>
>>> From manova on the data set I can get the diagonal of the matrix, 
>>> but not the others.
>>>
>>> manova(y ~ NULL)
>>>
>>> Terms:
>>>                 Residuals
>>> Scrub            179.5273
>>> Bl.wood          268.4347
>>> Con.wood         471.0845
>>> Grass            477.8014
>>> Deg. of Freedom        12
>>>
>>>
>>> Could anyone offer a suggestion ?
>>>
>>> Thanks
>>>
>>>
>>> Ken
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>>
>>>
>> Let   data.matrix    be the above difference matrix. You obtain the 
>> "raw sums of squares and cross-products matrix" by
>> R2 <- t(data.matrix) %*% data.matrix
>> or even
>> R2 <- crossprod(data.matrix, data.matrix)
>
> or, the preferred form,
>
> R2 <- crossprod(data.matrix)
>

Thanks for the help.


Neither of those approaches gives the final matrix with 179.5273 in the 
top left position!

Ken



From andy_liaw at merck.com  Tue Jun 15 19:50:47 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 15 Jun 2004 13:50:47 -0400
Subject: [R] installing my own package - problems with INDEX
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7EC6@usrymx25.merck.com>

> From: Pamela Hall
> 
> YEAH YEAH YEAH!!  I got a bunch of functions to install with 
> their help pages and even help.search works!!!  The INDEX 
> file is just fine.  YEAH YEAH YEAH!
> 
> This is what I have learned:
> 
> 1.  DO NOT concatenate the *.Rd files. This happens in the 
> installation process.  This is probably completely evident to 
> y'all, but since the only working examples one has to look at 
> are already installed packages which have concatenated Rd 
> files...its a wee bit confusing to novices.  But NOW, I get it.

If you had looked at any of the _source_ package on CRAN (.tar.gz) instead
of the installed packages, you will see that the Rd files are one topic per
file.

Andy
 
> 2.  When one does not concatenate *.Rd files,  (and adds a 
> few print lines to R CMD check when messing around to try and 
> figure out what R CMD check is doing...) then the error 
> messages make much more sense and I was able to find errors.  
> And as you suspected there were many more than just a missing bracket.
> 
> so, now I can mess around with the quality of the package 
> since at least it now EXISTS!
> ...time for a different subject header.
> 
> Thank y'all very much. 
> -ph
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From andy_liaw at merck.com  Tue Jun 15 20:00:31 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 15 Jun 2004 14:00:31 -0400
Subject: [R] To run or not to run examples, CTFS package
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7EC8@usrymx25.merck.com>

My $0.02...

> From: Pamela Hall
> 
> Hi again;
> 
> I have placed many examples in the CTFS Rd pages because my 
> audience really really likes lots of examples in order to 
> understand the options available in a functions.  At this 
> time I have set them all to \dontrun{}.  
> 
> however, this isn't because the example command line with its 
> function and options won't work, but rather that CTFS 
> specific datasets must exist to have the function output mean 
> anything (random number generation is not useful for 
> understanding the function output).  The CTFS datasets are 
> quite large...20-40 MB and for some functions many other 
> datasets need to be available.   The functions can also take 
> quite a bit of time to run in some cases, performing many simulations.
> 
> So, my questions are:
> 
> 1.  Does the R CMD check command actually run all of the 
> examples if \dontrun{} isn't in the Rd file?  And won't this 
> take a lot of time with real datasets?

Yes.  It might take a long time, depending on the specifics of your `real
data'.
 
> 2.  Should I consider making short versions of the datasets 
> for the use of \example{} ?  This is trivial in some cases, 
> but not in others as the function only makes sense on the 
> entire dataset.  however, in those cases it is, so far, with 
> some of the shorter datasets.

I would use the minimal amount of data that's sufficient to demonstrate
whatever points needed to be made in the example, and no more.
 
> 3.  Or can I just leave everything as \dontrun{} and if my 
> audience want to actually run a function, then can cut and 
> paste the line out of the man page or just type it in with 
> their versions of the correct datasets attached?

One other possibility is to create a package vignette...

Cheers,
Andy

 
> -ph
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From dmurdoch at pair.com  Tue Jun 15 20:11:40 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Tue, 15 Jun 2004 14:11:40 -0400
Subject: [R] How avoid coercion to character
In-Reply-To: <200406151745.i5FHjHum019967@hypatia.math.ethz.ch>
References: <200406151745.i5FHjHum019967@hypatia.math.ethz.ch>
Message-ID: <vqeuc0tp7haaqhkvvvttvcqn06alk6qek3@4ax.com>

On Tue, 15 Jun 2004 14:45:14 -0300, Eduardo Dutra de Armas
<edarmas at carpa.ciagri.usp.br> wrote :

>Hi R-users!
>I'd like to know how to avoid automatic coercion of numeric variables
>passed to array() to character, when there is one categorical variable
>between others. I have tried I() function, but it is just functional
>in data.frame.

You can't.  An array is just a vector with a dimension attribute, and
vectors have to be all one type.  Since in general you can't coerce
text to numbers, R coerces the numbers to text.

You probably want a data.frame if you have mixed types, or if that
doesn't have enough dimensions for you, then build things out of lists
(which is how data.frames are constructed).

Duncan Murdoch



From richard.kittler at amd.com  Tue Jun 15 20:31:58 2004
From: richard.kittler at amd.com (richard.kittler@amd.com)
Date: Tue, 15 Jun 2004 11:31:58 -0700
Subject: [R] Coercing a dataframe column to datetime
Message-ID: <858788618A93D111B45900805F85267A0BCB2D99@caexmta3.amd.com>

Thank you! The next step in the conversion still fails and I can't seem to find any examples in the archives.  The result of the function 'as.POSIXct(strptime())' within the 'sapply' comes back as numeric rather than POSIXct as expected: 

> ds <- cbind(1:2, c("02/27/92 23:03:20", "02/27/92 22:29:56")); ds 
     [,1] [,2]               
[1,] "1"  "02/27/92 23:03:20"
[2,] "2"  "02/27/92 22:29:56"
> q <- sapply(ds[,2], function(x) as.POSIXct(strptime(x,"%m/%d/%y %H:%M:%S")))
> class(q) 
[1] "numeric"
> q
02/27/92 23:03:20 02/27/92 22:29:56 
        699260600         699258596 

--Rich

Richard Kittler 
AMD TDG
408-749-4099

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: Monday, June 14, 2004 12:05 PM
To: Kittler, Richard
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Coercing a dataframe column to datetime


You have forgotten as.POSIXct is needed too.

On Mon, 14 Jun 2004 richard.kittler at amd.com wrote:

> I am trying to coerce a data frame column from character to datetime using strptime but keep getting an error because the length of the coerced object is always 9.  What am I doing wrong here:   
> 
> .................................................................
> > ds <- cbind(1:2, c("02/27/92 23:03:20", "02/27/92 22:29:56")); ds
>      [,1] [,2]               
> [1,] "1"  "02/27/92 23:03:20"
> [2,] "2"  "02/27/92 22:29:56"
> >  
> > q <- strptime(ds[,2], "%m/%d/%y %H:%M:%S"); q
> [1] "1992-02-27 23:03:20" "1992-02-27 22:29:56"
> > 
> > ds[,2] <- q
> Error in "[<-"(`*tmp*`, , 2, value = q) : number of items to replace 
> is not a multiple of replacement length
> > 
> > length(q)
> [1] 9
> 
> .................................................................
> 
> --Rich
> 
> Richard Kittler
> AMD TDG
> 408-749-4099
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From FowlerM at mar.dfo-mpo.gc.ca  Tue Jun 15 20:45:55 2004
From: FowlerM at mar.dfo-mpo.gc.ca (Fowler, Mark)
Date: Tue, 15 Jun 2004 15:45:55 -0300
Subject: [R] S/R/RWeb/ODBC
Message-ID: <1A4AC4BAB9C50A42854582B69B08C034023550AF@MSGMARBIO05>

I'm looking for an optimal approach to access Oracle databases via RWeb
applications. I'm new to R but familiar with programming functions and web
pages for the S+ Statserver. I'm now going through the motions of migrating
S+/Statserver applications to R/RWeb as a feasability exercise. I can access
databases using ODBC directly in R or S, and using Statserver, but I have
not succeeded at extracting into R in RWebs batch mode. I can 'require
RODBC' in .Rprofile with apparent success, but the results of odbcConnect
differ from those when the command is typed into the R commands window.
Instead of a parameter list I get a -1. Does anyone know the solution to
this problem? Also, might anyone know the comparative merits of using some
PERL module (like DBD::Oracle) to do the extraction, as opposed to using
RODBC (assuming RODBC can be implemented in batch mode)? I'm currently using
Apache on Windows XP if relevant, but LINUX may be the final host (we'll
compare, but those more Web-wise than I expect LINUX to outperform Windows
for our purposes). 

> 	Mark Fowler
> 	Marine Fish Division
> 	Bedford Inst of Oceanography
> 	Dept Fisheries & Oceans
> 	Dartmouth NS Canada
> 	fowlerm at mar.dfo-mpo.gc.ca
> 
>



From ripley at stats.ox.ac.uk  Tue Jun 15 20:57:20 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 15 Jun 2004 19:57:20 +0100 (BST)
Subject: [R] load function to R GUI
In-Reply-To: <40CF28BC.1070605@cs.nyu.edu>
Message-ID: <Pine.LNX.4.44.0406151955290.2001-100000@gannet.stats>

On Tue, 15 Jun 2004, Evgueni Parilov wrote:

> Yep, shame on me. But on the other hand, this is a good example of a 
> vague menu
> item. I think that instead of "Source R code...", it should be "Load 
> source R code..."
> unless it can do more than just loading a source from R file.

It is `source' not `load', as you will find if you actually read the R 
documentation.  DO do as the posting guide asks before casting aspersions 
on the work of others!

> Prof Brian Ripley wrote:
> 
> >The first menu item on the `File' menu is called `Source R code' and calls
> >source().  Did you look through the menus?  If not, it would be worth
> >familiarizing yourself with them.
> >
> >On Mon, 14 Jun 2004, Evgueni Parilov wrote:
> >
> >  
> >
> >>Thanks!
> >>That was exactly what I wanted.
> >>Evgueni
> >>
> >>
> >>Ko-Kang Kevin Wang wrote:
> >>
> >>    
> >>
> >>>Hi,
> >>> 
> >>>
> >>>      
> >>>
> >>>>-----Original Message-----
> >>>>From: r-help-bounces at stat.math.ethz.ch
> >>>>   
> >>>>
> >>>>        
> >>>>
> >>> 
> >>>
> >>>      
> >>>
> >>>>Hi all!
> >>>>I looked through the manual and FAQ, and did not find any
> >>>>   
> >>>>
> >>>>        
> >>>>
> >>>information
> >>> 
> >>>
> >>>      
> >>>
> >>>>on how to load functions from files (with .R extension) to run them
> >>>>   
> >>>>
> >>>>        
> >>>>
> >>>in
> >>> 
> >>>
> >>>      
> >>>
> >>>>R GUI under Windows. The only way I know is to create and edit a
> >>>>function inside GUI. But what if I want to edit it in Emacs (do not
> >>>>want to use ESS) and then load into GUI?
> >>>>Any suggestions...
> >>>>   
> >>>>
> >>>>        
> >>>>
> >>>Do you mean ?source?
> >>>
> >>>i.e. save your function in, say, foo.R then use the source() function
> >>>to get it in.
> >>>
> >>>HTH
> >>>
> >>>Kevin
> >>>
> >>>
> >>>
> >>>
> >>> 
> >>>
> >>>      
> >>>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >>
> >>
> >>    
> >>
> >
> >  
> >
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Jun 15 21:04:09 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 15 Jun 2004 20:04:09 +0100 (BST)
Subject: [R] S/R/RWeb/ODBC
In-Reply-To: <1A4AC4BAB9C50A42854582B69B08C034023550AF@MSGMARBIO05>
Message-ID: <Pine.LNX.4.44.0406152003080.2001-100000@gannet.stats>

Sounds like a permissions/ownership problem.  odbcConnect returnign -1 
just means that the ODBC device manager failed.

On Tue, 15 Jun 2004, Fowler, Mark wrote:

> I'm looking for an optimal approach to access Oracle databases via RWeb
> applications. I'm new to R but familiar with programming functions and web
> pages for the S+ Statserver. I'm now going through the motions of migrating
> S+/Statserver applications to R/RWeb as a feasability exercise. I can access
> databases using ODBC directly in R or S, and using Statserver, but I have
> not succeeded at extracting into R in RWebs batch mode. I can 'require
> RODBC' in .Rprofile with apparent success, but the results of odbcConnect
> differ from those when the command is typed into the R commands window.
> Instead of a parameter list I get a -1. Does anyone know the solution to
> this problem? Also, might anyone know the comparative merits of using some
> PERL module (like DBD::Oracle) to do the extraction, as opposed to using
> RODBC (assuming RODBC can be implemented in batch mode)? I'm currently using
> Apache on Windows XP if relevant, but LINUX may be the final host (we'll
> compare, but those more Web-wise than I expect LINUX to outperform Windows
> for our purposes). 
> 
> > 	Mark Fowler
> > 	Marine Fish Division
> > 	Bedford Inst of Oceanography
> > 	Dept Fisheries & Oceans
> > 	Dartmouth NS Canada
> > 	fowlerm at mar.dfo-mpo.gc.ca
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From kirpich at fas.harvard.edu  Tue Jun 15 22:04:36 2004
From: kirpich at fas.harvard.edu (Yev Kirpichevsky)
Date: Tue, 15 Jun 2004 16:04:36 -0400
Subject: [R] building and installing a package in Windows
Message-ID: <6.1.0.6.2.20040615155957.01cd01b8@imap.fas.harvard.edu>

I'm trying to install a package in windows.  I have a package directory, 
which contains all the essentials: .Rd in the Man directory, DESCRIPTION 
file, etc.  I copied it to my R\bin directory, where the Rcmd file is located.
Then, when I try to run "Rcmd build mypackage" from that directory in DOS, 
I get the following error: "'perl' is not recognized as a command..."
I would appreciate any help on this matter.  Thank you very much!
-Yevgeniy



From sfwong at uh.edu  Tue Jun 15 22:11:24 2004
From: sfwong at uh.edu (Siew Fan Wong)
Date: Tue, 15 Jun 2004 15:11:24 -0500
Subject: [R] question on co-occurence matrix and subject distance matrix
Message-ID: <000001c45314$eebc0f90$38790781@Bauer.uh.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040615/032a369a/attachment.pl

From tlumley at u.washington.edu  Tue Jun 15 22:40:52 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 15 Jun 2004 13:40:52 -0700 (PDT)
Subject: [R] building and installing a package in Windows
In-Reply-To: <6.1.0.6.2.20040615155957.01cd01b8@imap.fas.harvard.edu>
References: <6.1.0.6.2.20040615155957.01cd01b8@imap.fas.harvard.edu>
Message-ID: <Pine.A41.4.58.0406151340180.184032@homer10.u.washington.edu>

On Tue, 15 Jun 2004, Yev Kirpichevsky wrote:

> I'm trying to install a package in windows.  I have a package directory,
> which contains all the essentials: .Rd in the Man directory, DESCRIPTION
> file, etc.  I copied it to my R\bin directory, where the Rcmd file is located.
> Then, when I try to run "Rcmd build mypackage" from that directory in DOS,
> I get the following error: "'perl' is not recognized as a command..."
> I would appreciate any help on this matter.  Thank you very much!

Either you don't have Perl installed on your computer or it isn't in the
path, so it isn't being found.

	-thomas



From tlumley at u.washington.edu  Tue Jun 15 22:46:24 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Tue, 15 Jun 2004 13:46:24 -0700 (PDT)
Subject: [R] To run or not to run examples, CTFS package
In-Reply-To: <p0611040ebcf4e46fdf8d@[192.168.2.5]>
References: <p0611040ebcf4e46fdf8d@[192.168.2.5]>
Message-ID: <Pine.A41.4.58.0406151341090.184032@homer10.u.washington.edu>

On Tue, 15 Jun 2004, Pamela Hall wrote:

> Hi again;
>
> I have placed many examples in the CTFS Rd pages because my audience
> really really likes lots of examples in order to understand the options
> available in a functions.  At this time I have set them all to
> \dontrun{}.
>
> however, this isn't because the example command line with its function
> and options won't work, but rather that CTFS specific datasets must
> exist to have the function output mean anything (random number
> generation is not useful for understanding the function output).  The
> CTFS datasets are quite large...20-40 MB and for some functions many
> other datasets need to be available.  The functions can also take quite
> a bit of time to run in some cases, performing many simulations.
>
> So, my questions are:
>
> 1.  Does the R CMD check command actually run all of the examples if
> \dontrun{} isn't in the Rd file?  And won't this take a lot of time with
> real datasets?

Yes, they are all run. For some packages this takes a long time, for
others it doesn't.

> 2.  Should I consider making short versions of the datasets for the use
> of \example{} ?  This is trivial in some cases, but not in others as the
> function only makes sense on the entire dataset.  however, in those
> cases it is, so far, with some of the shorter datasets.

If possible, it would be useful to have short versions for examples.

> 3.  Or can I just leave everything as \dontrun{} and if my audience want
> to actually run a function, then can cut and paste the line out of the
> man page or just type it in with their versions of the correct datasets
> attached?

You can leave things as \dontrun{}, but this should be a last resort.  The
automatic running of examples is really useful for catching bugs.

Also, if you want examples of how to perform various tasks it may be more
useful to have vignettes than examples.

	-thomas

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From ggrothendieck at myway.com  Tue Jun 15 23:33:20 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 15 Jun 2004 17:33:20 -0400 (EDT)
Subject: R: [R] slope estimations of teeth like data
Message-ID: <20040615213320.B9F4A394B@mprdmxin.myway.com>



I am not entirely sure what is essential and what is inessential in 
this problem.  In the yy data shown, the peaks occur in one step and
all the peaks are equal.  Just plotting

plot(diff(yy))

shows that there is a huge region of potential cutoffs for separating
out the large single step increases from the other differences.

If it continues to be true that the increases are large relative 
to everything else but the cutoff is not quite so obvious as 
here, we can calculate a cutoff by just sorting the unique
differences and partitioning them in every possible
way into small and large differences and then choosing the 
partition whose residual variance is least.   

Its actually pretty simple to do that from first pinciples, as
shown here, but there might be a function in R that does it
even more directly.

diffyy <- diff(yy)
yy. <- unique(sort(diffyy)) # unique diffs sorted
yy. <- yy.[-1] - diff(yy.)/2  # midpoints

# For each 2 class partition into small & large, get residual var
res <- sapply(yy., function(x) 
                     var(resid(lm(diffyy ~ factor(diffyy < x)))) )

cutoff <- yy.[which.min(res)]

# indices of points involved in rise
which(diffyy > cutoff) + 1

---

Petr Pikal <petr.pikal at precheza.cz>

 
On 15 Jun 2004 at 13:52, Vito Muggeo wrote: 

> Dear Petr, 
> Probably I don't understand exactly what you are looking for. 
> 
> However your "plot(x,c(y,z))" suggests a broken-line model for the 
> response "c(y,x)" versus the variables x. Therefore you could estimate 
> a segmented model to obtain (different) slope (and breakpoint) 
> estimates. See the package segmented. 

Thank you Vito, but it is not what I want. plot(x,c(y,z)) shows only one "spike" 
and I have many such spikes in actual data. 

My actual data look like those 

set.seed(1) 
y <- 0.03*x[1:100]+rnorm(100, mean=.001, sd=.03) 
z <- 3-rep(seq(1,100,10),each=10)*.03+rnorm(100,mean=.001, sd=.03) 
yy <- NULL 
for( i in 1:10) yy <- c(yy,c(y,z)[1:floor(runif(1)*200)]) 
y.l <- length(yy) 
plot(1:y.l, yy) 

x axis is actually a time and y is a weight of gradually filled conteiner, which is 
irregularly emptied. I want to do an hourly and/or daily averages of increases in 
weight (it can by done by aggregate) 

myfac <- gl(y.l/12,12,length=1271) #hopefully length is ok 

y.agg <- aggregate(diff(yy), list(myfac), mean) 
## there will be list(hod=cut(time.axis,"hour")) construction actually 

0.03 can be expected average result and some aggregated values ar OK but some 
are wrong as they include values from emptying time. 

*** This*** is probably what I need, I need to set some logical vector which will 
be TRUE when there was a filling time and FALSE during other times. And I 
need to specify it according a data I have available. 

Best what I was able to do was to consider filling time as a time when let say 

diff(yy) >= 0 

was between prespecified limits, but you know how it is with real life and 
prespecified limits. 

Or I can plot my data against time, manually find out regions which are correct 
and make a aggregation only with correct data. But there are 24*60*3 values 
each day so I prefer not to do it manually. 

Or finally I can throw away any hourly average which is not in set limits, but I 
prefer to throw away as little data as possible. 

I hope I was able to clarify the issue a bit. 

Thank you 
Best regards 
Petr 


> 
> best, 
> vito 
> 
> 
> 
> ----- Original Message ----- 
> From: Petr Pikal <petr.pikal at precheza.cz> 
> To: <r-help at stat.math.ethz.ch> 
> Sent: Tuesday, June 15, 2004 1:11 PM 
> Subject: [R] slope estimations of teeth like data 
> 
> 
> > Dear all 
> > 
> > Suppose I have teeth like data similar like 
> > 
> > x <- 1:200 
> > y <- 0.03*x[1:100]+rnorm(100, mean=.001, sd=.03) 
> > z <- 3-rep(seq(1,100,10),each=10)*.03+rnorm(100,mean=.001, sd=.03) 
> > plot(x,c(y,z)) 
> > 
> > and I want to have a gradient estimations for some values from 
> > increasing 
> part of 
> > data 
> > 
> > like 
> > 
> > y.agg <- aggregate(diff(c(y,z)), 
> > list(rep(seq(1,200,10),each=10)[1:199]), 
> mean) 
> > 
> > y.agg[1:10,] ##is OK, I want that 
> > y.agg[11:20,] ##is not OK, I do not want that 
> > 
> > actual data are similar but more irregular and have subsequent 
> > gradual 
> increases 
> > and decreases, more like 
> > 
> > set.seed(1) 
> > yy<-NULL 
> > for( i in 1:10) yy <- c(yy,c(y,z)[1:floor(runif(1)*200)]) 
> > length(yy) 
> > [1] 1098 
> > 
> > plot(1:1098,yy) 
> > 
> > Is there anybody who has some experience with such data, mainly how 
> > to 
> extract 
> > only increasing portions or to filter values of "yy" such as only 
> aggregated slopes 
> > from increasing parts are computed and other parts are set to NA. 
> Sometimes 
> > actual data have so long parts of steady or even slightly increasing 
> values at 
> > decreasing part that aggregated values are slightly positive 
> > although they 
> are 
> > actually from decreasing portion of data. 
> > 
> > Thank you 
> > Petr Pikal 
> > petr.pikal at precheza.cz 
> >



From Brian.J.GREGOR at odot.state.or.us  Tue Jun 15 23:51:06 2004
From: Brian.J.GREGOR at odot.state.or.us (Brian.J.GREGOR@odot.state.or.us)
Date: Tue, 15 Jun 2004 14:51:06 -0700
Subject: [R] oRegon R users group
Message-ID: <372EFF9FE4E42E419C978E7A305DC5FE0379AE32@exsalem5.odot.state.or.us>

R users in Oregon,

Please contact me if you would like to participate in an R users group. At
our agency, we use R extensively for transportation model development,
implementation and analysis. We would like to meet periodically with other R
users in various professions to exchange ideas on using R for data analysis
and modeling. The first meeting will be held in September or October.

We have a web page with users group information and links to our R site -
http://www.odot.state.or.us/tddtpau/orug.html.

Brian Gregor, P.E.
Transportation Planning Analysis Unit
Oregon Department of Transportation
Brian.J.GREGOR at odot.state.or.us
(503) 986-4120



From xt_wang at cs.concordia.ca  Wed Jun 16 00:17:36 2004
From: xt_wang at cs.concordia.ca (xt_wang@cs.concordia.ca)
Date: Tue, 15 Jun 2004 18:17:36 -0400
Subject: [R] About function "unif_rand()"
Message-ID: <1087337856.40cf7580a58c3@mailhost.cs.concordia.ca>


hello, everybody,

I met a problem that I want to generate a random uniform number by using 
function with c interface. 

What I found is only "unif_rand()", but its range is [0,1]. How can I create a 
uniform random variable by using a function with c interface.

Who can tell me what function it is and how to use?

I will appreciate for it very much.

Thanks in advance!

xiaotong wang



From kirpich at fas.harvard.edu  Wed Jun 16 00:21:56 2004
From: kirpich at fas.harvard.edu (Yev Kirpichevsky)
Date: Tue, 15 Jun 2004 18:21:56 -0400
Subject: [R] Fwd: building and installing a package in Windows
Message-ID: <6.1.0.6.2.20040615181727.01c54db8@imap.fas.harvard.edu>

Thanks to everyone who responded to my 'Perl' problem!   Indeed, it wasn't 
in the Path.

I now have a different problem and would appreciate any insights:
after I run "Rcmd build mypackage", I get the following error:
"'sh' is not recognized as a command, program, or batch file"
Thanks in advance,

-yevgeniy





>Date: Tue, 15 Jun 2004 16:04:36 -0400
>To: r-help at stat.math.ethz.ch
>From: Yev Kirpichevsky <kirpich at fas.harvard.edu>
>Subject: building and installing a package in Windows
>
>I'm trying to install a package in windows.  I have a package directory, 
>which contains all the essentials: .Rd in the Man directory, DESCRIPTION 
>file, etc.  I copied it to my R\bin directory, where the Rcmd file is located.
>Then, when I try to run "Rcmd build mypackage" from that directory in DOS, 
>I get the following error: "'perl' is not recognized as a command..."
>I would appreciate any help on this matter.  Thank you very much!
>-Yevgeniy



From andy_liaw at merck.com  Wed Jun 16 00:24:55 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 15 Jun 2004 18:24:55 -0400
Subject: [R] Fwd: building and installing a package in Windows
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7ED3@usrymx25.merck.com>

Please follow the instructions in readme.packages very, very, very, very
carefully.

Andy

> From: Yev Kirpichevsky
> 
> Thanks to everyone who responded to my 'Perl' problem!   
> Indeed, it wasn't 
> in the Path.
> 
> I now have a different problem and would appreciate any insights:
> after I run "Rcmd build mypackage", I get the following error:
> "'sh' is not recognized as a command, program, or batch file"
> Thanks in advance,
> 
> -yevgeniy



From andy_liaw at merck.com  Wed Jun 16 00:33:24 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 15 Jun 2004 18:33:24 -0400
Subject: [R] About function "unif_rand()"
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7ED4@usrymx25.merck.com>

What sort of "random uniform numbers" do you want to generate?  Without
qualifiers, "uniform" usually just refer to U(0,1).  If you want the
interval to be (a, b), just do

X = unif_rand() * (b-a) + a

Andy

> From: xt_wang at cs.concordia.ca
> 
> hello, everybody,
> 
> I met a problem that I want to generate a random uniform 
> number by using 
> function with c interface. 
> 
> What I found is only "unif_rand()", but its range is [0,1]. 
> How can I create a 
> uniform random variable by using a function with c interface.
> 
> Who can tell me what function it is and how to use?
> 
> I will appreciate for it very much.
> 
> Thanks in advance!
> 
> xiaotong wang
> 
>



From ltarca at rsvs.ulaval.ca  Wed Jun 16 01:08:33 2004
From: ltarca at rsvs.ulaval.ca (Tarca Adi Laurentiu)
Date: Tue, 15 Jun 2004 19:08:33 -0400
Subject: [R] building R libraries (windows)  - R CMD check problems
In-Reply-To: <200406151002.i5FA1tja028712@hypatia.math.ethz.ch>
References: <200406151002.i5FA1tja028712@hypatia.math.ethz.ch>
Message-ID: <6.0.0.22.2.20040615185101.01c1d3e0@biota.rsvs.ulaval.ca>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040615/50d20fcb/attachment.pl

From dmurdoch at pair.com  Wed Jun 16 01:11:03 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Tue, 15 Jun 2004 19:11:03 -0400
Subject: [R] About function "unif_rand()"
In-Reply-To: <1087337856.40cf7580a58c3@mailhost.cs.concordia.ca>
References: <1087337856.40cf7580a58c3@mailhost.cs.concordia.ca>
Message-ID: <q50vc0diretg06eed091bmioml12tk60q3@4ax.com>

On Tue, 15 Jun 2004 18:17:36 -0400, xt_wang at cs.concordia.ca wrote:

>hello, everybody,
>
>I met a problem that I want to generate a random uniform number by using 
>function with c interface. 
>
>What I found is only "unif_rand()", but its range is [0,1]. How can I create a 
>uniform random variable by using a function with c interface.
>
>Who can tell me what function it is and how to use?

It's not clear to me what you are asking for.  Do you want it to be
uniform on the range (A,B)?  Use A + (B-A)*unif_rand() or runif(A,B).
These are documented in the Writing R Extensions manual, along with a
large collection of other distributions (and instructions on how to
initialize them).

Duncan Murdoch



From dmurdoch at pair.com  Wed Jun 16 01:20:47 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Tue, 15 Jun 2004 19:20:47 -0400
Subject: [R] building R libraries (windows)  - R CMD check problems
In-Reply-To: <6.0.0.22.2.20040615185101.01c1d3e0@biota.rsvs.ulaval.ca>
References: <200406151002.i5FA1tja028712@hypatia.math.ethz.ch>
	<6.0.0.22.2.20040615185101.01c1d3e0@biota.rsvs.ulaval.ca>
Message-ID: <pt0vc0tc7lcahp5hmnv3rsqmunhiksdrub@4ax.com>

On Tue, 15 Jun 2004 19:08:33 -0400, Tarca Adi Laurentiu
<ltarca at rsvs.ulaval.ca> wrote:

>I am trying to build a R library called nnNorm but I have some troubles 
>checking and installing it.
...
>
>---------- Making package nnNorm ------------
>   adding build stamp to DESCRIPTION
>   installing R files
>   installing inst files
>FIND : format incorrect de param??tre

You've got a problem with your PATH.  "find.exe" is one of the
programs in the package of tools that Brian Ripley put together, but
you're trying to execute some other FIND program.  You need to set up
the path as described in $RHOME/readme.packages.

Duncan Murdoch



From murray.logan at sci.monash.edu.au  Wed Jun 16 01:25:03 2004
From: murray.logan at sci.monash.edu.au (Murray Logan)
Date: Wed, 16 Jun 2004 09:25:03 +1000
Subject: [R] multiple error strata in aov
Message-ID: <40CF854F.5050302@sci.monash.edu.au>

I am trying to perform a model 3 ANOVA for a 2 factor (say factor A and 
factor B) anova in which factor A is fixed and factor B is random. 
 Therefore, the error term for the test of factor A should be the A:B 
interaction term and the error terms for B and A:B should be the model 
residual (within) term.  I have tried to work out how to specify such 
error strata using aov, however, I have had little success.  Is there a 
way to specify just the interaction (A:B) as an error term using Error() 
within aov or provide a list of error strata.  
Ideally as Error=list(A="A:B",B="Within", A:B="Within")

In addition, I have tried using lme to perform this function, but again 
without much success.

Can anyone offer any ideas

Ta

Murray



From lauraholt_983 at hotmail.com  Wed Jun 16 01:43:04 2004
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Tue, 15 Jun 2004 18:43:04 -0500
Subject: [R] time series object
Message-ID: <BAY12-F1044pCfynIQW00063b48@hotmail.com>

Hi R People:

I have a monthly time series x.ts which runs from 1/1995 through 12/2003.

>x.ts <- ts(x,start=1995,freq=12)
>str(x.ts)
Time-Series [1:108] from 1995 to 2004: -1.638 -0.236  0.830 -0.548  0.363 
...
>

My question: is there a way to print the observations from 1/1999 to 6/1999, 
please?
(other than x.ts[49:54])

Thank you in advance!
R version 1.9.0 for Windows
Sincerely,
Laura
mailto: lauraholt_983 at hotmail.com


a trip to NY



From ggrothendieck at myway.com  Wed Jun 16 02:01:08 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 15 Jun 2004 20:01:08 -0400 (EDT)
Subject: [R] time series object
Message-ID: <20040616000108.561913979@mprdmxin.myway.com>




?window


Date:   Tue, 15 Jun 2004 18:43:04 -0500 
From:   Laura Holt <lauraholt_983 at hotmail.com>
To:   <r-help at stat.math.ethz.ch> 
Subject:   [R] time series object 

 
Hi R People:

I have a monthly time series x.ts which runs from 1/1995 through 12/2003.

>x.ts <- ts(x,start=1995,freq=12)
>str(x.ts)
Time-Series [1:108] from 1995 to 2004: -1.638 -0.236 0.830 -0.548 0.363 
...
>

My question: is there a way to print the observations from 1/1999 to 6/1999, 
please?
(other than x.ts[49:54])

Thank you in advance!



From Lorenz.Gygax at fat.admin.ch  Wed Jun 16 07:30:08 2004
From: Lorenz.Gygax at fat.admin.ch (Lorenz.Gygax@fat.admin.ch)
Date: Wed, 16 Jun 2004 07:30:08 +0200
Subject: [R] multiple error strata in aov
Message-ID: <BF74FADD4B44554CA7E53D0B5242CD6A018DB032@evd-s7014.evd.admin.ch>


Hi Murray

> I am trying to perform a model 3 ANOVA for a 2 factor (say factor A and 
> factor B) anova in which factor A is fixed and factor B is random. 
> ...
> In addition, I have tried using lme to perform this function, 
> but again without much success.

What did not work? And, did you read Pinheiro & Bates?
Personally, I find it easier to work with lme and this should be an easy
one.

What about lme (fixed= Y ~ A, random= ~ 1 | B) or
           lme (fixed= Y ~ A, random= ~ A | B)?

Regards, Lorenz
- 
Lorenz Gygax, Dr. sc. nat.
Tel: +41 (0)52 368 33 84 / lorenz.gygax at fat.admin.ch      

Center for proper housing of ruminants and pigs
Swiss Veterinary Office
agroscope FAT T??nikon, CH-8356 Ettenhausen / Switzerland
Fax : +41 (0)52 365 11 90 / Tel: +41 (0)52 368 31 31



From Lorenz.Gygax at fat.admin.ch  Wed Jun 16 07:40:26 2004
From: Lorenz.Gygax at fat.admin.ch (Lorenz.Gygax@fat.admin.ch)
Date: Wed, 16 Jun 2004 07:40:26 +0200
Subject: [R] mixed models question
Message-ID: <BF74FADD4B44554CA7E53D0B5242CD6A018DB033@evd-s7014.evd.admin.ch>


Hi Chris,

> I am trying to fit the following linear model to logged per capita 
> fecundity data (ie number of babies per female) for a mouse:
> 
> RsNRlS <- glm(formula = ln.fecundity ~ summer.rainfall + N + 
> lagged.rainfall + season, ....)
> 
> I am using this relationship in a simulation model, and the current 
> statistical model I have fit is unsatisfactory.  The problem is I get a 
> global estimate of variance (MSE), but I think it varies across subsets 
> of the data.  Specifically, seasons when there is lots of reproduction 
> (e.g. fall) tend to have high variance, while seasons with little 
> reproduction (e.g. summer) have small amounts of variance.  I am 
> looking for a method for estimating the coefficients in my linear 
> model, and estimating a separate error for subsets of the data (ie for 
> each of the 4 seasons).  The end goal is to take this linear model back 
> into my simulation model to make predictions about fecundity, but with 
> separate variance terms for subsets of the data.

Are you using glm because you need a specific distribution family (such like
poisson)?

If not, you could possibly use gls with the argument

weights= varFixed (~ season)

With that you estimate your parameters and at the same time you allow for
(and estimate) the different variances for the season.

If you need the poisson distribution, I am not quite sure what to do.
Perhaps glm also accepts this weight argument or perhaps you need to work
with a generalised procedure of lme (either from one of the new lme packages
or from MASS).

Regards, Lorenz
- 
Lorenz Gygax, Dr. sc. nat.
Tel: +41 (0)52 368 33 84 / lorenz.gygax at fat.admin.ch      

Center for proper housing of ruminants and pigs
Swiss Veterinary Office
agroscope FAT T??nikon, CH-8356 Ettenhausen / Switzerland
Fax : +41 (0)52 365 11 90 / Tel: +41 (0)52 368 31 31



From petr.pikal at precheza.cz  Wed Jun 16 08:10:00 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 16 Jun 2004 08:10:00 +0200
Subject: [R] Coercing a dataframe column to datetime
In-Reply-To: <858788618A93D111B45900805F85267A0BCB2D99@caexmta3.amd.com>
Message-ID: <40D00058.560.256647@localhost>



On 15 Jun 2004 at 11:31, richard.kittler at amd.com wrote:

> Thank you! The next step in the conversion still fails and I can't
> seem to find any examples in the archives.  The result of the function
> 'as.POSIXct(strptime())' within the 'sapply' comes back as numeric
> rather than POSIXct as expected: 
> 
> > ds <- cbind(1:2, c("02/27/92 23:03:20", "02/27/92 22:29:56")); ds 
>      [,1] [,2]               
> [1,] "1"  "02/27/92 23:03:20"
> [2,] "2"  "02/27/92 22:29:56"
> > q <- sapply(ds[,2], function(x) as.POSIXct(strptime(x,"%m/%d/%y
> > %H:%M:%S"))) class(q) 
> [1] "numeric"
> > q
> 02/27/92 23:03:20 02/27/92 22:29:56 
>         699260600         699258596 

Hi

Why do you use sapply? ds is included in some list? If not you can 
apply as.POSIXct directly to ds.

as.POSIXct(strptime(ds[,2],"%m/%d/%y %H:%M:%S"))

If you use sapply (or apply) the result is a vector or array which 
has to have the same class for all its elements. Therefore you get 
numeric representation of our dates (I suppose :-).

Cheers
Petr

> 
> --Rich
> 
> Richard Kittler 
> AMD TDG
> 408-749-4099
> 
> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
> Sent: Monday, June 14, 2004 12:05 PM
> To: Kittler, Richard
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Coercing a dataframe column to datetime
> 
> 
> You have forgotten as.POSIXct is needed too.
> 
> On Mon, 14 Jun 2004 richard.kittler at amd.com wrote:
> 
> > I am trying to coerce a data frame column from character to datetime
> > using strptime but keep getting an error because the length of the
> > coerced object is always 9.  What am I doing wrong here:   
> > 
> > .................................................................
> > > ds <- cbind(1:2, c("02/27/92 23:03:20", "02/27/92 22:29:56")); ds
> >      [,1] [,2]               
> > [1,] "1"  "02/27/92 23:03:20"
> > [2,] "2"  "02/27/92 22:29:56"
> > >  
> > > q <- strptime(ds[,2], "%m/%d/%y %H:%M:%S"); q
> > [1] "1992-02-27 23:03:20" "1992-02-27 22:29:56"
> > > 
> > > ds[,2] <- q
> > Error in "[<-"(`*tmp*`, , 2, value = q) : number of items to replace
> > is not a multiple of replacement length
> > > 
> > > length(q)
> > [1] 9
> > 
> > .................................................................
> > 
> > --Rich
> > 
> > Richard Kittler
> > AMD TDG
> > 408-749-4099
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list 
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> > 
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self) 1 South
> Parks Road,                     +44 1865 272866 (PA) Oxford OX1 3TG,
> UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From Torsten.Hothorn at rzmail.uni-erlangen.de  Wed Jun 16 08:04:08 2004
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Wed, 16 Jun 2004 08:04:08 +0200 (CEST)
Subject: [R] pmvt problem in multcomp
In-Reply-To: <87oeobimv3.wl@oak.dti.ne.jp>
References: <87vfirzwhq.wl@oak.dti.ne.jp>
	<Pine.LNX.4.51.0405231138160.19910@artemis.imbe.med.uni-erlangen.de>
	<87oeobimv3.wl@oak.dti.ne.jp>
Message-ID: <Pine.LNX.4.51.0406160801200.26629@artemis.imbe.med.uni-erlangen.de>


On Wed, 26 May 2004, Chihiro Kuroki wrote:

>

<snip>

>
> BTW, I have another strange example of simtest. I want to know
> why simtest returns these p-values.
>
> -- example 1 -------------------------------
> rm(list = ls())
> require(multcomp)
> y1 <- c(seq(3,7),seq(3,7))
> y2 <- c(rep(c(6,7,8,9),7))
> sort(runif(28),index=T) -> a
> y3 <- numeric(0)
> for(i in 1:28){
>   y3[i] <- y2[a$ix[i]]
> }
> y4 <- c(y1,y3,14,18)
>
> f2 <- factor(c(rep(1,10),rep(2,8),rep(3,8),rep(4,8),rep(5,6)))
> dat2 <- cbind(as.data.frame(y4),f2)
> summary(simtest(y4 ~ f2, data=dat2, type="Dunnett"))
>
> > dat2
>    y4 f2
> 1   3  1
> 2   4  1
> 3   5  1
> 4   6  1
> 5   7  1
> 6   3  1
> 7   4  1
> 8   5  1
> 9   6  1
> 10  7  1
> 11  6  2
> 12  7  2
> 13  6  2
> 14  9  2
> 15  7  2
> 16  8  2
> 17  6  2
> 18  8  2
> 19  9  3
> 20  8  3
> 21  7  3
> 22  9  3
> 23  6  3
> 24  8  3
> 25  9  3
> 26  7  3
> 27  7  4
> 28  9  4
> 29  6  4
> 30  6  4
> 31  9  4
> 32  8  4
> 33  7  4
> 34  9  4
> 35  6  5
> 36  8  5
> 37  8  5
> 38  7  5
> 39 14  5
> 40 18  5
> > summary(simtest(y4 ~ f2, data=dat2, type="Dunnett"))
>
> 	 Simultaneous tests: Dunnett contrasts
>
> Call:
> simtest.formula(formula = y4 ~ f2, data = dat2, type = "Dunnett")
>
> 	 Dunnett contrasts for factor f2
>
> Contrast matrix:
>           f21 f22 f23 f24 f25
> f22-f21 0  -1   1   0   0   0
> f23-f21 0  -1   0   1   0   0
> f24-f21 0  -1   0   0   1   0
> f25-f21 0  -1   0   0   0   1
>
>
> Absolute Error Tolerance:  0.001
>
> Coefficients:
>         Estimate t value Std.Err. p raw p Bonf p adj
> f25-f21    5.167  -4.644    1.022 0.000  0.000 0.000
> f23-f21    2.875  -2.813    1.022 0.008  0.024 0.022
> f24-f21    2.625  -2.569    1.022 0.015  0.029 0.028
> f22-f21    2.125  -2.079    1.113 0.045  0.045 0.045
> ---------------------------------
>
> I got the following inequality from the appended chart of a
> book.
>
> 2.558 < d(5, 35, 0.4263464, 0.05) < 2.598
>
> Are these "p adj" values right?

Chihiro,

Frank and I used your data to check the program and example with an
independent
algorithm and implementation (Westfall-Young stepdown resampling
procedure). Theory suggests that the
results should be similar to (but not necessarily the same as) those
obtained with multcomp in this special case. These are the adjusted
p-values obtained with the Westfall-Young approach for 100,000
replications:

0.0437
0.0260
0.0204
0.0001

which fit nicely with the ones obtained from multcomp.

Hope this helps & sorry for the delay,

Torsten

> --
> kuroki
> GnuPG fingerprint = 90FD FE79 905F 26F9 29C4  096F 8AA2 2C42 5130 1469
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From ripley at stats.ox.ac.uk  Wed Jun 16 09:02:31 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 16 Jun 2004 08:02:31 +0100 (BST)
Subject: [R] multiple error strata in aov
In-Reply-To: <40CF854F.5050302@sci.monash.edu.au>
Message-ID: <Pine.LNX.4.44.0406160755430.2857-100000@gannet.stats>

On Wed, 16 Jun 2004, Murray Logan wrote:

> I am trying to perform a model 3 ANOVA for a 2 factor (say factor A and 
> factor B) anova in which factor A is fixed and factor B is random. 
>  Therefore, the error term for the test of factor A should be the A:B 
> interaction term and the error terms for B and A:B should be the model 
> residual (within) term.  I have tried to work out how to specify such 
> error strata using aov, however, I have had little success.  Is there a 
> way to specify just the interaction (A:B) as an error term using Error() 
> within aov or provide a list of error strata.  
> Ideally as Error=list(A="A:B",B="Within", A:B="Within")

Error is documented in ?aov, which has a reference.  Have you read it?
It does contain the answer to your question.  I will not try to explain it 
here, and your attempt at specification suggests you do not understand the 
meaning of `strata' here.  The strata are function of the way the 
experiment was designed, not the AOV tables you want which should be a 
consequence of the design.

> In addition, I have tried using lme to perform this function, but again 
> without much success.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Jun 16 09:04:22 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 16 Jun 2004 08:04:22 +0100 (BST)
Subject: [R] time series object
In-Reply-To: <BAY12-F1044pCfynIQW00063b48@hotmail.com>
Message-ID: <Pine.LNX.4.44.0406160803090.2857-100000@gannet.stats>

window() is the normal way to do this.

On Tue, 15 Jun 2004, Laura Holt wrote:

> Hi R People:
> 
> I have a monthly time series x.ts which runs from 1/1995 through 12/2003.
> 
> >x.ts <- ts(x,start=1995,freq=12)
> >str(x.ts)
> Time-Series [1:108] from 1995 to 2004: -1.638 -0.236  0.830 -0.548  0.363 
> ...
> >
> 
> My question: is there a way to print the observations from 1/1999 to 6/1999, 
> please?
> (other than x.ts[49:54])

which does not give you a time series, BTW.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ahmlatif at yahoo.com  Wed Jun 16 09:23:04 2004
From: ahmlatif at yahoo.com (Mahbub Latif)
Date: Wed, 16 Jun 2004 00:23:04 -0700 (PDT)
Subject: [R] off topic: C/C++ codes for pseudo inverse
Message-ID: <20040616072304.34229.qmail@web41215.mail.yahoo.com>

Hi,

I am looking for C/C++ codes for computing generalized
inverse of a matrix. Can anyone help me in this
regard?

Thanks,

Mahbub.



From e.pebesma at geog.uu.nl  Wed Jun 16 09:48:34 2004
From: e.pebesma at geog.uu.nl (Edzer J. Pebesma)
Date: Wed, 16 Jun 2004 09:48:34 +0200
Subject: [R] [R-pkgs] gstat 0.9-12: cokriging cross validation and class name
	incompatibilities
Message-ID: <40CFFB52.4040102@geog.uu.nl>

I uploaded gstat 0.9-12 to CRAN, which has a few important changes:

1. Cokriging cross validation

Cokriging cross validation is now possible with the function gstat.cv:
you simply pass a multivariable gstat object, and cross validation is done
for the first variable in the object. Optionally, secondary variable 
records
at locations coinciding with the validation locations are removed.

2. Class name changes

Both gstat and geoR used the name "variogram" for -- highly incompatible --
objects that contain information about a variogram. This led to errors when
calling plot() with a variogram object calculated from one, but plotted by
the other package. I changed the class name into "gstatVariogram". The full
list of class name changes:

variogram -> gstatVariogram
point.pairs -> pointPairs
variogram.map -> variogramMap
variogram.cloud -> variogramCloud

You can still run scripts you have with the new packages, but e.g. plot(v)
when v is an old object of class "variogram" will not work with the new
package. As these are S3 classes, a simple solution would be to
reassign the class:

class(v) = c("gstatVariogram", "data.frame")

3. Compatibility with package sp and variogram maps
On http://sourceforge.net/projects/r-spatial/ beta releases for the upcoming
package sp are available; sp provides classes and methods for spatial data.
Instead of:

 > library(gstat)
 > data(meuse)
 > variogram(zinc~1, ~x+y, meuse)

you could, with package sp loaded, do:

 > coordinates(meuse) = ~x+y # promotes meuse to SpatialDataFrame
 > variogram(zinc~1, meuse) # no coordinates required.

In addition, sp provides classes for gridded data and polygon data. With sp
loaded, gstat can calculate variogram maps.

Any comments are welcome,
--
Edzer

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://www.stat.math.ethz.ch/mailman/listinfo/r-packages



From laura at env.leeds.ac.uk  Wed Jun 16 11:45:43 2004
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Wed, 16 Jun 2004 10:45:43 +0100 (BST)
Subject: [R] dev() error
Message-ID: <Pine.LNX.4.44.0406161042450.15904-100000@env-pc-phd13>

I don't know what I have done!! When I try to invoke a display function
via plot(), image() or whatever, my X11 window doesn't appear. I have
tried to type X11() but the following error message is displayed:

> x11()
Error in X11(display, width, height, pointsize, gamma, colortype,
maxcubesize,
:
        unable to start device X11
In addition: Warning message:
unable to open connection to X11 display`'

I have tried dev.cur() and dev.list() but these suggest that only my null
device is active and I am unsure how to activate my X11 device by any
other means!

I'm sure this is a stupid question but could anyone please help?

Thanks in advance..
Laura



From christian.hoffmann at wsl.ch  Wed Jun 16 12:04:48 2004
From: christian.hoffmann at wsl.ch (Christian Hoffmann)
Date: Wed, 16 Jun 2004 12:04:48 +0200
Subject: [R] start-up problems
Message-ID: <40D01B40.7070805@wsl.ch>

Hi,

After some reading and experimentation I found that I cannot solve this 
problem:

1.
I am starting R and check:

 > system("pwd")
/home/woodstock/hoffmacw/R/test
 > system("echo $HOME")
/home/woodstock/hoffmacw
 > system("echo $R_PROFILE")
/home/woodstock/hoffmacw/R
 > system("ls $R_PROFILE/.Rprofile")
/home/woodstock/hoffmacw/R/.Rprofile
 > system("cat $R_PROFILE/.Rprofile")   # gives

  system("pwd")
  (editedOn <- "2004-06-15, 11:10")

.Last  <- function() {
	cat("Adieu, delete all ps.out.*.ps files\n")
	system("rm -f ps.out.*.ps")
	system(" \n")
}
.First <- function() {
   options(digits=6)
   library(MASS)
   help.start()
   x11(display = "", width = 9, height = 7, pointsize = 12, gamma = 1, 
colortype = getOption("X11colortype"), maxcubesize = 256)
}
 >

Now

 > source("$R_PROFILE/.Rprofile")
Error in file(file, "r") : unable to open connection
In addition: Warning message:
cannot open file `$R_PROFILE/.Rprofile'
 >

but

 > source("~/R/.Rprofile")
/home/woodstock/hoffmacw/R/test
 >

This does IMHO not explain why .Rprofile is not being sourced at 
startup. help.start and x11 are *not* executed, of course.

Is there an unseen obstacle?


2.
When I manually source("~/R/.Rprofile"), after exiting and saving the 
work space, and restarting R I get:

R : Copyright 2004, The R Foundation for Statistical Computing
Version 1.9.0  (2004-04-12), ISBN 3-900051-00-3
....
[Previously saved workspace restored]

Loading required package: stats
Error in .First() : couldn't find function "help.start"
 > options(STERM='iESS', editor='emacsclient')
 >

Not finding  help.start is puzzling! Because of this error, x11 would 
not start. Exchanging the lines with help.start and x11 would just 
result in not finding x11!


Yet doing a manual
 > help.start()
is possible.

Sorry for the lengthy description, but perhaps there is quite a simple 
reason for this misbehaviour.

Christian

-- 
Dr.sc.math.Christian W. Hoffmann, 
http://www.wsl.ch/staff/christian.hoffmann
Mathematics + Statistical Computing   e-mail: christian.hoffmann at wsl.ch
Swiss Federal Research Institute WSL  Tel: ++41-44-73922-   -77  (office)
CH-8903 Birmensdorf, Switzerland             -11(exchange), -15  (fax)



From Torsten.Hothorn at rzmail.uni-erlangen.de  Wed Jun 16 12:06:02 2004
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Wed, 16 Jun 2004 12:06:02 +0200 (CEST)
Subject: [R] off topic: C/C++ codes for pseudo inverse
In-Reply-To: <20040616072304.34229.qmail@web41215.mail.yahoo.com>
References: <20040616072304.34229.qmail@web41215.mail.yahoo.com>
Message-ID: <Pine.LNX.4.51.0406161203500.26701@artemis.imbe.med.uni-erlangen.de>

On Wed, 16 Jun 2004, Mahbub Latif wrote:

> Hi,
>
> I am looking for C/C++ codes for computing generalized
> inverse of a matrix. Can anyone help me in this
> regard?
>

experimental, undocumented and for square matrices only:

SEXP svd (SEXP x) {

    /* experimental */

    SEXP jobu, jobv, u, v, method, dummy, ans;
    int i, p;

    if (!isMatrix(x) || !isReal(x))
        error("x is not a real matrix");

    PROTECT(method = allocVector(STRSXP, 1));
    SET_STRING_ELT(method, 0, mkChar("dgesdd"));
    PROTECT(jobu = allocVector(STRSXP, 1));
    SET_STRING_ELT(jobu, 0, mkChar("S"));
    PROTECT(jobv = allocVector(STRSXP, 1));
    SET_STRING_ELT(jobv, 0, mkChar(""));
    p = INTEGER(getAttrib(x, R_DimSymbol))[0];
    PROTECT(u = allocMatrix(REALSXP, p, p));
    PROTECT(v = allocMatrix(REALSXP, p, p));
    for (i = 0; i < p*p; i++) {
        REAL(u)[i] = 0;
        REAL(v)[i] = 0;
    }
    PROTECT(dummy = allocVector(REALSXP, p));
    PROTECT(ans = La_svd(jobu, jobv, x, dummy, u, v, method));
    UNPROTECT(7);
    return(ans);
}

void cMPinv (SEXP x, double tol, double *ans, double *rank) {

    SEXP svdx, d, u, vt;
    int i, j, p, k, *positive;
    double *dd, *du, *dvt;

    PROTECT(svdx = svd(x));
    d = VECTOR_ELT(svdx, 0);
    dd = REAL(d);
    u = VECTOR_ELT(svdx, 1);
    du = REAL(u);
    vt = VECTOR_ELT(svdx, 2);
    dvt = REAL(vt);
    p = LENGTH(d);

    tol = tol * dd[0];
    if (tol < 0.0) tol = 0.0;

    positive = Calloc(p, int);

    rank[0] = 0.0;
    for (i = 0; i < p; i++) {
        if (dd[i] > tol) {
            positive[i] = 1;
            rank[0]++;
        }
    }

    for (j = 0; j < p; j++) {
        if (positive[j]) {
            for (i = 0; i < p; i++)
                du[j * p + i] *= (1 / dd[j]);
        }
    }

    for (i = 0; i < p; i++) {
        for (j = 0; j < p; j++) {
            ans[j * p + i] = 0.0;
            for (k = 0; k < p; k++) {
                if (positive[k])
                    ans[j * p + i] += dvt[i * p + k] * du[p * k + j];
            }
        }
    }

    Free(positive);
    UNPROTECT(1);
}

SEXP MPinv (SEXP x, SEXP tol) {

    SEXP ans, mp, rank;
    int p;

    if (!isMatrix(x) || !isReal(x))
        error("x is not a real matrix");

    if (INTEGER(getAttrib(x, R_DimSymbol))[0] !=
        INTEGER(getAttrib(x, R_DimSymbol))[1])
        error("x is not a square matrix");

    if (!isReal(tol) || LENGTH(tol) != 1)
        error("tol is not a scalar real");

    p = INTEGER(getAttrib(x, R_DimSymbol))[0];

    PROTECT(ans = allocVector(VECSXP, 2));
    SET_VECTOR_ELT(ans, 0, mp = allocMatrix(REALSXP, p, p));
    SET_VECTOR_ELT(ans, 1, rank = allocVector(REALSXP, 1));

    cMPinv(x, REAL(tol)[0], REAL(mp), REAL(rank));

    UNPROTECT(1);
    return(ans);
}

you need to include `La_svd' from R.

Torsten

> Thanks,
>
> Mahbub.
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From ligges at statistik.uni-dortmund.de  Wed Jun 16 12:19:05 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 16 Jun 2004 12:19:05 +0200
Subject: [R] dev() error
In-Reply-To: <Pine.LNX.4.44.0406161042450.15904-100000@env-pc-phd13>
References: <Pine.LNX.4.44.0406161042450.15904-100000@env-pc-phd13>
Message-ID: <40D01E99.5000103@statistik.uni-dortmund.de>

Laura Quinn wrote:

> I don't know what I have done!! When I try to invoke a display function
> via plot(), image() or whatever, my X11 window doesn't appear. I have
> tried to type X11() but the following error message is displayed:
> 
> 
>>x11()
> 
> Error in X11(display, width, height, pointsize, gamma, colortype,
> maxcubesize,
> :
>         unable to start device X11
> In addition: Warning message:
> unable to open connection to X11 display`'

I guess you have logged via ssh by not allowing X forwarding, or you ave 
changed your UID using su or something like that...

Has R been compiled with X11 support?

Can you start any other program that uses X?

Uwe Ligges


> I have tried dev.cur() and dev.list() but these suggest that only my null
> device is active and I am unsure how to activate my X11 device by any
> other means!
> 
> I'm sure this is a stupid question but could anyone please help?
> 
> Thanks in advance..
> Laura
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From anders.sjogren at math.chalmers.se  Wed Jun 16 12:25:46 2004
From: anders.sjogren at math.chalmers.se (=?ISO-8859-1?Q?Anders_Sj=F6gren?=)
Date: Wed, 16 Jun 2004 12:25:46 +0200
Subject: [R] 64-bit R on OS X/G5?
Message-ID: <87D57BA0-BF7F-11D8-8D66-000393CE2F0C@math.chalmers.se>

Hi!

Does anyone know if there is a working 64-bit version of R for MacOs X 
on G5? If so, is there a memory limit anyway or could I theroetically 
use almost 8 GB in one session?

Best regards

Anders Sj??gren

PhD Student
Mathematical Statistics
Chalmers University of Technology
G??teborg, Sweden



From laura at env.leeds.ac.uk  Wed Jun 16 12:23:03 2004
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Wed, 16 Jun 2004 11:23:03 +0100 (BST)
Subject: [R] dev() error
In-Reply-To: <40D01E99.5000103@statistik.uni-dortmund.de>
Message-ID: <Pine.LNX.4.44.0406161120000.15904-100000@env-pc-phd13>

No nothing like that, I am running two parallel R sessions (something I
often do) and the other one is working absolutely fine - I am working
directly from my network, not via ssh, and certainly am not privvy to any
super user passwords!

On Wed, 16 Jun 2004, Uwe Ligges wrote:

> Laura Quinn wrote:
>
> > I don't know what I have done!! When I try to invoke a display function
> > via plot(), image() or whatever, my X11 window doesn't appear. I have
> > tried to type X11() but the following error message is displayed:
> >
> >
> >>x11()
> >
> > Error in X11(display, width, height, pointsize, gamma, colortype,
> > maxcubesize,
> > :
> >         unable to start device X11
> > In addition: Warning message:
> > unable to open connection to X11 display`'
>
> I guess you have logged via ssh by not allowing X forwarding, or you ave
> changed your UID using su or something like that...
>
> Has R been compiled with X11 support?
>
> Can you start any other program that uses X?
>
> Uwe Ligges
>
>
> > I have tried dev.cur() and dev.list() but these suggest that only my null
> > device is active and I am unsure how to activate my X11 device by any
> > other means!
> >
> > I'm sure this is a stupid question but could anyone please help?
> >
> > Thanks in advance..
> > Laura
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From dmb at mrc-dunn.cam.ac.uk  Wed Jun 16 12:52:54 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Wed, 16 Jun 2004 11:52:54 +0100 (BST)
Subject: [R] non-linear binning? power-law in R
In-Reply-To: <16582.51230.784308.792650@bushmills.inf.ed.ac.uk>
Message-ID: <Pine.LNX.4.21.0406161059540.20180-100000@mail.mrc-dunn.cam.ac.uk>


First, thanks to everyone who helped me get to grips with R in (x)emacs
(I get confused easily). Special thanks to Stephen Eglen for continued
support.

My question is about non-linear binning, or density functions over
distributions governed by a power law ...

y ~ mu*x**lambda	# In one of its forms 
                        # (can't find Pareto in the online help)

Looking at the following should show my problem....

x3 <- runif(10000)**3	# Probably a better (correct) way to do this

plot( density(x3,cut=0,bw=0.1))
plot( density(x3,cut=0,bw=0.01))
plot( density(x3,cut=0,bw=0.001))

plot(density(x3,cut=0,bw=0.1),  log='xy')
plot(density(x3,cut=0,bw=0.01), log='xy')
plot(density(x3,cut=0,bw=0.001),log='xy')

The upper three plots show that the bw has a big effect on the appearance
of the graph by rescaling based on the initial density at low values of x,
which is very high.

The lower plots show (I think) an error in the use of linear bins to view
a non linear trend. I would expect this curve to be linear on log-log
scales (from experience), and you can see the expected behavior in the
tails of these plots.

If you play with drawing these curves on top of each other they look OK
apart from at the beginning. However, changing the band width to 0.0001 has
a radical effect on these plots, and they begin to show a different trend
(look like they are being governed by a different power).

Hmmm....

x3log <- -log(x3)

plot( density(x3log,cut=0,bw=0.5),  log='y',col=1)

lines(density(x3log,cut=0,bw=0.2),  log='y',col=2)
lines(density(x3log,cut=0,bw=0.1),  log='y',col=3)
lines(density(x3log,cut=0,bw=0.01), log='y',col=4)

Sorry...


'Real' data of this form is usually discrete, with the value of 1 being
the most frequent (minimum) event, and higher values occurring less
frequently according to a power (power-law). This data can be easily
grouped into discrete bins, and frequency plotted on log scales. The
continuous data generated above requires some form of density estimation
or rescaling into discreet values (make the smallest value equal to 1 and
round everything else into an integer).

I see the aggregate function, but which function lets me simply count the
number of values in a class (integer bin)?

The analysis of even the discretized data is made more accurate by the use
of exponentially growing bins. This way you don't need to plot the data on
log scales, and the increasing variance associated with lower probability
events is handled by the increasing bin size (giving good accuracy of
power fitting). How can I easily (ignorantly) implement exponentially
increasing bin sizes?

Thanks for any feedback,

Dan.



From sdavis2 at mail.nih.gov  Wed Jun 16 12:32:38 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Wed, 16 Jun 2004 06:32:38 -0400
Subject: [R] non-linear binning? power-law in R
In-Reply-To: <Pine.LNX.4.21.0406161059540.20180-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <BCF59A06.94CC%sdavis2@mail.nih.gov>

Is ?cut what you need?

Sean


On 6/16/04 6:52 AM, "Dan Bolser" <dmb at mrc-dunn.cam.ac.uk> wrote:

> 
> First, thanks to everyone who helped me get to grips with R in (x)emacs
> (I get confused easily). Special thanks to Stephen Eglen for continued
> support.
> 
> My question is about non-linear binning, or density functions over
> distributions governed by a power law ...
> 
> y ~ mu*x**lambda    # In one of its forms
>                       # (can't find Pareto in the online help)
> 
> Looking at the following should show my problem....
> 
> x3 <- runif(10000)**3    # Probably a better (correct) way to do this
> 
> plot( density(x3,cut=0,bw=0.1))
> plot( density(x3,cut=0,bw=0.01))
> plot( density(x3,cut=0,bw=0.001))
> 
> plot(density(x3,cut=0,bw=0.1),  log='xy')
> plot(density(x3,cut=0,bw=0.01), log='xy')
> plot(density(x3,cut=0,bw=0.001),log='xy')
> 
> The upper three plots show that the bw has a big effect on the appearance
> of the graph by rescaling based on the initial density at low values of x,
> which is very high.
> 
> The lower plots show (I think) an error in the use of linear bins to view
> a non linear trend. I would expect this curve to be linear on log-log
> scales (from experience), and you can see the expected behavior in the
> tails of these plots.
> 
> If you play with drawing these curves on top of each other they look OK
> apart from at the beginning. However, changing the band width to 0.0001 has
> a radical effect on these plots, and they begin to show a different trend
> (look like they are being governed by a different power).
> 
> Hmmm....
> 
> x3log <- -log(x3)
> 
> plot( density(x3log,cut=0,bw=0.5),  log='y',col=1)
> 
> lines(density(x3log,cut=0,bw=0.2),  log='y',col=2)
> lines(density(x3log,cut=0,bw=0.1),  log='y',col=3)
> lines(density(x3log,cut=0,bw=0.01), log='y',col=4)
> 
> Sorry...
> 
> 
> 'Real' data of this form is usually discrete, with the value of 1 being
> the most frequent (minimum) event, and higher values occurring less
> frequently according to a power (power-law). This data can be easily
> grouped into discrete bins, and frequency plotted on log scales. The
> continuous data generated above requires some form of density estimation
> or rescaling into discreet values (make the smallest value equal to 1 and
> round everything else into an integer).
> 
> I see the aggregate function, but which function lets me simply count the
> number of values in a class (integer bin)?
> 
> The analysis of even the discretized data is made more accurate by the use
> of exponentially growing bins. This way you don't need to plot the data on
> log scales, and the increasing variance associated with lower probability
> events is handled by the increasing bin size (giving good accuracy of
> power fitting). How can I easily (ignorantly) implement exponentially
> increasing bin sizes?
> 
> Thanks for any feedback,
> 
> Dan.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Wed Jun 16 12:50:20 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 16 Jun 2004 11:50:20 +0100 (BST)
Subject: [R] start-up problems
In-Reply-To: <40D01B40.7070805@wsl.ch>
Message-ID: <Pine.LNX.4.44.0406161148480.11760-100000@gannet.stats>

Please do read the NEWS file.  help.start is in package utils, and that is
not available when .Rprofile is run.  Similarly for x11 and package
graphics.  This and the workarounds are described in the NEWS file for
1.9.0.

On Wed, 16 Jun 2004, Christian Hoffmann wrote:

> Hi,
> 
> After some reading and experimentation I found that I cannot solve this 
> problem:
> 
> 1.
> I am starting R and check:
> 
>  > system("pwd")
> /home/woodstock/hoffmacw/R/test
>  > system("echo $HOME")
> /home/woodstock/hoffmacw
>  > system("echo $R_PROFILE")
> /home/woodstock/hoffmacw/R
>  > system("ls $R_PROFILE/.Rprofile")
> /home/woodstock/hoffmacw/R/.Rprofile
>  > system("cat $R_PROFILE/.Rprofile")   # gives
> 
>   system("pwd")
>   (editedOn <- "2004-06-15, 11:10")
> 
> .Last  <- function() {
> 	cat("Adieu, delete all ps.out.*.ps files\n")
> 	system("rm -f ps.out.*.ps")
> 	system(" \n")
> }
> .First <- function() {
>    options(digits=6)
>    library(MASS)
>    help.start()
>    x11(display = "", width = 9, height = 7, pointsize = 12, gamma = 1, 
> colortype = getOption("X11colortype"), maxcubesize = 256)
> }
>  >
> 
> Now
> 
>  > source("$R_PROFILE/.Rprofile")
> Error in file(file, "r") : unable to open connection
> In addition: Warning message:
> cannot open file `$R_PROFILE/.Rprofile'
>  >
> 
> but
> 
>  > source("~/R/.Rprofile")
> /home/woodstock/hoffmacw/R/test
>  >
> 
> This does IMHO not explain why .Rprofile is not being sourced at 
> startup. help.start and x11 are *not* executed, of course.
> 
> Is there an unseen obstacle?
> 
> 
> 2.
> When I manually source("~/R/.Rprofile"), after exiting and saving the 
> work space, and restarting R I get:
> 
> R : Copyright 2004, The R Foundation for Statistical Computing
> Version 1.9.0  (2004-04-12), ISBN 3-900051-00-3
> ....
> [Previously saved workspace restored]
> 
> Loading required package: stats
> Error in .First() : couldn't find function "help.start"
>  > options(STERM='iESS', editor='emacsclient')
>  >
> 
> Not finding  help.start is puzzling! Because of this error, x11 would 
> not start. Exchanging the lines with help.start and x11 would just 
> result in not finding x11!
> 
> 
> Yet doing a manual
>  > help.start()
> is possible.
> 
> Sorry for the lengthy description, but perhaps there is quite a simple 
> reason for this misbehaviour.
> 
> Christian
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From s.volinia at unife.it  Wed Jun 16 12:54:59 2004
From: s.volinia at unife.it (Stefano Volinia)
Date: Wed, 16 Jun 2004 12:54:59 +0200
Subject: [R] Jobs openings in Italy
Message-ID: <40D02703.3000700@unife.it>

I apologize if this post is not relevant to this mailing list, but I 
feel it might be of interest for many of you and did not find any other 
more suitable R-list.


We are seeking for a couple of post-docs at University of Ferrara, 
Italy. The posts are in microarrays data mining and are for people with 
a degree in Statistics, IT or related. A documented interest in 
biological problems, network analysis and system biology would be an 
advantage.
These are up to three years appointments starting from Fall 2004. The 
project in which the two successful candidates will be involved is a 
Telethon  Facility for Microarray Analysis. The environment includes 
up-to-date molecular biology facilities, such as for example an 
Affymentrix station.The salary will be according to  experience.

*Biostatistician*
Requirements: Experience with R.  Must be able to execute statistical 
methods including linear and nonlinear models, Hidden Markov Models, and 
Bayesian methods. Familiarity with multiple testing, permutation, 
cross-validation, cluster analysis is needed.  Be able to perform 
simulations and test experimental design strategies.
To work closely with biologists in the area of microarray analysis and 
software development, by using both Bioconductor and R.
Strength in computer programming is a plus. Array data includes open 
source and Affymetrix platform.


*Bioinformatician*

Requirements: Good knowledge of object-oriented programming, SQL and 
proven skills in Perl and/or  C++. Bioinformatics background is a plus.

The position is for the further development of the GOAL online system 
(http://microarrays.unife.it), its integration with different databases 
and the implementation of tools for data analysis and visualization by 
WWW interfaces.
A knowledge of PHP, or Python together with experience in relational 
database design and management is desirable.


Bye
Stefano

-- 
Telethon Facility - Data Mining for Analysis of DNA Microarrays
Lab. di Genomica Funzionale 
Dip. di Morfologia ed Embriologia - Universita' degli Studi
Via Fossato di Mortara 64/b, 44100 Ferrara (ITALY)
Tel +39-0532-291714 or -291537                   Fax +39-0532-291533



From p.dalgaard at biostat.ku.dk  Wed Jun 16 12:58:17 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 16 Jun 2004 12:58:17 +0200
Subject: [R] dev() error
In-Reply-To: <Pine.LNX.4.44.0406161120000.15904-100000@env-pc-phd13>
References: <Pine.LNX.4.44.0406161120000.15904-100000@env-pc-phd13>
Message-ID: <x23c4vlqgm.fsf@biostat.ku.dk>

Laura Quinn <laura at env.leeds.ac.uk> writes:

> No nothing like that, I am running two parallel R sessions (something I
> often do) and the other one is working absolutely fine - I am working
> directly from my network, not via ssh, and certainly am not privvy to any
> super user passwords!

> > > Error in X11(display, width, height, pointsize, gamma, colortype,
> > > maxcubesize,
> > > :
> > >         unable to start device X11
> > > In addition: Warning message:
> > > unable to open connection to X11 display`'

Well, *something* zapped your DISPLAY environment variable... You
might try  Sys.getenv("DISPLAY") in the session that works and
Sys.putenv(DISPLAY=....) in the one that doesn't.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Wed Jun 16 13:15:56 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 16 Jun 2004 12:15:56 +0100 (BST)
Subject: [R] 64-bit R on OS X/G5?
In-Reply-To: <87D57BA0-BF7F-11D8-8D66-000393CE2F0C@math.chalmers.se>
Message-ID: <Pine.LNX.4.44.0406161151540.11760-100000@gannet.stats>

R works on 64-bit platforms and has for quite a while, currently I know of 
people using machines with 16Gb RAM on 64-bit platforms (Alpha, Sparc, 
AMD64).

Do you have a full 64-bit compiler and runtime for MacOS X?  If so `all'
you need to do is to run compile under it.  One quote I picked up was

	macos X has 64 bit support with gcc 3.3 and a g5

which if correct suggests this is possible.

On Wed, 16 Jun 2004, Anders Sj??gren wrote:

> Hi!
> 
> Does anyone know if there is a working 64-bit version of R for MacOs X 
> on G5? If so, is there a memory limit anyway or could I theroetically 
> use almost 8 GB in one session?

There is of course a memory limit but it far exceeds 8GB.  There is a 
limit on the length of vectors still (2^31 - 1) and a few other similar 
limits.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Ted.Harding at nessie.mcc.ac.uk  Wed Jun 16 12:58:12 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 16 Jun 2004 11:58:12 +0100 (BST)
Subject: [R] dev() error
In-Reply-To: <Pine.LNX.4.44.0406161120000.15904-100000@env-pc-phd13>
Message-ID: <XFMail.040616115812.Ted.Harding@nessie.mcc.ac.uk>

On 16-Jun-04 Laura Quinn wrote:
> No nothing like that, I am running two parallel R sessions (something I
> often do) and the other one is working absolutely fine - I am working
> directly from my network, not via ssh, and certainly am not privvy to
> any super user passwords!

It looks as though you may be logged in to two different machines
for your two R sessions, in which case it looks as though one of
them has ascertained the X display for your machine (i.e. the one
you are sitting at), and the other has not. (Indeed, the one that
works might be your machine itself, in which case it knows anyway).

Test:

1. In the xterm logged in to the machine which works, before starting R,
   enter

     echo $DISPLAY

2. On the xterm logged in to the machine which does not work, before
   starting R, enter

     echo $DISPLAY

For example, I'm using machine "compo" here on which is an xterm
logged in to another machine called "brandy". So, in this xterm,

  ted at brandy:~ > echo $DISPLAY
  compo.fort.knox.uk:0.0

showing that "brandy" knows the X display destination for "compo".

So, when you do (1) above, you should see something similar. If I'm
right, when you do (2) above, you will probably just get a blank
response.

If that happens, then you have to see about getting the (2) machine
to set the DISPLAY variable when you log in to it. This can be tricky:
I've known cases where the cause was quite deeply hidden. Basically,
the information for the DISPLAY variable is ascertained by a script
which is run when you log in to the remote machine, which may be
a sub-script of some other script, ...

I hope this helps,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 16-Jun-04                                       Time: 11:58:12
------------------------------ XFMail ------------------------------



From laura at env.leeds.ac.uk  Wed Jun 16 13:39:16 2004
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Wed, 16 Jun 2004 12:39:16 +0100 (BST)
Subject: [R] dev() error
In-Reply-To: <XFMail.040616115812.Ted.Harding@nessie.mcc.ac.uk>
Message-ID: <Pine.LNX.4.44.0406161237070.15904-100000@env-pc-phd13>

Thank you for all your help!

I have finally solved the issue by simply externally saving all my R
session info and restarting the session from another place, it seems to be
working this time. It seems I was getting my pathways a little mixed up.

Thanks again.
Laura

On Wed, 16 Jun 2004 Ted.Harding at nessie.mcc.ac.uk wrote:

> On 16-Jun-04 Laura Quinn wrote:
> > No nothing like that, I am running two parallel R sessions (something I
> > often do) and the other one is working absolutely fine - I am working
> > directly from my network, not via ssh, and certainly am not privvy to
> > any super user passwords!
>
> It looks as though you may be logged in to two different machines
> for your two R sessions, in which case it looks as though one of
> them has ascertained the X display for your machine (i.e. the one
> you are sitting at), and the other has not. (Indeed, the one that
> works might be your machine itself, in which case it knows anyway).
>
> Test:
>
> 1. In the xterm logged in to the machine which works, before starting R,
>    enter
>
>      echo $DISPLAY
>
> 2. On the xterm logged in to the machine which does not work, before
>    starting R, enter
>
>      echo $DISPLAY
>
> For example, I'm using machine "compo" here on which is an xterm
> logged in to another machine called "brandy". So, in this xterm,
>
>   ted at brandy:~ > echo $DISPLAY
>   compo.fort.knox.uk:0.0
>
> showing that "brandy" knows the X display destination for "compo".
>
> So, when you do (1) above, you should see something similar. If I'm
> right, when you do (2) above, you will probably just get a blank
> response.
>
> If that happens, then you have to see about getting the (2) machine
> to set the DISPLAY variable when you log in to it. This can be tricky:
> I've known cases where the cause was quite deeply hidden. Basically,
> the information for the DISPLAY variable is ascertained by a script
> which is run when you log in to the remote machine, which may be
> a sub-script of some other script, ...
>
> I hope this helps,
> Ted.
>
>
> --------------------------------------------------------------------
> E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
> Fax-to-email: +44 (0)870 167 1972
> Date: 16-Jun-04                                       Time: 11:58:12
> ------------------------------ XFMail ------------------------------
>



From FowlerM at mar.dfo-mpo.gc.ca  Wed Jun 16 13:50:46 2004
From: FowlerM at mar.dfo-mpo.gc.ca (Fowler, Mark)
Date: Wed, 16 Jun 2004 08:50:46 -0300
Subject: [R] S/R/RWeb/ODBC
Message-ID: <1A4AC4BAB9C50A42854582B69B08C034023550B0@MSGMARBIO05>

Yes, however the same odbcConnect syntax works in the R Commands window. But
seemingly not from within a function in .Rprofile when using RWeb to run the
function in batch mode. Should mention I have no problem running the
function if I bring the data in from the PERL script that runs R. 

E.g.  (whatever edited)

Batch (passing function call via an RWeb PERL script)
library(RODBC)
channel<-odbcConnect('xyz',uid='whatever',pw='whatever',dsn='whatever',case=
'nochange')
cat("\n",channel)

-1

Interactive
> library(RODBC)
>
channel<-odbcConnect('xyz',uid='whatever',pw='whatever',dsn='whatever',case=
'nochange')
Warning message: 
NAs introduced by coercion 

> channel
RODB Connection 0
Details:
  case=nochange
  DSN=whatever
  UID=whatever
  PWD=whatever
  DBQ=whatever
  DBA=W
  APA=T
  FEN=T
  QTO=T
  FRC=10
  FDL=10
  LOB=T
  RST=T
  FRL=F
  MTS=F
  CSR=F
  PFC=10
  TLO=0


>	Mark Fowler
>	Marine Fish Division
>	Bedford Inst of Oceanography
>	Dept Fisheries & Oceans
>	Dartmouth NS Canada
>	fowlerm at mar.dfo-mpo.gc.ca
>


-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: June 15, 2004 4:04 PM
To: Fowler, Mark
Cc: 'r-help at stat.math.ethz.ch'
Subject: Re: [R] S/R/RWeb/ODBC


Sounds like a permissions/ownership problem.  odbcConnect returnign -1 
just means that the ODBC device manager failed.

On Tue, 15 Jun 2004, Fowler, Mark wrote:

> I'm looking for an optimal approach to access Oracle databases via 
> RWeb applications. I'm new to R but familiar with programming 
> functions and web pages for the S+ Statserver. I'm now going through 
> the motions of migrating
> S+/Statserver applications to R/RWeb as a feasability exercise. I can 
> S+access
> databases using ODBC directly in R or S, and using Statserver, but I 
> have not succeeded at extracting into R in RWebs batch mode. I can 
> 'require RODBC' in .Rprofile with apparent success, but the results of 
> odbcConnect differ from those when the command is typed into the R 
> commands window. Instead of a parameter list I get a -1. Does anyone 
> know the solution to this problem? Also, might anyone know the 
> comparative merits of using some PERL module (like DBD::Oracle) to do 
> the extraction, as opposed to using RODBC (assuming RODBC can be 
> implemented in batch mode)? I'm currently using Apache on Windows XP 
> if relevant, but LINUX may be the final host (we'll compare, but those 
> more Web-wise than I expect LINUX to outperform Windows for our 
> purposes).
> 
> > 	Mark Fowler
> > 	Marine Fish Division
> > 	Bedford Inst of Oceanography
> > 	Dept Fisheries & Oceans
> > 	Dartmouth NS Canada
> > 	fowlerm at mar.dfo-mpo.gc.ca
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From petr.pikal at precheza.cz  Wed Jun 16 13:54:20 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Wed, 16 Jun 2004 13:54:20 +0200
Subject: R: [R] slope estimations of teeth like data
In-Reply-To: <20040615213320.B9F4A394B@mprdmxin.myway.com>
Message-ID: <40D0510C.12231.160DDDC@localhost>

Thank you for your insights to my problem. I finally came up with 
function which sets an index for increasing resp decreasing parts of 
my data.

increasing<-function(x, cutoff=0, s=3, cas.osa=cas.osa, ...)
{
x.agg<-aggregate(x, list(hod=cut(cas.osa,"5 min")), mean, 
na.rm=T)

index<-diff(x.agg[,2, drop=T])<cuttof

ind<-outer(which(index),(-s):(s*2),"+")
	ind<-ind[ind>0 & ind <= length(index)]
	index[ind]<-TRUE
idx<-rep(index,each=5)
idx<-c(rep(idx[1],5),idx)
invisible(idx)
}


I have much more points than in my toy example so there is no big 
problem with throwing away some data. However, both packages 
you (Achim, Vito) suggested seems to be useful in further 
processing of my time series data

Thanks again.
Petr


On 15 Jun 2004 at 17:33, Gabor Grothendieck wrote:

> 
> 
> I am not entirely sure what is essential and what is inessential in
> this problem.  In the yy data shown, the peaks occur in one step and
> all the peaks are equal.  Just plotting
> 
> plot(diff(yy))
> 
> shows that there is a huge region of potential cutoffs for separating
> out the large single step increases from the other differences.
> 
> If it continues to be true that the increases are large relative to
> everything else but the cutoff is not quite so obvious as here, we can
> calculate a cutoff by just sorting the unique differences and
> partitioning them in every possible way into small and large
> differences and then choosing the partition whose residual variance is
> least.   
> 
> Its actually pretty simple to do that from first pinciples, as
> shown here, but there might be a function in R that does it
> even more directly.
> 
> diffyy <- diff(yy)
> yy. <- unique(sort(diffyy)) # unique diffs sorted
> yy. <- yy.[-1] - diff(yy.)/2  # midpoints
> 
> # For each 2 class partition into small & large, get residual var
> res <- sapply(yy., function(x) 
>                      var(resid(lm(diffyy ~ factor(diffyy < x)))) )
> 
> cutoff <- yy.[which.min(res)]
> 
> # indices of points involved in rise
> which(diffyy > cutoff) + 1
> 
> ---
> 
> Petr Pikal <petr.pikal at precheza.cz>
> 
> 
> On 15 Jun 2004 at 13:52, Vito Muggeo wrote: 
> 
> > Dear Petr, 
> > Probably I don't understand exactly what you are looking for. 
> > 
> > However your "plot(x,c(y,z))" suggests a broken-line model for the
> > response "c(y,x)" versus the variables x. Therefore you could
> > estimate a segmented model to obtain (different) slope (and
> > breakpoint) estimates. See the package segmented. 
> 
> Thank you Vito, but it is not what I want. plot(x,c(y,z)) shows only
> one "spike" and I have many such spikes in actual data. 
> 
> My actual data look like those 
> 
> set.seed(1) 
> y <- 0.03*x[1:100]+rnorm(100, mean=.001, sd=.03) 
> z <- 3-rep(seq(1,100,10),each=10)*.03+rnorm(100,mean=.001, sd=.03) yy
> <- NULL for( i in 1:10) yy <- c(yy,c(y,z)[1:floor(runif(1)*200)]) y.l
> <- length(yy) plot(1:y.l, yy) 
> 
> x axis is actually a time and y is a weight of gradually filled
> conteiner, which is irregularly emptied. I want to do an hourly and/or
> daily averages of increases in weight (it can by done by aggregate) 
> 
> myfac <- gl(y.l/12,12,length=1271) #hopefully length is ok 
> 
> y.agg <- aggregate(diff(yy), list(myfac), mean) 
> ## there will be list(hod=cut(time.axis,"hour")) construction actually
> ## 
> 
> 0.03 can be expected average result and some aggregated values ar OK
> but some are wrong as they include values from emptying time. 
> 
> *** This*** is probably what I need, I need to set some logical vector
> which will be TRUE when there was a filling time and FALSE during
> other times. And I need to specify it according a data I have
> available. 
> 
> Best what I was able to do was to consider filling time as a time when
> let say 
> 
> diff(yy) >= 0 
> 
> was between prespecified limits, but you know how it is with real life
> and prespecified limits. 
> 
> Or I can plot my data against time, manually find out regions which
> are correct and make a aggregation only with correct data. But there
> are 24*60*3 values each day so I prefer not to do it manually. 
> 
> Or finally I can throw away any hourly average which is not in set
> limits, but I prefer to throw away as little data as possible. 
> 
> I hope I was able to clarify the issue a bit. 
> 
> Thank you 
> Best regards 
> Petr 
> 
> 
> > 
> > best, 
> > vito 
> > 
> > 
> > 
> > ----- Original Message ----- 
> > From: Petr Pikal <petr.pikal at precheza.cz> 
> > To: <r-help at stat.math.ethz.ch> 
> > Sent: Tuesday, June 15, 2004 1:11 PM 
> > Subject: [R] slope estimations of teeth like data 
> > 
> > 
> > > Dear all 
> > > 
> > > Suppose I have teeth like data similar like 
> > > 
> > > x <- 1:200 
> > > y <- 0.03*x[1:100]+rnorm(100, mean=.001, sd=.03) 
> > > z <- 3-rep(seq(1,100,10),each=10)*.03+rnorm(100,mean=.001, sd=.03)
> > > plot(x,c(y,z)) 
> > > 
> > > and I want to have a gradient estimations for some values from
> > > increasing 
> > part of 
> > > data 
> > > 
> > > like 
> > > 
> > > y.agg <- aggregate(diff(c(y,z)), 
> > > list(rep(seq(1,200,10),each=10)[1:199]), 
> > mean) 
> > > 
> > > y.agg[1:10,] ##is OK, I want that 
> > > y.agg[11:20,] ##is not OK, I do not want that 
> > > 
> > > actual data are similar but more irregular and have subsequent
> > > gradual 
> > increases 
> > > and decreases, more like 
> > > 
> > > set.seed(1) 
> > > yy<-NULL 
> > > for( i in 1:10) yy <- c(yy,c(y,z)[1:floor(runif(1)*200)]) 
> > > length(yy) 
> > > [1] 1098 
> > > 
> > > plot(1:1098,yy) 
> > > 
> > > Is there anybody who has some experience with such data, mainly
> > > how to 
> > extract 
> > > only increasing portions or to filter values of "yy" such as only 
> > aggregated slopes 
> > > from increasing parts are computed and other parts are set to NA. 
> > Sometimes 
> > > actual data have so long parts of steady or even slightly
> > > increasing 
> > values at 
> > > decreasing part that aggregated values are slightly positive
> > > although they 
> > are 
> > > actually from decreasing portion of data. 
> > > 
> > > Thank you 
> > > Petr Pikal 
> > > petr.pikal at precheza.cz 
> > > 
> 
> 
> _______________________________________________
> No banners. No pop-ups. No kidding.


Petr Pikal
petr.pikal at precheza.cz



From imosqueira at suk.azti.es  Wed Jun 16 15:00:44 2004
From: imosqueira at suk.azti.es (Iago Mosqueira)
Date: Wed, 16 Jun 2004 14:00:44 +0100
Subject: [R] Loading 'akward' data file
Message-ID: <1087390843.14217.66.camel@xurelo>

Hello,

I need to load a somehow diffilcult data file. It has lines with
variable names followed by a variable number of rows and columns of
data, separated from the next variable sometimes by a blank line,
sometimes simply by the new variable name. For example:

var1
123.33
var2
938

var3
1,1,1,1,1,1
var4
1,2,3,4,5
1,2,3,4,5

What would be the best startegy for loading a file like this? I would
like to have it staored as a list with the variable names used to name
the slots. Any pointers?

Many thanks,


Iago


 
--
Iago Mosqueira

Unidad de Investigacin Marina
Marine Research Division

AZTI Fundazioa

Txatxarramendi Ugartea z/g
48395 Sukarrieta, Bizkaia
Spain

Tfno. +(34) 94 602 94 00
Fax   +(34) 94 602 94 01

email imosqueira at suk.azti.es



From dmb at mrc-dunn.cam.ac.uk  Wed Jun 16 14:21:47 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Wed, 16 Jun 2004 13:21:47 +0100 (BST)
Subject: [R] non-linear binning? power-law in R
In-Reply-To: <BCF59A06.94CC%sdavis2@mail.nih.gov>
Message-ID: <Pine.LNX.4.21.0406161320400.23006-100000@mail.mrc-dunn.cam.ac.uk>

On Wed, 16 Jun 2004, Sean Davis wrote:

>Is ?cut what you need?

This is giving the cleanest results yet.

Cheers,
Dan.


>
>Sean
>
>
>On 6/16/04 6:52 AM, "Dan Bolser" <dmb at mrc-dunn.cam.ac.uk> wrote:
>
>> 
>> First, thanks to everyone who helped me get to grips with R in (x)emacs
>> (I get confused easily). Special thanks to Stephen Eglen for continued
>> support.
>> 
>> My question is about non-linear binning, or density functions over
>> distributions governed by a power law ...
>> 
>> y ~ mu*x**lambda    # In one of its forms
>>                       # (can't find Pareto in the online help)
>> 
>> Looking at the following should show my problem....
>> 
>> x3 <- runif(10000)**3    # Probably a better (correct) way to do this
>> 
>> plot( density(x3,cut=0,bw=0.1))
>> plot( density(x3,cut=0,bw=0.01))
>> plot( density(x3,cut=0,bw=0.001))
>> 
>> plot(density(x3,cut=0,bw=0.1),  log='xy')
>> plot(density(x3,cut=0,bw=0.01), log='xy')
>> plot(density(x3,cut=0,bw=0.001),log='xy')
>> 
>> The upper three plots show that the bw has a big effect on the appearance
>> of the graph by rescaling based on the initial density at low values of x,
>> which is very high.
>> 
>> The lower plots show (I think) an error in the use of linear bins to view
>> a non linear trend. I would expect this curve to be linear on log-log
>> scales (from experience), and you can see the expected behavior in the
>> tails of these plots.
>> 
>> If you play with drawing these curves on top of each other they look OK
>> apart from at the beginning. However, changing the band width to 0.0001 has
>> a radical effect on these plots, and they begin to show a different trend
>> (look like they are being governed by a different power).
>> 
>> Hmmm....
>> 
>> x3log <- -log(x3)
>> 
>> plot( density(x3log,cut=0,bw=0.5),  log='y',col=1)
>> 
>> lines(density(x3log,cut=0,bw=0.2),  log='y',col=2)
>> lines(density(x3log,cut=0,bw=0.1),  log='y',col=3)
>> lines(density(x3log,cut=0,bw=0.01), log='y',col=4)
>> 
>> Sorry...
>> 
>> 
>> 'Real' data of this form is usually discrete, with the value of 1 being
>> the most frequent (minimum) event, and higher values occurring less
>> frequently according to a power (power-law). This data can be easily
>> grouped into discrete bins, and frequency plotted on log scales. The
>> continuous data generated above requires some form of density estimation
>> or rescaling into discreet values (make the smallest value equal to 1 and
>> round everything else into an integer).
>> 
>> I see the aggregate function, but which function lets me simply count the
>> number of values in a class (integer bin)?
>> 
>> The analysis of even the discretized data is made more accurate by the use
>> of exponentially growing bins. This way you don't need to plot the data on
>> log scales, and the increasing variance associated with lower probability
>> events is handled by the increasing bin size (giving good accuracy of
>> power fitting). How can I easily (ignorantly) implement exponentially
>> increasing bin sizes?
>> 
>> Thanks for any feedback,
>> 
>> Dan.
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>> 
>



From andy_liaw at merck.com  Wed Jun 16 14:10:01 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 16 Jun 2004 08:10:01 -0400
Subject: [R] 64-bit R on OS X/G5?
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7EDC@usrymx25.merck.com>

> From: Prof Brian Ripley
> 
> R works on 64-bit platforms and has for quite a while, 
> currently I know of 
> people using machines with 16Gb RAM on 64-bit platforms 
> (Alpha, Sparc, 
> AMD64).

I guess you could add AIX to that list, although not without pain (no
readline because the readline installed on that system is 32-bit).  If I'm
not mistaken, 64-bit R on Irix has also been done.

Best,
Andy
 
> Do you have a full 64-bit compiler and runtime for MacOS X?  
> If so `all'
> you need to do is to run compile under it.  One quote I picked up was
> 
> 	macos X has 64 bit support with gcc 3.3 and a g5
> 
> which if correct suggests this is possible.
> 
> On Wed, 16 Jun 2004, Anders Sj??gren wrote:
> 
> > Hi!
> > 
> > Does anyone know if there is a working 64-bit version of R 
> for MacOs X 
> > on G5? If so, is there a memory limit anyway or could I 
> theroetically 
> > use almost 8 GB in one session?
> 
> There is of course a memory limit but it far exceeds 8GB.  There is a 
> limit on the length of vectors still (2^31 - 1) and a few 
> other similar 
> limits.
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From andy_liaw at merck.com  Wed Jun 16 14:31:27 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 16 Jun 2004 08:31:27 -0400
Subject: [R] Loading 'akward' data file
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7EDF@usrymx25.merck.com>

Generally you'd use file() to open the file, then use readLines(), say
inside a while() loop to read one `chunk' at a time.  However, your example
looks a bit strange.  The possibility of empty line makes it a bit more
complicated, by that last couple of lines seems to suggest that you could
have a line of data follow by another line of data without variable label.
If that's true, I don't know how you would parse the file...

Andy

> From: Iago Mosqueira
> 
> Hello,
> 
> I need to load a somehow diffilcult data file. It has lines with
> variable names followed by a variable number of rows and columns of
> data, separated from the next variable sometimes by a blank line,
> sometimes simply by the new variable name. For example:
> 
> var1
> 123.33
> var2
> 938
> 
> var3
> 1,1,1,1,1,1
> var4
> 1,2,3,4,5
> 1,2,3,4,5
> 
> What would be the best startegy for loading a file like this? I would
> like to have it staored as a list with the variable names used to name
> the slots. Any pointers?
> 
> Many thanks,
> 
> 
> Iago
> 
> 
>  
> --
> Iago Mosqueira
> 
> Unidad de Investigacin Marina
> Marine Research Division
> 
> AZTI Fundazioa
> 
> Txatxarramendi Ugartea z/g
> 48395 Sukarrieta, Bizkaia
> Spain
> 
> Tfno. +(34) 94 602 94 00
> Fax   +(34) 94 602 94 01
> 
> email imosqueira at suk.azti.es
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Luisr at frs.fo  Wed Jun 16 14:33:25 2004
From: Luisr at frs.fo (Luis Rideau Cruz)
Date: Wed, 16 Jun 2004 13:33:25 +0100
Subject: [R] Fwd: problem in long select from RODBC
Message-ID: <s0d04c32.018@ffdata.setur.fo>



Luis Ridao Cruz
Fiskiranns?knarstovan
N?at?n 1
P.O. Box 3051
FR-110 T?rshavn
Faroe Islands
Phone:             +298 353900
Phone(direct): +298 353912
Mobile:             +298 580800
Fax:                 +298 353901
E-mail:              luisr at frs.fo
Web:                www.frs.fo
-------------- next part --------------
An embedded message was scrubbed...
From: "Luis Rideau Cruz" <Luisr at frs.fo>
Subject: problem in long select from RODBC
Date: Wed, 16 Jun 2004 13:30:41 +0100
Size: 940
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040616/2d73e3a3/attachment.mht

From wolski at molgen.mpg.de  Wed Jun 16 14:42:42 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Wed, 16 Jun 2004 14:42:42 +0200
Subject: [R] Re: problem in long select from RODBC
In-Reply-To: MCSID(0).771856670
References: MCSID(0).771856670
Message-ID: <200406161442420572.2DB27646@mail.math.fu-berlin.de>

Hallo!

You can protect the line break with a backslashes


sqlQuery(channel, "select dir, c \
 from firsthit3 \
 where dir LIKE \"%SCHULEN%\"")

Sincerely

Eryk

*********** REPLY SEPARATOR  ***********

On 16.06.2004 at 14:37 Luis Rideau Cruz wrote:

>Hi all
>
>I have a very long sql select.
>The problem is that (as selects have to be in one line) is so long that
>when I copy-paste from the text file there is a "sintax error" .I have
>checked the select and it is working in SQL Plus but I believe the problem
>lies in the length of it.
>
>Can you help me?
>
>Thank you
>
>Luis Ridao Cruz
>Fiskiranns??knarstovan
>N??at??n 1
>P.O. Box 3051
>FR-110 T??rshavn
>Faroe Islands
>Phone:             +298 353900
>Phone(direct): +298 353912
>Mobile:             +298 580800
>Fax:                 +298 353901
>E-mail:              luisr at frs.fo
>Web:                www.frs.fo



From rpeng at jhsph.edu  Wed Jun 16 14:51:03 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Wed, 16 Jun 2004 08:51:03 -0400
Subject: [R] non-linear binning? power-law in R
In-Reply-To: <Pine.LNX.4.21.0406161059540.20180-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0406161059540.20180-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <40D04237.4080902@jhsph.edu>

You can use hist(x, br, plot = FALSE)$counts.

-roger

Dan Bolser wrote:
> First, thanks to everyone who helped me get to grips with R in (x)emacs
> (I get confused easily). Special thanks to Stephen Eglen for continued
> support.
> 
> My question is about non-linear binning, or density functions over
> distributions governed by a power law ...
> 
> y ~ mu*x**lambda	# In one of its forms 
>                         # (can't find Pareto in the online help)
> 
> Looking at the following should show my problem....
> 
> x3 <- runif(10000)**3	# Probably a better (correct) way to do this
> 
> plot( density(x3,cut=0,bw=0.1))
> plot( density(x3,cut=0,bw=0.01))
> plot( density(x3,cut=0,bw=0.001))
> 
> plot(density(x3,cut=0,bw=0.1),  log='xy')
> plot(density(x3,cut=0,bw=0.01), log='xy')
> plot(density(x3,cut=0,bw=0.001),log='xy')
> 
> The upper three plots show that the bw has a big effect on the appearance
> of the graph by rescaling based on the initial density at low values of x,
> which is very high.
> 
> The lower plots show (I think) an error in the use of linear bins to view
> a non linear trend. I would expect this curve to be linear on log-log
> scales (from experience), and you can see the expected behavior in the
> tails of these plots.
> 
> If you play with drawing these curves on top of each other they look OK
> apart from at the beginning. However, changing the band width to 0.0001 has
> a radical effect on these plots, and they begin to show a different trend
> (look like they are being governed by a different power).
> 
> Hmmm....
> 
> x3log <- -log(x3)
> 
> plot( density(x3log,cut=0,bw=0.5),  log='y',col=1)
> 
> lines(density(x3log,cut=0,bw=0.2),  log='y',col=2)
> lines(density(x3log,cut=0,bw=0.1),  log='y',col=3)
> lines(density(x3log,cut=0,bw=0.01), log='y',col=4)
> 
> Sorry...
> 
> 
> 'Real' data of this form is usually discrete, with the value of 1 being
> the most frequent (minimum) event, and higher values occurring less
> frequently according to a power (power-law). This data can be easily
> grouped into discrete bins, and frequency plotted on log scales. The
> continuous data generated above requires some form of density estimation
> or rescaling into discreet values (make the smallest value equal to 1 and
> round everything else into an integer).
> 
> I see the aggregate function, but which function lets me simply count the
> number of values in a class (integer bin)?
> 
> The analysis of even the discretized data is made more accurate by the use
> of exponentially growing bins. This way you don't need to plot the data on
> log scales, and the increasing variance associated with lower probability
> events is handled by the increasing bin size (giving good accuracy of
> power fitting). How can I easily (ignorantly) implement exponentially
> increasing bin sizes?
> 
> Thanks for any feedback,
> 
> Dan.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng



From ripley at stats.ox.ac.uk  Wed Jun 16 15:15:16 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 16 Jun 2004 14:15:16 +0100 (BST)
Subject: [R] Fwd: problem in long select from RODBC
In-Reply-To: <s0d04c32.018@ffdata.setur.fo>
Message-ID: <Pine.LNX.4.44.0406161412490.14341-100000@gannet.stats>

We seem to have received your message several times but no details of the 
commands you used.  Please read the posting guide and try again (it is 
possible attachments have been removed).

It is quite possible both ODBC and RODBC have line length limits, but you 
have the source code for the latter and the docs for the former so you cna 
investigate this as easily as I can.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jfox at mcmaster.ca  Wed Jun 16 15:17:50 2004
From: jfox at mcmaster.ca (John Fox)
Date: Wed, 16 Jun 2004 09:17:50 -0400
Subject: [R] Loading 'akward' data file
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7EDF@usrymx25.merck.com>
Message-ID: <20040616131751.XTDE26030.tomts20-srv.bellnexxia.net@JohnDesktop8300>

Dear Andy and Iago,

Here's a quick and dirty solution which, I think, produces the desired
result. The solution depends upon the structure of the data file being the
same as Iago's example, but could be made more flexible:

> readFile <- function(fileName){
+     con <- file(fileName, open="r")
+     lines <- readLines(con)
+     result <- list()
+     line <- 1
+     first <- TRUE
+     while (line <= length(lines)) {
+         if (length(grep("^var", lines[line])) > 0) {
+             if (!first) result[[name]] <- values
+             first <- FALSE
+             name <- lines[line]
+             values <- NULL
+             }
+         else {
+             values <- c(values,
eval(parse(text=paste("c(",lines[line],")"))))
+             }
+         line <- line + 1
+         }
+     result[[name]] <- values
+     close(con)
+     result
+     }
>     
> readFile("c:/temp/test.txt")
$var1
[1] 123.33

$var2
[1] 938

$var3
[1] 1 1 1 1 1 1

$var4
 [1] 1 2 3 4 5 1 2 3 4 5

I hope that this helps,
 John 

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Liaw, Andy
> Sent: Wednesday, June 16, 2004 7:31 AM
> To: 'Iago Mosqueira'; r-help at stat.math.ethz.ch
> Subject: RE: [R] Loading 'akward' data file
> 
> Generally you'd use file() to open the file, then use 
> readLines(), say inside a while() loop to read one `chunk' at 
> a time.  However, your example looks a bit strange.  The 
> possibility of empty line makes it a bit more complicated, by 
> that last couple of lines seems to suggest that you could 
> have a line of data follow by another line of data without 
> variable label.
> If that's true, I don't know how you would parse the file...
> 
> Andy
> 
> > From: Iago Mosqueira
> > 
> > Hello,
> > 
> > I need to load a somehow diffilcult data file. It has lines with 
> > variable names followed by a variable number of rows and columns of 
> > data, separated from the next variable sometimes by a blank line, 
> > sometimes simply by the new variable name. For example:
> > 
> > var1
> > 123.33
> > var2
> > 938
> > 
> > var3
> > 1,1,1,1,1,1
> > var4
> > 1,2,3,4,5
> > 1,2,3,4,5
> > 
> > What would be the best startegy for loading a file like 
> this? I would 
> > like to have it staored as a list with the variable names 
> used to name 
> > the slots. Any pointers?
> > 
> > Many thanks,
> > 
> > 
> > Iago



From to166 at columbia.edu  Wed Jun 16 16:18:29 2004
From: to166 at columbia.edu (Todd Ogden)
Date: Wed, 16 Jun 2004 10:18:29 -0400
Subject: [R] subset and lme
Message-ID: <1087395509.40d056b567b37@cubmail.cc.columbia.edu>

I'm puzzled by the following problem, which appears when 
attempting to run an analysis on part of a dataset:

If I try:

  csubset <- dat$Diagnosis==0
  cont <- lme(fixed=cform,
                   random = ~1|StudyName,
                      data=dat,subset=csubset,na.action=na.omit)

Then I get:

Error in eval(expr, envir, enclos) : Object "csubset" not found

But if I do instead:

  cdat <- dat[dat$Diagnosis==0,]
  cont <- lme(fixed=cform,
                   random = ~1|StudyName,
                      data=cdat,na.action=na.omit)

Then everything is fine.

I'm puzzled that the object can't be found.  Maybe I'm 
overlooking something obvious?

Best,

Todd Ogden



From Ted.Harding at nessie.mcc.ac.uk  Wed Jun 16 16:02:33 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Wed, 16 Jun 2004 15:02:33 +0100 (BST)
Subject: [R] Suggested mod to interaction()
Message-ID: <XFMail.040616150233.Ted.Harding@nessie.mcc.ac.uk>

May I suggest a slight modification to interaction()?

It's purely cosmetic, but I found it convenient.
It would have no effect on standard use of the function.

In the code below, the change consists of adding an
argument 'sep' (default = ".", which is the standard
behaviour), and changing the line

        else as.vector(outer(l, lvs, paste, sep = "."))
to
        else as.vector(outer(l, lvs, paste, sep = sep))

-----------------------------------------------------
interaction<-function (..., drop = FALSE, sep=".")
{
    args <- list(...)
    narg <- length(args)
    if (narg == 1 && is.list(args[[1]])) {
        args <- args[[1]]
        narg <- length(args)
    }
    ans <- 0
    lvs <- NULL
    for (i in narg:1) {
        f <- args[[i]]
        if (!is.factor(f)) 
            f <- factor(f)
        l <- levels(f)
        ans <- ans * length(l) + as.integer(f) - 1
        lvs <- if (i == narg) 
            l
        else as.vector(outer(l, lvs, paste, sep = sep))
    }
    ans <- ans + 1
    if (drop) {
        f <- unique(ans[!is.na(ans)])
        ans <- match(ans, f)
        lvs <- lvs[f]
    }
    ans <- as.integer(ans)
    levels(ans) <- lvs
    class(ans) <- "factor"
    ans
}
-----------------------------------------------------

Best wishes to all,
Ted.



--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 16-Jun-04                                       Time: 15:02:33
------------------------------ XFMail ------------------------------



From ym at climpact.com  Wed Jun 16 16:38:36 2004
From: ym at climpact.com (Yves Magliulo)
Date: 16 Jun 2004 16:38:36 +0200
Subject: [R] gam
Message-ID: <1087396716.4291.13.camel@new-york.climpact.net>

hi,

i'm working with mgcv packages and specially gam. My exemple is:

>test<-gam(B~s(pred1)+s(pred2))
>plot(test,pages=1)

when ploting test, you can view pred1 vs s(pred1, edf[1] ) & pred2 vs
s(pred2, edf[2] )

I would like to know if there is a way to access to those terms
(s(pred1) & s(pred2)). Does someone know how?

the purpose is to access to equation of smooths terms in order to have
the equation of my additive model.

best regards,

-- 
Yves Magliulo, Climatology research departement <ym at climpact.com>
Climpact



From lists at svenhartenstein.de  Wed Jun 16 16:47:58 2004
From: lists at svenhartenstein.de (Sven Hartenstein)
Date: Wed, 16 Jun 2004 16:47:58 +0200
Subject: [R] extracting p-value of aov F-statistic
In-Reply-To: <40CE039D.4070706@optonline.net> (Chuck Cleland's message of
	"Mon, 14 Jun 2004 15:59:25 -0400")
References: <87fz8y12uj.fsf@svenhartenstein.de>
	<6.0.3.0.2.20040614104541.020a13b0@pop4.attglobal.net>
	<87acz62cb4.fsf@svenhartenstein.de> <40CE039D.4070706@optonline.net>
Message-ID: <87hdtbh84h.fsf@svenhartenstein.de>

Hi, 

Chuck Cleland <ccleland at optonline.net> writes:
> as.matrix(LZ.aov[[1]][,5])

That works perfectly. Thank you very much!

Sven



From P.Lemmens at nici.kun.nl  Wed Jun 16 16:48:03 2004
From: P.Lemmens at nici.kun.nl (Paul Lemmens)
Date: Wed, 16 Jun 2004 16:48:03 +0200
Subject: [R] subset and lme
In-Reply-To: <1087395509.40d056b567b37@cubmail.cc.columbia.edu>
References: <1087395509.40d056b567b37@cubmail.cc.columbia.edu>
Message-ID: <A2A7A4B14FBA58CCB7D5ED7F@lemmens.socsci.kun.nl>

Hoi Todd,

--On woensdag 16 juni 2004 10:18 -0400 Todd Ogden <to166 at columbia.edu> 
wrote:

> I'm puzzled by the following problem, which appears when
> attempting to run an analysis on part of a dataset:
>
> If I try:
>
>   csubset <- dat$Diagnosis==0
>
This just creates a vector of booleans that indicate (the row numbers) for 
which positions in dat Diagnosis==0.

>
>   cdat <- dat[dat$Diagnosis==0,]
>
This OTOH, uses the above vector to index the rows of dat, indeed selecting 
those rows from dat that have Diagnosis==0. This is assigned to cdat. You 
could have done

cdat <- dat[csubset,]

as well


HTH



-- 
Paul Lemmens
NICI, University of Nijmegen              ASCII Ribbon Campaign /"\
Montessorilaan 3 (B.01.05)                    Against HTML Mail \ /
NL-6525 HR Nijmegen                                              X
The Netherlands                                                 / \
Phonenumber    +31-24-3612648
Fax            +31-24-3616066



From dmb at mrc-dunn.cam.ac.uk  Wed Jun 16 17:07:05 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Wed, 16 Jun 2004 16:07:05 +0100 (BST)
Subject: [R] non-linear binning? power-law in R
In-Reply-To: <40D04237.4080902@jhsph.edu>
Message-ID: <Pine.LNX.4.21.0406161557570.23006-100000@mail.mrc-dunn.cam.ac.uk>

On Wed, 16 Jun 2004, Roger D. Peng wrote:

>You can use hist(x, br, plot = FALSE)$counts.
>
>-roger

OK, I made this...


y4 <- runif(100000)**4
y4log <- -log(y4)
y4bin20 <- hist(y4log,20,plot=FALSE)$counts
y4bin20log <- log(y4bin20)

plot(y4bin20log)

for(i in 2:20){
 y4 <- runif(100000)**4
 y4log <- -log(y4)
 y4bin20 <- hist(y4log,20,plot=FALSE)$counts
 y4bin20log <- log(y4bin20)
 points(y4bin20log,col=i)
}


I am seeing something very strange about 1 loop in every five or ten of
the above...

Can someone try this and let me know if every fifth run or so they see a
totally different trend? Basically looks like only 10 bins are being used
at random during the run!?

Hopefully you will also see what I am trying to do, which is to highlight
the increased variance in the tail...

How would I estimate the slope of y4bin20log? (sorry for the names).

Thanks for the help,
Dan.

>
>Dan Bolser wrote:
>> First, thanks to everyone who helped me get to grips with R in (x)emacs
>> (I get confused easily). Special thanks to Stephen Eglen for continued
>> support.
>> 
>> My question is about non-linear binning, or density functions over
>> distributions governed by a power law ...
>> 
>> y ~ mu*x**lambda	# In one of its forms 
>>                         # (can't find Pareto in the online help)
>> 
>> Looking at the following should show my problem....
>> 
>> x3 <- runif(10000)**3	# Probably a better (correct) way to do this
>> 
>> plot( density(x3,cut=0,bw=0.1))
>> plot( density(x3,cut=0,bw=0.01))
>> plot( density(x3,cut=0,bw=0.001))
>> 
>> plot(density(x3,cut=0,bw=0.1),  log='xy')
>> plot(density(x3,cut=0,bw=0.01), log='xy')
>> plot(density(x3,cut=0,bw=0.001),log='xy')
>> 
>> The upper three plots show that the bw has a big effect on the appearance
>> of the graph by rescaling based on the initial density at low values of x,
>> which is very high.
>> 
>> The lower plots show (I think) an error in the use of linear bins to view
>> a non linear trend. I would expect this curve to be linear on log-log
>> scales (from experience), and you can see the expected behavior in the
>> tails of these plots.
>> 
>> If you play with drawing these curves on top of each other they look OK
>> apart from at the beginning. However, changing the band width to 0.0001 has
>> a radical effect on these plots, and they begin to show a different trend
>> (look like they are being governed by a different power).
>> 
>> Hmmm....
>> 
>> x3log <- -log(x3)
>> 
>> plot( density(x3log,cut=0,bw=0.5),  log='y',col=1)
>> 
>> lines(density(x3log,cut=0,bw=0.2),  log='y',col=2)
>> lines(density(x3log,cut=0,bw=0.1),  log='y',col=3)
>> lines(density(x3log,cut=0,bw=0.01), log='y',col=4)
>> 
>> Sorry...
>> 
>> 
>> 'Real' data of this form is usually discrete, with the value of 1 being
>> the most frequent (minimum) event, and higher values occurring less
>> frequently according to a power (power-law). This data can be easily
>> grouped into discrete bins, and frequency plotted on log scales. The
>> continuous data generated above requires some form of density estimation
>> or rescaling into discreet values (make the smallest value equal to 1 and
>> round everything else into an integer).
>> 
>> I see the aggregate function, but which function lets me simply count the
>> number of values in a class (integer bin)?
>> 
>> The analysis of even the discretized data is made more accurate by the use
>> of exponentially growing bins. This way you don't need to plot the data on
>> log scales, and the increasing variance associated with lower probability
>> events is handled by the increasing bin size (giving good accuracy of
>> power fitting). How can I easily (ignorantly) implement exponentially
>> increasing bin sizes?
>> 
>> Thanks for any feedback,
>> 
>> Dan.
>> 
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>> 
>
>



From P.Lemmens at nici.kun.nl  Wed Jun 16 16:57:02 2004
From: P.Lemmens at nici.kun.nl (Paul Lemmens)
Date: Wed, 16 Jun 2004 16:57:02 +0200
Subject: [R] subset(..., drop=TRUE) doesn't seem to work.
Message-ID: <DCF7E268195AB308453EFAA8@lemmens.socsci.kun.nl>

Hello!

If I read ?subset, the workings of the argument drop (to me) seem to imply 
equivalence of A and B (R 1.9.0):

#A
dd <- data.frame(rt=rnorm(10), c=factor(gl(2,5)))
dd <- subset(dd, c==1)
dd$c <- dd$c[, drop=TRUE]
table(dd$c)

1
5
	

#B
dd <- data.frame(rt=rnorm(10), c=factor(gl(2,5)))
dd <- subset(dd, c==1, drop=TRUE)
table(dd$c)

1 2
5 0

So to lose the second level of dd$c, in method B I still need to 'dd$c <- 
dd$c[, drop=TRUE]', while the manual seems to imply that with the drop 
argument to subset() this would not be necessary.


Could you comment?

kind regards,
Paul


-- 
Paul Lemmens
NICI, University of Nijmegen              ASCII Ribbon Campaign /"\
Montessorilaan 3 (B.01.05)                    Against HTML Mail \ /
NL-6525 HR Nijmegen                                              X
The Netherlands                                                 / \
Phonenumber    +31-24-3612648
Fax            +31-24-3616066



From rpeng at jhsph.edu  Wed Jun 16 16:58:21 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Wed, 16 Jun 2004 10:58:21 -0400
Subject: [R] non-linear binning? power-law in R
In-Reply-To: <Pine.LNX.4.21.0406161557570.23006-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0406161557570.23006-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <40D0600D.7010407@jhsph.edu>

When you specify a single number to the `br' argument of `hist' it is 
only a suggestion.  See the help page, ?hist.  If you want to use the 
same breakpoints everytime, you need to specify the locations explicity.

-roger

Dan Bolser wrote:
> On Wed, 16 Jun 2004, Roger D. Peng wrote:
> 
> 
>>You can use hist(x, br, plot = FALSE)$counts.
>>
>>-roger
> 
> 
> OK, I made this...
> 
> 
> y4 <- runif(100000)**4
> y4log <- -log(y4)
> y4bin20 <- hist(y4log,20,plot=FALSE)$counts
> y4bin20log <- log(y4bin20)
> 
> plot(y4bin20log)
> 
> for(i in 2:20){
>  y4 <- runif(100000)**4
>  y4log <- -log(y4)
>  y4bin20 <- hist(y4log,20,plot=FALSE)$counts
>  y4bin20log <- log(y4bin20)
>  points(y4bin20log,col=i)
> }
> 
> 
> I am seeing something very strange about 1 loop in every five or ten of
> the above...
> 
> Can someone try this and let me know if every fifth run or so they see a
> totally different trend? Basically looks like only 10 bins are being used
> at random during the run!?
> 
> Hopefully you will also see what I am trying to do, which is to highlight
> the increased variance in the tail...
> 
> How would I estimate the slope of y4bin20log? (sorry for the names).
> 
> Thanks for the help,
> Dan.
> 
> 
>>Dan Bolser wrote:
>>
>>>First, thanks to everyone who helped me get to grips with R in (x)emacs
>>>(I get confused easily). Special thanks to Stephen Eglen for continued
>>>support.
>>>
>>>My question is about non-linear binning, or density functions over
>>>distributions governed by a power law ...
>>>
>>>y ~ mu*x**lambda	# In one of its forms 
>>>                        # (can't find Pareto in the online help)
>>>
>>>Looking at the following should show my problem....
>>>
>>>x3 <- runif(10000)**3	# Probably a better (correct) way to do this
>>>
>>>plot( density(x3,cut=0,bw=0.1))
>>>plot( density(x3,cut=0,bw=0.01))
>>>plot( density(x3,cut=0,bw=0.001))
>>>
>>>plot(density(x3,cut=0,bw=0.1),  log='xy')
>>>plot(density(x3,cut=0,bw=0.01), log='xy')
>>>plot(density(x3,cut=0,bw=0.001),log='xy')
>>>
>>>The upper three plots show that the bw has a big effect on the appearance
>>>of the graph by rescaling based on the initial density at low values of x,
>>>which is very high.
>>>
>>>The lower plots show (I think) an error in the use of linear bins to view
>>>a non linear trend. I would expect this curve to be linear on log-log
>>>scales (from experience), and you can see the expected behavior in the
>>>tails of these plots.
>>>
>>>If you play with drawing these curves on top of each other they look OK
>>>apart from at the beginning. However, changing the band width to 0.0001 has
>>>a radical effect on these plots, and they begin to show a different trend
>>>(look like they are being governed by a different power).
>>>
>>>Hmmm....
>>>
>>>x3log <- -log(x3)
>>>
>>>plot( density(x3log,cut=0,bw=0.5),  log='y',col=1)
>>>
>>>lines(density(x3log,cut=0,bw=0.2),  log='y',col=2)
>>>lines(density(x3log,cut=0,bw=0.1),  log='y',col=3)
>>>lines(density(x3log,cut=0,bw=0.01), log='y',col=4)
>>>
>>>Sorry...
>>>
>>>
>>>'Real' data of this form is usually discrete, with the value of 1 being
>>>the most frequent (minimum) event, and higher values occurring less
>>>frequently according to a power (power-law). This data can be easily
>>>grouped into discrete bins, and frequency plotted on log scales. The
>>>continuous data generated above requires some form of density estimation
>>>or rescaling into discreet values (make the smallest value equal to 1 and
>>>round everything else into an integer).
>>>
>>>I see the aggregate function, but which function lets me simply count the
>>>number of values in a class (integer bin)?
>>>
>>>The analysis of even the discretized data is made more accurate by the use
>>>of exponentially growing bins. This way you don't need to plot the data on
>>>log scales, and the increasing variance associated with lower probability
>>>events is handled by the increasing bin size (giving good accuracy of
>>>power fitting). How can I easily (ignorantly) implement exponentially
>>>increasing bin sizes?
>>>
>>>Thanks for any feedback,
>>>
>>>Dan.
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>>
>>
>>
> 
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng



From sundar.dorai-raj at PDF.COM  Wed Jun 16 17:02:01 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Wed, 16 Jun 2004 08:02:01 -0700
Subject: [R] subset and lme
In-Reply-To: <1087395509.40d056b567b37@cubmail.cc.columbia.edu>
References: <1087395509.40d056b567b37@cubmail.cc.columbia.edu>
Message-ID: <40D060E9.1070007@pdf.com>

Todd Ogden wrote:

> I'm puzzled by the following problem, which appears when 
> attempting to run an analysis on part of a dataset:
> 
> If I try:
> 
>   csubset <- dat$Diagnosis==0
>   cont <- lme(fixed=cform,
>                    random = ~1|StudyName,
>                       data=dat,subset=csubset,na.action=na.omit)
> 
> Then I get:
> 
> Error in eval(expr, envir, enclos) : Object "csubset" not found
> 
> But if I do instead:
> 
>   cdat <- dat[dat$Diagnosis==0,]
>   cont <- lme(fixed=cform,
>                    random = ~1|StudyName,
>                       data=cdat,na.action=na.omit)
> 
> Then everything is fine.
> 
> I'm puzzled that the object can't be found.  Maybe I'm 
> overlooking something obvious?
> 

Todd,
   What version of R/nlme? I just tried the following:

library(nlme)
data(Orthodont)
csubset <- Orthodont$Sex == "Male"
fm1 <- lme(distance ~ age, data = Orthodont,
            random = ~ 1, subset = csubset)
fm2 <- lme(distance ~ age, data = Orthodont,
            random = ~ 1, subset = Sex == "Male")
fm3 <- lme(distance ~ age, data = Orthodont[csubset, ],
            random = ~ 1)

 > R.version.string
[1] "R version 1.9.0, 2004-05-06"
R> library(help = "nlme")

		Information on Package 'nlme'

Description:

Package: nlme
Version: 3.1-48
Date: 2004/01/14
<snip>

--sundar



From agostino.manzato at osmer.fvg.it  Wed Jun 16 17:03:52 2004
From: agostino.manzato at osmer.fvg.it (Agostino.Manzato@osmer.fvg.it)
Date: Wed, 16 Jun 2004 17:03:52 +0200
Subject: [R] Is input pre-processing needed for nnet module?
Message-ID: <40D06158.EA1D758@osmer.fvg.it>

Hi,
I used the nnet R module to classify my data using Neural Networks:
nnet(input_matrix, obs_vect, size=h, linout=FALSE, entropy=TRUE)
I used as NN input my "raw" data. 
After that I tried to use the normalized input data (with z-scores,
i.e. mean=0 and std=1) and have found NNs with a little smaller 
Cross Entropy Error.
My question is: 
Is it *wrong* to feed nnet directly with the raw input data?

I found in
http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html
that it depends on the minimization training algorithm:
- "Steepest descent is very sensitive to scaling.
- Quasi-Newton and conjugate gradient methods... therefore are scale 
sensitive. However,... are less scale sensitive than pure gradient
descent.
- Newton-Raphson and Gauss-Newton, if implemented correctly, are
theoretically invariant under scale changes..."

I know that nnet is a Quasi-Newton algorithm, so it make sense 
that I found a small improvement using the normalized data.
Can someone confirme if it is really so?

Thank you very much!

-- 
Agostino.Manzato at osmer.fvg.it



From dmb at mrc-dunn.cam.ac.uk  Wed Jun 16 17:24:12 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Wed, 16 Jun 2004 16:24:12 +0100 (BST)
Subject: [R] non-linear binning? power-law in R
In-Reply-To: <40D0600D.7010407@jhsph.edu>
Message-ID: <Pine.LNX.4.21.0406161619090.23006-100000@mail.mrc-dunn.cam.ac.uk>

On Wed, 16 Jun 2004, Roger D. Peng wrote:

>When you specify a single number to the `br' argument of `hist' it is 
>only a suggestion.  See the help page, ?hist.  If you want to use the 
>same breakpoints everytime, you need to specify the locations explicity.

OK, I can see why this would be so, but none the less 2 and only 2 very
distinct trends are coming out of the simulation.

By hand I get...

abline(11.1,-0.5)

abline(12.1,-1.2)

On the plot (now below).  This works every time, nothing in between, just
these two trends. Am I being stupid / is this a local phenomenon only?


>
>-roger
>
>Dan Bolser wrote:
>> On Wed, 16 Jun 2004, Roger D. Peng wrote:
>> 
>> 
>>>You can use hist(x, br, plot = FALSE)$counts.
>>>
>>>-roger
>> 
>> 
>> OK, I made this...
>> 
>> 
>> y4 <- runif(100000)**4
>> y4log <- -log(y4)
>> y4bin20 <- hist(y4log,20,plot=FALSE)$counts
>> y4bin20log <- log(y4bin20)
>> 
>> plot(y4bin20log)
>> 
>> for(i in 2:20){
>>  y4 <- runif(100000)**4
>>  y4log <- -log(y4)
>>  y4bin20 <- hist(y4log,20,plot=FALSE)$counts
>>  y4bin20log <- log(y4bin20)
>>  points(y4bin20log,col=i)
>> }
>> 
>> 
>> I am seeing something very strange about 1 loop in every five or ten of
>> the above...
>> 
>> Can someone try this and let me know if every fifth run or so they see a
>> totally different trend? Basically looks like only 10 bins are being used
>> at random during the run!?
>> 
>> Hopefully you will also see what I am trying to do, which is to highlight
>> the increased variance in the tail...
>> 
>> How would I estimate the slope of y4bin20log? (sorry for the names).
>> 
>> Thanks for the help,
>> Dan.
>> 
>> 
>>>Dan Bolser wrote:
>>>
>>>>First, thanks to everyone who helped me get to grips with R in (x)emacs
>>>>(I get confused easily). Special thanks to Stephen Eglen for continued
>>>>support.
>>>>
>>>>My question is about non-linear binning, or density functions over
>>>>distributions governed by a power law ...
>>>>
>>>>y ~ mu*x**lambda	# In one of its forms 
>>>>                        # (can't find Pareto in the online help)
>>>>
>>>>Looking at the following should show my problem....
>>>>
>>>>x3 <- runif(10000)**3	# Probably a better (correct) way to do this
>>>>
>>>>plot( density(x3,cut=0,bw=0.1))
>>>>plot( density(x3,cut=0,bw=0.01))
>>>>plot( density(x3,cut=0,bw=0.001))
>>>>
>>>>plot(density(x3,cut=0,bw=0.1),  log='xy')
>>>>plot(density(x3,cut=0,bw=0.01), log='xy')
>>>>plot(density(x3,cut=0,bw=0.001),log='xy')
>>>>
>>>>The upper three plots show that the bw has a big effect on the appearance
>>>>of the graph by rescaling based on the initial density at low values of x,
>>>>which is very high.
>>>>
>>>>The lower plots show (I think) an error in the use of linear bins to view
>>>>a non linear trend. I would expect this curve to be linear on log-log
>>>>scales (from experience), and you can see the expected behavior in the
>>>>tails of these plots.
>>>>
>>>>If you play with drawing these curves on top of each other they look OK
>>>>apart from at the beginning. However, changing the band width to 0.0001 has
>>>>a radical effect on these plots, and they begin to show a different trend
>>>>(look like they are being governed by a different power).
>>>>
>>>>Hmmm....
>>>>
>>>>x3log <- -log(x3)
>>>>
>>>>plot( density(x3log,cut=0,bw=0.5),  log='y',col=1)
>>>>
>>>>lines(density(x3log,cut=0,bw=0.2),  log='y',col=2)
>>>>lines(density(x3log,cut=0,bw=0.1),  log='y',col=3)
>>>>lines(density(x3log,cut=0,bw=0.01), log='y',col=4)
>>>>
>>>>Sorry...
>>>>
>>>>
>>>>'Real' data of this form is usually discrete, with the value of 1 being
>>>>the most frequent (minimum) event, and higher values occurring less
>>>>frequently according to a power (power-law). This data can be easily
>>>>grouped into discrete bins, and frequency plotted on log scales. The
>>>>continuous data generated above requires some form of density estimation
>>>>or rescaling into discreet values (make the smallest value equal to 1 and
>>>>round everything else into an integer).
>>>>
>>>>I see the aggregate function, but which function lets me simply count the
>>>>number of values in a class (integer bin)?
>>>>
>>>>The analysis of even the discretized data is made more accurate by the use
>>>>of exponentially growing bins. This way you don't need to plot the data on
>>>>log scales, and the increasing variance associated with lower probability
>>>>events is handled by the increasing bin size (giving good accuracy of
>>>>power fitting). How can I easily (ignorantly) implement exponentially
>>>>increasing bin sizes?
>>>>
>>>>Thanks for any feedback,
>>>>
>>>>Dan.
>>>>
>>>>______________________________________________
>>>>R-help at stat.math.ethz.ch mailing list
>>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>>>
>>>
>>>
>> 
>> 
>
>



From p.dalgaard at biostat.ku.dk  Wed Jun 16 17:06:26 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 16 Jun 2004 17:06:26 +0200
Subject: [R] subset(..., drop=TRUE) doesn't seem to work.
In-Reply-To: <DCF7E268195AB308453EFAA8@lemmens.socsci.kun.nl>
References: <DCF7E268195AB308453EFAA8@lemmens.socsci.kun.nl>
Message-ID: <x2pt7zk0el.fsf@biostat.ku.dk>

Paul Lemmens <P.Lemmens at nici.kun.nl> writes:

> Hello!
> 
> If I read ?subset, the workings of the argument drop (to me) seem to
> imply equivalence of A and B (R 1.9.0):
> 
> #A
> dd <- data.frame(rt=rnorm(10), c=factor(gl(2,5)))
> dd <- subset(dd, c==1)
> dd$c <- dd$c[, drop=TRUE]
> table(dd$c)
> 
> 1
> 5
> 	
> 
> #B
> dd <- data.frame(rt=rnorm(10), c=factor(gl(2,5)))
> dd <- subset(dd, c==1, drop=TRUE)
> table(dd$c)
> 
> 1 2
> 5 0
> 
> So to lose the second level of dd$c, in method B I still need to 'dd$c
> <-
> dd$c[, drop=TRUE]', while the manual seems to imply that with the drop
> argument to subset() this would not be necessary.
> 
> 
> Could you comment?

Looks like a documentation bug. The actual code ends up  doing

    x[r, vars, drop = drop]

and "[.data.frame" will not drop factor levels. I wonder if it ever
did... 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From richard.kittler at amd.com  Wed Jun 16 17:13:34 2004
From: richard.kittler at amd.com (richard.kittler@amd.com)
Date: Wed, 16 Jun 2004 08:13:34 -0700
Subject: [R] Coercing a dataframe column to datetime
Message-ID: <858788618A93D111B45900805F85267A0BCB2D9E@caexmta3.amd.com>

I understand I can do it directly with only one column.  It was meant to be a simple case to illustrate my problem.  I need to do it over many columns in the real application.

--Rich

Richard Kittler 
AMD TDG
408-749-4099

-----Original Message-----
From: Petr Pikal [mailto:petr.pikal at precheza.cz] 
Sent: Tuesday, June 15, 2004 11:10 PM
To: Kittler, Richard
Cc: r-help at stat.math.ethz.ch
Subject: RE: [R] Coercing a dataframe column to datetime




On 15 Jun 2004 at 11:31, richard.kittler at amd.com wrote:

> Thank you! The next step in the conversion still fails and I can't 
> seem to find any examples in the archives.  The result of the function 
> 'as.POSIXct(strptime())' within the 'sapply' comes back as numeric 
> rather than POSIXct as expected:
> 
> > ds <- cbind(1:2, c("02/27/92 23:03:20", "02/27/92 22:29:56")); ds
>      [,1] [,2]               
> [1,] "1"  "02/27/92 23:03:20"
> [2,] "2"  "02/27/92 22:29:56"
> > q <- sapply(ds[,2], function(x) as.POSIXct(strptime(x,"%m/%d/%y
> > %H:%M:%S"))) class(q)
> [1] "numeric"
> > q
> 02/27/92 23:03:20 02/27/92 22:29:56 
>         699260600         699258596 

Hi

Why do you use sapply? ds is included in some list? If not you can 
apply as.POSIXct directly to ds.

as.POSIXct(strptime(ds[,2],"%m/%d/%y %H:%M:%S"))

If you use sapply (or apply) the result is a vector or array which 
has to have the same class for all its elements. Therefore you get 
numeric representation of our dates (I suppose :-).

Cheers
Petr

> 
> --Rich
> 
> Richard Kittler
> AMD TDG
> 408-749-4099
> 
> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Sent: Monday, June 14, 2004 12:05 PM
> To: Kittler, Richard
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] Coercing a dataframe column to datetime
> 
> 
> You have forgotten as.POSIXct is needed too.
> 
> On Mon, 14 Jun 2004 richard.kittler at amd.com wrote:
> 
> > I am trying to coerce a data frame column from character to datetime 
> > using strptime but keep getting an error because the length of the
> > coerced object is always 9.  What am I doing wrong here:   
> > 
> > .................................................................
> > > ds <- cbind(1:2, c("02/27/92 23:03:20", "02/27/92 22:29:56")); ds
> >      [,1] [,2]               
> > [1,] "1"  "02/27/92 23:03:20"
> > [2,] "2"  "02/27/92 22:29:56"
> > >  
> > > q <- strptime(ds[,2], "%m/%d/%y %H:%M:%S"); q
> > [1] "1992-02-27 23:03:20" "1992-02-27 22:29:56"
> > > 
> > > ds[,2] <- q
> > Error in "[<-"(`*tmp*`, , 2, value = q) : number of items to replace 
> > is not a multiple of replacement length
> > > 
> > > length(q)
> > [1] 9
> > 
> > .................................................................
> > 
> > --Rich
> > 
> > Richard Kittler
> > AMD TDG
> > 408-749-4099
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> > 
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self) 1 South
> Parks Road,                     +44 1865 272866 (PA) Oxford OX1 3TG,
> UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From A.Roux at hull.ac.uk  Wed Jun 16 17:23:14 2004
From: A.Roux at hull.ac.uk (Alet Roux)
Date: Wed, 16 Jun 2004 16:23:14 +0100
Subject: [R] Compiling C++ package source: linking problem?
Message-ID: <1087399394.40d065e21d663@webmail1.hull.ac.uk>

Dear All

I'm currently developing a package for R (1.9.0) on Win32, with C++ source code.
Having followed the instructions in readme.packages, my code compiles fine with
R CMD SHLIB (as well as R CMD check) ... until I start using the internal R
functions. 

(Interesting: Rprintf seems to be the exception.) For instance, the following
code compiles fine:

#include <R.h>
#include <Rdefines.h>
...
SEXP whatever (SEXP model)
{
	Rprintf ("Hello, here I am!\n");
        return model;
}

However, the following doesn't compile at all:

#include <R.h>
#include <Rdefines.h>
...
SEXP whatever (SEXP model)
{
	SEXP anotherModel;
	PROTECT(anotherModel = NEW_NUMERIC(4));
	UNPROTECT(1);
        return anotherModel;
}

An example of compiler feedback from R CMD SHLIB (I have mingw 3.1.0):
...: undefined reference to 'Rf_allocVector(unsigned,int)'
...: undefined reference to 'Rf_protect(SEXPREC*)'
...: undefined reference to 'Rf_unprotect(int)'

Can anybody tell me what the matter is? Did I miss something?

My PATH variable is as follows:

C:\R\tools;C:\Perl\bin\;C:\mingw\bin;c:\R\rw1090\bin;C:\Program
Files\MiKTeX\miktex\bin;...;C:\Program Files\HTML Help Workshop

I should mention that I'm a complete R and C++ newbie; any help would be
sincerely appreciated.

Regards

Alet

-------------------------------------
Alet Roux
Department of Mathematics
University of Hull
Kingston upon Hull
HU6 7RX
United Kingdom
URL: http://www.hull.ac.uk/php/mapar/
Tel: +44 (1482) 466463
Fax: +44 (1482) 466218



From P.Lemmens at nici.kun.nl  Wed Jun 16 17:27:29 2004
From: P.Lemmens at nici.kun.nl (Paul Lemmens)
Date: Wed, 16 Jun 2004 17:27:29 +0200
Subject: [R] subset(..., drop=TRUE) doesn't seem to work.
In-Reply-To: <x2pt7zk0el.fsf@biostat.ku.dk>
References: <DCF7E268195AB308453EFAA8@lemmens.socsci.kun.nl>
	<x2pt7zk0el.fsf@biostat.ku.dk>
Message-ID: <11677237B5977C39793F1D7F@lemmens.socsci.kun.nl>

Dear Peter,

--On woensdag 16 juni 2004 17:06 +0200 Peter Dalgaard 
<p.dalgaard at biostat.ku.dk> wrote:

> Paul Lemmens <P.Lemmens at nici.kun.nl> writes:
>
>> Hello!
>>
>> If I read ?subset, the workings of the argument drop (to me) seem to
>> imply equivalence of A and B (R 1.9.0):
>>
>> # A
>> dd <- data.frame(rt=rnorm(10), c=factor(gl(2,5)))
>> dd <- subset(dd, c==1)
>> dd$c <- dd$c[, drop=TRUE]
>> table(dd$c)
>>
>> 1
>> 5
>> 	
>>
>> # B
>> dd <- data.frame(rt=rnorm(10), c=factor(gl(2,5)))
>> dd <- subset(dd, c==1, drop=TRUE)
>> table(dd$c)
>>
>> 1 2
>> 5 0
>>
>> So to lose the second level of dd$c, in method B I still need to 'dd$c
>> <-
>> dd$c[, drop=TRUE]', while the manual seems to imply that with the drop
>> argument to subset() this would not be necessary.
>>
>>
>> Could you comment?
>
> Looks like a documentation bug. The actual code ends up  doing
>
>     x[r, vars, drop = drop]
>
> and "[.data.frame" will not drop factor levels. I wonder if it ever
> did...
>
Bottomline: unless I find the time to submit a patch for '[.data.frame', 
I'll need to use the more elaborate way of dropping the unused levels?

Does "will not drop" imply that it cannot be programmed, should not be 
programmed, or has not been programmed yet?


kind regards,
Paul



-- 
Paul Lemmens
NICI, University of Nijmegen              ASCII Ribbon Campaign /"\
Montessorilaan 3 (B.01.05)                    Against HTML Mail \ /
NL-6525 HR Nijmegen                                              X
The Netherlands                                                 / \
Phonenumber    +31-24-3612648
Fax            +31-24-3616066



From spencer.graves at pdf.com  Wed Jun 16 17:28:39 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 16 Jun 2004 10:28:39 -0500
Subject: [R] nonlinear modeling with rational functions?
Message-ID: <40D06727.2060401@pdf.com>

      Rational functions (ratios of polynomials) often provide good 
approximations to many functions.  Does anyone know of any literature on 
nonlinear modeling with rational functions, sequential estimation, 
diagnostics, etc.?  I know I can do it with "nls" and other nonlinear 
regression functions, but I'm wondering what literature might exist 
discussing how a search for an appropriate rational approximation? 

      Thanks,
      Spencer Graves



From york at zipcon.net  Wed Jun 16 17:32:15 2004
From: york at zipcon.net (Anne York)
Date: Wed, 16 Jun 2004 08:32:15 -0700 (PDT)
Subject: [R] import SYSTAT .syd file?
Message-ID: <Pine.LNX.4.60.0406160822100.1926@sasquatch>

On Tue, 15 Jun 2004 Jonathan Baron <baron at psych.upenn.edu> wrote:

Does anyone know how to read a SYSTAT .syd file on Linux?
(Splus 6 does it, but it is easier to find a Windows box
with Systat than to download their demo.  I'm wondering
if there is a better way than either of these options.)

Jon
--
Jonathan Baron, Professor of Psychology, University of 
Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron
R search page:        http://finzi.psych.upenn.edu


The commercial package dbmscopy has a Linux version. 
I have used dbmscopy for several years and have been happy 
with it as it converts data files among many spreadsheets and 
statistics programs.

http://www.conceptual.com/dbmscopt.htm


However, somewhat recently they were purchased by SAS, so 
I'm not sure of current state of the program. There are 
probably other commercial packages as well.

Anne



From laura at env.leeds.ac.uk  Wed Jun 16 17:34:32 2004
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Wed, 16 Jun 2004 16:34:32 +0100 (BST)
Subject: [R] apply/looping query
Message-ID: <Pine.LNX.4.44.0406161625210.15904-100000@env-pc-phd13>

I have several large matrices each having perhaps one or two
data points missing. For instance one point in 2881 is missing. As I want to perform
various analyses on these matrices I feel it is not
unreasonable to linearly interpolate over the missing points. I want to basically "fill in
the gaps" of the original matrix - the following piece of code
doesn't work and i'm not sure where i'm going wrong:

x=1:2881
my.new.matrix<-matrix(nrow=2881,ncol=20)
for(i in 1:20){
my.new.matrix[[i]]<-approx(x,my.matrix[,i],n=2881)
}

the error message says:

Error: more elements supplied than there are to replace

where am I going wrong??

Thanks in advance..
Laura



From p.dalgaard at biostat.ku.dk  Wed Jun 16 17:35:51 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 16 Jun 2004 17:35:51 +0200
Subject: [R] subset(..., drop=TRUE) doesn't seem to work.
In-Reply-To: <11677237B5977C39793F1D7F@lemmens.socsci.kun.nl>
References: <DCF7E268195AB308453EFAA8@lemmens.socsci.kun.nl>
	<x2pt7zk0el.fsf@biostat.ku.dk>
	<11677237B5977C39793F1D7F@lemmens.socsci.kun.nl>
Message-ID: <x2k6y7jz1k.fsf@biostat.ku.dk>

Paul Lemmens <P.Lemmens at nici.kun.nl> writes:

> >> Could you comment?
> >
> > Looks like a documentation bug. The actual code ends up  doing
> >
> >     x[r, vars, drop = drop]
> >
> > and "[.data.frame" will not drop factor levels. I wonder if it ever
> > did...
> >
> Bottomline: unless I find the time to submit a patch for
> '[.data.frame', I'll need to use the more elaborate way of dropping
> the unused levels?
> 
> Does "will not drop" imply that it cannot be programmed, should not be
> programmed, or has not been programmed yet?

Cannot. The drop argument is doing something different. 

Anyways, the way out is

 d2 <- subset(dd,c==1)
 ifac <- sapply(dd,is.factor)
 d2[ifac] <- lapply(d2[ifac],factor)

or 

 d2 <- subset(dd,c==1)
 d2[] <- lapply(d2, function(x) if (is.factor(x)) factor(x) else x) 



-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From B.Rowlingson at lancaster.ac.uk  Wed Jun 16 17:46:07 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Wed, 16 Jun 2004 16:46:07 +0100
Subject: [R] apply/looping query
In-Reply-To: <Pine.LNX.4.44.0406161625210.15904-100000@env-pc-phd13>
References: <Pine.LNX.4.44.0406161625210.15904-100000@env-pc-phd13>
Message-ID: <40D06B3F.4020302@lancaster.ac.uk>

Laura Quinn wrote:

> x=1:2881
> my.new.matrix<-matrix(nrow=2881,ncol=20)
> for(i in 1:20){
> my.new.matrix[[i]]<-approx(x,my.matrix[,i],n=2881)
> }
> 
> the error message says:
> 
> Error: more elements supplied than there are to replace
> 
> where am I going wrong??

approx() returns a list with $x and $y components

I'd use [,x] instead of [[i]] to replace columns.

Hence:

  for(i in 1:20){
   my.new.matrix[,i]<-approx(x,my.matrix[,i],n=2881)$y
  }

Baz



From simon at stats.gla.ac.uk  Wed Jun 16 17:48:26 2004
From: simon at stats.gla.ac.uk (Simon Wood)
Date: Wed, 16 Jun 2004 16:48:26 +0100 (BST)
Subject: [R] gam
In-Reply-To: <1087396716.4291.13.camel@new-york.climpact.net>
References: <1087396716.4291.13.camel@new-york.climpact.net>
Message-ID: <Pine.SOL.4.58.0406161638110.25229@moon.stats.gla.ac.uk>

> i'm working with mgcv packages and specially gam. My exemple is:
>
> >test<-gam(B~s(pred1)+s(pred2))
> >plot(test,pages=1)
>
> when ploting test, you can view pred1 vs s(pred1, edf[1] ) & pred2 vs
> s(pred2, edf[2] )
>
> I would like to know if there is a way to access to those terms
> (s(pred1) & s(pred2)). Does someone know how?

Depends a bit on what sort of access you want... You can use predict.gam
to obtain the estimated value of each smooth  term at any set of pred1 or
pred2 values you supply (along with standard errors).

The underlying equations are somewhat unwieldy, but are given in
Wood, S.N. (2003) Thin plate regression splines. J.R.Statist.Soc.B
65(1):95-114 ... if you need to evaluate the smooth in another program or
something then you'd probably need to transform the t.p.r.s. parameters back
to thin plate spline parameters and use the t.p.s. basis.

- You can also change the smoothing basis to one which is easier to write
down - the "cr" basis (see ?s) parameterizes a 1-d cubic spline in terms of the
function values at the knots, for example.

- Or you can add a smoothing basis of your own design and hence specify
the equations of the smooth yourself: ?p.spline gives an example.


>
> the purpose is to access to equation of smooths terms in order to have
> the equation of my additive model.

best,
Simon

_____________________________________________________________________
> Simon Wood simon at stats.gla.ac.uk        www.stats.gla.ac.uk/~simon/
>>  Department of Statistics, University of Glasgow, Glasgow, G12 8QQ
>>>   Direct telephone: (0)141 330 4530          Fax: (0)141 330 4814



From ripley at stats.ox.ac.uk  Wed Jun 16 17:53:26 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 16 Jun 2004 16:53:26 +0100 (BST)
Subject: [R] Is input pre-processing needed for nnet module?
In-Reply-To: <40D06158.EA1D758@osmer.fvg.it>
Message-ID: <Pine.LNX.4.44.0406161652290.14482-100000@gannet.stats>

If you read the reference on the help page, you will find out the answer.
After all, as the DESCRIPTION says, this is support software for a book.

On Wed, 16 Jun 2004, Agostino.Manzato at osmer.fvg.it wrote:

> Hi,
> I used the nnet R module to classify my data using Neural Networks:
> nnet(input_matrix, obs_vect, size=h, linout=FALSE, entropy=TRUE)
> I used as NN input my "raw" data. 
> After that I tried to use the normalized input data (with z-scores,
> i.e. mean=0 and std=1) and have found NNs with a little smaller 
> Cross Entropy Error.
> My question is: 
> Is it *wrong* to feed nnet directly with the raw input data?
> 
> I found in
> http://www.faqs.org/faqs/ai-faq/neural-nets/part2/section-16.html
> that it depends on the minimization training algorithm:
> - "Steepest descent is very sensitive to scaling.
> - Quasi-Newton and conjugate gradient methods... therefore are scale 
> sensitive. However,... are less scale sensitive than pure gradient
> descent.
> - Newton-Raphson and Gauss-Newton, if implemented correctly, are
> theoretically invariant under scale changes..."
> 
> I know that nnet is a Quasi-Newton algorithm, so it make sense 
> that I found a small improvement using the normalized data.
> Can someone confirme if it is really so?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From MSchwartz at MedAnalytics.com  Wed Jun 16 17:53:43 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Wed, 16 Jun 2004 10:53:43 -0500
Subject: [R] import SYSTAT .syd file?
In-Reply-To: <Pine.LNX.4.60.0406160822100.1926@sasquatch>
References: <Pine.LNX.4.60.0406160822100.1926@sasquatch>
Message-ID: <1087401223.4446.28.camel@localhost.localdomain>

On Wed, 2004-06-16 at 10:32, Anne York wrote:
> On Tue, 15 Jun 2004 Jonathan Baron <baron at psych.upenn.edu> wrote:
> 
> Does anyone know how to read a SYSTAT .syd file on Linux?
> (Splus 6 does it, but it is easier to find a Windows box
> with Systat than to download their demo.  I'm wondering
> if there is a better way than either of these options.)
> 
> Jon
> 
> The commercial package dbmscopy has a Linux version. 
> I have used dbmscopy for several years and have been happy 
> with it as it converts data files among many spreadsheets and 
> statistics programs.
> 
> http://www.conceptual.com/dbmscopt.htm
> 
> 
> However, somewhat recently they were purchased by SAS, so 
> I'm not sure of current state of the program. There are 
> probably other commercial packages as well.
> 
> Anne


Hi Jon and Anne!

One other commercial product to check out is Stat/Transfer.  More
information on supported formats is at:
http://www.stattransfer.com/html/formats.html

They do support Windows, MacOS and Unix/Linux. Demo downloads are
available from: http://www.stattransfer.com/html/download.html

Unix/Linux pricing is available at:
http://www.stattransfer.com/html/prices_-_unix.html.

HTH,

Marc Schwartz



From ogden at stat.sc.edu  Wed Jun 16 17:55:38 2004
From: ogden at stat.sc.edu (Todd Ogden)
Date: Wed, 16 Jun 2004 11:55:38 -0400 (EDT)
Subject: [R] subset and lme
In-Reply-To: <40D060E9.1070007@pdf.com> from "Sundar Dorai-Raj" at Jun 16,
	2004 08:02:01 AM
Message-ID: <200406161555.i5GFtc1e017027@fhat.stat.sc.edu>

Version of R - the problem is both on 1.8 and 1.9.

Version of nlme - 3.1-48

It turns out that the problem is deeper than what I just posted -- in
other calls to lme, I get error messages saying objects not found,
even when the objects in question are created explicitly before the
lme function call or even arguments to the function that's making the
lme calls!  I can do print(obj) just before the lme call, but then if
obj is used in the lme call, it's not found.

Todd
to166 at columbia.edu

> 
> Todd Ogden wrote:
> 
> > I'm puzzled by the following problem, which appears when 
> > attempting to run an analysis on part of a dataset:
> > 
> > If I try:
> > 
> >   csubset <- dat$Diagnosis==0
> >   cont <- lme(fixed=cform,
> >                    random = ~1|StudyName,
> >                       data=dat,subset=csubset,na.action=na.omit)
> > 
> > Then I get:
> > 
> > Error in eval(expr, envir, enclos) : Object "csubset" not found
> > 
> > But if I do instead:
> > 
> >   cdat <- dat[dat$Diagnosis==0,]
> >   cont <- lme(fixed=cform,
> >                    random = ~1|StudyName,
> >                       data=cdat,na.action=na.omit)
> > 
> > Then everything is fine.
> > 
> > I'm puzzled that the object can't be found.  Maybe I'm 
> > overlooking something obvious?
> > 
> 
> Todd,
>    What version of R/nlme? I just tried the following:
> 
> library(nlme)
> data(Orthodont)
> csubset <- Orthodont$Sex == "Male"
> fm1 <- lme(distance ~ age, data = Orthodont,
>             random = ~ 1, subset = csubset)
> fm2 <- lme(distance ~ age, data = Orthodont,
>             random = ~ 1, subset = Sex == "Male")
> fm3 <- lme(distance ~ age, data = Orthodont[csubset, ],
>             random = ~ 1)
> 
>  > R.version.string
> [1] "R version 1.9.0, 2004-05-06"
> R> library(help = "nlme")
> 
> 		Information on Package 'nlme'
> 
> Description:
> 
> Package: nlme
> Version: 3.1-48
> Date: 2004/01/14
> <snip>
> 
> --sundar
> 
> 
> 
> 


--



From ripley at stats.ox.ac.uk  Wed Jun 16 17:58:34 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 16 Jun 2004 16:58:34 +0100 (BST)
Subject: [R] Compiling C++ package source: linking problem?
In-Reply-To: <1087399394.40d065e21d663@webmail1.hull.ac.uk>
Message-ID: <Pine.LNX.4.44.0406161655410.14482-100000@gannet.stats>

On Wed, 16 Jun 2004, Alet Roux wrote:

> Dear All
> 
> I'm currently developing a package for R (1.9.0) on Win32, with C++ source code.
> Having followed the instructions in readme.packages, my code compiles fine with
> R CMD SHLIB (as well as R CMD check) ... until I start using the internal R
> functions. 
> 
> (Interesting: Rprintf seems to be the exception.) For instance, the following
> code compiles fine:
> 
> #include <R.h>
> #include <Rdefines.h>
> ...
> SEXP whatever (SEXP model)
> {
> 	Rprintf ("Hello, here I am!\n");
>         return model;
> }
> 
> However, the following doesn't compile at all:
> 
> #include <R.h>
> #include <Rdefines.h>
> ...
> SEXP whatever (SEXP model)
> {
> 	SEXP anotherModel;
> 	PROTECT(anotherModel = NEW_NUMERIC(4));
> 	UNPROTECT(1);
>         return anotherModel;
> }
> 
> An example of compiler feedback from R CMD SHLIB (I have mingw 3.1.0):
> ...: undefined reference to 'Rf_allocVector(unsigned,int)'
> ...: undefined reference to 'Rf_protect(SEXPREC*)'
> ...: undefined reference to 'Rf_unprotect(int)'
> 
> Can anybody tell me what the matter is? Did I miss something?

Are you linking against the R.dll via its import library libR.a?  Did the 
latter get made correctly?  That is where those entry points are.

More complete compiler output would have helped here.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Jun 16 18:04:42 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 16 Jun 2004 17:04:42 +0100 (BST)
Subject: [R] subset(..., drop=TRUE) doesn't seem to work.
In-Reply-To: <11677237B5977C39793F1D7F@lemmens.socsci.kun.nl>
Message-ID: <Pine.LNX.4.44.0406161659040.14482-100000@gannet.stats>

On Wed, 16 Jun 2004, Paul Lemmens wrote:

> Dear Peter,
> 
> --On woensdag 16 juni 2004 17:06 +0200 Peter Dalgaard 
> <p.dalgaard at biostat.ku.dk> wrote:
> 
> > Paul Lemmens <P.Lemmens at nici.kun.nl> writes:
> >
> >> Hello!
> >>
> >> If I read ?subset, the workings of the argument drop (to me) seem to
> >> imply equivalence of A and B (R 1.9.0):
> >>
> >> # A
> >> dd <- data.frame(rt=rnorm(10), c=factor(gl(2,5)))
> >> dd <- subset(dd, c==1)
> >> dd$c <- dd$c[, drop=TRUE]
> >> table(dd$c)
> >>
> >> 1
> >> 5
> >> 	
> >>
> >> # B
> >> dd <- data.frame(rt=rnorm(10), c=factor(gl(2,5)))
> >> dd <- subset(dd, c==1, drop=TRUE)
> >> table(dd$c)
> >>
> >> 1 2
> >> 5 0
> >>
> >> So to lose the second level of dd$c, in method B I still need to 'dd$c
> >> <-
> >> dd$c[, drop=TRUE]', while the manual seems to imply that with the drop
> >> argument to subset() this would not be necessary.
> >>
> >>
> >> Could you comment?
> >
> > Looks like a documentation bug. The actual code ends up  doing
> >
> >     x[r, vars, drop = drop]
> >
> > and "[.data.frame" will not drop factor levels. I wonder if it ever
> > did...

No, AFAIK. It was definitely not documented to last November when that 
comment was added to ?subset.

> Bottomline: unless I find the time to submit a patch for '[.data.frame', 
> I'll need to use the more elaborate way of dropping the unused levels?
> 
> Does "will not drop" imply that it cannot be programmed, should not be 
> programmed, or has not been programmed yet?

It is designed not to.  Look at how ?"[.data.frame" documents it.  We
don't want it altered and it would break a lot of code to do so.  I think
the author of those lines was under a misapprension.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From richard.kittler at amd.com  Wed Jun 16 18:37:11 2004
From: richard.kittler at amd.com (richard.kittler@amd.com)
Date: Wed, 16 Jun 2004 09:37:11 -0700
Subject: [R] Coercing a dataframe column to datetime
Message-ID: <858788618A93D111B45900805F85267A0BCB2DA1@caexmta3.amd.com>

Sorry to bother you all with this one. I determined that the problem is solved if I use 'lapply' rather than 'sapply'.  Since sapply returns the result as a vector it must be making a best guess of numeric for the class of the result (?)

--Rich

Richard Kittler 
AMD TDG
408-749-4099

-----Original Message-----
From: Kittler, Richard 
Sent: Tuesday, June 15, 2004 11:32 AM
To: 'Prof Brian Ripley'
Cc: r-help at stat.math.ethz.ch
Subject: RE: [R] Coercing a dataframe column to datetime


Thank you! The next step in the conversion still fails and I can't seem to find any examples in the archives.  The result of the function 'as.POSIXct(strptime())' within the 'sapply' comes back as numeric rather than POSIXct as expected: 

> ds <- cbind(1:2, c("02/27/92 23:03:20", "02/27/92 22:29:56")); ds
     [,1] [,2]               
[1,] "1"  "02/27/92 23:03:20"
[2,] "2"  "02/27/92 22:29:56"
> q <- sapply(ds[,2], function(x) as.POSIXct(strptime(x,"%m/%d/%y 
> %H:%M:%S")))
> class(q) 
[1] "numeric"
> q
02/27/92 23:03:20 02/27/92 22:29:56 
        699260600         699258596 

--Rich

Richard Kittler 
AMD TDG
408-749-4099

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: Monday, June 14, 2004 12:05 PM
To: Kittler, Richard
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] Coercing a dataframe column to datetime


You have forgotten as.POSIXct is needed too.

On Mon, 14 Jun 2004 richard.kittler at amd.com wrote:

> I am trying to coerce a data frame column from character to datetime using strptime but keep getting an error because the length of the coerced object is always 9.  What am I doing wrong here:   
> 
> .................................................................
> > ds <- cbind(1:2, c("02/27/92 23:03:20", "02/27/92 22:29:56")); ds
>      [,1] [,2]               
> [1,] "1"  "02/27/92 23:03:20"
> [2,] "2"  "02/27/92 22:29:56"
> >  
> > q <- strptime(ds[,2], "%m/%d/%y %H:%M:%S"); q
> [1] "1992-02-27 23:03:20" "1992-02-27 22:29:56"
> > 
> > ds[,2] <- q
> Error in "[<-"(`*tmp*`, , 2, value = q) : number of items to replace
> is not a multiple of replacement length
> > 
> > length(q)
> [1] 9
> 
> .................................................................
> 
> --Rich
> 
> Richard Kittler
> AMD TDG
> 408-749-4099
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Wed Jun 16 18:45:24 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 16 Jun 2004 18:45:24 +0200
Subject: [R] subset(..., drop=TRUE) doesn't seem to work.
In-Reply-To: <Pine.LNX.4.44.0406161659040.14482-100000@gannet.stats>
References: <Pine.LNX.4.44.0406161659040.14482-100000@gannet.stats>
Message-ID: <x2fz8vjvtn.fsf@biostat.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> No, AFAIK. It was definitely not documented to last November when that 
> comment was added to ?subset.
> 
> > Bottomline: unless I find the time to submit a patch for '[.data.frame', 
> > I'll need to use the more elaborate way of dropping the unused levels?
> > 
> > Does "will not drop" imply that it cannot be programmed, should not be 
> > programmed, or has not been programmed yet?
> 
> It is designed not to.  Look at how ?"[.data.frame" documents it.  We
> don't want it altered and it would break a lot of code to do so.  I think
> the author of those lines was under a misapprension.

Not to say confused... (Brian is tactful enough not to say it was me).
Looking back through my emails from then, I can't say it's all that
surprising. It came from a feature request (the author of which shall
remain anonymous):

...
> noticed that subset.data.frame has drop=FALSE `hard-wired`
> into it, making it problematic in case of factors (depending
> on required behaviour). 

and with 3 days to go before 1.8.1, I seem to have just implemented
the suggestion without thinking. (I think we did have a drop.factor at
some point back in the stone age where factors were a primitive data
type, which may have contributed to the confusion.)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From tlumley at u.washington.edu  Wed Jun 16 19:46:17 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Wed, 16 Jun 2004 10:46:17 -0700 (PDT)
Subject: [R] subset and lme
In-Reply-To: <200406161555.i5GFtc1e017027@fhat.stat.sc.edu>
References: <200406161555.i5GFtc1e017027@fhat.stat.sc.edu>
Message-ID: <Pine.A41.4.58.0406161045450.111504@homer03.u.washington.edu>

On Wed, 16 Jun 2004, Todd Ogden wrote:

> Version of R - the problem is both on 1.8 and 1.9.
>
> Version of nlme - 3.1-48
>
> It turns out that the problem is deeper than what I just posted -- in
> other calls to lme, I get error messages saying objects not found,
> even when the objects in question are created explicitly before the
> lme function call or even arguments to the function that's making the
> lme calls!  I can do print(obj) just before the lme call, but then if
> obj is used in the lme call, it's not found.
>

Is this inside a function or at the R prompt?

	-thomas





> Todd
> to166 at columbia.edu
>
> >
> > Todd Ogden wrote:
> >
> > > I'm puzzled by the following problem, which appears when
> > > attempting to run an analysis on part of a dataset:
> > >
> > > If I try:
> > >
> > >   csubset <- dat$Diagnosis==0
> > >   cont <- lme(fixed=cform,
> > >                    random = ~1|StudyName,
> > >                       data=dat,subset=csubset,na.action=na.omit)
> > >
> > > Then I get:
> > >
> > > Error in eval(expr, envir, enclos) : Object "csubset" not found
> > >
> > > But if I do instead:
> > >
> > >   cdat <- dat[dat$Diagnosis==0,]
> > >   cont <- lme(fixed=cform,
> > >                    random = ~1|StudyName,
> > >                       data=cdat,na.action=na.omit)
> > >
> > > Then everything is fine.
> > >
> > > I'm puzzled that the object can't be found.  Maybe I'm
> > > overlooking something obvious?
> > >
> >
> > Todd,
> >    What version of R/nlme? I just tried the following:
> >
> > library(nlme)
> > data(Orthodont)
> > csubset <- Orthodont$Sex == "Male"
> > fm1 <- lme(distance ~ age, data = Orthodont,
> >             random = ~ 1, subset = csubset)
> > fm2 <- lme(distance ~ age, data = Orthodont,
> >             random = ~ 1, subset = Sex == "Male")
> > fm3 <- lme(distance ~ age, data = Orthodont[csubset, ],
> >             random = ~ 1)
> >
> >  > R.version.string
> > [1] "R version 1.9.0, 2004-05-06"
> > R> library(help = "nlme")
> >
> > 		Information on Package 'nlme'
> >
> > Description:
> >
> > Package: nlme
> > Version: 3.1-48
> > Date: 2004/01/14
> > <snip>
> >
> > --sundar
> >
> >
> >
> >
>
>
> --
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From ogden at stat.sc.edu  Wed Jun 16 19:58:02 2004
From: ogden at stat.sc.edu (Todd Ogden)
Date: Wed, 16 Jun 2004 13:58:02 -0400 (EDT)
Subject: [R] subset and lme
In-Reply-To: <Pine.A41.4.58.0406161045450.111504@homer03.u.washington.edu>
	from "Thomas Lumley" at Jun 16, 2004 10:46:17 AM
Message-ID: <200406161758.i5GHw2GV017158@fhat.stat.sc.edu>

Douglas Grove kindly pointed out that, if I had read and understood
the lme help page, I would have realized that whatever I put as the
subset argument has to be an object in the argument for data.  That
was my problem all along -- if I had used

subset=Diagnosis==0

in the lme function call, that would have been fine.  But I originally had

subset=dat$Diagnosis==0

and that didn't work because dat was not an object in dat.

So the problem was caused by my incorrect usage of lme.

Many thanks for the help and suggestions!

Todd


> Is this inside a function or at the R prompt?
> 
> 	-thomas
> 
> 
> 
> 
> 
> > Todd
> > to166 at columbia.edu
> >
> > >
> > > Todd Ogden wrote:
> > >
> > > > I'm puzzled by the following problem, which appears when
> > > > attempting to run an analysis on part of a dataset:
> > > >
> > > > If I try:
> > > >
> > > >   csubset <- dat$Diagnosis==0
> > > >   cont <- lme(fixed=cform,
> > > >                    random = ~1|StudyName,
> > > >                       data=dat,subset=csubset,na.action=na.omit)
> > > >
> > > > Then I get:
> > > >
> > > > Error in eval(expr, envir, enclos) : Object "csubset" not found
> > > >
> > > > But if I do instead:
> > > >
> > > >   cdat <- dat[dat$Diagnosis==0,]
> > > >   cont <- lme(fixed=cform,
> > > >                    random = ~1|StudyName,
> > > >                       data=cdat,na.action=na.omit)
> > > >
> > > > Then everything is fine.
> > > >
> > > > I'm puzzled that the object can't be found.  Maybe I'm
> > > > overlooking something obvious?
> > > >
> > >
> > > Todd,
> > >    What version of R/nlme? I just tried the following:
> > >
> > > library(nlme)
> > > data(Orthodont)
> > > csubset <- Orthodont$Sex == "Male"
> > > fm1 <- lme(distance ~ age, data = Orthodont,
> > >             random = ~ 1, subset = csubset)
> > > fm2 <- lme(distance ~ age, data = Orthodont,
> > >             random = ~ 1, subset = Sex == "Male")
> > > fm3 <- lme(distance ~ age, data = Orthodont[csubset, ],
> > >             random = ~ 1)
> > >
> > >  > R.version.string
> > > [1] "R version 1.9.0, 2004-05-06"
> > > R> library(help = "nlme")
> > >
> > > 		Information on Package 'nlme'
> > >
> > > Description:
> > >
> > > Package: nlme
> > > Version: 3.1-48
> > > Date: 2004/01/14
> > > <snip>
> > >
> > > --sundar
> > >
> > >
> > >
> > >
> >
> >
> > --
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> 
> Thomas Lumley			Assoc. Professor, Biostatistics
> tlumley at u.washington.edu	University of Washington, Seattle
> 


--



From clint at ecy.wa.gov  Wed Jun 16 20:21:56 2004
From: clint at ecy.wa.gov (Clint Bowman)
Date: Wed, 16 Jun 2004 11:21:56 -0700 (PDT)
Subject: [R] Aggregating on Water Year Rather Than Calendar Year
Message-ID: <Pine.LNX.4.44.0406161111250.18003-100000@aeolus.ecy.wa.gov>

The US water year extends from 01 October yyyy-1 through 30 September yyyy
and is referenced by the year starting on the included 01 January yyyy.  
I'd like to be able to find the annual means for the water year.  To do so 
I've taken the input date-time, which is in the usual format 

"1991-10-07 10:35:00"

changed it by:

w$d<-as.POSIXct(w$date.time)

Now I can add an offset of 92 days 

w$w.year<-as.POSIXct(w$d+7948800)

and have the years correspond to the "water year."

Now I wish to obtain some means by something like:

waterT<-aggregate(w$value[w$param.name=="Temperature"],
list(w$w.year[w$param.name=="Temperature"]),mean)

Except that I need to work on the year and being a neophyte in date 
arithmetic I'm not finding a working method.

TIA

Clint

-- 
Clint Bowman			INTERNET:	clint at ecy.wa.gov
Air Quality Modeler		INTERNET:	clint at math.utah.edu
Department of Ecology		VOICE:		(360) 407-6815
PO Box 47600			FAX:		(360) 407-7534
Olympia, WA 98504-7600



From notulei at yahoo.com  Wed Jun 16 21:08:32 2004
From: notulei at yahoo.com (Alex Nu)
Date: Wed, 16 Jun 2004 12:08:32 -0700 (PDT)
Subject: [R] printing R generated postcript files
In-Reply-To: <1087396716.4291.13.camel@new-york.climpact.net>
Message-ID: <20040616190832.28013.qmail@web60108.mail.yahoo.com>

 
 Whenever I print R generated postcript files,
 the right part is being chopped.

 The file looks ok, if I see the file with
 ghostview or other ps viewers.

 I wonder if someone had a problem like that 
 before.

 Thanks

 Alex

 P.S. I'm using R-1.9.0 on debian



From rpeng at jhsph.edu  Wed Jun 16 21:13:39 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Wed, 16 Jun 2004 15:13:39 -0400
Subject: [R] printing R generated postcript files
In-Reply-To: <20040616190832.28013.qmail@web60108.mail.yahoo.com>
References: <20040616190832.28013.qmail@web60108.mail.yahoo.com>
Message-ID: <40D09BE3.5000908@jhsph.edu>

What papersize are you using?  What is getOption("papersize")?  If R 
thinks you're using a4 and you print on letter, the plot will be 
chopped off.

-roger

Alex Nu wrote:
>  
>  Whenever I print R generated postcript files,
>  the right part is being chopped.
> 
>  The file looks ok, if I see the file with
>  ghostview or other ps viewers.
> 
>  I wonder if someone had a problem like that 
>  before.
> 
>  Thanks
> 
>  Alex
> 
>  P.S. I'm using R-1.9.0 on debian
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng



From edd at debian.org  Wed Jun 16 21:23:03 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 16 Jun 2004 14:23:03 -0500
Subject: [R] printing R generated postcript files
In-Reply-To: <20040616190832.28013.qmail@web60108.mail.yahoo.com>
References: <1087396716.4291.13.camel@new-york.climpact.net>
	<20040616190832.28013.qmail@web60108.mail.yahoo.com>
Message-ID: <20040616192303.GA16351@sonny.eddelbuettel.com>

On Wed, Jun 16, 2004 at 12:08:32PM -0700, Alex Nu wrote:
>  
>  Whenever I print R generated postcript files,
>  the right part is being chopped.
> 
>  The file looks ok, if I see the file with
>  ghostview or other ps viewers.
> 
>  I wonder if someone had a problem like that 
>  before.
> 
>  Thanks
> 
>  Alex
> 
>  P.S. I'm using R-1.9.0 on debian

That may be related. I only recently set R_PAPERSIZE to be driven from
Debian's global paperconf setting which is supposed result in
/etc/R/Renviron having

## edd Apr 2004:  use Debian's paperconf settings, with thanks to Matej Cepl
R_PAPERSIZE=${R_PAPERSIZE-$(cat /etc/papersize)}

yet I just noticed that my own installation doesn't have that. And yes,
looks like a build-time patch failed. bNeed to check that ...

Anyway, see of correcting R_PAPERSIZE to what you really use (letter,
maybe?) helps...

Dirk

-- 
FEATURE:  VW Beetle license plate in California



From macq at llnl.gov  Wed Jun 16 22:57:54 2004
From: macq at llnl.gov (Don MacQueen)
Date: Wed, 16 Jun 2004 13:57:54 -0700
Subject: [R] Aggregating on Water Year Rather Than Calendar Year
In-Reply-To: <Pine.LNX.4.44.0406161111250.18003-100000@aeolus.ecy.wa.gov>
References: <Pine.LNX.4.44.0406161111250.18003-100000@aeolus.ecy.wa.gov>
Message-ID: <p06002007bcf6602e7a7f@[128.115.153.6]>

It's not clear where your problem is.

Did w$w.year come out wrong?
Or did the aggregate() function fail?

For getting the water year, I would do it differently:
  (this is for a U.S. English locale, and minimally tested)

    tmp <- as.POSIXlt(w$d)$year+1900
    w$w.year <- ifelse( format(w$d,'%b') %in% c('Oct','Nov','Dec'), tmp+1, tmp)

Then you can do things like
   table(w$w.year)
   table(w$w.year,w$param.name)
to help find out if w.year came out like it should.

You'll have to provide error messages or something if the problem is 
with using aggregate().

At 11:21 AM -0700 6/16/04, Clint Bowman wrote:
>The US water year extends from 01 October yyyy-1 through 30 September yyyy
>and is referenced by the year starting on the included 01 January yyyy. 
>I'd like to be able to find the annual means for the water year.  To do so
>I've taken the input date-time, which is in the usual format
>
>"1991-10-07 10:35:00"
>
>changed it by:
>
>w$d<-as.POSIXct(w$date.time)
>
>Now I can add an offset of 92 days
>
>w$w.year<-as.POSIXct(w$d+7948800)

Try   w$w.year <- w$d+7948800

>
>and have the years correspond to the "water year."
>
>Now I wish to obtain some means by something like:
>
>waterT<-aggregate(w$value[w$param.name=="Temperature"],
>list(w$w.year[w$param.name=="Temperature"]),mean)
>
>Except that I need to work on the year and being a neophyte in date
>arithmetic I'm not finding a working method.
>
>TIA
>
>Clint
>
>--
>Clint Bowman			INTERNET:	clint at ecy.wa.gov
>Air Quality Modeler		INTERNET:	clint at math.utah.edu
>Department of Ecology		VOICE:		(360) 407-6815
>PO Box 47600			FAX:		(360) 407-7534
>Olympia, WA 98504-7600
>

-Don

-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From clint at ecy.wa.gov  Wed Jun 16 23:09:03 2004
From: clint at ecy.wa.gov (Clint Bowman)
Date: Wed, 16 Jun 2004 14:09:03 -0700 (PDT)
Subject: [R] Aggregating on Water Year Rather Than Calendar Year
In-Reply-To: <p06002007bcf6602e7a7f@[128.115.153.6]>
Message-ID: <Pine.LNX.4.44.0406161407120.18003-100000@aeolus.ecy.wa.gov>

Aha!  I was unclear of the way to extract the year from the function call 
(which your example shows.)  The following works wonders:

w$w.year<-as.POSIXlt(w$d+7948800)$year+1900

Again, thanks,

Clint

On Wed, 16 Jun 2004, Don MacQueen wrote:

> It's not clear where your problem is.
> 
> Did w$w.year come out wrong?
> Or did the aggregate() function fail?
> 
> For getting the water year, I would do it differently:
>   (this is for a U.S. English locale, and minimally tested)
> 
>     tmp <- as.POSIXlt(w$d)$year+1900
>     w$w.year <- ifelse( format(w$d,'%b') %in% c('Oct','Nov','Dec'), tmp+1, tmp)
> 
> Then you can do things like
>    table(w$w.year)
>    table(w$w.year,w$param.name)
> to help find out if w.year came out like it should.
> 
> You'll have to provide error messages or something if the problem is 
> with using aggregate().
> 
> At 11:21 AM -0700 6/16/04, Clint Bowman wrote:
> >The US water year extends from 01 October yyyy-1 through 30 September yyyy
> >and is referenced by the year starting on the included 01 January yyyy. 
> >I'd like to be able to find the annual means for the water year.  To do so
> >I've taken the input date-time, which is in the usual format
> >
> >"1991-10-07 10:35:00"
> >
> >changed it by:
> >
> >w$d<-as.POSIXct(w$date.time)
> >
> >Now I can add an offset of 92 days
> >
> >w$w.year<-as.POSIXct(w$d+7948800)
> 
> Try   w$w.year <- w$d+7948800
> 
> >
> >and have the years correspond to the "water year."
> >
> >Now I wish to obtain some means by something like:
> >
> >waterT<-aggregate(w$value[w$param.name=="Temperature"],
> >list(w$w.year[w$param.name=="Temperature"]),mean)
> >
> >Except that I need to work on the year and being a neophyte in date
> >arithmetic I'm not finding a working method.
> >
> >TIA
> >
> >Clint
> >
> >--
> >Clint Bowman			INTERNET:	clint at ecy.wa.gov
> >Air Quality Modeler		INTERNET:	clint at math.utah.edu
> >Department of Ecology		VOICE:		(360) 407-6815
> >PO Box 47600			FAX:		(360) 407-7534
> >Olympia, WA 98504-7600
> >
> 
> -Don
> 
> 

-- 
Clint Bowman			INTERNET:	clint at ecy.wa.gov
Air Quality Modeler		INTERNET:	clint at math.utah.edu
Department of Ecology		VOICE:		(360) 407-6815
PO Box 47600			FAX:		(360) 407-7534
Olympia, WA 98504-7600



From ckjmaner at yahoo.com  Wed Jun 16 23:14:16 2004
From: ckjmaner at yahoo.com (Charles Maner)
Date: Wed, 16 Jun 2004 14:14:16 -0700 (PDT)
Subject: [R] erf function documentation
Message-ID: <20040616211416.19252.qmail@web40308.mail.yahoo.com>


Hi all.  I may be wrong, (and often am), but in trying
to determine how to calculate the erf function, the
documentation for 'pnorm' states:

## if you want the so-called 'error function'
erf <- function(x) 2 * pnorm(x * sqrt(2)) - 1
## and the so-called 'complementary error function'
erfc <- function(x) 2 * pnorm(x * sqrt(2),
lower=FALSE)

Should, instead, it read:
## if you want the so-called 'error function'
erf <- function(x) 2 * pnorm(x / sqrt(2)) - 1
## and the so-called 'complementary error function'
erfc <- function(x) 2 * pnorm(x / sqrt(2),
lower=FALSE)

I've looked at a couple references and they all show
that 'x' should be divided by, not multiplied by,
'sqrt(2)'.

Again, I may be incorrect.  If so, kindly let me know.
 But, if I am correct, perhaps the documentation could
be corrected in a subsequent R release.


Thanks,
Charles



From ripley at stats.ox.ac.uk  Wed Jun 16 23:34:29 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 16 Jun 2004 22:34:29 +0100 (BST)
Subject: [R] Aggregating on Water Year Rather Than Calendar Year
In-Reply-To: <Pine.LNX.4.44.0406161407120.18003-100000@aeolus.ecy.wa.gov>
Message-ID: <Pine.LNX.4.44.0406162232260.19556-100000@gannet.stats>

On Wed, 16 Jun 2004, Clint Bowman wrote:

> Aha!  I was unclear of the way to extract the year from the function call 
> (which your example shows.)  The following works wonders:

For future reference, ?months (and ?weekdays? and ?quarters and ?julian)
tells you.  It's hard to thing of a really obvious place to put this.

> 
> w$w.year<-as.POSIXlt(w$d+7948800)$year+1900
> 
> Again, thanks,
> 
> Clint
> 
> On Wed, 16 Jun 2004, Don MacQueen wrote:
> 
> > It's not clear where your problem is.
> > 
> > Did w$w.year come out wrong?
> > Or did the aggregate() function fail?
> > 
> > For getting the water year, I would do it differently:
> >   (this is for a U.S. English locale, and minimally tested)
> > 
> >     tmp <- as.POSIXlt(w$d)$year+1900
> >     w$w.year <- ifelse( format(w$d,'%b') %in% c('Oct','Nov','Dec'), tmp+1, tmp)
> > 
> > Then you can do things like
> >    table(w$w.year)
> >    table(w$w.year,w$param.name)
> > to help find out if w.year came out like it should.
> > 
> > You'll have to provide error messages or something if the problem is 
> > with using aggregate().
> > 
> > At 11:21 AM -0700 6/16/04, Clint Bowman wrote:
> > >The US water year extends from 01 October yyyy-1 through 30 September yyyy
> > >and is referenced by the year starting on the included 01 January yyyy. 
> > >I'd like to be able to find the annual means for the water year.  To do so
> > >I've taken the input date-time, which is in the usual format
> > >
> > >"1991-10-07 10:35:00"
> > >
> > >changed it by:
> > >
> > >w$d<-as.POSIXct(w$date.time)
> > >
> > >Now I can add an offset of 92 days
> > >
> > >w$w.year<-as.POSIXct(w$d+7948800)
> > 
> > Try   w$w.year <- w$d+7948800
> > 
> > >
> > >and have the years correspond to the "water year."
> > >
> > >Now I wish to obtain some means by something like:
> > >
> > >waterT<-aggregate(w$value[w$param.name=="Temperature"],
> > >list(w$w.year[w$param.name=="Temperature"]),mean)
> > >
> > >Except that I need to work on the year and being a neophyte in date
> > >arithmetic I'm not finding a working method.
> > >
> > >TIA
> > >
> > >Clint
> > >
> > >--
> > >Clint Bowman			INTERNET:	clint at ecy.wa.gov
> > >Air Quality Modeler		INTERNET:	clint at math.utah.edu
> > >Department of Ecology		VOICE:		(360) 407-6815
> > >PO Box 47600			FAX:		(360) 407-7534
> > >Olympia, WA 98504-7600
> > >
> > 
> > -Don
> > 
> > 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From clint at ecy.wa.gov  Wed Jun 16 23:38:25 2004
From: clint at ecy.wa.gov (Clint Bowman)
Date: Wed, 16 Jun 2004 14:38:25 -0700 (PDT)
Subject: [R] Aggregating on Water Year Rather Than Calendar Year
In-Reply-To: <Pine.LNX.4.44.0406162232260.19556-100000@gannet.stats>
Message-ID: <Pine.LNX.4.44.0406161434580.18003-100000@aeolus.ecy.wa.gov>

You are so correct as I didn't find it under ?year or ?fiscal.year (or
?year.fiscal) (again in the US begins on 1 October) or even ?year.water.

On Wed, 16 Jun 2004, Prof Brian Ripley wrote:

> 
> For future reference, ?months (and ?weekdays? and ?quarters and ?julian)
> tells you.  It's hard to thing of a really obvious place to put this.
> 
> > 

-- 
Clint Bowman			INTERNET:	clint at ecy.wa.gov
Air Quality Modeler		INTERNET:	clint at math.utah.edu
Department of Ecology		VOICE:		(360) 407-6815
PO Box 47600			FAX:		(360) 407-7534
Olympia, WA 98504-7600



From edarmas at esalq.usp.br  Wed Jun 16 23:44:27 2004
From: edarmas at esalq.usp.br (Eduardo Dutra de Armas)
Date: Wed, 16 Jun 2004 18:44:27 -0300
Subject: [R] Resolution of plots
Message-ID: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAAlkN94tU7pE2294PPHpOIAsKAAAAQAAAA83fv+UECbkC6ZrS7GiA7MQEAAAAA@esalq.usp.br>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040616/e18ae0d3/attachment.pl

From ckjmaner at yahoo.com  Wed Jun 16 23:46:24 2004
From: ckjmaner at yahoo.com (Charles Maner)
Date: Wed, 16 Jun 2004 14:46:24 -0700 (PDT)
Subject: [R] erf function--answered
Message-ID: <20040616214624.30106.qmail@web40301.mail.yahoo.com>


Hi all.  Yep, I answered my question as I was able to
compare it to an actual erf table of values.  The
documentation is indeed correct.  So, kindly forgive
my ignorance as I retract the question.


Thanks,
Charles



From ripley at stats.ox.ac.uk  Wed Jun 16 23:46:55 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 16 Jun 2004 22:46:55 +0100 (BST)
Subject: [R] erf function documentation
In-Reply-To: <20040616211416.19252.qmail@web40308.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0406162235160.19571-100000@gannet.stats>

On Wed, 16 Jun 2004, Charles Maner wrote:

> 
> Hi all.  I may be wrong, (and often am), but in trying
> to determine how to calculate the erf function, the
> documentation for 'pnorm' states:
> 
> ## if you want the so-called 'error function'
> erf <- function(x) 2 * pnorm(x * sqrt(2)) - 1
> ## and the so-called 'complementary error function'
> erfc <- function(x) 2 * pnorm(x * sqrt(2),
> lower=FALSE)
> 
> Should, instead, it read:
> ## if you want the so-called 'error function'
> erf <- function(x) 2 * pnorm(x / sqrt(2)) - 1
> ## and the so-called 'complementary error function'
> erfc <- function(x) 2 * pnorm(x / sqrt(2),
> lower=FALSE)
> 
> I've looked at a couple references and they all show

Which are?  `Both', surely.

> that 'x' should be divided by, not multiplied by,
> 'sqrt(2)'.
> 
> Again, I may be incorrect.  If so, kindly let me know.
>  But, if I am correct, perhaps the documentation could
> be corrected in a subsequent R release.

According to Abramowitz and Stegun section 7.1, 

erf z = \frac{2}{\sqrt{pi} \int^z_0 e^_{-t^2} dt

Now pnorm(x) = \frac{1}{sqrt{2\pi}} \int^x_{-\infty} e^_{-u^2/2} du

so pnorm(x) - 1/2 = \frac{1}{sqrt{2\pi}} \int^x_0 e^_{-u^2/2} du

Now substitute t = u/sqrt{2}

pnorm(x) - 1/2 = \frac{1}{sqrt{2\pi}} \int^{x\sqrt{2}}_0 e^_{-t^2} dt\sqrt{2}

so pnorm(x) - 1/2 = 1/2 erf x\sqrt{2} or erf z = 2 pnorm(x/\sqrt{2} - 1

as you suggest.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Jun 16 23:51:55 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 16 Jun 2004 22:51:55 +0100 (BST)
Subject: [R] Resolution of plots
In-Reply-To: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAAlkN94tU7pE2294PPHpOIAsKAAAAQAAAA83fv+UECbkC6ZrS7GiA7MQEAAAAA@esalq.usp.br>
Message-ID: <Pine.LNX.4.44.0406162247250.19571-100000@gannet.stats>

You will have to tell us more.  Exporting how: to what format using what 
device and what exact command on what operating system?

The only device I know of that even knows about dpi is bitmap() and that
has no such limit unless imposed by your implementation of ghostscript.


On Wed, 16 Jun 2004, Eduardo Dutra de Armas wrote:

> Does anyone know how to increase the resolution of plots to be exported?
> I'm exporting some plots but the maximum resolution that I get is 96
> dpi. I want to get files with 300dpi.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Jun 17 00:00:50 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 16 Jun 2004 23:00:50 +0100 (BST)
Subject: [R] erf function documentation
In-Reply-To: <Pine.LNX.4.44.0406162235160.19571-100000@gannet.stats>
Message-ID: <Pine.LNX.4.44.0406162256150.19694-100000@gannet.stats>

On Wed, 16 Jun 2004, Prof Brian Ripley wrote:

Aargh don't press keys this late in the day or I'll get the wrong ones.

According to Abramowitz and Stegun section 7.1, 
 
erf z = \frac{2}{\sqrt{pi}} \int^z_0 e^_{-t^2} dt
 
Now pnorm(x) = \frac{1}{sqrt{2\pi}} \int^x_{-\infty} e^_{-u^2/2} du
 
so pnorm(x) - 1/2 = \frac{1}{sqrt{2\pi}} \int^x_0 e^_{-u^2/2} du
 
Now substitute t = u/sqrt{2}

pnorm(x) - 1/2 = \frac{1}{sqrt{2\pi}} \int^{x/\sqrt{2}}_0 e^_{-t^2} dt\sqrt{2}

so pnorm(x) - 1/2 = 1/2 erf(x/\sqrt{2}) or erf(z) = 2 pnorm(x\sqrt{2}) - 1

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dmurdoch at pair.com  Thu Jun 17 00:12:58 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Wed, 16 Jun 2004 18:12:58 -0400
Subject: [R] erf function documentation
In-Reply-To: <20040616211416.19252.qmail@web40308.mail.yahoo.com>
References: <20040616211416.19252.qmail@web40308.mail.yahoo.com>
Message-ID: <nme1d0tbrhlsn8gu17lrjnq2hu2fmri428@4ax.com>

On Wed, 16 Jun 2004 14:14:16 -0700 (PDT), Charles Maner
<ckjmaner at yahoo.com> wrote:

>Hi all.  I may be wrong, (and often am), but in trying
>to determine how to calculate the erf function, the
>documentation for 'pnorm' states:
>
>## if you want the so-called 'error function'
>erf <- function(x) 2 * pnorm(x * sqrt(2)) - 1
>## and the so-called 'complementary error function'
>erfc <- function(x) 2 * pnorm(x * sqrt(2),
>lower=FALSE)
>
>Should, instead, it read:
>## if you want the so-called 'error function'
>erf <- function(x) 2 * pnorm(x / sqrt(2)) - 1
>## and the so-called 'complementary error function'
>erfc <- function(x) 2 * pnorm(x / sqrt(2),
>lower=FALSE)
>
>I've looked at a couple references and they all show
>that 'x' should be divided by, not multiplied by,
>'sqrt(2)'.

The "Mathworld" web page at 

http://mathworld.wolfram.com/NormalDistributionFunction.html

shows

pnorm(x) - 1/2 = (1/2)*erf(x/sqrt(2))

(note that their Phi(x) = pnorm(x) - 1/2).  This agrees with the pnorm
documentation once you rearrange things.

Duncan Murdoch



From jemond at ucsd.edu  Thu Jun 17 00:13:22 2004
From: jemond at ucsd.edu (Jennifer Emond)
Date: Wed, 16 Jun 2004 15:13:22 -0700
Subject: [R] Using for (i in ...) command with function
Message-ID: <611ADBB5-BFE2-11D8-A40A-000A95CA4DA6@ucsd.edu>

Hi,

I have defined a function, and I want to apply this function for 
different values of i, yet, when I use a for command, it only
executes the function for the first value of i.

My functions are long, but here is an example.  The problem is if I run 
the following code:

tab<-function(data,x){
a.length<-length(data[,x])
return(a.length)
}

sum<-function(data,x){
b.sum<-sum(data[,x])
return(b.sum)
}

final <- function(data,c){
     if(is.factor(data[,x])==TRUE){
     tab(data,x)
   } else{
     sum(data,c)}
   }


Then try to run:
for (i in 1:10){
	final(data,i)
}

I get only the output for function for when i=1, and not for each value.

Thanks in advance,
Jenn


Jennifer A. Emond, MS
Statistician
Department of Biostatistics, UCSD
9500 Gilman Drive
M/C 0949
La Jolla, CA  92093-0949
(858) 622-5877
jemond at ucsd.edu



From A.Roux at hull.ac.uk  Thu Jun 17 01:09:52 2004
From: A.Roux at hull.ac.uk (Alet Roux)
Date: Thu, 17 Jun 2004 00:09:52 +0100
Subject: [R] Compiling C++ package source: linking problem?
In-Reply-To: <Pine.LNX.4.44.0406161655410.14482-100000@gannet.stats>
References: <Pine.LNX.4.44.0406161655410.14482-100000@gannet.stats>
Message-ID: <1087427392.40d0d34044610@webmail1.hull.ac.uk>

Dear Prof Ripley

libR.a seems to be created fine, as far as this layman can tell. Executing 

make libR.a

in C:\R\rw1090\src\gnuwin32 produces the single line

dlltools -k --as as  --dllname R.dll --def R.exp --output-lib libR.a

and the resulting file libR.a is 1583 Kb big. I just tried copying my c++ file
into C:\R\rw1090\src\gnuwin32, and executing R CMD SHLIB as below. It seems to
work (so libR.a must be fine), but is not an ideal solution, for obvious
reasons.

Now, compiling my second example below (it does in fact have an extern
"C"-wrapper), I typed

R CMD SHLIB montecarlo.cpp

in the appropriate directory, and obtained (slight alterations in order to make
the text fit).

making montecarlo.d from montecarlo.cpp
g++ --shared -s -o montecarlo.dll montecarlo.def montecarlo.a
    -LC:/R/rw1090/src/gnuwin32 -lg2c -lR
montecarlo.a(montecarlo.o.b)(.text+0x28):montecarlo.cpp: undefined reference 
    to 'Rf_allocVector(unsigned, int)'
montecarlo.a(montecarlo.o.b)(.text+0x33):montecarlo.cpp: undefined reference 
    to 'Rf_protect(SEXPREC*)'
montecarlo.a(montecarlo.o.b)(.text+0x3f):montecarlo.cpp: undefined reference 
    to 'Rf_unprotect(int)'
make: ** [montecarlo.dll] Error 1

To me, it seems that libR.a is simply ignored (I had the same impression with
Rdll.lib under MSVC++ 6.0; the linker found and read the file, but gave similar
errors as above).

With regards to the versions of software I'm using:

Rtools package -- downloaded two days ago
gcc -- 3.2.3 (mingw special 20030504-1)
perl -- 5.6.1 from activestate
no cygwin

Thank you very much in advance.

Alet

Quoting Prof Brian Ripley <ripley at stats.ox.ac.uk>:

> On Wed, 16 Jun 2004, Alet Roux wrote:
> 
> > Dear All
> > 
> > I'm currently developing a package for R (1.9.0) on Win32, with C++ source
> code.
> > Having followed the instructions in readme.packages, my code compiles fine
> with
> > R CMD SHLIB (as well as R CMD check) ... until I start using the internal
> R
> > functions. 
> > 
> > (Interesting: Rprintf seems to be the exception.) For instance, the
> following
> > code compiles fine:
> > 
> > #include <R.h>
> > #include <Rdefines.h>
> > ...
> > SEXP whatever (SEXP model)
> > {
> > 	Rprintf ("Hello, here I am!\n");
> >         return model;
> > }
> > 
> > However, the following doesn't compile at all:
> > 
> > #include <R.h>
> > #include <Rdefines.h>
> > ...
> > SEXP whatever (SEXP model)
> > {
> > 	SEXP anotherModel;
> > 	PROTECT(anotherModel = NEW_NUMERIC(4));
> > 	UNPROTECT(1);
> >         return anotherModel;
> > }
> > 
> > An example of compiler feedback from R CMD SHLIB (I have mingw 3.1.0):
> > ...: undefined reference to 'Rf_allocVector(unsigned,int)'
> > ...: undefined reference to 'Rf_protect(SEXPREC*)'
> > ...: undefined reference to 'Rf_unprotect(int)'
> > 
> > Can anybody tell me what the matter is? Did I miss something?
> 
> Are you linking against the R.dll via its import library libR.a?  Did the 
> latter get made correctly?  That is where those entry points are.
> 
> More complete compiler output would have helped here.
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> 


-------------------------------------
Alet Roux
Department of Mathematics
University of Hull
Kingston upon Hull
HU6 7RX
United Kingdom
URL: http://www.hull.ac.uk/php/mapar/
Tel: +44 (1482) 466463
Fax: +44 (1482) 466218



From ripley at stats.ox.ac.uk  Thu Jun 17 01:14:42 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 17 Jun 2004 00:14:42 +0100 (BST)
Subject: [R] Using for (i in ...) command with function
In-Reply-To: <611ADBB5-BFE2-11D8-A40A-000A95CA4DA6@ucsd.edu>
Message-ID: <Pine.LNX.4.44.0406170004330.19940-100000@gannet.stats>

Might this be the issue:

> for(i in 1:10) {i}
>

? Note, no output.  If you want to print the result of final(data, i) ina
for() loop you need to do so explicitly.

A second issue is that your example code defines sum() in terms of sum() 
and so is recursive.  You should get the error message

Error in sum(data[, x]) : evaluation nested too deeply: infinite recursion 
/ options(expression=)?

At least I do with your code below.


On Wed, 16 Jun 2004, Jennifer Emond wrote:

> Hi,
> 
> I have defined a function, and I want to apply this function for 
> different values of i, yet, when I use a for command, it only
> executes the function for the first value of i.
> 
> My functions are long, but here is an example.  The problem is if I run 
> the following code:
> 
> tab<-function(data,x){
> a.length<-length(data[,x])
> return(a.length)
> }
> 
> sum<-function(data,x){
> b.sum<-sum(data[,x])
> return(b.sum)
> }
> 
> final <- function(data,c){
>      if(is.factor(data[,x])==TRUE){
>      tab(data,x)
>    } else{
>      sum(data,c)}
>    }
> 
> 
> Then try to run:
> for (i in 1:10){
> 	final(data,i)
> }
> 
> I get only the output for function for when i=1, and not for each value.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From notulei at yahoo.com  Thu Jun 17 01:17:25 2004
From: notulei at yahoo.com (Alex Nu)
Date: Wed, 16 Jun 2004 16:17:25 -0700 (PDT)
Subject: [R] printing R generated postcript files
In-Reply-To: <40D09BE3.5000908@jhsph.edu>
Message-ID: <20040616231725.17437.qmail@web60103.mail.yahoo.com>

 
 getOption("papersize")  resulted in A4

 I tried to set it to letter  modifying
 /etc/R/Renviron

 as suggested by Dirk,

R_PAPERSIZE=${R_PAPERSIZE-$(cat /etc/papersize)}

 but this is what I get when trying to generate
 a postscript file

Error in postscript("times.ps") : invalid page type
``cat /etc/papersize`' (postscript)

Meanwhile I manually  set to letter.

 thanks

Alex



From ripley at stats.ox.ac.uk  Thu Jun 17 01:29:36 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 17 Jun 2004 00:29:36 +0100 (BST)
Subject: [R] Compiling C++ package source: linking problem?
In-Reply-To: <1087427392.40d0d34044610@webmail1.hull.ac.uk>
Message-ID: <Pine.LNX.4.44.0406170015331.19940-100000@gannet.stats>

On Thu, 17 Jun 2004, Alet Roux wrote:

> Dear Prof Ripley
> 
> libR.a seems to be created fine, as far as this layman can tell. Executing 
> 
> make libR.a
> 
> in C:\R\rw1090\src\gnuwin32 produces the single line
> 
> dlltools -k --as as  --dllname R.dll --def R.exp --output-lib libR.a
> 
> and the resulting file libR.a is 1583 Kb big. I just tried copying my
> c++ file into C:\R\rw1090\src\gnuwin32, and executing R CMD SHLIB as
> below. It seems to work (so libR.a must be fine), but is not an ideal
> solution, for obvious reasons.

You should not be working in C:\R\rw1090\src\gnuwin32, at least not in a 
system compiled there.

You can find out via nm -pg libR.a | grep Rf_allocVector

> Now, compiling my second example below (it does in fact have an extern
> "C"-wrapper), I typed
> 
> R CMD SHLIB montecarlo.cpp
> 
> in the appropriate directory, and obtained (slight alterations in order to make
> the text fit).
> 
> making montecarlo.d from montecarlo.cpp
> g++ --shared -s -o montecarlo.dll montecarlo.def montecarlo.a
>     -LC:/R/rw1090/src/gnuwin32 -lg2c -lR
> montecarlo.a(montecarlo.o.b)(.text+0x28):montecarlo.cpp: undefined reference 
>     to 'Rf_allocVector(unsigned, int)'
> montecarlo.a(montecarlo.o.b)(.text+0x33):montecarlo.cpp: undefined reference 
>     to 'Rf_protect(SEXPREC*)'
> montecarlo.a(montecarlo.o.b)(.text+0x3f):montecarlo.cpp: undefined reference 
>     to 'Rf_unprotect(int)'
> make: ** [montecarlo.dll] Error 1
> 
> To me, it seems that libR.a is simply ignored (I had the same impression with
> Rdll.lib under MSVC++ 6.0; the linker found and read the file, but gave similar
> errors as above).

You appear to have C++ entry points, not C ones (note that arg sequences).
Again, look at the actual compiled code with nm to see what the symbol 
names are.  Using your code below

[c:/R/rw1091/src/gnuwin32/tmp]% nm -g montecarlo.o
         U __Z10Rf_protectP7SEXPREC
         U __Z12Rf_unprotecti
         U __Z14Rf_allocVectorji
00000000 T __Z8whateverP7SEXPREC

but if I use the correct inclusion of C headers

extern "C" {
#include <R.h>
#include <Rdefines.h>
}

SEXP whatever (SEXP model)
{
   SEXP anotherModel;
   PROTECT(anotherModel = NEW_NUMERIC(4));
   UNPROTECT(1);
   return anotherModel;
}

it works.  So the problem seems to be your using C headers as if they 
were C++ headers, and nothing to do with libR.a.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at myway.com  Thu Jun 17 02:53:24 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 16 Jun 2004 20:53:24 -0400 (EDT)
Subject: [R] Aggregating on Water Year Rather Than Calendar Year
Message-ID: <20040617005324.C631C397A@mprdmxin.myway.com>



Try this to get the US water year:

require(chron)
with(month.day.year(unclass(as.Date(w$date.time))), year+(month>9))

Date:   Wed, 16 Jun 2004 11:21:56 -0700 (PDT) 
From:   Clint Bowman <clint at ecy.wa.gov>
To:   <r-help at stat.math.ethz.ch> 
Subject:   [R] Aggregating on Water Year Rather Than Calendar Year 

 
The US water year extends from 01 October yyyy-1 through 30 September yyyy
and is referenced by the year starting on the included 01 January yyyy. 
I'd like to be able to find the annual means for the water year. To do so 
I've taken the input date-time, which is in the usual format 

"1991-10-07 10:35:00"

changed it by:

w$d<-as.POSIXct(w$date.time)

Now I can add an offset of 92 days 

w$w.year<-as.POSIXct(w$d+7948800)

and have the years correspond to the "water year."

Now I wish to obtain some means by something like:

waterT<-aggregate(w$value[w$param.name=="Temperature"],
list(w$w.year[w$param.name=="Temperature"]),mean)

Except that I need to work on the year and being a neophyte in date 
arithmetic I'm not finding a working method.

TIA



From edd at debian.org  Thu Jun 17 03:25:20 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Wed, 16 Jun 2004 20:25:20 -0500
Subject: [R] printing R generated postcript files
In-Reply-To: <20040616231725.17437.qmail@web60103.mail.yahoo.com>
References: <40D09BE3.5000908@jhsph.edu>
	<20040616231725.17437.qmail@web60103.mail.yahoo.com>
Message-ID: <20040617012519.GA19987@sonny.eddelbuettel.com>

On Wed, Jun 16, 2004 at 04:17:25PM -0700, Alex Nu wrote:
>  
>  getOption("papersize")  resulted in A4
> 
>  I tried to set it to letter  modifying
>  /etc/R/Renviron
> 
>  as suggested by Dirk,
> 
> R_PAPERSIZE=${R_PAPERSIZE-$(cat /etc/papersize)}
> 
>  but this is what I get when trying to generate
>  a postscript file
> 
> Error in postscript("times.ps") : invalid page type
> ``cat /etc/papersize`' (postscript)

Confirmed. That suggestion I had gotten needed more testing than I gave it.
Doug once adjusted this from postinst, maybe I'll try the same again.

> Meanwhile I manually  set to letter.

Good to know.

Regards, Dirk

-- 
FEATURE:  VW Beetle license plate in California



From morenzp at rogers.com  Thu Jun 17 04:12:49 2004
From: morenzp at rogers.com (Phil)
Date: Wed, 16 Jun 2004 22:12:49 -0400
Subject: [R] Tutorial for graphics
References: <200406161006.i5GA3HSG004990@hypatia.math.ethz.ch>
Message-ID: <002901c45410$96a7b640$5f87fea9@bloor.phub.net.cable.rogers.com>

Hi all,

I'm coming to R from Matlab and I'm finding it difficult to find a good
introduction to graphics in R.  (The best I've found so far is Ch. 4 of
Emmanuel Paradis "R for Beginners".  Still, I have been unable to discover
simple things like how to resize the axes on an existing plot, how to add
(or change) axis labels on an existing plot, etc.  Can anyone point me to a
suitable tutorial, or even tell me how to perform those tasks?

Also, Matlab's graphical widget has the ability to zoom (and unzoom) by
drawing a rectangle on the graph with the mouse.  Is there anything similar
in R?

Thanks in advance,

Phil Morenz



From jmacdon at med.umich.edu  Thu Jun 17 04:12:48 2004
From: jmacdon at med.umich.edu (James MacDonald)
Date: Wed, 16 Jun 2004 22:12:48 -0400
Subject: [R] Tutorial for graphics
Message-ID: <s0d0c5f7.056@med-gwia-02a.med.umich.edu>

I don't know of a good tutorial, but I find that looking at ?par is
usually good for answering most graphing questions.

To resize axes, look at xlim and ylim under ?par. As for changing the
axes, you can suppress the axes using e.g., for the x-axis xaxt="n" in
your plotting function call, and then add your own axis using the axis()
function (see ?axis).

HTH,

Jim



James W. MacDonald
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623
>>> "Phil" <morenzp at rogers.com> 06/16/04 10:12 PM >>>
Hi all,

I'm coming to R from Matlab and I'm finding it difficult to find a good
introduction to graphics in R.  (The best I've found so far is Ch. 4 of
Emmanuel Paradis "R for Beginners".  Still, I have been unable to
discover
simple things like how to resize the axes on an existing plot, how to
add
(or change) axis labels on an existing plot, etc.  Can anyone point me
to a
suitable tutorial, or even tell me how to perform those tasks?

Also, Matlab's graphical widget has the ability to zoom (and unzoom) by
drawing a rectangle on the graph with the mouse.  Is there anything
similar
in R?

Thanks in advance,

Phil Morenz

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From jinss at hkusua.hku.hk  Thu Jun 17 05:04:39 2004
From: jinss at hkusua.hku.hk (Jin Shusong)
Date: Thu, 17 Jun 2004 11:04:39 +0800
Subject: [R] Tutorial for graphics
In-Reply-To: <002901c45410$96a7b640$5f87fea9@bloor.phub.net.cable.rogers.com>
References: <200406161006.i5GA3HSG004990@hypatia.math.ethz.ch>
	<002901c45410$96a7b640$5f87fea9@bloor.phub.net.cable.rogers.com>
Message-ID: <20040617030439.GA4508@S77.hku.hk>

On Wed, Jun 16, 2004 at 10:12:49PM -0400, Phil wrote:
> Hi all,
> 
> I'm coming to R from Matlab and I'm finding it difficult to find a good
> introduction to graphics in R.  (The best I've found so far is Ch. 4 of
> Emmanuel Paradis "R for Beginners".  Still, I have been unable to discover
> simple things like how to resize the axes on an existing plot, how to add
> (or change) axis labels on an existing plot, etc.  Can anyone point me to a
> suitable tutorial, or even tell me how to perform those tasks?
> 
> Also, Matlab's graphical widget has the ability to zoom (and unzoom) by
> drawing a rectangle on the graph with the mouse.  Is there anything similar
> in R?
> 
> Thanks in advance,
> 
> Phil Morenz
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
Dear Phil,

  I think that the official document "an introduction to R"
is quite good for beginners.  In chapter 12, there are many
details to show how to handle the graphics.  

  You can also find that R is a command driven software and the
ability of mouse is weak.
-- 
  Jin



From christian_mora at vtr.net  Thu Jun 17 05:25:52 2004
From: christian_mora at vtr.net (Christian Mora)
Date: Wed, 16 Jun 2004 23:25:52 -0400
Subject: [R] Tutorial for graphics
In-Reply-To: <002901c45410$96a7b640$5f87fea9@bloor.phub.net.cable.rogers.com>
Message-ID: <000001c4541a$cb5ae0b0$393d68c8@CPQ26719243321>

A fairly good tutorial is "Gr??ficos Estad??sticos con R" by Juan Carlos
Correa and Nelfi Gonz??lez (PDF [2110kB]) that you can find in
Documentation/Contributed (in Spanish though). 

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Phil
Sent: Wednesday, June 16, 2004 10:13 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Tutorial for graphics


Hi all,

I'm coming to R from Matlab and I'm finding it difficult to find a good
introduction to graphics in R.  (The best I've found so far is Ch. 4 of
Emmanuel Paradis "R for Beginners".  Still, I have been unable to
discover simple things like how to resize the axes on an existing plot,
how to add (or change) axis labels on an existing plot, etc.  Can anyone
point me to a suitable tutorial, or even tell me how to perform those
tasks?

Also, Matlab's graphical widget has the ability to zoom (and unzoom) by
drawing a rectangle on the graph with the mouse.  Is there anything
similar in R?

Thanks in advance,

Phil Morenz

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Thu Jun 17 06:32:35 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 17 Jun 2004 00:32:35 -0400 (EDT)
Subject: [R] Aggregating on Water Year Rather Than Calendar Year
Message-ID: <20040617043235.C7943394A@mprdmxin.myway.com>



And here is a slightly shorter variant:

   with(as.POSIXlt(w$date.time), 1900+year+(mon>8) )

---
Gabor Grothendieck <ggrothendieck at myway.com>:

Try this to get the US water year:

require(chron)
with(month.day.year(unclass(as.Date(w$date.time))), year+(month>9))

Date: Wed, 16 Jun 2004 11:21:56 -0700 (PDT) 
From: Clint Bowman <clint at ecy.wa.gov>
To: <r-help at stat.math.ethz.ch> 
Subject: [R] Aggregating on Water Year Rather Than Calendar Year 


The US water year extends from 01 October yyyy-1 through 30 September yyyy
and is referenced by the year starting on the included 01 January yyyy. 
I'd like to be able to find the annual means for the water year. To do so 
I've taken the input date-time, which is in the usual format 

"1991-10-07 10:35:00"

changed it by:

w$d<-as.POSIXct(w$date.time)

Now I can add an offset of 92 days 

w$w.year<-as.POSIXct(w$d+7948800)

and have the years correspond to the "water year."

Now I wish to obtain some means by something like:

waterT<-aggregate(w$value[w$param.name=="Temperature"],
list(w$w.year[w$param.name=="Temperature"]),mean)

Except that I need to work on the year and being a neophyte in date 
arithmetic I'm not finding a working method.

TIA



From tpapp at axelero.hu  Thu Jun 17 08:40:13 2004
From: tpapp at axelero.hu (Tamas Papp)
Date: Thu, 17 Jun 2004 08:40:13 +0200
Subject: [R] nonlinear modeling with rational functions?
In-Reply-To: <40D06727.2060401@pdf.com>
References: <40D06727.2060401@pdf.com>
Message-ID: <20040617064013.GA1396@localhost>

On Wed, Jun 16, 2004 at 10:28:39AM -0500, Spencer Graves wrote:

>      Rational functions (ratios of polynomials) often provide good 
> approximations to many functions.  Does anyone know of any literature on 
> nonlinear modeling with rational functions, sequential estimation, 
> diagnostics, etc.?  I know I can do it with "nls" and other nonlinear 
> regression functions, but I'm wondering what literature might exist 
> discussing how a search for an appropriate rational approximation? 

The book

@Book{judd98,
  author =       {Judd, Kenneth L},
  title =        {Numerical methods in economics},
  publisher =    {MIT Press},
  year =         1998
}

provides some information about constructing rational approximations
(aka Pad?? approximations) for functions with known algebraic forms,
but not for the estimation from data.

However, the text has some references, in particular

Cuyt, A and L Wuytack. 1986. Nonlinear Numerical Methods: Theory and
Practice. Amsterdam: North-Holland

which you might find helpful.

Best,

Tamas

-- 
Tam??s K. Papp
E-mail: tpapp at axelero.hu
Please try to send only (latin-2) plain text, not HTML or other garbage.



From laura at env.leeds.ac.uk  Thu Jun 17 09:02:40 2004
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Thu, 17 Jun 2004 08:02:40 +0100 (BST)
Subject: [R] PCA and AR on circular data
Message-ID: <Pine.LNX.4.44.0406170759520.15904-100000@env-pc-phd13>

Could anyone advise the best method of performing PCA and/or AR modelling
on circular data? Until now I have been working with vectors
(speed*direction),  but am now wanting to look purely at the directional
aspect.

Thanks in advance..
Laura



From k.wang at auckland.ac.nz  Thu Jun 17 09:57:39 2004
From: k.wang at auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Thu, 17 Jun 2004 19:57:39 +1200
Subject: [R] Date Calculation
Message-ID: <20040617075643.DOTH24839.mta3-rme.xtra.co.nz@kevinlpt>

Hi,

I've been playing with:
> joinDate <- format(strptime(as.vector(forum[,2]), "%d-%b-%y"),
+                    "%d-%b-%Y")
> today <- format(strptime(as.vector("14-Jun-04"), "%d-%b-%Y"),
+                 "%d-%b-%Y")
> joinDate
 [1] "04-Feb-2004" "13-Feb-2004" "26-Feb-2004" "27-Feb-2004"
"27-Feb-2004"
 [6] "27-Feb-2004" "29-Feb-2004" "01-Mar-2004" "02-Mar-2004"
"07-Mar-2004"
[11] "08-Mar-2004" "17-Mar-2004" "20-Mar-2004" "22-Mar-2004"
"22-Mar-2004"
[16] "23-Mar-2004" "23-Mar-2004" "24-Mar-2004" "01-Apr-2004"
"01-Apr-2004"
[21] "01-Apr-2004" "01-Apr-2004" "02-Apr-2004" "06-Apr-2004"
"09-Apr-2004"
[26] "11-Apr-2004" "14-Apr-2004" "03-May-2004" "04-May-2004"
"30-May-2004"
[31] "01-Jun-2004" "10-Jun-2004" "14-Jun-2004" "17-Jun-2004"
"17-Jun-2004"
> today
[1] "14-Jun-0004"
> joinDate - today
Error in joinDate - today : non-numeric argument to binary operator

But it didn't quite work.  What I'd like joinDate - today to return is
the number of days to today, since joinDate.  I'm sure it has been asked
before however a search on r-help didn't found me any relevant
information *_*.

Cheers,

Kevin

--------------------------------------------
Ko-Kang Kevin Wang, MSc(Hon)
SLC Stats Workshops Co-ordinator
The University of Auckland
New Zealand



From P.Lemmens at nici.kun.nl  Thu Jun 17 10:03:24 2004
From: P.Lemmens at nici.kun.nl (Paul Lemmens)
Date: Thu, 17 Jun 2004 10:03:24 +0200
Subject: [R] subset(..., drop=TRUE) doesn't seem to work.
In-Reply-To: <x2k6y7jz1k.fsf@biostat.ku.dk>
References: <DCF7E268195AB308453EFAA8@lemmens.socsci.kun.nl>
	<x2pt7zk0el.fsf@biostat.ku.dk>
	<11677237B5977C39793F1D7F@lemmens.socsci.kun.nl>
	<x2k6y7jz1k.fsf@biostat.ku.dk>
Message-ID: <1B1DAC9FF213B3CEBA826CB4@lemmens.socsci.kun.nl>

Hoi Peter,

--On woensdag 16 juni 2004 17:35 +0200 Peter Dalgaard 
<p.dalgaard at biostat.ku.dk> wrote:
> Anyways, the way out is
>
>  d2 <- subset(dd,c==1)
>  ifac <- sapply(dd,is.factor)
>  d2[ifac] <- lapply(d2[ifac],factor)
>
> or
>
>  d2 <- subset(dd,c==1)
>  d2[] <- lapply(d2, function(x) if (is.factor(x)) factor(x) else x)
>
My toolbox.r now contains:

my.subset <- function(x, drop.unused.levels=FALSE, ...) {
  subsetted <- subset(x, ...)

  if (drop.unused.levels) {
    subsetted[] <- lapply(subsetted,
                      function(x) if (is.factor(x)) factor(x) else x)
  }

  subsetted
}


Thnx for all the help!



-- 
Paul Lemmens
NICI, University of Nijmegen              ASCII Ribbon Campaign /"\
Montessorilaan 3 (B.01.05)                    Against HTML Mail \ /
NL-6525 HR Nijmegen                                              X
The Netherlands                                                 / \
Phonenumber    +31-24-3612648
Fax            +31-24-3616066



From maechler at stat.math.ethz.ch  Thu Jun 17 10:04:12 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 17 Jun 2004 10:04:12 +0200
Subject: [R] nonlinear modeling with rational functions?
In-Reply-To: <20040617064013.GA1396@localhost>
References: <40D06727.2060401@pdf.com>
	<20040617064013.GA1396@localhost>
Message-ID: <16593.20604.18219.639897@gargle.gargle.HOWL>

>>>>> "Tamas" == Tamas Papp <tpapp at axelero.hu>
>>>>>     on Thu, 17 Jun 2004 08:40:13 +0200 writes:

    Tamas> On Wed, Jun 16, 2004 at 10:28:39AM -0500, Spencer Graves wrote:
    >> Rational functions (ratios of polynomials) often provide good 
    >> approximations to many functions.  Does anyone know of any literature on 
    >> nonlinear modeling with rational functions, sequential estimation, 
    >> diagnostics, etc.?  I know I can do it with "nls" and other nonlinear 
    >> regression functions, but I'm wondering what literature might exist 
    >> discussing how a search for an appropriate rational approximation? 

    Tamas> The book

    Tamas> @Book{judd98,
    Tamas>  author =       {Judd, Kenneth L},
    Tamas>  title =        {Numerical methods in economics},
    Tamas>  publisher =    {MIT Press},
    Tamas>  year =         1998
    Tamas> }

    Tamas> provides some information about constructing rational approximations
    Tamas> (aka Pad?? approximations) for functions with known algebraic forms,
    Tamas> but not for the estimation from data.

    Tamas> However, the text has some references, in particular

    Tamas> Cuyt, A and L Wuytack. 1986. Nonlinear Numerical Methods: Theory and
    Tamas> Practice. Amsterdam: North-Holland

    Tamas> which you might find helpful.

But be aware:  
In these contexts, one is interested in  L_inf() norm
approximations, not in L_2 or even L_1 as we are in statistics.

L_inf aka "sup-norm" aka "minimax" or "worst case" approximation
makes much sense in the context of numerical approximation:
e.g. you might want a Pad?? approximation of the sin() function with
a maximal (absolute) error of 1e-12 to be implemented for
your pocket calculator.

OTOH, the sup-norm is quite a bad idea for data fitting, since
there, errors are typically either normal or more heavy tailed,
i.e. if using an L_p norm, it should be p <= 2.

Hoping this helps even more,
Martin Maechler



From maechler at stat.math.ethz.ch  Thu Jun 17 10:27:02 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 17 Jun 2004 10:27:02 +0200
Subject: [R] Date Calculation
In-Reply-To: <20040617075643.DOTH24839.mta3-rme.xtra.co.nz@kevinlpt>
References: <20040617075643.DOTH24839.mta3-rme.xtra.co.nz@kevinlpt>
Message-ID: <16593.21974.960323.947172@gargle.gargle.HOWL>

>>>>> "KKWa" == Ko-Kang Kevin Wang <k.wang at auckland.ac.nz>
>>>>>     on Thu, 17 Jun 2004 19:57:39 +1200 writes:

    KKWa> Hi,
    KKWa> I've been playing with:
    >> joinDate <- format(strptime(as.vector(forum[,2]), "%d-%b-%y"),
    KKWa> +                    "%d-%b-%Y")
    >> today <- format(strptime(as.vector("14-Jun-04"), "%d-%b-%Y"),
    KKWa> +                 "%d-%b-%Y")
    >> joinDate
    KKWa> [1] "04-Feb-2004" "13-Feb-2004" "26-Feb-2004" "27-Feb-2004"
    KKWa> "27-Feb-2004"
    KKWa> [6] "27-Feb-2004" "29-Feb-2004" "01-Mar-2004" "02-Mar-2004"
    KKWa> "07-Mar-2004"
    KKWa> ................................
    >> today
    KKWa> [1] "14-Jun-0004"
    >> joinDate - today
    KKWa> Error in joinDate - today : non-numeric argument to binary operator

    KKWa> But it didn't quite work.  What I'd like joinDate -
    KKWa> today to return is the number of days to today, since
    KKWa> joinDate.  I'm sure it has been asked before however a
    KKWa> search on r-help didn't found me any relevant
    KKWa> information *_*.

Kevin, 
       [[did you have tough day? usually your Q/A are much better ;-()]]

both joinDate and today are results of format() and hence
character vectors (with no time information left).  
You didn't really expect  "-" to work with characters?

OTOH,  "-" properly does work with POSIX*t objects,
see, e.g.,  ?difftime

Regards,
Martin



From ripley at stats.ox.ac.uk  Thu Jun 17 10:26:57 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 17 Jun 2004 09:26:57 +0100 (BST)
Subject: [R] Date Calculation
In-Reply-To: <20040617075643.DOTH24839.mta3-rme.xtra.co.nz@kevinlpt>
Message-ID: <Pine.LNX.4.44.0406170925340.7061-100000@gannet.stats>

You have formatted the dates, so you are trying to subtract character 
strings.

Convert to Date instead (although you can subtract POSIXlt dates).

On Thu, 17 Jun 2004, Ko-Kang Kevin Wang wrote:

> Hi,
> 
> I've been playing with:
> > joinDate <- format(strptime(as.vector(forum[,2]), "%d-%b-%y"),
> +                    "%d-%b-%Y")
> > today <- format(strptime(as.vector("14-Jun-04"), "%d-%b-%Y"),
> +                 "%d-%b-%Y")
> > joinDate
>  [1] "04-Feb-2004" "13-Feb-2004" "26-Feb-2004" "27-Feb-2004"
> "27-Feb-2004"
>  [6] "27-Feb-2004" "29-Feb-2004" "01-Mar-2004" "02-Mar-2004"
> "07-Mar-2004"
> [11] "08-Mar-2004" "17-Mar-2004" "20-Mar-2004" "22-Mar-2004"
> "22-Mar-2004"
> [16] "23-Mar-2004" "23-Mar-2004" "24-Mar-2004" "01-Apr-2004"
> "01-Apr-2004"
> [21] "01-Apr-2004" "01-Apr-2004" "02-Apr-2004" "06-Apr-2004"
> "09-Apr-2004"
> [26] "11-Apr-2004" "14-Apr-2004" "03-May-2004" "04-May-2004"
> "30-May-2004"
> [31] "01-Jun-2004" "10-Jun-2004" "14-Jun-2004" "17-Jun-2004"
> "17-Jun-2004"
> > today
> [1] "14-Jun-0004"
> > joinDate - today
> Error in joinDate - today : non-numeric argument to binary operator
> 
> But it didn't quite work.  What I'd like joinDate - today to return is
> the number of days to today, since joinDate.  I'm sure it has been asked
> before however a search on r-help didn't found me any relevant
> information *_*.
> 
> Cheers,
> 
> Kevin
> 
> --------------------------------------------
> Ko-Kang Kevin Wang, MSc(Hon)
> SLC Stats Workshops Co-ordinator
> The University of Auckland
> New Zealand
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Timur.Elzhov at jinr.ru  Thu Jun 17 10:32:29 2004
From: Timur.Elzhov at jinr.ru (Timur Elzhov)
Date: Thu, 17 Jun 2004 12:32:29 +0400
Subject: [R] Tutorial for graphics
In-Reply-To: <002901c45410$96a7b640$5f87fea9@bloor.phub.net.cable.rogers.com>
References: <200406161006.i5GA3HSG004990@hypatia.math.ethz.ch>
	<002901c45410$96a7b640$5f87fea9@bloor.phub.net.cable.rogers.com>
Message-ID: <20040617083229.GA625@nf034.jinr.ru>

On Wed, Jun 16, 2004 at 10:12:49PM -0400, Phil wrote:

> I'm coming to R from Matlab and I'm finding it difficult to find a good
> introduction to graphics in R.  (The best I've found so far is Ch. 4 of
> Emmanuel Paradis "R for Beginners".  Still, I have been unable to discover
> simple things like how to resize the axes on an existing plot

I'm afraid it's not possible to resize existing axis, by mouse. But if
you want to add the another plot with differnet scale, you could do it
this way:
  plot(...)        # you normally plot 1st graph
  par(new = TRUE)  # 2nd graph won't clean the frame
  plot(..., axes = FALSE, xlab = "", ylab = "")  # plotting 2ng graph
  axis(4)



> how to add (or change) axis labels on an existing plot, etc.

You can plot without axes plot(..., axes = FALSE), then add axes, with
labels or not, ticks with any length, and so on.

> Can anyone point me to a suitable tutorial, or even tell me how to
> perform those tasks?

  help(plot.default)
  help(par)

> Also, Matlab's graphical widget has the ability to zoom (and unzoom) by
> drawing a rectangle on the graph with the mouse.  Is there anything similar
> in R?

Probably there are R packages perfoming such a thing, but I do not use
R interactively at all. I change xlim() and/or ylim() in R script, and
source() it again :)


--
WBR,
Timur



From k.wang at auckland.ac.nz  Thu Jun 17 10:41:46 2004
From: k.wang at auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Thu, 17 Jun 2004 20:41:46 +1200
Subject: [R] Date Calculation
In-Reply-To: <16593.21974.960323.947172@gargle.gargle.HOWL>
Message-ID: <20040617084048.EJNW24839.mta3-rme.xtra.co.nz@kevinlpt>

Hi,

> -----Original Message-----
> From: Martin Maechler [mailto:maechler at stat.math.ethz.ch]
> Kevin,
>        [[did you have tough day? usually your Q/A are much
> better ;-()]]

Thanks to those who have replied, and yes shame on me......

[I also realised I can just use Sys.Date() to get today's date,
instead of typing it in......I really had a tough day *_*]

Cheers,

Kevin



From meinhardploner at gmx.net  Thu Jun 17 11:26:18 2004
From: meinhardploner at gmx.net (Meinhard Ploner)
Date: Thu, 17 Jun 2004 11:26:18 +0200
Subject: [R] packages & data-sets
Message-ID: <6347B676-C040-11D8-AA4E-000A95BB5DE0@gmx.net>


It's possible to create a package with functions and data,
from which the use

library(pkg-name)

"attaches" not only the functions, but also the data?
I want avoid to use

data(dataset, package="name")

because this makes a global copy of the data-set ...

Anyone could help me?

Meinhard



From ripley at stats.ox.ac.uk  Thu Jun 17 12:15:01 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 17 Jun 2004 11:15:01 +0100 (BST)
Subject: [R] packages & data-sets
In-Reply-To: <6347B676-C040-11D8-AA4E-000A95BB5DE0@gmx.net>
Message-ID: <Pine.LNX.4.44.0406171109570.7154-100000@gannet.stats>

On Thu, 17 Jun 2004, Meinhard Ploner wrote:

> It's possible to create a package with functions and data,
> from which the use
> 
> library(pkg-name)
> 
> "attaches" not only the functions, but also the data?
> I want avoid to use
> 
> data(dataset, package="name")
> 
> because this makes a global copy of the data-set ...

What do you mean by `global'?  You cannot have access to R data without
having it in memory and having it visible.  You don't need to have it in
the user workspace (.GlobalEnv), though.  Suggestions:

1) See how MASS does it.  Data objects are loaded into the MASS namespace 
on first use.

2) See ?attach and attach an R save file containing your datasets -- 
wasteful unless you use them all at once.

It is planned that data() will be superseded by a better mechanism in R 
2.0.0 (but that plan has slipped a bit already, which is why MASS does it 
differently).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From kuroki at oak.dti.ne.jp  Thu Jun 17 13:00:51 2004
From: kuroki at oak.dti.ne.jp (Chihiro Kuroki)
Date: Thu, 17 Jun 2004 20:00:51 +0900
Subject: [R] Question : simtest result
In-Reply-To: <Pine.LNX.4.51.0406160801200.26629@artemis.imbe.med.uni-erlangen.de>
References: <87vfirzwhq.wl@oak.dti.ne.jp>
	<Pine.LNX.4.51.0405231138160.19910@artemis.imbe.med.uni-erlangen.de>
	<87oeobimv3.wl@oak.dti.ne.jp>
	<Pine.LNX.4.51.0406160801200.26629@artemis.imbe.med.uni-erlangen.de>
Message-ID: <873c4uqwik.wl@oak.dti.ne.jp>

Dear Mr.Torsten:

At Wed, 16 Jun 2004 08:04:08 +0200 (CEST),
Torsten Hothorn wrote:
> > Call:
> > simtest.formula(formula = y4 ~ f2, data = dat2, type = "Dunnett")
> >
> > 	 Dunnett contrasts for factor f2
> >
> > Contrast matrix:
> >           f21 f22 f23 f24 f25
> > f22-f21 0  -1   1   0   0   0
> > f23-f21 0  -1   0   1   0   0
> > f24-f21 0  -1   0   0   1   0
> > f25-f21 0  -1   0   0   0   1
> >
> >
> > Absolute Error Tolerance:  0.001
> >
> > Coefficients:
> >         Estimate t value Std.Err. p raw p Bonf p adj
> > f25-f21    5.167  -4.644    1.022 0.000  0.000 0.000
> > f23-f21    2.875  -2.813    1.022 0.008  0.024 0.022
> > f24-f21    2.625  -2.569    1.022 0.015  0.029 0.028
> > f22-f21    2.125  -2.079    1.113 0.045  0.045 0.045
> > ---------------------------------
> 
> Chihiro,
> 
> Frank and I used your data to check the program and example with an
> independent
> algorithm and implementation (Westfall-Young stepdown resampling
> procedure). Theory suggests that the
> results should be similar to (but not necessarily the same as) those
> obtained with multcomp in this special case. These are the adjusted
> p-values obtained with the Westfall-Young approach for 100,000
> replications:
> 
> which fit nicely with the ones obtained from multcomp.
> 
> Hope this helps & sorry for the delay,

Thank you.

At Mon, 07 Jun 2004 12:14:26 +0900,
myself-oak wrote:
> dunnett(dat2,1,2)
> rho=0.426
> group:5 t=4.644 p=0.000
> group:3 t=2.813 p=0.028
> group:4 t=2.569 p=0.051 --- (B)
> group:2 t=2.079 p=0.145
> (sorted in order of p values.)
> 
> p values are different although t values are equal.
> 
> > > I got the following inequality from the appended chart of a
> > > book.
> > >
> > 
> > hm, without knowing what
> > 
> > > 2.558 < d(5, 35, 0.4263464, 0.05) < 2.598
> > 
> > means it is hard to tell what the problem is. Could you please explain it
> > further?
> 
> The alternative hypothesis is "two sided".
> 
> When significant level is equal to 0.05 , number of groups=5,
> df of error=35 and rho=0.426, I think that absolute t-value
> should be between 2.558 and 2.598.
> 
> So, (B) is easy to understand for me than (A).

You said "adj p" values are ...

> 0.0437
> 0.0260
> 0.0204
> 0.0001

I used SPSS ver.10 and got the following result.

Dunnett t (two sided) 

 | --------------- | ---------- | -------- | -------- | ------------------ | 
 |                 | mean diff. | s.e.     | adj p    | 95% C.I.           | 
 | ------ | ------ | (I-J)      |          |          | ---------- | ----- | 
 | (I) V3 | (J) V3 |            |          |          |            |       | 
 | ------ | ------ | ---------- | -------- | -------- | ---------- | ----- | 
 | 2      | 1      | 2.125      | 1.022    | .145     | -.507      | 4.757 | 
 | ------ | ------ | ---------- | -------- | -------- | ---------- | ----- | 
 | 3      | 1      | 2.875(*)   | 1.022    | .028     | .243       | 5.507 | 
 | ------ | ------ | ---------- | -------- | -------- | ---------- | ----- | 
 | 4      | 1      | 2.625      | 1.022    | .051     | -7.325E-03 | 5.257 | 
 | ------ | ------ | ---------- | -------- | -------- | ---------- | ----- | 
 | 5      | 1      | 5.167(*)   | 1.113    | .000     | 2.301      | 8.032 | 
 | ------ | ------ | ---------- | -------- | -------- | ---------- | ----- | 

I might make a mistake in the way to use the simtest()...How should I think? 

Best regards,
-- 
kuroki
GnuPG fingerprint = 90FD FE79 905F 26F9 29C4  096F 8AA2 2C42 5130 1469



From Luisr at frs.fo  Thu Jun 17 13:10:35 2004
From: Luisr at frs.fo (Luis Rideau Cruz)
Date: Thu, 17 Jun 2004 12:10:35 +0100
Subject: [R] How to order a vector
Message-ID: <s0d18a4d.059@ffdata.setur.fo>

Hi all

I have a vector like this 

2003 2002 2001 2000 1999 1998 1997 1996 
 106   105  106   106    105   106   101   107

How can I get it sorted right(1996....2003)?

Thank you

Luis Ridao Cruz
Fiskiranns??knarstovan
N??at??n 1
P.O. Box 3051
FR-110 T??rshavn
Faroe Islands
Phone:             +298 353900
Phone(direct): +298 353912
Mobile:             +298 580800
Fax:                 +298 353901
E-mail:              luisr at frs.fo
Web:                www.frs.fo



From tobias.verbeke at bivv.be  Thu Jun 17 13:14:55 2004
From: tobias.verbeke at bivv.be (tobias.verbeke@bivv.be)
Date: Thu, 17 Jun 2004 13:14:55 +0200
Subject: [R] How to order a vector
In-Reply-To: <s0d18a4d.059@ffdata.setur.fo>
Message-ID: <OF41A0E11E.C034A4D7-ONC1256EB6.003DC03D-C1256EB6.003DF7E4@BIVV.BE>





r-help-bounces at stat.math.ethz.ch wrote on 17/06/2004 13:10:35:

> Hi all
>
> I have a vector like this
>
> 2003 2002 2001 2000 1999 1998 1997 1996
>  106   105  106   106    105   106   101   107
>
> How can I get it sorted right(1996....2003)?

> rev(1:4)
[1] 4 3 2 1

HTH,
Tobias



From wolski at molgen.mpg.de  Thu Jun 17 13:15:05 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Thu, 17 Jun 2004 13:15:05 +0200
Subject: [R] How to order a vector
In-Reply-To: <s0d18a4d.059@ffdata.setur.fo>
References: <s0d18a4d.059@ffdata.setur.fo>
Message-ID: <200406171315050099.0B09ABCB@mail.math.fu-berlin.de>

Hi!
I assume that 2003 2002 are the names of your vector myvector

myvector[order(names(myvector))]

Eryk

*********** REPLY SEPARATOR  ***********

On 6/17/2004 at 12:10 PM Luis Rideau Cruz wrote:

>>>Hi all
>>>
>>>I have a vector like this 
>>>
>>>2003 2002 2001 2000 1999 1998 1997 1996 
>>> 106   105  106   106    105   106   101   107
>>>
>>>How can I get it sorted right(1996....2003)?
>>>
>>>Thank you
>>>
>>>Luis Ridao Cruz
>>>Fiskiranns??knarstovan
>>>N??at??n 1
>>>P.O. Box 3051
>>>FR-110 T??rshavn
>>>Faroe Islands
>>>Phone:             +298 353900
>>>Phone(direct): +298 353912
>>>Mobile:             +298 580800
>>>Fax:                 +298 353901
>>>E-mail:              luisr at frs.fo
>>>Web:                www.frs.fo
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



Dipl. bio-chem. Eryk Witold Wolski    @    MPI-Moleculare Genetic   
Ihnestrasse 63-73 14195 Berlin       'v'    
tel: 0049-30-83875219               /   \    
mail: wolski at molgen.mpg.de        ---W-W----    http://www.molgen.mpg.de/~wolski



From sdavis2 at mail.nih.gov  Thu Jun 17 13:07:20 2004
From: sdavis2 at mail.nih.gov (Sean Davis)
Date: Thu, 17 Jun 2004 07:07:20 -0400
Subject: [R] How to order a vector
In-Reply-To: <s0d18a4d.059@ffdata.setur.fo>
Message-ID: <BCF6F3A8.952A%sdavis2@mail.nih.gov>

See ?order.  In particular, if you do: a <- order(names(vec)), a will
contain the indices of the vector in order, so vec[a] will be in the correct
order.

Sean

On 6/17/04 7:10 AM, "Luis Rideau Cruz" <Luisr at frs.fo> wrote:

> Hi all
> 
> I have a vector like this
> 
> 2003 2002 2001 2000 1999 1998 1997 1996
> 106   105  106   106    105   106   101   107
> 
> How can I get it sorted right(1996....2003)?
> 
> Thank you
> 
> Luis Ridao Cruz
> Fiskiranns??knarstovan
> N??at??n 1
> P.O. Box 3051
> FR-110 T??rshavn
> Faroe Islands
> Phone:             +298 353900
> Phone(direct): +298 353912
> Mobile:             +298 580800
> Fax:                 +298 353901
> E-mail:              luisr at frs.fo
> Web:                www.frs.fo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From swhite at aegis-semi.com  Thu Jun 17 16:04:19 2004
From: swhite at aegis-semi.com (Steven White)
Date: Thu, 17 Jun 2004 10:04:19 -0400
Subject: [R] Using predict.lm()
Message-ID: <200406171004.19676.swhite@aegis-semi.com>

Greetings,

Following the example in help(predict.lm):

     x <- rnorm(15)
     y <- x + rnorm(15)
     new <- data.frame(x = seq(-3, 3, 0.5))
     predict(lm(y ~ x), new)

predicts the response elements corresponding to new$x as can be viewed by:

     plot(x,y)
     lines(new$x,predict(lm(y ~ x), new))

I am trying to extend this fitting and prediction over a variety of factors as 
follows:

     f<-rep(c("FIRST","SECOND"),each=15)
     f<-as.factor(f)
     x<-rep(rnorm(15),2)
     y<-x+rnorm(length(x))
     old<-data.frame(f=f,x=x,y=y)
     new<-data.frame(f=rep(levels(f),each=length(seq(-4,4,0.2))),x=seq(-4,4,0.2))

...where variable new simply substitutes a differing domain than old. When I 
try to predict on the frame new using x & y, I get a response that 
corresponds to the length of new:

     predict(lm(y~x),new)

but when I use the same variables from within the frame old, the frame new is 
ignored:

     predict(lm(old$y~old$x),new)

...results in a response the length of old$x (presumably predicting over the 
values of old$x). Furthermore, this behavior also precludes using something 
more useful, i.e.:

     predict(lm(old$y~old$f/(1+old$x)-1),new)

to return predictions over a number of factors over redefined domains. In my 
case, I am attempting to do 2nd order polynomial fitting over noisy data 
collected for a large number of factors (~85). The data were collected for 
each factor at convenient (and therefore dissimilar) points within a common 
domain, but I need to compare the responses of each factor at similar points 
within the common domain.

I am obviously missing something here because I continue to be puzzled by the 
result. I had thought (perhaps erroneously) that lm() would return a model 
object that would permit prediction. Indeed:

     lm(old$y~old$f/(1+old$x)-1)

...results in:

Call:
lm(formula = old$y ~ old$f/(1 + old$x) - 1)

Coefficients:
       old$fFIRST        old$fSECOND   old$fFIRST:old$x  old$fSECOND:old$x
         -0.08489           -0.05839            1.15351            0.72981

which clearly provides a model fit for each factor, and identifies the factor 
from which each model coefficient was extracted, so lm() does provide the 
capability to predict over the factors. It seems however (as nearly as I can 
tell), that predict simply ignores the frame new altogether, failing even to 
provide a warning.

Is this the intended behavior? Have I missed something very simple or have a 
fundamental misunderstanding of how this should work? Lastly, I'd appreciate 
any suggestions that avoid the lengthy and wholly undesirable "brute force" 
approach I an now considering.

Thanks & Best Regards,
Steve



From ripley at stats.ox.ac.uk  Thu Jun 17 16:25:30 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 17 Jun 2004 15:25:30 +0100 (BST)
Subject: [R] Using predict.lm()
In-Reply-To: <200406171004.19676.swhite@aegis-semi.com>
Message-ID: <Pine.LNX.4.44.0406171516200.20729-100000@gannet.stats>

On Thu, 17 Jun 2004, Steven White wrote:

> Following the example in help(predict.lm):
> 
>      x <- rnorm(15)
>      y <- x + rnorm(15)
>      new <- data.frame(x = seq(-3, 3, 0.5))
>      predict(lm(y ~ x), new)
> 
> predicts the response elements corresponding to new$x as can be viewed by:
> 
>      plot(x,y)
>      lines(new$x,predict(lm(y ~ x), new))

Note that the model is fitted to `x' and new contains `x'.  You haven't 
copied that.

> I am trying to extend this fitting and prediction over a variety of factors as 
> follows:
> 
>      f<-rep(c("FIRST","SECOND"),each=15)
>      f<-as.factor(f)
>      x<-rep(rnorm(15),2)
>      y<-x+rnorm(length(x))
>      old<-data.frame(f=f,x=x,y=y)
>      new<-data.frame(f=rep(levels(f),each=length(seq(-4,4,0.2))),x=seq(-4,4,0.2))
> 
> ...where variable new simply substitutes a differing domain than old. When I 
> try to predict on the frame new using x & y, I get a response that 
> corresponds to the length of new:
> 
>      predict(lm(y~x),new)
> 
> but when I use the same variables from within the frame old, 

That you have not done correctly: see ?lm.

> the frame new is ignored:

No, it is not ignored but it does not contain a variable named `old$x' and 
your workspace does.  newdata is the first place to look for variables, 
but not the only place.

>      predict(lm(old$y~old$x),new)
> 
> ...results in a response the length of old$x (presumably predicting over the 
> values of old$x). Furthermore, this behavior also precludes using something 
> more useful, i.e.:
> 
>      predict(lm(old$y~old$f/(1+old$x)-1),new)
> 
> to return predictions over a number of factors over redefined domains. In my 
> case, I am attempting to do 2nd order polynomial fitting over noisy data 
> collected for a large number of factors (~85). The data were collected for 
> each factor at convenient (and therefore dissimilar) points within a common 
> domain, but I need to compare the responses of each factor at similar points 
> within the common domain.
> 
> I am obviously missing something here because I continue to be puzzled by the 
> result. I had thought (perhaps erroneously) that lm() would return a model 
> object that would permit prediction. 

Indeed it does.

> Indeed:
> 
>      lm(old$y~old$f/(1+old$x)-1)
> 
> ...results in:
> 
> Call:
> lm(formula = old$y ~ old$f/(1 + old$x) - 1)
> 
> Coefficients:
>        old$fFIRST        old$fSECOND   old$fFIRST:old$x  old$fSECOND:old$x
>          -0.08489           -0.05839            1.15351            0.72981
> 
> which clearly provides a model fit for each factor, and identifies the factor 
> from which each model coefficient was extracted, so lm() does provide the 
> capability to predict over the factors. It seems however (as nearly as I can 
> tell), that predict simply ignores the frame new altogether, failing even to 
> provide a warning.

Nope.  You just haven't set new to match your fit.

> Is this the intended behavior? Have I missed something very simple or have a 
> fundamental misunderstanding of how this should work?

Yes, yes.  You should be using

	lm(y ~ f/(1+x)-1, data=old)

etc, although in your example you could omit data=old.  That is in all 
good books on the S language ....

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From HerwigMeschke at t-online.de  Thu Jun 17 16:49:55 2004
From: HerwigMeschke at t-online.de (Dr. Herwig Meschke)
Date: Thu, 17 Jun 2004 16:49:55 +0200
Subject: [R] non-linear binning? power-law in R
Message-ID: <40D1CBB3.24542.5CD1AA5@localhost>

Why not try to avoid binning (and density plot) at all? An alternative 
could be a qqplot (as a log-log-plot), e.g.

plot(ppoints(length(x4)), x4[order(x4)], log="xy")
abline(lm(log(x4[order(x4)])~log(ppoints(length(x4)))), col="red")

If the assumptions of uniform distribution and power transformation 
y=a*x**b are true, the coefficient of lm estimates the exponent b.

Herwig

-- 
Dr. Herwig Meschke
Wissenschaftliche Beratung
Hagsbucher Weg 27
D-89150 Laichingen

phone +49 7333 210 417 / fax +49 7333 210 418
email HerwigMeschke at t-online.de



From tlumley at u.washington.edu  Thu Jun 17 17:00:53 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 17 Jun 2004 08:00:53 -0700 (PDT)
Subject: [R] Resolution of plots
In-Reply-To: <Pine.LNX.4.44.0406162247250.19571-100000@gannet.stats>
References: <Pine.LNX.4.44.0406162247250.19571-100000@gannet.stats>
Message-ID: <Pine.A41.4.58.0406170744550.270076@homer03.u.washington.edu>

On Wed, 16 Jun 2004, Prof Brian Ripley wrote:

> You will have to tell us more.  Exporting how: to what format using what
> device and what exact command on what operating system?
>
> The only device I know of that even knows about dpi is bitmap() and that
> has no such limit unless imposed by your implementation of ghostscript.

There is an issue with PNG. libpng provides png_set_pHYs to set resolution
(in pixels/metre) but provides a default if it is not set.  We don't set
it, and so get the default resolution.

The default is good for screen display, and irrelevant if you are
including the file in a document, but photo editing software will show the
true resolution, and publishers may care.

One way to change the resoluton of PNG files is to use SNG
(sng.sourceforge.net) to translate the PNG to a text format, edit the pHYs
chunk and translate it back.


	-thomas



From grenyer at virginia.edu  Thu Jun 17 17:42:29 2004
From: grenyer at virginia.edu (Rich Grenyer)
Date: Thu, 17 Jun 2004 11:42:29 -0400
Subject: [R] 2D Kolmogorov-Smirnov test: solution
Message-ID: <F0FDAA2E-C074-11D8-88EB-000A95F0D0D2@virginia.edu>

Hi - A little while ago I posted a question about the implementation of  
a two-dimensional analog of the Kolmogorov-Smirnov test in R[1][2]. As  
there isn't one, as far as I know, people might be interested in a very  
fast C++ implementation called MUAC which is available as a function  
and as a standalone program from  
http://www.acooke.org/jara/muac/index.html. Apparently the code is  
interesting from a computer science viewpoint, and an MPI-powered  
parallel version is available from  
http://beowulf.lcs.mit.edu/18.337-2002/projects-2002/ianchan/KS2D/ 
Project%20Page.htm

If anyone has any experience of this code, and in particular words of  
warning, I'd love to hear about them (off-list would be best).

Rich



[1] Peacock J, Monthly Notices of the Royal Astronomical Society, 1983,  
vol 202 p615: Two-Dimensional Goodness-of-Fit Testing in Astronomy

[2] Fasano G, Franceschini A. Monthly Notices of the Royal Astronomical  
Society, 1987 vol 225 p 155: A Multidimensional Version of the  
Kolmogorov-Smirnov Test


--------------------
Rich Grenyer, Ph.D.
Biology Department - University of Virginia
Gilmer Hall
Charlottesville, Virginia
VA 22904
United States of America

tel: (+1) 434 982 5629
fax: (+1) 434 982 5626
http://faculty.virginia.edu/gittleman/rich



From maechler at stat.math.ethz.ch  Thu Jun 17 18:07:47 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 17 Jun 2004 18:07:47 +0200
Subject: [R] Re: Clustering in R
In-Reply-To: <8975119BCD0AC5419D61A9CF1A923E9517460F@iahce2knas1.iah.bbsrc.reserved>
References: <8975119BCD0AC5419D61A9CF1A923E9517460F@iahce2knas1.iah.bbsrc.reserved>
Message-ID: <16593.49619.392838.196401@gargle.gargle.HOWL>

Thanks a lot, Michael!

I cc to R-help, where this question really belongs {as the
'Subject' suggests itself...} -- please drop 'bioconductor' from
CC'ing further replies.

>>>>> "michael" == michael watson (IAH-C) <michael.watson at bbsrc.ac.uk>
>>>>>     on Thu, 17 Jun 2004 09:16:59 +0100 writes:

    michael> OK, admittedly it is not incredibly simple, but it
    michael> is not *that* difficult.

    michael> If you are familiar with R, it should take you an
    michael> hour or two; if unfamiliar, perhaps a day or two.

    michael> The commands you want (and need to read the help on) are:

    michael> hclust
    michael> plclust
    michael> cutree

and I would add  identify.hclust()   {and rect.hclust()}
a very neat but not known / used enough function
a link to which I have just added to the help(hclust) page.
Look at its examples {not with example() since they are
"dontrun"} correcting the extraneous "." in the last (and
coolest!) example!

    michael> dendrogram
    michael> as.dendrogram
    michael> heatmap

where you use "dendrogram"s produced from "hclust" objects via
as.dendrogram(<hc-obj>) or also "twins" objects produced from
package cluster's agnes() or diana() via  
 as.dendrogram(as.hclust( <twins-obj> ) )

help(dendrogram)  also mentions  
"[[" (and shows examples) and cut() for cutting dendrograms and shows
how you can depict dendrograms into its parts.

    michael> With intelligent use of hclust -> cutree -> subsetting -> hclust
    michael> (in that order) you will be able to drill down
    michael> into your dendrogram and create sub-trees - until
    michael> you get to the level where you can see your gene
    michael> names.

or also
   hclust -> as.dendrogram -> cut -> ..
			   -> [[  ->

Note that there also is  reorder.dendrogram() for reordering
dendrogram nodes ``sensibly'' --- something that heatmap() does,
but you can play with quite a bit.
Further, note Catherine Hurley's  "gclus" package which
orders/reorders 'hclust' objects directly, but with a more
interesting algorithm. 

Note that I'd strongly recommend to use R 1.9.1 beta for these,
since I know which bugs in the dendrogram code I have fixed
since R 1.9.0...

    michael> An important message to take home here is that if
    michael> you have 14000 genes and therefore 14000 labels,
    michael> it's going to be difficult to display your tree in
    michael> ANY software, including the expensive commercial products.

not showing the labels and using identify.hclust() and the
command line to extract the indices of observations in
clusters (and subclusters) and visualize them in other, non-dendrogram plots,
might well be feasible.

    michael> Let me know how you get on

    michael> Thanks
    michael> Mick

    michael> -----Original Message-----
    michael> From: wmak at brandeisedu [mailto:wmak at brandeis.edu] 
    michael> Sent: 16 June 2004 21:26
    michael> To: bioconductor at stat.math.ethz.ch
    michael> Subject: [BioC] Clustering in R

    >> Dear list members,

    >> I'm an undergrad and I work in a lab at Brandeis.
    >> I am trying to cluster around 14,000 genes across 6
    >> microarray experiments.  Two of these experiments
    >> are replicates.  I have decided to use R since it
    >> seems to be the most complete and flexible software
    >> package for normalization and clustering of
    >> microarray data.

    >> The problem is that I am new to clustering and to
    >> R.  Just to mention of a few of the problems I'm
    >> having: the dendrogram that is drawn by R from the
    >> agnes object is far too dense to see any of the
    >> gene names; kmeans won't work, returning an error
    >> saying that my data has NAs in it (there weren't
    >> any missing values in the original table though);
    >> I'd like to be able to see a heatmap or a
    >> cumulative plot of expression profiles for genes
    >> that are clustered together or are on the same
    >> branch of the dendrogram.

    >> I know that these questions are probably very
    >> simple, but I can't seem to find the answer to them
    >> online or in the documentation.  If anyone can
    >> answer these questions or direct me toward
    >> resources that deal with clustering in R or
    >> BioConductor, a basic tutorial that takes a
    >> practical approach to it, I would really appreciate
    >> it.  Any other reading material that isn't too
    >> heavy on statistics that deals with clustering for
    >> that matter, would be very helpful.

    >> Thank you in advance,
    >> Wayne Mak



From danbebber at forestecology.co.uk  Thu Jun 17 18:15:12 2004
From: danbebber at forestecology.co.uk (Dan Bebber)
Date: Thu, 17 Jun 2004 17:15:12 +0100
Subject: [R] Error with arima()
Message-ID: <000001c45486$44b3c740$442501a3@plants.ox.ac.uk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040617/e0b9c3cb/attachment.pl

From rishi at iqmail.net  Thu Jun 17 18:23:10 2004
From: rishi at iqmail.net (Rishi Ganti)
Date: Thu, 17 Jun 2004 09:23:10 -0700
Subject: [R] disappointed with x-axes in hist and density plots
Message-ID: <3482f4375baf48e682072db55dbf4f76.rishi@iqmail.net>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040617/24128d38/attachment.pl

From gabriel.serendip at hipernet.com.br  Thu Jun 17 18:45:42 2004
From: gabriel.serendip at hipernet.com.br (Gabriel Erbano)
Date: Thu, 17 Jun 2004 13:45:42 -0300
Subject: [R] L =?iso-8859-1?q?=E9?= vy distribution?
Message-ID: <BCF75106.1DC6%gabriel.serendip@hipernet.com.br>

Hi all,

Does R support the L??vy distribution? I looked everywhere and couldn't find
it.

Thanks for any help!

Gabriel



From tlumley at u.washington.edu  Thu Jun 17 18:53:33 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Thu, 17 Jun 2004 09:53:33 -0700 (PDT)
Subject: [R] disappointed with x-axes in hist and density plots
In-Reply-To: <3482f4375baf48e682072db55dbf4f76.rishi@iqmail.net>
References: <3482f4375baf48e682072db55dbf4f76.rishi@iqmail.net>
Message-ID: <Pine.A41.4.58.0406170953060.196404@homer12.u.washington.edu>

On Thu, 17 Jun 2004, Rishi Ganti wrote:

> I've got a few issues with the x-axes in the histogram and density plots.  First,
> often the default x-axis doesn't even extend to the length of my data. R often draws
> histogram bars  (or density lines) farther than the drawn x-axis extends. For example,
> I might have a histogram bar at -15,000. But I wouldn't know that, because the most
> negative number on the x-axis is -10,000.  The second issue is the use of scientific
> notation. Yes I can read it, but I don't prefer it. Is there any way for R just
> to print out 1000000 and not 1e+6 on these charts?  Thanks for your help.  Rishi
>

You can use the axis() function to draw axes with any set of labels you
want.

	-thomas



From sundar.dorai-raj at PDF.COM  Thu Jun 17 19:00:25 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Thu, 17 Jun 2004 10:00:25 -0700
Subject: [R] L =?ISO-8859-1?Q?=E9_vy_distribution=3F?=
In-Reply-To: <BCF75106.1DC6%gabriel.serendip@hipernet.com.br>
References: <BCF75106.1DC6%gabriel.serendip@hipernet.com.br>
Message-ID: <40D1CE29.1020402@pdf.com>



Gabriel Erbano wrote:

> Hi all,
> 
> Does R support the L??vy distribution? I looked everywhere and couldn't find
> it.
> 
> Thanks for any help!
> 
> Gabriel
> 

I believe Jim Lindsey's rmutil package supports this though I've never 
used it:

http://popgen0146uns50.unimaas.nl/~jlindsey/rcode.html

BTW, I found this out by doing an R site search at

http://cran.r-project.org/ -> Search -> R site search

and typing in "Levy distribution". I got 13 hits the first of which was 
Lindsey's help page.

--sundar



From ripley at stats.ox.ac.uk  Thu Jun 17 19:00:08 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 17 Jun 2004 18:00:08 +0100 (BST)
Subject: [R] Error with arima()
In-Reply-To: <000001c45486$44b3c740$442501a3@plants.ox.ac.uk>
Message-ID: <Pine.LNX.4.44.0406171756560.20880-100000@gannet.stats>

It means that the CSS estimates from the model are invalid.  That's 
possible as CSS does not enforce validity.  It probably means the model is 
inappropriate.

On Thu, 17 Jun 2004, Dan Bebber wrote:

> Could someone please give a brief explanation, or pointer to an explanation,
> of the following error:
> 
> > arima(ts.growth, order = c(1,0,0),include.mean=T)
> Error in arima(ts.growth, order = c(1, 0, 0), include.mean = T) :
>         non-stationary AR part from CSS
> 
> and why it does not arise with
> 
> > arima0(ts.growth, order = c(1,0,0))

Not the same algorithm.  These algorithms only find a local minimum.

You are asking the question the wrong way round: unless you know there is 
uniquely defined estimator, why would you expect the same results?  
Especially if the model is a poor fit.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From erich.neuwirth at univie.ac.at  Thu Jun 17 19:06:10 2004
From: erich.neuwirth at univie.ac.at (Erich Neuwirth)
Date: Thu, 17 Jun 2004 19:06:10 +0200
Subject: [R] R help in Firefox on Windows XP
Message-ID: <40D1CF82.9080000@univie.ac.at>

I had to reinstall my machine, so I installed Firefox 0.9 as browser
I am using WinXP and R 1.9.1 beta.
Now search in R html help does not work.
I checked that the Java VM is working correctlt, Sun's test site says
my installation is OK.
Firefoxalso tells me that

Applet Searchengine loaded
Applet Searchengine started

it just does not find anything.
Does anybody know how to solve this?

Erich



From BDANWOOD at politics.tamu.edu  Thu Jun 17 19:17:25 2004
From: BDANWOOD at politics.tamu.edu (B.Dan Wood)
Date: Thu, 17 Jun 2004 12:17:25 -0500
Subject: [R] beta regression in R
Message-ID: <F0D57DBFB1CB414583E20D009E58D2FB9FE196@politics.tamu.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040617/138fffab/attachment.pl

From gabriel.serendip at hipernet.com.br  Thu Jun 17 19:22:19 2004
From: gabriel.serendip at hipernet.com.br (Gabriel Erbano)
Date: Thu, 17 Jun 2004 14:22:19 -0300
Subject: [R] L =?ISO-8859-1?B?6SA=?=vy distribution?
In-Reply-To: <40D1CE29.1020402@pdf.com>
Message-ID: <BCF7599B.1DDA%gabriel.serendip@hipernet.com.br>

Actually, I tried searching the R site, but it just returned 4 hits, and one
of them was about Lindsey's package. But since it was not listed in the
packages section, I thought it was broken... Well, my bad!

Thanks!


> From: Sundar Dorai-Raj <sundar.dorai-raj at pdf.com>
> Organization: PDF Solutions, Inc.
> Reply-To: <sundar.dorai-raj at pdf.com>
> Date: Thu, 17 Jun 2004 10:00:25 -0700
> To: Gabriel Erbano <gabriel.serendip at hipernet.com.br>
> Cc: <r-help at stat.math.ethz.ch>
> Subject: Re: [R] L ?? vy distribution?
> 
> 
> 
> Gabriel Erbano wrote:
> 
>> Hi all,
>> 
>> Does R support the L??vy distribution? I looked everywhere and couldn't find
>> it.
>> 
>> Thanks for any help!
>> 
>> Gabriel
>> 
> 
> I believe Jim Lindsey's rmutil package supports this though I've never
> used it:
> 
> http://popgen0146uns50.unimaas.nl/~jlindsey/rcode.html
> 
> BTW, I found this out by doing an R site search at
> 
> http://cran.r-project.org/ -> Search -> R site search
> 
> and typing in "Levy distribution". I got 13 hits the first of which was
> Lindsey's help page.
> 
> --sundar
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ajayshah at mayin.org  Thu Jun 17 19:26:29 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Thu, 17 Jun 2004 22:56:29 +0530
Subject: [R] Question on lists and vectors of lists
Message-ID: <20040617172629.GA10270@igidr.ac.in>

I have an elementary programming question. Could someone please point
me in the right direction?

I have a function which will run for thousands of companies. At each
invocation, it returns 2 numbers. I plan to do something like:

  think_one_firm <- function(filename) {
     # Do stuff
     return(list(x=x,y=y))
  }

So for each of the firms in my dataset, I will call
think_one_firm(datafilename) and it will give me back two numbers x
and y. I could say:

  l = think_one_firm("blah")
  print(l$x); print(l$y);

and all would be fine.

What I want to do is: To tuck away the returned lists into a vector or
a data frame.

At the end, I would like to endup with a data structure allfirms
containing one 'row' for each firm. I guess this could be a data
frame, or a matrix, or a vector of lists (if such a thing exists). In
case it's a data frame, I would say allfirms$x[400] to access the 'x'
value returned for the 400th firm.

How would I do this?

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From maechler at stat.math.ethz.ch  Thu Jun 17 19:27:23 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 17 Jun 2004 19:27:23 +0200
Subject: [R] disappointed with x-axes in hist and density plots
In-Reply-To: <Pine.A41.4.58.0406170953060.196404@homer12.u.washington.edu>
References: <3482f4375baf48e682072db55dbf4f76.rishi@iqmail.net>
	<Pine.A41.4.58.0406170953060.196404@homer12.u.washington.edu>
Message-ID: <16593.54395.355129.879259@gargle.gargle.HOWL>

>>>>> "TL" == Thomas Lumley <tlumley at u.washington.edu>
>>>>>     on Thu, 17 Jun 2004 09:53:33 -0700 (PDT) writes:

    TL> On Thu, 17 Jun 2004, Rishi Ganti wrote:

    >> I've got a few issues with the x-axes in the histogram
    >> and density plots.  First, often the default x-axis
    >> doesn't even extend to the length of my data. R often
    >> draws histogram bars (or density lines) farther than the
    >> drawn x-axis extends. For example, I might have a
    >> histogram bar at -15,000. But I wouldn't know that,
    >> because the most negative number on the x-axis is
    >> -10,000.  The second issue is the use of scientific
    >> notation. Yes I can read it, but I don't prefer it. Is
    >> there any way for R just to print out 1000000 and not
    >> 1e+6 on these charts?  Thanks for your help.  Rishi
    >> 

    TL> You can use the axis() function to draw axes with any set of labels you
    TL> want.

and I've recently written the following to show someone how to 
"beautify" the situation (when "1e<n>" labels appear: BTW, under
	  windows there's an etra space in there which IMO makes
	  the labels much uglier) :


###----------------- Do "a 10^k" labeling instead of "a e<k>" ---
x <- 1e7*(-10:50)
y <- dnorm(x, m=10e7, s=20e7)
plot(x,y)

axTexpr <- function(side, at = axTicks(side, axp=axp, usr=usr, log=log),
                    axp = NULL, usr = NULL, log = NULL)
{
    ## Purpose: Do "a 10^k" labeling instead of "a e<k>"
    ##	      this auxiliary should return 'at' and 'label' (expression)
    ## ----------------------------------------------------------------------
    ## Arguments: as for axTicks()
    ## ----------------------------------------------------------------------
    ## Author: Martin Maechler, Date:  7 May 2004, 18:01
    eT <- floor(log10(abs(at)))# at == 0 case is dealt with below
    mT <- at / 10^eT
    ss <- lapply(seq(along = at),
                 function(i) if(at[i] == 0) quote(0) else
                 substitute(A %*% 10^E, list(A=mT[i], E=eT[i])))
    do.call("expression", ss)
}

plot(x,y, axes= FALSE, frame=TRUE)
aX <- axTicks(1); axis(1, at=aX, label= axTexpr(1, aX))
if(FALSE) # rather the next one
aY <- axTicks(2); axis(2, at=aY, label= axTexpr(2, aY))
## or rather (horizontal labels on y-axis):
aY <- axTicks(2); axis(2, at=aY, label= axTexpr(2, aY), las=2)

----------

I hope this decreases your deception..
Further note that you can always call  box() after hist()
which may also improve the picture to your eyes.

Regards,
Martin Maechler



From rishi at iqmail.net  Thu Jun 17 19:28:57 2004
From: rishi at iqmail.net (Rishi Ganti)
Date: Thu, 17 Jun 2004 10:28:57 -0700
Subject: [R] disappointed with x-axes in hist and density plots
Message-ID: <ab579672fb504fc7930f4c145a307543.rishi@iqmail.net>

Thanks, but even with axis() I can't get the x-axis to extend to the sides.

Try, e.g., 

x = rnorm(1000)

you should have some values in excess of 3 (or below -3).

I want to draw the x-axis from -4 to 4, thus encapsulating all points.

axis(1,-4:4)

but it won't draw. It TRIES to draw it, but I don't see a -4 or 4 on the plot.


----- Original Message -----
From: Thomas Lumley
Sent: 6/17/2004 9:53:33 AM
To: rishi at post.harvard.edu
Cc: r-help at stat.math.ethz.ch
Subject: Re: [R] disappointed with x-axes in hist and density plots

> On Thu, 17 Jun 2004, Rishi Ganti wrote:
> 
> > I've got a few issues with the x-axes in the histogram and density plots.  First,
> > often the default x-axis doesn't even extend to the length of my data. R often draws
> > histogram bars  (or density lines) farther than the drawn x-axis extends. For example,
> > I might have a histogram bar at -15,000. But I wouldn't know that, because the most
> > negative number on the x-axis is -10,000.  The second issue is the use of scientific
> > notation. Yes I can read it, but I don't prefer it. Is there any way for R just
> > to print out 1000000 and not 1e+6 on these charts?  Thanks for your help.  Rishi
> >
> 
> You can use the axis() function to draw axes with any set of labels you
> want.
> 
> 	-thomas



From MSchwartz at MedAnalytics.com  Thu Jun 17 19:45:38 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Thu, 17 Jun 2004 12:45:38 -0500
Subject: [R] R help in Firefox on Windows XP
In-Reply-To: <40D1CF82.9080000@univie.ac.at>
References: <40D1CF82.9080000@univie.ac.at>
Message-ID: <1087494337.4446.113.camel@localhost.localdomain>

On Thu, 2004-06-17 at 12:06, Erich Neuwirth wrote:
> I had to reinstall my machine, so I installed Firefox 0.9 as browser
> I am using WinXP and R 1.9.1 beta.
> Now search in R html help does not work.
> I checked that the Java VM is working correctlt, Sun's test site says
> my installation is OK.
> Firefoxalso tells me that
> 
> Applet Searchengine loaded
> Applet Searchengine started
> 
> it just does not find anything.
> Does anybody know how to solve this?
> 
> Erich

Erich,

Do you also have JavaScript enabled in the Firefox Tools -> Options
settings?

Both Java and JavaScript need to be enabled for the help.start() search
engine to function properly.

I reviewed the release notes at
http://www.mozilla.org/products/firefox/releases/ and did not see
anything relating to Java there, as had been the case with prior
releases. The message that you are getting on the status line suggests
that the R search applet is being found and properly enabled, which is
typically the primary source of problems.

Check the above and let us know.

Marc Schwartz



From ramasamy at cancer.org.uk  Thu Jun 17 19:46:11 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: 17 Jun 2004 18:46:11 +0100
Subject: [R] Question on lists and vectors of lists
In-Reply-To: <20040617172629.GA10270@igidr.ac.in>
References: <20040617172629.GA10270@igidr.ac.in>
Message-ID: <1087494371.4398.44.camel@vpn202001.lif.icnet.uk>

Try this :

silly.fn <- function(x){ return( c(x^2, x^3) ) }

output <- matrix( nr=100, nc=2 )
for(i in 1:100){
 my.x <- rnorm(1)
 output[i, ] <- silly.fn( my.x )
 rownames(output)[i] <- paste("Dataset", i, sep="")
}
colnames(output) <- c("squared", "cubed")

Notice that silly.fn returns a vector which is set to be the ith row of
output. 

It is also possible to speed this with the apply family but this depends
on your input format. For example if your input was a list where the
first element corresponds to 1st company information in dataframe format

sapply( my.list.of.matrices, function(single.mat) some.fn(single.mat) )
 
Another option is to use output[i, ] <- unlist(think_one_firm("blah"))
where unlist hopefully will convert the list to vector.


On Thu, 2004-06-17 at 18:26, Ajay Shah wrote:
> I have an elementary programming question. Could someone please point
> me in the right direction?
> 
> I have a function which will run for thousands of companies. At each
> invocation, it returns 2 numbers. I plan to do something like:
> 
>   think_one_firm <- function(filename) {
>      # Do stuff
>      return(list(x=x,y=y))
>   }
> 
> So for each of the firms in my dataset, I will call
> think_one_firm(datafilename) and it will give me back two numbers x
> and y. I could say:
> 
>   l = think_one_firm("blah")
>   print(l$x); print(l$y);
> 
> and all would be fine.
> 
> What I want to do is: To tuck away the returned lists into a vector or
> a data frame.
> 
> At the end, I would like to endup with a data structure allfirms
> containing one 'row' for each firm. I guess this could be a data
> frame, or a matrix, or a vector of lists (if such a thing exists). In
> case it's a data frame, I would say allfirms$x[400] to access the 'x'
> value returned for the 400th firm.
> 
> How would I do this?



From Achim.Zeileis at wu-wien.ac.at  Thu Jun 17 20:04:35 2004
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 17 Jun 2004 20:04:35 +0200
Subject: [R] Question on lists and vectors of lists
In-Reply-To: <20040617172629.GA10270@igidr.ac.in>
References: <20040617172629.GA10270@igidr.ac.in>
Message-ID: <20040617200435.69f7b30b.Achim.Zeileis@wu-wien.ac.at>

On Thu, 17 Jun 2004 22:56:29 +0530 Ajay Shah wrote:

> I have an elementary programming question. Could someone please point
> me in the right direction?
> 
> I have a function which will run for thousands of companies. At each
> invocation, it returns 2 numbers. I plan to do something like:
> 
>   think_one_firm <- function(filename) {
>      # Do stuff
>      return(list(x=x,y=y))
>   }
> 
> So for each of the firms in my dataset, I will call
> think_one_firm(datafilename) and it will give me back two numbers x
> and y. I could say:
> 
>   l = think_one_firm("blah")
>   print(l$x); print(l$y);
> 
> and all would be fine.
> 
> What I want to do is: To tuck away the returned lists into a vector or
> a data frame.
> 
> At the end, I would like to endup with a data structure allfirms
> containing one 'row' for each firm. I guess this could be a data
> frame, or a matrix, or a vector of lists (if such a thing exists). In
> case it's a data frame, I would say allfirms$x[400] to access the 'x'
> value returned for the 400th firm.
> 
> How would I do this?

If I understand correctly what you are trying to do, you could do the
following:

Suppose you have some firms:

firms <- c("blah", "foo", "bar")

and a function which computes something based on these firms

thinkOneFirm <- function(firm) list(x = rnorm(1), y = rnorm(1))

(which for this example just returns random numbers, but let's imagine
it's something more sensible :))

And then 

rval1 <- sapply(firms, thinkOneFirm)

gives you a matrix with the numeric results. To get the desired
data.frame you need to transpose and coerce

rval2 <- as.data.frame(t(rval1))

and then you can get value x for firm "foo" by

rval2$x["foo"]

or

rval2["foo", "x"]

hth,
Z

> -- 
> Ajay Shah                                                   Consultant
> ajayshah at mayin.org                      Department of Economic Affairs
> http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From djw1005 at cam.ac.uk  Thu Jun 17 20:07:47 2004
From: djw1005 at cam.ac.uk (Damon Wischik)
Date: Thu, 17 Jun 2004 19:07:47 +0100 (BST)
Subject: [R] R help in Firefox on Windows XP
In-Reply-To: <40D1CF82.9080000@univie.ac.at>
Message-ID: <Pine.SOL.3.96.1040617190333.4951A-100000@draco.cus.cam.ac.uk>


Erich Neuwirth wrote: 
> I had to reinstall my machine, so I installed Firefox 0.9 as browser I
> am using WinXP and R 1.9.1 beta.  Now search in R html help does not
> work.

A workaround (which is slow, provisional, and as yet untested on your
configuration) is to use a different help engine, namely

> source("http://www.statslab.cam.ac.uk/~djw1005/Stats/Interests/search.R")
> helpHTML()

Damon.



From ripley at stats.ox.ac.uk  Thu Jun 17 20:10:36 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 17 Jun 2004 19:10:36 +0100 (BST)
Subject: [R] R help in Firefox on Windows XP
In-Reply-To: <40D1CF82.9080000@univie.ac.at>
Message-ID: <Pine.LNX.4.44.0406171909220.6100-100000@gannet.stats>

Works for me.  Did it work with Firefox 0.8?  (I upgraded from 0.8.)
I would try 0.8 and then upgrade.

On Thu, 17 Jun 2004, Erich Neuwirth wrote:

> I had to reinstall my machine, so I installed Firefox 0.9 as browser
> I am using WinXP and R 1.9.1 beta.
> Now search in R html help does not work.
> I checked that the Java VM is working correctlt, Sun's test site says
> my installation is OK.
> Firefoxalso tells me that
> 
> Applet Searchengine loaded
> Applet Searchengine started
> 
> it just does not find anything.
> Does anybody know how to solve this?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From chrysopa at insecta.ufv.br  Thu Jun 17 20:04:32 2004
From: chrysopa at insecta.ufv.br (Ronaldo Reis Jr.)
Date: Thu, 17 Jun 2004 15:04:32 -0300
Subject: [R] problem with restore and some .RData
Message-ID: <200406171504.33033.chrysopa@insecta.ufv.br>

Hi,

I have problem with the restore function in some .RData using R 1.9.0
Look the error:

[ronaldo at zeus RAnalise]$ R
...
Error: object 'family' not found whilst loading namespace 'MASS'
Fatal error: unable to restore saved data in .RData

But if I load this .RData with the load() function all objects are recovered.

[ronaldo at zeus RAnalise]$ R --no-restore-data
...
> ls()
character(0)
> load(".RData")
 [1] ".Traceback"               "RespY"                   
 [3] "Trat1"                    "Trat2" 
...
[63] "varied.371.84210116.9"    "varied.371.842101169.5"  
[65] "varied.371.8421011695.12"

All recovered objects work fine. I can use q() and save workspace
image to quit session. But to load this .RData I need to use R with
--no-restore-data option and load the .RData manually.

I try to find the problem, but I dont find this.

Anybody have any idea?

Thanks
Ronaldo

ps. how to initiate R in xemacs with the --no-restore-data option?
    M-x R --no-restore-data dont work

-- 
Hlade's Law:
	If you have a difficult task, give it to a lazy person --
	they will find an easier way to do it.
--
|>   // | \\   [***********************************]
|   ( ??   ?? )  [Ronaldo Reis J??nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36571-000 Vi??osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-2532                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From andy_liaw at merck.com  Thu Jun 17 20:40:35 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 17 Jun 2004 14:40:35 -0400
Subject: [R] disappointed with x-axes in hist and density plots
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7EFF@usrymx25.merck.com>

You haven't read ?axis, I guess.  Try using the at= argument.

Whatever you do not like about the default output, chances are you can
customize it to your heart's content...

Andy

> From: Rishi Ganti
> 
> Thanks, but even with axis() I can't get the x-axis to extend 
> to the sides.
> 
> Try, e.g., 
> 
> x = rnorm(1000)
> 
> you should have some values in excess of 3 (or below -3).
> 
> I want to draw the x-axis from -4 to 4, thus encapsulating all points.
> 
> axis(1,-4:4)
> 
> but it won't draw. It TRIES to draw it, but I don't see a -4 
> or 4 on the plot.
> 
> 
> ----- Original Message -----
> From: Thomas Lumley
> Sent: 6/17/2004 9:53:33 AM
> To: rishi at post.harvard.edu
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] disappointed with x-axes in hist and density plots
> 
> > On Thu, 17 Jun 2004, Rishi Ganti wrote:
> > 
> > > I've got a few issues with the x-axes in the histogram 
> and density plots.  First,
> > > often the default x-axis doesn't even extend to the 
> length of my data. R often draws
> > > histogram bars  (or density lines) farther than the drawn 
> x-axis extends. For example,
> > > I might have a histogram bar at -15,000. But I wouldn't 
> know that, because the most
> > > negative number on the x-axis is -10,000.  The second 
> issue is the use of scientific
> > > notation. Yes I can read it, but I don't prefer it. Is 
> there any way for R just
> > > to print out 1000000 and not 1e+6 on these charts?  
> Thanks for your help.  Rishi
> > >
> > 
> > You can use the axis() function to draw axes with any set 
> of labels you
> > want.
> > 
> > 	-thomas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From Achim.Zeileis at wu-wien.ac.at  Thu Jun 17 20:45:31 2004
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Thu, 17 Jun 2004 20:45:31 +0200
Subject: [R] disappointed with x-axes in hist and density plots
In-Reply-To: <ab579672fb504fc7930f4c145a307543.rishi@iqmail.net>
References: <ab579672fb504fc7930f4c145a307543.rishi@iqmail.net>
Message-ID: <20040617204531.49a3ecfa.Achim.Zeileis@wu-wien.ac.at>

On Thu, 17 Jun 2004 10:28:57 -0700 Rishi Ganti wrote:

> Thanks, but even with axis() I can't get the x-axis to extend to the
> sides.
> 
> Try, e.g., 
> 
> x = rnorm(1000)
> 
> you should have some values in excess of 3 (or below -3).
> 
> I want to draw the x-axis from -4 to 4, thus encapsulating all points.
> 
> axis(1,-4:4)
> 
> but it won't draw. It TRIES to draw it, but I don't see a -4 or 4 on
> the plot.

Well you need to make enough space before!

When you have got

R> x <- rnorm(1000)
R> y <- rnorm(1000)

you need to make sure that the desired range is covered by the plot:

R> plot(x, y, axes = FALSE, xlim = c(-4, 4))

then you add the x-axis

R> axis(1, at = -4:4)

and y-axis and a box.

R> axis(2)
R> box()

Consult the manuals and ?plot, ?par for more information.
hth,
Z

> 
> ----- Original Message -----
> From: Thomas Lumley
> Sent: 6/17/2004 9:53:33 AM
> To: rishi at post.harvard.edu
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] disappointed with x-axes in hist and density plots
> 
> > On Thu, 17 Jun 2004, Rishi Ganti wrote:
> > 
> > > I've got a few issues with the x-axes in the histogram and density
> > > plots.  First, often the default x-axis doesn't even extend to the
> > > length of my data. R often draws histogram bars  (or density
> > > lines) farther than the drawn x-axis extends. For example, I might
> > > have a histogram bar at -15,000. But I wouldn't know that, because
> > > the most negative number on the x-axis is -10,000.  The second
> > > issue is the use of scientific notation. Yes I can read it, but I
> > > don't prefer it. Is there any way for R just to print out 1000000
> > > and not 1e+6 on these charts?  Thanks for your help.  Rishi
> > >
> > 
> > You can use the axis() function to draw axes with any set of labels
> > you want.
> > 
> > 	-thomas
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From jfkincaidsu at netscape.net  Thu Jun 17 20:53:05 2004
From: jfkincaidsu at netscape.net (Joel F. Kincaid)
Date: Thu, 17 Jun 2004 14:53:05 -0400
Subject: [R] disappointed with x-axes in hist and density plots
In-Reply-To: <ab579672fb504fc7930f4c145a307543.rishi@iqmail.net>
References: <ab579672fb504fc7930f4c145a307543.rishi@iqmail.net>
Message-ID: <40D1E891.7090700@netscape.net>

  x <- rnorm(1000)
 > y <- rnorm(1000)
 > plot(x,y)
 > axis(1,-4,4)
(speculation that attempting above ... not what you want to do...)
rather do

 > plot(x,y,xlim = c(-4,4))


Rishi Ganti wrote:
> Thanks, but even with axis() I can't get the x-axis to extend to the sides.
> 
> Try, e.g., 
> 
> x = rnorm(1000)
> 
> you should have some values in excess of 3 (or below -3).
> 
> I want to draw the x-axis from -4 to 4, thus encapsulating all points.
> 
> axis(1,-4:4)
> 
> but it won't draw. It TRIES to draw it, but I don't see a -4 or 4 on the plot.
> 
> 
> ----- Original Message -----
> From: Thomas Lumley
> Sent: 6/17/2004 9:53:33 AM
> To: rishi at post.harvard.edu
> Cc: r-help at stat.math.ethz.ch
> Subject: Re: [R] disappointed with x-axes in hist and density plots
> 
> 
>>On Thu, 17 Jun 2004, Rishi Ganti wrote:
>>
>>
>>>I've got a few issues with the x-axes in the histogram and density plots.  First,
>>>often the default x-axis doesn't even extend to the length of my data. R often draws
>>>histogram bars  (or density lines) farther than the drawn x-axis extends. For example,
>>>I might have a histogram bar at -15,000. But I wouldn't know that, because the most
>>>negative number on the x-axis is -10,000.  The second issue is the use of scientific
>>>notation. Yes I can read it, but I don't prefer it. Is there any way for R just
>>>to print out 1000000 and not 1e+6 on these charts?  Thanks for your help.  Rishi
>>>
>>
>>You can use the axis() function to draw axes with any set of labels you
>>want.
>>
>>  -thomas
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From jmacdon at med.umich.edu  Thu Jun 17 20:54:03 2004
From: jmacdon at med.umich.edu (James MacDonald)
Date: Thu, 17 Jun 2004 14:54:03 -0400
Subject: [R] R help in Firefox on Windows XP
Message-ID: <s0d1b097.082@med-gwia-02a.med.umich.edu>

Works for me with Firefox 0.8.



James W. MacDonald
Affymetrix and cDNA Microarray Core
University of Michigan Cancer Center
1500 E. Medical Center Drive
7410 CCGC
Ann Arbor MI 48109
734-647-5623

>>> Prof Brian Ripley <ripley at stats.ox.ac.uk> 06/17/04 02:10PM >>>
Works for me.  Did it work with Firefox 0.8?  (I upgraded from 0.8.)
I would try 0.8 and then upgrade.

On Thu, 17 Jun 2004, Erich Neuwirth wrote:

> I had to reinstall my machine, so I installed Firefox 0.9 as browser
> I am using WinXP and R 1.9.1 beta.
> Now search in R html help does not work.
> I checked that the Java VM is working correctlt, Sun's test site
says
> my installation is OK.
> Firefoxalso tells me that
> 
> Applet Searchengine loaded
> Applet Searchengine started
> 
> it just does not find anything.
> Does anybody know how to solve this?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk 
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/ 
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From p.dalgaard at biostat.ku.dk  Thu Jun 17 21:02:37 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 17 Jun 2004 21:02:37 +0200
Subject: [R] disappointed with x-axes in hist and density plots
In-Reply-To: <ab579672fb504fc7930f4c145a307543.rishi@iqmail.net>
References: <ab579672fb504fc7930f4c145a307543.rishi@iqmail.net>
Message-ID: <x2k6y6t3ci.fsf@biostat.ku.dk>

"Rishi Ganti" <rishi at iqmail.net> writes:

> Thanks, but even with axis() I can't get the x-axis to extend to the sides.
> 
> Try, e.g., 
> 
> x = rnorm(1000)
> 
> you should have some values in excess of 3 (or below -3).
> 
> I want to draw the x-axis from -4 to 4, thus encapsulating all points.
> 
> axis(1,-4:4)
> 
> but it won't draw. It TRIES to draw it, but I don't see a -4 or 4 on the plot.

Well, the x limits likely don't extend that far. (You have noticed by
now that the axis extents are not the same as the sides of the
bounding box, right?) So set them:

hist(x,xlim=c(-4,4))

Or generically: hist(x,xlim=range(pretty(x))) tends to give the kind
of external axis that you're requesting. Less efficient use of space
though (since the axis is always longer than it needed to be).


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From christopher.knight at plant-sciences.oxford.ac.uk  Thu Jun 17 21:19:53 2004
From: christopher.knight at plant-sciences.oxford.ac.uk (Chris Knight)
Date: 17 Jun 2004 20:19:53 +0100
Subject: [R] nlme graphics in a loop problem
Message-ID: <1087499993.1357.169.camel@dops7118.plants.ox.ac.uk>

Hi, I'm fitting mixed effects models using the lme function of the nlme
package. This involves using the various associated plot functions.
However, when I attempt to fit a number of models using an loop, whilst
the models work, the plot functions fail. As a trivial example, the
following works:

library(nlme)

graphics.off()
x<-c(1:10)
y<-c(1:4,7:12)
b<-as.factor(c(rep("A",5),rep("B",5)))
model<-lme(y~x, random=~1|b)
plot(model)

however the following, identical code in a loop, doesn't:

graphics.off()
for(i in 1:2){
x<-c(1:10)
y<-c(1:4,7:12)
b<-as.factor(c(rep("A",5),rep("B",5)))
model<-lme(y~x, random=~1|b)
plot(model)
}

Mostly this is only inconvenient, since a similar plot may be created
successfully within the loop using plot(fitted(model),resid(model)),
however, I'd be keen to find out whether this is a general problem/sign
of something deeper or I'm just missing something easy that could sort
it out.

Thanks,
Chris
-- 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Dr. Christopher Knight                               Tel:+44 1865 275111
Dept. Plant Sciences                                     +44 1865 275790
South Parks Road
Oxford     OX1 3RB                                   Fax:+44 1865 275074
` ?? . , ,><(((??> 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



From andy_liaw at merck.com  Thu Jun 17 21:27:09 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 17 Jun 2004 15:27:09 -0400
Subject: [R] nlme graphics in a loop problem
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7F02@usrymx25.merck.com>

Could it be that you need print(plot(model)) inside the loop?  I believe
plot() methods in nlme are mostly lattice graphics, which needs to be
explicitly print()ed inside functions and loops.

Andy

> From: Chris Knight
> 
> Hi, I'm fitting mixed effects models using the lme function 
> of the nlme
> package. This involves using the various associated plot functions.
> However, when I attempt to fit a number of models using an 
> loop, whilst
> the models work, the plot functions fail. As a trivial example, the
> following works:
> 
> library(nlme)
> 
> graphics.off()
> x<-c(1:10)
> y<-c(1:4,7:12)
> b<-as.factor(c(rep("A",5),rep("B",5)))
> model<-lme(y~x, random=~1|b)
> plot(model)
> 
> however the following, identical code in a loop, doesn't:
> 
> graphics.off()
> for(i in 1:2){
> x<-c(1:10)
> y<-c(1:4,7:12)
> b<-as.factor(c(rep("A",5),rep("B",5)))
> model<-lme(y~x, random=~1|b)
> plot(model)
> }
> 
> Mostly this is only inconvenient, since a similar plot may be created
> successfully within the loop using plot(fitted(model),resid(model)),
> however, I'd be keen to find out whether this is a general 
> problem/sign
> of something deeper or I'm just missing something easy that could sort
> it out.
> 
> Thanks,
> Chris
> -- 
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> Dr. Christopher Knight                               Tel:+44 
> 1865 275111
> Dept. Plant Sciences                                     +44 
> 1865 275790
> South Parks Road
> Oxford     OX1 3RB                                   Fax:+44 
> 1865 275074
> ` ?? . , ,><(((??> 
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From jemond at ucsd.edu  Thu Jun 17 21:36:58 2004
From: jemond at ucsd.edu (Jennifer Emond)
Date: Thu, 17 Jun 2004 12:36:58 -0700
Subject: [R] nlme graphics in a loop problem
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7F02@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF7F02@usrymx25.merck.com>
Message-ID: <B249F2E0-C095-11D8-8742-000A95CA4DA6@ucsd.edu>

yes, use the print command, and if you can save each plot in the loop, 
that will work.
I had to do this for a loop and tables I was making.  I had to save the 
first table to a matrix, then bind each successive table to that 
matrix, then print the final matrix.  Not sure how to do this for a 
plot, though...


On Jun 17, 2004, at 12:27 PM, Liaw, Andy wrote:

> Could it be that you need print(plot(model)) inside the loop?  I 
> believe
> plot() methods in nlme are mostly lattice graphics, which needs to be
> explicitly print()ed inside functions and loops.
>
> Andy
>
>> From: Chris Knight
>>
>> Hi, I'm fitting mixed effects models using the lme function
>> of the nlme
>> package. This involves using the various associated plot functions.
>> However, when I attempt to fit a number of models using an
>> loop, whilst
>> the models work, the plot functions fail. As a trivial example, the
>> following works:
>>
>> library(nlme)
>>
>> graphics.off()
>> x<-c(1:10)
>> y<-c(1:4,7:12)
>> b<-as.factor(c(rep("A",5),rep("B",5)))
>> model<-lme(y~x, random=~1|b)
>> plot(model)
>>
>> however the following, identical code in a loop, doesn't:
>>
>> graphics.off()
>> for(i in 1:2){
>> x<-c(1:10)
>> y<-c(1:4,7:12)
>> b<-as.factor(c(rep("A",5),rep("B",5)))
>> model<-lme(y~x, random=~1|b)
>> plot(model)
>> }
>>
>> Mostly this is only inconvenient, since a similar plot may be created
>> successfully within the loop using plot(fitted(model),resid(model)),
>> however, I'd be keen to find out whether this is a general
>> problem/sign
>> of something deeper or I'm just missing something easy that could sort
>> it out.
>>
>> Thanks,
>> Chris
>> -- 
>> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>> Dr. Christopher Knight                               Tel:+44
>> 1865 275111
>> Dept. Plant Sciences                                     +44
>> 1865 275790
>> South Parks Road
>> Oxford     OX1 3RB                                   Fax:+44
>> 1865 275074
>> ` ?? . , ,><(((??>
>> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>
Jennifer A. Emond, MS
Statistician
Department of Biostatistics, UCSD
9500 Gilman Drive
M/C 0949
La Jolla, CA  92093-0949
(858) 622-5877
jemond at ucsd.edu



From andy_liaw at merck.com  Thu Jun 17 21:41:53 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 17 Jun 2004 15:41:53 -0400
Subject: [R] nlme graphics in a loop problem
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7F03@usrymx25.merck.com>

Use array for your tables and list for plots; e.g.,

allTable <- array(NA, dim=c(numRow, numCol, numIter))
allPlot <- vector(mode="list", length=numIter)

for (i in 1:numIter) {
    ...
    allPlot[[i]] <- plot(...)
    allTable[,,i] <- however you generate the table
}

HTH,
Andy

> From: Jennifer Emond 
> 
> yes, use the print command, and if you can save each plot in 
> the loop, 
> that will work.
> I had to do this for a loop and tables I was making.  I had 
> to save the 
> first table to a matrix, then bind each successive table to that 
> matrix, then print the final matrix.  Not sure how to do this for a 
> plot, though...
> 
> 
> On Jun 17, 2004, at 12:27 PM, Liaw, Andy wrote:
> 
> > Could it be that you need print(plot(model)) inside the loop?  I 
> > believe
> > plot() methods in nlme are mostly lattice graphics, which 
> needs to be
> > explicitly print()ed inside functions and loops.
> >
> > Andy
> >
> >> From: Chris Knight
> >>
> >> Hi, I'm fitting mixed effects models using the lme function
> >> of the nlme
> >> package. This involves using the various associated plot functions.
> >> However, when I attempt to fit a number of models using an
> >> loop, whilst
> >> the models work, the plot functions fail. As a trivial example, the
> >> following works:
> >>
> >> library(nlme)
> >>
> >> graphics.off()
> >> x<-c(1:10)
> >> y<-c(1:4,7:12)
> >> b<-as.factor(c(rep("A",5),rep("B",5)))
> >> model<-lme(y~x, random=~1|b)
> >> plot(model)
> >>
> >> however the following, identical code in a loop, doesn't:
> >>
> >> graphics.off()
> >> for(i in 1:2){
> >> x<-c(1:10)
> >> y<-c(1:4,7:12)
> >> b<-as.factor(c(rep("A",5),rep("B",5)))
> >> model<-lme(y~x, random=~1|b)
> >> plot(model)
> >> }
> >>
> >> Mostly this is only inconvenient, since a similar plot may 
> be created
> >> successfully within the loop using 
> plot(fitted(model),resid(model)),
> >> however, I'd be keen to find out whether this is a general
> >> problem/sign
> >> of something deeper or I'm just missing something easy 
> that could sort
> >> it out.
> >>
> >> Thanks,
> >> Chris
> >> -- 
> >> 
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> >> Dr. Christopher Knight                               Tel:+44
> >> 1865 275111
> >> Dept. Plant Sciences                                     +44
> >> 1865 275790
> >> South Parks Road
> >> Oxford     OX1 3RB                                   Fax:+44
> >> 1865 275074
> >> ` ?? . , ,><(((??>
> >> 
> ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> >>
> >> ______________________________________________
> >> R-help at stat.math.ethz.ch mailing list
> >> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >> PLEASE do read the posting guide!
> >> http://www.R-project.org/posting-guide.html
> >>
> >>
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> >
> >
> Jennifer A. Emond, MS
> Statistician
> Department of Biostatistics, UCSD
> 9500 Gilman Drive
> M/C 0949
> La Jolla, CA  92093-0949
> (858) 622-5877
> jemond at ucsd.edu
> 
> 
>



From ripley at stats.ox.ac.uk  Thu Jun 17 22:08:57 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 17 Jun 2004 21:08:57 +0100 (BST)
Subject: [R] problem with restore and some .RData
In-Reply-To: <200406171504.33033.chrysopa@insecta.ufv.br>
Message-ID: <Pine.LNX.4.44.0406172103240.6235-100000@gannet.stats>

The short answer is to add importFrom(stats, family) to MASS's NAMESPACE 
and reinstall.  Another way is to use

library(stats)

in your .Rprofile in that directory.

The real question is why your .RData is loading namespace MASS.  
Presumably some object in your workspace has MASS in its environment.
Please find out which and tell us exactly how you created it.  (Have you 
copied a MASS object, for example?)

On Thu, 17 Jun 2004, Ronaldo Reis Jr. wrote:

> Hi,
> 
> I have problem with the restore function in some .RData using R 1.9.0
> Look the error:
> 
> [ronaldo at zeus RAnalise]$ R
> ...
> Error: object 'family' not found whilst loading namespace 'MASS'
> Fatal error: unable to restore saved data in .RData
> 
> But if I load this .RData with the load() function all objects are recovered.
> 
> [ronaldo at zeus RAnalise]$ R --no-restore-data
> ...
> > ls()
> character(0)
> > load(".RData")
>  [1] ".Traceback"               "RespY"                   
>  [3] "Trat1"                    "Trat2" 
> ...
> [63] "varied.371.84210116.9"    "varied.371.842101169.5"  
> [65] "varied.371.8421011695.12"
> 
> All recovered objects work fine. I can use q() and save workspace
> image to quit session. But to load this .RData I need to use R with
> --no-restore-data option and load the .RData manually.
> 
> I try to find the problem, but I dont find this.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From wmak at brandeis.edu  Thu Jun 17 22:32:19 2004
From: wmak at brandeis.edu (wmak@brandeis.edu)
Date: Thu, 17 Jun 2004 16:32:19 -0400
Subject: [R] Re: Clustering in R
In-Reply-To: <16593.49619.392838.196401@gargle.gargle.HOWL>
References: <8975119BCD0AC5419D61A9CF1A923E9517460F@iahce2knas1.iah.bbsrc.reserved>
	<16593.49619.392838.196401@gargle.gargle.HOWL>
Message-ID: <1087504339.40d1ffd31fbe4@webmail.undergrad.brandeis.edu>

Thank you for all the responses.  I've downloaded TIGR MeV, it seems to do
everything I need it to do.  The only problem is that none of the algorithms
seem to work (K-means, hierarchical) giving errors that just say -2.  I think
probably the reason for this is that I'm running linux (since there aren't any
available windows machines), and MeV was debugged in windows.  It seems that
the lab is getting a new windows machine soon so at that time I'll be able to
try it out.

In the meantime, I'm going to see if I can get workable data by removing
unexpressed genes and using hclust with cutree to get into each of the
branches.  From the replies I've gotten, this should probably work, I just
need to sit down and try it. If I get stuck on either of these, I know where
to ask for more help.  At least now I have viable directions.  Thanks again
for everyone who responded, it was enormously helpful. 

Wayne

Quoting Martin Maechler <maechler at stat.math.ethz.ch>:

> Thanks a lot, Michael!
> 
> I cc to R-help, where this question really belongs {as the
> 'Subject' suggests itself...} -- please drop 'bioconductor' from
> CC'ing further replies.
> 
> >>>>> "michael" == michael watson (IAH-C) <michael.watson at bbsrc.ac.uk>
> >>>>>     on Thu, 17 Jun 2004 09:16:59 +0100 writes:
> 
>     michael> OK, admittedly it is not incredibly simple, but it
>     michael> is not *that* difficult.
> 
>     michael> If you are familiar with R, it should take you an
>     michael> hour or two; if unfamiliar, perhaps a day or two.
> 
>     michael> The commands you want (and need to read the help on) are:
> 
>     michael> hclust
>     michael> plclust
>     michael> cutree
> 
> and I would add  identify.hclust()   {and rect.hclust()}
> a very neat but not known / used enough function
> a link to which I have just added to the help(hclust) page.
> Look at its examples {not with example() since they are
> "dontrun"} correcting the extraneous "." in the last (and
> coolest!) example!
> 
>     michael> dendrogram
>     michael> as.dendrogram
>     michael> heatmap
> 
> where you use "dendrogram"s produced from "hclust" objects via
> as.dendrogram(<hc-obj>) or also "twins" objects produced from
> package cluster's agnes() or diana() via  
>  as.dendrogram(as.hclust( <twins-obj> ) )
> 
> help(dendrogram)  also mentions  
> "[[" (and shows examples) and cut() for cutting dendrograms and shows
> how you can depict dendrograms into its parts.
> 
>     michael> With intelligent use of hclust -> cutree -> subsetting ->
> hclust
>     michael> (in that order) you will be able to drill down
>     michael> into your dendrogram and create sub-trees - until
>     michael> you get to the level where you can see your gene
>     michael> names.
> 
> or also
>    hclust -> as.dendrogram -> cut -> ..
> 			   -> [[  ->
> 
> Note that there also is  reorder.dendrogram() for reordering
> dendrogram nodes ``sensibly'' --- something that heatmap() does,
> but you can play with quite a bit.
> Further, note Catherine Hurley's  "gclus" package which
> orders/reorders 'hclust' objects directly, but with a more
> interesting algorithm. 
> 
> Note that I'd strongly recommend to use R 1.9.1 beta for these,
> since I know which bugs in the dendrogram code I have fixed
> since R 1.9.0...
> 
>     michael> An important message to take home here is that if
>     michael> you have 14000 genes and therefore 14000 labels,
>     michael> it's going to be difficult to display your tree in
>     michael> ANY software, including the expensive commercial products.
> 
> not showing the labels and using identify.hclust() and the
> command line to extract the indices of observations in
> clusters (and subclusters) and visualize them in other, non-dendrogram
> plots,
> might well be feasible.
> 
>     michael> Let me know how you get on
> 
>     michael> Thanks
>     michael> Mick
> 
>     michael> -----Original Message-----
>     michael> From: wmak at brandeisedu [mailto:wmak at brandeis.edu] 
>     michael> Sent: 16 June 2004 21:26
>     michael> To: bioconductor at stat.math.ethz.ch
>     michael> Subject: [BioC] Clustering in R
> 
>     >> Dear list members,
> 
>     >> I'm an undergrad and I work in a lab at Brandeis.
>     >> I am trying to cluster around 14,000 genes across 6
>     >> microarray experiments.  Two of these experiments
>     >> are replicates.  I have decided to use R since it
>     >> seems to be the most complete and flexible software
>     >> package for normalization and clustering of
>     >> microarray data.
> 
>     >> The problem is that I am new to clustering and to
>     >> R.  Just to mention of a few of the problems I'm
>     >> having: the dendrogram that is drawn by R from the
>     >> agnes object is far too dense to see any of the
>     >> gene names; kmeans won't work, returning an error
>     >> saying that my data has NAs in it (there weren't
>     >> any missing values in the original table though);
>     >> I'd like to be able to see a heatmap or a
>     >> cumulative plot of expression profiles for genes
>     >> that are clustered together or are on the same
>     >> branch of the dendrogram.
> 
>     >> I know that these questions are probably very
>     >> simple, but I can't seem to find the answer to them
>     >> online or in the documentation.  If anyone can
>     >> answer these questions or direct me toward
>     >> resources that deal with clustering in R or
>     >> BioConductor, a basic tutorial that takes a
>     >> practical approach to it, I would really appreciate
>     >> it.  Any other reading material that isn't too
>     >> heavy on statistics that deals with clustering for
>     >> that matter, would be very helpful.
> 
>     >> Thank you in advance,
>     >> Wayne Mak
>



From p.connolly at hortresearch.co.nz  Thu Jun 17 22:57:43 2004
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Fri, 18 Jun 2004 08:57:43 +1200
Subject: [R] Resolution of plots
In-Reply-To: <Pine.A41.4.58.0406170744550.270076@homer03.u.washington.edu>;
	from tlumley@u.washington.edu on Thu, Jun 17, 2004 at 08:00:53AM -0700
References: <Pine.LNX.4.44.0406162247250.19571-100000@gannet.stats>
	<Pine.A41.4.58.0406170744550.270076@homer03.u.washington.edu>
Message-ID: <20040618085743.J2137@hortresearch.co.nz>

On Thu, 17-Jun-2004 at 08:00AM -0700, Thomas Lumley wrote:

|> On Wed, 16 Jun 2004, Prof Brian Ripley wrote:
|> 
|> > You will have to tell us more.  Exporting how: to what format using what
|> > device and what exact command on what operating system?
|> >
|> > The only device I know of that even knows about dpi is bitmap() and that
|> > has no such limit unless imposed by your implementation of ghostscript.
|> 
|> There is an issue with PNG. libpng provides png_set_pHYs to set resolution
|> (in pixels/metre) but provides a default if it is not set.  We don't set
|> it, and so get the default resolution.


I don't follow.  Is that in relation to the function bitmap()0 or to
png()?



-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From ripley at stats.ox.ac.uk  Thu Jun 17 23:15:11 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 17 Jun 2004 22:15:11 +0100 (BST)
Subject: [R] Resolution of plots
In-Reply-To: <20040618085743.J2137@hortresearch.co.nz>
Message-ID: <Pine.LNX.4.44.0406172210360.6367-100000@gannet.stats>

On Fri, 18 Jun 2004, Patrick Connolly wrote:

> On Thu, 17-Jun-2004 at 08:00AM -0700, Thomas Lumley wrote:
> 
> |> On Wed, 16 Jun 2004, Prof Brian Ripley wrote:
> |> 
> |> > You will have to tell us more.  Exporting how: to what format using what
> |> > device and what exact command on what operating system?
> |> >
> |> > The only device I know of that even knows about dpi is bitmap() and that
> |> > has no such limit unless imposed by your implementation of ghostscript.
> |> 
> |> There is an issue with PNG. libpng provides png_set_pHYs to set resolution
> |> (in pixels/metre) but provides a default if it is not set.  We don't set
> |> it, and so get the default resolution.
> 
> 
> I don't follow.  Is that in relation to the function bitmap() or to
> png()?

png().  That records a nominal 72dpi in the info, although it isn't much
used even by PhotoShop/GIMP etc.  (Almost all digital cameras record JPEGs
set to 72dpi, for example. If you want to use such a program to resize to
a physical size it is very easy to edit their idea of the dpi.)  

The original question was about a 96dpi limit and I still await
elucidation of what that was about.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From EFG at Stowers-Institute.org  Fri Jun 18 00:14:39 2004
From: EFG at Stowers-Institute.org (Glynn, Earl)
Date: Thu, 17 Jun 2004 17:14:39 -0500
Subject: [R] Since "package.contents" is deprecated,
	what is replacement function?
Message-ID: <200406172214.i5HMEjZE025702@hypatia.math.ethz.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040617/b013a52a/attachment.pl

From p.dalgaard at biostat.ku.dk  Fri Jun 18 00:39:11 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 18 Jun 2004 00:39:11 +0200
Subject: [R] [TIP] Apropos resolution of plots
In-Reply-To: <Pine.LNX.4.44.0406172210360.6367-100000@gannet.stats>
Message-ID: <x21xkdu7w0.fsf@biostat.ku.dk>


If you need to make JPEG plots for the web or suchlike, here's a
method for "poor mans antialiasing" that seems to turn out rather
nice:

[Requires ghostscript, netpbm]

In R (lifted out of example(contour))

bitmap("out.ppm","ppmraw",res=4*72, pointsize=12)
     data("volcano")
     rx <- range(x <- 10*1:nrow(volcano))
     ry <- range(y <- 10*1:ncol(volcano))
     ry <- ry + c(-1,1) * (diff(rx) - diff(ry))/2
     tcol <- terrain.colors(12)
     par(pty = "s", bg = "lightcyan")
     plot(x = 0, y = 0,type = "n", xlim = rx, ylim = ry, xlab = "", ylab = "")
     u <- par("usr")
     rect(u[1], u[3], u[2], u[4], col = tcol[8], border = "red")
     contour(x, y, volcano, col = tcol[2], lty = "solid", add = TRUE,
             vfont = c("sans serif", "plain"))
     title("A Topographic Map of Maunga Whau", font = 4)
     abline(h = 200*0:4, v = 200*0:4, col = "white", lty = 2, lwd= 0.2)
dev.off()

and then from the command line

  pnmsmooth -size 5 5 out.ppm > smooth.ppm
  pnmscale .25 smooth.ppm > aa.ppm
  pnmtojpeg -quality 100 aa.ppm > aa.jpg

Notice that the intermediate files tend to get rather large even
though the end result is quite compact, so don't forget to clean up:

$ ls -l *.ppm *.jpg
-rw-rw-r--    1 pd       pd          75393 Jun 18 00:34 aa.jpg
-rw-rw-r--    1 pd       pd         559887 Jun 18 00:34 aa.ppm
-rw-rw-r--    1 pd       pd        8958022 Jun 18 00:34 out.ppm
-rw-rw-r--    1 pd       pd        8957969 Jun 18 00:34 smooth.ppm

You could in principle use a lower -quality in the final jpeg
conversion, but at least to my eyes some unsightly artifacts creep in
rather quickly.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From knussear at biodiversity.unr.edu  Fri Jun 18 00:47:19 2004
From: knussear at biodiversity.unr.edu (knussear)
Date: Thu, 17 Jun 2004 15:47:19 -0700
Subject: [R] Manova question.
In-Reply-To: <40CF2DBD.3090503@wisc.edu>
References: <B49DEDFD-BED3-11D8-9708-000A959A19D8@biodiversity.unr.edu>
	<40CF1B2F.2010409@bgc-jena.mpg.de> <40CF2DBD.3090503@wisc.edu>
Message-ID: <49A936BA-C0B0-11D8-9063-000A959A19D8@biodiversity.unr.edu>


On Jun 15, 2004, at 10:11 AM, Douglas Bates wrote:

> Jens Schumacher wrote:
>> knussear wrote:
>>> Hi list,
>>>
>>> I'm attempting to re-create a Repeated Measures Compositional 
>>> Analysis as
>>>  described in the work by Aebischer et. al. (Ecology. 1993. 74(5): 
>>> 1313-1325).
>>>
>>> In this paper they describe transitions of data into a log ratio 
>>> difference matrix, from which they obtain two matrices using a 
>>> monova routine.
>>>
>>> I am able to produce the second of the two matrices, but I'm having 
>>> trouble with the first.
>>>
>>> the difference matrix going in is given here.
>>>
>>> Animal    Scrub    Bl wood    Con wood     Grass
>>> 1    0.970    -2.380    -5.154    -9.408
>>> 2    1.217    -0.173    -4.955    -5.521
>>> 3    1.178    -0.248    -4.089    0.338
>>> 4    0.520    0.466    -4.801    -1.946
>>> 5    8.445    9.319    10.753    8.171
>>> 6    8.654    9.327    10.732    8.152
>>> 7    8.429    9.350    10.818    8.141
>>> 8    9.120    9.565    3.813    8.127
>>> 9    9.227    9.882    3.813    7.779
>>> 10    9.423    8.086    3.813    8.539
>>> 11    9.626    9.392    3.813    8.135
>>> 12    9.234    8.302    3.813    8.537
>>> 13    8.672    8.908    9.832    8.416
>>>
>>>
>>> And the first of the matrices is given here, and is "matrix of 
>>> mean-corrected sums of squares and cross products calculated from 
>>> the difference matrix."
>>>
>>>
>>>     Scrub    Bl wood    Con wood     Grass
>>> Scrub    179.52    214.59    244.58    273.75
>>> Bl wood    214.59    268.44    314.35    343.86
>>> Con wood     244.58    314.35    471.09    400.22
>>> Grass    273.75    343.86    400.22    477.78
>>>
>>>
>>> From manova on the data set I can get the diagonal of the matrix, 
>>> but not the others.
>>>
>>> manova(y ~ NULL)
>>>
>>> Terms:
>>>                 Residuals
>>> Scrub            179.5273
>>> Bl.wood          268.4347
>>> Con.wood         471.0845
>>> Grass            477.8014
>>> Deg. of Freedom        12
>>>
>>>
>>> Could anyone offer a suggestion ?
>>>
>>> Thanks
>>>
>>>
>>> Ken
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>>
>>>
>> Let   data.matrix    be the above difference matrix. You obtain the 
>> "raw sums of squares and cross-products matrix" by
>> R2 <- t(data.matrix) %*% data.matrix
>> or even
>> R2 <- crossprod(data.matrix, data.matrix)
>
> or, the preferred form,
>
> R2 <- crossprod(data.matrix)
>
>

I'm not sure if this helps but I get the correct matrices if I use JMP. 
It refers to them as E&H matrices. Is there a way to get R to produce 
these?

Thanks

Ken



From fzh113 at hecky.it.northwestern.edu  Fri Jun 18 02:04:46 2004
From: fzh113 at hecky.it.northwestern.edu (Fred)
Date: Thu, 17 Jun 2004 19:04:46 -0500
Subject: [R] Is there an easy way to generate linearly independent vectors
Message-ID: <000e01c454c7$de2aaba0$a7560d18@f0z6305>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040617/812e56ab/attachment.pl

From bennetp at mcmaster.ca  Fri Jun 18 02:37:11 2004
From: bennetp at mcmaster.ca (Patrick Bennett)
Date: Thu, 17 Jun 2004 20:37:11 -0400
Subject: [R] can't get text to appear over individual panels in multi-panel
	plot
Message-ID: <A34058C2-C0BF-11D8-A80C-000393D55244@mcmaster.ca>

I'm trying to learn how to create Trellis multi-panel plots, but I'm 
having some trouble reproducing the graphs shown in Venables & Ripley 
(2002) (e.g., Figs 4.14 & 4.15). Actually, everything looks fine except 
for the fact that I can't see any text above the individual panels.

I'm using R 1.9.0 for OS-X running on Mac OSX 10.3.3, and I'm drawing 
the graphs into the Quartz device.

Sorry for bothering the list with what is probably a trivial question, 
but I've run out of ideas...

Any help would be appreciated.


Cheers,
pjb


Patrick J. Bennett, Ph.D.
Professor & Canada Research Chair
Department of Psychology
McMaster University
1280 Main St. West
Hamilton, ON L8S 4K1 Canada

Office: 905-525-9140 ext. 23012				FAX: 905-529-6225
URL: www.psychology.mcmaster.ca/bennett/ 		www.chairs.gc.ca

"I understand small business growth. I was one." -- George W. Bush, 
President of the United States of America



From baron at psych.upenn.edu  Fri Jun 18 02:45:57 2004
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Thu, 17 Jun 2004 20:45:57 -0400
Subject: [R] Is there an easy way to generate linearly independent vectors
In-Reply-To: <000e01c454c7$de2aaba0$a7560d18@f0z6305>
References: <000e01c454c7$de2aaba0$a7560d18@f0z6305>
Message-ID: <20040618004557.GA7245@psych>

On 06/17/04 19:04, Fred wrote:
>Dear R-listers:
>
>I am trying to test an algorithm on a set of linearly independent vectors
>{x1,x2,...,xn}.

Well, here's an idea, for 10 vectors of length 10,
as columns of a matrix m1.  The 11th seems to be needed.

m1 <- matrix(rnorm(110),10,11)
for (i in 2:11) {
 m1[,i] <- resid(lm(m1[,i] ~ m1[, 1:(i-1)]))
}

Test it with cor(m1[,-11])

I'm sure there are better ways.

Of course
m1 <- matrix(rnorm(100),10,10)
is ALMOST what you want.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron



From andy_liaw at merck.com  Fri Jun 18 02:50:24 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 17 Jun 2004 20:50:24 -0400
Subject: [R] Is there an easy way to generate linearly independent vec tors
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7F05@usrymx25.merck.com>

I believe eigen(), svd() and qr() can all do it.

Andy

> From: Jonathan Baron
> 
> On 06/17/04 19:04, Fred wrote:
> >Dear R-listers:
> >
> >I am trying to test an algorithm on a set of linearly 
> independent vectors
> >{x1,x2,...,xn}.
> 
> Well, here's an idea, for 10 vectors of length 10,
> as columns of a matrix m1.  The 11th seems to be needed.
> 
> m1 <- matrix(rnorm(110),10,11)
> for (i in 2:11) {
>  m1[,i] <- resid(lm(m1[,i] ~ m1[, 1:(i-1)]))
> }
> 
> Test it with cor(m1[,-11])
> 
> I'm sure there are better ways.
> 
> Of course
> m1 <- matrix(rnorm(100),10,10)
> is ALMOST what you want.
> 
> Jon
> -- 
> Jonathan Baron, Professor of Psychology, University of Pennsylvania
> Home page:            http://www.sas.upenn.edu/~baron



From deepayan at stat.wisc.edu  Fri Jun 18 02:59:20 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 17 Jun 2004 19:59:20 -0500
Subject: [R] can't get text to appear over individual panels in
	multi-panel plot
In-Reply-To: <A34058C2-C0BF-11D8-A80C-000393D55244@mcmaster.ca>
References: <A34058C2-C0BF-11D8-A80C-000393D55244@mcmaster.ca>
Message-ID: <200406171959.20136.deepayan@stat.wisc.edu>

On Thursday 17 June 2004 19:37, Patrick Bennett wrote:
> I'm trying to learn how to create Trellis multi-panel plots, but I'm
> having some trouble reproducing the graphs shown in Venables & Ripley
> (2002) (e.g., Figs 4.14 & 4.15). Actually, everything looks fine
> except for the fact that I can't see any text above the individual
> panels.
>
> I'm using R 1.9.0 for OS-X running on Mac OSX 10.3.3, and I'm drawing
> the graphs into the Quartz device.
>
> Sorry for bothering the list with what is probably a trivial
> question, but I've run out of ideas...
>
> Any help would be appreciated.

I don't have that edition, and it's difficult to guess without code, but 
have you looked at the scripts that come with the MASS package ? They 
are supposed to reproduce the analysis done in the book (I'm assuming 
that the book you are referring to is in fact MASS). It might be as 
simple as replacing text() calls by ltext().

Deepayan



From fzh113 at hecky.it.northwestern.edu  Fri Jun 18 03:54:38 2004
From: fzh113 at hecky.it.northwestern.edu (Fred)
Date: Thu, 17 Jun 2004 20:54:38 -0500
Subject: [R] Is there an easy way to generate linearly independent vec tors
References: <3A822319EB35174CA3714066D590DCD504AF7F05@usrymx25.merck.com>
Message-ID: <000b01c454d7$379d3d10$a7560d18@f0z6305>

I want to get linearly independent vectors, not orthogonal ones.
The functions eigen, svd, I think it may provide orthogonal
vectors which are not what I expect.


----- Original Message ----- 
From: "Liaw, Andy" <andy_liaw at merck.com>
To: "'Jonathan Baron'" <baron at psych.upenn.edu>; "Fred"
<fzh113 at hecky.it.northwestern.edu>
Cc: "R-help mailing list" <r-help at stat.math.ethz.ch>
Sent: Thursday, June 17, 2004 7:50 PM
Subject: RE: [R] Is there an easy way to generate linearly independent vec
tors


> I believe eigen(), svd() and qr() can all do it.
>
> Andy
>
> > From: Jonathan Baron
> >
> > On 06/17/04 19:04, Fred wrote:
> > >Dear R-listers:
> > >
> > >I am trying to test an algorithm on a set of linearly
> > independent vectors
> > >{x1,x2,...,xn}.
> >
> > Well, here's an idea, for 10 vectors of length 10,
> > as columns of a matrix m1.  The 11th seems to be needed.
> >
> > m1 <- matrix(rnorm(110),10,11)
> > for (i in 2:11) {
> >  m1[,i] <- resid(lm(m1[,i] ~ m1[, 1:(i-1)]))
> > }
> >
> > Test it with cor(m1[,-11])
> >
> > I'm sure there are better ways.
> >
> > Of course
> > m1 <- matrix(rnorm(100),10,10)
> > is ALMOST what you want.
> >
> > Jon
> > -- 
> > Jonathan Baron, Professor of Psychology, University of Pennsylvania
> > Home page:            http://www.sas.upenn.edu/~baron
>
>
> --------------------------------------------------------------------------
----
> Notice:  This e-mail message, together with any attachments, contains
information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New
Jersey, USA 08889), and/or its affiliates (which may be known outside the
United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as
Banyu) that may be confidential, proprietary copyrighted and/or legally
privileged. It is intended solely for the use of the individual or entity
named on this message.  If you are not the intended recipient, and have
received this message in error, please notify us immediately by reply e-mail
and then delete it from your system.
> --------------------------------------------------------------------------
----
>



From bennetp at mcmaster.ca  Fri Jun 18 05:24:08 2004
From: bennetp at mcmaster.ca (Patrick Bennett)
Date: Thu, 17 Jun 2004 23:24:08 -0400
Subject: [R] can't get text to appear over individual panels in
	multi-panel plot
In-Reply-To: <200406171959.20136.deepayan@stat.wisc.edu>
References: <A34058C2-C0BF-11D8-A80C-000393D55244@mcmaster.ca>
	<200406171959.20136.deepayan@stat.wisc.edu>
Message-ID: <F5DEEDB8-C0D6-11D8-9C1A-000393D55244@mcmaster.ca>

I neglected to say that I am using the R-Aqua interface and the MASS, 
grid, & lattice packages.

Here is one specific example where I'm having trouble.

After loading the crabs data set, I create the figure with the 
following code (which is taken from MASS):

lcrabs.pc<-predict(princomp(log(crabs[,4:8])))
sex<-crabs$sex;levels(sex)<-c("Female","Male")
sp<-crabs$sp;levels(sp)<-c("Blue","Orange")
splom(~lcrabs.pc[,1:3] | sp*sex,cex=0.5,pscales=0)

The figure is plotted in the Quartz window: everything looks OK except 
for the lack of text above the individual panels.



Patrick J. Bennett, Ph.D.
Professor & Canada Research Chair
Department of Psychology
McMaster University
1280 Main St. West
Hamilton, ON L8S 4K1 Canada

Office: 905-525-9140 ext. 23012				FAX: 905-529-6225
URL: www.psychology.mcmaster.ca/bennett/ 		www.chairs.gc.ca

"Verbosity leads to unclear, inarticulate things." -- Dan Quayle
On Jun 17, 2004, at 8:59 PM, Deepayan Sarkar wrote:

> On Thursday 17 June 2004 19:37, Patrick Bennett wrote:
>> I'm trying to learn how to create Trellis multi-panel plots, but I'm
>> having some trouble reproducing the graphs shown in Venables & Ripley
>> (2002) (e.g., Figs 4.14 & 4.15). Actually, everything looks fine
>> except for the fact that I can't see any text above the individual
>> panels.
>>
>> I'm using R 1.9.0 for OS-X running on Mac OSX 10.3.3, and I'm drawing
>> the graphs into the Quartz device.
>>
>> Sorry for bothering the list with what is probably a trivial
>> question, but I've run out of ideas...
>>
>> Any help would be appreciated.
>
> I don't have that edition, and it's difficult to guess without code, 
> but
> have you looked at the scripts that come with the MASS package ? They
> are supposed to reproduce the analysis done in the book (I'm assuming
> that the book you are referring to is in fact MASS). It might be as
> simple as replacing text() calls by ltext().
>
> Deepayan
>



From deepayan at stat.wisc.edu  Fri Jun 18 05:47:25 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Thu, 17 Jun 2004 22:47:25 -0500
Subject: [R] can't get text to appear over individual panels in
	multi-panel plot
In-Reply-To: <F5DEEDB8-C0D6-11D8-9C1A-000393D55244@mcmaster.ca>
References: <A34058C2-C0BF-11D8-A80C-000393D55244@mcmaster.ca>
	<200406171959.20136.deepayan@stat.wisc.edu>
	<F5DEEDB8-C0D6-11D8-9C1A-000393D55244@mcmaster.ca>
Message-ID: <200406172247.25893.deepayan@stat.wisc.edu>

On Thursday 17 June 2004 22:24, Patrick Bennett wrote:
> I neglected to say that I am using the R-Aqua interface and the MASS,
> grid, & lattice packages.
>
> Here is one specific example where I'm having trouble.
>
> After loading the crabs data set, I create the figure with the
> following code (which is taken from MASS):
>
> lcrabs.pc<-predict(princomp(log(crabs[,4:8])))
> sex<-crabs$sex;levels(sex)<-c("Female","Male")
> sp<-crabs$sp;levels(sp)<-c("Blue","Orange")
> splom(~lcrabs.pc[,1:3] | sp*sex,cex=0.5,pscales=0)
>
> The figure is plotted in the Quartz window: everything looks OK
> except for the lack of text above the individual panels.

Here's what I get (on a pdf device), and it looks OK to me.

http://www.stat.wisc.edu/~deepayan/R/crabs.pdf

If this is not what you see, then there might be a problem with your 
device driver. 

Deepayan



From vs017185 at mnsi.net  Fri Jun 18 05:58:10 2004
From: vs017185 at mnsi.net (russell alexander)
Date: Fri, 18 Jun 2004 03:58:10 -0000
Subject: [R] msm
Message-ID: <001801c1c9ca$a08a9ce0$b17dfea9@vs017185>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040618/2b86c83f/attachment.pl

From deepayan at stat.wisc.edu  Fri Jun 18 07:26:38 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Fri, 18 Jun 2004 00:26:38 -0500
Subject: [R] can't get text to appear over individual panels in
	multi-panel plot
In-Reply-To: <A769FEBA-C0DB-11D8-9C1A-000393D55244@mcmaster.ca>
References: <A34058C2-C0BF-11D8-A80C-000393D55244@mcmaster.ca>
	<200406172247.25893.deepayan@stat.wisc.edu>
	<A769FEBA-C0DB-11D8-9C1A-000393D55244@mcmaster.ca>
Message-ID: <200406180026.38335.deepayan@stat.wisc.edu>


On Thursday 17 June 2004 22:57, Patrick Bennett wrote:

> yes, i can reproduce that same graph when i print to the pdf-device.
> but the panel titles do not appear when I print to the Quartz-device.

Hmm. I won't be able to help you then, let's hope someone else can.

Deepayan

> On Jun 17, 2004, at 11:47 PM, Deepayan Sarkar wrote:
> > On Thursday 17 June 2004 22:24, Patrick Bennett wrote:
> >> I neglected to say that I am using the R-Aqua interface and the
> >> MASS, grid, & lattice packages.
> >>
> >> Here is one specific example where I'm having trouble.
> >>
> >> After loading the crabs data set, I create the figure with the
> >> following code (which is taken from MASS):
> >>
> >> lcrabs.pc<-predict(princomp(log(crabs[,4:8])))
> >> sex<-crabs$sex;levels(sex)<-c("Female","Male")
> >> sp<-crabs$sp;levels(sp)<-c("Blue","Orange")
> >> splom(~lcrabs.pc[,1:3] | sp*sex,cex=0.5,pscales=0)
> >>
> >> The figure is plotted in the Quartz window: everything looks OK
> >> except for the lack of text above the individual panels.
> >
> > Here's what I get (on a pdf device), and it looks OK to me.
> >
> > http://www.stat.wisc.edu/~deepayan/R/crabs.pdf
> >
> > If this is not what you see, then there might be a problem with
> > your device driver.
> >
> > Deepayan



From jari.oksanen at oulu.fi  Fri Jun 18 08:28:35 2004
From: jari.oksanen at oulu.fi (Jari Oksanen)
Date: Fri, 18 Jun 2004 09:28:35 +0300
Subject: [R] can't get text to appear over individual panels in multi-panel
	plot
In-Reply-To: <200406180026.38335.deepayan@stat.wisc.edu>
References: <A34058C2-C0BF-11D8-A80C-000393D55244@mcmaster.ca>
	<200406172247.25893.deepayan@stat.wisc.edu>
	<A769FEBA-C0DB-11D8-9C1A-000393D55244@mcmaster.ca>
	<200406180026.38335.deepayan@stat.wisc.edu>
Message-ID: <BA63696F-C0F0-11D8-B5CA-000A95C76CA8@oulu.fi>


On 18 Jun 2004, at 8:26, Deepayan Sarkar wrote:

>
> On Thursday 17 June 2004 22:57, Patrick Bennett wrote:
>
>> yes, i can reproduce that same graph when i print to the pdf-device.
>> but the panel titles do not appear when I print to the Quartz-device.
>
> Hmm. I won't be able to help you then, let's hope someone else can.

I think this is a problem with the quartz device. I have often see that 
margin texts are plotted even in ordinary plot() if quartz thinks there 
is no space for them. They do still appear if you copy the screen 
graphics as a pdf file. In Linux (my principal platform) I typically 
reduce the white margins, but if I use the same mar pars in MacOS X I 
won't get axis labels. Quartz is the culprit I suppose.

Actually, in your example I couldn't get the texts when I saved the 
plot as a pdf (menu entry). However, when I opened an X11 device, the 
text was reproduced OK.  So it looks like a quartz problem.

For X11 in MacOS X: It may not be in the default installation, but it 
is in the installation CD/DVD of MacOS X. Then you got to start it 
explicitly before launching x11() within R shell. In general, I 
wouldn't recommend using x11() in Mac, since quartz() looks so much 
better: x11 looks just as clumsy as x11 in Linux or the ordinary 
Windows plotting device in some other OS. -- And beware: I have a 
suspicion that if you stop your X11 in MacOS X, your mouse will die at 
logout and you got to boot (or restart the mouse demon if you know  who 
he is).

cheers, jari oksanen
--
Jari Oksanen, Oulu, Finland



From tjrc at sanger.ac.uk  Fri Jun 18 09:47:08 2004
From: tjrc at sanger.ac.uk (Tim Cutts)
Date: Fri, 18 Jun 2004 08:47:08 +0100
Subject: [R] Compiling R with Intel compilers - recommended options?
Message-ID: <B3594208-C0FB-11D8-9862-000A95B2B140@sanger.ac.uk>


Hi,

I'm a sysadmin who's been tasked with installing R on our 1000-node 
compute cluster.
I have licences for the Intel C and FORTRAN compilers, so I'm using the 
following to compile:

CFLAGS="-O2 -axWK"
FFLAGS=$CFLAGS
CXXFLAGS=$CFLAGS

CC=icc
F77=ifort
CXX=icc

FPICFLAGS=-fpic

./configure --without-x --without-tcltk

The compilation seems to go OK, with a few warnings.  What concerns me 
is when I run the make check - how long should this take?  I've had one 
running for over 12 hours now, and it's still on:

running code in 'base-Ex.R' ...

and hasn't produced any output since; it just sits there burning CPU.  
I guess my question is:  I'm sure someone has successfully compiled R 
on X86 Linux using the Intel compilers before - what options did you 
use to make it work?  And how long should this check phase take?

Many thanks...

Tim

-- 
Dr Tim Cutts
Informatics Systems Group
Wellcome Trust Sanger Institute
Hinxton, Cambridge, CB10 1SA, UK



From ripley at stats.ox.ac.uk  Fri Jun 18 10:11:42 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 18 Jun 2004 09:11:42 +0100 (BST)
Subject: [R] Compiling R with Intel compilers - recommended options?
In-Reply-To: <B3594208-C0FB-11D8-9862-000A95B2B140@sanger.ac.uk>
Message-ID: <Pine.LNX.4.44.0406180854400.7213-100000@gannet.stats>

What OS and hardware is this?  Eventually you mention `X86 Linux' but are
you using x86 Linux and if so how fast processors?  On a 3GHz machine make
check takes a couple of minutes using gcc (and in our experience Portland
Group is faster than gcc -- we have not tried Intel as local advice
suggests it is slower than PG).

Which version of R?  R 1.9.0 does this with gcc 3.4.0 on Linux ix86, at
the first use of LAPACK.  So if you are not trying 1.9.1beta (to be
released on Monday), please do so.  After that, make check does give
output so where exactly does it hang, and if you interrupt that check, how
far has it got in the output file?

On Fri, 18 Jun 2004, Tim Cutts wrote:

> 
> Hi,
> 
> I'm a sysadmin who's been tasked with installing R on our 1000-node 
> compute cluster.
> I have licences for the Intel C and FORTRAN compilers, so I'm using the 
> following to compile:
> 
> CFLAGS="-O2 -axWK"
> FFLAGS=$CFLAGS
> CXXFLAGS=$CFLAGS
> 
> CC=icc
> F77=ifort
> CXX=icc
> 
> FPICFLAGS=-fpic
> 
> ./configure --without-x --without-tcltk
> 
> The compilation seems to go OK, with a few warnings.  What concerns me 
> is when I run the make check - how long should this take?  I've had one 
> running for over 12 hours now, and it's still on:
> 
> running code in 'base-Ex.R' ...
> 
> and hasn't produced any output since; it just sits there burning CPU.  
> I guess my question is:  I'm sure someone has successfully compiled R 
> on X86 Linux using the Intel compilers before - what options did you 
> use to make it work?  

Did you search the archives?  I found a lot of hits, none recent for ix86 
Linux.

> And how long should this check phase take?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Ted.Harding at nessie.mcc.ac.uk  Fri Jun 18 10:15:38 2004
From: Ted.Harding at nessie.mcc.ac.uk ( (Ted Harding))
Date: Fri, 18 Jun 2004 09:15:38 +0100 (BST)
Subject: [R] Is there an easy way to generate linearly independent ve
In-Reply-To: <000b01c454d7$379d3d10$a7560d18@f0z6305>
Message-ID: <XFMail.040618091538.Ted.Harding@nessie.mcc.ac.uk>

On 18-Jun-04 Fred wrote:
> I want to get linearly independent vectors, not orthogonal ones.
> The functions eigen, svd, I think it may provide orthogonal
> vectors which are not what I expect.

It depends what sort of characteristics you want your non-orthogonal
linearly independent vectors to have.

It's very easy to produce examples of such vectors: simple and
easy examples are like (e.g. for n=4)

  1 0 0 0
  1 1 0 0
  1 1 1 0
  1 1 1 1

or

  1 0 0 0
  1 1 0 0
  0 1 1 0
  0 0 1 1

and similar (all of which can be extended downwards arbitrarily
if you want n linearly independent vectors in k > n dimensions).

If these are too simplistic for you, please indicate how "interesting"
you want them to be.

The suggestions about using random numbers should work: it's not
likely that an NxN matrix of random numbers will be near singular,
so a test of the matrix to reject any which are too close is going
to be fairly efiicient.

E.g. you might consider a simple test like accepting the matrix M
if

  max(abs(eigen(M)$values)))/min(abs(eigen(M)$values))) < 10

(the "10" is arbitrary, for illustration).

Hope this helps,
Ted.


--------------------------------------------------------------------
E-Mail: (Ted Harding) <Ted.Harding at nessie.mcc.ac.uk>
Fax-to-email: +44 (0)870 167 1972
Date: 18-Jun-04                                       Time: 09:15:38
------------------------------ XFMail ------------------------------



From laura at env.leeds.ac.uk  Fri Jun 18 10:51:51 2004
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Fri, 18 Jun 2004 09:51:51 +0100 (BST)
Subject: [R] multiple plots for data sets of inconsistent size
Message-ID: <Pine.LNX.4.44.0406180932130.23233-100000@env-pc-phd13>

I am wanting to plot pca loadings onto sites superimposed on a contour
map. There are a maximum of 20 sites at which loadings might appear -
however due to the nature of my data, missing data has meant that some
stations have not been included in some of the pca.

For example, I am performing pca for 5 different variables at 20 different
sites for a number of different time intervals and want to perform a cross
comparison to see where key trends lie. Where data is not available for
any particular station at any given time interval I am still keen to
analyse the principal components on the others.

I have been using the following code to produce the plots I want:

for (i in 1:4) {
 image(east.grid,north.grid,t(map.matrix),col=my.colors,
                axes=TRUE,xlab="",ylab="")

text(stations$x.axis,stations$y.axis,labels=as.character(stations$station))
 title(paste("Component",i))
 pc <- p6.5am.pca$rotation[,i]
 pc.pos <- (pc > 0)
 if (any(pc.pos)) {
  symbols(stations$x.axis[pc.pos],stations$y.axis[pc.pos],
        circles=scale*pc[pc.pos],lwd=2,fg="red",inches=FALSE,add=TRUE)
 }
 if (any(!pc.pos)) {
  symbols(stations$x.axis[!pc.pos],stations$y.axis[!pc.pos],
        circles=-scale*pc[!pc.pos],lwd=2,lty=2,fg="blue",inches=FALSE,
        add=TRUE)
 }
 box()
}

My problem is this - due to the large number of plots required I would
like to be able to batch process the results, but in the case of applying
the above code when one (or more) station(s) is missing from the pca I get
an error message as the x and y lengths differ. Is there a way I can get the code
to simply return a zero loading at a site if it hasn't been included in
the calculation of the pca? (or is there another better way around this
problem?)

I hope that this question makes some sense.

Any ideas?
Thanks,
laura



From davidD at qimr.edu.au  Fri Jun 18 11:08:45 2004
From: davidD at qimr.edu.au (David Duffy)
Date: Fri, 18 Jun 2004 19:08:45 +1000 (EST)
Subject: [R] OT: virus affecting a list subscriber
In-Reply-To: <Pine.LNX.4.58.0403101048200.10284@orpheus.qimr.edu.au>
References: <200403091131.i29BL0Td000655@hypatia.math.ethz.ch>
	<Pine.LNX.4.58.0403101048200.10284@orpheus.qimr.edu.au>
Message-ID: <Pine.LNX.4.58.0406181900270.11805@orpheus.qimr.edu.au>

I presume I am not alone in receiving return-to-sender mailings provoked
by a well known virus eg

r-core at isem.univ-montp2.fr
    (reason: 550 <r-core at isem.univ-montp2.fr>... User unknown)
torsten.hothorn at isem.univ-montp2.fr
    (reason: 550 <torsten.hothorn at isem.univ-montp2.fr>... User unknown)
bcaffo at isem.univ-montp2.fr
    (reason: 550 <bcaffo at isem.univ-montp2.fr>... User unknown)
fharrell at isem.univ-montp2.fr
    (reason: 550 <fharrell at isem.univ-montp2.fr>... User unknown)
r.hankin at isem.univ-montp2.fr
    (reason: 550 <r.hankin at isem.univ-montp2.fr>... User unknown)
therneau at isem.univ-montp2.fr
    (reason: 550 <therneau at isem.univ-montp2.fr>... User unknown)
bill.venables at isem.univ-montp2.fr
    (reason: 550 <bill.venables at isem.univ-montp2.fr>... User unknown)

If one has not recently virus-checked one's computer recently, it might
be a good idea.  I *of course* run linux, so it can't be me ;)



From jeroen.clarysse at easynet.be  Fri Jun 18 11:23:54 2004
From: jeroen.clarysse at easynet.be (jeroen clarysse)
Date: Fri, 18 Jun 2004 11:23:54 +0200
Subject: [R] [Q] Newbie (continued.. at least I got R running allready  :-)
Message-ID: <01f801c45515$f96de6e0$b566210a@p102pw181>

Hi all

a week ago, I posted a newbie question in data smoothing &
maximum-extraction with R. I got quite a lot of response, but I'm still
kinda stuck with it...

I'll restate the problem : i got a datafile with 2400 measuerements (every
250msec) of a CO2 measurement device, capturing the breath of a subject. I
uploaded such a sample here :

http://www.psy.kuleuven.ac.be/leerpsy/data.csv

now I wish to figure out where each breath expiration ceiling takes place ,
as shown on this graph :

http://www.psy.kuleuven.ac.be/leerpsy/graph.bmp


I'm kinda stuck on how to get this running in R.

I really hope someone can help me out. If you guys can get me running, I
promise to promote R as often as I can here on our faculty (which still uses
Statistica for almost everything)

thanks a million in advance !



From dmb at mrc-dunn.cam.ac.uk  Fri Jun 18 11:57:09 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Fri, 18 Jun 2004 10:57:09 +0100 (BST)
Subject: [R] non-linear binning? power-law in R
In-Reply-To: <40D1CBB3.24542.5CD1AA5@localhost>
Message-ID: <Pine.LNX.4.21.0406181051370.15511-100000@mail.mrc-dunn.cam.ac.uk>

On Thu, 17 Jun 2004, Dr. Herwig Meschke wrote:

>Why not try to avoid binning (and density plot) at all? An alternative 
>could be a qqplot (as a log-log-plot), e.g.
>
>plot(ppoints(length(x4)), x4[order(x4)], log="xy")
>abline(lm(log(x4[order(x4)])~log(ppoints(length(x4)))), col="red")
>
>If the assumptions of uniform distribution and power transformation 
>y=a*x**b are true, the coefficient of lm estimates the exponent b.


Thanks, this looks very cool (although I am going to have to learn what it
all means ;)

However, playing with the above with the following for example...


x4 <- runif(100)**4

plot(ppoints(length(x4)), x4[order(x4)], log="xy")
abline(lm(log(x4[order(x4)])~log(ppoints(length(x4)))), col="red")


Shows (perhaps after a few repeats) that the fitted curve is dominated by
the rare events, and the rare events have the highest variance, leading to
potential big errors. 

By uniformly binning the log transformed data you group the rarest values
in the bigest bin, and can therefore get better estimates of the true
slope of the curve. 

My problem is now a technical one of working out how to do this, so isn't
too fundamental. 

I can post up the differences in the values (and error) of the estimated
curves when I get round to doing this.

Thanks again for the help,

Dan.


>
>Herwig
>
>



From jinss at hkusua.hku.hk  Fri Jun 18 12:13:06 2004
From: jinss at hkusua.hku.hk (Jin Shusong)
Date: Fri, 18 Jun 2004 18:13:06 +0800
Subject: [R] Compiling R with Intel compilers - recommended options?
In-Reply-To: <Pine.LNX.4.44.0406180854400.7213-100000@gannet.stats>
References: <B3594208-C0FB-11D8-9862-000A95B2B140@sanger.ac.uk>
	<Pine.LNX.4.44.0406180854400.7213-100000@gannet.stats>
Message-ID: <20040618101306.GB2548@S77.hku.hk>

Dear Cutts and Prof. Ripley,
On Fri, Jun 18, 2004 at 09:11:42AM +0100, Prof Brian Ripley wrote:
Some benchmarks show icc is faster than gcc and pgcc.  Even
fortran compiler ifort is also faster than pgf77 (See the
result on www.polyhedron.com).  But I still recommend gcc
and g77 as c and fortran compilers if you are on X86 Linux
platforms.  I once compiled R 1.9.0 by icc and ifort.  It
failed some of the checks.  I also found that the difference
between the speed of gcc+g77 and icc+ifort was not
significant when I 'make test-Gct'.
My platform Gentoo Linux pentium4 1.5GHz, 256M RAM and kernel
2.4.26.
gcc version 3.3
icc and ifort version 8.0 non-commercial version.

BTW, can you tell me the flags you used to compile R by pgcc
and pgf77. Thank you.

   Jin
> What OS and hardware is this?  Eventually you mention `X86 Linux' but are
> you using x86 Linux and if so how fast processors?  On a 3GHz machine make
> check takes a couple of minutes using gcc (and in our experience Portland
> Group is faster than gcc -- we have not tried Intel as local advice
> suggests it is slower than PG).
> 
> Which version of R?  R 1.9.0 does this with gcc 3.4.0 on Linux ix86, at
> the first use of LAPACK.  So if you are not trying 1.9.1beta (to be
> released on Monday), please do so.  After that, make check does give
> output so where exactly does it hang, and if you interrupt that check, how
> far has it got in the output file?
> 
> On Fri, 18 Jun 2004, Tim Cutts wrote:
> 
> > 
> > Hi,
> > 
> > I'm a sysadmin who's been tasked with installing R on our 1000-node 
> > compute cluster.
> > I have licences for the Intel C and FORTRAN compilers, so I'm using the 
> > following to compile:
> > 
> > CFLAGS="-O2 -axWK"
> > FFLAGS=$CFLAGS
> > CXXFLAGS=$CFLAGS
> > 
> > CC=icc
> > F77=ifort
> > CXX=icc
> > 
> > FPICFLAGS=-fpic
> > 
> > ./configure --without-x --without-tcltk
> > 
> > The compilation seems to go OK, with a few warnings.  What concerns me 
> > is when I run the make check - how long should this take?  I've had one 
> > running for over 12 hours now, and it's still on:
> > 
> > running code in 'base-Ex.R' ...
> > 
> > and hasn't produced any output since; it just sits there burning CPU.  
> > I guess my question is:  I'm sure someone has successfully compiled R 
> > on X86 Linux using the Intel compilers before - what options did you 
> > use to make it work?  
> 
> Did you search the archives?  I found a lot of hits, none recent for ix86 
> Linux.
> 
> > And how long should this check phase take?
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From tjrc at sanger.ac.uk  Fri Jun 18 12:49:52 2004
From: tjrc at sanger.ac.uk (Tim Cutts)
Date: Fri, 18 Jun 2004 11:49:52 +0100
Subject: [R] Compiling R with Intel compilers - recommended options?
In-Reply-To: <Pine.LNX.4.44.0406180854400.7213-100000@gannet.stats>
References: <Pine.LNX.4.44.0406180854400.7213-100000@gannet.stats>
Message-ID: <3A27A81E-C115-11D8-B8FD-000A95B2B140@sanger.ac.uk>


Sorry, I was a bit sparing with details.  Early in the morning, and all 
that.

Many thanks for the swift reply!

On 18 Jun 2004, at 9:11 am, Prof Brian Ripley wrote:

> What OS and hardware is this?

Hardware:  700+ machines are 800 MHz Pentium III, 1 GB RAM, Red Hat 7.2
            168 machines are dual 2.8 GHz Xeon, 4 GB RAM, Red Hat 8.0

I'm compiling on the older machines to make sure there won't be library 
problems.

The compiler versions are:

Intel(R) C++ Compiler for 32-bit applications, Version 8.0   Build 
20031016Z Package ID: l_cc_p_8.0.055

Intel(R) Fortran Compiler for 32-bit applications, Version 8.0   Build 
20040412Z Package ID: l_fc_pc_8.


> Eventually you mention `X86 Linux' but are
> you using x86 Linux and if so how fast processors?  On a 3GHz machine 
> make
> check takes a couple of minutes using gcc (and in our experience 
> Portland
> Group is faster than gcc -- we have not tried Intel as local advice
> suggests it is slower than PG).

So 12 hours is bit excessive.  :-)  At least now I know.  Thanks!

> Which version of R?  R 1.9.0 does this with gcc 3.4.0 on Linux ix86, at
> the first use of LAPACK.

Ah.  This is indeed 1.9.0, since that is what was asked for.

>   So if you are not trying 1.9.1beta (to be
> released on Monday), please do so.  After that, make check does give
> output so where exactly does it hang, and if you interrupt that check, 
> how
> far has it got in the output file?

With 1.9.0, I never get an output file.  On interrupting the make 
check, it says:

make[4]: *** Deleting file `base-Ex.Rout'

and hangs again.  I have to background it and kill it.

The same thing happens with 1.9.1.  I think I'll use gcc/g77 as 
suggested by others to get a working install for now, and then I will 
look into a better optimised version at a more leisurely pace.

Regards,

Tim

-- 
Dr Tim Cutts
Informatics Systems Group
Wellcome Trust Sanger Institute
Hinxton, Cambridge, CB10 1SA, UK



From ripley at stats.ox.ac.uk  Fri Jun 18 12:58:39 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 18 Jun 2004 11:58:39 +0100 (BST)
Subject: [R] Compiling R with Intel compilers - recommended options?
In-Reply-To: <3A27A81E-C115-11D8-B8FD-000A95B2B140@sanger.ac.uk>
Message-ID: <Pine.LNX.4.44.0406181151340.22753-100000@gannet.stats>

On Fri, 18 Jun 2004, Tim Cutts wrote:

> Sorry, I was a bit sparing with details.  Early in the morning, and all 
> that.
> 
> Many thanks for the swift reply!
> 
> On 18 Jun 2004, at 9:11 am, Prof Brian Ripley wrote:
> 
> > What OS and hardware is this?
> 
> Hardware:  700+ machines are 800 MHz Pentium III, 1 GB RAM, Red Hat 7.2
>             168 machines are dual 2.8 GHz Xeon, 4 GB RAM, Red Hat 8.0
> 
> I'm compiling on the older machines to make sure there won't be library 
> problems.
> 
> The compiler versions are:
> 
> Intel(R) C++ Compiler for 32-bit applications, Version 8.0   Build 
> 20031016Z Package ID: l_cc_p_8.0.055
> 
> Intel(R) Fortran Compiler for 32-bit applications, Version 8.0   Build 
> 20040412Z Package ID: l_fc_pc_8.
> 
> 
> > Eventually you mention `X86 Linux' but are you using x86 Linux and if
> > so how fast processors?  On a 3GHz machine make check takes a couple
> > of minutes using gcc (and in our experience Portland Group is faster
> > than gcc -- we have not tried Intel as local advice suggests it is
> > slower than PG).
> 
> So 12 hours is bit excessive.  :-)  At least now I know.  Thanks!
> 
> > Which version of R?  R 1.9.0 does this with gcc 3.4.0 on Linux ix86, at
> > the first use of LAPACK.
> 
> Ah.  This is indeed 1.9.0, since that is what was asked for.
> 
> >   So if you are not trying 1.9.1beta (to be released on Monday),
> > please do so.  After that, make check does give output so where
> > exactly does it hang, and if you interrupt that check, how far has it
> > got in the output file?
> 
> With 1.9.0, I never get an output file.  On interrupting the make 
> check, it says:
> 
> make[4]: *** Deleting file `base-Ex.Rout'
> 
> and hangs again.  I have to background it and kill it.

Looks like the same issue as gcc 3.4.0.

> The same thing happens with 1.9.1.  I think I'll use gcc/g77 as 
> suggested by others to get a working install for now, and then I will 
> look into a better optimised version at a more leisurely pace.

Fine.  The issue on gcc 3.4.0 is that if you compile 
src/modules/lapack/dlamc.f without -ffloat-store it loops forever.
I suspect you need to compile it without optimization or with the 
equivalent of -ffloat-store.  It's a separate file in 1.9.1 to make this 
easier/less penalty and configure sets -ffloat-store if g77 is detected.
LAPACK calls Fortran functions to try to force stores, and that worked on 
versions of gcc <= 3.3.3 (and of course only machines with extended 
registers like ix86 need the subterfuge).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tjrc at sanger.ac.uk  Fri Jun 18 13:37:02 2004
From: tjrc at sanger.ac.uk (Tim Cutts)
Date: Fri, 18 Jun 2004 12:37:02 +0100
Subject: [R] Compiling R with Intel compilers - recommended options?
In-Reply-To: <Pine.LNX.4.44.0406181151340.22753-100000@gannet.stats>
References: <Pine.LNX.4.44.0406181151340.22753-100000@gannet.stats>
Message-ID: <D0EE7770-C11B-11D8-B8FD-000A95B2B140@sanger.ac.uk>


On 18 Jun 2004, at 11:58 am, Prof Brian Ripley wrote:

> Fine.  The issue on gcc 3.4.0 is that if you compile
> src/modules/lapack/dlamc.f without -ffloat-store it loops forever.
> I suspect you need to compile it without optimization or with the
> equivalent of -ffloat-store.  It's a separate file in 1.9.1 to make 
> this
> easier/less penalty and configure sets -ffloat-store if g77 is 
> detected.
> LAPACK calls Fortran functions to try to force stores, and that worked 
> on
> versions of gcc <= 3.3.3 (and of course only machines with extended
> registers like ix86 need the subterfuge).

 From my reading of the Intel compiler documentation, the equivalent 
option for icc and ifort is -mp, so I'll have a go at that.

Tim

-- 
Dr Tim Cutts
Informatics Systems Group
Wellcome Trust Sanger Institute
Hinxton, Cambridge, CB10 1SA, UK



From lhill at ipimar.pt  Fri Jun 18 13:38:02 2004
From: lhill at ipimar.pt (Louize Hill)
Date: Fri, 18 Jun 2004 12:38:02 +0100
Subject: [R] set working directory
Message-ID: <002001c45528$b6cb5c10$36040a0a@Louisept>

I have just upgraded from rw1081 to rw1090 (using Windows 2000).

Now when I type the command:

> setwd('d:/folder_name)

I get the following error message:

Error in setwd(dir) : cannot change working directory

If I use the "change dir" tab in the file menu i can sucessfully change
working directories, but this is not so convenient as i have my whole model
stored as a text file that I prefer to copy into R.



From Saghir.Bashir at UCB-Group.com  Fri Jun 18 13:54:28 2004
From: Saghir.Bashir at UCB-Group.com (Bashir Saghir (Aztek Global))
Date: Fri, 18 Jun 2004 13:54:28 +0200
Subject: [R] set working directory
Message-ID: <3EBA5559F490D61189430002A5F0AE8905632703@ntexcrd.braine.ucb>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040618/b400550d/attachment.pl

From ligges at statistik.uni-dortmund.de  Fri Jun 18 14:05:25 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Fri, 18 Jun 2004 14:05:25 +0200
Subject: [R] set working directory
In-Reply-To: <3EBA5559F490D61189430002A5F0AE8905632703@ntexcrd.braine.ucb>
References: <3EBA5559F490D61189430002A5F0AE8905632703@ntexcrd.braine.ucb>
Message-ID: <40D2DA85.7020405@statistik.uni-dortmund.de>

Bashir Saghir (Aztek Global) wrote:

> Try adding a missing singe quote (') at the end of your working directory.
> 
> setwd('d:/folder_name')  

Well, that was a mispelling in the mail, but not in the real example, 
since not specifying the quote causes a syntax error.

I'm quite sure the folder-name was misspelled in the real example.....

Uwe Ligges


> S.
> 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Louize Hill
> Sent: Friday, June 18, 2004 13:38
> To: r-help at stat.math.ethz.ch
> Subject: [R] set working directory
> 
> 
> I have just upgraded from rw1081 to rw1090 (using Windows 2000).
> 
> Now when I type the command:
> 
> 
>>setwd('d:/folder_name)
> 
> 
> I get the following error message:
> 
> Error in setwd(dir) : cannot change working directory
> 
> If I use the "change dir" tab in the file menu i can sucessfully change
> working directories, but this is not so convenient as i have my whole model
> stored as a text file that I prefer to copy into R.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> 
> --------------------------------------------------------- 
> Legal Notice: This electronic mail and its attachments are i...{{dropped}}
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From erich.neuwirth at univie.ac.at  Fri Jun 18 14:30:02 2004
From: erich.neuwirth at univie.ac.at (Erich Neuwirth)
Date: Fri, 18 Jun 2004 14:30:02 +0200
Subject: [R] R help in Firefox on Windows XP
In-Reply-To: <Pine.LNX.4.44.0406171909220.6100-100000@gannet.stats>
References: <Pine.LNX.4.44.0406171909220.6100-100000@gannet.stats>
Message-ID: <40D2E04A.3000004@univie.ac.at>

Thank you Brian,
Installing 0.8 first and then upgrading solved the problem.
I noticed that installing 0.9 from scratch creates a registry key
[HKEY_LOCAL_MACHINE\SOFTWARE\mozilla.org\Mozilla]
"CurrentVersion"="1.7"
Installing 0.8 creates
[HKEY_LOCAL_MACHINE\SOFTWARE\mozilla.org\Mozilla]
"CurrentVersion"="1.6"
and upgrading to 0.9 afterwards does not change this entry.

This might be the reason.



Prof Brian Ripley wrote:

> Works for me.  Did it work with Firefox 0.8?  (I upgraded from 0.8.)
> I would try 0.8 and then upgrade.
>
> On Thu, 17 Jun 2004, Erich Neuwirth wrote:
>
>  
>
>> I had to reinstall my machine, so I installed Firefox 0.9 as browser
>> I am using WinXP and R 1.9.1 beta.
>> Now search in R html help does not work.
>> I checked that the Java VM is working correctlt, Sun's test site says
>> my installation is OK.
>> Firefoxalso tells me that
>>
>> Applet Searchengine loaded
>> Applet Searchengine started
>>
>> it just does not find anything.
>> Does anybody know how to solve this?
>>   
>
>
>  
>


Prof Brian Ripley wrote:

>Works for me.  Did it work with Firefox 0.8?  (I upgraded from 0.8.)
>I would try 0.8 and then upgrade.
>
>On Thu, 17 Jun 2004, Erich Neuwirth wrote:
>
>  
>
>>I had to reinstall my machine, so I installed Firefox 0.9 as browser
>>I am using WinXP and R 1.9.1 beta.
>>Now search in R html help does not work.
>>I checked that the Java VM is working correctlt, Sun's test site says
>>my installation is OK.
>>Firefoxalso tells me that
>>
>>Applet Searchengine loaded
>>Applet Searchengine started
>>
>>it just does not find anything.
>>Does anybody know how to solve this?
>>    
>>
>
>  
>



From ggrothendieck at myway.com  Fri Jun 18 14:34:25 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 18 Jun 2004 08:34:25 -0400 (EDT)
Subject: [R] [Q] Newbie (continued.. at least I got R running allready :-)
Message-ID: <20040618123425.F06BA39C8@mprdmxin.myway.com>



Check out ?turnpoints in the pastecs library.  

Try 

   example(turnpoints)
   str(Nauplii.tp)

to see the data structure you get back.  It identifies the peaks
and pits and other associated information.

Date:   Fri, 18 Jun 2004 11:23:54 +0200 
From:   jeroen clarysse <jeroen.clarysse at easynet.be>
To:   <r-help at stat.math.ethz.ch> 
Subject:   [R] [Q] Newbie (continued.. at least I got R running allready :-) 

 
Hi all

a week ago, I posted a newbie question in data smoothing &
maximum-extraction with R. I got quite a lot of response, but I'm still
kinda stuck with it...

I'll restate the problem : i got a datafile with 2400 measuerements (every
250msec) of a CO2 measurement device, capturing the breath of a subject. I
uploaded such a sample here :

http://www.psy.kuleuven.ac.be/leerpsy/data.csv

now I wish to figure out where each breath expiration ceiling takes place ,
as shown on this graph :

http://www.psy.kuleuven.ac.be/leerpsy/graph.bmp


I'm kinda stuck on how to get this running in R.

I really hope someone can help me out. If you guys can get me running, I
promise to promote R as often as I can here on our faculty (which still uses
Statistica for almost everything)

thanks a million in advance !



From HerwigMeschke at t-online.de  Fri Jun 18 14:47:45 2004
From: HerwigMeschke at t-online.de (Dr. Herwig Meschke)
Date: Fri, 18 Jun 2004 14:47:45 +0200
Subject: [R] non-linear binning? power-law in R
In-Reply-To: <Pine.LNX.4.21.0406181051370.15511-100000@mail.mrc-dunn.cam.ac.uk>
References: <40D1CBB3.24542.5CD1AA5@localhost>
Message-ID: <40D30091.8510.A83BA17@localhost>



From kurt.sys at pandora.be  Fri Jun 18 14:50:32 2004
From: kurt.sys at pandora.be (groensels)
Date: Fri, 18 Jun 2004 12:50:32 +0000
Subject: [R] R as controller
Message-ID: <W626833020342921087563032@asteria.telenet-ops.be>

Hi all,

this may be a 'stupid' question, but it would be great if it exists. What's it about? For the control of experiments, special (and expensive) software is often necessary. The main 'difficulty' I have is to read data from different (measurement) devices and send them to some actuators (reading from a serial device and the protocols). Is there a package in R for reading and writing to a serial port, including the protocols for ADAM modules etc? Or is there anyone having plans to program such a package? If it would exist, R could be as a controller, which would make life much easier, at least my life :).

tnx,
Kurt



From jarioksa at sun3.oulu.fi  Fri Jun 18 15:04:25 2004
From: jarioksa at sun3.oulu.fi (Jari Oksanen)
Date: 18 Jun 2004 16:04:25 +0300
Subject: [R] can't get text to appear over individual panels in
	multi-panel plot
In-Reply-To: <BA63696F-C0F0-11D8-B5CA-000A95C76CA8@oulu.fi>
References: <A34058C2-C0BF-11D8-A80C-000393D55244@mcmaster.ca>
	<200406172247.25893.deepayan@stat.wisc.edu>
	<A769FEBA-C0DB-11D8-9C1A-000393D55244@mcmaster.ca>
	<200406180026.38335.deepayan@stat.wisc.edu>
	<BA63696F-C0F0-11D8-B5CA-000A95C76CA8@oulu.fi>
Message-ID: <1087563865.19072.28.camel@biol102145.oulu.fi>

On Fri, 2004-06-18 at 09:28, Jari Oksanen wrote:
> On 18 Jun 2004, at 8:26, Deepayan Sarkar wrote:
> 
> >
> > On Thursday 17 June 2004 22:57, Patrick Bennett wrote:
> >
> >> yes, i can reproduce that same graph when i print to the pdf-device.
> >> but the panel titles do not appear when I print to the Quartz-device.
> >
> > Hmm. I won't be able to help you then, let's hope someone else can.
> 
I had a closer look at this, and it indeed looks like quartz() is anally
checking that there is enough space for text or it refuses to print it
at all. Like I wrote, the command worked with x11() device in MacOS X,
but failed with default quartz(). I checked again (in another machine),
and it seems that you may get text if you expand the par.strip: try
adding

 par.strip.text=list(lines=2) 

in your Lattice plotting command (lines=1.8 was the smallest that worked
in my case). 

This is a fault (``undesirable feature'') in quartz. This doesn't
concern Lattice only, but all graphics commands: quartz() refuses to
show axis labels or titles in "too narrow" margins, or to write text too
close to axes (if xpd is not set) in quite ordinary plot(). 

cheers, jari oksanen
-- 
Jari Oksanen -- Dept Biology, Univ Oulu, 90014 Oulu, Finland
email jari.oksanen at oulu.fi, homepage http://cc.oulu.fi/~jarioksa/



From ripley at stats.ox.ac.uk  Fri Jun 18 15:15:02 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 18 Jun 2004 14:15:02 +0100 (BST)
Subject: [R] R as controller
In-Reply-To: <W626833020342921087563032@asteria.telenet-ops.be>
Message-ID: <Pine.LNX.4.44.0406181409050.3506-100000@gannet.stats>

What OS is this?  How do you access a serial port?  On most modern OSes
user processes such as R have no direct access to serial ports.  If access
is via a Unix device, binary connections will probably work.

In my limited experience that expensive software runs under a low-level
OS, MS-DOS or a real-time OS.

On Fri, 18 Jun 2004, groensels wrote:

> this may be a 'stupid' question, but it would be great if it exists.
> What's it about? For the control of experiments, special (and expensive)
> software is often necessary. The main 'difficulty' I have is to read
> data from different (measurement) devices and send them to some
> actuators (reading from a serial device and the protocols). Is there a
> package in R for reading and writing to a serial port, including the
> protocols for ADAM modules etc? Or is there anyone having plans to
> program such a package? If it would exist, R could be as a controller,
> which would make life much easier, at least my life :).


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From chris.jackson at imperial.ac.uk  Fri Jun 18 15:33:06 2004
From: chris.jackson at imperial.ac.uk (Chris Jackson)
Date: Fri, 18 Jun 2004 14:33:06 +0100
Subject: [R] msm
In-Reply-To: <001801c1c9ca$a08a9ce0$b17dfea9@vs017185>
References: <001801c1c9ca$a08a9ce0$b17dfea9@vs017185>
Message-ID: <40D2EF12.1040409@imperial.ac.uk>

russell alexander wrote:

>Hello,
>
>I'm writing about msm. It may be that consistent users of Markov  models have a good idea as to what constitutes workable data for a model. I think of  general rules,  in basic statistical studies where n is limited to exclude fairly precise figures in the lower range. 
>On the other hand Markov models don't seem to be often enough used for parameters to be as well laid out. 
>I also get the feeling that msm is organized to work optimally with certain sizes and shapes of data. Is there a source that anyone is aware of on this? (I have the Nelder text on optimization, and also have a feeling that what's possible is pretty closely connected with optimization questions)
>

It's very difficult to give general guidelines for how much data is 
sufficient to estimate a continuous-time Markov model.  There are two 
distinct forms of data which these models are used for.  The simplest 
case is when you have observed the entire trajectory of the process.   
In this case, complex transition matrices can sometimes be estimated 
with relatively small datasets.    However, if you only have 
observations at arbitrary times, then certain models will result in very 
flat likelihoods.  In particular, for models with reversible transitions 
(recurrent states) there can be an infinite number of possible paths 
followed in between two arbitrary times.  Then you will need 
substantially more data.  Models with non-recurrent states are generally 
easier to estimate. 

I'd just suggest that you try out the models you are interested in on 
your data.  Choosing a suitable optimization technique can often help, 
but sometimes models are simply over-parameterised.   I don't mind 
discussing Markov models on r-help, but if you have a question about msm 
it's probably safer to mail me directly as the author, as it is an 
obscure contributed package which, as far as I am aware, very few people 
use!

Chris

-- 
Christopher Jackson <chris.jackson at imperial.ac.uk>, Research Associate,
Department of Epidemiology and Public Health, Imperial College
School of Medicine, Norfolk Place, London W2 1PG, tel. 020 759 43371



From moconnell at insightful.com  Fri Jun 18 15:50:57 2004
From: moconnell at insightful.com (Michael O'Connell)
Date: Fri, 18 Jun 2004 09:50:57 -0400
Subject: [R] S+ArrayAnalyzer and BioConductor Announcement
Message-ID: <61D7107976B46045BFCAA8BD301E82950DB1B8@nc2kexch01.insightful.com>

Greetings to all and apologies for any cross-postings. If you know of anyone
else who could benefit from this announcement please forward to them. Best
regards and good luck to everyone, Michael O'Connell. 
BioConductor Project and Insightful Corp. Collaboration Announcement
This week, Insightful Corporation announced the availability of
S+ArrayAnalyzer version 2.0, an updated module for S-PLUS based on
collaboration with the BioConductor Project - a widely-used open source and
open development software project for the analysis and comprehension of
genomic data. 
The software will be described in a free 1 hour public web seminar on
Tuesday, June 22nd, 2004 at 11:30 AM EST / 8:30 AM PST entitled:
Using S+ArrayAnalyzer 2.0 to Improve Gene Expression Analysis and Deploy Best
Practices
You can register for this event at
http://www.insightful.com/news_events/webcasts/pharm04/arrayanalyzer.asp
The collaboration between Insightful and BioConductor delivers benefits to
commercial and academic researchers analyzing high-throughput assay data and
microarray experiments:
- S+ArrayAnalyzer S+AA embraces and extends the Bioconductor 1.4 object model
so that both offerings share a common framework. This allows researchers in
the field to create new software methods and algorithms for two widely-used
software platforms using a single underlying object model and to use the same
syntax for future collaborative research. 
- S+ArrayAnalyzer adds many features that can improve productivity for users,
such as installers, more data access options, a guided-workflow interface,
interactive graphics with hyperlinked annotation and Web deployment of
applications. 
- S+ArrayAnalyzer is a commercially supported product from Insightful, a
long-established vendor with fully staffed tech support, training, and
consulting.
This collaboration between Insightful and the BioConductor Group has brought
the expertise of the open-source and research communities and the rigorous
development environment of the commercial software world together. Through
programs such as sponsorship of a graduate student in the BioConductor
project, and feedback from its professional Quality Assurance testing team,
Insightful's participation in this collaboration has served to improve the
availability and reliability of microarray analysis software for both the
academic research and commercial user. In addition, through its relationships
with customers in commercial drug discovery and awareness of their unique
needs, Insightful has helped drive continued advancements and innovation in
both the open source and commercial offerings: for example, a new
differential expression library (lpe), written by Insightful and the
University of Virginia, has been ported to R and included in both
S+ArrayAnalyzer and Bioconductor v1.4.
"The BioConductor-Insightful collaboration helps ensure the distribution and
high-level end-user support of key bioinformatics tools to the broadest
possible population of researchers," said spokespeople for The BioConductor
Project. "BioConductor is an academic project focused on creation and
distribution of high-quality open source statistical and computational
methods and software for bioinformatics, while Insightful has over a decade
of experience in user interface design, documentation, and user support for
statistical computing. The results of this collaboration should benefit
researchers in both commercial and non-profit organizations."
"Our collaboration with BioConductor is a blueprint for how Insightful can
work with the open source community to bring highly innovative and productive
data analysis applications to the widest possible user base," said Jeff
Coombs, CEO of Insightful. "This cooperation improves the analytic solutions
available to our common base of S programmers and the non-statisticians who
benefit from using applications developed with S-PLUS and R."
S+ArrayAnalyzer is available now with pricing for commercial and academic
organizations available by calling (800)569-0123 x479, or via email at
S-PLUS.pharma at insightful.com
ABOUT BIOCONDUCTOR
BioConductor is an academic project focused on creation and distribution of
high-quality open source statistical and computational methods and software
for bioinformatics. It includes contributions to the management and analysis
of cDNA and oligonucleotide microarray and SAGE platforms, to the structure
and flexible manipulation of genomic annotation data, and to the
computational infrastructure required for effective bioinformatics, including
GUI component libraries, novel structures and algorithms for networks and
graphical systems, and improvements to the R language, especially in the
domain of programmatic verification, documentation and distribution of
software packages. BioConductor is widely used by investigators on many
different platforms and is a basis for training in statistical genomics and
bioinformatics in a number of prominent universities on four continents.
Spokespeople for the BioConductor Project: S. Dudoit, Division of
Biostatistics, University of California, Berkeley; R.A. Irizarry, Department
of Biostatistics, Johns Hopkins University; V.J. Carey, Harvard Medical
School; R. Gentleman, Harvard School of Public Health.
ABOUT INSIGHTFUL 
Insightful Corporation (NASDAQ:IFUL) provides enterprises with scalable data
analysis solutions that drive better decisions faster by revealing patterns,
trends and relationships. The company is a leading supplier of software and
services for statistical data analysis, data mining and knowledge access
enabling clients to gain intelligence from numeric and text data.
Insightful products include S-PLUS??, Insightful Miner, S-PLUS Server?? and
InFact??. Insightful consulting services provide specialized expertise and
proven processes for the design, development and deployment of customized
solutions. The company has been delivering industry-leading, high-ROI
solutions for 17 years to thousands of companies in financial services,
pharmaceuticals, biotechnology, telecommunications, manufacturing, plus
government and research institutions. 
Headquartered in Seattle, Insightful has offices in New York City, North
Carolina, France, Switzerland, and the United Kingdom, with distributors
around the world. For more information, visit www.insightful.com, email
info at insightful.com or call 1-800-569-0123.



From dieter.haering at fal.admin.ch  Fri Jun 18 16:13:29 2004
From: dieter.haering at fal.admin.ch (dieter.haering@fal.admin.ch)
Date: Fri, 18 Jun 2004 16:13:29 +0200
Subject: [R] Barplots and error indicators: Some R-Code
Message-ID: <8C0D34B2D97FAD488303175752120A4B011D0779@evd-s7015.evd.admin.ch>

I' ve seen that several people are looking for a function that creates a
barplot with an error indicators (I was one of them myself). Maybe you will
find the following code helpful (There are some examples how to use it at
the end):


# Creates a barplot. 
#bar.plot() needs a datavector for the height of bars and a error
#indicator for the interval 
#many of the usual R parameters can be set: e.g. ylim, main, col, etc.
#The direction of the error indicator can be specified as "bo" for both,
"lo" lower, "up"  #upper
#The width of the indicator hat is set as a percentage of the x-width of the
plot:e.g.  *hat=0.05

bar.plot<-function(data, err.ind, ind.side=NA, hat=NA, ylim=c(0,
max(data+err.ind,  na.rm=TRUE)*1.2), col=NA, xlab="your x-lab",  ylab="your
y-lab", main="your main",  names.arg=NA, space=0.2){
if (is.na(ind.side)){
ind.side<-"up"}	#only upper error indicator (default), else: "lo" or "bo"
if (is.na(hat)){
hat<-0.01}	#fraction of the x axis that defines the size of the error
indicator hat
if (is.na(col)){col<-"white"} #the default color is white
x.cor<-barplot(data, ylim=ylim, xlab=xlab, ylab=ylab, main=main,
names.arg=names.arg,   col=col, space=space)
smidge<-diff(par("usr")[1:2])*hat
up.ind<-data+err.ind
lo.ind<-data-err.ind
segments(0,0,x.cor,0)	# draws X-Axis
if (ind.side=="up"){
segments(x.cor, data, x.cor, up.ind)
segments(x.cor, up.ind, x.cor+smidge, up.ind)
segments(x.cor, up.ind, x.cor-smidge, up.ind)
}else{
if (ind.side=="bo"){
segments(x.cor, data, x.cor, up.ind)
segments(x.cor, up.ind, x.cor+smidge, up.ind)
segments(x.cor, up.ind, x.cor-smidge, up.ind)
segments(x.cor, data, x.cor, lo.ind)
segments(x.cor, lo.ind, x.cor+smidge, lo.ind)
segments(x.cor, lo.ind, x.cor-smidge, lo.ind)
}else{
segments(x.cor, data, x.cor, lo.ind)
segments(x.cor, lo.ind, x.cor+smidge, lo.ind)
segments(x.cor, lo.ind, x.cor-smidge, lo.ind)
}
}
}


#Examples
par(mfrow=c(2,2))

data<-c(3.2, 3.3, 3.6, 4, 3.1, 3.3, 3.1, 4.5, 3.2, 3.3, 3.1, 3.4)
se<-sqrt(data)
bar.plot(data, se) #easy example

data<-c(3.2, 3.3, 3.6, 4, 3.1, 3.3, 3.1, 4.5, 3.2, 3.3, 3.1, 3.4)
se<-c(sqrt(data))
bar.plot(data, se,ind.side="lo", ylim=c(0, 10), col="lavender", space=0.25,
hat=0) 

data<-c(15, 15, 19, 22)
se<-c(3, 5, 6, 4.5)
nam<-c("L.c","O.v","C.i","L.u")
bar.plot(data, se, col="orange", ind.side="bo", hat=0.05, main="",
xlab="species", ylab="CT  conc.")

data<-c(4,5,1,1.3,6,7.1,5,2.6)
se<-c(sqrt(data))
nam<-rep(c("C","T"),4)
spa<-rep(c(1.5, 0.2),4)
col<-rep(c("green","red"),4)
bar.plot(data, se, col=col, ind.side="up", main="Trees !", ylab="CT conc.",
space=spa,  names.arg=nam)





***********************************************************************
Dieter H??ring
Eidg. Forschungsanstalt f??r Agraroekologie und Landbau (FAL)
Reckenholzstrasse 191
8046 Z??rich

Tel.       01 / 377 71 62
FAX      01 / 377 72 01
mailto:dieter.haering at fal.admin.ch
www.reckenholz.ch



From tlumley at u.washington.edu  Fri Jun 18 16:15:32 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 18 Jun 2004 07:15:32 -0700 (PDT)
Subject: [R] can't get text to appear over individual panels in multi-panel
	plot
In-Reply-To: <200406180026.38335.deepayan@stat.wisc.edu>
References: <A34058C2-C0BF-11D8-A80C-000393D55244@mcmaster.ca>
	<200406172247.25893.deepayan@stat.wisc.edu>
	<A769FEBA-C0DB-11D8-9C1A-000393D55244@mcmaster.ca>
	<200406180026.38335.deepayan@stat.wisc.edu>
Message-ID: <Pine.A41.4.58.0406180714470.164712@homer09.u.washington.edu>

On Fri, 18 Jun 2004, Deepayan Sarkar wrote:

>
> On Thursday 17 June 2004 22:57, Patrick Bennett wrote:
>
> > yes, i can reproduce that same graph when i print to the pdf-device.
> > but the panel titles do not appear when I print to the Quartz-device.
>
> Hmm. I won't be able to help you then, let's hope someone else can.

I get the the panel titles on a quartz() device.

	-thomas

>
> Deepayan
>
> > On Jun 17, 2004, at 11:47 PM, Deepayan Sarkar wrote:
> > > On Thursday 17 June 2004 22:24, Patrick Bennett wrote:
> > >> I neglected to say that I am using the R-Aqua interface and the
> > >> MASS, grid, & lattice packages.
> > >>
> > >> Here is one specific example where I'm having trouble.
> > >>
> > >> After loading the crabs data set, I create the figure with the
> > >> following code (which is taken from MASS):
> > >>
> > >> lcrabs.pc<-predict(princomp(log(crabs[,4:8])))
> > >> sex<-crabs$sex;levels(sex)<-c("Female","Male")
> > >> sp<-crabs$sp;levels(sp)<-c("Blue","Orange")
> > >> splom(~lcrabs.pc[,1:3] | sp*sex,cex=0.5,pscales=0)
> > >>
> > >> The figure is plotted in the Quartz window: everything looks OK
> > >> except for the lack of text above the individual panels.
> > >
> > > Here's what I get (on a pdf device), and it looks OK to me.
> > >
> > > http://www.stat.wisc.edu/~deepayan/R/crabs.pdf
> > >
> > > If this is not what you see, then there might be a problem with
> > > your device driver.
> > >
> > > Deepayan
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>

Thomas Lumley			Assoc. Professor, Biostatistics
tlumley at u.washington.edu	University of Washington, Seattle



From tlumley at u.washington.edu  Fri Jun 18 16:18:07 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Fri, 18 Jun 2004 07:18:07 -0700 (PDT)
Subject: [R] can't get text to appear over individual panels in multi-panel
	plot
In-Reply-To: <BA63696F-C0F0-11D8-B5CA-000A95C76CA8@oulu.fi>
References: <A34058C2-C0BF-11D8-A80C-000393D55244@mcmaster.ca>
	<200406172247.25893.deepayan@stat.wisc.edu>
	<A769FEBA-C0DB-11D8-9C1A-000393D55244@mcmaster.ca>
	<200406180026.38335.deepayan@stat.wisc.edu>
	<BA63696F-C0F0-11D8-B5CA-000A95C76CA8@oulu.fi>
Message-ID: <Pine.A41.4.58.0406180715560.164712@homer09.u.washington.edu>

On Fri, 18 Jun 2004, Jari Oksanen wrote:

>
> On 18 Jun 2004, at 8:26, Deepayan Sarkar wrote:
>
> >
> > On Thursday 17 June 2004 22:57, Patrick Bennett wrote:
> >
> >> yes, i can reproduce that same graph when i print to the pdf-device.
> >> but the panel titles do not appear when I print to the Quartz-device.
> >
> > Hmm. I won't be able to help you then, let's hope someone else can.
>
> I think this is a problem with the quartz device. I have often see that
> margin texts are plotted even in ordinary plot() if quartz thinks there
> is no space for them. They do still appear if you copy the screen
> graphics as a pdf file. In Linux (my principal platform) I typically
> reduce the white margins, but if I use the same mar pars in MacOS X I
> won't get axis labels. Quartz is the culprit I suppose.
>

It would be useful to send bug reports for this sort of thing...


> Actually, in your example I couldn't get the texts when I saved the
> plot as a pdf (menu entry). However, when I opened an X11 device, the
> text was reproduced OK.  So it looks like a quartz problem.

I get the right output on both quartz() and x11().


> Windows plotting device in some other OS. -- And beware: I have a
> suspicion that if you stop your X11 in MacOS X, your mouse will die at
> logout and you got to boot (or restart the mouse demon if you know  who
> he is).

I have never seen this problem.

	-thomas



From wolski at molgen.mpg.de  Fri Jun 18 16:20:24 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Fri, 18 Jun 2004 16:20:24 +0200
Subject: [R] Initializing SparseM matrix matrix.csc
Message-ID: <200406181620240696.385915E6@mail.math.fu-berlin.de>

Hi!

Would like to initialize a huge matrix.csc (Pacakge SparseM) with all elements 0
and afterwards set a few alements nonzero.
The matrix which I like to allocate is so huge that I can not use 
A <- matrix(a,n1,p)
before:
A.csr <- as.matrix.csc(A)
because I can not allocate such a huge matrix A.
But I believe that the much more memmory efficient model in case of csc matrix should do it for a sparse matrix.


How to do this.
Sincerely Eryk



From rkoenker at uiuc.edu  Fri Jun 18 16:29:56 2004
From: rkoenker at uiuc.edu (roger koenker)
Date: Fri, 18 Jun 2004 09:29:56 -0500
Subject: [R] Initializing SparseM matrix matrix.csc
In-Reply-To: <200406181620240696.385915E6@mail.math.fu-berlin.de>
References: <200406181620240696.385915E6@mail.math.fu-berlin.de>
Message-ID: <F8CDBCD5-C133-11D8-8762-000A95A7E3AA@uiuc.edu>

 > A <- as.matrix.csr(0,5,4)
 > A
An object of class "matrix.csr"
Slot "ra":
[1] 0

Slot "ja":
[1] 1

Slot "ia":
[1] 1 2 2 2 2 2

Slot "dimension":
[1] 5 4

 > A[3,4] <- 3
 > A
An object of class "matrix.csr"
Slot "ra":
[1] 0 3

Slot "ja":
[1] 1 4

Slot "ia":
[1] 1 2 2 3 3 3

Slot "dimension":
[1] 5 4
 > as.matrix(A)
      [,1] [,2] [,3] [,4]
[1,]    0    0    0    0
[2,]    0    0    0    0
[3,]    0    0    0    3
[4,]    0    0    0    0
[5,]    0    0    0    0


url:	www.econ.uiuc.edu/~roger        	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820

On Jun 18, 2004, at 9:20 AM, Wolski wrote:

> Hi!
>
> Would like to initialize a huge matrix.csc (Pacakge SparseM) with all 
> elements 0
> and afterwards set a few alements nonzero.
> The matrix which I like to allocate is so huge that I can not use
> A <- matrix(a,n1,p)
> before:
> A.csr <- as.matrix.csc(A)
> because I can not allocate such a huge matrix A.
> But I believe that the much more memmory efficient model in case of 
> csc matrix should do it for a sparse matrix.
>
>
> How to do this.
> Sincerely Eryk
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From f.harrell at vanderbilt.edu  Fri Jun 18 16:48:03 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 18 Jun 2004 09:48:03 -0500
Subject: [R] Barplots and error indicators: Some R-Code
In-Reply-To: <8C0D34B2D97FAD488303175752120A4B011D0779@evd-s7015.evd.admin.ch>
References: <8C0D34B2D97FAD488303175752120A4B011D0779@evd-s7015.evd.admin.ch>
Message-ID: <40D300A3.30801@vanderbilt.edu>

dieter.haering at fal.admin.ch wrote:
> I' ve seen that several people are looking for a function that creates a
> barplot with an error indicators (I was one of them myself). Maybe you will
> find the following code helpful (There are some examples how to use it at
> the end):
> 
> 
> # Creates a barplot. 
> #bar.plot() needs a datavector for the height of bars and a error
> #indicator for the interval 
> #many of the usual R parameters can be set: e.g. ylim, main, col, etc.
> #The direction of the error indicator can be specified as "bo" for both,
> "lo" lower, "up"  #upper
> #The width of the indicator hat is set as a percentage of the x-width of the
> plot:e.g.  *hat=0.05
> 
> bar.plot<-function(data, err.ind, ind.side=NA, hat=NA, ylim=c(0,
> max(data+err.ind,  na.rm=TRUE)*1.2), col=NA, xlab="your x-lab",  ylab="your
> y-lab", main="your main",  names.arg=NA, space=0.2){
> if (is.na(ind.side)){
> ind.side<-"up"}	#only upper error indicator (default), else: "lo" or "bo"
> if (is.na(hat)){
> hat<-0.01}	#fraction of the x axis that defines the size of the error
> indicator hat
> if (is.na(col)){col<-"white"} #the default color is white
> x.cor<-barplot(data, ylim=ylim, xlab=xlab, ylab=ylab, main=main,
> names.arg=names.arg,   col=col, space=space)
> smidge<-diff(par("usr")[1:2])*hat
> up.ind<-data+err.ind
> lo.ind<-data-err.ind
> segments(0,0,x.cor,0)	# draws X-Axis
> if (ind.side=="up"){
> segments(x.cor, data, x.cor, up.ind)
> segments(x.cor, up.ind, x.cor+smidge, up.ind)
> segments(x.cor, up.ind, x.cor-smidge, up.ind)
> }else{
> if (ind.side=="bo"){
> segments(x.cor, data, x.cor, up.ind)
> segments(x.cor, up.ind, x.cor+smidge, up.ind)
> segments(x.cor, up.ind, x.cor-smidge, up.ind)
> segments(x.cor, data, x.cor, lo.ind)
> segments(x.cor, lo.ind, x.cor+smidge, lo.ind)
> segments(x.cor, lo.ind, x.cor-smidge, lo.ind)
> }else{
> segments(x.cor, data, x.cor, lo.ind)
> segments(x.cor, lo.ind, x.cor+smidge, lo.ind)
> segments(x.cor, lo.ind, x.cor-smidge, lo.ind)
> }
> }
> }
> 
> 
> #Examples
> par(mfrow=c(2,2))
> 
> data<-c(3.2, 3.3, 3.6, 4, 3.1, 3.3, 3.1, 4.5, 3.2, 3.3, 3.1, 3.4)
> se<-sqrt(data)
> bar.plot(data, se) #easy example
> 
> data<-c(3.2, 3.3, 3.6, 4, 3.1, 3.3, 3.1, 4.5, 3.2, 3.3, 3.1, 3.4)
> se<-c(sqrt(data))
> bar.plot(data, se,ind.side="lo", ylim=c(0, 10), col="lavender", space=0.25,
> hat=0) 
> 
> data<-c(15, 15, 19, 22)
> se<-c(3, 5, 6, 4.5)
> nam<-c("L.c","O.v","C.i","L.u")
> bar.plot(data, se, col="orange", ind.side="bo", hat=0.05, main="",
> xlab="species", ylab="CT  conc.")
> 
> data<-c(4,5,1,1.3,6,7.1,5,2.6)
> se<-c(sqrt(data))
> nam<-rep(c("C","T"),4)
> spa<-rep(c(1.5, 0.2),4)
> col<-rep(c("green","red"),4)
> bar.plot(data, se, col=col, ind.side="up", main="Trees !", ylab="CT conc.",
> space=spa,  names.arg=nam)
> 
> 
> 
> 
> 
> ***********************************************************************
> Dieter H??ring
> Eidg. Forschungsanstalt f??r Agraroekologie und Landbau (FAL)
> Reckenholzstrasse 191
> 8046 Z??rich
> 
> Tel.       01 / 377 71 62
> FAX      01 / 377 72 01
> mailto:dieter.haering at fal.admin.ch
> www.reckenholz.ch
> 

Bar charts have many problems as pointed out in Bill Cleveland's book 
Elements of Graphing Data.  Bar charts with error bars have even more 
problems.  I prefer dot plots with error bars.  The Dotplot function in 
the Hmisc package will make such graphs.  Hmisc's xYplot will do 
likewise for line graphs, including an option for error bands and shaded 
error bands.

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From Michael.Saupe at hte-company.de  Fri Jun 18 19:19:23 2004
From: Michael.Saupe at hte-company.de (Michael Saupe)
Date: Fri, 18 Jun 2004 18:19:23 +0100
Subject: [R] Problems running RSPerl w. SuSe 9.0 / Perl 5.8.1
Message-ID: <E843F0337279D611A0B500306E00C18548288A@hte-exchange.intra.hte-company.de>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040618/17052ab3/attachment.pl

From ripley at stats.ox.ac.uk  Fri Jun 18 18:27:24 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 18 Jun 2004 17:27:24 +0100 (BST)
Subject: [R] Resolution of plots
In-Reply-To: <Pine.A41.4.58.0406170744550.270076@homer03.u.washington.edu>
Message-ID: <Pine.LNX.4.44.0406181716330.12599-100000@gannet.stats>

On Thu, 17 Jun 2004, Thomas Lumley wrote:

> On Wed, 16 Jun 2004, Prof Brian Ripley wrote:
> 
> > You will have to tell us more.  Exporting how: to what format using what
> > device and what exact command on what operating system?
> >
> > The only device I know of that even knows about dpi is bitmap() and that
> > has no such limit unless imposed by your implementation of ghostscript.
> 
> There is an issue with PNG. libpng provides png_set_pHYs to set resolution
> (in pixels/metre) but provides a default if it is not set.  We don't set
> it, and so get the default resolution.

That seems not to be quite correct.  First, the pHYs chunk is optional in
PNG files.  If present, what is recorded in the file is the units,

PNG_RESOLUTION_UNKNOWN (dpi) or PNG_RESOLUTION_METER (pixels/metre)

plus two numbers (x and y res).  Reading applications are allowed to treat
missing pHYs data or numbers 0 as they like, and for example PhotoShop and
GIMP display both as 72 dpi.

So I think what we are doing is entirely within the spec.  I am in the 
process of adding an arg to allow this to be set (with square pixels, all 
R supports).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Rau at demogr.mpg.de  Fri Jun 18 18:32:18 2004
From: Rau at demogr.mpg.de (Rau, Roland)
Date: Fri, 18 Jun 2004 18:32:18 +0200
Subject: [R] Barplots and error indicators: Some R-Code
Message-ID: <3699CDBC4ED5D511BE6400306E1C0D81030A0A6F@hermes.demogr.mpg.de>

Dear all,

> -----Original Message-----
> From:	Frank E Harrell Jr [SMTP:f.harrell at vanderbilt.edu]
> Sent:	Friday, June 18, 2004 4:48 PM
> To:	dieter.haering at fal.admin.ch
> Cc:	r-help at stat.math.ethz.ch
> Subject:	Re: [R] Barplots and error indicators: Some R-Code
> 
> Bar charts have many problems as pointed out in Bill Cleveland's book 
> Elements of Graphing Data.  Bar charts with error bars have even more 
> problems.  I prefer dot plots with error bars.  The Dotplot function in 
> 
	it might be a bit off-topic but can anyone suggest some online
material concerning good graph / bad graph examples?
	I imagine something like:
	a) These are the data and this is the main feature of the data which
should be represented.
	b) This is a bad idea how to represent a)
	c) This is a good idea how to represent a) 

	I know there are excellent books by Cleveland and also by Tufte
("The Elements of Graphing Data", "The Visual Display of Quantitative
Information", "The cognitive style of PowerPoint", ...) but it would be
sometimes just more convenient to refer to some online material.

	Maybe even some online material about the problems of barplots with
error bars?

	Thank you very much,
	Roland



+++++
This mail has been sent through the MPI for Demographic Rese...{{dropped}}



From ripley at stats.ox.ac.uk  Fri Jun 18 18:35:39 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 18 Jun 2004 17:35:39 +0100 (BST)
Subject: [R] Problems running RSPerl w. SuSe 9.0 / Perl 5.8.1
In-Reply-To: <E843F0337279D611A0B500306E00C18548288A@hte-exchange.intra.hte-company.de>
Message-ID: <Pine.LNX.4.44.0406181732010.12599-100000@gannet.stats>

Exactly the same question was asked here by

From: Emmanuel Engelhart <Emmanuel.Engelhart at hte-company.de>
Date: Fri, 11 Jun 2004 13:31:17 +0100

so please talk to your colleagues (and note what

> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

says about doing your homework).

Note also that RSPerl is part of the Omegahat project which has its own
mailing lists (which may currently be down) and R-help is not the
appropriate place for question about third-party addons to R.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From spencer.graves at pdf.com  Fri Jun 18 18:43:48 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 18 Jun 2004 11:43:48 -0500
Subject: [R] set working directory
In-Reply-To: <40D2DA85.7020405@statistik.uni-dortmund.de>
References: <3EBA5559F490D61189430002A5F0AE8905632703@ntexcrd.braine.ucb>
	<40D2DA85.7020405@statistik.uni-dortmund.de>
Message-ID: <40D31BC4.2060600@pdf.com>

         I agree with Uwe:  I just tried "setwd" missing a quote in R 
1.9.1 alpha and got "syntax error".  When I had quotes paired but the 
name was not a valid directory, I got the error Louize reported:  "Error 
in setwd(dir) : cannot change working directory".  When I gave a valid 
directory, the command worked. 

      To make sure I don't make spelling errors in a directory under 
Windows 2000, I copy the address from the Windows Explorer into 
something like MS Word first, replace "\" with "/" and then copy the 
result into R. 

      hope this helps.  spencer graves

Uwe Ligges wrote:

> Bashir Saghir (Aztek Global) wrote:
>
>> Try adding a missing singe quote (') at the end of your working 
>> directory.
>>
>> setwd('d:/folder_name')  
>
>
> Well, that was a mispelling in the mail, but not in the real example, 
> since not specifying the quote causes a syntax error.
>
> I'm quite sure the folder-name was misspelled in the real example.....
>
> Uwe Ligges
>
>
>> S.
>>
>>
>> -----Original Message-----
>> From: r-help-bounces at stat.math.ethz.ch
>> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Louize Hill
>> Sent: Friday, June 18, 2004 13:38
>> To: r-help at stat.math.ethz.ch
>> Subject: [R] set working directory
>>
>>
>> I have just upgraded from rw1081 to rw1090 (using Windows 2000).
>>
>> Now when I type the command:
>>
>>
>>> setwd('d:/folder_name)
>>
>>
>>
>> I get the following error message:
>>
>> Error in setwd(dir) : cannot change working directory
>>
>> If I use the "change dir" tab in the file menu i can sucessfully change
>> working directories, but this is not so convenient as i have my whole 
>> model
>> stored as a text file that I prefer to copy into R.
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>>
>> --------------------------------------------------------- Legal 
>> Notice: This electronic mail and its attachments are i...{{dropped}}
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From f.harrell at vanderbilt.edu  Fri Jun 18 20:37:15 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Fri, 18 Jun 2004 13:37:15 -0500
Subject: [R] Barplots and error indicators: Some R-Code
In-Reply-To: <3699CDBC4ED5D511BE6400306E1C0D81030A0A6F@hermes.demogr.mpg.de>
References: <3699CDBC4ED5D511BE6400306E1C0D81030A0A6F@hermes.demogr.mpg.de>
Message-ID: <40D3365B.4030306@vanderbilt.edu>

Rau, Roland wrote:
> Dear all,
> 
> 
>>-----Original Message-----
>>From:	Frank E Harrell Jr [SMTP:f.harrell at vanderbilt.edu]
>>Sent:	Friday, June 18, 2004 4:48 PM
>>To:	dieter.haering at fal.admin.ch
>>Cc:	r-help at stat.math.ethz.ch
>>Subject:	Re: [R] Barplots and error indicators: Some R-Code
>>
>>Bar charts have many problems as pointed out in Bill Cleveland's book 
>>Elements of Graphing Data.  Bar charts with error bars have even more 
>>problems.  I prefer dot plots with error bars.  The Dotplot function in 
>>
> 
> 	it might be a bit off-topic but can anyone suggest some online
> material concerning good graph / bad graph examples?
> 	I imagine something like:
> 	a) These are the data and this is the main feature of the data which
> should be represented.
> 	b) This is a bad idea how to represent a)
> 	c) This is a good idea how to represent a) 
> 
> 	I know there are excellent books by Cleveland and also by Tufte
> ("The Elements of Graphing Data", "The Visual Display of Quantitative
> Information", "The cognitive style of PowerPoint", ...) but it would be
> sometimes just more convenient to refer to some online material.
> 
> 	Maybe even some online material about the problems of barplots with
> error bars?
> 
> 	Thank you very much,
> 	Roland
> 
This doesn't cover everything you want but look at 
http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/StatGraphCourse

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From jazevedo at provide.com.br  Fri Jun 18 20:45:17 2004
From: jazevedo at provide.com.br (Joao Pedro W. de Azevedo)
Date: Fri, 18 Jun 2004 19:45:17 +0100
Subject: [R] how to store estimates results as scalars of a matrix?
Message-ID: <000001c45564$66a88ff0$21a2f080@Lepc204>

Dear R users,

I've written a loop to generate Moran's test (spdep package) on serval
subsamples of a large dataset. See below a short example.

My loop is working fine, however I would like to be able to store the test
results as lines of a matrix, that I would latter be able to export as a
dataset. My problem is that I'm not sure how I could do this using R.

Any help will be much appreciated.

All the very best,

JP



coords2 <- as.matrix(jcdist.data[1:87, 6:7])

col.tri.nb<-tri2nb(coords2)

for(n in c(1,88,175,262,349)) {
    f<- n+86
    work <- jcdist.data[n:f, 10:12]

    res <-moran.test(spNamedVec("res1", work), nb2listw(col.tri.nb,
style="W"))
    moran<-res$estimate[1]
    upper<-res$estimate[1] + (qnorm(0.025, lower.tail=FALSE)
*sqrt(res$estimate[3]))
    lower<-res$estimate[1] - (qnorm(0.025, lower.tail=FALSE)
*sqrt(res$estimate[3]))
    
    print(moran)
    print(upper)
    print(lower)

}

+ 
+ }
Moran I statistic 
         0.659114 
Moran I statistic 
        0.7802115 
Moran I statistic 
        0.5380164 
Moran I statistic 
         0.650799 
Moran I statistic 
         0.771808 
Moran I statistic 
        0.5297899 
Moran I statistic 
        0.6513354 
Moran I statistic 
        0.7723458 
Moran I statistic 
        0.5303249 
Moran I statistic 
        0.6614869 
Moran I statistic 
        0.7825066 
Moran I statistic 
        0.5404672 
Moran I statistic 
        0.6429097 
Moran I statistic 
        0.7638195 
Moran I statistic 
        0.5219998



From aolinto_r at bignet.com.br  Fri Jun 18 21:30:14 2004
From: aolinto_r at bignet.com.br (Antonio Olinto)
Date: Fri, 18 Jun 2004 16:30:14 -0300
Subject: [R] cross table
Message-ID: <1087587014.40d342c610340@webmail2.bignet.com.br>

Hi,

I have a dataframe with 3 columns: month (1 to 12), length and length class.

I'm trying to make a cross table with the counts of lengths per length class 
and month. I don??t have all classes per month.

    | 1 2 3 ...
-------------
120 | 0 1 0
150 | 5 6 0
170 | 3 0 7
...

I have already tryed many commands, without success - unfortunately.

May someone help me? Thanks in advance!

Antonio Olinto



-------------------------------------------------
WebMail Bignet - O seu provedor do litoral
www.bignet.com.br



From Kalderf at gmx.de  Fri Jun 18 22:44:13 2004
From: Kalderf at gmx.de (F.Kalder)
Date: Fri, 18 Jun 2004 22:44:13 +0200 (MEST)
Subject: [R] Another NEWBIE
Message-ID: <23011.1087591453@www47.gmx.net>

Hi,

I'm a very fresh newbie to R.

My first main question is, what the limitations of R are, what methods can R
NOT do, esp. compared to (a) SPSS and (b) SAS? 

The second question is, how do you handle the data entry, data management
and data manipulation in R, to me it seems to be really complicated and
confusing?! Are there a kind of "helping tools"?

The third question: are there differences in linux and windows versions of
R? At the monemt I'm running R on a WinXP System. Is this ok or would a
Linux solutuon be the better way (for using R)?

I hope my questions are not to lame ...


Cheers, Frank

--



From ealaca at ucdavis.edu  Fri Jun 18 22:57:56 2004
From: ealaca at ucdavis.edu (Emilio A. Laca)
Date: Fri, 18 Jun 2004 13:57:56 -0700
Subject: [R] Html help does not work in Mac OSX 10.3.4
Message-ID: <BCF8A564.9F10%ealaca@ucdavis.edu>

I recently upgraded from R 1.8 to 1.9. I removed 1.8 following the
instructions. Html help has not worked since. When htmlhelp="TRUE" the
help.start() command results in the "patience" message and nothing else
happens. I am using mac osx 10.3.4. Help worked fine when I was using R 1.8.

I need help help ;-] Thanks!
-- 
Emilio A. Laca     
One Shields Avenue, 2306 PES Building
Agronomy and Range Science                    ealaca at ucdavis.edu
University of California                      fax: (530) 752-4361
Davis, California  95616                            (530) 754-4083



From wang at galton.uchicago.edu  Fri Jun 18 23:32:58 2004
From: wang at galton.uchicago.edu (Yong Wang)
Date: Fri, 18 Jun 2004 16:32:58 -0500 (CDT)
Subject: [R] Double centering  matrix construction 
In-Reply-To: <200406181002.i5IA2C2x021087@hypatia.math.ethz.ch>
References: <200406181002.i5IA2C2x021087@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.58.0406181621360.24917@aitken.uchicago.edu>

Dear All
do you know how to transform a matrix to a so-called double centering 
matrix such that sum(col) and sum(row) of the transformed matrix are all 0 
vector. 

thanks

regards
yong wang



From cepl at surfbest.net  Fri Jun 18 17:43:20 2004
From: cepl at surfbest.net (Matej Cepl)
Date: Fri, 18 Jun 2004 17:43:20 +0200
Subject: [R] Re: problem in long select from RODBC
In-Reply-To: <200406161442420572.2DB27646@mail.math.fu-berlin.de>
References: <MCSID.771856670@neu.edu>
	<200406161442420572.2DB27646@mail.math.fu-berlin.de>
Message-ID: <200406181743.20098.cepl@surfbest.net>

On Wednesday 16 of June 2004 14:42, Wolski wrote:
> sqlQuery(channel, "select dir, c \
>  from firsthit3 \
>  where dir LIKE \"%SCHULEN%\"")

Another option is paste(). This works for me (selectedCities is a constant 
set outside of this function):

	getData <- function(var,condition,whatTotal="sum(vctcnt)") {
	    temp <- sqlQuery(pg,paste(
	        "select year, month, ori, ", whatTotal," as total",
	        "from victims where ori in ",selectedCities,
	        condition,
	        "group by year,month,ori",
	        "order by year,ori"))
		...
	}

and it is run like:

	data.youth    <- getData("murdyouth"," and offage <= 24 ",
		whatTotal="sum(vctcnt*wtus)")

Have a nice day,

	Matej

-- 
Matej Cepl, http://www.ceplovi.cz/matej
GPG Finger: 89EF 4BC6 288A BF43 1BAB  25C3 E09F EF25 D964 84AC
138 Highland Ave. #10, Somerville, Ma 02143, (617) 623-1488
 
The state is the great fictitious entity by which everyone seeks
to live at the expense of everyone else.
      -- Frederick Bastiat



From cepl at surfbest.net  Fri Jun 18 17:26:26 2004
From: cepl at surfbest.net (Matej Cepl)
Date: Fri, 18 Jun 2004 17:26:26 +0200
Subject: [R] printing R generated postcript files
In-Reply-To: <20040616192303.GA16351@sonny.eddelbuettel.com>
References: <1087396716.4291.13.camel@new-york.climpact.net>
	<20040616190832.28013.qmail@web60108.mail.yahoo.com>
	<20040616192303.GA16351@sonny.eddelbuettel.com>
Message-ID: <200406181726.28655.cepl@surfbest.net>

On Wednesday 16 of June 2004 21:23, Dirk Eddelbuettel wrote:
> That may be related. I only recently set R_PAPERSIZE to be driven from
> Debian's global paperconf setting which is supposed result in
> /etc/R/Renviron having
> 
> ## edd Apr 2004:  use Debian's paperconf settings, with thanks to Matej 
Cepl
> R_PAPERSIZE=${R_PAPERSIZE-$(cat /etc/papersize)}
> 
> yet I just noticed that my own installation doesn't have that. And yes,
> looks like a build-time patch failed. bNeed to check that ...

This probably means that you should install libpaper-utils (especially 
papersize(5) may be helpful).

Matej	

-- 
Matej Cepl, http://www.ceplovi.cz/matej
GPG Finger: 89EF 4BC6 288A BF43 1BAB  25C3 E09F EF25 D964 84AC
138 Highland Ave. #10, Somerville, Ma 02143, (617) 623-1488
 
A modest little person, with much to be modest about.
      -- Winston Churchill



From ggrothendieck at myway.com  Sat Jun 19 01:13:07 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 18 Jun 2004 19:13:07 -0400 (EDT)
Subject: [R] Double centering  matrix construction 
Message-ID: <20040618231307.39CDE39CB@mprdmxin.myway.com>



?sweep

Date:   Fri, 18 Jun 2004 16:32:58 -0500 (CDT) 
From:   Yong Wang <wang at galton.uchicago.edu>
To:   <r-help at stat.math.ethz.ch> 
Subject:   [R] Double centering matrix construction  

 
Dear All
do you know how to transform a matrix to a so-called double centering 
matrix such that sum(col) and sum(row) of the transformed matrix are all 0 
vector. 

thanks

regards
yong wang



From spencer.graves at pdf.com  Sat Jun 19 01:48:17 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 18 Jun 2004 18:48:17 -0500
Subject: [R] cross table
In-Reply-To: <1087587014.40d342c610340@webmail2.bignet.com.br>
References: <1087587014.40d342c610340@webmail2.bignet.com.br>
Message-ID: <40D37F41.6080501@pdf.com>

	  PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

	  What have you tried?  In particular, have you tried "table"?  Dates and times may present difficulties, but help.search('table') in R 1.9.1 alpha under Windows 2000 just produced a number of options.  

	  hope this helps.  spencer graves    

Antonio Olin wrote:

>Hi,
>
>I have a dataframe with 3 columns: month (1 to 12), length and length class.
>
>I'm trying to make a cross table with the counts of lengths per length class 
>and month. I don??t have all classes per month.
>
>    | 1 2 3 ...
>-------------
>120 | 0 1 0
>150 | 5 6 0
>170 | 3 0 7
>...
>
>I have already tryed many commands, without success - unfortunately.
>
>May someone help me? Thanks in advance!
>
>Antonio Olin
>
>
>
>-------------------------------------------------
>WebMail Bignet - O seu provedor do litoral
>www.bignet.com.br
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From spencer.graves at pdf.com  Sat Jun 19 02:03:44 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 18 Jun 2004 19:03:44 -0500
Subject: [R] Another NEWBIE
In-Reply-To: <23011.1087591453@www47.gmx.net>
References: <23011.1087591453@www47.gmx.net>
Message-ID: <40D382E0.803@pdf.com>

      SPSS and SAS are data analysis packages with some scripting 
capabilities.  The S language is an object oriented programming language 
for statistics.  If you want to analyze data using traditional 
techniques, use SPSS or SAS or Statistica or Excel or you-name-it.  If 
you need to invent new statistical techniques tailored to some 
particular application, then you need S, and its most popular current 
dialect seems to me to be R.  There are doubtless things that SPSS, SAS, 
etc., can do that cannot be accomplished in R with 2 or 10 fairly 
obvious commands.  However, I believe that if R had been available 35 
years ago, SAS, SPSS, etc., would have been written in R (or in some 
other dialect of S not subject to the GNU license, which attorneys 
consider controversial and dangerous).  Beyond these generalities, I 
believe you can find answers to many of your questions with "the posting 
guide! http://www.R-project.org/posting-guide.html". 

      hope this helps.  spencer graves

F.Kalder wrote:

>Hi,
>
>I'm a very fresh newbie to R.
>
>My first main question is, what the limitations of R are, what methods can R
>NOT do, esp. compared to (a) SPSS and (b) SAS? 
>
>The second question is, how do you handle the data entry, data management
>and data manipulation in R, to me it seems to be really complicated and
>confusing?! Are there a kind of "helping tools"?
>
>The third question: are there differences in linux and windows versions of
>R? At the monemt I'm running R on a WinXP System. Is this ok or would a
>Linux solutuon be the better way (for using R)?
>
>I hope my questions are not to lame ...
>
>
>Cheers, Frank
>
>--
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From edd at debian.org  Sat Jun 19 02:20:20 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Fri, 18 Jun 2004 19:20:20 -0500
Subject: [R] printing R generated postcript files
In-Reply-To: <200406181726.28655.cepl@surfbest.net>
References: <1087396716.4291.13.camel@new-york.climpact.net>
	<20040616190832.28013.qmail@web60108.mail.yahoo.com>
	<20040616192303.GA16351@sonny.eddelbuettel.com>
	<200406181726.28655.cepl@surfbest.net>
Message-ID: <20040619002020.GA18838@sonny.eddelbuettel.com>

On Fri, Jun 18, 2004 at 05:26:26PM +0200, Matej Cepl wrote:
> On Wednesday 16 of June 2004 21:23, Dirk Eddelbuettel wrote:
> > That may be related. I only recently set R_PAPERSIZE to be driven from
> > Debian's global paperconf setting which is supposed result in
> > /etc/R/Renviron having
> > 
> > ## edd Apr 2004:  use Debian's paperconf settings, with thanks to Matej 
> Cepl
> > R_PAPERSIZE=${R_PAPERSIZE-$(cat /etc/papersize)}
> > 
> > yet I just noticed that my own installation doesn't have that. And yes,
> > looks like a build-time patch failed. bNeed to check that ...
> 
> This probably means that you should install libpaper-utils (especially 
> papersize(5) may be helpful).

Hm, when I tested this the shell expansion failed even though I have
libpaper-utils installed:

edd at basebud:~> dpkg -l | grep "libpaper"
ii  libpaper-dev   1.1.14         Library for handling paper characteristics (
ii  libpaper-utils 1.1.14         Library for handling paper characteristics (
ii  libpaper1      1.1.14         Library for handling paper characteristics
ii  libpaperg      1.1.14         Library for handling paper characteristics (
ii  libpaperg-dev  1.1.14         Library for handling paper characteristics (
edd at basebud:~> cat /etc/papersize
letter
edd at basebud:~> grep ^R_PAPERSIZE /etc/R/Re
Renviron           Renviron.dpkg-old  Renviron~
dd at basebud:~> echo 'postscript("/tmp/foo.ps"); plot(rnorm(100)); dev.off()'
| R --vanilla --silent
> postscript("/tmp/foo.ps"); plot(rnorm(100)); dev.off()
Error in postscript("/tmp/foo.ps") : invalid page type $(cat 
/etc/papersize)' (postscript)
Execution halted
edd at basebud:~>

I tried all quote tick variants around   $(cat /etc/papersize)  as well
without any luck.  

In case you have that working, could you wave the cluebat at me?  I somehow
suspect that this cannot work but I may miss some magic shell pixie dust ...

Dirk


-- 
FEATURE:  VW Beetle license plate in California



From vograno at evafunds.com  Sat Jun 19 02:48:36 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Fri, 18 Jun 2004 17:48:36 -0700
Subject: [R] setGeneric / standardGeneric when args are not "literals"
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A5568984@phost015.EVAFUNDS.intermedia.net>

Hi,
 
This works

> setGeneric("clear", function(obj) standardGeneric("clear"))
[1] "clear"
 
but this doesn't. Why?

> funName <- "clear"
> setGeneric(funName, function(obj) standardGeneric(funName))
Error in .recursiveCallTest(body, fname) : 
 (converted from warning) The body of the generic function for "clear"
calls standardGeneric to dispatch on a different name ("funName")!
 

This is R-1.8.1 on RH-7.3
 
 
I came across it while trying to write a helper function that would
"safely" create generics when a function with such a name already
exists. Here is what I adapted from S4Objects but it doesn't work
becuase of the above-mentioned problem. Any suggestion how to make it
work, please?
 
setMakeGenericMethod <- function(methodName, className, fun) {
  # sets a method and creates the generics if neccessary
  if (!isGeneric(methodName)) {
    if (is.function(methodName)) {
      fun.default <- get(methodName)
    }
    else {
      fun.default <- function(object) standardGeneric(methodName)
    }
  }
 
  setGeneric(methodName, fun.default)
 
  setMethod(methodName, className, fun)
}


Thanks,
Vadim



From vograno at evafunds.com  Sat Jun 19 02:57:54 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Fri, 18 Jun 2004 17:57:54 -0700
Subject: [R] setGeneric / standardGeneric when args are not "literals" -
	corrected
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A5568986@phost015.EVAFUNDS.intermedia.net>

This is a correction to my previous message, I forgot to swap two lines
in the body of setMakeGenericMethod. Sorry about that. The correct (full
message) reads like this:

Hi,
 
This works

> setGeneric("clear", function(obj) standardGeneric("clear"))
[1] "clear"
 
but this doesn't. Why?

> funName <- "clear"
> setGeneric(funName, function(obj) standardGeneric(funName))
Error in .recursiveCallTest(body, fname) : 
 (converted from warning) The body of the generic function for "clear"
calls standardGeneric to dispatch on a different name ("funName")!
 

This is R-1.8.1 on RH-7.3
 
 
I came across it while trying to write a helper function that would
"safely" create generics when a function with such a name already
exists. Here is what I adapted from S4Objects but it doesn't work
because of the above-mentioned problem. Any suggestion how to make it
work, please?
 
setMakeGenericMethod <- function(methodName, className, fun) {
  # sets a method and creates the generics if necessary
  if (!isGeneric(methodName)) {
    if (is.function(methodName)) {
      fun.default <- get(methodName)
    }
    else {
      assign(methodName, methodName)

      browser()
      fun.default <- function(object) standardGeneric(methodName)
    }

    setGeneric(methodName, fun.default)
  }

  setMethod(methodName, className, fun)
}

Thanks,
Vadim



From ripley at stats.ox.ac.uk  Sat Jun 19 05:52:47 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 19 Jun 2004 04:52:47 +0100 (BST)
Subject: [R] Another NEWBIE
In-Reply-To: <23011.1087591453@www47.gmx.net>
Message-ID: <Pine.LNX.4.44.0406190445190.13630-100000@gannet.stats>

On Fri, 18 Jun 2004, F.Kalder wrote:

> I'm a very fresh newbie to R.

First piece of advice: read the posting guide before posting, and in 
particular use a meaningful subject line.

> My first main question is, what the limitations of R are, what methods can R
> NOT do, esp. compared to (a) SPSS and (b) SAS? 

R is a full-featured programming language, with no such limitations.

> The second question is, how do you handle the data entry, data management
> and data manipulation in R, to me it seems to be really complicated and
> confusing?! Are there a kind of "helping tools"?

There is documentation.  For example, chapter 2 of MASS (see the FAQ or 
the posting guide) is devoted to this, and R ships with a `Data 
Import/Manual'.  We don't know your background or skill level, but the FAQ 
points you to lots of documentation.

> The third question: are there differences in linux and windows versions of
> R? At the monemt I'm running R on a WinXP System. Is this ok or would a
> Linux solutuon be the better way (for using R)?

That's in the README of the Windows version.  BTW, perhaps you should ask 
in a suitable forum what the limitations of Windows are relative to Linux 
since like any application R is limited by the OS it runs on.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From jinss at hkusua.hku.hk  Sat Jun 19 10:23:03 2004
From: jinss at hkusua.hku.hk (Jin Shusong)
Date: Sat, 19 Jun 2004 16:23:03 +0800
Subject: [R] Cluseter of time series
Message-ID: <20040619082303.GA6503@S77.hku.hk>

Dear All R users,

  I have many time series. Each of them follows the
structure below,

x_t =  a_0 + a_1 x_{t-1} + a_2 x_{t-2} + \epsilon_t  if x_{t-d} < \theta
       b_0 + b_1 x_{t-1} + b_2 x_{t-2} + \epsilon_t  if x_{t-d} >= \theta

I hope to classify these series into a few groups.  Assume
the delay parameter d is same among all the series but
the threshold parameters \theta are different. Also the
parameters a_i,b_i of the series are same if these series
belong to the same group but are different if the series
belong to different group.  Can you give me some advices in
classifying these series by cluster analysis.  Thank you in
advance.
-- 

   Jin



From connelle at si.edu  Fri Jun 18 23:42:23 2004
From: connelle at si.edu (Ellen Connell)
Date: Fri, 18 Jun 2004 17:42:23 -0400
Subject: [R] Help with Building an R Package in Windows
Message-ID: <s0d3d3fa.016@simail1.si.edu>

Hello R Helpers!

I'm a windows user who is trying to compile a package that was sent to me by a Mac user.  The file was of type .tar.gz, I extracted its elements and saved them to my R directory and then followed all of the steps towards building a package found in the readme.packages file of R, ie. installing the tools.  

However, I get errors in the command prompt.  The readme.packages file lists 2 approaches.  Both ways I get the same error 1.  using one of 2 commands: when I try: R CMD INSTALL package, I get the error - " 'R' is not recognized as an internal or external command, operable program or batch file."  2. or when I try: make pkg-mypkg, I get the same error (" 'make'  is not recognized as an internal or external command, operable program or batch file.")  

So my conclusions were that the computer wasn't able to recognize make.exe (one of the tools that I'd downloaded, installed and put into my R directory).   Could anyone suggest further steps?



From ripley at stats.ox.ac.uk  Sat Jun 19 12:10:26 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 19 Jun 2004 11:10:26 +0100 (BST)
Subject: [R] Help with Building an R Package in Windows
In-Reply-To: <s0d3d3fa.016@simail1.si.edu>
Message-ID: <Pine.LNX.4.44.0406191104430.21980-100000@gannet.stats>

On Fri, 18 Jun 2004, Ellen Connell wrote:

> I'm a windows user who is trying to compile a package that was sent to
> me by a Mac user.  The file was of type .tar.gz, I extracted its
> elements and saved them to my R directory and then followed all of the
> steps towards building a package found in the readme.packages file of R,

You definitely have not followed *all* the steps.

> ie. installing the tools.

That's not the only step there.

> However, I get errors in the command prompt.  The readme.packages file
> lists 2 approaches.  Both ways I get the same error 1.  using one of 2
> commands: when I try: R CMD INSTALL package, I get the error - " 'R' is
> not recognized as an internal or external command, operable program or
> batch file."  2. or when I try: make pkg-mypkg, I get the same error ("
> 'make' is not recognized as an internal or external command, operable
> program or batch file.")
> 
> So my conclusions were that the computer wasn't able to recognize
> make.exe (one of the tools that I'd downloaded, installed and put into
> my R directory).  Could anyone suggest further steps?

>From the top of the file readme.packages

    *** This file contains a lot of prescriptive comments.  They are
    here as a result of bitter experience.  Please do not report problems
    to R-help unless you have followed all the prescriptions. ***

Please do as you are asked.  In particular note the comments

  All of these need to be installed and in your path, and the
  appropriate environment variables set.

  Assuming that ...\rw10xx\bin is in your path, use

ignoring which is almost certainly your problem.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From savano at superig.com.br  Sat Jun 19 13:38:22 2004
From: savano at superig.com.br (Savano)
Date: Sat, 19 Jun 2004 08:38:22 -0300
Subject: [R] executables
Message-ID: <5.1.0.14.2.20040619083611.00bbb828@pop.superig.com.br>

UseR's,

In R, Can I create executables files like in Matlab?

thanks

Savano



From ligges at statistik.uni-dortmund.de  Sat Jun 19 13:38:02 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 19 Jun 2004 13:38:02 +0200
Subject: [R] how to store estimates results as scalars of a matrix?
In-Reply-To: <000001c45564$66a88ff0$21a2f080@Lepc204>
References: <000001c45564$66a88ff0$21a2f080@Lepc204>
Message-ID: <40D4259A.8030901@statistik.uni-dortmund.de>

Joao Pedro W. de Azevedo wrote:
> Dear R users,
> 
> I've written a loop to generate Moran's test (spdep package) on serval
> subsamples of a large dataset. See below a short example.
> 
> My loop is working fine, however I would like to be able to store the test
> results as lines of a matrix, that I would latter be able to export as a
> dataset. My problem is that I'm not sure how I could do this using R.
> 
> Any help will be much appreciated.
> 
> All the very best,
> 
> JP
> 
> 
> 
> coords2 <- as.matrix(jcdist.data[1:87, 6:7])
> 
> col.tri.nb<-tri2nb(coords2)
> 
> for(n in c(1,88,175,262,349)) {
>     f<- n+86
>     work <- jcdist.data[n:f, 10:12]
> 
>     res <-moran.test(spNamedVec("res1", work), nb2listw(col.tri.nb,
> style="W"))
>     moran<-res$estimate[1]
>     upper<-res$estimate[1] + (qnorm(0.025, lower.tail=FALSE)
> *sqrt(res$estimate[3]))
>     lower<-res$estimate[1] - (qnorm(0.025, lower.tail=FALSE)
> *sqrt(res$estimate[3]))
>     
>     print(moran)
>     print(upper)
>     print(lower)

[...]

 > }


What you really want is something along the lines (untested!):

  coords2 <- as.matrix(jcdist.data[1:87, 6:7])
  col.tri.nb <- tri2nb(coords2)
  N <- c(1, 88, 175, 262, 349)
  # generate the transposed matrix of results, because
  # it is faster to assign into columns rathger than into rows:
  results <- mattrix(nrow = 3, ncol = length(N))
  # the following calculations shouldn't be done within the loop -
  # efficiency!
  f <- N + 86
  qn <- qnorm(0.025, lower.tail = FALSE)
  for(i in seq(along = N)) {
      work <- jcdist.data[n[i]:f[i], 10:12]
      res <- moran.test(spNamedVec("res1", work),
                        nb2listw(col.tri.nb, style = "W"))
      # calculate the following just once:
      ci <- qn * sqrt(res$estimate[3])
      results[,i] <- res$estimate[1] + c(1, ci, -ci)
  }
  t(results)
  colnames(results) <- c("Moran", "upper", "lower")


Uwe Ligges



From ligges at statistik.uni-dortmund.de  Sat Jun 19 13:47:56 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 19 Jun 2004 13:47:56 +0200
Subject: [R] executables
In-Reply-To: <5.1.0.14.2.20040619083611.00bbb828@pop.superig.com.br>
References: <5.1.0.14.2.20040619083611.00bbb828@pop.superig.com.br>
Message-ID: <40D427EC.9030500@statistik.uni-dortmund.de>

Savano wrote:

> UseR's,
> 
> In R, Can I create executables files like in Matlab?

No.

Uwe Ligges


> thanks
> 
> Savano
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Sat Jun 19 14:54:40 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Sat, 19 Jun 2004 08:54:40 -0400
Subject: [R] executables
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7F0F@usrymx25.merck.com>

I guess someone's been trying to.  See:

http://www.cs.rice.edu/Colloquia/garvin-04-21.shtml
http://hipersoft.cs.rice.edu/rcc/

Andy

> From: Uwe Ligges
> 
> Savano wrote:
> 
> > UseR's,
> > 
> > In R, Can I create executables files like in Matlab?
> 
> No.
> 
> Uwe Ligges
> 
> 
> > thanks
> > 
> > Savano
> >



From deepayan at stat.wisc.edu  Sat Jun 19 17:21:11 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Sat, 19 Jun 2004 10:21:11 -0500
Subject: [R] polygons around clusters of identically valued nodes in
	levelplot()
In-Reply-To: <200406142058.i5EKwOA32058@snow.pnl.gov>
References: <200406142058.i5EKwOA32058@snow.pnl.gov>
Message-ID: <200406191021.11829.deepayan@stat.wisc.edu>

On Monday 14 June 2004 15:58, Scott Waichler wrote:
> I'm looking for a way to plot lines on top of a levelplot(),
> where the lines are borders between cells of different values.
> The clines() function provides contours suitable for continuous
> data.  I am dealing with discrete values, spatial clusters of nodes
> where each cluster has an integer value, and I want to plot the
> borderlines between these areas.
> So, in a levelplot having nodes with values 0, 1, or 2, I would
> like to plot lines between nodes having the values 0 and 1, 0 and 2
> and 0 and 2. The contouring concept gives two lines for adjacent
> nodes having values of 0 and 1.

I don't entirely understand what you are trying to achieve (would it be 
something like voronoi lines?). In general, lines can be added to 
lattice panels using functions like llines, lsegments, etc. in a custom 
panel function, e.g., 

levelplot(z ~ x * y, ...
          panel = function(x, y, z, ...) {
              panel.levelplot(x, y, z, ...)
              ## do calculations on what lines you 
              ## want to draw, producing variables
              ## xx and yy, say
              llines(xx, yy)
          })

Hope that helps,

Deepayan



From Kalderf at gmx.de  Sat Jun 19 18:15:19 2004
From: Kalderf at gmx.de (F.Kalder)
Date: Sat, 19 Jun 2004 18:15:19 +0200 (MEST)
Subject: [R] Another NEWBIE
Message-ID: <6411.1087661719@www45.gmx.net>

Hi,

Thank you all who anwered me. 

I think, I mainly thought to understand the difference between SPSS /SAS and
R, but didn't really get the point (what explains the question, wich metods
R can't do). Maybe, because I don't have much experience with programming
(near to none). My background in stats goes also only back to indroductory
classes and an advanced course in multivariate statistics. To this, I'm
working with Hair, Anderson, Tatham & Blacks's "Multivariate Data Analysis"
(5th Ed.) as my ressource, mainly with questionnaire analysis (Reliability
Analysis and Factor Analysis, also MDS, Conjoint etc. plus sometimes
standard MANOVA, Multiplke Regression etc.). So, maybe my stats aren't
sophisticated enough to use R, I'm just a standard user of applied
statistical methods, not an academic researcher or even a statistician. It
was mainly a descision by costs, because R is free software. 
With the concept, I completely mistook the R concept as a programming
environment more as a kind of advanced SPSS Syntax (because I also would
call it "programming" when using it), which I now know, is completely wrong.

So, I again thank for your help.


Cheers, Frank.

--



From arin99 at rediffmail.com  Sat Jun 19 19:05:44 2004
From: arin99 at rediffmail.com (Arin Basu)
Date: 19 Jun 2004 17:05:44 -0000
Subject: [R] Charts and Graphs
Message-ID: <20040619170544.10863.qmail@webmail45.rediffmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040619/d13691a9/attachment.pl

From f.harrell at vanderbilt.edu  Sat Jun 19 14:18:31 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sat, 19 Jun 2004 12:18:31 +0000
Subject: [R] Another NEWBIE
In-Reply-To: <6411.1087661719@www45.gmx.net>
References: <6411.1087661719@www45.gmx.net>
Message-ID: <40D42F17.9060602@vanderbilt.edu>

F.Kalder wrote:
> Hi,
> 
> Thank you all who anwered me. 
> 
> I think, I mainly thought to understand the difference between SPSS /SAS and
> R, but didn't really get the point (what explains the question, wich metods
> R can't do). Maybe, because I don't have much experience with programming
> (near to none). My background in stats goes also only back to indroductory
> classes and an advanced course in multivariate statistics. To this, I'm
> working with Hair, Anderson, Tatham & Blacks's "Multivariate Data Analysis"
> (5th Ed.) as my ressource, mainly with questionnaire analysis (Reliability
> Analysis and Factor Analysis, also MDS, Conjoint etc. plus sometimes
> standard MANOVA, Multiplke Regression etc.). So, maybe my stats aren't
> sophisticated enough to use R, I'm just a standard user of applied
> statistical methods, not an academic researcher or even a statistician. It
> was mainly a descision by costs, because R is free software. 
> With the concept, I completely mistook the R concept as a programming
> environment more as a kind of advanced SPSS Syntax (because I also would
> call it "programming" when using it), which I now know, is completely wrong.
> 
> So, I again thank for your help.
> 
> 
> Cheers, Frank.
> 
> --

You'll find (eventually) that you can do everything you need in R short 
of accurately getting certain P-values in mixed effect linear models for 
which SAS does a good job.  It's a question of finding the right books, 
online manuals (main R manuals as well as user contributed ones - see 
especially "Simple R" to start), understanding specific functions, and 
perhaps above all, finding examples to work from.  For data manipulation 
in particular (recoding, reshaping data, etc.), R is now superior to all 
other systems unless the database is truly massive.  For statistical 
reporting R is also way ahead of the pack (e.g., Sweave with LaTeX). 
SAS and SPSS no longer even complement R in my opinion, except for SAS's 
scalability in processing massive databases and some features (but not 
model diagnostics or graphics) of PROC MIXED.

You can think of using R as programming, but for application of many 
popular analytic methods I prefer to think of it as finding example 
scripts and modifying them according to your needs.
-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From sdhyok at email.unc.edu  Sun Jun 20 04:59:57 2004
From: sdhyok at email.unc.edu (Shin)
Date: Sat, 19 Jun 2004 22:59:57 -0400
Subject: [R] A way to list only variables or functions?
Message-ID: <1087700394.2043.2.camel@dhcp9792.dhcp.unc.edu>

I am curious if there is any way to list only variables or functions in
current environment, rather than listing all objects? Thanks.

-- 
Daehyok Shin (Peter)
Geography Department
Univ. of North Carolina-Chapel Hill



From ripley at stats.ox.ac.uk  Sun Jun 20 09:30:50 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 20 Jun 2004 08:30:50 +0100 (BST)
Subject: [R] A way to list only variables or functions?
In-Reply-To: <1087700394.2043.2.camel@dhcp9792.dhcp.unc.edu>
Message-ID: <Pine.LNX.4.44.0406200822030.5815-100000@gannet.stats>

On Sat, 19 Jun 2004, Shin wrote:

> I am curious if there is any way to list only variables or functions in
> current environment, rather than listing all objects? Thanks.

Not really.  What you can do is list the objects and then get the objects 
and look at the modes, as ls.str does.  That could easily be modified to 
just list, but you may find what it gives helpful anyway.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ciradhwg at mweb.co.zw  Sun Jun 20 09:54:30 2004
From: ciradhwg at mweb.co.zw (Hwange Project)
Date: Sun, 20 Jun 2004 09:54:30 +0200
Subject: [R] Constrained non-linear mixed models 
In-Reply-To: <Pine.LNX.4.44.0405131345490.16197-100000@gannet.stats>
Message-ID: <IEEFKHGFNDGNNFCJALMFEECCCFAA.ciradhwg@mweb.co.zw>

Dear r-helpers,
does anyone knows how to constrain the value of parameters in a non-linear
mixed model:
I've got something like that:
nlme(log(response)~z+y*log(var1)+x*log(var2)+log((b*var3+a)/(b*var4+a)^x)
where 0<x<1, a<1 and (b*(var3 or var4: range of values similar)+a) between 0
and 1.
or even better if you know how to linearize it !!

and another one :
What means ?
Error in chol((value + t(value))/2) : the leading minor of order 1 is not
positive definite
(my statical books do not go far enough in the detail.
It must be about the Cholesky something if I remember well what I read
long-time ago).

I'm sorry to bother you with this kind of things, but I'm in a remote place
(~18??45'S,26??45'E)
(even if e-mails are working at the incredible speed of 4.8 kbits/sec...)
without a lot of statistical or computer ressources. Thanks,
simon

---------------------------------------------------------
Simon Chamaill??
Hwange Project
CIRAD-CNRS
Hwange Main Camp Research
P. Bag 5776, Dete
(+00 263) 18-647
ciradhwg at mweb.co.zw <mailto:ciradhwg at mweb.co.zw>



From spencer.graves at pdf.com  Sun Jun 20 15:18:40 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Sun, 20 Jun 2004 08:18:40 -0500
Subject: [R] Constrained non-linear mixed models
In-Reply-To: <IEEFKHGFNDGNNFCJALMFEECCCFAA.ciradhwg@mweb.co.zw>
References: <IEEFKHGFNDGNNFCJALMFEECCCFAA.ciradhwg@mweb.co.zw>
Message-ID: <40D58EB0.7050402@pdf.com>


WHY THIS FUNCTIONAL FORM? 

      If this were my problem, I would first want to know why we needed 
this particular functional form and why (b*var3+a) and (b*var4+a) had to 
be between 0 and 1.  When I see things like this, my first impulse is 
that these may only be approximations to something else, and then I 
would seek a different approximation that might plausibly be better and 
was not subject to the constraints. 

CREATIVE PLOTS? 

         What kind(s) of random model(s) are you considering?  What 
kinds of plots have you made?  Before I did heavy modeling, I'd compute 
correlations and make plots designed to explore the chosen relationship 
from different perspectives, e.g., using lattice graphics, contour and 
perspective / wireframe plots, etc.  Such plots might suggest 
alternative functional forms. 

PENALIZING "nlme"? 

      If I still needed to fit the model you suggested, I might first 
rewrite it something like the following: 

      nlme(log(response)~z+y*log(var1)+x*log(var2)+log01(b*var3+a, 
xlim)-x*log01(b*var4+a, xlim), ...

      where log01 is something like the following [***UNTESTED***]: 

log01 <- function(zz, xlim){
# log(zz) if xlim[1]<zz<xlim[2]
# else something that goes balistic othis range
# with rate controlled by xlim[3]; 
#     log01 must be continuous and monotonic. 
  zz.sm <- (zz<xlim[1])
  zz.lg <- (zz>xlim[2])
  log.zz <- rep(NA, length(zz))
  log.zz[zz.sm] <- (log(xlim[1])-
      (exp(xlim[3]*(xlim[1]-zz[zz.sm]))-1))
  log.zz[zz.lg] <- (log(xlim[2])+
      (exp(xlim[3]*(zz[zz.lg]-xlim[2]))-1))
  zz.gd <- !(zz.sm | zz.lg)
  log.zz[zz.gd] <- log(zz[zz.gd])
  log.zz
}

      I would first run it with xlim something like c(0.1, 0.9, 0.01).  
I'd then look at the cases for which I got log01 outside xlim[1:2] and 
(gradually) adjust xlim to get something close to what I wanted, e.g., 
like c(0.01, 0.99, 100).  If this didn't work well, I might also try to 
modify log01 so it was also differentiable in zz. 

FITTING SUBSETS?      
 
      I might also try fitting the same model to different subsets of 
the data, perhaps suggested by the most plausible random effects models, 
then use the parameter estimates as data for a second stage analysis, 
following Box and Hunter, "A Useful Method for Model Building" paper;  
you should be able to find a complete citation on the web. 

MARKOV CHAIN MONTE CARLO (MCMC)? 

      I think the "gold standard" for this type of problem is MCMC.  
There is software available for that, but I haven't used it and 
therefore can't comment.  However, before I got to that point, I would 
make lots of plots and try to several alternative, simpler models to 
convince myself that this was both appropriate and necessary. 

      hope this helps.  spencer graves

Hwange Project wrote:

>Dear r-helpers,
>does anyone knows how to constrain the value of parameters in a non-linear
>mixed model:
>I've got something like that:
>nlme(log(response)~z+y*log(var1)+x*log(var2)+log((b*var3+a)/(b*var4+a)^x)
>where 0<x<1, a<1 and (b*(var3 or var4: range of values similar)+a) between 0
>and 1.
>or even better if you know how to linearize it !!
>
>and another one :
>What means ?
>Error in chol((value + t(value))/2) : the leading minor of order 1 is not
>positive definite
>(my statical books do not go far enough in the detail.
>It must be about the Cholesky something if I remember well what I read
>long-time ago).
>
>I'm sorry to bother you with this kind of things, but I'm in a remote place
>(~18??45'S,26??45'E)
>(even if e-mails are working at the incredible speed of 4.8 kbits/sec...)
>without a lot of statistical or computer ressources. Thanks,
>simon
>
>---------------------------------------------------------
>Simon Chamaill??
>Hwange Project
>CIRAD-CNRS
>Hwange Main Camp Research
>P. Bag 5776, Dete
>(+00 263) 18-647
>ciradhwg at mweb.co.zw <mailto:ciradhwg at mweb.co.zw>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From ggrothendieck at myway.com  Sun Jun 20 16:05:41 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 20 Jun 2004 10:05:41 -0400 (EDT)
Subject: [R] A way to list only variables or functions?
Message-ID: <20040620140541.0D6B412D20@mprdmxin.myway.com>



These two functions will list the functions and variables
respectively:

ls.funs <- function(env=sys.frame(-1))unlist(lapply(ls(env=env),function(x)if(is.function(get(x)))x))

ls.vars <- function(env=sys.frame(-1))unlist(lapply(ls(env=env),function(x)if(!is.function(get(x)))x))


To use:

ls.funs()
ls.vars()


Date:   Sat, 19 Jun 2004 22:59:57 -0400 
From:   Shin <sdhyok at email.unc.edu>
To:   R Help <r-help at stat.math.ethz.ch> 
Subject:   [R] A way to list only variables or functions? 

 
I am curious if there is any way to list only variables or functions in
current environment, rather than listing all objects? Thanks.

-- 
Daehyok Shin (Peter)
Geography Department
Univ. of North Carolina-Chapel Hill



From AKROPOLIS-SA at PERLITZGROUP.com  Sun Jun 20 16:35:43 2004
From: AKROPOLIS-SA at PERLITZGROUP.com (Systemaufsicht)
Date: Sun, 20 Jun 2004 16:35:43 +0200
Subject: [R] [MailServer Notification] To External Sender: a virus was found
	a nd action taken.
Message-ID: <67D857E22461D711ABD0000102B0F0EE233474@akropolis.perlitzgroup.local>

ScanMail for Microsoft Exchange took action on the message.  The message
details were: 
Sender = r-help at lists.r-project.org
Recipient(s) = schrank at m2c.de;
Subject = i know your document!
Scanning time = 06/20/2004 16:35:42
Engine/Pattern = 7.000-1004/1.909.00

Action taken on message:
The attachment more.zip contained WORM_NETSKY.C virus. ScanMail took the
action: Deleted. 
ScanMail has detected a virus in an email you sent.schrank at m2c.de;



From dvanbrunt at mac.com  Sun Jun 20 16:57:30 2004
From: dvanbrunt at mac.com (David L. Van Brunt, Ph.D.)
Date: Sun, 20 Jun 2004 09:57:30 -0500
Subject: [R] Anyone out there have the XLF Fortran compiler for OS X?
In-Reply-To: <20040528131537.57660028.Achim.Zeileis@wu-wien.ac.at>
References: <20040528131537.57660028.Achim.Zeileis@wu-wien.ac.at>
Message-ID: <276E652A-C2CA-11D8-A49A-000393B2A94A@mac.com>

I have code that will crash R every time on an OS X machine, but runs 
fine on other platforms. The developer I've been working with suspects 
it's a compiler issue, but I only have the same ones he does (g77).

Obviously, the way to test this would be to compile R using another 
compiler. XLF is the (by IBM, distributed by Absoft) is the only other 
one I know of. It's pretty expensive on its own, so if somebody has a 
license and would be willing to help out, please drop me a line.

Thanks!



From sdhyok at email.unc.edu  Sun Jun 20 17:51:30 2004
From: sdhyok at email.unc.edu (Shin, Daehyok)
Date: Sun, 20 Jun 2004 11:51:30 -0400
Subject: [R] A way to list only variables or functions?
In-Reply-To: <20040620140541.0D6B412D20@mprdmxin.myway.com>
Message-ID: <OAEOKPIGCLDDHAEMCAKIEEKNCOAA.sdhyok@email.unc.edu>

Neat! Thanks.
How about incorporating this support into standard commands, ls() or
objects()?

Daehyok Shin (Peter)

> -----Original Message-----
> From: Gabor Grothendieck [mailto:ggrothendieck at myway.COM]
> Sent: Sunday, June 20, 2004 AM 10:06
> To: sdhyok at email.unc.edu; r-help at stat.math.ethz.ch
> Subject: RE: [R] A way to list only variables or functions?
>
>
>
>
> These two functions will list the functions and variables
> respectively:
>
> ls.funs <-
> function(env=sys.frame(-1))unlist(lapply(ls(env=env),function(x)if
> (is.function(get(x)))x))
>
> ls.vars <-
> function(env=sys.frame(-1))unlist(lapply(ls(env=env),function(x)if
> (!is.function(get(x)))x))
>
>
> To use:
>
> ls.funs()
> ls.vars()
>
>
> Date:   Sat, 19 Jun 2004 22:59:57 -0400
> From:   Shin <sdhyok at email.unc.edu>
> To:   R Help <r-help at stat.math.ethz.ch>
> Subject:   [R] A way to list only variables or functions?
>
>
> I am curious if there is any way to list only variables or functions in
> current environment, rather than listing all objects? Thanks.
>
> --
> Daehyok Shin (Peter)
> Geography Department
> Univ. of North Carolina-Chapel Hill
>
>
>
> _______________________________________________
> No banners. No pop-ups. No kidding.

>



From sdhyok at email.unc.edu  Sun Jun 20 17:55:43 2004
From: sdhyok at email.unc.edu (Shin, Daehyok)
Date: Sun, 20 Jun 2004 11:55:43 -0400
Subject: [R] A way to list only variables or functions?
In-Reply-To: <003501c456d5$f3837d00$8b00a8c0@IBM>
Message-ID: <OAEOKPIGCLDDHAEMCAKIMEKNCOAA.sdhyok@email.unc.edu>

Interesting, but it seems too complex.
In my opinion, just listing functions or non-functions meets my need.
Anyway, thanks.

Daehyok Shin (Peter)

> -----Original Message-----
> From: S?en Merser [mailto:merser at image.dk]
> Sent: Sunday, June 20, 2004 AM 10:51
> To: Shin
> Subject: Re: [R] A way to list only variables or functions?
> 
> 
> hi
> forgot who actualy wrote this code but it works nicely
> use:
>     ls.objects(type='function')
> 
> regards soren
> 
>  ls.objects<-function (pos = 1, pattern, mode = "any", type = "any"){
>     Object.Name <- ls(pos = pos, envir = as.environment(pos), pattern =
> pattern)
>     Object.Mode <- rep("",length(Object.Name))
>     Object.Type <- rep("",length(Object.Name))
>     Variables <- rep("-",length(Object.Name))
>     Observations <- rep("-",length(Object.Name))
>     for (i in 1:length(Object.Name)){
> Object.Mode[[i]] <- mode(get(Object.Name[[i]]))
> if(is.list(get(Object.Name[[i]]))){
> if(is.null(class(get(Object.Name[[i]]))))
> Object.Type[[i]] <- c("unknown")
> else {
> Object.Attrib <- attributes(get(Object.Name[[i]]))
> if(length(Object.Attrib$class) == 1) {
> Object.Type[[i]] <- Object.Attrib$class
> if(Object.Type[[i]]=="data.frame"){
> Variables[[i]] <- as.character(length(Object.Attrib$names))
> Observations[[i]] <- as.character(length(Object.Attrib$row.names))
> }
> }
> else {
> if("data.frame" %in% Object.Attrib$class){
> Object.Type[[i]] <- "data.frame"
> Variables[[i]] <- as.character(length(Object.Attrib$names))
> Observations[[i]] <- as.character(length(Object.Attrib$row.names))
> }
> else {
> if("aov" %in% Object.Attrib$class) Object.Type[[i]] <- "aov"
> }
> }
> }
> }
> if(is.matrix(get(Object.Name[[i]]))){
> Object.Attrib <- dim(get(Object.Name[[i]]))
> Object.Type[[i]] <- c("matrix")
> Variables[[i]] <- as.character(Object.Attrib[2])
> Observations[[i]] <- as.character(Object.Attrib[1])
> }
> if(is.vector(get(Object.Name[[i]])) && (Object.Mode[[i]]=="character" ||
> Object.Mode[[i]]=="numeric")){
> Object.Type[[i]] <- c("vector")
> Variables[[i]] <- c("1")
> Observations[[i]] <- as.character(length(get(Object.Name[[i]])))
> }
> if(is.factor(get(Object.Name[[i]]))){
> Object.Type[[i]] <- c("factor")
> Variables[[i]] <- c("1")
> Observations[[i]] <- as.character(length(get(Object.Name[[i]])))
> }
> if(is.function(get(Object.Name[[i]]))) Object.Type[[i]] <- c("function")
>     }
>     objList <-
> data.frame(Object.Name,Object.Mode,Object.Type,Observations,Variables)
>     if(mode != "any") objList <- objList[objList[["Object.Mode"]] 
> == mode,]
>     if(type != "any") objList <- objList[objList[["Object.Type"]] 
> == type,]
>     return(objList)
> }
> ----- Original Message ----- 
> From: "Shin" <sdhyok at email.unc.edu>
> To: "R Help" <r-help at stat.math.ethz.ch>
> Sent: Sunday, June 20, 2004 4:59 AM
> Subject: [R] A way to list only variables or functions?
> 
> 
> > I am curious if there is any way to list only variables or functions in
> > current environment, rather than listing all objects? Thanks.
> >
> > -- 
> > Daehyok Shin (Peter)
> > Geography Department
> > Univ. of North Carolina-Chapel Hill
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> >
> 
> 
>



From ivo_welch-Rstat at mailblocks.com  Sun Jun 20 18:00:39 2004
From: ivo_welch-Rstat at mailblocks.com (ivo_welch-Rstat@mailblocks.com)
Date: Sun, 20 Jun 2004 09:00:39 -0700
Subject: [R] if syntax
Message-ID: <200406201600.i5KG0eUI020689@hypatia.math.ethz.ch>


I ran into an interesting oddity of R,
    if (0) { print(1); }
    else  { print(2); }
 is a syntax error, while
    if (0) { print(1); } else  { print(2); }
or
    if (0) { print(1);
    } else  { print(2); }
  is not.  I presume it has to do with the duality of the newline 
functioning as an end of command (;) character, though it still seems a 
bit odd, and it took me a while to figure out what was wrong.  I 
eventually figured out that to resolve this ambiguity, I would guess 
that ifelse() would be a preferred function.

I wanted to look up the internal R documentation for if via "?if", but 
this does not work.  making the latter work would be a good idea.

regards,  /iaw
---
ivo welch
professor of finance and economics
brown / nber / yale



From ripley at stats.ox.ac.uk  Sun Jun 20 18:02:40 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 20 Jun 2004 17:02:40 +0100 (BST)
Subject: [R] A way to list only variables or functions?
In-Reply-To: <OAEOKPIGCLDDHAEMCAKIEEKNCOAA.sdhyok@email.unc.edu>
Message-ID: <Pine.LNX.4.44.0406201658590.3158-100000@gannet.stats>

On Sun, 20 Jun 2004, Shin, Daehyok wrote:

> Neat! Thanks.

Note that these are not correct, as the get is not done from the 
correct environment.  The function ls.str I pointed you to is correct.

> How about incorporating this support into standard commands, ls() or
> objects()?

Well, there already is ls[f].str.

> Daehyok Shin (Peter)
> 
> > -----Original Message-----
> > From: Gabor Grothendieck [mailto:ggrothendieck at myway.COM]
> > Sent: Sunday, June 20, 2004 AM 10:06
> > To: sdhyok at email.unc.edu; r-help at stat.math.ethz.ch
> > Subject: RE: [R] A way to list only variables or functions?
> >
> >
> >
> >
> > These two functions will list the functions and variables
> > respectively:
> >
> > ls.funs <-
> > function(env=sys.frame(-1))unlist(lapply(ls(env=env),function(x)if
> > (is.function(get(x)))x))
> >
> > ls.vars <-
> > function(env=sys.frame(-1))unlist(lapply(ls(env=env),function(x)if
> > (!is.function(get(x)))x))
> >
> >
> > To use:
> >
> > ls.funs()
> > ls.vars()
> >
> >
> > Date:   Sat, 19 Jun 2004 22:59:57 -0400
> > From:   Shin <sdhyok at email.unc.edu>
> > To:   R Help <r-help at stat.math.ethz.ch>
> > Subject:   [R] A way to list only variables or functions?
> >
> >
> > I am curious if there is any way to list only variables or functions in
> > current environment, rather than listing all objects? Thanks.
> >
> > --
> > Daehyok Shin (Peter)
> > Geography Department
> > Univ. of North Carolina-Chapel Hill
> >
> >
> >
> > _______________________________________________
> > No banners. No pop-ups. No kidding.
> 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ramasamy at cancer.org.uk  Sun Jun 20 18:22:34 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: 20 Jun 2004 17:22:34 +0100
Subject: [R] if syntax
In-Reply-To: <200406201600.i5KG0eUI020689@hypatia.math.ethz.ch>
References: <200406201600.i5KG0eUI020689@hypatia.math.ethz.ch>
Message-ID: <1087748508.4606.1.camel@localhost.localdomain>

This has been discussed several times on this list. Note that line 2 of
paragraph 2 of help("if") says the following :

" In particular, you should not have a newline between '}' and 'else' to
avoid a syntax error in entering a 'if ... else' construct at the
keyboard or via 'source'. "


On Sun, 2004-06-20 at 17:00, ivo_welch-Rstat at mailblocks.com wrote:
> I ran into an interesting oddity of R,
>     if (0) { print(1); }
>     else  { print(2); }
>  is a syntax error, while
>     if (0) { print(1); } else  { print(2); }
> or
>     if (0) { print(1);
>     } else  { print(2); }
>   is not.  I presume it has to do with the duality of the newline 
> functioning as an end of command (;) character, though it still seems a 
> bit odd, and it took me a while to figure out what was wrong.  I 
> eventually figured out that to resolve this ambiguity, I would guess 
> that ifelse() would be a preferred function.
> 
> I wanted to look up the internal R documentation for if via "?if", but 
> this does not work.  making the latter work would be a good idea.
> 
> regards,  /iaw
> ---
> ivo welch
> professor of finance and economics
> brown / nber / yale
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From ml-r-help at epigenomics.com  Sun Jun 20 18:25:06 2004
From: ml-r-help at epigenomics.com (Matthias Burger)
Date: Sun, 20 Jun 2004 18:25:06 +0200
Subject: [R] if syntax
In-Reply-To: <200406201600.i5KG0eUI020689@hypatia.math.ethz.ch>
References: <200406201600.i5KG0eUI020689@hypatia.math.ethz.ch>
Message-ID: <40D5BA62.3030400@epigenomics.com>



try

?"if"

Best,

   Matthias


ivo_welch-Rstat at mailblocks.com wrote:
> 
> I ran into an interesting oddity of R,
>    if (0) { print(1); }
>    else  { print(2); }
> is a syntax error, while
>    if (0) { print(1); } else  { print(2); }
> or
>    if (0) { print(1);
>    } else  { print(2); }
>  is not.  I presume it has to do with the duality of the newline 
> functioning as an end of command (;) character, though it still seems a 
> bit odd, and it took me a while to figure out what was wrong.  I 
> eventually figured out that to resolve this ambiguity, I would guess 
> that ifelse() would be a preferred function.
> 
> I wanted to look up the internal R documentation for if via "?if", but 
> this does not work.  making the latter work would be a good idea.
> 
> regards,  /iaw
> ---
> ivo welch
> professor of finance and economics
> brown / nber / yale
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 

-- 
Matthias Burger

Bioinformatics R&D
Epigenomics AG                      www.epigenomics.com
Kleine Pr??sidentenstra??e 1          fax:   +49-30-24345-555
10178 Berlin Germany                phone: +49-30-24345-0



From ligges at statistik.uni-dortmund.de  Sun Jun 20 18:32:23 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 20 Jun 2004 18:32:23 +0200
Subject: [R] if syntax
In-Reply-To: <200406201600.i5KG0eUI020689@hypatia.math.ethz.ch>
References: <200406201600.i5KG0eUI020689@hypatia.math.ethz.ch>
Message-ID: <40D5BC17.4050102@statistik.uni-dortmund.de>

ivo_welch-Rstat at mailblocks.com wrote:

> 
> I ran into an interesting oddity of R,
>    if (0) { print(1); }
>    else  { print(2); }

In both cases the ";" is superflously .....


If if(){}else{} is within an expression (e.g. a function's body), both 
ways work.
Do you really want to use it outside a function's body? If so, I suggest 
to write:

if(...){
     ...
} else{
     ...
}



> is a syntax error, while
>    if (0) { print(1); } else  { print(2); }
> or
>    if (0) { print(1);
>    } else  { print(2); }
>  is not.  I presume it has to do with the duality of the newline 
> functioning as an end of command (;) character, though it still seems a 
> bit odd, and it took me a while to figure out what was wrong.  I 
> eventually figured out that to resolve this ambiguity, I would guess 
> that ifelse() would be a preferred function.

No. ifelse() is for the vectorized conditions. if(){} else{} is more 
efficient for length 1 conditions. Please read help("if") and 
help("ifelse").


> I wanted to look up the internal R documentation for if via "?if", but 
> this does not work.  making the latter work would be a good idea.

?"if" or help("if") do work perfectly.

Uwe Ligges


> regards,  /iaw
> ---
> ivo welch
> professor of finance and economics
> brown / nber / yale
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From cafa at comvest.unicamp.br  Sun Jun 20 18:46:10 2004
From: cafa at comvest.unicamp.br (Cezar Augusto de Freitas Anselmo)
Date: Sun, 20 Jun 2004 13:46:10 -0300 (BRT)
Subject: [R] hidden markov models in R?
Message-ID: <Pine.LNX.4.33.0406201344440.26898-100000@w3.comvest.unicamp.br>

Hi, friends!

Has R estimation (library, for example) to do estimation in HMM?

Thanks in advance,

========================================
Cezar Freitas
Estatistico - Comissao Permanente para os Vestibulares / UNICAMP
Probabilidade e Estatistica Aplicadas - IME / USP | IMECC / UNICAMP
Campinas | Sao Paulo, SP - Brasil



From ckjmaner at carolina.rr.com  Sun Jun 20 18:59:30 2004
From: ckjmaner at carolina.rr.com (Charles and Kimberly Maner)
Date: Sun, 20 Jun 2004 12:59:30 -0400
Subject: [R] Another NEWBIE
In-Reply-To: <200406201004.i5KA3BQB025403@hypatia.math.ethz.ch>
Message-ID: <200406201659.i5KGxfVw017615@ms-smtp-04-eri0.southeast.rr.com>


Hi Frank.  I am (somewhat) new to R as well, but almost a 10 yr SAS veteran.
I work for a very large US Bank and have spent a considerable part of my
career in Corp Mktg leveraging data for, arguably, data mining, next
purchase, attrition, balance diminishment and the like.  I am now managing
an Operations Research group in their Customer Service and Support (aka
Telephone/Call Center Support) within the forecasting and analytics group.
What I have found, broadly and personally, regarding R vs. sAS is the
following:

1.  You simply can't beat the price of R vs. Insightful Corp.'s S-Splus, not
to mention SAS.
2.  The support folks for R are among the very best, (e.g., most helpful,
energetic and enthusiastic to help)
3.  R is far, far leaner from what I have seen thus far for modeling,
binning/discretizing, graphing, etc. vs. SAS.
4.  SAS is, per a previous post, (quite debatably) superior for manipulation
and handling of fantastically large datasets.  I have found that R's
strength is not really in merging datasets and dataset manipulation.
Although, major caveat here, it greatly depends on what you need done to the
data.  For lagging, diffing, binning, R is superior.  For match merging, at
this stage, I vote for SAS.  (Again, I stress I've only 6-8 mos of moderate
R experience.)
5.  The challenge with R is, perhaps, it's very strength--language density.
Once I learn how to do something in R vs. SAS, R's code is fractionally as
large as SAS.  Literally, it may take 10 lines of code in SAS vs. a one
liner in R.  That's powerful.  However, due to my SAS experience, I've
banged out the SAS code and am still looking/hunting for the R equivalent.
However, once doing so, it's, borrowing from a popular vegetable drink
slogan, "Wow, I could have done that in R."
6.  And, lastly, while R is well documented, I seem to find one of the areas
of documentation somewhat lacking is a great big R "recipe" book.
(Suggestions, BTW, are welcome here.)  Documentation of the R language is in
place with more being published, (alongside S-Plus), annually.  However,
there does not appear to be, for example, an "R Transition Recipes for
Experienced SAS Users" book.  That, ultimately, is what would help me, (I
think.)  Again, the issue really is simply learning and using the language.
Experienced R users, I'm convinced, could do everything R I'm doing in SAS,
(with money left over for a few coffees at Starbuck's).

In conclusion, I still think that, given one's budget and projects, there's
a place for SAS and R to co-exist.  But, that paradigm diminishes as (1) the
size of the datasets become smaller and, (2) your problems are more
academic/researchy/specific in nature.  For graphing, esp. w/the Lattice
package, R is simply superior (IMHO), period, to SAS.  (For some reason, SAS
has just not felt the need to improve their graphics, at least the SAS/Graph
part of their offering.)  And, for the SAS lovers out there, this opinion is
mine only as I continue to be primarily a SAS client attempting to
transition to R.

Frank, while I've probably been too wordy, I've attempted to provide another
perspective for you.  Good luck.


Thanks,
Charles


------------------------------

Message: 7
Date: Sat, 19 Jun 2004 18:15:19 +0200 (MEST)
From: "F.Kalder" <Kalderf at gmx.de>
Subject: Re: [R] Another NEWBIE
To: r-help at stat.math.ethz.ch
Message-ID: <6411.1087661719 at www45.gmx.net>
Content-Type: text/plain; charset="us-ascii"

Hi,

Thank you all who anwered me. 

I think, I mainly thought to understand the difference between SPSS /SAS and
R, but didn't really get the point (what explains the question, wich metods
R can't do). Maybe, because I don't have much experience with programming
(near to none). My background in stats goes also only back to indroductory
classes and an advanced course in multivariate statistics. To this, I'm
working with Hair, Anderson, Tatham & Blacks's "Multivariate Data Analysis"
(5th Ed.) as my ressource, mainly with questionnaire analysis (Reliability
Analysis and Factor Analysis, also MDS, Conjoint etc. plus sometimes
standard MANOVA, Multiplke Regression etc.). So, maybe my stats aren't
sophisticated enough to use R, I'm just a standard user of applied
statistical methods, not an academic researcher or even a statistician. It
was mainly a descision by costs, because R is free software. 
With the concept, I completely mistook the R concept as a programming
environment more as a kind of advanced SPSS Syntax (because I also would
call it "programming" when using it), which I now know, is completely wrong.

So, I again thank for your help.


Cheers, Frank.

--



From edd at debian.org  Sun Jun 20 19:16:21 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Sun, 20 Jun 2004 12:16:21 -0500
Subject: [R] if syntax
In-Reply-To: <200406201600.i5KG0eUI020689@hypatia.math.ethz.ch>
References: <200406201600.i5KG0eUI020689@hypatia.math.ethz.ch>
Message-ID: <20040620171621.GA8156@sonny.eddelbuettel.com>

On Sun, Jun 20, 2004 at 09:00:39AM -0700, ivo_welch-Rstat at mailblocks.com wrote:
> I wanted to look up the internal R documentation for if via "?if", but 
> this does not work.  making the latter work would be a good idea.

FYI, it also works inside of (X)Emacs where the (highly recommend) ESS mode
is kind enough to translate   ?if   into   ?"if"  so that the desired help
is in fact shown.

Hth, Dirk

-- 
FEATURE:  VW Beetle license plate in California



From f.harrell at vanderbilt.edu  Sun Jun 20 14:44:24 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sun, 20 Jun 2004 12:44:24 +0000
Subject: [R] Another NEWBIE
In-Reply-To: <200406201659.i5KGxfVw017615@ms-smtp-04-eri0.southeast.rr.com>
References: <200406201659.i5KGxfVw017615@ms-smtp-04-eri0.southeast.rr.com>
Message-ID: <40D586A8.5050003@vanderbilt.edu>

Charles and Kimberly Maner wrote:
> Hi Frank.  I am (somewhat) new to R as well, but almost a 10 yr SAS veteran.
> I work for a very large US Bank and have spent a considerable part of my
> career in Corp Mktg leveraging data for, arguably, data mining, next
> purchase, attrition, balance diminishment and the like.  I am now managing
> an Operations Research group in their Customer Service and Support (aka
> Telephone/Call Center Support) within the forecasting and analytics group.
> What I have found, broadly and personally, regarding R vs. sAS is the
> following:
> 
> 1.  You simply can't beat the price of R vs. Insightful Corp.'s S-Splus, not
> to mention SAS.
> 2.  The support folks for R are among the very best, (e.g., most helpful,
> energetic and enthusiastic to help)
> 3.  R is far, far leaner from what I have seen thus far for modeling,
> binning/discretizing, graphing, etc. vs. SAS.

Thanks for your note.  I assume you meant to say 'more capable' rather 
than 'leaner'.

> 4.  SAS is, per a previous post, (quite debatably) superior for manipulation
> and handling of fantastically large datasets.  I have found that R's
> strength is not really in merging datasets and dataset manipulation.
> Although, major caveat here, it greatly depends on what you need done to the
> data.  For lagging, diffing, binning, R is superior.  For match merging, at
> this stage, I vote for SAS.  (Again, I stress I've only 6-8 mos of moderate
> R experience.)

You are right, for huge datasets.  For others, R is great, even for 
merging.  Many examples are provided in the Alzola and Harrell text on 
http://biostat.mc.vanderbilt.edu

> 5.  The challenge with R is, perhaps, it's very strength--language density.
> Once I learn how to do something in R vs. SAS, R's code is fractionally as
> large as SAS.  Literally, it may take 10 lines of code in SAS vs. a one
> liner in R.  That's powerful.  However, due to my SAS experience, I've
> banged out the SAS code and am still looking/hunting for the R equivalent.
> However, once doing so, it's, borrowing from a popular vegetable drink
> slogan, "Wow, I could have done that in R."

Yes I agree.

> 6.  And, lastly, while R is well documented, I seem to find one of the areas
> of documentation somewhat lacking is a great big R "recipe" book.
> (Suggestions, BTW, are welcome here.)  Documentation of the R language is in
> place with more being published, (alongside S-Plus), annually.  However,
> there does not appear to be, for example, an "R Transition Recipes for
> Experienced SAS Users" book.  That, ultimately, is what would help me, (I
> think.)  Again, the issue really is simply learning and using the language.
> Experienced R users, I'm convinced, could do everything R I'm doing in SAS,
> (with money left over for a few coffees at Starbuck's).

I agree.  What I really think is needed is a compendium of examples, 
especially for data manipulation.  I gave a talk about this last week; 
abstract is at 
http://biostat.mc.vanderbilt.edu/twiki/bin/view/Main/FrankHarrellrmanip 
with links to other places.  A meager attempt at navigating some of the 
more commonly used R functions is at 
http://biostat.mc.vanderbilt.edu/s/finder/finder.html

> 
> In conclusion, I still think that, given one's budget and projects, there's
> a place for SAS and R to co-exist.  But, that paradigm diminishes as (1) the
> size of the datasets become smaller and, (2) your problems are more
> academic/researchy/specific in nature.  For graphing, esp. w/the Lattice
> package, R is simply superior (IMHO), period, to SAS.  (For some reason, SAS
> has just not felt the need to improve their graphics, at least the SAS/Graph
> part of their offering.)  And, for the SAS lovers out there, this opinion is
> mine only as I continue to be primarily a SAS client attempting to
> transition to R.
> 
> Frank, while I've probably been too wordy, I've attempted to provide another
> perspective for you.  Good luck.

No, well said,

Thanks,

Frank

> 
> 
> Thanks,
> Charles
> 
> 
> ------------------------------
> 
> Message: 7
> Date: Sat, 19 Jun 2004 18:15:19 +0200 (MEST)
> From: "F.Kalder" <Kalderf at gmx.de>
> Subject: Re: [R] Another NEWBIE
> To: r-help at stat.math.ethz.ch
> Message-ID: <6411.1087661719 at www45.gmx.net>
> Content-Type: text/plain; charset="us-ascii"
> 
> Hi,
> 
> Thank you all who anwered me. 
> 
> I think, I mainly thought to understand the difference between SPSS /SAS and
> R, but didn't really get the point (what explains the question, wich metods
> R can't do). Maybe, because I don't have much experience with programming
> (near to none). My background in stats goes also only back to indroductory
> classes and an advanced course in multivariate statistics. To this, I'm
> working with Hair, Anderson, Tatham & Blacks's "Multivariate Data Analysis"
> (5th Ed.) as my ressource, mainly with questionnaire analysis (Reliability
> Analysis and Factor Analysis, also MDS, Conjoint etc. plus sometimes
> standard MANOVA, Multiplke Regression etc.). So, maybe my stats aren't
> sophisticated enough to use R, I'm just a standard user of applied
> statistical methods, not an academic researcher or even a statistician. It
> was mainly a descision by costs, because R is free software. 
> With the concept, I completely mistook the R concept as a programming
> environment more as a kind of advanced SPSS Syntax (because I also would
> call it "programming" when using it), which I now know, is completely wrong.
> 
> So, I again thank for your help.
> 
> 
> Cheers, Frank.
> 
> --



-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From ggrothendieck at myway.com  Sun Jun 20 20:31:31 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sun, 20 Jun 2004 14:31:31 -0400 (EDT)
Subject: [R] A way to list only variables or functions?
Message-ID: <20040620183131.4419539A0@mprdmxin.myway.com>



Here they are again modified to remove that bug:

ls.funs <- function(env=sys.frame(-1))unlist(lapply(ls(env=env),function(x)if(is.function(get(x,env=env)))x))

ls.var <- function(env=sys.frame(-1))unlist(lapply(ls(env=env),function(x)if(!is.function(get(x,env=env)))x))





Date:   Sun, 20 Jun 2004 17:02:40 +0100 (BST) 
From:   Prof Brian Ripley <ripley at stats.ox.ac.uk>
To:   Shin, Daehyok <sdhyok at email.unc.edu> 
Cc:   <r-help at stat.math.ethz.ch> 
Subject:   RE: [R] A way to list only variables or functions? 

 
On Sun, 20 Jun 2004, Shin, Daehyok wrote:

> Neat! Thanks.

Note that these are not correct, as the get is not done from the 
correct environment. The function ls.str I pointed you to is correct.

> How about incorporating this support into standard commands, ls() or
> objects()?

Well, there already is ls[f].str.

> Daehyok Shin (Peter)
> 
> > -----Original Message-----
> > From: Gabor Grothendieck [mailto:ggrothendieck at myway.COM]
> > Sent: Sunday, June 20, 2004 AM 10:06
> > To: sdhyok at email.unc.edu; r-help at stat.math.ethz.ch
> > Subject: RE: [R] A way to list only variables or functions?
> >
> >
> >
> >
> > These two functions will list the functions and variables
> > respectively:
> >
> > ls.funs <-
> > function(env=sys.frame(-1))unlist(lapply(ls(env=env),function(x)if
> > (is.function(get(x)))x))
> >
> > ls.vars <-
> > function(env=sys.frame(-1))unlist(lapply(ls(env=env),function(x)if
> > (!is.function(get(x)))x))
> >
> >
> > To use:
> >
> > ls.funs()
> > ls.vars()
> >
> >
> > Date: Sat, 19 Jun 2004 22:59:57 -0400
> > From: Shin <sdhyok at email.unc.edu>
> > To: R Help <r-help at stat.math.ethz.ch>
> > Subject: [R] A way to list only variables or functions?
> >
> >
> > I am curious if there is any way to list only variables or functions in
> > current environment, rather than listing all objects? Thanks.
> >
> > --
> > Daehyok Shin (Peter)
> > Geography Department
> > Univ. of North Carolina-Chapel Hill
> >
> >
> >
> > _______________________________________________
> > No banners. No pop-ups. No kidding.
> 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley, ripley at stats.ox.ac.uk
Professor of Applied Statistics, http://www.stats.ox.ac.uk/~ripley/
University of Oxford, Tel: +44 1865 272861 (self)
1 South Parks Road, +44 1865 272866 (PA)
Oxford OX1 3TG, UK Fax: +44 1865 272595



From chfrankl at wisc.edu  Sun Jun 20 20:57:59 2004
From: chfrankl at wisc.edu (Charles H. Franklin)
Date: Sun, 20 Jun 2004 13:57:59 -0500
Subject: [R] Sweave and echoing R comments
Message-ID: <40D5DE37.7090703@wisc.edu>

Is there any way to echo comments from an R source file into an 
SWeave->LaTeX document?

Example:

# Npop is population total
# Npoph0..Npoph2 are stratum totals
# Npoph is vector of stratum totals

Npop<-sum(to2000)

Npoph0<-sum(to2000[bg==0])
Npoph1<-sum(to2000[bg==1])
Npoph2<-sum(to2000[bg==2])

Npoph<-c(Npoph0,Npoph1,Npoph2)



In the final LaTeX document, I'd like the comments to be echoed so 
readers other than me have guidance about variable names etc.

I suppose advocates of literate programming might argue that if the 
comment is important it should be in the body of the .snw file, not 
merely as R comments. Still... it seems easier to use brief comments in 
the code itself and more explanatory text in the body of the document.

Searches of "Sweave" and "comment" in the list and documentation didn't 
turn up anything about echoing comments in either R or Sweave.


Thanks.

Charles

-- 

Charles H. Franklin
Professor, Political Science
University of Wisconsin, Madison
608-263-2022
franklin at polisci.wisc.edu
chfrankl at wisc.edu



From f_bouharaoui at yahoo.fr  Sun Jun 20 21:10:50 2004
From: f_bouharaoui at yahoo.fr (=?iso-8859-1?q?fatima=20bouharaoui?=)
Date: Sun, 20 Jun 2004 21:10:50 +0200 (CEST)
Subject: [R] problem locfit
Message-ID: <20040620191050.78026.qmail@web50405.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040620/175cf956/attachment.pl

From umalvarez at fata.unam.mx  Sun Jun 20 21:18:18 2004
From: umalvarez at fata.unam.mx (Ulises Mora Alvarez)
Date: Sun, 20 Jun 2004 14:18:18 -0500 (CDT)
Subject: [R] Html help does not work in Mac OSX 10.3.4
In-Reply-To: <BCF8A564.9F10%ealaca@ucdavis.edu>
Message-ID: <Pine.LNX.4.44.0406201416360.28906-100000@athena.fata.unam.mx>

Hello:

You are damn right! I haven't noticed because I rarely use the html help. 
I will be working on. I'll let you know.

Regards.




On Fri, 18 Jun 2004, Emilio A. Laca wrote:

> I recently upgraded from R 1.8 to 1.9. I removed 1.8 following the
> instructions. Html help has not worked since. When htmlhelp="TRUE" the
> help.start() command results in the "patience" message and nothing else
> happens. I am using mac osx 10.3.4. Help worked fine when I was using R 1.8.
> 
> I need help help ;-] Thanks!
> 

-- 
Ulises M. Alvarez
LAB. DE ONDAS DE CHOQUE
FISICA APLICADA Y TECNOLOGIA AVANZADA
UNAM
umalvarez at fata.unam.mx



From saurin_jani at yahoo.com  Sun Jun 20 22:15:12 2004
From: saurin_jani at yahoo.com (SAURIN)
Date: Sun, 20 Jun 2004 13:15:12 -0700 (PDT)
Subject: [R] regarding saving R graphis images directly to directory
Message-ID: <20040620201512.91404.qmail@web41114.mail.yahoo.com>

Dear R,

I am student at University of new haven, CT.I am trying to run my R scripts where I don't have
X11() authentication to my account. I run those R scripts and when I generate any graphics I get
error and it comes out from system. 

if possible , please let me know how can i run R scripts ...so, that I just SAVE BOX PLOT or
HISTOGRAM jpeg or png files to current directory without poping up on screen or without using any
devices or make them silent..or something.


Thakn you,
Saurin Jnai



From robin_gruna at hotmail.com  Sun Jun 20 22:34:24 2004
From: robin_gruna at hotmail.com (Robin Gruna)
Date: Sun, 20 Jun 2004 22:34:24 +0200
Subject: [R] Fw: Evaluating strings as variables
Message-ID: <BAY2-DAV6759GMFgJom0001059d@hotmail.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040620/5701a18f/attachment.pl

From p.dalgaard at biostat.ku.dk  Sun Jun 20 22:41:51 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 20 Jun 2004 22:41:51 +0200
Subject: [R] Fw: Evaluating strings as variables
In-Reply-To: <BAY2-DAV6759GMFgJom0001059d@hotmail.com>
References: <BAY2-DAV6759GMFgJom0001059d@hotmail.com>
Message-ID: <x2isdmdks0.fsf@biostat.ku.dk>

"Robin Gruna" <robin_gruna at hotmail.com> writes:

> I have the following problem: I have a list as follows,
> 
> > values <- list(red = 1, yellow = 2, blue = 3)
> 
> > values
> $red
> [1] 1
> 
> $yellow
> [1] 2
> 
> $blue
> [1] 3
> 
> There is also a vector containing the diffrent "colors" as character strings:
> 
> > colors <- c("red", "red", "blue", "yellow", "red", "blue")
> > colors
> [1] "red"    "red"    "blue"   "yellow" "red"    "blue"  
> 
> Now i can attach the list values to R:
> 
> > attach(values)
> > red
> [1] 1
> 
> etc...
> 
> Now to my problem: How can I make R in a simple way to evaluate the strings "red", "blue" etc. as variables, returning their numeric values ?
> As result I want to get a vector containing the values of the colors like this one:
> 
> > values.colors
> [1] 1 1 3 2 1 3

Forget about the attach() business, and do

values <- unlist(values) # or values <- c(red = 1, yellow = 2, blue = 3)
values.colors <- values[colors]

If you insist on going via variables, try

sapply(colors, get)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From deepayan at stat.wisc.edu  Sun Jun 20 23:06:10 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Sun, 20 Jun 2004 16:06:10 -0500
Subject: [R] regarding saving R graphis images directly to directory
In-Reply-To: <20040620201512.91404.qmail@web41114.mail.yahoo.com>
References: <20040620201512.91404.qmail@web41114.mail.yahoo.com>
Message-ID: <200406201606.10504.deepayan@stat.wisc.edu>

On Sunday 20 June 2004 15:15, SAURIN wrote:
> Dear R,
>
> I am student at University of new haven, CT.I am trying to run my R
> scripts where I don't have X11() authentication to my account. I run
> those R scripts and when I generate any graphics I get error and it
> comes out from system.
>
> if possible , please let me know how can i run R scripts ...so, that
> I just SAVE BOX PLOT or HISTOGRAM jpeg or png files to current
> directory without poping up on screen or without using any devices or
> make them silent..or something.

You could output to postscript (see ?postscript for details). This 
sounds like a UNIX/Linux system, so you could probably use 'convert' 
afterwards to get jpeg/png if you want.

You can start R with 'R -g none' to make postscript the default device.

Hope that helps,

Deepayan



From Kalderf at gmx.de  Sun Jun 20 23:05:54 2004
From: Kalderf at gmx.de (F.Kalder)
Date: Sun, 20 Jun 2004 23:05:54 +0200 (MEST)
Subject: [R] Another NEWBIE
Message-ID: <8626.1087765554@www19.gmx.net>

Hello,

And thanks again for your answers, perspectives and more...

So, as I understood, R can (nearly) do anything. So, also because it's free,
it is worth a try ;-).

I then next will start with reading some introductory texts. And, wow, I'm
quite 'overloaded', because there is so much stuff available, I don?t know
where to start and get a foot in the door. I think I take one of the advices
and will begin with the "Notes on the use of R for psychology experiments
and questionnaires" text. 

The hint to the Rcmdr package was nice :-). That was nearly like SPSS base
system. When at last to know, which package to use and for what kind of
problem is another thing of course ...

Still I have had problems with importing SPSS files. But I will read on it
too.

The factor analysis tool of Rcmdr I didn?t fully understand. So, there also
will be much work to do.

I also noticed that my stats abilities aren?t very profound. I?m so
'drilled' in doing 'standard stuff' and using more the SPSS output than
knowing exactly what I have to do, that I will have to have a much closer
look into a good book on stats ... well, the Hair et al. is a good book of
course, but any recommendations are welcome :-).

You all wrote about the graphics in R and those in Rcmdr I saw are to me
mostly the same as in SPSS. Nevertheless, I even don?t know when and for
what to use that whole bunch of graphics anyway ... still looking sometimes
a bit envy on huge 3D-graphics of multivariate bell curves and stuff on book
covers but can?t do anything with it.

The data entries by ASCII files are strange to me, because I?m so used to
work with a (the SPSS) spread sheet (mostly the good old typing in from
paper & pencil questionnaires), that I don?t know how to handle that yet.
Maybe using a SPSS- or at least Excel-like tool would be helpful for that.

So, my next move will be reading the mentioned text on questionnaires and
then some basic introduction on R, knowing then, how it principally works,
what?s with the packages, how to manage the data and so on ... I also think,
I have to do some homework on stats either ...

Thanks again ...

Frank


-- 
+++ Jetzt WLAN-Router fr alle DSL-Einsteiger und Wechsler +++
GMX DSL-Powertarife zudem 3 Monate gratis* http://www.gmx.net/dsl



From Matthias.Kohl at uni-bayreuth.de  Sun Jun 20 23:37:43 2004
From: Matthias.Kohl at uni-bayreuth.de (Matthias.Kohl@uni-bayreuth.de)
Date: Sun, 20 Jun 2004 23:37:43 +0200 (MEST)
Subject: [R] Fw: Evaluating strings as variables
In-Reply-To: <x2isdmdks0.fsf@biostat.ku.dk>
References: <BAY2-DAV6759GMFgJom0001059d@hotmail.com>
	<x2isdmdks0.fsf@biostat.ku.dk>
Message-ID: <1336.132.180.246.25.1087767463.squirrel@mail.uni-bayreuth.de>

> "Robin Gruna" <robin_gruna at hotmail.com> writes:
>
>> I have the following problem: I have a list as follows,
>>
>> > values <- list(red = 1, yellow = 2, blue = 3)
>>
>> > values
>> $red
>> [1] 1
>>
>> $yellow
>> [1] 2
>>
>> $blue
>> [1] 3
>>
>> There is also a vector containing the diffrent "colors" as character
>> strings:
>>
>> > colors <- c("red", "red", "blue", "yellow", "red", "blue")
>> > colors
>> [1] "red"    "red"    "blue"   "yellow" "red"    "blue"
>>
>> Now i can attach the list values to R:
>>
>> > attach(values)
>> > red
>> [1] 1
>>
>> etc...
>>
>> Now to my problem: How can I make R in a simple way to evaluate the
>> strings "red", "blue" etc. as variables, returning their numeric
>> values ? As result I want to get a vector containing the values of the
>> colors like this one:
>>
>> > values.colors
>> [1] 1 1 3 2 1 3
>
> Forget about the attach() business, and do
>
> values <- unlist(values) # or values <- c(red = 1, yellow = 2, blue = 3)
> values.colors <- values[colors]
>
> If you insist on going via variables, try
>
> sapply(colors, get)
>

or?
unlist(mget(colors, envir = as.environment(-1), inherits = TRUE))

Matthias

> --
>    O__  ---- Peter Dalgaard             Blegdamsvej 3
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From f.harrell at vanderbilt.edu  Sun Jun 20 22:34:50 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sun, 20 Jun 2004 20:34:50 +0000
Subject: [R] Sweave and echoing R comments
In-Reply-To: <40D5DE37.7090703@wisc.edu>
References: <40D5DE37.7090703@wisc.edu>
Message-ID: <40D5F4EA.3050608@vanderbilt.edu>

Charles H. Franklin wrote:
> Is there any way to echo comments from an R source file into an 
> SWeave->LaTeX document?
> 
> Example:
> 
> # Npop is population total
> # Npoph0..Npoph2 are stratum totals
> # Npoph is vector of stratum totals
> 
> Npop<-sum(to2000)
> 
> Npoph0<-sum(to2000[bg==0])
> Npoph1<-sum(to2000[bg==1])
> Npoph2<-sum(to2000[bg==2])
> 
> Npoph<-c(Npoph0,Npoph1,Npoph2)
> 
> 
> 
> In the final LaTeX document, I'd like the comments to be echoed so 
> readers other than me have guidance about variable names etc.
> 
> I suppose advocates of literate programming might argue that if the 
> comment is important it should be in the body of the .snw file, not 
> merely as R comments. Still... it seems easier to use brief comments in 
> the code itself and more explanatory text in the body of the document.
> 
> Searches of "Sweave" and "comment" in the list and documentation didn't 
> turn up anything about echoing comments in either R or Sweave.
> 
> 
> Thanks.
> 
> Charles
> 


I agree that it would be nice to see comments in the output.  I hope 
that Fritz Leisch will consider adding a hook that is a user-provided R 
function that take vectors of character strings representing a chunk of 
code and generating any desired LaTeX markup for pretty printing of the 
code.  The function would return character vectors in LaTeX format.  I 
like comments to appear in a smaller font, and for certain character 
translations, such as <- being replaced with a left arrow.

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From jfox at mcmaster.ca  Mon Jun 21 00:20:30 2004
From: jfox at mcmaster.ca (John Fox)
Date: Sun, 20 Jun 2004 18:20:30 -0400
Subject: [R] Another NEWBIE
In-Reply-To: <8626.1087765554@www19.gmx.net>
Message-ID: <20040620222030.ZTRQ1984.tomts5-srv.bellnexxia.net@JohnDesktop8300>

Dear Frank,

First, thank you for your kind remarks about the Rcmdr package.

Please note that the Rcmdr package is meant to be a basic statistics GUI for
R, to be used, for example, in an introductory statistics course. It covers
only a very small fraction of what's available in R.

Almost all of the statistical capabilities of the Rcmdr package are from
other packages. In particular, since you mention it, the factor-analysis
dialog simply calls the factanal() function in the stats package, which is
part of the standard R distribution. See ?factanal (or press the Help button
in the Rcmdr factor-analysis dialog) for details. As well, you can take a
look at the R commands that the Rcmdr generates.

Regards,
 John

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of F.Kalder
> Sent: Sunday, June 20, 2004 4:06 PM
> To: r-help at stat.math.ethz.ch
> Subject: Re: [R] Another NEWBIE
> 
> Hello,
> 
> And thanks again for your answers, perspectives and more...
> 
> So, as I understood, R can (nearly) do anything. So, also 
> because it's free, it is worth a try ;-).
> 
> I then next will start with reading some introductory texts. 
> And, wow, I'm quite 'overloaded', because there is so much 
> stuff available, I don?t know where to start and get a foot 
> in the door. I think I take one of the advices and will begin 
> with the "Notes on the use of R for psychology experiments 
> and questionnaires" text. 
> 
> The hint to the Rcmdr package was nice :-). That was nearly 
> like SPSS base system. When at last to know, which package to 
> use and for what kind of problem is another thing of course ...
> 
> Still I have had problems with importing SPSS files. But I 
> will read on it too.
> 
> The factor analysis tool of Rcmdr I didn?t fully understand. 
> So, there also will be much work to do.
> 
> I also noticed that my stats abilities aren?t very profound. 
> I?m so 'drilled' in doing 'standard stuff' and using more the 
> SPSS output than knowing exactly what I have to do, that I 
> will have to have a much closer look into a good book on 
> stats ... well, the Hair et al. is a good book of course, but 
> any recommendations are welcome :-).
> 
> You all wrote about the graphics in R and those in Rcmdr I 
> saw are to me mostly the same as in SPSS. Nevertheless, I 
> even don?t know when and for what to use that whole bunch of 
> graphics anyway ... still looking sometimes a bit envy on 
> huge 3D-graphics of multivariate bell curves and stuff on 
> book covers but can?t do anything with it.
> 
> The data entries by ASCII files are strange to me, because 
> I?m so used to work with a (the SPSS) spread sheet (mostly 
> the good old typing in from paper & pencil questionnaires), 
> that I don?t know how to handle that yet.
> Maybe using a SPSS- or at least Excel-like tool would be 
> helpful for that.
> 
> So, my next move will be reading the mentioned text on 
> questionnaires and then some basic introduction on R, knowing 
> then, how it principally works, what?s with the packages, how 
> to manage the data and so on ... I also think, I have to do 
> some homework on stats either ...
> 
> Thanks again ...
> 
> Frank
>



From zchen at cceb.upenn.edu  Mon Jun 21 00:32:46 2004
From: zchen at cceb.upenn.edu (Zhen Chen)
Date: Sun, 20 Jun 2004 18:32:46 -0400
Subject: [R] Greater than 1 or less than 1?
Message-ID: <C0C00B6F-C309-11D8-9181-000A95CD7524@cceb.upenn.edu>

I have problem evaluating the expression h = exp(x)/(exp(exp(x))-1) for 
large negative x. This expression is actually the probability
that y = 1 when y is a Poisson random variable truncated at 0, hence 
must satisfy 0 <= h <= 1. However, when
x < -18, I may get an h value that is  larger than 1 while the true 
value should be a quantity that is smaller than but very close to 1.
For example when x = -19, h = 1.00000000031174. I tried to use 
different form of h and none of
them give me an h value that is less than or equal to 1. I also tried 
to find patterns, but discovered none: for some x < -18, h is below 1; 
for others
h is greater than 1. Is there any trick that enables me to obtain 
theoretically correct results from this expression?

Thanks.

Zhen Chen, Ph.D.
Assistant Professor of Biostatistics
Department of Biostatistics & Epidemiology
University of Pennsylvania School of Medicine
423 Guardian Drive -- 625 Blockley Hall
Philadelphia, PA  19104-6021

voice:  215-573-8545
fax:    215-573-4865
email:  zchen at cceb.upenn.edu



From p.dalgaard at biostat.ku.dk  Mon Jun 21 00:52:51 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 21 Jun 2004 00:52:51 +0200
Subject: [R] Fw: Evaluating strings as variables
In-Reply-To: <1336.132.180.246.25.1087767463.squirrel@mail.uni-bayreuth.de>
References: <BAY2-DAV6759GMFgJom0001059d@hotmail.com>
	<x2isdmdks0.fsf@biostat.ku.dk>
	<1336.132.180.246.25.1087767463.squirrel@mail.uni-bayreuth.de>
Message-ID: <x2acyxeta4.fsf@biostat.ku.dk>

Matthias.Kohl at uni-bayreuth.de writes:

> > sapply(colors, get)
> >
> 
> or?
> unlist(mget(colors, envir = as.environment(-1), inherits = TRUE))

Ah, yes, forgot about mget. Doesn't seem to make things much cleaner
though... (although get() also needs a bit more care about
environments). 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Mon Jun 21 01:13:07 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 21 Jun 2004 01:13:07 +0200
Subject: [R] Greater than 1 or less than 1?
In-Reply-To: <C0C00B6F-C309-11D8-9181-000A95CD7524@cceb.upenn.edu>
References: <C0C00B6F-C309-11D8-9181-000A95CD7524@cceb.upenn.edu>
Message-ID: <x2659lescc.fsf@biostat.ku.dk>

"Zhen Chen" <zchen at cceb.upenn.edu> writes:

> I have problem evaluating the expression h = exp(x)/(exp(exp(x))-1)
> for large negative x. This expression is actually the probability
> that y = 1 when y is a Poisson random variable truncated at 0, hence
> must satisfy 0 <= h <= 1. However, when
> x < -18, I may get an h value that is  larger than 1 while the true
> value should be a quantity that is smaller than but very close to 1.
> For example when x = -19, h = 1.00000000031174. I tried to use
> different form of h and none of
> them give me an h value that is less than or equal to 1. I also tried
> to find patterns, but discovered none: for some x < -18, h is below 1;
> for others
> h is greater than 1. Is there any trick that enables me to obtain
> theoretically correct results from this expression?

expm1() should help...

What platform is this? I don't see it until x=-30:

> exp(-30)
[1] 9.357623e-14
> exp(exp(-30))-1
[1] 9.348078e-14

which is of course "wrong" since exp(x) > x + 1 mathematically.
However, notice that in double precision calculations the latter value
is only accurate up to 1e-15 or so. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From tlumley at u.washington.edu  Mon Jun 21 01:16:57 2004
From: tlumley at u.washington.edu (Thomas Lumley)
Date: Sun, 20 Jun 2004 16:16:57 -0700 (PDT)
Subject: [R] Greater than 1 or less than 1?
In-Reply-To: <C0C00B6F-C309-11D8-9181-000A95CD7524@cceb.upenn.edu>
References: <C0C00B6F-C309-11D8-9181-000A95CD7524@cceb.upenn.edu>
Message-ID: <Pine.A41.4.58.0406201606570.11050@homer04.u.washington.edu>

On Sun, 20 Jun 2004, Zhen Chen wrote:

> I have problem evaluating the expression h = exp(x)/(exp(exp(x))-1) for
> large negative x. This expression is actually the probability
> that y = 1 when y is a Poisson random variable truncated at 0, hence
> must satisfy 0 <= h <= 1. However, when


You would be better off using the functions provided for the Poisson
distribution.

Either
dpois(1,exp(x))/ppois(0.5,exp(x),lower.tail=FALSE)
or to get 1-h
ppois(1.5,exp(x),lower.tail=FALSE)/ppois(0.5,exp(x),lower.tail=FALSE)

	-thomas



From ray at mcs.vuw.ac.nz  Mon Jun 21 01:24:56 2004
From: ray at mcs.vuw.ac.nz (Ray Brownrigg)
Date: Mon, 21 Jun 2004 11:24:56 +1200 (NZST)
Subject: [R] Greater than 1 or less than 1?
Message-ID: <200406202324.i5KNOuLI001516@tahi.mcs.vuw.ac.nz>

> Date: Sun, 20 Jun 2004 18:32:46 -0400
> From: "Zhen Chen" <zchen at cceb.upenn.edu>
> 
> I have problem evaluating the expression h = exp(x)/(exp(exp(x))-1) for 
> large negative x. This expression is actually the probability
> that y = 1 when y is a Poisson random variable truncated at 0, hence 
> must satisfy 0 <= h <= 1. However, when
> x < -18, I may get an h value that is  larger than 1 while the true 
> value should be a quantity that is smaller than but very close to 1.
> For example when x = -19, h = 1.00000000031174. I tried to use 
> different form of h and none of
> them give me an h value that is less than or equal to 1. I also tried 
> to find patterns, but discovered none: for some x < -18, h is below 1; 
> for others
> h is greater than 1. Is there any trick that enables me to obtain 
> theoretically correct results from this expression?
> 
exp(x)/(exp(exp(x)) - 1) is less than 1 for me for x = -18 with R-1.9.0
on a Solaris 9 system, a NetBSD 2.0C system and a Windows XP system.

You could try exp(x)/expm1(exp(x)), but if the longer expression doesn't
work, then this may not either, though it does give slightly different
results on my systems.

E.g.:
> x = -19
> exp(x)/(exp(exp(x)) - 1) - 1
[1] -2.047911e-09
> exp(x)/(expm1(exp(x))) - 1
[1] -2.801398e-09

log(1 + x) and exp(x) - 1 are often treated specially in math libraries
for when x << 0.

Ray Brownrigg



From I.Visser at uva.nl  Mon Jun 21 06:43:35 2004
From: I.Visser at uva.nl (Ingmar Visser)
Date: Mon, 21 Jun 2004 06:43:35 +0200
Subject: [R] hidden markov models in R?
Message-ID: <3FA98898AEAA88478AD68CEE89A68C483BEDC6@rea04.fmg.uva.nl>

Hi Cezar, 

Download and install the repeated package by Jim Lindsey and type ?hidden

bye, ingmar

 

Hi, friends!

Has R estimation (library, for example) to do estimation in HMM?

Thanks in advance,

========================================
Cezar Freitas
Estatistico - Comissao Permanente para os Vestibulares / UNICAMP
Probabilidade e Estatistica Aplicadas - IME / USP | IMECC / UNICAMP
Campinas | Sao Paulo, SP - Brasil

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From imosqueira at suk.azti.es  Mon Jun 21 09:35:34 2004
From: imosqueira at suk.azti.es (Iago Mosqueira)
Date: Mon, 21 Jun 2004 08:35:34 +0100
Subject: [R] Cross build Makefile
Message-ID: <1087803333.2858.15.camel@xurelo>

Hello,

I am trying to use Yan and Rossini's Makefile for cross building Windows
versions of R packages in Linux with R 1.9.0. When compiling R with the
mingw tools I get an error about expm1 being undeclared when first found
at src/main/arithmetic.c:1019

If I fiddle a bit with it later on I also get errors about log1p bein
undeclared.

Any idea what should I look for?

I am using R 1.9.0 in Debian, with R-mathlib avaliable, and gcc 3.3.

Thanks,


iago



From ripley at stats.ox.ac.uk  Mon Jun 21 09:00:48 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 21 Jun 2004 08:00:48 +0100 (BST)
Subject: [R] Cross build Makefile
In-Reply-To: <1087803333.2858.15.camel@xurelo>
Message-ID: <Pine.LNX.4.44.0406210742320.31373-100000@gannet.stats>

On Mon, 21 Jun 2004, Iago Mosqueira wrote:

> Hello,
> 
> I am trying to use Yan and Rossini's Makefile for cross building Windows
> versions of R packages in Linux with R 1.9.0. When compiling R with the
> mingw tools I get an error about expm1 being undeclared when first found
> at src/main/arithmetic.c:1019
> 
> If I fiddle a bit with it later on I also get errors about log1p bein
> undeclared.
> 
> Any idea what should I look for?
> 
> I am using R 1.9.0 in Debian, with R-mathlib avaliable, and gcc 3.3.

Did you build your own cross-compiler, or where did you get it from?

log1p is definitely declared in math.h these days, but it used not to be, 
so that one might be due to using too old a cross-compiler.

For expm1, the Windows config.h has /* #undef HAVE_EXPM1 */ which means it 
is declared in Rmath.h and compiled up as part of libnmath.a.  Here all I 
can suggest is that you check the headers files are correct and that you 
are finding the ones for cross-compiling and not for Linux compiling 
(which can be a problem if the cross-compiler was built incorrectly or you 
configured R in that source tree without the right options).

BTW, I think this topic might be more appropriate on R-devel, as most 
readers of R-help will not know what `cross build' is.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Friedrich.Leisch at ci.tuwien.ac.at  Mon Jun 21 09:28:17 2004
From: Friedrich.Leisch at ci.tuwien.ac.at (Friedrich.Leisch@ci.tuwien.ac.at)
Date: Mon, 21 Jun 2004 09:28:17 +0200
Subject: [R] Sweave and echoing R comments
In-Reply-To: <40D5F4EA.3050608@vanderbilt.edu>
References: <40D5DE37.7090703@wisc.edu>
	<40D5F4EA.3050608@vanderbilt.edu>
Message-ID: <16598.36369.678082.232730@galadriel.ci.tuwien.ac.at>

>>>>> On Sun, 20 Jun 2004 20:34:50 +0000,
>>>>> Frank E Harrell (FEH) wrote:

  > Charles H. Franklin wrote:
  >> Is there any way to echo comments from an R source file into an 
  SWeave-> LaTeX document?
  >> 
  >> Example:
  >> 
  >> # Npop is population total
  >> # Npoph0..Npoph2 are stratum totals
  >> # Npoph is vector of stratum totals
  >> 
  >> Npop<-sum(to2000)
  >> 
  >> Npoph0<-sum(to2000[bg==0])
  >> Npoph1<-sum(to2000[bg==1])
  >> Npoph2<-sum(to2000[bg==2])
  >> 
  >> Npoph<-c(Npoph0,Npoph1,Npoph2)
  >> 
  >> 
  >> 
  >> In the final LaTeX document, I'd like the comments to be echoed so 
  >> readers other than me have guidance about variable names etc.
  >> 
  >> I suppose advocates of literate programming might argue that if the 
  >> comment is important it should be in the body of the .snw file, not 
  >> merely as R comments. Still... it seems easier to use brief comments in 
  >> the code itself and more explanatory text in the body of the document.
  >> 
  >> Searches of "Sweave" and "comment" in the list and documentation didn't 
  >> turn up anything about echoing comments in either R or Sweave.
  >> 
  >> 
  >> Thanks.
  >> 
  >> Charles
  >> 


  > I agree that it would be nice to see comments in the output.  I hope 
  > that Fritz Leisch will consider adding a hook that is a user-provided R 
  > function that take vectors of character strings representing a chunk of 
  > code and generating any desired LaTeX markup for pretty printing of the 
  > code.  The function would return character vectors in LaTeX format.  I 
  > like comments to appear in a smaller font, and for certain character 
  > translations, such as <- being replaced with a left arrow.

I agree that it would be nice, and it is on my list of things I want
to do over the summer ...

Best,

-- 
-------------------------------------------------------------------
                        Friedrich Leisch 
Institut f??r Statistik                     Tel: (+43 1) 58801 10715
Technische Universit??t Wien                Fax: (+43 1) 58801 10798
Wiedner Hauptstra??e 8-10/1071
A-1040 Wien, Austria             http://www.ci.tuwien.ac.at/~leisch



From angel_lul at hotmail.com  Mon Jun 21 10:26:57 2004
From: angel_lul at hotmail.com (Angel Lopez)
Date: Mon, 21 Jun 2004 10:26:57 +0200
Subject: [R] sunrise, sunset calculation
Message-ID: <40D69BD1.3000804@hotmail.com>

Are there any functions available to calculate sunrise and sunset times 
for given latitude,longitude and dates?
If not, I'll appreciatte any pointers to C code I could use/port.
Thanks,
Angel



From mdsumner at utas.edu.au  Mon Jun 21 11:04:00 2004
From: mdsumner at utas.edu.au (Mike Sumner)
Date: Mon, 21 Jun 2004 19:04:00 +1000
Subject: [R] sunrise, sunset calculation
In-Reply-To: <40D69BD1.3000804@hotmail.com>
References: <40D69BD1.3000804@hotmail.com>
Message-ID: <6.0.0.22.1.20040621185538.02141548@postoffice.utas.edu.au>

At 06:26 PM 6/21/2004, Angel Lopez wrote:

>Are there any functions available to calculate sunrise and sunset times 
>for given latitude,longitude and dates?
>If not, I'll appreciatte any pointers to C code I could use/port.
>Thanks,
>Angel


Hello, I have R code for determining solar elevation for given lat,lon,time 
- they were translated from the Javascript on these pages.

  http://www.srrb.noaa.gov/highlights/sunrise/sunrise.html
  http://www.srrb.noaa.gov/highlights/sunrise/azel.html

I use them in a fairly specialized way, but I can dig it out and probably 
help you get started.

Cheers, Mike.




###############################################

Michael Sumner - PhD. candidate
Maths and Physics (ACE CRC & IASOS) and Zoology (AWRU)
University of Tasmania
Private Bag 77, Hobart, Tas 7001, Australia
Phone: 6226 1752


Overall, SAS is about 11 years behind R and S-Plus in statistical 
capabilities (last year it was
about 10 years behind) in my estimation.
    -- Frank Harrell (SAS User, 1969-1991)
       R-help (September 2003)



From maechler at stat.math.ethz.ch  Mon Jun 21 11:58:06 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 21 Jun 2004 11:58:06 +0200
Subject: [R] sunrise, sunset calculation
In-Reply-To: <6.0.0.22.1.20040621185538.02141548@postoffice.utas.edu.au>
References: <40D69BD1.3000804@hotmail.com>
	<6.0.0.22.1.20040621185538.02141548@postoffice.utas.edu.au>
Message-ID: <16598.45358.190016.665492@gargle.gargle.HOWL>

>>>>> "Mike" == Mike Sumner <mdsumner at utas.edu.au>
>>>>>     on Mon, 21 Jun 2004 19:04:00 +1000 writes:

    Mike> At 06:26 PM 6/21/2004, Angel Lopez wrote:
    >> Are there any functions available to calculate sunrise and sunset times 
    >> for given latitude,longitude and dates?
    >> If not, I'll appreciatte any pointers to C code I could use/port.
    >> Thanks,
    >> Angel


    Mike> Hello, I have R code for determining solar elevation
    Mike> for given lat,lon,time - they were translated from the
    Mike> Javascript on these pages.

    Mike> http://www.srrb.noaa.gov/highlights/sunrise/sunrise.html
    Mike> http://www.srrb.noaa.gov/highlights/sunrise/azel.html

    Mike> I use them in a fairly specialized way, but I can dig
    Mike> it out and probably help you get started.

that might be nice to have in an R package.
sunrise/sunset would then be the zeros [ "uniroot()" ] of the
sun elevations?

Note that ESS (and other Emacs) users have it at least at their
finger tips:  
       M-x sunrise-sunset
(and there's lunar phases, holidays, dairy etc; only partly also available from 
 [Tools] -> [Display Calendar] -> {{new menu entries when inside calendar}}

So, you can also find the emacs-lisp code (in
..../lisp/calendar/solar.el but only when you've got a "source"
installation of emacs).

Martin Maechler



From adrian at maths.uwa.edu.au  Mon Jun 21 12:14:03 2004
From: adrian at maths.uwa.edu.au (Adrian Baddeley)
Date: Mon, 21 Jun 2004 18:14:03 +0800
Subject: [R] modifying a glm object
Message-ID: <16598.46315.199482.8448@maths.uwa.edu.au>


I need to modify a glm object directly, by assigning new values
to the fitted parameters.

(i.e. the canonical parameters stored in object$coefficients 
and returned by 'coef').

Is there a "safe" way to do this, e.g. so that 'residuals' and 'predict' 
give the correct results for the new model?

thanks
Adrian Baddeley



From angel_lul at hotmail.com  Mon Jun 21 13:09:55 2004
From: angel_lul at hotmail.com (Angel Lopez)
Date: Mon, 21 Jun 2004 13:09:55 +0200
Subject: [R] sunrise, sunset calculation
Message-ID: <40D6C203.80301@hotmail.com>

Martin Maechler wrote:

 >>>>>> "Mike" == Mike Sumner <mdsumner at utas.edu.au>
 >>>>>>    on Mon, 21 Jun 2004 19:04:00 +1000 writes:
 >
 >
 >
 >     Mike> At 06:26 PM 6/21/2004, Angel Lopez wrote:
 >     >> Are there any functions available to calculate sunrise and 
sunset times     >> for given latitude,longitude and dates?
 >     >> If not, I'll appreciatte any pointers to C code I could use/port.
 >     >> Thanks,
 >     >> Angel
 >
 >
 >     Mike> Hello, I have R code for determining solar elevation
 >     Mike> for given lat,lon,time - they were translated from the
 >     Mike> Javascript on these pages.
 >
 >     Mike> http://www.srrb.noaa.gov/highlights/sunrise/sunrise.html
 >     Mike> http://www.srrb.noaa.gov/highlights/sunrise/azel.html
 >
 >     Mike> I use them in a fairly specialized way, but I can dig
 >     Mike> it out and probably help you get started.
 >
 > that might be nice to have in an R package.
 > sunrise/sunset would then be the zeros [ "uniroot()" ] of the
 > sun elevations?
 >

Well, not always ;-) , it depends on how you define sunrise/sunset, 
there are various definitions (astrological, nautical, civil).
The most complete code I know of (also pointed to me by another user in 
private) is sunriset.c. It would be quite easy to use this C code and 
write a wrapper functions in R. Because it was me who asked first I'll 
try to do it.
However using noaa algorithms might be better, the sunriset.c gives 
slight differences to noaa (and to emacs sunrise-sunset), I don't know 
which one is better but it might be better to say that R code 'uses noaa 
  algorithms'. Any other pointers welcomed.
Angel


 > Note that ESS (and other Emacs) users have it at least at their
 > finger tips:         M-x sunrise-sunset
 > (and there's lunar phases, holidays, dairy etc; only partly also 
available from  [Tools] -> [Display Calendar] -> {{new menu entries when 
inside calendar}}
 >
 > So, you can also find the emacs-lisp code (in
 > ..../lisp/calendar/solar.el but only when you've got a "source"
 > installation of emacs).
 >
 > Martin Maechler
 >
 > .
 >



From p.dalgaard at biostat.ku.dk  Mon Jun 21 13:38:51 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 21 Jun 2004 13:38:51 +0200
Subject: [R] R 1.9.1 is released
Message-ID: <x21xk987jo.fsf@biostat.ku.dk>


I've rolled up R-1.9.1.tgz a short while ago. This is a maintenance
version mainly to fix a number of minor bugs and issues (the most
annoying one seems to have been the change to barplots of tables) and
some installation issues.

Because of the relocation the CVS archives, there is no direct access
to the files until they show up on the CRAN master site. You can then
get it from

http://cran.r-project.org/src/base/R-1.9.1.tgz

or wait for it to be mirrored at a CRAN site nearer to you. Binaries
for various platforms will appear in due course.
 
There is also a version split for floppies.

These are the md5sums for the freshly created files, in case you wish
to check that they are uncorrupted:

c8201425506e5c077ef1936e19ea2f51  R-1.9.1.tgz
7683777a649e099d9c278ebc58fcbc56  R-1.9.1.tgz-split.aa
c0b8dba896ab7cf1fd2e597f400bfe07  R-1.9.1.tgz-split.ab
ded2cc8335f338f80cf5380d6394f7ab  R-1.9.1.tgz-split.ac
1d35502fdf6876e71e5218603a47394d  R-1.9.1.tgz-split.ad
51a7cfa9effcd9e61fbb44ee78a86de9  R-1.9.1.tgz-split.ae
5419f461683e8c98cd8cfaa308df0b40  R-1.9.1.tgz-split.af
183e16b38224b3c6c39b400aaa6172a0  R-1.9.1.tgz-split.ag


        For the R Core Team
        Peter Dalgaard



Here's the relevant bit of the NEWS file:


		CHANGES IN R VERSION 1.9.1


NEW FEATURES

    o	as.Date() now has a method for "POSIXlt" objects.

    o	mean() has a method for "difftime" objects and so summary()
	works for such objects.

    o	legend() has a new argument 'pt.cex'.

    o	plot.ts() has more arguments, particularly 'yax.flip'.

    o	heatmap() has a new 'keep.dendro' argument.

    o	The default barplot method now handles vectors and 1-d arrays
	(e.g., obtained by table()) the same, and uses grey instead of
	heat color palettes in these cases.  (Also fixes PR#6776.)

    o	nls() now looks for variables and functions in its formula in
	the environment of the formula before the search path, in the
	same way lm() etc look for variables in their formulae.


INSTALLATION ISSUES

    o	src/modules/X11/dataentry.c would not build on some XFree
	4.4.0 systems.	(This is a bug in their header files but we have
	added a workaround.)

    o	Building with gcc/g77 3.4.0 on ix86 platforms failed to produce
	a working build: the critical LAPACK routines are now compiled
	with -ffloat-store.

    o	Added patches to enable 64-bit builds on AIX 5.1: see the R-admin
	manual for details.

    o	Added some patches to allow non-IEEE-754 installations to work
	reasonably well.  (Infs and NAs are still not handled properly
	in complex arithmetic and functions such as sin().  See also
	Deprecated, as support for non-IEEE-754 installations is about
	to be removed.)

    o	Installation will now work in Estonian (et_EE*) locales, which
	sort z before u.  (PR#6958)


DEPRECATED & DEFUNCT

    o	Support for non-IEEE-754 arithmetic (which has been untested
	for some time) will be removed in the next full release.

    o	Direct use of R INSTALL|REMOVE|BATCH|COMPILE|SHLIB is
	deprecated: use R CMD instead.

    o	The gnome/GNOME graphics device is deprecated and will be
	removed in the next full release.


BUG FIXES

    o	pbinom(q, N, prob) is now more accurate when prob is close to 0.
	(PR#6757)

    o	pcauchy(x, .., log.p) is now more accurate for large x,
	particularly when log.p = TRUE. (PR#6756)

    o	pgeom(q, prob, lower.tail, log.p) is now (sometimes much) more
	accurate when prob is very small. (PR#6792)

	The code for pgeom(prob=1) assumed IEEE 754 arithmetic, and
	gave NaNs under gcc 3.4.0 -fPIC, for example.

    o	makeARIMA() was not handling an ARMA(0, 0) model correctly.

    o	as.Date() was failing on factors.  (PR#6779)

    o	min(), max() and range() were failing on "difftime" objects.

    o	as.data.frame.list() could fail on some unusual list names.
	(PR#6782)

    o	type.convert() ignored na.strings when no conversion was done.
	(PR#6781, not needed for its primary use in read.table.)

    o	Fixed a clipping problem in the quartz() device.

    o	Subsetting a factor swapped the order of the attributes, which
	identical() cares about.  (PR#6799)

    o	The L-BFGS-B option of optim() apparently needs part of its
	workspace zeroed.  (PR#6720)

    o	extractAIC.survreg() needed updating.

    o	When using the header Rmath.h in standalone mode, the case where
	TRUE, FALSE are already defined is now handled correctly.

    o	Package utils now exports several functions that are needed for
	writing Sweave drivers.

    o	Comparison of two lists/expressions was giving nonsensical
	(and often random) answers, and is now an error.

    o	The C-level function ncols was returning a random answer
	(often 0) for a 1D array.  This caused model.matrix to
	misbehave (perhaps segfault) if a term was a 1D array.	(PR#6838)

    o	The configure script now finds the pdf viewers ggv and gpdf.

    o	Workaround for the problems strptime on MacOS X has with dates
	before 1900.

    o	'R CMD build' works in a directory whose path contains spaces.
	(PR#6830 under Unix/Linux: it already worked under Windows.)
	Also 'R CMD check'.

    o	mosaicplot() stops cleanly if given a table containing missing values.

    o	install.packages() from a local CRAN was broken.

    o	bxp() fixed for e.g.,  boxplot(..., border=2:4)

    o	approx(list(x=rep(NaN,9), y=1:9), xout=NaN) does not seg.fault
	anymore (PR#6809).

    o	plot(1, pch=NA) does not give an error anymore and
	plot(1:2, pch=c("o",NA)) only prints one symbol	 (PR#6876).

    o	diffinv(matrix(3, 7,0)) now works.

    o	plot.ts(z) for multivariate 'z' now properly draws all 'nc' xlab`s
	when nc > 1 and obeys 'ann=FALSE' or 'axes=FALSE'.

    o	aggregate(.data.frame) failed if the answer would have had one
	row.

    o	recordPlot() and replayPlot() failed to duplicate the display
	list, so further plotting altered the saved or replayed object.

    o	Assignments of the form adf[i,j] <- value now accept a
	data-frame value as well as a list value.

    o	dir.create() sometimes erroneously continued to report a directory
	already existed after the first instance.  (PR#6892)

    o	arima.sim() allows a null model.

    o	which.min() & which.max()'s C code now PROTECT()'s its result.

    o	Building standalone nmath did not support some of the DEBUG options.

    o   mle() got confused if start value list was not in same order as
        arguments of likelihood function (reported by Ben Bolker)

    o	backsolve(r, x, k) now allows k < nrow(x) - as its documentation
	always claimed.

    o	update.packages("mgcv") and old.packages(*) now give a better error
	message; and installed.packages("mgcv") properly returns <empty>.

    o	stats:::as.dendrogram.hclust() is documented and no longer re-sorts
	the two children at each node.	This fixes as.dendrogram(hh) for
	the case where hh is a "reordered" hclust object.

	plot.dendrogram(x) now draws leaves 'x' more sensibly.

	reorder.dendrogram() now results in a dendrogram with correct
	"midpoint"s, and hence reordered dendrograms are plotted correctly.

	stats:::midcache.dendrogram() and hence the reorder() and rev()
	dendrogram methods do not return bloated dendrograms.

    o	heatmap(*, labRow=., labCol=.) now also reorders the labels when
	specified---not only when using default labels.

    o   Copying lattice (grid) output to another device now works again
        (There were intermittent problems in 1.9.0 - PR#6915, #6947/8.)

    o	hist() uses a more robust choice of its 'diddle' factor, used
	to detect if an observation is on a bin boundary.  (PR#6931)

    o	jitter(x) now returns x when length(x) == 0.

    o	Under some rare circumstances the locale-specific tables used by
	the perl=TRUE option to grep() etc were being corrupted and so
	matches were missed.

    o	qbinom(*, prob = 0, lower.tail = FALSE) now properly gives 0.
	(PR#6972)

    o	Class "octmode" needed a "[" method to preserve the class: see
	example(file.info) for an example.


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907

_______________________________________________
R-announce at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-announce



From petr.pikal at precheza.cz  Mon Jun 21 14:27:20 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 21 Jun 2004 14:27:20 +0200
Subject: [R] [Q] Newbie (continued.. at least I got R running allready :-)
In-Reply-To: <20040618123425.F06BA39C8@mprdmxin.myway.com>
Message-ID: <40D6F048.6.F9E7C4@localhost>

Hi

On 18 Jun 2004 at 8:34, Gabor Grothendieck wrote:

> 
> 
> Check out ?turnpoints in the pastecs library.  
> 
> Try 
> 
>    example(turnpoints)
>    str(Nauplii.tp)
> 
> to see the data structure you get back.  It identifies the peaks
> and pits and other associated information.

I am not sure if this is exactly what Jeroen really wants. As I went 
through the picture (BTW it could be better if it had been slightly 
smaller:), it seems to me that the "peaks" are very flat. If you look 
at them they are like that

                     *****************************
        *******                                                        **
  ***                                                                          * 
 *							       *
*                                                                                   *
  ^    ^            ^   
  1    2            3 

and he/she wants to know where it starts and where it ends. What I 
am not sure from the picture above is, if the peak starts at point 1, 
2 or 3. Function rle can be of some help in that particular case, but 
it depends what they want really to extract 

based on data provided

vyber <- rle(co2)$values>20 & rle(co2)$lengths>5 
# gives values co2 which are bigger than 20 and which are 
simultaneously repeated more than five times

 rle(co2)$values[vyber]
 [1] 35 36 37 32 35 35 34 32 35 36 35 35 36 37 35 37 37 35 36 37 
35 35 37 30 29 36 37 37 32 25 36 38 39
# here are the values

> rle(co2)$lengths[vyber]
 [1]  6 14 29 16 50 18 16 20  7 29 31  6 10 18  7 28 52  6 10 22 34  
6 10 13 11 10 10 23 10  7  8  6 47
# and here is their lengths

But it can be found that e.g. first three values are from the first 
peak.

So I suppose they need to look more closely on what they really 
want to know and than try to code it in R.

Cheers
Petr


> 
> Date:   Fri, 18 Jun 2004 11:23:54 +0200 
> From:   jeroen clarysse <jeroen.clarysse at easynet.be>
> To:   <r-help at stat.math.ethz.ch> 
> Subject:   [R] [Q] Newbie (continued.. at least I got R running
> allready :-) 
> 
> 
> Hi all
> 
> a week ago, I posted a newbie question in data smoothing &
> maximum-extraction with R. I got quite a lot of response, but I'm
> still kinda stuck with it...
> 
> I'll restate the problem : i got a datafile with 2400 measuerements
> (every 250msec) of a CO2 measurement device, capturing the breath of a
> subject. I uploaded such a sample here :
> 
> http://www.psy.kuleuven.ac.be/leerpsy/data.csv
> 
> now I wish to figure out where each breath expiration ceiling takes
> place , as shown on this graph :
> 
> http://www.psy.kuleuven.ac.be/leerpsy/graph.bmp
> 
> 
> I'm kinda stuck on how to get this running in R.
> 
> I really hope someone can help me out. If you guys can get me running,
> I promise to promote R as often as I can here on our faculty (which
> still uses Statistica for almost everything)
> 
> thanks a million in advance !
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From ozric at web.de  Mon Jun 21 15:03:04 2004
From: ozric at web.de (Christian Schulz)
Date: Mon, 21 Jun 2004 15:03:04 +0200
Subject: [R] reshape speed improvement?
Message-ID: <200406211503.04483.ozric@web.de>

Hi,

have anybody a hint how  i can perform my speed for
a big reshape jobs?

My  "long" data.frame have  dim  1.342.116    3    and the "wide" data.frame 
could occur  (if i don't filter the timevar) max. in a 
246744 x  1444 data.frame with a lot of NA what could be set to 0.

Neverthless i didn't need really 1444 columns 
(idependent's for data-mining)  , but  didn't getting
a finish reshape  (for ~100  different values in timevar)  since 
saturday morning on a 1200MHZ Duron-512MB/ R.1.9.1 /Linux.

I check different  min--vsize  and min-nsize  to control the swap, but it 
doesn't help really?

An attempt do this in database with sql doesn't success, too - because 
i recognize that's not trivial construct the columns 
automatic/dynamic like reshape makes it possible.

Thanks for hints/comments
regards,christian



From silviakirkman at yahoo.com  Mon Jun 21 15:01:13 2004
From: silviakirkman at yahoo.com (Silvia Kirkman)
Date: Mon, 21 Jun 2004 06:01:13 -0700 (PDT)
Subject: [R] frequencies
Message-ID: <20040621130113.2471.qmail@web50508.mail.yahoo.com>

Hi

I have a set of fish lengths (cm) which I'd like to
have divided into bins as specified by myself. I want
to classify my bins as:

0<=x<0.5
0.5<=x<1
1<=x<1.5
1.5<=x<2
and so on...

I'd like the frequencies to be given as a vector
because I need to use these values for further
analysis.

Please can someone help me with this. I can't find a
command in R that allows one to do this.

Many thanks.

Silvia Kirkman



From JonesW at kssg.com  Mon Jun 21 14:52:34 2004
From: JonesW at kssg.com (Wayne Jones)
Date: Mon, 21 Jun 2004 13:52:34 +0100
Subject: [R] frequencies
Message-ID: <6B5A9304046AD411BD0200508BDFB6CB02BD0D37@gimli.middleearth.kssg.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040621/e112891e/attachment.pl

From bxc at steno.dk  Mon Jun 21 15:07:30 2004
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Mon, 21 Jun 2004 15:07:30 +0200
Subject: [R] frequencies
Message-ID: <0ABD88905D18E347874E0FB71C0B29E901D8E512@exdkba022.novo.dk>

You want the function cut(), followed by table().

Bendix

----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc
----------------------



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Silvia Kirkman
> Sent: Monday, June 21, 2004 4:01 PM
> To: r-help at stat.math.ethz.ch
> Subject: [R] frequencies
> 
> 
> Hi
> 
> I have a set of fish lengths (cm) which I'd like to
> have divided into bins as specified by myself. I want
> to classify my bins as:
> 
> 0<=x<0.5
> 0.5<=x<1
> 1<=x<1.5
> 1.5<=x<2
> and so on...
> 
> I'd like the frequencies to be given as a vector
> because I need to use these values for further
> analysis.
> 
> Please can someone help me with this. I can't find a
> command in R that allows one to do this.
> 
> Many thanks.
> 
> Silvia Kirkman
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> PLEASE 
> do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From petr.pikal at precheza.cz  Mon Jun 21 15:13:04 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 21 Jun 2004 15:13:04 +0200
Subject: [R] frequencies
In-Reply-To: <20040621130113.2471.qmail@web50508.mail.yahoo.com>
Message-ID: <40D6FB00.25940.123C4BF@localhost>



On 21 Jun 2004 at 6:01, Silvia Kirkman wrote:

> Hi
> 
> I have a set of fish lengths (cm) which I'd like to
> have divided into bins as specified by myself. I want
> to classify my bins as:
> 
> 0<=x<0.5
> 0.5<=x<1
> 1<=x<1.5
> 1.5<=x<2
> and so on...

Hallo

?cut and ?table should be what you are looking for

Cheers
Petr



> 
> I'd like the frequencies to be given as a vector
> because I need to use these values for further
> analysis.
> 
> Please can someone help me with this. I can't find a
> command in R that allows one to do this.
> 
> Many thanks.
> 
> Silvia Kirkman
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From christoph.lange at tuebingen.mpg.de  Mon Jun 21 15:27:02 2004
From: christoph.lange at tuebingen.mpg.de (Christoph Lange)
Date: Mon, 21 Jun 2004 15:27:02 +0200
Subject: [R] Visual stimulus presentation using R?
Message-ID: <20040621132702.GB32168@sesame.kyb.local>


Dear all!

Although the Psycho-Toolbox for Matlab is free software, Matlab isn't.
I'm planning to do an experiment where it's essentail to travel to the
subjects, not let the subjects come to where the Matlab licences are
:-(

So I need to use a free software for my experiment if I don't want to
by an extra Matlab licence (which I don't want to).

Did anyone ever try to do presentation of visual stimuli (images,
practically, with a little bit of text in my case) with R? I looked
into the documentation of rgl, but what's lacking there is (as far as
I saw) the possibility to also read (unbuffered) keyboard input.

So what I need is:

  1. put images onto the (full!)screen (qick)
  2. read keyboard input
  3. write results (to an R structure, presumably)

Any idea, suggestion?


Cheers,
  Christoph.

-- 
Christoph Lange
MPI fuer biologische Kybernetik  |Phone: +49-7071-601-607|
Postfach 2169, D-72012 Tuebingen |FAX:   +49-7071-601-616|



From ramasamy at cancer.org.uk  Mon Jun 21 15:19:49 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: 21 Jun 2004 14:19:49 +0100
Subject: [R] frequencies
In-Reply-To: <20040621130113.2471.qmail@web50508.mail.yahoo.com>
References: <20040621130113.2471.qmail@web50508.mail.yahoo.com>
Message-ID: <1087823988.5138.8.camel@vpn202001.lif.icnet.uk>

See the example in help("cut"). You will require the option right=FALSE
in cut() or you can try hist.

x <- abs(rnorm(100))
table( cut(x, seq(0, max(ceiling(x)), by=0.5), right=FALSE ))
hist(x, breaks=seq(0, max(ceiling(x)), by=0.5), plot=FALSE)


On Mon, 2004-06-21 at 14:01, Silvia Kirkman wrote:
> Hi
> 
> I have a set of fish lengths (cm) which I'd like to
> have divided into bins as specified by myself. I want
> to classify my bins as:
> 
> 0<=x<0.5
> 0.5<=x<1
> 1<=x<1.5
> 1.5<=x<2
> and so on...
> 
> I'd like the frequencies to be given as a vector
> because I need to use these values for further
> analysis.
> 
> Please can someone help me with this. I can't find a
> command in R that allows one to do this.
> 
> Many thanks.
> 
> Silvia Kirkman
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From idimakos at upatras.gr  Mon Jun 21 15:43:36 2004
From: idimakos at upatras.gr (Ioannis Dimakos)
Date: Mon, 21 Jun 2004 16:43:36 +0300
Subject: [R] Visual stimulus presentation using R?
In-Reply-To: <20040621132702.GB32168@sesame.kyb.local>
References: <20040621132702.GB32168@sesame.kyb.local>
Message-ID: <40D6E608.90403@upatras.gr>

-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Christoph Lange wrote:

Have you considered DMDX from the Univ. of Arizona
http://www.u.arizona.edu/~kforster/dmdx/overview.htm

It is a rather capable program, runs on Windows platforms, and may help
you with your situation.

Ioannis

| Dear all!
|
| Although the Psycho-Toolbox for Matlab is free software, Matlab isn't.
| I'm planning to do an experiment where it's essentail to travel to the
| subjects, not let the subjects come to where the Matlab licences are
| :-(
|
| So I need to use a free software for my experiment if I don't want to
| by an extra Matlab licence (which I don't want to).
|
| Did anyone ever try to do presentation of visual stimuli (images,
| practically, with a little bit of text in my case) with R? I looked
| into the documentation of rgl, but what's lacking there is (as far as
| I saw) the possibility to also read (unbuffered) keyboard input.
|
| So what I need is:
|
|   1. put images onto the (full!)screen (qick)
|   2. read keyboard input
|   3. write results (to an R structure, presumably)
|
| Any idea, suggestion?
|
|
| Cheers,
|   Christoph.
|


- --
Ioannis Dimakos
University of Patras
Dept. of Elementary Education
Division of Psychology
Patras, GR-26500, Greece
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.2.1 (GNU/Linux)
Comment: Using GnuPG with Mozilla - http://enigmail.mozdev.org

iD8DBQFA1uYI+es8s84M8DARAm1OAJ9ELdYYJclj7AboOM8mFT6fQ5ccEwCeI/f+
YqltOvS89aPrLvuIq/mNCyA=
=TZ65
-----END PGP SIGNATURE-----



From spencer.graves at pdf.com  Mon Jun 21 15:44:07 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 21 Jun 2004 08:44:07 -0500
Subject: [R] Visual stimulus presentation using R?
In-Reply-To: <20040621132702.GB32168@sesame.kyb.local>
References: <20040621132702.GB32168@sesame.kyb.local>
Message-ID: <40D6E627.5090305@pdf.com>

      Have you considered open source alternatives to Matlab such as 
Scilab?  I know nothing about their capabilities, but Google produced 
2410 hits for "Matlab clones", the first of which 
(http://www.dspguru.com/sw/opendsp/mathclo2.htm) seemed quite useful. 

      hope this helps.  spencer graves

Christoph Lange wrote:

>Dear all!
>
>Although the Psycho-Toolbox for Matlab is free software, Matlab isn't.
>I'm planning to do an experiment where it's essentail to travel to the
>subjects, not let the subjects come to where the Matlab licences are
>:-(
>
>So I need to use a free software for my experiment if I don't want to
>by an extra Matlab licence (which I don't want to).
>
>Did anyone ever try to do presentation of visual stimuli (images,
>practically, with a little bit of text in my case) with R? I looked
>into the documentation of rgl, but what's lacking there is (as far as
>I saw) the possibility to also read (unbuffered) keyboard input.
>
>So what I need is:
>
>  1. put images onto the (full!)screen (qick)
>  2. read keyboard input
>  3. write results (to an R structure, presumably)
>
>Any idea, suggestion?
>
>
>Cheers,
>  Christoph.
>
>  
>



From sdhyok at email.unc.edu  Mon Jun 21 15:53:35 2004
From: sdhyok at email.unc.edu (Shin, Daehyok)
Date: Mon, 21 Jun 2004 09:53:35 -0400
Subject: [R] A way to list only variables or functions?
In-Reply-To: <Pine.LNX.4.44.0406201658590.3158-100000@gannet.stats>
Message-ID: <OAEOKPIGCLDDHAEMCAKIKELECOAA.sdhyok@email.unc.edu>

Glad to know useful functions.
How about adding lsv.str function to list only variables bound to values?
In my opinion, we are more interested in values than functions in the
process of data analysis.

In addition, the simple solution of Grothendieck to display only names of
objects has its own practical value.

Daehyok Shin

> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Sent: Sunday, June 20, 2004 PM 12:03
> To: Shin, Daehyok
> Cc: r-help at stat.math.ethz.ch
> Subject: RE: [R] A way to list only variables or functions?
>
>
> On Sun, 20 Jun 2004, Shin, Daehyok wrote:
>
> > Neat! Thanks.
>
> Note that these are not correct, as the get is not done from the
> correct environment.  The function ls.str I pointed you to is correct.
>
> > How about incorporating this support into standard commands, ls() or
> > objects()?
>
> Well, there already is ls[f].str.
>
> > Daehyok Shin (Peter)
> >
> > > -----Original Message-----
> > > From: Gabor Grothendieck [mailto:ggrothendieck at myway.COM]
> > > Sent: Sunday, June 20, 2004 AM 10:06
> > > To: sdhyok at email.unc.edu; r-help at stat.math.ethz.ch
> > > Subject: RE: [R] A way to list only variables or functions?
> > >
> > >
> > >
> > >
> > > These two functions will list the functions and variables
> > > respectively:
> > >
> > > ls.funs <-
> > > function(env=sys.frame(-1))unlist(lapply(ls(env=env),function(x)if
> > > (is.function(get(x)))x))
> > >
> > > ls.vars <-
> > > function(env=sys.frame(-1))unlist(lapply(ls(env=env),function(x)if
> > > (!is.function(get(x)))x))
> > >
> > >
> > > To use:
> > >
> > > ls.funs()
> > > ls.vars()
> > >
> > >
> > > Date:   Sat, 19 Jun 2004 22:59:57 -0400
> > > From:   Shin <sdhyok at email.unc.edu>
> > > To:   R Help <r-help at stat.math.ethz.ch>
> > > Subject:   [R] A way to list only variables or functions?
> > >
> > >
> > > I am curious if there is any way to list only variables or
> functions in
> > > current environment, rather than listing all objects? Thanks.
> > >
> > > --
> > > Daehyok Shin (Peter)
> > > Geography Department
> > > Univ. of North Carolina-Chapel Hill
> > >
> > >
> > >
> > > _______________________________________________
> > > No banners. No pop-ups. No kidding.
> >
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>
>

--
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sdhyok at email.unc.edu  Mon Jun 21 15:57:15 2004
From: sdhyok at email.unc.edu (Shin, Daehyok)
Date: Mon, 21 Jun 2004 09:57:15 -0400
Subject: [R] binary data
In-Reply-To: <830D8D4719112B418ABBC3A0EBA9581272EA1D@webmail.scitegic.com>
Message-ID: <OAEOKPIGCLDDHAEMCAKIAELFCOAA.sdhyok@email.unc.edu>

I use NetCDF file format using ncdf package for binary data handling.
For more complex structure, HDF format will be better, which I do not try in
R.

Daehyok Shin

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch [mailto:r-help-bounces at stat.math.
> ethz.ch]On Behalf Of Moises Hassan
> Sent: Tuesday, June 08, 2004 PM 1:21
> To: R Help
> Subject: [R] binary data
>
>
> What's the preferred way in R for handling samples with binary data
> (like chemical fingerprints encoded as hexadecimal strings with 0's and
> 1's indicating the absence or presence of chemical features) in methods
> such as clustering and MDS. Do you always have to expand the fingerprint
> data into individual variables (which can be a few hundreds) or can they
> be used directly as binary data by some of these methods.
>
>
>
> Thanks,  Moises
>
>
>
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From christoph.lehmann at gmx.ch  Mon Jun 21 16:06:51 2004
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: 21 Jun 2004 16:06:51 +0200
Subject: [R] Visual stimulus presentation using R?
In-Reply-To: <20040621132702.GB32168@sesame.kyb.local>
References: <20040621132702.GB32168@sesame.kyb.local>
Message-ID: <1087826811.2714.32.camel@christophl>

Hi Christoph

I have never done such stuff with R (and I don't know, e.g. how good the
timing would be, in case you are interested in reaction times and so on)

but have a look at 

www.visionegg.org

it's a stimulus-presentation framework, written by Andrew Straw entirely
in python and has lots of great features.

cheers

Christoph

P.S please let me know if you succeed with a solution in R!
On Mon, 2004-06-21 at 15:27, Christoph Lange wrote:
> Dear all!
> 
> Although the Psycho-Toolbox for Matlab is free software, Matlab isn't.
> I'm planning to do an experiment where it's essentail to travel to the
> subjects, not let the subjects come to where the Matlab licences are
> :-(
> 
> So I need to use a free software for my experiment if I don't want to
> by an extra Matlab licence (which I don't want to).
> 
> Did anyone ever try to do presentation of visual stimuli (images,
> practically, with a little bit of text in my case) with R? I looked
> into the documentation of rgl, but what's lacking there is (as far as
> I saw) the possibility to also read (unbuffered) keyboard input.
> 
> So what I need is:
> 
>   1. put images onto the (full!)screen (qick)
>   2. read keyboard input
>   3. write results (to an R structure, presumably)
> 
> Any idea, suggestion?
> 
> 
> Cheers,
>   Christoph.
-- 
Christoph Lehmann <christoph.lehmann at gmx.ch>



From ripley at stats.ox.ac.uk  Mon Jun 21 16:26:19 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 21 Jun 2004 15:26:19 +0100 (BST)
Subject: [R] A way to list only variables or functions?
In-Reply-To: <OAEOKPIGCLDDHAEMCAKIKELECOAA.sdhyok@email.unc.edu>
Message-ID: <Pine.LNX.4.44.0406211525420.23735-100000@gannet.stats>

What is a `variable'?  R has object,s some of which are function and all 
are variable or not depending where they are located.

On Mon, 21 Jun 2004, Shin, Daehyok wrote:

> Glad to know useful functions.
> How about adding lsv.str function to list only variables bound to values?
> In my opinion, we are more interested in values than functions in the
> process of data analysis.
> 
> In addition, the simple solution of Grothendieck to display only names of
> objects has its own practical value.
> 
> Daehyok Shin
> 
> > -----Original Message-----
> > From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> > Sent: Sunday, June 20, 2004 PM 12:03
> > To: Shin, Daehyok
> > Cc: r-help at stat.math.ethz.ch
> > Subject: RE: [R] A way to list only variables or functions?
> >
> >
> > On Sun, 20 Jun 2004, Shin, Daehyok wrote:
> >
> > > Neat! Thanks.
> >
> > Note that these are not correct, as the get is not done from the
> > correct environment.  The function ls.str I pointed you to is correct.
> >
> > > How about incorporating this support into standard commands, ls() or
> > > objects()?
> >
> > Well, there already is ls[f].str.
> >
> > > Daehyok Shin (Peter)
> > >
> > > > -----Original Message-----
> > > > From: Gabor Grothendieck [mailto:ggrothendieck at myway.COM]
> > > > Sent: Sunday, June 20, 2004 AM 10:06
> > > > To: sdhyok at email.unc.edu; r-help at stat.math.ethz.ch
> > > > Subject: RE: [R] A way to list only variables or functions?
> > > >
> > > >
> > > >
> > > >
> > > > These two functions will list the functions and variables
> > > > respectively:
> > > >
> > > > ls.funs <-
> > > > function(env=sys.frame(-1))unlist(lapply(ls(env=env),function(x)if
> > > > (is.function(get(x)))x))
> > > >
> > > > ls.vars <-
> > > > function(env=sys.frame(-1))unlist(lapply(ls(env=env),function(x)if
> > > > (!is.function(get(x)))x))
> > > >
> > > >
> > > > To use:
> > > >
> > > > ls.funs()
> > > > ls.vars()
> > > >
> > > >
> > > > Date:   Sat, 19 Jun 2004 22:59:57 -0400
> > > > From:   Shin <sdhyok at email.unc.edu>
> > > > To:   R Help <r-help at stat.math.ethz.ch>
> > > > Subject:   [R] A way to list only variables or functions?
> > > >
> > > >
> > > > I am curious if there is any way to list only variables or
> > functions in
> > > > current environment, rather than listing all objects? Thanks.
> > > >
> > > > --
> > > > Daehyok Shin (Peter)
> > > > Geography Department
> > > > Univ. of North Carolina-Chapel Hill
> > > >
> > > >
> > > >
> > > > _______________________________________________
> > > > No banners. No pop-ups. No kidding.
> > >
> > > >
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> >
> >
> 
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dmurdoch at pair.com  Mon Jun 21 16:39:54 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Mon, 21 Jun 2004 10:39:54 -0400
Subject: [R] A way to list only variables or functions?
In-Reply-To: <OAEOKPIGCLDDHAEMCAKIKELECOAA.sdhyok@email.unc.edu>
References: <Pine.LNX.4.44.0406201658590.3158-100000@gannet.stats>
	<OAEOKPIGCLDDHAEMCAKIKELECOAA.sdhyok@email.unc.edu>
Message-ID: <vasdd01ctero57ub14dumk6g066lditfam@4ax.com>

On Mon, 21 Jun 2004 09:53:35 -0400, "Shin, Daehyok"
<sdhyok at email.unc.edu> wrote :

>Glad to know useful functions.
>How about adding lsv.str function to list only variables bound to values?
>In my opinion, we are more interested in values than functions in the
>process of data analysis.

In R, functions often contain useful information about data (in their
attached environments).  For example, the result of a smoothing fit
could include a function that calculates the fitted value at any
point.  So the distinction between functions and values isn't as clear
as you seem to be thinking.

However, it would be useful to get a slightly more informative version
of ls(), that returned a data.frame containing the name, length,
class, and other useful information for each object. Then if you
didn't want to see functions, you'd just select based on the class (or
mode, or some other column).

I seem to recall that S-PLUS has such a function, but I forget the
name of it.   Probably R does too, on CRAN if not in the base
packages.

Duncan Murdoch



From lists at svenhartenstein.de  Mon Jun 21 16:53:56 2004
From: lists at svenhartenstein.de (Sven Hartenstein)
Date: Mon, 21 Jun 2004 16:53:56 +0200
Subject: [R] Another NEWBIE
In-Reply-To: <8626.1087765554@www19.gmx.net> (F. Kalder's message of "Sun,
	20 Jun 2004 23:05:54 +0200 (MEST)")
References: <8626.1087765554@www19.gmx.net>
Message-ID: <87r7s9klmj.fsf@svenhartenstein.de>

Hi, 

"F.Kalder" <Kalderf at gmx.de> writes:

> The data entries by ASCII files are strange to me, because I?m so used
> to work with a (the SPSS) spread sheet (mostly the good old typing in
> from paper & pencil questionnaires), that I don?t know how to handle
> that yet.  Maybe using a SPSS- or at least Excel-like tool would be
> helpful for that.

Try

x <- edit(data.frame())

This gives you a spread sheet like interface. But I'm not sure whether
this is available in the windows version of R.

If you prefer using SPSS for entering data: the "foreign" package allows
importing SPSS data files.

Sven



From andy_liaw at merck.com  Mon Jun 21 16:56:11 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 21 Jun 2004 10:56:11 -0400
Subject: [R] A way to list only variables or functions?
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7F20@usrymx25.merck.com>

> From: Duncan Murdoch
> 
> On Mon, 21 Jun 2004 09:53:35 -0400, "Shin, Daehyok"
> <sdhyok at email.unc.edu> wrote :
> 
> >Glad to know useful functions.
> >How about adding lsv.str function to list only variables 
> bound to values?
> >In my opinion, we are more interested in values than functions in the
> >process of data analysis.
> 
> In R, functions often contain useful information about data (in their
> attached environments).  For example, the result of a smoothing fit
> could include a function that calculates the fitted value at any
> point.  So the distinction between functions and values isn't as clear
> as you seem to be thinking.
> 
> However, it would be useful to get a slightly more informative version
> of ls(), that returned a data.frame containing the name, length,
> class, and other useful information for each object. Then if you
> didn't want to see functions, you'd just select based on the class (or
> mode, or some other column).
> 
> I seem to recall that S-PLUS has such a function, but I forget the
> name of it.   Probably R does too, on CRAN if not in the base
> packages.

There are several variants of this posted to R-help and R-devel.  As an
example, search for ls.obj() or ls.object().

Andy

 
> Duncan Murdoch
>



From petr.pikal at precheza.cz  Mon Jun 21 17:01:10 2004
From: petr.pikal at precheza.cz (Petr Pikal)
Date: Mon, 21 Jun 2004 17:01:10 +0200
Subject: [R] A way to list only variables or functions?
In-Reply-To: <vasdd01ctero57ub14dumk6g066lditfam@4ax.com>
References: <OAEOKPIGCLDDHAEMCAKIKELECOAA.sdhyok@email.unc.edu>
Message-ID: <40D71456.18985.500C10@localhost>



On 21 Jun 2004 at 10:39, Duncan Murdoch wrote:

> On Mon, 21 Jun 2004 09:53:35 -0400, "Shin, Daehyok"
> <sdhyok at email.unc.edu> wrote :
> 
> >Glad to know useful functions.
> >How about adding lsv.str function to list only variables bound to
> >values? In my opinion, we are more interested in values than
> >functions in the process of data analysis.
> 
> In R, functions often contain useful information about data (in their
> attached environments).  For example, the result of a smoothing fit
> could include a function that calculates the fitted value at any
> point.  So the distinction between functions and values isn't as clear
> as you seem to be thinking.
> 
> However, it would be useful to get a slightly more informative version
> of ls(), that returned a data.frame containing the name, length,
> class, and other useful information for each object. Then if you
> didn't want to see functions, you'd just select based on the class (or
> mode, or some other column).
> 
> I seem to recall that S-PLUS has such a function, but I forget the
> name of it.   Probably R does too, on CRAN if not in the base
> packages.

Some time ago there was a thread about such matter too and from 
that time i use a function

> ls.objects
function (pos = 1, pattern, order.by) 
{
    napply <- function(names, fn) sapply(names, function(x) 
fn(get(x, 
        pos = pos)))
    names <- ls(pos = pos, pattern = pattern)
    obj.class <- napply(names, function(x) as.character(class(x))[1])
    obj.mode <- napply(names, mode)
    obj.type <- ifelse(is.na(obj.class), obj.mode, obj.class)
    obj.size <- napply(names, object.size)
    obj.dim <- t(napply(names, function(x) 
as.numeric(dim(x))[1:2]))
    vec <- is.na(obj.dim)[, 1] & (obj.type != "function")
    obj.dim[vec, 1] <- napply(names, length)[vec]
    out <- data.frame(obj.type, obj.size, obj.dim)
    names(out) <- c("Type", "Size", "Rows", "Columns")
    if (!missing(order.by)) 
        out <- out[order(out[[order.by]]), ]
    out
}

which gives some more information about objects than plain ls()

Cheers
Petr

> 
> Duncan Murdoch
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html

Petr Pikal
petr.pikal at precheza.cz



From andy_liaw at merck.com  Mon Jun 21 17:05:59 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 21 Jun 2004 11:05:59 -0400
Subject: [R] problem locfit
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7F22@usrymx25.merck.com>

Please do read the posting guide, as the footer suggests, on how to ask
questions in a way that induce useful answers.

>From the little bit that you showed, you stored the result of locfit() to an
object named `fitbmt', but then try to look at an object named `fitbmt31'.
Is that what you really intended to do?  

Andy

> From: fatima bouharaoui
> 
> I have a problem with the use of locfit with censured data, 
> when I carry out locfit by: 
> fitbmt<-locfit(~recur,data=BMTAGE11,cens=df.status,family="haz
> ard",alpha=0.5)
>  it does not give me any message, but if I want to obtain the 
> graph or even if I ask for (fitbmt) made it gives me the 
> following message: 
>  
> > fitbmt31
> Problem: Object "fitbmt31" not found, while calling 
> subroutine slocfit 
> Use traceback() to see the call stack
> I have this problem with several dataset ,but I don't know 
> what is the problem
> thank you for your help
> 
> ---------------------------------
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From sdhyok at email.unc.edu  Mon Jun 21 17:18:10 2004
From: sdhyok at email.unc.edu (Shin, Daehyok)
Date: Mon, 21 Jun 2004 11:18:10 -0400
Subject: [R] A way to list only variables or functions?
In-Reply-To: <Pine.LNX.4.44.0406211525420.23735-100000@gannet.stats>
Message-ID: <OAEOKPIGCLDDHAEMCAKIEELHCOAA.sdhyok@email.unc.edu>

I am not familar with the unique terminology of R.
What I mean with a variable is a sort of a reference which points to a
function or a value.

Daehyok Shin

> -----Original Message-----
> From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> Sent: Monday, June 21, 2004 AM 10:26
> To: Shin, Daehyok
> Cc: r-help at stat.math.ethz.ch
> Subject: RE: [R] A way to list only variables or functions?
>
>
> What is a `variable'?  R has object,s some of which are function and all
> are variable or not depending where they are located.
>
> On Mon, 21 Jun 2004, Shin, Daehyok wrote:
>
> > Glad to know useful functions.
> > How about adding lsv.str function to list only variables bound
> to values?
> > In my opinion, we are more interested in values than functions in the
> > process of data analysis.
> >
> > In addition, the simple solution of Grothendieck to display
> only names of
> > objects has its own practical value.
> >
> > Daehyok Shin
> >
> > > -----Original Message-----
> > > From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk]
> > > Sent: Sunday, June 20, 2004 PM 12:03
> > > To: Shin, Daehyok
> > > Cc: r-help at stat.math.ethz.ch
> > > Subject: RE: [R] A way to list only variables or functions?
> > >
> > >
> > > On Sun, 20 Jun 2004, Shin, Daehyok wrote:
> > >
> > > > Neat! Thanks.
> > >
> > > Note that these are not correct, as the get is not done from the
> > > correct environment.  The function ls.str I pointed you to is correct.
> > >
> > > > How about incorporating this support into standard commands, ls() or
> > > > objects()?
> > >
> > > Well, there already is ls[f].str.
> > >
> > > > Daehyok Shin (Peter)
> > > >
> > > > > -----Original Message-----
> > > > > From: Gabor Grothendieck [mailto:ggrothendieck at myway.COM]
> > > > > Sent: Sunday, June 20, 2004 AM 10:06
> > > > > To: sdhyok at email.unc.edu; r-help at stat.math.ethz.ch
> > > > > Subject: RE: [R] A way to list only variables or functions?
> > > > >
> > > > >
> > > > >
> > > > >
> > > > > These two functions will list the functions and variables
> > > > > respectively:
> > > > >
> > > > > ls.funs <-
> > > > > function(env=sys.frame(-1))unlist(lapply(ls(env=env),function(x)if
> > > > > (is.function(get(x)))x))
> > > > >
> > > > > ls.vars <-
> > > > > function(env=sys.frame(-1))unlist(lapply(ls(env=env),function(x)if
> > > > > (!is.function(get(x)))x))
> > > > >
> > > > >
> > > > > To use:
> > > > >
> > > > > ls.funs()
> > > > > ls.vars()
> > > > >
> > > > >
> > > > > Date:   Sat, 19 Jun 2004 22:59:57 -0400
> > > > > From:   Shin <sdhyok at email.unc.edu>
> > > > > To:   R Help <r-help at stat.math.ethz.ch>
> > > > > Subject:   [R] A way to list only variables or functions?
> > > > >
> > > > >
> > > > > I am curious if there is any way to list only variables or
> > > functions in
> > > > > current environment, rather than listing all objects? Thanks.
> > > > >
> > > > > --
> > > > > Daehyok Shin (Peter)
> > > > > Geography Department
> > > > > Univ. of North Carolina-Chapel Hill
> > > > >
> > > > >
> > > > >
> > > > > _______________________________________________
> > > > > No banners. No pop-ups. No kidding.
> > > >
> > > > >
> > > >
> > > > ______________________________________________
> > > > R-help at stat.math.ethz.ch mailing list
> > > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > > > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> > >
> > >
> >
> > --
> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865 272861 (self)
> > 1 South Parks Road,                     +44 1865 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> >
> >
> >
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>



From Nye04 at gw.medctr.ohio-state.edu  Mon Jun 21 17:20:42 2004
From: Nye04 at gw.medctr.ohio-state.edu (Mark Nye)
Date: Mon, 21 Jun 2004 11:20:42 -0400
Subject: [R] Novice question: Smooth interpolation of survival curve
Message-ID: <s0d6c4a3.046@gw4.medctr.ohio-state.edu>

Greetings,
How might one go about generating a smooth interpolation of a survival
curve generated by the survfit function in the survival package? I am
able to package my variables by standard methods 

>x<-(survfit(Surv(time),data) 

and plot the "step" curve, but would like to obtain a smooth estimation
curve for the purpose of approximating survival time between steps on
the survival curve. Is there a function in the survival package to
perform this task? I have looked into other standard smoothing methods,
but it seems to me that the survfit function doesn't allow for the
packaging of data into separate vectors, such as would be required by
approxfun et al.

Many thanks,
Mark



From sdhyok at email.unc.edu  Mon Jun 21 17:23:28 2004
From: sdhyok at email.unc.edu (Shin, Daehyok)
Date: Mon, 21 Jun 2004 11:23:28 -0400
Subject: [R] A way to list only variables or functions?
In-Reply-To: <40D71456.18985.500C10@localhost>
Message-ID: <OAEOKPIGCLDDHAEMCAKIAELICOAA.sdhyok@email.unc.edu>

Thanks, Petr.
The ls.objects function displays a really nice summary about objects.
Surely, I will use it more often than ls or ls.str.

Daehyok Shin

> -----Original Message-----
> From: Petr Pikal [mailto:petr.pikal at precheza.cz]
> Sent: Monday, June 21, 2004 AM 11:01
> To: Duncan Murdoch; r-help at stat.math.ethz.ch; sdhyok at email.unc.edu
> Subject: Re: [R] A way to list only variables or functions?
> 
> 
> 
> 
> On 21 Jun 2004 at 10:39, Duncan Murdoch wrote:
> 
> > On Mon, 21 Jun 2004 09:53:35 -0400, "Shin, Daehyok"
> > <sdhyok at email.unc.edu> wrote :
> > 
> > >Glad to know useful functions.
> > >How about adding lsv.str function to list only variables bound to
> > >values? In my opinion, we are more interested in values than
> > >functions in the process of data analysis.
> > 
> > In R, functions often contain useful information about data (in their
> > attached environments).  For example, the result of a smoothing fit
> > could include a function that calculates the fitted value at any
> > point.  So the distinction between functions and values isn't as clear
> > as you seem to be thinking.
> > 
> > However, it would be useful to get a slightly more informative version
> > of ls(), that returned a data.frame containing the name, length,
> > class, and other useful information for each object. Then if you
> > didn't want to see functions, you'd just select based on the class (or
> > mode, or some other column).
> > 
> > I seem to recall that S-PLUS has such a function, but I forget the
> > name of it.   Probably R does too, on CRAN if not in the base
> > packages.
> 
> Some time ago there was a thread about such matter too and from 
> that time i use a function
> 
> > ls.objects
> function (pos = 1, pattern, order.by) 
> {
>     napply <- function(names, fn) sapply(names, function(x) 
> fn(get(x, 
>         pos = pos)))
>     names <- ls(pos = pos, pattern = pattern)
>     obj.class <- napply(names, function(x) as.character(class(x))[1])
>     obj.mode <- napply(names, mode)
>     obj.type <- ifelse(is.na(obj.class), obj.mode, obj.class)
>     obj.size <- napply(names, object.size)
>     obj.dim <- t(napply(names, function(x) 
> as.numeric(dim(x))[1:2]))
>     vec <- is.na(obj.dim)[, 1] & (obj.type != "function")
>     obj.dim[vec, 1] <- napply(names, length)[vec]
>     out <- data.frame(obj.type, obj.size, obj.dim)
>     names(out) <- c("Type", "Size", "Rows", "Columns")
>     if (!missing(order.by)) 
>         out <- out[order(out[[order.by]]), ]
>     out
> }
> 
> which gives some more information about objects than plain ls()
> 
> Cheers
> Petr
> 
> > 
> > Duncan Murdoch
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> 
> Petr Pikal
> petr.pikal at precheza.cz
> 
> 
>



From ripley at stats.ox.ac.uk  Mon Jun 21 17:24:15 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 21 Jun 2004 16:24:15 +0100 (BST)
Subject: [R] A way to list only variables or functions?
In-Reply-To: <OAEOKPIGCLDDHAEMCAKIEELHCOAA.sdhyok@email.unc.edu>
Message-ID: <Pine.LNX.4.44.0406211622520.26652-100000@gannet.stats>

On Mon, 21 Jun 2004, Shin, Daehyok wrote:

> I am not familar with the unique terminology of R.

And that's why we have documentation and hope that people will read it.

> What I mean with a variable is a sort of a reference which points to a
> function or a value.

In that case, ls() does what you asked for.  All visible objects are 
symbols bound to values.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maechler at stat.math.ethz.ch  Mon Jun 21 17:27:19 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Mon, 21 Jun 2004 17:27:19 +0200
Subject: [R] R 1.9.1 (source) available by rsync
In-Reply-To: <x21xk987jo.fsf@biostat.ku.dk>
References: <x21xk987jo.fsf@biostat.ku.dk>
Message-ID: <16598.65111.608879.532701@gargle.gargle.HOWL>

Thanks to Doug Bates (for whom I'm substituting),

The source for R-1.9.1 has been available within half an hour of
Peter's R-announcement from rsync.R-project.org.
You can, for example, use

   rsync -avC rsync.r-project.org::r-release ./R-1.9.1

to obtain a copy of the sources which [you won't be able to use
unless you *compile* them, see the R Administration/Installation
manual.

Binary (aka "pre compiled") distributions for platforms such as
Windows and Mac OS X will be available from the CRAN sites in a
few days.  This will apply as well to GNU/Linux versions such 
Debian, Redhat, SuSE, ...

Martin Maechler



From jfox at mcmaster.ca  Mon Jun 21 17:33:20 2004
From: jfox at mcmaster.ca (John Fox)
Date: Mon, 21 Jun 2004 11:33:20 -0400
Subject: [R] Visual stimulus presentation using R?
In-Reply-To: <20040621132702.GB32168@sesame.kyb.local>
Message-ID: <20040621153320.GKWT1984.tomts5-srv.bellnexxia.net@JohnDesktop8300>

Dear Christoph,

Several people have suggested alternative software, and that may well be the
way to go, but if you want to stick with R, then it should be possible to
use Tcl/Tk via the tcltk package to do what you want. Whether the result
would be sufficiently responsive for your needs is hard to tell without
trying it.

I hope this helps,
 John

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Christoph Lange
> Sent: Monday, June 21, 2004 8:27 AM
> To: R Help List
> Subject: [R] Visual stimulus presentation using R?
> 
> 
> Dear all!
> 
> Although the Psycho-Toolbox for Matlab is free software, Matlab isn't.
> I'm planning to do an experiment where it's essentail to 
> travel to the subjects, not let the subjects come to where 
> the Matlab licences are :-(
> 
> So I need to use a free software for my experiment if I don't 
> want to by an extra Matlab licence (which I don't want to).
> 
> Did anyone ever try to do presentation of visual stimuli 
> (images, practically, with a little bit of text in my case) 
> with R? I looked into the documentation of rgl, but what's 
> lacking there is (as far as I saw) the possibility to also 
> read (unbuffered) keyboard input.
> 
> So what I need is:
> 
>   1. put images onto the (full!)screen (qick)
>   2. read keyboard input
>   3. write results (to an R structure, presumably)
> 
> Any idea, suggestion?
> 
> 
> Cheers,
>   Christoph.
>



From mark3ve at hotmail.com  Mon Jun 21 18:09:26 2004
From: mark3ve at hotmail.com (Mark Nye)
Date: Mon, 21 Jun 2004 12:09:26 -0400
Subject: [R] Novice question: Smooth interpolation of survival curve
Message-ID: <BAY2-F138vhNCvjvQ8r00079076@hotmail.com>

Greetings,
How might one go about generating a smooth interpolation of a survival curve 
generated by the survfit function in the survival package? I am able to 
package my variables by standard methods

>x<-(survfit(Surv(time),data)

and plot the "step" curve, but would like to obtain a smooth estimation 
curve for the purpose of approximating survival time between steps on the 
survival curve. Is there a function in the survival package to perform this 
task? I have looked into other standard smoothing methods, but it seems to 
me that the survfit function doesn't allow for the packaging of data into 
separate vectors, such as would be required by approxfun et al.

Many thanks,
Mark


Security. http://clinic.mcafee.com/clinic/ibuy/campaign.asp?cid=3963



From ggrothendieck at myway.com  Mon Jun 21 18:28:32 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 21 Jun 2004 12:28:32 -0400 (EDT)
Subject: [R] A way to list only variables or functions?
Message-ID: <20040621162832.49E2539EC@mprdmxin.myway.com>


 
>> In R, functions often contain useful information about data (in their
>> attached environments). 
>There are several variants of this posted to R-help and R-devel. As >an
>example, search for ls.obj() or ls.object().

Check out

?browseEnv



From drf5n at maplepark.com  Mon Jun 21 18:40:13 2004
From: drf5n at maplepark.com (drf5n@maplepark.com)
Date: Mon, 21 Jun 2004 11:40:13 -0500 (CDT)
Subject: [R] Contrib package downloads?
Message-ID: <Pine.LNX.4.58.0406211135120.1563@maplepark.com>

Where can I download the contrib packages?

http://lib.stat.cmu.edu/R/CRAN/src/contrib/PACKAGES.html points to a list
of packages available for download. but the packages aren't there:

http://lib.stat.cmu.edu/R/CRAN/src/contrib/Descriptions/Hmisc.html
http://lib.stat.cmu.edu/R/CRAN/src/contrib/Descriptions/ncdf.html
http://lib.stat.cmu.edu/R/CRAN/src/contrib/Descriptions/hdf5.html

Thanks for your time,
Dave
-- 
 Dave Forrest                                    (804)642-0662h
 drf5n at maplepark.com               http://maplepark.com/~drf5n/



From drf5n at maplepark.com  Mon Jun 21 18:47:58 2004
From: drf5n at maplepark.com (drf5n@maplepark.com)
Date: Mon, 21 Jun 2004 11:47:58 -0500 (CDT)
Subject: [R] Contrib package downloads?
In-Reply-To: <Pine.LNX.4.58.0406211135120.1563@maplepark.com>
References: <Pine.LNX.4.58.0406211135120.1563@maplepark.com>
Message-ID: <Pine.LNX.4.58.0406211142590.1563@maplepark.com>

On Mon, 21 Jun 2004 drf5n at maplepark.com wrote:

> Where can I download the contrib packages?
>
> http://lib.stat.cmu.edu/R/CRAN/src/contrib/PACKAGES.html points to a list
> of packages available for download. but the packages aren't there:
>
> http://lib.stat.cmu.edu/R/CRAN/src/contrib/Descriptions/Hmisc.html
> http://lib.stat.cmu.edu/R/CRAN/src/contrib/Descriptions/ncdf.html
> http://lib.stat.cmu.edu/R/CRAN/src/contrib/Descriptions/hdf5.html

Oops -- they are there but my aged netscape doesn't render the tables when
the /table html tag is missing, leaving me with a description page that
ends with:

    Downloads:


I've solved my problem, but could someone make the contrib pages close
the table tag so the HTML is well formed?

Thanks again.

Dave
-- 
 Dave Forrest                                    (804)642-0662h
 drf5n at maplepark.com               http://maplepark.com/~drf5n/



From B.Rowlingson at lancaster.ac.uk  Mon Jun 21 16:50:24 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 21 Jun 2004 15:50:24 +0100
Subject: [R] A way to list only variables or functions?
In-Reply-To: <vasdd01ctero57ub14dumk6g066lditfam@4ax.com>
References: <Pine.LNX.4.44.0406201658590.3158-100000@gannet.stats>	<OAEOKPIGCLDDHAEMCAKIKELECOAA.sdhyok@email.unc.edu>
	<vasdd01ctero57ub14dumk6g066lditfam@4ax.com>
Message-ID: <40D6F5B0.4060501@lancaster.ac.uk>

Duncan Murdoch wrote:


> I seem to recall that S-PLUS has such a function, but I forget the
> name of it.   Probably R does too, on CRAN if not in the base
> packages.

objects.summary() I think it was.

It always bothered me that the Nth thing you teach Unix users is 'ls' 
and the N+1th thing is 'ls -l' (for very small N). Then you teach them 
R, and there's no 'ls -l' equivalent immediately obvious.

  Not sure what such a function could show, there being no permissions, 
or dates on R objects, but object.size() and class/mode at least would 
be useful.

  So is something like this in CRAN? I say +1 to putting it in the base 
packages....

Barry



From lauraholt_983 at hotmail.com  Mon Jun 21 18:58:44 2004
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Mon, 21 Jun 2004 11:58:44 -0500
Subject: [R] question about latex command in Hmisc
Message-ID: <BAY12-F342aTMxSpB2800002053@hotmail.com>

Hi R People:

I got the following error from using the "latex" command from Hmisc;

>latex(bbm)
'latex' is not recognized as an internal or external command,
operable program or batch file.
Warning message:
cmd execution failed with error code 1 in: shell(cmd, wait = TRUE, intern = 
output)
>

I was printing a matrix to a Latex File.  This worked fine in Version 1.8.
This is from Version 1.9.0 for Windows.

Any suggestions would be much appreciated.

Thanks in advance!
Sincerely,
Laura
mailto: lauraholt_983 at hotmail.com



From kbartz at loyaltymatrix.com  Mon Jun 21 19:31:11 2004
From: kbartz at loyaltymatrix.com (kevin bartz)
Date: Mon, 21 Jun 2004 10:31:11 -0700
Subject: [R] question about latex command in Hmisc
In-Reply-To: <BAY12-F342aTMxSpB2800002053@hotmail.com>
Message-ID: <20040621173603.6750E400EB@omta14.mta.everyone.net>

When you do "latex" from the command line (Run -> cmd), what does DOS say?
Maybe latex isn't in your PATH.

Kevin

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Laura Holt
Sent: Monday, June 21, 2004 9:59 AM
To: r-help at stat.math.ethz.ch
Subject: [R] question about latex command in Hmisc

Hi R People:

I got the following error from using the "latex" command from Hmisc;

>latex(bbm)
'latex' is not recognized as an internal or external command,
operable program or batch file.
Warning message:
cmd execution failed with error code 1 in: shell(cmd, wait = TRUE, intern = 
output)
>

I was printing a matrix to a Latex File.  This worked fine in Version 1.8.
This is from Version 1.9.0 for Windows.

Any suggestions would be much appreciated.

Thanks in advance!
Sincerely,
Laura
mailto: lauraholt_983 at hotmail.com

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From rxg218 at psu.edu  Mon Jun 21 19:44:57 2004
From: rxg218 at psu.edu (Rajarshi Guha)
Date: Mon, 21 Jun 2004 13:44:57 -0400
Subject: [R] visualizing a list of probabilities
Message-ID: <1087839896.9453.89.camel@blue.chem.psu.edu>

Hi,
  I'm using nnet to work on a 2 class classification problem. The result
of my code is data.frame of true class, predicted class and associated
probability.

One way of summarizing the data is by a confusion matrix. However are
there any graphical ways I could represent the data - specifically, I'd
like to show the probabilities associated with each member of my
prediction set? 

(I would rather not simply list the probabilities in a table)

Thanks,

-------------------------------------------------------------------
Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
-------------------------------------------------------------------
Entropy isn't what it used to be.



From andy_liaw at merck.com  Mon Jun 21 19:50:56 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 21 Jun 2004 13:50:56 -0400
Subject: [R] visualizing a list of probabilities
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7F2C@usrymx25.merck.com>

Not exactly sure what you want, but you might want to look at the margin()
function and the associated plot() method for margin objects in the
randokmForest package.  (You won't be able to use them directly on an nnet
object, but the code should help.)

Cheers,
Andy

> From: Rajarshi Guha
> 
> Hi,
>   I'm using nnet to work on a 2 class classification problem. 
> The result
> of my code is data.frame of true class, predicted class and associated
> probability.
> 
> One way of summarizing the data is by a confusion matrix. However are
> there any graphical ways I could represent the data - 
> specifically, I'd
> like to show the probabilities associated with each member of my
> prediction set? 
> 
> (I would rather not simply list the probabilities in a table)
> 
> Thanks,
> 
> -------------------------------------------------------------------
> Rajarshi Guha <rxg218 at psu.edu> <http://jijo.cjb.net>
> GPG Fingerprint: 0CCA 8EE2 2EEB 25E2 AB04 06F7 1BB9 E634 9B87 56EE
> -------------------------------------------------------------------
> Entropy isn't what it used to be.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ajayshah at mayin.org  Mon Jun 21 20:24:24 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Mon, 21 Jun 2004 23:54:24 +0530
Subject: [R] Elementary sapply question
Message-ID: <20040621182424.GE3897@igidr.ac.in>

I am discovering sapply! :-) Could you please help me with a very
elementary question?

Here is what I know. The following two programs generate the same answer.

--------------------------------+----------------------------------------
       Loops version            |          sapply version
--------------------------------+----------------------------------------
                                |
f <- function(x) {              |       f <- function(x) { 
  return(x*x)                   |         return(x*x)      
}                               |       }                  
values = c(2,4,8)               |       values = c(2,4,8)  
answers=numeric(3)              |       answers = sapply(values, f)
for (i in 1:3) {                |       
  answers[i] = f(values[i])     |
}                               |

and this is cool!

My problem is this. Suppose I have:
     pythagorean <- function(x, y) {
       return(x*x + y*y)
     }

then how do I utilise sapply to replace
     fixed.x = 3
     y.values = c(3,4,5)
     answers=numeric(3)
     for (i in 1:3) {
         answers[i] = pythagorean(fixed.x, y.values[i])
     }

?

I have read the sapply docs, and don't know how to tell him that the
list values that he'll iterate over "fit in" as y.values[i].

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From f.harrell at vanderbilt.edu  Mon Jun 21 15:53:48 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Mon, 21 Jun 2004 13:53:48 +0000
Subject: [R] question about latex command in Hmisc
In-Reply-To: <BAY12-F342aTMxSpB2800002053@hotmail.com>
References: <BAY12-F342aTMxSpB2800002053@hotmail.com>
Message-ID: <40D6E86C.8080104@vanderbilt.edu>

Laura Holt wrote:
> Hi R People:
> 
> I got the following error from using the "latex" command from Hmisc;
> 
>> latex(bbm)
> 
> 'latex' is not recognized as an internal or external command,
> operable program or batch file.
> Warning message:
> cmd execution failed with error code 1 in: shell(cmd, wait = TRUE, 
> intern = output)
> 
>>
> 
> I was printing a matrix to a Latex File.  This worked fine in Version 1.8.
> This is from Version 1.9.0 for Windows.
> 
> Any suggestions would be much appreciated.
> 
> Thanks in advance!
> Sincerely,
> Laura
> mailto: lauraholt_983 at hotmail.com

The latex and latex viewer executables must be in your path if you are 
using the print method for a latex object (which you are, implicitly). 
If you don't want to invoke any executables, do

  w <- latex(object, file='foo')

-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From andy_liaw at merck.com  Mon Jun 21 20:49:48 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 21 Jun 2004 14:49:48 -0400
Subject: [R] Elementary sapply question
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7F2F@usrymx25.merck.com>

At least two ways:

1. Use extra argument in the function being sapply()'ed; e.g.,

> f <- function(x, y) x*x + y*y
> x <- 3:5
> sapply(x, f, 3)
[1] 18 25 34

[See the "..." argument in ?sapply.]

2. More generally, if both x and y are vectors (of the same length), then
you can use mapply(); e.g.,

> x <- 1:3
> y <- 3:5
> pyth <- function(x, y) x*x + y*y
> mapply(pyth, x, y)
[1] 10 20 34

HTH,
Andy

> From: Ajay Shah
> 
> I am discovering sapply! :-) Could you please help me with a very
> elementary question?
> 
> Here is what I know. The following two programs generate the 
> same answer.
> 
> --------------------------------+-----------------------------
> -----------
>        Loops version            |          sapply version
> --------------------------------+-----------------------------
> -----------
>                                 |
> f <- function(x) {              |       f <- function(x) { 
>   return(x*x)                   |         return(x*x)      
> }                               |       }                  
> values = c(2,4,8)               |       values = c(2,4,8)  
> answers=numeric(3)              |       answers = sapply(values, f)
> for (i in 1:3) {                |       
>   answers[i] = f(values[i])     |
> }                               |
> 
> and this is cool!
> 
> My problem is this. Suppose I have:
>      pythagorean <- function(x, y) {
>        return(x*x + y*y)
>      }
> 
> then how do I utilise sapply to replace
>      fixed.x = 3
>      y.values = c(3,4,5)
>      answers=numeric(3)
>      for (i in 1:3) {
>          answers[i] = pythagorean(fixed.x, y.values[i])
>      }
> 
> ?
> 
> I have read the sapply docs, and don't know how to tell him that the
> list values that he'll iterate over "fit in" as y.values[i].
> 
> -- 
> Ajay Shah                                                   Consultant
> ajayshah at mayin.org                      Department of Economic Affairs
> http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ajayshah at mayin.org  Mon Jun 21 20:57:30 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Tue, 22 Jun 2004 00:27:30 +0530
Subject: [R] Elementary sapply question
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7F2F@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF7F2F@usrymx25.merck.com>
Message-ID: <20040621185730.GH3897@igidr.ac.in>

I had asked:
  > > My problem is this. Suppose I have:
  > >      pythagorean <- function(x, y) {
  > >        return(x*x + y*y)
  > >      }
  > > 
  > > then how do I utilise sapply to replace
  > >      fixed.x = 3
  > >      y.values = c(3,4,5)
  > >      answers=numeric(3)
  > >      for (i in 1:3) {
  > >          answers[i] = pythagorean(fixed.x, y.values[i])
  > >      }

On Mon, Jun 21, 2004 at 02:49:48PM -0400, Liaw, Andy wrote:
> At least two ways:
> 
> 1. Use extra argument in the function being sapply()'ed; e.g.,
> 
> > f <- function(x, y) x*x + y*y
> > x <- 3:5
> > sapply(x, f, 3)
> [1] 18 25 34
> 
> [See the "..." argument in ?sapply.]

I am aware of the "..." in sapply(). I am unable to understand how
sapply will know where to utilise the x[i] values: as the 1st arg or
the 2nd arg for f(x, y)?

That is, when I say:

     sapply(x, f, 3)

how does sapply know that I mean:

    for (i in 3:5) {
        f(i, 3)
    }

and not

    for (i in 3:5) {
        f(3, i)
    }

How would we force sapply to use one or the other interpretation?

Thanks,

        -ans.

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From desany.brian at gene.com  Mon Jun 21 21:03:41 2004
From: desany.brian at gene.com (Brian Desany)
Date: Mon, 21 Jun 2004 12:03:41 -0700
Subject: [R] Elementary sapply question
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7F2F@usrymx25.merck.com>
Message-ID: <200406211903.i5LJ3feT007680@ohm.gene.com>

 
Looking in ?mapply, I executed the examples:

> mapply(rep, 1:4, 4:1)
[[1]]
[1] 1 1 1 1

[[2]]
[1] 2 2 2

[[3]]
[1] 3 3

[[4]]
[1] 4

> mapply(rep, times=1:4, x=4:1)
[[1]]
[1] 4

[[2]]
[1] 3 3

[[3]]
[1] 2 2 2

[[4]]
[1] 1 1 1 1


I can guess that because these are 2 examples, it is no surprise that the
results are different. Why is this? If ?mapply is giving me a clue, I'm not
seeing it.

Thanks,
-Brian.


-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Liaw, Andy
Sent: Monday, June 21, 2004 11:50 AM
To: 'Ajay Shah'; r-help
Subject: RE: [R] Elementary sapply question

At least two ways:

1. Use extra argument in the function being sapply()'ed; e.g.,

> f <- function(x, y) x*x + y*y
> x <- 3:5
> sapply(x, f, 3)
[1] 18 25 34

[See the "..." argument in ?sapply.]

2. More generally, if both x and y are vectors (of the same length), then
you can use mapply(); e.g.,

> x <- 1:3
> y <- 3:5
> pyth <- function(x, y) x*x + y*y
> mapply(pyth, x, y)
[1] 10 20 34

HTH,
Andy

> From: Ajay Shah
> 
> I am discovering sapply! :-) Could you please help me with a very
> elementary question?
> 
> Here is what I know. The following two programs generate the 
> same answer.
> 
> --------------------------------+-----------------------------
> -----------
>        Loops version            |          sapply version
> --------------------------------+-----------------------------
> -----------
>                                 |
> f <- function(x) {              |       f <- function(x) { 
>   return(x*x)                   |         return(x*x)      
> }                               |       }                  
> values = c(2,4,8)               |       values = c(2,4,8)  
> answers=numeric(3)              |       answers = sapply(values, f)
> for (i in 1:3) {                |       
>   answers[i] = f(values[i])     |
> }                               |
> 
> and this is cool!
> 
> My problem is this. Suppose I have:
>      pythagorean <- function(x, y) {
>        return(x*x + y*y)
>      }
> 
> then how do I utilise sapply to replace
>      fixed.x = 3
>      y.values = c(3,4,5)
>      answers=numeric(3)
>      for (i in 1:3) {
>          answers[i] = pythagorean(fixed.x, y.values[i])
>      }
> 
> ?
> 
> I have read the sapply docs, and don't know how to tell him that the
> list values that he'll iterate over "fit in" as y.values[i].
> 
> -- 
> Ajay Shah                                                   Consultant
> ajayshah at mayin.org                      Department of Economic Affairs
> http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Mon Jun 21 21:08:36 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 21 Jun 2004 20:08:36 +0100 (BST)
Subject: [R] Elementary sapply question
In-Reply-To: <20040621185730.GH3897@igidr.ac.in>
Message-ID: <Pine.LNX.4.44.0406211959490.27704-100000@gannet.stats>

You really ought to name ... arguments. sapply(x, f, y=3) makes it clear 
that f(xx, y=3) is called.  But `optional arguments' necessarily come 
after compulsory  ones, which resolves the ambiguity you see.

Please note that

1) functions return their values
2) a function body is an expression, so { } is only needed to group 
expressions.  And the same for for() loops.

Andy's f is *much* easier to read than your pythagorean.

3) It is very hard to parse code in which both <- and = are used for 
assignment. Please use <- to avoid hard to read code.


On Tue, 22 Jun 2004, Ajay Shah wrote:

> I had asked:
>   > > My problem is this. Suppose I have:
>   > >      pythagorean <- function(x, y) {
>   > >        return(x*x + y*y)
>   > >      }
>   > > 
>   > > then how do I utilise sapply to replace
>   > >      fixed.x = 3
>   > >      y.values = c(3,4,5)
>   > >      answers=numeric(3)
>   > >      for (i in 1:3) {
>   > >          answers[i] = pythagorean(fixed.x, y.values[i])
>   > >      }
> 
> On Mon, Jun 21, 2004 at 02:49:48PM -0400, Liaw, Andy wrote:
> > At least two ways:
> > 
> > 1. Use extra argument in the function being sapply()'ed; e.g.,
> > 
> > > f <- function(x, y) x*x + y*y
> > > x <- 3:5
> > > sapply(x, f, 3)
> > [1] 18 25 34
> > 
> > [See the "..." argument in ?sapply.]
> 
> I am aware of the "..." in sapply(). I am unable to understand how
> sapply will know where to utilise the x[i] values: as the 1st arg or
> the 2nd arg for f(x, y)?
> 
> That is, when I say:
> 
>      sapply(x, f, 3)
> 
> how does sapply know that I mean:
> 
>     for (i in 3:5) {
>         f(i, 3)
>     }
> 
> and not
> 
>     for (i in 3:5) {
>         f(3, i)
>     }
> 
> How would we force sapply to use one or the other interpretation?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sundar.dorai-raj at PDF.COM  Mon Jun 21 21:10:00 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Mon, 21 Jun 2004 14:10:00 -0500
Subject: [R] Elementary sapply question
In-Reply-To: <200406211903.i5LJ3feT007680@ohm.gene.com>
References: <200406211903.i5LJ3feT007680@ohm.gene.com>
Message-ID: <40D73288.7040109@pdf.com>



Brian Desany wrote:

>  
> Looking in ?mapply, I executed the examples:
> 
> 
>>mapply(rep, 1:4, 4:1)
> 
> [[1]]
> [1] 1 1 1 1
> 
> [[2]]
> [1] 2 2 2
> 
> [[3]]
> [1] 3 3
> 
> [[4]]
> [1] 4
> 
> 
>>mapply(rep, times=1:4, x=4:1)
> 
> [[1]]
> [1] 4
> 
> [[2]]
> [1] 3 3
> 
> [[3]]
> [1] 2 2 2
> 
> [[4]]
> [1] 1 1 1 1
> 
> 
> I can guess that because these are 2 examples, it is no surprise that the
> results are different. Why is this? If ?mapply is giving me a clue, I'm not
> seeing it.
> 
> Thanks,
> -Brian.
> 

The clue you seek is in ?rep which has "x" then "times" as its 
arguments. The first example is equivalent to mapply(rep, x = 1:4, times 
= 4:1) whereas the second example you were more explicit about the 
arguments order.

--sundar


> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Liaw, Andy
> Sent: Monday, June 21, 2004 11:50 AM
> To: 'Ajay Shah'; r-help
> Subject: RE: [R] Elementary sapply question
> 
> At least two ways:
> 
> 1. Use extra argument in the function being sapply()'ed; e.g.,
> 
> 
>>f <- function(x, y) x*x + y*y
>>x <- 3:5
>>sapply(x, f, 3)
> 
> [1] 18 25 34
> 
> [See the "..." argument in ?sapply.]
> 
> 2. More generally, if both x and y are vectors (of the same length), then
> you can use mapply(); e.g.,
> 
> 
>>x <- 1:3
>>y <- 3:5
>>pyth <- function(x, y) x*x + y*y
>>mapply(pyth, x, y)
> 
> [1] 10 20 34
> 
> HTH,
> Andy
> 
> 
>>From: Ajay Shah
>>
>>I am discovering sapply! :-) Could you please help me with a very
>>elementary question?
>>
>>Here is what I know. The following two programs generate the 
>>same answer.
>>
>>--------------------------------+-----------------------------
>>-----------
>>       Loops version            |          sapply version
>>--------------------------------+-----------------------------
>>-----------
>>                                |
>>f <- function(x) {              |       f <- function(x) { 
>>  return(x*x)                   |         return(x*x)      
>>}                               |       }                  
>>values = c(2,4,8)               |       values = c(2,4,8)  
>>answers=numeric(3)              |       answers = sapply(values, f)
>>for (i in 1:3) {                |       
>>  answers[i] = f(values[i])     |
>>}                               |
>>
>>and this is cool!
>>
>>My problem is this. Suppose I have:
>>     pythagorean <- function(x, y) {
>>       return(x*x + y*y)
>>     }
>>
>>then how do I utilise sapply to replace
>>     fixed.x = 3
>>     y.values = c(3,4,5)
>>     answers=numeric(3)
>>     for (i in 1:3) {
>>         answers[i] = pythagorean(fixed.x, y.values[i])
>>     }
>>
>>?
>>
>>I have read the sapply docs, and don't know how to tell him that the
>>list values that he'll iterate over "fit in" as y.values[i].
>>
>>-- 
>>Ajay Shah                                                   Consultant
>>ajayshah at mayin.org                      Department of Economic Affairs
>>http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! 
>>http://www.R-project.org/posting-guide.html
>>
>>
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Mon Jun 21 21:13:48 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 21 Jun 2004 20:13:48 +0100 (BST)
Subject: [R] Elementary sapply question
In-Reply-To: <200406211903.i5LJ3feT007680@ohm.gene.com>
Message-ID: <Pine.LNX.4.44.0406212009120.27704-100000@gannet.stats>

Named arguments are matched by name.  Unnamed arguments are matched by 
position.

S's argument matching rules are quite complex.  It normally helps clarity
to name arguments in all but the simplest calls.

rep(1, 4) and rep(times=1, x=4) are different calls -- the second could 
be called perverse, but at least you don't need to remember the arg seqne 
of rep().  (How many of you know it by heart?  Is each before or 
after length.out?)

On Mon, 21 Jun 2004, Brian Desany wrote:

>  
> Looking in ?mapply, I executed the examples:
> 
> > mapply(rep, 1:4, 4:1)
> [[1]]
> [1] 1 1 1 1
> 
> [[2]]
> [1] 2 2 2
> 
> [[3]]
> [1] 3 3
> 
> [[4]]
> [1] 4
> 
> > mapply(rep, times=1:4, x=4:1)
> [[1]]
> [1] 4
> 
> [[2]]
> [1] 3 3
> 
> [[3]]
> [1] 2 2 2
> 
> [[4]]
> [1] 1 1 1 1
> 
> 
> I can guess that because these are 2 examples, it is no surprise that the
> results are different. Why is this? If ?mapply is giving me a clue, I'm not
> seeing it.
> 
> Thanks,
> -Brian.
> 
> 
> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Liaw, Andy
> Sent: Monday, June 21, 2004 11:50 AM
> To: 'Ajay Shah'; r-help
> Subject: RE: [R] Elementary sapply question
> 
> At least two ways:
> 
> 1. Use extra argument in the function being sapply()'ed; e.g.,
> 
> > f <- function(x, y) x*x + y*y
> > x <- 3:5
> > sapply(x, f, 3)
> [1] 18 25 34
> 
> [See the "..." argument in ?sapply.]
> 
> 2. More generally, if both x and y are vectors (of the same length), then
> you can use mapply(); e.g.,
> 
> > x <- 1:3
> > y <- 3:5
> > pyth <- function(x, y) x*x + y*y
> > mapply(pyth, x, y)
> [1] 10 20 34
> 
> HTH,
> Andy
> 
> > From: Ajay Shah
> > 
> > I am discovering sapply! :-) Could you please help me with a very
> > elementary question?
> > 
> > Here is what I know. The following two programs generate the 
> > same answer.
> > 
> > --------------------------------+-----------------------------
> > -----------
> >        Loops version            |          sapply version
> > --------------------------------+-----------------------------
> > -----------
> >                                 |
> > f <- function(x) {              |       f <- function(x) { 
> >   return(x*x)                   |         return(x*x)      
> > }                               |       }                  
> > values = c(2,4,8)               |       values = c(2,4,8)  
> > answers=numeric(3)              |       answers = sapply(values, f)
> > for (i in 1:3) {                |       
> >   answers[i] = f(values[i])     |
> > }                               |
> > 
> > and this is cool!
> > 
> > My problem is this. Suppose I have:
> >      pythagorean <- function(x, y) {
> >        return(x*x + y*y)
> >      }
> > 
> > then how do I utilise sapply to replace
> >      fixed.x = 3
> >      y.values = c(3,4,5)
> >      answers=numeric(3)
> >      for (i in 1:3) {
> >          answers[i] = pythagorean(fixed.x, y.values[i])
> >      }
> > 
> > ?
> > 
> > I have read the sapply docs, and don't know how to tell him that the
> > list values that he'll iterate over "fit in" as y.values[i].
> > 
> > -- 
> > Ajay Shah                                                   Consultant
> > ajayshah at mayin.org                      Department of Economic Affairs
> > http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi
> > 
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> > http://www.R-project.org/posting-guide.html
> > 
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From tplate at blackmesacapital.com  Mon Jun 21 21:14:20 2004
From: tplate at blackmesacapital.com (Tony Plate)
Date: Mon, 21 Jun 2004 13:14:20 -0600
Subject: [R] Elementary sapply question
In-Reply-To: <20040621185730.GH3897@igidr.ac.in>
References: <3A822319EB35174CA3714066D590DCD504AF7F2F@usrymx25.merck.com>
	<20040621185730.GH3897@igidr.ac.in>
Message-ID: <6.1.0.6.2.20040621130249.04058388@mailhost.blackmesacapital.com>

At Monday 12:57 PM 6/21/2004, Ajay Shah wrote:
>[...snip...]
>I am aware of the "..." in sapply(). I am unable to understand how
>sapply will know where to utilise the x[i] values: as the 1st arg or
>the 2nd arg for f(x, y)?
>
>That is, when I say:
>
>      sapply(x, f, 3)
>
>how does sapply know that I mean:
>
>     for (i in 3:5) {
>         f(i, 3)
>     }
>
>and not
>
>     for (i in 3:5) {
>         f(3, i)
>     }
>
>How would we force sapply to use one or the other interpretation?

All the functions in the apply() family construct the call by just 
appending the additional arguments after the first.  If you supply argument 
names for the additional arguments, those will be supplied to the function 
called.  This can be used to force different interpretations of 
arguments.  E.g:

 > sapply(3:5, function(x, y) {return(y)}, 1)
[1] 1 1 1
 > sapply(3:5, function(x, y) {return(y)}, y=1)
[1] 1 1 1
 > sapply(3:5, function(x, y) {return(y)}, x=1)
[1] 3 4 5
 > sapply(3:5, function(x, y) {return(y)}, z=1)
Error in FUN(X[[as.integer(1)]], ...) : unused argument(s) (z ...)
 >

In the third example, the actual set of arguments in the call to the 
anonymous function is something like (3, x=1), so the standard argument 
interpretation rules result in the arguments having the values y=3, x=1.

hope this help,

Tony Plate




>Thanks,
>
>         -ans.
>
>--
>Ajay Shah                                                   Consultant
>ajayshah at mayin.org                      Department of Economic Affairs
>http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Mon Jun 21 21:16:56 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 21 Jun 2004 15:16:56 -0400
Subject: [R] Elementary sapply question
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7F31@usrymx25.merck.com>

If you have something like

lapply(x, f, ...)

What lapply() does (which is the same as what sapply() does, except sapply()
tries to `simplify' the result) is roughly:

result <- vector(mode="list", length=length(x))
for (i in seq(along(x)) {
    result[[i]] <- f(x[i], ...)
}

I.e., it takes the first argument of lapply() and assume the function
supplied as the second argument is to be applied to the components of the
first argument one by one.  Another thing you seem not to realize or
remember is that in R, arguments to functions are matched by positions, if
not named.

Compare the output of:

lapply(5, function(x, y) x*x + y*y, 1:3)
lapply(1:3, function(x, y) x*x + y*y, 5)

and change lapply() to sapply() and you'll probably see the light...

HTH,
Andy

> From: Ajay Shah [mailto:ajayshah at mayin.org] 
> 
> I had asked:
>   > > My problem is this. Suppose I have:
>   > >      pythagorean <- function(x, y) {
>   > >        return(x*x + y*y)
>   > >      }
>   > > 
>   > > then how do I utilise sapply to replace
>   > >      fixed.x = 3
>   > >      y.values = c(3,4,5)
>   > >      answers=numeric(3)
>   > >      for (i in 1:3) {
>   > >          answers[i] = pythagorean(fixed.x, y.values[i])
>   > >      }
> 
> On Mon, Jun 21, 2004 at 02:49:48PM -0400, Liaw, Andy wrote:
> > At least two ways:
> > 
> > 1. Use extra argument in the function being sapply()'ed; e.g.,
> > 
> > > f <- function(x, y) x*x + y*y
> > > x <- 3:5
> > > sapply(x, f, 3)
> > [1] 18 25 34
> > 
> > [See the "..." argument in ?sapply.]
> 
> I am aware of the "..." in sapply(). I am unable to understand how
> sapply will know where to utilise the x[i] values: as the 1st arg or
> the 2nd arg for f(x, y)?
> 
> That is, when I say:
> 
>      sapply(x, f, 3)
> 
> how does sapply know that I mean:
> 
>     for (i in 3:5) {
>         f(i, 3)
>     }
> 
> and not
> 
>     for (i in 3:5) {
>         f(3, i)
>     }
> 
> How would we force sapply to use one or the other interpretation?
> 
> Thanks,
> 
>         -ans.
> 
> -- 
> Ajay Shah                                                   Consultant
> ajayshah at mayin.org                      Department of Economic Affairs
> http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi
> 
>



From lists at svenhartenstein.de  Mon Jun 21 21:23:03 2004
From: lists at svenhartenstein.de (Sven Hartenstein)
Date: Mon, 21 Jun 2004 21:23:03 +0200
Subject: [R] Welch-JM-Test or Brown-Forsythe-Test in R?
Message-ID: <87isdkwwa0.fsf@svenhartenstein.de>

Hi, 

I want to test mean differences of > 2 groups with heterogenous
variances and wonder whether Welch-JM-Test and/or Brown-Forsythe-Test
are available in R. (These two are the ones I found in the literature
and SPSS provides, I'm open for alternatives.)

Any help apreciated!


Sven



From matthew_wiener at merck.com  Mon Jun 21 21:22:41 2004
From: matthew_wiener at merck.com (Wiener, Matthew)
Date: Mon, 21 Jun 2004 15:22:41 -0400
Subject: [R] Welch-JM-Test or Brown-Forsythe-Test in R?
Message-ID: <45AAE6FD142DCB43A38C00A11FF5DF3EE3647A@uswsmx03.merck.com>

Does oneway.test do what you want?

Hope this helps,

Matt Wiener

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Sven Hartenstein
Sent: Monday, June 21, 2004 3:23 PM
To: r-help at stat.math.ethz.ch
Subject: [R] Welch-JM-Test or Brown-Forsythe-Test in R?


Hi, 

I want to test mean differences of > 2 groups with heterogenous
variances and wonder whether Welch-JM-Test and/or Brown-Forsythe-Test
are available in R. (These two are the ones I found in the literature
and SPSS provides, I'm open for alternatives.)

Any help apreciated!


Sven

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From jserra at midway.uchicago.edu  Mon Jun 21 21:37:15 2004
From: jserra at midway.uchicago.edu (Joan Serra)
Date: Mon, 21 Jun 2004 14:37:15 -0500 (CDT)
Subject: [R] setwd problems
Message-ID: <Pine.GSO.4.21.0406211433180.19038-100000@harper.uchicago.edu>

Hello,

I am using R for Windows and I receive error messages when trying to
change my working directory:

> setwd('C:\BACC_R')
Error in setwd(dir) : cannot change working directory

I would really appreciate your help,
Joan Serra



From k.wang at auckland.ac.nz  Mon Jun 21 21:39:30 2004
From: k.wang at auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Tue, 22 Jun 2004 07:39:30 +1200
Subject: [R] setwd problems
In-Reply-To: <Pine.GSO.4.21.0406211433180.19038-100000@harper.uchicago.edu>
Message-ID: <20040621193945.JIUM24839.mta3-rme.xtra.co.nz@kevinlpt>

Hi,

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
>
> Hello,
>
> I am using R for Windows and I receive error messages when trying to
> change my working directory:
>
> > setwd('C:\BACC_R')
> Error in setwd(dir) : cannot change working directory

Have you tried:
  setwd("C:/BACC_R")
Or
  setwd("C:\\BACC_R")

HTH

Kevin

--------------------------------------------
Ko-Kang Kevin Wang, MSc(Hon)
SLC Stats Workshops Co-ordinator
The University of Auckland
New Zealand



From lists at svenhartenstein.de  Mon Jun 21 21:42:27 2004
From: lists at svenhartenstein.de (Sven Hartenstein)
Date: Mon, 21 Jun 2004 21:42:27 +0200
Subject: [R] Welch-JM-Test or Brown-Forsythe-Test in R?
In-Reply-To: <45AAE6FD142DCB43A38C00A11FF5DF3EE3647A@uswsmx03.merck.com>
	(Matthew Wiener's message of "Mon, 21 Jun 2004 15:22:41 -0400")
References: <45AAE6FD142DCB43A38C00A11FF5DF3EE3647A@uswsmx03.merck.com>
Message-ID: <87eko8wvdo.fsf@svenhartenstein.de>

Hi, 

* "Wiener, Matthew" <matthew_wiener at merck.com> writes:
> Does oneway.test do what you want?

Yes, guess so. Thank you!

BTW, how could I have found this without asking? I only found lm(),
aov() when googling, and in the help() pages of R on these functions
there's no reference to oneway.test...

Thanks again, 

Sven



From sundar.dorai-raj at PDF.COM  Mon Jun 21 21:44:17 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Mon, 21 Jun 2004 14:44:17 -0500
Subject: [R] setwd problems
In-Reply-To: <Pine.GSO.4.21.0406211433180.19038-100000@harper.uchicago.edu>
References: <Pine.GSO.4.21.0406211433180.19038-100000@harper.uchicago.edu>
Message-ID: <40D73A91.1060305@pdf.com>



Joan Serra wrote:

> Hello,
> 
> I am using R for Windows and I receive error messages when trying to
> change my working directory:
> 
> 
>>setwd('C:\BACC_R')
> 
> Error in setwd(dir) : cannot change working directory
> 
> I would really appreciate your help,
> Joan Serra
> 

Hi Joan,

Please review the following:

http://cran.r-project.org/bin/windows/rw-FAQ.html

In rw-FAQ 4.1 it says:

<quote>
4.1 What should I expect to behave differently from the Unix version of R?
<snip>
Paths to files (e.g. in source()) can be specified with either "/" or "\\".
</quote>

Try setwd('C:/BACC_R') or setwd('C:\\BACC_R').


--sundar



From kwright at eskimo.com  Mon Jun 21 22:35:33 2004
From: kwright at eskimo.com (Kevin Wright)
Date: Mon, 21 Jun 2004 13:35:33 -0700 (PDT)
Subject: [R] Bookmarklet for searching R documentation
Message-ID: <200406212035.NAA26351@eskimo.com>

I recently posted a request for a Mozilla search engine plugin for the R 
java search applet.  Having recieved no response, I pursued this myself and 
came up with an alternative.  I found a "bookmarklet" that was used to 
submit searches to Amazon.  I modified the code to submit searches to the R 
search engine.  I tested the following using Firefox 0.9 on Windows 2000.

Create a bookmark called "search R doc" (or whatever) with the following 
text as the "location".  (Note, this is one long line)

javascript:(function(){ function getSearchString (promptString) { s = null;if (document.selection && document.selection.createRange) { s =document.selection.createRange().text; } else if (document.getSelection) { s= document.getSelection(); } if (! (s && s.length)) { s =prompt(promptString,''); } return s; } searchString = getSearchString('Search R help:'); if (searchString != null) { if(searchString.length) { location ='file://c:/R/rw1090/doc/html/search/SearchObject.html?'+escape(searchString); } else { location ='file://c:/R/rw1090/doc/html/search/SearchEngine.html'; } }  })();
 
 
 
Useage notes:

0. Modify the path (in the script) to SearchObject.html and 
SearchString.html depending on your operating system and R version.

1. If text is highlighted when the bookmark is clicked, the selected text 
is submitted to the search engine without any prompt.

2. If no text is selected, a small prompt dialog requests you to enter a 
search string.  Pressing 'enter' will simply open the search page while entering a search word will send the word to the search engine.

3. I prefer to modify one line in the SearchObject.html file so that 
titles, keywords, names are included in the search.  Use this line:
line = line + document.SearchEngine.search (searchstring,true,true,true);
(Note, with a fresh install of R 1.9.0 the only place "SearchObject.html" i
s used is in lattice's lset.html file).  Attention developers: would it 
make sense to modify this file in the CVS tree?  Maybe there is a 
better method for quick searches?

4. The search is a little slow the first time as the java applet loads.

5. Note: Firefox loses hyperlinks when clicking a hyperlink and then 
clicking 'back'.  This has been discussed on the email list.

6. Sit back, relax, and enjoy!  Your mileage may vary, but I like this a lot. 
Thanks to the creator of the original "Search Amazon" bookmarklet.
 
Best,  Kevin Wright



From machud at intellektik.informatik.tu-darmstadt.de  Mon Jun 21 22:53:42 2004
From: machud at intellektik.informatik.tu-darmstadt.de (Marco Chiarandini)
Date: Mon, 21 Jun 2004 22:53:42 +0200 (CEST)
Subject: [R] nonparametric two-way structure all-pairwise comparisons
Message-ID: <Pine.LNX.4.58.0406212229480.8203@kika.intellektik.informatik.tu-darmstadt.de>

Hello,

I am conducting a two-way analysis of variance. The ANOVA assumption
are not met, hence I need to use non-parametrical methods. In particular
I am interested in all-pairwise comparisons between the levels of one of
the two factors.

If I transform the data in ranks, can I reuse the TukeyHSD method to
produce confidence intervals? If this is not correct, is there any
method implemented in R to produce confidence intervals in the
non-parametrical context?


Thank you for the help!


Marco



--
Marco Chiarandini, Fachgebiet Intellektik, Fachbereich Informatik,
Technische Universit??t Darmstadt, Hochschulstra??e 10,
D-64289 Darmstadt - Germany, Office: S2/02 Raum E317
Tel: +49 (0)6151 16-6802 Fax: +49 (0)6151 16-5326
email: machud at intellektik.informatik.tu-darmstadt.de
web page: http://www.intellektik.informatik.tu-darmstadt.de/~machud



From mdsumner at utas.edu.au  Tue Jun 22 01:51:38 2004
From: mdsumner at utas.edu.au (Mike Sumner)
Date: Tue, 22 Jun 2004 09:51:38 +1000
Subject: [R] sunrise, sunset calculation
In-Reply-To: <40D6C203.80301@hotmail.com>
References: <40D6C203.80301@hotmail.com>
Message-ID: <6.0.0.22.1.20040622095005.0217f228@postoffice.utas.edu.au>

It was through Hill et al. (2001) that I eventually found the NOAA website:

  http://www.soest.hawaii.edu/PFRP/elec.tagdata/tagdata.html

  Hill, Roger D., and Melinda J. Braun (2001). Geolocation by
  Light Level - The Next Step: Latitude. In: J.R. Sibert and J. Nielsen
(Eds.), Electronic Tagging and Tracking in Marine Fisheries (pp. 315-330).
Kluwer Academic Publishers, The Netherlands.

At 09:09 PM 6/21/2004, Angel Lopez wrote:
>but it might be better to say that R code 'uses noaa  algorithms'. Any 
>other pointers welcomed.
>Angel





###############################################

Michael Sumner - PhD. candidate
Maths and Physics (ACE CRC & IASOS) and Zoology (AWRU)
University of Tasmania
Private Bag 77, Hobart, Tas 7001, Australia
Phone: 6226 1752


Overall, SAS is about 11 years behind R and S-Plus in statistical 
capabilities (last year it was
about 10 years behind) in my estimation.
    -- Frank Harrell (SAS User, 1969-1991)
       R-help (September 2003)



From dunn at usq.edu.au  Tue Jun 22 02:02:16 2004
From: dunn at usq.edu.au (Peter Dunn)
Date: Tue, 22 Jun 2004 10:02:16 +1000
Subject: [R] Using xtable with summaries of lm objects
Message-ID: <40D77708.4060404@usq.edu.au>

Hi all

Suppose I do the following:

set.seed(1000)
library(xtable)
x <- runif( 10 )
y <- 1 + 2*x + rnorm( length(x) )
test.lm <- lm( y ~ x )
summary( test.lm )
xtable ( summary( test.lm ) )

The final  xtable  output follows:

% latex table generated in R 1.8.1 by xtable 1.2-2 package
% Tue Jun 22 09:56:36 2004
\begin{table}[ht]
\begin{center}
\begin{tabular}{rrrrr}
\hline
  & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\
\hline
(Intercept) & 0.5731 & 0.4396 & 1.30 & 0.2286 \\
x & 1.9680 & 0.8894 & 2.21 & 0.0578 \\
\hline
\end{tabular}
\end{center}
\end{table}


Notice this line:
    & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\

This will not LaTeX correctly, as teh string `$$' occurs;
presumably it should read something like:
    & Estimate & Std. Error & t value & Pr({$>$}{$|$}t$|$) \\
or (better IMHO):
    & Estimate & Std. Error & $t$ value & Pr($>|t|$) \\

I searched the archives, and couldn't find any reference to this
bug; perhaps its just me!  Is there a known workaround (as I want
to auto-generate these table using  Sweave; I know I could
cut-and-paste the table, and correct as appropriate).

Thanks as always.

P.


 > version
          _
platform i686-pc-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status
major    1
minor    8.1
year     2003
month    11
day      21
language R



-- 
Dr Peter Dunn          (USQ CRICOS No. 00244B)
   Web:    http://www.sci.usq.edu.au/staff/dunn
   Email:  dunn @ usq.edu.au
Opinions expressed are mine, not those of USQ.  Obviously...



From mdsumner at utas.edu.au  Tue Jun 22 02:32:06 2004
From: mdsumner at utas.edu.au (Mike Sumner)
Date: Tue, 22 Jun 2004 10:32:06 +1000
Subject: [R] sunrise, sunset calculation
In-Reply-To: <6.0.0.22.1.20040622095005.0217f228@postoffice.utas.edu.au>
References: <40D6C203.80301@hotmail.com>
	<6.0.0.22.1.20040622095005.0217f228@postoffice.utas.edu.au>
Message-ID: <6.0.0.22.1.20040622103012.02159978@postoffice.utas.edu.au>

The Johns Hopkins University/Applied Physics Laboratory also has an IDL 
library with solar elevation code in a file named sunstuff.pro

http://fermi.jhuapl.edu/s1r/idl/s1rlib/local_idl.html



>At 09:09 PM 6/21/2004, Angel Lopez wrote:
>>but it might be better to say that R code 'uses noaa  algorithms'. Any 
>>other pointers welcomed.
>>Angel
>
>
>
>
>
>###############################################
>
>Michael Sumner - PhD. candidate
>Maths and Physics (ACE CRC & IASOS) and Zoology (AWRU)
>University of Tasmania
>Private Bag 77, Hobart, Tas 7001, Australia
>Phone: 6226 1752
>
>
>Overall, SAS is about 11 years behind R and S-Plus in statistical 
>capabilities (last year it was
>about 10 years behind) in my estimation.
>    -- Frank Harrell (SAS User, 1969-1991)
>       R-help (September 2003)
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html





###############################################

Michael Sumner - PhD. candidate
Maths and Physics (ACE CRC & IASOS) and Zoology (AWRU)
University of Tasmania
Private Bag 77, Hobart, Tas 7001, Australia
Phone: 6226 1752


Overall, SAS is about 11 years behind R and S-Plus in statistical 
capabilities (last year it was
about 10 years behind) in my estimation.
    -- Frank Harrell (SAS User, 1969-1991)
       R-help (September 2003)



From umalvarez at fata.unam.mx  Tue Jun 22 02:34:45 2004
From: umalvarez at fata.unam.mx (Ulises Mora Alvarez)
Date: Mon, 21 Jun 2004 19:34:45 -0500 (CDT)
Subject: [R] Html help does not work in Mac OSX 10.3.4
In-Reply-To: <BCF8A564.9F10%ealaca@ucdavis.edu>
Message-ID: <Pine.LNX.4.44.0406211853270.1008-100000@athena.fata.unam.mx>

Hello!

If I launch R from a console I get:


R : Copyright 2004, The R Foundation for Statistical Computing
Version 1.9.0  (2004-04-12), ISBN 3-900051-00-3

> help.start()
Making links in per-session dir ...
If /usr/bin/open is already running, it is *not* restarted, and you
    must switch to its window.
Otherwise, be patient ...
> dyld: /usr/bin/open version mismatch for library: 
/usr/local/lib/libxml2.2.dylib (compatibility version of user: 9.0.0 
greater than library's version: 8.0.0)


This was with the default (browser="/usr/bin/open").

I also tried with:

> options(bowser="/usr/bin/open -a /Applications/firefox.app")
> help.start()
Making links in per-session dir ...
If /usr/bin/open is already running, it is *not* restarted, and you
    must switch to its window.
Otherwise, be patient ...
> dyld: /usr/bin/open version mismatch for library: 
/usr/local/lib/libxml2.2.dylib (compatibility version of user: 9.0.0 
greater than library's version: 8.0.0)
dyld: /usr/bin/open version mismatch for library: 
/usr/local/lib/libxml2.2.dylib (compatibility version of user: 9.0.0 
greater than library's version: 8.0.0)


After that, I check my fink installation:

.~/$ fink -V
Package manager version: 0.20.2
Distribution version: 0.7.0.cvs 

And...

.~/fink list libxml

 i   libxml2            2.6.7-1        XML parsing library, version 2
 i   libxml2-bin        2.6.7-1        XML parsing library, version 2
 i   libxml2-shlibs     2.6.7-1        XML parsing library, version 2
 

So far, no upgrades are available in fink's repositories. Perhaps Stefano
could help us. So, I'm submitting a copy of this mail. I don't know if 
this qualify as a bug, but I'm including R-bugs. I'll be not able to work 
on this full-time till Saturday. 

Regards.

On Fri, 18 Jun 2004, Emilio A. Laca wrote:

> I recently upgraded from R 1.8 to 1.9. I removed 1.8 following the
> instructions. Html help has not worked since. When htmlhelp="TRUE" the
> help.start() command results in the "patience" message and nothing else
> happens. I am using mac osx 10.3.4. Help worked fine when I was using R 
1.8.
> 
> I need help help ;-] Thanks!
> 

-- 
Ulises M. Alvarez
LAB. DE ONDAS DE CHOQUE
FISICA APLICADA Y TECNOLOGIA AVANZADA
UNAM
umalvarez at fata.unam.mx



From kew at utas.edu.au  Tue Jun 22 05:30:40 2004
From: kew at utas.edu.au (Kathryn Wheatley)
Date: Tue, 22 Jun 2004 13:30:40 +1000
Subject: [R] axis labels for stripcharts
Message-ID: <5.1.1.6.0.20040622132526.02941fd0@postoffice.sandybay.utas.edu.au>


I've produced a stripchart but I would like to change the labelling of the 
x-axis. However, I can not get the scripts used in the plot() function (to 
eliminate labels) to work (i.e. xaxt="n" or axis=FALSE). Are there other 
scripts availalable that will work with stripchart?

Cheers,

Kathryn



From ggrothendieck at myway.com  Tue Jun 22 05:58:33 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 22 Jun 2004 03:58:33 +0000 (UTC)
Subject: [R] axis labels for stripcharts
References: <5.1.1.6.0.20040622132526.02941fd0@postoffice.sandybay.utas.edu.au>
Message-ID: <loom.20040622T055659-617@post.gmane.org>


?par

Kathryn Wheatley <kew <at> utas.edu.au> writes:
: I've produced a stripchart but I would like to change the labelling of the 
: x-axis. However, I can not get the scripts used in the plot() function (to 
: eliminate labels) to work (i.e. xaxt="n" or axis=FALSE). Are there other 
: scripts availalable that will work with stripchart?



From ok at cs.otago.ac.nz  Tue Jun 22 06:42:26 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Tue, 22 Jun 2004 16:42:26 +1200 (NZST)
Subject: [R] [Q] Newbie (continued.. at least I got R running allready :-)
Message-ID: <200406220442.i5M4gQ8K056214@atlas.otago.ac.nz>

"jeroen clarysse" <jeroen.clarysse at easynet.be> wrote:
	I'll restate the problem : i got a datafile with 2400 measuerements (every
	250msec) of a CO2 measurement device, capturing the breath of a subject. I
	uploaded such a sample here :
	
	http://www.psy.kuleuven.ac.be/leerpsy/data.csv
	
I stuffed that through an AWK script to convert
    hh:mm:ss:msc
to seconds and omit the INDEX field.  Plotting the
points, I noticed well-defined peaks, but they were flat-topped.

> x <- read.table("leer.dat", header=TRUE)

    This table has $t and $co2 columns.

> y <- rle(x$co2)
> n <- 2:(length(y$values)-1)
> b <- n[(y$values[n-1] < y$values[n]) & (y$values[n] > y$values[n+1])]
> i <- cumsum(y$lengths)[b]
> j <- i + 1 - y$lengths[b]
> plot(x, col="green")         # points in green
> points(x[j,], col="red")     # first point of each peak in red
> points(x[i,], col="black")   # last point of each peak in black

> g <- diff((x$t[i]+x$t[j])/2)

    This step first finds the mid-points of the peaks, and then
    finds the differences between them.

> g
 [1] 4.6125 3.3750 4.3875 3.4500 3.8625 3.6500 4.2750 3.9000 3.6750 4.8750
[11] 2.8375 2.9375 2.6875 2.3375 2.7375 2.4250

    There is quite a dramatic difference between the first ten inter-peak
    times and the last five.



From knoblauch at lyon.inserm.fr  Tue Jun 22 08:28:59 2004
From: knoblauch at lyon.inserm.fr (Ken Knoblauch)
Date: Tue, 22 Jun 2004 08:28:59 +0200
Subject: [R] Visual stimulus presentation using R?
Message-ID: <1087885739.40d7d1ab0d35d@webmail.lyon.inserm.fr>

Speaking of open source alternatives to Matlab, Ben Singer, as I
recall, had been working on using the Psycho-Toolbox from octave.
I don't know how far he got with that or what he is up to now but
his current web page (first hit on google for "Ben Singer") is:

http://www.princeton.edu/~bdsinger/

and I'm sure he'd be happy to let you know what he knows about it,
if you contacted him.

HTH

>     Have you considered open source alternatives to Matlab such as 
>Scilab?  I know nothing about their capabilities, but Google produced 
>2410 hits for "Matlab clones", the first of which 
>(http://www.dspguru.com/sw/opendsp/mathclo2.htm) seemed quite useful. 
>
>      hope this helps.  spencer graves
>
>Christoph Lange wrote:
>
>Dear all!
>
>Although the Psycho-Toolbox for Matlab is free software, Matlab isn't.
>I'm planning to do an experiment where it's essentail to travel to the
>subjects, not let the subjects come to where the Matlab licences are
>:-(
>
>So I need to use a free software for my experiment if I don't want to
>by an extra Matlab licence (which I don't want to).
>
>Did anyone ever try to do presentation of visual stimuli (images,
>practically, with a little bit of text in my case) with R? I looked
>into the documentation of rgl, but what's lacking there is (as far as
>I saw) the possibility to also read (unbuffered) keyboard input.
>
>So what I need is:
>
>  1. put images onto the (full!)screen (qick)
>  2. read keyboard input
>  3. write results (to an R structure, presumably)
>
>Any idea, suggestion?
>
>
>Cheers,
>  Christoph.
>
>  
>

____________________
Ken Knoblauch
Inserm U 371
Cerveau et Vision
18 avenue du Doyen Lepine
69675 Bron cedex
France
tel: +33 (0)4 72 91 34 77
fax: +33 (0)4 72 91 34 61
portable: 06 84 10 64 10



From lauraholt_983 at hotmail.com  Tue Jun 22 09:31:13 2004
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Tue, 22 Jun 2004 02:31:13 -0500
Subject: [R] question about window and inserting value
Message-ID: <BAY12-F14DGAIXWb9DY00004059@hotmail.com>

Hi again R People:

I have the following time series:
>x.ts
       Jan   Feb   Mar   Apr   May   Jun   Jul   Aug   Sep
2000  0.95  0.66  0.83  0.66 -1.45 -1.25  0.33  1.03 -0.48
2001  2.55  1.21 -1.10 -0.63  0.01 -2.20 -0.51  1.12  1.11
2002 -1.37  0.55 -0.63 -0.56  0.92 -1.73  0.59  0.77  0.30
2003  0.55 -0.01 -0.54  2.27 -1.29 -0.23  0.09 -0.50 -0.61
       Oct   Nov   Dec
2000  0.84 -1.35  1.51
2001 -1.68  0.32 -0.97
2002 -1.74 -1.30  1.61
2003  0.25  0.66 -0.05
>#For the March 2001 value:
>window(x.ts,start=c(2001,3),end=c(2001,3))
      Mar
2001 -1.1
>#Fine
>window(x.ts,start=c(2001,3),end=c(2001,3)) <- 10
Error: couldn't find function "window<-"
>#Doesn't work
>

Is there a good way to insert the value into the March 2001 location, 
please?
other than:
x.ts[15] <- 10

Thanks in advance!

R Version 1.9.0  for Windows

Sincerely,
Laura Holt
mailto: lauraholt_983 at hotmail.com



From jinss at hkusua.hku.hk  Tue Jun 22 10:39:09 2004
From: jinss at hkusua.hku.hk (Jin Shusong)
Date: Tue, 22 Jun 2004 16:39:09 +0800
Subject: [R] question about window and inserting value
In-Reply-To: <BAY12-F14DGAIXWb9DY00004059@hotmail.com>
References: <BAY12-F14DGAIXWb9DY00004059@hotmail.com>
Message-ID: <20040622083909.GA3449@S77.hku.hk>

On Tue, Jun 22, 2004 at 02:31:13AM -0500, Laura Holt wrote:
> Hi again R People:
> 
> I have the following time series:
> >x.ts
>       Jan   Feb   Mar   Apr   May   Jun   Jul   Aug   Sep
> 2000  0.95  0.66  0.83  0.66 -1.45 -1.25  0.33  1.03 -0.48
> 2001  2.55  1.21 -1.10 -0.63  0.01 -2.20 -0.51  1.12  1.11
> 2002 -1.37  0.55 -0.63 -0.56  0.92 -1.73  0.59  0.77  0.30
> 2003  0.55 -0.01 -0.54  2.27 -1.29 -0.23  0.09 -0.50 -0.61
>       Oct   Nov   Dec
> 2000  0.84 -1.35  1.51
> 2001 -1.68  0.32 -0.97
> 2002 -1.74 -1.30  1.61
> 2003  0.25  0.66 -0.05
> >#For the March 2001 value:
> >window(x.ts,start=c(2001,3),end=c(2001,3))
>      Mar
> 2001 -1.1
> >#Fine
> >window(x.ts,start=c(2001,3),end=c(2001,3)) <- 10
> Error: couldn't find function "window<-"
> >#Doesn't work
> >
> 
> Is there a good way to insert the value into the March 2001 location, 
> please?
> other than:
> x.ts[15] <- 10
> 
> Thanks in advance!
> 
> R Version 1.9.0  for Windows
> 
> Sincerely,
> Laura Holt
> mailto: lauraholt_983 at hotmail.com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
Dear Holt,

  Perhaps you can try this way,
  x.ts[time(x.ts)==(2001+(3-1)/12)] <- 10

   Jin



From rn001 at cebas.csic.es  Tue Jun 22 11:08:17 2004
From: rn001 at cebas.csic.es (javier garcia - CEBAS)
Date: Tue, 22 Jun 2004 11:08:17 +0200
Subject: [R] plot - labels in axis doubt
Message-ID: <200406220907.i5M97iG32209@natura.cebas.csic.es>

Hi all;
I need to add lines to a plot and an additional axis in the right side with a 
specific scale for this second serie of data. I've seen once this but I can't 
find it now nor can manage to do it. The tickmarks can ve the same that the 
default left axis, but how could I indicate the labels?
Let's say the left scale ranges from 0 to 100, and I need to say that the 
second scale is a specific ratio to this one (because I've choose that ratio 
between the max value of both series to multiply the second, and so be 
adapted to the plotting space of the first plotted serie)

Can someone tell me about any example? 

Thanks and best regards,

Javier



From rn001 at cebas.csic.es  Tue Jun 22 11:34:48 2004
From: rn001 at cebas.csic.es (javier garcia - CEBAS)
Date: Tue, 22 Jun 2004 11:34:48 +0200
Subject: [R] Fwd: plot - labels in axis doubt, Resolved
Message-ID: <200406220934.i5M9YEG00471@natura.cebas.csic.es>

Hi;
Sorry for my last question, disregrad it. Sometimes this happens. I've just 
realized that with axTicks() I extract the values of the first axis and I can 
these with the ratio to label the second axis.

Thanks
Javier
----------  Mensaje reenviado  ----------

Subject: plot - labels in axis doubt
Date: Tue, 22 Jun 2004 11:08:17 +0200
From: javier garcia - CEBAS <rn001 at cebas.csic.es>
To: R-Help <R-help at stat.math.ethz.ch>

Hi all;
I need to add lines to a plot and an additional axis in the right side with a
specific scale for this second serie of data. I've seen once this but I can't
find it now nor can manage to do it. The tickmarks can ve the same that the
default left axis, but how could I indicate the labels?
Let's say the left scale ranges from 0 to 100, and I need to say that the
second scale is a specific ratio to this one (because I've choose that ratio
between the max value of both series to multiply the second, and so be
adapted to the plotting space of the first plotted serie)

Can someone tell me about any example?

Thanks and best regards,

Javier



From Nathan.Weisz at uni-konstanz.de  Tue Jun 22 12:32:52 2004
From: Nathan.Weisz at uni-konstanz.de (Nathan Weisz)
Date: Tue, 22 Jun 2004 12:32:52 +0200
Subject: [R] Re: Visual stimulus presentation using R?
Message-ID: <842E5046-C437-11D8-8BEB-000393597A3C@uni-konstanz.de>

Hi Christoph,

if you have access to Macs somewhere in your department then you could 
use Psyscope. At the moment it's running under OS9. The OS X alpha 
version (I heard that a beta version is on the way) is not perfect yet, 
but should have no problems with what you intend (present picture, get 
response):
Download the OS9 version at:
http://psyscope.psy.cmu.edu/
The OS X Alpha version:
http://psy.ck.sissa.it/

Concerning analysis of responses I have had no problems so far feeding 
Matlab / Octave scripts with the Psyscope data-files and I guess it 
should be possible with R too.

Best,
Nathan

> Dear all!
>
> Although the Psycho-Toolbox for Matlab is free software, Matlab isn't.
> I'm planning to do an experiment where it's essentail to travel to the
> subjects, not let the subjects come to where the Matlab licences are
> :-(
>
> So I need to use a free software for my experiment if I don't want to
> by an extra Matlab licence (which I don't want to).
>
> Did anyone ever try to do presentation of visual stimuli (images,
> practically, with a little bit of text in my case) with R? I looked
> into the documentation of rgl, but what's lacking there is (as far as
> I saw) the possibility to also read (unbuffered) keyboard input.
>
> So what I need is:
>
>   1. put images onto the (full!)screen (qick)
>   2. read keyboard input
>   3. write results (to an R structure, presumably)
>
> Any idea, suggestion?
>
-----------------------------------------
Nathan Weisz
Department of Psychology
University of Konstanz
P.O. Box D25
D - 78457 Konstanz
GERMANY

Tel: +49 (0)7531 88- 4612
E-mail: Nathan.Weisz at uni-konstanz.de
http://www.clinical-psychology.uni-konstanz.de



From ajayshah at mayin.org  Tue Jun 22 12:38:00 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Tue, 22 Jun 2004 16:08:00 +0530
Subject: [R] SUMMARY: "elementary sapply question"
Message-ID: <20040622103800.GA32727@igidr.ac.in>

I am grateful to Andy Liaw, Douglas Grove, Brian Ripley, Tony Plate,
Dirk Eddelbuettel and Sundar Dorai-Raj all of whom got together and
drilled sense into my skull. I would like to take some effort into
explaining what the question was, that I was grappling with, and the
(nice) R way of solving the question.

My apologies: I am still a victim of too many years of writing C, so
I'm a bit dense and it takes me a while to comprehend. :-) My code
fragments are not intended to be 'serious code', they are just written
for maximum readability (atleast to my C-damaged brain). Please
forgive me if I'm not yet quite doing idiomatic R. I'm trying to learn
the lingo!

My question:
  When I have a f(x, y) and I do

  sapply(list, f)

  I know that sapply will run over list elems and run f() many
  times. How do I make him iterate over x values as opposed to
  iterating over y values? In:

     sapply(x, f, 3)

  how does sapply know that I mean:

    for (i in 3:5) {
        f(i, 3)
    }

  and not

    for (i in 3:5) {
        f(3, i)
    }

  How would we force sapply to use one or the other interpretation?


Here's what I learned.

Rule: sapply() uses your list to make 'the first arg' to the function.

  When you say
    > sapply(3:5, f)
  he's going to do f(3), f(4) and f(5).

Rule: sapply() allows you to supply extra args which will be passed
  into the function.

  When you say 
    > sapply(3:5, f, z=2)
  he's going to do f(3, z=2), f(4, z=2), f(5, z=2)

Fact: R does 'intelligent guesswork' when it comes to handling
function args. Watch:

  > myfunction <- function(x, y) x*x + y
  > myfunction(10,3)
  [1] 103
        In this case, he placed 10 as x and 3 as y because that's the
        order that they came in.
  > myfunction(y=3, x=10)
  [1] 103
        This works, even though they're in the wrong order, because I
        explicitly said that the 1st is y and the 2nd is x.
  > myfunction(y=3, 10)
  [1] 103
        This is interesting! I only disambiguated y. So he jumped to
        the conclusion that the lonely one was x.

With this in hand, think of how sapply would behave. If you say

  > sapply(3:5, f, 5)

He's going to do f(3, 5), f(4, 5), f(5, 5). In this case, R will infer
that you must mean x=3, y=5, and so on.

But if you say:

  > sapply(3:5, f, x=5)

He's going to do f(3, x=5), f(4, x=5), f(5, x=5). In this case, R will
infer that you mean _y_ takes the value 3 in the first case! When you
say f(3, x=5), R understands that you are doing f(5,3) or
f(x=5,y=3). Through this behaviour, you can use sapply to apply list
elements to any parameter of a function, not just the 1st.

Hence, it's easy to use sapply to work over all elems of a list for
any arg of any function. Faced with a list and f(x,y,z), if you wanted
to iterate the values for z, you would say

  > sapply(list, f, x=value, y=value)

He would repeatedly do things like f(list[i], x=value, y=value), and R
would crack that what you meant was for z to be list[i]. Cool!

To all those who helped me: I'm not sure I was accurately articulating
my question, but you helped me understand what I needed to find
out. Thanks! Hope this posting helps someone else out there.

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From ggrothendieck at myway.com  Tue Jun 22 13:08:57 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 22 Jun 2004 11:08:57 +0000 (UTC)
Subject: [R] question about window and inserting value
References: <BAY12-F14DGAIXWb9DY00004059@hotmail.com>
Message-ID: <loom.20040622T125153-146@post.gmane.org>


Try this:


"window<-" <- function(x,start=stats::start(x),end=stats::end(x),value) {
	x[match(window(time(x),start=start,end=end),time(x))] <- value
	x
}

window(x.ts, start=c(2000,3), end=c(2000,3)) <- 10




Laura Holt <lauraholt_983 <at> hotmail.com> writes:

: 
: Hi again R People:
: 
: I have the following time series:
: >x.ts
:        Jan   Feb   Mar   Apr   May   Jun   Jul   Aug   Sep
: 2000  0.95  0.66  0.83  0.66 -1.45 -1.25  0.33  1.03 -0.48
: 2001  2.55  1.21 -1.10 -0.63  0.01 -2.20 -0.51  1.12  1.11
: 2002 -1.37  0.55 -0.63 -0.56  0.92 -1.73  0.59  0.77  0.30
: 2003  0.55 -0.01 -0.54  2.27 -1.29 -0.23  0.09 -0.50 -0.61
:        Oct   Nov   Dec
: 2000  0.84 -1.35  1.51
: 2001 -1.68  0.32 -0.97
: 2002 -1.74 -1.30  1.61
: 2003  0.25  0.66 -0.05
: >#For the March 2001 value:
: >window(x.ts,start=c(2001,3),end=c(2001,3))
:       Mar
: 2001 -1.1
: >#Fine
: >window(x.ts,start=c(2001,3),end=c(2001,3)) <- 10
: Error: couldn't find function "window<-"
: >#Doesn't work
: >
: 
: Is there a good way to insert the value into the March 2001 location, 
: please?
: other than:
: x.ts[15] <- 10
: 
: Thanks in advance!
: 
: R Version 1.9.0  for Windows
: 
: Sincerely,
: Laura Holt
: mailto: lauraholt_983 <at> hotmail.com
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://www.stat.math.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
: 
:



From prechelt at pcpool.mi.fu-berlin.de  Tue Jun 22 13:42:15 2004
From: prechelt at pcpool.mi.fu-berlin.de (Lutz Prechelt)
Date: Tue, 22 Jun 2004 13:42:15 +0200
Subject: [R] RE: summaries (was: SUMMARY: "elementary sapply question")
Message-ID: <85D25331FFB7AE4C900EA467D4ADA392045A0F@circle.pcpool.mi.fu-berlin.de>

Ajay,

thank you very much for picking up that age-old habit of 
posting summaries.

It existed years ago on s-help and I find it is still a great
thing: I would not have bothered to read your original question
nor the answers you got, but I did read the summary -- and I
learned something quite interesting!

Maybe some others who receive multiple non-elementary answers to their
questions could sometimes bother to compile a summary so
that some of the bystanders be educated at low cost??
Unrealistic?
I'd be willing to do it for my own questions if it was at
least a little common.

  Lutz



From ajayshah at mayin.org  Tue Jun 22 13:45:26 2004
From: ajayshah at mayin.org (Ajay Shah)
Date: Tue, 22 Jun 2004 17:15:26 +0530
Subject: [R] Re: summaries (was: SUMMARY: "elementary sapply question")
In-Reply-To: <85D25331FFB7AE4C900EA467D4ADA392045A0F@circle.pcpool.mi.fu-berlin.de>
References: <85D25331FFB7AE4C900EA467D4ADA392045A0F@circle.pcpool.mi.fu-berlin.de>
Message-ID: <20040622114526.GC3897@igidr.ac.in>

On Tue, Jun 22, 2004 at 01:42:15PM +0200, Lutz Prechelt wrote:
> Ajay,
> 
> thank you very much for picking up that age-old habit of 
> posting summaries.
> 
> It existed years ago on s-help and I find it is still a great
> thing: I would not have bothered to read your original question
> nor the answers you got, but I did read the summary -- and I
> learned something quite interesting!
> 
> Maybe some others who receive multiple non-elementary answers to their
> questions could sometimes bother to compile a summary so
> that some of the bystanders be educated at low cost??
> Unrealistic?
> I'd be willing to do it for my own questions if it was at
> least a little common.

:-) I'm an old geezer; have been posting summaries on newsgroups in
this fashion since 1989.

-- 
Ajay Shah                                                   Consultant
ajayshah at mayin.org                      Department of Economic Affairs
http://www.mayin.org/ajayshah           Ministry of Finance, New Delhi



From William.Simpson at drdc-rddc.gc.ca  Tue Jun 22 14:44:33 2004
From: William.Simpson at drdc-rddc.gc.ca (Bill Simpson)
Date: Tue, 22 Jun 2004 08:44:33 -0400 (EDT)
Subject: [R] prcomp & eigenvectors
Message-ID: <Pine.LNX.4.44.0406220842420.18809-100000@localhost.localdomain>

I have the following situation I want to analyse with prcomp.

Each subject has a curve called the contrast sensitivity function (CSF). 
This curve's overall shape is due to the additive output of 3 "channels" 
(eigenvectors).

#this shows 3 SF channels; net CSF = c1 + c2+c3
x<-1:100
c1<-dnorm(x,mean=20,sd=20)
c2<-dnorm(x,mean=50,sd=20)
c3<-dnorm(x,mean=80,sd=20)
s1<-1;s2<-1;s3<-1
net<-c1*s1 + c2*s2 + c3*s3
plot(x,net)
lines(x,c1);lines(x,c2);lines(x,c3)

Given the CSFs of many subjects, I was hoping that prcomp could show me 
the shapes of the constituent channels. I checked this with a simulation.

####simulation where each subject has diff weighting (scores) of channels
nsim<-50
csf<-matrix(nrow=nsim,ncol=100)
#one row per subject, cols represent the csf
for(i in 1:nsim)
{
s1<-runif(1);s2<-runif(1);s3<-runif(1)
csf[i,]<-c1*s1 + c2*s2 + c3*s3
}
out<-prcomp(csf)

#the cols of out$rotation are the tuning curves of the various
#channels = factor loadings = eigenvectors
plot(out$rotation[,1],type="l",col=1)  #PC1
lines(out$rotation[,2],col=2)  #PC2
lines(out$rotation[,3],col=3)  #PC3

1. This plot of the three channels does not look like the original. Am I 
correct that the output of prcomp is a linear transformation of the true 
channels? If so, can anyone suggest how I might get the real channels out? 
Maybe  additional info (what?) is needed to be able to do this.

2. I can average all the input CSFs; how do find the average fitted output 
CSF using the out object?

Thanks very much for any help

Bill Simpson



From maechler at stat.math.ethz.ch  Tue Jun 22 14:49:52 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 22 Jun 2004 14:49:52 +0200
Subject: [R] Using xtable with summaries of lm objects
In-Reply-To: <40D77708.4060404@usq.edu.au>
References: <40D77708.4060404@usq.edu.au>
Message-ID: <16600.10992.122964.509126@gargle.gargle.HOWL>

>>>>> "Peter" == Peter Dunn <dunn at usq.edu.au>
>>>>>     on Tue, 22 Jun 2004 10:02:16 +1000 writes:

    Peter> Hi all
    Peter> Suppose I do the following:

    Peter> set.seed(1000)
    Peter> library(xtable)
    Peter> x <- runif( 10 )
    Peter> y <- 1 + 2*x + rnorm( length(x) )
    Peter> test.lm <- lm( y ~ x )
    Peter> summary( test.lm )
    Peter> xtable ( summary( test.lm ) )

    Peter> The final  xtable  output follows:

    Peter> % latex table generated in R 1.8.1 by xtable 1.2-2 package
    Peter> % Tue Jun 22 09:56:36 2004
    Peter> \begin{table}[ht]
    Peter> \begin{center}
    Peter> \begin{tabular}{rrrrr}
    Peter> \hline
    Peter> & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\
    Peter> \hline
    Peter>  (Intercept) & 0.5731 & 0.4396 & 1.30 & 0.2286 \\
    Peter>   x & 1.9680 & 0.8894 & 2.21 & 0.0578 \\
    Peter> \hline
    Peter> \end{tabular}
    Peter> \end{center}
    Peter> \end{table}


    Peter> Notice this line:
    Peter> & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\

    Peter> This will not LaTeX correctly, as teh string `$$' occurs;
    Peter> presumably it should read something like:
    Peter> & Estimate & Std. Error & t value & Pr({$>$}{$|$}t$|$) \\
    Peter> or (better IMHO):
    Peter> & Estimate & Std. Error & $t$ value & Pr($>|t|$) \\

    Peter> I searched the archives, and couldn't find any reference to this
    Peter> bug;
yes, it's a bug.

    Peter>  perhaps its just me!  Is there a known workaround (as I want
    Peter> to auto-generate these table using  Sweave; I know I could
    Peter> cut-and-paste the table, and correct as appropriate).

R and xtable are open source ... ;-)
and the posting guide tells you to ask package authors even
before posting to R-help..
(I've now at least CC'ed xtable's author).

When looking at the xtable source,
you'll realize quickly that  xtable/R/xtable.R
has the ingredients:

 xtable.summary.lm()  is the function that could be improved
 and
 xtable.anova  will probably serve as an example on how to
 treat the "Pr(>|t|)" as special case.

Theoretically better would be to deal with such "P(..)" strings
more ``package globally'', probably in xtable.data.frame() or
xtable.matrix()
{and obviate the need for both xtable.anova() and xtable.summary.lm()'s
 special treatment -- automatically dealing correctly with
 similar classes}.

--
Martin Maechler



From JonesW at kssg.com  Tue Jun 22 14:54:42 2004
From: JonesW at kssg.com (Wayne Jones)
Date: Tue, 22 Jun 2004 13:54:42 +0100
Subject: [R] k nearest neighbours
Message-ID: <6B5A9304046AD411BD0200508BDFB6CB02BD0D78@gimli.middleearth.kssg.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040622/0666a7cc/attachment.pl

From baron at psych.upenn.edu  Tue Jun 22 15:16:02 2004
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Tue, 22 Jun 2004 09:16:02 -0400
Subject: [R] Using xtable with summaries of lm objects
In-Reply-To: <16600.10992.122964.509126@gargle.gargle.HOWL>
References: <40D77708.4060404@usq.edu.au>
	<16600.10992.122964.509126@gargle.gargle.HOWL>
Message-ID: <20040622131602.GA12297@psych>

On 06/22/04 14:49, Martin Maechler wrote:
>>>>>> "Peter" == Peter Dunn <dunn at usq.edu.au>
>    Peter> & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\
>
>    Peter> This will not LaTeX correctly, as teh string `$$' occurs;
>    Peter> presumably it should read something like:
>    Peter> & Estimate & Std. Error & t value & Pr({$>$}{$|$}t$|$) \\
>    Peter> or (better IMHO):
>    Peter> & Estimate & Std. Error & $t$ value & Pr($>|t|$) \\
>
>    Peter> I searched the archives, and couldn't find any reference to this
>    Peter> bug;
>yes, it's a bug.

Not for my version of Latex, which is the one that comes with
tetex-2.0.2-13.  These are actually TeX commands, which Latex
honors.  It makes sense to interpret the first $ in $$ as the end
of text-math mode, rather than the beginning of display-math in
the middle of text-math.  But I agree that this could usefully be
changed.  The point of the present version is to make all the
letters normal rather than slanted.  I guess I favor the slanted
version in the last line, since that seems to be the custom in my
field for $t$.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron



From Timur.Elzhov at jinr.ru  Tue Jun 22 15:41:53 2004
From: Timur.Elzhov at jinr.ru (Timur Elzhov)
Date: Tue, 22 Jun 2004 17:41:53 +0400
Subject: [R] R mathlib
Message-ID: <20040622134153.GC6993@nf034.jinr.ru>

Hello, dear R experts!

I run R on Debian 'sarge' platform. There is 'r-mathlib' package in
Debian distribution, which is described as:
   $ dpkg --status r-mathlib
   ...
   This packages provides the libRmath shared and static libraries which
   can be called from standalone C or C++ code.

Well, I use runif() function in test_runif.c:

#include <Rmath.h>

main()
{
// ...
    printf("%f\n", runif(0, 1));
}

and compile it:

$ gcc -I/usr/lib/R/include -lRmath -lm -o test_runif test_runif.c 
 /tmp/ccmICWeD.o(.text+0x2d): In function `main':
 : undefined reference to `Rf_runif'
 collect2: ld returned 1 exit status

Replacing `-lm' with `-lR' solves the problem. So, I need libR.so anyway?

Thanks.

--
WBR,
Timur



From andy_liaw at merck.com  Tue Jun 22 16:01:36 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 22 Jun 2004 10:01:36 -0400
Subject: [R] k nearest neighbours
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7F3D@usrymx25.merck.com>

None that I know of, but it should be fairly easy to modify knn() in the
`class' package (part of the VR bundle) and the C code it calls so that the
indices of the neighbors are returned.

HTH,
Andy

> From: Wayne Jones
> 
> Hi there fellow R-users,
> 
> Does anyone know of a function which does exactly what 
> knearneigh{spdep}
> (finds the k nearest neighbours) does in the package spdep 
> but for more than
> 2D data?
> 
> Regards
> 
> Wayne
> 
> 
> 
> KSS Ltd
> Seventh Floor  St James's Buildings  79 Oxford Street  
> Manchester  M1 6SS  England
> Company Registration Number 2800886
> Tel: +44 (0) 161 228 0040	Fax: +44 (0) 161 236 6305
> mailto:kssg at kssg.com		http://www.kssg.com
>



From edd at debian.org  Tue Jun 22 16:10:00 2004
From: edd at debian.org (Dirk Eddelbuettel)
Date: Tue, 22 Jun 2004 09:10:00 -0500
Subject: [R] R mathlib
In-Reply-To: <20040622134153.GC6993@nf034.jinr.ru>
References: <20040622134153.GC6993@nf034.jinr.ru>
Message-ID: <20040622141000.GA5514@sonny.eddelbuettel.com>

On Tue, Jun 22, 2004 at 05:41:53PM +0400, Timur Elzhov wrote:
> Hello, dear R experts!
> 
> I run R on Debian 'sarge' platform. There is 'r-mathlib' package in
> Debian distribution, which is described as:
>    $ dpkg --status r-mathlib
>    ...
>    This packages provides the libRmath shared and static libraries which
>    can be called from standalone C or C++ code.
> 
> Well, I use runif() function in test_runif.c:
> 
> #include <Rmath.h>
> 
> main()
> {
> // ...
>     printf("%f\n", runif(0, 1));
> }
> 
> and compile it:
> 
> $ gcc -I/usr/lib/R/include -lRmath -lm -o test_runif test_runif.c 
>  /tmp/ccmICWeD.o(.text+0x2d): In function `main':
>  : undefined reference to `Rf_runif'
>  collect2: ld returned 1 exit status
> 
> Replacing `-lm' with `-lR' solves the problem. So, I need libR.so anyway?

I haven't glanced at the docs in a while, but I think you premiser is wrong.
The standalone library libRmath from the r-mathlib package offers
probability functions and other 'utilities', but not random number
generators which are stateful, can be selected and switched etc pp which
leads to the requirement of libR, and with that come possible requirements
for R initializations.

Whereas the standalone library really is that:

edd at basebud:~> head /usr/share/doc/r-mathlib/examples/test.c 
/*
 *  Mathlib : A C Library of Special Functions
 *  Copyright (C) 2000  The R Development Core Team
 *
 *  This program is free software; you can redistribute it and/or modify
 *  it under the terms of the GNU General Public License as published by
 *  the Free Software Foundation; either version 2 of the License, or
 *  (at your option) any later version.
 *
 *  This program is distributed in the hope that it will be useful,
edd at basebud:~> tail /usr/share/doc/r-mathlib/examples/test.c 
#define MATHLIB_STANDALONE 1
#include <Rmath.h>
 
int
main()
{
/* something to force the library to be included */
    qnorm(0.7, 0.0, 1.0, 0, 0);
    return 0;
}
edd at basebud:~> gcc -o /tmp/foo /usr/share/doc/r-mathlib/examples/test.c -lm -lRmath
edd at basebud:~> /tmp/foo
edd at basebud:~> 

So it compiles and builds standalone as promised.

Dirk

-- 
FEATURE:  VW Beetle license plate in California



From talitaperciano at hotmail.com  Tue Jun 22 16:18:42 2004
From: talitaperciano at hotmail.com (Talita Leite)
Date: Tue, 22 Jun 2004 11:18:42 -0300
Subject: [R] Numerical methods of integration
Message-ID: <BAY14-F17bdxT9Gxa0O000d25b6@hotmail.com>

Hi!

I want to know if there is a function in R that makes integration using 
numerical methods. Thank's for helping!



Talita Perciano Costa Leite
Graduanda em Ci??ncia da Computa????o
Universidade Federal de Alagoas - UFAL
Departamento de Tecnologia da Informa????o - TCI
Constru????o de Conhecimento por Agrupamento de Dados - CoCADa



From martin at ist.org  Tue Jun 22 16:36:35 2004
From: martin at ist.org (Martin Keller-Ressel)
Date: Tue, 22 Jun 2004 16:36:35 +0200
Subject: [R] Re: visualizing a list of probabilities
In-Reply-To: <200406221003.i5MA1iew023063@hypatia.math.ethz.ch>
References: <200406221003.i5MA1iew023063@hypatia.math.ethz.ch>
Message-ID: <opr9z2u9fwjigwsf@mail.ist.org>

If the input variables to your network are continuous you can visualize 
the relationship
between two input variables and the resulting output (class probability)
with image() or persp().

Here is an example (you need the mlbench package from CRAN to run this):

library(mlbench)
x <- as.data.frame(mlbench.spirals(400,cycles=1.5,sd=.1))
plot(x$x.1,x$x.2,col=unclass(x$classes))

nn1 <- nnet(classes ~ x.1 + x.2, data = x, size=20)
xval <- seq(-1.5,1.5,length=100)
map <- outer(xval,xval,FUN=function(x,y) 
{predict(nn1,data.frame(x.1=x,x.2=y))})
image(map)
par("usr"=c(-1.5,1.5,-1.5,1.5))
points(x$x.1,x$x.2,pch=as.numeric(x$classes)+15,col=as.numeric(x$classes)+4)

### or use:

persp(z=map,expand=.3,shade=.7,col="orange",phi=45,theta=180)


If you have more than 2 input variables you can keep the other ones at 
fixed levels and see what happens.

hth,

Martin Keller-Ressel



From patrick.drechsler at gmx.net  Tue Jun 22 16:29:44 2004
From: patrick.drechsler at gmx.net (Patrick Drechsler)
Date: Tue, 22 Jun 2004 16:29:44 +0200
Subject: [R] npmc function: 'x' must be atomic
Message-ID: <m3pt7rfyxz.fsf@pdrechsler.fqdn.th-h.de>

Hi,

I'm having trouble getting the function `npmc' working. I have a
2 column 30 row table called `mydata':

> npmc(mydata, df=2, alpha=0.05)
Error in sort(unique.default(x), na.last = TRUE) : 
	`x' must be atomic
> is.atomic(mydata)
[1] TRUE

Is there anything I have overlooked? Am I invoking the function
correctly? I wasn't able to find any hints in the archives
either.

TIA,

Patrick
-- 
But I don't really see running SETI at Home as practical as Folding at Home.
What, exactly, would be the benefit of finding intelligent aliens on
the other side of the galaxy?

Maybe they're broadcasting the principles of protein folding... 
[from bionet.*]



From ripley at stats.ox.ac.uk  Tue Jun 22 16:32:06 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 22 Jun 2004 15:32:06 +0100 (BST)
Subject: [R] R mathlib
In-Reply-To: <20040622141000.GA5514@sonny.eddelbuettel.com>
Message-ID: <Pine.LNX.4.44.0406221529210.29573-100000@gannet.stats>

The Rmath package does not have runif (nor does it say it has).  It does 
(at least in the R sources) have a README file, and it always helps to 
read READMEs.

On Tue, 22 Jun 2004, Dirk Eddelbuettel wrote:

> On Tue, Jun 22, 2004 at 05:41:53PM +0400, Timur Elzhov wrote:
> > Hello, dear R experts!
> > 
> > I run R on Debian 'sarge' platform. There is 'r-mathlib' package in
> > Debian distribution, which is described as:
> >    $ dpkg --status r-mathlib
> >    ...
> >    This packages provides the libRmath shared and static libraries which
> >    can be called from standalone C or C++ code.
> > 
> > Well, I use runif() function in test_runif.c:
> > 
> > #include <Rmath.h>
> > 
> > main()
> > {
> > // ...
> >     printf("%f\n", runif(0, 1));
> > }
> > 
> > and compile it:
> > 
> > $ gcc -I/usr/lib/R/include -lRmath -lm -o test_runif test_runif.c 
> >  /tmp/ccmICWeD.o(.text+0x2d): In function `main':
> >  : undefined reference to `Rf_runif'
> >  collect2: ld returned 1 exit status
> > 
> > Replacing `-lm' with `-lR' solves the problem. So, I need libR.so anyway?
> 
> I haven't glanced at the docs in a while, but I think you premiser is wrong.
> The standalone library libRmath from the r-mathlib package offers
> probability functions and other 'utilities', but not random number
> generators which are stateful, can be selected and switched etc pp which
> leads to the requirement of libR, and with that come possible requirements
> for R initializations.
> 
> Whereas the standalone library really is that:
> 
> edd at basebud:~> head /usr/share/doc/r-mathlib/examples/test.c 
> /*
>  *  Mathlib : A C Library of Special Functions
>  *  Copyright (C) 2000  The R Development Core Team
>  *
>  *  This program is free software; you can redistribute it and/or modify
>  *  it under the terms of the GNU General Public License as published by
>  *  the Free Software Foundation; either version 2 of the License, or
>  *  (at your option) any later version.
>  *
>  *  This program is distributed in the hope that it will be useful,
> edd at basebud:~> tail /usr/share/doc/r-mathlib/examples/test.c 
> #define MATHLIB_STANDALONE 1
> #include <Rmath.h>
>  
> int
> main()
> {
> /* something to force the library to be included */
>     qnorm(0.7, 0.0, 1.0, 0, 0);
>     return 0;
> }
> edd at basebud:~> gcc -o /tmp/foo /usr/share/doc/r-mathlib/examples/test.c -lm -lRmath
> edd at basebud:~> /tmp/foo
> edd at basebud:~> 
> 
> So it compiles and builds standalone as promised.
> 
> Dirk
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dimitris.rizopoulos at med.kuleuven.ac.be  Tue Jun 22 16:32:20 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Tue, 22 Jun 2004 16:32:20 +0200
Subject: [R] Numerical methods of integration
References: <BAY14-F17bdxT9Gxa0O000d25b6@hotmail.com>
Message-ID: <005301c45865$c06e93f0$ad133a86@www.domain>

There is an "integrate" function which performs one-dimensional
numerical integration. There is also the "adapt" package which
performs multidimensional (up to 20 dimensions) adaptive numerical
integration.

I hope this helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Doctoral Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "Talita Leite" <talitaperciano at hotmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Tuesday, June 22, 2004 4:18 PM
Subject: [R] Numerical methods of integration


> Hi!
>
> I want to know if there is a function in R that makes integration
using
> numerical methods. Thank's for helping!
>
>
>
> Talita Perciano Costa Leite
> Graduanda em Ci??ncia da Computa????o
> Universidade Federal de Alagoas - UFAL
> Departamento de Tecnologia da Informa????o - TCI
> Constru????o de Conhecimento por Agrupamento de Dados - CoCADa
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Tue Jun 22 16:38:38 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 22 Jun 2004 15:38:38 +0100 (BST)
Subject: [R] npmc function: 'x' must be atomic
In-Reply-To: <m3pt7rfyxz.fsf@pdrechsler.fqdn.th-h.de>
Message-ID: <Pine.LNX.4.44.0406221532360.29573-100000@gannet.stats>

On Tue, 22 Jun 2004, Patrick Drechsler wrote:

> Hi,
> 
> I'm having trouble getting the function `npmc' working. I have a
> 2 column 30 row table called `mydata':
> 
> > npmc(mydata, df=2, alpha=0.05)
> Error in sort(unique.default(x), na.last = TRUE) : 
> 	`x' must be atomic
> > is.atomic(mydata)
> [1] TRUE
> 
> Is there anything I have overlooked? 

traceback()/dump.frames/debugger() to pinpoint the error?

Telling us where you got npmc from? (I suspect you mean the one in package 
npmc.)

Noticing that `x' is not the same name as `mydata'?

> Am I invoking the function correctly? I wasn't able to find any hints in
> the archives either.

I am sure that the hint to make use of R's debugging facilities is there 
hundreds of times.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From patrick.drechsler at gmx.net  Tue Jun 22 17:05:33 2004
From: patrick.drechsler at gmx.net (Patrick Drechsler)
Date: Tue, 22 Jun 2004 17:05:33 +0200
Subject: [R] npmc function: 'x' must be atomic
References: <m3pt7rfyxz.fsf@pdrechsler.fqdn.th-h.de>
	<Pine.LNX.4.44.0406221532360.29573-100000@gannet.stats>
Message-ID: <m3eko7fxaa.fsf@pdrechsler.fqdn.th-h.de>


Hi Brian,

thanks for the quick reply! 

Brian Ripley wrote on 22 Jun 2004 15:38:38 MET:

> On Tue, 22 Jun 2004, Patrick Drechsler wrote:

[...npmc...]

> traceback()/dump.frames/debugger() to pinpoint the error?

Wasn't aware of those, thanks for the pointer.

> Telling us where you got npmc from? (I suspect you mean the one
> in package npmc.)

> install.packages("npmc")
<URL:http://cran.r-project.org/src/contrib/npmc_1.0.tar.gz>

> Noticing that `x' is not the same name as `mydata'?

I noticed that also but I have no idea where the `x' is coming from.

> I am sure that the hint to make use of R's debugging facilities
> is there hundreds of times.

Yup, thanks. I'll take a closer look at those facilities.


Here's another example (hopefully more complete):

--8<------------------------schnipp------------------------->8---
> rm(list=c(ls()))
> a <- 1:10;
> b <- rep(c(0,1),5)
> mydata <- cbind(a,b)
> library(npmc)
> npmc(mydata, df=2, alpha=0.05)
Error in sort(unique.default(x), na.last = TRUE) : 
	`x' must be atomic
> traceback()
4: stop("`x' must be atomic")
3: sort(unique.default(x), na.last = TRUE)
2: factor(dataset$class)
1: npmc(mydata, df = 2, alpha = 0.05)
> x
Error: Object "x" not found
--8<------------------------schnapp------------------------->8---

version:
platform i686-pc-linux-gnu
arch     i686             
os       linux-gnu        
system   i686, linux-gnu  
status   alpha            
major    1                
minor    9.1              
year     2004             
month    06               
day      07               
language R                

TIA (and sorry for the privat email -- I hit the wrong button),

Patrick
-- 
Black Holes result from God 
dividing the universe by zero.



From erich.neuwirth at univie.ac.at  Tue Jun 22 18:08:16 2004
From: erich.neuwirth at univie.ac.at (Erich Neuwirth)
Date: Tue, 22 Jun 2004 18:08:16 +0200
Subject: [R] [R-pkgs] R (D)COM server, new release
Message-ID: <40D85970.8090506@univie.ac.at>

Today we uploaded a new release of the
R (D)COM server for Windows to CRAN.
It is release 1.35

The software is much more stable, and there are
quite a few improvements.

Excerpts from the NEWS file:

RServerManager has been added. This provides a repository for R COM 
servers.  See the samples and the additional documentation

Partial rewrite of the code for SetSymbol() and GetSymbol().
Now arrays of VARIANTs are supported to some degree.
This makes the COM server accessible to many scripting languages

Sample usage for JScript, Python, and VBScript added

New tools library provided for Excel to remotely access COM server
without the need for local installation

The software should appear on CRAN in the "Other" section
within the next few days.


Thomas Baier
Erich Neuwirth




-- 
Erich Neuwirth, Computer Supported Didactics Working Group
Visit our SunSITE at http://sunsite.univie.ac.at
Phone: +43-1-4277-38624 Fax: +43-1-4277-9386

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://www.stat.math.ethz.ch/mailman/listinfo/r-packages



From spencer.graves at pdf.com  Tue Jun 22 18:05:46 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 22 Jun 2004 11:05:46 -0500
Subject: [R] Numerical methods of integration
In-Reply-To: <005301c45865$c06e93f0$ad133a86@www.domain>
References: <BAY14-F17bdxT9Gxa0O000d25b6@hotmail.com>
	<005301c45865$c06e93f0$ad133a86@www.domain>
Message-ID: <40D858DA.707@pdf.com>

      If you follow "the posting guide! 
http://www.R-project.org/posting-guide.html" and do an R-site search for 
"gauss-hermite quadrature", it will lead you to answers to a question I 
posted in May relating to this. 

      hope this helps.  spencer graves

Dimitris Rizopoulos wrote:

>There is an "integrate" function which performs one-dimensional
>numerical integration. There is also the "adapt" package which
>performs multidimensional (up to 20 dimensions) adaptive numerical
>integration.
>
>I hope this helps.
>
>Best,
>Dimitris
>
>----
>Dimitris Rizopoulos
>Doctoral Student
>Biostatistical Centre
>School of Public Health
>Catholic University of Leuven
>
>Address: Kapucijnenvoer 35, Leuven, Belgium
>Tel: +32/16/396887
>Fax: +32/16/337015
>Web: http://www.med.kuleuven.ac.be/biostat/
>     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
>
>
>
>----- Original Message ----- 
>From: "Talita Leite" <talitaperciano at hotmail.com>
>To: <r-help at stat.math.ethz.ch>
>Sent: Tuesday, June 22, 2004 4:18 PM
>Subject: [R] Numerical methods of integration
>
>
>  
>
>>Hi!
>>
>>I want to know if there is a function in R that makes integration
>>    
>>
>using
>  
>
>>numerical methods. Thank's for helping!
>>
>>
>>
>>Talita Perciano Costa Leite
>>Graduanda em Ci??ncia da Computa????o
>>Universidade Federal de Alagoas - UFAL
>>Departamento de Tecnologia da Informa????o - TCI
>>Constru????o de Conhecimento por Agrupamento de Dados - CoCADa
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide!
>>    
>>
>http://www.R-project.org/posting-guide.html
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From v.demartino2 at virgilio.it  Tue Jun 22 18:22:40 2004
From: v.demartino2 at virgilio.it (v.demartino2@virgilio.it)
Date: Tue, 22 Jun 2004 18:22:40 +0200
Subject: [R] ts & daily timeseries
Message-ID: <40D1B1D200014863@ims3e.cp.tin.it>

I have defined a daily timeseries for the 365 days of 2003 issuing:

myts = ts(dati[,2:10],frequency=365,)
> myts
Time Series:
Start = c(1, 1) 
End = c(1, 365) 
Frequency = 365 
  
and 

mytime =  as.POSIXct(strptime(as.character(dati[,1]),format="%Y-%m-%d"))

contains the dates from "2003-01-01" to "2003-12-31"

How can I combine mytime and myts in order to list the timeseries according
to the more natural

2003-01-01    xxxxx  xxxxx   xxxxx ......
2003-01-02    xxxxx  xxxxx   xxxxx ......
...........
2003-12-31    xxxxx  xxxxx   xxxxx ......

I've googled on this subject to no avail.

Thanks
Vittorio



From lenon at fstrf-wi.org  Tue Jun 22 17:54:58 2004
From: lenon at fstrf-wi.org (Patrick Lenon)
Date: Tue, 22 Jun 2004 10:54:58 -0500
Subject: [R] Grouped AND stacked bar charts possible in R?
Message-ID: <40D85652.3070005@fstrf-wi.org>

Good day all,

My statisticians want an R procedure that will produce grouped stacked 
barplots.  Barplot will
stack or group, but not both.  The ftable function can produce a table
of the exact form they want, but the barplot doesn't show all the
divisions we want.

For an example, here's the sample from the help file for "ftable:"
 
data(Titanic)
ftable(Titanic, row.vars = 1:3)
ftable(Titanic, row.vars = 1:2, col.vars = "Survived")
ftable(Titanic, row.vars = 2:1, col.vars = "Survived")

Now take it a step further to try to add another dimension:

b <- ftable(Titanic, row.vars=1:3)

                   Survived  No Yes
Class Sex    Age                  
1st   Male   Child            0   5
             Adult          118  57
      Female Child            0   1
             Adult            4 140
2nd   Male   Child            0  11
             Adult          154  14
      Female Child            0  13
             Adult           13  80
3rd   Male   Child           35  13
             Adult          387  75
      Female Child           17  14
             Adult           89  76
Crew  Male   Child            0   0
             Adult          670 192
      Female Child            0   0
             Adult            3  20

barplot(b)
barplot(b, beside=T))

Neither resulting barplot is satisfactory.  The first stacks all the
subdivisions of "Survived = Yes" and "Survived = No" together.  The
second is closer because it creates two groups, but it lists
combinations side-by-side that we'd like stacked. In the above example
"No" and "Yes" would be stacked on bars labeled "Male" or "Female"
in groups by Class.

I've taken a look through the R-Help archives and looked through the
contributed packages, but haven't found anything yet.

If you have any thoughts how we might produce groups of stacked bars
from an ftable, we would appreciate it.



From ripley at stats.ox.ac.uk  Tue Jun 22 18:41:15 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 22 Jun 2004 17:41:15 +0100 (BST)
Subject: [R] ts & daily timeseries
In-Reply-To: <40D1B1D200014863@ims3e.cp.tin.it>
Message-ID: <Pine.LNX.4.44.0406221737440.32732-100000@gannet.stats>

data.frame(time=mytime, ts=myts)

would appear to be what you are looking for.

On Tue, 22 Jun 2004 v.demartino2 at virgilio.it wrote:

> I have defined a daily timeseries for the 365 days of 2003 issuing:
> 
> myts = ts(dati[,2:10],frequency=365,)
> > myts
> Time Series:
> Start = c(1, 1) 
> End = c(1, 365) 
> Frequency = 365 
>   
> and 
> 
> mytime =  as.POSIXct(strptime(as.character(dati[,1]),format="%Y-%m-%d"))
> 
> contains the dates from "2003-01-01" to "2003-12-31"

Why not use the Date class?  However, if all you want is the character 
representation of the dates you appear to have those in dati[1].

> How can I combine mytime and myts in order to list the timeseries according
> to the more natural
> 
> 2003-01-01    xxxxx  xxxxx   xxxxx ......
> 2003-01-02    xxxxx  xxxxx   xxxxx ......
> ...........
> 2003-12-31    xxxxx  xxxxx   xxxxx ......

What do you want the first column to be?  Character strings? Dates?  What 
do you want to do with this?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Tom.Joy at rmbinternational.com  Tue Jun 22 18:53:44 2004
From: Tom.Joy at rmbinternational.com (Joy, Tom)
Date: Tue, 22 Jun 2004 17:53:44 +0100
Subject: [R] Using RExcel addin with version 1.9.1
Message-ID: <576A4B4724813147AF478CAC91B8CF71A98260@bank.rmbi.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040622/8bca8453/attachment.pl

From ligges at statistik.uni-dortmund.de  Tue Jun 22 18:56:51 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Tue, 22 Jun 2004 18:56:51 +0200
Subject: [R] Using RExcel addin with version 1.9.1
In-Reply-To: <576A4B4724813147AF478CAC91B8CF71A98260@bank.rmbi.com>
References: <576A4B4724813147AF478CAC91B8CF71A98260@bank.rmbi.com>
Message-ID: <40D864D3.8050106@statistik.uni-dortmund.de>

Joy, Tom wrote:

> I have just upgraded from version 1.8.1 to 1.9.1 to allow me to use the Rmetrics functions. However the RExcel addin from the (D) COM Server I used before seems to have stopped working with the error message "Could not Start R" followed by "Installation Problem: unable to load connector"
> 
> can anyone help with this problem?

Maybe the paths are not correctly registered?
Try to reinstall the DCOM Server. Also, you may want to try out a new 
DCOM server announced by Erich Neuwirth and Thomas Baier a few hours 
ago. (check the mailing list's archives for details).

Uwe Ligges



From christophe.grova at mail.mcgill.ca  Tue Jun 22 20:19:05 2004
From: christophe.grova at mail.mcgill.ca (christophe grova)
Date: Tue, 22 Jun 2004 12:19:05 -0600
Subject: [R] Need for advise for Correspondence Analysis
Message-ID: <001e01c45885$6742eb30$6400a8c0@cgrova>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040622/8f0ab365/attachment.pl

From HStevens at MUOhio.edu  Tue Jun 22 20:27:44 2004
From: HStevens at MUOhio.edu (Martin Henry H. Stevens)
Date: Tue, 22 Jun 2004 14:27:44 -0400
Subject: [R] Need for advise for Correspondence Analysis
In-Reply-To: <001e01c45885$6742eb30$6400a8c0@cgrova>
References: <001e01c45885$6742eb30$6400a8c0@cgrova>
Message-ID: <DA747730-C479-11D8-A4F8-000A958F43CC@MUOhio.edu>

You may want to look at the documentation for the vegan package.
Hank Stevens
On Jun 22, 2004, at 2:19 PM, christophe grova wrote:

>
> Dear R users,
>
> I m quite a novice in using R for factor analysis and I would need 
> some help to choose the right function.
> I have a contingency table and I would like to perform a  
> Correspondence analysis on this table, followed by a hirarchical 
> clustering of my variables projected in on the first principal 
> components.
>
> Here are my question :
>
> - what is the more appropriate function to do so ... I already tried 
> by using the function 'corresp' in the MASS package
> - it seems to work ... but how is it possible to get all the 
> information concerning the non-centered PCA used by corresp 
> (eigen-values, inertia, scree plot, square cos, ....)
>
> In a second step I would like to use hirarchical clustering from the 
> results of the correspondence analysis ...
>
> If "Table" is my contingency table (i.e., the number of individuals 
> seen in each case) ... I tried to implment it as follow (for the 2 
> first components for instance, but I would be interesting at looking 
> to the other components ...I did not manage to get the eigen values !) 
> :
>
> A <- corresp(Table, nf = 2)
> biplot(A)
> hc <- hclust(dist(A$cscore), "ward")
> plot(hc)
>
> Is that ok ??? ... here it is exemplae using Ward method for 
> clustering ...
> I would also be interested in using method of mutual neighbors  to 
> identify the clusters ... is it possible using hclust ?
>
> Any help would be really appreciated,
>
> Best regards ...
>
> Christophe
>
> ***************************
> Christophe Grova, PhD
> PostDoc - EEG department
> Montreal Neurological Institute, McGill University
> 3801 University Street, Montreal, Quebec, Canada, H3A 2B4
> email : christophe.grova at mail.mcgill.ca
> tel : (514) 398 2184
> fax : (514) 398 8106
> web: http://idm.univ-rennes1.fr/users/grova
> ***************************
>
> 	[[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>
>
Dr. Martin Henry H. Stevens, Assistant Professor
338 Pearson Hall
Botany Department
Miami University
Oxford, OH 45056

Office: (513) 529-4206
Lab: (513) 529-4262
FAX: (513) 529-4243
http://www.cas.muohio.edu/botany/bot/henry.html
http://www.muohio.edu/ecology/
http://www.muohio.edu/botany/
"E Pluribus Unum"



From MSchwartz at MedAnalytics.com  Tue Jun 22 20:32:40 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Tue, 22 Jun 2004 13:32:40 -0500
Subject: [R] Grouped AND stacked bar charts possible in R?
In-Reply-To: <40D85652.3070005@fstrf-wi.org>
References: <40D85652.3070005@fstrf-wi.org>
Message-ID: <1087929160.1567.87.camel@localhost.localdomain>

On Tue, 2004-06-22 at 10:54, Patrick Lenon wrote:
> Good day all,
> 
> My statisticians want an R procedure that will produce grouped stacked 
> barplots.  Barplot will
> stack or group, but not both.  The ftable function can produce a table
> of the exact form they want, but the barplot doesn't show all the
> divisions we want.
> 
> For an example, here's the sample from the help file for "ftable:"
>  
> data(Titanic)
> ftable(Titanic, row.vars = 1:3)
> ftable(Titanic, row.vars = 1:2, col.vars = "Survived")
> ftable(Titanic, row.vars = 2:1, col.vars = "Survived")
> 
> Now take it a step further to try to add another dimension:
> 
> b <- ftable(Titanic, row.vars=1:3)
> 
>                    Survived  No Yes
> Class Sex    Age                  
> 1st   Male   Child            0   5
>              Adult          118  57
>       Female Child            0   1
>              Adult            4 140
> 2nd   Male   Child            0  11
>              Adult          154  14
>       Female Child            0  13
>              Adult           13  80
> 3rd   Male   Child           35  13
>              Adult          387  75
>       Female Child           17  14
>              Adult           89  76
> Crew  Male   Child            0   0
>              Adult          670 192
>       Female Child            0   0
>              Adult            3  20
> 
> barplot(b)
> barplot(b, beside=T))
> 
> Neither resulting barplot is satisfactory.  The first stacks all the
> subdivisions of "Survived = Yes" and "Survived = No" together.  The
> second is closer because it creates two groups, but it lists
> combinations side-by-side that we'd like stacked. In the above example
> "No" and "Yes" would be stacked on bars labeled "Male" or "Female"
> in groups by Class.
> 
> I've taken a look through the R-Help archives and looked through the
> contributed packages, but haven't found anything yet.
> 
> If you have any thoughts how we might produce groups of stacked bars
> from an ftable, we would appreciate it.


I think that you are trying to plot too much information in a single
graphic. The result of a multi-dimensional barplot is likely to be very
difficult to interpret visually.

You would likely be better served to determine, within the multiple
dimensions, what your conditioning and grouping dimensions need to be
and then consider a lattice based plot.

I would urge you to consider using either barchart() or perhaps
dotplot() in lattice, which are designed to handle multivariable charts
of this nature.

Use:

library(lattice)

Then for general information

?Lattice

and then

?barchart

for more function specific information and examples of graphics with
each function.

For the Titanic data that you have above, you could do something like:

# Convert the multi-dimensional table to a 
# data frame. Assumes you have already done
# data(Titanic)
MyData <- as.data.frame(Titanic)

# Take a look at the structure
MyData

   Class    Sex   Age Survived Freq
1    1st   Male Child       No    0
2    2nd   Male Child       No    0
3    3rd   Male Child       No   35
4   Crew   Male Child       No    0
5    1st Female Child       No    0
6    2nd Female Child       No    0
7    3rd Female Child       No   17
8   Crew Female Child       No    0
9    1st   Male Adult       No  118
10   2nd   Male Adult       No  154
11   3rd   Male Adult       No  387
12  Crew   Male Adult       No  670
13   1st Female Adult       No    4
14   2nd Female Adult       No   13
15   3rd Female Adult       No   89
16  Crew Female Adult       No    3
17   1st   Male Child      Yes    5
18   2nd   Male Child      Yes   11
19   3rd   Male Child      Yes   13
20  Crew   Male Child      Yes    0
21   1st Female Child      Yes    1
22   2nd Female Child      Yes   13
23   3rd Female Child      Yes   14
24  Crew Female Child      Yes    0
25   1st   Male Adult      Yes   57
26   2nd   Male Adult      Yes   14
27   3rd   Male Adult      Yes   75
28  Crew   Male Adult      Yes  192
29   1st Female Adult      Yes  140
30   2nd Female Adult      Yes   80
31   3rd Female Adult      Yes   76
32  Crew Female Adult      Yes   20

# Now do a plot. Use 'library(lattice)' here first
# if you had not already done so above for help.
barchart(Freq ~ Survived | Age * Sex, groups = Class, data = MyData,
         auto.key = list(points = FALSE, rectangles = TRUE, space
         = "right", title = "Class", border = TRUE), xlab = "Survived",
         ylim = c(0, 800))

The above barchart will create a four panel plot, where the four main
panels will contain the combinations of Sex and Age. Within each panel
will be two groups of bars, one each for the Survived Yes/No status.
Within each group will be one bar for each Class. 

That is one quick way of grouping things, but you can alter that and
other plot attributes easily.

HTH,

Marc Schwartz



From Jan.Verbesselt at agr.kuleuven.ac.be  Tue Jun 22 21:22:39 2004
From: Jan.Verbesselt at agr.kuleuven.ac.be (Jan Verbesselt)
Date: Tue, 22 Jun 2004 21:22:39 +0200
Subject: [R] ARIMA() fitting with XREG
Message-ID: <002f01c4588e$488b3720$1145210a@agr.ad10.intern.kuleuven.ac.be>

Hi R-helpers,

I would like to derive a "realistic" R?? between X(t)(time serie1) and
Y(t)(time serie2) from a fitted ARIMA model.

The actual ARIMA model is constructed like this;
Y[t] = a[1]Y[t-1] + ... + a[p]Y[t-p] + k*X(t)
 
*Y=Serie2
*X=Serie1=Xreg
*AR(p=8)

The correlation between X(t)(serie1) and Y(t)(serie2) is given by how a
large part of the variance is explained:  k*X(t)/a(p) (a(p)=> AR(8)).
If this is correct, how could I decompose the model to obtain this?

Thanks a lot!
Jan

> regressie1$arma
ar ma sar sma period diff sdiff
8  0   0   0     36    0     2


_______________________________________________________________________
Jan Verbesselt 
Research Associate 
Lab of Geomatics and Forest Engineering K.U. Leuven
Vital Decosterstraat 102. B-3000 Leuven Belgium 
Tel:+32-16-329750   Fax: +32-16-329760
http://gloveg.kuleuven.ac.be/



From pwilkinson at videotron.ca  Tue Jun 22 21:24:25 2004
From: pwilkinson at videotron.ca (Peter Wilkinson)
Date: Tue, 22 Jun 2004 15:24:25 -0400
Subject: [R] write.table when keeping column headers (names of Columns in
 matrix) and row numbers 
Message-ID: <6.0.3.0.0.20040622151953.01afaca0@pop.videotron.ca>

When using the write.table (say for a tab delimited file) command on a 
matrix with Row and Columns, the column headers are always being left 
shifted into the column where the row numbers are being placed. One can see 
this when you open up the tab delimited file in excel.

Is there a better command for this, or is this supposed to be a 'feature'.

Peter



From chrysopa at insecta.ufv.br  Tue Jun 22 22:04:56 2004
From: chrysopa at insecta.ufv.br (Ronaldo Reis Jr.)
Date: Tue, 22 Jun 2004 17:04:56 -0300
Subject: [R] problem with restore and some .RData
In-Reply-To: <Pine.LNX.4.44.0406172103240.6235-100000@gannet.stats>
References: <Pine.LNX.4.44.0406172103240.6235-100000@gannet.stats>
Message-ID: <200406221704.58433.chrysopa@insecta.ufv.br>

Em Thursday 17 June 2004 17:08, Prof Brian Ripley escreveu:
> The short answer is to add importFrom(stats, family) to MASS's NAMESPACE
> and reinstall.  Another way is to use
>
> library(stats)
>
> in your .Rprofile in that directory.

Thanks, its works.

> The real question is why your .RData is loading namespace MASS.
> Presumably some object in your workspace has MASS in its environment.
> Please find out which and tell us exactly how you created it.  (Have you
> copied a MASS object, for example?)

The problem occour whem I make a model whit glmmPQL.

INte
Ronaldo
-- 
Quando pobre come frango, um dos dois est?? doente. 
                -- Apar??cio Torelly, o Bar??o de Itarar??
--
|>   // | \\   [***********************************]
|   ( ??   ?? )  [Ronaldo Reis J??nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36571-000 Vi??osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-2532                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From ripley at stats.ox.ac.uk  Tue Jun 22 22:00:03 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 22 Jun 2004 21:00:03 +0100 (BST)
Subject: [R] write.table when keeping column headers (names of Columns
	in matrix) and row numbers 
In-Reply-To: <6.0.3.0.0.20040622151953.01afaca0@pop.videotron.ca>
Message-ID: <Pine.LNX.4.44.0406222057250.451-100000@gannet.stats>

>From the details of ?write.table  (and in the Data Import/Export manual 
and in MASS4 ...)

     Normally there is no column name for a column of row names.  If
     'col.names=NA' a blank column name is added.  This can be used to
     write CSV files for input to spreadsheets.

What can we possible do to make this more obvious?

On Tue, 22 Jun 2004, Peter Wilkinson wrote:

> When using the write.table (say for a tab delimited file) command on a 
> matrix with Row and Columns, the column headers are always being left 
> shifted into the column where the row numbers are being placed. One can see 
> this when you open up the tab delimited file in excel.
> 
> Is there a better command for this, or is this supposed to be a 'feature'.

It is a genuinely useful feature, fully documented.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From devshruti at hotmail.com  Tue Jun 22 22:25:30 2004
From: devshruti at hotmail.com (devshruti pahuja)
Date: Tue, 22 Jun 2004 20:25:30 +0000
Subject: [R] Regression Modeling query
Message-ID: <BAY9-F42Xng77T4kfjD0003cf53@hotmail.com>

Hi All

I received a raw data set with one record per tennis player (both male and 
female) and then i cured it by aggregation i.e by 4 age groups, 2 gender 
levels and 6 income levels. Gender and Income are categorical variables. 
Please advise me how to use 'R' to model this data set (Actually, i want to 
know the right regression technique and steps to do that, including removing 
outliers at the begining and looking for hidden interactions).

Thanks

Warm Regards
Dev



From devshruti at hotmail.com  Tue Jun 22 22:45:15 2004
From: devshruti at hotmail.com (devshruti pahuja)
Date: Tue, 22 Jun 2004 20:45:15 +0000
Subject: [R] Regression Modeling query
Message-ID: <BAY9-F35Nvne1L3gJcY0000f6a2@hotmail.com>

1. response variable : rankings for a tennis player(not taking account 
career breaks/injury)

2  rankings  Vs player performance & player performance (age, gender, 
income).
   Different gender might throw up surprises as in female tennis rankings, 
we see not much change
   in one playing season where as in men's tennis it's fluctuating. Also, i 
would like to which age   group reflects the prime of a tennis player and 
hence i've changed continuous variables to categorical.

Please advise

Thanks

-Dev


>From: "Peter Flom" <flom at ndri.org>
>To: <devshruti at hotmail.com>
>Subject: Re: [R] Regression Modeling query
>The first question is what you are trying to find out.  What is your
>dependent variable?  What are your hypotheses?
>
>The second question is why you categorized continuous varaibles (age
>and income)? This is almost always a bad idea
>
>
>
>Peter
>
>Peter L. Flom, PhD
>Assistant Director, Statistics and Data Analysis Core
>Center for Drug Use and HIV Research
>National Development and Research Institutes
>71 W. 23rd St
>www.peterflom.com
>New York, NY 10010
>(212) 845-4485 (voice)
>(917) 438-0894 (fax)
>
>
>
> >>> "devshruti pahuja" <devshruti at hotmail.com> 6/22/2004 4:25:30 PM
> >>>
>Hi All
>
>I received a raw data set with one record per tennis player (both male
>and
>female) and then i cured it by aggregation i.e by 4 age groups, 2
>gender
>levels and 6 income levels. Gender and Income are categorical
>variables.
>Please advise me how to use 'R' to model this data set (Actually, i
>want to
>know the right regression technique and steps to do that, including
>removing
>outliers at the begining and looking for hidden interactions).
>
>Thanks
>
>Warm Regards
>Dev
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
>http://www.R-project.org/posting-guide.html



From devshruti at hotmail.com  Tue Jun 22 22:52:53 2004
From: devshruti at hotmail.com (devshruti pahuja)
Date: Tue, 22 Jun 2004 20:52:53 +0000
Subject: [R] Regression Modeling query
Message-ID: <BAY9-F62NLTqzyHuJAu00008109@hotmail.com>

Please ..... this is not a homework problem and i don't want spoon feeding 
but just the right way to approach the problem or the reference from regular 
texts. I'm doing this for my own learning and understanding of regression 
modeling.

I'm familiar with most of the techniques of regression  modeling but had a 
problem with my last project which had 17 predictors and i was able to 
unravel very few interactions despite using most of the techniques that were 
there in some standard textbooks. I want an insight from those who have 
modelled fuzzy data --> logical tables and then tried to get some behavioral 
insights.

I don't mind coding my own R functions and possess a sound familiarity with 
Numerical Linear Algebra and Matrix theory. Secondly, i don't want to dig 
deep into books to find little useful. Any references that you provide will 
be pretty helpful

Please advise

-Dev


>From: Berton Gunter <gunter.berton at gene.com>
>To: devshruti pahuja <devshruti at hotmail.com>
>Subject: Re: [R] Regression Modeling query
>Date: Tue, 22 Jun 2004 13:44:13 -0700
>MIME-Version: 1.0
>X-Sender: "Berton Gunter" <bgunter at smtp.gene.com>
>Received: from compton.gene.com ([192.12.78.250]) by mc8-f14.hotmail.com 
>with Microsoft SMTPSVC(5.0.2195.6824); Tue, 22 Jun 2004 13:45:15 -0700
>Received: from gene.com (dhcp164-33.gene.com [128.137.164.33])by 
>compton.gene.com (Switch-3.1.4/Switch-3.1.0) with ESMTP id 
>i5MKiDr3021487for <devshruti at hotmail.com>; Tue, 22 Jun 2004 13:44:13 -0700 
>(PDT)
>X-Message-Info: JGTYoYF78jHuGzi9i5wyM1lYLSA9WFC7
>Message-ID: <40D89A1D.EEB8EC99 at gene.com>
>Organization: Genentech, Inc.
>X-Mailer: Mozilla 4.78 [en]C-CCK-MCD   (Windows NT 5.0; U)
>X-Accept-Language: en
>References: <BAY9-F42Xng77T4kfjD0003cf53 at hotmail.com>
>Return-Path: bgunter at gene.com
>X-OriginalArrivalTime: 22 Jun 2004 20:45:15.0723 (UTC) 
>FILETIME=[D27351B0:01C45899]
>
>devshruti pahuja wrote:
>
> > Hi All
> >
> > I received a raw data set with one record per tennis player (both male 
>and
> > female) and then i cured it by aggregation i.e by 4 age groups, 2 gender
> > levels and 6 income levels. Gender and Income are categorical variables.
> > Please advise me how to use 'R' to model this data set (Actually, i want 
>to
> > know the right regression technique and steps to do that, including 
>removing
> > outliers at the begining and looking for hidden interactions).
> >
>
>--- in which case, shouldn't we also sign and hand in the homework for you?
>
>-- Bert
>



From roebuck at odin.mdacc.tmc.edu  Tue Jun 22 23:03:29 2004
From: roebuck at odin.mdacc.tmc.edu (Paul Roebuck)
Date: Tue, 22 Jun 2004 16:03:29 -0500 (CDT)
Subject: [R] [Q] GET_DIM() crash on Windows only
Message-ID: <Pine.OSF.4.58.0406221516520.368009@odin.mdacc.tmc.edu>

I have the following contrived code in package format.
On Solaris and Mac OS X, code runs just fine. On Windows,
it crashes the R environment with the "Send Bug Report"
dialog. I tried R 1.8.1 (Win2K) and R 1.9 (WinXP) binaries
with the same result. PCs otherwise appear properly
configured for creating R packages. Anything blatantly
wrong? Suggestions?

TIA



Relevant files from package (getdim):


R/getdim.R
----------
getdim <- function(x)
    .Call("getdim",
          as.matrix(x),
          PACKAGE="getdim");


R/zzz.R
-------
.First.lib <- function(libname, pkgname) {
    if (version$major == 1 && version$minor < 8.1) {
        stop("Requires R 1.8.1 or later")
    }
    library.dynam("getdim", pkgname, libname)
}


src/getdim.c
------------
#include <R.h>
#include <Rdefines.h>
#include <R_ext/PrtUtil.h>

int GetMatrixDimen(SEXP vntX, int *nrow, int *ncol)
{
    SEXP vntXdim;
    int nX;

#ifdef DEBUG_GETDIM
    REprintf("In GetMatrixDimen()...\n");
#endif

/**** Code crashes here on Windoze ******/
    PROTECT(vntXdim = GET_DIM(vntX));
#ifdef DEBUG_GETDIM
    REprintf("\tgot dimension object\n");
#endif

    nX = GET_LENGTH(vntXdim);
#ifdef DEBUG_GETDIM
    REprintf("\tgot length from dimension object\n");
#endif

    if (nX == 2)
    {
        int *piXdim = INTEGER_POINTER(vntXdim);

        *nrow = piXdim[0];
        *ncol = piXdim[1];
    }
    else
    {
        *nrow = -1;
        *ncol = -1;
    }
    UNPROTECT(1);

    return nX;
}


SEXP getdim(SEXP vntX)
{
    SEXP vntOut;
    int m, n;

#ifdef DEBUG_GETDIM
    REprintf("In getdim(x)...\n");
#endif

    if (GetMatrixDimen(vntX, &m, &n) != 2)
    {
        error("'x' is not a two dimensional matrix");
        /*NOTREACHED*/
    }

#ifdef DEBUG_GETDIM
    REprintf("\tm = %d\n", m);
    REprintf("\tn = %d\n", n);
#endif

    PROTECT(vntOut = NEW_INTEGER(2));
    INTEGER(vntOut)[0] = m;
    INTEGER(vntOut)[1] = n;
    UNPROTECT(1);

    return vntOut;
}


tests/getdim.R
--------------
library(getdim)

test.getdim <- function(input, expected) {
    result <- getdim(input);
    identical(all.equal(result, expected), TRUE);
}

mat <- matrix(1:6, 3, 2)
mat.expected <- c(3, 2)
test.getdim(mat, mat.expected)

vec <- 1:6
vec.expected <- c(6, 1)
test.getdim(vec, vec.expected)

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From lauraholt_983 at hotmail.com  Tue Jun 22 23:05:43 2004
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Tue, 22 Jun 2004 16:05:43 -0500
Subject: [R] garch and prediction
Message-ID: <BAY12-F77rblZz02H67000061a6@hotmail.com>

Hi R People:

I used a garch object in the prediction command.  The original time series 
was 1000 entries long.

When I used:

predict(x.arch,n.ahead=2)

the output had 100 values.

I'm confused.

Does anyone have any suggestions, please?  Perhaps garch has its own special 
predict function.

Thanks,
Laura Holt
mailto: lauraholt_983 at hotmail.com



From pwilkinson at videotron.ca  Tue Jun 22 23:13:29 2004
From: pwilkinson at videotron.ca (Peter Wilkinson)
Date: Tue, 22 Jun 2004 17:13:29 -0400
Subject: trying again: [R] write.table when keeping column headers
	(names of Columns in matrix) and row numbers
In-Reply-To: <Pine.LNX.4.44.0406222057250.451-100000@gannet.stats>
References: <6.0.3.0.0.20040622151953.01afaca0@pop.videotron.ca>
	<Pine.LNX.4.44.0406222057250.451-100000@gannet.stats>
Message-ID: <6.0.3.0.0.20040622170513.01b3fbb0@pop.videotron.ca>

Actually .... I was not clear .... I will rephrase

I read what is posted below in the help file already ... I read them often.

****I want to keep the row references****

My question is _when you keep the row names_, why is the command 
write.table implemented in such a way that the column names are left 
shifted starting in the column where the row names are written?  Does it 
not make sense to have the column names where they are supposed to be, 
starting in the second column, if the column names _are_ included in the 
first column?Is there a feature in this that I am missing; why is it 
implemented in this way?

Peter


At 04:00 PM 6/22/2004, Prof Brian Ripley wrote:
> From the details of ?write.table  (and in the Data Import/Export manual
>and in MASS4 ...)
>
>      Normally there is no column name for a column of row names.  If
>      'col.names=NA' a blank column name is added.  This can be used to
>      write CSV files for input to spreadsheets.
>
>What can we possible do to make this more obvious?
>
>On Tue, 22 Jun 2004, Peter Wilkinson wrote:
>
> > When using the write.table (say for a tab delimited file) command on a
> > matrix with Row and Columns, the column headers are always being left
> > shifted into the column where the row numbers are being placed. One can 
> see
> > this when you open up the tab delimited file in excel.
> >
> > Is there a better command for this, or is this supposed to be a 'feature'.
>
>It is a genuinely useful feature, fully documented.
>
>--
>Brian D. Ripley,                  ripley at stats.ox.ac.uk
>Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
>University of Oxford,             Tel:  +44 1865 272861 (self)
>1 South Parks Road,                     +44 1865 272866 (PA)
>Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Jun 22 23:19:56 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 22 Jun 2004 22:19:56 +0100 (BST)
Subject: trying again: [R] write.table when keeping column headers (names
	of Columns in matrix) and row numbers
In-Reply-To: <6.0.3.0.0.20040622170513.01b3fbb0@pop.videotron.ca>
Message-ID: <Pine.LNX.4.44.0406222215420.626-100000@gannet.stats>

On Tue, 22 Jun 2004, Peter Wilkinson wrote:

> Actually .... I was not clear .... I will rephrase
> 
> I read what is posted below in the help file already ... I read them often.

But you have not understood it.  Here is an actual example:

> library(MASS)
> write.table(hills, col.names=NA)
"" "dist" "climb" "time"
"Greenmantle" 2.5 650 16.083
"Carnethy" 6 2500 48.35
"Craig Dunain" 6 900 33.65
"Ben Rha" 7.5 800 45.6
"Ben Lomond" 8 3070 62.267
"Goatfell" 8 2866 73.217
...

As I at least can see, the column names do start in the second column, and
the row names are there.

> ****I want to keep the row references****
> 
> My question is _when you keep the row names_, why is the command 
> write.table implemented in such a way that the column names are left 
> shifted starting in the column where the row names are written? 

That is not true if col.names=NA.

>  Does it 
> not make sense to have the column names where they are supposed to be, 

They are where they are supposed to be, *as documented*

> starting in the second column, if the column names _are_ included in the 
> first column?Is there a feature in this that I am missing; why is it 
> implemented in this way?

So read.table can read the table in what is a standard format.

> Peter
> 
> 
> At 04:00 PM 6/22/2004, Prof Brian Ripley wrote:
> > From the details of ?write.table  (and in the Data Import/Export manual
> >and in MASS4 ...)
> >
> >      Normally there is no column name for a column of row names.  If
> >      'col.names=NA' a blank column name is added.  This can be used to
> >      write CSV files for input to spreadsheets.
> >
> >What can we possible do to make this more obvious?
> >
> >On Tue, 22 Jun 2004, Peter Wilkinson wrote:
> >
> > > When using the write.table (say for a tab delimited file) command on a
> > > matrix with Row and Columns, the column headers are always being left
> > > shifted into the column where the row numbers are being placed. One can 
> > see
> > > this when you open up the tab delimited file in excel.
> > >
> > > Is there a better command for this, or is this supposed to be a 'feature'.
> >
> >It is a genuinely useful feature, fully documented.
> >
> >--
> >Brian D. Ripley,                  ripley at stats.ox.ac.uk
> >Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> >University of Oxford,             Tel:  +44 1865 272861 (self)
> >1 South Parks Road,                     +44 1865 272866 (PA)
> >Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From pwilkinson at videotron.ca  Tue Jun 22 23:47:50 2004
From: pwilkinson at videotron.ca (Peter Wilkinson)
Date: Tue, 22 Jun 2004 17:47:50 -0400
Subject: trying again: [R] write.table when keeping column headers 
	(names of Columns in matrix) and row numbers
In-Reply-To: <Pine.LNX.4.44.0406222215420.626-100000@gannet.stats>
References: <6.0.3.0.0.20040622170513.01b3fbb0@pop.videotron.ca>
	<Pine.LNX.4.44.0406222215420.626-100000@gannet.stats>
Message-ID: <6.0.3.0.0.20040622172938.01b47b68@pop.videotron.ca>


> > My question is _when you keep the row names_, why is the command
> > write.table implemented in such a way that the column names are left
> > shifted starting in the column where the row names are written?
>
>That is not true if col.names=NA.


I get it now ...

I think this is related to the fact that I have not 'named' the rows in the 
table with the names command, as it was intended to be used.



From iyarmulnik at bear.com  Tue Jun 22 23:49:38 2004
From: iyarmulnik at bear.com (Yarmulnik, Igor (Exchange))
Date: Tue, 22 Jun 2004 17:49:38 -0400
Subject: [R] semi-continuous variables in lpSolve package
Message-ID: <427079F58A993A47BC3FCC084E3D57AB03460F06@whexchmb03.bsna.bsroot.bear.com>


    Hi,

     I am working with lpSolve "R" package by Sam Buttrey, buttrey at nps.navy.mil ,
   which is interface to lp_solve linear/integer programming system.
   You can find information about lp_solve at http://groups.yahoo.com/group/lp_solve/   (free registration required).

   lpSolve (R package) supports linear and integer programming but it does not support semi-continuous variables
  and Special Ordered Sets (SOS) which are supported by lp_solve .
 
   I would like to know if there is a way to use semi-continuous variables in lpSolve ?
   Does someone know how? 

   Best regards, 

   Igor  Yarmulnik   iyarmulnik at bear.com
   
 

 



***********************************************************************
Bear Stearns is not responsible for any recommendation, solicitation, 
offer or agreement or any information about any transaction, customer 
account or account activity contained in this communication.



From michele.alzetta at aliceposta.it  Wed Jun 23 00:21:17 2004
From: michele.alzetta at aliceposta.it (Michele Alzetta)
Date: Wed, 23 Jun 2004 00:21:17 +0200
Subject: [R] Mandrake RPM's ... addio ?
Message-ID: <1087942877.9112.45.camel@localhost>

Hallo all;

I've built the usual Mandrake rpm for R 1.9.1, it will be picked up by
Martyn and subsequently propagate through CRAN in the next couple of
days or so. Impatient folk could also download a copy from a link on my
home page ( web.rossoalice.it/michele.alzetta/ ).

This time I've only prepared an rpm for mandrake 10.0 official, and the
reason is soon told: both my home systems have gone over to gentoo for
good, and installing a distro just for the purpose of building an rpm
which I'll never use (and will have little time to check and debug if
things go wrong) is already a rather foolish thing. Installing multiple
versions of the same distro for such a purpose is pure madness ! 

The usual SRPM I've made is of course available so that people with
mandrake 9.* can build their own rpm's. (Unfortunately mandrake cooker's
R is still back at 1.8.1, and in the last few years nobody at mandrake
has shown much real interest in maintaining this package - stable
versions of R have lacked an official R package for quite a while). 

My first build of an R rpm dates back to R 1.3.0 - summer 2001 (didn't
remember that - a quick search and to my surprise I found these old
rpm's still floating around on odd mirrors on the net) - so if nobody
else steps in I guess I'll keep on guaranteeing mdk rpms for the current
version ... but if any active mandrake user would like to try his hand
at this I'm available as a tutor :-)

-- 
Michele Alzetta <michele.alzetta at aliceposta.it>



From andy_liaw at merck.com  Wed Jun 23 01:52:57 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 22 Jun 2004 19:52:57 -0400
Subject: [R] Regression Modeling query
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7F4C@usrymx25.merck.com>

Perhaps you have not read the posting guide, as the footer suggested?
There's a good reason why Bert said what he said.  To tell you the truth, I
was going to reply with something like:  I'd like to know how to get R to
cook dinner for me.  Can anyone help?

You really should learn how to ask questions in ways that will compell
people to respond with useful replies.  You have not told us the goal of the
analysis, the purpose for the model, the context the data came from, etc.,
not to say that the question you asked is so general that it has little to
do with R, other than the fact that you want to do it with R, and R-help is
not really the place to ask general stat questions.  You shouldn't expect
free stat consulting from a software mailing list.

Given the little info that you provided, I can recommend Prof. Harrell's
book "Regression Modelling Strategies".  You should read it over and over
until you can't get it out of your head.

Andy

> From: devshruti pahuja
> 
> Please ..... this is not a homework problem and i don't want 
> spoon feeding 
> but just the right way to approach the problem or the 
> reference from regular 
> texts. I'm doing this for my own learning and understanding 
> of regression 
> modeling.
> 
> I'm familiar with most of the techniques of regression  
> modeling but had a 
> problem with my last project which had 17 predictors and i 
> was able to 
> unravel very few interactions despite using most of the 
> techniques that were 
> there in some standard textbooks. I want an insight from 
> those who have 
> modelled fuzzy data --> logical tables and then tried to get 
> some behavioral 
> insights.
> 
> I don't mind coding my own R functions and possess a sound 
> familiarity with 
> Numerical Linear Algebra and Matrix theory. Secondly, i don't 
> want to dig 
> deep into books to find little useful. Any references that 
> you provide will 
> be pretty helpful
> 
> Please advise
> 
> -Dev
> 
> 
> >From: Berton Gunter <gunter.berton at gene.com>
> >To: devshruti pahuja <devshruti at hotmail.com>
> >Subject: Re: [R] Regression Modeling query
> >Date: Tue, 22 Jun 2004 13:44:13 -0700
> >MIME-Version: 1.0
> >X-Sender: "Berton Gunter" <bgunter at smtp.gene.com>
> >Received: from compton.gene.com ([192.12.78.250]) by 
> mc8-f14.hotmail.com 
> >with Microsoft SMTPSVC(5.0.2195.6824); Tue, 22 Jun 2004 
> 13:45:15 -0700
> >Received: from gene.com (dhcp164-33.gene.com [128.137.164.33])by 
> >compton.gene.com (Switch-3.1.4/Switch-3.1.0) with ESMTP id 
> >i5MKiDr3021487for <devshruti at hotmail.com>; Tue, 22 Jun 2004 
> 13:44:13 -0700 
> >(PDT)
> >X-Message-Info: JGTYoYF78jHuGzi9i5wyM1lYLSA9WFC7
> >Message-ID: <40D89A1D.EEB8EC99 at gene.com>
> >Organization: Genentech, Inc.
> >X-Mailer: Mozilla 4.78 [en]C-CCK-MCD   (Windows NT 5.0; U)
> >X-Accept-Language: en
> >References: <BAY9-F42Xng77T4kfjD0003cf53 at hotmail.com>
> >Return-Path: bgunter at gene.com
> >X-OriginalArrivalTime: 22 Jun 2004 20:45:15.0723 (UTC) 
> >FILETIME=[D27351B0:01C45899]
> >
> >devshruti pahuja wrote:
> >
> > > Hi All
> > >
> > > I received a raw data set with one record per tennis 
> player (both male 
> >and
> > > female) and then i cured it by aggregation i.e by 4 age 
> groups, 2 gender
> > > levels and 6 income levels. Gender and Income are 
> categorical variables.
> > > Please advise me how to use 'R' to model this data set 
> (Actually, i want 
> >to
> > > know the right regression technique and steps to do that, 
> including 
> >removing
> > > outliers at the begining and looking for hidden interactions).
> > >
> >
> >--- in which case, shouldn't we also sign and hand in the 
> homework for you?
> >
> >-- Bert
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ggrothendieck at myway.com  Wed Jun 23 03:04:57 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 23 Jun 2004 01:04:57 +0000 (UTC)
Subject: trying again: [R] write.table when keeping column headers
	=?utf-8?b?CShuYW1lcw==?= of Columns in matrix) and row numbers
References: <6.0.3.0.0.20040622170513.01b3fbb0@pop.videotron.ca>
	<Pine.LNX.4.44.0406222215420.626-100000@gannet.stats>
	<6.0.3.0.0.20040622172938.01b47b68@pop.videotron.ca>
Message-ID: <loom.20040623T030311-778@post.gmane.org>

Peter Wilkinson <pwilkinson <at> videotron.ca> writes:
: > > My question is _when you keep the row names_, why is the command
: > > write.table implemented in such a way that the column names are left
: > > shifted starting in the column where the row names are written?
: >
: >That is not true if col.names=NA.
: 
: I get it now ...
: 
: I think this is related to the fact that I have not 'named' the rows in the 
: table with the names command, as it was intended to be used.

One other thing that might help would be to transfer the data from R to
Excel using HTML rather than .csv .   For an example, see:

  http://tolstoy.newcastle.edu.au/R/help/04/04/0460.html



From cparent at sfu.ca  Wed Jun 23 03:28:18 2004
From: cparent at sfu.ca (Christine Parent)
Date: Tue, 22 Jun 2004 18:28:18 -0700
Subject: [R] Logistic regression: categorical predictor and continuous
	response
Message-ID: <BCFE2AC2.24FD%cparent@sfu.ca>

Hi all,

I am fairly new to R, and not a stats expert. I am having trouble finding
information to help me analyzing my set of data.

I got morphological data on land snail shells of different species from the
Galapagos Islands. Each island has up to 6 different vegetation zones, and
snails have adapted to up to 4 of these zones on each of the major islands.

I would like to test if there is any correlation between the morphology of
the shells which is continuous (I ran a PCA analysis on the morphological
data, and the variation can be parted into two main principal components:
PC1 for size and PC2 for shape) and the vegetation zone where these snails
are found (which is a categorical predictor variable).

I believe I should run a logistic regression on this, but all I could find
in R was an analysis where the predictor was continuous and the response was
categorical. Could anyone direct me to the relevant ref or how to write the
proper script to run a logistic regression analysis where the predictor
would be categorical and the response continuous?

Any help or advice on this would be greatly appreciated!

Many thanks,

Christine
  
Christine Parent
Ph.D. candidate
Department of Biological Sciences
Simon Fraser University
cparent at sfu.ca



From yanyu at CS.UCLA.EDU  Wed Jun 23 03:44:31 2004
From: yanyu at CS.UCLA.EDU (Yan Yu)
Date: Tue, 22 Jun 2004 18:44:31 -0700 (PDT)
Subject: [R] SOS:) help on install package
Message-ID: <Pine.GSO.4.58.0406221842360.27587@panther.cs.ucla.edu>

HI,
i tried to install package by using
update.packages( "locfit" )

I got the following error:
trying URL `http://cran.r-project.org/src/contrib/PACKAGES'
Content type `text/plain; charset=iso-8859-1' length 172492 bytes
opened URL
.......... .......... .......... .......... ..........
.......... .......... .......... .......... ..........
.......... .......... .......... .......... ..........
.......... ........
downloaded 168Kb

Error in "colnames<-"(`*tmp*`, value = c("Package", "LibPath", pkgFlds)) :
        attempt to set colnames on object with less than two dimensions

IS there anyone know what is wrong, and
what other commands to use to install a package in R?

many thanks,
yan



From jinss at hkusua.hku.hk  Wed Jun 23 03:46:29 2004
From: jinss at hkusua.hku.hk (Jin Shusong)
Date: Wed, 23 Jun 2004 09:46:29 +0800
Subject: [R] ts & daily timeseries
In-Reply-To: <40D1B1D200014863@ims3e.cp.tin.it>
References: <40D1B1D200014863@ims3e.cp.tin.it>
Message-ID: <20040623014629.GA2559@S77.hku.hk>

On Tue, Jun 22, 2004 at 06:22:40PM +0200, v.demartino2 at virgilio.it wrote:
> I have defined a daily timeseries for the 365 days of 2003 issuing:
> 
> myts = ts(dati[,2:10],frequency=365,)
> > myts
> Time Series:
> Start = c(1, 1) 
> End = c(1, 365) 
> Frequency = 365 
>   
> and 
> 
> mytime =  as.POSIXct(strptime(as.character(dati[,1]),format="%Y-%m-%d"))
> 
> contains the dates from "2003-01-01" to "2003-12-31"
> 
> How can I combine mytime and myts in order to list the timeseries according
> to the more natural
> 
> 2003-01-01    xxxxx  xxxxx   xxxxx ......
> 2003-01-02    xxxxx  xxxxx   xxxxx ......
> ...........
> 2003-12-31    xxxxx  xxxxx   xxxxx ......
> 
> I've googled on this subject to no avail.
> 
> Thanks
> Vittorio
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
Dear Vittorio,

  I think that the its package can help you.
  Good luck.

      Jin



From andy_liaw at merck.com  Wed Jun 23 03:55:25 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 22 Jun 2004 21:55:25 -0400
Subject: [R] SOS:) help on install package
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7F4D@usrymx25.merck.com>

This was asked recently on R-help.  You are calling update.packages() wrong:
The first argument it expects is lib.loc, which is the directory where the
packages are to be installed.  (You can see that either from
?update.packages or args(update.packages).)  You shouldn't be calling
update.packages() with package names.

install.packages("locfit") is probably what you really want to do.

Andy

> From: Yan Yu
> 
> HI,
> i tried to install package by using
> update.packages( "locfit" )
> 
> I got the following error:
> trying URL `http://cran.r-project.org/src/contrib/PACKAGES'
> Content type `text/plain; charset=iso-8859-1' length 172492 bytes
> opened URL
> .......... .......... .......... .......... ..........
> .......... .......... .......... .......... ..........
> .......... .......... .......... .......... ..........
> .......... ........
> downloaded 168Kb
> 
> Error in "colnames<-"(`*tmp*`, value = c("Package", 
> "LibPath", pkgFlds)) :
>         attempt to set colnames on object with less than two 
> dimensions
> 
> IS there anyone know what is wrong, and
> what other commands to use to install a package in R?
> 
> many thanks,
> yan
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From yanyu at CS.UCLA.EDU  Wed Jun 23 04:10:24 2004
From: yanyu at CS.UCLA.EDU (Yan Yu)
Date: Tue, 22 Jun 2004 19:10:24 -0700 (PDT)
Subject: [R] SOS:) help on install package
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7F4D@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF7F4D@usrymx25.merck.com>
Message-ID: <Pine.GSO.4.58.0406221906361.27587@panther.cs.ucla.edu>

i seem to have some progress by using what you suggested, thanks!
i got the following:
Warning message:
argument `lib' is missing: using /usr/local/lib/R/site-library in:
install.packages("locfit")

i wonder what is wrong?
thanks,
yan


On Tue, 22 Jun 2004, Liaw, Andy wrote:

> This was asked recently on R-help.  You are calling update.packages() wrong:
> The first argument it expects is lib.loc, which is the directory where the
> packages are to be installed.  (You can see that either from
> ?update.packages or args(update.packages).)  You shouldn't be calling
> update.packages() with package names.
>
> install.packages("locfit") is probably what you really want to do.
>
> Andy
>
> > From: Yan Yu
> >
> > HI,
> > i tried to install package by using
> > update.packages( "locfit" )
> >
> > I got the following error:
> > trying URL `http://cran.r-project.org/src/contrib/PACKAGES'
> > Content type `text/plain; charset=iso-8859-1' length 172492 bytes
> > opened URL
> > .......... .......... .......... .......... ..........
> > .......... .......... .......... .......... ..........
> > .......... .......... .......... .......... ..........
> > .......... ........
> > downloaded 168Kb
> >
> > Error in "colnames<-"(`*tmp*`, value = c("Package",
> > "LibPath", pkgFlds)) :
> >         attempt to set colnames on object with less than two
> > dimensions
> >
> > IS there anyone know what is wrong, and
> > what other commands to use to install a package in R?
> >
> > many thanks,
> > yan
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From andy_liaw at merck.com  Wed Jun 23 04:14:54 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 22 Jun 2004 22:14:54 -0400
Subject: [R] SOS:) help on install package
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7F4E@usrymx25.merck.com>

Please, do read the help pages of functions you are trying to use.  If you
don't tell install.packages() where to install the package, it will try to
guess where to put it, and then warn you about it.

Andy

> From: Yan Yu [mailto:yanyu at CS.UCLA.EDU] 
> 
> i seem to have some progress by using what you suggested, thanks!
> i got the following:
> Warning message:
> argument `lib' is missing: using /usr/local/lib/R/site-library in:
> install.packages("locfit")
> 
> i wonder what is wrong?
> thanks,
> yan
> 
> 
> On Tue, 22 Jun 2004, Liaw, Andy wrote:
> 
> > This was asked recently on R-help.  You are calling 
> update.packages() wrong:
> > The first argument it expects is lib.loc, which is the 
> directory where the
> > packages are to be installed.  (You can see that either from
> > ?update.packages or args(update.packages).)  You shouldn't 
> be calling
> > update.packages() with package names.
> >
> > install.packages("locfit") is probably what you really want to do.
> >
> > Andy
> >
> > > From: Yan Yu
> > >
> > > HI,
> > > i tried to install package by using
> > > update.packages( "locfit" )
> > >
> > > I got the following error:
> > > trying URL `http://cran.r-project.org/src/contrib/PACKAGES'
> > > Content type `text/plain; charset=iso-8859-1' length 172492 bytes
> > > opened URL
> > > .......... .......... .......... .......... ..........
> > > .......... .......... .......... .......... ..........
> > > .......... .......... .......... .......... ..........
> > > .......... ........
> > > downloaded 168Kb
> > >
> > > Error in "colnames<-"(`*tmp*`, value = c("Package",
> > > "LibPath", pkgFlds)) :
> > >         attempt to set colnames on object with less than two
> > > dimensions
> > >
> > > IS there anyone know what is wrong, and
> > > what other commands to use to install a package in R?
> > >
> > > many thanks,
> > > yan
> > >
> > > ______________________________________________
> > > R-help at stat.math.ethz.ch mailing list
> > > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > > PLEASE do read the posting guide!
> > > http://www.R-project.org/posting-guide.html
> > >
> > >
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> >
> 
>



From bdanwood at politics.tamu.edu  Wed Jun 23 06:05:17 2004
From: bdanwood at politics.tamu.edu (B. Dan Wood)
Date: Tue, 22 Jun 2004 23:05:17 -0500
Subject: [R] restricted glm estimation
Message-ID: <000001c458d7$4bcdadc0$6501a8c0@DansLaptop>

R users,

 

Is there a package or procedure that implements restricted estimation using
glm? I need to do a poisson regression under glm with one parameter fixed at
a specific value. Just at a glance, I don't see a way to do it using glm.

 

Thanks. 

B. Dan Wood

 

 

 


From wang at galton.uchicago.edu  Wed Jun 23 06:14:57 2004
From: wang at galton.uchicago.edu (Yong Wang)
Date: Tue, 22 Jun 2004 23:14:57 -0500 (CDT)
Subject: [R] a question on  useage of "table"
In-Reply-To: <200406221002.i5MA1ieV023063@hypatia.math.ethz.ch>
References: <200406221002.i5MA1ieV023063@hypatia.math.ethz.ch>
Message-ID: <Pine.LNX.4.58.0406222300240.5474@aitken.uchicago.edu>

Hi, all
the question is as follows
> v
 [1] 1 2 3 4 5 2 3 4 5 6
> sv<-sort(table(v))
> sv
v
1 6 2 3 4 5 
1 1 2 2 2 2 


what is the easy and quick way to extract the numerical vector 
(1,6,2,3,4,5)
from sv?

thank you in advance for any suggestion.
regards
y.w



From astephen at efs.mq.edu.au  Wed Jun 23 06:39:10 2004
From: astephen at efs.mq.edu.au (Alec Stephenson)
Date: Wed, 23 Jun 2004 14:39:10 +1000
Subject: [R] a question on  useage of "table"
Message-ID: <s0d9961e.013@efs04.efs.mq.edu.au>

as.numeric(names(sv)) is one way.

>>> Yong Wang <wang at galton.uchicago.edu> 06/23/04 02:14pm >>>
Hi, all
the question is as follows
> v
 [1] 1 2 3 4 5 2 3 4 5 6
> sv<-sort(table(v))
> sv
v
1 6 2 3 4 5 
1 1 2 2 2 2 


what is the easy and quick way to extract the numerical vector 
(1,6,2,3,4,5)
from sv?

thank you in advance for any suggestion.
regards
y.w

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From d.scott at auckland.ac.nz  Wed Jun 23 06:54:52 2004
From: d.scott at auckland.ac.nz (David Scott)
Date: Wed, 23 Jun 2004 16:54:52 +1200 (NZST)
Subject: [R] a question on  useage of "table"
In-Reply-To: <Pine.LNX.4.58.0406222300240.5474@aitken.uchicago.edu>
Message-ID: <Pine.LNX.4.44.0406231654140.17947-100000@stat71.stat.auckland.ac.nz>

On Tue, 22 Jun 2004, Yong Wang wrote:

> Hi, all
> the question is as follows
> > v
>  [1] 1 2 3 4 5 2 3 4 5 6
> > sv<-sort(table(v))
> > sv
> v
> 1 6 2 3 4 5 
> 1 1 2 2 2 2 
> 
> 
> what is the easy and quick way to extract the numerical vector 
> (1,6,2,3,4,5)
> from sv?
> 

> as.numeric(names(sv))
[1] 1 6 2 3 4 5


David Scott





_________________________________________________________________
David Scott	Department of Statistics, Tamaki Campus
		The University of Auckland, PB 92019
		Auckland	NEW ZEALAND
Phone: +64 9 373 7599 ext 86830		Fax: +64 9 373 7000
Email:	d.scott at auckland.ac.nz 


Graduate Officer, Department of Statistics



From ripley at stats.ox.ac.uk  Wed Jun 23 08:32:16 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 23 Jun 2004 07:32:16 +0100 (BST)
Subject: [R] restricted glm estimation
In-Reply-To: <000001c458d7$4bcdadc0$6501a8c0@DansLaptop>
Message-ID: <Pine.LNX.4.44.0406230729500.1592-100000@gannet.stats>

On Tue, 22 Jun 2004, B. Dan Wood wrote:

> Is there a package or procedure that implements restricted estimation using
> glm? I need to do a poisson regression under glm with one parameter fixed at
> a specific value. Just at a glance, I don't see a way to do it using glm.

Use offset(), which allows you to enter a term with coefficient fixed at 
one (and hence at any value by including the fixed value in the offset).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Jun 23 08:26:40 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 23 Jun 2004 07:26:40 +0100 (BST)
Subject: [R] Logistic regression: categorical predictor and continuous
	response
In-Reply-To: <BCFE2AC2.24FD%cparent@sfu.ca>
Message-ID: <Pine.LNX.4.44.0406230719360.1592-100000@gannet.stats>

You can a logistic non-linear regresion with a continuous response -- see 
SSlogis in package stats (using nls).

However, I think

> I believe I should run a logistic regression on this

is the problem.  You have six groups of multivariate observations and you
want to know if and how they differ by group(?)  If so the usual
approaches are

- linear and quadratic discriminant analysis, very sensitive to joint 
normality.  (lda and qda in package MASS),

- logistic discrimination.  As you have > 2 groups, use multinom in 
package nnet.

If you were here I would be directing you to our statistics consulting 
service -- do you have a local source of statistics help?


On Tue, 22 Jun 2004, Christine Parent wrote:

> Hi all,
> 
> I am fairly new to R, and not a stats expert. I am having trouble finding
> information to help me analyzing my set of data.
> 
> I got morphological data on land snail shells of different species from the
> Galapagos Islands. Each island has up to 6 different vegetation zones, and
> snails have adapted to up to 4 of these zones on each of the major islands.
> 
> I would like to test if there is any correlation between the morphology of
> the shells which is continuous (I ran a PCA analysis on the morphological
> data, and the variation can be parted into two main principal components:
> PC1 for size and PC2 for shape) and the vegetation zone where these snails
> are found (which is a categorical predictor variable).
> 
> I believe I should run a logistic regression on this, but all I could find
> in R was an analysis where the predictor was continuous and the response was
> categorical. Could anyone direct me to the relevant ref or how to write the
> proper script to run a logistic regression analysis where the predictor
> would be categorical and the response continuous?
> 
> Any help or advice on this would be greatly appreciated!
> 
> Many thanks,
> 
> Christine
>   
> Christine Parent
> Ph.D. candidate
> Department of Biological Sciences
> Simon Fraser University
> cparent at sfu.ca

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Wed Jun 23 09:31:41 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 23 Jun 2004 09:31:41 +0200
Subject: [R] [Q] GET_DIM() crash on Windows only
In-Reply-To: <Pine.OSF.4.58.0406221516520.368009@odin.mdacc.tmc.edu>
References: <Pine.OSF.4.58.0406221516520.368009@odin.mdacc.tmc.edu>
Message-ID: <40D931DD.7000406@statistik.uni-dortmund.de>

Paul Roebuck wrote:

> I have the following contrived code in package format.
> On Solaris and Mac OS X, code runs just fine. On Windows,
> it crashes the R environment with the "Send Bug Report"
> dialog. I tried R 1.8.1 (Win2K) and R 1.9 (WinXP) binaries
> with the same result. PCs otherwise appear properly
> configured for creating R packages. Anything blatantly
> wrong? Suggestions?


Works for me (R-1.9.1, WinNT4.0), even with gctorture(TRUE).
Did you use the recommended compiler and tools?

Uwe Ligges


> TIA
> 
> 
> 
> Relevant files from package (getdim):
> 
> 
> R/getdim.R
> ----------
> getdim <- function(x)
>     .Call("getdim",
>           as.matrix(x),
>           PACKAGE="getdim");
> 
> 
> R/zzz.R
> -------
> .First.lib <- function(libname, pkgname) {
>     if (version$major == 1 && version$minor < 8.1) {
>         stop("Requires R 1.8.1 or later")
>     }
>     library.dynam("getdim", pkgname, libname)
> }
> 
> 
> src/getdim.c
> ------------
> #include <R.h>
> #include <Rdefines.h>
> #include <R_ext/PrtUtil.h>
> 
> int GetMatrixDimen(SEXP vntX, int *nrow, int *ncol)
> {
>     SEXP vntXdim;
>     int nX;
> 
> #ifdef DEBUG_GETDIM
>     REprintf("In GetMatrixDimen()...\n");
> #endif
> 
> /**** Code crashes here on Windoze ******/
>     PROTECT(vntXdim = GET_DIM(vntX));
> #ifdef DEBUG_GETDIM
>     REprintf("\tgot dimension object\n");
> #endif
> 
>     nX = GET_LENGTH(vntXdim);
> #ifdef DEBUG_GETDIM
>     REprintf("\tgot length from dimension object\n");
> #endif
> 
>     if (nX == 2)
>     {
>         int *piXdim = INTEGER_POINTER(vntXdim);
> 
>         *nrow = piXdim[0];
>         *ncol = piXdim[1];
>     }
>     else
>     {
>         *nrow = -1;
>         *ncol = -1;
>     }
>     UNPROTECT(1);
> 
>     return nX;
> }
> 
> 
> SEXP getdim(SEXP vntX)
> {
>     SEXP vntOut;
>     int m, n;
> 
> #ifdef DEBUG_GETDIM
>     REprintf("In getdim(x)...\n");
> #endif
> 
>     if (GetMatrixDimen(vntX, &m, &n) != 2)
>     {
>         error("'x' is not a two dimensional matrix");
>         /*NOTREACHED*/
>     }
> 
> #ifdef DEBUG_GETDIM
>     REprintf("\tm = %d\n", m);
>     REprintf("\tn = %d\n", n);
> #endif
> 
>     PROTECT(vntOut = NEW_INTEGER(2));
>     INTEGER(vntOut)[0] = m;
>     INTEGER(vntOut)[1] = n;
>     UNPROTECT(1);
> 
>     return vntOut;
> }
> 
> 
> tests/getdim.R
> --------------
> library(getdim)
> 
> test.getdim <- function(input, expected) {
>     result <- getdim(input);
>     identical(all.equal(result, expected), TRUE);
> }
> 
> mat <- matrix(1:6, 3, 2)
> mat.expected <- c(3, 2)
> test.getdim(mat, mat.expected)
> 
> vec <- 1:6
> vec.expected <- c(6, 1)
> test.getdim(vec, vec.expected)
> 
> ----------------------------------------------------------
> SIGSIG -- signature too long (core dumped)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From angel_lul at hotmail.com  Wed Jun 23 09:58:31 2004
From: angel_lul at hotmail.com (Angel Lopez)
Date: Wed, 23 Jun 2004 09:58:31 +0200
Subject: [R] chronological clustering
Message-ID: <40D93827.10108@hotmail.com>

Does anybody know of any R functions to perform chronological clustering 
as explained in:

Legendre, P., S. Dallot & L. Legendre. 1985. Succession of species 
within a community: chronological clustering, with applications to 
marine and freshwater zooplankton. American Naturalist 125: 257-288.

http://www.fas.umontreal.ca/BIOL/legendre/reprints/succession_of_species.pdf

Thanks,
Angel



From lauraholt_983 at hotmail.com  Wed Jun 23 10:34:38 2004
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Wed, 23 Jun 2004 03:34:38 -0500
Subject: [R] GARCH and forecasting
Message-ID: <BAY12-F65EjVXGgaH8N00007835@hotmail.com>

Dear R People:

Is there a way to forecast with GARCH modeling as found in tseries, please?

When I use the predict command, I get an output of length 100, regardless of 
what I put in the n.ahead steps.

R Version 1.9.0

Thanks in advance.
Sincerely,
Laura
mailto: lauraholt_983 at hotmail.com


download! http://toolbar.msn.click-url.com/go/onm00200413ave/direct/01/



From ftcheung at hkusua.hku.hk  Wed Jun 23 10:50:54 2004
From: ftcheung at hkusua.hku.hk (Frankie Cheung)
Date: Wed, 23 Jun 2004 16:50:54 +0800 (HKT)
Subject: [R] R 1.9.1 compilation error
Message-ID: <Pine.GSO.4.10.10406231629001.7233-100000@hkusua>

Dear Sir/Madam,

I encounter some problem duuring compilation of R 1.9.1 on AIX 5.1, after
running "./configure" then I type "make" to compile:

# make
.....
gcc -I../../src/extra/zlib -I../../src/extra/bzip2 -I../../src/extra/pcre
-I. -I../../src/include -I../../src/include -I/usr/local/include
-DHAVE_CONFIG_H -mno-fp-in-toc  -g -O2 -c registration.c -o registration.o
g77   -g -O2 -c xxxpr.f -o xxxpr.o
gcc -Wl,-bdynamic -Wl,-bE:../../etc/R.exp -Wl,-bM:SRE -L/usr/local/lib -o
R.bin  CConverters.o Rdynload.o RNG.o apply.o arithmetic.o apse.o array.o
attrib.o base.o bind.o builtin.o character.o coerce.o colors.o complex.o
connections.o context.o cov.o cum.o dcf.o datetime.o debug.o devPS.o
devPicTeX.o deparse.o deriv.o devices.o dotcode.o dounzip.o dstruct.o
duplicate.o engine.o envir.o errors.o eval.o format.o fourier.o gram.o
gram-ex.o graphics.o identical.o internet.o iosupport.o lapack.o list.o
logic.o main.o mapply.o match.o memory.o model.o names.o objects.o optim.o
optimize.o options.o par.o paste.o pcre.o platform.o plot.o plot3d.o
plotmath.o print.o printarray.o printvector.o printutils.o qsort.o
random.o regex.o relop.o saveload.o scan.o seq.o serialize.o size.o sort.o
source.o split.o sprintf.o subassign.o subscript.o subset.o summary.o
unique.o util.o version.o vfonts.o registration.o xxxpr.o
../unix/libunix.a ../appl/libappl.a ../nmath/libnmath.a   -L/usr/local/lib
-L/usr/local/lib/gcc-lib/powerpc-ibm-aix5.1.0.0/3.3.3
-L/usr/local/lib/gcc-lib/powerpc-ibm-aix5.1.0.0/3.3.3/../../../../powerpc-ibm-aix5.1.0.0/lib
-L/usr/local/lib/gcc-lib/powerpc-ibm-aix5.1.0.0/3.3.3/../../.. -lfrtbegin
-lg2c -lm -lgcc_s
/usr/local/lib/gcc-lib/powerpc-ibm-aix5.1.0.0/3.3.3/libgcc.a -lg
/lib/crt0.o ../extra/zlib/libz.a ../extra/bzip2/libbz2.a
../extra/pcre/libpcre.a -ldl -lm -lc
/usr/local/bin/ld: target dynamic not found
collect2: ld returned 1 exit status
make[3]: *** [R.bin] Error 1
make[3]: Leaving directory `/d1/ftcheung_GoIn/tmp/R-1.9.1/src/main'
make[2]: *** [R] Error 2
make[2]: Leaving directory `/d1/ftcheung_GoIn/tmp/R-1.9.1/src/main'
make[1]: *** [R] Error 1
make[1]: Leaving directory `/d1/ftcheung_GoIn/tmp/R-1.9.1/src'
make: *** [R] Error 1

I've tried to use AIX xlc compiler and GCC compiler (v2.95 and v3.3.3) but
all of them result to the same error message shown above. Can anyone give
me some hint of how to solve it?

with regards,
Frankie Cheung 
The University of Hong Kong



From a.trapletti at bluewin.ch  Wed Jun 23 10:52:47 2004
From: a.trapletti at bluewin.ch (Adrian Trapletti)
Date: Wed, 23 Jun 2004 10:52:47 +0200
Subject: [R] GARCH and forecasting
In-Reply-To: <BAY12-F65EjVXGgaH8N00007835@hotmail.com>
References: <BAY12-F65EjVXGgaH8N00007835@hotmail.com>
Message-ID: <40D944DF.7090300@bluewin.ch>

Laura Holt wrote:

> Dear R People:
>
> Is there a way to forecast with GARCH modeling as found in tseries, 
> please?
>
> When I use the predict command, I get an output of length 100, 
> regardless of what I put in the n.ahead steps.

n.ahead is no argument to predict.garch. See ?predict.garch. Only 
one-step ahead predictions are possible. For a prediction with no target 
observation available, use genuine = T.

Best
Adrian

>
> R Version 1.9.0
>
> Thanks in advance.
> Sincerely,
> Laura
> mailto: lauraholt_983 at hotmail.com
>
> _________________________________________________________________
> MSN Toolbar provides one-click access to Hotmail from any Web page - 
> FREE download! 
> http://toolbar.msn.click-url.com/go/onm00200413ave/direct/01/
>
>



From monica.palaseanu-lovejoy at stud.man.ac.uk  Wed Jun 23 11:07:28 2004
From: monica.palaseanu-lovejoy at stud.man.ac.uk (Monica Palaseanu-Lovejoy)
Date: Wed, 23 Jun 2004 10:07:28 +0100
Subject: [R] Automatic routine - help
Message-ID: <E1Bd3jE-0009WH-Vy@serenity.mcc.ac.uk>

Hi,

I would like to write a little automatic routine in R, but i am a too 
much of a beginner for that. I will appreciate any help regarding this 
particular problem.

Let?s suppose I have a data.frame with j columns (from 1 to n) and i 
rows (from 1 to p). I would like to write a procedure which reads 
every column j (j from 1 to n) and compare each value with the 
interval [0,1]. If z(i,j) is less than 0, then replace z(i,j) with 0. If z(i,j) 
is greater than 1, then replace z(i,j) with 1. If z(i,j) is inside the 
interval [0,1] then don?t change. In the end I would like to have a 
new data.frame with the new values. 

I am not sure how complicated or long such a procedure might be, 
so I will be very grateful for any help.

Thank you in advance,

Monica



From ripley at stats.ox.ac.uk  Wed Jun 23 11:11:01 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 23 Jun 2004 10:11:01 +0100 (BST)
Subject: [R] R 1.9.1 compilation error (on AIX 5.1)
In-Reply-To: <Pine.GSO.4.10.10406231629001.7233-100000@hkusua>
Message-ID: <Pine.LNX.4.44.0406230954330.16683-100000@gannet.stats>

We do have alpha/beta test periods for new releases of R, so would anyone 
who did test AIX please confirm that they did succeed.  (No one reported 
an error, including yourself: are any AIX users interested in helping 
having R available for AIX?)   It is much better to have such reports 
during alpha test, or at least beta test.

On Wed, 23 Jun 2004, Frankie Cheung wrote:

> Dear Sir/Madam,
> 
> I encounter some problem duuring compilation of R 1.9.1 on AIX 5.1, after
> running "./configure" then I type "make" to compile:
> 
> # make
> .....
> gcc -I../../src/extra/zlib -I../../src/extra/bzip2 -I../../src/extra/pcre
> -I. -I../../src/include -I../../src/include -I/usr/local/include
> -DHAVE_CONFIG_H -mno-fp-in-toc  -g -O2 -c registration.c -o registration.o
> g77   -g -O2 -c xxxpr.f -o xxxpr.o
> gcc -Wl,-bdynamic -Wl,-bE:../../etc/R.exp -Wl,-bM:SRE -L/usr/local/lib -o
> R.bin  CConverters.o Rdynload.o RNG.o apply.o arithmetic.o apse.o array.o
> attrib.o base.o bind.o builtin.o character.o coerce.o colors.o complex.o
> connections.o context.o cov.o cum.o dcf.o datetime.o debug.o devPS.o
> devPicTeX.o deparse.o deriv.o devices.o dotcode.o dounzip.o dstruct.o
> duplicate.o engine.o envir.o errors.o eval.o format.o fourier.o gram.o
> gram-ex.o graphics.o identical.o internet.o iosupport.o lapack.o list.o
> logic.o main.o mapply.o match.o memory.o model.o names.o objects.o optim.o
> optimize.o options.o par.o paste.o pcre.o platform.o plot.o plot3d.o
> plotmath.o print.o printarray.o printvector.o printutils.o qsort.o
> random.o regex.o relop.o saveload.o scan.o seq.o serialize.o size.o sort.o
> source.o split.o sprintf.o subassign.o subscript.o subset.o summary.o
> unique.o util.o version.o vfonts.o registration.o xxxpr.o
> ../unix/libunix.a ../appl/libappl.a ../nmath/libnmath.a   -L/usr/local/lib
> -L/usr/local/lib/gcc-lib/powerpc-ibm-aix5.1.0.0/3.3.3
> -L/usr/local/lib/gcc-lib/powerpc-ibm-aix5.1.0.0/3.3.3/../../../../powerpc-ibm-aix5.1.0.0/lib
> -L/usr/local/lib/gcc-lib/powerpc-ibm-aix5.1.0.0/3.3.3/../../.. -lfrtbegin
> -lg2c -lm -lgcc_s
> /usr/local/lib/gcc-lib/powerpc-ibm-aix5.1.0.0/3.3.3/libgcc.a -lg
> /lib/crt0.o ../extra/zlib/libz.a ../extra/bzip2/libbz2.a
> ../extra/pcre/libpcre.a -ldl -lm -lc
> /usr/local/bin/ld: target dynamic not found
> collect2: ld returned 1 exit status
> make[3]: *** [R.bin] Error 1
> make[3]: Leaving directory `/d1/ftcheung_GoIn/tmp/R-1.9.1/src/main'
> make[2]: *** [R] Error 2
> make[2]: Leaving directory `/d1/ftcheung_GoIn/tmp/R-1.9.1/src/main'
> make[1]: *** [R] Error 1
> make[1]: Leaving directory `/d1/ftcheung_GoIn/tmp/R-1.9.1/src'
> make: *** [R] Error 1
> 
> I've tried to use AIX xlc compiler and GCC compiler (v2.95 and v3.3.3) but
> all of them result to the same error message shown above. Can anyone give
> me some hint of how to solve it?

You appear not to be using the standard loader (`/usr/local/bin/ld').  
Please set MAIN_LDFLAGS (ideally in config.site before configure, but you
can edit Makeconf now) to whatever your loader needs.  If this is GNU ld
then I guess you need -Wl,--export-dynamic.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ligges at statistik.uni-dortmund.de  Wed Jun 23 11:14:42 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 23 Jun 2004 11:14:42 +0200
Subject: [R] Automatic routine - help
In-Reply-To: <E1Bd3jE-0009WH-Vy@serenity.mcc.ac.uk>
References: <E1Bd3jE-0009WH-Vy@serenity.mcc.ac.uk>
Message-ID: <40D94A02.9060102@statistik.uni-dortmund.de>

Monica Palaseanu-Lovejoy wrote:

> Hi,
> 
> I would like to write a little automatic routine in R, but i am a too 
> much of a beginner for that. I will appreciate any help regarding this 
> particular problem.
> 
> Let?s suppose I have a data.frame with j columns (from 1 to n) and i 
> rows (from 1 to p). I would like to write a procedure which reads 
> every column j (j from 1 to n) and compare each value with the 
> interval [0,1]. If z(i,j) is less than 0, then replace z(i,j) with 0. If z(i,j) 
> is greater than 1, then replace z(i,j) with 1. If z(i,j) is inside the 
> interval [0,1] then don?t change. In the end I would like to have a 
> new data.frame with the new values. 
> 
> I am not sure how complicated or long such a procedure might be, 
> so I will be very grateful for any help.
> 
> Thank you in advance,
> 
> Monica


If all columns of your data.frame are numeric:

z[z<0] <- 0
z[z>1] <- 1

Uwe Ligges



From ripley at stats.ox.ac.uk  Wed Jun 23 11:15:54 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 23 Jun 2004 10:15:54 +0100 (BST)
Subject: [R] Automatic routine - help
In-Reply-To: <E1Bd3jE-0009WH-Vy@serenity.mcc.ac.uk>
Message-ID: <Pine.LNX.4.44.0406231013090.16683-100000@gannet.stats>

How about

newDF <- DF
newDF[] <- lapply(DF, function(x) pmax(0, pmin(x, 1)))

as in

> DF <- data.frame(x=rnorm(5), y=rnorm(5))
> newDF <- DF
> newDF[] <- lapply(DF, function(x) pmax(0, pmin(x, 1)))
> newDF
           x         y
1 0.31872426 1.0000000
2 1.00000000 0.0000000
3 1.00000000 0.4510969
4 0.04753697 1.0000000
5 0.89016978 0.0000000


On Wed, 23 Jun 2004, Monica Palaseanu-Lovejoy wrote:

> Hi,
> 
> I would like to write a little automatic routine in R, but i am a too 
> much of a beginner for that. I will appreciate any help regarding this 
> particular problem.
> 
> Let?s suppose I have a data.frame with j columns (from 1 to n) and i 
> rows (from 1 to p). I would like to write a procedure which reads 
> every column j (j from 1 to n) and compare each value with the 
> interval [0,1]. If z(i,j) is less than 0, then replace z(i,j) with 0. If z(i,j) 
> is greater than 1, then replace z(i,j) with 1. If z(i,j) is inside the 
> interval [0,1] then don?t change. In the end I would like to have a 
> new data.frame with the new values. 
> 
> I am not sure how complicated or long such a procedure might be, 
> so I will be very grateful for any help.
> 
> Thank you in advance,
> 
> Monica
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From agustin.perez at umh.es  Wed Jun 23 11:19:50 2004
From: agustin.perez at umh.es (Perez Martin, Agustin)
Date: Wed, 23 Jun 2004 11:19:50 +0200
Subject: [R] legend
Message-ID: <5AFDDD57E2771B409224CD858CC6DE0D02DAB464@mailer-e051.umh.es>

DeaR UseRs:

I want to put a legend in my plot. In the first line of the legend I want to
put a box filled but in the second one I would like to put a lty=2

Of course it must appear with different colors.

Thanks.



From Philippe.Hupe at curie.fr  Wed Jun 23 11:23:04 2004
From: Philippe.Hupe at curie.fr (=?ISO-8859-1?Q?Philippe_Hup=E9?=)
Date: Wed, 23 Jun 2004 11:23:04 +0200
Subject: [R] Colored label on dendrogram
Message-ID: <40D94BF8.2050107@curie.fr>

Hi,

Can someone tell me how to put labels of differente colors on a tree 
dendrogram.

Thanks

-- 
Philippe Hup??
UMR 144 - Service Bioinformatique
Institut Curie
Laboratoire de Transfert (4??me ??tage)
26 rue d'Ulm
75005 Paris - France
 	
Email :  Philippe.Hupe at curie.fr
T??l :	 +33 (0)1 44 32 42 75
Fax :  	 +33 (0)1 42 34 65 28



From k.wang at auckland.ac.nz  Wed Jun 23 11:25:50 2004
From: k.wang at auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Wed, 23 Jun 2004 21:25:50 +1200
Subject: [R] legend
References: <5AFDDD57E2771B409224CD858CC6DE0D02DAB464@mailer-e051.umh.es>
Message-ID: <005d01c45904$12a8cc50$6633d882@stat.auckland.ac.nz>

Hi,

----- Original Message ----- 
From: "Perez Martin, Agustin" <agustin.perez at umh.es>
To: "lista R help (E-mail)" <r-help at stat.math.ethz.ch>
Sent: Wednesday, June 23, 2004 9:19 PM
Subject: [R] legend


> DeaR UseRs:
>
> I want to put a legend in my plot. In the first line of the legend I want
to
> put a box filled but in the second one I would like to put a lty=2

Have you looked at ?lengend

It's got some good examples.

Kevin



From monica.palaseanu-lovejoy at stud.man.ac.uk  Wed Jun 23 11:31:03 2004
From: monica.palaseanu-lovejoy at stud.man.ac.uk (Monica Palaseanu-Lovejoy)
Date: Wed, 23 Jun 2004 10:31:03 +0100
Subject: [R] Automatic routine - help. THANKS!
In-Reply-To: <Pine.LNX.4.44.0406231013090.16683-100000@gannet.stats>
References: <E1Bd3jE-0009WH-Vy@serenity.mcc.ac.uk>
Message-ID: <E1Bd463-000DFO-Qp@serenity.mcc.ac.uk>

Hi,

Thank you so much for the little example - this was exactly what i 
was looking for. Just i didn't know what to search after in R. I will 
change it for my data needs.

Thanks again,

Monica



From bxc at steno.dk  Wed Jun 23 11:33:00 2004
From: bxc at steno.dk (BXC (Bendix Carstensen))
Date: Wed, 23 Jun 2004 11:33:00 +0200
Subject: [R] Automatic routine - help
Message-ID: <0ABD88905D18E347874E0FB71C0B29E901D8E58C@exdkba022.novo.dk>

for (j in 1:n) df[,j] <- pmin( pmax( df[,j], 0 ), 1 )

should do the job.

Bendix Carstensen

----------------------
Bendix Carstensen
Senior Statistician
Steno Diabetes Center
Niels Steensens Vej 2
DK-2820 Gentofte
Denmark
tel: +45 44 43 87 38
mob: +45 30 75 87 38
fax: +45 44 43 07 06
bxc at steno.dk
www.biostat.ku.dk/~bxc
----------------------



> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch 
> [mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Monica 
> Palaseanu-Lovejoy
> Sent: Wednesday, June 23, 2004 11:07 AM
> To: r-help at stat.math.ethz.ch
> Subject: [R] Automatic routine - help
> 
> 
> Hi,
> 
> I would like to write a little automatic routine in R, but i am a too 
> much of a beginner for that. I will appreciate any help 
> regarding this 
> particular problem.
> 
> Let's suppose I have a data.frame with j columns (from 1 to n) and i 
> rows (from 1 to p). I would like to write a procedure which reads 
> every column j (j from 1 to n) and compare each value with the 
> interval [0,1]. If z(i,j) is less than 0, then replace z(i,j) 
> with 0. If z(i,j) 
> is greater than 1, then replace z(i,j) with 1. If z(i,j) is 
> inside the 
> interval [0,1] then don't change. In the end I would like to have a 
> new data.frame with the new values. 
> 
> I am not sure how complicated or long such a procedure might be, 
> so I will be very grateful for any help.
> 
> Thank you in advance,
> 
> Monica
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> PLEASE 
> do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From monica.palaseanu-lovejoy at stud.man.ac.uk  Wed Jun 23 11:49:53 2004
From: monica.palaseanu-lovejoy at stud.man.ac.uk (Monica Palaseanu-Lovejoy)
Date: Wed, 23 Jun 2004 10:49:53 +0100
Subject: [R] Automatic routine - help
In-Reply-To: <0ABD88905D18E347874E0FB71C0B29E901D8E58C@exdkba022.novo.dk>
Message-ID: <E1Bd4OE-000G5R-UH@serenity.mcc.ac.uk>

Hi,

This works beautifully as well.

Thanks,

Monica



From Isabelle.ZABALZA-MEZGHANI at ifp.fr  Wed Jun 23 12:18:36 2004
From: Isabelle.ZABALZA-MEZGHANI at ifp.fr (ZABALZA-MEZGHANI Isabelle)
Date: Wed, 23 Jun 2004 12:18:36 +0200
Subject: [R] How to define stopping criterium for Optim with L-BFGS-B
Message-ID: <488C02265C6AD611BF200002A542182F05B5952E@irnts22.ifp.fr>

Hi,

I am using optim with a L-BFGS-B method to minimize a function. As I've
understood, the way to specify a tolerance for stopping optimization is
through "factr" argument.
My function, is by construction, minimal when equal to 1. I wonder if there
is any way to pass this info to "optim". If not, how "factr" argument works
(I am quite confused about the relationship between this argument and the
Machine eps). Please, could someone give me an example ?

Thanks in advance

Isabelle.

I Zabalza-Mezghani
IFP - France

__________________________

Ce message (et toutes ses pi??ces jointes ??ventuelles) est confidentiel et ??tabli ?? l'intention exclusive de ses destinataires. Toute utilisation de ce message non conforme ?? sa destination, toute diffusion ou toute publication, totale ou partielle, est interdite, sauf autorisation expresse. L'IFP d??cline toute responsabilit?? au titre de ce message.

This message and any attachments (the message) are confident...{{dropped}}



From B.Rowlingson at lancaster.ac.uk  Wed Jun 23 12:24:12 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Wed, 23 Jun 2004 11:24:12 +0100
Subject: [R] Automatic routine - help
In-Reply-To: <40D94A02.9060102@statistik.uni-dortmund.de>
References: <E1Bd3jE-0009WH-Vy@serenity.mcc.ac.uk>
	<40D94A02.9060102@statistik.uni-dortmund.de>
Message-ID: <40D95A4C.5080307@lancaster.ac.uk>

Uwe Ligges wrote:
> Monica Palaseanu-Lovejoy wrote:
> 
>> Hi,
>>
>> I would like to write a little automatic routine in R, but i am a too 
>> much of a beginner for that. I will appreciate any help regarding this 
>> particular problem.

> If all columns of your data.frame are numeric:
> 
> z[z<0] <- 0
> z[z>1] <- 1
> 

  For added fun, you can wrap any of the methods given on the list into 
a function. For example:

  hardLimit <- function(z, min=0, max=1){
    z[z < min] <- min
    z[z > max] <- max
    return(z)
}

  Then you can do:

  z <- hardLimit(z)

  if you want to overwrite z, or:

  y <- hardLimit(z)

  to create a new data frame.

  Note how the default min and max arguments are 0 and 1, and make the 
function more flexible. You can also do:

  x <- hardLimit(z, min=-1)

  and that sets everything below -1 to the value -1.

Welcome to the world of R development!

Baz



From ripley at stats.ox.ac.uk  Wed Jun 23 12:42:06 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 23 Jun 2004 11:42:06 +0100 (BST)
Subject: [R] Automatic routine - help
In-Reply-To: <40D95A4C.5080307@lancaster.ac.uk>
Message-ID: <Pine.LNX.4.44.0406231126140.16922-100000@gannet.stats>

On Wed, 23 Jun 2004, Barry Rowlingson wrote:

> Uwe Ligges wrote:
> > Monica Palaseanu-Lovejoy wrote:
> > 
> >> Hi,
> >>
> >> I would like to write a little automatic routine in R, but i am a too 
> >> much of a beginner for that. I will appreciate any help regarding this 
> >> particular problem.
> 
> > If all columns of your data.frame are numeric:
> > 
> > z[z<0] <- 0
> > z[z>1] <- 1
> > 
> 
>   For added fun, you can wrap any of the methods given on the list into 
> a function. For example:
> 
>   hardLimit <- function(z, min=0, max=1){
>     z[z < min] <- min
>     z[z > max] <- max
>     return(z)
> }
> 
>   Then you can do:
> 
>   z <- hardLimit(z)
> 
>   if you want to overwrite z, or:
> 
>   y <- hardLimit(z)
> 
>   to create a new data frame.
> 
>   Note how the default min and max arguments are 0 and 1, and make the 
> function more flexible. You can also do:
> 
>   x <- hardLimit(z, min=-1)
> 
>   and that sets everything below -1 to the value -1.
> 
> Welcome to the world of R development!

First off, if you do start programming, you need to program up the 
comments too.  So as Uwe said

> > If all columns of your data.frame are numeric:

you need (untested)

hardLimit <- function(z, min=0, max=1)
{
    if(!(is.numeric(z) || all(sapply(z, is.numeric)) )) 
        stop("z is not a numeric vector, array or data frame")
    z[z < min] <- min
    z[z > max] <- max
    z
}

since the code will also work for numeric vectors and arrays.  Then you 
need to check if min < max or the order matters ....


However, if you want to do this at all efficiently for a data frame, start
with my solution not Uwe's, which creates several arrays the size of the
one you started with (two for the logical values, and one of the
intermediate answer) and does a for loop over columns internally at least
four times.

When operating on data frames it is usually best to work column by column, 
hence the

newDF[] <- lapply(DF, some_function_for_one_column)

paradigm.  (Changing just the values keeps all the attributes such as row 
names and col names.)


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Jun 23 12:47:13 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 23 Jun 2004 11:47:13 +0100 (BST)
Subject: [R] How to define stopping criterium for Optim with L-BFGS-B
In-Reply-To: <488C02265C6AD611BF200002A542182F05B5952E@irnts22.ifp.fr>
Message-ID: <Pine.LNX.4.44.0406231143010.16922-100000@gannet.stats>

On Wed, 23 Jun 2004, ZABALZA-MEZGHANI Isabelle wrote:

> I am using optim with a L-BFGS-B method to minimize a function. As I've
> understood, the way to specify a tolerance for stopping optimization is
> through "factr" argument.
> My function, is by construction, minimal when equal to 1. I wonder if there
> is any way to pass this info to "optim". 

No, as the problem is assumed to be scaled so this is approximately true.

> If not, how "factr" argument works
> (I am quite confused about the relationship between this argument and the
> Machine eps). Please, could someone give me an example ?

The (absolute) convergence tolerance is factr times the Machine eps, and
the help page does already give you an example.  You can read the
references for further details.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From a.beckerman at sheffield.ac.uk  Wed Jun 23 12:53:10 2004
From: a.beckerman at sheffield.ac.uk (Andrew Beckerman)
Date: Wed, 23 Jun 2004 11:53:10 +0100
Subject: [R] converting apply output
Message-ID: <847BDFC0-C503-11D8-BC4D-000A95CD7F02@sheffield.ac.uk>

Hi -

platform powerpc-apple-darwin6.8
status
major    1
minor    9.0
year     2004

I am trying to deal with the output of apply().  As indicated, when 
each call to 'FUN' returns a vector of length 'n', then 'apply'  
returns an array of dimension 'c(n, dim(X)[MARGIN])'.  However, I would 
like this to be a list in the same format as is produced when 'FUN' 
return vectors of different lengths ('apply'   returns a list of length 
'dim(X)[MARGIN]').

e.g.
tt1<-c(0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 
0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ,0 ,0 ,0 ,0 ,0 ,0 
,1 ,0 ,1 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,1 ,1 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,1 ,0 ,0 ,0 
,1 ,0 ,0 ,0 ,0 ,0 ,1 ,0 ,0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 
0, 0, 0, 0, 1, 0, 1, 0)
m1<-matrix(tt1,10,10)
out<-apply(m1,2,function(x) which(x==1))

produces an array,
 > out
      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
[1,]    8    9    7    8    6    7    6    6    6     7
[2,]    9   10    8    9    8    8   10   10    7     9

but I would like out as a list of 10 elements with two elements in 
each, e.g.

[[1]]
[1]  8 9

[[2]]
[1] 9 10
etc.

I have tried apply(out,2,function(x) list(x))), but the subsrcripting 
is not equal to the pattern when FUN returns a vectors of different 
length.  Any help would be appreciated.

Cheers
andrew



From v.demartino2 at virgilio.it  Wed Jun 23 15:33:52 2004
From: v.demartino2 at virgilio.it (Vittorio)
Date: Wed, 23 Jun 2004 14:33:52 +0100
Subject: [R] ts & daily timeseries
In-Reply-To: <Pine.LNX.4.44.0406221737440.32732-100000@gannet.stats>
References: <Pine.LNX.4.44.0406221737440.32732-100000@gannet.stats>
Message-ID: <200406231433.52651.v.demartino2@virgilio.it>

On Tuesday 22 June 2004 17:41, Prof Brian Ripley wrote:
> data.frame(time=mytime, ts=myts)
>
> would appear to be what you are looking for.
>
> On Tue, 22 Jun 2004 v.demartino2 at virgilio.it wrote:
> > I have defined a daily timeseries for the 365 days of 2003 issuing:
> >
> > myts = ts(dati[,2:10],frequency=365,)
> >
> > > myts
> >
> > Time Series:
> > Start = c(1, 1)
> > End = c(1, 365)
> > Frequency = 365
> >
> > and
> >
> > mytime =  as.POSIXct(strptime(as.character(dati[,1]),format="%Y-%m-%d"))
> >
> > contains the dates from "2003-01-01" to "2003-12-31"
>
> Why not use the Date class?  However, if all you want is the character
> representation of the dates you appear to have those in dati[1].
>
> > How can I combine mytime and myts in order to list the timeseries
> > according to the more natural
> >
> > 2003-01-01    xxxxx  xxxxx   xxxxx ......
> > 2003-01-02    xxxxx  xxxxx   xxxxx ......
> > ...........
> > 2003-12-31    xxxxx  xxxxx   xxxxx ......
>
> What do you want the first column to be?  Character strings? Dates?  What
> do you want to do with this?

As a newbye becoming more and more confident with R, I'm having a go at neural 
networks by means of your nnet package trying to model daily electricity 
demand with respect to time, weather extreme temperatures,air humidity, sky 
coverage (don't know if is correct in English, I mean: a measurement of the 
intensity of daylight going to very cloudy to completely clear), and type of 
day (working days, Mon, Sat, Sun).

Therefore, among many other things, I need to:

1) Window different spells of the year to study them separatedly;
2) Print in a human readable way the list of data of the spell  under 
observation;
3) Plot the same spells as timeseries, that is with a time referenced x-axis.

That's all.

Thanks 

Vittorio



From olafur.ingolfsson at imr.no  Wed Jun 23 13:36:43 2004
From: olafur.ingolfsson at imr.no (Ingolfsson, Olafur)
Date: Wed, 23 Jun 2004 13:36:43 +0200
Subject: [R] Tick marks in xyplot
Message-ID: <1612616523F26F48AB55BC8F5D47917C298C11@post2.imr.no>

Dear R group

I am making multiple xyplot, and would like to have tick marks on bottom and left in EACH panel, but only tick labels at the bottom and left of the whole graph.
I have browsed the internet, as well as the help page without success.
If anyone could help me find the path to the solution I would appreciate.
Here is an example, this is the best I could do:

lset(col.whitebg())
x.data <- rnorm(16,20,7);y.data <- rnorm(16,.55,.25); z.data <- sample(1:4,16,replace=T)
xyplot(y.data~x.data|z.data, layout=c(2,2),
       scales=list(x=list(alternating=F),y=list(alternating=F),tck=c(-1,0)))

Cheers

??lafur A. Ing??lfsson
Institute of Marine Research
PO Box 1870 Nordnes
5817 Bergen, Norway



From lecoutre at stat.ucl.ac.be  Wed Jun 23 14:13:45 2004
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Wed, 23 Jun 2004 14:13:45 +0200
Subject: [R] converting apply output
In-Reply-To: <847BDFC0-C503-11D8-BC4D-000A95CD7F02@sheffield.ac.uk>
References: <847BDFC0-C503-11D8-BC4D-000A95CD7F02@sheffield.ac.uk>
Message-ID: <6.0.1.1.2.20040623141250.021fc640@stat4ux.stat.ucl.ac.be>


Hi,

Why not try with the data.frame structure, wich internally yet consists in 
a list:

 > lapply(as.data.frame(m1),function(x) which(x==1))
$V1
[1] 8 9
$V2
[1]  9 10
[...]

Eric

At 12:53 23/06/2004, Andrew Beckerman wrote:
>Hi -
>
>platform powerpc-apple-darwin6.8
>status
>major    1
>minor    9.0
>year     2004
>
>I am trying to deal with the output of apply().  As indicated, when each 
>call to 'FUN' returns a vector of length 'n', then 'apply'
>returns an array of dimension 'c(n, dim(X)[MARGIN])'.  However, I would 
>like this to be a list in the same format as is produced when 'FUN' return 
>vectors of different lengths ('apply'   returns a list of length 
>'dim(X)[MARGIN]').
>
>e.g.
>tt1<-c(0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 
>0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ,0 ,0 ,0 ,0 ,0 ,0 ,1 ,0 
>,1 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,1 ,1 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,1 ,0 ,0 ,0 ,1 ,0 ,0 
>,0 ,0 ,0 ,1 ,0 ,0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
>1, 0, 1, 0)
>m1<-matrix(tt1,10,10)
>out<-apply(m1,2,function(x) which(x==1))
>
>produces an array,
> > out
>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
>[1,]    8    9    7    8    6    7    6    6    6     7
>[2,]    9   10    8    9    8    8   10   10    7     9
>
>but I would like out as a list of 10 elements with two elements in each, e.g.
>
>[[1]]
>[1]  8 9
>
>[[2]]
>[1] 9 10
>etc.
>
>I have tried apply(out,2,function(x) list(x))), but the subsrcripting is 
>not equal to the pattern when FUN returns a vectors of different 
>length.  Any help would be appreciated.
>
>Cheers
>andrew
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Eric Lecoutre
UCL /  Institut de Statistique
Voie du Roman Pays, 20
1348 Louvain-la-Neuve
Belgium

tel: (+32)(0)10473050
lecoutre at stat.ucl.ac.be
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre

If the statistics are boring, then you've got the wrong numbers. -Edward 
Tufte



From chrysopa at insecta.ufv.br  Wed Jun 23 14:23:30 2004
From: chrysopa at insecta.ufv.br (Ronaldo Reis Jr.)
Date: Wed, 23 Jun 2004 09:23:30 -0300
Subject: [R] gui for MacOS X / Aqua
Message-ID: <200406230923.30599.chrysopa@insecta.ufv.br>

Hi,

anybody know if Rcmdr or any other GUI like this work with R/Aqua?

Thanks
Ronaldo
-- 
Let's organize this thing and take all the fun out of it.
--
|>   // | \\   [***********************************]
|   ( ??   ?? )  [Ronaldo Reis J??nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36571-000 Vi??osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-2532                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From Ivailo.Partchev at uni-jena.de  Wed Jun 23 14:33:27 2004
From: Ivailo.Partchev at uni-jena.de (Ivailo Partchev)
Date: Wed, 23 Jun 2004 14:33:27 +0200
Subject: [R] Grouped AND stacked bar charts possible in R?
In-Reply-To: <1087929160.1567.87.camel@localhost.localdomain>
References: <40D85652.3070005@fstrf-wi.org>
	<1087929160.1567.87.camel@localhost.localdomain>
Message-ID: <40D97897.1070907@uni-jena.de>

This may have been answered before, but a nice possibility to plot the 
Titanic data is offered by "mosaic plots", as introduced (I think) by 
John Hartigan and implemented in the VCD package

Ivailo Partchev
Jena

Marc Schwartz wrote:

>On Tue, 2004-06-22 at 10:54, Patrick Lenon wrote:
>  
>
>>Good day all,
>>
>>My statisticians want an R procedure that will produce grouped stacked 
>>barplots.  Barplot will
>>stack or group, but not both.  The ftable function can produce a table
>>of the exact form they want, but the barplot doesn't show all the
>>divisions we want.
>>
>>For an example, here's the sample from the help file for "ftable:"
>> 
>>data(Titanic)
>>ftable(Titanic, row.vars = 1:3)
>>ftable(Titanic, row.vars = 1:2, col.vars = "Survived")
>>ftable(Titanic, row.vars = 2:1, col.vars = "Survived")
>>
>>Now take it a step further to try to add another dimension:
>>
>>b <- ftable(Titanic, row.vars=1:3)
>>
>>                   Survived  No Yes
>>Class Sex    Age                  
>>1st   Male   Child            0   5
>>             Adult          118  57
>>      Female Child            0   1
>>             Adult            4 140
>>2nd   Male   Child            0  11
>>             Adult          154  14
>>      Female Child            0  13
>>             Adult           13  80
>>3rd   Male   Child           35  13
>>             Adult          387  75
>>      Female Child           17  14
>>             Adult           89  76
>>Crew  Male   Child            0   0
>>             Adult          670 192
>>      Female Child            0   0
>>             Adult            3  20
>>
>>barplot(b)
>>barplot(b, beside=T))
>>
>>Neither resulting barplot is satisfactory.  The first stacks all the
>>subdivisions of "Survived = Yes" and "Survived = No" together.  The
>>second is closer because it creates two groups, but it lists
>>combinations side-by-side that we'd like stacked. In the above example
>>"No" and "Yes" would be stacked on bars labeled "Male" or "Female"
>>in groups by Class.
>>
>>I've taken a look through the R-Help archives and looked through the
>>contributed packages, but haven't found anything yet.
>>
>>If you have any thoughts how we might produce groups of stacked bars
>>from an ftable, we would appreciate it.
>>    
>>
>
>
>I think that you are trying to plot too much information in a single
>graphic. The result of a multi-dimensional barplot is likely to be very
>difficult to interpret visually.
>
>You would likely be better served to determine, within the multiple
>dimensions, what your conditioning and grouping dimensions need to be
>and then consider a lattice based plot.
>
>I would urge you to consider using either barchart() or perhaps
>dotplot() in lattice, which are designed to handle multivariable charts
>of this nature.
>
>Use:
>
>library(lattice)
>
>Then for general information
>
>?Lattice
>
>and then
>
>?barchart
>
>for more function specific information and examples of graphics with
>each function.
>
>For the Titanic data that you have above, you could do something like:
>
># Convert the multi-dimensional table to a 
># data frame. Assumes you have already done
># data(Titanic)
>MyData <- as.data.frame(Titanic)
>
># Take a look at the structure
>MyData
>
>   Class    Sex   Age Survived Freq
>1    1st   Male Child       No    0
>2    2nd   Male Child       No    0
>3    3rd   Male Child       No   35
>4   Crew   Male Child       No    0
>5    1st Female Child       No    0
>6    2nd Female Child       No    0
>7    3rd Female Child       No   17
>8   Crew Female Child       No    0
>9    1st   Male Adult       No  118
>10   2nd   Male Adult       No  154
>11   3rd   Male Adult       No  387
>12  Crew   Male Adult       No  670
>13   1st Female Adult       No    4
>14   2nd Female Adult       No   13
>15   3rd Female Adult       No   89
>16  Crew Female Adult       No    3
>17   1st   Male Child      Yes    5
>18   2nd   Male Child      Yes   11
>19   3rd   Male Child      Yes   13
>20  Crew   Male Child      Yes    0
>21   1st Female Child      Yes    1
>22   2nd Female Child      Yes   13
>23   3rd Female Child      Yes   14
>24  Crew Female Child      Yes    0
>25   1st   Male Adult      Yes   57
>26   2nd   Male Adult      Yes   14
>27   3rd   Male Adult      Yes   75
>28  Crew   Male Adult      Yes  192
>29   1st Female Adult      Yes  140
>30   2nd Female Adult      Yes   80
>31   3rd Female Adult      Yes   76
>32  Crew Female Adult      Yes   20
>
># Now do a plot. Use 'library(lattice)' here first
># if you had not already done so above for help.
>barchart(Freq ~ Survived | Age * Sex, groups = Class, data = MyData,
>         auto.key = list(points = FALSE, rectangles = TRUE, space
>         = "right", title = "Class", border = TRUE), xlab = "Survived",
>         ylim = c(0, 800))
>
>The above barchart will create a four panel plot, where the four main
>panels will contain the combinations of Sex and Age. Within each panel
>will be two groups of bars, one each for the Survived Yes/No status.
>Within each group will be one bar for each Class. 
>
>That is one quick way of grouping things, but you can alter that and
>other plot attributes easily.
>
>HTH,
>
>Marc Schwartz
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>  
>



From chrysopa at insecta.ufv.br  Wed Jun 23 14:33:38 2004
From: chrysopa at insecta.ufv.br (Ronaldo Reis Jr.)
Date: Wed, 23 Jun 2004 09:33:38 -0300
Subject: [R] Sciviews
Message-ID: <200406230933.38931.chrysopa@insecta.ufv.br>

Anybody use Sciviews with Linux or MacOS X? It work with these systems?

Thanks
Ronaldo
-- 
A hacker does for love what others would not do for money.
--
|>   // | \\   [***********************************]
|   ( ??   ?? )  [Ronaldo Reis J??nior                ]
|>      V      [UFV/DBA-Entomologia                ]
|    /     \   [36571-000 Vi??osa - MG              ]
|>  /(.''`.)\  [Fone: 31-3899-2532                 ]
|  /(: :'  :)\ [chrysopa at insecta.ufv.br            ]
|>/ (`. `'` ) \[ICQ#: 5692561 | LinuxUser#: 205366 ]
|    ( `-  )   [***********************************]
|>>  _/   \_Powered by GNU/Debian Woody/Sarge



From deepayan at stat.wisc.edu  Wed Jun 23 14:32:51 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Wed, 23 Jun 2004 07:32:51 -0500
Subject: [R] Tick marks in xyplot
In-Reply-To: <1612616523F26F48AB55BC8F5D47917C298C11@post2.imr.no>
References: <1612616523F26F48AB55BC8F5D47917C298C11@post2.imr.no>
Message-ID: <200406230732.52206.deepayan@stat.wisc.edu>

On Wednesday 23 June 2004 06:36, Ingolfsson, Olafur wrote:
> Dear R group
>
> I am making multiple xyplot, and would like to have tick marks on
> bottom and left in EACH panel, but only tick labels at the bottom and
> left of the whole graph. I have browsed the internet, as well as the
> help page without success. If anyone could help me find the path to
> the solution I would appreciate. Here is an example, this is the best
> I could do:

I don't think there is good way to do this in the way you expect (I'm 
assuming you want the same axis limits for each panel). If you really 
want this, look at the following options:

1. You can specify the limits, tick positions, and axis labels 
individually for each panel (see documentation for 'scales' in ?xyplot; 
all the relevant components - at, lab, etc - can be lists). Use this in 
conjunction with relation = "free". You need to supply empty strings as 
labels for all the 'inside' panels.

2. The big problem this would have is that the same amount of space 
would be allocated for the axis labels in each of the panels, even if 
that space is not used for the inside ones. One (undocumented and 
unreliable) way to get around that would be to use negative values of 
'between'.

Hope that helps,

Deepayan



From monica.palaseanu-lovejoy at stud.man.ac.uk  Wed Jun 23 14:53:27 2004
From: monica.palaseanu-lovejoy at stud.man.ac.uk (Monica Palaseanu-Lovejoy)
Date: Wed, 23 Jun 2004 13:53:27 +0100
Subject: [R] Automatic routine - NEW
Message-ID: <E1Bd7Ft-000J9l-Gx@serenity.mcc.ac.uk>

Hi Again,

First of all thank you for all the responses to my previous query. 
Your answers were very helpful and I did the job ;-). Now I hope you 
can answer as quick the following (sorry I am invading you with 
trivial questions):

Let?s use again the following data.frame example:
DF <- data.frame(x=rnorm(5), y=rnorm(5))

I want to obtain a new data.frame (or matrix) that contains only n 
rows (from the i rows DF has), and all the columns. If I have to do it 
step by step I would do something like that, for example:

a3 <- DF[3,]
a4 <- DF[4,]
a5 <- DF[5,]
b <- data.frame(a3, a4, a5)
c <- matrix(b, nrow=3, ncol=2, byrow=TRUE)

Now I want to do the same in one go, so I wrote:

for (i in 3:5)
{
	d[i] <- DF[i,]
	e <- data.frame(d[i])
	f <- matrix(e, ncol=2, nrow=3, byrow=TRUE)
}

Which of course gives me errors and the matrix f has all elements 
equal with DF[5,5]. If I don?t use [i] after d, the resulting f matrix is 
made up from the DF[5,] elements (which is quite normal since i 
replaces itself ...). So ..... How is this done correctly?

I am really appreciating your time and effort to answer me,

Monica



From phgrosjean at sciviews.org  Wed Jun 23 15:02:21 2004
From: phgrosjean at sciviews.org (Philippe Grosjean)
Date: Wed, 23 Jun 2004 15:02:21 +0200
Subject: [R] Sciviews
In-Reply-To: <200406230933.38931.chrysopa@insecta.ufv.br>
Message-ID: <MABBLJDICACNFOLGIHJOEEDBEJAA.phgrosjean@sciviews.org>

Ronaldo Reis Jr wrote:
>Anybody use Sciviews with Linux or MacOS X? It work with these systems?

Not yet... but we are rewritting a part of the code so that some features
become platform-independent. Please, leave us a little time to archive this.
There are tens of thousands of lines of code in SciViews... so, this is a
very long process!
Thanks,

Philippe Grosjean

.......................................................<??}))><....
 ) ) ) ) )
( ( ( ( (   Prof. Philippe Grosjean
\  ___   )
 \/ECO\ (   Numerical Ecology of Aquatic Systems
 /\___/  )  Mons-Hainaut University, Pentagone
/ ___  /(   8, Av. du Champ de Mars, 7000 Mons, Belgium
 /NUM\/  )
 \___/\ (   phone: + 32.65.37.34.97, fax: + 32.65.37.33.12
       \ )  email: Philippe.Grosjean at umh.ac.be
 ) ) ) ) )  SciViews project coordinator (http://www.sciviews.org)
( ( ( ( (
...................................................................



From sundar.dorai-raj at PDF.COM  Wed Jun 23 14:59:29 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Wed, 23 Jun 2004 07:59:29 -0500
Subject: [R] [Q] GET_DIM() crash on Windows only
In-Reply-To: <40D931DD.7000406@statistik.uni-dortmund.de>
References: <Pine.OSF.4.58.0406221516520.368009@odin.mdacc.tmc.edu>
	<40D931DD.7000406@statistik.uni-dortmund.de>
Message-ID: <40D97EB1.9090400@pdf.com>


Uwe Ligges wrote:

> Paul Roebuck wrote:
> 
>> I have the following contrived code in package format.
>> On Solaris and Mac OS X, code runs just fine. On Windows,
>> it crashes the R environment with the "Send Bug Report"
>> dialog. I tried R 1.8.1 (Win2K) and R 1.9 (WinXP) binaries
>> with the same result. PCs otherwise appear properly
>> configured for creating R packages. Anything blatantly
>> wrong? Suggestions?
> 
> 
> 
> Works for me (R-1.9.1, WinNT4.0), even with gctorture(TRUE).
> Did you use the recommended compiler and tools?
> 
> Uwe Ligges
> 
> 

And for me on R-1.9.0, Win2000.

--sundar



From sundar.dorai-raj at PDF.COM  Wed Jun 23 14:59:47 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Wed, 23 Jun 2004 07:59:47 -0500
Subject: [R] [Q] GET_DIM() crash on Windows only
In-Reply-To: <40D931DD.7000406@statistik.uni-dortmund.de>
References: <Pine.OSF.4.58.0406221516520.368009@odin.mdacc.tmc.edu>
	<40D931DD.7000406@statistik.uni-dortmund.de>
Message-ID: <40D97EC3.5020509@pdf.com>


Uwe Ligges wrote:

> Paul Roebuck wrote:
> 
>> I have the following contrived code in package format.
>> On Solaris and Mac OS X, code runs just fine. On Windows,
>> it crashes the R environment with the "Send Bug Report"
>> dialog. I tried R 1.8.1 (Win2K) and R 1.9 (WinXP) binaries
>> with the same result. PCs otherwise appear properly
>> configured for creating R packages. Anything blatantly
>> wrong? Suggestions?
> 
> 
> 
> Works for me (R-1.9.1, WinNT4.0), even with gctorture(TRUE).
> Did you use the recommended compiler and tools?
> 
> Uwe Ligges
> 
> 

And for me on R-1.9.0, Win2000.

--sundar

<snip>



From andy_liaw at merck.com  Wed Jun 23 15:09:03 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 23 Jun 2004 09:09:03 -0400
Subject: [R] Automatic routine - NEW
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7F51@usrymx25.merck.com>

Sounds to me that you are looking for something like:

subDF <- DF[3:5,]

Andy

> From: Monica Palaseanu-Lovejoy
> 
> Hi Again,
> 
> First of all thank you for all the responses to my previous query. 
> Your answers were very helpful and I did the job ;-). Now I hope you 
> can answer as quick the following (sorry I am invading you with 
> trivial questions):
> 
> Let's use again the following data.frame example:
> DF <- data.frame(x=rnorm(5), y=rnorm(5))
> 
> I want to obtain a new data.frame (or matrix) that contains only n 
> rows (from the i rows DF has), and all the columns. If I have 
> to do it 
> step by step I would do something like that, for example:
> 
> a3 <- DF[3,]
> a4 <- DF[4,]
> a5 <- DF[5,]
> b <- data.frame(a3, a4, a5)
> c <- matrix(b, nrow=3, ncol=2, byrow=TRUE)
> 
> Now I want to do the same in one go, so I wrote:
> 
> for (i in 3:5)
> {
> 	d[i] <- DF[i,]
> 	e <- data.frame(d[i])
> 	f <- matrix(e, ncol=2, nrow=3, byrow=TRUE)
> }
> 
> Which of course gives me errors and the matrix f has all elements 
> equal with DF[5,5]. If I don't use [i] after d, the resulting 
> f matrix is 
> made up from the DF[5,] elements (which is quite normal since i 
> replaces itself ...). So .... How is this done correctly?
> 
> I am really appreciating your time and effort to answer me,
> 
> Monica
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From bitwrit at ozemail.com.au  Wed Jun 23 15:30:59 2004
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Wed, 23 Jun 2004 23:30:59 +1000
Subject: [R] legend
In-Reply-To: <5AFDDD57E2771B409224CD858CC6DE0D02DAB464@mailer-e051.umh.es>
References: <5AFDDD57E2771B409224CD858CC6DE0D02DAB464@mailer-e051.umh.es>
Message-ID: <20040623132756.WCSD5317.smta00.mail.ozemail.net@there>

Perez Martin, Agustin wrote:
> DeaR UseRs:
>
> I want to put a legend in my plot. In the first line of the legend I want
> to put a box filled but in the second one I would like to put a lty=2
>
> Of course it must appear with different colors.
>
I think I wrote this function about a year ago for someone, but I couldn't 
find it anywhere. Here it is again.

Jim
-------------- next part --------------
add.legend.bars<-function(legend.info,whichbars,col,border="black") {
 nelements<-length(legend.info$text$y)
 left<-rep(legend.info$rect$left+
  0.1*(legend.info$text$x[1]-legend.info$rect$left),nelements)
 right<-rep(legend.info$rect$left+
  0.8*(legend.info$text$x[1]-legend.info$rect$left),nelements)
 top<-legend.info$text$y+(legend.info$text$y[1]-legend.info$text$y[2])/3
 bottom<-top-(legend.info$text$y[1]-legend.info$text$y[2])/1.5
 rect(left[whichbars],bottom[whichbars],
  right[whichbars],top[whichbars],
  col=col,border=border)
}

From spencer.graves at pdf.com  Wed Jun 23 15:46:14 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Wed, 23 Jun 2004 08:46:14 -0500
Subject: [R] Automatic routine - NEW
In-Reply-To: <E1Bd7Ft-000J9l-Gx@serenity.mcc.ac.uk>
References: <E1Bd7Ft-000J9l-Gx@serenity.mcc.ac.uk>
Message-ID: <40D989A6.2090609@pdf.com>

  The documentation is good for things like this. In most (all recent?) 
versions of R, "help.start()" brings up a help page. Select "An 
Introduction to R", and go to "Vector Indices". Also look at "Arrays and 
Matrices: Array Indexing". There, you will find that "DF[3:5,]" might be 
what you want.

Actually, I'm not certain that your "c" below is what you want, because 
class(c[1,1]) is "list", and I could not do arithmetic on it. I 
discovered that when I tried all.equal(c, DF[3:5,]). {Also, "c" is the 
name of a very useful function; try "?c". It is usually wise to avoid 
using names of functions for matrices of data.frames. R will select the 
one you want from the context in most but not all cases.}

hope this helps.
spencer graves

Monica Palaseanu-Lovejoy wrote:

>Hi Again,
>
>First of all thank you for all the responses to my previous query. 
>Your answers were very helpful and I did the job ;-). Now I hope you 
>can answer as quick the following (sorry I am invading you with 
>trivial questions):
>
>Let?s use again the following data.frame example:
>DF <- data.frame(x=rnorm(5), y=rnorm(5))
>
>I want to obtain a new data.frame (or matrix) that contains only n 
>rows (from the i rows DF has), and all the columns. If I have to do it 
>step by step I would do something like that, for example:
>
>a3 <- DF[3,]
>a4 <- DF[4,]
>a5 <- DF[5,]
>b <- data.frame(a3, a4, a5)
>c <- matrix(b, nrow=3, ncol=2, byrow=TRUE)
>
>Now I want to do the same in one go, so I wrote:
>
>for (i in 3:5)
>{
>	d[i] <- DF[i,]
>	e <- data.frame(d[i])
>	f <- matrix(e, ncol=2, nrow=3, byrow=TRUE)
>}
>
>Which of course gives me errors and the matrix f has all elements 
>equal with DF[5,5]. If I don?t use [i] after d, the resulting f matrix is 
>made up from the DF[5,] elements (which is quite normal since i 
>replaces itself ...). So ..... How is this done correctly?
>
>I am really appreciating your time and effort to answer me,
>
>Monica
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From monica.palaseanu-lovejoy at stud.man.ac.uk  Wed Jun 23 15:49:13 2004
From: monica.palaseanu-lovejoy at stud.man.ac.uk (Monica Palaseanu-Lovejoy)
Date: Wed, 23 Jun 2004 14:49:13 +0100
Subject: [R] Automatic routine - NEW
In-Reply-To: <40D989A6.2090609@pdf.com>
References: <E1Bd7Ft-000J9l-Gx@serenity.mcc.ac.uk>
Message-ID: <E1Bd87x-0002yE-RY@serenity.mcc.ac.uk>

Hi Spencer,

Your answer is very helpful. I was wondering if i should write again 
to the list to ask where i suppose to read about indices and things 
liek that since it seems they are very useful in lots of things. But 
you already gave me the answer. 

Thanks again,

Monica



From ramasamy at cancer.org.uk  Wed Jun 23 15:53:49 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: 23 Jun 2004 14:53:49 +0100
Subject: [R] Automatic routine - NEW
In-Reply-To: <E1Bd7Ft-000J9l-Gx@serenity.mcc.ac.uk>
References: <E1Bd7Ft-000J9l-Gx@serenity.mcc.ac.uk>
Message-ID: <1087998829.4394.26.camel@vpn202001.lif.icnet.uk>

On Wed, 2004-06-23 at 13:53, Monica Palaseanu-Lovejoy wrote:
> Hi Again,
> 
> First of all thank you for all the responses to my previous query. 
> Your answers were very helpful and I did the job ;-). Now I hope you 
> can answer as quick the following (sorry I am invading you with 
> trivial questions):
> 
> Lets use again the following data.frame example:
> DF <- data.frame(x=rnorm(5), y=rnorm(5))
> 
> I want to obtain a new data.frame (or matrix) that contains only n 
> rows (from the i rows DF has), and all the columns. If I have to do it 
> step by step I would do something like that, for example:
> 
> a3 <- DF[3,]
> a4 <- DF[4,]
> a5 <- DF[5,]
> b <- data.frame(a3, a4, a5)
> c <- matrix(b, nrow=3, ncol=2, byrow=TRUE)

1. You need to read the section on subsetting a matrix or dataframe and
sequence generation. e.g. help("[") and help(":")

2. b <- DF[3:5, ] should do the trick. Also please learn the difference
between a matrix and dataframe.

3. 'c' is a built in function. Do not create objects with 'c' or any
other built in function if possible.

> Now I want to do the same in one go, so I wrote:
> 
> for (i in 3:5)
> {
> 	d[i] <- DF[i,]
> 	e <- data.frame(d[i])
> 	f <- matrix(e, ncol=2, nrow=3, byrow=TRUE)
> }
> 
> Which of course gives me errors and the matrix f has all elements 
> equal with DF[5,5]. If I dont use [i] after d, the resulting f matrix is 
> made up from the DF[5,] elements (which is quite normal since i 
> replaces itself ...). So . How is this done correctly?

You have not define 'd' before the loop. So you cannot subset or assign
to 'd'. Read the errors and it should say something like 'Error: Object
"d" not found'.

> I am really appreciating your time and effort to answer me,

I think you better the read the R manuals or Peter Daalgard's book on
Introduction to R. It might take you a couple of hours/days but the time
spent is well worth it.

> Monica
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From martina.renninger at tns-infratest.com  Wed Jun 23 16:06:05 2004
From: martina.renninger at tns-infratest.com (Martina Renninger)
Date: Wed, 23 Jun 2004 16:06:05 +0200
Subject: [R] Covered Labels
Message-ID: <000001c4592b$3aacde90$5e011e0a@wet.eu.nfowg.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040623/04ad85ff/attachment.pl

From MSchwartz at MedAnalytics.com  Wed Jun 23 16:16:04 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Wed, 23 Jun 2004 09:16:04 -0500
Subject: [R] Covered Labels
In-Reply-To: <000001c4592b$3aacde90$5e011e0a@wet.eu.nfowg.com>
References: <000001c4592b$3aacde90$5e011e0a@wet.eu.nfowg.com>
Message-ID: <1088000164.1567.213.camel@localhost.localdomain>

On Wed, 2004-06-23 at 09:06, Martina Renninger wrote:
> Dear All!

> How can I cope with overlapping or covered labels (covered by labels
> from other data points) in plots?


Presuming that you are using text() to identify points in a plot, you
can use the 'cex' argument (which defaults to 1) to reduce the size of
the font. So in this case, try values <1, for example:

text(x, y, labels = YourText, cex = 0.8)

Possibly depending upon how many points you have, you can also adjust
the position of the label with respect to the data points by using
'adj', 'pos' and 'offset'.

See ?text for more information.

HTH,

Marc Schwartz



From m.sutter at schweiz.ch  Wed Jun 23 16:35:41 2004
From: m.sutter at schweiz.ch (M. Sutter)
Date: Wed, 23 Jun 2004 16:35:41 +0200
Subject: [R] grid lines when plotting its object
Message-ID: <40D9953D.4040405@schweiz.ch>

Dear R community

When plotting an its object (with 'plot(..)), grid lines are 
automatically added which I cannot control with parameter 'tck/tcl'.
Furthermore the vertical grid lines (time axis) do not comply with the 
automatically generated tickmark locations.

Can somebody tell me how to handle these grid lines ?
How can I switch off the grid lines ?

Thanks a lot for any advice and I'd like to thank all the R 
developpers/experts for spending their precious time answering questions 
on the r-help mailing list and making it very valuable for R users like me.

Package: its
Version: 0.2.2
Date: 2004/01/06

 > version
         _               
platform i686-pc-linux-gnu
arch     i686            
os       linux-gnu       
system   i686, linux-gnu 
status                   
major    1               
minor    8.1              --> I know, 9.1 is released ... I 
suppose/hope, thats not the problem
year     2003            
month    11              
day      21              
language R               
 >



From andy_liaw at merck.com  Wed Jun 23 17:02:09 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 23 Jun 2004 11:02:09 -0400
Subject: [R] R 1.9.1 compilation error (on AIX 5.1)
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7F53@usrymx25.merck.com>

> From: Prof Brian Ripley
> 
> We do have alpha/beta test periods for new releases of R, so 
> would anyone 
> who did test AIX please confirm that they did succeed.  (No 
> one reported 
> an error, including yourself: are any AIX users interested in helping 
> having R available for AIX?)   It is much better to have such reports 
> during alpha test, or at least beta test.

I just tried compiling R-1.9.1 as 64-bit on powerpc-ibm-aix5.1.0.0 using
xlc/xlf.  The compile went fine, and the only part of make check that failed
was internet.R (since there's no 'Net access from that box).  The GCC on
that box is probably too outdated to try, unfortunately.

Best,
Andy

 
> On Wed, 23 Jun 2004, Frankie Cheung wrote:
> 
> > Dear Sir/Madam,
> > 
> > I encounter some problem duuring compilation of R 1.9.1 on 
> AIX 5.1, after
> > running "./configure" then I type "make" to compile:
> > 
> > # make
> > .....
> > gcc -I../../src/extra/zlib -I../../src/extra/bzip2 
> -I../../src/extra/pcre
> > -I. -I../../src/include -I../../src/include -I/usr/local/include
> > -DHAVE_CONFIG_H -mno-fp-in-toc  -g -O2 -c registration.c -o 
> registration.o
> > g77   -g -O2 -c xxxpr.f -o xxxpr.o
> > gcc -Wl,-bdynamic -Wl,-bE:../../etc/R.exp -Wl,-bM:SRE 
> -L/usr/local/lib -o
> > R.bin  CConverters.o Rdynload.o RNG.o apply.o arithmetic.o 
> apse.o array.o
> > attrib.o base.o bind.o builtin.o character.o coerce.o 
> colors.o complex.o
> > connections.o context.o cov.o cum.o dcf.o datetime.o debug.o devPS.o
> > devPicTeX.o deparse.o deriv.o devices.o dotcode.o dounzip.o 
> dstruct.o
> > duplicate.o engine.o envir.o errors.o eval.o format.o 
> fourier.o gram.o
> > gram-ex.o graphics.o identical.o internet.o iosupport.o 
> lapack.o list.o
> > logic.o main.o mapply.o match.o memory.o model.o names.o 
> objects.o optim.o
> > optimize.o options.o par.o paste.o pcre.o platform.o plot.o plot3d.o
> > plotmath.o print.o printarray.o printvector.o printutils.o qsort.o
> > random.o regex.o relop.o saveload.o scan.o seq.o 
> serialize.o size.o sort.o
> > source.o split.o sprintf.o subassign.o subscript.o subset.o 
> summary.o
> > unique.o util.o version.o vfonts.o registration.o xxxpr.o
> > ../unix/libunix.a ../appl/libappl.a ../nmath/libnmath.a   
> -L/usr/local/lib
> > -L/usr/local/lib/gcc-lib/powerpc-ibm-aix5.1.0.0/3.3.3
> > 
> -L/usr/local/lib/gcc-lib/powerpc-ibm-aix5.1.0.0/3.3.3/../../..
> /../powerpc-ibm-aix5.1.0.0/lib
> > 
> -L/usr/local/lib/gcc-lib/powerpc-ibm-aix5.1.0.0/3.3.3/../../..
>  -lfrtbegin
> > -lg2c -lm -lgcc_s
> > /usr/local/lib/gcc-lib/powerpc-ibm-aix5.1.0.0/3.3.3/libgcc.a -lg
> > /lib/crt0.o ../extra/zlib/libz.a ../extra/bzip2/libbz2.a
> > ../extra/pcre/libpcre.a -ldl -lm -lc
> > /usr/local/bin/ld: target dynamic not found
> > collect2: ld returned 1 exit status
> > make[3]: *** [R.bin] Error 1
> > make[3]: Leaving directory `/d1/ftcheung_GoIn/tmp/R-1.9.1/src/main'
> > make[2]: *** [R] Error 2
> > make[2]: Leaving directory `/d1/ftcheung_GoIn/tmp/R-1.9.1/src/main'
> > make[1]: *** [R] Error 1
> > make[1]: Leaving directory `/d1/ftcheung_GoIn/tmp/R-1.9.1/src'
> > make: *** [R] Error 1
> > 
> > I've tried to use AIX xlc compiler and GCC compiler (v2.95 
> and v3.3.3) but
> > all of them result to the same error message shown above. 
> Can anyone give
> > me some hint of how to solve it?
> 
> You appear not to be using the standard loader 
> (`/usr/local/bin/ld').  
> Please set MAIN_LDFLAGS (ideally in config.site before 
> configure, but you
> can edit Makeconf now) to whatever your loader needs.  If 
> this is GNU ld
> then I guess you need -Wl,--export-dynamic.
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From umalvarez at fata.unam.mx  Wed Jun 23 17:08:50 2004
From: umalvarez at fata.unam.mx (Ulises Mora Alvarez)
Date: Wed, 23 Jun 2004 10:08:50 -0500 (CDT)
Subject: [R] gui for MacOS X / Aqua
In-Reply-To: <200406230923.30599.chrysopa@insecta.ufv.br>
Message-ID: <Pine.LNX.4.44.0406231001470.7969-100000@athena.fata.unam.mx>

Hello,

Yes, indeed. All you have to do it's: launch your X server (take a look 
at your /Applications/Utilities folder), and then launch R from an xterm.

If you don't have your X server installed, take a look at the RAqua-FAQ 
and the R-FAQ on CRAN.

If you need more help, please don't hesitate in letting me know.

Good look.

On Wed, 23 Jun 2004, Ronaldo Reis Jr. wrote:

> Hi,
> 
> anybody know if Rcmdr or any other GUI like this work with R/Aqua?
> 
> Thanks
> Ronaldo
> 

-- 
Ulises M. Alvarez
LAB. DE ONDAS DE CHOQUE
FISICA APLICADA Y TECNOLOGIA AVANZADA
UNAM
umalvarez at fata.unam.mx



From xiaoliu at jhmi.edu  Wed Jun 23 17:10:31 2004
From: xiaoliu at jhmi.edu (XIAO LIU)
Date: Wed, 23 Jun 2004 11:10:31 -0400
Subject: [R] Perl--R interface
Message-ID: <84467821.78218446@jhmimail.jhmi.edu>

R users:

My R is 1.8.1 in Linux.  How can I call R in Perl process? And call Perl from R?

Thanks

Xiao



From mlbiss at yahoo.com  Wed Jun 23 17:22:36 2004
From: mlbiss at yahoo.com (Marea Bissonnette)
Date: Wed, 23 Jun 2004 08:22:36 -0700 (PDT)
Subject: [R] Problem with XML package compilation on AIX
Message-ID: <20040623152236.73223.qmail@web60708.mail.yahoo.com>

Hi,

I hope someone has seen this problem before.  I am
trying to install the XML package from
http://cran.r-project.org/src/contrib/XML_0.95-6.tar.gz.

Using 

> R CMD INSTALL XML_0.95-6.tar.gz 

I get the following error message:

/usr/vac/bin/xlc_r -I/usr/local/lib/R/include -DLIBXML
-I/opt/freeware/include/libxml2
-DUSE_EXTERNAL_SUBSET=1 -DROOT_HAS_DTD_NODE=1
-DDUMP_WITH_ENCODING=1 -DUSE_XML_VERSION_H=1
-DXML_ELEMENT_ETYPE=1 -DXML_ATTRIBUTE_ATYPE=1
-DNO_XML_HASH_SCANNER_RETURN=1 -DUSE_R=1 -D_R_=1 
-DHAVE_VALIDITY=1 -I. -DLIBXML2=1 -I/usr/local/include
    -O -qstrict -c DocParse.c -o DocParse.o
"DocParse.c", line 113.29: 1506-280 (E) Function
argument assignment between types "const unsigned
char*" and "char*" is not allowed.
and then more of the same on other lines.

Does anyone know what I can do to get this to compile
happy and install?  Any help would be appreciated.

I'll put the full transcript inline below in case that
helps.

M.


[root at node1] /export/home/mlb# R CMD INSTALL XML*.gz
* Installing *source* package 'XML' ...
checking for gcc... gcc
checking for C compiler default output... a.out
checking whether the C compiler works... yes
checking whether we are cross compiling... no
checking for suffix of executables... 
checking for suffix of object files... o
checking whether we are using the GNU C compiler...
yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ANSI C... none
needed
checking how to run the C preprocessor... gcc -E
checking for xml-config... no
checking for xml2-config... /usr/bin/xml2-config
Using libxml version 2.
Located parser file
-I/opt/freeware/include/libxml2/parser.h
Checking for 1.8:  -I/opt/freeware/include/libxml2
Using libxml2.*
checking for gzopen in -lz... yes
checking for xmlParseFile in -lxml2... yes
checking for xmlHashSize in -lxml2... yes
Using built-in xmlHashSize
Checking DTD parsing (presence of externalSubset)...
checking for xmlHashSize in -lxml2... yes
Found xmlHashSize
checking for xmlDocDumpFormatMemoryEnc in -lxml2...
yes
checking libxml/xmlversion.h usability... yes
checking libxml/xmlversion.h presence... yes
checking for libxml/xmlversion.h... yes
Expat:  FALSE
Checking for return type of xmlHashScan element
routine.
No return value for xmlHashScan

****************************************
Configuration information:

Libxml settings

libxml include directory:
-I/opt/freeware/include/libxml2
libxml library directory: -L/opt/freeware/lib -lxml2
-lz -lpthread -liconv -lm -lz  -lxml2
libxml 2:                 -DLIBXML2=1

Compilation flags:         -DLIBXML
-I/opt/freeware/include/libxml2
-DUSE_EXTERNAL_SUBSET=1 -DROOT_HAS_DTD_NODE=1
-DDUMP_WITH_ENCODING=1 -DUSE_XML_VERSION_H=1
-DXML_ELEMENT_ETYPE=1 -DXML_ATTRIBUTE_ATYPE=1
-DNO_XML_HASH_SCANNER_RETURN=1
Link flags:               -L/opt/freeware/lib -lxml2
-lz -lpthread -liconv -lm -lz  -lxml2

****************************************
configure: creating ./config.status
config.status: creating src/Makevars
config.status: creating R/supports.R
config.status: creating inst/scripts/RSXML.csh
config.status: creating inst/scripts/RSXML.bsh
** libs
        /usr/vac/bin/xlc_r -I/usr/local/lib/R/include
-DLIBXML -I/opt/freeware/include/libxml2
-DUSE_EXTERNAL_SUBSET=1 -DROOT_HAS_DTD_NODE=1
-DDUMP_WITH_ENCODING=1 -DUSE_XML_VERSION_H=1
-DXML_ELEMENT_ETYPE=1 -DXML_ATTRIBUTE_ATYPE=1
-DNO_XML_HASH_SCANNER_RETURN=1 -DUSE_R=1 -D_R_=1 
-DHAVE_VALIDITY=1 -I. -DLIBXML2=1 -I/usr/local/include
    -O -qstrict -c DocParse.c -o DocParse.o
"DocParse.c", line 113.29: 1506-280 (E) Function
argument assignment between types "const unsigned
char*" and "char*" is not allowed.
"DocParse.c", line 113.17: 1506-068 (E) Operation
between types "char*" and "unsigned char*" is not
allowed.
"DocParse.c", line 176.88: 1506-275 (S) Unexpected
text ',' encountered.
"DocParse.c", line 205.17: 1506-068 (E) Operation
between types "const char*" and "const unsigned char*"
is not allowed.
"DocParse.c", line 261.56: 1506-280 (E) Function
argument assignment between types "const char*" and
"const unsigned char*" is not allowed.
"DocParse.c", line 292.22: 1506-196 (E) Initialization
between types "char*" and "unsigned char*" is not
allowed.
"DocParse.c", line 300.29: 1506-280 (E) Function
argument assignment between types "char*" and
"unsigned char*" is not allowed.
"DocParse.c", line 334.75: 1506-280 (E) Function
argument assignment between types "const char*" and
"const unsigned char*" is not allowed.
"DocParse.c", line 354.61: 1506-280 (E) Function
argument assignment between types "const char*" and
"const unsigned char*" is not allowed.
"DocParse.c", line 384.19: 1506-068 (E) Operation
between types "const char*" and "const unsigned char*"
is not allowed.
"DocParse.c", line 481.98: 1506-280 (E) Function
argument assignment between types "const char*" and
"unsigned char*" is not allowed.
"DocParse.c", line 484.86: 1506-280 (E) Function
argument assignment between types "const char*" and
"const unsigned char*" is not allowed.
"DocParse.c", line 502.55: 1506-280 (E) Function
argument assignment between types "const char*" and
"const unsigned char*" is not allowed.
"DocParse.c", line 603.65: 1506-280 (E) Function
argument assignment between types "const char*" and
"const unsigned char*" is not allowed.
"DocParse.c", line 675.147: 1506-280 (E) Function
argument assignment between types "const char*" and
"unsigned char*" is not allowed.
"DocParse.c", line 686.67: 1506-280 (E) Function
argument assignment between types "const char*" and
"const unsigned char*" is not allowed.
make: 1254-004 The error code from the last command is
1.


Stop.
ERROR: compilation failed for package 'XML'
** Removing '/usr/local/lib/R/library/XML'



From karlknoblich at yahoo.de  Wed Jun 23 18:07:19 2004
From: karlknoblich at yahoo.de (=?iso-8859-1?q?Karl=20Knoblick?=)
Date: Wed, 23 Jun 2004 18:07:19 +0200 (CEST)
Subject: [R] Fitting function with if-clause (nls; e.g. heaviside)
Message-ID: <20040623160719.48174.qmail@web52508.mail.yahoo.com>

Hallo!

I want to fit a function. The function is e.g.:
y = c+m1*x if x<0, c+m2*x if x>=0
where m1, m2 and c is a parameter and x, y are
variables of a data frame.

I think using  nls is appropriate. But I do not know,
how to type this formula in nls. Can anybody help?

(If there is a possibility to use a Heaviside-function
this would be enough.)

Karl


	

	
		
___________________________________________________________
Bestellen Sie Y! DSL und erhalten Sie die AVM "FritzBox SL" f??r 0.
Sie sparen 119 und bekommen 2 Monate Grundgeb??hrbefreiung.



From ggrothendieck at myway.com  Wed Jun 23 18:10:14 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 23 Jun 2004 16:10:14 +0000 (UTC)
Subject: [R] Automatic routine - help
References: <E1Bd3jE-0009WH-Vy@serenity.mcc.ac.uk>
Message-ID: <loom.20040623T175631-690@post.gmane.org>


You already have a few solutions but here is one more:

   (z >= 0 & z <= 1) * z + (z > 1)

It uses the fact that TRUE and FALSE act as 1 and 0 in arithmetic.

Slightly shorter but trickier is:

   (z*z <= z) * z + (z > 1)

which uses the fact that z*z <= z only in the interval [0,1] .

Monica Palaseanu-Lovejoy <monica.palaseanu-lovejoy <at> stud.man.ac.uk> writes:

: 
: Hi,
: 
: I would like to write a little automatic routine in R, but i am a too 
: much of a beginner for that. I will appreciate any help regarding this 
: particular problem.
: 
: Lets suppose I have a data.frame with j columns (from 1 to n) and i 
: rows (from 1 to p). I would like to write a procedure which reads 
: every column j (j from 1 to n) and compare each value with the 
: interval [0,1]. If z(i,j) is less than 0, then replace z(i,j) with 0. If z
(i,j) 
: is greater than 1, then replace z(i,j) with 1. If z(i,j) is inside the 
: interval [0,1] then dont change. In the end I would like to have a 
: new data.frame with the new values. 
: 
: I am not sure how complicated or long such a procedure might be, 
: so I will be very grateful for any help.
: 
: Thank you in advance,
: 
: Monica
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://www.stat.math.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
: 
:



From Arne.Muller at aventis.com  Wed Jun 23 18:17:14 2004
From: Arne.Muller at aventis.com (Arne.Muller@aventis.com)
Date: Wed, 23 Jun 2004 18:17:14 +0200
Subject: [R] Perl--R interface
Message-ID: <C80ECAFA2ACC1B45BE45D133ED660ADE01846130@crbsmxsusr04.pharma.aventis.com>

Hi,

look at http://www.omegahat.org/RSPerl/index.html. 

	regards,
	
	Arne

--
Arne Muller, Ph.D.
Toxicogenomics, Aventis Pharma
arne dot muller domain=aventis com

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of XIAO LIU
> Sent: 23 June 2004 17:11
> To: r-help at stat.math.ethz.ch
> Subject: [R] Perl--R interface
> 
> 
> R users:
> 
> My R is 1.8.1 in Linux.  How can I call R in Perl process? 
> And call Perl from R?
> 
> Thanks
> 
> Xiao
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From ripley at stats.ox.ac.uk  Wed Jun 23 18:20:28 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 23 Jun 2004 17:20:28 +0100 (BST)
Subject: [R] Fitting function with if-clause (nls; e.g. heaviside)
In-Reply-To: <20040623160719.48174.qmail@web52508.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0406231717450.11134-100000@gannet.stats>

On Wed, 23 Jun 2004, Karl Knoblick wrote:

> I want to fit a function. The function is e.g.:
> y = c+m1*x if x<0, c+m2*x if x>=0
> where m1, m2 and c is a parameter and x, y are
> variables of a data frame.
> 
> I think using  nls is appropriate. But I do not know,
> how to type this formula in nls. Can anybody help?

It's a linear model (linear in the params)

lm( y ~ I(x*(x>0) + I(x*(x<0)))

although I would define some new variables to make that easier to read, 
e.g.

xplus <- x*(x>0)
xminus <- x*(x<0)
lm(y ~ xplus + xminus)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Tom.Joy at rmbinternational.com  Wed Jun 23 18:35:37 2004
From: Tom.Joy at rmbinternational.com (Joy, Tom)
Date: Wed, 23 Jun 2004 17:35:37 +0100
Subject: [R] help with win.print
Message-ID: <576A4B4724813147AF478CAC91B8CF71A98272@bank.rmbi.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040623/8c939ee4/attachment.pl

From ripley at stats.ox.ac.uk  Wed Jun 23 18:56:19 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 23 Jun 2004 17:56:19 +0100 (BST)
Subject: [R] help with win.print
In-Reply-To: <576A4B4724813147AF478CAC91B8CF71A98272@bank.rmbi.com>
Message-ID: <Pine.LNX.4.44.0406231754410.11269-100000@gannet.stats>

Do remember that you need to double backslashes in R strings ... see the 
rw-FAQ.  As I recall forward slashes work here too.

On Wed, 23 Jun 2004, Joy, Tom wrote:

> I am trying to batch the printing of some graphs using win.print when I leave the printer option blank it prompts me for a printer each time and then works fine when I put the printer name in however using the following code I get the error below. Does the printer need to be locally installed or can it print to a network printer like I am trying to?
> 
> win.print(width=8, height=10.5,printer="\\LBPS\LBMARK_4200")
>  
> Error in devga(paste("win.print:", printer, sep = ""), width, height,  : 
>         unable to start device devga

Try printer="\\\\LBPS\\LBMARK_4200"

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From laura at env.leeds.ac.uk  Wed Jun 23 18:59:44 2004
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Wed, 23 Jun 2004 17:59:44 +0100 (BST)
Subject: [R] Contour plots of PCA loadings
Message-ID: <Pine.LNX.4.44.0406231754530.15416-100000@env-pc-phd13>

Hi,

I was wondering if there is a way of plotting smoothed contours of PCA
loadings onto a map. At the moment I have the loadings plotted as
positive/negative (red/blue) scaled circles centred around where the
physical stations are on a map - but I was wondering if there is a nicer
way of plotting this data, perhaps as contours of "regions of influence"
or somesuch?

Any suggestions would be gratefully received!

Thank you in advance,
Laura



From ligges at statistik.uni-dortmund.de  Wed Jun 23 19:04:04 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 23 Jun 2004 19:04:04 +0200
Subject: [R] help with win.print
In-Reply-To: <Pine.LNX.4.44.0406231754410.11269-100000@gannet.stats>
References: <Pine.LNX.4.44.0406231754410.11269-100000@gannet.stats>
Message-ID: <40D9B804.3020307@statistik.uni-dortmund.de>

Prof Brian Ripley wrote:

> Do remember that you need to double backslashes in R strings ... see the 
> rw-FAQ.  As I recall forward slashes work here too.
> 
> On Wed, 23 Jun 2004, Joy, Tom wrote:
> 
> 
>>I am trying to batch the printing of some graphs using win.print when I leave the printer option blank it prompts me for a printer each time and then works fine when I put the printer name in however using the following code I get the error below. Does the printer need to be locally installed or can it print to a network printer like I am trying to?
>>
>>win.print(width=8, height=10.5,printer="\\LBPS\LBMARK_4200")
>> 
>>Error in devga(paste("win.print:", printer, sep = ""), width, height,  : 
>>        unable to start device devga
> 
> 
> Try printer="\\\\LBPS\\LBMARK_4200"
> 

I think it is only possible to specify names that are knwon by Windows 
to be installed printers.
Hence after installing the printer and calling it, e.g., LBMARK4200 on 
your system, you can say "printer = "LBMARK4200"

Uwe Ligges



From roebuck at odin.mdacc.tmc.edu  Wed Jun 23 19:41:49 2004
From: roebuck at odin.mdacc.tmc.edu (Paul Roebuck)
Date: Wed, 23 Jun 2004 12:41:49 -0500 (CDT)
Subject: [R] [Q] GET_DIM() crash on Windows only
In-Reply-To: <40D931DD.7000406@statistik.uni-dortmund.de>
References: <Pine.OSF.4.58.0406221516520.368009@odin.mdacc.tmc.edu>
	<40D931DD.7000406@statistik.uni-dortmund.de>
Message-ID: <Pine.OSF.4.58.0406231209310.424935@odin.mdacc.tmc.edu>

On Wed, 23 Jun 2004, Uwe Ligges wrote:

> Paul Roebuck wrote:
>
> > I have the following contrived code in package format.
> > On Solaris and Mac OS X, code runs just fine. On Windows,
> > it crashes the R environment with the "Send Bug Report"
> > dialog. I tried R 1.8.1 (Win2K) and R 1.9 (WinXP) binaries
> > with the same result. PCs otherwise appear properly
> > configured for creating R packages. Anything blatantly
> > wrong? Suggestions?
>
> Works for me (R-1.9.1, WinNT4.0), even with gctorture(TRUE).
> Did you use the recommended compiler and tools?

To the best of my knowledge, I did. I upgraded to R 1.9.1 to no
avail. What else should I look for at this point? Is there an
R package developer setup lint utility?



Here's my script:

@cls
@SETLOCAL
@set RBINDIR=C:\R\rw1091\bin
@set TOOLSBINDIR=C:\Rtools\bin
@set MINGWBINDIR=C:\MinGW\bin
@set PERLBINDIR=C:\Perl\bin
@set TEXBINDIR=C:\PROGRA~1\TeXLive\bin\win32
@set HCCBINDIR=C:\PROGRA~1\HTMLHE~1
@set PATH=%TOOLSBINDIR%;%RBINDIR%;%MINGWBINDIR%;%PERLBINDIR%;%TEXBINDIR%;%HCCBINDIR%;%WINDIR%\system32;%WINDIR%
@echo PATH=%PATH%
Rcmd build -binary getdim
Rcmd check getdim
@ENDLOCAL


Script output
-----------------
PATH=C:\Rtools\bin;C:\R\rw1091\bin;C:\MinGW\bin;C:\Perl\bin;C:\PROGRA~1\TeXLive\
bin\win32;C:\PROGRA~1\HTMLHE~1;C:\WINDOWS\system32;C:\WINDOWS

Z:\R\examples\getdim>Rcmd build -binary getdim
* checking for file 'getdim/DESCRIPTION' ... OK
installing R.css in C:/tmp/Rbuild.2304


---------- Making package getdim ------------
  adding build stamp to DESCRIPTION
  making DLL ...
  ... DLL made
  installing DLL
  installing R files
  installing man source files
  installing indices
  installing help
 >>> Building/Updating help pages for package 'getdim'
     Formats: text html latex example
  getdim                            text    html    latex   example
 >>> Building/Updating help pages for package 'getdim'
     Formats: chm
  getdim                                                            chm
Microsoft HTML Help Compiler 4.74.8702

Compiling z:\R\examples\getdim\getdim\chm\getdim.chm


Compile time: 0 minutes, 1 second
2       Topics
1       Local link
0       Internet links
1       Graphic


Created z:\R\examples\getdim\getdim\chm\getdim.chm, 20,587 bytes
Compression increased file by 9,197 bytes.
  adding MD5 sums

* DONE

* building 'getdim_1.0.zip'
  adding: getdim/ (stored 0%)
  adding: getdim/chtml/ (stored 0%)
  adding: getdim/chtml/getdim.chm (deflated 38%)
  adding: getdim/CONTENTS (deflated 20%)
  adding: getdim/DESCRIPTION (deflated 30%)
  adding: getdim/help/ (stored 0%)
  adding: getdim/help/AnIndex (deflated 13%)
  adding: getdim/help/getdim (deflated 45%)
  adding: getdim/html/ (stored 0%)
  adding: getdim/html/00Index.html (deflated 49%)
  adding: getdim/html/getdim.html (deflated 43%)
  adding: getdim/INDEX (stored 0%)
  adding: getdim/latex/ (stored 0%)
  adding: getdim/latex/getdim.tex (deflated 45%)
  adding: getdim/libs/ (stored 0%)
  adding: getdim/libs/getdim.dll (deflated 66%)
  adding: getdim/man/ (stored 0%)
  adding: getdim/man/getdim.Rd (deflated 30%)
  adding: getdim/MD5 (deflated 37%)
  adding: getdim/Meta/ (stored 0%)
  adding: getdim/Meta/hsearch.rds (deflated 64%)
  adding: getdim/Meta/Rd.rds (deflated 66%)
  adding: getdim/R/ (stored 0%)
  adding: getdim/R/getdim (deflated 37%)
  adding: getdim/R-ex/ (stored 0%)
  adding: getdim/R-ex/getdim.R (deflated 23%)


Z:\R\examples\getdim>Rcmd check getdim
* checking for working latex ... OK
* using log directory 'Z:/R/examples/getdim/getdim.Rcheck'
* checking for file 'getdim/DESCRIPTION' ... OK
* checking if this is a source package ... OK

installing R.css in Z:/R/examples/getdim/getdim.Rcheck


---------- Making package getdim ------------
  adding build stamp to DESCRIPTION
  making DLL ...
  ... DLL made
  installing DLL
  installing R files
  installing man source files
  installing indices
  installing help
 >>> Building/Updating help pages for package 'getdim'
     Formats: text html latex example
  getdim                            text    html    latex   example
 >>> Building/Updating help pages for package 'getdim'
     Formats: chm
  adding MD5 sums

* DONE

* checking package directory ... OK
* checking for portable file names ... OK
* checking DESCRIPTION meta-information ... OK
* checking package dependencies ... OK
* checking index information ... OK
* checking package subdirectories ... OK
* checking R files for syntax errors ... OK
* checking R files for library.dynam ... OK
* checking S3 generic/method consistency ... OK
* checking for replacement functions with final arg not named 'value' ...
OK
* checking foreign function calls ... OK
* checking Rd files ... OK
* checking for missing documentation entries ... OK
* checking for code/documentation mismatches ... OK
* checking Rd \usage sections ... OK
* checking for CRLF line endings in C sources/headers ... OK
* creating getdim-Ex.R ... OK
* checking examples ... ERROR
Running examples in getdim-Ex.R failed.
The error most likely occurred in:

> ### * getdim
>
> flush(stderr()); flush(stdout())
>
> ### Name: getdim
> ### Title: Return dimensions of matrix
> ### Aliases: getdim
> ### Keywords: internal
>
> ### ** Examples
>
> x <- matrix(1:6, 3, 2)
> getdim(x)
In getdim(x)...
In GetMatrixDimen()...

Z:\R\examples\getdim>



----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From roebuck at odin.mdacc.tmc.edu  Wed Jun 23 19:47:33 2004
From: roebuck at odin.mdacc.tmc.edu (Paul Roebuck)
Date: Wed, 23 Jun 2004 12:47:33 -0500 (CDT)
Subject: [R] Perl--R interface
In-Reply-To: <84467821.78218446@jhmimail.jhmi.edu>
References: <84467821.78218446@jhmimail.jhmi.edu>
Message-ID: <Pine.OSF.4.58.0406231245130.424935@odin.mdacc.tmc.edu>

On Wed, 23 Jun 2004, XIAO LIU wrote:

> My R is 1.8.1 in Linux.  How can I call R in Perl process?
> And call Perl from R?
>

This is only one direction but worth a look as an
alternative.

<http://search.cpan.org/~gmpassos/Statistics-R-0.01/>

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From a.beckerman at sheffield.ac.uk  Wed Jun 23 19:48:20 2004
From: a.beckerman at sheffield.ac.uk (Andrew Beckerman)
Date: Wed, 23 Jun 2004 18:48:20 +0100
Subject: [R] converting apply output
In-Reply-To: <6.0.1.1.2.20040623141250.021fc640@stat4ux.stat.ucl.ac.be>
References: <847BDFC0-C503-11D8-BC4D-000A95CD7F02@sheffield.ac.uk>
	<6.0.1.1.2.20040623141250.021fc640@stat4ux.stat.ucl.ac.be>
Message-ID: <83F3324E-C53D-11D8-90BB-000A95CD7F02@sheffield.ac.uk>

Thanks eric... I figured this routine out as well.

cvt<-function(dat){
x<-as.list(rep(0,dim(dat)[2]))
for(i in 1:dim(dat)[2]){
x[[i]]<-dat[,i]
x}}

# get ragged array of 1's
dat<-apply(mat,2,function(x) which(x==1))
# deal with this using cvt to creat list
if(is.null(dim(dat))) dat2<-dat else dat2<-cvt(dat)

On 23 Jun 2004, at 13:13, Eric Lecoutre wrote:

>
> Hi,
>
> Why not try with the data.frame structure, wich internally yet  
> consists in a list:
>
> > lapply(as.data.frame(m1),function(x) which(x==1))
> $V1
> [1] 8 9
> $V2
> [1]  9 10
> [...]
>
> Eric
>
> At 12:53 23/06/2004, Andrew Beckerman wrote:
>> Hi -
>>
>> platform powerpc-apple-darwin6.8
>> status
>> major    1
>> minor    9.0
>> year     2004
>>
>> I am trying to deal with the output of apply().  As indicated, when  
>> each call to 'FUN' returns a vector of length 'n', then 'apply'
>> returns an array of dimension 'c(n, dim(X)[MARGIN])'.  However, I  
>> would like this to be a list in the same format as is produced when  
>> 'FUN' return vectors of different lengths ('apply'   returns a list  
>> of length 'dim(X)[MARGIN]').
>>
>> e.g.
>> tt1<-c(0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,  
>> 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ,0 ,0 ,0 ,0 ,0  
>> ,0 ,1 ,0 ,1 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,1 ,1 ,0 ,0 ,0 ,0 ,0 ,0 ,0 ,1 ,0  
>> ,0 ,0 ,1 ,0 ,0 ,0 ,0 ,0 ,1 ,0 ,0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,  
>> 0, 0, 0, 0, 0, 0, 1, 0, 1, 0)
>> m1<-matrix(tt1,10,10)
>> out<-apply(m1,2,function(x) which(x==1))
>>
>> produces an array,
>> > out
>>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
>> [1,]    8    9    7    8    6    7    6    6    6     7
>> [2,]    9   10    8    9    8    8   10   10    7     9
>>
>> but I would like out as a list of 10 elements with two elements in  
>> each, e.g.
>>
>> [[1]]
>> [1]  8 9
>>
>> [[2]]
>> [1] 9 10
>> etc.
>>
>> I have tried apply(out,2,function(x) list(x))), but the subsrcripting  
>> is not equal to the pattern when FUN returns a vectors of different  
>> length.  Any help would be appreciated.
>>
>> Cheers
>> andrew
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!  
>> http://www.R-project.org/posting-guide.html
>
> Eric Lecoutre
> UCL /  Institut de Statistique
> Voie du Roman Pays, 20
> 1348 Louvain-la-Neuve
> Belgium
>
> tel: (+32)(0)10473050
> lecoutre at stat.ucl.ac.be
> http://www.stat.ucl.ac.be/ISpersonnel/lecoutre
>
> If the statistics are boring, then you've got the wrong numbers.  
> -Edward Tufte
>
>
------------------------------------------------------------------------ 
---------
Dr. Andrew Beckerman
Department of Animal and Plant Sciences, University of Sheffield,
Alfred Denny Building, Western Bank, Sheffield S10 2TN, UK
ph +44 (0)114 222 0026; fx +44 (0)114 222 0002
http://www.shef.ac.uk/beckslab
------------------------------------------------------------------------ 
----------



From dmb at mrc-dunn.cam.ac.uk  Wed Jun 23 20:29:54 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Wed, 23 Jun 2004 19:29:54 +0100 (BST)
Subject: [R] Sorting elements in a data.frame
Message-ID: <Pine.LNX.4.21.0406231924040.17979-100000@mail.mrc-dunn.cam.ac.uk>


Hi,

I have data like this....

print(x)

ID	VAL1	VAL2
1	2	6
2	4	9
3	45	12
4	99	44

What I would like is data like this...

ID	VAL1	VAL2
1	2	6
2	4	9
3	12	45
4	44	99


So that my analysis of the ratio VAL2/VAL1 is somehow uniform.

Any advice is welcome!

Dan.



From andy_liaw at merck.com  Wed Jun 23 20:16:43 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 23 Jun 2004 14:16:43 -0400
Subject: [R] Sorting elements in a data.frame
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7F5B@usrymx25.merck.com>

Do you mean:

> pmax(x[,1], x[,2]) / pmin(x[,1], x[,2])
[1] 3.00 2.25 3.75 2.25

??

Andy

> From: Dan Bolser
> 
> Hi,
> 
> I have data like this....
> 
> print(x)
> 
> ID	VAL1	VAL2
> 1	2	6
> 2	4	9
> 3	45	12
> 4	99	44
> 
> What I would like is data like this...
> 
> ID	VAL1	VAL2
> 1	2	6
> 2	4	9
> 3	12	45
> 4	44	99
> 
> 
> So that my analysis of the ratio VAL2/VAL1 is somehow uniform.
> 
> Any advice is welcome!
> 
> Dan.



From dmb at mrc-dunn.cam.ac.uk  Wed Jun 23 20:49:31 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Wed, 23 Jun 2004 19:49:31 +0100 (BST)
Subject: [R] Sorting elements in a data.frame
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7F5B@usrymx25.merck.com>
Message-ID: <Pine.LNX.4.21.0406231942530.17979-100000@mail.mrc-dunn.cam.ac.uk>

On Wed, 23 Jun 2004, Liaw, Andy wrote:

>Do you mean:
>
>> pmax(x[,1], x[,2]) / pmin(x[,1], x[,2])


Magic! 

I needed a p!





>Andy
>
>> From: Dan Bolser
>> 
>> Hi,
>> 
>> I have data like this....
>> 
>> print(x)
>> 
>> ID	VAL1	VAL2
>> 1	2	6
>> 2	4	9
>> 3	45	12
>> 4	99	44
>> 
>> What I would like is data like this...
>> 
>> ID	VAL1	VAL2
>> 1	2	6
>> 2	4	9
>> 3	12	45
>> 4	44	99
>> 
>> 
>> So that my analysis of the ratio VAL2/VAL1 is somehow uniform.
>> 
>> Any advice is welcome!
>> 
>> Dan.
>
>
>------------------------------------------------------------------------------
>Notice:  This e-mail message, together with any attachments, contains information of Merck & Co., Inc. (One Merck Drive, Whitehouse Station, New Jersey, USA 08889), and/or its affiliates (which may be known outside the United States as Merck Frosst, Merck Sharp & Dohme or MSD and in Japan, as Banyu) that may be confidential, proprietary copyrighted and/or legally privileged. It is intended solely for the use of the individual or entity named on this message.  If you are not the intended recipient, and have received this message in error, please notify us immediately by reply e-mail and then delete it from your system.
>------------------------------------------------------------------------------
>



From ihok at hotmail.com  Wed Jun 23 20:45:12 2004
From: ihok at hotmail.com (Jack Tanner)
Date: Wed, 23 Jun 2004 14:45:12 -0400
Subject: [R] assigning from multiple return values
Message-ID: <40D9CFB8.2020702@hotmail.com>

I know that if I have a function that returns multiple values, I should
do return(list(foo, bar)). But what do I do on the recieving end?

fn <- function(x) {
   return(list(foo, bar))
}

I know that at this point I could say

values.list <- fn(x)

and then access

values.list[1]
values.list[2]

But that's hideous. I'd rather be able to say something like

list(local_foo, local_bar) <- fn(x)

and have the right thing happen. I realize that it's my responsibility
to not screw up and say instead

list(local_bar, local_foo)

Any suggestions?

-JT



From roebuck at odin.mdacc.tmc.edu  Wed Jun 23 21:06:24 2004
From: roebuck at odin.mdacc.tmc.edu (Paul Roebuck)
Date: Wed, 23 Jun 2004 14:06:24 -0500 (CDT)
Subject: [R] assigning from multiple return values
In-Reply-To: <40D9CFB8.2020702@hotmail.com>
References: <40D9CFB8.2020702@hotmail.com>
Message-ID: <Pine.OSF.4.58.0406231400550.424935@odin.mdacc.tmc.edu>

On Wed, 23 Jun 2004, Jack Tanner wrote:

> I know that if I have a function that returns multiple values, I should
> do return(list(foo, bar)). But what do I do on the recieving end?
>
> fn <- function(x) {
>    return(list(foo, bar))
> }
>
> I know that at this point I could say
>
> values.list <- fn(x)
>
> and then access
>
> values.list[1]
> values.list[2]
>
> But that's hideous. I'd rather be able to say something like
>
> list(local_foo, local_bar) <- fn(x)
>
> and have the right thing happen. I realize that it's my responsibility
> to not screw up and say instead
>
> list(local_bar, local_foo)
>
> Any suggestions?
>

How about naming the list elements for clarity then?

fn <- function(x) {
    foo <- x;
    bar <- 1/x;
    return(list(foo = foo,
                bar = bar));
}

x <- 10;
values.list <- fn(x);
print(values.list$foo);
print(values.list$bar);


----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From eduarmasrs at yahoo.com.br  Wed Jun 23 21:30:24 2004
From: eduarmasrs at yahoo.com.br (Eduardo Dutra de Armas)
Date: Wed, 23 Jun 2004 16:30:24 -0300
Subject: [R] Sphericity test
Message-ID: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAAlkN94tU7pE2294PPHpOIAsKAAAAQAAAABZZzvAwzh0y1t16IweS2YwEAAAAA@yahoo.com.br>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040623/2d1e0083/attachment.pl

From ggrothendieck at myway.com  Wed Jun 23 21:43:41 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 23 Jun 2004 19:43:41 +0000 (UTC)
Subject: [R] assigning from multiple return values
References: <40D9CFB8.2020702@hotmail.com>
Message-ID: <loom.20040623T213120-112@post.gmane.org>


Here are two approaches assuming foo is "zz" and bar is 3.

FIRST

You could pass the return variables in the argument list and then
assign them in the caller's frame like this:

fn <- function(x,y) {
	assign(as.character(substitute(x)), "zz", sys.frame(-1))
	assign(as.character(substitute(y)), 3, sys.frame(-1))
}
fn(a,b)  # sets a to "zz" and b to 3

SECOND

You can make this a bit prettier, though not perfect, like this:


"list2<-" <- function(x,y,value) {
	assign(as.character(substitute(y)), value[[2]], sys.frame(-1))
	value[[1]]
}
fn <- function()list("zz",3)
a <- 1 # first arg must exist prior to invoking list2. Its value not important.
list2(a,b) <- fn()


The two problems with list2 are:

1. the first argument must exist prior to invoking list2 although its
actual value is immaterial since it just gets overwritten anyways.

2. It only works for 2 args although you could write a list3, list4, etc.

Maybe someone could comment on these deficiencies.


Jack Tanner <ihok <at> hotmail.com> writes:

: 
: I know that if I have a function that returns multiple values, I should
: do return(list(foo, bar)). But what do I do on the recieving end?
: 
: fn <- function(x) {
:    return(list(foo, bar))
: }
: 
: I know that at this point I could say
: 
: values.list <- fn(x)
: 
: and then access
: 
: values.list[1]
: values.list[2]
: 
: But that's hideous. I'd rather be able to say something like
: 
: list(local_foo, local_bar) <- fn(x)
: 
: and have the right thing happen. I realize that it's my responsibility
: to not screw up and say instead
: 
: list(local_bar, local_foo)
: 
: Any suggestions?
: 
: -JT
:



From andy_liaw at merck.com  Wed Jun 23 21:51:10 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Wed, 23 Jun 2004 15:51:10 -0400
Subject: [R] assigning from multiple return values
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7F63@usrymx25.merck.com>

My $0.02:

If Jack feels that the fact that functions usually return a single list, and
one needs to access the components of the list separately, is somehow
hideous, then I'd rather suggest that R is perhaps the wrong language for
him.

To me the suggested `workarounds' are by far much more hideous...  They turn
perfectly tranparent code into...  gee, I don't even know how to begin
describing them...

Andy

> From: Gabor Grothendieck
> 
> 
> Here are two approaches assuming foo is "zz" and bar is 3.
> 
> FIRST
> 
> You could pass the return variables in the argument list and then
> assign them in the caller's frame like this:
> 
> fn <- function(x,y) {
> 	assign(as.character(substitute(x)), "zz", sys.frame(-1))
> 	assign(as.character(substitute(y)), 3, sys.frame(-1))
> }
> fn(a,b)  # sets a to "zz" and b to 3
> 
> SECOND
> 
> You can make this a bit prettier, though not perfect, like this:
> 
> 
> "list2<-" <- function(x,y,value) {
> 	assign(as.character(substitute(y)), value[[2]], sys.frame(-1))
> 	value[[1]]
> }
> fn <- function()list("zz",3)
> a <- 1 # first arg must exist prior to invoking list2. Its 
> value not important.
> list2(a,b) <- fn()
> 
> 
> The two problems with list2 are:
> 
> 1. the first argument must exist prior to invoking list2 although its
> actual value is immaterial since it just gets overwritten anyways.
> 
> 2. It only works for 2 args although you could write a list3, 
> list4, etc.
> 
> Maybe someone could comment on these deficiencies.
> 
> 
> Jack Tanner <ihok <at> hotmail.com> writes:
> 
> : 
> : I know that if I have a function that returns multiple 
> values, I should
> : do return(list(foo, bar)). But what do I do on the recieving end?
> : 
> : fn <- function(x) {
> :    return(list(foo, bar))
> : }
> : 
> : I know that at this point I could say
> : 
> : values.list <- fn(x)
> : 
> : and then access
> : 
> : values.list[1]
> : values.list[2]
> : 
> : But that's hideous. I'd rather be able to say something like
> : 
> : list(local_foo, local_bar) <- fn(x)
> : 
> : and have the right thing happen. I realize that it's my 
> responsibility
> : to not screw up and say instead
> : 
> : list(local_bar, local_foo)
> : 
> : Any suggestions?
> : 
> : -JT
> :
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From ligges at statistik.uni-dortmund.de  Wed Jun 23 21:56:13 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Wed, 23 Jun 2004 21:56:13 +0200
Subject: [R] [Q] GET_DIM() crash on Windows only
In-Reply-To: <Pine.OSF.4.58.0406231209310.424935@odin.mdacc.tmc.edu>
References: <Pine.OSF.4.58.0406221516520.368009@odin.mdacc.tmc.edu>
	<40D931DD.7000406@statistik.uni-dortmund.de>
	<Pine.OSF.4.58.0406231209310.424935@odin.mdacc.tmc.edu>
Message-ID: <40D9E05D.4020102@statistik.uni-dortmund.de>

Paul Roebuck wrote:
> On Wed, 23 Jun 2004, Uwe Ligges wrote:
> 
> 
>>Paul Roebuck wrote:
>>
>>
>>>I have the following contrived code in package format.
>>>On Solaris and Mac OS X, code runs just fine. On Windows,
>>>it crashes the R environment with the "Send Bug Report"
>>>dialog. I tried R 1.8.1 (Win2K) and R 1.9 (WinXP) binaries
>>>with the same result. PCs otherwise appear properly
>>>configured for creating R packages. Anything blatantly
>>>wrong? Suggestions?
>>
>>Works for me (R-1.9.1, WinNT4.0), even with gctorture(TRUE).
>>Did you use the recommended compiler and tools?
> 
> 
> To the best of my knowledge, I did. I upgraded to R 1.9.1 to no
> avail. What else should I look for at this point? Is there an
> R package developer setup lint utility?
> 
> 
> 
> Here's my script:
> 
> @cls
> @SETLOCAL
> @set RBINDIR=C:\R\rw1091\bin
> @set TOOLSBINDIR=C:\Rtools\bin
> @set MINGWBINDIR=C:\MinGW\bin
> @set PERLBINDIR=C:\Perl\bin
> @set TEXBINDIR=C:\PROGRA~1\TeXLive\bin\win32
> @set HCCBINDIR=C:\PROGRA~1\HTMLHE~1
> @set PATH=%TOOLSBINDIR%;%RBINDIR%;%MINGWBINDIR%;%PERLBINDIR%;%TEXBINDIR%;%HCCBINDIR%;%WINDIR%\system32;%WINDIR%
> @echo PATH=%PATH%
> Rcmd build -binary getdim

"--binary" is documented, but it should not matter in this case.


> Rcmd check getdim
> @ENDLOCAL
> 
> 
> Script output
> -----------------
> PATH=C:\Rtools\bin;C:\R\rw1091\bin;C:\MinGW\bin;C:\Perl\bin;C:\PROGRA~1\TeXLive\
> bin\win32;C:\PROGRA~1\HTMLHE~1;C:\WINDOWS\system32;C:\WINDOWS

Since there is no further error message, it seems to be OK.
Which version of gcc are you using - and which runtime version?

You might want to send me both the source and the binary version of your 
package in a private message, and I'll try some further checks....

  Uwe Ligges



> Z:\R\examples\getdim>Rcmd build -binary getdim
> * checking for file 'getdim/DESCRIPTION' ... OK
> installing R.css in C:/tmp/Rbuild.2304
> 
> 
> ---------- Making package getdim ------------
>   adding build stamp to DESCRIPTION
>   making DLL ...
>   ... DLL made
>   installing DLL
>   installing R files
>   installing man source files
>   installing indices
>   installing help
>  >>> Building/Updating help pages for package 'getdim'
>      Formats: text html latex example
>   getdim                            text    html    latex   example
>  >>> Building/Updating help pages for package 'getdim'
>      Formats: chm
>   getdim                                                            chm
> Microsoft HTML Help Compiler 4.74.8702
> 
> Compiling z:\R\examples\getdim\getdim\chm\getdim.chm
> 
> 
> Compile time: 0 minutes, 1 second
> 2       Topics
> 1       Local link
> 0       Internet links
> 1       Graphic
> 
> 
> Created z:\R\examples\getdim\getdim\chm\getdim.chm, 20,587 bytes
> Compression increased file by 9,197 bytes.
>   adding MD5 sums
> 
> * DONE
> 
> * building 'getdim_1.0.zip'
>   adding: getdim/ (stored 0%)
>   adding: getdim/chtml/ (stored 0%)
>   adding: getdim/chtml/getdim.chm (deflated 38%)
>   adding: getdim/CONTENTS (deflated 20%)
>   adding: getdim/DESCRIPTION (deflated 30%)
>   adding: getdim/help/ (stored 0%)
>   adding: getdim/help/AnIndex (deflated 13%)
>   adding: getdim/help/getdim (deflated 45%)
>   adding: getdim/html/ (stored 0%)
>   adding: getdim/html/00Index.html (deflated 49%)
>   adding: getdim/html/getdim.html (deflated 43%)
>   adding: getdim/INDEX (stored 0%)
>   adding: getdim/latex/ (stored 0%)
>   adding: getdim/latex/getdim.tex (deflated 45%)
>   adding: getdim/libs/ (stored 0%)
>   adding: getdim/libs/getdim.dll (deflated 66%)
>   adding: getdim/man/ (stored 0%)
>   adding: getdim/man/getdim.Rd (deflated 30%)
>   adding: getdim/MD5 (deflated 37%)
>   adding: getdim/Meta/ (stored 0%)
>   adding: getdim/Meta/hsearch.rds (deflated 64%)
>   adding: getdim/Meta/Rd.rds (deflated 66%)
>   adding: getdim/R/ (stored 0%)
>   adding: getdim/R/getdim (deflated 37%)
>   adding: getdim/R-ex/ (stored 0%)
>   adding: getdim/R-ex/getdim.R (deflated 23%)
> 
> 
> Z:\R\examples\getdim>Rcmd check getdim
> * checking for working latex ... OK
> * using log directory 'Z:/R/examples/getdim/getdim.Rcheck'
> * checking for file 'getdim/DESCRIPTION' ... OK
> * checking if this is a source package ... OK
> 
> installing R.css in Z:/R/examples/getdim/getdim.Rcheck
> 
> 
> ---------- Making package getdim ------------
>   adding build stamp to DESCRIPTION
>   making DLL ...
>   ... DLL made
>   installing DLL
>   installing R files
>   installing man source files
>   installing indices
>   installing help
>  >>> Building/Updating help pages for package 'getdim'
>      Formats: text html latex example
>   getdim                            text    html    latex   example
>  >>> Building/Updating help pages for package 'getdim'
>      Formats: chm
>   adding MD5 sums
> 
> * DONE
> 
> * checking package directory ... OK
> * checking for portable file names ... OK
> * checking DESCRIPTION meta-information ... OK
> * checking package dependencies ... OK
> * checking index information ... OK
> * checking package subdirectories ... OK
> * checking R files for syntax errors ... OK
> * checking R files for library.dynam ... OK
> * checking S3 generic/method consistency ... OK
> * checking for replacement functions with final arg not named 'value' ...
> OK
> * checking foreign function calls ... OK
> * checking Rd files ... OK
> * checking for missing documentation entries ... OK
> * checking for code/documentation mismatches ... OK
> * checking Rd \usage sections ... OK
> * checking for CRLF line endings in C sources/headers ... OK
> * creating getdim-Ex.R ... OK
> * checking examples ... ERROR
> Running examples in getdim-Ex.R failed.
> The error most likely occurred in:
> 
> 
>>### * getdim
>>
>>flush(stderr()); flush(stdout())
>>
>>### Name: getdim
>>### Title: Return dimensions of matrix
>>### Aliases: getdim
>>### Keywords: internal
>>
>>### ** Examples
>>
>>x <- matrix(1:6, 3, 2)
>>getdim(x)
> 
> In getdim(x)...
> In GetMatrixDimen()...
> 
> Z:\R\examples\getdim>
> 
> 
> 
> ----------------------------------------------------------
> SIGSIG -- signature too long (core dumped)
>



From p.dalgaard at biostat.ku.dk  Wed Jun 23 22:23:43 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 23 Jun 2004 22:23:43 +0200
Subject: [R] assigning from multiple return values
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7F63@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF7F63@usrymx25.merck.com>
Message-ID: <x23c4m6n1s.fsf@biostat.ku.dk>

"Liaw, Andy" <andy_liaw at merck.com> writes:

> My $0.02:
> 
> If Jack feels that the fact that functions usually return a single list, and
> one needs to access the components of the list separately, is somehow
> hideous, then I'd rather suggest that R is perhaps the wrong language for
> him.
> 
> To me the suggested `workarounds' are by far much more hideous...  They turn
> perfectly tranparent code into...  gee, I don't even know how to begin
> describing them...

I'm not sure that the idea of multiple assignment as such is all that
horrid. Other languages have them and it's actually kind of fun to see
if you can come up with a neat implementation. 

However, Gabor is definitely on the wrong track with list()<- It is
the kind of thing that looks like a good idea initially, but if you
think things through from a consistency viewpoint, you realize that it
doesn't quite make sense. Replacement functions are just that -
replacement functions. They logically require that there is an object
within which there is something to replace.


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.connolly at hortresearch.co.nz  Wed Jun 23 22:49:33 2004
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Thu, 24 Jun 2004 08:49:33 +1200
Subject: [R] Tick marks in xyplot
In-Reply-To: <200406230732.52206.deepayan@stat.wisc.edu>; from
	deepayan@stat.wisc.edu on Wed, Jun 23, 2004 at 07:32:51AM -0500
References: <1612616523F26F48AB55BC8F5D47917C298C11@post2.imr.no>
	<200406230732.52206.deepayan@stat.wisc.edu>
Message-ID: <20040624084933.G11533@hortresearch.co.nz>

On Wed, 23-Jun-2004 at 07:32AM -0500, Deepayan Sarkar wrote:

[...]

|> 1. You can specify the limits, tick positions, and axis labels 
|> individually for each panel (see documentation for 'scales' in ?xyplot; 
|> all the relevant components - at, lab, etc - can be lists). 

How difficult would it be to make mgp one of the 'relevant
components'?  I often find I'd like to decrease the mgp[2] setting.  I
think that's the only reason why I'd like to have mgp in the list, so
if there's a smarter way of achieving that, mgp would be unnecessary.

Is there a smarter way?

-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From ihok at hotmail.com  Wed Jun 23 23:06:00 2004
From: ihok at hotmail.com (Jack Tanner)
Date: Wed, 23 Jun 2004 17:06:00 -0400
Subject: [R] assigning from multiple return values
In-Reply-To: <200406231907.i5NJ712l027877@erdos.math.unb.ca>
References: <200406231907.i5NJ712l027877@erdos.math.unb.ca>
Message-ID: <40D9F0B8.504@hotmail.com>

Rolf Turner wrote:

>	> fn <- function(x) {
>		list(local_foo=foo, local_bar=bar)
>	}
>  
>
OK, that works nicely. (And thanks to Paul Roebuck, who mentioned the
values.list$foo notation.)

>(the ``return(...)'' is brought to you by your Department of
>Redundancy Department.)
>  
>
Aha! LISP reards its head. So what happens if I do

fn <- function(x) {
  some stuff
  ...
  end of stuff
  list(foo = foo, bar = bar)
}

return.value <- fn(x)

Will return.value ever contain any residuals from "stuff", or will it
always be exactly the result of the last statement, list(foo...) here?

>The very fact that you are talking about ``multiple values''
>indicates that you're not really understanding what's going on.
>Functions in R always return just ***one*** value.  That value is an
>object which may have an arbitrarily complicated (list structure).
>Related objects get bundled up into lists, giving you just one object
>to deal with.  Then you can pass that one object to another function,
>which can then pick apart that object and use the components in the
>appropriate way without you, the user, having to fiddle with them.
>  
>
Fine, but the whole notion of loose coupling goes out the window if
you're passing lists of related (but very different) objects from
function to function, and each function has to be aware of the order of
the items in the list.

Liaw, Andy wrote:

> If Jack feels that the fact that functions usually return a single 
> list, and
> one needs to access the components of the list separately, is somehow
> hideous

I don't; I think that this is fine when you can access the components of
the list by name only, without regard to order.

Thanks to all for your help.



From Kang.Changku at epamail.epa.gov  Wed Jun 23 23:24:14 2004
From: Kang.Changku at epamail.epa.gov (Kang.Changku@epamail.epa.gov)
Date: Wed, 23 Jun 2004 17:24:14 -0400
Subject: [R]  Error message handling
Message-ID: <OF19DA473F.F86749A6-ON85256EBC.0074B9E6-85256EBC.0075939E@epamail.epa.gov>





Dear, R experts.
Does anybody have experience with 'optim' function?

I have an error message as the following.

Error in optim(transcoefs, fn = hfdeviance, gr = hfdeviance.grad, method
= "BFGS",  :
initial value in vmmin is not finite

I want to make a comment when this happen.
Is there way I can put *my* message after this error occur?

Thanks in advance

+++++++++++++++++++++++++++++++++++++++++++++++++++++
Changku Kang
National Center for Environmental Assessment
EPA  (B211F)
919-541-1396
919-541-0245 (fax)
Kang.Changku at epa.gov

Graduate Student
Department of Statistics, NCSU
ckang2 at stat.ncsu.edu
919-513-2956
+++++++++++++++++++++++++++++++++++++++++++++++++++++



From deepayan at cs.wisc.edu  Wed Jun 23 23:28:34 2004
From: deepayan at cs.wisc.edu (Deepayan Sarkar)
Date: Wed, 23 Jun 2004 16:28:34 -0500
Subject: [R] Tick marks in xyplot
In-Reply-To: <20040624084933.G11533@hortresearch.co.nz>
References: <1612616523F26F48AB55BC8F5D47917C298C11@post2.imr.no>
	<200406230732.52206.deepayan@stat.wisc.edu>
	<20040624084933.G11533@hortresearch.co.nz>
Message-ID: <1088026114.40d9f60223fdd@www-auth.cs.wisc.edu>

Quoting Patrick Connolly <p.connolly at hortresearch.co.nz>:

> On Wed, 23-Jun-2004 at 07:32AM -0500, Deepayan Sarkar wrote:
> 
> [...]
> 
> |> 1. You can specify the limits, tick positions, and axis labels 
> |> individually for each panel (see documentation for 'scales' in ?xyplot; 
> |> all the relevant components - at, lab, etc - can be lists). 
> 
> How difficult would it be to make mgp one of the 'relevant
> components'?  I often find I'd like to decrease the mgp[2] setting.  I
> think that's the only reason why I'd like to have mgp in the list, so
> if there's a smarter way of achieving that, mgp would be unnecessary.

These (and other similar things) are currently hard-coded, but I have plans to
make these part of the lattice settings eventually. This is completely
vapourware right now, but I have hopes of getting it done in time for R 2.0.0.

If you mean different values of these for different panels though, I don't think
 that's ever going to happen.

Deepayan



From p.connolly at hortresearch.co.nz  Wed Jun 23 23:29:12 2004
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Thu, 24 Jun 2004 09:29:12 +1200
Subject: [R] A way to list only variables or functions?
In-Reply-To: <40D6F5B0.4060501@lancaster.ac.uk>; from
	B.Rowlingson@lancaster.ac.uk on Mon, Jun 21, 2004 at 03:50:24PM +0100
References: <Pine.LNX.4.44.0406201658590.3158-100000@gannet.stats>
	<OAEOKPIGCLDDHAEMCAKIKELECOAA.sdhyok@email.unc.edu>
	<vasdd01ctero57ub14dumk6g066lditfam@4ax.com>
	<40D6F5B0.4060501@lancaster.ac.uk>
Message-ID: <20040624092912.H11533@hortresearch.co.nz>

On Mon, 21-Jun-2004 at 03:50PM +0100, Barry Rowlingson wrote:

|> Duncan Murdoch wrote:
|> 
|> 
|> > I seem to recall that S-PLUS has such a function, but I forget the
|> > name of it.   Probably R does too, on CRAN if not in the base
|> > packages.
|> 
|> objects.summary() I think it was.
|> 
|> It always bothered me that the Nth thing you teach Unix users is 'ls' 
|> and the N+1th thing is 'ls -l' (for very small N). Then you teach them 
|> R, and there's no 'ls -l' equivalent immediately obvious.
|> 
|>   Not sure what such a function could show, there being no permissions, 
|> or dates on R objects, but object.size() and class/mode at least would 
|> be useful.
|> 
|>   So is something like this in CRAN? I say +1 to putting it in the base 
|> packages....

I hesitate to mention it again.  Last time I did there was minimal
interest.  However, the question has been asked again, so perhaps now
is different....

I made myself a function in the S-PLUS days which I've modified to
work in R.  It involved adding another few functions to add dates to
objects.

This is the sort of output it gives:

        Object         Mode   Rows Cols Len    Date   
 1 last.warning     list       --   --    1 NA/NA/NA  
 2 sim.notuff17C.df dataframe  100  9     9 09/06/2004
 3 box.sim          function   --   --    1 04/06/2004
 4 box.simC         function   --   --    1 04/06/2004
 5 multi.sim        function   --   --    1 04/06/2004
 6 multi.simC       function   --   --    1 04/06/2004
 7 sim.100B         function   --   --    1 04/06/2004
 8 sim.100C         function   --   --    1 04/06/2004
 9 sim.100.df       dataframe  100  9     9 04/06/2004
10 sim.notuff17.df  dataframe  100  9     9 04/06/2004
11 sim.notuff.df    dataframe  100  9     9 04/06/2004
12 sim.tuff.df      dataframe  100  9     9 04/06/2004
13 aa               numeric    100  9   900 03/06/2004
14 sim.100A         function   --   --    1 03/06/2004
15 sim.100          function   --   --    1 01/06/2004
16 develop.df       dataframe  161  7     7 28/05/2004
17 glm.both         function   --   --    1 24/05/2004
18 lm.gall          function   --   --    1 24/05/2004
19 aphidgall.df     dataframe  79   4     4 12/05/2004
20 pears.gall       function   --   --    1 13/11/2003
21 glm.gallEPS      function   --   --    1 27/08/2003

I've toyed with the idea of adding an object size column but it's not
important enough for my use.  Since I revisit projects over a period
of years at times, the date is very useful information -- in fact,
it's the main reason why I wrote it.

My code is not elegant enough for an esteemed place on CRAN.  I could
make it a lot better myself if I spent the time on it, but it works
well enough for me as it is, so in that sense, it ain't broke.
However, if anyone is interested in having such functionality my code
could be a good starting point.

Best

-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From kkthird at yahoo.com  Wed Jun 23 23:32:39 2004
From: kkthird at yahoo.com (KKThird@Yahoo.Com)
Date: Wed, 23 Jun 2004 14:32:39 -0700 (PDT)
Subject: [R] nlme questions (e.g., specifying group membership,
	changing options)
Message-ID: <20040623213239.20303.qmail@web52503.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040623/7ba7b5a1/attachment.pl

From fciclone at bol.com.br  Thu Jun 24 00:08:41 2004
From: fciclone at bol.com.br (fciclone)
Date: Wed, 23 Jun 2004 19:08:41 -0300
Subject: [R] anova
Message-ID: <HZS86H$0B8A0834B5A1C973B8DBD1B0A9079A00@bol.com.br>

Could someone please tell me if it is a way to define 
the number of decimals (for example 4 digits) in a 
presentation of anova() results. I couldn't apply round
()function.

Thanks in advance.
Alex
 
__________________________________________________________________________
Acabe com aquelas janelinhas que pulam na sua tela.
AntiPop-up UOL - ?? gr??tis!
http://antipopup.uol.com.br/



From umalvarez at fata.unam.mx  Thu Jun 24 00:23:10 2004
From: umalvarez at fata.unam.mx (Ulises Mora Alvarez)
Date: Wed, 23 Jun 2004 17:23:10 -0500 (CDT)
Subject: [R] anova
In-Reply-To: <HZS86H$0B8A0834B5A1C973B8DBD1B0A9079A00@bol.com.br>
Message-ID: <Pine.LNX.4.44.0406231722260.9688-100000@athena.fata.unam.mx>

Hi!

options(digits = 5)

Good look.

On Wed, 23 Jun 2004, fciclone wrote:

> Could someone please tell me if it is a way to define 
> the number of decimals (for example 4 digits) in a 
> presentation of anova() results. I couldn't apply round
> ()function.
> 
> Thanks in advance.
> Alex
>  
> __________________________________________________________________________
> Acabe com aquelas janelinhas que pulam na sua tela.
> AntiPop-up UOL - ?? gr??tis!
> http://antipopup.uol.com.br/
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Ulises M. Alvarez
LAB. DE ONDAS DE CHOQUE
FISICA APLICADA Y TECNOLOGIA AVANZADA
UNAM
umalvarez at fata.unam.mx



From ggrothendieck at myway.com  Thu Jun 24 01:26:25 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 23 Jun 2004 23:26:25 +0000 (UTC)
Subject: [R] assigning from multiple return values
References: <40D9CFB8.2020702@hotmail.com>
	<loom.20040623T213120-112@post.gmane.org>
Message-ID: <loom.20040624T012333-858@post.gmane.org>


I think I've found a workaround that avoids the two problems in the
replacement function approach.    With the definitions of list
and [<-.result shown, one can write list[a,b] on the
left side of an assignment where the right side of the assignment
evaluates to a list of the same length (or if the list on the right
side is shorter then the otherwise unfilled variables are set to NA and
if the list on the right side is longer the excess entries are ignored).   
Unlike the previous workaround using a replacement function, one
can have a variable number of arguments to list[] and the first 
argument to list[] no longer has to be predefined.

list <- structure(NA,class="result")
"[<-.result" <- function(x,...,value) {
   args <- as.list(match.call())
   args <- args[-c(1:2,length(args))]
   length(value) <- length(args)
   for(i in seq(along=args))
      eval(substitute(x <- v,list(x=args[[i]],v=value[[i]])),env=sys.frame(-1))
   x
}

# it is used like this:

x <- 1:4
fn <- function() list("zz",99)
list[a,x[2]] <- fn()






Gabor Grothendieck <ggrothendieck <at> myway.com> writes:

: 
: Here are two approaches assuming foo is "zz" and bar is 3.
: 
: FIRST
: 
: You could pass the return variables in the argument list and then
: assign them in the caller's frame like this:
: 
: fn <- function(x,y) {
: 	assign(as.character(substitute(x)), "zz", sys.frame(-1))
: 	assign(as.character(substitute(y)), 3, sys.frame(-1))
: }
: fn(a,b)  # sets a to "zz" and b to 3
: 
: SECOND
: 
: You can make this a bit prettier, though not perfect, like this:
: 
: "list2<-" <- function(x,y,value) {
: 	assign(as.character(substitute(y)), value[[2]], sys.frame(-1))
: 	value[[1]]
: }
: fn <- function()list("zz",3)
: a <- 1 # first arg must exist prior to invoking list2. Its value not 
important.
: list2(a,b) <- fn()
: 
: The two problems with list2 are:
: 
: 1. the first argument must exist prior to invoking list2 although its
: actual value is immaterial since it just gets overwritten anyways.
: 
: 2. It only works for 2 args although you could write a list3, list4, etc.
: 
: Maybe someone could comment on these deficiencies.
: 
: Jack Tanner <ihok <at> hotmail.com> writes:
: 
: : 
: : I know that if I have a function that returns multiple values, I should
: : do return(list(foo, bar)). But what do I do on the recieving end?
: : 
: : fn <- function(x) {
: :    return(list(foo, bar))
: : }
: : 
: : I know that at this point I could say
: : 
: : values.list <- fn(x)
: : 
: : and then access
: : 
: : values.list[1]
: : values.list[2]
: : 
: : But that's hideous. I'd rather be able to say something like
: : 
: : list(local_foo, local_bar) <- fn(x)
: : 
: : and have the right thing happen. I realize that it's my responsibility
: : to not screw up and say instead
: : 
: : list(local_bar, local_foo)
: : 
: : Any suggestions?
: : 
: : -JT
: :
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://www.stat.math.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
: 
:



From bdanwood at politics.tamu.edu  Thu Jun 24 02:36:30 2004
From: bdanwood at politics.tamu.edu (B. Dan Wood)
Date: Wed, 23 Jun 2004 19:36:30 -0500
Subject: [R] truncated normal regression
Message-ID: <000001c45983$4b1c7300$6501a8c0@DansLaptop>

Does anyone have example code for a truncated normal regression using the
survival package? Or any other package? Thanks.

 

 

 

 

 


From ggrothendieck at myway.com  Thu Jun 24 03:04:05 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 24 Jun 2004 01:04:05 +0000 (UTC)
Subject: [R] assigning from multiple return values
References: <40D9CFB8.2020702@hotmail.com>
	<loom.20040623T213120-112@post.gmane.org>
	<loom.20040624T012333-858@post.gmane.org>
Message-ID: <loom.20040624T024133-335@post.gmane.org>



Just a few more examples:

# swap a and b without explicitly creating a temporary
a <- 1; b <- 2
list[a,b] <- list(b,a)

# get eigenvectors and eigenvalues
list[eval, evec] <- eigen(cbind(1,1:3,3:1))

# get today's month, day, year
require(chron)
list[Month, Day, Year] <- month.day.year(unclass(Sys.Date()))

# get first two components of linear model ignoring rest
list[Coef, Resid] <- lm(rnorm(10) ~ seq(10))





Gabor Grothendieck <ggrothendieck <at> myway.com> writes:

: 
: I think I've found a workaround that avoids the two problems in the
: replacement function approach.    With the definitions of list
: and [<-.result shown, one can write list[a,b] on the
: left side of an assignment where the right side of the assignment
: evaluates to a list of the same length (or if the list on the right
: side is shorter then the otherwise unfilled variables are set to NA and
: if the list on the right side is longer the excess entries are ignored).   
: Unlike the previous workaround using a replacement function, one
: can have a variable number of arguments to list[] and the first 
: argument to list[] no longer has to be predefined.
: 
: list <- structure(NA,class="result")
: "[<-.result" <- function(x,...,value) {
:    args <- as.list(match.call())
:    args <- args[-c(1:2,length(args))]
:    length(value) <- length(args)
:    for(i in seq(along=args))
:       eval(substitute(x <- v,list(x=args[[i]],v=value[[i]])),env=sys.frame(-
1))
:    x
: }
: 
: # it is used like this:
: 
: x <- 1:4
: fn <- function() list("zz",99)
: list[a,x[2]] <- fn()
: 
: Gabor Grothendieck <ggrothendieck <at> myway.com> writes:
: 
: : 
: : Here are two approaches assuming foo is "zz" and bar is 3.
: : 
: : FIRST
: : 
: : You could pass the return variables in the argument list and then
: : assign them in the caller's frame like this:
: : 
: : fn <- function(x,y) {
: : 	assign(as.character(substitute(x)), "zz", sys.frame(-1))
: : 	assign(as.character(substitute(y)), 3, sys.frame(-1))
: : }
: : fn(a,b)  # sets a to "zz" and b to 3
: : 
: : SECOND
: : 
: : You can make this a bit prettier, though not perfect, like this:
: : 
: : "list2<-" <- function(x,y,value) {
: : 	assign(as.character(substitute(y)), value[[2]], sys.frame(-1))
: : 	value[[1]]
: : }
: : fn <- function()list("zz",3)
: : a <- 1 # first arg must exist prior to invoking list2. Its value not 
: important.
: : list2(a,b) <- fn()
: : 
: : The two problems with list2 are:
: : 
: : 1. the first argument must exist prior to invoking list2 although its
: : actual value is immaterial since it just gets overwritten anyways.
: : 
: : 2. It only works for 2 args although you could write a list3, list4, etc.
: : 
: : Maybe someone could comment on these deficiencies.
: : 
: : Jack Tanner <ihok <at> hotmail.com> writes:
: : 
: : : 
: : : I know that if I have a function that returns multiple values, I should
: : : do return(list(foo, bar)). But what do I do on the recieving end?
: : : 
: : : fn <- function(x) {
: : :    return(list(foo, bar))
: : : }
: : : 
: : : I know that at this point I could say
: : : 
: : : values.list <- fn(x)
: : : 
: : : and then access
: : : 
: : : values.list[1]
: : : values.list[2]
: : : 
: : : But that's hideous. I'd rather be able to say something like
: : : 
: : : list(local_foo, local_bar) <- fn(x)
: : : 
: : : and have the right thing happen. I realize that it's my responsibility
: : : to not screw up and say instead
: : : 
: : : list(local_bar, local_foo)
: : : 
: : : Any suggestions?
: : : 
: : : -JT
: : :
: : 
: : ______________________________________________
: : R-help <at> stat.math.ethz.ch mailing list
: : https://www.stat.math.ethz.ch/mailman/listinfo/r-help
: : PLEASE do read the posting guide! http://www.R-project.org/posting-
guide.html
: : 
: :
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://www.stat.math.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
: 
:



From ok at cs.otago.ac.nz  Thu Jun 24 03:12:06 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Thu, 24 Jun 2004 13:12:06 +1200 (NZST)
Subject: [R] Automatic routine - help
Message-ID: <200406240112.i5O1C6hF094030@atlas.otago.ac.nz>

I note that ?min actually contains a relevant example:

    plot(x, pmin(cH, pmax(-cH, x)), type='b', main= "Huber's function")
            ^^^^^^^^^^^^^^^^^^^^^^

It's always worth timing things.  The timing tests below are the best
of several repetitions, to avoid worries about things like paging and GC.

Make a data frame with 10 variables and 2000 cases.

    > m <- matrix(rnorm(10*2000), ncol=10)
    > colnames(m) <- LETTERS[1:10]
    > z <- as.data.frame(m)

See how long it takes to just make a copy of z.

    > system.time(w <- z+0)
    [1] 0.08 0.01 0.16 0.00 0.00

See how long pmax(pmin(...)...) takes.

    > system.time(w <- pmax(pmin(z, 1), 0))
    [1] 0.08 0.01 0.16 0.00 0.00

See how long the z[z < 0] <- 0; z[z > 1] <- 1 trick takes.

    > system.time({ w <- z; w[w < 0] <- 0; w[w > 1] <- 1 })
    [1] 0.23 0.01 0.31 0.00 0.00



From unung at enciety.com  Thu Jun 24 05:49:29 2004
From: unung at enciety.com (Unung Istopo Hartanto)
Date: Thu, 24 Jun 2004 10:49:29 +0700
Subject: [R] Can R handle twin peaks - normal distribution
Message-ID: <1088048968.3787.2.camel@IT05>

Hi R Users,

Sorry if its out of topic. I would like to ask you about twin peaks -
normal distribution. How R can handle it, any example to explain it in
R.

Thanks,

regards,

Unung



From bitwrit at ozemail.com.au  Thu Jun 24 06:05:09 2004
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Thu, 24 Jun 2004 14:05:09 +1000
Subject: [R] Covered Labels
References: <000001c4592b$3aacde90$5e011e0a@wet.eu.nfowg.com> 
Message-ID: <20040624042919.CLHH17080.smta01.mail.ozemail.net@there>

Martina Renninger wrote:
> Dear All!
>
>
>
> How can I cope with overlapping or covered labels (covered by labels
> from other data points) in plots?

This doesn't solve every such problem, but it has helped me in the past.
You will probably have to expand the xlim and ylim a bit to fit the extreme 
labels onto the plot.

Also, there seems to be a typo in the HTML help for dist(), mentioning 
as.matrix.dist() which should be as.matrix().

Jim
-------------- next part --------------
# thigmophobe returns the direction (as n|e|s|w) _away_ from
# the nearest point where x and y are vectors of 2D coordinates

thigmophobe<-function(x,y) {
 # get the current upper and lower limits of the plot
 plot.span<-par("usr")
 x.span<-plot.span[2] - plot.span[1]
 y.span<-plot.span[4] - plot.span[3]
 # if either axis is logarithmic, transform the values into logarithms
 if(par("xlog")) x<-log(x)
 if(par("ylog")) y<-log(y)
 # scale the values to the plot span
 # this avoids the numerically larger
 # axis dominating the distance measure
 x<-x/x.span
 y<-y/y.span
 # get the distance matrix as a full matrix
 xy.dist<-as.matrix(dist(cbind(x,y)))
 lenx<-length(x)
 nearest.index<-rep(0,lenx)
 for(index in 1:lenx)
  nearest.index[index]<-as.numeric(names(which.min(xy.dist[-index,index])))
 # get the x and y differences for each point to the nearest point
 xdiff<-x - x[nearest.index]
 ydiff<-y - y[nearest.index]
 # first set the east/west direction
 dir.ew<-ifelse(xdiff > 0,"e","w")
 # now do the north/south
 dir.ns<-ifelse(ydiff > 0,"n","s")
 dir.away<-ifelse(abs(xdiff)>abs(ydiff),dir.ew,dir.ns)
 return(dir.away)
}

# thigmophobe.labels positions labels at points so that they
# are most distant from the nearest other point, where the
# points are described as x and y coordinates.

thigmophobe.labels<-function(x,y,labels,...) {
 if(!missing(x) && !missing(y)) {
  lenx<-length(x)
  if(missing(labels)) labels<-as.character(1:lenx)
  dir.away<-thigmophobe(x,y)
  xadjust<-strwidth("M")
  yadjust<-strheight("M")*1.5
  text.adj<-rep(0,lenx)
  for(i in 1:lenx) {
   if(dir.away[i] == "n") {
    y[i]<-y[i]+yadjust
    text.adj[i]<-0.5
   }
   if(dir.away[i] == "e") {
    x[i]<-x[i]+xadjust
    text.adj[i]<-0
   }
   if(dir.away[i] == "s") {
    y[i]<-y[i]-yadjust
    text.adj[i]<-0.5
   }
   if(dir.away[i] == "w") {
    x[i]<-x[i]-xadjust
    text.adj[i]<-1
   }
  }
  text(x,y,labels,...)
 }
 else
  cat("Usage: thigmophobe.labels(x,y,labels=1:length(x),...)\n")
}

From ok at cs.otago.ac.nz  Thu Jun 24 06:46:44 2004
From: ok at cs.otago.ac.nz (Richard A. O'Keefe)
Date: Thu, 24 Jun 2004 16:46:44 +1200 (NZST)
Subject: [R] assigning from multiple return values
Message-ID: <200406240446.i5O4kiTE096519@atlas.otago.ac.nz>

Gabor Grothendieck <ggrothendieck at myway.com> wrote:
	fn <- function(x,y) {
		assign(as.character(substitute(x)), "zz", sys.frame(-1))
		assign(as.character(substitute(y)), 3, sys.frame(-1))
	}
	fn(a,b)  # sets a to "zz" and b to 3
	
	"list2<-" <- function(x,y,value) {
		assign(as.character(substitute(y)), value[[2]], sys.frame(-1))
		value[[1]]
	}
	fn <- function()list("zz",3)
	a <- 1 # first arg must exist prior to invoking list2. Its value not important.
	list2(a,b) <- fn()
	
There is still another way, which doesn't use substitute, as.character, or
assign.  Instead of returning two results, accept an extra argument which
is a function that decides what to do with them.

    > f <- function(handler) { handler("zz", 3) }
    > f(function(x, y) { a <<- x; b <<- y })
    > a
    [1] "zz"
    > b
    [1] 3
    > g <- function() {
    +     u <- v <- NULL # make it clear these are local to g
    +     f(function(x,y) {u <<- x; v <<- y})
    +     list(u, v)  # see what we got
    + }
    > g()
    [[1]]
    [1] "zz"

    [[2]]
    [1] 3

Whenever I would have wanted to pass "a variable" to an R function,
this is what I do instead.  It works so well that I am still happily
ignorant of how to use substitute().  It's also much more compiler-friendly.



From ripley at stats.ox.ac.uk  Thu Jun 24 08:37:52 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 24 Jun 2004 07:37:52 +0100 (BST)
Subject: [R]  Error message handling
In-Reply-To: <OF19DA473F.F86749A6-ON85256EBC.0074B9E6-85256EBC.0075939E@epamail.epa.gov>
Message-ID: <Pine.LNX.4.44.0406240734330.1575-100000@gannet.stats>

On Wed, 23 Jun 2004 Kang.Changku at epamail.epa.gov wrote:

> Dear, R experts.
> Does anybody have experience with 'optim' function?

Yes.

> I have an error message as the following.
> 
> Error in optim(transcoefs, fn = hfdeviance, gr = hfdeviance.grad, method
> = "BFGS",  :
> initial value in vmmin is not finite
> 
> I want to make a comment when this happen.

     Function 'fn' can return 'NA' or 'Inf' if the function cannot be
     evaluated at the supplied value, but the initial value must have a
     computable finite value of 'fn'. (Except for method '"L-BFGS-B"'
     where the values should always be finite.)

It's your error so you can control it (by not making the error).

> Is there way I can put *my* message after this error occur?

You can use try/tryCatch and similar constructs.  But it would be better
to use a valid starting value as optim asks, and you can so that by
calling hfdeviance(transcoefs) and checking it is finite.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Thu Jun 24 08:48:30 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 24 Jun 2004 07:48:30 +0100 (BST)
Subject: [R] Covered Labels
In-Reply-To: <20040624042919.CLHH17080.smta01.mail.ozemail.net@there>
Message-ID: <Pine.LNX.4.44.0406240742400.1575-100000@gannet.stats>

On Thu, 24 Jun 2004, Jim Lemon wrote:

> Martina Renninger wrote:
> > Dear All!
> >
> >
> >
> > How can I cope with overlapping or covered labels (covered by labels
> > from other data points) in plots?
> 
> This doesn't solve every such problem, but it has helped me in the past.
> You will probably have to expand the xlim and ylim a bit to fit the extreme 
> labels onto the plot.
> 
> Also, there seems to be a typo in the HTML help for dist(), mentioning 
> as.matrix.dist() which should be as.matrix().

Not a typo: here is a function as.matrix.dist, try
getAnywhere("as.matrix.dist").  More elegant would be `the "dist" method
of as.matrix().'

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sb at ihe.se  Thu Jun 24 08:54:44 2004
From: sb at ihe.se (Sixten Borg)
Date: Thu, 24 Jun 2004 08:54:44 +0200
Subject: [R] R 1.9.0, special characters in variable names.
Message-ID: <s0da96ee.093@gwmail.ihe.se>

Hello all,

I upgraded from R 1.8.1 to 1.9.0 (Windows XP), and spotted an odd thing.

The last three letters in the Swedish alphabet are ??, ?? and ??. (In case they don't show correctly: they are a with a ring, a with two dots, and o with two dots (HTML: &aring;  &auml; &ouml;).

When I use these as variable names in a data.frame, odd things happen:
In R 1.8.1, ?? (&aring;) doesn't work while the others do.
In R 1.9.0, ?? (&ouml;) doesn't work while the others do.

Please find examples below. It would be nice if all three could be used in variable names. At least in Sweden :-)

Thanks...
Sixten.

#
# R 1.9.0: ?? is renamed to X.
#

> data.frame(a=1, ??=2, ??=3, ??=4)
  a ?? ?? X.
1 1 2 3  4
> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    1              
minor    9.0            
year     2004           
month    04             
day      12             
language R              
> 


#
# R 1.8.1: ?? is renamed to X.
#

> data.frame(a=1, ??=2, ??=3, ??=4)
  a X. ?? ??
1 1  2 3 4
> version
         _              
platform i386-pc-mingw32
arch     i386           
os       mingw32        
system   i386, mingw32  
status                  
major    1              
minor    8.1            
year     2003           
month    11             
day      21             
language R          


#



From olafur.ingolfsson at imr.no  Thu Jun 24 09:08:48 2004
From: olafur.ingolfsson at imr.no (Ingolfsson, Olafur)
Date: Thu, 24 Jun 2004 09:08:48 +0200
Subject: [R] Tick marks in xyplot
Message-ID: <1612616523F26F48AB55BC8F5D47917C298C12@post2.imr.no>

Thank you for the tips, it is also good to know that the packages are getting even better.

I however solved it this way: 

lset(col.whitebg())
x.data <- rnorm(16,20,7);y.data <- rnorm(16,.55,.25); z.data <- sample(1:4,16,replace=T)
xyplot(y.data~x.data|z.data, layout=c(2,2),xlim=c(4,39),ylim=c(-.04,1.04),
       scales=list(x=list(alternating=F),y=list(alternating=F),tck=c(-1,0)),
       panel = function(x,y,...)
              {
               x.t <- c(5,5,5,10,10,10,15,15,15,20,20,20,25,25,25,30,30,30,35,35,35,rep(c(4,4,4.7),6))
       	       y.t <- c(rep(c(-0.04,-0.02,-0.04),7),-0.04,0,0,0,0.2,0.2,0.2,0.4,0.4,0.4,0.6,0.6,0.6,0.8,0.8,0.8,1,1)
               panel.xyplot(x.t,y.t,type="l",col=1)
               panel.xyplot(x,y)
              } )

I'm not trying to convince anyone that this is 'the elegant way', but it does the job.

Thanks again

Olafur A.I.



From ripley at stats.ox.ac.uk  Thu Jun 24 09:12:51 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 24 Jun 2004 08:12:51 +0100 (BST)
Subject: [R] R 1.9.0, special characters in variable names.
In-Reply-To: <s0da96ee.093@gwmail.ihe.se>
Message-ID: <Pine.LNX.4.44.0406240803380.5685-100000@gannet.stats>

This is a function of the OS set by your locale, and there is nothing we
can do about it.  It is done by the C call isalpha in do_makenames in
src/main/character.c.

My Windows XP machine in Swedish does accept all three, using the 
CRAN-compiled version of rw1091.exe, so something is up with yours.  
Sorry, can't help.

On Thu, 24 Jun 2004, Sixten Borg wrote:

> Hello all,
> 
> I upgraded from R 1.8.1 to 1.9.0 (Windows XP), and spotted an odd thing.
> 
> The last three letters in the Swedish alphabet are ??, ?? and ??. (In case they don't show correctly: they are a with a ring, a with two dots, and o with two dots (HTML: &aring;  &auml; &ouml;).
> 
> When I use these as variable names in a data.frame, odd things happen:
> In R 1.8.1, ?? (&aring;) doesn't work while the others do.
> In R 1.9.0, ?? (&ouml;) doesn't work while the others do.
> 
> Please find examples below. It would be nice if all three could be used in variable names. At least in Sweden :-)
> 
> Thanks...
> Sixten.
> 
> #
> # R 1.9.0: ?? is renamed to X.
> #
> 
> > data.frame(a=1, ??=2, ??=3, ??=4)
>   a ?? ?? X.
> 1 1 2 3  4
> > version
>          _              
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    1              
> minor    9.0            
> year     2004           
> month    04             
> day      12             
> language R              
> > 
> 
> 
> #
> # R 1.8.1: ?? is renamed to X.
> #
> 
> > data.frame(a=1, ??=2, ??=3, ??=4)
>   a X. ?? ??
> 1 1  2 3 4
> > version
>          _              
> platform i386-pc-mingw32
> arch     i386           
> os       mingw32        
> system   i386, mingw32  
> status                  
> major    1              
> minor    8.1            
> year     2003           
> month    11             
> day      21             
> language R          
> 
> 
> #
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Thu Jun 24 09:25:32 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 24 Jun 2004 09:25:32 +0200
Subject: [R] R 1.9.0, special characters in variable names.
In-Reply-To: <s0da96ee.093@gwmail.ihe.se>
References: <s0da96ee.093@gwmail.ihe.se>
Message-ID: <x27jtxxvrn.fsf@biostat.ku.dk>

"Sixten Borg" <sb at ihe.se> writes:

> Hello all,
> 
> I upgraded from R 1.8.1 to 1.9.0 (Windows XP), and spotted an odd thing.
> 
> The last three letters in the Swedish alphabet are ??, ?? and ??. (In case they don't show correctly: they are a with a ring, a with two dots, and o with two dots (HTML: &aring;  &auml; &ouml;).
> 
> When I use these as variable names in a data.frame, odd things happen:
> In R 1.8.1, ?? (&aring;) doesn't work while the others do.
> In R 1.9.0, ?? (&ouml;) doesn't work while the others do.
> 
> Please find examples below. It would be nice if all three could be used in variable names. At least in Sweden :-)

I suspect this is an OS/locale issue rather than an R one -- we're at
the mercy of whatever the isprint function/macro returns for a given
locale. Works fine on Linux with LC_CTYPE=da_DK

> > data.frame(a=1, ??=2, ??=3, ??=4)
>   a ?? ?? X.
> 1 1 2 3  4

> data.frame(a=1, ??=2, ??=3, ??=4, ??=5, ??=6)
  a ?? ?? ?? ?? ??
1 1 2 3 4 5 6

(1.9.0 on RH8)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From olafur.ingolfsson at imr.no  Thu Jun 24 09:59:49 2004
From: olafur.ingolfsson at imr.no (Ingolfsson, Olafur)
Date: Thu, 24 Jun 2004 09:59:49 +0200
Subject: [R] R 1.9.0, special characters in variable names.
Message-ID: <1612616523F26F48AB55BC8F5D47917C298C13@post2.imr.no>

"Sixten Borg" <sb at ihe.se> writes:
When I use these as variable names in a data.frame, odd things happen:
> data.frame(a=1, ??=2, ??=3, ??=4)
>   a ?? ?? X.
> 1 1 2 3  4
 ---- 
I your variables only include numbers (or only characters), this works

XX <-  cbind(a=1, ??=2, ??=3, ??=4, ??=5, ??=6)
> XX
     a ?? ?? ?? ?? ??
[1,] 1 2 3 4 5 6
But this doesn't
> data.frame(XX)
  a ?? ?? X. X?? X.
1 1 2 3  4  5  6

i.e. it is the data.frame function that manages to mess up the variable names for us Windows users



From ligges at statistik.uni-dortmund.de  Thu Jun 24 10:07:52 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 24 Jun 2004 10:07:52 +0200
Subject: [R] R 1.9.0, special characters in variable names.
In-Reply-To: <1612616523F26F48AB55BC8F5D47917C298C13@post2.imr.no>
References: <1612616523F26F48AB55BC8F5D47917C298C13@post2.imr.no>
Message-ID: <40DA8BD8.8030505@statistik.uni-dortmund.de>

Ingolfsson, Olafur wrote:

> "Sixten Borg" <sb at ihe.se> writes:
> When I use these as variable names in a data.frame, odd things happen:
> 
>>data.frame(a=1, ??=2, ??=3, ??=4)
>>  a ?? ?? X.
>>1 1 2 3  4
> 
>  ---- 
> I your variables only include numbers (or only characters), this works
> 
> XX <-  cbind(a=1, ??=2, ??=3, ??=4, ??=5, ??=6)
> 
>>XX
> 
>      a ?? ?? ?? ?? ??
> [1,] 1 2 3 4 5 6
> But this doesn't
> 
>>data.frame(XX)
> 
>   a ?? ?? X. X?? X.
> 1 1 2 3  4  5  6
> 
> i.e. it is the data.frame function that manages to mess up the variable names for us Windows users

No, it is related to your Windows setting / version.

I still get

  > data.frame(XX)
    a ?? ?? ?? ?? ??
  1 1 2 3 4 5 6

using the german locales (german version of WinNT 4.0 SP6, R-1.9.1).

Uwe Ligges



From ripley at stats.ox.ac.uk  Thu Jun 24 10:20:55 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 24 Jun 2004 09:20:55 +0100 (BST)
Subject: [R] R 1.9.0, special characters in variable names.
In-Reply-To: <1612616523F26F48AB55BC8F5D47917C298C13@post2.imr.no>
Message-ID: <Pine.LNX.4.44.0406240917300.5935-100000@gannet.stats>

On Thu, 24 Jun 2004, Ingolfsson, Olafur wrote:

> "Sixten Borg" <sb at ihe.se> writes:
> When I use these as variable names in a data.frame, odd things happen:
> > data.frame(a=1, ??=2, ??=3, ??=4)
> >   a ?? ?? X.
> > 1 1 2 3  4
>  ---- 
> I your variables only include numbers (or only characters), this works
> 
> XX <-  cbind(a=1, ??=2, ??=3, ??=4, ??=5, ??=6)
> > XX
>      a ?? ?? ?? ?? ??
> [1,] 1 2 3 4 5 6
> But this doesn't
> > data.frame(XX)
>   a ?? ?? X. X?? X.
> 1 1 2 3  4  5  6
> 
> i.e. it is the data.frame function that manages to mess up the variable
> names for us Windows users

No.  It is your OS that is at fault, plus the user who did not think 
to use check.names=FALSE to work around the problems of his machine.

Can we stop blaming R for things which are not its fault, especially as 
that has already been pointed out twice this morning?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sb at ihe.se  Thu Jun 24 10:49:08 2004
From: sb at ihe.se (Sixten Borg)
Date: Thu, 24 Jun 2004 10:49:08 +0200
Subject: Summary [R] R 1.9.0, special characters in variable names.
Message-ID: <s0dab1c8.010@gwmail.ihe.se>


Summary: 
The locale setting in the operating system seems to be involved in what confused me a little bit.
Thank you all for your help, especially the suggested work-around  data.frame(..., check.names=F) which works very well.

A mystery still to be solved is why two versions of R, running on the same machine on the same time, behaves differently.
Please do not respond to this on the list. I very much welcome you not to respond at all.

Sixten.

>>> Prof Brian Ripley <ripley at stats.ox.ac.uk> 2004-06-24 10:20:55 >>>
> Can we stop blaming R for things which are not its fault, especially as 
> that has already been pointed out twice this morning?



From maechler at stat.math.ethz.ch  Thu Jun 24 11:04:32 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Thu, 24 Jun 2004 11:04:32 +0200
Subject: [R] Can R handle twin peaks - normal distribution
In-Reply-To: <1088048968.3787.2.camel@IT05>
References: <1088048968.3787.2.camel@IT05>
Message-ID: <16602.39200.708340.137931@gargle.gargle.HOWL>

>>>>> "Unung" == Unung Istopo Hartanto <unung at enciety.com>
>>>>>     on Thu, 24 Jun 2004 10:49:29 +0700 writes:

    Unung> Hi R Users, Sorry if its out of topic. I would like
    Unung> to ask you about twin peaks - normal
    Unung> distribution. How R can handle it, any example to
    Unung> explain it in R.

It's not off-topic but you didn't really say what you want
(read the posting guide as indicated in the last line of this message!)
so I guess "Twin Peaks - Normal" means a mixture of two normal
distributions.

There's the small 
package  'nor1mix'  for univariate normal mixtures plotting, RNG
etc, and the much more extensive package
'mclust' which allows to *estimate* multivariate (now including
uni-variate) normal mixture model parameters.

Is it what you've wanted?
Martin Maechler



From Virgilio.Gomez at uv.es  Thu Jun 24 11:11:54 2004
From: Virgilio.Gomez at uv.es (Virgilio =?ISO-8859-1?Q?G=F3mez?= Rubio)
Date: Thu, 24 Jun 2004 11:11:54 +0200
Subject: [R] More problems with lattice and postscript
Message-ID: <1088068313.10531.24.camel@chomsky.estadi.uv.es>

Dear List members,


I am trying to produce some trellis graphics and to save them in a
postscript file but I only get blank files. R behaviour is certainly
strange because I use a loop to generate the graphics (see code below).
When I change the loop variable myself the postscript graphics are OK.

I am using R 1.9.1 (2004-06-21) on Debian GNU/Linux which I try to keep
updated on a daily basis. I am aware of some problems in R 1.9.0 but
I don't know if it has been fixed yet. And nobody mentioned problems
with loops.

Any ideas? Thanks in advance.

Best regards,

Virgilio



for(centre in 1:3)
{
        for(method in 1:12)
        {

                auxdata<-
       
as.data.frame(t(table.thres[c(method,16),1+(centre-1)*250+(1:250)]))

                names(auxdata)<-c("Threshold", "Parameters")

                trellis.device(postscript,
file=paste("plots/thresold-",method,"-",centre,".eps", sep=""),
onefile=FALSE, color=TRUE)


                bwplot(Threshold~Parameters, data=auxdata,
groups=Parameters,
                        main=list(label=get.method(method), cex=1.5),
                        horizontal=FALSE, pch=".", scales=list(cex=1.5),
                        par.strip.text=list(cex=1.5))
                dev.off()
        }
}



From ripley at stats.ox.ac.uk  Thu Jun 24 11:47:14 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 24 Jun 2004 10:47:14 +0100 (BST)
Subject: [R] More problems with lattice and postscript
In-Reply-To: <1088068313.10531.24.camel@chomsky.estadi.uv.es>
Message-ID: <Pine.LNX.4.44.0406241030140.9689-100000@gannet.stats>

On Thu, 24 Jun 2004, Virgilio G??mez Rubio wrote:

> I am trying to produce some trellis graphics and to save them in a
> postscript file but I only get blank files. R behaviour is certainly
> strange because I use a loop to generate the graphics (see code below).
> When I change the loop variable myself the postscript graphics are OK.
> 
> I am using R 1.9.1 (2004-06-21) on Debian GNU/Linux which I try to keep
> updated on a daily basis. I am aware of some problems in R 1.9.0 but
> I don't know if it has been fixed yet.

It has.  See the NEWS file.

> And nobody mentioned problems with loops.

They are mentioned frequently.  You have to explicitly print lattice 
plots, such as bwplot.  See ?Lattice.

> Any ideas? Thanks in advance.
> 
> Best regards,
> 
> Virgilio
> 
> 
> 
> for(centre in 1:3)
> {
>         for(method in 1:12)
>         {
> 
>                 auxdata<-
>        
> as.data.frame(t(table.thres[c(method,16),1+(centre-1)*250+(1:250)]))
> 
>                 names(auxdata)<-c("Threshold", "Parameters")
> 
>                 trellis.device(postscript,
> file=paste("plots/thresold-",method,"-",centre,".eps", sep=""),
> onefile=FALSE, color=TRUE)
> 
> 
>                 bwplot(Threshold~Parameters, data=auxdata,
> groups=Parameters,
>                         main=list(label=get.method(method), cex=1.5),
>                         horizontal=FALSE, pch=".", scales=list(cex=1.5),
>                         par.strip.text=list(cex=1.5))
>                 dev.off()
>         }
> }

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From maj at stats.waikato.ac.nz  Thu Jun 24 12:09:54 2004
From: maj at stats.waikato.ac.nz (Murray Jorgensen)
Date: Thu, 24 Jun 2004 22:09:54 +1200
Subject: Summary [R] R 1.9.0, special characters in variable names.
In-Reply-To: <s0dab1c8.010@gwmail.ihe.se>
References: <s0dab1c8.010@gwmail.ihe.se>
Message-ID: <40DAA872.1070201@stats.waikato.ac.nz>

This is not what I would call a summary. A summary should:

1.	State the original question.

2.	Give a pre'cis of the responses.

Murray Jorgensen

Sixten Borg wrote:

> Summary: 
> The locale setting in the operating system seems to be involved in what confused me a little bit.
> Thank you all for your help, especially the suggested work-around  data.frame(..., check.names=F) which works very well.
> 
> A mystery still to be solved is why two versions of R, running on the same machine on the same time, behaves differently.
> Please do not respond to this on the list. I very much welcome you not to respond at all.
> 
> Sixten.
> 
> 
>>>>Prof Brian Ripley <ripley at stats.ox.ac.uk> 2004-06-24 10:20:55 >>>
>>
>>Can we stop blaming R for things which are not its fault, especially as 
>>that has already been pointed out twice this morning?
> 
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Dr Murray Jorgensen      http://www.stats.waikato.ac.nz/Staff/maj.html
Department of Statistics, University of Waikato, Hamilton, New Zealand
Email: maj at waikato.ac.nz                                Fax 7 838 4155
Phone  +64 7 838 4773 wk    +64 7 849 6486 home    Mobile 021 1395 862



From unung at enciety.com  Thu Jun 24 13:20:06 2004
From: unung at enciety.com (Unung Istopo Hartanto)
Date: Thu, 24 Jun 2004 18:20:06 +0700
Subject: [R] Can R handle twin peaks - normal distribution
In-Reply-To: <16602.39200.708340.137931@gargle.gargle.HOWL>
References: <1088048968.3787.2.camel@IT05>
	<16602.39200.708340.137931@gargle.gargle.HOWL>
Message-ID: <1088076006.5615.6.camel@IT05>

Yes, I need normal mixtures plotting, because i found my data not normal
and closer to bimodal.

Thanks for All, I've installed package "nor1mix". And its solve my
problem.

regards,

Unung 


On Thu, 2004-06-24 at 16:04, Martin Maechler wrote:
> >>>>> "Unung" == Unung Istopo Hartanto <unung at enciety.com>
> >>>>>     on Thu, 24 Jun 2004 10:49:29 +0700 writes:
> 
>     Unung> Hi R Users, Sorry if its out of topic. I would like
>     Unung> to ask you about twin peaks - normal
>     Unung> distribution. How R can handle it, any example to
>     Unung> explain it in R.
> 
> It's not off-topic but you didn't really say what you want
> (read the posting guide as indicated in the last line of this message!)
> so I guess "Twin Peaks - Normal" means a mixture of two normal
> distributions.
> 
> There's the small 
> package  'nor1mix'  for univariate normal mixtures plotting, RNG
> etc, and the much more extensive package
> 'mclust' which allows to *estimate* multivariate (now including
> uni-variate) normal mixture model parameters.
> 
> Is it what you've wanted?
> Martin Maechler



From fionajsanderson at yahoo.co.uk  Thu Jun 24 14:01:59 2004
From: fionajsanderson at yahoo.co.uk (=?iso-8859-1?q?Fiona=20Sanderson?=)
Date: Thu, 24 Jun 2004 13:01:59 +0100 (BST)
Subject: [R] "Set-up files corrupted" error message
Message-ID: <20040624120159.56121.qmail@web25201.mail.ukl.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040624/02b5fd77/attachment.pl

From ligges at statistik.uni-dortmund.de  Thu Jun 24 14:15:03 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Thu, 24 Jun 2004 14:15:03 +0200
Subject: [R] "Set-up files corrupted" error message
In-Reply-To: <20040624120159.56121.qmail@web25201.mail.ukl.yahoo.com>
References: <20040624120159.56121.qmail@web25201.mail.ukl.yahoo.com>
Message-ID: <40DAC5C7.9060402@statistik.uni-dortmund.de>

Fiona Sanderson wrote:

> I've tried to download rw1090.exe 3 times and rw1081.exe once onto a Windows XP from the UK mirrors (I've tried both of them). When the download is complete and I double-click on the icon I get an error message telling me the set-up files are corrupted and to obtain a new copy of the program. I have spent quite a lot of time checking the message archives but I can't find a solution. If anyone else has one I would really appreciate it if you could let me know.
>  
> Thank you.
>  
> FIona Sanderson

I guess an error with a proxy, since the file on UK's CRAN mirror is fine.

a) Try to get a version from another mirror (CRAN master already has 
R-1.9.1 which you should install rather than the outdated versions 
mentioned above)

b) Try to bypass your proxy.

c) Your system might be corrupted, if this happens for other versions 
from other mirrors as well.

Uwe Ligges



From Stefan.Etschberger at wiwi.uni-augsburg.de  Thu Jun 24 15:12:36 2004
From: Stefan.Etschberger at wiwi.uni-augsburg.de (Stefan Etschberger)
Date: Thu, 24 Jun 2004 15:12:36 +0200
Subject: [R] Catching R's (D)COM output in perl
Message-ID: <40DAD344.5060701@wiwi.uni-augsburg.de>

Hi,

i'm using Perl to control R under Windows via the Win32::OLE module and 
R's (D)COM server. This works fine, if i send commands and/or simple 
data structures to R. I am also able to get matrix-shaped return values 
into my Perl program, like this

# snip ----------------------------------------------------
my $R = Win32::OLE->new('StatConnectorSrv.StatConnector');
$R->Init('R');
my $matrix = $R->Evaluate('matrix(c(1,"a",2,"b",3,"c"),2,3)');
# ---------------------------------------------------- snap

What i did not achieve is to catch ALL the output from R which goes 
normally to the console window.

# this won't collect R's summary formatting as a string ------
my $summary = $R->Evaluate('summary(c(1,2,3))');
# ------------------------------------------------------------

I guess i have to figure out how to use the '"console" device (Active X 
control)',
and maybe use $R->SetCharacterOutputDevice() in Perl but i was neither 
getting this to work nor was i able to find the right spot containing 
this information.

Does someone have a clue?

Thanks in advance,

Stefan
----------------
Stefan Etschberger
Institut f??r Statistik und Mathematische Wirtschaftstheorie
Universit??t Augsburg
D-86135 Augsburg


	
	


	
	


	stefan.etschberger at wiwi.uni-augsburg.de 
<mailto:stefan.etschberger at wiwi.uni-augsburg.de>



From coursol at cristal.math.u-psud.fr  Thu Jun 24 15:22:37 2004
From: coursol at cristal.math.u-psud.fr (Jean Coursol)
Date: Thu, 24 Jun 2004 15:22:37 +0200 (CEST)
Subject: [R] Bug in parse; memory access test forgotten ?
Message-ID: <200406241322.PAA09735@jacaranda.math.u-psud.fr>

I was exploring the polynom library with students:

library(polynom)
x <- polynomial()
z <- (1+x)^100
f <- as.function(z)     #  => Segmentation fault  (with R-1.8.1 and R-1.9.0) !!!

Debugging at hand:

as.function.polynomial <- function (x, ...) 
{
    p <- x
    horner <- function(p) {
        a <- as.character(rev(unclass(p)))
        h <- a[1]
        while (length(a <- a[-1]) > 0) {
            h <- paste("x*(", h, ")", sep = "")
            if (a[1] != 0) 
                h <- paste(a[1], " + ", h, sep = "")
        }
        h
    }
    f <- function(x) NULL
    body(f) <- parse(text = horner(p))[[1]]
    f
}

horner <- function(p) {
        a <- as.character(rev(unclass(p)))
        h <- a[1]
        while (length(a <- a[-1]) > 0) {
            h <- paste("x*(", h, ")", sep = "")
            if (a[1] != 0)
                h <- paste(a[1], " + ", h, sep = "")
        }
        h
}

x <- polynomial()
z <- (1+x)^100
zh <- horner(z)

nchar(zh)
[1] 2404

parse(text = zh) # => Segmentation fault (it ran one time !!!)

# The R solution (VR S programming p 95)

as.function.polynomial <- function(p) {
         function(x) { v <- 0; for (a in rev(p)) v <- a + x*v; v }
         }
# is running perfectly...

Jean Coursol



From ggrothendieck at myway.com  Thu Jun 24 15:44:43 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 24 Jun 2004 13:44:43 +0000 (UTC)
Subject: [R] assigning from multiple return values
References: <40D9CFB8.2020702@hotmail.com>
	<loom.20040623T213120-112@post.gmane.org>
	<loom.20040624T012333-858@post.gmane.org>
	<loom.20040624T024133-335@post.gmane.org>
Message-ID: <loom.20040624T152508-798@post.gmane.org>


Here is a minor update with support for empty arguments.  They
are just thrown away eliminating the need to use a dummy name
for them.

list <- structure(NA,class="result")
"[<-.result" <- function(x,...,value) {
   args <- as.list(match.call())
   args <- args[-c(1:2,length(args))]
   length(value) <- length(args)
   for(i in seq(along=args)) {
     a <- args[[i]]
     if(!missing(a)) eval.parent(substitute(a <- v,list(a=a,v=value[[i]])))
   }
   x
}

# it is used like this:

list[QR,,QRaux]  <- qr(c(1,1:3,3:1))
list[,Green,Blue]  <- col2rgb("aquamarine")


Gabor Grothendieck <ggrothendieck <at> myway.com> writes:

: 
: Just a few more examples:
: 
: # swap a and b without explicitly creating a temporary
: a <- 1; b <- 2
: list[a,b] <- list(b,a)
: 
: # get eigenvectors and eigenvalues
: list[eval, evec] <- eigen(cbind(1,1:3,3:1))
: 
: # get today's month, day, year
: require(chron)
: list[Month, Day, Year] <- month.day.year(unclass(Sys.Date()))
: 
: # get first two components of linear model ignoring rest
: list[Coef, Resid] <- lm(rnorm(10) ~ seq(10))
: 
: Gabor Grothendieck <ggrothendieck <at> myway.com> writes:
: 
: : 
: : I think I've found a workaround that avoids the two problems in the
: : replacement function approach.    With the definitions of list
: : and [<-.result shown, one can write list[a,b] on the
: : left side of an assignment where the right side of the assignment
: : evaluates to a list of the same length (or if the list on the right
: : side is shorter then the otherwise unfilled variables are set to NA and
: : if the list on the right side is longer the excess entries are ignored).   
: : Unlike the previous workaround using a replacement function, one
: : can have a variable number of arguments to list[] and the first 
: : argument to list[] no longer has to be predefined.
: : 
: : list <- structure(NA,class="result")
: : "[<-.result" <- function(x,...,value) {
: :    args <- as.list(match.call())
: :    args <- args[-c(1:2,length(args))]
: :    length(value) <- length(args)
: :    for(i in seq(along=args))
: :       eval(substitute(x <- v,list(x=args[[i]],v=value[[i]])),env=sys.frame
(-
: 1))
: :    x
: : }
: : 
: : # it is used like this:
: : 
: : x <- 1:4
: : fn <- function() list("zz",99)
: : list[a,x[2]] <- fn()
: : 
: : Gabor Grothendieck <ggrothendieck <at> myway.com> writes:
: : 
: : : 
: : : Here are two approaches assuming foo is "zz" and bar is 3.
: : : 
: : : FIRST
: : : 
: : : You could pass the return variables in the argument list and then
: : : assign them in the caller's frame like this:
: : : 
: : : fn <- function(x,y) {
: : : 	assign(as.character(substitute(x)), "zz", sys.frame(-1))
: : : 	assign(as.character(substitute(y)), 3, sys.frame(-1))
: : : }
: : : fn(a,b)  # sets a to "zz" and b to 3
: : : 
: : : SECOND
: : : 
: : : You can make this a bit prettier, though not perfect, like this:
: : : 
: : : "list2<-" <- function(x,y,value) {
: : : 	assign(as.character(substitute(y)), value[[2]], sys.frame(-1))
: : : 	value[[1]]
: : : }
: : : fn <- function()list("zz",3)
: : : a <- 1 # first arg must exist prior to invoking list2. Its value not 
: : important.
: : : list2(a,b) <- fn()
: : : 
: : : The two problems with list2 are:
: : : 
: : : 1. the first argument must exist prior to invoking list2 although its
: : : actual value is immaterial since it just gets overwritten anyways.
: : : 
: : : 2. It only works for 2 args although you could write a list3, list4, etc.
: : : 
: : : Maybe someone could comment on these deficiencies.
: : : 
: : : Jack Tanner <ihok <at> hotmail.com> writes:
: : : 
: : : : 
: : : : I know that if I have a function that returns multiple values, I should
: : : : do return(list(foo, bar)). But what do I do on the recieving end?
: : : : 
: : : : fn <- function(x) {
: : : :    return(list(foo, bar))
: : : : }
: : : : 
: : : : I know that at this point I could say
: : : : 
: : : : values.list <- fn(x)
: : : : 
: : : : and then access
: : : : 
: : : : values.list[1]
: : : : values.list[2]
: : : : 
: : : : But that's hideous. I'd rather be able to say something like
: : : : 
: : : : list(local_foo, local_bar) <- fn(x)
: : : : 
: : : : and have the right thing happen. I realize that it's my responsibility
: : : : to not screw up and say instead
: : : : 
: : : : list(local_bar, local_foo)
: : : : 
: : : : Any suggestions?
: : : : 
: : : : -JT
: : : :
: : : 
: : : ______________________________________________
: : : R-help <at> stat.math.ethz.ch mailing list
: : : https://www.stat.math.ethz.ch/mailman/listinfo/r-help
: : : PLEASE do read the posting guide! http://www.R-project.org/posting-
: guide.html
: : : 
: : :
: : 
: : ______________________________________________
: : R-help <at> stat.math.ethz.ch mailing list
: : https://www.stat.math.ethz.ch/mailman/listinfo/r-help
: : PLEASE do read the posting guide! http://www.R-project.org/posting-
guide.html
: : 
: :
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://www.stat.math.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
: 
:



From ggrothendieck at myway.com  Thu Jun 24 16:08:57 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Thu, 24 Jun 2004 14:08:57 +0000 (UTC)
Subject: [R] tree model with at most one split point per variable
Message-ID: <loom.20040624T160128-928@post.gmane.org>


I would like to create a tree model with at most one split point per variable
using tree, rpart or other routine.  Its OK if a variable enters at more
than one node but if it does then all splits for that variable should be
at the same point.  The idea is that I want to be able to summarize the
data as binary factors with the chosen split points.  I don't want to
have three level or more factors.

For example, the following shows that the first split is with Petal.Length
splitting at 2.45; however, there are other splits of Petal.Length at
4.95.  I want to disallow that.

R> data(iris)
R> tree(Species ~., data = iris)
node), split, n, deviance, yval, (yprob)
      * denotes terminal node

 1) root 150 329.600 setosa ( 0.33333 0.33333 0.33333 )  
   2) Petal.Length < 2.45 50   0.000 setosa ( 1.00000 0.00000 0.00000 ) *
   3) Petal.Length > 2.45 100 138.600 versicolor ( 0.00000 0.50000 0.50000 )  
     6) Petal.Width < 1.75 54  33.320 versicolor ( 0.00000 0.90741 0.09259 )  
      12) Petal.Length < 4.95 48   9.721 versicolor ( 0.00000 0.97917 
0.02083 )  
        24) Sepal.Length < 5.15 5   5.004 versicolor ( 0.00000 0.80000 
0.20000 ) *
        25) Sepal.Length > 5.15 43   0.000 versicolor ( 0.00000 1.00000 
0.00000 ) *
      13) Petal.Length > 4.95 6   7.638 virginica ( 0.00000 0.33333 0.66667 ) *
     7) Petal.Width > 1.75 46   9.635 virginica ( 0.00000 0.02174 0.97826 )  
      14) Petal.Length < 4.95 6   5.407 virginica ( 0.00000 0.16667 0.83333 ) *
      15) Petal.Length > 4.95 40   0.000 virginica ( 0.00000 0.00000 1.00000 ) 
*



From spencer.graves at pdf.com  Thu Jun 24 16:29:23 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 24 Jun 2004 07:29:23 -0700
Subject: [R] Can R handle twin peaks - normal distribution
In-Reply-To: <1088076006.5615.6.camel@IT05>
References: <1088048968.3787.2.camel@IT05>	<16602.39200.708340.137931@gargle.gargle.HOWL>
	<1088076006.5615.6.camel@IT05>
Message-ID: <40DAE543.9020406@pdf.com>

      Have you made normal probability plots?  Mixtures of a small 
number of normal distributions will appear as a collection of straight 
line segments with breakpoints related to the mixing percentages, slopes 
of the lines proportional to the standard deviations, etc.  I don't 
understand the math behind this, but some simple simulations will expose 
the patterns.  See, e.g., McLachlan and Basford (1988) Mixture Models 
(Marcel Dekker). 

      hope this helps.  spencer graves

Unung Istopo Hartanto wrote:

>Yes, I need normal mixtures plotting, because i found my data not normal
>and closer to bimodal.
>
>Thanks for All, I've installed package "nor1mix". And its solve my
>problem.
>
>regards,
>
>Unung 
>
>
>On Thu, 2004-06-24 at 16:04, Martin Maechler wrote:
>  
>
>>>>>>>"Unung" == Unung Istopo Hartanto <unung at enciety.com>
>>>>>>>    on Thu, 24 Jun 2004 10:49:29 +0700 writes:
>>>>>>>              
>>>>>>>
>>    Unung> Hi R Users, Sorry if its out of topic. I would like
>>    Unung> to ask you about twin peaks - normal
>>    Unung> distribution. How R can handle it, any example to
>>    Unung> explain it in R.
>>
>>It's not off-topic but you didn't really say what you want
>>(read the posting guide as indicated in the last line of this message!)
>>so I guess "Twin Peaks - Normal" means a mixture of two normal
>>distributions.
>>
>>There's the small 
>>package  'nor1mix'  for univariate normal mixtures plotting, RNG
>>etc, and the much more extensive package
>>'mclust' which allows to *estimate* multivariate (now including
>>uni-variate) normal mixture model parameters.
>>
>>Is it what you've wanted?
>>Martin Maechler
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From spencer.graves at pdf.com  Thu Jun 24 16:53:15 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 24 Jun 2004 07:53:15 -0700
Subject: [R] nlme questions (e.g., specifying group membership,	changing
	options)
In-Reply-To: <20040623213239.20303.qmail@web52503.mail.yahoo.com>
References: <20040623213239.20303.qmail@web52503.mail.yahoo.com>
Message-ID: <40DAEADB.3030104@pdf.com>

      Are your categorical variables factors or ordered factors?  If 
yes, lm and many other functions including, I believe, nlme, will 
automatically create the required dummy variables using contrasts 
specified by options()$contrasts.  Consider the following: 

 > options("contrasts")
$contrasts
        unordered           ordered
"contr.treatment"      "contr.poly"

 > lm(y~x, DF)

Call:
lm(formula = y ~ x, data = DF)

Coefficients:
(Intercept)           xb           xc 
        1.5          2.0          4.0 

 > contr.treatment(3)
  2 3
1 0 0
2 1 0
3 0 1
 > options(contrasts=c(unordered="contr.helmert", ordered="contr.poly"))
 > lm(y~x, DF)

Call:
lm(formula = y ~ x, data = DF)

Coefficients:
(Intercept)           x1           x2 
        3.5          1.0          1.0 

 > contr.helmert(3)
  [,1] [,2]
1   -1   -1
2    1   -1
3    0    2
 >

      Have you studied Pinheiro and Bates (2000) Mixed-Effects Models in 
S and S-Plus (Springer)?  I found answers to questions like yours in 
that book. 

      hope this helps.  spencer graves

KKThird at Yahoo.Com wrote:

>I'm trying to better understand the nlme package and have a few questions.
>
> 
>
>1.) 
>
>Other than using various coding strategies (e.g., dummy coding, effect coding), is there a way to identify group membership (i.e., treatment) directly? For example, the following code will fit a two group logistic growth curve (where 'Score' is repeatedly measured over 'Time' for each of the individuals (ID)):
>
> 
>
>nlme(Score ~ (ALPHA + Group)/(1+exp(-(GAMMA + Group)*(Time - (BETA+Group)))),
>
>            data=LE,
>
>            fixed=ALPHA + BETA + GAMMA ~ 1,
>
>            random=ALPHA + BETA + GAMMA ~ 1,
>
>            groups=~ID,
>
>            start = c(ALPHA = 1, BETA = 3.25, GAMMA = 2.5))
>
>            
>
>Rather than specifying the effect of Group in such a manner, is there a simpler way to identify group membership in order to test the effect of group differences on the parameters of the model? I thought (removing the dummy codes and) specifying group membership by: 'groups=~ID/Groups' might work, but an error is returned. I also thought specifying group membership by :'fixed=ALPHA + BETA + GAMMA ~ 1' might work, but an error is also returned.  
>
> 
>
>2.) 
>
>When will, that is under what circumstances, will there be something different than '~1' on the right hand side of the 'fixed' and 'random' specification lines?
>
> 
>
>3.)
>
>Given that I figure out a way to specify group membership/treatment, how are starting values for both groups specified? Can the covariance structure also be given starting values?
>
> 
>
>Sorry for what might turn out to be simply questions. But, as of yet I've not been able to understand exactly what is going on. Thanks for any help you might be able to provide.
>
>Have a good one,
>
>Ken
>
>
>		
>---------------------------------
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From dmurdoch at pair.com  Thu Jun 24 16:53:54 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Thu, 24 Jun 2004 10:53:54 -0400
Subject: [R] "Set-up files corrupted" error message
In-Reply-To: <20040624120159.56121.qmail@web25201.mail.ukl.yahoo.com>
References: <20040624120159.56121.qmail@web25201.mail.ukl.yahoo.com>
Message-ID: <olnld09oq0stot1p7pb21te2ghd988b9ks@4ax.com>

On Thu, 24 Jun 2004 13:01:59 +0100 (BST), Fiona Sanderson
<fionajsanderson at yahoo.co.uk> wrote :

>I've tried to download rw1090.exe 3 times and rw1081.exe once onto a Windows XP from the UK mirrors (I've tried both of them). When the download is complete and I double-click on the icon I get an error message telling me the set-up files are corrupted and to obtain a new copy of the program. I have spent quite a lot of time checking the message archives but I can't find a solution. If anyone else has one I would really appreciate it if you could let me know.

I'd get rw1091.exe instead, and probably try a different mirror, or a
different machine to do the downloads.  Those are both old releases,
and we'd have heard long ago if all of the mirrors had corrupted
copies:  so there's something wrong with the exact procedure you're
using.

For instance, there may be issues in the way the UK mirror
communicates with your particular machine.  Trying a different mirror
or a different machine could solve either of these.

Or there may be problems with a proxy server running at your site.
Trying a different mirror might help with this.

Or there may be a bug in whatever version of the web browser you're
using.  Trying a different machine might get you a different browser.

I can't think of anything else to try.

Duncan Murdoch



From ayalahec at msu.edu  Thu Jun 24 17:23:25 2004
From: ayalahec at msu.edu (Hector L. Ayala-del-Rio)
Date: Thu, 24 Jun 2004 11:23:25 -0400
Subject: [R] Xgobi graphs
Message-ID: <6F75F0E7-C5F2-11D8-A525-000393DB5846@msu.edu>

I have generated a 3D graph from data in R using the Xgobi package.  
Does anybody know how to generate a postscript file of the Xgobi 
graph??  I have searched everywhere and I can not find it.

Thanks in advance

Hector

H??ctor L. Ayala-del-R??o, Ph.D.
Center for Microbial Ecology &
Center for Genomic and Evolutionary Studies
on Microbial Life at Low Temperatures
Michigan State University
545 Plant & Soil Sciences Building
East Lansing, MI 48824-1325
Phone: 517-353-9021
Fax: 517-353-2917



From tplate at blackmesacapital.com  Thu Jun 24 17:42:02 2004
From: tplate at blackmesacapital.com (Tony Plate)
Date: Thu, 24 Jun 2004 09:42:02 -0600
Subject: [R] Re: summaries (was: SUMMARY: "elementary sapply
  question")
In-Reply-To: <20040622114526.GC3897@igidr.ac.in>
References: <85D25331FFB7AE4C900EA467D4ADA392045A0F@circle.pcpool.mi.fu-berlin.de>
	<20040622114526.GC3897@igidr.ac.in>
Message-ID: <6.1.0.6.2.20040624092519.0425d010@mailhost.blackmesacapital.com>

Posting summaries is customary (or used to be) on S-news, where it was 
customary to reply to the poster, and not always the whole list.  (Whereas 
on R it is requested that replies be posted to the entire list, which makes 
summaries less necessary.)

However, a good summary can be a very useful thing (and Ajay's summary was 
very nicely done).  What about making it the custom on R-list that the 
recipient of helpful responses post a summary on a Wiki?  This could be a 
good way for recipients of help to give something back to the community, 
and it might provide a sufficient input of energy to take a wiki past 
critical mass, such as the one mentioned by Gabor Grothendieck last year:

 > From: "Gabor Grothendieck" <ggrothendieck at myway.com>
MIME-Version: 1.0
 > Date: Wed, 17 Dec 2003 11:53:59 -0500 (EST)
 > [snip]  Actually someone did set up an R wiki some time ago at:
 >
 >  http://fawn.unibw-hamburg.de/cgi-bin/Rwiki.pl?RwikiHome
 >
 > yet no one really used it.  Some critical mass of use is needed
 > to get such a project off the ground.

Comments?

-- Tony Plate



From fm3a004 at math.uni-hamburg.de  Thu Jun 24 17:45:51 2004
From: fm3a004 at math.uni-hamburg.de (Christian Hennig)
Date: Thu, 24 Jun 2004 17:45:51 +0200 (MET DST)
Subject: [R] Xgobi graphs
In-Reply-To: <6F75F0E7-C5F2-11D8-A525-000393DB5846@msu.edu>
Message-ID: <Pine.GSO.3.95q.1040624174509.28083D-100000@sun11.math.uni-hamburg.de>

XGobi menu -> file -> print

Christian

On Thu, 24 Jun 2004, Hector L. Ayala-del-Rio wrote:

> I have generated a 3D graph from data in R using the Xgobi package.  
> Does anybody know how to generate a postscript file of the Xgobi 
> graph??  I have searched everywhere and I can not find it.
> 
> Thanks in advance
> 
> Hector
> 
> H??ctor L. Ayala-del-R??o, Ph.D.
> Center for Microbial Ecology &
> Center for Genomic and Evolutionary Studies
> on Microbial Life at Low Temperatures
> Michigan State University
> 545 Plant & Soil Sciences Building
> East Lansing, MI 48824-1325
> Phone: 517-353-9021
> Fax: 517-353-2917
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

***********************************************************************
Christian Hennig
Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
#######################################################################
ich empfehle www.boag-online.de



From F.Garavito at lse.ac.uk  Thu Jun 24 17:58:09 2004
From: F.Garavito at lse.ac.uk (Garavito,F (pgr))
Date: Thu, 24 Jun 2004 16:58:09 +0100
Subject: [R] Rotate a plot generated by wireframe
Message-ID: <7C0B459BCBF0BB46A52A5818DFEDEEF702AA6033@exs1.backup>

Is there any way to rotate a plot generated by the wireframe function (lattice)?

 

Thank you,

 

Fabian Garavito



From spencer.graves at pdf.com  Thu Jun 24 18:14:33 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 24 Jun 2004 09:14:33 -0700
Subject: [R] Rotate a plot generated by wireframe
In-Reply-To: <7C0B459BCBF0BB46A52A5818DFEDEEF702AA6033@exs1.backup>
References: <7C0B459BCBF0BB46A52A5818DFEDEEF702AA6033@exs1.backup>
Message-ID: <40DAFDE9.7030108@pdf.com>

      Have you considered the "screen" argument to wireframe, as 
described in the documentation, including one of the examples? 

      hope this helps.  spencer graves

Garavito,F (pgr) wrote:

>Is there any way to rotate a plot generated by the wireframe function (lattice)?
>
> 
>
>Thank you,
>
> 
>
>Fabian Garavito
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From rkoenker at uiuc.edu  Thu Jun 24 18:15:13 2004
From: rkoenker at uiuc.edu (roger koenker)
Date: Thu, 24 Jun 2004 11:15:13 -0500
Subject: [R] Rotate a plot generated by wireframe
In-Reply-To: <7C0B459BCBF0BB46A52A5818DFEDEEF702AA6033@exs1.backup>
References: <7C0B459BCBF0BB46A52A5818DFEDEEF702AA6033@exs1.backup>
Message-ID: <AC64FDCA-C5F9-11D8-8CFA-000A95A7E3AA@uiuc.edu>

For this sort of thing the package rgl is  the way to go...



url:	www.econ.uiuc.edu/~roger        	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820

On Jun 24, 2004, at 10:58 AM, Garavito,F (pgr) wrote:

> Is there any way to rotate a plot generated by the wireframe function 
> (lattice)?
>
>
>
> Thank you,
>
>
>
> Fabian Garavito
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From karlknoblich at yahoo.de  Thu Jun 24 18:40:22 2004
From: karlknoblich at yahoo.de (=?iso-8859-1?q?Karl=20Knoblick?=)
Date: Thu, 24 Jun 2004 18:40:22 +0200 (CEST)
Subject: [R] More problems with lattice and postscript
Message-ID: <20040624164022.64897.qmail@web52510.mail.yahoo.com>


Try

print(bwplot(...YOUR PARAMETERS...))

Best wishes,
Karl


> 
> Dear List members, 
> 
> 
> 
> I am trying to produce some trellis graphics and to
save them in a 
> postscript file but I only get blank files. R
behaviour is certainly 
> strange because I use a loop to generate the
graphics (see code below). 
> When I change the loop variable myself the
postscript graphics are OK. 
> 
> 
> I am using R 1.9.1 (2004-06-21) on Debian GNU/Linux
which I try to keep 
> updated on a daily basis. I am aware of some
problems in R 1.9.0 but 
> I don't know if it has been fixed yet. And nobody
mentioned problems 
> with loops. 
> 
> 
> Any ideas? Thanks in advance. 
> 
> 
> Best regards, 
> 
> 
> Virgilio 
> 
> 
> 
> 
> for(centre in 1:3) 
> { 
>         for(method in 1:12) 
>         { 
> 
> 
>                 auxdata<- 
>         
>
as.data.frame(t(table.thres[c(method,16),1+(centre-1)*250+(1:250)]))

> 
> 
>                 names(auxdata)<-c("Threshold",
"Parameters") 
> 
> 
>                 trellis.device(postscript, 
>
file=paste("plots/thresold-",method,"-",centre,".eps",
sep=""), 
> onefile=FALSE, color=TRUE) 
> 
> 
> 
>                 bwplot(Threshold~Parameters,
data=auxdata, 
> groups=Parameters, 
>                        
main=list(label=get.method(method), cex=1.5), 
>                         horizontal=FALSE, pch=".",
scales=list(cex=1.5), 
>                        
par.strip.text=list(cex=1.5)) 
>                 dev.off() 
>         } 
> } 
> 
> 




	

	
		
___________________________________________________________
Bestellen Sie Y! DSL und erhalten Sie die AVM "FritzBox SL" f??r 0.
Sie sparen 119 und bekommen 2 Monate Grundgeb??hrbefreiung.



From baylac at mnhn.fr  Thu Jun 24 18:50:46 2004
From: baylac at mnhn.fr (Michel Baylac)
Date: Thu, 24 Jun 2004 18:50:46 +0200
Subject: [R] R and neighbor joining algorithm
Message-ID: <200406241850.46307.baylac@mnhn.fr>

Does anybody knows about a R implementation of the neighbor joining algorithm 
? I made a Google search without success. Perhaps somebody already cooked 
something ?
Thanks in advance

Michel Baylac
Mus??um National d'Histoire Naturelle
Systematic and Evolution Dpt
CNRS UMR 2695



From sdhyok at email.unc.edu  Thu Jun 24 20:12:28 2004
From: sdhyok at email.unc.edu (Shin, Daehyok)
Date: Thu, 24 Jun 2004 14:12:28 -0400
Subject: [R] system shell emulation in R
Message-ID: <OAEOKPIGCLDDHAEMCAKIEENLCOAA.sdhyok@email.unc.edu>

Rather than using system() to execute a shell command,
is there a way to emulate the system shell itself in R?
For instance, if the function is shell, 

> getwd()
[1] "/home/a/b"
> shell()          # From R to system shell
$cd ..              # cd command in system shell
$CTRL+D	       # Return to R
> getwd()
[1] "/home/a"

Thanks in advance.

Daehyok Shin (Peter)
Terrestrial Hydrological Ecosystem Modellers
Geography Department
University of North Carolina-Chapel Hill
sdhyok at email.unc.edu

"We can do no great things, 
only small things with great love."
                         - Mother Teresa



From asyoung at gmail.com  Thu Jun 24 20:13:43 2004
From: asyoung at gmail.com (Andrew Young)
Date: Thu, 24 Jun 2004 13:13:43 -0500
Subject: [R] more socketConnection questions
Message-ID: <3e350f9704062411134977866c@mail.gmail.com>

I made a post awhile ago that went unanswered regarding the
socketConnection and socketSelect functions.  As a reminder, I was
having problems where after opening a socket connection using:

conn <- socketConnection("localhost",port=8080)

And then checking to see if the connection was ready using:

socketSelect(list(conn))

It takes around 20 seconds for this second call to return with TRUE.

I am now using R 1.9.1 on Linux.  I wrote two separate functions to
investigate further what was going on:

findSocket1 <- function(wait=0) {
  conn <- socketConnection(host="localhost",port=8080)
  system(paste("sleep",wait))
  print(system.time(socketSelect(list(conn))))
  closeAllConnections()
}

findSocket2 <- function(wait=0) {
  sock <- make.socket(host="localhost",port=8080)
  system(paste("sleep",wait))
  print(system.time(write.socket(sock,"HELLO")))
  closeAllConnections()
}

findSocket2 (which uses make.socket, etc...) returns no problem:

> findSocket2()
[1] 0 0 0 0 0

However, running findSocket1:

> findSocket1()
[1]  0.00  0.00 19.99  0.00  0.00
> findSocket1(10)
[1] 0.00 0.00 9.99 0.00 0.00
> findSocket1(20)
[1] 0 0 0 0 0

So it seems that after creating a socketConnection using
socketConnection(), I must wait 20 seconds before I am able to use the
connection.  One solution would be to use make.socket, read.socket and
write.socket--however, I am trying to write binary R data to the
connection, and write.socket requires a string.  So I guess I am
asking two separate questions, either of which will help me.

1) Is there a way to write binary data to a socket created using make.socket() ?

2) Why exactly does the connection opened using socketConnection()
require 20 seconds between the socketConnection() call and the
connection being ready?

I am almost inclined to delve into the internal code of
socketConnection to figure out the answer to (2)...

Any responses are greatly appreciated.

-Andrew Young
asyoung at gmail.com
http://www.rho-project.org/



From laura at env.leeds.ac.uk  Thu Jun 24 20:20:51 2004
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Thu, 24 Jun 2004 19:20:51 +0100 (BST)
Subject: [R] Problem with predict arima
Message-ID: <Pine.LNX.4.44.0406241913330.15416-100000@env-pc-phd13>

I have fitted an arima(0,0,2) model to my data, and am trying to plot a
forecast for the next 15 time steps, but each time i try I am given the
following error message:

Error in .cbind.ts(list(...), makeNames(...), dframe = dframe, union =
TRUE) :
        non-time series not of the correct length

I am calculating as follows:

my.arima<-arima(my.list,order=c(0,0,2))
my.forecast<-predict(my.arima,15)
ts.plot(my.list,my.forecast$pred,my.forecast$pred+2*my.forecast$se,
        myforecast$pred-2*my.forecast$se)

my.list has 1 column and 2880 rows...I don't understand why I am getting
this error message - any advice?

Thanks,
Laura

Laura Quinn
Institute of Atmospheric Science
School of the Environment
University of Leeds
Leeds
LS2 9JT

tel: +44 113 343 1596
fax: +44 113 343 6716
mail: laura at env.leeds.ac.uk



From ripley at stats.ox.ac.uk  Thu Jun 24 20:28:25 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 24 Jun 2004 19:28:25 +0100 (BST)
Subject: [R] system shell emulation in R
In-Reply-To: <OAEOKPIGCLDDHAEMCAKIEENLCOAA.sdhyok@email.unc.edu>
Message-ID: <Pine.LNX.4.44.0406241924220.21622-100000@gannet.stats>

On Thu, 24 Jun 2004, Shin, Daehyok wrote:

> Rather than using system() to execute a shell command,
> is there a way to emulate the system shell itself in R?
> For instance, if the function is shell, 
> 
> > getwd()
> [1] "/home/a/b"
> > shell()          # From R to system shell
> $cd ..              # cd command in system shell
> $CTRL+D	       # Return to R
> > getwd()
> [1] "/home/a"

system("/bin/sh") does this for you under a Unix-alike.  But as spawning a 
shell as cd-ing in the child does not change the working directory of the 
parent, the emulation would be imperfect if this worked in R.

Do try it in your favourite shell!

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From roebuck at odin.mdacc.tmc.edu  Thu Jun 24 20:39:07 2004
From: roebuck at odin.mdacc.tmc.edu (Paul Roebuck)
Date: Thu, 24 Jun 2004 13:39:07 -0500 (CDT)
Subject: [R] [SOLVED] GET_DIM() crash on Windows only
Message-ID: <Pine.OSF.4.58.0406241311290.483973@odin.mdacc.tmc.edu>

Paul Roebuck wrote:

>I have the following contrived code in package format.
>On Solaris and Mac OS X, code runs just fine. On Windows,
>it crashes the R environment with the "Send Bug Report"
>dialog. I tried R 1.8.1 (Win2K) and R 1.9 (WinXP) binaries
>with the same result. PCs otherwise appear properly
>configured for creating R packages. Anything blatantly
>wrong? Suggestions?

The culprit of my troubles turned out to be the GCC compiler
option '-ansi' in "src/Makevars". Why this causes a problem
is not known at this time. My Makevars for the example program
was as below:

    PKG_CFLAGS=-Wall -ansi -pedantic -DDEBUG_RWT -I./
    PKG_LIBS=-lm

Creating a Windows-specific config file "src/Makevars.win"
without that option solves the problem.

    PKG_CFLAGS=-Wall -pedantic -DDEBUG_RWT -I./
    PKG_LIBS=-lm

Many thanks to Uwe Ligges for figuring out the cause.

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From sdhyok at email.unc.edu  Thu Jun 24 22:18:15 2004
From: sdhyok at email.unc.edu (Shin, Daehyok)
Date: Thu, 24 Jun 2004 16:18:15 -0400
Subject: [R] system shell emulation in R
In-Reply-To: <Pine.LNX.4.44.0406241924220.21622-100000@gannet.stats>
Message-ID: <OAEOKPIGCLDDHAEMCAKIEENOCOAA.sdhyok@email.unc.edu>

Is it difficult in R to create a function calling system() with user's
inputs iteratively?

Daehyok Shin (Peter)

> -----Original Message-----
> From: r-help-bounces at stat.math.ethz.ch
> [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Prof Brian Ripley
> Sent: Thursday, June 24, 2004 PM 2:28
> To: Shin, Daehyok
> Cc: R, Help
> Subject: Re: [R] system shell emulation in R
>
>
> On Thu, 24 Jun 2004, Shin, Daehyok wrote:
>
> > Rather than using system() to execute a shell command,
> > is there a way to emulate the system shell itself in R?
> > For instance, if the function is shell,
> >
> > > getwd()
> > [1] "/home/a/b"
> > > shell()          # From R to system shell
> > $cd ..              # cd command in system shell
> > $CTRL+D	       # Return to R
> > > getwd()
> > [1] "/home/a"
>
> system("/bin/sh") does this for you under a Unix-alike.  But as spawning a
> shell as cd-ing in the child does not change the working directory of the
> parent, the emulation would be imperfect if this worked in R.
>
> Do try it in your favourite shell!
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html
>



From alexi at silicongenetics.com  Thu Jun 24 22:23:22 2004
From: alexi at silicongenetics.com (Alexi Zubiria)
Date: Thu, 24 Jun 2004 13:23:22 -0700
Subject: [R] The "median" function in R does not work properly.
Message-ID: <0D3431E0FD777844A4ED9507E4D698D907D510@incoming.sigenetics.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040624/ba35f8dc/attachment.pl

From zestril60637 at yahoo.com  Thu Jun 24 22:30:26 2004
From: zestril60637 at yahoo.com (m i)
Date: Thu, 24 Jun 2004 13:30:26 -0700 (PDT)
Subject: [R] Bayesian Meta Regression model question
Message-ID: <20040624203026.91071.qmail@web50501.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040624/9f4d9bc6/attachment.pl

From dmurdoch at pair.com  Thu Jun 24 22:30:38 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Thu, 24 Jun 2004 16:30:38 -0400
Subject: [R] The "median" function in R does not work properly.
In-Reply-To: <0D3431E0FD777844A4ED9507E4D698D907D510@incoming.sigenetics.com>
References: <0D3431E0FD777844A4ED9507E4D698D907D510@incoming.sigenetics.com>
Message-ID: <8cemd0l8f80gekfmoogmgli24hd1ek9rrb@4ax.com>

On Thu, 24 Jun 2004 13:23:22 -0700, "Alexi Zubiria"
<alexi at silicongenetics.com> wrote :

>Hi, 
>
> 
>
>1.) The "median" function does not work well.  

It works fine for me.  You were trying to take the median of a list.
It only knows how to take the median of a vector.

Use dataf [,2:9] not dataf [2:9] to get a vector.

Duncan Murdoch



From dmurdoch at pair.com  Thu Jun 24 22:34:10 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Thu, 24 Jun 2004 16:34:10 -0400
Subject: [R] The "median" function in R does not work properly.
In-Reply-To: <0D3431E0FD777844A4ED9507E4D698D907D510@incoming.sigenetics.com>
References: <0D3431E0FD777844A4ED9507E4D698D907D510@incoming.sigenetics.com>
Message-ID: <piemd0dip0qtiv0lv8hq5uklvr42p7ndtd@4ax.com>

Oops, missed something below:

On Thu, 24 Jun 2004 13:23:22 -0700, "Alexi Zubiria"
<alexi at silicongenetics.com> wrote :

>Hi, 
>
> 
>
>1.) The "median" function does not work well.  

It works fine for me.  You were trying to take the median of a list.
It only knows how to take the median of a vector of numbers.

Use as.matrix(dataf [,2:9]) not dataf [2:9] to get a numeric matrix
(which is a vector of numbers).

Duncan Murdoch



From spencer.graves at pdf.com  Thu Jun 24 22:36:26 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Thu, 24 Jun 2004 13:36:26 -0700
Subject: [R] Bayesian Meta Regression model question
In-Reply-To: <20040624203026.91071.qmail@web50501.mail.yahoo.com>
References: <20040624203026.91071.qmail@web50501.mail.yahoo.com>
Message-ID: <40DB3B4A.2070905@pdf.com>

      Have you considered "lme"? 

      hope this helps.  spencer graves

m i wrote:

>Hi:
> 
>Is there a command in R (similar to hblm I guess) that would
>allow me to fit a clustered version of the hierarchichal
>bayes model, 
> 
>Y_ij = X_ij\beta + \delta_i + \epsilon_ij
> 
>the \epsilon_ij have known variances,
>\delta_i ~ N(0,\tau^2)
>etc..
> 
>Thanks!
> 
> 
>
>
>		
>---------------------------------
>
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From sundar.dorai-raj at PDF.COM  Thu Jun 24 22:38:19 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Thu, 24 Jun 2004 15:38:19 -0500
Subject: [R] The "median" function in R does not work properly.
In-Reply-To: <0D3431E0FD777844A4ED9507E4D698D907D510@incoming.sigenetics.com>
References: <0D3431E0FD777844A4ED9507E4D698D907D510@incoming.sigenetics.com>
Message-ID: <40DB3BBB.8020209@pdf.com>



Alexi Zubiria wrote:

> Hi, 
> 
>  
> 
> 1.) The "median" function does not work well.  Please refer to the data
> below (same data is attached as txt-delimited).  This is what I try to
> do in R:
> 
>  
> 
> median ( dataf [2:9] )  
> 
>  
> 
> I get warning:  "needs numeric data"
> 
>  
> 
> 2.) BUT if apply the median to a single vector: 
> 
>  
> 
> median ( dataf [,2]] )
> 
>  
> 
> then it works:
> 
>  
> 
>  
> 
> 3.) How come the "median" function does not let me take the median of
> all 8 vectors at once?  Obviously, if I can perform the test on a single
> array, then it means I have numeric data because I do not get warning.
> Also, if I look at the "mode" for each array I get "numeric".
> 
>  
> 
> 

median *does* work properly and as documented:

Arguments:

        x: a numeric vector containing the values whose median is to be
           computed.

so you supplied a `list' or `data.frame' not a vector. I'm not sure what 
you expected, but if you want to obtain the median of each column in 
your matrix, then use ?apply:

apply(as.matrix(dataf), 2, median)

Or if you want to get the median of all vectors (a single number) then use

median(as.matrix(dataf))

I assume from your post you supplied a data.frame, however if dataf 
really is a matrix (try data.class(dataf)) then median(dataf) will 
accomplish the last line (at least on R-1.9.1 for win2000).

--sundar



From hodgess at gator.uhd.edu  Thu Jun 24 22:38:11 2004
From: hodgess at gator.uhd.edu (Erin Hodgess)
Date: Thu, 24 Jun 2004 15:38:11 -0500
Subject: [R] problem with arima
Message-ID: <200406242038.i5OKcBe25412@gator.dt.uh.edu>

Hi Laura!

in your last line, you have myforecast$pred instead of my.forecast$pred

Hope this helps!

If that's not it, try
str(my.list)
str(my.forecast$pred)
and check to see that they are both time series

Sincerely,
Erin Hodgess
Associate Professor
Department of Computer and Mathematical Sciences
University of Houston - Downtown
mailto: hodgess at gator.uhd.edu



From: Laura Quinn <laura at env.leeds.ac.uk>
Subject: [R] Problem with predict arima


I have fitted an arima(0,0,2) model to my data, and am trying to plot a
forecast for the next 15 time steps, but each time i try I am given the
following error message:

Error in .cbind.ts(list(...), makeNames(...), dframe = dframe, union =
TRUE) :
        non-time series not of the correct length

I am calculating as follows:

my.arima<-arima(my.list,order=c(0,0,2))
my.forecast<-predict(my.arima,15)
ts.plot(my.list,my.forecast$pred,my.forecast$pred+2*my.forecast$se,
        myforecast$pred-2*my.forecast$se)

my.list has 1 column and 2880 rows...I don't understand why I am getting
this error message - any advice?

Thanks,
Laura

Laura Quinn
Institute of Atmospheric Science
School of the Environment
University of Leeds
Leeds
LS2 9JT



From levin001 at 123mail.cl  Thu Jun 24 22:36:56 2004
From: levin001 at 123mail.cl (Peyuco Porras Porras .)
Date: Thu, 24 Jun 2004 16:36:56 -0400
Subject: [R] Proportions - basic question
Message-ID: <9fbfa9ece2.9ece29fbfa@123mail.cl>

Hi,
How can I get descriptive statitics (mean, se, etc) for a variable expressed in percentage? (like summary() for a continous var) Can I tell R to do that?
Thank you
PP



From ripley at stats.ox.ac.uk  Thu Jun 24 23:30:40 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Thu, 24 Jun 2004 22:30:40 +0100 (BST)
Subject: [R] system shell emulation in R
In-Reply-To: <OAEOKPIGCLDDHAEMCAKIEENOCOAA.sdhyok@email.unc.edu>
Message-ID: <Pine.LNX.4.44.0406242230250.22582-100000@gannet.stats>

On Thu, 24 Jun 2004, Shin, Daehyok wrote:

> Is it difficult in R to create a function calling system() with user's
> inputs iteratively?

See the example below!

> 
> Daehyok Shin (Peter)
> 
> > -----Original Message-----
> > From: r-help-bounces at stat.math.ethz.ch
> > [mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Prof Brian Ripley
> > Sent: Thursday, June 24, 2004 PM 2:28
> > To: Shin, Daehyok
> > Cc: R, Help
> > Subject: Re: [R] system shell emulation in R
> >
> >
> > On Thu, 24 Jun 2004, Shin, Daehyok wrote:
> >
> > > Rather than using system() to execute a shell command,
> > > is there a way to emulate the system shell itself in R?
> > > For instance, if the function is shell,
> > >
> > > > getwd()
> > > [1] "/home/a/b"
> > > > shell()          # From R to system shell
> > > $cd ..              # cd command in system shell
> > > $CTRL+D	       # Return to R
> > > > getwd()
> > > [1] "/home/a"
> >
> > system("/bin/sh") does this for you under a Unix-alike.  But as spawning a
> > shell as cd-ing in the child does not change the working directory of the
> > parent, the emulation would be imperfect if this worked in R.
> >
> > Do try it in your favourite shell!
> >
> > --
> > Brian D. Ripley,                  ripley at stats.ox.ac.uk
> > Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> > University of Oxford,             Tel:  +44 1865 272861 (self)
> > 1 South Parks Road,                     +44 1865 272866 (PA)
> > Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
> > http://www.R-project.org/posting-guide.html
> >
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rolf at math.unb.ca  Thu Jun 24 23:38:20 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Thu, 24 Jun 2004 18:38:20 -0300 (ADT)
Subject: [R] Proportions - basic question
Message-ID: <200406242138.i5OLcKk8011785@erdos.math.unb.ca>


You wrote:

> How can I get descriptive statitics (mean, se, etc) for a variable
> expressed in percentage? (like summary() for a continous var) Can I
> tell R to do that?

Percentages (and proportions) ***are*** continuous variates.

					cheers,

						Rolf Turner
						rolf at math.unb.ca



From hastie at stanford.edu  Fri Jun 25 00:12:31 2004
From: hastie at stanford.edu (Trevor Hastie)
Date: Thu, 24 Jun 2004 15:12:31 -0700
Subject: [R] problem with model.matrix
Message-ID: <01ec01c45a38$5854d700$ec6640ab@stuk>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040624/25c7ee71/attachment.pl

From jsanchez at fs.fed.us  Fri Jun 25 00:38:51 2004
From: jsanchez at fs.fed.us (Jose Sanchez)
Date: Thu, 24 Jun 2004 15:38:51 -0700
Subject: [R] help in importing data
Message-ID: <OF46755EEC.01821C61-ON88256EBD.007C2C48-88256EBD.007C684A@notes.fs.fed.us>





hi,

i was wondering if you can give me an example on how to import a data set
in csv format. i tried  the manual's example, but have been getting an
error message:Error: syntax error.

sincerely,

jose sanchez



From k.wang at auckland.ac.nz  Fri Jun 25 00:39:32 2004
From: k.wang at auckland.ac.nz (Ko-Kang Kevin Wang)
Date: Fri, 25 Jun 2004 10:39:32 +1200
Subject: [R] help in importing data
References: <OF46755EEC.01821C61-ON88256EBD.007C2C48-88256EBD.007C684A@notes.fs.fed.us>
Message-ID: <00c501c45a3c$1e4f0810$6633d882@stat.auckland.ac.nz>

Hi,

Try read.csv()

i.e. take a look at ?read.csv

HTH

Kevin

----- Original Message ----- 
From: "Jose Sanchez" <jsanchez at fs.fed.us>
To: "R-help at lists.R-project.org" <R-help at stat.math.ethz.ch>
Sent: Friday, June 25, 2004 10:38 AM
Subject: [R] help in importing data


>
>
>
>
> hi,
>
> i was wondering if you can give me an example on how to import a data set
> in csv format. i tried  the manual's example, but have been getting an
> error message:Error: syntax error.
>
> sincerely,
>
> jose sanchez
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>



From p.dalgaard at biostat.ku.dk  Fri Jun 25 00:45:23 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 25 Jun 2004 00:45:23 +0200
Subject: [R] problem with model.matrix
In-Reply-To: <01ec01c45a38$5854d700$ec6640ab@stuk>
References: <01ec01c45a38$5854d700$ec6640ab@stuk>
Message-ID: <x2fz8kini4.fsf@biostat.ku.dk>

"Trevor Hastie" <hastie at stanford.edu> writes:

> This does not:
> 
> > model.matrix(~I(pos>3),data=data.frame(pos=c(1:2)))
> Error in "contrasts<-"(`*tmp*`, value = "contr.treatment") : 
>  contrasts can be applied only to factors with 2 or more levels

Oh yes it does:

>  model.matrix(~I(pos>3),data=data.frame(pos=c(1:2)))
  (Intercept) I(pos > 3)TRUE
1           1              0
2           1              0
attr(,"assign")
[1] 0 1
attr(,"contrasts")
attr(,"contrasts")$"I(pos > 3)"
[1] "contr.treatment"

(RedHat,SuSE,x86,amd64)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From andy_liaw at merck.com  Fri Jun 25 00:51:01 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 24 Jun 2004 18:51:01 -0400
Subject: [R] problem with model.matrix
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7F70@usrymx25.merck.com>

> From: Peter Dalgaard
> 
> "Trevor Hastie" <hastie at stanford.edu> writes:
> 
> > This does not:
> > 
> > > model.matrix(~I(pos>3),data=data.frame(pos=c(1:2)))
> > Error in "contrasts<-"(`*tmp*`, value = "contr.treatment") : 
> >  contrasts can be applied only to factors with 2 or more levels
> 
> Oh yes it does:

Ditto here.  (WinXPPro/R-1.9.0/P3)

Andy
 
> >  model.matrix(~I(pos>3),data=data.frame(pos=c(1:2)))
>   (Intercept) I(pos > 3)TRUE
> 1           1              0
> 2           1              0
> attr(,"assign")
> [1] 0 1
> attr(,"contrasts")
> attr(,"contrasts")$"I(pos > 3)"
> [1] "contr.treatment"
> 
> (RedHat,SuSE,x86,amd64)
> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: 
> (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: 
> (+45) 35327907
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From naras at stanford.edu  Fri Jun 25 01:04:16 2004
From: naras at stanford.edu (Balasubramanian Narasimhan)
Date: Thu, 24 Jun 2004 16:04:16 -0700
Subject: [R] problem with model.matrix
References: <01ec01c45a38$5854d700$ec6640ab@stuk>
Message-ID: <02ba01c45a40$0a07b2e0$516640ab@Bheeshma>

Trevor's problem is reproducible on  R 1.8.1 (which is what he was using) on
both Linux RHEL 3.0 and Solaris and R 1.8.0 on Windows.


----- Original Message ----- 
From: "Trevor Hastie" <hastie at stanford.edu>
To: <r-help at stat.math.ethz.ch>
Sent: Thursday, June 24, 2004 3:12 PM
Subject: [R] problem with model.matrix


> This works:
>
>
> > model.matrix(~I(pos>3),data=data.frame(pos=c(1:5)))
>   (Intercept) I(pos > 3)TRUE
> 1           1              0
> 2           1              0
> 3           1              0
> 4           1              1
> 5           1              1
> attr(,"assign")
> [1] 0 1
> attr(,"contrasts")
> attr(,"contrasts")$"I(pos > 3)"
> [1] "contr.treatment"
>
>
> This does not:
>
> > model.matrix(~I(pos>3),data=data.frame(pos=c(1:2)))
> Error in "contrasts<-"(`*tmp*`, value = "contr.treatment") :
>  contrasts can be applied only to factors with 2 or more levels
>
>
> --------------------------------------------------------------------
>   Trevor Hastie                                  hastie at stanford.edu
>   Professor, Department of Statistics, Stanford University
>   Phone: (650) 725-2231 (Statistics)          Fax: (650) 725-8977
>   (650) 498-5233 (Biostatistics)   Fax: (650) 725-6951
>   URL: http://www-stat.stanford.edu/~hastie
>   address: room 104, Department of Statistics, Sequoia Hall
>            390 Serra Mall, Stanford University, CA 94305-4065
> --------------------------------------------------------------------
>
> [[alternative HTML version deleted]]
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>
>



From berwin at maths.uwa.edu.au  Fri Jun 25 02:45:27 2004
From: berwin at maths.uwa.edu.au (Berwin A Turlach)
Date: Fri, 25 Jun 2004 08:45:27 +0800
Subject: [R] problem with model.matrix
In-Reply-To: <02ba01c45a40$0a07b2e0$516640ab@Bheeshma>
References: <01ec01c45a38$5854d700$ec6640ab@stuk>
	<02ba01c45a40$0a07b2e0$516640ab@Bheeshma>
Message-ID: <16603.30119.66272.619934@bossiaea.maths.uwa.edu.au>

>>>>> "BN" == Balasubramanian Narasimhan <naras at stanford.edu> writes:

    BN> Trevor's problem is reproducible on R 1.8.1 (which is what he
    BN> was using) on both Linux RHEL 3.0 and Solaris and R 1.8.0 on
    BN> Windows.
Yep, on my Linux box (details below) I have the same problem with R
1.8.1 but no problem under R 1.9.0 or R 1.9.1.

Time to upgrade I would say. :)

Cheers,

        Berwin

--please do not edit the information below--

Version:
 platform = i686-pc-linux-gnu
 arch = i686
 os = linux-gnu
 system = i686, linux-gnu
 status = 
 major = 1
 minor = 8.1
 year = 2003
 month = 11
 day = 21
 language = R

Search Path:
 .GlobalEnv, package:methods, package:ctest, package:mva, package:modreg, package:nls, package:ts, Autoloads, package:base



From gerifalte28 at hotmail.com  Fri Jun 25 04:12:28 2004
From: gerifalte28 at hotmail.com (F Z)
Date: Fri, 25 Jun 2004 02:12:28 +0000
Subject: [R] Unique.data.frame...still getting duplicates
Message-ID: <BAY99-F672fPmsSVhGO000013f1@hotmail.com>

Hi there

I have a data frame with about 65,000 rows and 8 variables.  I am trying to 
get rid of the double entries of a factor variable "ID" so I can get a 
unique observation for each ID

I tried:

>dupl_unique.data.frame(data[ID,]) #I obtain a data frame with 21,547 
>observations..so far so good, but then when I check for duplicates

>d_duplicated(dupl2$ID)
>summary(as.factor(d))
FALSE  TRUE
  6836 14711

Meaning that I am still getting 14,711 duplicates!

I tried changing the ID type to integer and repeated the process but I got 
dentical results....what am I missing?

Thanks!



From andy_liaw at merck.com  Fri Jun 25 04:31:51 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Thu, 24 Jun 2004 22:31:51 -0400
Subject: [R] Unique.data.frame...still getting duplicates
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7F72@usrymx25.merck.com>

> From: F Z
> 
> Hi there
> 
> I have a data frame with about 65,000 rows and 8 variables.  
> I am trying to 
> get rid of the double entries of a factor variable "ID" so I 
> can get a 
> unique observation for each ID
> 
> I tried:
> 
> >dupl_unique.data.frame(data[ID,]) #I obtain a data frame with 21,547 
> >observations..so far so good, but then when I check for duplicates
> 
> >d_duplicated(dupl2$ID)
> >summary(as.factor(d))
> FALSE  TRUE
>   6836 14711
> 
> Meaning that I am still getting 14,711 duplicates!
> 
> I tried changing the ID type to integer and repeated the 
> process but I got 
> dentical results....what am I missing?

1.  Upgrade your version of R.  (That will teach you about using `_' for
assignment!)

2.  Call generics, not the methods; i.e., unique() instead of
unique.data.frame().

3.  You want a data frame where the IDs are unique, not the combination of
columns.  Use:

    dupl <- data[unique(ID),]

BTW, where did `dupl2' come from?

Andy
 
> Thanks!
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From astephen at efs.mq.edu.au  Fri Jun 25 04:45:26 2004
From: astephen at efs.mq.edu.au (Alec Stephenson)
Date: Fri, 25 Jun 2004 12:45:26 +1000
Subject: [R] Unique.data.frame...still getting duplicates
Message-ID: <s0dc1e88.043@efs04.efs.mq.edu.au>

data[!duplicated(data$ID),] 
will do. Your unique(data[ID,]) removes duplicated rows in data[ID,],
assuming the object ID exists.



Alec Stephenson                                               
Department of Statistics
Macquarie University
NSW 2109, Australia 

>>> "F Z" <gerifalte28 at hotmail.com> 06/25/04 12:12pm >>>
Hi there

I have a data frame with about 65,000 rows and 8 variables.  I am
trying to 
get rid of the double entries of a factor variable "ID" so I can get a

unique observation for each ID

I tried:

>dupl_unique.data.frame(data[ID,]) #I obtain a data frame with 21,547 
>observations..so far so good, but then when I check for duplicates

>d_duplicated(dupl2$ID)
>summary(as.factor(d))
FALSE  TRUE
  6836 14711

Meaning that I am still getting 14,711 duplicates!

I tried changing the ID type to integer and repeated the process but I
got 
dentical results....what am I missing?

Thanks!

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help 
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Fri Jun 25 08:06:42 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 25 Jun 2004 07:06:42 +0100 (BST)
Subject: [R] problem with model.matrix
In-Reply-To: <16603.30119.66272.619934@bossiaea.maths.uwa.edu.au>
Message-ID: <Pine.LNX.4.44.0406250705190.26426-100000@gannet.stats>

On Fri, 25 Jun 2004, Berwin A Turlach wrote:

> >>>>> "BN" == Balasubramanian Narasimhan <naras at stanford.edu> writes:
> 
>     BN> Trevor's problem is reproducible on R 1.8.1 (which is what he
>     BN> was using) on both Linux RHEL 3.0 and Solaris and R 1.8.0 on
>     BN> Windows.
> Yep, on my Linux box (details below) I have the same problem with R
> 1.8.1 but no problem under R 1.9.0 or R 1.9.1.
> 
> Time to upgrade I would say. :)

Here is the reason (NEWS for 1.9.0)

    o	contrasts() turns a logical variable into a factor.  This now
	always has levels c("FALSE", "TRUE") even if only one (or
	none) of these occur in the variable.


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Fri Jun 25 08:13:10 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 25 Jun 2004 07:13:10 +0100 (BST)
Subject: [R] Unique.data.frame...still getting duplicates
In-Reply-To: <BAY99-F672fPmsSVhGO000013f1@hotmail.com>
Message-ID: <Pine.LNX.4.44.0406250708550.26426-100000@gannet.stats>

Your code cannot possibly work in a recent version of R, so please try the
current version (1.9.1).

data[ID, ] is what?  Why not just call unique() on ID?

BTW, if you call methods such as unique.data.frame you are adding possible 
course of error -- here I suspect data[ID, ] is not what you intend.
Please call the generic.

On Fri, 25 Jun 2004, F Z wrote:

> Hi there
> 
> I have a data frame with about 65,000 rows and 8 variables.  I am trying to 
> get rid of the double entries of a factor variable "ID" so I can get a 
> unique observation for each ID
> 
> I tried:
> 
> >dupl_unique.data.frame(data[ID,]) #I obtain a data frame with 21,547 
> >observations..so far so good, but then when I check for duplicates
> 
> >d_duplicated(dupl2$ID)
> >summary(as.factor(d))
> FALSE  TRUE
>   6836 14711
> 
> Meaning that I am still getting 14,711 duplicates!
> 
> I tried changing the ID type to integer and repeated the process but I got 
> dentical results....what am I missing?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From nikko at hailmail.net  Fri Jun 25 09:30:44 2004
From: nikko at hailmail.net (Nicholas Lewin-Koh)
Date: Fri, 25 Jun 2004 15:30:44 +0800
Subject: [R] Matrix: Help with syntax and comparison with SparseM
Message-ID: <1088148644.24614.199153300@webmail.messagingengine.com>

Hi,
I am writing some basic smoothers in R for cleaning some spectral data.
I wanted to see if I could get close to matlab for speed, so I was
trying to compare SparseM
with Matrix to see which could do the choleski decomposition the
fastest.

Here is the function using SparseM
difsm <- function(y, lambda, d){
# Smoothing with a finite difference penalty
# y:      signal to be smoothed
# lambda: smoothing parameter
# d:      order of differences in penalty (generally 2)
# based on code by Paul Eilers 2002
  require(SparseM)
  m <- length(y)
  E <- as(m,"matrix.diag.csr")
  D <- diff(E,differences=d)
  B <- E + (lambda * t(D)%*%D)
  z <- solve(B,y)
  z
}

To do this in Matrix, I am not sure how to get an Identity matrix of
dimension m. From looking at the documentation I would think that
 E<-new("cscMatrix", nrow=m, i=integer(m),x=rep(1,m),p=0:(m-1)))
Should do what I want, but it fails?
> m<-10
> E<-new("cscMatrix", nrow=m, i=integer(m),x=rep(1,m),p=0:(m-1))
Error in initialize(value, ...) : Invalid names for slots of class
cscMatrix: nrow
> E<-new("cscMatrix", i=integer(m),x=rep(1,m),p=0:(m-1))
Error in validObject(.Object) : Invalid "cscMatrix" object: last element
of slot p must match length of slots i and x

Granted I am not very fascile with the S4 classes, so I may be doing
something stupid.
Also to do the next step there is no diff method for matrices in Matrix,
that would be nice
:)

I guess the last step would be easy
z <- solve((E + (lambda * crossprod(D))),y)

But I can't get the Identity matrix???

Thanks for the help,
it is probably obvious, but I am missing it.

Nicholas



From msylvest at uclink.berkeley.edu  Fri Jun 25 09:48:51 2004
From: msylvest at uclink.berkeley.edu (Matthew David Sylvester)
Date: Fri, 25 Jun 2004 00:48:51 -0700
Subject: [R] Simulating from a Multivariate Normal Distribution Using a
 Correlation Matrix
Message-ID: <web-3604335@calmail-ma.berkeley.edu>

Hello,
I would like to simulate randomly from a multivariate normal distribution using a correlation 
matrix, rho.  I do not have sigma.  I have searched the help archive and the R documentation as 
well as doing a standard google search.  What I have seen is that one can either use rmvnorm in 
the package: mvtnorm or mvrnorm in the package: MASS.  I believe I read somewhere that the latter 
was more robust.  I have seen conflicting (or at least seemingly conflicting to me, a relative 
statistics novice), views on whether one can use the correlation matrix with these commands 
instead of the covariance matrix.  I thought that if the commands standardized the covariance 
matrix, then it would not matter, but I end up with larger values when I test the covariance 
matrix versus when I test rho.  So, my question is, if one does not know sigma, can they use rho? 
 And, if so, which command (or is there another) is better to use?  I gather that both use eigen 
decomposition?  Thank you so much  in advance for your help.
Best,
Matt



From dimitris.rizopoulos at med.kuleuven.ac.be  Fri Jun 25 10:04:54 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Fri, 25 Jun 2004 10:04:54 +0200
Subject: [R] Simulating from a Multivariate Normal Distribution Using a
	Correlation Matrix
References: <web-3604335@calmail-ma.berkeley.edu>
Message-ID: <00c601c45a8b$1a0cd8a0$ad133a86@www.domain>

Hi Matt,

the correlation marix can be regarded as a covaraince matrix with unit
variance, so I think you could use it instead of your Sigma matrix,
e.g.,

rho <- cbind(c(1, .3, .1), c(.3, 1, .2), c(.1, .2, 1))
library(MASS)
x <- mvrnorm(5000, mu=1:3, Sigma=rho)
var(x)
          [,1]      [,2]      [,3]
[1,] 1.0234457 0.3101399 0.1146355
[2,] 0.3101399 0.9923358 0.2076328
[3,] 0.1146355 0.2076328 1.0115746

cor(x)
          [,1]      [,2]      [,3]
[1,] 1.0000000 0.3077485 0.1126647
[2,] 0.3077485 1.0000000 0.2072372
[3,] 0.1126647 0.2072372 1.0000000


Moreover, if you want different variance you could use

rho <- cbind(c(1, .3, .1), c(.3, 1, .2), c(.1, .2, 1))
vars <- 5
library(MASS)
x <- mvrnorm(5000, mu=1:3, Sigma=vars*rho)
var(x)
          [,1]      [,2]      [,3]
[1,] 4.9036297 1.4153234 0.4396337
[2,] 1.4153234 4.8801609 0.8947988
[3,] 0.4396337 0.8947988 4.8844880

cor(x)
           [,1]      [,2]       [,3]
[1,] 1.00000000 0.2893209 0.08983026
[2,] 0.28932087 1.0000000 0.18327313
[3,] 0.08983026 0.1832731 1.00000000


I hope this helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Doctoral Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm




----- Original Message ----- 
From: "Matthew David Sylvester" <msylvest at uclink.berkeley.edu>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, June 25, 2004 9:48 AM
Subject: [R] Simulating from a Multivariate Normal Distribution Using
a Correlation Matrix


> Hello,
> I would like to simulate randomly from a multivariate normal
distribution using a correlation
> matrix, rho.  I do not have sigma.  I have searched the help archive
and the R documentation as
> well as doing a standard google search.  What I have seen is that
one can either use rmvnorm in
> the package: mvtnorm or mvrnorm in the package: MASS.  I believe I
read somewhere that the latter
> was more robust.  I have seen conflicting (or at least seemingly
conflicting to me, a relative
> statistics novice), views on whether one can use the correlation
matrix with these commands
> instead of the covariance matrix.  I thought that if the commands
standardized the covariance
> matrix, then it would not matter, but I end up with larger values
when I test the covariance
> matrix versus when I test rho.  So, my question is, if one does not
know sigma, can they use rho?
>  And, if so, which command (or is there another) is better to use?
I gather that both use eigen
> decomposition?  Thank you so much  in advance for your help.
> Best,
> Matt
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Fri Jun 25 10:13:00 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 25 Jun 2004 09:13:00 +0100 (BST)
Subject: [R] Simulating from a Multivariate Normal Distribution Using a
	Correlation Matrix
In-Reply-To: <web-3604335@calmail-ma.berkeley.edu>
Message-ID: <Pine.LNX.4.44.0406250909560.32191-100000@gannet.stats>

Short answer: to know a MVN distribution you need to know the mean vector 
and the covariance matrix.  If you don't know a distribution you cannot 
simulate from it.

So you need to know the marginal variances (the diagonal of the covariance 
matrix).  If you have those, you can form the covariance matrix and use 
rmvnorm or mvrnorm.  If you are willing to assume they are one, you have 
the covariance (= correlation matrix).  If you don't know the marginal 
variances the problem is incompletely specified.

On Fri, 25 Jun 2004, Matthew David Sylvester wrote:

> Hello,
> I would like to simulate randomly from a multivariate normal distribution using a correlation 
> matrix, rho.  I do not have sigma.  I have searched the help archive and the R documentation as 
> well as doing a standard google search.  What I have seen is that one can either use rmvnorm in 
> the package: mvtnorm or mvrnorm in the package: MASS.  I believe I read somewhere that the latter 
> was more robust.  I have seen conflicting (or at least seemingly conflicting to me, a relative 
> statistics novice), views on whether one can use the correlation matrix with these commands 
> instead of the covariance matrix.  I thought that if the commands standardized the covariance 
> matrix, then it would not matter, but I end up with larger values when I test the covariance 
> matrix versus when I test rho.  So, my question is, if one does not know sigma, can they use rho? 
>  And, if so, which command (or is there another) is better to use?  I gather that both use eigen 
> decomposition?  Thank you so much  in advance for your help.
> Best,
> Matt
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Anne.Olga.Piotet at omsv.vd.ch  Fri Jun 25 10:37:10 2004
From: Anne.Olga.Piotet at omsv.vd.ch (Anne Piotet)
Date: Fri, 25 Jun 2004 10:37:10 +0200
Subject: [R] avoiding loops
Message-ID: <007501c45a8f$9b713f90$83dad10a@prod.omsv.ch>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040625/63f64f47/attachment.pl

From ripley at stats.ox.ac.uk  Fri Jun 25 10:53:04 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 25 Jun 2004 09:53:04 +0100 (BST)
Subject: [R] avoiding loops
In-Reply-To: <007501c45a8f$9b713f90$83dad10a@prod.omsv.ch>
Message-ID: <Pine.LNX.4.44.0406250949060.32429-100000@gannet.stats>

On Fri, 25 Jun 2004, Anne Piotet wrote:

> I'd like to compute the values
> x(i)-x(n-i-1) for i=1 n/2 or similar   (as way of testing distribution symmetry hypothesis). It is very easy to do that with a loop but can that be avoided? 

Is that really what you want (not x[n-i+1])?  x - rev(x) gives you the
latter for all i.  Or try

ind <- seq(1, floor(n/2))  (you don't need the floor, but it is clearer)
x[ind] - x[n - ind + 1]

(or -1 if you prefer).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From robin_gruna at hotmail.com  Fri Jun 25 11:07:42 2004
From: robin_gruna at hotmail.com (Robin Gruna)
Date: Fri, 25 Jun 2004 11:07:42 +0200
Subject: [R] String manipulation
Message-ID: <BAY2-DAV9PaAn9pmzpE0000f203@hotmail.com>

Hi,
let's see, if someone can help my with this one:
I have the string as follows:
> str<-("one","two","three")

Now I want to concatenate the items to one string, seperateted by space or
something else,
>str
>"one, two, three"
If possible without a loop.

My actual goal ist to create string like
>str.names
>"female = names1, male = names2"
and pass it as argument to list(), intending to create a list
>names.list<-list( female = names1, male = names2)

Thanks a lot,
Robin



From h.andersson at nioo.knaw.nl  Fri Jun 25 11:01:07 2004
From: h.andersson at nioo.knaw.nl (Henrik Andersson)
Date: Fri, 25 Jun 2004 11:01:07 +0200
Subject: [R] What happened to the excellent R-Newsletter ?
Message-ID: <cbgpm2$b50$1@sea.gmane.org>

The last newsletter found on www.r-project.org is from December 2003.

I was looking forward to the next issue, will there be one soon, or what 
happened ?

While I'm waiting for the newsletter I think I will buy some books about 
R and statistics. Is Venables & Ripley, MASS good if you want to learn 
about multivariate data analysis (PCA, PLS, correspondence analysis) and 
how to do it in R ?

Henrik Andersson



From dimitris.rizopoulos at med.kuleuven.ac.be  Fri Jun 25 11:16:30 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Fri, 25 Jun 2004 11:16:30 +0200
Subject: [R] String manipulation
References: <BAY2-DAV9PaAn9pmzpE0000f203@hotmail.com>
Message-ID: <001901c45a95$1a52fdd0$ad133a86@www.domain>

Hi Robin,

regarding you first question you could use,

str <- c("one","two","three")
paste(str, collapse=", ")

hoewver, describing what you actually want to do I'd use,

names1 <- letters[1:10]
names2 <- letters[1:20]
lis <- lapply(1:2, function(x) get(paste("names", x, sep="")))
names(lis) <- c("female", "male")
lis

I hope this helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Doctoral Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm


----- Original Message ----- 
From: "Robin Gruna" <robin_gruna at hotmail.com>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, June 25, 2004 11:07 AM
Subject: [R] String manipulation


> Hi,
> let's see, if someone can help my with this one:
> I have the string as follows:
> > str<-("one","two","three")
>
> Now I want to concatenate the items to one string, seperateted by
space or
> something else,
> >str
> >"one, two, three"
> If possible without a loop.
>
> My actual goal ist to create string like
> >str.names
> >"female = names1, male = names2"
> and pass it as argument to list(), intending to create a list
> >names.list<-list( female = names1, male = names2)
>
> Thanks a lot,
> Robin
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From lecoutre at stat.ucl.ac.be  Fri Jun 25 11:30:58 2004
From: lecoutre at stat.ucl.ac.be (Eric Lecoutre)
Date: Fri, 25 Jun 2004 11:30:58 +0200
Subject: [R] String manipulation
In-Reply-To: <001901c45a95$1a52fdd0$ad133a86@www.domain>
References: <BAY2-DAV9PaAn9pmzpE0000f203@hotmail.com>
	<001901c45a95$1a52fdd0$ad133a86@www.domain>
Message-ID: <6.0.1.1.2.20040625112648.02183a68@stat4ux.stat.ucl.ac.be>


Hi,

Just to show there are always different ways to handle such problems:

You could also prepare a matrix (or data.frame) containing those data (in 
fact, I suspect you do have data in that form).

 > names1 <- letters[1:10]
 > names2 <- letters[1:20]
 > data <- cbind(genre=rep(c("female","male"),c(10,20)), 
names=c(names1,names2))
 > data
       genre    names
  [1,] "female" "a"
  [2,] "female" "b"
  #[...] continued

 > split(data[,"names"],data[,"genre"])
$female
  [1] "a" "b" "c" "d" "e" "f" "g" "h" "i" "j"

$male
  [1] "a" "b" "c" "d" "e" "f" "g" "h" "i" "j" "k" "l" "m" "n" "o" "p" "q" 
"r" "s" "t"

See ?split

Eric


At 11:16 25/06/2004, Dimitris Rizopoulos wrote:
>Hi Robin,
>
>regarding you first question you could use,
>
>str <- c("one","two","three")
>paste(str, collapse=", ")
>
>hoewver, describing what you actually want to do I'd use,
>
>names1 <- letters[1:10]
>names2 <- letters[1:20]
>lis <- lapply(1:2, function(x) get(paste("names", x, sep="")))
>names(lis) <- c("female", "male")
>lis
>
>I hope this helps.
>
>Best,
>Dimitris
>
>----
>Dimitris Rizopoulos
>Doctoral Student
>Biostatistical Centre
>School of Public Health
>Catholic University of Leuven
>
>Address: Kapucijnenvoer 35, Leuven, Belgium
>Tel: +32/16/396887
>Fax: +32/16/337015
>Web: http://www.med.kuleuven.ac.be/biostat/
>      http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
>
>
>----- Original Message -----
>From: "Robin Gruna" <robin_gruna at hotmail.com>
>To: <r-help at stat.math.ethz.ch>
>Sent: Friday, June 25, 2004 11:07 AM
>Subject: [R] String manipulation
>
>
> > Hi,
> > let's see, if someone can help my with this one:
> > I have the string as follows:
> > > str<-("one","two","three")
> >
> > Now I want to concatenate the items to one string, seperateted by
>space or
> > something else,
> > >str
> > >"one, two, three"
> > If possible without a loop.
> >
> > My actual goal ist to create string like
> > >str.names
> > >"female = names1, male = names2"
> > and pass it as argument to list(), intending to create a list
> > >names.list<-list( female = names1, male = names2)
> >
> > Thanks a lot,
> > Robin
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide!
>http://www.R-project.org/posting-guide.html
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html

Eric Lecoutre
UCL /  Institut de Statistique
Voie du Roman Pays, 20
1348 Louvain-la-Neuve
Belgium

tel: (+32)(0)10473050
lecoutre at stat.ucl.ac.be
http://www.stat.ucl.ac.be/ISpersonnel/lecoutre

If the statistics are boring, then you've got the wrong numbers. -Edward 
Tufte



From Tristan.Lefebure at univ-lyon1.fr  Fri Jun 25 11:32:41 2004
From: Tristan.Lefebure at univ-lyon1.fr (Lefebure Tristan)
Date: Fri, 25 Jun 2004 11:32:41 +0200
Subject: [R] String manipulation
In-Reply-To: <BAY2-DAV9PaAn9pmzpE0000f203@hotmail.com>
References: <BAY2-DAV9PaAn9pmzpE0000f203@hotmail.com>
Message-ID: <200406251132.41966.lefebure@univ-lyon1.fr>

Hi
look at ?paste 

> paste("one","two","three",sep=",")
[1] "one,two,three"





On Friday 25 June 2004 11:07, Robin Gruna wrote:
> Hi,
> let's see, if someone can help my with this one:
>
> I have the string as follows:
> > str<-("one","two","three")
>
> Now I want to concatenate the items to one string, seperateted by space or
> something else,
>
> >str
> >"one, two, three"
>
> If possible without a loop.
>
> My actual goal ist to create string like
>
> >str.names
> >"female = names1, male = names2"
>
> and pass it as argument to list(), intending to create a list
>
> >names.list<-list( female = names1, male = names2)
>
> Thanks a lot,
> Robin
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
> http://www.R-project.org/posting-guide.html



From spencer.graves at pdf.com  Fri Jun 25 11:33:23 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Fri, 25 Jun 2004 02:33:23 -0700
Subject: [R] String manipulation
In-Reply-To: <BAY2-DAV9PaAn9pmzpE0000f203@hotmail.com>
References: <BAY2-DAV9PaAn9pmzpE0000f203@hotmail.com>
Message-ID: <40DBF163.8080107@pdf.com>

      Does the following do what you want (or supply the missing gaps): 

 > Args <- paste(c("female", "male"), "= names", 1:2, sep="")
 > Args
[1] "female= names1" "male= names2" 
 > listText <- paste("names.list<-list(",
+         paste(Args, collapse=","), ")")
 > listText
[1] "names.list<-list( female= names1,male= names2 )"
 > names1 <- c("Sue", "Betty")
 > names2 <- c("Bill", "Sam")
 > eval(parse(text=listText))
 > names.list
$female
[1] "Sue"   "Betty"

$male
[1] "Bill" "Sam"

      hope this helps.  spencer graves
p.s.  "str" is the name of a function to "Compactly Display the 
Structure of an Arbitrary R Object".  I prefer to avoid masking 
functions with local objects, even though R can tell which object is 
desired in many but not all contexts.  To avoid this, I often test a 
name before I use it.  When I don't get, "Object ... not found", I try 
something different, e.g., using capital letters somewhere or adding a 
"." at the end.  This also has the advantage of introducing me or 
reminding me of functions I may not have known. 

Robin Gruna wrote:

>Hi,
>let's see, if someone can help my with this one:
>I have the string as follows:
>  
>
>>str<-("one","two","three")
>>    
>>
>
>Now I want to concatenate the items to one string, seperateted by space or
>something else,
>  
>
>>str
>>"one, two, three"
>>    
>>
>If possible without a loop.
>
>My actual goal ist to create string like
>  
>
>>str.names
>>"female = names1, male = names2"
>>    
>>
>and pass it as argument to list(), intending to create a list
>  
>
>>names.list<-list( female = names1, male = names2)
>>    
>>
>
>Thanks a lot,
>Robin
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From syed at saudionline.com.sa  Fri Jun 25 11:41:32 2004
From: syed at saudionline.com.sa (Syed Gillani)
Date: Fri, 25 Jun 2004 12:41:32 +0300
Subject: [R] What happened to the excellent R-Newsletter ?
References: <cbgpm2$b50$1@sea.gmane.org>
Message-ID: <004201c45a98$9a04ef40$e90566d4@gillani>

Hi Henrik,
MASS, of course, is very good. In addition, for a comprehensive
introduction, I would strongly recommend Peter Dalgaard's "Introductory
Statistics with R".
Very readable and neophyte-friendly , ISwR should be a standard pocket book
for beginners.
Gillani.

----- Original Message -----
From: "Henrik Andersson" <h.andersson at nioo.knaw.nl>
To: <r-help at stat.math.ethz.ch>
Sent: Friday, June 25, 2004 12:01 PM
Subject: [R] What happened to the excellent R-Newsletter ?


> The last newsletter found on www.r-project.org is from December 2003.
>
> I was looking forward to the next issue, will there be one soon, or what
> happened ?
>
> While I'm waiting for the newsletter I think I will buy some books about
> R and statistics. Is Venables & Ripley, MASS good if you want to learn
> about multivariate data analysis (PCA, PLS, correspondence analysis) and
> how to do it in R ?
>
> Henrik Andersson
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html
>
>



From Martin.Posch at univie.ac.at  Fri Jun 25 11:54:41 2004
From: Martin.Posch at univie.ac.at (Martin Posch)
Date: Fri, 25 Jun 2004 11:54:41 +0200
Subject: [R] Sweave: R code in self defined TeX-commands
Message-ID: <6.1.2.0.1.20040625113001.01b4eea8@ims02.mstat.univie.ac.at>

Hi,

I need to produce a standard report for several variables in Sweave
and thus would need the possibility to define a TeX-command which includes 
R-code like

\newcommand{\meansd}[1]{The mean is \Sexpr{mean(#1)} and the standard 
deviation is  \Sexpr{sd(#1)} .
                      }
and then just write

\meansd{age}

in the latex code to get the whole sentence.


The above does not work, since Sweave ignores the \newcommand and does not 
expand the \statistics

Is there an alternative way to achieve this?

Thanks,
Martin Posch



Department of Medical Statistics
Medical University of Vienna
Schwarzspanierstr. 17, A-1090 Vienna
Tel.:  +43-1-4277-63205



From Joris.DeWolf at cropdesign.com  Fri Jun 25 12:14:59 2004
From: Joris.DeWolf at cropdesign.com (Joris DeWolf)
Date: Fri, 25 Jun 2004 12:14:59 +0200
Subject: [R] difference in order() between Linux and Windows with mixtures of
 caps and normal letters
Message-ID: <40DBFB23.8070600@cropdesign.com>


Could anybody explain this difference in the function order() between R 
under windows and R under Linux ?
It seems that under Linux the order in of character dsitinguishes 
between caps and normal letters and sorts them starting with the 
capitals (see below).
How can I avoid this (I mean: get the same results as I get under Windows)?
Thanks
Joris


Under Windows R 1.6.2

 > mstring <- c("b","c","a","t","d")
 > mstring[order(mstring)]
[1] "a" "b" "c" "d" "t"
 > Mstring <- c("b","c","a","T","D")
 > Mstring[order(Mstring)]
[1] "a" "b" "c" "D" "T"


the same under Linux R.1.6.2

 > mstring <- c("b","c","a","t","d")
 > mstring[order(mstring)]
[1] "a" "b" "c" "d" "t"
 > Mstring <- c("b","c","a","T","D")
 > Mstring[order(Mstring)]
[1] "D" "T" "a" "b" "c"






-- 

====================================================================== 
Joris De Wolf
CropDesign N.V. 
Plant Evaluation Group
Technologiepark 3 
B-9052 Zwijnaarde 
Belgium 
Tel. : +32 9 242 91 55
Fax  : +32 9 241 91 73
====================================================================== 



confidentiality notice:
The information contained in this e-mail is confidential and...{{dropped}}



From ripley at stats.ox.ac.uk  Fri Jun 25 12:45:53 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 25 Jun 2004 11:45:53 +0100 (BST)
Subject: [R] difference in order() between Linux and Windows with mixtures
	of caps and normal letters
In-Reply-To: <40DBFB23.8070600@cropdesign.com>
Message-ID: <Pine.LNX.4.44.0406251133480.32591-100000@gannet.stats>

This is nothing to do with the type of OS and everything to do with the
locales in use.  It looks like you have the C locale in Linux and an
English-language locale in Windows.  Since you don't tell us where in the
world you are, you most likely are from the country that doesn't know
there are any others.* So I suggest you want

LANG=en_US or
LC_COLLATE=en_US

set as your locale when running R (set in .Renviron, say?)

?Sys.getlocale will give you the full scoop, and ?Comparison explains this
exact issue.

BTW, R 1.6.2 is *really* old and my memory of how things work that far
back is probably imperfect (although by chance it is the earliest version
I have still running).


On Fri, 25 Jun 2004, Joris DeWolf wrote:

> Could anybody explain this difference in the function order() between R 
> under windows and R under Linux ?
> It seems that under Linux the order in of character dsitinguishes 
> between caps and normal letters and sorts them starting with the 
> capitals (see below).
> How can I avoid this (I mean: get the same results as I get under Windows)?
> Thanks
> Joris
> 
> 
> Under Windows R 1.6.2
> 
>  > mstring <- c("b","c","a","t","d")
>  > mstring[order(mstring)]
> [1] "a" "b" "c" "d" "t"
>  > Mstring <- c("b","c","a","T","D")
>  > Mstring[order(Mstring)]
> [1] "a" "b" "c" "D" "T"
> 
> 
> the same under Linux R.1.6.2
> 
>  > mstring <- c("b","c","a","t","d")
>  > mstring[order(mstring)]
> [1] "a" "b" "c" "d" "t"
>  > Mstring <- c("b","c","a","T","D")
>  > Mstring[order(Mstring)]
> [1] "D" "T" "a" "b" "c"


* The `National Science Foundation' sends requests to international
referees with no country identification, not even in the address the
telephone code, the email address ....

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ggrothendieck at myway.com  Fri Jun 25 13:33:51 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 25 Jun 2004 11:33:51 +0000 (UTC)
Subject: [R] String manipulation
References: <BAY2-DAV9PaAn9pmzpE0000f203@hotmail.com>
	<001901c45a95$1a52fdd0$ad133a86@www.domain>
Message-ID: <loom.20040625T132516-971@post.gmane.org>


Or perhaps this minor simplification:

vars <- c("names1", "names2")  # or use paste or ls(pattern="names[1-2]")
lis <- lapply(vars, get)
names(lis) <- c("female", "male")

Dimitris Rizopoulos <dimitris.rizopoulos <at> med.kuleuven.ac.be> writes:

: 
: Hi Robin,
: 
: regarding you first question you could use,
: 
: str <- c("one","two","three")
: paste(str, collapse=", ")
: 
: hoewver, describing what you actually want to do I'd use,
: 
: names1 <- letters[1:10]
: names2 <- letters[1:20]
: lis <- lapply(1:2, function(x) get(paste("names", x, sep="")))
: names(lis) <- c("female", "male")
: lis
: 
: I hope this helps.
: 
: Best,
: Dimitris
: 
: ----
: Dimitris Rizopoulos
: Doctoral Student
: Biostatistical Centre
: School of Public Health
: Catholic University of Leuven
: 
: Address: Kapucijnenvoer 35, Leuven, Belgium
: Tel: +32/16/396887
: Fax: +32/16/337015
: Web: http://www.med.kuleuven.ac.be/biostat/
:      http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm
: 
: ----- Original Message ----- 
: From: "Robin Gruna" <robin_gruna <at> hotmail.com>
: To: <r-help <at> stat.math.ethz.ch>
: Sent: Friday, June 25, 2004 11:07 AM
: Subject: [R] String manipulation
: 
: 
: > Hi,
: > let's see, if someone can help my with this one:
: > I have the string as follows:
: > > str<-("one","two","three")
: >
: > Now I want to concatenate the items to one string, seperateted by
: space or
: > something else,
: > >str
: > >"one, two, three"
: > If possible without a loop.
: >
: > My actual goal ist to create string like
: > >str.names
: > >"female = names1, male = names2"
: > and pass it as argument to list(), intending to create a list
: > >names.list<-list( female = names1, male = names2)
: >
: > Thanks a lot,
: > Robin



From christoph.lehmann at gmx.ch  Fri Jun 25 13:51:08 2004
From: christoph.lehmann at gmx.ch (Christoph Lehmann)
Date: Fri, 25 Jun 2004 13:51:08 +0200
Subject: [R] sweave: graphics not at the expected location in the pdf
Message-ID: <1088164268.1290.41.camel@christophl>

Hi 

I use sweave for excellent pdf output (thank you- Friedrich Leisch). I
have just one problem. Quite often it happens, that the graphics are not
at the place where I expect them, but (often on a separate page) later
on in the pdf. How can I fix this, means how can I define, that I want a
graphic exactly here and now in the document?

Many thanks and best regards

Christoph
-- 
Christoph Lehmann <christoph.lehmann at gmx.ch>



From rkoenker at uiuc.edu  Fri Jun 25 14:59:30 2004
From: rkoenker at uiuc.edu (roger koenker)
Date: Fri, 25 Jun 2004 07:59:30 -0500
Subject: [R] Matrix: Help with syntax and comparison with SparseM
In-Reply-To: <1088148644.24614.199153300@webmail.messagingengine.com>
References: <1088148644.24614.199153300@webmail.messagingengine.com>
Message-ID: <7F22CE6C-C6A7-11D8-8CFA-000A95A7E3AA@uiuc.edu>

I'll defer to others on the best way to implement this in Matrix, but it
probably should be noted that for banded structure like this sort of
Whittaker problem more
specialized algorithms have a real advantage.  It  might
require very large problems to see this, however.


url:	www.econ.uiuc.edu/~roger        	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820

On Jun 25, 2004, at 2:30 AM, Nicholas Lewin-Koh wrote:

> Hi,
> I am writing some basic smoothers in R for cleaning some spectral data.
> I wanted to see if I could get close to matlab for speed, so I was
> trying to compare SparseM
> with Matrix to see which could do the choleski decomposition the
> fastest.
>
> Here is the function using SparseM
> difsm <- function(y, lambda, d){
> # Smoothing with a finite difference penalty
> # y:      signal to be smoothed
> # lambda: smoothing parameter
> # d:      order of differences in penalty (generally 2)
> # based on code by Paul Eilers 2002
>   require(SparseM)
>   m <- length(y)
>   E <- as(m,"matrix.diag.csr")
>   D <- diff(E,differences=d)
>   B <- E + (lambda * t(D)%*%D)
>   z <- solve(B,y)
>   z
> }
>
> To do this in Matrix, I am not sure how to get an Identity matrix of
> dimension m. From looking at the documentation I would think that
>  E<-new("cscMatrix", nrow=m, i=integer(m),x=rep(1,m),p=0:(m-1)))
> Should do what I want, but it fails?
>> m<-10
>> E<-new("cscMatrix", nrow=m, i=integer(m),x=rep(1,m),p=0:(m-1))
> Error in initialize(value, ...) : Invalid names for slots of class
> cscMatrix: nrow
>> E<-new("cscMatrix", i=integer(m),x=rep(1,m),p=0:(m-1))
> Error in validObject(.Object) : Invalid "cscMatrix" object: last 
> element
> of slot p must match length of slots i and x
>
> Granted I am not very fascile with the S4 classes, so I may be doing
> something stupid.
> Also to do the next step there is no diff method for matrices in 
> Matrix,
> that would be nice
> :)
>
> I guess the last step would be easy
> z <- solve((E + (lambda * crossprod(D))),y)
>
> But I can't get the Identity matrix???
>
> Thanks for the help,
> it is probably obvious, but I am missing it.
>
> Nicholas
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From slacey at umich.edu  Fri Jun 25 15:23:08 2004
From: slacey at umich.edu (Steven Lacey)
Date: Fri, 25 Jun 2004 09:23:08 -0400
Subject: [R] understanding nlm
Message-ID: <000001c45ab7$92110340$5081d38d@lsa.adsroot.itcs.umich.edu>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040625/8f2b5325/attachment.pl

From bates at stat.wisc.edu  Fri Jun 25 15:34:37 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: Fri, 25 Jun 2004 08:34:37 -0500
Subject: [R] Matrix: Help with syntax and comparison with SparseM
In-Reply-To: <1088148644.24614.199153300@webmail.messagingengine.com>
References: <1088148644.24614.199153300@webmail.messagingengine.com>
Message-ID: <40DC29ED.8060101@stat.wisc.edu>

Nicholas Lewin-Koh wrote:

> Hi,
> I am writing some basic smoothers in R for cleaning some spectral data.
> I wanted to see if I could get close to matlab for speed, so I was
> trying to compare SparseM
> with Matrix to see which could do the choleski decomposition the
> fastest.
> 
> Here is the function using SparseM
> difsm <- function(y, lambda, d){
> # Smoothing with a finite difference penalty
> # y:      signal to be smoothed
> # lambda: smoothing parameter
> # d:      order of differences in penalty (generally 2)
> # based on code by Paul Eilers 2002
>   require(SparseM)
>   m <- length(y)
>   E <- as(m,"matrix.diag.csr")
>   D <- diff(E,differences=d)
>   B <- E + (lambda * t(D)%*%D)
>   z <- solve(B,y)
>   z
> }
> 
> To do this in Matrix, I am not sure how to get an Identity matrix of
> dimension m. From looking at the documentation I would think that
>  E<-new("cscMatrix", nrow=m, i=integer(m),x=rep(1,m),p=0:(m-1)))
> Should do what I want, but it fails?
> 
>>m<-10
>>E<-new("cscMatrix", nrow=m, i=integer(m),x=rep(1,m),p=0:(m-1))
> 
> Error in initialize(value, ...) : Invalid names for slots of class
> cscMatrix: nrow
> 
>>E<-new("cscMatrix", i=integer(m),x=rep(1,m),p=0:(m-1))
> 
> Error in validObject(.Object) : Invalid "cscMatrix" object: last element
> of slot p must match length of slots i and x
> 
> Granted I am not very fascile with the S4 classes, so I may be doing
> something stupid.
> Also to do the next step there is no diff method for matrices in Matrix,
> that would be nice
> :)
> 
> I guess the last step would be easy
> z <- solve((E + (lambda * crossprod(D))),y)
> 
> But I can't get the Identity matrix???
> 
> Thanks for the help,
> it is probably obvious, but I am missing it.

It is not really that obvious.  Current versions of the Matrix package 
have limitations, especially with respect to constructors.  Essentially 
that Matrix package implements what I needed at the time.

The easiest way to generate a sparse matrix is to create a tripletMatrix 
object and coerce it to a cscMatrix, or in your case perhaps an 
sscMatrix (symmetric sparse column-oriented matrix) object.

An identity matrix could be generated by

m <- 10
E <- as(new("tripletMatrix", Dim = as.integer(c(m, m)), i = 1:m,
             j = 1:m, x = rep(1, m)), "cscMatrix")

(I hope that is correct.  I am writing this email in a system that does 
not do parenthesis matching for me so I may have mistakes in there.)



From laura at env.leeds.ac.uk  Fri Jun 25 15:41:30 2004
From: laura at env.leeds.ac.uk (Laura Quinn)
Date: Fri, 25 Jun 2004 14:41:30 +0100 (BST)
Subject: [R] further problems with arima..
Message-ID: <Pine.LNX.4.44.0406251437060.22020-100000@env-pc-phd13>

Sorry to keep quizzing the r-helpers but I'm having real difficulty with
arima.

I am happy that i have chosen the best fitting arima model for my
data,(in this case arima(1,1,1) but when i use the predict function the
forecast damps to a constant value after just two or three values, and
from a time series of almost 3000 values i am left with only a couple of
predicted values. I have tried aggregating my data to condense finely
spaced time points into larger clumps but to no avail. please can someone
offer a little advise on how i might improve on this?

thank you,
Laura Quinn
Institute of Atmospheric Science
School of the Environment
University of Leeds
Leeds
LS2 9JT

tel: +44 113 343 1596
fax: +44 113 343 6716
mail: laura at env.leeds.ac.uk



From snvk4u at yahoo.co.in  Fri Jun 25 15:51:51 2004
From: snvk4u at yahoo.co.in (neela v)
Date: Fri, 25 Jun 2004 06:51:51 -0700 (PDT)
Subject: [R] help on integration
In-Reply-To: <200406251004.i5PA1nq7014863@hypatia.math.ethz.ch>
Message-ID: <20040625135151.99663.qmail@web8203.mail.in.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040625/042f72a9/attachment.pl

From ggrothendieck at myway.com  Fri Jun 25 16:02:36 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 25 Jun 2004 14:02:36 +0000 (UTC)
Subject: [R] understanding nlm
References: <000001c45ab7$92110340$5081d38d@lsa.adsroot.itcs.umich.edu>
Message-ID: <loom.20040625T155940-540@post.gmane.org>


Some things to try are:

1. the nls function
2. replacing p with 1/p

Steven Lacey <slacey <at> umich.edu> writes:

: 
: Hi, 
: 
: I am using the nlm() function to fit the following exponential function to
: some data by minimizing squared differences between predicted and observed
: values:
: 
: csexponential<- function(x, t1, ti, p){
:     ti + abs(t1 - ti)*(exp(-(p*(x-1))))
: }
: 
: As background, the data is performance measured across time. As you might
: imagine, we get rapid improvement across the first couple of time points and
: then the improvement becomes more gradual. In psychology this is known as
: the power law of practice. 
: 
: For some cases the learning is so rapid that the function appears to have a
: notch (imagine an "L" shaped function). In these cases the parameter
: estimate for the power, p, is large (typically p>15). 
: 
: I have repeated the fitting procedure on the same set of data and "appear"
: to have found that with the same starting values and arguments to nlm I get
: somewhat different values for p in those cases of extremely rapid learning
: described above. For example, one time p=21 and another p=23. The relative
: change is not huge, but I would like the parameter estimates to be stable
: across replications with the same data/settings.
: 
: It is certainly possible that I changed something in the code inadvertantly
: and that is why I am observing these discrepancies. However, it is also
: possible that there may be some random decision making within nlm. So that
: if nlm finds a space where the fits are equally good, it may return slightly
: different values each time it is run because it lands in a different
: location.
: 
: I suspect my later interpretation is false because the estimated values do
: not change after each and every replication of the analysis. But I thought I
: might ask. 
: 
: Thanks for any insight you can provide, 
: Steve Lacey



From ivo_welch-Rstat at mailblocks.com  Fri Jun 25 16:11:22 2004
From: ivo_welch-Rstat at mailblocks.com (ivo_welch-Rstat@mailblocks.com)
Date: Fri, 25 Jun 2004 07:11:22 -0700
Subject: [R] circle / oval / semicircle ?
Message-ID: <200406251411.i5PEBNY1004525@hypatia.math.ethz.ch>


hi:  where would I find facilities to draw circles, ovals, and 
semicircles?   (or should I construct them myself using curve?)

regards, /ivo



From ripley at stats.ox.ac.uk  Fri Jun 25 16:20:53 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 25 Jun 2004 15:20:53 +0100 (BST)
Subject: [R] help on integration
In-Reply-To: <20040625135151.99663.qmail@web8203.mail.in.yahoo.com>
Message-ID: <Pine.LNX.4.44.0406251516410.3829-100000@gannet.stats>

On Fri, 25 Jun 2004, neela v wrote:

> Dear Sir

This is an email list, not a person.

> I am an analyst based in India, actively working on price analysis and
> forecasting.

>  I would request you to let me know is it possible to write front end in
> MS VB and call R function to do the statistical analysis. The data is
> inputed through the form designed in VB.

Yes, it is possible, in many ways.  Take a look at the Software->Other
page on CRAN, and the R-D(COM) interface, for one example.   Or go 
straight to

http://cran.r-project.org/contrib/extra/dcom/RSrv135.html

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From deepayan at stat.wisc.edu  Fri Jun 25 16:27:16 2004
From: deepayan at stat.wisc.edu (Deepayan Sarkar)
Date: Fri, 25 Jun 2004 09:27:16 -0500
Subject: [R] circle / oval / semicircle ?
In-Reply-To: <200406251411.i5PEBNY1004525@hypatia.math.ethz.ch>
References: <200406251411.i5PEBNY1004525@hypatia.math.ethz.ch>
Message-ID: <200406250927.16188.deepayan@stat.wisc.edu>

On Friday 25 June 2004 09:11, ivo_welch-Rstat at mailblocks.com wrote:
> hi:  where would I find facilities to draw circles, ovals, and
> semicircles?   (or should I construct them myself using curve?)

The latter probably, but using lines rather than curve (these being 
implicit/parametric functions). e.g. 

tt = seq(0, 2 * pi, length = 50)
plot(cos(tt), sin(tt), type = 'l')

Deepayan



From macq at llnl.gov  Fri Jun 25 16:28:46 2004
From: macq at llnl.gov (Don MacQueen)
Date: Fri, 25 Jun 2004 07:28:46 -0700
Subject: [R] Sweave: R code in self defined TeX-commands
In-Reply-To: <6.1.2.0.1.20040625113001.01b4eea8@ims02.mstat.univie.ac.at>
References: <6.1.2.0.1.20040625113001.01b4eea8@ims02.mstat.univie.ac.at>
Message-ID: <p0600200dbd01e45e8104@[128.115.153.6]>

Here are a couple of ideas (not tested). Neither is as succinct as 
what you had in mind.


\newcommand{\meansd}{The mean is \Sexpr{mean(tmpvar)} and the 
standard deviation is  \Sexpr{sd(tmpvar)} .

<<echo=FALSE,results=hide>>=
tmpvar <- age
@
\meansd

<<echo=FALSE,results=hide>>=
tmpvar <- weight
@
\meansd

<<echo=FALSE,results=hide>>=
tmpvar <- height
@
\meansd


###### or perhaps ########
<<echo=FALSE,results=hide>>=
meansd <- function(x) cat('The mean is ',mean(x),' and the standard 
deviation is ',sd(x),'.',sep='')
@

<<results=tex>>=
meansd(age)
@

<<results=tex>>=
meansd(weight)
@

<<results=tex>>=
meansd(height)
@

-Don

At 11:54 AM +0200 6/25/04, Martin Posch wrote:
>Hi,
>
>I need to produce a standard report for several variables in Sweave
>and thus would need the possibility to define a TeX-command which 
>includes R-code like
>
>\newcommand{\meansd}[1]{The mean is \Sexpr{mean(#1)} and the 
>standard deviation is  \Sexpr{sd(#1)} .
>                      }
>and then just write
>
>\meansd{age}
>
>in the latex code to get the whole sentence.
>
>
>The above does not work, since Sweave ignores the \newcommand and 
>does not expand the \statistics
>
>Is there an alternative way to achieve this?
>
>Thanks,
>Martin Posch
>
>
>
>Department of Medical Statistics
>Medical University of Vienna
>Schwarzspanierstr. 17, A-1090 Vienna
>Tel.:  +43-1-4277-63205
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From rpeng at jhsph.edu  Fri Jun 25 16:47:54 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Fri, 25 Jun 2004 10:47:54 -0400
Subject: [R] circle / oval / semicircle ?
In-Reply-To: <200406251411.i5PEBNY1004525@hypatia.math.ethz.ch>
References: <200406251411.i5PEBNY1004525@hypatia.math.ethz.ch>
Message-ID: <40DC3B1A.1070006@jhsph.edu>

The `ellipse' package is useful for drawing, well, ellipses/circles, 
but more in statistical context.

-roger

ivo_welch-Rstat at mailblocks.com wrote:
> 
> hi:  where would I find facilities to draw circles, ovals, and 
> semicircles?   (or should I construct them myself using curve?)
> 
> regards, /ivo
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From macq at llnl.gov  Fri Jun 25 16:49:06 2004
From: macq at llnl.gov (Don MacQueen)
Date: Fri, 25 Jun 2004 07:49:06 -0700
Subject: [R] sweave: graphics not at the expected location in the pdf
In-Reply-To: <1088164268.1290.41.camel@christophl>
References: <1088164268.1290.41.camel@christophl>
Message-ID: <p0600200ebd01e9cfc76b@[128.115.153.6]>

This is actually a LaTeX question, not an R (or even Sweave, strictly 
speaking) question.

Here is what I have done. In the preamble, specify

\usepackage{here}

Then later on specify [H] when including a figure, as in:

\begin{figure}[H]
   \begin{center}
<<fig=TRUE,width=6.5,height=4>>=
plot(x,y)
@
     \caption{My plot\label{fig1}}
   \end{center}
\end{figure}

Is the "here" package *required* in order to use [H]? I don't know. I 
just found it mentioned in a LaTeX book, tried it, and it worked. The 
most helpful book I've found so far is
      Guide to LaTeX (fourth edition), by Helmut Kopka and Patrick W. Daly

And yes, Sweave is great!

-Don

At 1:51 PM +0200 6/25/04, Christoph Lehmann wrote:
>Hi
>
>I use sweave for excellent pdf output (thank you- Friedrich Leisch). I
>have just one problem. Quite often it happens, that the graphics are not
>at the place where I expect them, but (often on a separate page) later
>on in the pdf. How can I fix this, means how can I define, that I want a
>graphic exactly here and now in the document?
>
>Many thanks and best regards
>
>Christoph
>--
>Christoph Lehmann <christoph.lehmann at gmx.ch>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
--------------------------------------
Don MacQueen
Environmental Protection Department
Lawrence Livermore National Laboratory
Livermore, CA, USA



From gerifalte28 at hotmail.com  Fri Jun 25 16:50:48 2004
From: gerifalte28 at hotmail.com (F Z)
Date: Fri, 25 Jun 2004 14:50:48 +0000
Subject: [R] Unique.data.frame...still getting duplicates
Message-ID: <BAY99-F271xF4KUo7Zn00006604@hotmail.com>

Thanks to Alec Stevenson, Andy Liaw and Prof. Brian Ripley.  I tried Alec's 
suggestion;

>data[!duplicated(data$ID),] d_duplicated(dupl$ID)
>summary(as.factor(d))
FALSE
21547 #it worked!

Thanks again!

>From: "Alec Stephenson" <astephen at efs.mq.edu.au>
>To: <gerifalte28 at hotmail.com>, <r-help at stat.math.ethz.ch>
>Subject: Re: [R] Unique.data.frame...still getting duplicates
>Date: Fri, 25 Jun 2004 12:45:26 +1000
>
>data[!duplicated(data$ID),]
>will do. Your unique(data[ID,]) removes duplicated rows in data[ID,],
>assuming the object ID exists.
>
>
>
>Alec Stephenson
>Department of Statistics
>Macquarie University
>NSW 2109, Australia
>
> >>> "F Z" <gerifalte28 at hotmail.com> 06/25/04 12:12pm >>>
>Hi there
>
>I have a data frame with about 65,000 rows and 8 variables.  I am
>trying to
>get rid of the double entries of a factor variable "ID" so I can get a
>
>unique observation for each ID
>
>I tried:
>
> >dupl_unique.data.frame(data[ID,]) #I obtain a data frame with 21,547
> >observations..so far so good, but then when I check for duplicates
>
> >d_duplicated(dupl2$ID)
> >summary(as.factor(d))
>FALSE  TRUE
>   6836 14711
>
>Meaning that I am still getting 14,711 duplicates!
>
>I tried changing the ID type to integer and repeated the process but I
>got
>dentical results....what am I missing?
>
>Thanks!
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide!
>http://www.R-project.org/posting-guide.html



From B.Rowlingson at lancaster.ac.uk  Fri Jun 25 17:14:29 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 25 Jun 2004 16:14:29 +0100
Subject: [R] circle / oval / semicircle ?
In-Reply-To: <200406251411.i5PEBNY1004525@hypatia.math.ethz.ch>
References: <200406251411.i5PEBNY1004525@hypatia.math.ethz.ch>
Message-ID: <40DC4155.8010600@lancaster.ac.uk>

ivo_welch-Rstat at mailblocks.com wrote:
> 
> hi:  where would I find facilities to draw circles, ovals, and 
> semicircles?   (or should I construct them myself using curve?)
> 

We did semicircles last month, only they were called half-circles then:

http://tolstoy.newcastle.edu.au/R/help/04/05/0849.html

lines() and polygons() can draw just about anything.

Baz



From tpapp at axelero.hu  Fri Jun 25 17:02:48 2004
From: tpapp at axelero.hu (Tamas Papp)
Date: Fri, 25 Jun 2004 17:02:48 +0200
Subject: [R] sweave: graphics not at the expected location in the pdf
In-Reply-To: <1088164268.1290.41.camel@christophl>
References: <1088164268.1290.41.camel@christophl>
Message-ID: <20040625150248.GA2055@axelero.hu>

On Fri, Jun 25, 2004 at 01:51:08PM +0200, Christoph Lehmann wrote:

> I use sweave for excellent pdf output (thank you- Friedrich Leisch). I
> have just one problem. Quite often it happens, that the graphics are not
> at the place where I expect them, but (often on a separate page) later
> on in the pdf. How can I fix this, means how can I define, that I want a
> graphic exactly here and now in the document?

This is not an R problem.  Standard LaTeX is squeamish about putting
figures on a pages where the fit is "tight".

Put

\renewcommand\floatpagefraction{.99}
\renewcommand\topfraction{.99}
\renewcommand\bottomfraction{.99}
\renewcommand\textfraction{.01}

in the document preamble.  You might have to play around with the
values a bit, make sure that \textfraction and the others add up to 1.

Best,

Tamas

-- 
Tam??s K. Papp
E-mail: tpapp at axelero.hu (preferred, especially for large messages)
        tpapp at westel900.net
Please try to send only (latin-2) plain text, not HTML or other garbage.



From ivo_welch at mailblocks.com  Fri Jun 25 17:22:04 2004
From: ivo_welch at mailblocks.com (ivo welch)
Date: Fri, 25 Jun 2004 11:22:04 -0400
Subject: [R] circle / oval / semicircle ?
In-Reply-To: <40DC4155.8010600@lancaster.ac.uk>
References: <200406251411.i5PEBNY1004525@hypatia.math.ethz.ch>
	<40DC4155.8010600@lancaster.ac.uk>
	<40DC431C.9000108@mailblocks.com>
	<ivo_welch-0ndu3ASHevkEO5qhayy6NAaXGmKxntH@mailblocks.com>
	<ivo_welch-0otu3ASPevkE5hjn52+luTwsP7CqUm0@mailblocks.com>
Message-ID: <ivo_welch-0otu3ASXevkEdaP1oMY1H1hjpb1ndpI@mailblocks.com>


thanks, guys.  circles might be a good candidate for a graphics 
primitive in the next R version, if only because---unlike the solutions 
here---I believe that postscript/pdf have scaleable primitives---though 
I am no postscript expert.  of course, if I knew what I was talking 
about, I would just write the function and submit it.  the closest I 
have found was in http://wtonline.vitalnews.com/Pages/Tip0163.html

regards, /ivo



From B.Rowlingson at lancaster.ac.uk  Fri Jun 25 17:32:55 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Fri, 25 Jun 2004 16:32:55 +0100
Subject: [R] circle / oval / semicircle ?
In-Reply-To: <ivo_welch-0ndu3ASHevkEO5qhayy6NAaXGmKxntH@mailblocks.com>
References: <200406251411.i5PEBNY1004525@hypatia.math.ethz.ch>
	<40DC4155.8010600@lancaster.ac.uk>
	<40DC431C.9000108@mailblocks.com>
	<ivo_welch-0ndu3ASHevkEO5qhayy6NAaXGmKxntH@mailblocks.com>
Message-ID: <40DC45A7.9050402@lancaster.ac.uk>

ivo welch wrote:
> 
> thanks, guys.  circles might be a good candidate for a graphics 
> primitive in the next R version, 

  The 'symbols' function can draw circles, as well as a few other things 
- stars, squares, rectangles, boxplots and thermometers.

  Postscript produced with symbols' circles option does produce true 
scalable circles, but I don't think you can have true scalable 
postscript ellipses without some major hacking on R's device driver code!

Baz



From lhill at ipimar.pt  Fri Jun 25 17:48:03 2004
From: lhill at ipimar.pt (Louize Hill)
Date: Fri, 25 Jun 2004 16:48:03 +0100
Subject: [R] trouble using boot package
Message-ID: <018b01c45acb$ccec5dc0$36040a0a@Louisept>

Hello,

I am trying to carry out a bootstrap analysis (using the boot package) on a
table and cannot work out how to get the results I need!
I have a table ("d2") with 4 columns: "ID_code", "Age", "Quarter" and
"StomWt". Age (0-5) and Quarter (1-4) are my strata
Therefore I wish to estimate the confidence intervals for the mean StomWt
for each Age and Quarter.

If I do this manually for each age / quarter I get the following:

> boot (d2$StomWt[d2$Age=="0" & d2$Quarter=="1"], mean, R=999)

ORDINARY NONPARAMETRIC BOOTSTRAP

Call:
boot(data = d2$StomWt[d2$Age == "0" & d2$Quarter == "1"], statistic = mean,
    R = 999)

Bootstrap Statistics :
    original  bias    std. error
t1*    3.365       0           0

Firstly - shouldn't the bias and std error be something other than 0?
Furthermore, I would like to know if there is a way (function?) I can use
that would automate this?

Thank you for your help
Louize Hill



From ggrothendieck at myway.com  Fri Jun 25 17:54:34 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 25 Jun 2004 15:54:34 +0000 (UTC)
Subject: [R] circle / oval / semicircle ?
References: <200406251411.i5PEBNY1004525@hypatia.math.ethz.ch>
	<40DC4155.8010600@lancaster.ac.uk>
	<40DC431C.9000108@mailblocks.com>
	<ivo_welch-0ndu3ASHevkEO5qhayy6NAaXGmKxntH@mailblocks.com>
	<40DC45A7.9050402@lancaster.ac.uk>
Message-ID: <loom.20040625T175025-781@post.gmane.org>

Barry Rowlingson <B.Rowlingson <at> lancaster.ac.uk> writes:

:   The 'symbols' function can draw circles, as well as a few other things 
: - stars, squares, rectangles, boxplots and thermometers.

For circles, one can also use plot like this:

   plot(1:5,pch=19,cex=1:5,col=rainbow(5))

See balloonplot (the actual call to plot is in balloonplot.default) in
package gregmisc for another example of this technique.



From sundar.dorai-raj at PDF.COM  Fri Jun 25 18:09:24 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Fri, 25 Jun 2004 11:09:24 -0500
Subject: [R] circle / oval / semicircle ?
In-Reply-To: <200406251411.i5PEBNY1004525@hypatia.math.ethz.ch>
References: <200406251411.i5PEBNY1004525@hypatia.math.ethz.ch>
Message-ID: <40DC4E34.8020706@pdf.com>



ivo_welch-Rstat at mailblocks.com wrote:

> 
> hi:  where would I find facilities to draw circles, ovals, and 
> semicircles?   (or should I construct them myself using curve?)
> 
> regards, /ivo
> 

Here's a thread from the archive:

http://finzi.psych.upenn.edu/R/Rhelp02/archive/4217.html

--sundar



From GPetris at uark.edu  Fri Jun 25 18:10:43 2004
From: GPetris at uark.edu (Giovanni Petris)
Date: Fri, 25 Jun 2004 11:10:43 -0500 (CDT)
Subject: [R] Installing on Windows packages build on Unix
Message-ID: <200406251610.i5PGAhJd024475@definetti.uark.edu>


Hello,

I wanted to share with a colleague a few R functions that I wrote. To
this purpose, I created a small package on my machine (Unix) and
emailed it to her. Now she is having troubles installing the package
on her Windows machine. It seems that on her side, install.packages
looks for a "zip" file - while I have created a "tar.gz" file. I tried
to build the package with the --use-zip options but that didn't work. 

Any suggestions?

TIA,
Giovanni

-- 

 __________________________________________________
[                                                  ]
[ Giovanni Petris                 GPetris at uark.edu ]
[ Department of Mathematical Sciences              ]
[ University of Arkansas - Fayetteville, AR 72701  ]
[ Ph: (479) 575-6324, 575-8630 (fax)               ]
[ http://definetti.uark.edu/~gpetris/              ]
[__________________________________________________]



From partha_bagchi at hgsi.com  Fri Jun 25 18:22:42 2004
From: partha_bagchi at hgsi.com (partha_bagchi@hgsi.com)
Date: Fri, 25 Jun 2004 12:22:42 -0400
Subject: [R] Memory.limit
Message-ID: <OF2C8B5D25.32DB22E3-ON85256EBE.00599D65-85256EBE.0059F846@hgsi.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040625/35025986/attachment.pl

From wolski at molgen.mpg.de  Fri Jun 25 18:23:34 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Fri, 25 Jun 2004 18:23:34 +0200
Subject: [R] Installing on Windows packages build on Unix
In-Reply-To: <200406251610.i5PGAhJd024475@definetti.uark.edu>
References: <200406251610.i5PGAhJd024475@definetti.uark.edu>
Message-ID: <200406251823340142.16A21EE5@mail.math.fu-berlin.de>

Hallo Giovanni!

You have to build a windows package on linux/unix.

cran.r-project.org/doc/contrib/cross-build.pdf

Eryk


*********** REPLY SEPARATOR  ***********

On 6/25/2004 at 11:10 AM Giovanni Petris wrote:

>>>Hello,
>>>
>>>I wanted to share with a colleague a few R functions that I wrote. To
>>>this purpose, I created a small package on my machine (Unix) and
>>>emailed it to her. Now she is having troubles installing the package
>>>on her Windows machine. It seems that on her side, install.packages
>>>looks for a "zip" file - while I have created a "tar.gz" file. I tried
>>>to build the package with the --use-zip options but that didn't work. 
>>>
>>>Any suggestions?
>>>
>>>TIA,
>>>Giovanni
>>>
>>>-- 
>>>
>>> __________________________________________________
>>>[                                                  ]
>>>[ Giovanni Petris                 GPetris at uark.edu ]
>>>[ Department of Mathematical Sciences              ]
>>>[ University of Arkansas - Fayetteville, AR 72701  ]
>>>[ Ph: (479) 575-6324, 575-8630 (fax)               ]
>>>[ http://definetti.uark.edu/~gpetris/              ]
>>>[__________________________________________________]
>>>
>>>______________________________________________
>>>R-help at stat.math.ethz.ch mailing list
>>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



Dipl. bio-chem. Eryk Witold Wolski    @    MPI-Moleculare Genetic   
Ihnestrasse 63-73 14195 Berlin       'v'    
tel: 0049-30-83875219               /   \    
mail: wolski at molgen.mpg.de        ---W-W----    http://www.molgen.mpg.de/~wolski



From Roger.Bivand at nhh.no  Fri Jun 25 18:27:24 2004
From: Roger.Bivand at nhh.no (Roger Bivand)
Date: Fri, 25 Jun 2004 18:27:24 +0200 (CEST)
Subject: [R] Installing on Windows packages build on Unix
In-Reply-To: <200406251610.i5PGAhJd024475@definetti.uark.edu>
Message-ID: <Pine.LNX.4.44.0406251818210.18685-100000@reclus.nhh.no>

On Fri, 25 Jun 2004, Giovanni Petris wrote:

> 
> Hello,
> 
> I wanted to share with a colleague a few R functions that I wrote. To
> this purpose, I created a small package on my machine (Unix) and
> emailed it to her. Now she is having troubles installing the package
> on her Windows machine. It seems that on her side, install.packages
> looks for a "zip" file - while I have created a "tar.gz" file. I tried
> to build the package with the --use-zip options but that didn't work. 

The package you built was, almost certainly, a source package when what 
your colleague needs to install is a binary package, rolled up in a *.zip. 
If you'll be doing this more than once, follow R for Windows FAQ 3.1:

"... read the file readme.packages. You will need to collect and install 
several tools to use this: you can download them via the portal at 
http://www.murdoch-sutherland.com/Rtools/. Once you have done so, just run 
R CMD INSTALL pkgname."

on a Windows machine - then you'll be able to do this whenever you need 
to (see "Simple ports" for your case). It just works when you follow the 
marked path through the magic swamp; if you leave the path, you sink 
without trace.

> 
> Any suggestions?
> 
> TIA,
> Giovanni
> 
> 

-- 
Roger Bivand
Economic Geography Section, Department of Economics, Norwegian School of
Economics and Business Administration, Breiviksveien 40, N-5045 Bergen,
Norway. voice: +47 55 95 93 55; fax +47 55 95 93 93
e-mail: Roger.Bivand at nhh.no



From dardiakm at jmu.edu  Fri Jun 25 19:10:51 2004
From: dardiakm at jmu.edu (dardiakm@jmu.edu)
Date: Fri, 25 Jun 2004 13:10:51 -0400
Subject: [R] Fortran in R??
Message-ID: <70e0ff26.561417c8.1d681700@mpmail2.jmu.edu>

Hello,
  We have been trying to use a well working Fortran program 
in R but have been unable to upload it as part of our 
undergraduate research.  We are working with windows 98.  
We've read all of what we can find, but cannot figure it 
out.  We've tried the dyn.load commands and whatnot.  Do we 
need a package in order to do this?  Do we need to do it on a 
computer that has the Fortran program on it...and should we 
compile it before we even attempt to upload it?  Do you have 
any suggestions?  Thanks for your time. 
Kris Dardia



From dmurdoch at pair.com  Fri Jun 25 19:14:25 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Fri, 25 Jun 2004 13:14:25 -0400
Subject: [R] Fortran in R??
In-Reply-To: <70e0ff26.561417c8.1d681700@mpmail2.jmu.edu>
References: <70e0ff26.561417c8.1d681700@mpmail2.jmu.edu>
Message-ID: <78nod096sks013h4219e13f0m42pl6d851@4ax.com>

On Fri, 25 Jun 2004 13:10:51 -0400, <dardiakm at jmu.edu> wrote :

>Hello,
>  We have been trying to use a well working Fortran program 
>in R but have been unable to upload it as part of our 
>undergraduate research.  We are working with windows 98.  
>We've read all of what we can find, but cannot figure it 
>out.  We've tried the dyn.load commands and whatnot.  Do we 
>need a package in order to do this?  Do we need to do it on a 
>computer that has the Fortran program on it...and should we 
>compile it before we even attempt to upload it?  Do you have 
>any suggestions?  Thanks for your time. 

You definitely need to compile it before using dyn.load.  Instructions
for how to do this sort of thing are in Writing R Extensions manual,
which is distributed with R.

It is usually easiest to compile using the R script, e.g.

R CMD SHLIB myprog.f

Duncan Murdoch



From dgrove at fhcrc.org  Fri Jun 25 19:35:03 2004
From: dgrove at fhcrc.org (Douglas Grove)
Date: Fri, 25 Jun 2004 10:35:03 -0700 (PDT)
Subject: [R] alternate rank method
Message-ID: <Pine.LNX.4.44.0406251007400.19740-100000@jerboa.fhcrc.org>

Hi,

I'm wondering if anyone can point me to a function that will
allow me to do a ranking that treats ties differently than
rank() provides for?

I'd like a method that will assign to the elements of each 
tie group the largest rank. 

An example:  

For the vector 'v', I'd like the method to return 'rv'

 v:  1 2 3 3 3 4 5 5 6 7
rv:  1 2 5 5 5 6 8 8 9 10


Thanks,
Doug Grove



From egcp at hotmail.com  Fri Jun 25 19:36:30 2004
From: egcp at hotmail.com (E GCP)
Date: Fri, 25 Jun 2004 17:36:30 +0000
Subject: [R] rgl installation problems
Message-ID: <BAY22-F15q0NTY9agv3000096d1@hotmail.com>

Hi!

I'm new to R, but have worked with Splus before. I installed several 
packages in R (R-1.9.1) without problems, but when I try to install rgl 
(rgl_0.64-13.tar.gz). I get the following, and the package does not install. 
Any help would be greatly appreciated. I'm running R in redhat 9.

Thanks,
Enrique
-------
[my pc]# R CMD INSTALL rgl_0.64-13.tar.gz
* Installing *source* package 'rgl' ...
checking build system type... i686-pc-linux-gnu
checking host system type... i686-pc-linux-gnu
checking for gcc... gcc
checking for C compiler default output file name... a.out
checking whether the C compiler works... yes
checking whether we are cross compiling... no
checking for suffix of executables...
checking for suffix of object files... o
checking whether we are using the GNU C compiler... yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ANSI C... none needed
checking how to run the C preprocessor... gcc -E
checking for X... libraries /usr/X11R6/lib, headers /usr/X11R6/include

checking for libpng-config... no
checking libpng in /usr/local... not found
checking libpng in /usr... found
configure: creating ./config.status
config.status: creating src/Makevars
** libs
g++ -I/usr/lib/R/include -I/usr/X11R6/include -DHAVE_PNG_H -I/usr/include 
-I/usr
/local/include  -Wall -pedantic -fno-exceptions -fno-rtti -fPIC  -O2 -g 
-march=i
386 -mcpu=i686 -c x11lib.cpp -o x11lib.o
g++ -I/usr/lib/R/include -I/usr/X11R6/include -DHAVE_PNG_H -I/usr/include 
-I/usr
/local/include  -Wall -pedantic -fno-exceptions -fno-rtti -fPIC  -O2 -g 
-march=i
386 -mcpu=i686 -c x11gui.cpp -o x11gui.o
g++ -I/usr/lib/R/include -I/usr/X11R6/include -DHAVE_PNG_H -I/usr/include 
-I/usr
/local/include  -Wall -pedantic -fno-exceptions -fno-rtti -fPIC  -O2 -g 
-march=i
386 -mcpu=i686 -c types.cpp -o types.o
g++ -I/usr/lib/R/include -I/usr/X11R6/include -DHAVE_PNG_H -I/usr/include 
-I/usr
/local/include  -Wall -pedantic -fno-exceptions -fno-rtti -fPIC  -O2 -g 
-march=i
386 -mcpu=i686 -c math.cpp -o math.o
g++ -I/usr/lib/R/include -I/usr/X11R6/include -DHAVE_PNG_H -I/usr/include 
-I/usr
/local/include  -Wall -pedantic -fno-exceptions -fno-rtti -fPIC  -O2 -g 
-march=i
386 -mcpu=i686 -c fps.cpp -o fps.o
g++ -I/usr/lib/R/include -I/usr/X11R6/include -DHAVE_PNG_H -I/usr/include 
-I/usr
/local/include  -Wall -pedantic -fno-exceptions -fno-rtti -fPIC  -O2 -g 
-march=i
386 -mcpu=i686 -c pixmap.cpp -o pixmap.o
g++ -I/usr/lib/R/include -I/usr/X11R6/include -DHAVE_PNG_H -I/usr/include 
-I/usr
/local/include  -Wall -pedantic -fno-exceptions -fno-rtti -fPIC  -O2 -g 
-march=i
386 -mcpu=i686 -c gui.cpp -o gui.o
g++ -I/usr/lib/R/include -I/usr/X11R6/include -DHAVE_PNG_H -I/usr/include 
-I/usr
/local/include  -Wall -pedantic -fno-exceptions -fno-rtti -fPIC  -O2 -g 
-march=i
386 -mcpu=i686 -c api.cpp -o api.o
g++ -I/usr/lib/R/include -I/usr/X11R6/include -DHAVE_PNG_H -I/usr/include 
-I/usr
/local/include  -Wall -pedantic -fno-exceptions -fno-rtti -fPIC  -O2 -g 
-march=i
386 -mcpu=i686 -c device.cpp -o device.o
g++ -I/usr/lib/R/include -I/usr/X11R6/include -DHAVE_PNG_H -I/usr/include 
-I/usr
/local/include  -Wall -pedantic -fno-exceptions -fno-rtti -fPIC  -O2 -g 
-march=i
386 -mcpu=i686 -c devicemanager.cpp -o devicemanager.o
g++ -I/usr/lib/R/include -I/usr/X11R6/include -DHAVE_PNG_H -I/usr/include 
-I/usr
/local/include  -Wall -pedantic -fno-exceptions -fno-rtti -fPIC  -O2 -g 
-march=i
386 -mcpu=i686 -c rglview.cpp -o rglview.o
g++ -I/usr/lib/R/include -I/usr/X11R6/include -DHAVE_PNG_H -I/usr/include 
-I/usr
/local/include  -Wall -pedantic -fno-exceptions -fno-rtti -fPIC  -O2 -g 
-march=i
386 -mcpu=i686 -c scene.cpp -o scene.o
g++ -I/usr/lib/R/include -I/usr/X11R6/include -DHAVE_PNG_H -I/usr/include 
-I/usr
/local/include  -Wall -pedantic -fno-exceptions -fno-rtti -fPIC  -O2 -g 
-march=i
386 -mcpu=i686 -c glgui.cpp -o glgui.o
g++  -L/usr/local/lib -o rgl.so x11lib.o x11gui.o types.o math.o fps.o 
pixmap.o
gui.o api.o device.o devicemanager.o rglview.o scene.o glgui.o 
-L/usr/X11R6/lib
-L/usr/lib -lstdc++ -lX11 -lXext -lGL -lGLU -lpng
/usr/lib/gcc-lib/i386-redhat-linux/3.2.2/../../../crt1.o(.text+0x18): In 
functio
n `_start':
../sysdeps/i386/elf/start.S:77: undefined reference to `main'
x11lib.o(.text+0x84): In function `set_R_handler':
/tmp/R.INSTALL.8663/rgl/src/x11gui.h:33: undefined reference to 
`R_InputHandlers
'
x11lib.o(.text+0x92):/tmp/R.INSTALL.8663/rgl/src/x11gui.h:33: undefined 
referenc
e to `addInputHandler'
x11lib.o(.text+0xfb): In function `unset_R_handler':
/tmp/R.INSTALL.8663/rgl/src/x11lib.cpp:52: undefined reference to 
`R_InputHandle
rs'
x11lib.o(.text+0x103):/tmp/R.INSTALL.8663/rgl/src/x11lib.cpp:52: undefined 
refer
ence to `removeInputHandler'
collect2: ld returned 1 exit status
make: *** [rgl.so] Error 1
ERROR: compilation failed for package 'rgl'
** Removing '/usr/lib/R/library/rgl'
[my pc]



From ripley at stats.ox.ac.uk  Fri Jun 25 19:45:05 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 25 Jun 2004 18:45:05 +0100 (BST)
Subject: [R] circle / oval / semicircle ?
In-Reply-To: <ivo_welch-0otu3ASXevkEdaP1oMY1H1hjpb1ndpI@mailblocks.com>
Message-ID: <Pine.LNX.4.44.0406251830060.4054-100000@gannet.stats>

Circles *are* a graphics primitive in the sense that every R device driver
has to support them.  See PS_Circle and PDF_Circle in file
src/main/devPS.c Note that PDF does not have arcs/circle as a primitive,
unlike PS.

One gotcha is that you have to be able to draw circles clipped to a 
viewport.

OTOH, adding a graphic primitive (and we seem to need image for e.g. 
better drawing of the output of image and for icons etc) is a very serious 
undertaking.

The R interfaces are symbols() and grid.circle.


On Fri, 25 Jun 2004, ivo welch wrote:


> thanks, guys.  circles might be a good candidate for a graphics 
> primitive in the next R version, if only because---unlike the solutions 
> here---I believe that postscript/pdf have scaleable primitives---though 
> I am no postscript expert.  of course, if I knew what I was talking 
> about, I would just write the function and submit it.  the closest I 
> have found was in http://wtonline.vitalnews.com/Pages/Tip0163.html

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From sundar.dorai-raj at PDF.COM  Fri Jun 25 19:52:36 2004
From: sundar.dorai-raj at PDF.COM (Sundar Dorai-Raj)
Date: Fri, 25 Jun 2004 12:52:36 -0500
Subject: [R] alternate rank method
In-Reply-To: <Pine.LNX.4.44.0406251007400.19740-100000@jerboa.fhcrc.org>
References: <Pine.LNX.4.44.0406251007400.19740-100000@jerboa.fhcrc.org>
Message-ID: <40DC6664.3040208@pdf.com>



Douglas Grove wrote:

> Hi,
> 
> I'm wondering if anyone can point me to a function that will
> allow me to do a ranking that treats ties differently than
> rank() provides for?
> 
> I'd like a method that will assign to the elements of each 
> tie group the largest rank. 
> 
> An example:  
> 
> For the vector 'v', I'd like the method to return 'rv'
> 
>  v:  1 2 3 3 3 4 5 5 6 7
> rv:  1 2 5 5 5 6 8 8 9 10
> 
> 
> Thanks,
> Doug Grove
> 

How about

rv <- rowSums(outer(v, v, ">="))

Adapted from Prof. Ripley's reply in the following thread:

http://finzi.psych.upenn.edu/R/Rhelp02/archive/31993.html

HTH,

--sundar



From ripley at stats.ox.ac.uk  Fri Jun 25 19:56:25 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 25 Jun 2004 18:56:25 +0100 (BST)
Subject: [R] Memory.limit
In-Reply-To: <OF2C8B5D25.32DB22E3-ON85256EBE.00599D65-85256EBE.0059F846@hgsi.com>
Message-ID: <Pine.LNX.4.44.0406251846070.4054-100000@gannet.stats>

This is on Windows only.  The units for setting are in Mb (to avoid 
overflow problems above 2Gb).


On Fri, 25 Jun 2004 partha_bagchi at hgsi.com wrote:

> I am playing with memory limits and trying to understand how R allocates 
> and increases available memory.

This is not the setting for that.  This controls the internal malloc's 
limits.

> Here is what I have:
> 
> > memory.limit()
> [1] 536068096
> > memory.limit(size = memory.limit()*2)
> Error in memory.size(size) : cannot decrease memory limit
> > memory.limit(size = 1024^3)
> Error in memory.size(size) : cannot decrease memory limit
> > memory.limit(size = 1073741824)
> Error in memory.size(size) : cannot decrease memory limit
> >
> > version
>          _ 
> platform i386-pc-mingw32
> arch     i386 
> os       mingw32 
> system   i386, mingw32 
> status 
> major    1 
> minor    9.1 
> year     2004 
> month    06 
> day      21 
> language R 
> >

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dgrove at fhcrc.org  Fri Jun 25 20:04:20 2004
From: dgrove at fhcrc.org (Douglas Grove)
Date: Fri, 25 Jun 2004 11:04:20 -0700 (PDT)
Subject: [R] alternate rank method
In-Reply-To: <40DC6664.3040208@pdf.com>
Message-ID: <Pine.LNX.4.44.0406251103260.19871-100000@jerboa.fhcrc.org>

I should have specified an additional constraint:

I'm going to need to use this repeatedly on large
vectors (length 10^6), so something efficient is
needed.


On Fri, 25 Jun 2004, Sundar Dorai-Raj wrote:

> Douglas Grove wrote:
> 
> > Hi,
> > 
> > I'm wondering if anyone can point me to a function that will
> > allow me to do a ranking that treats ties differently than
> > rank() provides for?
> > 
> > I'd like a method that will assign to the elements of each 
> > tie group the largest rank. 
> > 
> > An example:  
> > 
> > For the vector 'v', I'd like the method to return 'rv'
> > 
> >  v:  1 2 3 3 3 4 5 5 6 7
> > rv:  1 2 5 5 5 6 8 8 9 10
> > 
> > 
> > Thanks,
> > Doug Grove
> > 
> 
> How about
> 
> rv <- rowSums(outer(v, v, ">="))
> 
> Adapted from Prof. Ripley's reply in the following thread:
> 
> http://finzi.psych.upenn.edu/R/Rhelp02/archive/31993.html
> 
> HTH,
> 
> --sundar
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From etr at ravelgrane.com  Fri Jun 25 20:21:10 2004
From: etr at ravelgrane.com (Erik T. Ray)
Date: Fri, 25 Jun 2004 14:21:10 -0400
Subject: [R] R 1.9.1 package installation problems
Message-ID: <6ED4BBC4-C6D4-11D8-9557-000393D47C5C@ravelgrane.com>

Hello,

I am writing as an administrator, not as an R user, so forgive me if I 
am not completely knowledgeable about R.

I have a user who is creating an R package for windows from a Linux 
environment using the crossbuild environment by Jun Yan and A.J. 
Rossini. The packages she generated worked fine until she tried to 
install in R 1.9.1 for Windows. Now when she installs with

   install.packages( "Zelig", CRAN="http://gking.harvard.edu" )

it results in two errors.

First, it claims there are some missing files. When I look in the zip 
file, the files are there, but they coexist with other files that have 
the same name differing only in case. So there is both 'help/zelig' and 
'help/Zelig', and this is causing R to think that one of them is 
missing.

Second, R says that many of the files have bad MD5 checksums. I 
generated some MD5 digests using the linux command 'openssl dgst -md5' 
and compared against the ones created by the R Crossbuild script. They 
are different. So I made a new MD5 file and stuck it in the package. 
This time, the installation resulted in just three files having bad 
checksums, and they happen to be the counterparts to the "missing" 
files. In other words, it claims that "html/zelig" is missing, and 
"html/Zelig" has a bad MD5 checksum.

So I have two problems. First, MD5 checksums are not being generated 
correctly by the cross builder. At worst, I could re-generate those, so 
it is not a big deal. More problematic, there seems to be a bug in the 
way R 1.9.1 for windows imports packages that contain files whose names 
differ only by case.

Does anyone else notice this problem, or is it a known issue? Is there 
a workaround?

Thanks.

--
Erik Ray                     Unix Systems Administrator
Harvard-MIT Data Center      Harvard University
phone: (617) 496-5097        mobile: (781) 710-1162



From dmurdoch at pair.com  Fri Jun 25 20:21:33 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Fri, 25 Jun 2004 14:21:33 -0400
Subject: [R] rgl installation problems
In-Reply-To: <BAY22-F15q0NTY9agv3000096d1@hotmail.com>
References: <BAY22-F15q0NTY9agv3000096d1@hotmail.com>
Message-ID: <ojqod0hrjf0n5p75e0do3e4o0k19rdbe57@4ax.com>

On Fri, 25 Jun 2004 17:36:30 +0000, "E GCP" <egcp at hotmail.com> wrote :

>Hi!
>
>I'm new to R, but have worked with Splus before. I installed several 
>packages in R (R-1.9.1) without problems, but when I try to install rgl 
>(rgl_0.64-13.tar.gz). I get the following, and the package does not install. 
>Any help would be greatly appreciated. I'm running R in redhat 9.

The missing reference R_InputHandlers is declared in the
$RHOME/src/include/R_ext/eventloop.h file, and I believe is compiled
into the library R_X11 (with some extension).  You don't seem to have
that in the list of libraries:

>g++  -L/usr/local/lib -o rgl.so x11lib.o x11gui.o types.o math.o fps.o 
>pixmap.o
>gui.o api.o device.o devicemanager.o rglview.o scene.o glgui.o 
>-L/usr/X11R6/lib
>-L/usr/lib -lstdc++ -lX11 -lXext -lGL -lGLU -lpng
>/usr/lib/gcc-lib/i386-redhat-linux/3.2.2/../../../crt1.o(.text+0x18): In 
>functio
>n `_start':
>../sysdeps/i386/elf/start.S:77: undefined reference to `main'
>x11lib.o(.text+0x84): In function `set_R_handler':
>/tmp/R.INSTALL.8663/rgl/src/x11gui.h:33: undefined reference to 
>`R_InputHandlers

I don't know what you'll need to do to fix this, since I'm using
Windows, so none of this stuff happens there, and I could be
completely wrong about it.  

Duncan Murdoch



From ggrothendieck at myway.com  Fri Jun 25 20:21:30 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Fri, 25 Jun 2004 18:21:30 +0000 (UTC)
Subject: [R] alternate rank method
References: <Pine.LNX.4.44.0406251007400.19740-100000@jerboa.fhcrc.org>
Message-ID: <loom.20040625T202007-323@post.gmane.org>


Here are a couple of solutions:

rx <- rank(x)
order(-order(rev(x)))[match(rx,rx)]

rx <- rank(x)
(2*rx-rank(x,tie="first"))[match(rx,rx)]


Douglas Grove <dgrove <at> fhcrc.org> writes:

: 
: Hi,
: 
: I'm wondering if anyone can point me to a function that will
: allow me to do a ranking that treats ties differently than
: rank() provides for?
: 
: I'd like a method that will assign to the elements of each 
: tie group the largest rank. 
: 
: An example:  
: 
: For the vector 'v', I'd like the method to return 'rv'
: 
:  v:  1 2 3 3 3 4 5 5 6 7
: rv:  1 2 5 5 5 6 8 8 9 10
: 
: 
: Thanks,
: Doug Grove



From ivo_welch-Rstat at mailblocks.com  Fri Jun 25 21:05:22 2004
From: ivo_welch-Rstat at mailblocks.com (ivo_welch-Rstat@mailblocks.com)
Date: Fri, 25 Jun 2004 12:05:22 -0700
Subject: [R] text, pos suggestion
Message-ID: <200406251905.i5PJ5NmF006735@hypatia.math.ethz.ch>


suggestion:  could the R team please add positions 5 through 8 for the 
arg parameter in text(), which would select the diagonals (northeast, 
southeast, southwest, northwest)?

sincerely,  /ivo welch

PS:  thanks for all the earlier circle help.  knowing some of the 
predispositions of the R developers, I probably should not suggest 
adding to the documentation
  > ?circle
   please see ?symbols and ?grid.circle



From sdhyok at email.unc.edu  Fri Jun 25 21:17:23 2004
From: sdhyok at email.unc.edu (Shin, Daehyok)
Date: Fri, 25 Jun 2004 15:17:23 -0400
Subject: [R] Module for Recursive Instrumental Variable Method
Message-ID: <OAEOKPIGCLDDHAEMCAKICEPBCOAA.sdhyok@email.unc.edu>

I am looking for R programs implementing RIV(Recursive Instrumental
Variable) method
to identify and estimate constant parameter TF(Transfer Function) models.
Help please, if you know it.

Daehyok Shin (Peter)



From ripley at stats.ox.ac.uk  Fri Jun 25 21:14:24 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 25 Jun 2004 20:14:24 +0100 (BST)
Subject: [R] rgl installation problems
In-Reply-To: <ojqod0hrjf0n5p75e0do3e4o0k19rdbe57@4ax.com>
Message-ID: <Pine.LNX.4.44.0406252009120.4172-100000@gannet.stats>

Here's what should happen on RH9 (with the latest libpng and gcc in 
/usr/local/lib)

g++ -shared -L/usr/local/lib -o rgl.so x11lib.o x11gui.o types.o math.o 
fps.o pixmap.o gui.o api.o device.o devicemanager.o rglview.o scene.o 
glgui.o -L/usr/X11R6/lib -L/usr/local/lib -Wl,-rpath,/usr/local/lib 
-lpng12 -lz -lm -lstdc++ -lX11 -lXext -lGL -lGLU -lpng12 -lz -lm

Note that he had no --shared but did have crt1.o, that is was trying to 
build a standalone executable and not a shared object.

Something is wrong with the R installation's rules to make shared 
libraries.

On Fri, 25 Jun 2004, Duncan Murdoch wrote:

> On Fri, 25 Jun 2004 17:36:30 +0000, "E GCP" <egcp at hotmail.com> wrote :
> 
> >Hi!
> >
> >I'm new to R, but have worked with Splus before. I installed several 
> >packages in R (R-1.9.1) without problems, but when I try to install rgl 
> >(rgl_0.64-13.tar.gz). I get the following, and the package does not install. 
> >Any help would be greatly appreciated. I'm running R in redhat 9.
> 
> The missing reference R_InputHandlers is declared in the
> $RHOME/src/include/R_ext/eventloop.h file, and I believe is compiled
> into the library R_X11 (with some extension).  You don't seem to have
> that in the list of libraries:
> 
> >g++  -L/usr/local/lib -o rgl.so x11lib.o x11gui.o types.o math.o fps.o 
> >pixmap.o
> >gui.o api.o device.o devicemanager.o rglview.o scene.o glgui.o 
> >-L/usr/X11R6/lib
> >-L/usr/lib -lstdc++ -lX11 -lXext -lGL -lGLU -lpng
> >/usr/lib/gcc-lib/i386-redhat-linux/3.2.2/../../../crt1.o(.text+0x18): In 
> >functio
> >n `_start':
> >../sysdeps/i386/elf/start.S:77: undefined reference to `main'
> >x11lib.o(.text+0x84): In function `set_R_handler':
> >/tmp/R.INSTALL.8663/rgl/src/x11gui.h:33: undefined reference to 
> >`R_InputHandlers
> 
> I don't know what you'll need to do to fix this, since I'm using
> Windows, so none of this stuff happens there, and I could be
> completely wrong about it.  


-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From canty at math.mcmaster.ca  Fri Jun 25 21:44:29 2004
From: canty at math.mcmaster.ca (Angelo Canty)
Date: Fri, 25 Jun 2004 15:44:29 -0400 (EDT)
Subject: [R] trouble using boot package
In-Reply-To: <018b01c45acb$ccec5dc0$36040a0a@Louisept>
Message-ID: <Pine.LNX.4.44.0406251535180.4014-100000@mathserv>

Please read the help file!

mean is not a valid statistic to boot.

mean.boot <- function(x, i) mean(x[i])

is valid.

You can easily return as many values from boot as you like.
For example

mean.boot1 <- function(x, i)
    tapply(x$StomWt[i], list(x$Age, x$Quarter)

Of course if you want to do this, you need to use a stratified
bootstrap using the strata argument to boot
strata=tapply(d2$StomWt, list(d2$Age, d2$Quarter)) should work.

Angelo Canty

On Fri, 25 Jun 2004, Louize Hill wrote:

> Hello,
> 
> I am trying to carry out a bootstrap analysis (using the boot package) on a
> table and cannot work out how to get the results I need!
> I have a table ("d2") with 4 columns: "ID_code", "Age", "Quarter" and
> "StomWt". Age (0-5) and Quarter (1-4) are my strata
> Therefore I wish to estimate the confidence intervals for the mean StomWt
> for each Age and Quarter.
> 
> If I do this manually for each age / quarter I get the following:
> 
> > boot (d2$StomWt[d2$Age=="0" & d2$Quarter=="1"], mean, R=999)
> 
> ORDINARY NONPARAMETRIC BOOTSTRAP
> 
> Call:
> boot(data = d2$StomWt[d2$Age == "0" & d2$Quarter == "1"], statistic = mean,
>     R = 999)
> 
> Bootstrap Statistics :
>     original  bias    std. error
> t1*    3.365       0           0
> 
> Firstly - shouldn't the bias and std error be something other than 0?
> Furthermore, I would like to know if there is a way (function?) I can use
> that would automate this?
> 
> Thank you for your help
> Louize Hill
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
------------------------------------------------------------------
|   Angelo J. Canty                Email: cantya at mcmaster.ca     |
|   Mathematics and Statistics     Phone: (905) 525-9140 x 27079 |
|   McMaster University            Fax  : (905) 522-0935         |
|   1280 Main St. W.                                             |
|   Hamilton ON L8S 4K1                                          |



From ripley at stats.ox.ac.uk  Fri Jun 25 21:50:15 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 25 Jun 2004 20:50:15 +0100 (BST)
Subject: [R] R 1.9.1 package installation problems
In-Reply-To: <6ED4BBC4-C6D4-11D8-9557-000393D47C5C@ravelgrane.com>
Message-ID: <Pine.LNX.4.44.0406252020200.4172-100000@gannet.stats>

On Fri, 25 Jun 2004, Erik T. Ray wrote:

> I am writing as an administrator, not as an R user, so forgive me if I 
> am not completely knowledgeable about R.

(We might expect you to know about file systems, surely?)

> I have a user who is creating an R package for windows from a Linux 
> environment using the crossbuild environment by Jun Yan and A.J. 
> Rossini. 

I can't comment on that environment (I can't even find it despite having
read an article in R-News about a Makefile by those authors).  However,
the R team provides (and documents) cross-building, so why not use that
instead?

> The packages she generated worked fine until she tried to 
> install in R 1.9.1 for Windows. Now when she installs with
> 
>    install.packages( "Zelig", CRAN="http://gking.harvard.edu" )
> 
> it results in two errors.
> 
> First, it claims there are some missing files. When I look in the zip 
> file, the files are there, but they coexist with other files that have 
> the same name differing only in case. So there is both 'help/zelig' and 
> 'help/Zelig', and this is causing R to think that one of them is 
> missing.

For your information, Windows filenames are case-insensitive, and zelig
and Zelig cannot exist in the same directory.  So the problem lies with
the package in using such name pairs.  The R documentation does warn about
this, and R's tools do check for it. That package should fail `R CMD
check' -- this should be reported to its author.

> Second, R says that many of the files have bad MD5 checksums. I 
> generated some MD5 digests using the linux command 'openssl dgst -md5' 
> and compared against the ones created by the R Crossbuild script. They 
> are different. So I made a new MD5 file and stuck it in the package. 
> This time, the installation resulted in just three files having bad 
> checksums, and they happen to be the counterparts to the "missing" 
> files. In other words, it claims that "html/zelig" is missing, and 
> "html/Zelig" has a bad MD5 checksum.

Not so for packages built with the R team's tools - I have just checked 
one.  It is quite likely that the transfer of files corrupted the files -- 
how was that done?  (We do not support direct cross-building of package 
zip files, mainly because of pitfalls at this stage. My guess is that the 
line endings got changed from LF to CRLF.)

You can just delete the MD5 file if you are sure the files are correct.

> So I have two problems. First, MD5 checksums are not being generated 
> correctly by the cross builder. 

You present no evidence for that claim, and I cannot reproduce it.

> At worst, I could re-generate those, so 
> it is not a big deal. More problematic, there seems to be a bug in the 
> way R 1.9.1 for windows imports packages that contain files whose names 
> differ only by case.

A bug in the package, compounded by the user not running R CMD check.

> Does anyone else notice this problem, or is it a known issue? Is there 
> a workaround?

R CMD check
Care in moving files between file systems.

Hope that helps ....

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Fri Jun 25 22:02:36 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 25 Jun 2004 22:02:36 +0200
Subject: [R] R 1.9.1 package installation problems
In-Reply-To: <6ED4BBC4-C6D4-11D8-9557-000393D47C5C@ravelgrane.com>
References: <6ED4BBC4-C6D4-11D8-9557-000393D47C5C@ravelgrane.com>
Message-ID: <x2k6xvl82r.fsf@biostat.ku.dk>

"Erik T. Ray" <etr at ravelgrane.com> writes:

> Hello,
> 
> I am writing as an administrator, not as an R user, so forgive me if I
> am not completely knowledgeable about R.
> 
> I have a user who is creating an R package for windows from a Linux
> environment using the crossbuild environment by Jun Yan and A.J.
> Rossini. The packages she generated worked fine until she tried to
> install in R 1.9.1 for Windows. Now when she installs with
> 
>    install.packages( "Zelig", CRAN="http://gking.harvard.edu" )
> 
> it results in two errors.
> 
> First, it claims there are some missing files. When I look in the zip
> file, the files are there, but they coexist with other files that have
> the same name differing only in case. So there is both 'help/zelig'
> and 'help/Zelig', and this is causing R to think that one of them is
> missing.

Yes, Windows will do that sort of thing to you. This stems from
the Zelig sources themselves, 

[pd at titmouse tmp]$ tar tvfz ~/Zelig_1.1-2.tar.gz | grep -i /zelig.Rd
-rw-r--r-- king/king      2905 2004-04-17 02:53:49 Zelig/man/Zelig.Rd
-rw-r--r-- king/king      2779 2004-06-18 23:13:21 Zelig/man/zelig.Rd

but pretty obviously, that's a bad idea.... Incidentally, "R CMD
check" finds multiple problems with the package, but filenames
differing only in case is not among them, so it looks like the checker
could need improvement in that area. 
 
> Second, R says that many of the files have bad MD5 checksums. I
> generated some MD5 digests using the linux command 'openssl dgst -md5'
> and compared against the ones created by the R Crossbuild script. They
> are different. So I made a new MD5 file and stuck it in the package.
> This time, the installation resulted in just three files having bad
> checksums, and they happen to be the counterparts to the "missing"
> files. In other words, it claims that "html/zelig" is missing, and
> "html/Zelig" has a bad MD5 checksum.

The OS can't tell them apart, so it's a small wonder...

> So I have two problems. First, MD5 checksums are not being generated
> correctly by the cross builder. At worst, I could re-generate those,
> so it is not a big deal. More problematic, there seems to be a bug in
> the way R 1.9.1 for windows imports packages that contain files whose
> names differ only by case.
> 
> Does anyone else notice this problem, or is it a known issue? Is there
> a workaround?

Rename the source files! I don't think there's any other solution. 

It might be worth digging into the MD5 issue a bit more. Should be
completely orthogonal to the file name issue:

[pd at titmouse tmp]$ md5sum test1.tex
0a158be8f97a4134ab648d90b009e850  test1.tex
[pd at titmouse tmp]$ cp test1.tex TEST1.TEX
[pd at titmouse tmp]$ md5sum TEST1.TEX
0a158be8f97a4134ab648d90b009e850  TEST1.TEX
[pd at titmouse tmp]$ openssl dgst -md5 test1.tex
MD5(test1.tex)= 0a158be8f97a4134ab648d90b009e850

However, there might be an issue with zip/unzip if it converts to CRLF
line endings or such.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Fri Jun 25 22:17:08 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Fri, 25 Jun 2004 21:17:08 +0100 (BST)
Subject: [R] R 1.9.1 package installation problems
In-Reply-To: <x2k6xvl82r.fsf@biostat.ku.dk>
Message-ID: <Pine.LNX.4.44.0406252115390.5755-100000@gannet.stats>

On 25 Jun 2004, Peter Dalgaard wrote:

> "Erik T. Ray" <etr at ravelgrane.com> writes:
> 
> > Hello,
> > 
> > I am writing as an administrator, not as an R user, so forgive me if I
> > am not completely knowledgeable about R.
> > 
> > I have a user who is creating an R package for windows from a Linux
> > environment using the crossbuild environment by Jun Yan and A.J.
> > Rossini. The packages she generated worked fine until she tried to
> > install in R 1.9.1 for Windows. Now when she installs with
> > 
> >    install.packages( "Zelig", CRAN="http://gking.harvard.edu" )
> > 
> > it results in two errors.
> > 
> > First, it claims there are some missing files. When I look in the zip
> > file, the files are there, but they coexist with other files that have
> > the same name differing only in case. So there is both 'help/zelig'
> > and 'help/Zelig', and this is causing R to think that one of them is
> > missing.
> 
> Yes, Windows will do that sort of thing to you. This stems from
> the Zelig sources themselves, 
> 
> [pd at titmouse tmp]$ tar tvfz ~/Zelig_1.1-2.tar.gz | grep -i /zelig.Rd
> -rw-r--r-- king/king      2905 2004-04-17 02:53:49 Zelig/man/Zelig.Rd
> -rw-r--r-- king/king      2779 2004-06-18 23:13:21 Zelig/man/zelig.Rd
> 
> but pretty obviously, that's a bad idea.... Incidentally, "R CMD
> check" finds multiple problems with the package, but filenames
> differing only in case is not among them, so it looks like the checker
> could need improvement in that area. 

Yes, although I believe it once did (and believed it still did when I 
replied).  It will be added shortly.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From dmurdoch at pair.com  Fri Jun 25 22:25:29 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Fri, 25 Jun 2004 16:25:29 -0400
Subject: [R] text, pos suggestion
In-Reply-To: <200406251905.i5PJ5NmF006735@hypatia.math.ethz.ch>
References: <200406251905.i5PJ5NmF006735@hypatia.math.ethz.ch>
Message-ID: <i0uod090ch4f4niv9fva5ob8jb1asit186@4ax.com>

On Fri, 25 Jun 2004 12:05:22 -0700, <ivo_welch-Rstat at mailblocks.com>
wrote :

>suggestion:  could the R team please add positions 5 through 8 for the 
>arg parameter in text(), which would select the diagonals (northeast, 
>southeast, southwest, northwest)?

You mean "pos", not arg, right?  Why not just use adj?  You could
define constants northeast = c(1,1), etc, and use 

text(..., adj=northeast)

and it would be a lot clearer than 

text(..., pos=5)

>PS:  thanks for all the earlier circle help.  knowing some of the 
>predispositions of the R developers, I probably should not suggest 
>adding to the documentation
>  > ?circle
>   please see ?symbols and ?grid.circle

I think it's reasonable to add "circle" as a concept entry, which
would mean help.search('circle') would find symbols.  It already finds
grid.circle.

Duncan Murdoch



From dgrove at fhcrc.org  Sat Jun 26 00:15:16 2004
From: dgrove at fhcrc.org (Douglas Grove)
Date: Fri, 25 Jun 2004 15:15:16 -0700 (PDT)
Subject: [R] ties in runif() output
Message-ID: <Pine.LNX.4.44.0406251456100.20625-100000@jerboa.fhcrc.org>

I get ties in output from runif() when I generate as few as 10^5
variates and get quite a lot when I generate 10^6.  Is this 
expected??  I haven't seen any duplication with rnorm(10^6), but
see varying amounts of duplication using rexp(), rbeta() and
rgamma().  I would have thought that there'd be enough precision
that one wouldn't get ties until generating samples larger than this..


> set.seed(222)
> sum(duplicated(runif(10^5)))
[1] 4

> sum(duplicated(runif(10^6)))
[1] 140


platform i686-pc-linux-gnu
arch     i686
os       linux-gnu
system   i686, linux-gnu
status   Patched
major    1
minor    9.0
year     2004
month    04
day      13
language R


Thanks,
Doug Grove



From cdbroeckling at noble.org  Sat Jun 26 00:38:22 2004
From: cdbroeckling at noble.org (Broeckling, Corey)
Date: Fri, 25 Jun 2004 17:38:22 -0500
Subject: [R] simple questions
Message-ID: <6B4878EA887E6C45AAA5AAC4C6017B0E0101163B@mail-1.noble.org>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040625/4157eea3/attachment.pl

From baron at psych.upenn.edu  Sat Jun 26 00:51:13 2004
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Fri, 25 Jun 2004 18:51:13 -0400
Subject: [R] simple questions
In-Reply-To: <6B4878EA887E6C45AAA5AAC4C6017B0E0101163B@mail-1.noble.org>
References: <6B4878EA887E6C45AAA5AAC4C6017B0E0101163B@mail-1.noble.org>
Message-ID: <20040625225113.GA28256@psych>

On 06/25/04 17:38, Broeckling, Corey wrote:
>Secondly, I am trying to convert the raw dataset into a normalized dataset,
>with each data point normalized to its column mean.  I have tried dividing
>the dataset by the list generated using the colMeans function, but the
>results of this procedure generate the correct values for some columns and
>not others.

For this question, look at scale.  That is:
?scale

I don't understand the other questions.

-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron
R search page:        http://finzi.psych.upenn.edu/



From york at zipcon.net  Sat Jun 26 06:40:51 2004
From: york at zipcon.net (Anne York)
Date: Fri, 25 Jun 2004 21:40:51 -0700 (PDT)
Subject: [R] Installing on Windows packages build on Unix
Message-ID: <Pine.LNX.4.60.0406252137560.31322@sasquatch>

Giovanni Petris wrote:
> Hello,
>
> I wanted to share with a colleague a few R functions that 
> I wrote. To this purpose, I created a small package on my machine 
> (Unix) and  emailed it to her. Now she is having troubles 
> installing the package  on her Windows machine. It seems 
> that on her side, install.packages
> looks for a "zip" file - while I have created a "tar.gz" file. I tried
> to build the package with the --use-zip options but that didn't work.
>
> Any suggestions?
>
> TIA,
> Giovanni
>
If the functions only consist of R-code, you can just send 
your colleague a text file with the function definitions.
Not elegant, but effective.



From patrick.drechsler at gmx.net  Sat Jun 26 07:36:01 2004
From: patrick.drechsler at gmx.net (Patrick Drechsler)
Date: Sat, 26 Jun 2004 07:36:01 +0200
Subject: [R] sweave: graphics not at the expected location in the pdf
References: <1088164268.1290.41.camel@christophl>
Message-ID: <m33c4izxry.fsf@pdrechsler.fqdn.th-h.de>

Hi Christoph,

Christoph Lehmann wrote on 25 Jun 2004 12:51:08 MET:

[...]
> Quite often it happens, that the graphics are not at the place
> where I expect them, but (often on a separate page) later on in
> the pdf. How can I fix this, means how can I define, that I
> want a graphic exactly here and now in the document?

Your email address implies that you can read german so here's
the German LaTeX FAQ pointer:

,----[ http://www.dante.de/faq/de-tex-faq/html/de-tex-faq.html ]
| 6.1.1 Wieso werden die meisten meiner Abbildungen an das Ende des
| Kapitels oder Dokuments verschoben?
| 
| 6.1.2 Wie kann ich die Default-Plazierungseinschrnkungen ndern?
| Ich mchte meine Abbildungen auch `h'ier plazieren, ohne jedesmal
| das optionale Argument angeben zu mssen.
| 
| 6.1.3 Wie mu ich die `float'-Parameter ndern, so da die
| Abbildungen und Tafeln gnstiger innerhalb eines Abschnitts
| verteilt werden?
| 
| 6.1.13 Wie kann ich Abbildungen oder Tafeln, die nicht innerhalb
| einer `figure'- oder `table'-Umgebung stehen, dennoch mit einer
| Bildunterschrift (Legende) versehen?
`----

Especially the last option might be interesting for you (using
the packet capt-of).

There's also an english FAQ at 

<URL:http://www.tex.ac.uk/cgi-bin/texfaq2html?label=floats>

showing some more approaches.

HTH,

Patrick
-- 
"You know the world is going crazy when the best rapper is a white
guy, the best golfer is a black guy, the Swiss hold the America's Cup,
France is accusing the US of arrogance, and Germany doesn't want to go
to war."          -- Charles Barkley



From ripley at stats.ox.ac.uk  Sat Jun 26 08:15:17 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sat, 26 Jun 2004 07:15:17 +0100 (BST)
Subject: [R] ties in runif() output
In-Reply-To: <Pine.LNX.4.44.0406251456100.20625-100000@jerboa.fhcrc.org>
Message-ID: <Pine.LNX.4.44.0406260707090.15220-100000@gannet.stats>

On Fri, 25 Jun 2004, Douglas Grove wrote:

> I get ties in output from runif() when I generate as few as 10^5
> variates and get quite a lot when I generate 10^6.  Is this 
> expected??  

It should have been.

> I haven't seen any duplication with rnorm(10^6), but
> see varying amounts of duplication using rexp(), rbeta() and
> rgamma().  I would have thought that there'd be enough precision
> that one wouldn't get ties until generating samples larger than this..

Did you do the calculations?  Please do so. There are about 2e9 possible
values of the standard generators.

> qbirthday(classes=2e9)
[1] 52655

Statisticians ought to know about the birthday problem!

(rnorm is different because the default generator uses two uniforms, 
deliberately to increase the precision.)

> > set.seed(222)
> > sum(duplicated(runif(10^5)))
> [1] 4

That's unusually high, BTW.

> > sum(duplicated(runif(10^6)))
> [1] 140

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From adaniels100 at yahoo.com  Sat Jun 26 08:16:32 2004
From: adaniels100 at yahoo.com (A Daniels)
Date: Fri, 25 Jun 2004 23:16:32 -0700 (PDT)
Subject: [R] Trouble Using Boot Package Also
Message-ID: <20040626061632.80303.qmail@web90106.mail.scd.yahoo.com>

Dear Help List,

I'm not an R-pro.  I just decided to try it because of
the many available advanced analysis packages. I
wonder if any of you have encountered this type of
error message in bootstrapping the median survival
time.  I couldn't seem to find a problem with the
command which I just patterned from the help files.

I hope you could take a look at this.  Much thanks for
any help.

A.Daniels


> data.f<-function(data){
+ s<-survfit(Surv(data$time,data$cens))
+ m<-min(s$time[s$surv<0.5])
+ }
> s1<-survfit(Surv(time,cens),data=abc)
> s2<-survfit(Surv(time-0.001*cens,1-cens),data=abc)
> med.cond<-censboot(data=abc,
statistic=data.f,R=499,F.surv=s1,G.surv=s2,sim="cond")
Error in sample(length(x), size, replace, prob) : 
        invalid first argument
> abc
   time cens group
1     9    1     1
2    13    1     1
3    13    0     1
4    18    1     1
5    23    1     1
6    28    0     1
7    31    1     1
8    34    1     1
9    45    0     1
10   48    1     1
11   48    0     1
12    5    1     2
13    5    1     2
14    8    1     2
15    8    1     2
16   12    1     2
17   16    0     2
18   23    1     2
19   27    1     2
20   30    1     2
21   33    1     2
22   43    1     2
23   45    1     2
>



From ccleland at optonline.net  Sat Jun 26 11:12:22 2004
From: ccleland at optonline.net (Chuck Cleland)
Date: Sat, 26 Jun 2004 05:12:22 -0400
Subject: [R] simple questions
In-Reply-To: <6B4878EA887E6C45AAA5AAC4C6017B0E0101163B@mail-1.noble.org>
References: <6B4878EA887E6C45AAA5AAC4C6017B0E0101163B@mail-1.noble.org>
Message-ID: <40DD3DF6.2060800@optonline.net>

   If I understand what you are after, here is an example with 
two response variables and two factors:

 > mydata <- data.frame(Y1 = rnorm(20), Y2 = rnorm(20),
   FACT1 = rep(c("A", "B"), 10), FACT2 = rep(c("X", "Y"),
   c(10,10)))

 > my.aov <- aov(cbind(Y1, Y2) ~ FACT1*FACT2, data = mydata)

 > summary(my.aov)

  Response Y1 :
             Df  Sum Sq Mean Sq F value Pr(>F)
FACT1        1  0.0382  0.0382  0.0292 0.8664
FACT2        1  0.1248  0.1248  0.0954 0.7613
FACT1:FACT2  1  0.0616  0.0616  0.0471 0.8309
Residuals   16 20.9203  1.3075

  Response Y2 :
             Df  Sum Sq Mean Sq F value Pr(>F)
FACT1        1  0.1634  0.1634  0.1660 0.6891
FACT2        1  0.5685  0.5685  0.5777 0.4583
FACT1:FACT2  1  0.0167  0.0167  0.0170 0.8980
Residuals   16 15.7444  0.9840

The same approach will work with more response variables and more 
factors.

Broeckling, Corey wrote:
> I am a new user or R, and am so far very impressed with its capabilities.
> However, I have no programming experience, and am having some issues in
> trying to tell the software what I want done.  There are basically two
> issues which I am currently grappling with.  The first, I have a data
> matrix, with two factors and dozens of response variables.  I am interested
> on conducting ANOVAs on each of the variables individually (in addition to
> doing multivariate analyses).  I have been trying to get the AOV function to
> recognized a list of variables, but always receive error messages.  I am
> sure there is a way to do this, but I haven't been able to figure it out.
> Suggestions as to what I am doing wrong?
> ...

-- 
Chuck Cleland, Ph.D.
NDRI, Inc.
71 West 23rd Street, 8th floor
New York, NY 10010
tel: (212) 845-4495 (Tu, Th)
tel: (732) 452-1424 (M, W, F)
fax: (917) 438-0894



From ligges at statistik.uni-dortmund.de  Sat Jun 26 13:08:32 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sat, 26 Jun 2004 13:08:32 +0200
Subject: [R] Installing on Windows packages build on Unix
In-Reply-To: <Pine.LNX.4.60.0406252137560.31322@sasquatch>
References: <Pine.LNX.4.60.0406252137560.31322@sasquatch>
Message-ID: <40DD5930.90503@statistik.uni-dortmund.de>

Anne York wrote:
> Giovanni Petris wrote:
> 
>> Hello,
>>
>> I wanted to share with a colleague a few R functions that I wrote. To 
>> this purpose, I created a small package on my machine (Unix) and  
>> emailed it to her. Now she is having troubles installing the package  
>> on her Windows machine. It seems that on her side, install.packages
>> looks for a "zip" file - while I have created a "tar.gz" file. I tried
>> to build the package with the --use-zip options but that didn't work.
>>
>> Any suggestions?
>>
>> TIA,
>> Giovanni
>>
> If the functions only consist of R-code, you can just send your 
> colleague a text file with the function definitions.
> Not elegant, but effective.

Well, without documentation and R CMD check, it's really not that 
elegant nor effective, in many cases.

It is possible to install source packages under Windows. Your colleague 
just has to learn how to do it and there is documentation that tells us 
how to do it (e.g. readme.packages).

Uwe Ligges



From ggrothendieck at myway.com  Sat Jun 26 17:40:14 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Sat, 26 Jun 2004 15:40:14 +0000 (UTC)
Subject: [R] alternate rank method
References: <Pine.LNX.4.44.0406251007400.19740-100000@jerboa.fhcrc.org>
	<loom.20040625T202007-323@post.gmane.org>
Message-ID: <loom.20040626T173403-785@post.gmane.org>


Thinking about this a bit more, the following is slightly simpler 
and should be slightly faster too:

	order(order(rev(x)),decreasing=T)[match(x,x)]


Gabor Grothendieck <ggrothendieck <at> myway.com> writes:
: 
: Here are a couple of solutions:
: 
: rx <- rank(x)
: order(-order(rev(x)))[match(rx,rx)]
: 
: rx <- rank(x)
: (2*rx-rank(x,tie="first"))[match(rx,rx)]
: 
: Douglas Grove <dgrove <at> fhcrc.org> writes:
: 
: : 
: : Hi,
: : 
: : I'm wondering if anyone can point me to a function that will
: : allow me to do a ranking that treats ties differently than
: : rank() provides for?
: : 
: : I'd like a method that will assign to the elements of each 
: : tie group the largest rank. 
: : 
: : An example:  
: : 
: : For the vector 'v', I'd like the method to return 'rv'
: : 
: :  v:  1 2 3 3 3 4 5 5 6 7
: : rv:  1 2 5 5 5 6 8 8 9 10
: : 
: : 
: : Thanks,
: : Doug Grove



From dgrove at fhcrc.org  Sun Jun 27 06:39:40 2004
From: dgrove at fhcrc.org (Douglas Grove)
Date: Sat, 26 Jun 2004 21:39:40 -0700 (PDT)
Subject: [R] ties in runif() output
In-Reply-To: <Pine.LNX.4.44.0406260707090.15220-100000@gannet.stats>
Message-ID: <Pine.LNX.4.44.0406262134050.24805-100000@jerboa.fhcrc.org>

On Sat, 26 Jun 2004, Prof Brian Ripley wrote:

> On Fri, 25 Jun 2004, Douglas Grove wrote:
> 
> > I get ties in output from runif() when I generate as few as 10^5
> > variates and get quite a lot when I generate 10^6.  Is this 
> > expected??  
> 
> It should have been.
> 
> > I haven't seen any duplication with rnorm(10^6), but
> > see varying amounts of duplication using rexp(), rbeta() and
> > rgamma().  I would have thought that there'd be enough precision
> > that one wouldn't get ties until generating samples larger than this..
> 
> Did you do the calculations?  Please do so. There are about 2e9 possible
> values of the standard generators.

I know little about the limitations of random number generation 
and didn't realize that only 2e9 values were obtainable.
I could have done the math myself had I known

Thanks very much for your help,
Doug


> > qbirthday(classes=2e9)
> [1] 52655
> 
> Statisticians ought to know about the birthday problem!
> 
> (rnorm is different because the default generator uses two uniforms, 
> deliberately to increase the precision.)
> 
> > > set.seed(222)
> > > sum(duplicated(runif(10^5)))
> > [1] 4
> 
> That's unusually high, BTW.
> 
> > > sum(duplicated(runif(10^6)))
> > [1] 140
> 
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From liulei at l.imap.itd.umich.edu  Sun Jun 27 09:18:43 2004
From: liulei at l.imap.itd.umich.edu (Lei Liu)
Date: Sun, 27 Jun 2004 03:18:43 -0400
Subject: [R] help in R calling C function
Message-ID: <5.1.0.14.1.20040627030746.014936b8@l.imap.itd.umich.edu>

Hi there,

I want to call a C function in R. I have some experience on it, but this 
time I need to call another C function in the "main" C function. As a 
simple example, I use the following C code:

#include <stdio.h>
#include <math.h>
#include <stdlib.h>
#include <time.h>


void main(double *alpha)
{
	double test();
	double beta;
	beta= *alpha *2 + test(*alpha);
}

double test(double *alpha)
{
	double value;
	value = *alpha *3;
	return(value);
}

I used "gcc -c main.c" and "Rcmd SHLIB main.o" and I got the main.dll file. 
In R I use the following code to call the C function "main":

dyn.load("c:/Program Files/R/rw1060/bin/main.dll")
a=4.2
b=.C('main', as.double(a))

But the R system crashed. I know I can only define the function type in C 
as "void" to be called by R. But what if I want to call another C function 
in the "main" C function? Thank you for your help!

Sincerely,

Lei Liu



From merser at image.dk  Sun Jun 27 10:16:40 2004
From: merser at image.dk (=?iso-8859-1?Q?S=F8ren_Merser?=)
Date: Sun, 27 Jun 2004 10:16:40 +0200
Subject: [R] subset drop unused levels
Message-ID: <000b01c45c1f$24e649b0$8b00a8c0@IBM>

hi there

tried to use subset with drop=TRUE, but all the 'old' levels are preserved,
i.e. when calling e.g. ftable a lot of zeros are displayed

>x<-subset(LREG,  (kir=='AA' | kir=='BB') & (type=='t1' | otype=='t2'),
drop=TRUE, select=c(event, kir, type))
> ftable(x)

i explicit have to call factor like
>ftable(event~factor(kir)+factor(type))

any hints?

regards soren



From p.dalgaard at biostat.ku.dk  Sun Jun 27 11:38:21 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 27 Jun 2004 11:38:21 +0200
Subject: [R] subset drop unused levels
In-Reply-To: <000b01c45c1f$24e649b0$8b00a8c0@IBM>
References: <000b01c45c1f$24e649b0$8b00a8c0@IBM>
Message-ID: <x23c4hbasy.fsf@biostat.ku.dk>

S??ren Merser <merser at image.dk> writes:

> hi there
> 
> tried to use subset with drop=TRUE, but all the 'old' levels are preserved,
> i.e. when calling e.g. ftable a lot of zeros are displayed
> 
> >x<-subset(LREG,  (kir=='AA' | kir=='BB') & (type=='t1' | otype=='t2'),
> drop=TRUE, select=c(event, kir, type))
> > ftable(x)
> 
> i explicit have to call factor like
> >ftable(event~factor(kir)+factor(type))
> 
> any hints?

Yes, the drop argument is misdocumented. It actually does the same as
the drop argument to [.data.frame. It was on this very list just
eleven days ago, see e.g. the below ref., including what to do if you
really want to get rid of unused levels

http://www.mail-archive.com/r-help at stat.math.ethz.ch/msg22459.html

(yet another interface to the archives...)

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ligges at statistik.uni-dortmund.de  Sun Jun 27 12:49:43 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 27 Jun 2004 12:49:43 +0200
Subject: [R] help in R calling C function
In-Reply-To: <5.1.0.14.1.20040627030746.014936b8@l.imap.itd.umich.edu>
References: <5.1.0.14.1.20040627030746.014936b8@l.imap.itd.umich.edu>
Message-ID: <40DEA647.2000107@statistik.uni-dortmund.de>

Lei Liu wrote:
> Hi there,
> 
> I want to call a C function in R. I have some experience on it, but this 
> time I need to call another C function in the "main" C function. As a 
> simple example, I use the following C code:
> 
> #include <stdio.h>
> #include <math.h>
> #include <stdlib.h>
> #include <time.h>
> 
> 
> void main(double *alpha)

1. don't call it "main".
2. alpha must not be a pointer.


> {
>     double test();
>     double beta;
>     beta= *alpha *2 + test(*alpha);
> }
>
> double test(double *alpha)
> {
>     double value;
>     value = *alpha *3;
>     return(value);
> }
> 
> I used "gcc -c main.c" and "Rcmd SHLIB main.o" and I got the main.dll 
> file. In R I use the following code to call the C function "main":

3. "Rcmd SHLIB main" is sufficient.


> dyn.load("c:/Program Files/R/rw1060/bin/main.dll")

4. Why do you compile in R's /bin directory? That's not necessary, it's 
rather confusing.

Uwe Ligges

> a=4.2
> b=.C('main', as.double(a))
>
> But the R system crashed. I know I can only define the function type in 
> C as "void" to be called by R. But what if I want to call another C 
> function in the "main" C function? Thank you for your help!
> 
> Sincerely,
> 
> Lei Liu
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From lists at svenhartenstein.de  Sun Jun 27 13:15:41 2004
From: lists at svenhartenstein.de (Sven Hartenstein)
Date: Sun, 27 Jun 2004 13:15:41 +0200
Subject: [R] Sphericity test
In-Reply-To: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAAlkN94tU7pE2294PPHpOIAsKAAAAQAAAABZZzvAwzh0y1t16IweS2YwEAAAAA@yahoo.com.br>
	(Eduardo
	Dutra de Armas's message of "Wed, 23 Jun 2004 16:30:24 -0300")
References: <!~!UENERkVCMDkAAQACAAAAAAAAAAAAAAAAABgAAAAAAAAAlkN94tU7pE2294PPHpOIAsKAAAAQAAAABZZzvAwzh0y1t16IweS2YwEAAAAA@yahoo.com.br>
Message-ID: <87r7s19rqa.fsf@svenhartenstein.de>

"Eduardo Dutra de Armas" <eduarmasrs at yahoo.com.br> writes:

> Does anybody know if there are functions or packages in R for
> sphericity tests like Bartlett's or Mauchly's?

help(bartlett.test)


Sven



From zhuw at mail.smu.edu  Sun Jun 27 10:27:52 2004
From: zhuw at mail.smu.edu (Zhu Wang)
Date: Sun, 27 Jun 2004 08:27:52 +0000
Subject: [R] Re: help in R calling C function (Lei Liu)
In-Reply-To: <200406271000.i5RA0dPZ006004@hypatia.math.ethz.ch>
References: <200406271000.i5RA0dPZ006004@hypatia.math.ethz.ch>
Message-ID: <1088324872.2539.6.camel@zwang.stat.smu.edu>


> Message: 4
> Date: Sun, 27 Jun 2004 03:18:43 -0400
> From: Lei Liu <liulei at l.imap.itd.umich.edu>
> Subject: [R] help in R calling C function
> To: r-help at stat.math.ethz.ch
> Message-ID: <5.1.0.14.1.20040627030746.014936b8 at l.imap.itd.umich.edu>
> Content-Type: text/plain; charset="us-ascii"; format=flowed
> 
> Hi there,
> 
> I want to call a C function in R. I have some experience on it, but this 
> time I need to call another C function in the "main" C function. As a 
> simple example, I use the following C code:
> 
> #include <stdio.h>
> #include <math.h>
> #include <stdlib.h>
> #include <time.h>
> 
> 
> void main(double *alpha)
> {
> 	double test();
> 	double beta;
> 	beta= *alpha *2 + test(*alpha);
> }
> 
> double test(double *alpha)
> {
> 	double value;
> 	value = *alpha *3;
> 	return(value);
> }
> 
> I used "gcc -c main.c" and "Rcmd SHLIB main.o" and I got the main.dll file. 
> In R I use the following code to call the C function "main":

I do not have too much experience using C, but do you need to compile
your test.c as well, something like

gcc -c main.c test.c

> dyn.load("c:/Program Files/R/rw1060/bin/main.dll")

I also suggest you to double check the path is correct.
> a=4.2
> b=.C('main', as.double(a))

Here you probably mean

a <- 4.2
b <- .C('main',as.double(a))

> But the R system crashed. I know I can only define the function type in C 
> as "void" to be called by R. But what if I want to call another C function 
> in the "main" C function? Thank you for your help!
> 
> Sincerely,
> 
> Lei Liu
> 
> 
> 

-- 
Zhu Wang

Statistical Science Department
Southern Methodist University
Dallas, TX 75275-0332



From ligges at statistik.uni-dortmund.de  Sun Jun 27 15:50:48 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Sun, 27 Jun 2004 15:50:48 +0200
Subject: [R] Re: help in R calling C function (Lei Liu)
In-Reply-To: <1088324872.2539.6.camel@zwang.stat.smu.edu>
References: <200406271000.i5RA0dPZ006004@hypatia.math.ethz.ch>
	<1088324872.2539.6.camel@zwang.stat.smu.edu>
Message-ID: <40DED0B8.7040100@statistik.uni-dortmund.de>

Zhu Wang wrote:
>>Message: 4
>>Date: Sun, 27 Jun 2004 03:18:43 -0400
>>From: Lei Liu <liulei at l.imap.itd.umich.edu>
>>Subject: [R] help in R calling C function
>>To: r-help at stat.math.ethz.ch
>>Message-ID: <5.1.0.14.1.20040627030746.014936b8 at l.imap.itd.umich.edu>
>>Content-Type: text/plain; charset="us-ascii"; format=flowed
>>
>>Hi there,
>>
>>I want to call a C function in R. I have some experience on it, but this 
>>time I need to call another C function in the "main" C function. As a 
>>simple example, I use the following C code:
>>
>>#include <stdio.h>
>>#include <math.h>
>>#include <stdlib.h>
>>#include <time.h>
>>
>>
>>void main(double *alpha)
>>{
>>	double test();
>>	double beta;
>>	beta= *alpha *2 + test(*alpha);
>>}
>>
>>double test(double *alpha)
>>{
>>	double value;
>>	value = *alpha *3;
>>	return(value);
>>}
>>
>>I used "gcc -c main.c" and "Rcmd SHLIB main.o" and I got the main.dll file. 
>>In R I use the following code to call the C function "main":
> 
> 
> I do not have too much experience using C, 

If you don't know, why do you post an answer without checking the 
manuals and trying out what you are suggesting? Your answer won't help 
and is at least partly incorrect.


 > but do you need to compile
> your test.c as well, something like
> 
> gcc -c main.c test.c

Well, if the code is within 2 files, you want to specify

R CMD SHLIB main.c test.c

(instead of 'R CMD SHLIB main')

and main.dll will be made (and linked correctly!). This way you do not 
need to bother with any flags for gcc.


>>dyn.load("c:/Program Files/R/rw1060/bin/main.dll")
> 
> 
> I also suggest you to double check the path is correct.
> 
>>a=4.2
>>b=.C('main', as.double(a))
> 
> 
> Here you probably mean

a=4.2 *is* correct syntax as described in the green book as well as in 
the manuals (just a matter of taste whether you really want to use it).

Please check out my former answer pointing out what really went wrong in 
the C code ...

Uwe Ligges


> a <- 4.2
> b <- .C('main',as.double(a))
> 
> 
>>But the R system crashed. I know I can only define the function type in C 
>>as "void" to be called by R. But what if I want to call another C function 
>>in the "main" C function? Thank you for your help!
>>
>>Sincerely,
>>
>>Lei Liu
>>
>>
>>
> 
>



From anne.piotet at urbanet.ch  Sun Jun 27 18:45:26 2004
From: anne.piotet at urbanet.ch (Anne)
Date: Sun, 27 Jun 2004 18:45:26 +0200
Subject: [R] back transformation from avas
Message-ID: <000e01c45c66$272f08f0$6c00a8c0@mtd4>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040627/f6eec9a7/attachment.pl

From liulei at l.imap.itd.umich.edu  Sun Jun 27 19:55:03 2004
From: liulei at l.imap.itd.umich.edu (Lei Liu)
Date: Sun, 27 Jun 2004 13:55:03 -0400
Subject: [R] Answer to help in R calling C function
Message-ID: <5.1.0.14.1.20040627134237.015b19f8@l.imap.itd.umich.edu>

Hi there,

Thank you all for your timely response. I find the right way to do it. 
There is a good example in Dr. Burns website:

http://www.burns-stat.com/pages/Spoetry/Spoetry.pdf

Page 174.

So this is the right way to do:

#include <stdio.h>
#include <math.h>
#include <stdlib.h>
#include <time.h>

double test(double alpha)
{
	double value;
	value = alpha *3;
	return(value);
}


void main2(double *alpha, double *beta)
{
	double test(double);
	*beta= *alpha *2 + test(*alpha);
}

After compiling it by MINGW,  you can use the following code in R:

dyn.load("c:/Program Files/R/rw1060/bin/main2.dll")
a=4.2
b=0
c=.C('main2', as.double(a), as.double(b))[[2]]

Hope this helps!

Sincerely,

Lei Liu



From f.harrell at vanderbilt.edu  Mon Jun 28 01:47:27 2004
From: f.harrell at vanderbilt.edu (Frank E Harrell Jr)
Date: Sun, 27 Jun 2004 18:47:27 -0500
Subject: [R] back transformation from avas
In-Reply-To: <000e01c45c66$272f08f0$6c00a8c0@mtd4>
References: <000e01c45c66$272f08f0$6c00a8c0@mtd4>
Message-ID: <40DF5C8F.6010107@vanderbilt.edu>

Anne wrote:
> Hello R helpers!
>  I'm using the avas function form package acepack (called from areg.boot package Hmisc)  to estimate automatically transformations of predictors (in this case monotonous) and response.
> 
> Well, it seems to work quite well, but I have 3 basic questions:
> - which set of basis functions is used in this procedure?
> - how do I back transform my estimate (y hat ) to the originasl scale?

Please read docs for areg.boot.  predict.areg.boot does this already, 
using reverse linear interpolation.

> - is there a closed analytical description of the model used? If so, how do I get it?

No

Frank

> 
> Thanks a lot
> Anne
> 
> 
> ----------------------------------------------------
> Anne Piotet
>  Email: anne.piotet at m-td.com
> ---------------------------------------------------
> M-TD Modelling and Technology Development
> PSE-C
> CH-1015 Lausanne
> Switzerland
> Tel: +41 21 693 83 98
> Fax: +41 21 646 41 33
> --------------------------------------------------
>  
> 	[[alternative HTML version deleted]]
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 


-- 
Frank E Harrell Jr   Professor and Chair           School of Medicine
                      Department of Biostatistics   Vanderbilt University



From baize at 3dculture.com  Sun Jun 27 22:38:47 2004
From: baize at 3dculture.com (Harolddd)
Date: Sun, 27 Jun 2004 13:38:47 -0700
Subject: [R] Installing on Windows packages build on Unix
In-Reply-To: <200406271001.i5RA0dPx006004@hypatia.math.ethz.ch>
Message-ID: <FIELLCIFBNNJMMGPHDDNGEIBIJAA.baize@3dculture.com>

> Giovanni Petris wrote:
> 
>> Hello,
>>
>> I wanted to share with a colleague a few R functions that I wrote. To 
>> this purpose, I created a small package on my machine (Unix) and  
>> emailed it to her. Now she is having troubles installing the package  
>> on her Windows machine. It seems that on her side, install.packages
>> looks for a "zip" file - while I have created a "tar.gz" file. I tried
>> to build the package with the --use-zip options but that didn't work.
>>
>> Any suggestions?
>>
>> TIA,
>> Giovanni
>>

Your Windows using friend can convert the "tar.gz" file to 
"zip" using the open-source utility "7-zip". Available at 
http://www.7-zip.org/  It doesn't conform to many Windows 
conventions, so reading the help files will be necessary. 
I found that it was necessary to extract a "tar.gz" file 
twice, once to extract from "gz" and again for "tar". 


Harold Baize



From ripley at stats.ox.ac.uk  Sun Jun 27 22:57:53 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Sun, 27 Jun 2004 21:57:53 +0100 (BST)
Subject: [R] Installing on Windows packages build on Unix
In-Reply-To: <FIELLCIFBNNJMMGPHDDNGEIBIJAA.baize@3dculture.com>
Message-ID: <Pine.LNX.4.44.0406272154240.29510-100000@gannet.stats>

On Sun, 27 Jun 2004, Harolddd wrote:

> > Giovanni Petris wrote:
> > 
> >> I wanted to share with a colleague a few R functions that I wrote. To 
> >> this purpose, I created a small package on my machine (Unix) and  
> >> emailed it to her. Now she is having troubles installing the package  
> >> on her Windows machine. It seems that on her side, install.packages
> >> looks for a "zip" file - while I have created a "tar.gz" file. I tried
> >> to build the package with the --use-zip options but that didn't work.
> 
> Your Windows using friend can convert the "tar.gz" file to 
> "zip" using the open-source utility "7-zip". Available at 
> http://www.7-zip.org/  It doesn't conform to many Windows 
> conventions, so reading the help files will be necessary. 
> I found that it was necessary to extract a "tar.gz" file 
> twice, once to extract from "gz" and again for "tar". 

That's pointless as the .zip file is a binary distribution and the .tar.gz 
file is a source distribution.  (There are plenty of Windows tools to 
unpack .tar.gz files, including the tar in our Rtools distribution.)

This Q is answered in the rw-FAQ, Q3.1.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Sun Jun 27 23:06:20 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 27 Jun 2004 23:06:20 +0200
Subject: [R] Installing on Windows packages build on Unix
In-Reply-To: <FIELLCIFBNNJMMGPHDDNGEIBIJAA.baize@3dculture.com>
References: <FIELLCIFBNNJMMGPHDDNGEIBIJAA.baize@3dculture.com>
Message-ID: <x24qow3e43.fsf@biostat.ku.dk>

"Harolddd" <baize at 3dculture.com> writes:

> > Giovanni Petris wrote:
> > 
> >> Hello,
> >>
> >> I wanted to share with a colleague a few R functions that I wrote. To 
> >> this purpose, I created a small package on my machine (Unix) and  
> >> emailed it to her. Now she is having troubles installing the package  
> >> on her Windows machine. It seems that on her side, install.packages
> >> looks for a "zip" file - while I have created a "tar.gz" file. I tried
> >> to build the package with the --use-zip options but that didn't work.
> >>
> >> Any suggestions?
> >>
> >> TIA,
> >> Giovanni
> >>
> 
> Your Windows using friend can convert the "tar.gz" file to 
> "zip" using the open-source utility "7-zip". Available at 
> http://www.7-zip.org/  It doesn't conform to many Windows 
> conventions, so reading the help files will be necessary. 
> I found that it was necessary to extract a "tar.gz" file 
> twice, once to extract from "gz" and again for "tar". 

Ack! No!

Windows packages are *binary* packages, Unix's are *source* packages.
Using a conversion tool to let one masquerade as the other is not
going to cause anything but trouble. We're having trouble enough with
people using WinZip and it's treatment of .tar.gz files as "just
another kind of ZIP".

One thing that *might* work for a package with no compiled code (it
used to work, but I'm not sure it still does) is to install the
package under Unix and then use zip to wrap up the installed files:

E.g.

  mkdir tmp
  R CMD INSTALL mypkg -l tmp
  cd tmp
  zip -lr mypkg mypkg

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From p.dalgaard at biostat.ku.dk  Mon Jun 28 00:06:53 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 28 Jun 2004 00:06:53 +0200
Subject: [R] Installing on Windows packages build on Unix
In-Reply-To: <x24qow3e43.fsf@biostat.ku.dk>
References: <FIELLCIFBNNJMMGPHDDNGEIBIJAA.baize@3dculture.com>
	<x24qow3e43.fsf@biostat.ku.dk>
Message-ID: <x2zn6o1wqq.fsf@biostat.ku.dk>

Peter Dalgaard <p.dalgaard at biostat.ku.dk> writes:

> "Harolddd" <baize at 3dculture.com> writes:
> > > Giovanni Petris wrote:
...
> > >> on her Windows machine. It seems that on her side, install.packages
> > >> looks for a "zip" file - while I have created a "tar.gz" file. I tried
> > >> to build the package with the --use-zip options but that didn't work.
....
> > Your Windows using friend can convert the "tar.gz" file to 
> > "zip" using the open-source utility "7-zip". Available at 
> > http://www.7-zip.org/  It doesn't conform to many Windows 
> > conventions, so reading the help files will be necessary. 
> > I found that it was necessary to extract a "tar.gz" file 
> > twice, once to extract from "gz" and again for "tar". 
> 
> Ack! No!
> 
> Windows packages are *binary* packages, Unix's are *source* packages.

Ooops, wait a minute. We can also build binaries on Unix with "R CMD
build --binary", and they *will* be .tar.gz files. I get a rather
peculiar error message from Perl if I try, but I do get the file and
it seems that the content is equivalent to that in the zip file I
described. (Of course you can always convert .tar.gz to .zip on Unix:
just unpack with tar and repack with zip -- just be careful where you
do it or you'll end up overwriting files!)

...
> used to work, but I'm not sure it still does) is to install the
> package under Unix and then use zip to wrap up the installed files:
> 
> E.g.
> 
>   mkdir tmp
>   R CMD INSTALL mypkg -l tmp
>   cd tmp
>   zip -lr mypkg mypkg

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From xiaoliu at jhmi.edu  Mon Jun 28 01:24:46 2004
From: xiaoliu at jhmi.edu (XIAO LIU)
Date: Sun, 27 Jun 2004 19:24:46 -0400
Subject: [R] direction of axes of plot
Message-ID: <11cdca121154.12115411cdca@jhmimail.jhmi.edu>

R users:

I want X-Y plotting with axes in reverse direction such as (0, -1, -2, -3, ....).  How can I do it?

Thanks in advance

Xiao



From MSchwartz at MedAnalytics.com  Mon Jun 28 02:08:41 2004
From: MSchwartz at MedAnalytics.com (Marc Schwartz)
Date: Sun, 27 Jun 2004 19:08:41 -0500
Subject: [R] direction of axes of plot
In-Reply-To: <11cdca121154.12115411cdca@jhmimail.jhmi.edu>
References: <11cdca121154.12115411cdca@jhmimail.jhmi.edu>
Message-ID: <1088381320.3750.114.camel@localhost.localdomain>

On Sun, 2004-06-27 at 18:24, XIAO LIU wrote:
> R users:
> 
> I want X-Y plotting with axes in reverse direction such as (0, -1, -2,
> -3, ....).  How can I do it?
> 
> Thanks in advance
> 
> Xiao

If I am understanding what you want, the following should give you an
example:

# Create x and y with negative values
x <- -1:-10
y <- -1:-10

# Show regular plot
plot(x, y)

# Now plot using -x and -y
# Do not plot the axes or annotation
plot(-x, -y, axes = FALSE, ann = FALSE)

# Now label both x and y axes with negative
# labels. Use pretty() to get standard tick mark locations
# and use rev() to create tick mark labels in reverse order
axis(1, at = pretty(-x), labels = rev(pretty(x)))
axis(2, at = pretty(-y), labels = rev(pretty(y)))

HTH,

Marc Schwartz



From ihaka at stat.auckland.ac.nz  Mon Jun 28 06:23:56 2004
From: ihaka at stat.auckland.ac.nz (Ross Ihaka)
Date: Mon, 28 Jun 2004 16:23:56 +1200
Subject: [R] direction of axes of plot
In-Reply-To: <11cdca121154.12115411cdca@jhmimail.jhmi.edu>
References: <11cdca121154.12115411cdca@jhmimail.jhmi.edu>
Message-ID: <40DF9D5C.8050102@stat.auckland.ac.nz>

XIAO LIU wrote:
> R users:
> 
> I want X-Y plotting with axes in reverse direction such as (0, -1, -2, -3, ....).  How can I do it?
> 
> Thanks in advance

Use the xlim and ylim arguments to plot.

	x = -(1:10)
	y = rnorm(10)

	# Standard plot
	plot(x, y)

	# Reversed x-axis
	plot(x, y, xlim=rev(range(x)))

-- 
Ross Ihaka                         Email:  ihaka at stat.auckland.ac.nz
Department of Statistics           Phone:  (64-9) 373-7599 x 85054
University of Auckland             Fax:    (64-9) 373-7018
Private Bag 92019, Auckland
New Zealand



From jhowison at syr.edu  Mon Jun 28 06:57:07 2004
From: jhowison at syr.edu (James Howison)
Date: Mon, 28 Jun 2004 00:57:07 -0400
Subject: [R] R via ssh login on OS X?
Message-ID: <9AFB7DC8-C8BF-11D8-BA01-00306579408C@syr.edu>

I have an ssh only login to a G5 on which I am hoping to run some 
analyses.  The situation is complicated by the fact that the computer's 
owner is away for the summer (and thus also only has shell login).

R is installed and there is a symlink to /usr/local/bin/R but when I 
try to launch it I get:

[jhowison at euro]$ R
kCGErrorRangeCheck : Window Server communications from outside of 
session allowed for root and console user only
INIT_Processeses(), could not establish the default connection to the 
WindowServer.Abort trap

I though, ah ha, I need to tell it not to use the GUI but to no avail:

[jhowison at euro]$ R --gui=none
kCGErrorRangeCheck : Window Server communications from outside of 
session allowed for root and console user only
INIT_Processeses(), could not establish the default connection to the 
WindowServer.Abort trap

I'm embarrassed to say that I'm writing to the list without having the 
latest version installed---because I can't install it at the moment.  I 
am using R 1.8.1.  I have tried to compile the latest from source but 
there is no F77 compiler. I thought I'd ask around before going down 
the "put local dependencies in the home folder" to compile this route 
(any hints on doing that would be great though) ...

Can other people get R command-line to work with logged in remotely via 
ssh?  Any hints?
Is this something that is fixed in more recent versions?

I think I can see one other route:  getting the computer's owner to 
install fink and their version remotely ... but I'm open to all "don't 
bother the professor when he's on holiday" options ...

Thanks in advance,

--James Howison



From h.andersson at nioo.knaw.nl  Mon Jun 28 09:43:18 2004
From: h.andersson at nioo.knaw.nl (Henrik Andersson)
Date: Mon, 28 Jun 2004 09:43:18 +0200
Subject: [R] direction of axes of plot
In-Reply-To: <40DF9D5C.8050102@stat.auckland.ac.nz>
References: <11cdca121154.12115411cdca@jhmimail.jhmi.edu>
	<40DF9D5C.8050102@stat.auckland.ac.nz>
Message-ID: <cboi88$o2u$1@sea.gmane.org>

Assume I want to put this into a function, how do I redefine the margins 
to fit better.

I did par(mar=c(4.1,4.1,5.1,2.1) inside the function, but then it is not 
possible anymore to change them, I just want to change the default for 
this plotting function, how ?

#----------------------------------------------------------
#Function to be improved
#-----------------------------------------------------
plot.revy <- function(x,y,...)
   {
     plot(x,y,ylim=rev(range(y)),axes = FALSE,...)
     axis(2)
     axis(3)
     box()
   }

Ross Ihaka wrote:

> XIAO LIU wrote:
> 
>> R users:
>>
>> I want X-Y plotting with axes in reverse direction such as (0, -1, -2, 
>> -3, ....).  How can I do it?
>>
>> Thanks in advance
> 
> 
> Use the xlim and ylim arguments to plot.
> 
>     x = -(1:10)
>     y = rnorm(10)
> 
>     # Standard plot
>     plot(x, y)
> 
>     # Reversed x-axis
>     plot(x, y, xlim=rev(range(x)))
>



From ihaka at stat.auckland.ac.nz  Mon Jun 28 11:41:28 2004
From: ihaka at stat.auckland.ac.nz (Ross Ihaka)
Date: Mon, 28 Jun 2004 21:41:28 +1200
Subject: [R] direction of axes of plot
In-Reply-To: <cboi88$o2u$1@sea.gmane.org>
References: <11cdca121154.12115411cdca@jhmimail.jhmi.edu>
	<40DF9D5C.8050102@stat.auckland.ac.nz> <cboi88$o2u$1@sea.gmane.org>
Message-ID: <40DFE7C8.3010702@stat.auckland.ac.nz>

Henrik Andersson wrote:
> Assume I want to put this into a function, how do I redefine the margins 
> to fit better.
> 
> I did par(mar=c(4.1,4.1,5.1,2.1) inside the function, but then it is not 
> possible anymore to change them, I just want to change the default for 
> this plotting function, how ?

You can save the old par values and restore them
after creating your plot.

     # reset par values, saving the old ones
     oldpar = par(mar=c(4.1,4.1,5.1,2.1))

     # create stunning customized plot here

     # restore the old par values
     par(oldpar)

When embedding this in a function like yours it is better
to use the on.exit() function to restore the old values.

     f = function(...) {
         oldpar = par(mar=c(4.1,4.1,5.1,2.1))
         on.exit(par(oldpar))

         # create stunning customized plot here
     }

Error exits from the function will then also restore the old
par values.

-- 
Ross Ihaka                         Email:  ihaka at stat.auckland.ac.nz
Department of Statistics           Phone:  (64-9) 373-7599 x 85054
University of Auckland             Fax:    (64-9) 373-7018
Private Bag 92019, Auckland
New Zealand



From Arne.Muller at aventis.com  Mon Jun 28 11:45:49 2004
From: Arne.Muller at aventis.com (Arne.Muller@aventis.com)
Date: Mon, 28 Jun 2004 11:45:49 +0200
Subject: [R] unbalanced design for anova with low number of replicates
Message-ID: <C80ECAFA2ACC1B45BE45D133ED660ADE010BF225@crbsmxsusr04.pharma.aventis.com>

Hello,

I'm wondering what's the best way to analyse an unbalanced design with a low number of replicates. I'm not a statistician, and I'm looking for some direction for this problem.

I've a 2 factor design:

Factor batch with 3 levels, and factor dose within each batch with 5 levels. Dose level 1 in batch one is replicated 4 times, level 3 is replicated only 2 times. all other levels are replicated 3 times, except for batch level 3, for which dose 4 is missing. 

I've realised that the other of the factors is critical for the outcome of the anova (using lm and anova).

I guess the impact wouldn't be strong if there was a reasonably large numbe rof replicates within each cell (even though not balanced). However, since I've only 0 to 4 replicates I'm worried that the standard anova may not be the way to go.

Are there special packages for unbalanced designs like this?

	kind regards,

	Arne

--
Arne Muller, Ph.D.
Toxicogenomics, Aventis Pharma
arne dot muller domain=aventis com



From Torsten.Hothorn at rzmail.uni-erlangen.de  Mon Jun 28 10:59:26 2004
From: Torsten.Hothorn at rzmail.uni-erlangen.de (Torsten Hothorn)
Date: Mon, 28 Jun 2004 10:59:26 +0200 (CEST)
Subject: [R] alternate rank method
In-Reply-To: <Pine.LNX.4.44.0406251103260.19871-100000@jerboa.fhcrc.org>
References: <Pine.LNX.4.44.0406251103260.19871-100000@jerboa.fhcrc.org>
Message-ID: <Pine.LNX.4.51.0406281058250.21721@artemis.imbe.med.uni-erlangen.de>


On Fri, 25 Jun 2004, Douglas Grove wrote:

> I should have specified an additional constraint:
>
> I'm going to need to use this repeatedly on large
> vectors (length 10^6), so something efficient is
> needed.
>

give function `irank' in package `exactRankTests' a try.

Best,

Torsten


>
> On Fri, 25 Jun 2004, Sundar Dorai-Raj wrote:
>
> > Douglas Grove wrote:
> >
> > > Hi,
> > >
> > > I'm wondering if anyone can point me to a function that will
> > > allow me to do a ranking that treats ties differently than
> > > rank() provides for?
> > >
> > > I'd like a method that will assign to the elements of each
> > > tie group the largest rank.
> > >
> > > An example:
> > >
> > > For the vector 'v', I'd like the method to return 'rv'
> > >
> > >  v:  1 2 3 3 3 4 5 5 6 7
> > > rv:  1 2 5 5 5 6 8 8 9 10
> > >
> > >
> > > Thanks,
> > > Doug Grove
> > >
> >
> > How about
> >
> > rv <- rowSums(outer(v, v, ">="))
> >
> > Adapted from Prof. Ripley's reply in the following thread:
> >
> > http://finzi.psych.upenn.edu/R/Rhelp02/archive/31993.html
> >
> > HTH,
> >
> > --sundar
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>
>



From temiz at deprem.gov.tr  Mon Jun 28 13:16:47 2004
From: temiz at deprem.gov.tr (temiz)
Date: Mon, 28 Jun 2004 14:16:47 +0300
Subject: [R] creating slope angle surface
Message-ID: <40DFFE1F.2060803@deprem.gov.tr>

hello

is there any way creating slope angle surface from
interpolated topography surface ?

kind regards

Ahmet Temiz
TURKEY


______________________________________
Inflex - installed on mailserver for domain @deprem.gov.tr
Queries to: postmaster at deprem.gov.tr

______________________________________
The views and opinions expressed in this e-mail message are ...{{dropped}}



From merser at tiscali.dk  Mon Jun 28 13:55:50 2004
From: merser at tiscali.dk (merser@tiscali.dk)
Date: Mon, 28 Jun 2004 13:55:50 +0200
Subject: [R] subset drop unused levels
Message-ID: <40AD09090000475B@cpfe6.be.tisc.dk>

thank you
sorry, but i missed that thread
your solution works (of cause)

hopefully the code and not the documentation will be corrected as the drop
argument comes very convenient or maybe as a new option:  drop.unused.levels=T
 
regards soren

btw 
how do i interpret an assigment to a square bracket?
d2[] <- lapply(d2, function(x) if (is.factor(x)) factor(x) else x) 


----- Original Message ----- 
From: "Peter Dalgaard" <p.dalgaard at biostat.ku.dk>
To: "S??ren Merser" <merser at image.dk>
Cc: "R - help" <r-help at stat.math.ethz.ch>
Sent: Sunday, June 27, 2004 11:38 AM
Subject: Re: [R] subset drop unused levels


> S??ren Merser <merser at image.dk> writes:
> 
> > hi there
> > 
> > tried to use subset with drop=TRUE, but all the 'old' levels are preserved,
> > i.e. when calling e.g. ftable a lot of zeros are displayed
> > 
> > >x<-subset(LREG,  (kir=='AA' | kir=='BB') & (type=='t1' | otype=='t2'),
> > drop=TRUE, select=c(event, kir, type))
> > > ftable(x)
> > 
> > i explicit have to call factor like
> > >ftable(event~factor(kir)+factor(type))
> > 
> > any hints?
> 
> Yes, the drop argument is misdocumented. It actually does the same as
> the drop argument to [.data.frame. It was on this very list just
> eleven days ago, see e.g. the below ref., including what to do if you
> really want to get rid of unused levels
> 
> http://www.mail-archive.com/r-help at stat.math.ethz.ch/msg22459.html
> 
> (yet another interface to the archives...)
> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From B.Rowlingson at lancaster.ac.uk  Mon Jun 28 14:01:20 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Mon, 28 Jun 2004 13:01:20 +0100
Subject: [R] creating slope angle surface
In-Reply-To: <40DFFE1F.2060803@deprem.gov.tr>
References: <40DFFE1F.2060803@deprem.gov.tr>
Message-ID: <40E00890.5020905@lancaster.ac.uk>

temiz wrote:
> hello
> 
> is there any way creating slope angle surface from
> interpolated topography surface ?

  The GRASS GIS can do calculations of slope, and aspect, and so on from 
spot heights. Suggest you install GRASS and then maybe do data transfer 
from R using the GRASS package from CRAN.

Baz

Grass: http://grass.itc.it/index.html



From STEL at glostruphosp.kbhamt.dk  Mon Jun 28 14:13:10 2004
From: STEL at glostruphosp.kbhamt.dk (Ladelund, Steen)
Date: Mon, 28 Jun 2004 14:13:10 +0200
Subject: [R] Sweave: R code in self defined TeX-commands
Message-ID: <F9E47473E3BCD1118C0500204808C390061A0867@GLO_003>


I do something like

<<echo=f>>=
needed <- function(var){
  a <- mean(var)
  b <- sd(var)
  c <- paste("The mean is ",a," and the var is ",b,collapse="")
  return(c)
}
@ 

\Sexpr{needed(something)}

Wich is to do the macro in R instead of LaTeX2e

Steen Ladelund 

-----Oprindelig meddelelse-----
Fra: Martin Posch [mailto:Martin.Posch at univie.ac.at] 
Sendt: 25. juni 2004 11:55
Til: r-help at stat.math.ethz.ch
Emne: [R] Sweave: R code in self defined TeX-commands


Hi,

I need to produce a standard report for several variables in Sweave and thus
would need the possibility to define a TeX-command which includes 
R-code like

\newcommand{\meansd}[1]{The mean is \Sexpr{mean(#1)} and the standard 
deviation is  \Sexpr{sd(#1)} .
                      }
and then just write

\meansd{age}

in the latex code to get the whole sentence.


The above does not work, since Sweave ignores the \newcommand and does not 
expand the \statistics

Is there an alternative way to achieve this?

Thanks,
Martin Posch



Department of Medical Statistics
Medical University of Vienna
Schwarzspanierstr. 17, A-1090 Vienna
Tel.:  +43-1-4277-63205

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From krcabrer at perseus.unalmed.edu.co  Mon Jun 28 14:43:12 2004
From: krcabrer at perseus.unalmed.edu.co (Kenneth Cabrera)
Date: Mon, 28 Jun 2004 07:43:12 -0500
Subject: [R] Problem with Rcmr in R 1.9.0patched
Message-ID: <opsaa1mavkfaouaq@200.24.8.4>

Dear R users:

I install the new R version (1.9.1 patched) in a W2K platform
in the E:\rw1091patch path.
Then I reinstall ALL the packages from CRAN, using the
"Packages > Install package(s) from CRAN" menu option.
But when I call the library

library(Rcmdr)

It doen't do any thing.

What am I missing?

Thank you for your help.

PS: Also when I look for the help files using
help.start(), it answers me:
"Could not open file"
file://localhost/E:/rw1091/library/Rcmdr/html/00Index.html

-- 
Kenneth Cabrera
Universidad Nacional de Colombia
Tel Of 430 9351
Cel 315 504 9339
Medell??n



From andy_liaw at merck.com  Mon Jun 28 14:40:00 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Mon, 28 Jun 2004 08:40:00 -0400
Subject: [R] unbalanced design for anova with low number of
 replicates
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7F7E@usrymx25.merck.com>

Arne,

For this sort of things, my suggestion is that you try to seek assistance
from local statistician(s), and I'd be very surprised if there's none for
you.

Data arising from unbalanced designs need no special tools for model
fitting.  The garden variety linear models work just fine.  The problem is
what do you do with the model(s):  The inference is where the problem is.
Unbalanced design introduce (partial) confounding among factors, which makes
assessment of effects tricky.  It really depends on what your goal is.  As
long as you can formulate that in terms of parameters in the `fullest' model
you are willing to consider, then it just comes down to testing specific
(general) linear hypotheses, which can be done `by hand'.  (I believe Greg
also has a glh() or something like that in the gregmisc package.  I'd guess
Prof. Fox's `car' package also has something similar.)

[BTW, if you are going to treat `batch' as a random effect, you might want
to look into mixed effect models; i.e., lme() in either lme4 or nlme.]

Best,
Andy

> From: Arne.Muller at aventis.com
> 
> Hello,
> 
> I'm wondering what's the best way to analyse an unbalanced 
> design with a low number of replicates. I'm not a 
> statistician, and I'm looking for some direction for this problem.
> 
> I've a 2 factor design:
> 
> Factor batch with 3 levels, and factor dose within each batch 
> with 5 levels. Dose level 1 in batch one is replicated 4 
> times, level 3 is replicated only 2 times. all other levels 
> are replicated 3 times, except for batch level 3, for which 
> dose 4 is missing. 
> 
> I've realised that the other of the factors is critical for 
> the outcome of the anova (using lm and anova).
> 
> I guess the impact wouldn't be strong if there was a 
> reasonably large numbe rof replicates within each cell (even 
> though not balanced). However, since I've only 0 to 4 
> replicates I'm worried that the standard anova may not be the 
> way to go.
> 
> Are there special packages for unbalanced designs like this?
> 
> 	kind regards,
> 
> 	Arne
> 
> --
> Arne Muller, Ph.D.
> Toxicogenomics, Aventis Pharma
> arne dot muller domain=aventis com
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 
>



From spencer.graves at pdf.com  Mon Jun 28 15:11:50 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 28 Jun 2004 06:11:50 -0700
Subject: [R] subset drop unused levels
In-Reply-To: <40AD09090000475B@cpfe6.be.tisc.dk>
References: <40AD09090000475B@cpfe6.be.tisc.dk>
Message-ID: <40E01916.6030409@pdf.com>

      "an assignment to a square bracket" implies that the object 
already exists and gives an error if it does not.  Consider the following: 

 > a[] <- 1:3
Error: Object "a" not found
 > a <- 1:2
 > a[] <- 1:3
Warning message:
number of items to replace is not a multiple of replacement length
 >

      hope this helps.  spencer graves

merser at tiscali.dk wrote:

>thank you
>sorry, but i missed that thread
>your solution works (of cause)
>
>hopefully the code and not the documentation will be corrected as the drop
>argument comes very convenient or maybe as a new option:  drop.unused.levels=T
> 
>regards soren
>
>btw 
>how do i interpret an assigment to a square bracket?
>d2[] <- lapply(d2, function(x) if (is.factor(x)) factor(x) else x) 
>
>
>----- Original Message ----- 
>From: "Peter Dalgaard" <p.dalgaard at biostat.ku.dk>
>To: "S??ren Merser" <merser at image.dk>
>Cc: "R - help" <r-help at stat.math.ethz.ch>
>Sent: Sunday, June 27, 2004 11:38 AM
>Subject: Re: [R] subset drop unused levels
>
>
>  
>
>>S??ren Merser <merser at image.dk> writes:
>>
>>    
>>
>>>hi there
>>>
>>>tried to use subset with drop=TRUE, but all the 'old' levels are preserved,
>>>i.e. when calling e.g. ftable a lot of zeros are displayed
>>>
>>>      
>>>
>>>>x<-subset(LREG,  (kir=='AA' | kir=='BB') & (type=='t1' | otype=='t2'),
>>>>        
>>>>
>>>drop=TRUE, select=c(event, kir, type))
>>>      
>>>
>>>>ftable(x)
>>>>        
>>>>
>>>i explicit have to call factor like
>>>      
>>>
>>>>ftable(event~factor(kir)+factor(type))
>>>>        
>>>>
>>>any hints?
>>>      
>>>
>>Yes, the drop argument is misdocumented. It actually does the same as
>>the drop argument to [.data.frame. It was on this very list just
>>eleven days ago, see e.g. the below ref., including what to do if you
>>really want to get rid of unused levels
>>
>>http://www.mail-archive.com/r-help at stat.math.ethz.ch/msg22459.html
>>
>>(yet another interface to the archives...)
>>
>>-- 
>>   O__  ---- Peter Dalgaard             Blegdamsvej 3  
>>  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>> (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
>>~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907
>>
>>______________________________________________
>>R-help at stat.math.ethz.ch mailing list
>>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>>
>>    
>>
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From ripley at stats.ox.ac.uk  Mon Jun 28 15:20:40 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 28 Jun 2004 14:20:40 +0100 (BST)
Subject: [R] subset drop unused levels
In-Reply-To: <40E01916.6030409@pdf.com>
Message-ID: <Pine.LNX.4.44.0406281415110.17148-100000@gannet.stats>

That's not the purpose, though.  MASS4 p.29 says

  This implies all possible values for
  the index.  It is really only useful on the receiving side, where it
  replaces the contents of the vector but keeps other aspects (the
  class, the length, the names, \dots).

See also R-lang, which says

@strong{Empty}.  The expression @code{x[]} returns @code{x}, but drops
``irrelevant'' attributes from the result.  Only @code{names} and in
multi-dimensional arrays @code{dim} and @code{dimnames} attributes are
retained.

but does not (yet) cover what it calls `Subset assignment'.


On Mon, 28 Jun 2004, Spencer Graves wrote:

>       "an assignment to a square bracket" implies that the object 
> already exists and gives an error if it does not.  Consider the following: 
> 
>  > a[] <- 1:3
> Error: Object "a" not found
>  > a <- 1:2
>  > a[] <- 1:3
> Warning message:
> number of items to replace is not a multiple of replacement length
>  >
> 
>       hope this helps.  spencer graves
> 
> merser at tiscali.dk wrote:
> 
> >thank you
> >sorry, but i missed that thread
> >your solution works (of cause)
> >
> >hopefully the code and not the documentation will be corrected as the drop
> >argument comes very convenient or maybe as a new option:  drop.unused.levels=T
> > 
> >regards soren
> >
> >btw 
> >how do i interpret an assigment to a square bracket?
> >d2[] <- lapply(d2, function(x) if (is.factor(x)) factor(x) else x) 
> >
> >
> >----- Original Message ----- 
> >From: "Peter Dalgaard" <p.dalgaard at biostat.ku.dk>
> >To: "S??ren Merser" <merser at image.dk>
> >Cc: "R - help" <r-help at stat.math.ethz.ch>
> >Sent: Sunday, June 27, 2004 11:38 AM
> >Subject: Re: [R] subset drop unused levels
> >
> >
> >  
> >
> >>S??ren Merser <merser at image.dk> writes:
> >>
> >>    
> >>
> >>>hi there
> >>>
> >>>tried to use subset with drop=TRUE, but all the 'old' levels are preserved,
> >>>i.e. when calling e.g. ftable a lot of zeros are displayed
> >>>
> >>>      
> >>>
> >>>>x<-subset(LREG,  (kir=='AA' | kir=='BB') & (type=='t1' | otype=='t2'),
> >>>>        
> >>>>
> >>>drop=TRUE, select=c(event, kir, type))
> >>>      
> >>>
> >>>>ftable(x)
> >>>>        
> >>>>
> >>>i explicit have to call factor like
> >>>      
> >>>
> >>>>ftable(event~factor(kir)+factor(type))
> >>>>        
> >>>>
> >>>any hints?
> >>>      
> >>>
> >>Yes, the drop argument is misdocumented. It actually does the same as
> >>the drop argument to [.data.frame. It was on this very list just
> >>eleven days ago, see e.g. the below ref., including what to do if you
> >>really want to get rid of unused levels
> >>
> >>http://www.mail-archive.com/r-help at stat.math.ethz.ch/msg22459.html
> >>
> >>(yet another interface to the archives...)
> >>
> >>-- 
> >>   O__  ---- Peter Dalgaard             Blegdamsvej 3  
> >>  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
> >> (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
> >>~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907
> >>
> >>______________________________________________
> >>R-help at stat.math.ethz.ch mailing list
> >>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >>
> >>    
> >>
> >
> >______________________________________________
> >R-help at stat.math.ethz.ch mailing list
> >https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> >PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >  
> >
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From spencer.graves at pdf.com  Mon Jun 28 15:22:12 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 28 Jun 2004 06:22:12 -0700
Subject: [R] direction of axes of plot
In-Reply-To: <1088381320.3750.114.camel@localhost.localdomain>
References: <11cdca121154.12115411cdca@jhmimail.jhmi.edu>
	<1088381320.3750.114.camel@localhost.localdomain>
Message-ID: <40E01B84.5010107@pdf.com>

      If you want code that will work in both S-Plus and R, please use 
this solution;  Ross Ihaka's solution using xlim=rev(range(x)) uses 
fewer commands and may be more elegant but generates errors in S-Plus 
(6.2 and, I suspect, earlier releases as well, though I did not test 
them just now).  Best Wishes, Spencer Graves

Marc Schwartz wrote:

>On Sun, 2004-06-27 at 18:24, XIAO LIU wrote:
>  
>
>>R users:
>>
>>I want X-Y plotting with axes in reverse direction such as (0, -1, -2,
>>-3, ....).  How can I do it?
>>
>>Thanks in advance
>>
>>Xiao
>>    
>>
>
>If I am understanding what you want, the following should give you an
>example:
>
># Create x and y with negative values
>x <- -1:-10
>y <- -1:-10
>
># Show regular plot
>plot(x, y)
>
># Now plot using -x and -y
># Do not plot the axes or annotation
>plot(-x, -y, axes = FALSE, ann = FALSE)
>
># Now label both x and y axes with negative
># labels. Use pretty() to get standard tick mark locations
># and use rev() to create tick mark labels in reverse order
>axis(1, at = pretty(-x), labels = rev(pretty(x)))
>axis(2, at = pretty(-y), labels = rev(pretty(y)))
>
>HTH,
>
>Marc Schwartz
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From ligges at statistik.uni-dortmund.de  Mon Jun 28 15:42:05 2004
From: ligges at statistik.uni-dortmund.de (Uwe Ligges)
Date: Mon, 28 Jun 2004 15:42:05 +0200
Subject: [R] Problem with Rcmr in R 1.9.0patched
In-Reply-To: <opsaa1mavkfaouaq@200.24.8.4>
References: <opsaa1mavkfaouaq@200.24.8.4>
Message-ID: <40E0202D.6030603@statistik.uni-dortmund.de>

Kenneth Cabrera wrote:

> Dear R users:
> 
> I install the new R version (1.9.1 patched) in a W2K platform
> in the E:\rw1091patch path.
> Then I reinstall ALL the packages from CRAN, using the
> "Packages > Install package(s) from CRAN" menu option.
> But when I call the library
> 
> library(Rcmdr)
> 
> It doen't do any thing.
> 
> What am I missing?

Hmm. The binary package for Windows is corrupted. Something went wrong 
during the build of the binary package. A working one will appear on 
CRAN master tomorrow.

Please reinstall in a few days when the file has been propagated to your 
mirror.

Uwe Ligges


> Thank you for your help.
> 
> PS: Also when I look for the help files using
> help.start(), it answers me:
> "Could not open file"
> file://localhost/E:/rw1091/library/Rcmdr/html/00Index.html
>



From spencer.graves at pdf.com  Mon Jun 28 15:59:35 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Mon, 28 Jun 2004 06:59:35 -0700
Subject: [R] creating slope angle surface
In-Reply-To: <40E00890.5020905@lancaster.ac.uk>
References: <40DFFE1F.2060803@deprem.gov.tr> <40E00890.5020905@lancaster.ac.uk>
Message-ID: <40E02447.5070905@pdf.com>

      Another alternative might follow an example in Venables and Ripley 
(2002, p. 76-77) Modern Applied Statistics with S (Springer): 

topo.loess <- loess(z~x*y, topo, degree=2, span=0.25)
topo.mar <- list(x=seq(0, 6.5, .2), y=seq(0, 6.5, .2))
topo.lo <- predict(topo.loess, expand.grid(topo.mar))
par(pty="s")
contour(topo.mar$x, topo.mar$y, topo.lo,
  xlab="", ylab="", levels=seq(700, 1000, 25), cex=0.7)
points(topo$x, topo$y)
par(pty="m")

      They also provide a solution using contourplot.  However, in R 
1.9.1 alpha, I got an error on that, 'couldn't find function "mat2tr"';  
it should be fairly easy to work around that error if it has not already 
been fixed in R 1.9.1, which I have not yet installed. 

      hope this helps.  spencer graves

Barry Rowlingson wrote:

> temiz wrote:
>
>> hello
>>
>> is there any way creating slope angle surface from
>> interpolated topography surface ?
>
>
>  The GRASS GIS can do calculations of slope, and aspect, and so on 
> from spot heights. Suggest you install GRASS and then maybe do data 
> transfer from R using the GRASS package from CRAN.
>
> Baz
>
> Grass: http://grass.itc.it/index.html
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Mon Jun 28 16:16:04 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 28 Jun 2004 15:16:04 +0100 (BST)
Subject: [R] creating slope angle surface
In-Reply-To: <40E02447.5070905@pdf.com>
Message-ID: <Pine.GSO.4.31.0406281510200.23661-100000@toucan.stats>

On Mon, 28 Jun 2004, Spencer Graves wrote:

>       Another alternative might follow an example in Venables and Ripley
> (2002, p. 76-77) Modern Applied Statistics with S (Springer):
>
> topo.loess <- loess(z~x*y, topo, degree=2, span=0.25)
> topo.mar <- list(x=seq(0, 6.5, .2), y=seq(0, 6.5, .2))
> topo.lo <- predict(topo.loess, expand.grid(topo.mar))
> par(pty="s")
> contour(topo.mar$x, topo.mar$y, topo.lo,
>   xlab="", ylab="", levels=seq(700, 1000, 25), cex=0.7)
> points(topo$x, topo$y)
> par(pty="m")
>
>       They also provide a solution using contourplot.  However, in R
> 1.9.1 alpha, I got an error on that, 'couldn't find function "mat2tr"';
> it should be fairly easy to work around that error if it has not already
> been fixed in R 1.9.1, which I have not yet installed.

You do need to consult the R scripts for R differences -- MASS4 p.12.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272860 (secr)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From EDMOND_ORIORDAN at NYMC.EDU  Mon Jun 28 16:23:24 2004
From: EDMOND_ORIORDAN at NYMC.EDU (ORIORDAN EDMOND)
Date: Mon, 28 Jun 2004 10:23:24 -0400
Subject: [R] Boosting
Message-ID: <FB27C4ED7608FC499A15ED5BBC9B3FB602532CF7@mail1.nymc.edu>

Hi 
Does anybody have a package/code for Real Adaboost that works in R?
I need to classify a very large binary data set
Any help greatly appreciated
cheers
ed



From kmw at mail.rockefeller.edu  Mon Jun 28 16:23:41 2004
From: kmw at mail.rockefeller.edu (Knut M. Wittkowski)
Date: Mon, 28 Jun 2004 10:23:41 -0400
Subject: [R] alternate rank method
Message-ID: <5.1.0.14.0.20040628101852.00a41da0@imap.rockefeller.edu>

Try:


MxRank <- function(x, na.last = "keep")
{
         if (na.last != "keep")
                 return(rank(x, na.last))
         else {
                 r <- x*NA
                 NoWarn(r[is.orderable(x)] <- rank(x,na.last=NA))
                 return(r)
} }


Knut M. Wittkowski, PhD,DSc
------------------------------------------
The Rockefeller University, GCRC
Experimental Design and Biostatistics
1230 York Ave #121B, Box 322, NY,NY 10021
+1(212)327-7175, +1(212)327-8450 (Fax)
kmw at rockefeller.edu
http://www.rucares.org/clinicalresearch/dept/biometry/



From msvika at mscc.huji.ac.il  Mon Jun 28 17:30:40 2004
From: msvika at mscc.huji.ac.il (Vicky Landsman)
Date: Mon, 28 Jun 2004 17:30:40 +0200
Subject: [R] R, maximization problem and  MINUIT package
Message-ID: <006c01c45d24$ded71560$9200a8c0@Home3>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040628/cd58bd1f/attachment.pl

From Laure.Malherbe at ineris.fr  Mon Jun 28 16:40:40 2004
From: Laure.Malherbe at ineris.fr (Laure MALHERBE)
Date: Mon, 28 Jun 2004 16:40:40 +0200
Subject: [R] question about PCA
Message-ID: <s0e04a2a.046@sirius.prive.ineris.fr>

I would like to perform a principal component analysis using the
function princomp. How is it possible to add illustrative variables or
individuals ?

Thanks in advance

Laure Malherbe



From Jens_Praestgaard at hgsi.com  Mon Jun 28 17:02:10 2004
From: Jens_Praestgaard at hgsi.com (Jens_Praestgaard@hgsi.com)
Date: Mon, 28 Jun 2004 11:02:10 -0400
Subject: [R] Jens Praestgaard/Hgsi is out of the office.
Message-ID: <OF5E580AC5.A74EC4FE-ON85256EC1.005298AE-85256EC1.005298AE@hgsi.com>

I will be out of the office starting  06/28/2004 and will not return until
06/30/2004.

Jens Praestgaard is  out of the office until June 30 and  will respond to
your message when he returns.
Thank you



From fzh113 at hecky.it.northwestern.edu  Mon Jun 28 17:06:27 2004
From: fzh113 at hecky.it.northwestern.edu (Fred)
Date: Mon, 28 Jun 2004 10:06:27 -0500
Subject: [R] How to determine the number of dominant eigenvalues in PCA
Message-ID: <000a01c45d21$7f1d7f40$a7560d18@f0z6305>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040628/0a1e13bc/attachment.pl

From ripley at stats.ox.ac.uk  Mon Jun 28 17:23:36 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 28 Jun 2004 16:23:36 +0100 (BST)
Subject: [R] How to determine the number of dominant eigenvalues in PCA
In-Reply-To: <000a01c45d21$7f1d7f40$a7560d18@f0z6305>
Message-ID: <Pine.LNX.4.44.0406281619330.3477-100000@gannet.stats>

On Mon, 28 Jun 2004, Fred wrote:

> I want to know if there is some easy and reliable way
> to estimate the number of dominant eigenvalues
> when applying PCA on sample covariance matrix.

The short answer is `no' since it depends what you want to do PCA for (and 
there are many possible uses).

> Assume x-axis is the number of eigenvalues (1, 2, ....,n), and y-axis is the 
> corresponding eigenvalues (a1,a2,..., an) arranged in desceding order.
> So this x-y plot will be a decreasing curve. Someone mentioned using the elbow (knee) method
> to find the point that the maximal curvature of this curve occurs.
> The number at this point would be the number of dominant eigenvalues.

It's not a curve!  If you joins the points by line it is piecewise linear 
and has curvature nowhere.

See ?screeplot and its references, since the plot is called a `scree
plot'.  It's a well known technique in all good textbooks on PCA.

> But I could not find any reference papers on this idea.
> Does anyone has tried this method or knows more details on this?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From gavin.simpson at ucl.ac.uk  Mon Jun 28 17:44:52 2004
From: gavin.simpson at ucl.ac.uk (Gavin Simpson)
Date: Mon, 28 Jun 2004 16:44:52 +0100
Subject: [R] How to determine the number of dominant eigenvalues in PCA
In-Reply-To: <000a01c45d21$7f1d7f40$a7560d18@f0z6305>
References: <000a01c45d21$7f1d7f40$a7560d18@f0z6305>
Message-ID: <40E03CF4.4060503@ucl.ac.uk>

Fred wrote:
> Dear All,
> 
> I want to know if there is some easy and reliable way to estimate the
> number of dominant eigenvalues when applying PCA on sample covariance
> matrix.
> 
> Assume x-axis is the number of eigenvalues (1, 2, ....,n), and y-axis
> is the corresponding eigenvalues (a1,a2,..., an) arranged in
> desceding order. So this x-y plot will be a decreasing curve. Someone
> mentioned using the elbow (knee) method to find the point that the
> maximal curvature of this curve occurs. The number at this point
> would be the number of dominant eigenvalues.
> 
> But I could not find any reference papers on this idea. Does anyone
> has tried this method or knows more details on this?
> 
> Thanks for your point.
> 
> Fred
> 

Try this reference from the field of ecology:

@Article{571,
   Author         = {D. A. Jackson},
   Title          = {Stopping rules in principal components analysis: a
                    comparison of heuristic and statistical approaches},
   Journal        = {Ecology},
   Volume         = {74},
   Number         = {8},
   Pages          = {2204--2214},
   month          = {},
   year           = 1993
}

Gav
-- 
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%
Gavin Simpson                     [T] +44 (0)20 7679 5522
ENSIS Research Fellow             [F] +44 (0)20 7679 7565
ENSIS Ltd. & ECRC                 [E] gavin.simpson at ucl.ac.uk
UCL Department of Geography       [W] http://www.ucl.ac.uk/~ucfagls/cv/
26 Bedford Way                    [W] http://www.ucl.ac.uk/~ucfagls/
London.  WC1H 0AP.
%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%~%



From fzh113 at hecky.it.northwestern.edu  Mon Jun 28 19:14:52 2004
From: fzh113 at hecky.it.northwestern.edu (Fred)
Date: Mon, 28 Jun 2004 12:14:52 -0500
Subject: [R] How to determine the number of dominant eigenvalues in PCA
In-Reply-To: <Pine.LNX.4.44.0406281619330.3477-100000@gannet.stats>
Message-ID: <000001c45d33$6cb987b0$ab8d7ca5@FYOC1>

Thanks, Prof Ripley

However, I searched some references on scree plot
On deciding the number of dominant eigenvalues, and
Found that this is still a subjective method.
That is, no explicit way or formulation to choose
the number from the 2-D scree plot.
Am I right?

Fred

-----Original Message-----
From: Prof Brian Ripley [mailto:ripley at stats.ox.ac.uk] 
Sent: Monday, June 28, 2004 10:24 AM
To: Fred
Cc: R-help mailing list
Subject: Re: [R] How to determine the number of dominant eigenvalues in
PCA

On Mon, 28 Jun 2004, Fred wrote:

> I want to know if there is some easy and reliable way
> to estimate the number of dominant eigenvalues
> when applying PCA on sample covariance matrix.

The short answer is `no' since it depends what you want to do PCA for
(and 
there are many possible uses).

> Assume x-axis is the number of eigenvalues (1, 2, ....,n), and y-axis
is the 
> corresponding eigenvalues (a1,a2,..., an) arranged in desceding order.
> So this x-y plot will be a decreasing curve. Someone mentioned using
the elbow (knee) method
> to find the point that the maximal curvature of this curve occurs.
> The number at this point would be the number of dominant eigenvalues.

It's not a curve!  If you joins the points by line it is piecewise
linear 
and has curvature nowhere.

See ?screeplot and its references, since the plot is called a `scree
plot'.  It's a well known technique in all good textbooks on PCA.

> But I could not find any reference papers on this idea.
> Does anyone has tried this method or knows more details on this?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From egcp at hotmail.com  Mon Jun 28 19:31:18 2004
From: egcp at hotmail.com (E GCP)
Date: Mon, 28 Jun 2004 17:31:18 +0000
Subject: [R] rgl installation problems
Message-ID: <BAY22-F289KfYDovvQ0000037fc@hotmail.com>

Thanks for your quick replies, and excuse my naivete, but how do I fix the 
problem, so the rgl package installs?

Thanks again,
Enrique

&gt;From: Prof Brian Ripley &lt;ripley at stats.ox.ac.uk&gt;
&gt;Subject: Re: [R] rgl installation problems
&gt;Date: Fri, 25 Jun 2004 20:14:24 +0100 (BST)
&gt;
&gt;Here's what should happen on RH9 (with the latest libpng and gcc in
&gt;/usr/local/lib)
&gt;
&gt;g++ -shared -L/usr/local/lib -o rgl.so x11lib.o x11gui.o types.o math.o
&gt;fps.o pixmap.o gui.o api.o device.o devicemanager.o rglview.o scene.o
&gt;glgui.o -L/usr/X11R6/lib -L/usr/local/lib -Wl,-rpath,/usr/local/lib
&gt;-lpng12 -lz -lm -lstdc++ -lX11 -lXext -lGL -lGLU -lpng12 -lz -lm
&gt;
&gt;Note that he had no --shared but did have crt1.o, that is was trying to
&gt;build a standalone executable and not a shared object.
&gt;
&gt;Something is wrong with the R installation's rules to make shared
&gt;libraries.
&gt;
&gt;On Fri, 25 Jun 2004, Duncan Murdoch wrote:
&gt;
&gt; &gt; On Fri, 25 Jun 2004 17:36:30 +0000, &quot;E GCP&quot; 
&lt;egcp at hotmail.com&gt; wrote :
&gt; &gt;
&gt; &gt; &gt;Hi!
&gt; &gt; &gt;
&gt; &gt; &gt;I'm new to R, but have worked with Splus before. I installed 
several
&gt; &gt; &gt;packages in R (R-1.9.1) without problems, but when I try to 
install rgl
&gt; &gt; &gt;(rgl_0.64-13.tar.gz). I get the following, and the package 
does not install.
&gt; &gt; &gt;Any help would be greatly appreciated. I'm running R in redhat 
9.
&gt; &gt;
&gt; &gt; The missing reference R_InputHandlers is declared in the
&gt; &gt; $RHOME/src/include/R_ext/eventloop.h file, and I believe is 
compiled
&gt; &gt; into the library R_X11 (with some extension).  You don't seem to 
have
&gt; &gt; that in the list of libraries:
&gt; &gt;
&gt; &gt; &gt;g++  -L/usr/local/lib -o rgl.so x11lib.o x11gui.o types.o 
math.o fps.o
&gt; &gt; &gt;pixmap.o
&gt; &gt; &gt;gui.o api.o device.o devicemanager.o rglview.o scene.o glgui.o
&gt; &gt; &gt;-L/usr/X11R6/lib
&gt; &gt; &gt;-L/usr/lib -lstdc++ -lX11 -lXext -lGL -lGLU -lpng
&gt; &gt; 
&gt;/usr/lib/gcc-lib/i386-redhat-linux/3.2.2/../../../crt1.o(.text+0x18): In
&gt; &gt; &gt;functio
&gt; &gt; &gt;n `_start':
&gt; &gt; &gt;../sysdeps/i386/elf/start.S:77: undefined reference to `main'
&gt; &gt; &gt;x11lib.o(.text+0x84): In function `set_R_handler':
&gt; &gt; &gt;/tmp/R.INSTALL.8663/rgl/src/x11gui.h:33: undefined reference 
to
&gt; &gt; &gt;`R_InputHandlers
&gt; &gt;
&gt; &gt; I don't know what you'll need to do to fix this, since I'm using
&gt; &gt; Windows, so none of this stuff happens there, and I could be
&gt; &gt; completely wrong about it.
&gt;



From wolski at molgen.mpg.de  Mon Jun 28 19:36:52 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Mon, 28 Jun 2004 19:36:52 +0200
Subject: [R] How to determine the number of dominant eigenvalues
  in PCA
In-Reply-To: <000a01c45d21$7f1d7f40$a7560d18@f0z6305>
References: <000a01c45d21$7f1d7f40$a7560d18@f0z6305>
Message-ID: <200406281936520782.025D54A9@mail.math.fu-berlin.de>

Hi!

There is a chapter in the book from Hrdl about the interpretation of PCs available online.
http://www.quantlet.com/mdstat/scripts/mva/htmlbook/mvahtmlframe93.html


About determining the number of dominant eigenvalues is a chapter in book of A. Handl  (available online but in german.) 
http://www.quantlet.com/mdstat/scripts/mst/html/msthtmlframe56.html

Two references to this topic from this online book.
Cattell, R. B. (1966): The scree test for the number of factors. Multivariate Behavioral Research, 1, 245-276 
Kaiser, H. F. (1960): The application of electronic computers to factor analysis. Educ. Psychol. Meas., 20, 141-151 



Hope this helps.
Sincerely Eryk



*********** REPLY SEPARATOR  ***********

On 28.06.2004 at 10:06 Fred wrote:

>Dear All,
>
>I want to know if there is some easy and reliable way
>to estimate the number of dominant eigenvalues
>when applying PCA on sample covariance matrix.
>
>Assume x-axis is the number of eigenvalues (1, 2, ....,n), and y-axis is
>the 
>corresponding eigenvalues (a1,a2,..., an) arranged in desceding order.
>So this x-y plot will be a decreasing curve. Someone mentioned using the
>elbow (knee) method
>to find the point that the maximal curvature of this curve occurs.
>The number at this point would be the number of dominant eigenvalues.
>
>But I could not find any reference papers on this idea.
>Does anyone has tried this method or knows more details on this?
>
>Thanks for your point.
>
>Fred
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Mon Jun 28 19:47:24 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 28 Jun 2004 18:47:24 +0100 (BST)
Subject: [R] rgl installation problems
In-Reply-To: <BAY22-F289KfYDovvQ0000037fc@hotmail.com>
Message-ID: <Pine.LNX.4.44.0406281843380.3814-100000@gannet.stats>

On Mon, 28 Jun 2004, E GCP wrote:

> Thanks for your quick replies, and excuse my naivete, but how do I fix the 
> problem, so the rgl package installs?

We have no idea what is wrong on your system -- all we can tell is that
you have done something wrong but we were not sitting at your shoulder
when you did.  Perhaps you should try re-building R from scratch, paying
close attention to any messages?

Meanwhile, try to follow the posting guide and not HTML-ize your mail.

> Thanks again,
> Enrique
> 
> &gt;From: Prof Brian Ripley &lt;ripley at stats.ox.ac.uk&gt;
> &gt;Subject: Re: [R] rgl installation problems
> &gt;Date: Fri, 25 Jun 2004 20:14:24 +0100 (BST)
> &gt;
> &gt;Here's what should happen on RH9 (with the latest libpng and gcc in
> &gt;/usr/local/lib)
> &gt;
> &gt;g++ -shared -L/usr/local/lib -o rgl.so x11lib.o x11gui.o types.o math.o
> &gt;fps.o pixmap.o gui.o api.o device.o devicemanager.o rglview.o scene.o
> &gt;glgui.o -L/usr/X11R6/lib -L/usr/local/lib -Wl,-rpath,/usr/local/lib
> &gt;-lpng12 -lz -lm -lstdc++ -lX11 -lXext -lGL -lGLU -lpng12 -lz -lm
> &gt;
> &gt;Note that he had no --shared but did have crt1.o, that is was trying to
> &gt;build a standalone executable and not a shared object.
> &gt;
> &gt;Something is wrong with the R installation's rules to make shared
> &gt;libraries.
> &gt;
> &gt;On Fri, 25 Jun 2004, Duncan Murdoch wrote:
> &gt;
> &gt; &gt; On Fri, 25 Jun 2004 17:36:30 +0000, &quot;E GCP&quot; 
> &lt;egcp at hotmail.com&gt; wrote :
> &gt; &gt;
> &gt; &gt; &gt;Hi!
> &gt; &gt; &gt;
> &gt; &gt; &gt;I'm new to R, but have worked with Splus before. I installed 
> several
> &gt; &gt; &gt;packages in R (R-1.9.1) without problems, but when I try to 
> install rgl
> &gt; &gt; &gt;(rgl_0.64-13.tar.gz). I get the following, and the package 
> does not install.
> &gt; &gt; &gt;Any help would be greatly appreciated. I'm running R in redhat 
> 9.
> &gt; &gt;
> &gt; &gt; The missing reference R_InputHandlers is declared in the
> &gt; &gt; $RHOME/src/include/R_ext/eventloop.h file, and I believe is 
> compiled
> &gt; &gt; into the library R_X11 (with some extension).  You don't seem to 
> have
> &gt; &gt; that in the list of libraries:
> &gt; &gt;
> &gt; &gt; &gt;g++  -L/usr/local/lib -o rgl.so x11lib.o x11gui.o types.o 
> math.o fps.o
> &gt; &gt; &gt;pixmap.o
> &gt; &gt; &gt;gui.o api.o device.o devicemanager.o rglview.o scene.o glgui.o
> &gt; &gt; &gt;-L/usr/X11R6/lib
> &gt; &gt; &gt;-L/usr/lib -lstdc++ -lX11 -lXext -lGL -lGLU -lpng
> &gt; &gt; 
> &gt;/usr/lib/gcc-lib/i386-redhat-linux/3.2.2/../../../crt1.o(.text+0x18): In
> &gt; &gt; &gt;functio
> &gt; &gt; &gt;n `_start':
> &gt; &gt; &gt;../sysdeps/i386/elf/start.S:77: undefined reference to `main'
> &gt; &gt; &gt;x11lib.o(.text+0x84): In function `set_R_handler':
> &gt; &gt; &gt;/tmp/R.INSTALL.8663/rgl/src/x11gui.h:33: undefined reference 
> to
> &gt; &gt; &gt;`R_InputHandlers
> &gt; &gt;
> &gt; &gt; I don't know what you'll need to do to fix this, since I'm using
> &gt; &gt; Windows, so none of this stuff happens there, and I could be
> &gt; &gt; completely wrong about it.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From wolski at molgen.mpg.de  Mon Jun 28 19:55:33 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Mon, 28 Jun 2004 19:55:33 +0200
Subject: [R] RMysql installation problem.
Message-ID: <200406281955330171.026E6D3F@mail.math.fu-berlin.de>

Hi!

I am trying to install the RMySQL package.  The installation stops with the following error message.

path to mysql is set.
setenv PKG_CPPFLAGS /home/arabidopsis/software/R1.9.1/linux/mysql/include
setenv PKG_LIBS /home/arabidopsis/software/R1.9.1/linux/mysql/lib

R CMD INSTALL RMySQL_0.5-5.tar.gz 
#....cut cut.
creating src/Makevars
** libs
gcc -I/home/arabidopsis/software/R1.9.1/linux/lib/R/include /home/arabidopsis/software/R1.9.1/linux/mysql/include -I/usr/local/include -D__NO_MATH_INLINES -mieee-fp  -fPIC  -g -O2 -c RS-DBI.c -o RS-DBI.o
gcc: cannot specify -o with -c or -S and multiple compilations
make: *** [RS-DBI.o] Error 1
ERROR: compilation failed for package 'RMySQL'
** Removing '/home/arabidopsis/software/R1.9.1/linux/lib/R/library/RMySQL'


Thought this is because R and Mysql are compiled with different compilers.  Because of this error I just installed the newest R and mysql from source.
Please help.

Eryk



From gunter.berton at gene.com  Mon Jun 28 20:06:12 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Mon, 28 Jun 2004 11:06:12 -0700
Subject: [R] How to determine the number of dominant eigenvalues in PCA
References: <Pine.LNX.4.44.0406281619330.3477-100000@gannet.stats>
Message-ID: <40E05E14.415D4B73@gene.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040628/8fc941a4/attachment.pl

From p.dalgaard at biostat.ku.dk  Mon Jun 28 20:05:34 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 28 Jun 2004 20:05:34 +0200
Subject: [R] rgl installation problems
In-Reply-To: <Pine.LNX.4.44.0406281843380.3814-100000@gannet.stats>
References: <Pine.LNX.4.44.0406281843380.3814-100000@gannet.stats>
Message-ID: <x2vfhbblsh.fsf@biostat.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> On Mon, 28 Jun 2004, E GCP wrote:
> 
> > Thanks for your quick replies, and excuse my naivete, but how do I fix the 
> > problem, so the rgl package installs?
> 
> We have no idea what is wrong on your system -- all we can tell is that
> you have done something wrong but we were not sitting at your shoulder
> when you did.  Perhaps you should try re-building R from scratch, paying
> close attention to any messages?
> 
> Meanwhile, try to follow the posting guide and not HTML-ize your mail.

Another thing to try is to install Martyn's RPM (for FC1) instead of
what is there now. Seems to get things right for me on RH8. 

The demo is amazingly smooth even on this ancient machine, BTW.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From lauraholt_983 at hotmail.com  Mon Jun 28 20:53:16 2004
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Mon, 28 Jun 2004 13:53:16 -0500
Subject: [R] plotting with an its object
Message-ID: <BAY12-F93xYHLBQhExk0001bbce@hotmail.com>

Dear R People:

I have an its object.  The original its is the Dow Jones Industrial average 
from March 23, 1990 to March 23, 2000.  I did the following;

dj1 = original series

>dj2 <- log(dj1)
>ddj2 <- diff(dj2)
>

Ok so far.
Now I would like to plot this series with a certain y limit:
>min(ddj2)
[1] -0.07454905
>max(ddj2)
[1] 0.04860535
>plot(ddj2,ylim=c(-0.1,0.1))
Error in plot.default(x, y, xaxt = "n", xlab = xlab, axes = axes, frame.plot 
= frame.plot,  :
        formal argument "ylim" matched by multiple actual arguments
>
What is wrong, please?

I checked the following:
        formal argument "ylim" matched by multiple actual arguments
>?plot.its
Error in help("plot.its") : No documentation for `plot.its' in specified 
packages and libraries:
  you could try `help.search("plot.its")'
>its.plot
Error: Object "its.plot" not found
>?its.plot
Error in help("its.plot") : No documentation for `its.plot' in specified 
packages and libraries:
  you could try `help.search("its.plot")'
>
but to no avail.

R Version 1.9.1 for Windows

Thanks in advance for any suggestions.

Sincerely,
Laura
mailto: lauraholt_983 at hotmail.com


Guide! http://dollar.msn.com



From wolski at molgen.mpg.de  Mon Jun 28 20:57:08 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Mon, 28 Jun 2004 20:57:08 +0200
Subject: [R] RMysql installation problem. / Problem solved!
In-Reply-To: <200406281955330171.026E6D3F@mail.math.fu-berlin.de>
References: <200406281955330171.026E6D3F@mail.math.fu-berlin.de>
Message-ID: <200406282057080214.02A6D109@mail.math.fu-berlin.de>

It works!!!! 

with:
R CMD INSTALL --configure-args='--with-mysql-dir="/home/arabidopsis/software/R1.9.1/linux/mysql/"' RMySQL_0.5-5.tar.gz

dunno why.

Sorry for the annoyance.

Eryk

*********** REPLY SEPARATOR  ***********

On 28.06.2004 at 19:55 Wolski wrote:

>Hi!
>
>I am trying to install the RMySQL package.  The installation stops with
>the following error message.
>
>path to mysql is set.
>setenv PKG_CPPFLAGS /home/arabidopsis/software/R1.9.1/linux/mysql/include
>setenv PKG_LIBS /home/arabidopsis/software/R1.9.1/linux/mysql/lib
>
>R CMD INSTALL RMySQL_0.5-5.tar.gz 
>#....cut cut.
>creating src/Makevars
>** libs
>gcc -I/home/arabidopsis/software/R1.9.1/linux/lib/R/include
>/home/arabidopsis/software/R1.9.1/linux/mysql/include -I/usr/local/include
>-D__NO_MATH_INLINES -mieee-fp  -fPIC  -g -O2 -c RS-DBI.c -o RS-DBI.o
>gcc: cannot specify -o with -c or -S and multiple compilations
>make: *** [RS-DBI.o] Error 1
>ERROR: compilation failed for package 'RMySQL'
>** Removing '/home/arabidopsis/software/R1.9.1/linux/lib/R/library/RMySQL'
>
>
>Thought this is because R and Mysql are compiled with different compilers.
> Because of this error I just installed the newest R and mysql from source.
>Please help.
>
>Eryk
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From mepstein at umich.edu  Mon Jun 28 21:33:01 2004
From: mepstein at umich.edu (Michael Epstein)
Date: Mon, 28 Jun 2004 15:33:01 -0400 (EDT)
Subject: [R] Unable to start R due to error with tempdir()
In-Reply-To: <mailman.6440.1088450084.23181.r-help@stat.math.ethz.ch>
Message-ID: <Pine.GSO.4.40.0406281520360.8292-100000@compute15.sph.umich.edu>

Hello,

I run R 1.9.1 on a Windows XP machine. In the last week, I have started to
get the following error message (about 85% of the time) when I attempt to
start R:

"Fatal error: cannot find unused tempdir name"

I get this message whether I attempt to start R in interactive or batch
mode.

Looking at the help-manual description of tempdir didn't provide me with
any obvious clues to the problem. I also tried uninstalling and
reinstalling R 1.9.1, but this was of no help to the problem either.

Any help would be greatly appreciated. Thank you very much.

Best,

Mike Epstein



From GPetris at uark.edu  Mon Jun 28 21:43:48 2004
From: GPetris at uark.edu (Giovanni Petris)
Date: Mon, 28 Jun 2004 14:43:48 -0500 (CDT)
Subject: [R] Installing on Windows packages build on Unix
In-Reply-To: <x24qow3e43.fsf@biostat.ku.dk> (message from Peter Dalgaard on
	Sun, 27 Jun 2004 23:06:20 +0200)
References: <FIELLCIFBNNJMMGPHDDNGEIBIJAA.baize@3dculture.com>
	<x24qow3e43.fsf@biostat.ku.dk>
Message-ID: <200406281943.i5SJhm7m012151@definetti.uark.edu>


Thank you to all who responded.
In the end the simple solution suggested by Peter Dalgaard 
worked for us: install the package and zip the resulting
directory. (So, incidentally, this still works with R 1.8.0)

Thanks,
Giovanni



From ripley at stats.ox.ac.uk  Mon Jun 28 21:50:37 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 28 Jun 2004 20:50:37 +0100 (BST)
Subject: [R] Unable to start R due to error with tempdir()
In-Reply-To: <Pine.GSO.4.40.0406281520360.8292-100000@compute15.sph.umich.edu>
Message-ID: <Pine.LNX.4.44.0406282046140.19514-100000@gannet.stats>

Take a look at your temporary dir, and see if you can clear it out.  
?tempdir tells you where it is.


On Mon, 28 Jun 2004, Michael Epstein wrote:

> Hello,
> 
> I run R 1.9.1 on a Windows XP machine. In the last week, I have started to
> get the following error message (about 85% of the time) when I attempt to
> start R:
> 
> "Fatal error: cannot find unused tempdir name"
> 
> I get this message whether I attempt to start R in interactive or batch
> mode.
> 
> Looking at the help-manual description of tempdir didn't provide me with
> any obvious clues to the problem. I also tried uninstalling and
> reinstalling R 1.9.1, but this was of no help to the problem either.
> 
> Any help would be greatly appreciated. Thank you very much.
> 
> Best,
> 
> Mike Epstein
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From mfowle at navicominc.com  Mon Jun 28 21:57:34 2004
From: mfowle at navicominc.com (Mark Fowle)
Date: Mon, 28 Jun 2004 15:57:34 -0400
Subject: [R] Unable to start R due to error with tempdir()
Message-ID: <00B717603414D21187AD00104B94A2DAB23C58@EXCHANGE>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040628/9d1b1053/attachment.pl

From ggrothendieck at myway.com  Mon Jun 28 21:53:21 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Mon, 28 Jun 2004 19:53:21 +0000 (UTC)
Subject: [R] plotting with an its object
References: <BAY12-F93xYHLBQhExk0001bbce@hotmail.com>
Message-ID: <loom.20040628T215017-584@post.gmane.org>


Try using yrange instead of ylim.

getAnywhere("plotIts") will get you the source of the routine you are
looking for.

---

Laura Holt <lauraholt_983 <at> hotmail.com> writes:

: Dear R People:
: 
: I have an its object.  The original its is the Dow Jones Industrial average 
: from March 23, 1990 to March 23, 2000.  I did the following;
: 
: dj1 = original series
: 
: >dj2 <- log(dj1)
: >ddj2 <- diff(dj2)
: >
: 
: Ok so far.
: Now I would like to plot this series with a certain y limit:
: >min(ddj2)
: [1] -0.07454905
: >max(ddj2)
: [1] 0.04860535
: >plot(ddj2,ylim=c(-0.1,0.1))
: Error in plot.default(x, y, xaxt = "n", xlab = xlab, axes = axes, frame.plot 
: = frame.plot,  :
:         formal argument "ylim" matched by multiple actual arguments
: >
: What is wrong, please?
: 
: I checked the following:
:         formal argument "ylim" matched by multiple actual arguments
: >?plot.its
: Error in help("plot.its") : No documentation for `plot.its' in specified 
: packages and libraries:
:   you could try `help.search("plot.its")'
: >its.plot
: Error: Object "its.plot" not found
: >?its.plot
: Error in help("its.plot") : No documentation for `its.plot' in specified 
: packages and libraries:
:   you could try `help.search("its.plot")'
: >
: but to no avail.
: 
: R Version 1.9.1 for Windows
: 
: Thanks in advance for any suggestions.
: 
: Sincerely,
: Laura
: mailto: lauraholt_983 <at> hotmail.com



From mepstein at umich.edu  Mon Jun 28 21:56:44 2004
From: mepstein at umich.edu (Michael Epstein)
Date: Mon, 28 Jun 2004 15:56:44 -0400 (EDT)
Subject: [R] Unable to start R due to error with tempdir()
In-Reply-To: <Pine.LNX.4.44.0406282046140.19514-100000@gannet.stats>
Message-ID: <Pine.GSO.4.40.0406281552540.8326-100000@compute15.sph.umich.edu>

Are you implying that I should delete all the Rtmp* directories in my
temporary dir? Thanks much.

On Mon, 28 Jun 2004, Prof Brian Ripley wrote:

> Take a look at your temporary dir, and see if you can clear it out.
> ?tempdir tells you where it is.
>
>
> On Mon, 28 Jun 2004, Michael Epstein wrote:
>
> > Hello,
> >
> > I run R 1.9.1 on a Windows XP machine. In the last week, I have started to
> > get the following error message (about 85% of the time) when I attempt to
> > start R:
> >
> > "Fatal error: cannot find unused tempdir name"
> >
> > I get this message whether I attempt to start R in interactive or batch
> > mode.
> >
> > Looking at the help-manual description of tempdir didn't provide me with
> > any obvious clues to the problem. I also tried uninstalling and
> > reinstalling R 1.9.1, but this was of no help to the problem either.
> >
> > Any help would be greatly appreciated. Thank you very much.
> >
> > Best,
> >
> > Mike Epstein
> >
> > ______________________________________________
> > R-help at stat.math.ethz.ch mailing list
> > https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> > PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> >
> >
>
> --
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>

------------------------------------------------------------------------------
Michael Epstein                                           Work: (734) 615-5114
U-M School of Public Health                               Fax:  (734) 763-2215
Department of Biostatistics
M2517, SPH II                                               mepstein at umich.edu

"The true scientist never loses the faculty of amazement. It is the
essence of his/her being."

			-Hans Selye



From roebuck at odin.mdacc.tmc.edu  Mon Jun 28 22:42:24 2004
From: roebuck at odin.mdacc.tmc.edu (Paul Roebuck)
Date: Mon, 28 Jun 2004 15:42:24 -0500 (CDT)
Subject: [R] R via ssh login on OS X?
In-Reply-To: <9AFB7DC8-C8BF-11D8-BA01-00306579408C@syr.edu>
References: <9AFB7DC8-C8BF-11D8-BA01-00306579408C@syr.edu>
Message-ID: <Pine.OSF.4.58.0406281526200.252750@odin.mdacc.tmc.edu>

On Mon, 28 Jun 2004, James Howison wrote:

> I have an ssh only login to a G5 on which I am hoping to run some
> analyses.  The situation is complicated by the fact that the computer's
> owner is away for the summer (and thus also only has shell login).
>
> R is installed and there is a symlink to /usr/local/bin/R but when I
> try to launch it I get:
>
> [jhowison at euro]$ R
> kCGErrorRangeCheck : Window Server communications from outside of
> session allowed for root and console user only
> INIT_Processeses(), could not establish the default connection to the
> WindowServer.Abort trap
>
> I though, ah ha, I need to tell it not to use the GUI but to no avail:
>
> [jhowison at euro]$ R --gui=none
> kCGErrorRangeCheck : Window Server communications from outside of
> session allowed for root and console user only
> INIT_Processeses(), could not establish the default connection to the
> WindowServer.Abort trap
>
> I'm embarrassed to say that I'm writing to the list without having the
> latest version installed---because I can't install it at the moment.  I
> am using R 1.8.1.  I have tried to compile the latest from source but
> there is no F77 compiler. I thought I'd ask around before going down
> the "put local dependencies in the home folder" to compile this route
> (any hints on doing that would be great though) ...
>
> Can other people get R command-line to work with logged in remotely via
> ssh?  Any hints?
> Is this something that is fixed in more recent versions?
>
> I think I can see one other route:  getting the computer's owner to
> install fink and their version remotely ... but I'm open to all "don't
> bother the professor when he's on holiday" options ...

I suffered similarly attempting to run R via CGI; I never found
a workaround for remote access (also running 1.8.1 with Panther).
Seemed to have something to do with running an application requiring
access to graphics but not being the user currently "owning" the dock.
I did not determine if the limitation was due to R implementation or
operating system software.

F77 not necessary; use 'f2c' instead. But don't bother with Fink since
it's not necessary to build it. No 'sudo' access either? Is the user
still logged in (screenlocked) or are you just lacking administrative
access?

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From olivier_collignon at hotmail.com  Mon Jun 28 22:49:37 2004
From: olivier_collignon at hotmail.com (Olivier Collignon)
Date: Mon, 28 Jun 2004 13:49:37 -0700
Subject: [R] R client connection OLAP cube (SQL Analysis Services /
	PivotTable Service)
Message-ID: <BAY18-F17kULb3ggKPP0002cbd1@hotmail.com>

I have been doing data analysis/modeling in R, connecting to SQL databases 
with RODBC (winXP client with R1.9.0 and win2k SQL server 2000).

I am now trying to leverage some of the OLAP features to keep the data 
intensive tasks on the DB server side and only keep the analytical tasks 
within R (optimize use of memory). Is there any package that would allow to 
connect to OLAP cubes (as a client to SQL Analysis Services PivotTable 
Service) from an R session (similar to the RODBC package)?

Can this be done directly with special connection parameters (from R) 
through the RODBC package or do I have to setup an intermediary XL table 
(pivottable linked to the OLAP cube) and then connect to the XL data source 
from R?

I would appreciate any reference / pointer to OLAP/R configuration 
instructions. Thanks.

Olivier Collignon



From ripley at stats.ox.ac.uk  Mon Jun 28 23:01:02 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Mon, 28 Jun 2004 22:01:02 +0100 (BST)
Subject: [R] Unable to start R due to error with tempdir()
In-Reply-To: <Pine.GSO.4.40.0406281552540.8326-100000@compute15.sph.umich.edu>
Message-ID: <Pine.LNX.4.44.0406282159080.19590-100000@gannet.stats>

On Mon, 28 Jun 2004, Michael Epstein wrote:

> Are you implying that I should delete all the Rtmp* directories in my
> temporary dir? Thanks much.

Yes.  It there are very many it might explain this.

> On Mon, 28 Jun 2004, Prof Brian Ripley wrote:
> 
> > Take a look at your temporary dir, and see if you can clear it out.
> > ?tempdir tells you where it is.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From kbartz at loyaltymatrix.com  Tue Jun 29 00:30:49 2004
From: kbartz at loyaltymatrix.com (kevin bartz)
Date: Mon, 28 Jun 2004 15:30:49 -0700
Subject: [R] text length in grid
Message-ID: <20040628223550.F0A614051A@omta14.mta.everyone.net>

Hello! I first would like to compliment the authors of grid on what has been
a wonderfully useful package for me. Now, my question: Is there any way I
can specify the size of some grid.text using grid units?

I must label the regions of a plot. The regions can be either very small or
very large, so I would like to label each by fitting its text to the size of
the region in question. Ideally, I could have nice, big text labeling the
big regions and tiny type in the small regions.

Let me try to illustrate my scenario. Imagine that my plot looks like this:

pushViewport(viewport(layout = grid.layout(5, 2, c(9,1), 1:5)))
for (i in 1:5) {
  pushViewport(viewport(layout.pos.row = i, layout.pos.col = 1))
  pushViewport(viewport(x = 0, width = i/5, height = .8, just = "left"))
  grid.rect(gp = gpar(fill = rainbow(5)[i]))
  grid.yaxis(main = F)
  grid.text("I am some descriptive text.")
  popViewport()
  popViewport()
}

Unfortunately, the descriptive text overlaps with the axis for the red
rectangle! Is there any way, given the dimensions of some region on the grid
and the desired text, that I can fit this text to the region?

Sorry if I missed something in the documentation. I really did look at
?grid.text, all the documentation for the grid package, all the PDFs posted
on the grid page as well as previous r-help postings, but none seemed to
address this question. Thanks so much for any help you can provide!

Kevin



From p.murrell at auckland.ac.nz  Tue Jun 29 01:42:21 2004
From: p.murrell at auckland.ac.nz (Paul Murrell)
Date: Tue, 29 Jun 2004 11:42:21 +1200
Subject: [R] text length in grid
References: <20040628223550.F0A614051A@omta14.mta.everyone.net>
Message-ID: <40E0ACDD.3070200@stat.auckland.ac.nz>

Hi

Here's one way to do it:

fitText <- function(text) {
   tw <- convertWidth(stringWidth(text), "inches", valueOnly=TRUE)
   vw <- convertWidth(unit(1, "npc"), "inches", valueOnly=TRUE)
   cex <- vw/tw
   grid.text(text, gp=gpar(cex=cex))
}

pushViewport(viewport(layout = grid.layout(5, 2, c(9,1), 1:5)))
for (i in 1:5) {
   pushViewport(viewport(layout.pos.row = i, layout.pos.col = 1))
   pushViewport(viewport(x = 0, width = i/5, height = .8, just = "left"))
   grid.rect(gp = gpar(fill = rainbow(5)[i]))
   grid.yaxis(main = F)
   fitText("I am some descriptive text.")
   popViewport()
   popViewport()
}

This should be ok for producing a fixed-size graphic, but if you change 
the size of the device (e.g., resize the window), the calculations will 
become incorrect.  There are ways around that;  let me know if you want 
to know more.

Paul


kevin bartz wrote:
> Hello! I first would like to compliment the authors of grid on what has been
> a wonderfully useful package for me. Now, my question: Is there any way I
> can specify the size of some grid.text using grid units?
> 
> I must label the regions of a plot. The regions can be either very small or
> very large, so I would like to label each by fitting its text to the size of
> the region in question. Ideally, I could have nice, big text labeling the
> big regions and tiny type in the small regions.
> 
> Let me try to illustrate my scenario. Imagine that my plot looks like this:
> 
> pushViewport(viewport(layout = grid.layout(5, 2, c(9,1), 1:5)))
> for (i in 1:5) {
>   pushViewport(viewport(layout.pos.row = i, layout.pos.col = 1))
>   pushViewport(viewport(x = 0, width = i/5, height = .8, just = "left"))
>   grid.rect(gp = gpar(fill = rainbow(5)[i]))
>   grid.yaxis(main = F)
>   grid.text("I am some descriptive text.")
>   popViewport()
>   popViewport()
> }
> 
> Unfortunately, the descriptive text overlaps with the axis for the red
> rectangle! Is there any way, given the dimensions of some region on the grid
> and the desired text, that I can fit this text to the region?
> 
> Sorry if I missed something in the documentation. I really did look at
> ?grid.text, all the documentation for the grid package, all the PDFs posted
> on the grid page as well as previous r-help postings, but none seemed to
> address this question. Thanks so much for any help you can provide!
> 
> Kevin
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html


-- 
Dr Paul Murrell
Department of Statistics
The University of Auckland
Private Bag 92019
Auckland
New Zealand
64 9 3737599 x85392
paul at stat.auckland.ac.nz
http://www.stat.auckland.ac.nz/~paul/



From allegro1978 at hotmail.com  Tue Jun 29 05:48:57 2004
From: allegro1978 at hotmail.com (Steve S)
Date: Tue, 29 Jun 2004 13:48:57 +1000
Subject: [R] A strange question on probability
Message-ID: <BAY12-F125cPuIdVKRt0001db0d@hotmail.com>

Dear All,

I wonder if there is a probability distribution where you can specify when a 
certain event start and finish within a fixed period? For example I might 
specify the number of period to be 5, and a random vector from this 
distribution might give me:
0 1 1 1 0

where 1 is always adjacent to each other?

This can never happen: 0 0 1 0 1 for example.

My apology for this strange question!

Any advice very welcome.

Steve.


http://ad.au.doubleclick.net/clk;8097459;9106288;b?http://www.anz.com/aus/promo/qantas5000ninemsn 
   [AU only]



From adiamond at fas.harvard.edu  Tue Jun 29 05:59:44 2004
From: adiamond at fas.harvard.edu (adiamond@fas.harvard.edu)
Date: Mon, 28 Jun 2004 23:59:44 -0400
Subject: [R] strucchange-esque inference for glms ?
Message-ID: <1088481584.40e0e930ded1e@webmail.fas.harvard.edu>

hello R-world,

according to the strucchange package .pdf, "all procedures in this package are 
concerned with testing or assessing deviations from stability in the classical 
linear regression model."

i'd like to test/assess deviations from stability in the Poisson model.

is there a way to modify the strucchange package to suit my purposes, or should 
i use be using another package,   or is this a tough nut to crack? :)

my application is detecting the onset of a flu outbreak as new daily data 
trickles in from each morning from local hospitals.  seems to me like the same 
sort of inferential goal that strucchange refers to as "monitoring of 
structural change."

thank you in advance.
cheers,

alexis



From drewbrewit at yahoo.com  Tue Jun 29 06:37:01 2004
From: drewbrewit at yahoo.com (Nick Drew)
Date: Mon, 28 Jun 2004 21:37:01 -0700 (PDT)
Subject: [R] Numbers as symbols
Message-ID: <20040629043701.21283.qmail@web50907.mail.yahoo.com>

I want to use question numbers from my survey data
(fake data below) as markers in a scatterplot. I'm
using "as.character" to convert question numbers to
characters. However, plot truncates the 0 (zero) off
of question 10. How can I get the ending zero so I can
add questions 11 - 20?
 
# Sample code below
Question <- c(1,2,3,4,5,6,7,8,9,10)
Performance <-
c(3.5,3.6,3.7,3.8,3.9,4,4.1,4.2,4.3,4.4)
Importance <-
c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.99)
Survey <-
data.frame(cbind(Question,Performance,Importance))
 
nums <-as.character(Question)
plot(Performance, Importance, xlab='Graph',
ylab='',pch=nums, axes=T)
 
 
Thanks in advance for your help.
 
~Nick



From julian.taylor at adelaide.edu.au  Tue Jun 29 07:06:31 2004
From: julian.taylor at adelaide.edu.au (Julian Taylor)
Date: Tue, 29 Jun 2004 14:36:31 +0930
Subject: [R] Numbers as symbols
References: <20040629043701.21283.qmail@web50907.mail.yahoo.com>
Message-ID: <40E0F8D7.20B8B856@adelaide.edu.au>



Nick Drew wrote:
> 
> I want to use question numbers from my survey data
> (fake data below) as markers in a scatterplot. I'm
> using "as.character" to convert question numbers to
> characters. However, plot truncates the 0 (zero) off
> of question 10. How can I get the ending zero so I can
> add questions 11 - 20?
> 
> # Sample code below
> Question <- c(1,2,3,4,5,6,7,8,9,10)
> Performance <-
> c(3.5,3.6,3.7,3.8,3.9,4,4.1,4.2,4.3,4.4)
> Importance <-
> c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.99)
> Survey <-
> data.frame(cbind(Question,Performance,Importance))

Change the plot commands to this:

> nums <-as.character(Question)
> plot(Performance, Importance, xlab='Graph', ylab='', axes=T, type = "n")
> text(Performance, Importance, nums)

The inclusion of the rest of the question numbers, 11-20, can be done
identically.

hth,
Julian

-- 
---
Julian Taylor			phone: +61 8 8303 6751
ARC Research Associate            fax: +61 8 8303 6761
BiometricsSA,                  mobile: +61 4 1638 8180  
University of Adelaide/SARDI    email: julian.taylor at adelaide.edu.au
Private Mail Bag 1                www:
http://www.BiometricsSA.adelaide.edu.au
Glen Osmond SA 5064

"There is no spoon."   -- Orphan boy  
---



From umalvarez at fata.unam.mx  Tue Jun 29 07:22:39 2004
From: umalvarez at fata.unam.mx (Ulises Mora Alvarez)
Date: Tue, 29 Jun 2004 00:22:39 -0500 (CDT)
Subject: [R] R via ssh login on OS X?
In-Reply-To: <Pine.OSF.4.58.0406281526200.252750@odin.mdacc.tmc.edu>
Message-ID: <Pine.LNX.4.44.0406282332030.2577-100000@athena.fata.unam.mx>

Hi!

If you are trying to log in from another Mac to the G5 there are some 
details to bear in mind, though. If you are indeed trying from a Mac, I'd 
suggest you to launch your local X server; then, from an xterm 'ssh -X...' 
to the G5. Of course, if the sshd on the G5 is configured so that its  
/etc/sshd_config says 'X11Forwarding no' you'll be not able to use the X11 
device for graphics; but you can search for a solution on the list 
files.  

Good look.

On Mon, 28 Jun 2004, Paul Roebuck wrote:

> On Mon, 28 Jun 2004, James Howison wrote:
> 
> > I have an ssh only login to a G5 on which I am hoping to run some
> > analyses.  The situation is complicated by the fact that the computer's
> > owner is away for the summer (and thus also only has shell login).
> >
> > R is installed and there is a symlink to /usr/local/bin/R but when I
> > try to launch it I get:
> >
> > [jhowison at euro]$ R
> > kCGErrorRangeCheck : Window Server communications from outside of
> > session allowed for root and console user only
> > INIT_Processeses(), could not establish the default connection to the
> > WindowServer.Abort trap
> >
> > I though, ah ha, I need to tell it not to use the GUI but to no avail:
> >
> > [jhowison at euro]$ R --gui=none
> > kCGErrorRangeCheck : Window Server communications from outside of
> > session allowed for root and console user only
> > INIT_Processeses(), could not establish the default connection to the
> > WindowServer.Abort trap
> >
> > I'm embarrassed to say that I'm writing to the list without having the
> > latest version installed---because I can't install it at the moment.  I
> > am using R 1.8.1.  I have tried to compile the latest from source but
> > there is no F77 compiler. I thought I'd ask around before going down
> > the "put local dependencies in the home folder" to compile this route
> > (any hints on doing that would be great though) ...
> >
> > Can other people get R command-line to work with logged in remotely via
> > ssh?  Any hints?
> > Is this something that is fixed in more recent versions?
> >
> > I think I can see one other route:  getting the computer's owner to
> > install fink and their version remotely ... but I'm open to all "don't
> > bother the professor when he's on holiday" options ...
> 
> I suffered similarly attempting to run R via CGI; I never found
> a workaround for remote access (also running 1.8.1 with Panther).
> Seemed to have something to do with running an application requiring
> access to graphics but not being the user currently "owning" the dock.
> I did not determine if the limitation was due to R implementation or
> operating system software.
> 
> F77 not necessary; use 'f2c' instead. But don't bother with Fink since
> it's not necessary to build it. No 'sudo' access either? Is the user
> still logged in (screenlocked) or are you just lacking administrative
> access?
> 
> ----------------------------------------------------------
> SIGSIG -- signature too long (core dumped)
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Ulises M. Alvarez
LAB. DE ONDAS DE CHOQUE
FISICA APLICADA Y TECNOLOGIA AVANZADA
UNAM
umalvarez at fata.unam.mx



From p.connolly at hortresearch.co.nz  Tue Jun 29 07:34:35 2004
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Tue, 29 Jun 2004 17:34:35 +1200
Subject: [R] Numbers as symbols
In-Reply-To: <20040629043701.21283.qmail@web50907.mail.yahoo.com>; from
	drewbrewit@yahoo.com on Mon, Jun 28, 2004 at 09:37:01PM -0700
References: <20040629043701.21283.qmail@web50907.mail.yahoo.com>
Message-ID: <20040629173435.Y11533@hortresearch.co.nz>

On Mon, 28-Jun-2004 at 09:37PM -0700, Nick Drew wrote:

|> I want to use question numbers from my survey data
|> (fake data below) as markers in a scatterplot. I'm
|> using "as.character" to convert question numbers to
|> characters. However, plot truncates the 0 (zero) off
|> of question 10. How can I get the ending zero so I can
|> add questions 11 - 20?
|>  
|> # Sample code below
|> Question <- c(1,2,3,4,5,6,7,8,9,10)
|> Performance <-
|> c(3.5,3.6,3.7,3.8,3.9,4,4.1,4.2,4.3,4.4)
|> Importance <-
|> c(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.99)
|> Survey <-
|> data.frame(cbind(Question,Performance,Importance))
|>  
|> nums <-as.character(Question)
|> plot(Performance, Importance, xlab='Graph',
|> ylab='',pch=nums, axes=T)

Try this instead:

plot(Performance, Importance, xlab='Graph',
ylab='',pch=" ", axes=T, adj = 0)

text(Performance, Importance, text=nums)

HTH

-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From jhowison at syr.edu  Tue Jun 29 09:04:36 2004
From: jhowison at syr.edu (James Howison)
Date: Tue, 29 Jun 2004 03:04:36 -0400
Subject: [R] R via ssh login on OS X?
In-Reply-To: <Pine.LNX.4.44.0406282332030.2577-100000@athena.fata.unam.mx>
References: <Pine.LNX.4.44.0406282332030.2577-100000@athena.fata.unam.mx>
Message-ID: <94DDCC7E-C99A-11D8-BA01-00306579408C@syr.edu>


On Jun 29, 2004, at 1:22 AM, Ulises Mora Alvarez wrote:

> Hi!
>
> If you are trying to log in from another Mac to the G5 there are some
> details to bear in mind, though. If you are indeed trying from a Mac, 
> I'd
> suggest you to launch your local X server; then, from an xterm 'ssh 
> -X...'
> to the G5. Of course, if the sshd on the G5 is configured so that its
> /etc/sshd_config says 'X11Forwarding no' you'll be not able to use the 
> X11
> device for graphics; but you can search for a solution on the list
> files.

That's good thinking.  That hadn't occurred to me and would be great 
for the graphical stuff.  Goes to show that Xwindows has the right idea 
for networked graphics while aqua is hopeless in that regard.

I don't think that this problem happens in R-1.9.1 because if I ssh 
into my laptop from a remote box as a non-logged-in user R behaves 
perfectly on the commandline.  Or maybe the install on the G5 is 
fubared.

Happily I have managed to solve my immediate problem on the G5 by 
compiling a copy of R in my home directory.  This wasn't the easiest 
primarily because I didn't have f2c installed (and because I don't have 
root I couldn't put it in the normal place).  I'm going to say how I 
did it in case this is handy for others (frankly I hope others don't 
have to go through this ;)

I grabbed the f2c code from 
ftp://netlib.bell-labs.com:21/netlib/f2c/src.tar
and libf2c from http://www.netlib.org/f2c/libf2c.zip

Both built ok.  I moved the f2c executable, f2c.h and libf2c.a into 
~/f2c.  Don't forget to run ranlib over libf2c

I set the environment variables:

LDFLAGS=-L$HOME/f2c/
CPPFLAGS=-I$HOME/f2c/   (for some reason the --includedir just didn't 
seem to work ...)

(had to remember to do this before configure)

then did

./configure --prefix=$HOME/Rinstall/ --enable-R-framework=no 
--with-x=no --with-lapack=no

and then

make

This basically worked but for some reason lapack was still trying to 
build and that was failing, so I deleted it from the appropriate 
makefile and the rest of the compile went fine.  The lapack confusion 
stopped some of the recommended modules from building but I didn't need 
those (just sna which built fine from CRAN).

I didn't do the actual install but am just using the full path to R.  
It is working fine in command-line mode now and the calculations are 
running as I type.

I didn't test this but I did also read that people are able to get 
around the "need graphical launching access" by using OS X fast user 
switching.

Thanks everyone!

--J

>
> Good look.
>
> On Mon, 28 Jun 2004, Paul Roebuck wrote:
>
>> On Mon, 28 Jun 2004, James Howison wrote:
>>
>>> I have an ssh only login to a G5 on which I am hoping to run some
>>> analyses.  The situation is complicated by the fact that the 
>>> computer's
>>> owner is away for the summer (and thus also only has shell login).
>>>
>>> R is installed and there is a symlink to /usr/local/bin/R but when I
>>> try to launch it I get:
>>>
>>> [jhowison at euro]$ R
>>> kCGErrorRangeCheck : Window Server communications from outside of
>>> session allowed for root and console user only
>>> INIT_Processeses(), could not establish the default connection to the
>>> WindowServer.Abort trap
>>>
>>> I though, ah ha, I need to tell it not to use the GUI but to no 
>>> avail:
>>>
>>> [jhowison at euro]$ R --gui=none
>>> kCGErrorRangeCheck : Window Server communications from outside of
>>> session allowed for root and console user only
>>> INIT_Processeses(), could not establish the default connection to the
>>> WindowServer.Abort trap
>>>
>>> I'm embarrassed to say that I'm writing to the list without having 
>>> the
>>> latest version installed---because I can't install it at the moment. 
>>>  I
>>> am using R 1.8.1.  I have tried to compile the latest from source but
>>> there is no F77 compiler. I thought I'd ask around before going down
>>> the "put local dependencies in the home folder" to compile this route
>>> (any hints on doing that would be great though) ...
>>>
>>> Can other people get R command-line to work with logged in remotely 
>>> via
>>> ssh?  Any hints?
>>> Is this something that is fixed in more recent versions?
>>>
>>> I think I can see one other route:  getting the computer's owner to
>>> install fink and their version remotely ... but I'm open to all 
>>> "don't
>>> bother the professor when he's on holiday" options ...
>>
>> I suffered similarly attempting to run R via CGI; I never found
>> a workaround for remote access (also running 1.8.1 with Panther).
>> Seemed to have something to do with running an application requiring
>> access to graphics but not being the user currently "owning" the dock.
>> I did not determine if the limitation was due to R implementation or
>> operating system software.
>>
>> F77 not necessary; use 'f2c' instead. But don't bother with Fink since
>> it's not necessary to build it. No 'sudo' access either? Is the user
>> still logged in (screenlocked) or are you just lacking administrative
>> access?
>>
>> ----------------------------------------------------------
>> SIGSIG -- signature too long (core dumped)
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>>
>
> -- 
> Ulises M. Alvarez
> LAB. DE ONDAS DE CHOQUE
> FISICA APLICADA Y TECNOLOGIA AVANZADA
> UNAM
> umalvarez at fata.unam.mx
>
>
>
>
>
--James
+1 315 395 4056



From ripley at stats.ox.ac.uk  Tue Jun 29 09:43:35 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 29 Jun 2004 08:43:35 +0100 (BST)
Subject: [R] R via ssh login on OS X?
In-Reply-To: <94DDCC7E-C99A-11D8-BA01-00306579408C@syr.edu>
Message-ID: <Pine.LNX.4.44.0406290837170.22082-100000@gannet.stats>

Did you look at the notes on MacOS X in the R-admin manual (as the INSTALL 
file asks)?  That would have told you why lapack failed, and I think you 
should redo your build following the advice there.

On Tue, 29 Jun 2004, James Howison wrote:

[...]

> then did
> 
> ./configure --prefix=$HOME/Rinstall/ --enable-R-framework=no 
> --with-x=no --with-lapack=no

Note 

   --with-blas='-framework vecLib' --with-lapack 

is `strongly recommended', and on some versions of MacOS X `appears to be
the only way to build R'.

> and then
> 
> make
> 
> This basically worked but for some reason lapack was still trying to 
> build and that was failing, so I deleted it from the appropriate 
> makefile and the rest of the compile went fine.  The lapack confusion 
> stopped some of the recommended modules from building but I didn't need 
> those (just sna which built fine from CRAN).

[...]

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From wolski at molgen.mpg.de  Tue Jun 29 09:44:39 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Tue, 29 Jun 2004 09:44:39 +0200
Subject: [R] camberra distance?
Message-ID: <200406290944390154.05657F06@mail.math.fu-berlin.de>

Hi!

Its not an R specific question but had no idea where to ask elsewhere.

Does anyone know the orginal reference to the CAMBERA  DISTANCE?

Eryk.

Ps.:
I knew that its an out of topic question (sorry).
Can anyone reccomend a mailing list where such questions are in topic?



From Matthias.Templ at statistik.gv.at  Tue Jun 29 10:27:24 2004
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Tue, 29 Jun 2004 10:27:24 +0200
Subject: [R] Calculate correctly, but gives an error message
Message-ID: <83536658864BC243BE3C06D7E936ABD50153A5FA@xchg1.statistik.local>

Hi!

I will calculate sum??s in the following way:
E.g.: 


a <- rpois(100,20)
b <- rpois(100,5)

x <- data.frame(cbind(a,b))

# the sum??s should be calculated based on a.

attach(x) 

sort.nace <- unique(sort(x[,1]))

sum1 <- matrix(ncol=1, nrow=length(sort.nace))

# I calculate the sum of all values of b, which have the same category in a. Eg.:

sum1[1,] <- sum(subset(x, a==sort.nace[1], select=b), na.rm=TRUE)

# or:

sum1[5,] <- sum(subset(x, a==sort.nace[5], select=b), na.rm=TRUE)

# all is ok.
# but when I do it in a loop, I get an error message.

sum1 <- matrix(ncol=1, nrow=length(sort.nace))
for(i in 1:length(nace)){
	sum1[i,] <- sum(subset(x, a==sort.nace[i], select=b), na.rm=TRUE)
	}
#Error in Summary.data.frame(..., na.rm = na.rm) : 
#        only defined on a data frame with all numeric or complex variables

# but sum1 was correct calculated(!):

# sum1
#       [,1]
#  [1,]    3
#  [2,]    4
#  [3,]    8
#  [4,]    7
#  [5,]   41
#  [6,]   57
#  [7,]   38
#  [8,]   15
#  [9,]   44
# [10,]   72
# [11,]   57
# [12,]   27
# [13,]   12
# [14,]   24
# [15,]   29
# [16,]   16
# [17,]   19
# [18,]    3
# [19,]    4
# [20,]    8
# [21,]    6

The variables of x are of class numeric. 
My function, which should calculate these sum??s, does not work, because of this error.

Can anybody please tell me, what I??m doing wrong?

(I think it would be better to write a sapply instead of the loop, but I have two functions (sum and subset) and it is very hard for me to use the sapply here correctly)

Thanks,
Matthias



From bitwrit at ozemail.com.au  Tue Jun 29 10:42:35 2004
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Tue, 29 Jun 2004 18:42:35 +1000
Subject: [R] A strange question on probability
In-Reply-To: <BAY12-F125cPuIdVKRt0001db0d@hotmail.com>
References: <BAY12-F125cPuIdVKRt0001db0d@hotmail.com>
Message-ID: <20040629083728.BZXR18361.smta10.mail.ozemail.net@there>

On Tuesday 29 June 2004 01:48 pm, Steve S wrote:
> Dear All,
>
> I wonder if there is a probability distribution where you can specify when
> a certain event start and finish within a fixed period? For example I might
> specify the number of period to be 5, and a random vector from this
> distribution might give me:
> 0 1 1 1 0
>
> where 1 is always adjacent to each other?
>
> This can never happen: 0 0 1 0 1 for example.
>
Well, I'll have a go. Let's call it the start-finish distribution. We have a  
p (period) and d (duration). As there must be an "off" observation (otherwise 
we don't know the duration), It's fairly easy to enumerate the outcomes for a 
given period:

d	start(s)	finish(f)	count
1	1:n-1	2:n	n-1
2	1:n-2	3:n	n-2
...
n-1	1	n-1	1

Assuming that all outcomes are equally likely, the total number of outcomes 
is:

n(n-1)/2

thus the probability of a given d occurring is:

P[d|n] = 2(n-d)/n(n-1)

The probabilities of s and f over all d are inverse over the values k in 1:n

P[s=k|n] = (n-k-1)/(n-1)
P[f=k|n] = (k-1)/(n-1)

giving, I think, a monotonic function for s and f.

> My apology for this strange question!
>
My apology if this is no use at all.

Jim



From dimitris.rizopoulos at med.kuleuven.ac.be  Tue Jun 29 10:38:34 2004
From: dimitris.rizopoulos at med.kuleuven.ac.be (Dimitris Rizopoulos)
Date: Tue, 29 Jun 2004 10:38:34 +0200
Subject: [R] Calculate correctly, but gives an error message
References: <83536658864BC243BE3C06D7E936ABD50153A5FA@xchg1.statistik.local>
Message-ID: <009401c45db4$77efd7c0$ad133a86@www.domain>

Hi Matthias,

I think you just need,

lapply(split(b, a), sum, na.rm=TRUE)

I hope this helps.

Best,
Dimitris

----
Dimitris Rizopoulos
Doctoral Student
Biostatistical Centre
School of Public Health
Catholic University of Leuven

Address: Kapucijnenvoer 35, Leuven, Belgium
Tel: +32/16/396887
Fax: +32/16/337015
Web: http://www.med.kuleuven.ac.be/biostat/
     http://www.student.kuleuven.ac.be/~m0390867/dimitris.htm



----- Original Message ----- 
From: "TEMPL Matthias" <Matthias.Templ at statistik.gv.at>
To: <R-help at stat.math.ethz.ch>
Sent: Tuesday, June 29, 2004 10:27 AM
Subject: [R] Calculate correctly, but gives an error message


> Hi!
>
> I will calculate sum??s in the following way:
> E.g.:
>
>
> a <- rpois(100,20)
> b <- rpois(100,5)
>
> x <- data.frame(cbind(a,b))
>
> # the sum??s should be calculated based on a.
>
> attach(x)
>
> sort.nace <- unique(sort(x[,1]))
>
> sum1 <- matrix(ncol=1, nrow=length(sort.nace))
>
> # I calculate the sum of all values of b, which have the same
category in a. Eg.:
>
> sum1[1,] <- sum(subset(x, a==sort.nace[1], select=b), na.rm=TRUE)
>
> # or:
>
> sum1[5,] <- sum(subset(x, a==sort.nace[5], select=b), na.rm=TRUE)
>
> # all is ok.
> # but when I do it in a loop, I get an error message.
>
> sum1 <- matrix(ncol=1, nrow=length(sort.nace))
> for(i in 1:length(nace)){
> sum1[i,] <- sum(subset(x, a==sort.nace[i], select=b), na.rm=TRUE)
> }
> #Error in Summary.data.frame(..., na.rm = na.rm) :
> #        only defined on a data frame with all numeric or complex
variables
>
> # but sum1 was correct calculated(!):
>
> # sum1
> #       [,1]
> #  [1,]    3
> #  [2,]    4
> #  [3,]    8
> #  [4,]    7
> #  [5,]   41
> #  [6,]   57
> #  [7,]   38
> #  [8,]   15
> #  [9,]   44
> # [10,]   72
> # [11,]   57
> # [12,]   27
> # [13,]   12
> # [14,]   24
> # [15,]   29
> # [16,]   16
> # [17,]   19
> # [18,]    3
> # [19,]    4
> # [20,]    8
> # [21,]    6
>
> The variables of x are of class numeric.
> My function, which should calculate these sum??s, does not work,
because of this error.
>
> Can anybody please tell me, what I??m doing wrong?
>
> (I think it would be better to write a sapply instead of the loop,
but I have two functions (sum and subset) and it is very hard for me
to use the sapply here correctly)
>
> Thanks,
> Matthias
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Tue Jun 29 10:41:34 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 29 Jun 2004 09:41:34 +0100 (BST)
Subject: [R] Calculate correctly, but gives an error message
In-Reply-To: <83536658864BC243BE3C06D7E936ABD50153A5FA@xchg1.statistik.local>
Message-ID: <Pine.LNX.4.44.0406290934100.22206-100000@gannet.stats>

You are using an undefined object `nace'.  If I use sort.nace, it works

> sum1 <- matrix(ncol=1, nrow=length(sort.nace))
> for(i in 1:length(sort.nace)){  # not length(nace)
+         sum1[i,] <- sum(subset(x, a==sort.nace[i], select=b), 
na.rm=TRUE)
+         }
> 

I think you were trying to access rows beyond your matrix, so the error 
occurred after sum1 was filled.

I have little idea what you are trying to do: might it be tapply(x$b, a,
sum)?  At least that gives the same answer as the corrected code.


On Tue, 29 Jun 2004, TEMPL Matthias wrote:

> Hi!
> 
> I will calculate sum??s in the following way:
> E.g.: 
> 
> 
> a <- rpois(100,20)
> b <- rpois(100,5)
> 
> x <- data.frame(cbind(a,b))
> 
> # the sum??s should be calculated based on a.
> 
> attach(x) 
> 
> sort.nace <- unique(sort(x[,1]))
> 
> sum1 <- matrix(ncol=1, nrow=length(sort.nace))
> 
> # I calculate the sum of all values of b, which have the same category in a. Eg.:
> 
> sum1[1,] <- sum(subset(x, a==sort.nace[1], select=b), na.rm=TRUE)
> 
> # or:
> 
> sum1[5,] <- sum(subset(x, a==sort.nace[5], select=b), na.rm=TRUE)
> 
> # all is ok.
> # but when I do it in a loop, I get an error message.
> 
> sum1 <- matrix(ncol=1, nrow=length(sort.nace))
> for(i in 1:length(nace)){
> 	sum1[i,] <- sum(subset(x, a==sort.nace[i], select=b), na.rm=TRUE)
> 	}
> #Error in Summary.data.frame(..., na.rm = na.rm) : 
> #        only defined on a data frame with all numeric or complex variables
> 
> # but sum1 was correct calculated(!):
> 
> # sum1
> #       [,1]
> #  [1,]    3
> #  [2,]    4
> #  [3,]    8
> #  [4,]    7
> #  [5,]   41
> #  [6,]   57
> #  [7,]   38
> #  [8,]   15
> #  [9,]   44
> # [10,]   72
> # [11,]   57
> # [12,]   27
> # [13,]   12
> # [14,]   24
> # [15,]   29
> # [16,]   16
> # [17,]   19
> # [18,]    3
> # [19,]    4
> # [20,]    8
> # [21,]    6
> 
> The variables of x are of class numeric. 
> My function, which should calculate these sum??s, does not work, because of this error.
> 
> Can anybody please tell me, what I??m doing wrong?
> 
> (I think it would be better to write a sapply instead of the loop, but I have two functions (sum and subset) and it is very hard for me to use the sapply here correctly)
> 
> Thanks,
> Matthias
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From wolski at molgen.mpg.de  Tue Jun 29 10:55:01 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Tue, 29 Jun 2004 10:55:01 +0200
Subject: [R] camberra distance?
In-Reply-To: <41FA45064D9534448E73B46B367F71F10E725E@exwa2-per.wa.csiro.au>
References: <41FA45064D9534448E73B46B367F71F10E725E@exwa2-per.wa.csiro.au>
Message-ID: <200406291055010750.05A5F45E@mail.math.fu-berlin.de>

Thanks Mark.

Yes I mean canberra. 

Searching for canberra camberra by google I observed the following.
Searching for caMberra you will find a paper from 1997 where they write camberra instead of canberra dissimilarity for the meassure defined sum(|x_i - y_i| / |x_i + y_i|). Meanwhile there are plenty of articles on the net which  reference this paper from 1997 and write caMberra instead of canberra.  
May be because it is much harder to find an article about canberra distance using google (because of the city). 
A quite assertive argument to use distinctive names and to publish papers in journals which are free, online and can be searched by google.


Eryk


*********** REPLY SEPARATOR  ***********

On 29.06.2004 at 15:51 Mark.Palmer at csiro.au wrote:

>maybe you mean 'Canberra'?, if so it might have come from work in csiro
>in canberra back in the 60's/70's? Look for Lance & Williams 1967 ,
>possibly. Aust. Comput. J. 1, 15-20
>
>
>Mark Palmer				
>Environmetrics Monitoring for Management		
>CSIRO Mathematical and Information Sciences	
>Private bag 5, Wembley, Western Australia, 6913			
>Phone 		61-8-9333-6293
>Mobile              0427-50-2353
>Fax:		61-8-9333-6121
>Email:		Mark.Palmer at csiro.au 
>URL:		www.cmis.csiro.au/envir
>
>
>
>-----Original Message-----
>From: r-help-bounces at stat.math.ethz.ch
>[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Wolski
>Sent: Tuesday, 29 June 2004 3:45 PM
>To: R Help Mailing List
>Subject: [R] camberra distance?
>
>
>Hi!
>
>Its not an R specific question but had no idea where to ask elsewhere.
>
>Does anyone know the orginal reference to the CAMBERA  DISTANCE?
>
>Eryk.
>
>Ps.:
>I knew that its an out of topic question (sorry).
>Can anyone reccomend a mailing list where such questions are in topic?
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From Achim.Zeileis at wu-wien.ac.at  Tue Jun 29 11:09:34 2004
From: Achim.Zeileis at wu-wien.ac.at (Achim Zeileis)
Date: Tue, 29 Jun 2004 11:09:34 +0200
Subject: [R] strucchange-esque inference for glms ?
In-Reply-To: <1088481584.40e0e930ded1e@webmail.fas.harvard.edu>
References: <1088481584.40e0e930ded1e@webmail.fas.harvard.edu>
Message-ID: <20040629110934.6732c4a2.Achim.Zeileis@wu-wien.ac.at>

Alexis:

> according to the strucchange package .pdf, "all procedures in this
> package are concerned with testing or assessing deviations from
> stability in the classical linear regression model."
>
> i'd like to test/assess deviations from stability in the Poisson
> model.
> 
> is there a way to modify the strucchange package to suit my purposes,
> or should i use be using another package,   or is this a tough nut to
> crack? :)

As of version 1.2-0 strucchange supports tests for parameter
instability in much more general models including GLMs. A simple example
would be

R> library(strucchange)
R> data(BostonHomicide)
R> mcus <- gefp(homicides ~ population, family = poisson, fit = glm,
                data = BostonHomicide, vcov = kernHAC)
R> plot(mcus)
R> sctest(mcus)

See our technical report "Generalized M-fluctuation tests for Parameter
Instability" (linked from my web page) for the theory behind it.

> my application is detecting the onset of a flu outbreak as new daily
> data trickles in from each morning from local hospitals.  seems to me
> like the same sort of inferential goal that strucchange refers to as
> "monitoring of structural change."

In principile the theory established in the report above could also be
applied to monitoring, but I have neither worked the theory out nor
implemented a function which could handle monitoring in GLMs. But you
can contact me off-list if you are interested in this.

Best,
Achim



From dmb at mrc-dunn.cam.ac.uk  Tue Jun 29 12:04:18 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Tue, 29 Jun 2004 11:04:18 +0100 (BST)
Subject: [R] Several PCA questions...
Message-ID: <Pine.LNX.4.21.0406291051280.14774-100000@mail.mrc-dunn.cam.ac.uk>


Hi, I am doing PCA on several columns of data in a data.frame.

I am interested in particular rows of data which may have a particular
combination of 'types' of column values (without any pre-conception of
what they may be).

I do the following...

# My data table.
allDat <- read.table("big_select_thresh_5", header=1)

# Where some rows look like this...
# PDB     SUNID1  SUNID2  AA      CH      IPCA    PCA     IBB     BB
# 3sdh    14984   14985   6       10      24      24      93      116
# 3hbi    14986   14987   6       10      20      22      94      117
# 4sdh    14988   14989   6       10      20      20      104     122

# NB First three columns = row ID, last 6 = variables

attach(allDat)

# My columns of interest (variables).
part <- data.frame(AA,CH,IPCA,PCA,IBB,BB)

pc <- princomp(part)

plot(pc)

The above plot shows that 95% of the variance is due to the first
'Component' (which I assume is AA).

i.e. All the variables behave in quite much the same way.

I then did ...


biplot(pc)

Which showed some outliers with a numeric ID - How do I get back my old 3
part ID used in allDat?

In the above plot I saw all the variables (correctly named) pointing in
more or less the same direction (as shown by the variance). I then did the
following...

postscript(file="test.ps",paper="a4")

biplot(pc)

dev.off()

However, looking at test.ps shows that the arrows are missing (using
ggv)... Hmmm, they come back when I pstoimg then xv... never mind.


Finally, I would like to make a contour plot of the above biplot, is this
possible? (or even a good way to present the data?

Thanks very much for any feedback, 

Dan.



From dmb at mrc-dunn.cam.ac.uk  Tue Jun 29 12:19:22 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Tue, 29 Jun 2004 11:19:22 +0100 (BST)
Subject: [R] camberra distance?
In-Reply-To: <200406290944390154.05657F06@mail.math.fu-berlin.de>
Message-ID: <Pine.LNX.4.21.0406291118540.14774-100000@mail.mrc-dunn.cam.ac.uk>

On Tue, 29 Jun 2004, Wolski wrote:

>Hi!
>
>Its not an R specific question but had no idea where to ask elsewhere.
>
>Does anyone know the orginal reference to the CAMBERA  DISTANCE?
>
>Eryk.
>
>Ps.:
>I knew that its an out of topic question (sorry).
>Can anyone reccomend a mailing list where such questions are in topic?

sci.stat.consult (applied statistics and consulting) and 
sci.stat.math (mathematical stat and probability)

Both news groups.


>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From spencer.graves at pdf.com  Tue Jun 29 12:19:09 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 29 Jun 2004 03:19:09 -0700
Subject: [R] A strange question on probability
In-Reply-To: <20040629083728.BZXR18361.smta10.mail.ozemail.net@there>
References: <BAY12-F125cPuIdVKRt0001db0d@hotmail.com>
	<20040629083728.BZXR18361.smta10.mail.ozemail.net@there>
Message-ID: <40E1421D.7040602@pdf.com>

      Does the following do what you want: 

rseq <- function(n=1, length.=2){
  s1 <- sample(x=length., size=n, replace=TRUE)
  s2 <- sample(x=length., size=n, replace=TRUE)
  ranseq <- array(0, dim=c(n, length.))
  for(i in 1:n)
    ranseq[i, s1[i]:s2[i]] <- 1
  ranseq
}
set.seed(1)
rseq(9, 5)

 > set.seed(1)
 > rseq(9, 5)
      [,1] [,2] [,3] [,4] [,5]
 [1,]    1    1    0    0    0
 [2,]    0    1    0    0    0
 [3,]    1    1    1    0    0
 [4,]    0    0    0    1    1
 [5,]    0    1    0    0    0
 [6,]    0    0    0    1    1
 [7,]    0    0    1    1    1
 [8,]    0    0    0    1    0
 [9,]    0    0    0    1    1
 >
      hope this helps.  spencer graves

Jim Lemon wrote:

>On Tuesday 29 June 2004 01:48 pm, Steve S wrote:
>  
>
>>Dear All,
>>
>>I wonder if there is a probability distribution where you can specify when
>>a certain event start and finish within a fixed period? For example I might
>>specify the number of period to be 5, and a random vector from this
>>distribution might give me:
>>0 1 1 1 0
>>
>>where 1 is always adjacent to each other?
>>
>>This can never happen: 0 0 1 0 1 for example.
>>
>>    
>>
>Well, I'll have a go. Let's call it the start-finish distribution. We have a  
>p (period) and d (duration). As there must be an "off" observation (otherwise 
>we don't know the duration), It's fairly easy to enumerate the outcomes for a 
>given period:
>
>d	start(s)	finish(f)	count
>1	1:n-1	2:n	n-1
>2	1:n-2	3:n	n-2
>...
>n-1	1	n-1	1
>
>Assuming that all outcomes are equally likely, the total number of outcomes 
>is:
>
>n(n-1)/2
>
>thus the probability of a given d occurring is:
>
>P[d|n] = 2(n-d)/n(n-1)
>
>The probabilities of s and f over all d are inverse over the values k in 1:n
>
>P[s=k|n] = (n-k-1)/(n-1)
>P[f=k|n] = (k-1)/(n-1)
>
>giving, I think, a monotonic function for s and f.
>
>  
>
>>My apology for this strange question!
>>
>>    
>>
>My apology if this is no use at all.
>
>Jim
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>  
>



From baron at psych.upenn.edu  Tue Jun 29 12:33:29 2004
From: baron at psych.upenn.edu (Jonathan Baron)
Date: Tue, 29 Jun 2004 06:33:29 -0400
Subject: [R] Several PCA questions...
In-Reply-To: <Pine.LNX.4.21.0406291051280.14774-100000@mail.mrc-dunn.cam.ac.uk>
References: <Pine.LNX.4.21.0406291051280.14774-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <20040629103329.GA10736@psych>

On 06/29/04 11:04, Dan Bolser wrote:
>
>Hi, I am doing PCA on several columns of data in a data.frame.
>
>I am interested in particular rows of data which may have a particular
>combination of 'types' of column values (without any pre-conception of
>what they may be).
>
>I do the following...
>
># My data table.
>allDat <- read.table("big_select_thresh_5", header=1)
>
># Where some rows look like this...
># PDB     SUNID1  SUNID2  AA      CH      IPCA    PCA     IBB     BB
># 3sdh    14984   14985   6       10      24      24      93      116
># 3hbi    14986   14987   6       10      20      22      94      117
># 4sdh    14988   14989   6       10      20      20      104     122
>
># NB First three columns = row ID, last 6 = variables
>
>attach(allDat)
>
># My columns of interest (variables).
>part <- data.frame(AA,CH,IPCA,PCA,IBB,BB)
>
>pc <- princomp(part)
>
>plot(pc)
>
>The above plot shows that 95% of the variance is due to the first
>'Component' (which I assume is AA).

No.  It is the first principal component, which is some linear
combination of all the variables.  Try loadings(pc).  It sounds
like you need to read up on principal component analysis.

>i.e. All the variables behave in quite much the same way.
>
>I then did ...
>
>
>biplot(pc)
>
>Which showed some outliers with a numeric ID - How do I get back my old 3
>part ID used in allDat?

The numeric ID is taken from the row names of pc.  So, if the IDs
in question are 3 and 5, then alldat[c(3,5),] should work.

>In the above plot I saw all the variables (correctly named) pointing in
>more or less the same direction (as shown by the variance). I then did the
>following...
>
>postscript(file="test.ps",paper="a4")
>
>biplot(pc)
>
>dev.off()
>
>However, looking at test.ps shows that the arrows are missing (using
>ggv)... Hmmm, they come back when I pstoimg then xv... never mind.

I get red arrows for the components in both the original graph
and the ps output (R 1.9.1, Fedora Core 2).  This may be a
platform-specific problem or one specific to ggv.  I have neither
ggv nor pstoimg.  (But xv and gv both work.)

>Finally, I would like to make a contour plot of the above biplot, is this
>possible? (or even a good way to present the data?

No idea how to do this or why you would want it.

Jon
-- 
Jonathan Baron, Professor of Psychology, University of Pennsylvania
Home page:            http://www.sas.upenn.edu/~baron
R search page:        http://finzi.psych.upenn.edu/



From ripley at stats.ox.ac.uk  Tue Jun 29 12:36:01 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 29 Jun 2004 11:36:01 +0100 (BST)
Subject: [R] Several PCA questions...
In-Reply-To: <Pine.LNX.4.21.0406291051280.14774-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <Pine.LNX.4.44.0406291130380.22300-100000@gannet.stats>

On Tue, 29 Jun 2004, Dan Bolser wrote:

> Hi, I am doing PCA on several columns of data in a data.frame.
> 
> I am interested in particular rows of data which may have a particular
> combination of 'types' of column values (without any pre-conception of
> what they may be).
> 
> I do the following...
> 
> # My data table.
> allDat <- read.table("big_select_thresh_5", header=1)
> 
> # Where some rows look like this...
> # PDB     SUNID1  SUNID2  AA      CH      IPCA    PCA     IBB     BB
> # 3sdh    14984   14985   6       10      24      24      93      116
> # 3hbi    14986   14987   6       10      20      22      94      117
> # 4sdh    14988   14989   6       10      20      20      104     122
> 
> # NB First three columns = row ID, last 6 = variables
> 
> attach(allDat)
> 
> # My columns of interest (variables).
> part <- data.frame(AA,CH,IPCA,PCA,IBB,BB)
> 
> pc <- princomp(part)

Do you really want an unscaled PCA on that data set?  Looks unlikely (but 
then two of the columns are constant in the sample, which is also 
worrying).

> plot(pc)
> 
> The above plot shows that 95% of the variance is due to the first
> 'Component' (which I assume is AA).

No, it is the first (principal) component.  You did ask for P>C<A!

> i.e. All the variables behave in quite much the same way.

Or you failed to scale the data so one dominates.

> I then did ...
> 
> 
> biplot(pc)
> 
> Which showed some outliers with a numeric ID - How do I get back my old 3
> part ID used in allDat?

Set row names on your data frame.  Like almost all of R, it is the row 
names of a data frame that are used for labelling, and you did not give 
any so you got numbers.

> In the above plot I saw all the variables (correctly named) pointing in
> more or less the same direction (as shown by the variance). I then did the
> following...
> 
> postscript(file="test.ps",paper="a4")
> 
> biplot(pc)
> 
> dev.off()
> 
> However, looking at test.ps shows that the arrows are missing (using
> ggv)... Hmmm, they come back when I pstoimg then xv... never mind.

So ggv is unreliable, perhaps cannot cope with colours?

> Finally, I would like to make a contour plot of the above biplot, is this
> possible? (or even a good way to present the data?

What do you propose to represent by the contours?  Biplots have a 
well-defined interpretation in terms of distances and angles.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Jun 29 12:43:20 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 29 Jun 2004 11:43:20 +0100 (BST)
Subject: [R] camberra distance?
In-Reply-To: <Pine.LNX.4.21.0406291118540.14774-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <Pine.LNX.4.44.0406291136470.22300-100000@gannet.stats>

You may find it easier to search for `canberra distance', if that is
really what you intend (and your subject line and text differ in spelling
anyway).  See ?dist, and Google results for `cambera distance', which both
shows this a fairly common mispelling of the capital of Australia, and
suggests the correct spelling.

On Tue, 29 Jun 2004, Dan Bolser wrote:

> On Tue, 29 Jun 2004, Wolski wrote:
> 
> >Hi!
> >
> >Its not an R specific question but had no idea where to ask elsewhere.
> >
> >Does anyone know the orginal reference to the CAMBERA  DISTANCE?
> >
> >Eryk.
> >
> >Ps.:
> >I knew that its an out of topic question (sorry).
> >Can anyone reccomend a mailing list where such questions are in topic?
> 
> sci.stat.consult (applied statistics and consulting) and 
> sci.stat.math (mathematical stat and probability)
> 
> Both news groups.

I do think readers of such groups (and this one) expect Google etc to be 
used first.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bitwrit at ozemail.com.au  Tue Jun 29 12:14:09 2004
From: bitwrit at ozemail.com.au (Jim Lemon)
Date: Tue, 29 Jun 2004 20:14:09 +1000
Subject: [R] A strange question on probability
Message-ID: <20040629112202.DPNN12805.smta04.mail.ozemail.net@there>

Oops, the last two should have been:

P[s=k|n] = (n-k-1)/n(n-1)
P[f=k|n] = (k-1)/n(n-1)

Jim



From rkoenker at uiuc.edu  Tue Jun 29 14:29:41 2004
From: rkoenker at uiuc.edu (roger koenker)
Date: Tue, 29 Jun 2004 07:29:41 -0500
Subject: [R] R via ssh login on OS X?
In-Reply-To: <94DDCC7E-C99A-11D8-BA01-00306579408C@syr.edu>
References: <Pine.LNX.4.44.0406282332030.2577-100000@athena.fata.unam.mx>
	<94DDCC7E-C99A-11D8-BA01-00306579408C@syr.edu>
Message-ID: <FE8D4B8C-C9C7-11D8-B1B7-000A95A7E3AA@uiuc.edu>

My office G5 running R-devel has no problem with remote logins, either
mine, or my students, so I doubt there is something fatally flawed in
either the OS or R that is a problem.  X11Forwarding is off by default
so this does need to be changed, I believe.  I might add just for a
moment of schadenfreude that Stata's Mac version does seem to
make it impossible to run remotely even though their other unix
versions are happy to do so.


url:	www.econ.uiuc.edu/~roger        	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820

On Jun 29, 2004, at 2:04 AM, James Howison wrote:

>
> On Jun 29, 2004, at 1:22 AM, Ulises Mora Alvarez wrote:
>
>> Hi!
>>
>> If you are trying to log in from another Mac to the G5 there are some
>> details to bear in mind, though. If you are indeed trying from a Mac, 
>> I'd
>> suggest you to launch your local X server; then, from an xterm 'ssh 
>> -X...'
>> to the G5. Of course, if the sshd on the G5 is configured so that its
>> /etc/sshd_config says 'X11Forwarding no' you'll be not able to use 
>> the X11
>> device for graphics; but you can search for a solution on the list
>> files.
>
> That's good thinking.  That hadn't occurred to me and would be great 
> for the graphical stuff.  Goes to show that Xwindows has the right 
> idea for networked graphics while aqua is hopeless in that regard.
>
> I don't think that this problem happens in R-1.9.1 because if I ssh 
> into my laptop from a remote box as a non-logged-in user R behaves 
> perfectly on the commandline.  Or maybe the install on the G5 is 
> fubared.
>
> Happily I have managed to solve my immediate problem on the G5 by 
> compiling a copy of R in my home directory.  This wasn't the easiest 
> primarily because I didn't have f2c installed (and because I don't 
> have root I couldn't put it in the normal place).  I'm going to say 
> how I did it in case this is handy for others (frankly I hope others 
> don't have to go through this ;)
>
> I grabbed the f2c code from 
> ftp://netlib.bell-labs.com:21/netlib/f2c/src.tar
> and libf2c from http://www.netlib.org/f2c/libf2c.zip
>
> Both built ok.  I moved the f2c executable, f2c.h and libf2c.a into 
> ~/f2c.  Don't forget to run ranlib over libf2c
>
> I set the environment variables:
>
> LDFLAGS=-L$HOME/f2c/
> CPPFLAGS=-I$HOME/f2c/   (for some reason the --includedir just didn't 
> seem to work ...)
>
> (had to remember to do this before configure)
>
> then did
>
> ./configure --prefix=$HOME/Rinstall/ --enable-R-framework=no 
> --with-x=no --with-lapack=no
>
> and then
>
> make
>
> This basically worked but for some reason lapack was still trying to 
> build and that was failing, so I deleted it from the appropriate 
> makefile and the rest of the compile went fine.  The lapack confusion 
> stopped some of the recommended modules from building but I didn't 
> need those (just sna which built fine from CRAN).
>
> I didn't do the actual install but am just using the full path to R.  
> It is working fine in command-line mode now and the calculations are 
> running as I type.
>
> I didn't test this but I did also read that people are able to get 
> around the "need graphical launching access" by using OS X fast user 
> switching.
>
> Thanks everyone!
>
> --J
>
>>
>> Good look.
>>
>> On Mon, 28 Jun 2004, Paul Roebuck wrote:
>>
>>> On Mon, 28 Jun 2004, James Howison wrote:
>>>
>>>> I have an ssh only login to a G5 on which I am hoping to run some
>>>> analyses.  The situation is complicated by the fact that the 
>>>> computer's
>>>> owner is away for the summer (and thus also only has shell login).
>>>>
>>>> R is installed and there is a symlink to /usr/local/bin/R but when I
>>>> try to launch it I get:
>>>>
>>>> [jhowison at euro]$ R
>>>> kCGErrorRangeCheck : Window Server communications from outside of
>>>> session allowed for root and console user only
>>>> INIT_Processeses(), could not establish the default connection to 
>>>> the
>>>> WindowServer.Abort trap
>>>>
>>>> I though, ah ha, I need to tell it not to use the GUI but to no 
>>>> avail:
>>>>
>>>> [jhowison at euro]$ R --gui=none
>>>> kCGErrorRangeCheck : Window Server communications from outside of
>>>> session allowed for root and console user only
>>>> INIT_Processeses(), could not establish the default connection to 
>>>> the
>>>> WindowServer.Abort trap
>>>>
>>>> I'm embarrassed to say that I'm writing to the list without having 
>>>> the
>>>> latest version installed---because I can't install it at the 
>>>> moment.  I
>>>> am using R 1.8.1.  I have tried to compile the latest from source 
>>>> but
>>>> there is no F77 compiler. I thought I'd ask around before going down
>>>> the "put local dependencies in the home folder" to compile this 
>>>> route
>>>> (any hints on doing that would be great though) ...
>>>>
>>>> Can other people get R command-line to work with logged in remotely 
>>>> via
>>>> ssh?  Any hints?
>>>> Is this something that is fixed in more recent versions?
>>>>
>>>> I think I can see one other route:  getting the computer's owner to
>>>> install fink and their version remotely ... but I'm open to all 
>>>> "don't
>>>> bother the professor when he's on holiday" options ...
>>>
>>> I suffered similarly attempting to run R via CGI; I never found
>>> a workaround for remote access (also running 1.8.1 with Panther).
>>> Seemed to have something to do with running an application requiring
>>> access to graphics but not being the user currently "owning" the 
>>> dock.
>>> I did not determine if the limitation was due to R implementation or
>>> operating system software.
>>>
>>> F77 not necessary; use 'f2c' instead. But don't bother with Fink 
>>> since
>>> it's not necessary to build it. No 'sudo' access either? Is the user
>>> still logged in (screenlocked) or are you just lacking administrative
>>> access?
>>>
>>> ----------------------------------------------------------
>>> SIGSIG -- signature too long (core dumped)
>>>
>>> ______________________________________________
>>> R-help at stat.math.ethz.ch mailing list
>>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>>> PLEASE do read the posting guide! 
>>> http://www.R-project.org/posting-guide.html
>>>
>>
>> -- 
>> Ulises M. Alvarez
>> LAB. DE ONDAS DE CHOQUE
>> FISICA APLICADA Y TECNOLOGIA AVANZADA
>> UNAM
>> umalvarez at fata.unam.mx
>>
>>
>>
>>
>>
> --James
> +1 315 395 4056
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From dmb at mrc-dunn.cam.ac.uk  Tue Jun 29 15:51:14 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Tue, 29 Jun 2004 14:51:14 +0100 (BST)
Subject: [R] Several PCA questions...
In-Reply-To: <Pine.LNX.4.44.0406291130380.22300-100000@gannet.stats>
Message-ID: <Pine.LNX.4.21.0406291432050.14774-100000@mail.mrc-dunn.cam.ac.uk>


Thanks Jonathan and Brian for advise, all but for the last point I will
do more background reading. To come back to the last point...


>> Finally, I would like to make a contour plot of the above biplot, is this
>> possible? (or even a good way to present the data?


>Brian said:
>
>What do you propose to represent by the contours?  Biplots have a 
>well-defined interpretation in terms of distances and angles.
>
>Jonathan said:
>
>No idea how to do this or why you would want it.
>


Basically I would like to make a 2d smothed density, represented as a
countour plot. I would like to do this as a crude visual clustering of my 
data points.

i.e. instead of representing data by the row labels in the biplot, I would
like to see just a single dot for each data point. Then I would like to
only see the density of these points in 2d (hence contours).

For example...

x <- rnorm(1000)
y <- rnorm(1000)

plot(x,y)

library("MASS")

z <- kde2d(x,y)

contour(z)

I imagine the above in the context of my biplot, and I would like to see
peaks which represent clusters of datapoints in 2d. However, I don't know
how to get x,y coords from the pc object or the biplot function.


Thanks again for the other tips, I need to read more! I will just
throw one final question out there (perhaps to further highlight my
ignorance).

I thought that a significant factor in my data was the relative magnitude
of the different variables, so I thought about making a new variable for
each distinct pair of existing variables, and setting that new (pariwise)
variable to 1 or 0 depending on the relative magnitude of the two
component variables. Then I do PCA (or some other clustering (or a simple
grouping)) on this new set of variables, and hey presto, I have the
classes of my data points. Just an idea. Any good?

Cheers,
Dan.




>
>



From jazevedo at provide.com.br  Tue Jun 29 15:41:22 2004
From: jazevedo at provide.com.br (Joao Pedro W. de Azevedo)
Date: Tue, 29 Jun 2004 14:41:22 +0100
Subject: [R] discrete hazard rate analysis
Message-ID: <000601c45dde$c45898c0$21a2f080@Lepc204>

Dear R users,

I have more of a statistical/econometric question than straight R one.

I have a data set with the discrete hazard rate of small firms survival on
400 counties over a period of 9 years. This data was generated using census
information from the VAT registration number of each one of these business. 

I would like to analyze the effect of regional factors (deprivation index,
real wages, average schooling, population density, etc) on the variation of
these hazard rates across counties over time.

I've done a search in the economic literature on firm survival and regional
economics, but I could not find any reference that would resemble the type
of data or the problem that I would like to explore. I would like to know if
anyone in the list has any suggestion of references that I might have missed
in economics, or if people on any other fields know of any references of
people looking at any data that might resemble this one (I don't know, but
maybe epidemiology might have regional level data that might look at similar
issues).

Of course I would also like to know which R commands could assist me on this
analysis.

Any suggestions will be much appreciated.

All the very best,

JP


County Region time 1993 1994 1995 1996 1997 1998 1999 2000 2001
a South 6 months 95.0 95.1 95.5 95.7 96.8 96.8 96.9 95.9 98.0
a South 12 months 87.1 87.1 89.7 89.6 92.4 91.7 92.6 90.2 93.9
a South 18 months 79.5 79.8 83.3 83.0 86.4 86.7 85.8 84.7 
a South 24 months 73.3 73.1 77.8 78.0 80.6 81.0 79.6 79.2 
a South 30 months 68.0 67.3 72.8 72.3 75.8 75.1 74.1 
a South 36 months 63.7 62.9 69.0 67.1 70.8 68.7 68.5 
a South 42 months 59.3 59.1 65.4 62.6 65.8 64.2 
a South 48 months 56.1 56.2 61.6 59.1 61.2 59.6 

b South 6 months 94.2 96.0 96.3 96.7 96.5 97.0 97.0 96.1 97.1
b South 12 months 87.2 89.1 90.6 90.5 91.4 91.8 92.1 91.3 92.8
b South 18 months 79.9 82.0 84.2 84.5 85.8 85.8 86.3 86.2 
b South 24 months 73.9 75.9 78.1 79.0 80.5 79.8 80.8 80.4 
b South 30 months 68.2 70.0 73.2 74.2 75.6 74.3 75.8 
b South 36 months 64.0 65.4 69.0 70.3 70.4 69.6 71.0 
b South 42 months 60.2 60.8 65.4 66.0 66.1 64.9 
b South 48 months 56.6 57.5 62.0 61.7 61.7 61.1 

c South 6 months 93.2 94.0 94.6 95.6 95.7 95.8 95.9 96.6 97.2
c South 12 months 84.5 85.8 87.8 89.1 89.6 89.8 90.8 91.6 92.7
c South 18 months 77.2 78.9 80.7 83.3 84.1 83.8 84.1 86.7 
c South 24 months 69.8 72.8 75.1 77.2 78.1 78.0 78.7 80.7 
c South 30 months 63.8 66.3 69.3 71.9 72.9 73.0 73.3 
c South 36 months 59.4 61.7 65.3 67.8 68.5 68.2 68.3 
c South 42 months 55.8 57.3 60.9 63.7 64.0 63.6 
c South 48 months 52.4 53.7 57.6 60.3 59.9 59.0



From dj at research.bell-labs.com  Tue Jun 29 16:05:31 2004
From: dj at research.bell-labs.com (David James)
Date: Tue, 29 Jun 2004 10:05:31 -0400
Subject: [R] R client connection OLAP cube (SQL Analysis Services /
	PivotTable Service)
In-Reply-To: <BAY18-F17kULb3ggKPP0002cbd1@hotmail.com>;
	from olivier_collignon@hotmail.com on Mon, Jun 28, 2004 at
	01:49:37PM -0700
References: <BAY18-F17kULb3ggKPP0002cbd1@hotmail.com>
Message-ID: <20040629100531.A13304@jessie.research.bell-labs.com>

Dear Olivier,

I believe your best bet may be to connect to the database through
some kind of R-COM connection (either Thomas Baier and
Erich Neuwirth's R-(D)COM in CRAN or Duncan Temple Lang's at
http://www.omegahat.org/RDCOMClient).  For instance, the 
ADO MD (ActiveX Data Objects Multi-dimensional) COM object/library
allows you to connect to the OLAP database and query multiple cubes,
their axes and hierarchies, etc.  See the Microsoft Developer Network
(MSDN) for the gory details.

Hope this helps,

--
David

Olivier Collignon wrote:
> I have been doing data analysis/modeling in R, connecting to SQL databases 
> with RODBC (winXP client with R1.9.0 and win2k SQL server 2000).
> 
> I am now trying to leverage some of the OLAP features to keep the data 
> intensive tasks on the DB server side and only keep the analytical tasks 
> within R (optimize use of memory). Is there any package that would allow to 
> connect to OLAP cubes (as a client to SQL Analysis Services PivotTable 
> Service) from an R session (similar to the RODBC package)?
> 
> Can this be done directly with special connection parameters (from R) 
> through the RODBC package or do I have to setup an intermediary XL table 
> (pivottable linked to the OLAP cube) and then connect to the XL data source 
> from R?
> 
> I would appreciate any reference / pointer to OLAP/R configuration 
> instructions. Thanks.
> 
> Olivier Collignon
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From allan at stats.uct.ac.za  Tue Jun 29 16:34:37 2004
From: allan at stats.uct.ac.za (allan clark)
Date: Tue, 29 Jun 2004 16:34:37 +0200
Subject: [R] gambling problem
Message-ID: <40E17DFD.3B0B496A@stats.uct.ac.za>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040629/f8a8b4c1/attachment.pl

From stephane.dray at umontreal.ca  Tue Jun 29 17:00:29 2004
From: stephane.dray at umontreal.ca (Stephane DRAY)
Date: Tue, 29 Jun 2004 11:00:29 -0400
Subject: [R] binding rows from different matrices
Message-ID: <5.2.1.1.0.20040629105435.00b46bc8@magellan.umontreal.ca>

Hello list,
I have 3 matrices with same dimension :
 > veca=matrix(1:25,5,5)
 > vecb=matrix(letters[1:25],5,5)
 > vecc=matrix(LETTERS[1:25],5,5)

I would like to obtain a new matrix composed by alternating rows of these 
different matrices (row 1 of mat 1, row 1 of mat 2, row 1 of mat 3, row 2 
of mat 1.....)

I have found a solution to do it but it is not very pretty and I wonder if 
I can do it in an other way (perhaps with apply ) ?

 > res=matrix(0,1,5)
 > for(i in 1:5)
+ res=rbind(res,veca[i,],vecb[i,],vecc[i,])
 > res=res[-1,]
 > res
       [,1] [,2] [,3] [,4] [,5]
  [1,] "1"  "6"  "11" "16" "21"
  [2,] "a"  "f"  "k"  "p"  "u"
  [3,] "A"  "F"  "K"  "P"  "U"
  [4,] "2"  "7"  "12" "17" "22"
  [5,] "b"  "g"  "l"  "q"  "v"
  [6,] "B"  "G"  "L"  "Q"  "V"
  [7,] "3"  "8"  "13" "18" "23"
  [8,] "c"  "h"  "m"  "r"  "w"
  [9,] "C"  "H"  "M"  "R"  "W"
[10,] "4"  "9"  "14" "19" "24"
[11,] "d"  "i"  "n"  "s"  "x"
[12,] "D"  "I"  "N"  "S"  "X"
[13,] "5"  "10" "15" "20" "25"
[14,] "e"  "j"  "o"  "t"  "y"
[15,] "E"  "J"  "O"  "T"  "Y"
 >

Thanks in advance !

St??phane DRAY
-------------------------------------------------------------------------------------------------- 

D??partement des Sciences Biologiques
Universit?? de Montr??al, C.P. 6128, succursale centre-ville
Montr??al, Qu??bec H3C 3J7, Canada

Tel : 514 343 6111 poste 1233
E-mail : stephane.dray at umontreal.ca
-------------------------------------------------------------------------------------------------- 

Web                                          http://www.steph280.freesurf.fr/



From Isabel.Brito at curie.fr  Tue Jun 29 17:06:12 2004
From: Isabel.Brito at curie.fr (Isabel Brito)
Date: Tue, 29 Jun 2004 17:06:12 +0200
Subject: [R] give PAM  my own medoids
Message-ID: <5.0.2.1.2.20040629165605.02368120@mailhost.curie.fr>


Hello,

When using PAM (partitioning around medoids), I would like to skip the 
build-step and give the fonction my own medoids.

Do you know if it is possible, and how ?

Thank you very much.

Isabel



From Sebastien.Moretti at igs.cnrs-mrs.fr  Tue Jun 29 17:13:59 2004
From: Sebastien.Moretti at igs.cnrs-mrs.fr (Sebastien Moretti)
Date: Tue, 29 Jun 2004 17:13:59 +0200
Subject: [R] [ cor(x, y,use = "all.obs",method = c("spearman")) ]
Message-ID: <200406291713.59173.Sebastien.Moretti@igs.cnrs-mrs.fr>

Hello
I would like to know how cor()  handles ranks when some ranks are ex aequo.
Does it use Spearman Correlation Coefficient with correction of the formula ?
Thanks

-- 
Sebastien MORETTI
Linux User - #327894
CNRS - IGS
31 chemin Joseph Aiguier
13402 Marseille cedex 20, FRANCE
tel. +33 (0)4 91 16 44 55



From maechler at stat.math.ethz.ch  Tue Jun 29 17:19:43 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 29 Jun 2004 17:19:43 +0200
Subject: [R] give PAM  my own medoids
In-Reply-To: <5.0.2.1.2.20040629165605.02368120@mailhost.curie.fr>
References: <5.0.2.1.2.20040629165605.02368120@mailhost.curie.fr>
Message-ID: <16609.34959.998673.783523@gargle.gargle.HOWL>

Bonjour Isabel,

>>>>> "Isabel" == Isabel Brito <Isabel.Brito at curie.fr>
>>>>>     on Tue, 29 Jun 2004 17:06:12 +0200 writes:

    Isabel> Hello,

    Isabel> When using PAM (partitioning around medoids), I
    Isabel> would like to skip the build-step and give the
    Isabel> fonction my own medoids.

    Isabel> Do you know if it is possible, and how ?

unfortunately, it's not yet possible, but --- believe or not ---
this has been on my TODO list for 'cluster' (the package) for a
while now -- and your wish definitely raises the priority!
I want to do some checking for user input errors there, but this
is definitely not so much of work to do...

-> do nag me about it at least once a month till it'done.. ;-)

    Isabel> Thank you very much.

You're welcome,
Martin Maechler, Seminar fuer Statistik ETH Zurich



From rolf at math.unb.ca  Tue Jun 29 17:20:52 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Tue, 29 Jun 2004 12:20:52 -0300 (ADT)
Subject: [R] Different behaviour of unique(), R vs. Splus.
Message-ID: <200406291520.i5TFKqE4010650@erdos.math.unb.ca>

Apologies for the cross-posting, but I thought this snippet of info
might be vaguely interesting to both lists.

I did a ***brief*** search to see if this issue had previously been
discussed and found nothing.  So I thought I'd tell the list about a
difference in behaviour between unique() in R and unique() in Splus
which bit me just now.

I was trying to convert a package from Splus to R and got nonsense
answers in R.  Turned out that within the bowels of the package I was
doing something like

	u <- unique(y)

where y was a matrix of integer values.  In Splus this gives a
(short) vector of unique values.  In R it gives a matrix of the same
dimensionality as y, except that any duplicated rows are eliminated.

(This looks like being very useful --- once you know about it.  And
it was probably mentioned in the R release notes at one time, but, as
Dr. Hook says, ``I was stoned and I missed it.'')

E.g.
	set.seed(42)
	m <- matrix(sample(1:5,20,TRUE),5,4)
	u <- unique(m)

In R ``u'' is identical to ``m''; in Splus ``u'' is vector (of
length 5).

To get what I want in R I simply need to do

	u <- unique(as.vector(y))

Simple, once you know.  Took me a devil of a long time to track down
what was going wrong, but!

					cheers,

						Rolf Turner



From maechler at stat.math.ethz.ch  Tue Jun 29 17:21:47 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 29 Jun 2004 17:21:47 +0200
Subject: [R] Boosting
In-Reply-To: <FB27C4ED7608FC499A15ED5BBC9B3FB602532CF7@mail1.nymc.edu>
References: <FB27C4ED7608FC499A15ED5BBC9B3FB602532CF7@mail1.nymc.edu>
Message-ID: <16609.35083.452375.731692@gargle.gargle.HOWL>


>>>>> "ORIORDAN" == ORIORDAN EDMOND <EDMOND_ORIORDAN at NYMC.EDU>
>>>>>     on Mon, 28 Jun 2004 10:23:24 -0400 writes:

    ORIORDAN> Hi Does anybody have a package/code for Real
    ORIORDAN> Adaboost that works in R?  

Did you try the 'gbm' package from CRAN?

    ORIORDAN> very large binary data set Any help greatly
    ORIORDAN> appreciated cheers ed



From ahirsa at yahoo.com  Tue Jun 29 17:26:35 2004
From: ahirsa at yahoo.com (Ali Hirsa)
Date: Tue, 29 Jun 2004 08:26:35 -0700 (PDT)
Subject: [R] Quantile Regression in R
Message-ID: <20040629152635.20191.qmail@web12102.mail.yahoo.com>

I recently learn about Quantile Regression in R.
I am trying to study two time series (attached) by Quantile Regression in R.
I wrote the following code and do not know how to interpret the lines.
   
What kind of information can I get from them? Correlation for quantiles, 
conditional probabilties (i.e. P(X in Quantile i | Y in Quantile i)) , and etc.
Many thanks in advance for any help.

Best,
Ali

library("quantreg")
#help.start()

Data <- read.table("RESvsMOVE2.dat")
#
x <- Data[,2]
y <- Data[,1]

par(mfrow=c(2,2))

qqnorm(x,main="MOVE Norm Q-Q Plot", xlab="Normal Qunatiles",ylab = "MOVE Quantiles")
qqline(x)

qqnorm(y,main="Residuals Norm Q-Q Plot", xlab="Normal Qunatiles",ylab = "Residuals Quantiles")
qqline(y)

plot(x,y,xlab="MOVE",ylab="Residuals",cex=.5)

xx <- seq(min(x),max(x),.5)

# Just a linear regression
g <- coef(lm(y~x))
yy <- (g[1]+g[2]*(xx))
lines(xx,yy,col="yellow")

taus <- c(.05,.1,.25,.5,.75,.9,.95)

for(tau in taus){
        f <- coef(rq(y~x,tau=tau,method="pfn"))
        yy <- (f[1]+f[2]*(xx))
        if (tau ==.05){
             lines(xx,yy,col="red")
        }
        if (tau ==.95){
             lines(xx,yy,col="green")
        }
        if (tau != .05 & tau != .95){
             lines(xx,yy,col="blue")
        }
}



		
__________________________________



From james.holtman at convergys.com  Tue Jun 29 17:28:58 2004
From: james.holtman at convergys.com (james.holtman@convergys.com)
Date: Tue, 29 Jun 2004 11:28:58 -0400
Subject: [R] binding rows from different matrices
Message-ID: <OF8E98A68E.37DE1454-ON85256EC2.00550478@nd.convergys.com>





Try:

>  veca=matrix(1:25,5,5)
>  vecb=matrix(letters[1:25],5,5)
>  vecc=matrix(LETTERS[1:25],5,5)
> x.1 <- lapply(1:5,function(x)rbind(veca[x,],vecb[x,],vecc[x,]))
> do.call('rbind',x.1)
      [,1] [,2] [,3] [,4] [,5]
 [1,] "1"  "6"  "11" "16" "21"
 [2,] "a"  "f"  "k"  "p"  "u"
 [3,] "A"  "F"  "K"  "P"  "U"
 [4,] "2"  "7"  "12" "17" "22"
 [5,] "b"  "g"  "l"  "q"  "v"
 [6,] "B"  "G"  "L"  "Q"  "V"
 [7,] "3"  "8"  "13" "18" "23"
 [8,] "c"  "h"  "m"  "r"  "w"
 [9,] "C"  "H"  "M"  "R"  "W"
[10,] "4"  "9"  "14" "19" "24"
[11,] "d"  "i"  "n"  "s"  "x"
[12,] "D"  "I"  "N"  "S"  "X"
[13,] "5"  "10" "15" "20" "25"
[14,] "e"  "j"  "o"  "t"  "y"
[15,] "E"  "J"  "O"  "T"  "Y"
>
__________________________________________________________
James Holtman        "What is the problem you are trying to solve?"
Executive Technical Consultant  --  Office of Technology, Convergys
james.holtman at convergys.com
+1 (513) 723-2929


                                                                                                                   
                      Stephane DRAY                                                                                
                      <stephane.dray at umontr        To:       r-help at stat.math.ethz.ch                              
                      eal.ca>                      cc:                                                             
                      Sent by:                     Subject:  [R] binding rows from different matrices              
                      r-help-bounces at stat.m                                                                        
                      ath.ethz.ch                                                                                  
                                                                                                                   
                                                                                                                   
                      06/29/2004 11:00                                                                             
                                                                                                                   
                                                                                                                   




Hello list,
I have 3 matrices with same dimension :
 > veca=matrix(1:25,5,5)
 > vecb=matrix(letters[1:25],5,5)
 > vecc=matrix(LETTERS[1:25],5,5)

I would like to obtain a new matrix composed by alternating rows of these
different matrices (row 1 of mat 1, row 1 of mat 2, row 1 of mat 3, row 2
of mat 1.....)

I have found a solution to do it but it is not very pretty and I wonder if
I can do it in an other way (perhaps with apply ) ?

 > res=matrix(0,1,5)
 > for(i in 1:5)
+ res=rbind(res,veca[i,],vecb[i,],vecc[i,])
 > res=res[-1,]
 > res
       [,1] [,2] [,3] [,4] [,5]
  [1,] "1"  "6"  "11" "16" "21"
  [2,] "a"  "f"  "k"  "p"  "u"
  [3,] "A"  "F"  "K"  "P"  "U"
  [4,] "2"  "7"  "12" "17" "22"
  [5,] "b"  "g"  "l"  "q"  "v"
  [6,] "B"  "G"  "L"  "Q"  "V"
  [7,] "3"  "8"  "13" "18" "23"
  [8,] "c"  "h"  "m"  "r"  "w"
  [9,] "C"  "H"  "M"  "R"  "W"
[10,] "4"  "9"  "14" "19" "24"
[11,] "d"  "i"  "n"  "s"  "x"
[12,] "D"  "I"  "N"  "S"  "X"
[13,] "5"  "10" "15" "20" "25"
[14,] "e"  "j"  "o"  "t"  "y"
[15,] "E"  "J"  "O"  "T"  "Y"
 >

Thanks in advance !

St??phane DRAY
--------------------------------------------------------------------------------------------------


D??partement des Sciences Biologiques
Universit?? de Montr??al, C.P. 6128, succursale centre-ville
Montr??al, Qu??bec H3C 3J7, Canada

Tel : 514 343 6111 poste 1233
E-mail : stephane.dray at umontreal.ca
--------------------------------------------------------------------------------------------------


Web
http://www.steph280.freesurf.fr/

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Tue Jun 29 17:35:46 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 29 Jun 2004 16:35:46 +0100 (BST)
Subject: [R] binding rows from different matrices
In-Reply-To: <5.2.1.1.0.20040629105435.00b46bc8@magellan.umontreal.ca>
Message-ID: <Pine.LNX.4.44.0406291630250.5433-100000@gannet.stats>

You can almost always index in such problems: here is one way.

rbind(veca,vecb,vecc)[matrix(1:15, 3, byrow=T), ]

Take it apart of see how it works, if it is not immediately obvious.

On Tue, 29 Jun 2004, Stephane DRAY wrote:

> Hello list,
> I have 3 matrices with same dimension :
>  > veca=matrix(1:25,5,5)
>  > vecb=matrix(letters[1:25],5,5)
>  > vecc=matrix(LETTERS[1:25],5,5)
> 
> I would like to obtain a new matrix composed by alternating rows of these 
> different matrices (row 1 of mat 1, row 1 of mat 2, row 1 of mat 3, row 2 
> of mat 1.....)
> 
> I have found a solution to do it but it is not very pretty and I wonder if 
> I can do it in an other way (perhaps with apply ) ?
> 
>  > res=matrix(0,1,5)
>  > for(i in 1:5)
> + res=rbind(res,veca[i,],vecb[i,],vecc[i,])
>  > res=res[-1,]
>  > res
>        [,1] [,2] [,3] [,4] [,5]
>   [1,] "1"  "6"  "11" "16" "21"
>   [2,] "a"  "f"  "k"  "p"  "u"
>   [3,] "A"  "F"  "K"  "P"  "U"
>   [4,] "2"  "7"  "12" "17" "22"
>   [5,] "b"  "g"  "l"  "q"  "v"
>   [6,] "B"  "G"  "L"  "Q"  "V"
>   [7,] "3"  "8"  "13" "18" "23"
>   [8,] "c"  "h"  "m"  "r"  "w"
>   [9,] "C"  "H"  "M"  "R"  "W"
> [10,] "4"  "9"  "14" "19" "24"
> [11,] "d"  "i"  "n"  "s"  "x"
> [12,] "D"  "I"  "N"  "S"  "X"
> [13,] "5"  "10" "15" "20" "25"
> [14,] "e"  "j"  "o"  "t"  "y"
> [15,] "E"  "J"  "O"  "T"  "Y"

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rkoenker at uiuc.edu  Tue Jun 29 17:49:13 2004
From: rkoenker at uiuc.edu (roger koenker)
Date: Tue, 29 Jun 2004 10:49:13 -0500
Subject: [R] Quantile Regression in R
In-Reply-To: <20040629152635.20191.qmail@web12102.mail.yahoo.com>
References: <20040629152635.20191.qmail@web12102.mail.yahoo.com>
Message-ID: <DE9B3B5A-C9E3-11D8-B1B7-000A95A7E3AA@uiuc.edu>

The short answer to your question is that  quantile regression
estimates are estimating linear conditional quantile functions,
just like lm() is used to estimate conditional mean functions.

A longer answer would inevitably involve unpleasant suggestions
that you should follow the posting guide:

	a.)  send questions about packages to the maintainer, not R-help
	b.)  not attach datasets in modes that are stripped by R-help
	c.)  make a token effort to read the documentation and related 
literature



url:	www.econ.uiuc.edu/~roger        	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820

On Jun 29, 2004, at 10:26 AM, Ali Hirsa wrote:

> I recently learn about Quantile Regression in R.
> I am trying to study two time series (attached) by Quantile Regression 
> in R.
> I wrote the following code and do not know how to interpret the lines.
>
> What kind of information can I get from them? Correlation for 
> quantiles,
> conditional probabilties (i.e. P(X in Quantile i | Y in Quantile i)) , 
> and etc.
> Many thanks in advance for any help.
>
> Best,
> Ali
>
> library("quantreg")
> #help.start()
>
> Data <- read.table("RESvsMOVE2.dat")
> #
> x <- Data[,2]
> y <- Data[,1]
>
> par(mfrow=c(2,2))
>
> qqnorm(x,main="MOVE Norm Q-Q Plot", xlab="Normal Qunatiles",ylab = 
> "MOVE Quantiles")
> qqline(x)
>
> qqnorm(y,main="Residuals Norm Q-Q Plot", xlab="Normal Qunatiles",ylab 
> = "Residuals Quantiles")
> qqline(y)
>
> plot(x,y,xlab="MOVE",ylab="Residuals",cex=.5)
>
> xx <- seq(min(x),max(x),.5)
>
> # Just a linear regression
> g <- coef(lm(y~x))
> yy <- (g[1]+g[2]*(xx))
> lines(xx,yy,col="yellow")
>
> taus <- c(.05,.1,.25,.5,.75,.9,.95)
>
> for(tau in taus){
>         f <- coef(rq(y~x,tau=tau,method="pfn"))
>         yy <- (f[1]+f[2]*(xx))
>         if (tau ==.05){
>              lines(xx,yy,col="red")
>         }
>         if (tau ==.95){
>              lines(xx,yy,col="green")
>         }
>         if (tau != .05 & tau != .95){
>              lines(xx,yy,col="blue")
>         }
> }
>
>
>
> 		
> __________________________________
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From maechler at stat.math.ethz.ch  Tue Jun 29 17:49:28 2004
From: maechler at stat.math.ethz.ch (Martin Maechler)
Date: Tue, 29 Jun 2004 17:49:28 +0200
Subject: [R] alternate rank method
In-Reply-To: <Pine.LNX.4.51.0406281058250.21721@artemis.imbe.med.uni-erlangen.de>
References: <Pine.LNX.4.44.0406251103260.19871-100000@jerboa.fhcrc.org>
	<Pine.LNX.4.51.0406281058250.21721@artemis.imbe.med.uni-erlangen.de>
Message-ID: <16609.36744.346124.509258@gargle.gargle.HOWL>

>>>>> "Torsten" == Torsten Hothorn <Torsten.Hothorn at rzmail.uni-erlangen.de>
>>>>>     on Mon, 28 Jun 2004 10:59:26 +0200 (CEST) writes:

    Torsten> On Fri, 25 Jun 2004, Douglas Grove wrote:

    >> I should have specified an additional constraint:
    >> 
    >> I'm going to need to use this repeatedly on large vectors
    >> (length 10^6), so something efficient is needed.
    >> 

    Torsten> give function `irank' in package `exactRankTests' a
    Torsten> try.

As an answer to Torsten (who got it already orally) and Gabor's
original tricky suggestions:

I strongly believe this should happen in the same C code on
which R's base rank() function works and already implements the
*averaging* of ties.
Doing the analog of changing "average(..)" to min(..) or max(..)
shouldn't be hard and certainly will be more efficient than the
"workarounds" posted here.

Patches welcome...
since otherwise I'm not sure I'll get there in time.

Martin



From p.dalgaard at biostat.ku.dk  Tue Jun 29 17:58:32 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 29 Jun 2004 17:58:32 +0200
Subject: [R] binding rows from different matrices
In-Reply-To: <Pine.LNX.4.44.0406291630250.5433-100000@gannet.stats>
References: <Pine.LNX.4.44.0406291630250.5433-100000@gannet.stats>
Message-ID: <x2n02mfj9z.fsf@biostat.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> You can almost always index in such problems: here is one way.
> 
> rbind(veca,vecb,vecc)[matrix(1:15, 3, byrow=T), ]
> 
> Take it apart of see how it works, if it is not immediately obvious.

Or, a little longer, but perhaps more intuitive:

 matrix(aperm(array(c(veca,vecb,vecc),c(5,5,3)),c(3,1,2)),15)

I.e., convert to array, do generalized transpose, convert back to
matrix. Not that I got the index calculations right on first try....

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From riskpro_jw at hotmail.com  Tue Jun 29 18:01:48 2004
From: riskpro_jw at hotmail.com (J W)
Date: Tue, 29 Jun 2004 11:01:48 -0500
Subject: [R] Different behaviour of unique(), R vs. Splus.
Message-ID: <BAY18-F32HTXKGEJ0wc000363ae@hotmail.com>



From dgrove at fhcrc.org  Tue Jun 29 18:04:47 2004
From: dgrove at fhcrc.org (Douglas Grove)
Date: Tue, 29 Jun 2004 09:04:47 -0700 (PDT)
Subject: [R] alternate rank method
In-Reply-To: <16609.36744.346124.509258@gargle.gargle.HOWL>
Message-ID: <Pine.LNX.4.44.0406290859040.11861-100000@jerboa.fhcrc.org>

I agree.  These are obvious extensions to the options provided
now by rank.  I didn't suggest this as I am not a contributor and
don't feel comfortable asking others to do more work :)

Thanks,
Doug


On Tue, 29 Jun 2004, Martin Maechler wrote:

> >>>>> "Torsten" == Torsten Hothorn <Torsten.Hothorn at rzmail.uni-erlangen.de>
> >>>>>     on Mon, 28 Jun 2004 10:59:26 +0200 (CEST) writes:
> 
>     Torsten> On Fri, 25 Jun 2004, Douglas Grove wrote:
> 
>     >> I should have specified an additional constraint:
>     >> 
>     >> I'm going to need to use this repeatedly on large vectors
>     >> (length 10^6), so something efficient is needed.
>     >> 
> 
>     Torsten> give function `irank' in package `exactRankTests' a
>     Torsten> try.
> 
> As an answer to Torsten (who got it already orally) and Gabor's
> original tricky suggestions:
> 
> I strongly believe this should happen in the same C code on
> which R's base rank() function works and already implements the
> *averaging* of ties.
> Doing the analog of changing "average(..)" to min(..) or max(..)
> shouldn't be hard and certainly will be more efficient than the
> "workarounds" posted here.
> 
> Patches welcome...
> since otherwise I'm not sure I'll get there in time.
> 
> Martin
>



From riskpro_jw at hotmail.com  Tue Jun 29 18:09:21 2004
From: riskpro_jw at hotmail.com (J W)
Date: Tue, 29 Jun 2004 11:09:21 -0500
Subject: [R] Different behaviour of unique(), R vs. Splus.
Message-ID: <BAY18-F35WQ6rwsCxJ000026a25@hotmail.com>



From dmb at mrc-dunn.cam.ac.uk  Tue Jun 29 17:49:13 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Tue, 29 Jun 2004 16:49:13 +0100 (BST)
Subject: [R] Several PCA questions...
In-Reply-To: <Pine.LNX.4.44.0406291130380.22300-100000@gannet.stats>
Message-ID: <Pine.LNX.4.21.0406291644050.14774-100000@mail.mrc-dunn.cam.ac.uk>


Perhaps this question is less dumb... (in context below...)


On Tue, 29 Jun 2004, Prof Brian Ripley wrote:

>On Tue, 29 Jun 2004, Dan Bolser wrote:
>
>> Hi, I am doing PCA on several columns of data in a data.frame.
>> 
>> I am interested in particular rows of data which may have a particular
>> combination of 'types' of column values (without any pre-conception of
>> what they may be).
>> 
>> I do the following...
>> 
>> # My data table.
>> allDat <- read.table("big_select_thresh_5", header=1)
>> 
>> # Where some rows look like this...
>> # PDB     SUNID1  SUNID2  AA      CH      IPCA    PCA     IBB     BB
>> # 3sdh    14984   14985   6       10      24      24      93      116
>> # 3hbi    14986   14987   6       10      20      22      94      117
>> # 4sdh    14988   14989   6       10      20      20      104     122
>> 
>> # NB First three columns = row ID, last 6 = variables
>> 
>> attach(allDat)
>> 
>> # My columns of interest (variables).
>> part <- data.frame(AA,CH,IPCA,PCA,IBB,BB)
>> 
>> pc <- princomp(part)
>
>Do you really want an unscaled PCA on that data set?  Looks unlikely (but 
>then two of the columns are constant in the sample, which is also 
>worrying).


That is just sample bias. By unscaled I assume you mean something like
normalized?


>> plot(pc)
>> 
>> The above plot shows that 95% of the variance is due to the first
>> 'Component' (which I assume is AA).
>
>No, it is the first (principal) component.  You did ask for P>C<A!
>
>> i.e. All the variables behave in quite much the same way.
>
>Or you failed to scale the data so one dominates.

Yes.

I added the following to the above....


x <- colMeans(part)
partNorm <- part/x
pc1 <- princomp(partNorm)

plot(pc1)

biplot(pc1)

Which shows two major components, and possibly a third.

What I want to know is that given my data is not uniformly distributed, is
my normalization valid?

I know I should find this out via further investigation of PCA, but in
general if my variables have a very skewed distribution (possibly without
a theoretically definable mean) should I attempt to use any standard
clustering technique?

I guess I should log transform my data.

Cheers,
Dan.





>> I then did ...
>> 
>> 
>> biplot(pc)
>> 
>> Which showed some outliers with a numeric ID - How do I get back my old 3
>> part ID used in allDat?
>
>Set row names on your data frame.  Like almost all of R, it is the row 
>names of a data frame that are used for labelling, and you did not give 
>any so you got numbers.
>
>> In the above plot I saw all the variables (correctly named) pointing in
>> more or less the same direction (as shown by the variance). I then did the
>> following...
>> 
>> postscript(file="test.ps",paper="a4")
>> 
>> biplot(pc)
>> 
>> dev.off()
>> 
>> However, looking at test.ps shows that the arrows are missing (using
>> ggv)... Hmmm, they come back when I pstoimg then xv... never mind.
>
>So ggv is unreliable, perhaps cannot cope with colours?
>
>> Finally, I would like to make a contour plot of the above biplot, is this
>> possible? (or even a good way to present the data?
>
>What do you propose to represent by the contours?  Biplots have a 
>well-defined interpretation in terms of distances and angles.
>
>



From koerber at informatik.fh-kl.de  Tue Jun 29 18:28:23 2004
From: koerber at informatik.fh-kl.de (=?ISO-8859-1?Q?Hans_K=F6rber?=)
Date: Tue, 29 Jun 2004 18:28:23 +0200
Subject: [R] PAM clustering: using my own dissimilarity matrix
Message-ID: <40E198A7.2080604@informatik.fh-kl.de>

Hello,

I would like to use my own dissimilarity matrix in a PAM clustering with 
method "pam" (cluster package) instead of a dissimilarity matrix created 
by daisy.

I read data from a file containing the dissimilarity values using 
"read.csv". This creates a matrix (alternatively: an array or vector) 
which is not accepted by "pam": A call

    p<-pam(d,k=2,diss=TRUE)

yields an error message "Error in pam(d, k = 2, diss = TRUE) : x is not 
of class dissimilarity and can not be converted to this class." How can 
I convert the matrix d into a dissimilarity matrix suitable for "pam"?

I'm aware of a response by Friedrich Leisch to a similar question posed 
by Jose Quesada (quoted below). But as I understood the answer, the 
dissimilarity matrix there is calculated on the basis of (random) data.

Thank you in advance.
Hans

__________________________________

/>>>>> On Tue, 09 Jan 2001 15:42:30 -0700, /
/>>>>> Jose Quesada (JQ) wrote: /

/ > Hi, /
/ > I'm trying to use a similarity matrix (triangular) as input for 
pam() or /
/ > fanny() clustering algorithms. /
/ > The problem is that this algorithms can only accept a dissimilarity /
/ > matrix, normally generated by daisy(). /

/ > However, daisy only accept 'data matrix or dataframe. Dissimilarities /
/ > will be computed between the rows of x'. /
/ > Is there any way to say to that your data are already a similarity /
/ > matrix (triangular)? /
/ > In Kaufman and Rousseeuw's FORTRAN implementation (1990), they 
showed an /
/ > option like this one: /

/ > "Maybe you already have correlations coefficients between variables. /
/ > Your input data constist on a lower triangular matrix of pairwise /
/ > correlations. You wish to calculate dissimilarities between the /
/ > variables." /

/ > But I couldn't find this alternative in the R implementation. /

/ > I can not use foo <- as.dist(foo), neither daisy(foo...) because /
/ > "Dissimilarities will be computed between the rows of x", and this is /
/ > not /
/ > what I mean. /

/ > You can easily transform your similarities into dissimilarities like /
/ > this (also recommended in Kaufman and Rousseeuw ,1990): /

/ > foo <- (1 - abs(foo)) # where foo are similarities /

/ > But then pam() will complain like this: /

/ > " x is not of class dissimilarity and can not be converted to this /
/ > class." /

/ > Can anyone help me? I also appreciate any advice about other 
clustering /
/ > algorithms that can accept this type of input. /

Hmm, I don't understand your problem, because proceeding as the docs
describe it works for me ...

If foo is a similarity matrix (with 1 meaning identical objects), then

bar <- as.dist(1 - abs(foo))
fanny(bar, ...)

works for me:

## create a random 12x12 similarity matrix, make it symmetric and set the
## diagonal to 1
/> x <- matrix(runif(144), nc=12) /
/> x <- x+t(x) /
/> diag(x) <- 1 /

## now proceed as described in the docs
/> y <- as.dist(1-x) /
/> fanny(y, 3) /
iterations objective
 42.000000 3.303235
Membership coefficients:
        [,1] [,2] [,3]
1 0.3333333 0.3333333 0.3333333
2 0.3333333 0.3333333 0.3333333
3 0.3333334 0.3333333 0.3333333
4 0.3333333 0.3333333 0.3333333
...



From lauraholt_983 at hotmail.com  Tue Jun 29 18:26:23 2004
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Tue, 29 Jun 2004 11:26:23 -0500
Subject: [R] removing NA from an its object
Message-ID: <BAY12-F110i21nWMFhm0000981f@hotmail.com>

Hi again!

I have the following its object:
>class(x1)
[1] "its"
attr(,"package")
[1] "its"
>x1
             FTSE     DAX
2004-06-07 4491.6 4017.81
2004-06-08 4504.8 4018.95
2004-06-09 4489.5 3997.76
2004-06-10 4486.1 4021.64
2004-06-11 4484.0 4014.56
2004-06-14 4433.2 3948.65
2004-06-15 4458.6 3987.30
2004-06-16 4491.1 4003.24
2004-06-17 4493.3 3985.46
2004-06-18 4505.8 3999.79
2004-06-21 4502.2 3989.31
2004-06-22     NA 3928.39
2004-06-23     NA 3945.10
2004-06-24     NA 4007.05
2004-06-25     NA 4013.35
2004-06-28     NA 4069.35

I want to create an its object with no NAs; that is, if there is an NA in 
any column, strike the entire row.


I did the following:
>x2 <- its(na.omit(x1))
>x2
             FTSE     DAX
2004-06-07 4491.6 4017.81
2004-06-08 4504.8 4018.95
2004-06-09 4489.5 3997.76
2004-06-10 4486.1 4021.64
2004-06-11 4484.0 4014.56
2004-06-14 4433.2 3948.65
2004-06-15 4458.6 3987.30
2004-06-16 4491.1 4003.24
2004-06-17 4493.3 3985.46
2004-06-18 4505.8 3999.79
2004-06-21 4502.2 3989.31
attr(,"na.action")
2004-06-22 2004-06-23 2004-06-24 2004-06-25 2004-06-28
        12         13         14         15         16
attr(,"class")
[1] "omit"
>class(x2) <- "its"
>

My question:  is this the best way to accomplish the goal, please?  I tried 
apply with "all" and is.na but I got strange results.

Thanks.
R Version 1.9.1
Sincerely,
Laura
mailto: lauraholt_983 at hotmail.com


Married. http://lifeevents.msn.com/category.aspx?cid=married



From rivin at euclid.math.temple.edu  Tue Jun 29 18:35:39 2004
From: rivin at euclid.math.temple.edu (Igor Rivin)
Date: Tue, 29 Jun 2004 12:35:39 -0400 (EDT)
Subject: [R] naive question
Message-ID: <20040629163539.82DE32DD8C@euclid.math.temple.edu>


I have a 100Mb comma-separated file, and R takes several minutes to read it
(via read.table()). This is R 1.9.0 on a linux box with a couple gigabytes of
RAM. I am conjecturing that R is gc-ing, so maybe there is some command-line
arg I can give it to convince it that I have a lot of space, or?!

    Thanks!

	Igor



From wolski at molgen.mpg.de  Tue Jun 29 18:35:43 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Tue, 29 Jun 2004 18:35:43 +0200
Subject: [R] PAM clustering: using my own dissimilarity matrix
In-Reply-To: <40E198A7.2080604@informatik.fh-kl.de>
References: <40E198A7.2080604@informatik.fh-kl.de>
Message-ID: <200406291835430457.074BC7FA@mail.math.fu-berlin.de>

Hi!

If your x is your symmetric matrix containing the distances than cast it to an dist object using as.dist.
?as.dist.

Sincerely
Eryk

*********** REPLY SEPARATOR  ***********

On 29.06.2004 at 18:28 Hans Krber wrote:

>Hello,
>
>I would like to use my own dissimilarity matrix in a PAM clustering with 
>method "pam" (cluster package) instead of a dissimilarity matrix created 
>by daisy.
>
>I read data from a file containing the dissimilarity values using 
>"read.csv". This creates a matrix (alternatively: an array or vector) 
>which is not accepted by "pam": A call
>
>    p<-pam(d,k=2,diss=TRUE)
>
>yields an error message "Error in pam(d, k = 2, diss = TRUE) : x is not 
>of class dissimilarity and can not be converted to this class." How can 
>I convert the matrix d into a dissimilarity matrix suitable for "pam"?
>
>I'm aware of a response by Friedrich Leisch to a similar question posed 
>by Jose Quesada (quoted below). But as I understood the answer, the 
>dissimilarity matrix there is calculated on the basis of (random) data.
>
>Thank you in advance.
>Hans
>
>__________________________________
>
>/>>>>> On Tue, 09 Jan 2001 15:42:30 -0700, /
>/>>>>> Jose Quesada (JQ) wrote: /
>
>/ > Hi, /
>/ > I'm trying to use a similarity matrix (triangular) as input for 
>pam() or /
>/ > fanny() clustering algorithms. /
>/ > The problem is that this algorithms can only accept a dissimilarity /
>/ > matrix, normally generated by daisy(). /
>
>/ > However, daisy only accept 'data matrix or dataframe. Dissimilarities /
>/ > will be computed between the rows of x'. /
>/ > Is there any way to say to that your data are already a similarity /
>/ > matrix (triangular)? /
>/ > In Kaufman and Rousseeuw's FORTRAN implementation (1990), they 
>showed an /
>/ > option like this one: /
>
>/ > "Maybe you already have correlations coefficients between variables. /
>/ > Your input data constist on a lower triangular matrix of pairwise /
>/ > correlations. You wish to calculate dissimilarities between the /
>/ > variables." /
>
>/ > But I couldn't find this alternative in the R implementation. /
>
>/ > I can not use foo <- as.dist(foo), neither daisy(foo...) because /
>/ > "Dissimilarities will be computed between the rows of x", and this is /
>/ > not /
>/ > what I mean. /
>
>/ > You can easily transform your similarities into dissimilarities like /
>/ > this (also recommended in Kaufman and Rousseeuw ,1990): /
>
>/ > foo <- (1 - abs(foo)) # where foo are similarities /
>
>/ > But then pam() will complain like this: /
>
>/ > " x is not of class dissimilarity and can not be converted to this /
>/ > class." /
>
>/ > Can anyone help me? I also appreciate any advice about other 
>clustering /
>/ > algorithms that can accept this type of input. /
>
>Hmm, I don't understand your problem, because proceeding as the docs
>describe it works for me ...
>
>If foo is a similarity matrix (with 1 meaning identical objects), then
>
>bar <- as.dist(1 - abs(foo))
>fanny(bar, ...)
>
>works for me:
>
>## create a random 12x12 similarity matrix, make it symmetric and set the
>## diagonal to 1
>/> x <- matrix(runif(144), nc=12) /
>/> x <- x+t(x) /
>/> diag(x) <- 1 /
>
>## now proceed as described in the docs
>/> y <- as.dist(1-x) /
>/> fanny(y, 3) /
>iterations objective
> 42.000000 3.303235
>Membership coefficients:
>        [,1] [,2] [,3]
>1 0.3333333 0.3333333 0.3333333
>2 0.3333333 0.3333333 0.3333333
>3 0.3333334 0.3333333 0.3333333
>4 0.3333333 0.3333333 0.3333333
>...
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Tue Jun 29 18:35:56 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 29 Jun 2004 17:35:56 +0100 (BST)
Subject: [R] Several PCA questions...
In-Reply-To: <Pine.LNX.4.21.0406291644050.14774-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <Pine.LNX.4.44.0406291733540.6643-100000@gannet.stats>

See `cor' in ?princomp, and its references.  I meant `scale' as in ?scale.


On Tue, 29 Jun 2004, Dan Bolser wrote:

> 
> Perhaps this question is less dumb... (in context below...)
> 
> 
> On Tue, 29 Jun 2004, Prof Brian Ripley wrote:
> 
> >On Tue, 29 Jun 2004, Dan Bolser wrote:
> >
> >> Hi, I am doing PCA on several columns of data in a data.frame.
> >> 
> >> I am interested in particular rows of data which may have a particular
> >> combination of 'types' of column values (without any pre-conception of
> >> what they may be).
> >> 
> >> I do the following...
> >> 
> >> # My data table.
> >> allDat <- read.table("big_select_thresh_5", header=1)
> >> 
> >> # Where some rows look like this...
> >> # PDB     SUNID1  SUNID2  AA      CH      IPCA    PCA     IBB     BB
> >> # 3sdh    14984   14985   6       10      24      24      93      116
> >> # 3hbi    14986   14987   6       10      20      22      94      117
> >> # 4sdh    14988   14989   6       10      20      20      104     122
> >> 
> >> # NB First three columns = row ID, last 6 = variables
> >> 
> >> attach(allDat)
> >> 
> >> # My columns of interest (variables).
> >> part <- data.frame(AA,CH,IPCA,PCA,IBB,BB)
> >> 
> >> pc <- princomp(part)
> >
> >Do you really want an unscaled PCA on that data set?  Looks unlikely (but 
> >then two of the columns are constant in the sample, which is also 
> >worrying).
> 
> 
> That is just sample bias. By unscaled I assume you mean something like
> normalized?
> 
> 
> >> plot(pc)
> >> 
> >> The above plot shows that 95% of the variance is due to the first
> >> 'Component' (which I assume is AA).
> >
> >No, it is the first (principal) component.  You did ask for P>C<A!
> >
> >> i.e. All the variables behave in quite much the same way.
> >
> >Or you failed to scale the data so one dominates.
> 
> Yes.
> 
> I added the following to the above....
> 
> 
> x <- colMeans(part)
> partNorm <- part/x
> pc1 <- princomp(partNorm)
> 
> plot(pc1)
> 
> biplot(pc1)
> 
> Which shows two major components, and possibly a third.
> 
> What I want to know is that given my data is not uniformly distributed, is
> my normalization valid?
> 
> I know I should find this out via further investigation of PCA, but in
> general if my variables have a very skewed distribution (possibly without
> a theoretically definable mean) should I attempt to use any standard
> clustering technique?
> 
> I guess I should log transform my data.
> 
> Cheers,
> Dan.
> 
> 
> 
> 
> 
> >> I then did ...
> >> 
> >> 
> >> biplot(pc)
> >> 
> >> Which showed some outliers with a numeric ID - How do I get back my old 3
> >> part ID used in allDat?
> >
> >Set row names on your data frame.  Like almost all of R, it is the row 
> >names of a data frame that are used for labelling, and you did not give 
> >any so you got numbers.
> >
> >> In the above plot I saw all the variables (correctly named) pointing in
> >> more or less the same direction (as shown by the variance). I then did the
> >> following...
> >> 
> >> postscript(file="test.ps",paper="a4")
> >> 
> >> biplot(pc)
> >> 
> >> dev.off()
> >> 
> >> However, looking at test.ps shows that the arrows are missing (using
> >> ggv)... Hmmm, they come back when I pstoimg then xv... never mind.
> >
> >So ggv is unreliable, perhaps cannot cope with colours?
> >
> >> Finally, I would like to make a contour plot of the above biplot, is this
> >> possible? (or even a good way to present the data?
> >
> >What do you propose to represent by the contours?  Biplots have a 
> >well-defined interpretation in terms of distances and angles.
> >
> >
> 
> 

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From Jens_Praestgaard at hgsi.com  Tue Jun 29 17:01:03 2004
From: Jens_Praestgaard at hgsi.com (Jens_Praestgaard@hgsi.com)
Date: Tue, 29 Jun 2004 11:01:03 -0400
Subject: [R] Jens Praestgaard/Hgsi is out of the office.
Message-ID: <OF1B558278.FD09F1CC-ON85256EC2.00527E75-85256EC2.00527E75@hgsi.com>

I will be out of the office starting  06/28/2004 and will not return until
06/30/2004.

Jens Praestgaard is  out of the office until June 30 and  will respond to
your message when he returns.
Thank you



From kbartz at loyaltymatrix.com  Tue Jun 29 18:34:16 2004
From: kbartz at loyaltymatrix.com (kevin bartz)
Date: Tue, 29 Jun 2004 09:34:16 -0700
Subject: [R] removing NA from an its object
In-Reply-To: <BAY12-F110i21nWMFhm0000981f@hotmail.com>
Message-ID: <20040629163920.7A1AA40173@omta14.mta.everyone.net>

What did you try with "apply"? It seems to work for me. I did

x2[!apply(is.na(x2), 1, any),]

and got the desired results.

Kevin

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Laura Holt
Sent: Tuesday, June 29, 2004 9:26 AM
To: r-help at stat.math.ethz.ch
Subject: [R] removing NA from an its object

Hi again!

I have the following its object:
>class(x1)
[1] "its"
attr(,"package")
[1] "its"
>x1
             FTSE     DAX
2004-06-07 4491.6 4017.81
2004-06-08 4504.8 4018.95
2004-06-09 4489.5 3997.76
2004-06-10 4486.1 4021.64
2004-06-11 4484.0 4014.56
2004-06-14 4433.2 3948.65
2004-06-15 4458.6 3987.30
2004-06-16 4491.1 4003.24
2004-06-17 4493.3 3985.46
2004-06-18 4505.8 3999.79
2004-06-21 4502.2 3989.31
2004-06-22     NA 3928.39
2004-06-23     NA 3945.10
2004-06-24     NA 4007.05
2004-06-25     NA 4013.35
2004-06-28     NA 4069.35

I want to create an its object with no NAs; that is, if there is an NA in 
any column, strike the entire row.


I did the following:
>x2 <- its(na.omit(x1))
>x2
             FTSE     DAX
2004-06-07 4491.6 4017.81
2004-06-08 4504.8 4018.95
2004-06-09 4489.5 3997.76
2004-06-10 4486.1 4021.64
2004-06-11 4484.0 4014.56
2004-06-14 4433.2 3948.65
2004-06-15 4458.6 3987.30
2004-06-16 4491.1 4003.24
2004-06-17 4493.3 3985.46
2004-06-18 4505.8 3999.79
2004-06-21 4502.2 3989.31
attr(,"na.action")
2004-06-22 2004-06-23 2004-06-24 2004-06-25 2004-06-28
        12         13         14         15         16
attr(,"class")
[1] "omit"
>class(x2) <- "its"
>

My question:  is this the best way to accomplish the goal, please?  I tried 
apply with "all" and is.na but I got strange results.

Thanks.
R Version 1.9.1
Sincerely,
Laura
mailto: lauraholt_983 at hotmail.com


Married. http://lifeevents.msn.com/category.aspx?cid=married

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From jhowison at syr.edu  Tue Jun 29 19:32:57 2004
From: jhowison at syr.edu (James Howison)
Date: Tue, 29 Jun 2004 13:32:57 -0400
Subject: [R] R via ssh login on OS X?
In-Reply-To: <Pine.LNX.4.44.0406290837170.22082-100000@gannet.stats>
References: <Pine.LNX.4.44.0406290837170.22082-100000@gannet.stats>
Message-ID: <5BFE3674-C9F2-11D8-BA01-00306579408C@syr.edu>


On Jun 29, 2004, at 3:43 AM, Prof Brian Ripley wrote:

> Did you look at the notes on MacOS X in the R-admin manual (as the 
> INSTALL
> file asks)?  That would have told you why lapack failed, and I think 
> you
> should redo your build following the advice there.

Clearly I didn't read closely enough.  Thanks for the reminder.  The 
build and check completed successfully as a fully non-root build  with 
this sequence:

Compile f2c and libf2c and put f2c, f2c.h and libf2c.a in $HOME/f2c.  
Run ranlib on libf2c.a

mkdir $HOME/f2c

mkdir ~/Rinstall
mv R-1.9.1.tgz Rinstall/

PATH=$PATH:$HOME/f2c  (So that configure can find the f2c executable)
export LDFLAGS=-L$HOME/f2c/
export CPPFLAGS=-I$HOME/f2c/

./configure --prefix=$HOME/Rinstall/  --with-blas='-framework vecLib' 
--with-lapack

make
make check
make install

are all successful.

Built in this way it has no problems with remote login on OS X.  As I 
said I haven't found problems with remote log-in at at with 1.9.1

Thanks for the help everyone.

--J

> On Tue, 29 Jun 2004, James Howison wrote:
>
> [...]
>
>> then did
>>
>> ./configure --prefix=$HOME/Rinstall/ --enable-R-framework=no
>> --with-x=no --with-lapack=no
>
> Note
>
>    --with-blas='-framework vecLib' --with-lapack
>
> is `strongly recommended', and on some versions of MacOS X `appears to 
> be
> the only way to build R'.
>
>> and then
>>
>> make
>>
>> This basically worked but for some reason lapack was still trying to
>> build and that was failing, so I deleted it from the appropriate
>> makefile and the rest of the compile went fine.  The lapack confusion
>> stopped some of the recommended modules from building but I didn't 
>> need
>> those (just sna which built fine from CRAN).
>
> [...]
>
> -- 
> Brian D. Ripley,                  ripley at stats.ox.ac.uk
> Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
> University of Oxford,             Tel:  +44 1865 272861 (self)
> 1 South Parks Road,                     +44 1865 272866 (PA)
> Oxford OX1 3TG, UK                Fax:  +44 1865 272595
>
>
>
--James
+1 315 395 4056



From andy_liaw at merck.com  Tue Jun 29 19:41:41 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 29 Jun 2004 13:41:41 -0400
Subject: [R] RE: [S] Different behaviour of unique(), R vs. Splus.
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7FA8@usrymx25.merck.com>

The source of the incompatibility:

In S-PLUS 6.2:

> methods("unique")
               splus            splus          menu                 splus 
 "unique.data.frame" "unique.default" "unique.name" "unique.rowcol.names"


In R-1.9.1:

> methods("unique")
[1] unique.array      unique.data.frame unique.default    unique.matrix    


Unless there's some sort of coordination (or even just separate effort) on
either/both R Core and Insightful developers to make sure there's agreement
on what methods to provide in the base code, such problem can only get
worse, not better, I guess.

Best,
Andy


> From: Rolf Turner
> 
> Apologies for the cross-posting, but I thought this snippet of info
> might be vaguely interesting to both lists.
> 
> I did a ***brief*** search to see if this issue had previously been
> discussed and found nothing.  So I thought I'd tell the list about a
> difference in behaviour between unique() in R and unique() in Splus
> which bit me just now.
> 
> I was trying to convert a package from Splus to R and got nonsense
> answers in R.  Turned out that within the bowels of the package I was
> doing something like
> 
> 	u <- unique(y)
> 
> where y was a matrix of integer values.  In Splus this gives a
> (short) vector of unique values.  In R it gives a matrix of the same
> dimensionality as y, except that any duplicated rows are eliminated.
> 
> (This looks like being very useful --- once you know about it.  And
> it was probably mentioned in the R release notes at one time, but, as
> Dr. Hook says, ``I was stoned and I missed it.'')
> 
> E.g.
> 	set.seed(42)
> 	m <- matrix(sample(1:5,20,TRUE),5,4)
> 	u <- unique(m)
> 
> In R ``u'' is identical to ``m''; in Splus ``u'' is vector (of
> length 5).
> 
> To get what I want in R I simply need to do
> 
> 	u <- unique(as.vector(y))
> 
> Simple, once you know.  Took me a devil of a long time to track down
> what was going wrong, but!
> 
> 					cheers,
> 
> 						Rolf Turner
> --------------------------------------------------------------------
> This message was distributed by s-news at lists.biostat.wustl.edu.  To
> ...(s-news.. clipped)...

> 
>



From B.Rowlingson at lancaster.ac.uk  Tue Jun 29 20:00:56 2004
From: B.Rowlingson at lancaster.ac.uk (Barry Rowlingson)
Date: Tue, 29 Jun 2004 19:00:56 +0100
Subject: [R] anti-R vitriol
Message-ID: <40E1AE58.7000600@lancaster.ac.uk>

A colleague is receiving some data from another person. That person 
reads the data in SAS and it takes 30s and uses 64k RAM. That person 
then tries to read the data in R and it takes 10 minutes and uses a 
gigabyte of RAM. Person then goes on to say:

   It's not that I think SAS is such great software,
   it's not.  But I really hate badly designed
   software.  R is designed by committee.  Worse,
   it's designed by a committee of statisticians.
   They tend to confuse numerical analysis with
   computer science and don't have any idea about
   software development at all.  The result is R.

   I do hope [your colleague] won't have to waste time doing
   [this analysis] in an outdated and poorly designed piece
   of software like R.

Would any of the "committee" like to respond to this? Or shall we just 
slap our collective forehead and wonder how someone could get such a view?



Barry



From fm3a004 at math.uni-hamburg.de  Tue Jun 29 20:08:19 2004
From: fm3a004 at math.uni-hamburg.de (Christian Hennig)
Date: Tue, 29 Jun 2004 20:08:19 +0200 (MET DST)
Subject: [R] Goodness of fit test for estimated distribution
Message-ID: <Pine.GSO.3.95q.1040629195857.29889D-100000@sun11.math.uni-hamburg.de>

Hi,

is there any method for goodness of fit testing of an (as general as
possible) univariate distribution with parameters estimated, for normal, 
exponential, gamma distributions, say (e.g. the corrected p-values for 
the Kolmogorov-Smirnov or Chi-squared with corresponding ML estimation
method)? 
It seems that neither ks.test nor chisq.test handle estimated parameters.
I am aware of function goodfit in package vcd, which seems to it for some
discrete distributions.

Thank you for help,
Christian 


***********************************************************************
Christian Hennig
Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
#######################################################################
ich empfehle www.boag-online.de



From GPetris at uark.edu  Tue Jun 29 20:11:11 2004
From: GPetris at uark.edu (Giovanni Petris)
Date: Tue, 29 Jun 2004 13:11:11 -0500 (CDT)
Subject: [R] binding rows from different matrices
In-Reply-To: <x2n02mfj9z.fsf@biostat.ku.dk> (message from Peter Dalgaard on
	Tue, 29 Jun 2004 17:58:32 +0200)
References: <Pine.LNX.4.44.0406291630250.5433-100000@gannet.stats>
	<x2n02mfj9z.fsf@biostat.ku.dk>
Message-ID: <200406291811.i5TIBBZC013438@definetti.uark.edu>


Still another variation on the same theme:

> matrix(t(cbind(veca,vecb,vecc)),nc=5,byrow=T)

Giovanni

> Date: Tue, 29 Jun 2004 17:58:32 +0200
> From: Peter Dalgaard <p.dalgaard at biostat.ku.dk>
> Sender: r-help-bounces at stat.math.ethz.ch
> Cc: r-help at stat.math.ethz.ch, Stephane DRAY <stephane.dray at umontreal.ca>
> Precedence: list
> User-Agent: Gnus/5.09 (Gnus v5.9.0) Emacs/21.2
> Lines: 20
> 
> Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:
> 
> > You can almost always index in such problems: here is one way.
> > 
> > rbind(veca,vecb,vecc)[matrix(1:15, 3, byrow=T), ]
> > 
> > Take it apart of see how it works, if it is not immediately obvious.
> 
> Or, a little longer, but perhaps more intuitive:
> 
>  matrix(aperm(array(c(veca,vecb,vecc),c(5,5,3)),c(3,1,2)),15)
> 
> I.e., convert to array, do generalized transpose, convert back to
> matrix. Not that I got the index calculations right on first try....
> 
> -- 
>    O__  ---- Peter Dalgaard             Blegdamsvej 3  
>   c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
>  (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
> ~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> 


-- 

 __________________________________________________
[                                                  ]
[ Giovanni Petris                 GPetris at uark.edu ]
[ Department of Mathematical Sciences              ]
[ University of Arkansas - Fayetteville, AR 72701  ]
[ Ph: (479) 575-6324, 575-8630 (fax)               ]
[ http://definetti.uark.edu/~gpetris/              ]
[__________________________________________________]



From gunter.berton at gene.com  Tue Jun 29 20:18:55 2004
From: gunter.berton at gene.com (Berton Gunter)
Date: Tue, 29 Jun 2004 11:18:55 -0700
Subject: [R] anti-R vitriol
References: <40E1AE58.7000600@lancaster.ac.uk>
Message-ID: <40E1B28F.A9F2A8CA@gene.com>

My reaction, as a "mere" individual user: Of course, one cannot have any idea
what's really going on, so a rational reply to the rant is impossible. But, as
this list repeatedly demonstrates (and as we all have probably experienced), it
is possible to do things foolishly in any software.

Worth noting: John Chambers, the designer of the S language (of which R is an
implementation) won  an ACM computing award (readers -- please correct details of
this citation) for his achievement; so apparently the professional computing
community disagreed with the sentiments expressed in the rant.

Cheers,

--

Bert Gunter

Non-Clinical Biostatistics
Genentech
MS: 240B
Phone: 650-467-7374


"The business of the statistician is to catalyze the scientific learning
process."

 -- George E.P. Box

Barry Rowlingson wrote:

> A colleague is receiving some data from another person. That person
> reads the data in SAS and it takes 30s and uses 64k RAM. That person
> then tries to read the data in R and it takes 10 minutes and uses a
> gigabyte of RAM. Person then goes on to say:
>
>    It's not that I think SAS is such great software,
>    it's not.  But I really hate badly designed
>    software.  R is designed by committee.  Worse,
>    it's designed by a committee of statisticians.
>    They tend to confuse numerical analysis with
>    computer science and don't have any idea about
>    software development at all.  The result is R.
>
>    I do hope [your colleague] won't have to waste time doing
>    [this analysis] in an outdated and poorly designed piece
>    of software like R.
>
> Would any of the "committee" like to respond to this? Or shall we just
> slap our collective forehead and wonder how someone could get such a view?
>
> Barry
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From rpeng at jhsph.edu  Tue Jun 29 20:19:20 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Tue, 29 Jun 2004 14:19:20 -0400
Subject: [R] anti-R vitriol
In-Reply-To: <40E1AE58.7000600@lancaster.ac.uk>
References: <40E1AE58.7000600@lancaster.ac.uk>
Message-ID: <40E1B2A8.3080902@jhsph.edu>

I'm not too concerned about your colleague's view about R.  S/He 
doesn' have to like it, and I don't think anyone actually believes 
that R is designed to make *everyone* happy.  For me, R does about 99% 
of the things I need to do, but sadly, when I need to order a pizza, I 
still have to pick up the telephone.

What worries me more is that your colleague seems to have lost sight 
of the fact that just about all software development involves 
tradeoffs.  Although I've never used SAS, I've used other stat 
packages and it's clear that all of them (including R) have traded in 
some things to get out other things.  An example is R's potentially 
large memory usage, which, one might argue, trades in analyses of very 
large datasets but gets out a very powerful and elegant programming 
language.

Rather than use absolutes, I'd encourage your colleague to be more 
specific.  Rather than and say things like "R is poorly designed" I'd 
like to hear "R is poorly designed for [fill in the blank]".  Then we 
can get a better handle on the world in which s/he lives.

-roger

Barry Rowlingson wrote:
> A colleague is receiving some data from another person. That person 
> reads the data in SAS and it takes 30s and uses 64k RAM. That person 
> then tries to read the data in R and it takes 10 minutes and uses a 
> gigabyte of RAM. Person then goes on to say:
> 
>   It's not that I think SAS is such great software,
>   it's not.  But I really hate badly designed
>   software.  R is designed by committee.  Worse,
>   it's designed by a committee of statisticians.
>   They tend to confuse numerical analysis with
>   computer science and don't have any idea about
>   software development at all.  The result is R.
> 
>   I do hope [your colleague] won't have to waste time doing
>   [this analysis] in an outdated and poorly designed piece
>   of software like R.
> 
> Would any of the "committee" like to respond to this? Or shall we just 
> slap our collective forehead and wonder how someone could get such a view?
> 
> 
> 
> Barry
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From andy_liaw at merck.com  Tue Jun 29 20:20:41 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 29 Jun 2004 14:20:41 -0400
Subject: [R] anti-R vitriol
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7FAA@usrymx25.merck.com>

> From: Barry Rowlingson
> 
> A colleague is receiving some data from another person. That person 
> reads the data in SAS and it takes 30s and uses 64k RAM. That person 
> then tries to read the data in R and it takes 10 minutes and uses a 
> gigabyte of RAM. Person then goes on to say:
> 
>    It's not that I think SAS is such great software,
>    it's not.  But I really hate badly designed
>    software.  R is designed by committee.  Worse,
>    it's designed by a committee of statisticians.
>    They tend to confuse numerical analysis with
>    computer science and don't have any idea about
>    software development at all.  The result is R.
> 
>    I do hope [your colleague] won't have to waste time doing
>    [this analysis] in an outdated and poorly designed piece
>    of software like R.
> 
> Would any of the "committee" like to respond to this? Or 
> shall we just 
> slap our collective forehead and wonder how someone could get 
> such a view?
> 
> Barry
 

My $0.02:

R, being a flexible programming language, has an amazing ability to cope
with people's laziness/ignorance/inelegance, but it comes at a (sometimes
hefty) price.  While there is no specifics on the situation leading to the
person's comments, here's one (not as extreme) example that I happen to come
across today:

> system.time(spam <- read.table("data_dmc2003_train.txt", 
+                                 header=T, 
+                                 colClasses=c(rep("numeric", 833), 
+                                              "character")))
[1] 15.92  0.09 16.80    NA    NA
> system.time(spam <- read.table("data_dmc2003_train.txt", header=T))
[1] 187.29   0.60 200.19     NA     NA

My SAS ability is rather serverely limited, but AFAIK, one needs to specify
_all_ variables to be read into a dataset in order to read in the data in
SAS.  If one has that information, R can be very efficient as well.  Without
that information, one gets nothing in SAS, or just let R does the hard work.

Best,
Andy



From rkoenker at uiuc.edu  Tue Jun 29 20:25:13 2004
From: rkoenker at uiuc.edu (roger koenker)
Date: Tue, 29 Jun 2004 13:25:13 -0500
Subject: [R] Goodness of fit test for estimated distribution
In-Reply-To: <Pine.GSO.3.95q.1040629195857.29889D-100000@sun11.math.uni-hamburg.de>
References: <Pine.GSO.3.95q.1040629195857.29889D-100000@sun11.math.uni-hamburg.de>
Message-ID: <A977B352-C9F9-11D8-B1B7-000A95A7E3AA@uiuc.edu>

In full generality this is a quite difficult problem as discussed in
Durbin's (1973) SIAM monograph.  An elegant general approach
is provided by Khmaladze

@article{Khma:Arie:1981,
     author = {Khmaladze, E. V.},
     title = {Martingale approach in the theory of goodness-of-fit 
tests},
     year = {1981},
     journal = {Theory of Probability and its Applications (Transl of 
Teorija Verojatnostei i ee Primenenija)},
     volume = {26},
     pages = {240--257}
}

but I don't think that there is a general implementation of the 
approach for R, or
any other software environment, for that matter.

url:	www.econ.uiuc.edu/~roger        	Roger Koenker
email	rkoenker at uiuc.edu			Department of Economics
vox: 	217-333-4558				University of Illinois
fax:   	217-244-6678				Champaign, IL 61820

On Jun 29, 2004, at 1:08 PM, Christian Hennig wrote:

> Hi,
>
> is there any method for goodness of fit testing of an (as general as
> possible) univariate distribution with parameters estimated, for 
> normal,
> exponential, gamma distributions, say (e.g. the corrected p-values for
> the Kolmogorov-Smirnov or Chi-squared with corresponding ML estimation
> method)?
> It seems that neither ks.test nor chisq.test handle estimated 
> parameters.
> I am aware of function goodfit in package vcd, which seems to it for 
> some
> discrete distributions.
>
> Thank you for help,
> Christian
>
>
> ***********************************************************************
> Christian Hennig
> Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
> hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
> #######################################################################
> ich empfehle www.boag-online.de
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Tue Jun 29 20:32:35 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 29 Jun 2004 14:32:35 -0400
Subject: [R] job opening in Merck Research Labs, NJ
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7FAB@usrymx25.merck.com>

Apology for the cross-post...  Andy

==============================================

Job description:  Computational statistician/medical image analyst

The Biometrics Research Department at Merck Research Laboratories, Merck &
Co., Inc. in Rahway, NJ is seeking a highly motivated statistician/data
analyst to work in its basic research and drug discovery area.  The
applicant should have broad expertise in image processing, statistics, and
computer science, with substantial experience in medical imaging analysis
including design of experiments, image registration and segmentation,
statistical analysis, and pattern recognition.  The position will initially
involve providing statistical, mathematical, and software development
support for MRI and ultrasound imaging teams in preclinical research (i.e.,
animal studies, not human).  Merck has its own facilities for CT, PET, MRI,
and ultrasound imaging. We are looking for a Ph.D. with a background and/or
post-doctoral experience in at least one of the following fields:
Statistics, Electrical/Computer or Biomedical Engineering, Computer Science,
Applied Mathematics, or Physics.  Advanced computer programming skills
(including, but not limited to Matlab, C/C++, Visual Basic, SQL, IDL, or
PV-WAVE), and good communication skills are essential, as is familiarity
with  statistical software  like R and  Splus.  The position may also
involve general statistical consulting and training.  An ability to lead
statistical analysis efforts within a multidisciplinary team is required.
Strong candidates will also have interests and experience in computer
vision, machine learning/data mining, and/or signal processing. 

Our dedication to delivering quality medicines in innovative ways and our
commitment to bringing out the best in our people are just some of the
reasons why we're ranked among Fortune magazine's "100 Best Companies to
Work for in America."  We offer a competitive salary, an oustanding benefits
package, and a professional work environment with a company known for
scientific excellence.  To apply, please forward your CV or resume and cover
letter to

ATTENTION: Open Position
Vladimir Svetnik, Ph.D.
Biometrics Research Dept.
Merck Research Laboratories, RY33-300
126 E. Lincoln Avenue
Rahway, NJ 07065-0900
vladimir_svetnik at merck.com



From spencer.graves at pdf.com  Tue Jun 29 20:56:44 2004
From: spencer.graves at pdf.com (Spencer Graves)
Date: Tue, 29 Jun 2004 11:56:44 -0700
Subject: [R] Goodness of fit test for estimated distribution
In-Reply-To: <A977B352-C9F9-11D8-B1B7-000A95A7E3AA@uiuc.edu>
References: <Pine.GSO.3.95q.1040629195857.29889D-100000@sun11.math.uni-hamburg.de>
	<A977B352-C9F9-11D8-B1B7-000A95A7E3AA@uiuc.edu>
Message-ID: <40E1BB6C.4030002@pdf.com>

      What about Monte Carlo?  I recently produced (with help from 
contributors to this list) qq plots for certain complicated mixtures of 
distributions.  To evaluate goodness of fit, I produced Monte Carlo 
confidence intervals from 401 simulated qq plots and took the 11th and 
391st of them for each quantile.  {quantile(1:401, c(.025, .975)) = 
c(11, 391)}.  Something like this could be done to obtain a significance 
level for ks.test, for example. 

      This may not be as satisfying for some purposes as a clean, 
theoretical result, but it produced useful answers without busting the 
project budget too badly. 

      hope this helps. 
      spencer graves

roger koenker wrote:

> In full generality this is a quite difficult problem as discussed in
> Durbin's (1973) SIAM monograph.  An elegant general approach
> is provided by Khmaladze
>
> @article{Khma:Arie:1981,
>     author = {Khmaladze, E. V.},
>     title = {Martingale approach in the theory of goodness-of-fit tests},
>     year = {1981},
>     journal = {Theory of Probability and its Applications (Transl of 
> Teorija Verojatnostei i ee Primenenija)},
>     volume = {26},
>     pages = {240--257}
> }
>
> but I don't think that there is a general implementation of the 
> approach for R, or
> any other software environment, for that matter.
>
> url:    www.econ.uiuc.edu/~roger            Roger Koenker
> email    rkoenker at uiuc.edu            Department of Economics
> vox:     217-333-4558                University of Illinois
> fax:       217-244-6678                Champaign, IL 61820
>
> On Jun 29, 2004, at 1:08 PM, Christian Hennig wrote:
>
>> Hi,
>>
>> is there any method for goodness of fit testing of an (as general as
>> possible) univariate distribution with parameters estimated, for normal,
>> exponential, gamma distributions, say (e.g. the corrected p-values for
>> the Kolmogorov-Smirnov or Chi-squared with corresponding ML estimation
>> method)?
>> It seems that neither ks.test nor chisq.test handle estimated 
>> parameters.
>> I am aware of function goodfit in package vcd, which seems to it for 
>> some
>> discrete distributions.
>>
>> Thank you for help,
>> Christian
>>
>>
>> ***********************************************************************
>> Christian Hennig
>> Fachbereich Mathematik-SPST/ZMS, Universitaet Hamburg
>> hennig at math.uni-hamburg.de, http://www.math.uni-hamburg.de/home/hennig/
>> #######################################################################
>> ich empfehle www.boag-online.de
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide! 
>> http://www.R-project.org/posting-guide.html
>
>
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Tue Jun 29 21:40:19 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Tue, 29 Jun 2004 19:40:19 +0000 (UTC)
Subject: [R] anti-R vitriol
References: <40E1AE58.7000600@lancaster.ac.uk>
Message-ID: <loom.20040629T211134-405@post.gmane.org>


Barry Rowlingson <B.Rowlingson <at> lancaster.ac.uk> writes:

: A colleague is receiving some data from another person. That person 
: reads the data in SAS and it takes 30s and uses 64k RAM. That person 
: then tries to read the data in R and it takes 10 minutes and uses a 
: gigabyte of RAM. Person then goes on to say:
: 
:    It's not that I think SAS is such great software,
:    it's not.  But I really hate badly designed
:    software.  R is designed by committee.  Worse,
:    it's designed by a committee of statisticians.
:    They tend to confuse numerical analysis with
:    computer science and don't have any idea about
:    software development at all.  The result is R.
: 
:    I do hope [your colleague] won't have to waste time doing
:    [this analysis] in an outdated and poorly designed piece
:    of software like R.
: 
: Would any of the "committee" like to respond to this? Or shall we just 
: slap our collective forehead and wonder how someone could get such a view?

Does he have to repeatedly read in different large datasets or is this 
just a one time requirement?  In the latter case, he could read in the 
data, save it (using the save command), and then just load it (using
the load command) in subsequent sessions.  He would only have to wait 
10 minutes the first time.  If he has that much data its probably a 
large project and a one time hit of 10 minutes versus several days, 
weeks or months of work seems negligible.



From dimasmm at visgraf.impa.br  Tue Jun 29 22:45:13 2004
From: dimasmm at visgraf.impa.br (Dimas Martnez Morera)
Date: Tue, 29 Jun 2004 17:45:13 -0300
Subject: [R] fl_show_fselector
Message-ID: <200406291745.13797.dimasmm@visgraf.impa.br>

Hi everybody, 

I new to xforms and I'm trying to use fl_show_fselector. In fact I did it 
without any problem. But now I'm getting "segmentation fault" in the line of 
code:

filename =fl_show_fselector("Select file to open",".","*.off", "");

the message is:

In SetFont [fonts.c 224] Bad FontStyle request 0: 
Segmentation fault (core dumped)


I'm wondering what is happening here. Can anybody help me?

Thanks a lot,
                 Dimas



From kimai at Princeton.Edu  Tue Jun 29 21:05:16 2004
From: kimai at Princeton.Edu (Kosuke Imai)
Date: Tue, 29 Jun 2004 15:05:16 -0400 (EDT)
Subject: [R] [R-pkgs] MNP
Message-ID: <Pine.LNX.4.44.0406291502090.19397-100000@wws-6qcbw21.Princeton.EDU>

We would like to announce the release of our software, which is now 
available through CRAN.

MNP: R Package for Fitting the Multinomial Probit Models

Abstract:
MNP is a publicly available R package that fits the Bayesian multinomial
probit models via Markov chain Monte Carlo. Along with the standard
multinomial probit model, it can also fit models with different choice
sets for each observation, and complete or partial ordering of all the
available alternatives. The computation is based on the efficient marginal
data augmentation algorithm that is developed by Imai and van Dyk (2004)  
``A Bayesian Analysis of the Multinomial Probit Model Using the Data
Augmentation,'' Journal of Econometrics, Forthcoming.

Kosuke Imai, Department of Politics, Princeton University
Jordan R. Vance, Department of Computer Science, Princeton University
David A. van Dyk, Department of Statistics, University of California, Irvine

_______________________________________________
R-packages mailing list
R-packages at stat.math.ethz.ch
https://www.stat.math.ethz.ch/mailman/listinfo/r-packages



From ripley at stats.ox.ac.uk  Tue Jun 29 22:11:22 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 29 Jun 2004 21:11:22 +0100 (BST)
Subject: [R] naive question
In-Reply-To: <20040629163539.82DE32DD8C@euclid.math.temple.edu>
Message-ID: <Pine.LNX.4.44.0406292110340.6777-100000@gannet.stats>

There are hints in the R Data Import/Export Manual.  Just checking: you 
_have_ read it?

On Tue, 29 Jun 2004, Igor Rivin wrote:

> 
> I have a 100Mb comma-separated file, and R takes several minutes to read it
> (via read.table()). This is R 1.9.0 on a linux box with a couple gigabytes of
> RAM. I am conjecturing that R is gc-ing, so maybe there is some command-line
> arg I can give it to convince it that I have a lot of space, or?!

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Tue Jun 29 22:19:24 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 29 Jun 2004 21:19:24 +0100 (BST)
Subject: [R] Re: [S] Different behaviour of unique(), R vs. Splus.
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7FA8@usrymx25.merck.com>
Message-ID: <Pine.LNX.4.44.0406292115340.6777-100000@gannet.stats>

On Tue, 29 Jun 2004, Liaw, Andy wrote:

> The source of the incompatibility:
> 
> In S-PLUS 6.2:
> 
> > methods("unique")
>                splus            splus          menu                 splus 
>  "unique.data.frame" "unique.default" "unique.name" "unique.rowcol.names"
> 
> 
> In R-1.9.1:
> 
> > methods("unique")
> [1] unique.array      unique.data.frame unique.default    unique.matrix    
> 
> 
> Unless there's some sort of coordination (or even just separate effort) on
> either/both R Core and Insightful developers to make sure there's agreement
> on what methods to provide in the base code, such problem can only get
> worse, not better, I guess.

There are plans to that effect, but R moves much faster than a commercial 
product such as S-PLUS.

It seems to me a bad idea that unique (or foo) does different things for 
matrices and data frames, for as we see frequently, many users do not 
distinguish between them.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From rivin at euclid.math.temple.edu  Tue Jun 29 22:22:58 2004
From: rivin at euclid.math.temple.edu (Igor Rivin)
Date: Tue, 29 Jun 2004 16:22:58 -0400
Subject: [R] naive question
In-Reply-To: <Pine.LNX.4.44.0406292110340.6777-100000@gannet.stats>
References: <Pine.LNX.4.44.0406292110340.6777-100000@gannet.stats>
Message-ID: <40E1CFA2.mail47M1V991X@euclid.math.temple.edu>


I did read the Import/Export document. It is true that replacing
the read.table by read.csv and setting the commentChar="" speeds
things up some (a factor of two?) -- this is very far from acceptable performance,
being some two orders of magnitude worse than SAS (the IO of which is, in turn, much worse
than that of the unix utilities (awk, sort, and so on))   . Setting colClasses is suggested
(and has been suggested by some in response to my question), but for 
a frame with some 60 columns, this is a major nuisance.



From bates at stat.wisc.edu  Tue Jun 29 22:36:16 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 29 Jun 2004 15:36:16 -0500
Subject: [OT] Ordering pizza [was Re: [R] anti-R vitriol]
In-Reply-To: <40E1B2A8.3080902@jhsph.edu>
References: <40E1AE58.7000600@lancaster.ac.uk> <40E1B2A8.3080902@jhsph.edu>
Message-ID: <40E1D2C0.4070506@stat.wisc.edu>

Roger D. Peng wrote:

> I'm not too concerned about your colleague's view about R.  S/He doesn' 
> have to like it, and I don't think anyone actually believes that R is 
> designed to make *everyone* happy.  For me, R does about 99% of the 
> things I need to do, but sadly, when I need to order a pizza, I still 
> have to pick up the telephone.

There are several chains of pizzerias in the U.S. that provide for 
Internet-based ordering (e.g. www.papajohnsonline.com) so, with the 
Internet modules in R, it's only a matter of time before you will have a 
pizza-ordering function available.



From rolf at math.unb.ca  Tue Jun 29 22:39:09 2004
From: rolf at math.unb.ca (Rolf Turner)
Date: Tue, 29 Jun 2004 17:39:09 -0300 (ADT)
Subject: [OT] Ordering pizza [was Re: [R] anti-R vitriol]
Message-ID: <200406292039.i5TKd9ob022201@erdos.math.unb.ca>


Dang!  You're making me hungry!

			cheers,

				Rolf Turner



From ripley at stats.ox.ac.uk  Tue Jun 29 22:41:35 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Tue, 29 Jun 2004 21:41:35 +0100 (BST)
Subject: [OT] Ordering pizza [was Re: [R] anti-R vitriol]
In-Reply-To: <40E1D2C0.4070506@stat.wisc.edu>
Message-ID: <Pine.LNX.4.44.0406292135080.6777-100000@gannet.stats>

On Tue, 29 Jun 2004, Douglas Bates wrote:

> Roger D. Peng wrote:
> 
> > I'm not too concerned about your colleague's view about R.  S/He doesn' 
> > have to like it, and I don't think anyone actually believes that R is 
> > designed to make *everyone* happy.  For me, R does about 99% of the 
> > things I need to do, but sadly, when I need to order a pizza, I still 
> > have to pick up the telephone.
> 
> There are several chains of pizzerias in the U.S. that provide for 
> Internet-based ordering (e.g. www.papajohnsonline.com) so, with the 
> Internet modules in R, it's only a matter of time before you will have a 
> pizza-ordering function available.

Indeed, the GraphApp toolkit (used for the RGui interface under R for
Windows, but Guido forgot to include it) provides one (for use in Sydney,
Australia, we presume as that is where the GraphApp author hails from).
Alternatively, a Padovian has no need of ordering pizzas with both home 
and neighbourhood restaurants ....

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From fzh113 at hecky.it.northwestern.edu  Tue Jun 29 23:00:48 2004
From: fzh113 at hecky.it.northwestern.edu (Fred)
Date: Tue, 29 Jun 2004 16:00:48 -0500
Subject: [R] Is there a function for Principal Surface?
Message-ID: <000001c45e1c$27aa3d60$ab8d7ca5@FYOC1>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040629/04342792/attachment.pl

From lauraholt_983 at hotmail.com  Tue Jun 29 23:06:09 2004
From: lauraholt_983 at hotmail.com (Laura Holt)
Date: Tue, 29 Jun 2004 16:06:09 -0500
Subject: [R] abline and its objects
Message-ID: <BAY12-F77mibfvTqPKG00001948@hotmail.com>

Hi R People:

Is there a way to put an abline line for its objects on a plot, please?

I have an its object, ibm2, which runs from the January 2 through May 28.

>ibm2
              ibm
2004-01-02  91.55
2004-01-05  93.05
2004-01-06  93.06
2004-01-07  92.78
2004-01-08  93.04
2004-01-09  91.21
2004-01-12  91.55
2004-01-13  89.70
2004-01-14  90.31
2004-01-15  94.02
.
.
.
I plot the data.  No Problem.
Now I extract the first day of the month in this fashion.
>zi <- extractIts(ibm2,weekday=T,find="first",period="month")
>zi
             ibm
2004-01-02 91.55
2004-02-02 99.39
2004-03-01 97.04
2004-04-01 92.37
2004-05-03 88.02
>

Still ok.
I would like to put a vertical line at each of the zi values.

>abline(v=zi,type="h",col=2)
>lines(zi,type="h",col=2)
>
Nothing happens.

I tried creating another its object with NA in all but the zi places.  Then 
I used lines(test1)

Still nothing happened.

Any suggestions would be much appreciated.
R Version 1.9.1
Sincerely,
Laura H
mailto: lauraholt_983 at hotmail.com


Married. http://lifeevents.msn.com/category.aspx?cid=married



From kbartz at loyaltymatrix.com  Tue Jun 29 23:22:57 2004
From: kbartz at loyaltymatrix.com (Kevin Bartz)
Date: Tue, 29 Jun 2004 14:22:57 -0700
Subject: [R] abline and its objects
In-Reply-To: <BAY12-F77mibfvTqPKG00001948@hotmail.com>
Message-ID: <20040629212759.C70924022C@omta12.mta.everyone.net>

The problem is that you've instructed R to place lines at the values 91.55,
99.39, 97.04, 92.37 and 88.02, but these values do not correspond to the
user coordinates of the x-axis (which you've specified to be dates).

Luckily, the dates where you need lines are in the rownames of zi. You do
need to convert them to your user coordinates--and that depends on how plot
decides to specify your user coordinates, which hinges on the range of your
full data set (you clipped it).

What's on your x-axis? Do par("usr") when you have one of the plots open and
tell me what R says.

Kevin

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch] On Behalf Of Laura Holt
Sent: Tuesday, June 29, 2004 2:06 PM
To: r-help at stat.math.ethz.ch
Subject: [R] abline and its objects

Hi R People:

Is there a way to put an abline line for its objects on a plot, please?

I have an its object, ibm2, which runs from the January 2 through May 28.

>ibm2
              ibm
2004-01-02  91.55
2004-01-05  93.05
2004-01-06  93.06
2004-01-07  92.78
2004-01-08  93.04
2004-01-09  91.21
2004-01-12  91.55
2004-01-13  89.70
2004-01-14  90.31
2004-01-15  94.02
.
.
.
I plot the data.  No Problem.
Now I extract the first day of the month in this fashion.
>zi <- extractIts(ibm2,weekday=T,find="first",period="month")
>zi
             ibm
2004-01-02 91.55
2004-02-02 99.39
2004-03-01 97.04
2004-04-01 92.37
2004-05-03 88.02
>

Still ok.
I would like to put a vertical line at each of the zi values.

>abline(v=zi,type="h",col=2)
>lines(zi,type="h",col=2)
>
Nothing happens.

I tried creating another its object with NA in all but the zi places.  Then 
I used lines(test1)

Still nothing happened.

Any suggestions would be much appreciated.
R Version 1.9.1
Sincerely,
Laura H
mailto: lauraholt_983 at hotmail.com


Married. http://lifeevents.msn.com/category.aspx?cid=married

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From andy_liaw at merck.com  Wed Jun 30 00:00:29 2004
From: andy_liaw at merck.com (Liaw, Andy)
Date: Tue, 29 Jun 2004 18:00:29 -0400
Subject: [R] naive question
Message-ID: <3A822319EB35174CA3714066D590DCD504AF7FB2@usrymx25.merck.com>

> From: rivin at euclid.math.temple.edu
> 
> I did read the Import/Export document. It is true that replacing
> the read.table by read.csv and setting the commentChar="" speeds
> things up some (a factor of two?) -- this is very far from 
> acceptable performance,
> being some two orders of magnitude worse than SAS (the IO of 
> which is, in turn, much worse
> than that of the unix utilities (awk, sort, and so on))   . 
> Setting colClasses is suggested
> (and has been suggested by some in response to my question), but for 
> a frame with some 60 columns, this is a major nuisance.
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! 
> http://www.R-project.org/posting-guide.html

Please don't make _your_ nuisance into others'.  Do read the posting guide
as suggested above.  You have not provided any info for anyone to give you
any useful advice beyond those you said you received.

R is not all things to all people.  If you are so annoyed, why not use
SAS/awk/sort and so on?

[For my own education:  How do you read the file into SAS without specifying
column names and types?]

Andy



From cpwww at comcast.net  Wed Jun 30 01:23:15 2004
From: cpwww at comcast.net (Coburn Watson)
Date: Tue, 29 Jun 2004 16:23:15 -0700
Subject: [R] Issue with ROracle and results not being returned
Message-ID: <200406291623.15750.cpwww@comcast.net>

Hello,

I am using ROracle 5.5 with R 1.8.1. I am connecting to an oracle database
, issuing a query and attempting to fetch data from the result set.  I can see my 
session in the oracle database as well as the sql which was executed (including number of blocks hit, etc). 
 I have also verified that the SQL returns a valid result set from sqlplus.

Below is a sample trace of my session:

> library(ROracle)
> drv <- dbDriver("Oracle")
> con <- dbConnect(drv,"perf/perf at pc2ac1")
> rs1 <- dbSendQuery(con, statement = paste ("SELECT distinct api FROM et_log_data order by api"))
> df<- fetch(rs1,n=-1)
> summary(rs1,verbose=T)
<OraResult:(25657,0,3)>
  Statement: SELECT distinct api FROM et_log_data order by api
  Has completed? yes
  Affected rows: 0
  Rows fetched: -1
  Fields:
  name    Sclass     type len precision scale isVarLength nullOK
1  API character VARCHAR2  50         0     0        TRUE  FALSE
> summary(df,verbose=T)
     API
 Length:0
 Class :character
 Mode  :character
> df
[1] API
<0 rows> (or 0-length row.names)
> q()

Any ideas on why the data cannot be retrieved into the df object?   Please remove "_nospam" from email address to email me directly.  
Any help would be appreciated.

Coburn

(sample output from query via sqlplus)
SQL> SELECT distinct api FROM et_log_data order by api
  2  ;

API
--------------------------------------------------
ADD_ACCOUNT
ADD_COMMENT
ADD_DLR_CH_SUB
..........
.... 54 total rows returned



From egcp at hotmail.com  Wed Jun 30 00:52:02 2004
From: egcp at hotmail.com (E GCP)
Date: Tue, 29 Jun 2004 22:52:02 +0000
Subject: [R] rgl installation problems
Message-ID: <BAY22-F42MJ0VIWrllO000e2874@hotmail.com>

Thanks for your replies. I do not HTML-ize my mail, but free email accounts 
do that and there is not a switch to turn it off.  I apologize in advance.

I installed R from the redhat package provided by Martyn Plummer. It 
installed fine and without problems. I can use R and have installed and used 
other packages within R without any problems whatsoever. I do not think the 
problem is with R or its installation.   I do think there is a problem with 
the installation of rgl_0.64-13.tar.gz on RedHat 9 (linux).  So, if there is 
anybody out there who has installed succesfully rgl_0.64-13.tar.gz on RedHat 
9, I would like to know how.

Thanks so much,
Enrique

>From: Peter Dalgaard <p.dalgaard at biostat.ku.dk>
>To: Prof Brian Ripley <ripley at stats.ox.ac.uk>
>CC: E GCP <egcp at hotmail.com>, r-help at stat.math.ethz.ch
>Subject: Re: [R] rgl installation problems
>Date: 28 Jun 2004 20:05:34 +0200
>
>Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:
>
>>On Mon, 28 Jun 2004, E GCP wrote:
>>
>>>Thanks for your quick replies, and excuse my naivete, but how do I fix 
>>>the
>>>problem, so the rgl package installs?
>>
>>We have no idea what is wrong on your system -- all we can tell is that
>>you have done something wrong but we were not sitting at your shoulder
>>when you did.  Perhaps you should try re-building R from scratch, paying
>>close attention to any messages?
>>
>>Meanwhile, try to follow the posting guide and not HTML-ize your mail.
>
>Another thing to try is to install Martyn's RPM (for FC1) instead of
>what is there now. Seems to get things right for me on RH8.
>
>The demo is amazingly smooth even on this ancient machine, BTW.



From p.connolly at hortresearch.co.nz  Wed Jun 30 01:40:37 2004
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Wed, 30 Jun 2004 11:40:37 +1200
Subject: [R] Sorting elements in a data.frame
In-Reply-To: <Pine.LNX.4.21.0406231924040.17979-100000@mail.mrc-dunn.cam.ac.uk>;
	from dmb@mrc-dunn.cam.ac.uk on Wed, Jun 23, 2004 at 07:29:54PM +0100
References: <Pine.LNX.4.21.0406231924040.17979-100000@mail.mrc-dunn.cam.ac.uk>
Message-ID: <20040630114037.Z11533@hortresearch.co.nz>

On Wed, 23-Jun-2004 at 07:29PM +0100, Dan Bolser wrote:

|> 
|> Hi,
|> 
|> I have data like this....
|> 
|> print(x)
|> 
|> ID	VAL1	VAL2
|> 1	2	6
|> 2	4	9
|> 3	45	12
|> 4	99	44
|> 
|> What I would like is data like this...
|> 
|> ID	VAL1	VAL2
|> 1	2	6
|> 2	4	9
|> 3	12	45
|> 4	44	99
|> 
|> 
|> So that my analysis of the ratio VAL2/VAL1 is somehow uniform.

By "uniform", I'm guessing you want them to be >= 1

If z is a vector of VAL2/VAL1 values, you can make them all >= 1 this way.

z[z < 1] <- z[z < 1]^-1

Depending on just how you want to use them, there could be better ways
but I've done enough guessing for now.

HTH

-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From feldesmanm at pdx.edu  Wed Jun 30 01:54:22 2004
From: feldesmanm at pdx.edu (Marc R. Feldesman)
Date: Tue, 29 Jun 2004 16:54:22 -0700
Subject: [R] naive question
In-Reply-To: <40E1CFA2.mail47M1V991X@euclid.math.temple.edu>
References: <Pine.LNX.4.44.0406292110340.6777-100000@gannet.stats>
	<40E1CFA2.mail47M1V991X@euclid.math.temple.edu>
Message-ID: <6.0.3.0.2.20040629165243.02144a90@pop4.attglobal.net>

At 01:22 PM 6/29/2004, Igor Rivin wrote:
 >
 >I did read the Import/Export document. It is true that replacing
 >the read.table by read.csv and setting the commentChar="" speeds
 >things up some (a factor of two?) -- this is very far from acceptable
 >performance,
 >being some two orders of magnitude worse than SAS (the IO of which is, in
 >turn, much worse
 >than that of the unix utilities (awk, sort, and so on))   . Setting
 >colClasses is suggested
 >(and has been suggested by some in response to my question), but for
 >a frame with some 60 columns, this is a major nuisance.
 >
Feel free to contribute to the project.   Whining and complaining won't get 
you anywhere.  SAS *is* faster at I/O.  So what?




Dr. Marc R. Feldesman
Professor and Chairman Emeritus
Anthropology Department - Portland State University
email:  feldesmanm at pdx.edu
email:  feldesman at attglobal.net
fax:    503-725-3905


"Don't knock on my door if you don't know my Rottweiler's name"  Warren Zevon
"Its midnight and I'm not famous yet"  Jimmy Buffett



From karlknoblich at yahoo.de  Wed Jun 30 01:55:47 2004
From: karlknoblich at yahoo.de (=?iso-8859-1?q?Karl=20Knoblick?=)
Date: Wed, 30 Jun 2004 01:55:47 +0200 (CEST)
Subject: [R] nls fitting problems (singularity)
Message-ID: <20040629235547.47951.qmail@web52510.mail.yahoo.com>


Hallo!

I have a problem with fitting data with nls. The first
example with y1 (data frame df1) shows an error, the
second works fine.

Is there a possibility to get a fit (e.g. JMP can fit
also data I can not manage to fit with R). Sometimes I
also got an error singularity with starting
parameters.

# x-values
x<-c(-1,5,8,11,13,15,16,17,18,19,21,22)
# y1-values (first data set)
y1=c(-55,-22,-13,-11,-9.7,-1.4,-0.22,5.3,8.5,10,14,20)
# y2-values (second data set)
y2=c(-92,-42,-15,1.3,2.7,8.7,9.7,13,11,19,18,22)

# data frames
df1<-data.frame(x=x, y=y1)
df2<-data.frame(x=x, y=y2)

# start list for parameters
sl<-list( d=0, b=10, c1=90, c2=20) 

# y1-Analysis - Result: Error in ...  singular
gradient
nls(y~d+(x-b)*c1*(x-b<0)+(x-b)*c2*(x-b>=0), data=df1,
start=sl)
# y2-Analysis - Result: working...
nls(y~d+(x-b)*c1*(x-b<0)+(x-b)*c2*(x-b>=0), data=df2,
start=sl)

# plots to look at data
par(mfrow=c(1,2))
plot(df1$x,df1$y)
plot(df2$x,df2$y)

Perhaps there is another fitting routine? Can anybody
help?

Best wishes,
Karl


	

	
		
___________________________________________________________
Bestellen Sie Y! DSL und erhalten Sie die AVM "FritzBox SL" f??r 0.
Sie sparen 119 und bekommen 2 Monate Grundgeb??hrbefreiung.



From vograno at evafunds.com  Wed Jun 30 01:59:58 2004
From: vograno at evafunds.com (Vadim Ogranovich)
Date: Tue, 29 Jun 2004 16:59:58 -0700
Subject: [R] naive question
Message-ID: <C698D707214E6F4AB39AB7096C3DE5A5568C3A@phost015.EVAFUNDS.intermedia.net>

 R's IO is indeed 20 - 50 times slower than that of equivalent C code no
matter what you do, which has been a pain for some of us. It does
however help read the Import/Export tips as w/o them the ratio gets much
worse. As Gabor G. suggested in another mail, if you use the file
repeatedly you can convert it into internal format: read.table once into
R and save using save()... This is much faster.

In my experience R is not so good at large data sets, where large is
roughly 10% of your RAM.



From rossini at blindglobe.net  Wed Jun 30 02:15:50 2004
From: rossini at blindglobe.net (A.J. Rossini)
Date: Tue, 29 Jun 2004 17:15:50 -0700
Subject: [R] anti-R vitriol
In-Reply-To: <40E1AE58.7000600@lancaster.ac.uk> (Barry Rowlingson's message
	of "Tue, 29 Jun 2004 19:00:56 +0100")
References: <40E1AE58.7000600@lancaster.ac.uk>
Message-ID: <857jtpuci1.fsf@servant.blindglobe.net>

Barry Rowlingson <B.Rowlingson at lancaster.ac.uk> writes:

>    It's not that I think SAS is such great software,
>    it's not.  But I really hate badly designed
>    software.  R is designed by committee.  Worse,
>    it's designed by a committee of statisticians.
>    They tend to confuse numerical analysis with
>    computer science and don't have any idea about
>    software development at all.  The result is R.

They'd probably prefer computer scientists and numerical analysts who
confuse data munging with statistical data analysis, a common problem
in mixed departments...

best,
-tony

-- 
rossini at u.washington.edu            http://www.analytics.washington.edu/ 
Biomedical and Health Informatics   University of Washington
Biostatistics, SCHARP/HVTN          Fred Hutchinson Cancer Research Center
UW (Tu/Th/F): 206-616-7630 FAX=206-543-3461 | Voicemail is unreliable
FHCRC  (M/W): 206-667-7025 FAX=206-667-4812 | use Email

CONFIDENTIALITY NOTICE: This e-mail message and any attachme...{{dropped}}



From rivin at euclid.math.temple.edu  Wed Jun 30 02:19:26 2004
From: rivin at euclid.math.temple.edu (Igor Rivin)
Date: Tue, 29 Jun 2004 20:19:26 -0400
Subject: [R] naive question
In-Reply-To: <3A822319EB35174CA3714066D590DCD504AF7FB2@usrymx25.merck.com>
References: <3A822319EB35174CA3714066D590DCD504AF7FB2@usrymx25.merck.com>
Message-ID: <40E2070E.mailCK411OTFQ@euclid.math.temple.edu>


I was not particularly annoyed, just disappointed, since R seems like
a much better thing than SAS in general, and doing everything with a combination
of hand-rolled tools is too much work. However, I do need to work with very large data sets, and if it takes 20 minutes to read them in, I have to explore other
options (one of which might be S-PLUS, which claims scalability as a major 
, er, PLUS over R).



From rivin at euclid.math.temple.edu  Wed Jun 30 02:29:34 2004
From: rivin at euclid.math.temple.edu (rivin@euclid.math.temple.edu)
Date: Tue, 29 Jun 2004 20:29:34 -0400 (EDT)
Subject: [R] naive question
In-Reply-To: <6.0.3.0.2.20040629165243.02144a90@pop4.attglobal.net>
References: <Pine.LNX.4.44.0406292110340.6777-100000@gannet.stats>
	<40E1CFA2.mail47M1V991X@euclid.math.temple.edu>
	<6.0.3.0.2.20040629165243.02144a90@pop4.attglobal.net>
Message-ID: <60865.66.93.84.30.1088555374.squirrel@www.math.temple.edu>

> At 01:22 PM 6/29/2004, Igor Rivin wrote:
>  >
>  >I did read the Import/Export document. It is true that replacing the
> read.table by read.csv and setting the commentChar="" speeds things up
> some (a factor of two?) -- this is very far from acceptable
> performance,
>  >being some two orders of magnitude worse than SAS (the IO of which is,
> in turn, much worse
>  >than that of the unix utilities (awk, sort, and so on))   . Setting
> colClasses is suggested
>  >(and has been suggested by some in response to my question), but for a
> frame with some 60 columns, this is a major nuisance.
>  >
> Feel free to contribute to the project.   Whining and complaining won't
> get  you anywhere.  SAS *is* faster at I/O.  So what?
>
>
Sigh. Why are you being defensive? If you read my message, you will see
that what it comes down to is: I tried what to me are obvious things (some
of which
only become obvious after getting advice from people on this forum), and I
cannot get the system to perform acceptably. The "whining and complaining"
is actually an attempt to figure out whether I am missing something else
obvious, because I find it hard to believe that I am the first one to face
this [I know I am not, actually, because a gentleman emailed me a response
to the effect that he had to break up his similarly large file into
several pieces to get acceptable performance -- I would say that this
solution is rather hard on the user]; on this list's archive there was a
posting back in '98, asking basically the same question as mine). As for
contributing
to the project, perhaps getting a response of the form "this is slow
because we are trying to achieve this, and that, and the third thing, and
this is the best compromise we seem to have come up with" might be more
encouraging than "So what?"


  Igor



From dmb at mrc-dunn.cam.ac.uk  Wed Jun 30 02:57:01 2004
From: dmb at mrc-dunn.cam.ac.uk (Dan Bolser)
Date: Wed, 30 Jun 2004 01:57:01 +0100 (BST)
Subject: [R] Several PCA questions...
In-Reply-To: <Pine.LNX.4.44.0406291733540.6643-100000@gannet.stats>
Message-ID: <Pine.LNX.4.21.0406300149150.25632-100000@mail.mrc-dunn.cam.ac.uk>


I have the following problem

cov(allDat, method='kendall')

Where allDat is 11,000 by 6 data.frame.

Will the above ever finish on my home computer?



From bates at stat.wisc.edu  Wed Jun 30 02:56:09 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: Tue, 29 Jun 2004 19:56:09 -0500
Subject: [R] naive question
In-Reply-To: <40E2070E.mailCK411OTFQ@euclid.math.temple.edu>
References: <3A822319EB35174CA3714066D590DCD504AF7FB2@usrymx25.merck.com>
	<40E2070E.mailCK411OTFQ@euclid.math.temple.edu>
Message-ID: <40E20FA9.1030609@stat.wisc.edu>

Igor Rivin wrote:

> I was not particularly annoyed, just disappointed, since R seems like
> a much better thing than SAS in general, and doing everything with a combination
> of hand-rolled tools is too much work. However, I do need to work with very large data sets, and if it takes 20 minutes to read them in, I have to explore other
> options (one of which might be S-PLUS, which claims scalability as a major 
> , er, PLUS over R).


If you are routinely working with very large data sets it would be 
worthwhile learning to use a relational database (PostgreSQL, MySQL, 
even Access) to store the data and then access it from R with RODBC or 
one of the specialized database packages.

R is slow reading ASCII files because it is assembling the meta-data on 
the fly and it is continually checking the types of the variables being 
read.  If you know all this information and build it into your table 
definitions, reading the data will be much faster.

A disadvantage of this approach is the need to learn yet another 
language and system.  I was going to do an example but found I could not 
because I left all my SQL books at home (I'm travelling at the moment) 
and I couldn't remember the particular commands for loading a table from 
an ASCII file.



From pwilkinson at videotron.ca  Wed Jun 30 03:18:50 2004
From: pwilkinson at videotron.ca (Peter Wilkinson)
Date: Tue, 29 Jun 2004 21:18:50 -0400
Subject: [R] naive question
Message-ID: <6.1.1.1.2.20040629211243.01bd4ec0@pop.videotron.ca>

I am working with data sets that have 2 matrices of 300 columns by 19,000 
rows , and I manage to get the data loaded in a reasonable amount of time. 
Once its in I save the workspace and load from there. Once I start doing 
some work on the data, I am taking up about 600 Meg's of RAM out of the 1 
Gig I have in the computer.I will soon upgrade to 2 Gig because I will have 
to work with an even larger data matrix soon.

I must say that the speed of R given with what I have been doing, is 
acceptable.

Peter




At 07:59 PM 6/29/2004, Vadim Ogranovich wrote:
>  R's IO is indeed 20 - 50 times slower than that of equivalent C code no
>matter what you do, which has been a pain for some of us. It does
>however help read the Import/Export tips as w/o them the ratio gets much
>worse. As Gabor G. suggested in another mail, if you use the file
>repeatedly you can convert it into internal format: read.table once into
>R and save using save()... This is much faster.
>
>In my experience R is not so good at large data sets, where large is
>roughly 10% of your RAM.
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From rpeng at jhsph.edu  Wed Jun 30 04:06:31 2004
From: rpeng at jhsph.edu (Roger D. Peng)
Date: Tue, 29 Jun 2004 22:06:31 -0400
Subject: [R] naive question
In-Reply-To: <20040629163539.82DE32DD8C@euclid.math.temple.edu>
References: <20040629163539.82DE32DD8C@euclid.math.temple.edu>
Message-ID: <40E22027.2020008@jhsph.edu>

We need more details about your problem to provide any useful 
help.  Are all the variables numeric?  Are they all completely 
different?  Is it possible to use `colClasses'?

Also, having "a couple of gigabytes of RAM" is not necessarily 
useful if you're on a 32-bit OS since the total process size is 
usually limited to be less than ~3GB.

Believe it or not, complaints like these are not that common. 
1998 was a long time ago!

-roger

Igor Rivin wrote:

> I have a 100Mb comma-separated file, and R takes several minutes to read it
> (via read.table()). This is R 1.9.0 on a linux box with a couple gigabytes of
> RAM. I am conjecturing that R is gc-ing, so maybe there is some command-line
> arg I can give it to convince it that I have a lot of space, or?!
> 
>     Thanks!
> 
> 	Igor
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Roger D. Peng
http://www.biostat.jhsph.edu/~rpeng/



From dmurdoch at pair.com  Wed Jun 30 04:24:06 2004
From: dmurdoch at pair.com (Duncan Murdoch)
Date: Tue, 29 Jun 2004 22:24:06 -0400
Subject: [R] naive question
In-Reply-To: <C698D707214E6F4AB39AB7096C3DE5A5568C3A@phost015.EVAFUNDS.intermedia.net>
References: <C698D707214E6F4AB39AB7096C3DE5A5568C3A@phost015.EVAFUNDS.intermedia.net>
Message-ID: <9j84e01pr16iqlqq48ckdk20qred1con0u@4ax.com>

On Tue, 29 Jun 2004 16:59:58 -0700, "Vadim Ogranovich"
<vograno at evafunds.com> wrote:

> R's IO is indeed 20 - 50 times slower than that of equivalent C code no
>matter what you do, which has been a pain for some of us. 

Things like this shouldn't be a pain for long.  If C code works well,
why not use C?  It wouldn't be hard to write two C functions that 
1. counted the lines and 2. read them into preallocated vectors. 

Doing it this way you could use .C, you don't need to learn the
intricacies of .Call, and it should be about half the speed (since it
takes two passes) of fast C code, i.e. 10-25 times faster than the
read.* functions.

Then, if you felt really ambitious, you could write it in a way that
others could use, put it in a package, and suddenly R would have I/O
10-25 times faster than it does now.  You wouldn't try to make it as
flexible as current R code, but for reading these huge files people
are talking about, it would be worthwhile to go through a few extra
setup steps.  

Duncan Murdoch



From rivin at euclid.math.temple.edu  Wed Jun 30 04:31:53 2004
From: rivin at euclid.math.temple.edu (rivin@euclid.math.temple.edu)
Date: Tue, 29 Jun 2004 22:31:53 -0400 (EDT)
Subject: [R] naive question
In-Reply-To: <40E22027.2020008@jhsph.edu>
References: <20040629163539.82DE32DD8C@euclid.math.temple.edu>
	<40E22027.2020008@jhsph.edu>
Message-ID: <60880.66.93.84.30.1088562713.squirrel@www.math.temple.edu>

> We need more details about your problem to provide any useful
> help.  Are all the variables numeric?  Are they all completely
> different?  Is it possible to use `colClasses'?
>
It is possible, but very inconvenient. There are mostly numeric columns,
but some integer categories, and some string names. The total number is
high, so doing this by hand would take several minutes as well, so a
different solution is preferable. I did use as.is=TRUE, but that did not
seem to make a huge difference.
> Also, having "a couple of gigabytes of RAM" is not necessarily
> useful if you're on a 32-bit OS since the total process size is
> usually limited to be less than ~3GB.

True. "top" shows that the maximal memory usage for the process is about
700MB, so process size was not a limitation (but had I 512Mb, the
thrashing would have killed me...)
>
> Believe it or not, complaints like these are not that common.
> 1998 was a long time ago!

Alas...

>
> -roger
>
> Igor Rivin wrote:
>
>> I have a 100Mb comma-separated file, and R takes several minutes to
>> read it (via read.table()). This is R 1.9.0 on a linux box with a
>> couple gigabytes of RAM. I am conjecturing that R is gc-ing, so maybe
>> there is some command-line arg I can give it to convince it that I
>> have a lot of space, or?!
>>
>>     Thanks!
>>
>> 	Igor
>>
>> ______________________________________________
>> R-help at stat.math.ethz.ch mailing list
>> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>> PLEASE do read the posting guide!
>> http://www.R-project.org/posting-guide.html
>>
>
> --
> Roger D. Peng
> http://www.biostat.jhsph.edu/~rpeng/



From pwilkinson at videotron.ca  Wed Jun 30 04:38:22 2004
From: pwilkinson at videotron.ca (Peter Wilkinson)
Date: Tue, 29 Jun 2004 22:38:22 -0400
Subject: [R] naive question
In-Reply-To: <40E22027.2020008@jhsph.edu>
References: <20040629163539.82DE32DD8C@euclid.math.temple.edu>
	<40E22027.2020008@jhsph.edu>
Message-ID: <6.1.1.1.2.20040629223254.01bb4ec0@pop.videotron.ca>


>Also, having "a couple of gigabytes of RAM" is not necessarily useful if 
>you're on a 32-bit OS since the total process size is usually limited to 
>be less than ~3GB.

well 2^32 gives you more like 4 GB, how much of that can be given to a 
process ....  my highest workspace reached 1.2 Gig. I will add another Gig 
... or 2  ....

I am assuming that R can address more than 2 Gig memory,  does anybody know 
if R has some other limitation that might be lower than the OS?

Peter



From rivin at euclid.math.temple.edu  Wed Jun 30 04:46:11 2004
From: rivin at euclid.math.temple.edu (rivin@euclid.math.temple.edu)
Date: Tue, 29 Jun 2004 22:46:11 -0400 (EDT)
Subject: [R] naive question
In-Reply-To: <40E20FA9.1030609@stat.wisc.edu>
References: <3A822319EB35174CA3714066D590DCD504AF7FB2@usrymx25.merck.com>
	<40E2070E.mailCK411OTFQ@euclid.math.temple.edu>
	<40E20FA9.1030609@stat.wisc.edu>
Message-ID: <60885.66.93.84.30.1088563571.squirrel@www.math.temple.edu>

> Igor Rivin wrote:
>
>> I was not particularly annoyed, just disappointed, since R seems like
>> a much better thing than SAS in general, and doing everything with a
>> combination of hand-rolled tools is too much work. However, I do need
>> to work with very large data sets, and if it takes 20 minutes to read
>> them in, I have to explore other options (one of which might be
>> S-PLUS, which claims scalability as a major  , er, PLUS over R).
>
>
> If you are routinely working with very large data sets it would be
> worthwhile learning to use a relational database (PostgreSQL, MySQL,
> even Access) to store the data and then access it from R with RODBC or
> one of the specialized database packages.
>
I was thinking about that, but I had thought that this would help for
reading small pieces of the data (since subsetting would happen on the db
side), but not so much for reading big chunks. But it's certainly worth a
try....

> R is slow reading ASCII files because it is assembling the meta-data on
> the fly and it is continually checking the types of the variables being
> read.  If you know all this information and build it into your table
> definitions, reading the data will be much faster.

What do you mean by meta-data? Anyway, I agree that this would slow it
down, but I would suspect that even so there is a bit of room for
improvement, since five minutes for 12 million tokens comes out to
40000/second, which is really pretty bad on a 2-3 Ghz machine...
>
> A disadvantage of this approach is the need to learn yet another
> language and system.  I was going to do an example but found I could not
>  because I left all my SQL books at home (I'm travelling at the moment)
> and I couldn't remember the particular commands for loading a table from
>  an ASCII file.

Well, I will look into it (among other possibilities).



From p.connolly at hortresearch.co.nz  Wed Jun 30 04:53:14 2004
From: p.connolly at hortresearch.co.nz (Patrick Connolly)
Date: Wed, 30 Jun 2004 14:53:14 +1200
Subject: [R] naive question
In-Reply-To: <60880.66.93.84.30.1088562713.squirrel@www.math.temple.edu>;
	from rivin@euclid.math.temple.edu on Tue, Jun 29,
	2004 at 10:31:53PM -0400
References: <20040629163539.82DE32DD8C@euclid.math.temple.edu>
	<40E22027.2020008@jhsph.edu>
	<60880.66.93.84.30.1088562713.squirrel@www.math.temple.edu>
Message-ID: <20040630145314.A11533@hortresearch.co.nz>

On Tue, 29-Jun-2004 at 10:31PM -0400, rivin at euclid.math.temple.edu wrote:

|> > We need more details about your problem to provide any useful
|> > help.  Are all the variables numeric?  Are they all completely
|> > different?  Is it possible to use `colClasses'?
|> >
|> It is possible, but very inconvenient. There are mostly numeric columns,
|> but some integer categories, and some string names. The total number is
|> high, so doing this by hand would take several minutes as well, so a
|> different solution is preferable. I did use as.is=TRUE, but that did not

For the lazy typist, here's an idea:

Make a small subset of the datafile (say the first 20 rows) and read
that in with read.table.

X <- read.table("blah.txt", header = TRUE, ........)

Xclasses <- sapply(X, class)

Now we have a nice long vector that you can use in your colClasses
argument with the whole data.  Even if it needs a bit of editing, it
will save you typing in all those "numeric" strings.

HTH

-- 
Patrick Connolly
HortResearch
Mt Albert
Auckland
New Zealand 
Ph: +64-9 815 4200 x 7188
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~
I have the world`s largest collection of seashells. I keep it on all
the beaches of the world ... Perhaps you`ve seen it.  ---Steven Wright 
~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~.~



From rivin at euclid.math.temple.edu  Wed Jun 30 05:04:19 2004
From: rivin at euclid.math.temple.edu (rivin@euclid.math.temple.edu)
Date: Tue, 29 Jun 2004 23:04:19 -0400 (EDT)
Subject: [R] naive question
In-Reply-To: <20040630145314.A11533@hortresearch.co.nz>
References: <20040629163539.82DE32DD8C@euclid.math.temple.edu>
	<40E22027.2020008@jhsph.edu>
	<60880.66.93.84.30.1088562713.squirrel@www.math.temple.edu>
	<20040630145314.A11533@hortresearch.co.nz>
Message-ID: <60890.66.93.84.30.1088564659.squirrel@www.math.temple.edu>

> On Tue, 29-Jun-2004 at 10:31PM -0400, rivin at euclid.math.temple.edu
> wrote:
>
> |> > We need more details about your problem to provide any useful |> >
> help.  Are all the variables numeric?  Are they all completely |> >
> different?  Is it possible to use `colClasses'?
> |> >
> |> It is possible, but very inconvenient. There are mostly numeric
> columns, |> but some integer categories, and some string names. The
> total number is |> high, so doing this by hand would take several
> minutes as well, so a |> different solution is preferable. I did use
> as.is=TRUE, but that did not
>
> For the lazy typist, here's an idea:
>
> Make a small subset of the datafile (say the first 20 rows) and read
> that in with read.table.
>
> X <- read.table("blah.txt", header = TRUE, ........)
>
> Xclasses <- sapply(X, class)
>
> Now we have a nice long vector that you can use in your colClasses
> argument with the whole data.  Even if it needs a bit of editing, it
> will save you typing in all those "numeric" strings.
>
> HTH
>
Aha! That could be the right trick. I will try it and see how it works...

  Thanks,

      Igor



From rssolis at earthlink.net  Wed Jun 30 05:32:12 2004
From: rssolis at earthlink.net (Ruben Solis)
Date: Tue, 29 Jun 2004 22:32:12 -0500
Subject: [R] MacOS X binaries won't install
Message-ID: <135A2A15-CA46-11D8-934D-000A95E26AF6@earthlink.net>

I've tried installing the MacOS X binaries for R available at:

http://www.bioconductor.org/CRAN/

I'm running MacOS X version 10.2.8.

I get a message indicating the installation is successful, but when I 
double-click on the R icon that shows up in my Applications folder, the 
application seems to try to open but closes immediately.

I looked for  /Library/Frameworks/R.framework (by typing ls 
/Library/Frameworks) and it does not appear.  A global search for 
R.framework yields no results, so it seems that the installation is not 
working. (I was going to try command line execution.)

Any help would be appreciated.  Thanks! - RSS

Ruben S. Solis



From tmchoi at ris.chonnam.ac.kr  Wed Jun 30 05:56:52 2004
From: tmchoi at ris.chonnam.ac.kr (Taemyong Choi)
Date: Wed, 30 Jun 2004 12:56:52 +0900
Subject: [R] gap() in SAGx package - How to handle this situation?
Message-ID: <200406300356.i5U3u6Vt028959@ris.chonnam.ac.kr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040630/3f838944/attachment.pl

From ksm32 at student.canterbury.ac.nz  Wed Jun 30 06:45:43 2004
From: ksm32 at student.canterbury.ac.nz (Karla Meurk)
Date: Wed, 30 Jun 2004 16:45:43 +1200
Subject: [R] funny plotting
Message-ID: <40E24577.5050209@student.canterbury.ac.nz>

Hi, I just wanted to plot a boxplot with a nice curve going through it, 
I thought this would be a simple task but for some reason I can't get 
the two graphs on the same page accurately. Enclosed is the code showing 
the two plots seperately and together.  I would have thought it should 
work if I could use boxplot() then plot() overlayed but it won't allow 
the command add=TRUE (which has worked for me in the past).

Thanks

Carla

P.S. please excuse the clumsy code!

#Section 2 Data Set particle

dial<-rbind(-1,
-1,
0,
0,
0,
0,
1,
1,
1)
counts<-rbind(2,
3,
6,
7,
8,
9,
10,
12,
15)
particle<-as.data.frame(cbind(dial,counts),row.names=NULL)
names(particle)<-c("dial","counts")
attach(particle)

pois.particle<-glm(counts~dial,family=poisson)
x<-seq(-2,2,length=20)
y<-predict(pois.particle,data.frame(dial=x),type="response")

#Overlaying plots
x11()
boxplot(counts~dial,main="Boxplot of counts for dial setting and poisson 
fit",ylim=c(0,25))
lines(x,y)

#The seperate plots
x11()
boxplot(counts~dial,ylim=c(0,25))
x11()
plot(x,y,ylim=c(0,25),type="l")



From maustin at amgen.com  Wed Jun 30 07:40:21 2004
From: maustin at amgen.com (Austin, Matt)
Date: Tue, 29 Jun 2004 22:40:21 -0700
Subject: [R] funny plotting
Message-ID: <E7D5AB4811D20B489622AABA9C53859101F10F3E@teal-exch.amgen.com>

Try something like

plot(x,y)
box.dat <- boxplot(x=split(counts, dial), plot=FALSE)
bxp(box.dat, add=TRUE, at=c(-1, 0, 1), show.names=FALSE)

-----Original Message-----
From: r-help-bounces at stat.math.ethz.ch
[mailto:r-help-bounces at stat.math.ethz.ch]On Behalf Of Karla Meurk
Sent: Tuesday, June 29, 2004 21:46 PM
To: R-help at stat.math.ethz.ch
Subject: [R] funny plotting


Hi, I just wanted to plot a boxplot with a nice curve going through it, 
I thought this would be a simple task but for some reason I can't get 
the two graphs on the same page accurately. Enclosed is the code showing 
the two plots seperately and together.  I would have thought it should 
work if I could use boxplot() then plot() overlayed but it won't allow 
the command add=TRUE (which has worked for me in the past).

Thanks

Carla

P.S. please excuse the clumsy code!

#Section 2 Data Set particle

dial<-rbind(-1,
-1,
0,
0,
0,
0,
1,
1,
1)
counts<-rbind(2,
3,
6,
7,
8,
9,
10,
12,
15)
particle<-as.data.frame(cbind(dial,counts),row.names=NULL)
names(particle)<-c("dial","counts")
attach(particle)

pois.particle<-glm(counts~dial,family=poisson)
x<-seq(-2,2,length=20)
y<-predict(pois.particle,data.frame(dial=x),type="response")

#Overlaying plots
x11()
boxplot(counts~dial,main="Boxplot of counts for dial setting and poisson 
fit",ylim=c(0,25))
lines(x,y)

#The seperate plots
x11()
boxplot(counts~dial,ylim=c(0,25))
x11()
plot(x,y,ylim=c(0,25),type="l")

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From g0301021 at nus.edu.sg  Wed Jun 30 08:48:42 2004
From: g0301021 at nus.edu.sg (Sankalp Chaturvedi)
Date: Wed, 30 Jun 2004 14:48:42 +0800
Subject: [R] Help!!
Message-ID: <199B30229FBE3B469594C9861EA44549210002@MBOX21.stu.nus.edu.sg>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040630/3d62dfee/attachment.pl

From ggrothendieck at myway.com  Wed Jun 30 08:51:30 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 30 Jun 2004 06:51:30 +0000 (UTC)
Subject: [R] nls fitting problems (singularity)
References: <20040629235547.47951.qmail@web52510.mail.yahoo.com>
Message-ID: <loom.20040630T084819-79@post.gmane.org>


Have a look at optim (which supports a number of different algorithms via
the method= arg) and segmented in package segmented which does segmented
regression.

For example,

ss <- function(par) {
	b <- par[1]; c1 <- par[2]; c2 <- par[3]; d <- par[4]
	x <- df1$x; y <- df1$y
	sum((y - (d+(x-b)*c1*(x-b<0)+(x-b)*c2*(x-b>=0)))^2)
}
optim(sl,ss)



Karl Knoblick <karlknoblich <at> yahoo.de> writes:

: 
: Hallo!
: 
: I have a problem with fitting data with nls. The first
: example with y1 (data frame df1) shows an error, the
: second works fine.
: 
: Is there a possibility to get a fit (e.g. JMP can fit
: also data I can not manage to fit with R). Sometimes I
: also got an error singularity with starting
: parameters.
: 
: # x-values
: x<-c(-1,5,8,11,13,15,16,17,18,19,21,22)
: # y1-values (first data set)
: y1=c(-55,-22,-13,-11,-9.7,-1.4,-0.22,5.3,8.5,10,14,20)
: # y2-values (second data set)
: y2=c(-92,-42,-15,1.3,2.7,8.7,9.7,13,11,19,18,22)
: 
: # data frames
: df1<-data.frame(x=x, y=y1)
: df2<-data.frame(x=x, y=y2)
: 
: # start list for parameters
: sl<-list( d=0, b=10, c1=90, c2=20) 
: 
: # y1-Analysis - Result: Error in ...  singular
: gradient
: nls(y~d+(x-b)*c1*(x-b<0)+(x-b)*c2*(x-b>=0), data=df1,
: start=sl)
: # y2-Analysis - Result: working...
: nls(y~d+(x-b)*c1*(x-b<0)+(x-b)*c2*(x-b>=0), data=df2,
: start=sl)
: 
: # plots to look at data
: par(mfrow=c(1,2))
: plot(df1$x,df1$y)
: plot(df2$x,df2$y)
: 
: Perhaps there is another fitting routine? Can anybody
: help?
: 
: Best wishes,
: Karl
: 
: 
: 	
: 
: 	
: 		
: ___________________________________________________________
: Bestellen Sie Y! DSL und erhalten Sie die AVM "FritzBox SL" fr 0.
: Sie sparen 119 und bekommen 2 Monate Grundgebhrbefreiung.
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://www.stat.math.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
: 
:



From ggrothendieck at myway.com  Wed Jun 30 09:13:10 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 30 Jun 2004 07:13:10 +0000 (UTC)
Subject: [R] naive question
References: <3A822319EB35174CA3714066D590DCD504AF7FB2@usrymx25.merck.com>
	<40E2070E.mailCK411OTFQ@euclid.math.temple.edu>
	<40E20FA9.1030609@stat.wisc.edu>
Message-ID: <loom.20040630T085211-832@post.gmane.org>

Douglas Bates <bates <at> stat.wisc.edu> writes:

: If you are routinely working with very large data sets it would be 
: worthwhile learning to use a relational database (PostgreSQL, MySQL, 
: even Access) to store the data and then access it from R with RODBC or 
: one of the specialized database packages.

Along this line, if you have and already know another stat package then
you could read the data into that, write it out in that package's format
and then read the written out file into R.  The R foreign package 
and the R Hmisc package support a number of such foreign formats.
Don't know about speed -- it probably varies by format so you will
have to experiment. 

Not sure what the current status is of HDF5, CDF, netCDF, etc. are for R but
those may or may not be formats to consider as well.



From Matthias.Templ at statistik.gv.at  Wed Jun 30 09:34:49 2004
From: Matthias.Templ at statistik.gv.at (TEMPL Matthias)
Date: Wed, 30 Jun 2004 09:34:49 +0200
Subject: [R] anti-R vitriol
Message-ID: <83536658864BC243BE3C06D7E936ABD50153A604@xchg1.statistik.local>

Hi,

I wonder, why SAS should be better in time for reading a data in the system.
I have an example, that shows that R is (sometimes?, always?) faster.

-----------------
Data with 14432 observations and 120 variables.
Time for reading the data:

SAS 8e:
data testt;
set l1.lse01;run;

     	real time           1.46 seconds
      cpu time            0.18 seconds

R 1.9.0:
system.time(read.table("lse01.txt",header=T"))
[1] 0.63 0.06 6.22   NA   NA

----------------
And this is 2.5 times faster as SAS. 
(SAS reads the .sas7bdat and R the .txt file)

I??m working with SAS (I should working with SAS) and R (I'm going to work with R) on the same Computer. In my examples about time series and in something simple but also time consuming procedures like summaries,... R is always 2 times faster and sometimes 30 times faster (with the same results).
I think R is a great software and you can do more things as in SAS.
Some new developments in SAS 9, like COM-server to Excel, some new procedures, better graphs, ... is developed and implemented in R for many years ago.
Thanks to the R Development Team!!!

Matthias

> -----Urspr??ngliche Nachricht-----
> Von: Liaw, Andy [mailto:andy_liaw at merck.com] 
> Gesendet: Dienstag, 29. Juni 2004 20:21
> An: 'Barry Rowlingson'; R-help
> Betreff: RE: [R] anti-R vitriol
> 
> 
> > From: Barry Rowlingson
> > 
> > A colleague is receiving some data from another person. That person
> > reads the data in SAS and it takes 30s and uses 64k RAM. 
> That person 
> > then tries to read the data in R and it takes 10 minutes and uses a 
> > gigabyte of RAM. Person then goes on to say:
> > 
> >    It's not that I think SAS is such great software,
> >    it's not.  But I really hate badly designed
> >    software.  R is designed by committee.  Worse,
> >    it's designed by a committee of statisticians.
> >    They tend to confuse numerical analysis with
> >    computer science and don't have any idea about
> >    software development at all.  The result is R.
> > 
> >    I do hope [your colleague] won't have to waste time doing
> >    [this analysis] in an outdated and poorly designed piece
> >    of software like R.
> > 
> > Would any of the "committee" like to respond to this? Or
> > shall we just 
> > slap our collective forehead and wonder how someone could get 
> > such a view?
> > 
> > Barry
>  
> 
> My $0.02:
> 
> R, being a flexible programming language, has an amazing 
> ability to cope with people's laziness/ignorance/inelegance, 
> but it comes at a (sometimes
> hefty) price.  While there is no specifics on the situation 
> leading to the person's comments, here's one (not as extreme) 
> example that I happen to come across today:
> 
> > system.time(spam <- read.table("data_dmc2003_train.txt",
> +                                 header=T, 
> +                                 colClasses=c(rep("numeric", 833), 
> +                                              "character")))
> [1] 15.92  0.09 16.80    NA    NA
> > system.time(spam <- read.table("data_dmc2003_train.txt", header=T))
> [1] 187.29   0.60 200.19     NA     NA
> 
> My SAS ability is rather serverely limited, but AFAIK, one 
> needs to specify _all_ variables to be read into a dataset in 
> order to read in the data in SAS.  If one has that 
> information, R can be very efficient as well.  Without that 
> information, one gets nothing in SAS, or just let R does the 
> hard work.
> 
> Best,
> Andy
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list 
> https://www.stat.math.ethz.ch/mailman/listinfo> /r-help
> PLEASE 
> do read the posting guide! 
> http://www.R-project.org/posting-guide.html
>



From franck.siclon at paris7.jussieu.fr  Wed Jun 30 09:48:21 2004
From: franck.siclon at paris7.jussieu.fr (Franck Siclon)
Date: Wed, 30 Jun 2004 09:48:21 +0200
Subject: [R] Large addresses (Rgui.exe editing)
Message-ID: <6.0.0.22.2.20040630094649.01bf0b58@paris7.jussieu.fr>

I'm running R under a 32-bit Win2003 Server with 24 Go RAM ...
I read the FAQ and found that I have to edit my boot.ini file with the 
"/PAE" switch; Done.
I noticed also that I have to set the R image header with the flag 
"/LARGEADDRESSAWARE", but to do this I need Microsoft Visual Studio 6.0 
software that I do not have ... Because Pr Ripley clearly mentionned the 
steps to follow in the rw-FAQ, I'm wonderring whether someone has already 
edited the R executable in that way, and thus could share this "new exe 
file" with others ??
Many thanks !



From varrinmuriel at yahoo.fr  Wed Jun 30 09:51:47 2004
From: varrinmuriel at yahoo.fr (=?iso-8859-1?q?Varrin=20muriel?=)
Date: Wed, 30 Jun 2004 09:51:47 +0200 (CEST)
Subject: [R] GLM problem
Message-ID: <20040630075147.11149.qmail@web53101.mail.yahoo.com>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040630/0482cdf2/attachment.pl

From ripley at stats.ox.ac.uk  Wed Jun 30 10:23:29 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 30 Jun 2004 09:23:29 +0100 (BST)
Subject: [R] Large addresses (Rgui.exe editing)
In-Reply-To: <6.0.0.22.2.20040630094649.01bf0b58@paris7.jussieu.fr>
Message-ID: <Pine.LNX.4.44.0406300915590.28184-100000@gannet.stats>

It's undesirable to need Visual Studio (I suspect later versions than 6
also work), and Duncan Murdoch wrote some standalone code to do this which
he may be willing to share (but he is away until next week).

Meanwhile I have put edited .exe's for rw1091 at

http://www.stats.ox.ac.uk/pub/RWin/EditedEXEs.zip

but will not leave them there for ever.

On Wed, 30 Jun 2004, Franck Siclon wrote:

> I'm running R under a 32-bit Win2003 Server with 24 Go RAM ...
> I read the FAQ and found that I have to edit my boot.ini file with the 
> "/PAE" switch; Done.
> I noticed also that I have to set the R image header with the flag 
> "/LARGEADDRESSAWARE", but to do this I need Microsoft Visual Studio 6.0 
> software that I do not have ... Because Pr Ripley clearly mentionned the 
> steps to follow in the rw-FAQ, I'm wonderring whether someone has already 
> edited the R executable in that way, and thus could share this "new exe 
> file" with others ??
> Many thanks !

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From ripley at stats.ox.ac.uk  Wed Jun 30 10:31:20 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 30 Jun 2004 09:31:20 +0100 (BST)
Subject: [R] GLM problem
In-Reply-To: <20040630075147.11149.qmail@web53101.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0406300923500.28184-100000@gannet.stats>

On Wed, 30 Jun 2004, Varrin muriel wrote:

> HI, I am a studient, so don't be surprise if my question seems so simple
> for you...

> I have a dataframe with 6 qualitative variables divided in 33
> modalities, 2 qualitatives variables and 78 lines. I use a glm to know
> wich variables have interactions... I would like to know if its normal
> that one (the first in alphabetical order) of the modalities of each
> qualitatives variable doesn't appear in the results?

Yes.  That's the way R codes variables.  I'm not going to rewrite the
literature here, so please consult a good book (and naturally I will
recommend Venables & Ripley, 2002).  Others may be able to tell you if any
of the French guides on CRAN or elsewhere address this point (Emmanuel
Paradis's seems not to get that far).

> Here is what i did:
> glm<-glm(data$IP~data$temp*data$fvent*data$dirvent*data$Ensol+data$mois*data$mil, 
> family=gaussian)

Please use something like

glm(IP ~ temp+fvent, data=data, family=gaussian)

and don't call the result `glm' (or call your data, `data').  Also, this
is not a question about GLMs with that family but about linear models, so
why not use lm.

> summary(glm)
> anova(glm, test="Chisq")
> thanck you a lot and excuse me again for my little and perhaps simple question
> muriel varrin (french student in biostatistical)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From p.dalgaard at biostat.ku.dk  Wed Jun 30 10:50:45 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Jun 2004 10:50:45 +0200
Subject: [R] GLM problem
In-Reply-To: <Pine.LNX.4.44.0406300923500.28184-100000@gannet.stats>
References: <Pine.LNX.4.44.0406300923500.28184-100000@gannet.stats>
Message-ID: <x24qotsa3e.fsf@biostat.ku.dk>

Prof Brian Ripley <ripley at stats.ox.ac.uk> writes:

> and don't call the result `glm' (or call your data, `data').  Also, this
> is not a question about GLMs with that family but about linear models, so
> why not use lm.
> 
> > summary(glm)
> > anova(glm, test="Chisq")
                     *****

And shouldn't this be "F"? Do you really know that your observations
have variance 1?

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From nicolas.pelay at rd.francetelecom.com  Wed Jun 30 10:59:16 2004
From: nicolas.pelay at rd.francetelecom.com (zze-PELAY Nicolas FTRD/DMR/BEL)
Date: Wed, 30 Jun 2004 10:59:16 +0200
Subject: [R] Question about mesurating time
Message-ID: <D4EF10CCEB2CE742BEEA9838C590B0E802396E92@ftrdmel2.rd.francetelecom.fr>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040630/895ce135/attachment.pl

From wolski at molgen.mpg.de  Wed Jun 30 11:05:07 2004
From: wolski at molgen.mpg.de (Wolski)
Date: Wed, 30 Jun 2004 11:05:07 +0200
Subject: [R] Question about mesurating time
In-Reply-To: <D4EF10CCEB2CE742BEEA9838C590B0E802396E92@ftrdmel2.rd.francetelecom.fr>
References: <D4EF10CCEB2CE742BEEA9838C590B0E802396E92@ftrdmel2.rd.francetelecom.fr>
Message-ID: <200406301105070315.0AD5AA88@mail.math.fu-berlin.de>

?system.time



*********** REPLY SEPARATOR  ***********

On 30.06.2004 at 10:59 zze-PELAY Nicolas FTRD/DMR/BEL wrote:

>Hello , 
>Is there any function to mesurate the duration of a procedure (like tic
>and toc in matlab) ?
>
>Tic
>Source("procedure.R")
>Toc
>
>(toc is the duration between the execution of tic and the execution of
>toc)
>
>Thank you
>
>nicolas
>
>	[[alternative HTML version deleted]]
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ripley at stats.ox.ac.uk  Wed Jun 30 11:10:11 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 30 Jun 2004 10:10:11 +0100 (BST)
Subject: [R] Question about mesurating time
In-Reply-To: <D4EF10CCEB2CE742BEEA9838C590B0E802396E92@ftrdmel2.rd.francetelecom.fr>
Message-ID: <Pine.LNX.4.44.0406301003530.28489-100000@gannet.stats>

help.search("time")

gives

proc.time(base)         Running Time of R
system.time(base)       CPU Time Used

both of which could be used.

On Wed, 30 Jun 2004, zze-PELAY Nicolas FTRD/DMR/BEL wrote:

> Is there any function to mesurate the duration of a procedure (like tic
> and toc in matlab) ?
> 
> Tic
> Source("procedure.R")
> Toc
> 
> (toc is the duration between the execution of tic and the execution of
> toc)

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From karlknoblich at yahoo.de  Wed Jun 30 11:13:34 2004
From: karlknoblich at yahoo.de (=?iso-8859-1?q?Karl=20Knoblick?=)
Date: Wed, 30 Jun 2004 11:13:34 +0200 (CEST)
Subject: [R] Help!!
Message-ID: <20040630091334.54610.qmail@web52506.mail.yahoo.com>

Hi!

Have you downloaded the package multilevel first?

Best wishes,
Karl


	

	
		
___________________________________________________________
Bestellen Sie Y! DSL und erhalten Sie die AVM "FritzBox SL" f??r 0.
Sie sparen 119 und bekommen 2 Monate Grundgeb??hrbefreiung.



From Frank.Muehlau at aventis.com  Wed Jun 30 11:29:27 2004
From: Frank.Muehlau at aventis.com (Frank.Muehlau@aventis.com)
Date: Wed, 30 Jun 2004 11:29:27 +0200
Subject: [R] Installation of R-1.9.1.tgz
Message-ID: <CF488E309B18A14FA7BA13384A1C5BF9E4AE15@frasmxsusr10.pharma.aventis.com>



From plummer at iarc.fr  Wed Jun 30 11:31:59 2004
From: plummer at iarc.fr (Martyn Plummer)
Date: Wed, 30 Jun 2004 11:31:59 +0200
Subject: [R] rgl installation problems
In-Reply-To: <BAY22-F42MJ0VIWrllO000e2874@hotmail.com>
References: <BAY22-F42MJ0VIWrllO000e2874@hotmail.com>
Message-ID: <1088587919.2837.7.camel@nemo>

On Wed, 2004-06-30 at 00:52, E GCP wrote:
> Thanks for your replies. I do not HTML-ize my mail, but free email accounts 
> do that and there is not a switch to turn it off.  I apologize in advance.
> 
> I installed R from the redhat package provided by Martyn Plummer. It 
> installed fine and without problems. I can use R and have installed and used 
> other packages within R without any problems whatsoever. I do not think the 
> problem is with R or its installation.   I do think there is a problem with 
> the installation of rgl_0.64-13.tar.gz on RedHat 9 (linux).  So, if there is 
> anybody out there who has installed succesfully rgl_0.64-13.tar.gz on RedHat 
> 9, I would like to know how.

This is a little strange.  I'm now building RPMS for older Red Hat
versions on FC2 using a tool called Mach.  There is a possibility that
there is some configuration problem, but I can't see it. As Brian has
pointed out, you are missing the crucial "-shared" flag when building
the shared library for rgl. This comes from the line

SHLIB_CXXLDFLAGS = -shared

in the file /usr/lib/R/etc/Makeconf.  This is present in my latest RPM
for Red Hat ( R-1.9.1-0.fdr.2.rh90.i386.rpm ) so I don't know why it
isn't working for you.

Check that you do have the latest RPM and that you don't have a locally
built version of R in /usr/local since this will have precedence on your
PATH.

Martyn



From p.dalgaard at biostat.ku.dk  Wed Jun 30 12:20:39 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Jun 2004 12:20:39 +0200
Subject: [R] rgl installation problems
In-Reply-To: <1088587919.2837.7.camel@nemo>
References: <BAY22-F42MJ0VIWrllO000e2874@hotmail.com>
	<1088587919.2837.7.camel@nemo>
Message-ID: <x2vfh9qrd4.fsf@biostat.ku.dk>

Martyn Plummer <plummer at iarc.fr> writes:

> On Wed, 2004-06-30 at 00:52, E GCP wrote:
> > Thanks for your replies. I do not HTML-ize my mail, but free email accounts 
> > do that and there is not a switch to turn it off.  I apologize in advance.
> > 
> > I installed R from the redhat package provided by Martyn Plummer. It 
> > installed fine and without problems. I can use R and have installed and used 
> > other packages within R without any problems whatsoever. I do not think the 
> > problem is with R or its installation.   I do think there is a problem with 
> > the installation of rgl_0.64-13.tar.gz on RedHat 9 (linux).  So, if there is 
> > anybody out there who has installed succesfully rgl_0.64-13.tar.gz on RedHat 
> > 9, I would like to know how.
> 
> This is a little strange.  I'm now building RPMS for older Red Hat
> versions on FC2 using a tool called Mach.  There is a possibility that
> there is some configuration problem, but I can't see it. As Brian has
> pointed out, you are missing the crucial "-shared" flag when building
> the shared library for rgl. This comes from the line
> 
> SHLIB_CXXLDFLAGS = -shared
> 
> in the file /usr/lib/R/etc/Makeconf.  This is present in my latest RPM
> for Red Hat ( R-1.9.1-0.fdr.2.rh90.i386.rpm ) so I don't know why it
> isn't working for you.
> 
> Check that you do have the latest RPM and that you don't have a locally
> built version of R in /usr/local since this will have precedence on your
> PATH.

One thing to notice is that rgl does its own configure step (hadn't
noticed on my first build since g++ is so noisy about a number of
other things), and it is possible that something is getting messed up
on E's RH9 system.

Redirecting the output as in

R CMD INSTALL rgll_0.64-13.tar.gz 2>&1 | tee rgl.log

and poring over the result might prove informative.

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From john.maindonald at anu.edu.au  Wed Jun 30 13:46:27 2004
From: john.maindonald at anu.edu.au (John Maindonald)
Date: Wed, 30 Jun 2004 21:46:27 +1000
Subject: [R] anti-R vitriol
In-Reply-To: <200406301002.i5UA1jS3031849@hypatia.math.ethz.ch>
References: <200406301002.i5UA1jS3031849@hypatia.math.ethz.ch>
Message-ID: <1F2FA2F2-CA8B-11D8-842E-000A95CDA0F2@anu.edu.au>

I am curious.  What were the dimensions of this data set?  Did this 
person know use read.table(), or scan().  Did they know about the 
possibility of reading the data one part at a time?

The way that SAS processes the data row by row limits what can be done. 
  It is often possible with scant loss of information, and more 
satisfactory, to work with a subset of the large data set or with 
multiple subsets.  Neither SAS (in my somewhat dated experience of it) 
nor R is entirely satisfactory for this purpose.  But at least in R, 
given a subset that fits so easily into memory that the graphs are not 
masses of black, there are few logistic problems in doing, rapidly and 
interactively, a variety of manipulations and plots, with each new task 
taking advantage of the learning that has gone before.  To do that well 
in the SAS world, it is necessary to use something like JMP or its 
equivalent in one of the newer modules, which process data in a way 
that is not all that different from R.

I have wondered about possibilities for a suite of functions that would 
make it easy to process through R data that is stored in one large data 
set, with a mix of adding a new variable or variables, repeating a 
calculation on successive subsets of the data, producing predictions or 
suchlike for separate subsets, etc. Database connections may be the way 
to go (c.f., the Ripley and Fei Chen paper at ISI 2003), but it might 
also be useful to have a simple set of functions that would handle some 
standard requirements.

John Maindonald.

On 30 Jun 2004, at 8:02 PM, Barry Rowlingson 
<B.Rowlingson at lancaster.ac.uk> wrote:

> A colleague is receiving some data from another person. That person 
> reads the data in SAS and it takes 30s and uses 64k RAM. That person 
> then tries to read the data in R and it takes 10 minutes and uses a 
> gigabyte of RAM. Person then goes on to say:
>
>   It's not that I think SAS is such great software,
>   it's not.  But I really hate badly designed
>   software.  R is designed by committee.  Worse,
>   it's designed by a committee of statisticians.
>   They tend to confuse numerical analysis with
>   computer science and don't have any idea about
>   software development at all.  The result is R.
>
>   I do hope [your colleague] won't have to waste time doing
>   [this analysis] in an outdated and poorly designed piece
>   of software like R.
>
> Would any of the "committee" like to respond to this? Or shall we just 
> slap our collective forehead and wonder how someone could get such a 
> view?
>
John Maindonald             email: john.maindonald at anu.edu.au
phone : +61 2 (6125)3473    fax  : +61 2(6125)5549
Centre for Bioinformation Science, Room 1194,
John Dedman Mathematical Sciences Building (Building 27)
Australian National University, Canberra ACT 0200.



From angel_lul at hotmail.com  Wed Jun 30 13:51:32 2004
From: angel_lul at hotmail.com (Angel Lopez)
Date: Wed, 30 Jun 2004 13:51:32 +0200
Subject: [R] pca with missing values
Message-ID: <40E2A944.7030805@hotmail.com>

I need to perform a principal components analysis on a matrix with 
missing values.
I've searched the R web site but didn't manage to find anything.
Any pointers/guidelines are much appreciatted.
Angel



From ripley at stats.ox.ac.uk  Wed Jun 30 14:11:33 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 30 Jun 2004 13:11:33 +0100 (BST)
Subject: [R] pca with missing values
In-Reply-To: <40E2A944.7030805@hotmail.com>
Message-ID: <Pine.LNX.4.44.0406301309080.15704-100000@gannet.stats>

On Wed, 30 Jun 2004, Angel Lopez wrote:

> I need to perform a principal components analysis on a matrix with 
> missing values.
> I've searched the R web site but didn't manage to find anything.

    ?princomp

has a description of an na.action argument, and 

    help.search("missing values")

comes up with several relevant entries.

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From bates at stat.wisc.edu  Wed Jun 30 15:32:43 2004
From: bates at stat.wisc.edu (Douglas Bates)
Date: Wed, 30 Jun 2004 08:32:43 -0500
Subject: [R] nls fitting problems (singularity)
In-Reply-To: <loom.20040630T084819-79@post.gmane.org>
References: <20040629235547.47951.qmail@web52510.mail.yahoo.com>
	<loom.20040630T084819-79@post.gmane.org>
Message-ID: <40E2C0FB.3060303@stat.wisc.edu>

Often when nls doesn't converge there is a good reason for it.

I'm on a very slow internet connection these days and will not be able 
to look at the data myself but I ask you to bear in mind that, when 
dealing with nonlinear models, there are model/data set combinations for 
which there are no parameter estimates.


Gabor Grothendieck wrote:
> Have a look at optim (which supports a number of different algorithms via
> the method= arg) and segmented in package segmented which does segmented
> regression.
> 
> For example,
> 
> ss <- function(par) {
> 	b <- par[1]; c1 <- par[2]; c2 <- par[3]; d <- par[4]
> 	x <- df1$x; y <- df1$y
> 	sum((y - (d+(x-b)*c1*(x-b<0)+(x-b)*c2*(x-b>=0)))^2)
> }
> optim(sl,ss)
> 
> 
> 
> Karl Knoblick <karlknoblich <at> yahoo.de> writes:
> 
> : 
> : Hallo!
> : 
> : I have a problem with fitting data with nls. The first
> : example with y1 (data frame df1) shows an error, the
> : second works fine.
> : 
> : Is there a possibility to get a fit (e.g. JMP can fit
> : also data I can not manage to fit with R). Sometimes I
> : also got an error singularity with starting
> : parameters.
> : 
> : # x-values
> : x<-c(-1,5,8,11,13,15,16,17,18,19,21,22)
> : # y1-values (first data set)
> : y1=c(-55,-22,-13,-11,-9.7,-1.4,-0.22,5.3,8.5,10,14,20)
> : # y2-values (second data set)
> : y2=c(-92,-42,-15,1.3,2.7,8.7,9.7,13,11,19,18,22)
> : 
> : # data frames
> : df1<-data.frame(x=x, y=y1)
> : df2<-data.frame(x=x, y=y2)
> : 
> : # start list for parameters
> : sl<-list( d=0, b=10, c1=90, c2=20) 
> : 
> : # y1-Analysis - Result: Error in ...  singular
> : gradient
> : nls(y~d+(x-b)*c1*(x-b<0)+(x-b)*c2*(x-b>=0), data=df1,
> : start=sl)
> : # y2-Analysis - Result: working...
> : nls(y~d+(x-b)*c1*(x-b<0)+(x-b)*c2*(x-b>=0), data=df2,
> : start=sl)
> : 
> : # plots to look at data
> : par(mfrow=c(1,2))
> : plot(df1$x,df1$y)
> : plot(df2$x,df2$y)
> : 
> : Perhaps there is another fitting routine? Can anybody
> : help?
> : 
> : Best wishes,
> : Karl
> : 
> : 
> : 	
> : 
> : 	
> : 		
> : ___________________________________________________________
> : Bestellen Sie Y! DSL und erhalten Sie die AVM "FritzBox SL" fr 0.
> : Sie sparen 119 und bekommen 2 Monate Grundgebhrbefreiung.
> : 
> : ______________________________________________
> : R-help <at> stat.math.ethz.ch mailing list
> : https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> : PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> : 
> :
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From umalvarez at fata.unam.mx  Wed Jun 30 15:43:11 2004
From: umalvarez at fata.unam.mx (Ulises Mora Alvarez)
Date: Wed, 30 Jun 2004 08:43:11 -0500 (CDT)
Subject: [R] MacOS X binaries won't install
In-Reply-To: <135A2A15-CA46-11D8-934D-000A95E26AF6@earthlink.net>
Message-ID: <Pine.LNX.4.44.0406300839490.8500-100000@athena.fata.unam.mx>

Hi!

I'm afraid we need some more bits of information. Do you have 
administrative privileges?, which version of R are you trying to install?, 
Have you read the RMacOSX FAQ?


On Tue, 29 Jun 2004, Ruben Solis wrote:

> I've tried installing the MacOS X binaries for R available at:
> 
> http://www.bioconductor.org/CRAN/
> 
> I'm running MacOS X version 10.2.8.
> 
> I get a message indicating the installation is successful, but when I 
> double-click on the R icon that shows up in my Applications folder, the 
> application seems to try to open but closes immediately.
> 
> I looked for  /Library/Frameworks/R.framework (by typing ls 
> /Library/Frameworks) and it does not appear.  A global search for 
> R.framework yields no results, so it seems that the installation is not 
> working. (I was going to try command line execution.)
> 
> Any help would be appreciated.  Thanks! - RSS
> 
> Ruben S. Solis
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 

-- 
Ulises M. Alvarez
LAB. DE ONDAS DE CHOQUE
FISICA APLICADA Y TECNOLOGIA AVANZADA
UNAM
umalvarez at fata.unam.mx



From p.dalgaard at biostat.ku.dk  Wed Jun 30 16:04:00 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Jun 2004 16:04:00 +0200
Subject: [R] nls fitting problems (singularity)
In-Reply-To: <40E2C0FB.3060303@stat.wisc.edu>
References: <20040629235547.47951.qmail@web52510.mail.yahoo.com>
	<loom.20040630T084819-79@post.gmane.org>
	<40E2C0FB.3060303@stat.wisc.edu>
Message-ID: <x2r7rxqh0v.fsf@biostat.ku.dk>

Douglas Bates <bates at stat.wisc.edu> writes:

> Often when nls doesn't converge there is a good reason for it.
> 
> I'm on a very slow internet connection these days and will not be able
> to look at the data myself but I ask you to bear in mind that, when
> dealing with nonlinear models, there are model/data set combinations
> for which there are no parameter estimates.


In this particular case, the model describes a curve consisting of two
line segments that meet at the point (b,d)

> > : nls(y~d+(x-b)*c1*(x-b<0)+(x-b)*c2*(x-b>=0), data=df2,

Now if b is between the two smallest x, you can diddle b, c1, and d in
such a way that the value at x1 is constant. I.e. the model becomes
unidentifiable. Putting trace=T suggests that this is what happens in
this example.


-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From rivin at euclid.math.temple.edu  Wed Jun 30 16:05:15 2004
From: rivin at euclid.math.temple.edu (rivin@euclid.math.temple.edu)
Date: Wed, 30 Jun 2004 10:05:15 -0400 (EDT)
Subject: [R] formatting
Message-ID: <35615.205.228.12.39.1088604315.squirrel@www.math.temple.edu>



I could not figure this out from the documentation: is there a way to send
formatted non-graphical data to a fancy output device (eg, latex, pdf...)
For example, if I want to include the summary of a linear model in a
document, I might want to have it automatically texified.

 Thanks!

    Igor



From tobias.verbeke at bivv.be  Wed Jun 30 16:10:35 2004
From: tobias.verbeke at bivv.be (tobias.verbeke@bivv.be)
Date: Wed, 30 Jun 2004 16:10:35 +0200
Subject: [R] formatting
In-Reply-To: <35615.205.228.12.39.1088604315.squirrel@www.math.temple.edu>
Message-ID: <OF21E799F3.E679FA17-ONC1256EC3.004D9577-C1256EC3.004DE230@BIVV.BE>





r-help-bounces at stat.math.ethz.ch wrote on 30/06/2004 16:05:15:

>
>
> I could not figure this out from the documentation: is there a way to
send
> formatted non-graphical data to a fancy output device (eg, latex, pdf...)
> For example, if I want to include the summary of a linear model in a
> document, I might want to have it automatically texified.

Have a look at the latex() function in package Hmisc,
and/or at package xtable

If you want to use R code in a LaTeX document, you can
use Sweave; see

http://www.ci.tuwien.ac.at/~leisch/Sweave/

HTH,
Tobias



From fzh113 at hecky.it.northwestern.edu  Wed Jun 30 16:32:26 2004
From: fzh113 at hecky.it.northwestern.edu (Fred)
Date: Wed, 30 Jun 2004 09:32:26 -0500
Subject: [R] Principal Surface function help
Message-ID: <005201c45eaf$117fea60$a7560d18@f0z6305>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040630/544e2be6/attachment.pl

From sasprog474 at yahoo.com  Wed Jun 30 16:37:12 2004
From: sasprog474 at yahoo.com (Greg Tarpinian)
Date: Wed, 30 Jun 2004 07:37:12 -0700 (PDT)
Subject: [R] outlier tests
Message-ID: <20040630143712.69471.qmail@web41403.mail.yahoo.com>

I have been learning about some outlier tests -- Dixon
and Grubb, specifically -- for small data sets.  When
I try help.start() and search for outlier tests, the
only response I manage to find is the Bonferroni test
avaiable from the CAR package...  are there any other
packages the offer outlier tests?  Are the Dixon and
Grubb tests "good" for small samples or are others
more
recommended?

Much thanks in advance,

     Greg



From ripley at stats.ox.ac.uk  Wed Jun 30 17:12:59 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 30 Jun 2004 16:12:59 +0100 (BST)
Subject: [R] outlier tests
In-Reply-To: <20040630143712.69471.qmail@web41403.mail.yahoo.com>
Message-ID: <Pine.LNX.4.44.0406301600330.14510-100000@gannet.stats>

I thought outlier tests were mainly superseded two decades ago by the use
of robust methods -- they certainly were in analytical chemistry, for
example.  All outlier tests are "bad" in the sense that outliers will
damage the results long before they are detected.  See e.g.

@Article{AMC.89a,
  author       = "{Analytical Methods Committee}",
  title        = "Robust statistics --- how not to reject outliers. {Part}
                  1. {Basic} concepts",
  journal      = "The Analyst",
  volume       = "114",
  pages        = "1693--1697",
  year         = "1989",
}


On Wed, 30 Jun 2004, Greg Tarpinian wrote:

> I have been learning about some outlier tests -- Dixon and Grubb,
> specifically -- for small data sets.  When I try help.start() and search
> for outlier tests, the only response I manage to find is the Bonferroni
> test avaiable from the CAR package...  are there any other packages the
> offer outlier tests?

That's not an outlier test in the sense used by Dixon and Grubb, but is an 
illustration of the point about robust methods being better, in this case 
protecting better against multiple outliers.

> Are the Dixon and Grubb tests "good" for small samples or are others
> more recommended?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From angel_lul at hotmail.com  Wed Jun 30 17:30:08 2004
From: angel_lul at hotmail.com (Angel Lopez)
Date: Wed, 30 Jun 2004 17:30:08 +0200
Subject: [R] camberra distance?
In-Reply-To: <200406290944390154.05657F06@mail.math.fu-berlin.de>
References: <200406290944390154.05657F06@mail.math.fu-berlin.de>
Message-ID: <40E2DC80.7040903@hotmail.com>

 From Legendre & Legendre Numerical Ecology I read: "The Australians 
Lance & Williams (1967a) give several variants of the Manhattan metric 
including their Canberra metric (Lance & Williams 1966c)"
Lance &Williams (1967a) Mixed-data classificatory programs 
I.Agglomerative systems. Aust. Comput.J.1:15-20
Lance & Williams 1966c Computer programs for classification. Proc. 
ANCCAC Conference, Canberra, May 1966, Paper 12/3

HTH,
Angel

Wolski wrote:
> Hi!
> 
> Its not an R specific question but had no idea where to ask elsewhere.
> 
> Does anyone know the orginal reference to the CAMBERA  DISTANCE?
> 
> Eryk.
> 
> Ps.:
> I knew that its an out of topic question (sorry).
> Can anyone reccomend a mailing list where such questions are in topic?
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
> 
> .
>



From tplate at blackmesacapital.com  Wed Jun 30 17:34:25 2004
From: tplate at blackmesacapital.com (Tony Plate)
Date: Wed, 30 Jun 2004 09:34:25 -0600
Subject: [R] naive question
In-Reply-To: <40E2070E.mailCK411OTFQ@euclid.math.temple.edu>
References: <3A822319EB35174CA3714066D590DCD504AF7FB2@usrymx25.merck.com>
	<40E2070E.mailCK411OTFQ@euclid.math.temple.edu>
Message-ID: <6.1.0.6.2.20040630092512.07a03b68@mailhost.blackmesacapital.com>

As far as I know, read.table() in S-plus performs similarly to read.table() 
in R with respect to speed.  So, I wouldn't put high hopes in finding much 
satisfaction there.

I do frequently read large tables in S-plus, and with a considerable amount 
of work was able to speed things up significantly, mainly by using scan() 
with appropriate arguments.  It's possible that some of the add-on modules 
for S-plus (e.g., the data-mining module) have faster I/O, but I haven't 
investigated those.  I get the best read performance out of S-plus by using 
a homegrown binary file format with each column stored in a contiguous 
block of memory and meta data (i.e., column types and dimensions) stored at 
the start of the file.  The S-plus read function reads the columns one at a 
time using readRaw(). One would be able to do something similar in R.  If 
you have to read from a text file, then, as others have suggested, writing 
a C program wouldn't be that hard, as long as you make the format inflexible.

-- Tony Plate

At Tuesday 06:19 PM 6/29/2004, Igor Rivin wrote:

>I was not particularly annoyed, just disappointed, since R seems like
>a much better thing than SAS in general, and doing everything with a 
>combination
>of hand-rolled tools is too much work. However, I do need to work with 
>very large data sets, and if it takes 20 minutes to read them in, I have 
>to explore other
>options (one of which might be S-PLUS, which claims scalability as a major
>, er, PLUS over R).
>
>______________________________________________
>R-help at stat.math.ethz.ch mailing list
>https://www.stat.math.ethz.ch/mailman/listinfo/r-help
>PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html



From ggrothendieck at myway.com  Wed Jun 30 17:48:44 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 30 Jun 2004 15:48:44 +0000 (UTC)
Subject: [R] naive question
References: <3A822319EB35174CA3714066D590DCD504AF7FB2@usrymx25.merck.com>
	<40E2070E.mailCK411OTFQ@euclid.math.temple.edu>
	<6.1.0.6.2.20040630092512.07a03b68@mailhost.blackmesacapital.com>
Message-ID: <loom.20040630T173928-2@post.gmane.org>

Tony Plate <tplate <at> blackmesacapital.com> writes:

> I get the best read performance out of S-plus by using 
> a homegrown binary file format with each column stored in a contiguous 
> block of memory and meta data (i.e., column types and dimensions) stored at 
> the start of the file.  The S-plus read function reads the columns one at a 
> time using readRaw(). One would be able to do something similar in R.  If 
> you have to read from a text file, then, as others have suggested, writing 
> a C program wouldn't be that hard, as long as you make the format inflexible.


At one time there was a program around (independent of R) that would read in
a file in numerous formats including text and write out an R .rda file
(among other formats).  The .rda file could then be rapidly read into R
using the R load command.  Unfortunately the author withdrew it and it is 
no longer available.

I suspect that if someone came up with such a program in C code but to keep 
it simple just restricted it to ASCII input files with a minimum number 
of data types and .rda or homegrown binary output it would be of 
general interest to the R community.



From cpwww at comcast.net  Wed Jun 30 18:33:22 2004
From: cpwww at comcast.net (Coburn Watson)
Date: Wed, 30 Jun 2004 09:33:22 -0700
Subject: [R] Question about plotting related to roll-up
Message-ID: <200406300933.22951.cpwww@comcast.net>

Hello R'ers,

I have a large set of data which has many y samples for each unit x.  The data 
might look like:

Seconds	Response_time
----------    ----------------
0			0.150
0			0.202
0			0.065
1			0.110
1			0.280
2			0.230
2			0.156
3			0.070
3			0.185
3			0.255
3			0.311
3			0.120
4			
.... and so on

When I do a basic plot with type=l or the default of points it obviously plots 
every point.  What I would like to do is generate a line plot where the 
samples for each second are rolled up, averged and plotted with a bar which 
represents either std dev or some other aspect of variance.  Can someone 
recommend a plotting mechanism to achieve this?  I have adding lines using 
some of the smoothing functions but seem unable to remove the original plot 
line which is drawn (is there a way to just plot the data as feed through the 
smoothing function without the original data?).

Please remove "_nospam" from the email address to reply directly to me.

Thanks,

Coburn Watson
Software Performance Engineering
DST Innovis



From rivin at euclid.math.temple.edu  Wed Jun 30 18:08:26 2004
From: rivin at euclid.math.temple.edu (Igor Rivin)
Date: Wed, 30 Jun 2004 12:08:26 -0400
Subject: [R] naive question
In-Reply-To: <6.1.0.6.2.20040630092512.07a03b68@mailhost.blackmesacapital.com>
References: <3A822319EB35174CA3714066D590DCD504AF7FB2@usrymx25.merck.com>
	<40E2070E.mailCK411OTFQ@euclid.math.temple.edu>
	<6.1.0.6.2.20040630092512.07a03b68@mailhost.blackmesacapital.com>
Message-ID: <40E2E57A.mailLI1113AM4@euclid.math.temple.edu>


Thank you! It's interesting about S-Plus, since they apparently try to support
work with much larger data sets by writing everything out to disk (thus getting
around the, eg, address space limitations, I guess), so it is a little surprising
that they did not tweak the I/O more...

	Thanks again,

		Igor



From i.visser at uva.nl  Wed Jun 30 18:07:40 2004
From: i.visser at uva.nl (Ingmar Visser)
Date: Wed, 30 Jun 2004 18:07:40 +0200
Subject: [R] Mac OS  X 10.2.8 versus 10.3.4 package load failure
Message-ID: <BD08B1EC.4226%i.visser@uva.nl>


Hello All,

I have built a package depmix and build a source package file from it for
distribution. I have done all this on the following platform:

platform powerpc-apple-darwin6.8
arch     powerpc   
os       darwin6.8 
system   powerpc, darwin6.8
status             
major    1         
minor    9.0       
year     2004      
month    04        
day      12        
language R     

When I install the package using the menu Packages > Install from local file
> From source package file (tar.gz) the package installs without problems.

However when I do the same another MAC OS X (10.2.8), I get the following
error message:

Error in dyn.load(x, as.logical(local), as.logical(now)) :
    unable to
load shared library "/Users/564/Library/R/library/depmix/libs/depmix.so":

dlcompat: dyld: /Library/Frameworks/R.framework/Resources/bin/R.bin
Undefined 
symbols:
_btowc
_iswctype
_mbsrtowcs
_towlower
_towupper
_wcscoll
_wcsftime

_wcslen
_wcsrtombs
_wcsxfrm
_wctob
_wctype
_wmemchr
_wmemcmp
_wmemcpy
_wmemm
ove
_wmemset
Error in library(depmix) : .First.lib failed

Can anyone help me understand what the problem is here and how to solve it?



From ggrothendieck at myway.com  Wed Jun 30 18:27:29 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 30 Jun 2004 16:27:29 +0000 (UTC)
Subject: [R] Question about plotting related to roll-up
References: <200406300933.22951.cpwww@comcast.net>
Message-ID: <loom.20040630T182535-894@post.gmane.org>


boxplot(Response_time ~ seconds, data = my.data.frame)



Coburn Watson <cpwww <at> comcast.net> writes:

: 
: Hello R'ers,
: 
: I have a large set of data which has many y samples for each unit x.  The 
data 
: might look like:
: 
: Seconds	Response_time
: ----------    ----------------
: 0			0.150
: 0			0.202
: 0			0.065
: 1			0.110
: 1			0.280
: 2			0.230
: 2			0.156
: 3			0.070
: 3			0.185
: 3			0.255
: 3			0.311
: 3			0.120
: 4			
: .... and so on
: 
: When I do a basic plot with type=l or the default of points it obviously 
plots 
: every point.  What I would like to do is generate a line plot where the 
: samples for each second are rolled up, averged and plotted with a bar which 
: represents either std dev or some other aspect of variance.  Can someone 
: recommend a plotting mechanism to achieve this?  I have adding lines using 
: some of the smoothing functions but seem unable to remove the original plot 
: line which is drawn (is there a way to just plot the data as feed through 
the 
: smoothing function without the original data?).
: 
: Please remove "_nospam" from the email address to reply directly to me.
: 
: Thanks,
: 
: Coburn Watson
: Software Performance Engineering
: DST Innovis
: 
: ______________________________________________
: R-help <at> stat.math.ethz.ch mailing list
: https://www.stat.math.ethz.ch/mailman/listinfo/r-help
: PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
: 
:



From ripley at stats.ox.ac.uk  Wed Jun 30 18:36:26 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 30 Jun 2004 17:36:26 +0100 (BST)
Subject: [R] Mac OS  X 10.2.8 versus 10.3.4 package load failure
In-Reply-To: <BD08B1EC.4226%i.visser@uva.nl>
Message-ID: <Pine.LNX.4.44.0406301730390.18046-100000@gannet.stats>

The problem is that you compiled the code with wide characters, and on 
your 10.2.8 the support functions are not compiled into the R binary 
(which is what I would expect).

As to why you are getting wide chars, I suggest you ask on the R-SIG-Mac
list (https://www.stat.math.ethz.ch/mailman/listinfo/r-sig-mac), as this 
is very specialized.

On Wed, 30 Jun 2004, Ingmar Visser wrote:

> 
> Hello All,
> 
> I have built a package depmix and build a source package file from it for
> distribution. I have done all this on the following platform:
> 
> platform powerpc-apple-darwin6.8
> arch     powerpc   
> os       darwin6.8 
> system   powerpc, darwin6.8
> status             
> major    1         
> minor    9.0       
> year     2004      
> month    04        
> day      12        
> language R     
> 
> When I install the package using the menu Packages > Install from local file
> > From source package file (tar.gz) the package installs without problems.
> 
> However when I do the same another MAC OS X (10.2.8), I get the following
> error message:
> 
> Error in dyn.load(x, as.logical(local), as.logical(now)) :
>     unable to
> load shared library "/Users/564/Library/R/library/depmix/libs/depmix.so":
> 
> dlcompat: dyld: /Library/Frameworks/R.framework/Resources/bin/R.bin
> Undefined 
> symbols:
> _btowc
> _iswctype
> _mbsrtowcs
> _towlower
> _towupper
> _wcscoll
> _wcsftime
> 
> _wcslen
> _wcsrtombs
> _wcsxfrm
> _wctob
> _wctype
> _wmemchr
> _wmemcmp
> _wmemcpy
> _wmemm
> ove
> _wmemset
> Error in library(depmix) : .First.lib failed
> 
> Can anyone help me understand what the problem is here and how to solve it?

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From roebuck at odin.mdacc.tmc.edu  Wed Jun 30 18:44:51 2004
From: roebuck at odin.mdacc.tmc.edu (Paul Roebuck)
Date: Wed, 30 Jun 2004 11:44:51 -0500 (CDT)
Subject: [R] MacOS X binaries won't install
In-Reply-To: <135A2A15-CA46-11D8-934D-000A95E26AF6@earthlink.net>
References: <135A2A15-CA46-11D8-934D-000A95E26AF6@earthlink.net>
Message-ID: <Pine.OSF.4.58.0406301138390.379156@odin.mdacc.tmc.edu>

On Tue, 29 Jun 2004, Ruben Solis wrote:

> I've tried installing the MacOS X binaries for R available at:
>
> http://www.bioconductor.org/CRAN/
>
> I'm running MacOS X version 10.2.8.
>
> I get a message indicating the installation is successful, but when I
> double-click on the R icon that shows up in my Applications folder, the
> application seems to try to open but closes immediately.
>
> I looked for  /Library/Frameworks/R.framework (by typing ls
> /Library/Frameworks) and it does not appear.  A global search for
> R.framework yields no results, so it seems that the installation is not
> working. (I was going to try command line execution.)

Check your log file for related error messages.
Can also inspect the package contents to ensure scripts
have execute permission as that would also cause the behavior
you describe.

----------------------------------------------------------
SIGSIG -- signature too long (core dumped)



From ramasamy at cancer.org.uk  Wed Jun 30 18:45:17 2004
From: ramasamy at cancer.org.uk (Adaikalavan Ramasamy)
Date: Wed, 30 Jun 2004 17:45:17 +0100
Subject: [R] Question about plotting related to roll-up
In-Reply-To: <200406300933.22951.cpwww@comcast.net>
References: <200406300933.22951.cpwww@comcast.net>
Message-ID: <1088613898.3055.99.camel@vpn202001.lif.icnet.uk>

grp  <- rep(1:5, each=3)
resp <- rnorm(15)

mu <- tapply(resp, grp, mean)
s  <- tapply(resp, grp, sd)
stopifnot( identical( names(mu), names(s) ) )

LCL <- mu - 2*s  # lower confidence limit
UCL <- mu + 2*s
Here I choose 2 as we expect 95% of the data to fall under 4 sd.


# Type 1
plot(names(mu), mu, type="l", ylim=c( min(LCL), max(UCL) ))
lines(names(mu), UCL, lty=3)
lines(names(mu), LCL, lty=3)

Your group must contain only numeric values. Otherwise, you will need to
use a numerical coding followed by mtext() with proper characters.


# Type 2
plot(names(mu), mu, type="p", ylim=c( min(LCL), max(UCL) )) 
arrows( as.numeric(names(mu)), LCL, as.numeric(names(mu)), UCL, code=3,
angle=90, length=0.1 )


On Wed, 2004-06-30 at 17:33, Coburn Watson wrote:
> Hello R'ers,
> 
> I have a large set of data which has many y samples for each unit x.  The data 
> might look like:
> 
> Seconds	Response_time
> ----------    ----------------
> 0			0.150
> 0			0.202
> 0			0.065
> 1			0.110
> 1			0.280
> 2			0.230
> 2			0.156
> 3			0.070
> 3			0.185
> 3			0.255
> 3			0.311
> 3			0.120
> 4			
> .... and so on
> 
> When I do a basic plot with type=l or the default of points it obviously plots 
> every point.  What I would like to do is generate a line plot where the 
> samples for each second are rolled up, averged and plotted with a bar which 
> represents either std dev or some other aspect of variance.  Can someone 
> recommend a plotting mechanism to achieve this?  I have adding lines using 
> some of the smoothing functions but seem unable to remove the original plot 
> line which is drawn (is there a way to just plot the data as feed through the 
> smoothing function without the original data?).
> 
> Please remove "_nospam" from the email address to reply directly to me.
> 
> Thanks,
> 
> Coburn Watson
> Software Performance Engineering
> DST Innovis
> 
> ______________________________________________
> R-help at stat.math.ethz.ch mailing list
> https://www.stat.math.ethz.ch/mailman/listinfo/r-help
> PLEASE do read the posting guide! http://www.R-project.org/posting-guide.html
>



From tplate at blackmesacapital.com  Wed Jun 30 19:23:37 2004
From: tplate at blackmesacapital.com (Tony Plate)
Date: Wed, 30 Jun 2004 11:23:37 -0600
Subject: [R] naive question
In-Reply-To: <40E2E57A.mailLI1113AM4@euclid.math.temple.edu>
References: <3A822319EB35174CA3714066D590DCD504AF7FB2@usrymx25.merck.com>
	<40E2070E.mailCK411OTFQ@euclid.math.temple.edu>
	<6.1.0.6.2.20040630092512.07a03b68@mailhost.blackmesacapital.com>
	<40E2E57A.mailLI1113AM4@euclid.math.temple.edu>
Message-ID: <6.1.0.6.2.20040630102959.07a15818@mailhost.blackmesacapital.com>

To be careful, there's lots more to I/O than the functions read.table() & 
scan() -- I was only commenting on those, and no inference should be made 
about other aspects of S-plus I/O based on those comments!

I suspect that what has happened is that memory, CPU speed, and I/O speed 
have evolved at different rates, so what used to be acceptable code in 
read.table() (in both R and S-plus) is now showing its limitations and has 
reached the point where it can take half an hour to read in, on a 
readily-available computer, the largest data table that can be comfortably 
handled.  I'm speculating, but 10 years ago,  on a readily available 
computer, did it take half an hour to read in the largest data table that 
could be comfortably handled in S-plus or R?  People who encounter this now 
are surprised and disappointed, and IMHO, somewhat justifiably so.  The 
fact that R is an open source volunteer project suggests that the time is 
ripe for one of those disappointed people to fix the matter and contribute 
the function read.table.fast()!

-- Tony Plate

At Wednesday 10:08 AM 6/30/2004, Igor Rivin wrote:

>Thank you! It's interesting about S-Plus, since they apparently try to support
>work with much larger data sets by writing everything out to disk (thus 
>getting
>around the, eg, address space limitations, I guess), so it is a little 
>surprising
>that they did not tweak the I/O more...
>
>         Thanks again,
>
>                 Igor
>



From rivin at euclid.math.temple.edu  Wed Jun 30 19:56:43 2004
From: rivin at euclid.math.temple.edu (rivin@euclid.math.temple.edu)
Date: Wed, 30 Jun 2004 13:56:43 -0400 (EDT)
Subject: [R] naive question
In-Reply-To: <6.1.0.6.2.20040630102959.07a15818@mailhost.blackmesacapital.com>
References: <3A822319EB35174CA3714066D590DCD504AF7FB2@usrymx25.merck.com>
	<40E2070E.mailCK411OTFQ@euclid.math.temple.edu>
	<6.1.0.6.2.20040630092512.07a03b68@mailhost.blackmesacapital.com>
	<40E2E57A.mailLI1113AM4@euclid.math.temple.edu>
	<6.1.0.6.2.20040630102959.07a15818@mailhost.blackmesacapital.com>
Message-ID: <54156.205.228.12.40.1088618203.squirrel@www.math.temple.edu>

> I suspect that what has happened is that memory, CPU speed, and I/O
> speed  have evolved at different rates, so what used to be acceptable
> code in  read.table() (in both R and S-plus) is now showing its
> limitations and has  reached the point where it can take half an hour to
> read in, on a  readily-available computer, the largest data table that
> can be comfortably  handled.  I'm speculating, but 10 years ago,  on a
> readily available  computer, did it take half an hour to read in the
> largest data table that  could be comfortably handled in S-plus or R?

I did not use R ten years ago, but "reasonable" RAM amounts have
multiplied by roughly a factor of 10 (from 128Mb to 1Gb), CPU speeds have
gone up by a factor of 30 (from 90Mhz to 3Ghz), and disk space availabilty
has gone up probably by a factor of 10. So, unless the I/O performance
scales nonlinearly with size (a bit strange but not inconsistent with my R
experiments), I would think that things should have gotten faster (by the
wall clock, not slower). Of course, it is possible that the other
components of the R system have been worked on more -- I am not equipped
to comment...

  Igor

>



From thchung at tgen.org  Wed Jun 30 20:07:29 2004
From: thchung at tgen.org (Tae-Hoon Chung)
Date: Wed, 30 Jun 2004 11:07:29 -0700
Subject: [R] memory utilization question
Message-ID: <59CB2EE8-CAC0-11D8-8961-000A95B43CDE@tgen.org>

Hi, all;

I have a question on memory utilization of R.
Does R use only RAM memory or can I make it use virtual memory?
Currently, I am using Mac OS X 10.3.3 with 1.5 GB RAM.
However, I need more memory for some large size problem right now.

Thanks in advance,
Tae-Hoon Chung, Ph.D

Post-doctoral Research Fellow
Molecular Diagnostics and Target Validation Division
Translational Genomics Research Institute
1275 W Washington St, Tempe AZ 85281 USA
Phone: 602-343-8724



From Thomas.Ribarits at tuwien.ac.at  Wed Jun 30 20:25:42 2004
From: Thomas.Ribarits at tuwien.ac.at (Thomas Ribarits)
Date: Wed, 30 Jun 2004 20:25:42 +0200
Subject: [R] interval regression
Message-ID: <000101c45ecf$a7083270$0fab8280@eos.tuwien.ac.at>

An embedded and charset-unspecified text was scrubbed...
Name: not available
Url: https://stat.ethz.ch/pipermail/r-help/attachments/20040630/bae4cc47/attachment.pl

From daniel at sintesys.com.ar  Wed Jun 30 20:28:47 2004
From: daniel at sintesys.com.ar (daniel@sintesys.com.ar)
Date: Wed, 30 Jun 2004 15:28:47 -0300 (ART)
Subject: [R] Developing functions
In-Reply-To: <40E2DC80.7040903@hotmail.com>
References: <200406290944390154.05657F06@mail.math.fu-berlin.de>
	<40E2DC80.7040903@hotmail.com>
Message-ID: <56014.170.155.3.5.1088620127.squirrel@www.sintesys.com.ar>

Hi,
I??m new in R. I??m working with similarity coefficients for clustering
items. I created one function (coef), to calculate the coefficients from
two pairs of vectors and then, as an example, the function
simple_matching,
taking a data.frame(X) and using coef in a for cicle.
It works, but I believe it is a bad way to do so (I believe the for cicle
is not necessary). Somebody can suggest anything better.
Thanks
Daniel Rozengardt

coef<-function(x1,x2){a<-sum(ifelse(x1==1&x2==1,1,0));
b<-sum(ifelse(x1==1&x2==0,1,0));
c<-sum(ifelse(x1==0&x2==1,1,0));
d<-sum(ifelse(x1==0&x2==0,1,0));
ret<-cbind(a,b,c,d);
ret
}

simple_matching<-function(X) {
ret<-matrix(ncol=dim(X)[1],nrow=dim(X)[1]);
diag(ret)<-1;
for (i in 2:length(X[,1])) {
	for (j in i:length(X[,1])) {
	vec<-coef(X[i-1,],X[j,]);
	result<-(vec[1]+vec[3])/sum(vec);
	ret[i-1,j]<-result;
	ret[j,i-1]<-result}};
ret}



From p.dalgaard at biostat.ku.dk  Wed Jun 30 20:31:20 2004
From: p.dalgaard at biostat.ku.dk (Peter Dalgaard)
Date: 30 Jun 2004 20:31:20 +0200
Subject: [R] naive question
In-Reply-To: <54156.205.228.12.40.1088618203.squirrel@www.math.temple.edu>
References: <3A822319EB35174CA3714066D590DCD504AF7FB2@usrymx25.merck.com>
	<40E2070E.mailCK411OTFQ@euclid.math.temple.edu>
	<6.1.0.6.2.20040630092512.07a03b68@mailhost.blackmesacapital.com>
	<40E2E57A.mailLI1113AM4@euclid.math.temple.edu>
	<6.1.0.6.2.20040630102959.07a15818@mailhost.blackmesacapital.com>
	<54156.205.228.12.40.1088618203.squirrel@www.math.temple.edu>
Message-ID: <x2brj0rj7r.fsf@biostat.ku.dk>

<rivin at euclid.math.temple.edu> writes:

> I did not use R ten years ago, but "reasonable" RAM amounts have
> multiplied by roughly a factor of 10 (from 128Mb to 1Gb), CPU speeds have
> gone up by a factor of 30 (from 90Mhz to 3Ghz), and disk space availabilty
> has gone up probably by a factor of 10. So, unless the I/O performance
> scales nonlinearly with size (a bit strange but not inconsistent with my R
> experiments), I would think that things should have gotten faster (by the
> wall clock, not slower). Of course, it is possible that the other
> components of the R system have been worked on more -- I am not equipped
> to comment...

I think your RAM calculation is a bit off. in late 1993, 4MB systems
were the standard PC, with 16 or 32 MB on high-end workstations.
Comparable figures today are probably  256MB for the entry-level PC
and a couple GB in the high end. So that's more like a factor of 64.
On the other hand, CPU's have changed by more than the clock speed; in
particular, the number of clock cycles per FP calculation has
decreased considerably and is currently less than one in some
circumstances. 

-- 
   O__  ---- Peter Dalgaard             Blegdamsvej 3  
  c/ /'_ --- Dept. of Biostatistics     2200 Cph. N   
 (*) \(*) -- University of Copenhagen   Denmark      Ph: (+45) 35327918
~~~~~~~~~~ - (p.dalgaard at biostat.ku.dk)             FAX: (+45) 35327907



From ripley at stats.ox.ac.uk  Wed Jun 30 20:32:16 2004
From: ripley at stats.ox.ac.uk (Prof Brian Ripley)
Date: Wed, 30 Jun 2004 19:32:16 +0100 (BST)
Subject: [R] interval regression
In-Reply-To: <000101c45ecf$a7083270$0fab8280@eos.tuwien.ac.at>
Message-ID: <Pine.LNX.4.44.0406301930370.16135-100000@gannet.stats>

I believe from your description that this is interval-censored survival
and can be handled by survreg.

It would also be very easy to amend polr to do this.

On Wed, 30 Jun 2004, Thomas Ribarits wrote:

> does anyone have a quick answer to the question of how to carry out
> interval regression in R. I have found "ordered logit" and "ordered
> probit" as well as multinomial logit etc. The thing is, though, that I
> want to apply logit/probit to interval-coded data and I know the cell
> limits which are used to turn the quantitative response into an ordered
> factor. Hence, it does not make sense to estimate the cell limits (e.g.
> in zeta for the function polr).

-- 
Brian D. Ripley,                  ripley at stats.ox.ac.uk
Professor of Applied Statistics,  http://www.stats.ox.ac.uk/~ripley/
University of Oxford,             Tel:  +44 1865 272861 (self)
1 South Parks Road,                     +44 1865 272866 (PA)
Oxford OX1 3TG, UK                Fax:  +44 1865 272595



From angel_lul at hotmail.com  Wed Jun 30 20:37:01 2004
From: angel_lul at hotmail.com (Angel Lopez)
Date: Wed, 30 Jun 2004 20:37:01 +0200
Subject: [R] pca with missing values
In-Reply-To: <Pine.LNX.4.44.0406301309080.15704-100000@gannet.stats>
References: <Pine.LNX.4.44.0406301309080.15704-100000@gannet.stats>
Message-ID: <40E3084D.2080100@hotmail.com>

Thanks for the pointers.
I've read them with care but I don't seem capable of making it work.
For example, if I do:

data(USArrests)
USArrests2<-USArrests
USArrests2[1,1]<-NA
princomp(USArrests2, cor = TRUE, na.action = "na.omit")

I get the error message:
Error in cov.wt(z) : x must contain finite values only

I've tried changing the 'options' na.action and using other values than 
na.omit with no success.

The only way that I can make it work in some way was if I did:

USArrestsNA<-na.omit(USArrests)
princomp(USArrestsNA, cor = TRUE)


I've also obtained the same by giving the correlation matrix instead of 
the data frame:
princomp(covmat=cor(USArrestsNA))
Both solutions do the job by not using the row with the NA.
After more reading I thought I would get the same result by doing:
princomp(covmat=cor(USArrests2,use="complete.obs"))
but the result is slightly different. I can not manage to understand the 
difference.
Can someone give me some more light to keep going?
P.S:Using the solution above with na.omit would not be very good in my 
real world problem because it is relatively common to have an NA in a 
row. Maybe using 
princomp(covmat=cor(USArrests2,use="pairwise.complete.obs"))
would be a solution but I would like to understand the above before 
doing this next step.
Thanks,
Ange


Prof Brian Ripley wrote:
> On Wed, 30 Jun 2004, Angel Lopez wrote:
> 
> 
>>I need to perform a principal components analysis on a matrix with 
>>missing values.
>>I've searched the R web site but didn't manage to find anything.
> 
> 
>     ?princomp
> 
> has a description of an na.action argument, and 
> 
>     help.search("missing values")
> 
> comes up with several relevant entries.
>



From jgentry at jimmy.harvard.edu  Wed Jun 30 21:08:51 2004
From: jgentry at jimmy.harvard.edu (Jeff Gentry)
Date: Wed, 30 Jun 2004 15:08:51 -0400 (EDT)
Subject: [R] MacOS X binaries won't install
In-Reply-To: <Pine.OSF.4.58.0406301138390.379156@odin.mdacc.tmc.edu>
Message-ID: <Pine.SOL.4.20.0406301506370.19147-100000@jaws.dfci.harvard.edu>


Sorry, I missed the original message, so piggybacking off of a reply.

> On Tue, 29 Jun 2004, Ruben Solis wrote:
> > I've tried installing the MacOS X binaries for R available at:
> > http://www.bioconductor.org/CRAN/
> > I'm running MacOS X version 10.2.8.

Since this is coming out of our mirror (bioconductor), I'm curious if the
same problem occurs with the packages available at the other mirrors, or
if this is specific to the BioC mirror?

Thanks
-J



From ivo_welch-Rstat at mailblocks.com  Wed Jun 30 22:01:25 2004
From: ivo_welch-Rstat at mailblocks.com (ivo_welch-Rstat@mailblocks.com)
Date: Wed, 30 Jun 2004 13:01:25 -0700
Subject: [R] spam warning
Message-ID: <200406302001.i5UK1RDG011714@hypatia.math.ethz.ch>


hi chaps:  I just found out that this R post list is actively harvested 
by some spammers.  I used this account exclusively for r-help posts, 
and promptly received a nice email with links to nigeria, india, and 
china.  this is not a good thing.  can we obfuscate the email addresses 
of posters, so that at least automatic harvesting won't work?

sincerely,  /ivo welch
---
ivo welch
professor of finance and economics
brown / nber / yale



From ggrothendieck at myway.com  Wed Jun 30 22:05:15 2004
From: ggrothendieck at myway.com (Gabor Grothendieck)
Date: Wed, 30 Jun 2004 16:05:15 -0400 (EDT)
Subject: [R] Developing functions
Message-ID: <20040630200515.1645D12D12@mprdmxin.myway.com>



Without trying to understand your code in detail let me just 
assume you are trying to create a matrix, ret, whose i,j-th 
entry is some function, f, of row i of X and row j of X.

In that case this should do it:

	apply(X,1,function(x)apply(X,1,function(y)f(x,y)))


Date:   Wed, 30 Jun 2004 15:28:47 -0300 (ART) 
From:   <daniel at sintesys.com.ar>
To:   <r-help at stat.math.ethz.ch> 
Subject:   [R] Developing functions 

 
Hi,
Im new in R. Im working with similarity coefficients for clustering
items. I created one function (coef), to calculate the coefficients from
two pairs of vectors and then, as an example, the function
simple_matching,
taking a data.frame(X) and using coef in a for cicle.
It works, but I believe it is a bad way to do so (I believe the for cicle
is not necessary). Somebody can suggest anything better.
Thanks
Daniel Rozengardt

coef<-function(x1,x2){a<-sum(ifelse(x1==1&x2==1,1,0));
b<-sum(ifelse(x1==1&x2==0,1,0));
c<-sum(ifelse(x1==0&x2==1,1,0));
d<-sum(ifelse(x1==0&x2==0,1,0));
ret<-cbind(a,b,c,d);
ret
}

simple_matching<-function(X) {
ret<-matrix(ncol=dim(X)[1],nrow=dim(X)[1]);
diag(ret)<-1;
for (i in 2:length(X[,1])) {
     for (j in i:length(X[,1])) {
     vec<-coef(X[i-1,],X[j,]);
     result<-(vec[1]+vec[3])/sum(vec);
     ret[i-1,j]<-result;
     ret[j,i-1]<-result}};
ret}



From rivin at euclid.math.temple.edu  Wed Jun 30 22:25:53 2004
From: rivin at euclid.math.temple.edu (rivin@euclid.math.temple.edu)
Date: Wed, 30 Jun 2004 16:25:53 -0400 (EDT)
Subject: [R] naive question
In-Reply-To: <x2brj0rj7r.fsf@biostat.ku.dk>
References: <3A822319EB35174CA3714066D590DCD504AF7FB2@usrymx25.merck.com>
	<40E2070E.mailCK411OTFQ@euclid.math.temple.edu>
	<6.1.0.6.2.20040630092512.07a03b68@mailhost.blackmesacapital.com>
	<40E2E57A.mailLI1113AM4@euclid.math.temple.edu>
	<6.1.0.6.2.20040630102959.07a15818@mailhost.blackmesacapital.com>
	<54156.205.228.12.40.1088618203.squirrel@www.math.temple.edu>
	<x2brj0rj7r.fsf@biostat.ku.dk>
Message-ID: <61845.199.89.64.40.1088627153.squirrel@www.math.temple.edu>

> <rivin at euclid.math.temple.edu> writes:
>
>> I did not use R ten years ago, but "reasonable" RAM amounts have
>> multiplied by roughly a factor of 10 (from 128Mb to 1Gb), CPU speeds
>> have gone up by a factor of 30 (from 90Mhz to 3Ghz), and disk space
>> availabilty has gone up probably by a factor of 10. So, unless the I/O
>> performance scales nonlinearly with size (a bit strange but not
>> inconsistent with my R experiments), I would think that things should
>> have gotten faster (by the wall clock, not slower). Of course, it is
>> possible that the other components of the R system have been worked on
>> more -- I am not equipped to comment...
>
> I think your RAM calculation is a bit off. in late 1993, 4MB systems
> were the standard PC, with 16 or 32 MB on high-end workstations.

I beg to differ. In 1989, Mac II came standard with 8MB, NeXT came
standard with 16MB. By 1994, 16MB was pretty much standard on good quality
(= Pentium, of which the 90Mhz was the first example) PCs, with 32Mb
pretty common (though I suspect that most R/S-Plus users were on SUNs,
which were somewhat more plushly equipped).

> Comparable figures today are probably  256MB for the entry-level PC and
> a couple GB in the high end. So that's more like a factor of 64. On the
> other hand, CPU's have changed by more than the clock speed; in
> particular, the number of clock cycles per FP calculation has
> decreased considerably and is currently less than one in some
> circumstances.
>
I think that FP performance has increased more than integer performance,
which has pretty much kept pace with the clock speed. The compilers have
also improved a bit...

  Igor



From james.holtman at convergys.com  Wed Jun 30 22:38:24 2004
From: james.holtman at convergys.com (james.holtman@convergys.com)
Date: Wed, 30 Jun 2004 16:38:24 -0400
Subject: [R] naive question
Message-ID: <OFCE717DDB.6FB9F8B6-ON85256EC3.0070AE72@nd.convergys.com>





It is amazing the amount of time that has been spent on this issue.  In
most cases, if you do some timing studies using 'scan', you will find that
you can read some quite large data structures in a reasonable time.  If you
initial concern was having to wait 10 minutes to have your data read in,
you could have read in quite a few data sets by now.

When comparing speeds/feeds of processors, you also have to consider what
it being done on them.  Back in the "dark ages" we had a 1 MIP computer
with 4M of memory handling input from 200 users on a transaction system.
Today I need a 1GHZ computer with 512M to just handle me.  Now true, I am
doing a lot different processing on it.

With respect to I/O, you have to consider what is being read in and how it
is converted.  Each system/program has different requirements.  I have some
applications (running on a laptop) that can read in approximately 100K rows
of data per second (of course they are already binary).  On the other hand,
I can easily slow that down to 1K rows per second if I do not specify the
correct parameters to 'read.table'.

So go back and take a look at what you are doing, and instrument your code
to see where time is being spent.  The nice thing about R is that there are
a number of ways of approaching a solution and it you don't like the timing
of one way, try another.  That is half the fun of using R.
__________________________________________________________
James Holtman        "What is the problem you are trying to solve?"
Executive Technical Consultant  --  Office of Technology, Convergys
james.holtman at convergys.com
+1 (513) 723-2929


                                                                                                                   
                      <rivin at euclid.math.te                                                                        
                      mple.edu>                    To:       <p.dalgaard at biostat.ku.dk>                            
                      Sent by:                     cc:       r-help at stat.math.ethz.ch,                             
                      r-help-bounces at stat.m         tplate at blackmesacapital.com, rivin at euclid.math.temple.edu      
                      ath.ethz.ch                  Subject:  Re: [R] naive question                                
                                                                                                                   
                                                                                                                   
                      06/30/2004 16:25                                                                             
                                                                                                                   
                                                                                                                   




> <rivin at euclid.math.temple.edu> writes:
>
>> I did not use R ten years ago, but "reasonable" RAM amounts have
>> multiplied by roughly a factor of 10 (from 128Mb to 1Gb), CPU speeds
>> have gone up by a factor of 30 (from 90Mhz to 3Ghz), and disk space
>> availabilty has gone up probably by a factor of 10. So, unless the I/O
>> performance scales nonlinearly with size (a bit strange but not
>> inconsistent with my R experiments), I would think that things should
>> have gotten faster (by the wall clock, not slower). Of course, it is
>> possible that the other components of the R system have been worked on
>> more -- I am not equipped to comment...
>
> I think your RAM calculation is a bit off. in late 1993, 4MB systems
> were the standard PC, with 16 or 32 MB on high-end workstations.

I beg to differ. In 1989, Mac II came standard with 8MB, NeXT came
standard with 16MB. By 1994, 16MB was pretty much standard on good quality
(= Pentium, of which the 90Mhz was the first example) PCs, with 32Mb
pretty common (though I suspect that most R/S-Plus users were on SUNs,
which were somewhat more plushly equipped).

> Comparable figures today are probably  256MB for the entry-level PC and
> a couple GB in the high end. So that's more like a factor of 64. On the
> other hand, CPU's have changed by more than the clock speed; in
> particular, the number of clock cycles per FP calculation has
> decreased considerably and is currently less than one in some
> circumstances.
>
I think that FP performance has increased more than integer performance,
which has pretty much kept pace with the clock speed. The compilers have
also improved a bit...

  Igor

______________________________________________
R-help at stat.math.ethz.ch mailing list
https://www.stat.math.ethz.ch/mailman/listinfo/r-help
PLEASE do read the posting guide!
http://www.R-project.org/posting-guide.html



From egcp at hotmail.com  Wed Jun 30 23:04:52 2004
From: egcp at hotmail.com (E GCP)
Date: Wed, 30 Jun 2004 21:04:52 +0000
Subject: [R] rgl installation problems
Message-ID: <BAY22-F179EVP9BOJQv00024b75@hotmail.com>

Thanks. Adding the -shared to the Makeconf file fixed the problem. I 
installed R from R-1.9.1-0.fdr.2.rh90.i386.rpm , but for some reason the 
flag to share libraries was never set in SHLIB_CXXLDFLAGS.  
rgl_0.64-13.tar.gz now installed without problems.

Thanks again,
Enrique

>On Wed, 2004-06-30 at 00:52, E GCP wrote:
> > Thanks for your replies. I do not HTML-ize my mail, but free email 
>accounts > do that and there is not a switch to turn it off.  I apologize 
>in advance.
> > > I installed R from the redhat package provided by Martyn Plummer. It > 
>installed fine and without problems. I can use R and have installed and 
>used > other packages within R without any problems whatsoever. I do not 
>think the > problem is with R or its installation.   I do think there is a 
>problem with > the installation of rgl_0.64-13.tar.gz on RedHat 9 (linux).  
>So, if there is > anybody out there who has installed succesfully 
>rgl_0.64-13.tar.gz on RedHat > 9, I would like to know how.
>
>This is a little strange.  I'm now building RPMS for older Red Hat
>versions on FC2 using a tool called Mach.  There is a possibility that
>there is some configuration problem, but I can't see it. As Brian has
>pointed out, you are missing the crucial "-shared" flag when building
>the shared library for rgl. This comes from the line
>
>SHLIB_CXXLDFLAGS = -shared
>
>in the file /usr/lib/R/etc/Makeconf.  This is present in my latest RPM
>for Red Hat ( R-1.9.1-0.fdr.2.rh90.i386.rpm ) so I don't know why it
>isn't working for you.
>
>Check that you do have the latest RPM and that you don't have a locally
>built version of R in /usr/local since this will have precedence on your
>PATH.



